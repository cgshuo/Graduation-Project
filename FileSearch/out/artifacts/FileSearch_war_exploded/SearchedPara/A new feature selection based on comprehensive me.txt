 1. Introduction est Neighbors (KNN) ( Cover &amp; Hart, 1967 ), Decision tree, Rocchio ( Sebastiani, 2002 ), etc. dimensionality reduction has been a major research area.  X  Zhu, &amp; Zhang, 2011 ), and so on.

Among the above-mentioned feature selection algorithms, Document Frequency (DF) only measures the significance of a chines (SVMs) on three benchmark corpora (20-Newgroups, Reuters-21578 and WebKB), and compared it with six feature DF and OCFS, and is comparable with GINI and CHI when Support Vector Machines is used. sion and the future work direction are provided in the last section. 2. Related work are detailed as follows. 2.1. Improved Gini index fined as follows: where P ( t k | c i ) is the probability that the feature t feature t k belongs to category c i when the feature t k occurs. 2.2. Information Gain 1997 ). The Information Gain of a given feature t k with respect to the class c c rization. Information Gain of a feature t k toward a category c containing the term t over the total number of documents ( Youn &amp; Jeong, 2009 ). 2.3. Chi-square matical statistics. We used Chi-square testing to determine the independence of the feature t v 2 ( t k , c i ) = 0, the feature t k and the class c i are independent, so the feature t
Otherwise, the greater the value of the v 2 ( t k , c i ) is, the more category information the feature t is defined as follows: where N is the total number of messages; a ki is the frequency that feature t feature t k occurs and does not belong to category c i ; c t ; d ki is the number of times neither c i nor t k occurs. 2.4. Document Frequency
Document Frequency (DF) is a simple and effective feature selection method, and it computes the number of documents term is retained ( Sebastiani, 2002 ). The document frequency of a term is calculated as follows: 2.5. Orthogonal Centroid Feature Selection t is calculated as follows: to the total number of categories in the corpus. 2.6. DIA association factor the probability for the category c i to be assigned to a document if this document contains the term t factor determines the significance of the occurrence of the term t is defined by where P ( c i | t k ) refers to the conditional probability that the feature t 3. Algorithm description 3.1. Motivation tor space model was regarded as a word-to-document feature-appearance matrix where rows are the features and columns the term-to-category matrix. An interesting phenomenon was paid attention to during comparing the performance of the per. It comprehensively measures the significance of a term both in inter-category and intra-category. 3.2. Algorithm implementation ment for each term t k with respect to category c i as follows: where tf ( t k , c i ) is the frequency of a term t k in category c specific scores of a term ( Yang &amp; Pedersen, 1997 ). The Eq. (3) is used in this paper. probability that the feature t k belongs to category c i when the feature t tiplication in Eq. (1) can be considered as the probability that the feature t (3) can be represented as follows:
CMFS algorithm aims to compute the significance of a term t the term t k occurring in one category c i against other terms in this category; the significances of the term t first two steps are combined as the contribution of the term t are given as following.
 Algorithm 1.

Input: V  X  the term-to-category feature-appearance matrix where rows are the features and columns are categories Output: V s  X  the feature subset Step 1: obtains the number of categories (the number of columns in V ) X  X  C | Step 2: obtains the size of the feature vector space (the number of rows in V ) X  X  V | Step 3: for each column (each category) c i
Step 4: obtains the sum of frequency of all features in category c Step 5: end for Step 6: for each row (each feature) t k 2 V
Step 7: obtains the frequency of the feature t k in the entire training set  X  tf ( t Step 8: for each column (each category) c i
Step 9: obtains the frequency of the feature t k in category c
Step 10: calculates the significance of the feature t k against the category c
Step 11: obtains the maximum value of CMFS ( t k , c i ) X  CMFS ( t Step 12: end for Step 13: end for Step 14: ranks all features in V based on CMFS ( t k ) Step 15: selects top k features into V s
The algorithm is implemented using C++ standard template library (STL). The vocabulary is represented by a container map&lt;double, int&gt;) in which all elements are ordered and the top k feature can be selected. 3.3. Time complexity analysis of CMFS We assume that the term-to-category feature-appearance matrix V is given. Thus, the time complexity analysis of the CMFS indicates that: The calculation of the number of categories costs O (1).
 The estimation of the size of the feature vector space costs O (1).
 Estimating the sum of frequency of all features in every category costs O (| C | | V |). Computing the frequency of every feature in the entire training set costs O (| C | | V |). Computing the significance of every feature t k costs O (| C | | V |).
 Ranking all features in V based on CMFS( t k ) costs O (| V | log| V |) Combining the above costs, we can easily calculate the cost of the CMFS algorithm as follows: similar to improved Gini index and higher than that of DF and DIA. 4. Experimental setup 4.1. Classifiers
In this section, we briefly describe the Na X ve Bayes (NB) and Support Vector Machines (SVMs) used in our study. 4.1.1. Na X ve Bayes classifier document is independent from the occurrence of other terms. There are two commonly used models about Bayesian clas-mial model can generate higher accuracy than multivariate Bernoulli model. In this study, we use multinomial model. 4.1.2. Support Vector Machines machine. 4.2. Datasets In order to evaluate the performance of the proposed method, three benchmark datasets (Reuters-21578, WebKB and 20-
Newsgroups) were used in our experiment. In data preprocessing, all words were converted to lower case, punctuation marks were removed, stop lists were used, and no stemming was used. Document frequency of a term was applied in text representation and the 10-fold validation was adopted in this paper. 4.2.1. 20-Newsgroups
The 20-Newgroups were collected by Ken Lang (1995) and has become one of the standard corpora for text categorization. It contains 19997 newsgroup postings, and all documents were assigned evenly to 20 different UseNet groups. 4.2.2. Reuters-21578 egories. In this paper, we only considered the top 10 categories. 4.2.3. WebKB
The WebKB is a collection of web pages from four different college web sites. The 8282 web pages are non-uniformly corpus. 4.3. Performance measures per category. The formulas of the precision and recall for the category c where TP i is the number of the documents that is correctly classified to category c the category c i , FN i is the number of the documents belonging to category c micro-recall may be obtained as ber of the false positive documents. The micro F1 and Accuracy are defined in the following way: 5. Results 5.1. Experimental results on the 20-newsgroups dataset
Table 3 and Table 4 show the micro F1 measure result when Na X ve Bayes and Support Vector Machines are used on 20-
Table 4 indicates that the micro F1 performance of SVM used CMFS on 20-newsgroups is superior to that based on other accuracy curve of SVM used CMFS on 20-newgroups is only lower than that based GINI when the number of the selected highest point (76.62%) when the number of the selected features is 800. 5.2. Experimental results on the Reuters-21578 dataset
Tables 5 and 6 show the micro F1 measure result when the Na X ve Bayes and Support Vector Machines are used on Reu-selected feature is 1200 or 1400. 5.3. Experimental results on the WebKB dataset
The micro F1 performance of Na X ve Bayes and Support Vector Machines used on WebKB are listed in Tables 7 and 8 , the highest. Table 8 shows that the micro F1 of SVM based on CMFS outperforms that based on other algorithms when based on OCFS, GINI and CHI when the number of the selected features is 1400 or 1600. 6. Statistical analysis and discussions 6.1. Statistical analysis
In order to compare the performance of the proposed method with the previous approaches, Friedman and Iman and Dav-CHI when Support Vector Machines is used.
 6.2. Discussions rior to that based on the other feature selections.
 selected by CMFS can bring the more classification performance for Na X ve Bayes on 20-newgroups and Reuters-21578. features; others are indeed irrelevant to the text categorization task and should be further filtered out.
The performance of CMFS is, especially when Na X ve Bayes is used, not optimal on WebKB. Through analyzing on the fea-www.uvic.ca/ X  X  and does not contain any category information.
 egory and intra-category. The second one is that the global significance of a term t proved Gini index is calculated by summing the significance of the term t the term t k for every category or weighted summing the significance of the term t conditional probability that the feature t k belongs to category c the feature t k occurs in category c i , P ( t k | c i ) are greater than or equal to zero, the formula P  X  t increasing function. So the equation of the improved Gini index can be improved as follows: and SVM based on CMFS is very close to that based on GINI. 7. Conclusion
In this paper, we have proposed a novel feature selection algorithm, named CMFS, by which the significance of a term both in inter-category and intra-category are comprehensively measured. The efficiency of the proposed measure CMFS was examined through the experiments of text categorization with NB and SVM classifier. The results, comparing with and web dataset.
 Acknowledgments tronic Development Foundation of China under Grant no. 2009537.
 References
