 Many machine learning a lgorithms minimize a reg ularized risk [ 1 ]: (SVMs), exponential loss is used in Adaboost, and logistic regression uses th e logistic loss. problem has a unique glo bal optimum [ 3 ]. However, as was recen tly shown by Long and S ervedio the overwhelming impac t from the data with u  X  0 . There has been some r ecent and some not-In this paper, we continu e this line of inquiry and propose a non-convex lo ss function which is firmly grounded in prob ability theory. By extending logistic regres sion from the ex-ponential family to the t -exponential fam-ily, a natural extension of exponential family of distributions studied in statistical physics [ 6  X  10 ], we obtain the t -logistic regression algorithm. Furthermore , we show that a simple block coordinate d escent scheme can be used to solve the res ultant regularized risk minimization problem . Analysis of this procedure also intuitivel y explains why t -logistic regression is ab le to handle label noise.
 Our paper is structured a s follows: In sec-tion 2 we briefly review logistic regression especially in the context o f exponential fam-tive programming to desi gn an optimization strateg y. Experiments that comp are our new approach experimental results can b e found in the supplemen tary material. X Bayes rule, and making a standard iid assumption a bout the data allows us to write nential family of distribut ions with the log-partition fun ction g (  X  | x ) given by If we choose the feature map  X  ( x ,y ) = y that p ( y | x ;  X  ) is the logistic function By assuming a zero mea n isotropic Gaussian prio r N (0 , 1  X  logarithms, we can rewrit e ( 6 ) as thanks to ( 9 ) can be identified with th e logistic loss ( 5 ). The t -exponential function exp where (  X  ) + = max(  X  , 0) . Some examples are show n in Figure 2 . Clearly, exp are preserved: exp But exp exp t ( b ) . One can also define the inverse of exp t namely log t as Similarly, log tailed distributions which we will later exploit. Figure 2: Left: exp t defined as [ 9 , 13 ]: [ 9 , 13 ]: where Z (  X  ) = integrates to 1 .
 In addition, it is very clos e to being a moment gene rating function The proof is provided in the supplementary mater ial. A general version of this result appears as In where 1 &lt; t &lt; 2 , and compute the log-par tition function g t by noting that Even though no closed f orm solution exists, one can compute g t given  X  and x using numerical techniques efficiently.
 t -exponential family [ 14 ]. Recall that a one dimen sional Student X  X -t distribution is given by satisfying  X  ( v + 1) / 2 = 1 / (1  X  t ) and denote, then by some simple but tedious calculation (inclu ded in the supplementary material) Therefore, we work with the Student X  X -t prior in our setting: robust 3 alternative to the Gaussia n distribution [ 16 , 17 ].
 As before, if we let  X  ( x ,y ) = y we employ a different str ategy.
 Since log lently work with the log where p ( Y | X ) is independent of  X  . Using ( 13 ), ( 18 ), and ( 11 ), we can further write  X  J (  X  )  X  strategy for dealing with such problems. The optimal solutions to the problem ( 23 ) can be obtained by sol ving the following param etric problem (see Theorem 2. 1 of Kuno et al. [ 18 ]): l (  X  ) =  X  associated with it.
 minimize ( 24 ) with respect to  X  and  X  separately.  X  -Step : Assume that  X  is fixed, and denote  X  z n = z n (  X  ) to rewrite ( 24 ) as: Lagrangian and its gradie nt with respect to  X  n  X  can be written as Setting the gradient to 0 o btains  X  =  X  z n  X   X  K.K.T. conditions [ 3 ], we can conclude that  X  N algorithm robust to outlie rs.  X  logistic regression, excep t that each component has a weight  X  here. This is a standard uncons trained convex optimizati on problem which can be solved by any off the shelf solver. In our case we use the L-BFGS Qua si-Newton method. This requires us to compute the gradient  X   X  z n (  X  ) : for n = 1 ,...,m  X   X  z n + d (  X  ) =  X   X  l n (  X  ) = (1  X  t ) unit vector). q t ( y | x ;  X  ) is the escort distribution of p ( y | x ;  X  ) ( 16 ): of P (  X  ) . We include the proof in the supplementary materi al. Our experimental evaluat ion is designed to answer four natural questions: 1) How does the gener-important given that the a lgorithm is minimizing a non-convex loss.
 To answer the above que stions empirically we use six datasets, two of whic h are synthetic. The 21 to +1 if (see Table 1 in supplementary materia l for details).
 for parameter space Experiments were perform ed on a Qual-core machin e with Dual 2.5 Ghz proce ssor and 32 Gb RAM. t Mease-Wyner, Mushroom ; bottom: USPS-N, Adul t, Web) with and without 10% label noise. All Long-Servedio and Mush room datasets), with a slig ht edge on some datasets such as Mease-Wyner. outperformed by the prob it.
 clean and noisy training s amples which leads to ba d performance on noisy d atasets. t might be needed. Surpr isingly, the L-BFGS algo rithm, which is not desig ned to optimize non-with a trust-region approa ch. [ 21 ] with 10% label noise. Le ft to right, top: Long-Serv edio, Mease-Wyner, Mus hroom; bottom: USPS-points without (resp. with ) label noise. by performance of the probit fluctuates widely with dif ferent initial values of  X  . Mushroom; bottom: USP S-N, Adult, Web. [1] Choon Hui Teo, S. V . N. Vishwanthan, Alex J . Smola, and Quoc V. Le . Bundle methods for [2] S. Ben-David, N. Eiro n, and P.M. Long. On the difficulty of approximatel y maximizing agree-[3] S. Boyd and L. Vande nberghe. Convex Optimization . Cambridge University P ress, Cambridge, [5] Yoav Freund. A mor e robust boosting algorith m. Technical Report Arx iv/0905.2138, Arxiv, [11] Christopher Bishop. Pattern Recognition and Machine Learning . Springer, 2006. [13] Timothy D. Sears. Generalized Maximum Entropy, Convexity, and M achine Learning . PhD [18] Takahito Kuno, Yas utoshi Yajima, and Hiros hi Konno. An outer app roximation method for [19] David Mease and A braham Wyner. Evidence contrary to the statistical view of boosting. J. [22] C.C. Chang and C.J . Lin. LIBSVM: a library for support vector machines , 2001. Software [23] Fabian Sinz. UniverSVM: Support V ector Machine with Larg e Scale CCCP Functiona lity ,
