 In many domains, researchers are interested in predicting t he effects of interventions, or manipulat-ing variables, on other observed variables. Such predictio ns require knowledge of causal relation-ships between observed variables. There are existing asymp totically correct algorithms for learning such relationships from data, possibly with missing values and hidden variables [1][2][3], but these algorithms all assume that every variable is measured in a si ngle study. Datasets for such studies are not always readily available, often due to privacy, ethical , financial, and practical concerns. How-ever, given the increasing availability of large amounts of data, it is often possible to obtain several similar studies that individually measure subsets of the va riables a researcher is interested in and together include all such variables. For instance, models o f the United States and United Kingdom economies share some but not all variables, due to different financial recording conventions; fMRI studies with similar stimuli may record different variable s, since the images vary according to mag-net strength, data reduction procedures, etc.; and U.S. sta tes report some of the same educational testing variables, but also report state-specific variable s. In these cases, if each dataset has over-lapping variable(s) with at least one other dataset, e.g. if two datasets D variables V should be able to learn many of the causal relationships betw een the observed variables using this set of datasets. The existing algorithms, however, cannot i n general be directly applied to such cases, since they may require joint observations for variables tha t are not all measured in a single dataset. While this problem has been discussed in [4] and [5], there ar e no general, useful algorithms for learning causal relationships from data of this form. A typi cal response is to concatenate the datasets to form a single common dataset with missing values for the va riables that are not measured in each of the original datasets. Statistical matching [6] or multi ple imputation [7] procedures may then be used to fill in the missing values by assuming an underlying model (or small class of models), estimating model parameters using the available data, and t hen using this model to interpolate the missing values. While the assumption of some underlying mod el may be unproblematic in many lationships learned using the interpolated dataset that ar e between variables which are never jointly measured in single dataset will only be correct if the corres ponding relationships between variables in the assumed model happen to be causal relationships in the correct model. The Structural EM algorithm [8] avoids this problem by iteratively updating t he assumed model using the current inter-polated dataset and then reestimating values for the missin g data to form a new interpolated dataset until the model converges. The Structural EM algorithm is on ly justified, however, when missing data are missing at random (or indicator variables can be use d to make them so) [8]. The pattern of missing values in the concatenated datasets described ab ove is highly structured. Furthermore, Structural EM is a heuristic procedure and may converge to lo cal maxima. While this may not be problematic in practice when doing prediction, it is proble matic when learning causal relationships. Our experiments in section 4 show that Structural EM perform s poorly in this scenario. We present a novel, asymptotically correct algorithm X  X he Integration of Overlapping Networks (ION) algorithm  X  X or learning causal relationships (or more properly, the c omplete set of possible causal DAG structures) from data of this form. Section 2 prov ides the relevant background and terminology. Section 3 discusses the algorithm. Section 4 p resents experimental evaluations of the algorithm using synthetic and real-world data. Finally, se ction 5 provides conclusions. We now introduce some terminology. A directed graph G = hV , Ei is a set of nodes V , which rep-resent variables, and a set of directed edges E connecting distinct nodes. If two nodes are connected by an edge then the nodes are adjacent . For pairs of nodes { X, Y } X  X  , X is a parent (child) of Y , if there is a directed edge from X to Y ( Y to X ) in E . A trail in G is a sequence of nodes such that each consecutive pair of nodes in the sequence is adjacent in G and no node appears more than once in the sequence. A trail is a directed path if every edge between consecutive pairs of nodes points in the same direction. X is an ancestor ( descendant ) of Y if there is a directed path from X to Y ( Y to
X ). G is a directed acyclic graph (DAG) if for every pair { X, Y } X  X  , X is not both an ancestor and a descendent of Y (no directed cycles). A collider ( v-structure ) is a triple of nodes h X, Y, Z i such that X and Z are parents of Y . A trail is active given C  X  X  if (i) for every collider h X, Y, Z i in the trail either Y  X  C or some descendant of Y is in C and (ii) no other node in the trail is in C . For disjoint sets of nodes X , Y , and Z , X is d-separated (d-connected) from Y given Z if and only if there are no (at least one) active trails between any X  X  X and any Y  X  Y given Z .
 A Bayesian network B is a pair hG , Pi , where G = hV , Ei is a DAG and P is a joint probability distribution over the variables represented by the nodes in V such that P can be decomposed as follows: For B = hG , Pi , if X is d-separated from Y given Z in G , then X is conditionally independent of Y given Z in P [9]. For disjoint sets of nodes, X , Y , and Z in V , P is faithful to G if X is d-separated from Y given Z in G whenever X is conditionally independent of Y given Z in P [1]. B is a causal Bayesian network if an edge from X to Y indicates that X is a direct cause of Y relative to V . Most algorithms for causal discovery , or learning causal relationships from nonexperimental da ta, assume that the distribution over the observed variables P is decomposable according to a DAG G and P is faithful to G . The goal is to learn G using the data from P . Most causal discovery algorithms return a set of possible DAGs which entail the sam e d-separations and d-connections, e.g. the Markov equivalence class , rather than a single DAG. The DAGs in this set have the same adjacencies but only some of the same directed edges. The dir ected edges common to each DAG represent causal relationships that are learned from the da ta. If we admit the possibility that there may be unobserved (latent) common causes between observed v ariables, then this set of possible DAGs is usually larger.
 A partial ancestral graph (PAG) represents the set of DAGs in a particular Markov equiv alence class when latent common causes may be present. Nodes in a PAG corre spond to observed variables. Edges are of four types:  X   X  ,  X  X  X   X  ,  X  X  X  X  and  X   X   X  , where a  X  indicates either an  X  or  X  orientation, bidirected edges indicate the presence of a latent common ca use, and fully directed edges (  X   X  ) indicate that the directed edge is present in every DAG, e.g. a causal relationship. For { X, Y } X  X  , a possibly active trail between X and Y given Z  X  X  / { X, Y } is a trail in a PAG between X and Y such that some orientation of  X   X  X  on edges between consecutive nodes in the trail, to either  X  or  X  , makes the trail active given Z . The ION algorithm uses conditional independence informati on to discover the complete set of PAGs over a set of variables V that are consistent with a set of datasets over subsets of V which have overlapping variables. ION accepts as input a set of PAGs whi ch correspond to each of such datasets. A standard causal discovery algorithm that checks for laten t common causes, such as FCI [1] or GES to learn these PAGs that will be input to ION. Expert domain kn owledge can also be encoded in the input PAGs, if available. The ION algorithm is shown as algor ithm 1 and described below. The algorithm begins with the complete graph over V with all  X  endpoints and transfers nonadja-cencies and endpoint orientations from each G then remove the edge between X and Y , if X is directed into Y in G the edge between X and Y to  X  . Once these orientations and edge removals are made, the cha nges to the complete graph are propagated using the rules in [10], which provably make every change that is entailed by the current changes made to the graph. Lin es 4-9 find every possibly active trail for every { X, Y }  X  V given Z  X  V / { X, Y } such that X and Y are d-separated given Z in some G sets of minimal changes that are not subsets of other sets of c hanges, which make these paths no longer active. For each minimal hitting set, a new graph is co nstructed by making the changes in the set and propagating these changes. If the graph is consis tent with each G does not imply a d-separation for some { X, Y }  X  V given Z  X  V / { X, Y } such that X and Y are d-connected in some G 19 attempt to discover any additional PAGs that may be consis tent with each G edges from PAGs in the current set and propagating the change s. If some pair of nodes { X, Y } X  X  that are adjacent in a current PAG are d-connected given  X  in some G sets of edge removals which remove this edge.
 The ION algorithm is provably sound in the sense that the output PAGs are consistent with every G d-connection entailed by some G d-connection are mutually exclusive, exhaustive relation s.
 Theorem 3.1 (soundness) . If X and Y are d-separated (d-connected) given Z in some G X and Y are d-separated (d-connected) given Z in every H Proof Sketch. Every structure A by some G ing to a d-connection in some G propagating other changes which are provably correct by [10 ]) in lines 10-19 are edge removals, which can only create new d-separations. If a new d-separati on is created which corresponds to a d-connection in some G The ION algorithm is provably complete in the sense that if there is some structure H variables V that is consistent with every G Theorem 3.2 (completeness) . Let H { X, Y } X  X  , if X and Y are d-separated (d-connected) given Z  X  X  / { X, Y } in some G i  X  G , then X and Y are d-separated (d-connected) given Z in H Proof Sketch. Every change made at line 3 is provably necessary to ensure so undness. At least one graph added to A at line 8 provably has every adjacency (possibly more) in H endpoints on an edge found in H will provably produce H every G Thus, by theorems 3.1 and 3.2, ION is an asymptotically corre ct algorithm for learning the complete set of PAGs over V that are consistent with a set of datasets over subsets of V with overlapping variables, if the input PAGs are discovered using an asymtot ically correct algorithm that detects the presence of latent common causes, i.e. FCI, with each of thes e datasets.
 Finding all minimal hitting sets is an NP-complete problem [ 11]. Since learning a DAG structure from data is also an NP-complete problem [12], the ION algori thm, as given above, requires a superexponential (in V ) number of operations and is often computationally intract able even for small sizes of |V| . In practice, however, we can break the minimal hitting set p roblem into a sequence of smaller subproblems and use a branch and bound approach th at is tractable in many cases and still results in an asymptotically correct algorithm. We te sted several such strategies. The method which most effectively balanced time and space complexity t radeoffs was to first find all minimal hitting sets which make all possibly active trails of length 2 that correspond to d-separations in some G  X  G not active, then find the structures resulting from making an d propagating these changes that are consistent with every G the length of possibly active trails considered until trail s of all sizes are considered. We first used synthetic data to evaluate the performance of IO N with known ground truth. In the first experiment, we generated 100 random 4-node DAGs using the MC MC algorithm described in [13] with random discrete parameters (conditional probability tables for the factors in the decomposition shown in section 2). For each DAG, we then randomly chose two s ubsets of size 2 or 3 of the nodes in the DAG such that the union of the subsets included all 4 nod es and at least one overlapping variable between the two subsets was present. We used forwar d sampling to generate two i.i.d. samples of sizes N = 50 , N = 100 , N = 500 , N = 1000 and N = 2500 from the DAGs for only the variables in each subset. We used both FCI and GES wit h latent variable postprocessing to generate PAGs for each of these samples which were input to IO N. To evaluate the accuracy of ION, we counted the number of edge omission, edge commision, and o rientation errors (  X  instead of  X  ) for each PAG in the ION output set and averaged the results. Th ese results were then averaged across all of the 100 4-node structures. Figure 1 shows the averaged results for these methods along with 3 other methods we included for comparison. ION-FCI and ION-GES refer the the performance of ION when the input PAGs are obtained using the FCI algorithm a nd the GES algorithm with latent variable postprocessing, respectively. For Structural EM , we took each of the datasets over subsets of the nodes in each DAG and formed a concatenated dataset, as described in section 1, which was input to the Structural EM algorithm. 2 For FCI-baseline and GES-baseline , we used forward sampling to generate another i.i.d. sample of sizes N = 50 , N = 100 , N = 500 , N = 1000 and N = 2500 for all of the variables in each DAG and used these datasets as input for the FCI and GES with latent variable postprocessing algorithms, respecti vely, to obtain a measure for how well these algorithms perform when no data is missing. The average runt imes for each method are also reported in figure 1. Error bars show 95% confidence intervals. We first note the performance of Struct ural EM. Almost no edge omission errors are made, but more edge com missions errors are made than any of the other methods and the edge commission errors do not dec rease as the sample size increases. When we looked at the results, we found that Structural EM alw ays returned either the complete graph or a graph that was almost complete, indicating that St ructural EM is not a reliable method for causal discovery in this scenario where there is a highly structured pattern to the missing data. Furthermore, the runtime for Structural EM was considerabl y higher than any of the other methods. For the larger sample sizes (where more missing values need t o be estimated at each iteration), a single run required several hours in some instances. Due to i ts significant computation time, we were unable to use Structural EM with larger DAG structures s o it is excluded in the experiments below. The FCI-baseline and GES-baseline methods performe d similarly to previous simulations of them. The ION-FCI and ION-GES methods performed similarl y to the FCI-baseline and GES-baseline methods but made slightly more errors and showed sl ower convergence (due to the missing data). Very few edge commission errors were made. Slightly m ore edge omission errors were made, but these errors decrease as the sample size increases. Some edge orientation errors were made even for the larger sample sizes. This is due to the fact that each o f the algorithms returns an equivalence class of DAGs rather than a single DAG. Even if the correct equ ivalence class is discovered, errors result after comparing the ground truth DAG to every DAG in th e equivalence class and averaging. We also note that there are fewer orientation errors for the G ES-baseline and ION-GES methods on the two smallest sample sizes than all of the other sample siz es. While this may seem surprising, it is simply a result of the fact that more edge omission errors a re made for these cases. We repeated the above experiment for 3 similar cases where we used 6-node DAG structures rather than 4-node DAG structures: (i) two i.i.d. samples were gene rated for random subsets of sizes 2-5 with only 1 variable that is not overlapping between the two s ubsets; (ii) two i.i.d. samples were generated for random subsets of sizes 2-5 with only 2 variabl es that are not overlapping between the two subsets; (iii) three i.i.d. samples were generated f or random subsets of sizes 2-5 with only omission, edge commission, and orientation errors for each of these cases, respectively. In general, the performance in each case is similar to the performance fo r the 4-node case.
 We also tested the performance of ION-FCI using a real world d ataset measuring IQ and various neuroanatomical and other traits [14]. We divided the varia bles into two subsets with overlapping variables based on domain grounds: (a) variables that might be included in a study on the relationship between neuroanatomical traits and IQ; and (b) variables fo r a study on the relationship between IQ, sex, and genotype, with brain volume and head circumference included as possible confounders. Figures 5a and 5b show the FCI output PAGs when only the data fo r each of these subsets of the variables is provided as input, respectively. Figure 5c sho ws the output PAG of ION-FCI when these two resulting PAGs are used as input. We also ran FCI on the com plete dataset to have a comparison. Figure 5d shows this PAG. Figure 5: (a) FCI output PAG for variables in subset a, (b) FCI output PAG for variables in subset b, (c) ION output PAG when using the FCI ouput PAGs for variables in subset a and variables in subset b as input, and (d) FCI output PAG for all variables In this particular case, the output of ION-FCI consists of on ly a single PAG, which is identical to the result when FCI is given the complete dataset as input. Th is case shows that in some instances, ION-FCI can recover as much information about the true DAG st ructure as FCI even when less information can be extracted from the ION-FCI input. We note that the graphical structure of the complete PAG (figures 5c and 5d) is the union of the structures shown in figures 5a and 5b. While visually this may appear to be a trivial example for ION where all of the relevant information can be extracted in the first steps, there is in fact much processing required in later stages in the algorithm to determine the structure around the nonoverlapping varia bles. In practice, researchers are often unable to find or construc t a single, complete dataset containing of integrating information about causal relationships tha t can be discovered from a collection of datasets with related variables [5]. Standard causal disco very algorithms cannot be used, since they take only a single dataset as input. To address this open prob lem, we proposed the ION algorithm, an asymptotically correct algorithm for discovering the co mplete set of causal DAG structures that are consistent with such data.
 While the results presented in section 4 indicate that ION is useful in smaller domains when the branch and bound approach described in section 3 is used, a nu mber of issues must be addressed before ION or a simlar algorithm is useful for higher dimensi onal datasets. Probably the most sig-nificant problem is resolving contradictory information am ong overlapping variables in different input PAGs, i.e. X is a parent of Y in one PAG and a child of Y in another PAG, resulting from statistical errors or when the input samples are not identifi cally distributed. ION currently ignores such information rather than attempting to resolve it. This increases uncertainty and thus the size of the resulting output set of PAGs. Furthermore, simply ignor ing such information does not always avoid conflicts. In some of such cases, ION will not discover a ny PAGs which entail the correct d-separations and d-connections. Thus, no output PAGs are r eturned. When performing condi-tional independence tests or evaluating score functions, s tatistical errors occur more frequently as the dimensionality of a dataset increases, unless the sampl e size also increases at an exponential rate (resulting from the so-called curse of dimensionality ). Thus, until reliable methods for resolv-ing conflicting information from input PAGs are developed, I ON and similar algorithms will not in general be useful for higher dimensional datasets. Furth ermore, while the branch and bound approach described in section 3 is a significant improvement over other methods we tested for com-puting minimal hitting sets, its memory requirements are st ill considerable in some instances. Other algorithmic strategies should be explored in future resear ch.
 We thank Joseph Ramsey, Peter Spirtes, and Jiji Zhang for hel pful discussions and pointers. We thank Frank Wimberley for implementing the version of Struc tural EM we used. R.E.T. was sup-ported by the James S. McDonnell Foundation Causal Learning Collaborative Initiative. C.G. was supported by a grant from the James S. McDonnell Foundation.
 [1] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search . MIT Press, 2nd [2] J. Pearl. Causality: Models, Reasoning, and Inference . Cambridge University Press, 2000. [3] D. M. Chickering. Optimal structure identification with greedy search. Journal of Machine [4] D. Danks. Learning the causal structure of overlapping v ariable sets. In Discovery Science: [5] D. Danks. Scientific coherence and the fusion of experime ntal results. The British Journal for [6] S. R  X assler. Statistical Matching . Springer, 2002. [7] D. B. Rubin. Multiple Imputation for Nonresponse in Surveys . Wiley &amp; Sons, 1987. [8] N. Friedman. The Bayesian structural EM algorithm. In Proceedings of the 14th Conference [9] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference . [10] J. Zhang. A characterization of markov equivalence cla sses for causal models with latent [11] R. Greiner, B. A. Smith, and R. W. Wilkerson. A correctio n to the algorithm in Reiter X  X  theory [12] D. M. Chickering. Learning Bayesian networks is NP-com plete. In Proceedings of the 5th [13] G. Melanc  X on, I. Dutour, and M. Bousquet-M  X elou. Rando m generation of dags for graph draw-[14] M. J. Tramo, W. C. Loftus, R. L Green, T. A. Stukel, J. B. We aver, and M. S. Gazzaniga. Brain
