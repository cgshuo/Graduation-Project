 expressed as a query, QA has the goal to provide exact answers in order to satisfy a question. These are different modes of information access for different needs. English QA has been under active investigation since TREC 8 [1]. Chinese QA has been less studied and it is, like Chinese IR, considered more difficult because of difficulties in cently, both CLEF and NTCIR introduced mono/cross-lingual QA tasks for their evaluations. CLQA involves posing a question in language X and extracting answers from documents in language Y. Like CLIR, CLQA needs to handle the mismatch between the question and collection languages, and a common flexible approach is to translate questions, do retrieval and answering in language Y. 
The complexity of QA requires stronger NLP processing than for IR, such as seg-mentation, POS tagging, parsing, knowledge sources for semantic operations. As discussed in [2], off-the-shelf QA systems are rare unlike IR systems. There is advan-there are many languages in the world; for a web service offering QA/CLQA in mul-system might be preferred depending on how much effectiveness it can deliver. Researchers have studied CLQA among European languages in CLEF [3], and be-English-Chinese CLQA because evaluated data is not previously available. NTCIR-5 provides a large collection (~3GB) of BIG-5 encoded newspaper published in 2000 and 2001, 200 evaluated factoid questions in Chinese and their corresponding English counterparts, and their answers [5]. 
The paper is organized as follows: Section 2 presents our monolingual QA ap-proach. Section 3 discusses results of Chinese QA. Section 4 introduces our methods of translating English question to Chinese for E-C CLQA. Section 5 discusses results of CLQA and comparison with monolingual. Section 6 has our conclusion. their effectiveness within the NTCIR-5 environment. Fig.1 shows a common flow-chart for QA and CLQA. The top two steps involve preprocessing, translation and are different or active for CLQA to produce a query in Chinese. Monolingual QA as-We divide our QA procedures into several broad steps discussed below. 2.1 Question Classification An important step in QA is question analysis which tries to understand what a ques-tion wants. In NTCIR-5, answers are limited to nine categories [5]: person, location, organization, date, time, money, percentage,  X  X umex X  (numbers) and artifact.  X  X umex X  is any numeric entity other than money and percent;  X  X rtifact X  represents other objects like book titles, product names, etc. We denote a question X  X  class, C Q , by one of these longs to any other categories would be erroneous. 
In our experiments, two simplifications were made. First,  X  X rtifact X  category (a) was not implemented; an  X  X nknown X  class (x) was employed to tag all questions that failed to be classified into the other eight types.  X  X rtifact X -related questions may involve ontol-classification algorithm (Fig.1-i, Sec.4) are assigned to the corresponding Chinese ques-confounding factor. We have a simple Chinese question classifier that is based on usage templates of characters/words such as:  X   X ,  X ( | )( | |...) ( | | |...) X , and performs moderately like our English question classifier (Sec. 4.1). 2.2 Question Preprocessing, Indexing and Retrieval The next step, IR processing, is to obtain samples of text that have high probability of containing an answer to a Chinese question Q C . This has correlation with the probabil-retrieval engine [6] was employed to rank such units. The following three considera-tions guide our IR processing: a) questions are pre-processed to include double weighting of extracted entities to form q C (Fig.1-i): the idea is to improve the focus of gram and unigram indexing for retrieval (Fig.1-iii): this simple indexing strategy has been shown to be as effective as word indexing [6]. Moreover, factoid QA questions sentence for answer candidates lead to good QA results in English. 2.3 Answer Candidate Extraction From the retrieved sentences, one needs to identify substrings as candidate answers to a question. Sophisticated linguistic techniques can be employed to analyze the syntac-tic and semantic structure of each sentence, and match it with the question in order to decide where an answer may be. Here, we follo w the tradition of extracting all possi-ble answer candidates first [e.g. 8], and rank them later statistically. For this purpose, we employ BBN X  X  IdentiFinder [9], a COTS software. It is based on prior training and HMM decoding, can handle both English and Chinese texts separately, brackets  X  X umex X  and artifact. We augment this with our own numeric extraction module. Thus, given retrieved sentences, IdentiFinder extracts many entities whose unique entries we denote as a i with tag(a i ). All entities extracted by IdentiFinder form a pool next section for their ranking and answer identification. 2.4 Answer Candidate Ranking The last step (Fig.1-v) in QA is to identify which one in the pool of candidates is the P(a i S j |Q) of being an answer and support to question Q. The most likely answer would sion if one has sufficient known QA samples. Absent such data, we employ intuitive estimation of factors that may proportionately reflect the value of these probabilities. influencing factors include similarity between Q and S j (V s ), and retrieval depth (d). factors are discussed below and in Sec.3. (a) Categorical Evidence candidate X  X  tag(a i ). Since the determination of these classes may have uncertainty, we employ a graded category score V c for their measure: V c = 1; //default value if (C Q equals tag(a i ) ) else if (C Q  X  {p,l,o} and tag(a i )  X  {p,l,o}) else if (C Q  X  {x} and tag(a i )  X  {p,l,o}) score V c1 is given. When they disagree but both are named entities {p,l,o}, a medium still active for ranking. 
It is found that V c1 = 200, V c2 = 50, V c3 = 2 can give good results for our monolin-gual QA. These values (and those below) are estimated by using 3-way cross valida-tion on our 200 question set with 1/3 as held-out data. The search is rough and most likely not optimal. (b) In-Question/In-Web Evidence It is usual that a QA question seldom contains the answer explicitly. One can imagine stances are rare. We assume no such question exists and employ a score V w to reflect this fact: V w =1; //default value if (a i matches substring(q C ) ) V w =0; This binary evidence score can help eliminate some erroneous candidates. As the subheading implies, V w is meant to capture existence among web expansion terms as well. This is done for CLQA (Sec.4.3). For monolingual, web expansion has not been found helpful and is not used. (c) Proximity Evidence A sentence is retrieved because it has bigrams and/or unigrams covering some key-assume that the closer this candidate is to these coverings the more probable it is an answer. Suppose multiple coverings are found in a sentence containing one or more score V p-suc are accumulated. The pseudo-code for evaluating V p-pre score follows: Let c  X  {a Chinese character, a numeric sequence, or an English word}; for (each c preceding a candidate) { x=c; } A long sequence (&lt;10) of character/word match will be given higher weight because sequence from a candidate. We experimentally found both f(), g() as log(1+match-length), log(1+distance-from-candidate) are effective for monolingual QA. A similar procedure for evaluating V p-suc is done for coverings appearing after a candidate. The final score for proximity is: V p = 1 +  X  p * (V p-pre +V p-suc ), with  X  p = 0.5. As discussed before in Sec.2, we keep our approach simple by matching sub-strings rather than segmented words, and to work with returned sentences only with-out need for collection information. (d) Sentence Similarity Evidence likely be answers. The proximity score in (c) has part of this accounted through scor-provides a retrieval status value (RSV) for each sentence and reflects the probability following score V s is used: V s = 1/h(rank), with h() being the identity function. 
This not only keeps calculations simple; in certain retrieval situations such as web searching, RSV X  X  may not be available. (e) Candidate Frequency Evidence rence frequency f. We assume that the more often a candidate occurs, the more likely imity or similarity. We employ the following V f score to capture this information: V f = 1 + 0.1 * log (f) 
There are uncertainties in every step of our procedure. For example, we do not have  X  X rtifact X  as an answer class. The IR processing is itself fuzzy. Entity extraction and question analysis are uncertain. These determine the candidate pool. The func-tions and assumptions used in the evidence factors for candidate ranking may be unre-liable. Combination of the five evidence sources by multiplication: V=V c *V w * V *V s *V f is used for final ranking. This gives a more effective score for determining a candidate X  X  answer-hood than any other subsets. All the factors do not involve seg-mentation or syntactic analysis for estimation. In this section, we present monolingual QA results based on our simple approach and retrieval depth and question classification accuracy. 3.1 Retrieval Depth Retrieval depth (Fig.1-iv) refers to the number d of sentences returned from retrieval for QA processing. If too few sentences are used, correct answer(s) might not be re-called, while too many may cause erroneous candidates (fitting the evidence scoring) to rank high. Fig.2 plots retrieval depth vs. QA effectiveness. All five evidence fac-tors are used. Here, top-n (n=1,2,5) denote accuracy evaluation based on  X  X orrect and Document-Supported X  answers for the first n suggestions, while top-nU are based on groups show some bimodal behavior (except for top-5U) with peaks at around d=4 and also around d=13-50. 
For this monolingual environment, the questions are given in original Chinese and retrieval is relatively more accurate (compared to CLQA). When only a few (4) sen-system provides accuracy values of .295 (59 questions correctly answered, one sug-Mean reciprocal rank MRR=.3381 means that one may expect the first correct answer turned, noisy wrong answers get ranked high and lead to accuracy drop. However, as times approximating previous peak values. Thus retrieval relevance, which controls the order of sentences returned, may not necessarily be compatible with the existence of correct answers in returned sentences. The performance at the two approximate general, retrieval depth d=4 is better for precision-oriented (i.e. top-1,-2), while d=25 can be better for recall-oriented (e.g. top-5 accuracy .43) operations. All 13  X  X rtifact X  questions failed except #97. This has  X  X rtifact X  answer  X  which Identi-Finder tags as  X  X erson X  and our evidence criteria ranks it best. 
If  X  X orrect including Unsupported X  answers are also included as right (Fig.2 dashed-lines), the peaks at d=25 surpass those at d=4 except at top-1U. top-1U accu-top-5U, and leads to 126 of the 200 questions having at least one correct answer in top 5. Table 1b shows the evaluation for the two peaks in greater detail. MRR-U values of .4998/.5163 mean a correct answer at position 2 on average approximately. 
As comparison, the NTCIR-5 thirteen (including unofficial) blind Chinese QA runs attain top-1 accuracy ranging between .1-.375, with median=.29, and top-1U accuracy tively. The best result from ASQA system was achieved with use of NLP tools includ-ing segmentation and POS tagging, Hownet sense and machine learning[10]. 
NTCIR-5 median .290 
NTCIR-5 median .315 3.2 Question Classification Accuracy Our question classification has accuracy of about 80% as discussed in Sec.4.1. Here, compared to Tables 1a,b. As can be seen, having perfect question classification only buys our system about 5-6% improvements at top-1 and top-2 (d=4) for supported answers, and about 7% including  X  X nsupported X . The improvement rises to &gt;=9% for procedures could promote more good answers to top-1 position. Here, one starts with an English question and attempts to obtain a translated Chinese query of similar meaning. The English questions are in their most accurate form. We weighting of the evidence factors are also modified. 4.1 Question Classification with our own numeric extraction can provide eight of nine required answer categories (Sec.2.1) from sentences, we used an unknown class (x) as catch-all for questions that  X  X hat X ,  X  X ow X ..) in a question, and adjacent meta-keywords (e.g.: how  X  X any X , what  X  X ity X , how  X  X ong X ). The meta-keywords are obtained from the Cognitive Computation Group (University of Illinois, Urbana-Champaign http://l2r.cs.uiuc.edu/~cogcomp/ (http://www.cogsci.ed.ac.uk/~jamesc/taggers/MXPOST.html), followed with Collins X  phrase. Based on the words in the noun phrase, meta-keyword list and simple heuris-tics, we assign a possible answer class to the English question. We tested this classification procedure on a set of training questions (T0001-T0200), and attains &gt;80% precision. For the test set, it is about 78%. This should be more accurate than other QA processes. In contrast, some participants of NTCIR-5 achieve 92% [10] and 86.5% [11] accuracy. 4.2 English Question Translation For CLQA, one needs to preserve in a translated query the fidelity of the intent in the original question. Translation technology from English to Chinese is an approxima-general English translation tool together with our web-based entity/terminology-oriented translation (CHINET [12]). The latter mines translation pairs from patterns in bilingual snippets returned by using an English name/terminology as keyword in a web search engine. It is always current and complements MT nicely. 
For each question, three forms are derived: the raw English question, a named-entity list extracted via IdentiFinder, and the original statement plus 20 related terms the question are given higher weight than web-expansion terms. 
An interesting observation is that sometimes the web-expansion terms may contain a correct answer (in English) already. Previously, investigators performed indirect QA [14] by finding answers from the web, then locate supporting documents from the locally-oriented Chinese information. Moreover, a translation of the answer needs to evidence of answer-hood for candidates (Sec.4.3). 
Once a Chinese query q C is composed, indexing, retrieval and answer candidate pool formation is done similarly as in Sec.2.2-2.3. 4.3 Answer Candidate Ranking The answer ranking procedures need some changes for CLQA. Because retrieval is much noisier, most of the parameter values are modified to reflect more uncertainty. For Category Evidence (a), V c1 ,V c2 ,V c3 were set to 70, 30 and 5. For In-Question/In-Proximity Evidence (c), sharper drop-off with distance appears more useful than loga-that a different similarity score is helpful: V s =1+  X  s *(  X  i=1..5 m i *log(1+i) )/log(1+L s )/h(rank) Here, L s is the sentence length, m i is the number of overlaps between a sentence and q
C1 U q C2 of i characters, 1&lt; i &lt;5;  X  tween them, and does not need word segmentation, corpus statistics or retrieval RSV. For CLQA, retrieval is less accurate; many more sentences need to be returned to find Moreover, too many sentences cause some common entities like  X   X  having too dence is not used. In this section, we present CLQA results and compare to our monolingual QA and depth, question classification accuracy, and translation effects. 5.1 Retrieval Depth In CLQA, the translated queries lead to less accurate retrieval. Low retrieval depth is fit to our evidence formulae. Fig.3 shows CLQA results versus retrieval depths using four evidence factors (without frequency evidence). Most of the curves are uni-modal and attain a maximum at about d~100 (except for top-2U,-5U) to give the best Eng-lish-Chinese CLQA results (Table 3a,b). 
At accuracies of 0.155 (top-1, supported) to .455 (top-5, including  X  X nsupported X ), our cross-lingual approach attains 53% and 73% of our respective monolingual QA 200 English questions compared to getting 59 correct if one starts with original Chi-nese questions instead. Cross-lingual MRR .2094 (supported) vs. monolingual MRR .3381 means on average correct answers appear at about the 5 th position for CLIR vs. at the 3 rd position on average for monolingual QA. Thus the loss due to translation is substantial. As comparison, seven NTCIR-5 E-C CLQA runs attain top-1 accuracies ranging from .03 to .125 with median .075. Our value of .155 improves over the best value [15] by 24%. When  X  X nsupported X  are also counted, the top-1 accuracies from and top-5U .455 are substantially better. 
NTCIR-5 median .075 
NTCIR-5 median .095 In [8], investigators reported French-English CLQA MRR of 78% compared to English QA using  X  X enient X  evaluation. Our corresponding performance is 52%. French is more similar to English than Chinese. For English-Hindu CLQA studied in [4], the MRR achieved was .25 for 56 questions compared to our result of .2094. 5.2 Question Classification Accuracy The above experiments were repeated by assuming perfect question classification. Tables 4a,b show such results compared to Table 3a,b. Except for top-4, improve-ments are &lt;10%, and reflect similar situations in monolingual where other considera-tions may be just as important as classification accuracy improvement for this simple approach. 5.3 Effects of Translation Processes For CLQA, translation accuracy has the greatest effect on effectiveness. As discussed wrong results from one single translation. Here, we analyze the contribution of these processes to the final CLQA results. 
Fig.4 shows the accuracy, MRR results (same parameters as in Table 3) vs. dif-ferent translation processes.  X 1:Systran x2 X  on the x-axis denotes results using Systran MT alone for translation (q C1 U q C1 ). Here, many entity names are not trans-lated correctly and results are poor. Top-1 accuracy is only 0.08. When entity-oriented web-based translation was added  X 2:1+webtran X , top-1 improved to .09. When entities in questions were emphasized by extraction and translated terms via web-expansion  X 3+webxpan X : ~40% improvement at top-1 to .155. The last step  X 4a:2+webexpan X  shows that question entity extraction and translation (q
C2 ) is useful in the presence of web-expansion to anchor the retrieved sentences to (top-1=.15). These results show the usefulness of exploiting the source language questions. segmentation, Chinese syntactic or semantic analysis. It employs a COTS entity ex-traction software and our retrieval engine, and emphasizes on ranking candidates extracted from retrieved sentences via category, in-question/in-web, proximity, simi-larity, frequency evidence factors together. It attains a medium QA top-1 accuracy of .295, top-5 of .43 and MRR ~.33, all are exact answers with sentence support. Accu-for top-1, while d=25 is better for top-5. 
When applied to English-Chinese CLQA it attains top-1 accuracy of .155, .315 for top-5, and MRR ~.21, using retrieval depth d=100. These are 53%, 73% and 64% of monolingual values, due to question translation inaccuracies and less effective re-trievals. Three separate translation paths were employed for CLQA to hedge transla-original question plus web-assisted expansion terms with both translations. Accuracy improves successively with each of these translation processes. 
Like IR systems, a simple QA approach is easier to port to other languages. It can answers within top-5 to top-1 position. Future studies include better question classifi-cation, and the ability to extract artifacts. Additional translation paths or disambigua-tion techniques are helpful for improving CLQA. Reverse translation of answers back to source language is also a necessary future investigation. Acknowledgments. We like to thank the anonymous reviewers for their constructive comments. 
