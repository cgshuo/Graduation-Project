 Many applications (e.g., anomaly detection) concern sparse signals. This paper focuses on the problem of recovering a K -sparse signal x  X  R 1  X  N , i.e., K N and N i =1 1 { x i 0 } = K . In the mainstream framework of compressed sens-ing (CS), x is recovered from M linear measurements y = xS  X  R 1  X  M ,where S  X  R N  X  M is often a Gaussian (or Gaussian-like) design matrix.

In our proposed method, the design matrix S is generated from an  X  -stable distribution with  X   X  0. Our decoding al-gorithm mainly requires one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are  X  X ndetermined X  in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the (absolute) minimum estimator is able to filter out a majority of the zero coordinates. The gap estimator , which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates. Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery qual-ity, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements, our algorithm can still reliably re-cover a significant portion of the nonzero coordinates. H.2.8 [ Database Applications ]: Data Mining Algorithms, Performance, Theory Compressed Sensing, L0 Projections, Stable Distributions
The goal of Compressed Sensing (CS) [7,2]istorecovera sparse signal x  X  R 1  X  N from a small number of non-adaptive linear measurements y = xS , (typically) by convex opti-mization (e.g., linear programming). Here, y  X  R 1  X  M the vector of measurements and S  X  R N  X  M is the design matrix. In classical settings, entries of S are i.i.d. samples from the Gaussian distribution N (0 , 1), or a Gaussian-like distribution (e.g., a distribution with finite variance).
In this paper, we sample S from a heavy-tailed distribu-tion. Strikingly, using such a design matrix turns out to result in a simple and powerful solution to the problem of exact K -sparse recovery, i.e., N i =1 1 { x i =0 } = K .
Sparse recovery can be naturally suitable for: (i) the  X  X in-gle pixel camera X  type of applications; and (ii) the  X  X ata streams X  type of applications. The idea of compressed sens-ing may be traced back to prior papers such as [10, 8, 5].
It has been realized (and implemented by hardware) that can be more advantageous than sampling the vector itself. This is the foundation of the  X  X ingle pixel camera X  proposal. See the site https://sites.google.com/site/igorcarron2/ compressedsensinghardware for a list of single-pixel-camera type of applications. Fig. 1 provides an illustrative example. Figure 1: We reconstruct a 256  X  256 image (i.e., N =
Natural images are in general not as sparse as the example in Fig. 1. We nevertheless expect that in many practical scenarios, the sparsity assumption can be reasonable. For example, the differences between consecutive image/video frames taken by surveillance cameras are usually very sparse because the background remains still. In general, anomaly detection problems are often very sparse.

Another line of applications concerns data streams, which can be viewed as sparse dynamic vectors with entries rapidly varying over time. Due to the dynamic nature, it is nontriv-ial to know where the nonzero coordinates are, since the his-tory of streaming is usually not stored. Many problems can be formulated as sparse data streams. For example, video data are naturally streaming. A common task in databases is to find the  X  X eavy-hitters X  [23], e.g., product items with the highest total sales. Also, see some recent papers on com-pressed sensing for network applications [21, 28, 29].
For data stream applications, entries of the signals x are (rapidly) updated over time (by addition and deletion). At atime t ,the i t -th entry is updated by I t , i.e., x i t I . This is often referred to as the turnstile model [23]. As the projection operation is linear, i.e., y = xS ,wecan (re)generate corresponding entries of S on-demand when-ever one entry of x is altered, to update all entries of the measurement vector y . The use of stable random projec-tions for estimating the  X  -th frequency moment N i =1 | x (instead of the individual terms x i ) was studied in [14]. [16] proposed the use of geometric mean estimator for stable ran-dom projections, for estimating N i =1 | x i |  X  as well as the harmonic mean estimator for estimating N i =1 | x i |  X  when  X   X  0. When the data are nonnegative, the method named compressed counting [17, 18] based on skewed-stable distri-butions becomes particularly effective.
A random variable Z follows an  X  -stable distribution with unit scale, S (  X , 1), if its characteristic function is [24] When  X  =2(or  X  = 1), this is the normal (or Cauchy) dis-tribution. To sample from S (  X , 1), we sample independent exponential w  X  exp (1) and uniform u  X  unif (  X   X / 2 , X / variables, and then compute Z  X  S (  X , 1) by [3] If
S 1 ,S 2  X  S (  X , 1) i.i.d., then for any constants C 1 ,C have C 1 S 1 + C 2 S 2 = S  X  ( | C 1 |  X  + | C 2 |  X  ) 1 / X  S (  X , 1). More generally, N
In our numerical experiments with Matlab,  X  is taken to be 0 . 03 and no special data storage structure is needed. Our method can be intuitively illustrated by an  X  X dealized X  algo-rithm using the limit as  X   X  0.
We assume x  X  R 1  X  N is K -sparse. We obtain M linear measurements y = xS  X  R 1  X  M ,whereentriesof S  X  R N  X  M denoted by s ij , are i.i.d. samples from S (  X , 1) with a small (e.g., 0.03). That is, each measurement is y j = N i =1 x Our algorithm, which consists of two estimators, utilizes the
The absolute minimum estimator is defined as Algorithm 1 The proposed recovery algorithm.
 We prove that essentially M 0 = K log (( N  X  K ) / X  )measure-ments are sufficient for detecting all zeros with probability at least 1  X   X  . The actual required measurements will be sig-nificantly lower than M 0 if we use the minimum algorithm together with the gap estimator and the iterative process.
When |  X  x i,min | &gt; ,the gap estimator is used to estimate the magnitude of x i .Wefirstsort z i,j  X  X : z i, (1)  X  z i, (2) j  X  M  X  1. The gap estimator is simply  X  x We also derive the error bound Pr ( |  X  x i,gap  X  x i | &gt; M&lt;M 0 , we discover that it is better to apply the gap esti-mator iteratively, each time using the residual measurements only on the  X  X ndetermined X  coordinates; see Alg. 1.
Our procedure is intuitive from the ratio of two indepen-dent  X  -stable random variables, in the limit  X   X  0. Recall that, for each coordinate i , our observations are ( y j j =1to M . Naturally our first attempt was to use the joint likelihood of ( y j ,s ij ). However, our proposed method only utilizes the ratio statistics y j /s ij . We first explain why.
For convenience, we first define
Denote the density function of S (  X , 1) by f S ( s ). By a con-ditional probability argument, the joint density of ( y j derive the joint log-likelihood of ( y j ,s ij ), j =1to M l ( x Closed-form expressions of f S are in general not available (unless  X  =1 , 2). Interestingly, from the procedure (2) for sampling Z  X  S (  X , 1), we can guess that 1 / | Z |  X  is approx-imately w  X  exp (1) when  X   X  0. Indeed, as shown by [6], 1 / | Z |  X   X  exp (1) in distribution. Using this limit, the den-sity function f S ( s ) is approximately  X  2 e  X  X  s | the joint log-likelihood l ( x i , X  i ) is approximately which approaches infinity (i.e., the maximum likelihood) at the poles: y j  X  x i s ij =0, j =1to M . This is the reason why we use only the ratio statistics z i,j = y j /s ij . where S 1 ,S 2  X  S (  X , 1), i.i.d. Recall the definition and the problem boils down to finding the distribution of the ratio of two  X  -random variables with  X   X  0. Using the mate cumulative distribution function (CDF) of y j /s ij is The CDF of S 2 /S 1 is also given by (4) with x i =0,  X  i
Fig. 2 plots the approximate CDFs (4) for S 2 /S 1 (left panel) and y j /s ij (right panel, with x i = 0 and three values of  X   X  ). While the distribution of S 2 /S 1 is extremely heavy-tailed, about half of the probability mass concentrated near 0. This means, as  X   X  0, samples of | S 2 /S 1 | are equal likely to be either very close to zero or very large. Since (4) is only approximate, we also provide the simulations of S 2 /S in Fig. 3 to help verify the approximate CDF in Fig. 2. Figure 2: Approximate CDFs of S 2 /S 1 (left panel) and Figure 3: Simulations of | S 2 /S 1 | directly using the for-We consider K = 2, to illustrate the iterative process in Alg. 1. For simplicity, let x 1 = x 2 =1, x i =0 ,  X  3  X  i  X  N This way, the observations become y j = x 1 s 1 j + x 2 s s 1 j + s 2 j ,for j =1to M . The ratio statistics are z z
We assume an  X  X dealized X  algorithm, which allows us to use an extremely small  X  .As  X   X  0, s 2 j s 1 j is either (vir-Suppose, with M = 3 observations, the ratio statistics, for i =1 , 2, are: ( z 1 , 1 ,z 2 , 1 )=(1 ,  X  X  ), ( z 1 , 2 ,z ( z 1 , 3 ,z 2 , 3 )=(1 ,  X  X  ). Then we have seen z 1 ,j =1twice and this  X  X dealized X  algorithm is able to correctly estimate  X  x 1 = 1, as there is a  X  X luster X  of 1 X  X . After we have estimated x , we compute the residual r j = y j  X   X  x 1 s 1 j = s 2 j second iteration , the ratio statistics become This means we know x 2 = 1. We again compute the resid-uals, which become zero. In the third iteration ,allzero coordinates can be recovered. The most exciting part of this example is that, with M = 3 measurements, we can recov-ery a signal with K = 2, regardless of N .Wehopethis example helps understand why our algorithm performs so well empirically. We summarize the  X  X dealized X  algorithm: 1. The algorithm assumes  X   X  0, or as small as necessary. 2. As long as there are two observations y j /s ij in the This  X  X dealized X  algorithm can not be strictly implemented. When we use a small  X  instead of  X  = 0, the observations | y /s ij | will be between 0 and  X  , and we will not be able to identify the true x i with high confidence unless we see two essentially identical observations. As analyzed in Sec. 5, the proposed gap estimator is a practical surrogate.
Fig. 2 (right panel) shows that the distribution of y j /s is heavy-tailed, with a jump very near x i in the CDF. This means more than one observations (among M observations) will likely lie in the extremely narrow interval around x pending on the value of  X   X  (which is essentially K ). We are able to detect whether x i = 0 if there is just one observation near x i . To estimate the magnitude of x i , however, we need to see a  X  X luster X  of observations, e.g., two or more obser-vations which are essentially identical. This is the intuition for the minimum estimator and the gap estimator. Also, as one would expect, Fig. 2 shows that the performance will degrade (i.e., more observations are needed) as  X   X  i increases.
The gap estimator is a practical surrogate for the  X  X de-alized X  algorithm. Basically, for each i , if we sort the ob-ing observations corresponding to the minimum gap will be likely lying in a narrow neighborhood of x i , provided that the length of the minimum gap is very small, due to the heavy concentration of the probability mass about x i .
If the observed minimum gap is not small, we give up esti-mating this ( X  X ndetermined X ) coordinate in the current iter-ation. After we remove the (reliably) estimated coordinates, we may have a better chance of successfully recovering some of these undetermined coordinates because the effective  X  and the effective  X  N  X  are significantly reduced.
Both linear programming (LP) and orthogonal matching pursuit (OMP) utilize a design matrix sampled from Gaus-sian (i.e.,  X  -stable with  X  = 2) or Gaussian-like distribution. Here, we use S (2) to denote such a design matrix.
The well-known LP algorithm recovers the signal x by solving the following l 1 optimization problem: which is also commonly known as Basis Pursuit [4]. It has been proved that LP can recover x using M = O ( K log( N/K measurements [11]. This procedure is computationally pro-hibitive for large N (e.g., N =10 9 ). When there are mea-surement noises, the LP algorithm can be modified as other convex optimization problems, for example, the Lasso [25].
The orthogonal matching pursuit (OMP) algorithm [22] is a greedy iterative procedure. It typically proceeds with K iterations. At each iteration, it conducts univariate least squares for all the coordinates on the residuals, and chooses thecoordinatewhichmaxima lly reduces the overall square errors. At the end of each iteration, all chosen coordinates are used to update the residuals via a multivariate least square. [30, 12] showed that, under appropriate conditions, the required number of measurements of OMP is essentially O ( K log( N  X  K )), which improved the prior result in [26]. Our experimental study will focus on the comparisons with OMP and LP, as they are the basic and strong baselines. We are aware of other methods such as the  X  X essage-passing X  al-gorithm [9] and the  X  X parse matrix X  algorithm [13].
In parallel to this paper, we develop two other algorithms concurrently: (i) sparse recovery with compressed counting [19], by using skewed projections [17, 18], and (ii) sparse recovery with very sparse matrices [20], by using an idea similar to very sparse stable random projections [15] in KDD X 07.
To validate the procedure in Alg. 1, we provide some sim-ulations (and comparisons with LP and OMP), before pre-senting the theory. In each simulation, we randomly select K coordinates from a total of N coordinates. We set the magnitudes (and signs) of these K coordinates according to one of the two mechanisms. (i) Gaussian signals :theval-ues are sampled from Normal (0 , 5 2 ). (ii) Sign signals :we sian signals. The number of measurements M is chosen by where  X  =0 . 01 and  X   X  X  1 , 1 . 3 , 2 , 3 , 4 , 5 } .
Fig. 4 to Fig. 6 present instances of simulations, for sign signals, N = 100000 and K = 30. In each simulation (each figure), we generate the heavy-tailed design matrix S (with  X  =0 . 03) and the Gaussian design matrix S (2) (with  X  =2), using the same random variables ( w  X  X  and u  X  X ) as in (2). This provides shoulder-by-shoulder comparisons of our method with LP and OMP. We use the popular l 1 -magic package [1].
In Fig. 4, we let M = M 0 (i.e.,  X  =1). Forthis M ,all methods perform well. The left-top panel of Fig. 4 shows that the minimum estimator  X  x i,min can precisely identify all the nonzero coordinates. The right-top panel shows that the gap estimator  X  x i,gap applied on the coordinates identified by  X  x i,min , can accurately estimate the magnitudes. The label  X  min+gap(1)  X  means only one iteration is performed (which is good enough for  X  =1). Figure 4: Reconstruction results from one simulation, The bottom panels of Fig. 4 show that both OMP and LP also perform well when  X  = 1. OMP is noticeably more costly than our method (even though K is small) while LP is significantly much more expensive than all other methods.
We believe these plots of sample instances provide useful information, especially when M M 0 .If M is too small, then all methods will ultimately fail, but the failure pat-terns are important, for example, a  X  X atastrophic X  failure such that none of the reported nonzeros is correct will be very undesirable. Our method does not have such failures. Figure 5: Reconstruction results from one simulation,
Simulations in Fig. 5 use M = M 0 / 3 (i.e.,  X  =3). The minimum estimator  X  x i,min outputs a significant number of false positives but our method can still perfectly reconstruct signal using the gap estimator with one additional iteration (i.e., Min+Gap(2) ). In comparisons, both LP and OMP perform poorly and exhibit catastrophic failures.
Fig. 6 uses M = M 0 / 5 (i.e.,  X  = 5) to further demonstrate the robustness of our algorithm. As M is not large enough, a small fraction of nonzero coordinates are not recovered by our method, but there are no catastrophic failures. This point is of course already illustrated in Fig. 1 (with M  X  K
We also report the aggregated reconstruction errors and run times, using M = M 0 / X  with  X   X  X  1 , 1 . 3 , 2 , 3 , (
N, K )from { (10000 , 50) , (10000 , 100) , (100000 , 100) both Gaussian Normal (0 , 5 2 ) signals and sign signals. For each setting, we repeat the simulations 1000 times, except (
N, K ) = (100000 , 100), for which we only repeat 100 times.
For sparse recovery, it is crucial to correctly recover the nonzero locations. Here we use precision and recall Figure 6: Reconstruction results from one simulation, to compare the proposed absolute minimum estimator with LP decoding. Here, we view nonzero coordinates as  X  X osi-tives X  (p) and zero coordinates as  X  X egatives X  (n). Ideally, we hope to maximize  X  X rue positives X  (tp) and minimize  X  X alse positives X  (fp) and  X  X alse negatives X  (fn). In reality, we usu-ally hope to achieve at least perfect recalls so that the re-trieved set of coordinates contain all the true nonzeros.
Fig. 7 presents the (median) precision-recall curves. Our minimum estimator always produces essentially 100% re-calls, meaning that the true positives are always included for the next stage of reconstruction. In comparison, as M decreases, the recalls of LP decreases significantly. 4.2.2 Reconstruction Accuracy
The reconstruction accuracy is another useful measure of quality. We define the reconstruction error as Fig. 8 presents the median re construction errors. At M = M 0 (i.e.,  X  = 1), all methods perform well. For sign signals, both OMP and LP perform poorly as soon as  X &gt; 1 . 3or2 Figure 7: Median precision and recall curves, for com-and OMP results are particularly bad. For Gaussian signals, OMP can produce good results even when  X  =3.

Our method performs well, and 2 or 3 iterations of the gap estimation procedure help noticeably. One should keep in mind that errors defined by (7) may not always be as informative. For example, with M = M 0 / 5, Fig. 6 shows that, even though our method fails to recover a small frac-tion of nonzero coordinates, the recovered coordinates are accurate. In comparison, for OMP and LP, essentially none of the nonzero coordinates in Fig. 6 could be identified when M = M 0 / 5. We have seen the stability and reliability of our method in Fig. 1. In that example, even with M  X  K ,the reconstructed signal by our method is still informative. 4.2.3 Reconstruction Time
Fig. 9 confirms that LP is computationally expensive, us-ing the l 1 -magic package. In comparison, OMP is substan-tially more efficient than LP, although it is still much more costly than our algorithm, especially when K is not small.
In addition to the results reported in Fig. 9, we also ex-perimented with the SPGL1 package [27] (the faster .mex version) and found our method (implemented in Matlab) is still substantially much faster than SPGL1.
This section will develop the theoretical analysis of our method, including the minimum estimator and the gap esti-mator. The minimum estimator is not crucial once we have the gap estimator and the iterative process. We keep it in our procedure for two reasons. Firstly, it is faster than the gap estimator and is able to identify a majority of the zero Figure 8: Median reconstruction errors (7). When coordinates in the first iteration. Secondly, even if we just use one iteration, the required sample size for the minimum estimator M is essentially K log N/ X  , which already matches the complexity bounds in the compressed sensing literature.
Our analysis uses the distribution of the ratio of two in-dependent stable random variables, S 1 ,S 2  X  S (  X , 1). As a closed-form expression is not available, we compute the lower and upper bounds. First, we define where | S q ( u )= sin( based on (2) for generating  X  -stable random variables. The following lemmas provide useful bounds for F  X  ( t ). Lemma 1. For al l t  X  0 , Figure 9: Median reconstruction times, for comparing Lemma 2. If 0 &lt; X &lt; 1 / 3 ,then  X  2 =1 / cos (  X  X / (2  X  2  X  )) The const. C  X   X  1+1 / X  as  X   X  0 , C  X  &lt; 1 . 5 if  X   X  0 Lemma 3. For al l 0 &lt;s&lt;t , F  X  ( t )  X  F  X  ( s )  X  (1  X  s/t ) F  X  ( t )  X  ( t/s  X  1)
Recall the definition of the absolute min estimator: If nonzero entry. The task is to analyze the probability of in mind that, in the proposed method, i.e., Alg. 1, the min-imum estimator is just the first step for filtering out many true zero coordinates. False positives will have chance to be removed by the gap estimator and iterative process. 5.2.1 Analysis of False Positives
Theorem 1. If  X  = i.i.d. S (  X , 1) variables. When x i =0 , y j s ij =  X  S  X  =(1  X   X  ) / X  , By Lemma 1,
Pr ( |  X  x i,min | &gt;,x i =0)= Pr =[ Pr ( | S 2 /S 1 | &gt;/ X  )] M = 1  X  Pr | S 2 /S 1 | 1 / X   X  =(1  X  F  X  (  X  )) M  X  1  X  1
The assumption  X  =  X  small  X  because  X   X   X  K  X  1 /K , i.e., 1 /K &lt; 1 / 3. 5.2.2 Required Number of Measurements
We derive the required M , number of measurements, based on the false positive probability in Theorem 1. This result is useful if we just use one iteration, which matches the known complexity bounds in the compressed sensing literature.
Theorem 2. To ensure that the total number of false pos-itives is bounded by  X  ,itsufficestolet Since  X  =  X  as a convenient approximation. Note that the parameter affects the required M only through  X  . This means our algorithm is not sensitive to the choice of . For example, when  X  =0 . 03, then (10  X  3 )  X  =0 . 8128, (10  X  4 )  X  =0
Theorem 3. If  X   X  0 . 05 , | x i | +
Pr ( |  X  x i,min | X  , | x i | &gt; ) (16)  X   X   X   X  1  X  1  X  5.2.4 The Choice of Threshold
We can better understand the choice of from the false negative probability as shown in Theorem 3. Assume x i =0 and | x i | / = H i 1, the probability Pr ( |  X  x i,min | X  , | x upper bound is roughly As we usually choose M  X  M 0 = K log(( N  X  K ) / X  ), we have ordinates can be safely detected by the minimum estimator (i.e., the total false negatives should be less than  X  ), we need For sign signals, i.e., | x i | =1if x i = 0, we need 1 . 5 K = 100 (or 1000), it is sufficient to let =10  X  4 (or 10  X  Note that even with N =2 32 ,log( N ) = 22 is still not large.
For general signals, when the smallest H i dominates x i =0 , we just need the smallest H i &gt; 1 . 5  X  log N  X  K  X  / X  K term. In our experiments, for simplicity, we let =10  X  5 , for both sign signals and Gaussian signals. In general, with the gap estimator and the iterative process, we find the per-formance is not sensitive to as long as it is small.
The minimum estimator only detects the locations of nonzero coordinate (in the first iteration). To estimate the magni-tudes, we resort to the gap estimator, defined as
Theorem 4. Let  X  =(1  X   X  ) / X  . Suppose the existence of a &gt; 1 and integer k 0 &gt; 1 satisfying  X  ( a 0 k 0 / ( M Then where B ( M, a 0 k 0 /M )= Pr ( Binomial ( M, a 0 k 0 /M ) the binomial CDF, and  X  k, X  is defined as  X  k, X  =min u  X  (0 , 1) : 2 1  X 
Since k 0 in Theorem 4 only takes finite values, we can basically numerically evaluate G M,K  X  , X , X  to obtain the upper and are fixed, G is only a function of K  X  = K  X  , X , X  and the Figure 10: Values of the upper bound of the error prob-
Fig. 10 plots the upper bound G M,K  X  , X , X  for  X  =0 . 005 and 0.03, in terms of K  X  and M K  X  . For example, when using 5
K  X  and  X  = 0.005 / 0.03, the error probabilities are 0.042 / 0.084. In other word, in order for the error probability to be  X  0 . 05, it suffices to use M =5 K  X  if  X  =0 . 005. When  X  =0 . 03, we will have to use respectively M =7 K  X  measurements in order to achieve error probability &lt; 0
This way, the required sample size can be numerically computed from Theorem 4. Of course, these numerical val-ues are just the (possibly very conservative) upper bounds.
The error probability bound Pr ( |  X  x i,gap  X  x i | &gt; has two parts: min alized X  algorithm (assuming  X   X  0) and M  X  2 k = k 0 1+ 1 is the adjustment due to the use of  X &gt; 0. It is clear from the definition of  X  k, X  ,when  X  =0,wehave  X  k, X  =0and min is exactly the probability that one or zero observation falls in the region ( x i  X  e, x i + e )with e  X  0.

The error bound (20) holds for any small  X  and .With afixedsmall  X  , we can use enough measurements to bound Pr ( |  X  x i,gap  X  x i | &gt; )evenforaverysmall (for example, below the required precision). This means we can remove the reliably estimated x i and improve the reconstruction by iterations. This is why our procedure requires significantly smaller number of measurements than K log N/ X  . Figure 11: Reconstruction results from one simulation,
Our method is robust against measurement noise. In the literature, it is common to assume additive measurement noise y = xS + n ,whereeach n j is typically assumed to be n j  X  Normal 0 , X  2 N . We present a set of experiments with additive noise in Fig. 11. With N = 100000, K = 30, and M = M 0 (i.e.,  X  = 1), we have seen in the simulations in Sec. 4 that all methods perform well (when  X  = 0). In the presence of additive measurement noises (  X  =0 . 1), Fig. 11 illustrates that our proposed method still achieves perfect recovery while LP and OMP fail.

To understand this interesting phenomenon, we examine Without measurement noise, our algorithm uses observa-vations, most likely | S 1 | is very large. When S 1 is small, the observation is not useful anyway). This intuition ex-plains why our method is indifferent to measurement noise.
In parallel to this paper, we concurrently develop a new sparse recovery algorithm [19] using maximally-skewed sta-ble random projections [17, 18], which has a number of sig-nificant advantages over the method in this paper: (i) It al-lows thorough theoretical analysis at least for  X   X  (0 , 0 not just for  X  very close to 0. (ii) Both the theory and estimation procedure are much simpler. (iii) The accuracy is not as sensitive to  X  . The disadvantage is that [19] is restricted to nonnegative signals (which are common).
In addition, we also develop a sparse recovery algorithm based on  X  X ery sparse X  matrices [20], using an idea similar to very sparse stable random projections [15] in KDD X 07.
Compressed sensing has been an active area of research, as many important applications can be formulated as sparse recovery problems, for example, anomaly detections. In this paper, we present our study of using L0 projections for highly efficient exact sparse recovery. Our proposed pro-cedure consists of the minimum estimator (for detection), the gap estimator (for estimation), and the iterative pro-cess. The procedure is able to produce accurate recovery results with smaller number of measurements, compared to LP and OMP using traditional Gaussian (or Gaussian-like) design matrix. Our method utilizes the  X  -stable distribution with  X   X  0. The reported Matlab experiments use  X  =0 . 03. The algorithm is robust against measurement noises. Even without sufficient measurements, our method produces sta-ble (partial) recovery results with no catastrophic failures. Ping Li is partially supported by ONR-N000141310261, NSF-1131848, NSF-1249316, and AFOSR-FA9550-13-1-0137. Cun-Hui Zhang is partially supported by NSF-1209014, NSF-1106753, and NSA H98230-11-1-0205.
