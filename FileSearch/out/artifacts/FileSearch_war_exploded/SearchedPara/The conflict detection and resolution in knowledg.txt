 1. Introduction historians and general image-seekers, we had developed a serial of researches on semantic and ontology-based
One of the major motivations of Semantic Web (W3C, Hendler, Berners-Lee, &amp; Miller, 2002; McGuinness &amp; Harmelen, 2003; Motik &amp; Glavinic, 2000 ; DARPA ) is to enable web resources to support semantic information extraction and retrieval based on such formal description language standards as RDF, DAML, and OWL. The main idea is to have a set of well-defined machine readable semantic tags accessible by general web users and allow them to define sharable ontologies for a specific domain. The software agents can then utilize the sharable ontology to conduct inferences and analyze the domain knowledge on web. Each piece of knowledge or annotation in our systems is represented in terms of a standard rdf:triple as  X  Subject-slot, Predicate-slot, Object-slot  X  .
 semantic image retrieval system and the quality and quantity of annotations could affect the performance of and useful annotations by asking critical properties and suggesting possible domain common sense. However, in a web-based image annotation system, the annotators could be worldwide and their annotation behaviors can be unpredictable. As we know,  X  X  X n image is worth more than a thousand words X  X  and  X  X  X very one  X  s faults annotations to inconsistency.

Conducting consistent and complete/useful image annotation at the same time is not a trivial task and could be also a trade-off. When the quantity of annotations has been scaled up by an automatic process (e.g. AGA) or multiple annotating strategies are adopted, the contractions and conflicts of annotations can
This paper intends to resolve the annotation conflicts issues within the framework of the semantic web knowl-were used to evaluate the performance of image retrieval based on the annotations facilitated by automatic conflict resolution methods are described. In Section 7 , we discuss the experimental results and make conclusions. 2. Fundamental concepts and issues related to conflict resolution in annotation
Before we get into the core, some assumptions and fundamental concepts are introduced in this section. 2.1. The situations that a conflict happens A possible conflict in our image annotation and retrieval system happens in the following phases ( Fig. 1 ):
Conversion phase: a conflict happens in the translation from user  X  s annotation descriptions to the internal representation of the system.

Merging phase: a conflict happens in merging the annotations between two different annotators (or between an annotator and a software agent) for the same image.

Consistency of annotation: a conflict happens in the annotations (after the merging process). The annota-tion could still have potential contradictions in which are produced by a single annotator or by a merging process (Inference conflicts). For example, some descriptions could be right when we examine them individ-ually, however, they may lead to a wrong conclusion when they are putting together, or they may contradict to a fact that the system might not know yet. 2.2. A right annotation from different views
The annotation with multiple sources could face a difficult problem:  X  X  X ho/what is right? X  X  As we men-tioned,  X  X  X n image is worth more than a thousand words X  X , to detect a conflict in annotations that come from ods to resolve the  X  X  X ight annotation X  X  problem: 1. We adopt a single unique ontology as the base for translating the annotations and queries into semantic instances; it could map the annotations into a specific predefined domain. For example, the property of  X  X  X eight X  X  can only be annotated in the numbers going with a dimension in our system, then the adjective descriptions such as  X  X  X ery tall X  X  and  X  X  X hort X  X  will be pruned and ignored.  X  X  X irstName X  X  is a unique property for a specific person and it cannot be expressed in different terms, but other properties such as  X  X  X lias X  X  could have multiple values and can be annotated by more than one term. 3. For those Chinese terms sharing the same semantic code in our thesaurus are treated as the synonym. the most popular terms (used by most people) could be treated as the most appropriate annotation, even it could be wrong. 2.3. Assumptions
This study resolve the possible conflicts encountered in image annotation based on the semantic web rep-resentation. Its assumptions are: 1. The ontology is unified represented and error-proof, but can be incomplete. 2. An annotator can access the previous annotations that have been done by others. 4. The language in the system is in Mandarin Chinese; but the ontology is established in English (except some value in an object-slot that is expressed in Chinese). 2.4. Domain knowledge 2.4.1. Domain ontology 1999; Cranefield, 2001; Decker et al., 2000; Doan, Madhavan, Domingos, &amp; Halevy, 2002; Patel-Schneider,
Based on the pervious works, we realize that the expressing power and completeness of the ontology tend to affect the performance of the system. Therefore, we adopt the semantic web standard OWL to represent ontol-ogy in terms of rdf:triples. In our current domain ontology, there are 14 classes and 129 properties in the ogy are described in Fig. 2 .
 2000 ), in which the solid arrows denote the  X  X  X ubClassOf X  X  property and the dotted arrows denote the  X  X  X is-which is another subclass of  X  X  X hysical Thing X  X . We also have 129 additional properties in this domain for they are inherited from a super class  X  X  X hysicalThing X  X .

Someone may question about the disjoining relation between Animal and Item:  X  X  X ome items such as a teddy bear might have an animal looks but not a real animal bear X  X . To deal with such a problem, we devise a property named  X  X  X imicTo X  X  to specify an animal-like item. 2.4.2. The Mandarin Chinese thesaurus
The thesaurus is another important knowledge source for a software agent. It could provide term associ-ation; in particular: synonym, generalized (broader), and specialized (narrow) terms. We have constructed a Mandarin Chinese thesaurus based on a Chinese thesaurus known as  X  X  X ong-Yi Tsu Lin X  X , the total size of lexicons is more than 70,000 words that is organized in a semantic hierarchy, unfortunately, the antonym the synonyms of specific concept words. There are 12 categories/concept words in the first, 94 in the second, and 1428 in the third level respectively.
 2.4.3. The common sense inference rules
The common sense is a knowledge source for the system to reduce the efforts that might be needed for a human annotator in annotating an image. Common sense features refer to the simple, familiar and well-mon sense facts in any open information resources. Once the correlation among domain facts is established, it the information resources. In the annotation problem domain, we represent common sense as the association inference rules that can infer the concept association of one annotation based on others. One way to acquire the common sense association is to find triples (simple facts) that tend to have the same value in a class of objects based on their occurrence frequencies.

Example 1. Supposed that an annotator was annotating an image of a  X  X  X eneral X  X  and the AGA had recorded
According to the annotations, the AGA might be able to infer that the  X  X  X ender X  X  of a General could be a  X  X  X ale X  X  since most instances in the instance pool that were annotated as a  X  X  X eneral X  X  turned out to be a  X  X  X ale X  X  also.

Association rules ( Han &amp; Kamber, 2000 ) are used to implement the presupposition common sense (abbreviated as common sense below). An association rule can be expressed in terms of an associated relation of the rule) denoted as:
It indicates that a prerequisite implies the outcome. And both prerequisite and outcome could be a triple
Example 2. Supposed that all instances in Example 1 that are annotated as  X  Person, hasOccupation,  X  X  X en-can be written as an association rule: common sense. Later, when an annotator is annotating a new image using the triple  X  Person, hasOccupation, sense. It would be up to the human annotator to decide if the inferred common sense is correct. The way to determine an association rule as a common sense is using the confidence and support measures to decide if an
AGA will claim the corresponding triple as a common sense. The confidence and support measures are defined below.
 tain both prerequisites and outcomes. The confidence and support of an association rule Ri are defined as: number of total cases that refer to the same subject. 2.5. The reliability of knowledge and an annotator
The reliability of an annotator denotes a simple user model for the annotator. Since annotators do not tator, we define a measure called annotator faith degree (AFD) to indicate the reliability of an annotator. Definition. Annotator faith degree (AFD) is a real number between [0,1] that is a measure of how much an AGA could trust the annotator. It denotes as AFD( a ), where a is an annotator.

We also define knowledge faith degree (KFD) as a measure of the reliability of a piece of knowledge. In real world, no all knowledge is correct and accurate all the time. The uncertainty of knowledge usually leads to inconsistency and conflicts. Therefore recording a KFD for each piece of knowledge could help the system to deal with conflicts.

Definition. Knowledge faith degree (KFD) is a real number between [ 1,1] that is a measure of how much the system or a human annotator could trust a piece of knowledge. Denote as KFD AGA or a human annotator) and k is the piece of knowledge.

A little different from AFD, the number of KFD lies in the interval [ 1,1].  X  X  1 X  X  means that the system believes a piece of knowledge is absolutely wrong. On the other hand, the value should be  X  X 1 X  X  if the piece of knowledge is absolutely right. And a  X  X 0 X  X  denotes unknown. If we trust some one a with AFD( a )= d , and a believe in a piece of knowledge k that KFD a ( k )= f , so we can conclude that we believe in k through
KFD a ( k )= f , our belief of k through a will be AFD( a ) icate, Object  X  , so a KFD value is attached to each triple.

The KFD of the predefined knowledge in the thesauruses and domain ontology is set as either 1or1 (assumption 1 in Section 2.1 ) and the KFDs of commonsense inference rules are based on their support and confidence values as in formula (3) : where c is set as 0.1 in our system] is chosen as the enhanced function in our system, as show in Fig. 4 . The KFD of the inference results is calculated as where R ( k ) denotes the inferred results of rule R with an input (Prerequisite) k .
 The KFD of a piece of annotation k basically depends on the reliability of its annotator. where K b ( k ) denotes that the knowledge k is provided by annotator b .

Example 3. The KFD of an annotation T 1 by system s is a product of AFD of annotator a and its KFD of annotation T 1 (usually set as 1 or 1). And the KFD of an inferred result by rule R 1 is the product of KFD of rule R 1 and the KFD of its prerequisite P . Supposed that KFD dence( R 1) = 0.9, Support( R 1) = 0.3, P of R 1is T 1. Then KFD s (K a ( T 1)) = AFD( a )  X  KFD a ( T 1) = 0.8  X  1 = 0.8.
 KFD s ( R 1) = Confidence( R 1)  X  Enhanced_Function (Support( R 1)) = 0.9  X  0.886 = 0.798.
KFD s ( R 1( T 1)) = KFD s ( R 1)  X  KFD s ( P ) = 0.798  X  KFD
So the KFD of T 1 provided by a is 0.8, and the KDF of the inference result according to R 1 and its prere-quisite T 1 by the system s is 0.64. 3. Related work
The research on conflicts can be divided into two major paradigms, the knowledge merging conflicts flicts are usually referred to the knowledge merging conflicts.
 Similar to the database community ( Amgoud &amp; Parsons, 2002; Chomicki et al., 2000, Chomicki, Lobo, &amp; of knowledge. However, in comparison with database community, the semantic web community has to deal with more different types of knowledge sources and a more complicated ontology. Since the metadata descrip-the domain ontology usually can be still very complicated and the annotators may possess quite different con-cepts about the domain knowledge.

Many conflict problems in the semantic web community cannot be resolved using the database methods, because the ontology is different from the database schema (much larger and more complex). In addition, the knowledge sources in semantic web are quite open and the knowledge providers (annotators) are usually not under control and the annotations depend heavily upon their viewpoints. So the annotation conflict prob-lems cannot be resolved in a simple manner. They should be resolved individually based on different causes of conflict situations.
 as a defect in an ontology, a contradictory conflict after inferences, and an erroneous outcome from a faulty annotation (knowledge).

The related research communities to the annotation one are text-based information retrieval and extrac-tion communities known as TREC. However, most of the work in TREC ( Aronson et al., 2004; Ferres et al., 2004; Harabagiu et al., 2003; Hou, Teng, Lee, &amp; Chen, 2003; Lee et al., 2004, 2004a, 2004b; Lin, conflicts in their benchmark corpus caused by annotation. The annotations could be conducted by a single annotator or a team of annotators; but only a final standard annotation is used as benchmark for perfor-usually attempt to find the highest ranked answers rather than to resolve the possible contradictions in the answers. 4. The conspicuous conflicts
As mentioned in end of Section 1 , the conflicts are classified into two types, a conspicuous conflict and an inconspicuous conflict. The conspicuous conflict is discussed in this section.
 other with at least one attribute. The knowledge pieces could include the annotations from an annotator, the ontology (OWL) and background knowledge (the commonsense and a thesaurus). 4.1. The completeness of conspicuous conflicts
As the definition: two triples are related conspicuously only when at least one attribute are related to each other. A conspicuous conflict could happen at seven possible situations as shown in Table 1 .
Strictly speaking,  X  X  X ot equal subject X  X  (situations 2, 3, and 6) cannot be treated as conspicuous conflicts since they have different subjects. On the other hand, situations 1 and 5 would not become a conflict if they are sanctioned by the domain ontology. Therefore we classify the possible conspicuous conflicts into two types, data conflicts that deal with situations 4 and 7 and ontology conflicts that deal with others. 4.2. Data conflicts In our system, the data conflicts are defined as follows: pieces in the value (Object) slot of the triples.

The possible data conflicts for a specific subject in our domain include the following cases: 1. Duplicated data of a limited-cardinality property: Annotating a previously annotated annotation with lim-ited-cardinality (functional) property repeatedly. tated annotation with limited cardinality (functional) property. It includes (a) Synonym term, (b) General-ized term, (c) Specialized term, (d) Antonym term, and (e) Unrelated/different-view term. 3. Duplicated data of an unlimited-cardinality property: Annotating a previous annotated annotation with unlimited-cardinality property repeatedly. annotation with unlimited-cardinality property. It can also have five possible situations as in case 2. 5. Annotation at different abstraction levels: Different annotators may use concepts at different abstraction the different of annotation between  X  X  X erson X  X  and  X  X  X nimal X  X ,  X  X  X asColor X  X  and  X  X  X asSkinColor X  X , as well as  X  X  X aiwan X  X  and  X  X  X aipei X  X . 4.3. Data conflict detection and resolution
In order to detect the data conflicts, several simple detecting rules are shown in Table 2 . When the system
Table 2 , O 1  X  O 2 indicates that O1 and O2 are synonyms, O
And the possible resolution actions that the system may take to resolve the detected data conflicts are shown in Table 3 . Basically, the actions that the system can take are: abandon the new one, add the new one, replace the old one with the new one, replace the old one with the modified new one, and update
KFD. The action  X  X  X bandon NA X  X  means the system does not accept the new piece of annotation NA,  X  X  X dd NA X  X  means the system accepts the new piece of annotation,  X  X  X eplace PA by NA X  X  and  X  X  X eplace to either to remove the old one or to make some modifications on the new piece of annotation respectively. In when the PA have a specialized predicate, the NA will be modified, vice versa.  X  X  X pdate KFD X  X  means the system would update the KFD of a piece of annotation if more than one annotator adds the same piece of annotation.

According to the above conflict detection rules and resolution actions, we can resolve the data conflicts shown in Appendix A . For example: the  X  X  X uplicated data in limited-cardinality property X  X  will be detected, the new annotation (A1) and update the KFD (A5).
 the action  X  X  Add NA (A2)  X  X  is a standard action in our annotation system (if a new annotation is not aban-d may be modified during the annotation process.
 slot, R 7 X  X he cardinality number of predicates, and R 8 X  X  generalized relationship between two predicates.
Where the R 1to R 6 will be a conflict when they co-occur with R 7 (a limited cardinality with a contracted value), and R 8 is a detecting rule which concert about the relationship between two predicates, therefore, the R 8 should be considered first (if the R 8 matched, system do not need to consider the other rules), then since the R 1 (Duplicated value) and R 5 (Antonym value) will be a conflicts with entire state of cardinality number of the predicates, (that means duplicated/antonym is a conflict in any state of predicate), so the R 1 and R 5 can be examined individually.
 ical records of annotation could be very important information in conflict handling and also the contraction condition might change overtime as the reviewer had pointed out. However, it requires a huge storage to keep triples, if we have p images in our image base, then we would have to prepare n ability of annotators instead, that have indirectly taken into the historical annotation information into consideration. We have conducted other research on the annotator model ( Lee &amp; Soo, 2005a, 2005b ) that elaborate the annotator could be modeled precisely.
 4.4. Ontology (schema/structure) conflicts
Although the assumption is based on a unified and sharable ontology, however, the annotators still may very often violate or misunderstand the ontology. We define the ontology conflicts as follows: Definition. Ontology conflicts . The annotation of an annotator conflicts with the existing ontology. The possible ontology conflicts in our domain include: (the details of ontology conflicts are described in
Appendix B ) 1. Subject conflict. The conflict causes a wrong description about a subject that can be further divided into four cases. 2. Property conflict. The property conflict leads to a wrong property description that can be further divided into three cases. into three cases. tuples (subject, predicate, and object).

We use several simple checking rules in Table 5 to detect the ontology conflicts. When the system receives a since if a class C is a child of another class C 0 , C will inherit all the attributes of C that has nothing to do with C 0 , so does the omission of the specialization of a predicate. (Specialized(S)), so the actions the system can take are: Suggest a new triple  X  S * ,P,O  X  such that S * =
Specialized(S). 5. The inconspicuous conflicts of knowledge and are known as inconspicuous conflicts. We propose ways to detect the inconspicuous con-flicts and some rough methods for resolutions actions.
 edge) might not always show conspicuous conflicts. However, the combination of those pieces of knowledge may sometimes lead to inconsistency based on the background knowledge. Therefore we can also call it knowledge inconsistency conflicts. 5.1. The completeness of inconspicuous conflicts
Since  X  X  X n image is worth more than a thousand words X  X  and the range of annotation cannot be limited, the completeness of the inconspicuous conflicts cannot be guaranteed. In this study, we only discuss the possible rent version of semantic web ontology standard OWL (include three types), unreasonable annotations or might happen after an inference processing, The possible inconspicuous conflicts include five types. 5.2. The types of inconspicuous conflicts 5.2.1. One-to-one property
The functional property P ( x , y ) may imply another functional property P to-one attribute mapping in the ontology language may cause some one-to-one properties to become in con-flict. For example,  X  Person1, hasBody, Body1  X  and  X  Person2, hasBody, Body1  X  becomes a conflict, because  X  X  X asBody X  X  is a one-to-one property (namely, two different persons cannot own the same body in real world). 5.2.2. Not a Relation
The semantic web ontology languages (either DAML or OWL) lack of the capability to describe the  X  X  X asAncestor X  X  absolutely is NOT a reflexive and symmetric property, and  X  X  X asFriend X  X  is not a transitive friendship between me and John). So the declaration of relations should include is-relation (Positive), is-Not-relation (Negative), and No-relation (not need to declare). We propose four kinds of is-NOT-relation that include: (b) Not symmetric. P is-Not-symmetric implies A P B and B P A are conflicts. For example,  X  X  X asFather X  X  (d) Not Cyclic. P is-Not-cyclic implies A P B, B P C, and C P A are conflicts. For example,  X  X  X asFather X  X  5.2.3. Mutual exclusion
Most property relations can coexist for the same subject X  X bject pair. For example, the property relations  X  X  X asBoss X  X  and  X  X  X asFather X  X  can coexist for the same pair between your father and you because your father can be your boss as well. However, some property relations cannot coexist for the same subject X  X bject pair. son2  X  are in conflict because Person1 and Person2 can only has a unique relation among  X  X  X asFather X  X ,  X  X  X asBrother X  X , and  X  X  X asSister X  X  and cannot be coexist for two same pair of persons. 5.2.4. The data conflicts after logical inference the inferential conflicts easily, we define the following symbols: k i : a single piece of knowledge in term of a triple  X  S, P, O  X  . r i : a single (common sense) inference rule represented as  X  Prerequisite  X  !  X  Outcome  X  .
K x : knowledge and beliefs of actor x represented in terms of a set of triples { k
R x : inference rules of actor x (inference rules) as a set of rules { r
IK x ( K x , R x ): the inferred knowledge base of K x after applying inference rules R
The data conflicts caused by logical inference include the following four cases: (a) Lack of knowledge may induce the inference conflicts. If two annotators have different set of knowledge (usually they do), the one with less knowledge may infer differently from the one with more knowledge and thus cause inferred data conflicts.
 Example 4. Supposed there are two annotators a and b .

K a ={ k 1 , k 2 }, K b ={ k 1 }, R a = R b ={ r 1, r 2}. k 1 =  X  Person1, hasFirstName,  X  X  X acky X  X   X  . k 2 =  X  Person1, Live, Location1  X  ,  X  Location1, hasLocationName,  X  X  X ongKong X  X   X  . r 1:  X  Person, hasFirstName,  X  X  X acky X  X   X  !  X  Person, hasGender,  X  X  X emale X  X   X  (Jacky usually named female at
USA). r 2:  X  Person, hasFirstName,  X  X  X acky X  X   X  and  X  Location1, hasLocationName,  X  X  X ongKong X  X   X  !  X  Person, has-Gender,  X  X  X ale X  X   X  (Jacky usually is a name of a male at HongKong).
 K a ( k 1, k 2, r 2) = {  X  Person1, hasGender,  X  X  X ale X  X   X  }.
 IK b ( k 1, r 1) = {  X  Person1, hasGender, X  X  X emale X  X   X  }.

Therefore annotators a and b have inferred different genders for Person1 and it causes a data conflict. (b) A wrong piece of knowledge may cause inferred data conflicts. The mistakes of annotations may lead to inference failure and it happens quite often.
 Example 5. What if the k 2 of b is wrong?
K a ={ k 1 }, K b  X f k 2 g , R a = R b ={ r 1 }. k 1 =  X  Person1, hasGender,  X  X  X ale X  X   X  . k 2 =  X  Person1, hasFirstName,  X  X  X ichel X  X   X  (Typo error of  X  X  X ichael X  X ). r 1 :  X  Person, hasFirstName,  X  X  X ichel X  X   X  !  X  Person, hasGender,  X  X  X emale X  X   X  (Michel usually named female). K a ={  X  Person1, hasGender,  X  X  X ale X  X   X  }.

IK b ( k 1 , r 1 )={  X  Person1, hasGender,  X  X  X emale X  X   X  }.
Then annotators a and b apparently lead to inferred data conflict. (c) The faulty inference rules cause inferred data conflicts. The faulty or improper inference rules may lead training cases are too small or too old or are barrowed from other domains).
 Example 6
K a ={ k 1 , k 2 }, K b ={ k 1 }, R a  X  R b  X f r 1 g . k 1 =  X  Person1, hasLastName,  X  X  X hen X  X   X  . k 2 =  X  Person1, hasGender,  X  X  X emale X  X   X  . r 1 :  X  Person, hasLastName,  X  X  X hen X  X   X  !  X  Person, hasGender,  X  X  X ale X  X   X  (Chen is usually a family name of a male if the erroneous inference rule is unfortunately induced from a military domain where everyone is a male).
 K a ={  X  Person1, hasLastName,  X  X  X hen X  X   X  ,  X  Person1, hasGender,  X  X  X emale X  X   X  }.
 IK b  X  k 1 ; r 1  X  X fh Person1 ; hasLastName ; Chen i ; h Person1 ; hasGender ; male ig .
Then annotators a and b apparently lead to inferred data conflict. (d) Using different set of inference rules may lead to different influence results. The rules may be conflict with each other, especially when rules are generated from a statistical manner and the relaxations of chosen thresholds may lead to conflicts between different set of rules.

Example 7. If the rules are established when most soldiers are males and some data shows that Mary is a name of a female, it will bring data conflicts.

K a ={ k 1 , k 2 }, K b ={ k 1 , k 2 }, R a ={ r 1 }, R b r 1 :  X  Person, hasOccupation,  X  X  X oldier X  X   X  !  X  Person, hasGender,  X  X  X ale X  X   X  (A soldier usually is a male). r 2 :  X  Person, hasFirstName,  X  X  X ary X  X   X  !  X  Person, hasGender,  X  X  X emale X  X   X  (Mary is usually a name of a female).
 IK a ( k 1 , k 2 , r 1 )=  X  Mary, hasGender,  X  X  X ale X  X   X  .
 IK b ( k 1 , k 2 , r 2 )=  X  Mary, hasGender,  X  X  X emale X  X   X  .

Then annotators a and b apparently lead to inferred data conflict. 5.2.5. Unreasonable annotation
An unreasonable annotation refers to a triple that is in conflict with the real world knowledge that the sys-tem has not yet established. In some cases, the conflicts occur when some annotator makes an unreasonable annotation so that it is in conflict with the real world knowledge. The following case is an example that all annotations are legal except the last one that is unreasonable because the dogs in real world have no wings at all.
 Example 8  X  Animal1, hasCategoryName,  X  X  X og X  X   X  .
  X  Animal1, hasBody, Body1  X  .
  X  Body1, hasBodyPart, BodyPart1  X  .
  X  BodyPart1, hasBodyPartName,  X  X  X ing X  X   X  * .

Since the piece of knowledge  X  X  X ogs has no Wing X  X  is not in the system knowledge base, it is unlikely for the system to detect this unreasonable annotation. However, it is very difficult to formulate all unreasonable pieces of knowledge since they cannot be exhaustively enumerated. It is also related to the well-known frame problem in knowledge representation.
 5.3. The detection and resolution of inconspicuous conflicts
Most of conspicuous conflicts may be detected and resolved by simple rules such as in PDL ( Chomicki able facts X  X  could not be detected by simple detecting rules. It is because their occurrences are based on an incomplete and inconsistent knowledge or the correctness of an annotation. And it is a matter of  X  X  X ho/what lution could be a complex task.

The inconspicuous conflicts in the following cases are due to the incompleteness of domain ontology and they cannot be detected and resolved without related background knowledge. Therefore the first strategy we adopt is to add some additional features into the domain ontology. However, we also need to determine if a tion actions. 1. One-to-one property 2. Not a Relation 3. Mutual exclusion 4. Data conflicts after logical inference.
 (a) Inference conflicts due to the lack of a piece of knowledge (b) Wrong piece of knowledge induces inference conflicts. (c) The faulty inference rules induce data conflicts. (d) Different rule sets lead to inconsistent inference.
 5. Unreasonable annotation 5.4. Prevent the interacting conflicts among conflict resolutions
In the above conflict resolution process, a resolution of a conflict may sometimes cause a new conflict with and we cannot process them separately. Supposed that two annotations exist in the current knowledge base,
T1:  X  Person1, hasBody, Body1  X  and T2:  X  Person2, hasBody, Body2  X  with KFD(T1) &lt; KFD(T2). When a new annotation T 0  X  Person1, hasBody, Body2  X  with KFD(T1) &lt; KFD(T because  X  X  X asBody X  X  is a functional property. However, T 0 also a one-to-one property, and T 0 will be abandoned because its KFD is lower than T2. As a result in this case, abandon T1 could be a mistake, since the T 0 could be a wrong annotation with respect to the T2 and should not be used to abandon T1.
 In order to prevent the mistake during conflict resolutions, we adopted two separate queues as Removing
Queue and Adding Queue to keep the annotations to be removed and added respectively. Those annotations that are waiting to be added to the knowledge base are kept in the Adding Queue while those annotations that annotations that are in conflict with the adding annotations) from the knowledge base are kept in the Remov-ing Queue. In the following example, T2 is kept in the Adding Queue, therefore, T1 is kept in the Removing
Queue since T1 is in conflict with T2, and T3 and T4 are also kept in the Removing Queue since T3and T4 are inferred from T1.
 Adding Queue ={  X  T2  X  } Removing Queue ={(  X  T2  X  DC )  X  T1  X  ,(  X  T1  X  ! )  X  T3  X  ,(  X  T1  X  ! )  X  T4  X  }
Where the description between brackets indicates the reason of removing,  X  X  X C X  X  stands for  X  X  X ata conflicts X  X  and  X  X  !  X  X  stands for  X  X  X nference X  X .

Moreover, if T2 is not to be added for any reason (T2 was removed from the Adding queue before adding as an instance), then T1, T3 and T4 are taken away from the Removing Queue. Since the dependency and interactions among the annotations is taken care only at a single round of annotation in the system, it would not guarantee the consistency in the future (we cannot guarantee what will happen after T3 and T4 are removed from the instance, and there still could have conflicts between the Adding queue and the Removing queue). In this paper, the Adding queue has more priority than Removing queue, and it only overcomes part the problems. However, the conflict resolution strategies would gradually lead to more consistent and com-plete annotations. 6. Experiments and discussion 6.1. The initial settings for annotation experiments
In order to illustrate the conflict detection and resolution problems in annotation, we adopt the domain on many kinds of annotation conflicts in comparison to the other domains. We asked annotators to annotate at such restricted aspects as human relations and some fundamental profile for a person (first name, last name, their own annotations on a person. We chose 10 images. Each of the images has one to three celebrities con-dents including some graduates and undergraduates were grouped randomly into seven groups. Everyone in each group was asked to annotate the same three images that were randomly chosen from the 10 images.
In this way, each image was annotated by 16 X 20 annotators. In this paper, the number of annotation targets conflicts between image annotations by multiple annotators; if too many images are used as testing, the num-each image can be annotated by expert, common, and novice annotators, then we can examine and evaluate the conflict resolution method more easily.

After the annotation of the images, we estimated the AFD for each annotator. In Fig. 5 (a) X (d), the x -coor-dinate is the ID for each annotator (indexed from A00 to G08), and the y -coordinate indicates the AFD value for each annotator. Fig. 5 (d) shows that the average AFD for each of the 62 annotators that is averaged over the AFDs based on the annotation results from the first image as shown in Fig. 5 (a), the second image as shown in Fig. 5 (b) and the third image as shown in Fig. 5 (c) together. Each AFD value for an annotator over a specific image is calculated as following: 1. AFD is initially assigned as 0.5. 2. Each attribute of an image has x score for a correct annotation and x points for the incorrect annotation ( x = 0.1 in our case). 3. If there are m annotators who made a correct annotation for the attribute, then the AFD for each correct annotator is incremented by an amount x / m . 4. If there are n annotators who made an incorrect annotation for the attribute, then the AFD for each incor-rect annotator is decremented by an amount x / n .

Fig. 5 (d) shows that some annotators tend to have higher AFDs than others, which means they are more familiar with the celebrity domain in comparison to others. 6.2. Performance evaluation experiments
After the initial setting of experiments, we conducted the comparison experiments to evaluate the perfor-mance of the proposed automatic conflict detection and resolution methods during the annotation of images. For each image, we chose a dozen of annotators randomly to conduct two separate groups of experiments.
One was called the control group in which the annotators always replaced old annotations with new ones and abandoned the duplicated annotations. The other was called experiment group in which the system auto-matically detected and resolved the arising conflicts and assisted human annotation using the methods dis-cussed in the above sections. In Fig. 6 , it illustrated the comparison of performances in the two groups of experiments in terms of the average number of correct and incorrect annotations. It showed that the average number of correct annotations in the experiment group is in general higher than that in the control group and the average number of incorrect annotations in the experiment group is lower than that in the control group.
Fig. 7 shows the average accuracy of annotations for the 10 images in each group, where the y -coordinate is number. The accuracy of annotation is calculated using following formula:
Therefore, the average accuracies of control and experiment groups are very low, they are 0.1854 and 0.3092 respectively. As shown in Fig. 7 , the performance of automatic data conflict resolution methods improves 12.38% from 18.54% of accuracy in comparison to a na X   X  ve annotation system that simply replaces the old annotation by new ones in dealing with conflicts. Furthermore, for each of the 10 images, we also tested 100 different annotator sequences to examine the order sensitivity of the system. We calculated the variance for each of the two groups to examine the order sensitivity on each image, the variances are shown in
Fig. 8 . It shows that the experiment group (solid line) tends to have lower variance than the control group (dotted line). It implies that the automatic conflict resolution methods are less sensitive to the order of annotators.

The only exception in Fig. 8 is the image 3 in which the experiment group (Incorrect) has a slightly larger variance than the control group. The reason for this exception is the AFD model is not powerful enough to classify among annotators. The AFDs of annotators in image 3 of the experiment group are very similar (located between 0.5 and 0.7).
 7. Conclusions and future works
Conflicts in image annotation processes may affect the performance of image retrieval significantly. In designing an automated semantic annotation system for image retrieval, conflict detection and resolution com-ponents are necessary for an AGA to mediate in the loop to aid human annotators to annotate images correctly.

In order to evaluate the performance of conflict detection and resolution for image annotation, we first
It is worthwhile to mention that a good annotator model would help AGA to decide whether to trust the annotation by a given annotator. The annotator model could be a very important factor of this approach.
However, as we observe in Fig. 8 , the user model we adopt was not good enough. In future work, a more sophisticated annotator model should not only improve the conflict resolution problem, but also should be used in AGA to deal with the annotation guiding problem.

The experimental results justified our conjectures that the conflict detection and resolution approaches based on appropriate annotator model could help annotators to annotate the image effectively. The perfor-mance comparison experiment showed the average accuracy in experiment group was improved about 12.38%. The automatic data conflict resolution methods had improved the accuracy. However, even assisted by the automatic conflict resolution methods, the average annotation accuracy was still very low (namely
The problems of multiple relations, the negative knowledge, and the unreasonable annotations are just some a faulty knowledge model to capture all negative knowledge in the open world environment. The same reason applies to dealing with the problems of unreasonable annotations. For example, we encountered five annotations that were annotated with  X  X  X asAlias X  X  in which only two annotations were recognized correctly.
Even we can filter some unreasonable annotation to decrease the chance of committing to incorrect annota-work.

The issue:  X  X  X ow to determine a property is a One-To-One/is-Not-Relation/Mutual-exclusion property be resolved more easily.

In the paper, we only used a single AFD value instead of a more complicated AFD set to model an annotator. The rationale behind it is that the application domain in this study is very narrow and a simple model can be much easier to implement and evaluated. Of course, a more elaborated annotator model will 2005b ).

We also observe that several problems could affect the correctness using a general search engine approach for unreasonable annotation: 1. The polysemous terms will lead to a wrong conclusion. For example, if  X  X  X ogs X  X  and  X  X  X ings X  X  are the names of two rock bands that could relate them to each other, but it can be unreasonable in other annotation. 2. Some properties such as the name of a person, the quantity of some measure and a proper noun cannot be applicable. 3. The relationship of two terms cannot be defined precisely. When two terms are related in a unique way, we might not have proper expression to indicate the relations.

Although the search engine based approach has many drawbacks, but it provides at least some useful infor-our future work.

In a summary, we have classified different types of conflicts and have devised their corresponding conflict detection methods and have implemented the automatic conflict resolution strategies to assist human annota-tors. The methodologies proposed have shed some light in dealing with the automatic conflict resolution prob-lems in knowledge merging for image annotations. Acknowledgments This research is supported in part by Taiwan Ministry of Education Program for Promoting Academic
Excellence of Universities under grant number 89-E-FA04-1-4 and by National Science Council under grant number 93-3112-B-007-009.
 at the Stanford University School of Medicine (Prote  X  ge  X  ).
 Appendix A. The detecting rules and resolution actions for data conflicts particular, d &gt; 0 when the annotation is directly annotated by a human annotator]. 1. Duplicated data in limited-cardinality property.

D: R1, R7. A: A1, A5. 2. Different values in limited-cardinality property. (a) Synonym term.
 (b) Generalized term.
 (c) Specialized term.
 (d) Antonym term.
 (e) Irrelevant/different-view term 3. Duplicated data in unlimited-cardinality property.

D: R1 , : R7. A: A1, A5. 4. Different values in unlimited-cardinality property. (a) Synonym term.
 (b) Generalized term.
 (c) Specialized term.
 (d) Antonym term.
 (e) Unrelated term.
 5. Annotation precision (at different abstraction levels) (a) Different subject granularities.
 (b) Different property granularities. (c) Different Class Object granularities.
 (d) Different Data Object granularities.

Appendix B. The possible ontology conflicts the following four cases: (b) Using a wrong attribute-value pair to describe an unrelated subject. For example, in the triple  X  Item, (c) Using a wrong identity name to refer to a subject. For example, in the triple  X  Person2, hasGender, (d) Using an unknown subject name to refer to a subject. For example, in the triple  X  LEE, hasGender, 2. Property conflict. The property conflict leads to a wrong property description that can be further divided into the following three cases. (e) Using a wrong attribute concept to describe a related property. For example, in the triple  X  Item, 3. Object conflict. The object conflict leads to a wrong description about an object. (h) Using a wrong object type. For example, in the triple  X  Person, hasMother, Item  X  , the object should be (k) Quantity inconsistency (violation of cardinality constraint). The number of annotation does not fit the  X  X  X asGender X  X , and  X  X  X ction X  X  have nothing to do with each other.
 Appendix C. The detecting rules and resolving actions for ontology conflicts symbol  X  X  D  X  X  indicates the detecting rules,  X  X  A  X  X  indicates the resolution actions. 1. Subject conflict: (a) Using a wrong but related subject S.
 (b) Using a wrong and unrelated subject S.
 (c) Using a wrong subject identity name for S.
 (d) Using an unknown subject S.
 2. Property conflict: (a) Using a wrong but related property P.
 (b) Using a wrong and unrelated property P.
 (c) Using an unknown property P.
 3. Object conflicts: (a) Using a wrong and unrelated object O.
 (b) Using a wrong data type.
 (c) Using an unknown object O.
 (d) Quantity inconsistency (cardinality limitation).
 4. More than two slots are wrong: expressed in different and unrelated manners such as  X  Item, hasGender, Action  X  . This situation can be detected by tation and also to decrease the reliability of the annotator.
 Appendix D. The properties need to be prevented from inconspicuous conflicts
In our working domain ontology, the related properties that need to be prevented for each types are: References Type Example properties One-to-one hasBody
Not a reflexive hasBodyPart, composeOf, carry, hasMate, hasMother, hasFather,
Not a symmetric hasBodyPart, composeOf, carry, hasMother, hasFather, hasOlderBrother, Not a transitive hasMother, hasFather
Not a cyclic hasBodyPart, composeOf, carry, hasMother, hasFather, hasOlderBrother,
Mutual Exclusion {hasMate, hasMother, hasFather}, {hasYoungerBrother, hasOlderBrother, Further reading
