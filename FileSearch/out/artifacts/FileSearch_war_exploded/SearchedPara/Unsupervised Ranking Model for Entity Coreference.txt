 Entity coreference resolution has become a critical component for many Natural Language Processing (NLP) tasks. Systems requiring deep language un-derstanding, such as information extraction (Wellner et al., 2004), semantic event learning (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and named entity linking (Durrett and Klein, 2014; Ji et al., 2014) all benefit from entity coreference infor-mation.

Entity coreference resolution is the task of iden-tifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities. In recent years, several supervised entity corefer-ence resolution systems have been proposed, which, according to Ng (2010), can be categorized into three classes  X  mention-pair models (McCarthy and Lehnert, 1995), entity-mention models (Yang et al., 2008a; Haghighi and Klein, 2010; Lee et al., 2011) and ranking models (Yang et al., 2008b; Durrett and Klein, 2013; Fernandes et al., 2014)  X  among which ranking models recently obtained state-of-the-art performance. However, the manu-ally annotated corpora that these systems rely on are highly expensive to create, in particular when we want to build data for resource-poor languages (Ma and Xia, 2014). That makes unsupervised ap-proaches, which only require unannotated text for training, a desirable solution to this problem.
Several unsupervised learning algorithms have been applied to coreference resolution. Haghighi and Klein (2007) presented a mention-pair non-parametric fully-generative Bayesian model for un-supervised coreference resolution. Based on this model, Ng (2008) probabilistically induced corefer-ence partitions via EM clustering. Poon and Domin-gos (2008) proposed an entity-mention model that is able to perform joint inference across mentions by using Markov Logic. Unfortunately, these un-supervised systems X  performance on accuracy sig-nificantly falls behind those of supervised systems, and are even worse than the deterministic rule-based systems. Furthermore, there is no previous work exploring the possibility of developing an unsuper-vised ranking model which achieved state-of-the-art performance under supervised settings for entity coreference resolution.

In this paper, we propose an unsupervised genera-tive ranking model for entity coreference resolution. Our experimental results on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012) show that our unsupervised system outperforms the Stanford deterministic system (Lee et al., 2013) by 3.01% absolute on the CoNLL official metric. The contributions of this work are (i) proposing the first unsupervised ranking model for entity coreference resolution. (ii) giving empirical evaluations of this model on benchmark data sets. (iii) considerably narrowing the gap to supervised coreference reso-lution accuracy. 2.1 Notations and Definitions In the following, D = { m 0 ,m 1 ,...,m n } repre-sents a generic input document which is a sequence of coreference mentions, including the artificial root mention (denoted by m 0 ). The method to detect and extract these mentions is discussed later in Sec-tion 2.6. Let C = { c 1 ,c 2 ,...,c n } denote the coreference assignment of a given document, where each mention m i has an associated random vari-able c i taking values in the set { 0 ,i,...,i  X  1 } ; this variable specifies m i  X  X  selected antecedent ( c i  X  { 1 , 2 ,...,i  X  1 } ), or indicates that it begins a new coreference chain ( c i = 0 ). 2.2 Generative Ranking Model The following is a straightforward way to build a generative model for coreference: where we factorize the probabilities P ( D | C ) and P ( C ) into each position j by adopting appropri-ate independence assumptions that given the coref-erence assignment c j and corresponding corefer-ent mention m c with other mentions in front of it. This indepen-dent assumption is similar to that in the IBM 1 model on machine translation (Brown et al., 1993), where it assumes that given the corresponding En-glish word, the aligned foreign word is independent with other English and foreign words. We do not make any independent assumptions among different features (see Section 2.4 for details).

Inference in this model is efficient, because we can compute c j separately for each mention: The model is a so-called ranking model because it is able to identify the most probable candidate an-tecedent given a mention to be resolved. 2.3 Resolution Mode Variables According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions. For example, the Stanford system (Lee et al., 2013) uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two men-tions which satisfy special syntactic or semantic relations such as apposition or acronym. Moti-vated by this, we introduce resolution mode vari-ables  X  = {  X  1 ,..., X  n } , where for each mention j the variable  X  j  X  { str,prec,attr } indicates in which mode the mention should be resolved. In our model, we define three resolution modes  X  string-matching ( str ), precise-construct ( prec ), and attribute-matching ( attr )  X  and  X  is deterministic when D is given (i.e. P ( X  | D ) is a point distribu-tion). We determine  X  j for each mention m j in the following way:  X   X  j = str , if there exists a mention m i ,i &lt; j  X   X  j = prec , if there exists a mention m i ,i &lt; j  X   X  j = attr , if there is no mention m i ,i &lt; j Now, we can extend the generative model in Eq. 1 to: where we define P (  X  j | j ) to be uniform distribution. We model P ( m j | m c
Algorithm 1: Learning Model with EM lowing way: where  X  = { t,q } are parameters of our model. Note that in the attribute-matching mode (  X  j = attr ) we model P ( c j |  X  j ,j ) with parameter q , while in the other two modes, we use the uniform distribu-tion. It makes sense because the position informa-tion is important for coreference resolved by match-ing attributes of two mentions such as resolving pro-noun coreference, but not that important for those resolved by matching text or special relations like two mentions referring the same person and match-ing by the name.
 2.4 Features In this section, we describe the features we use to represent mentions. Specifically, as shown in Ta-ble 1, we use different features under different reso-lution modes. It should be noted that only the Dis-tance feature is designed for parameter q , all other features are designed for parameter t . 2.5 Model Learning For model learning, we run EM algorithm (Demp-ster et al., 1977) on our Model, treating D as ob-served data and C as latent variables. We run EM with 10 iterations and select the parameters achiev-ing the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.The de-tailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algo-rithm 1. We use uniform initialization for all the pa-rameters in our model.
 Several previous work has attempted to use EM for entity coreference resolution. Cherry and Bergsma (2005) and Charniak and Elsner (2009) applied EM for pronoun anaphora resolution. Ng (2008) probabilistically induced coreference parti-tions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model uti-lizing the most informative relations and achieved competitive performance with the Stanford system. 2.6 Mention Detection The basic rules we used to detect mentions are simi-lar to those of Lee et al. (2013), except that their sys-tem uses a set of filtering rules designed to discard instances of pleonastic it , partitives, certain quanti-fied noun phrases and other spurious mentions. Our system keeps partitives, quantified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions. 3.1 Experimental Setup Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Giga-word Corpus (years 1994-2010) (Parker et al., 2011) to train the model. Following previous work (Cham-bers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The cor-pora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees).
 Evaluation Metrics . We evaluate our model on three measures widely used in the literature: win, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005). In addition, we also report results on an-other two popular metrics: Mention-based CEAF (CEAF m ) and BLANC (Recasens and Hovy, 2011). All the results are given by the latest version of 3.2 Results and Comparison Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford : the Stanford system (Lee et al., 2011) and Multigraph : the unsupervised multigraph sys-tem (Martschat, 2013), and one unsupervised sys-tem, namely MIR : the unsupervised system using most informative relations (Moosavi and Strube, 2014). Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford sys-tem, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respec-tively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improve-ments of 2.62% and 3.02% on CoNLL F1 score.

To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art su-pervised coreference resolution systems  X  IMS : the second best system in the CoNLL 2012 shared task (Bj  X  orkelund and Farkas, 2012); Latent-Tree : the latent tree model (Fernandes et al., 2012) ob-taining the best results in the shared task; Berkeley : the Berkeley system with the final feature set (Dur-rett and Klein, 2013); LaSO : the structured percep-tron system with non-local features (Bj  X  orkelund and Kuhn, 2014); Latent-Strc : the latent structure sys-tem (Martschat and Strube, 2015); Model-Stack : the entity-centric system with model stacking (Clark and Manning, 2015); and Non-Linear : the non-linear mention-ranking model with feature represen-tations (Wiseman et al., 2015). Our unsupervised ranking model outperforms the supervised IMS sys-tem by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3. We proposed a new generative, unsupervised rank-ing model for entity coreference resolution into which we introduced resolution mode variables to distinguish mentions resolved by different cate-gories of information. Experimental results on the data from CoNLL-2012 shared task show that our system significantly improves the accuracy on dif-ferent evaluation metrics over the baseline systems.
One possible direction for future work is to dif-ferentiate more resolution modes. Another one is to add more precise or even event-based features to im-prove the model X  X  performance.
 This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.
 Appendix A. Derivation of Model Learning Formally, we iteratively estimate the model parame-ters  X  , employing the following EM algorithm: E-step: Compute the posterior probabilities of C , M-step: Calculate the new  X  0 that maximizes For simplicity, we denote: In addition, we use  X  (  X  j | j ) to denote the probability P (  X  j | j ) which is uniform distribution in our model. Moreover, we use the following notation for conve-nience:  X  ( m j ,m k ,j,k, X  j ) = t ( m j | m k , X  j ) q ( k |  X  j Then, we have Then the parameters t and q that maximize E where L jk can be calculated by where C (  X  j ) = { c 1 ,...,c j  X  1 ,c j +1 ,...,c n } . The above derivations correspond to the learning algo-rithm in Algorithm 1.
