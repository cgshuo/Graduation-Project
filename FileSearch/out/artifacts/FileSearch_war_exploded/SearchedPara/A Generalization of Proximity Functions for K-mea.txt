 K-means is a widely used partitional clustering method. A large amount of effort has been made on finding better proximity (distance) functions for K-means. However, the common characteristics of proximity functions remain un-known. To this end, in this paper, we show that all proxim-ity functions that fit K-means clustering can be generalized as K-means distance, which can be derived by a differen-tiable convex function. A general proof of sufficient and necessary conditions for K-means distance functions is also provided. In addition, we reveal that K-means has a gen-eral uniformization effect; that is, K-means tends to pro-duce clusters with relatively balanced cluster sizes. This uniformization effect of K-means exists regardless of prox-imity functions. Finally, we have conducted extensive ex-periments on various real-world data sets, and the results show the evidence of the uniformization effect. Also, we ob-served that external clustering validation measures, such as Entropy and Variance of Information (VI), have difficulty in measuring clustering quality if data have skewed distribu-tions on class sizes.
K-means [18] is a widely-used prototype-based cluster-ing algorithm. A key design issue of K-means clustering is the use of proximity functions. Intuitively, one can eas-ily understand that different choices of proximity functions for K-means can lead to quite different clustering results. In the literature, while a large amount of research work has been proposed on finding better proximity (distance) func-tions which can lead to a quick convergence, the common characteristics of proximity functions that fit K-means clus-tering (i.e. quickly converge to a solution) remain unknown.
Along this line, in this paper, we present a concept of  X  X -means distance X , which can be used to guide the choices of proximity functions that can fit K-means clustering. Indeed, we show that all proximity functions that fit K-means clus-tering can be generalized as K-means distance, which can be derived by a differentiable convex function. A general proof of sufficient and necessary conditions for K-means distance functions is also provided.

While K-means clustering can be used for a wide vari-ety of data types, it cannot be used for all the data types. For instance, Xiong et al. [25] revealed K-means has trou-bles in dealing with the case that the distributions of  X  X rue X  cluster sizes of the data are skewed. In particular, they showed that, if Euclidean distance is used as the proximity function, K-means tends to produce clusters with relatively balanced cluster sizes (this is also called the uniformiza-tion effect of K-means). However, this paper highlights that this uniformization effect exists regardless of proxim-ity functions used in K-means as long as these proximity functions are K-means distances. Specifically, we show that some well-known proximity functions of K-means, such as the cosine similarity, the coefficient of correlation [7], and the Bregman divergence [6], are K-means distances. When these proximity functions are used for K-means clustering, the uniformization effect cannot be avoided for data with skewed class distributions.

In addition, we have conducted extensive experiments on a number of real-world data sets from different applica-tion domains. Our experimental results show that K-means tends to produce the clusters in which the variation of the cluster sizes is smaller. This data variation is measured by the Coefficient of Variation (CV) [7]. The CV, described in more detail later, is a measure of dispersion of a data distri-bution and is a dimensionless number that allows compar-ison of the variation of populations that have significantly different mean values. In general, the larger the CV value is, the greater the variability is in the data.

Finally, as shown in our experimental results, after K-means clustering, the distributions of the resultant cluster sizes are in a much narrower interval compared to the dis-tributions of the  X  X rue X  cluster sizes. Indeed, the CV values of resultant cluster sizes are normally distributed and the 95% confidence interval is [0.09, 0.85]. The significance of the normal distribution is tested by the  X  2 statistic and the Shapiro-Wilk W statistic [22]. Also, we observed that some external clustering validation measures, such as En-tropy and Variance of Information (VI) [20], have difficulty in measuring clustering quality if data have skewed distri-butions on class sizes. When dealing with data sets with skewed distributions on the class sizes, both Entropy and VI have the favor on clustering algorithms, such as K-means, which tend to reduce high variation on the cluster sizes.
In this section, we characterize the distance (proximity) functions that fit K-means. We prove that, under certain assumptions, any distance function which can be used for K-means clustering must take some specific expression de-rived from a differentiable convex function. We call the family of such functions: The K-means distance .Forin-stance, the Bregman divergence as well as the cosine simi-larity are two different cases of the K-means distance.
K-means [18] is a prototype-based, simple partitional clustering technique which attempts to find the user-specified K clusters. These clusters are represented by their centroids (a cluster centroid is typically the mean of the points in the cluster). K-means has an objective function : and c i is its centroid. The clustering process of K-means is as follows. First, K initial centroids are selected. Then the two-phase iterations are launched. That is, in the first phase, every point in the data is assigned to the closest cen-troid, and each collection of points assigned to a centroid forms a cluster; then in the second phase, the centroid of each cluster is updated based on the points assigned to it. This process is repeated until no point changes clusters.
Note that there are several types of centroids can be used, e.g., the mean, the median, or the medroid. Since the mean has the unique advantage of high computational efficiency among other types of centroids, and can adapt to most of the existed distance functions, we restrict our analysis on the traditional K-means algorithm which takes the mean of the instances in each cluster as the centroid of the cluster. Definition 1 We say that a distance function F fits K-means, if the value of the K-means objective function us-ing F can be continuously (not strictly) decreased by the two-phase iterations.

Apparently, the distance functions that fit K-means must have the ability to facilitate the convergence of the two-phase iterations. Next, we give a lemma as follows. Lemma 1 A distance function F ( x, y ) : R d  X  R d  X  R fits K-means, if and only if  X  C = { x 1 ,x 2 ,  X  X  X  ,x n } X  R d
Proof : If we can prove that by using the distance func-tion and the mean as the centroid, the value of the objective function of K-means can decrease continuously, then the sufficient condition holds.

Let D ( A k ,U k ) denote the value of the objective func-tion after k iterations, where A denotes the phase of assign-ing instances to the nearest centroids, and U denotes the phase of updating the centroids. Then, in the next itera-tion, D ( A k +1 ,U k )  X  D ( A k ,U k ) , for each instance finds its closest centroid after the reassignment. Furthermore, the updated centroid, i.e., the new mean of each cluster, is the minimizer of the sum of the distances in that cluster, which implies that D ( A k +1 ,U k +1 )  X  D ( A k +1 ,U k ) . Therefore D ( A k +1 ,U k +1 )  X  D ( A k ,U k ) , and the equality holds if and only if there is no re-assignment in the k +1 iteration. Thus the sufficient condition holds.

On the contrary, if x is not one of the minimizers of i =1 F ( x i ,y ) , then D ( A D ( A k +1 ,U k ) . In other words, the update of the centroid for { x 1 ,  X  X  X  ,x n } may inversely increase the value of the objective function. So the necessary condition holds. Lemma 2 A differentiable function  X  : R d  X  R is convex if and only if  X  x, y  X  R d , X  ( x )  X   X  ( y )  X  ( x  X  y ) Further, if the equality holds if and only if x = y , then  X  is strictly convex.
 The proof of Lemma 2 can be found on page 70 in [5]. This lemma often serves as the sufficient and necessary con-ditions for a function being a (strictly) convex function.
Now the question is: How to characterize all the distance functions that fit K-means? To this end, we have the follow-ing theorem on one dimensional data.
 Theorem 1 Assume that F : R  X  R  X  R is a non-negative function such that: (1) F ( x, x )=0 ,  X  x  X  R ,(2) F and F x are continuous, and (3) F y is continuously differentiable on x , then F fits K-means if and only if there exists some differentiable convex function  X  : R  X  R such that
Proof : Note that F x and F y here denote the partial derivatives of F on x and y , respectively. We first prove the sufficient condition. For any cluster { x 1 ,  X  X  X  ,x n c
Furthermore, since n i =1 ( x i  X  c  X  )= 0 , and n i =1 ( x y )= n ( c  X   X  y ) ,wehave  X = n (  X  ( c  X  )  X   X  ( y )  X  ( c  X   X  y )  X  ( y )) = nF ( c
Thus c  X  = n i =1 x i /n is one of the minimizers of i =1 F ( x i ,y ) , so the sufficient condition follows from Lemma 1.

Then we prove the necessary condition. For each clus-ter { x 1 ,  X  X  X  ,x n } , since c  X  is one of the minimizers of i =1 F ( x i ,y ) , we can have
Without loss of generality, let x 1 = x 1 +  X  , x 2 = x 2  X &gt; 0 , we still have c  X  =( x 1 + x 2 + x 3 +  X  X  X  + x n which means Subtracting Equation (2) by Equation (1), we have F ( x 1 +  X , c  X  )  X  F y ( x 1 ,c  X  )=  X  ( F y ( x 2  X   X , c
Dividing both sides of Equation (3) by  X  and let  X   X  0 , we have F yx ( x 1 ,c  X  )= F yx ( x 2 ,c  X  ) . Similarly we have Therefore it must be F yx ( x, y )=  X  H ( y ) .So
We know that for n =2 , F y ( x 1 ,c  X  )+ F y ( x 2 ,c  X  )= 0 . If we replace F y ( x, y ) by Equation (4), then we have I ( c  X  )= H ( c  X  ) c  X  , which implies I ( y )= H ( y ) y . There-fore, we have Let  X  ( y )= H ( y ) dy , then
Since  X  x, y  X  R ,F ( x, y )  X  0 ,  X  (  X  ) is a convex function, which follows from Lemma 2. Thus the necessary condi-tion holds. So we complete the proof.

It is valuable to extend Theorem 1 to the multi-dimensional data case.
 Theorem 2 Assume that F : R d  X  R d  X  R is a non-negative function such that: (1) F ( x, x )  X  0 ,  X  x  X  R (2) F and F x are continuous, and (3) F y is continuously differentiable on x , then F fits K-means if and only if there exists some differentiable convex function  X  : R d  X  R such that
Due to the page limitation, we have to omit the proof of Theorem 2, which is similar to the proof of Theorem 1. Based on Theorem 2, we can have the precise definition of the K-means distance as follows.
 Definition 2 We say that a distance function F is a K-means distance, if there exists some differentiable convex function  X  : R d  X  R such that According to Theorem 2, the K-means distance fits the K-means clustering. And under certain acceptable assump-tions, the K-means distance is the only distance that fits K-means when the centroid type is the mean. Furthermore, please note that the K-means distance is a family of dis-tance functions with different  X  . Finally, we must point out that a K-means distance is not necessary to be a metric; that is, a K-means distance may not have symmetry and triangle inequality properties.
 Theorem 3 Given a differentiable function  X  : R d  X  R , let F ( x, y )=  X  ( x )  X   X  ( y )  X  ( x  X  y ) t  X   X  ( y ) , then  X  is strictly convex if and only if  X  C = { x 1 ,x 2 ,  X  X  X  ,x n
Proof : For any cluster { x 1 ,  X  X  X  ,x n } X  R d ,  X  y  X  R  X = n i =1 F ( x i ,y )  X  n i =1 F ( x i , x )= nF ( x, y ) . Therefore, according to Lemma 2, that  X  is strictly convex is equivalent to that  X   X  0 and the equality holds if and only if y = x . And the latter equivalent condition implies that x is the unique minimizer of n i =1 F ( x i ,y ) . So both the sufficient and necessary conditions hold.
 Remark: Theorem 3 can help us further divide the family of the K-means distances into two types. Type I are de-rived from strictly convex functions  X  , which implies that the mean is the unique minimizer of the sum of the distances in each cluster. On the contrary, type II K-means distances are derived from convex but not strictly convex functions  X  , which means the sum of the distances in each cluster has more than one minimizers. Next, we would like to introduce some widely used K-means distances of different types. Example 1 Let  X  ( x )= x 2 , then we can have a familiar K-means distance: F ( x, y )= x  X  y 2 , i.e., the squared Euclidean distance.
 Example 2 Let  X  ( x )=  X  H ( x ) , where x is a discrete prob-abilistic distribution, and H ( x ) is the entropy of x . Then we produce a K-means distance: F ( p, q )= D ( p q ) , where D ( p q ) is the relative entropy of distributions p and q . Remark: We can proved that both x 2 and  X  H ( x ) are strictly convex. That means the squared Euclidean distance and the relative entropy are type I K-means distances. Ac-tually, type I K-means distance has another name: Bregman divergence, first introduced by Bregman [6], and recently studied by Banerjee et al. [3]. Therefore, the Bregman di-vergence is not the K-means distance itself but just one type of it.
 Example 3 Let  X  ( x )= x , then we can induce a K-means distance
F ( x, y )= x  X  y  X  ( x  X  y ) t  X  y ( y )= x  X  Remark: Since x t y/ y = x cos(  X  )  X  x , thus F ( x, y )  X  0 , which implies  X  is convex. Furthermore,  X  C = { x minimizer of n i =1 F ( x i ,y ) , where k can be any positive real number. Thus according to Theorem 3, this distance is a type II K-means distance. Finally, we can show min Therefore F ( x, y ) is equivalent to the cosine similarity in K-means clustering. In other words, the cosine similarity can be transformed into a type II K-means distance. A sim-ilar measure, i.e., the coefficient of correlation , can also be equivalently transformed into a type II K-means distance, if the instances have been centralized and standardized to be x =( x  X   X  x ) / x  X   X  x .
 means distance, we can unify the type I and type II dis-tances. Specifically, it is interesting to show that the cosine similarity, which has long been regarded as a directional measure compared with the Bregman divergence, can be also equivalently transformed into a K-means distance. K-means. First, we provide the definition of the uniformiza-tion effect of K-means as follows.
 Definition 3 We say that the K-means clustering shows the uniformization effect, if the distribution of the cluster sizes by K-means is more uniform than the distribution of the class sizes. fect of K-means. As we know, the proximity function that fits K-means is the K-means distance, so the objective func-tion of K-means can be rewritten as follows. where kd ( x, y ) is the K-means distance, n i and x ( i ) size and the centroid (mean) of cluster i respectively. If we substitute kd ( x, y ) by  X  ( x )  X   X  ( y )  X  ( x  X  y ) t have ently x is the mean of all instances. Here, we consider the case that the cluster number k =2 and illustrate the uni-formization effect of K-means.
 sion point being x : small enough to be ignored. Also, it X  X  trivial to show that, given n = n 1 + n 2 , when cluster number k =2 Equation (7) and use Equation (8), we can get
Since  X  is convex,  X  2  X  is semi-positive definite, i.e.,
Therefore, if we isolate the effect of x (1)  X  x (2) , the max-imization of obj implies the maximization of n 1 n 2 , which leads to n 1 = n 2 = n/ 2 .

In the case of k&gt; 2 , i.e., the cluster number is greater than two, the situation is much more complicated. So we leave it to the experimental part.
Here, we explore the relationship of the uniformiza-tion effect and the centroid distances between the pairs of classes. Please note that the distance function used here is also the K-means distance.

Assume D is a data set with two classes C 1 = { x i } m i =1 and C 2 = { y j } n j =1 .Let x = m i =1 x i /m and y = j =1 y j /n denote their centroids respectively. Similar to the proof of Theorem 1, we can have
That means on average the distance between the in-stances of class C 1 ( C 2 ) and the centroid of class C 2 is larger than the distance between the same instances and their own centroid, and the gap is right the distance of the two centroids. In other words, the closer the two centroids can get, the closer the instances of one class and the centroid of the other class can be. Under such condition, we can ex-pect statistically that more instances from the larger class can be assigned to the centroid of the smaller class, which may result in a relatively balanced distribution  X  that is what we called the uniformization effect. By contrast, if the centroid distance is large, the uniformization effect can be ignorable even if the two classes are highly imbalanced.
Therefore, in general, the uniformization effect of K-means is strongly related to the centroid distances between the classes. Specifically, small centroid distances tend to induce a significant uniformization effect.
In this section, we present experimental results to demonstrate the uniformization effect of K-means cluster-ing. We also empirically study the cluster validation issues related to the uniformization effect.

In this subsection, we first introduce some important measures used in our experiments.
 Proximity Measures. We used three kinds of K-means dis-tances as shown in Table 1. In general, cosine distance shows merits on clustering high-dimensional data such as the document data and the gene expression data, and Eu-clidean distance prefers data sets with normal dimensional-ity. KL-divergence has information theoretical meanings on clustering words in document data sets [9].
 Cluster Validity Measures. We used two external clustering validation measures: Entropy and Variation of Information (VI), which are based on the information theory. Entropy measures the purity of the clusters with respect to the given class labels. It has been widely used in data mining commu-nity, and the details can be found in [25, 26, 23]. VI, a new measure introduced by an axiomatic view [20], measures the amount of information that is lost or gained in changing from the class set to the cluster set. In particular, VI is a true metric that satisfies some important axioms uniquely [20]. Details of VI can be found in [19]. In general, the lower the Entropy or the VI value is, the better the clustering perfor-mances.
 A Measure of Dispersion Degree. Here we introduce the Coefficient of Variation (CV) [7], which measures the dis-persion degree of a data set. The CV is defined as the ratio of the standard deviation to the mean. Given a set of data objects X = { x 1 ,x 2 ,...,x n } ,wehaveCV = s/  X  x where Note that there are some other statistics, such as standard deviation and skewness [7], which can also be used to characterize the dispersion of a data distribution. However, the standard deviation has no scalability; that is, the dispersion of the original data and stratified sample data is not equal if the standard deviation is used. Indeed, this does not agree with our intuition. Meanwhile, skewness cannot catch the dispersion in the situation that the data is symmetric but has high variance. In contrast, the CV is a dimensionless number that allows comparison of the variation of populations that have significantly different p mean values. In general, the larger the CV value is, the greater the variability is in the data. Experimental Tools . In our experiments, we used the MAT-LAB 7.1 [1] and CLUTO 2.1.1 [14] implementations of K-means. The MATLAB version is suitable for dense data sets, and we modified it so as to incorporate more K-means distances, such as KL-divergence. CLUTO is used to han-dle sparse data sets; that is, all and only the experimental results with cosine similarity as the proximity measure were produced by CLUTO. Note that the parameters of K-means in CLUTO were set to match the ones in MATLAB for the comparison purpose, and the cluster number K was set to match the class number of each data set.
 Experimental Data Sets . For our experiments, we used a number of real-world data sets that were obtained from dif-ferent application domains. Some characteristics of these data sets are shown in Table 3. The relevant notations can be found in Table 2.

Document Data Sets. The hitech and sports data sets were derived from the San Jose Mercury newspaper articles that were distributed as part of the TREC collec-tion (TIPSTER Vol. 3 ). The hitech data set contains documents about computers, electronics, health, medical, research, and technology; and the sports data set con-tains documents about baseball, basket-ball, bicycling, box-ing, football, golfing, and hockey. Data sets tr11 , tr12 , tr23 , tr31 , tr41 and tr45 were derived from the TREC-5[24], TREC-6 [24], and TREC-7 [24] collections. The classes of these data sets correspond to the documents that were judged relevant to particular queries. The la2 data set is part of the TREC-5 collection [24] and contains news articles from the Los Angeles Times. The ohscal data set was obtained from the OHSUMED collection [11], which contains documents from the antibodies, carcinoma, DNA, in-vitro, molecular sequence data, pregnancy, prog-nosis, receptors, risk factors, and tomography categories. The data sets re0 and re1 were from Reuters-21578 text categorization test collection Distribution 1 . 0 [15]. The data sets k1a and wap were from the WebACE project (WAP) [10]; each document corresponds to a web page listed in the subject hierarchy of Yahoo!. For all document cluster-ing data sets, we used a stop-list to remove common words, and the words were stemmed using Porter X  X  suffix-stripping algorithm [21].

Biomedical Data Sets. LungCancer and Leukemia data sets are from the Kent Ridge Biomedical Data Set Repository (KRBDSR) [16]. The LungCancer data set consists of samples of lung adenocarcinomas, squamous cell lung carcinomas, pulmonary carcinoid, small-cell lung carcinomas and normal lung described by 12600 genes. The Leukemia data set contains six subtypes of pediatric acute lymphoblastic leukemia samples and one group of samples that do not fit in any of the above 6 subtypes, and each sub-type is described by 12558 genes.

Other Data Sets. Besides the above high-dimensional data sets, we also used some data sets with normal di-mension sizes. Among them, the ecoli , optdigits , pendigits and segment data sets are from the well-known UCI repository, which have been widely used in data mining community. The rest eight data sets are from LIB-SVM repository [2], which contains data sets for the clas-sification purpose originally. Note that some of these data sets are right from the UCI repository which have been stan-dardized by LIBSVM for the classification task.
In this subsection, we demonstrate the existence of the uniformization effect of K-means. We first applied K-means with different types of distances on different types of data sets, then computed the CV 1 values for the resul-tant distributions of the cluster sizes. Thus we can compute the corresponding DCV values, which indicate the effect of K-means on the data distributions. Finally, we evaluated the clustering performances by two widely used measures: Entropy and VI.

Specifically, for cosine similarity, each document in-stance x was standardized to have unit length before ap-plying clustering, i.e., x =1 . For KL-divergence, we used the so-called co-clustering scheme; that is, we viewed the document data set as a  X  X ord-document X  matrix, and employed K-means with KL-divergence on the words first so as to get 100 word clusters, then reduced the features ac-cording to the word clusters, and finally employed K-means with KL-divergence again on the modified data set to get the final document clusters. The reason of co-clustering can be found in [9]. Also note that in practice we standardized each instance x to make d i =1 x i =1 , where x i is the i th attribute value of x .
 Table 4 shows the results. As can be seen, no matter what K-means distance we used, for the data sets with large CV values, K-means tends to reduce the variation on the cluster sizes of the clustering results, as indicated by the negative DCV values. This result indicates that, for data sets with highly imbalanced class sizes, the uniformization effect is dominant in the objective function. Also, there are five data sets including optdigits , pendigits , segment , ohscal and german.numer , which have small CV 0 val-ues. Indeed, for these data sets with relatively balanced class sizes, the uniformization effect of K-means is not sig-nificant and can be dominated by other factors, such as the centroid distances, the densities or the shapes of the classes.
Now let X  X  take a closer look on the CV 1 values, i.e., the distribution of the cluster sizes. Figure 1 shows the com-parisons of CV 1 and CV 0 values when applying different K-means distances. Please note that a dot line represents a data set in Table 4. As can be seen, after applying K-means, the distributions of the cluster sizes, i.e., the CV values, are in a much narrower interval. Also, we test the hypothesis that all the CV 1 values are normally distributed. The results show that the p values of the Shapiro-Wilk W statistic and  X  2 statistic are 0.887 and 0.478 respectively, which implies not to reject the null hypothesis (given the significance level  X  =0 . 05 ). Therefore we can compute the 95% confidence interval of CV 1 : [0.09, 0.85], given the mean and the standard deviation of CV 1 values are 0.470 and 0.192, respectively. In other words, for data sets with CV 0 values greater than 0.85, the uniformization effect of K-means will take place with a very high probability.
Since CV 1 values have such a narrow interval, we can expect that as the skewness of the distribution of class sizes increases, the results by K-means clustering tend to be farther away from the true ones. Figure 2(a) indeed shows this trend; that is, the absolute DCV values increase as the CV 0 values increase. Therefore, in general, applying K-means clustering on highly imbalanced data sets is not very effective, which can be indicated by the DCV measure.
Here, we illustrate that the uniformization effect of K-means can make negative impact on the cluster validity. More specifically, some well-known clustering validation measures may not have the ability to identify the uni-formization effect of K-means, so as to deliver unreliable scores on the clustering results. In this subsection, we will focus on two measures: Entropy and Variation of Informa-tion (VI). The former has been widely used in data mining community, and the latter was well established on the infor-mation theory and a set of axioms.
 Entropy. We first perform the analysis using the Entropy measure. Figure 2(b) and 2(c) show the Entropy values along data sets with increasing variation on the class sizes. The common ground of these figures is that the Entropy val-ues tend to be systematically lower (better) on results of highly imbalanced data sets. This seriously contradicts the findings above: K-means tends to produce poorer partitions on highly imbalanced data sets due to the uniformization effect. Therefore, we can suspect that, Entropy may not be suitable for evaluating the results produced by K-means with squared Euclidean distance or cosine similarity.
Furthermore, we explore the reason why Entropy is unre-liable on K-means. The key point is, Entropy only assesses the purity of each cluster, but does not promise to find ev-ery class of the data set. In detail, for a highly imbalanced data set, due to its uniformization effect, K-means tends to break down the large class and assign the pieces to different clusters. Since the number of instances of the large class
Figure 2. Relationships between CV 0 and Val-idation Measures. can be  X  X uge X  compared with other classes, the pieces of it can still be dominant in their corresponding clusters, so as to make the clusters be rather  X  X ure X , which results in a low Entropy value. Actually, this seeming  X  X urity X  is at the cost of missing many small classes in the data! A detailed illustration can also be found in [25].
 Variation of Information (VI). VI is a more sophisticated clustering validation measure which implicitly takes the in-tegrality of all classes in data into consideration. As a result, VI is also more complicated than Entropy  X  to directly un-derstand its relationship with K-means and the uniformiza-tion effect is rather difficult. Therefore, we here only evalu-ate them empirically.

Figure 2(d) shows the VI values along data sets with in-creasing variation on class sizes. A similar trend as Entropy can be found; that is, the VI values tend to be systemat-ically lower (better) on results of highly imbalanced data sets. Therefore, we can suspect that VI also has troubles on evaluating the K-means clustering, especially for highly imbalanced data sets.

Please note that Figure 2(d) is based on the Euclidean distance, however this observation can be extended to the situation when applying cosine similarity. We omit the re-sults based on cosine similarity due to the page limitation. Measures for K-means with KL-divergence. Here, we study the effectiveness of the two measures on evaluating results produced by K-means with KL-divergence. An in-teresting observation is that, compared with their previous performances on squared Euclidean distance or cosine sim-ilarity, both Entropy and VI measures show rather different behaviors on KL-divergence.

As indicated by Figure 3, the Entropy values increase as the CV 0 values go up. This is opposite to the trends in Fig-ure 2(b) and 2(c). This implies that, given KL-divergence as the K-means distance, Entropy can identify poor results by K-means on highly imbalanced data sets. As to VI, we can simply compute the coefficient of correlation between VI values and Entropy values. The 0.99 result implies that VI acts in a way very similar to Entropy; that is, the VI val-ues increase as the distributions of the resultant cluster sizes being farther away from the true ones. In other words, VI is also effective on evaluating results produce by K-means with KL-divergence.
Here, we highlight some research results which are mostly related to the main theme of this paper.

First, people have studied the impact of high dimension-ality on the performance of K-means, and found that the traditional Euclidean notion of proximity is not effective for K-means clustering on high-dimensional data sets. To meet this challenge, one research direction is to make use of dimensionality reduction techniques, such as multidimen-sional scaling (MDS) [4], principal components analysis (PCA) [13], and singular value decomposition (SVD) [8]. Another direction for this problem is to redefine the no-tions of proximity, e.g., by the Shared Nearest Neighbors (SNN) similarity [12]. Some other similarity measures, such as the cosine similarity, have also been used as prox-imity functions for clustering high-dimensional document data sets [26].

Second, there are a number of choices for the proxim-ity function that can be used in K-means. For instance, the Euclidean distance has been widely used, and the co-sine similarity, KL-divergence as well as Itakura-Saito dis-tance have also shown their advantages for document clus-tering [26], words clustering [9] and power spectra anal-ysis [17], respectively. Recently, in his inspiring work, Banerjee [3] proposed a general framework for K-means clustering that uses proximity functions based on Bregman divergences [6]. This work shares some common grounds with our work. However, in this paper, we propose a gen-eral concept of K-means distance for proximity functions that fit K-means. Some well-known proximity functions of K-means, such as the cosine similarity, the coefficient of correlation, and the Bregman divergence, are instances of K-means distance.

Finally, in their previous work [25], Xiong et al. prelimi-narily studied the uniformization effect of K-means and the cluster validation issues. However, their focus is merely on the squared Euclidean distance and the Entropy measure. Therefore, this paper is a natural extension of [25]. In-deed, we show that the uniformization effect exists no mat-ter which proximity function is used in K-means as long as this proximity function fits K-means clustering. Also, we show the impact of skewed cluster distributions on the per-formance of some other external cluster validation measure, such as Variation of Information (VI), which has been well established in the machine learning community [19, 20].
In this paper, we studied the generalization issues of proximity functions for K-means. Specifically, we showed that a proximity function that fits K-means can be derived from a differentiable convex function. We call such prox-imity functions as K-means distance. Also, we theoretically proved that some widely used proximity functions, such as the Bregman divergence and the cosine similarity, are the instances of K-means distance. In addition, we revealed that K-means has a general uniformization effect; that is, K-means tends to produce clusters with relatively uniform sizes regardless proximity functions used. Finally, experi-mental results on real-world data sets show the uniformiza-tion effect of K-means. We also observed that both Entropy and VI have difficulty in measuring clustering quality if the class sizes of data have skewed distributions. This research was partially supported by the National Science Foundation of China (NSFC) (Nos. 70621061, 70518002). Also, this research was supported in part by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick.

