
This paper investigates the use of supervised clustering in order to create sets of categories for classification of documents. We use information from a pre-existing taxonomy in order to supervise the creation of a set of related clusters, though with some freedom in defining and creating the classes. We show that the advantage of using supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address, but with a precise mathematical definition of each category. We then categorize documents using this a priori knowledge of the definition of each category. We also discuss a new technique to help the classifier distinguish better among closely related clusters. Finally, we show empirically that this categorization system utilizing a machine-derived taxonomy performs as well as a manual categorization process, but at a far lower cost. 
In recent years, the amount of online text data has grown greatly because of the increase in popularity of the World Wide Web. As a result, there is a need to provide effective content based retrieval, search, and filtering for these huge and unstructured online repositories. In this paper, we consider the problem of automated text categorization, in which we desire to find the closest matching subjects for a given text document. We assume that a pre-existing sample of training documents with the associated classes is available in order to provide the supervision to the categorization system. 
Several text classifiers have recently been proposed, such as those discussed in [2, 4, 51. These classifiers have shown excellent results on document collections KDD-99 San Diego CA USA 
Copyright ACM 1999 I-58113-143-7/99/08...$5.00 such as the Reuters dataset or the US patent database [2] and to a somewhat lesser extent on the web with the Yahoo! taxonomy. Categorization of web documents has proven to be especially difficult because of the widely varying style, authorship and vocabulary in different documents. 
Most of the above-mentioned categorizations are cre-ated using manual categorizations by subject experts. The apparent inaccuracy of automated classification methods on large document collections is a result of the fact that a large heterogeneous collection of man-ually categorized documents is usually a poor fit for any given classification model. Thus, it is interesting to investigate the construction of categorization systems which relax the restriction imposed by predefined sets of classes. We study the use of supervised clustering in order to create the categories which are related to, but not quite the same as a pre-existing taxonomy. 
The fact that we actually know the model used to construct each partition in the clustering ensures that we can theoretically obtain a perfect accuracy on this categorization when the same model is used by the categorizer. Therefore the quality of categorization depends completely on the quality and coherence of each cluster in the new taxonomy, rather than the accuracy of a training procedure on the original taxonomy. 
The use of unsupervised clustering for providing browsing capabilities has been discussed in earlier work [3]. Such methods do not use any kind of supervision from a pre-existing set of classes, and are attractive for creation of a small number of clusters such as fifty or so, though the clustering rapidly degrades in quality when it is used for finding more fine-grained partitions. Typically, when categories are related enough to contain overlapping vocabulary, unsupervised clustering methods are unable to create a good fine grained subject isolation. 
We also propose a classifier based on the clustering model which is able to distinguish between very closely related categories. The importance of creating a clas-sification process which is able to distinguish between closely related subjects has been discussed in earlier work [2]. This work uses a hierarchical taxonomy for effectively distinguishing between closely related cate-gories. Such classifiers are quite fast, though the accu-racy can be sensitive to the quality of the hierarchical organization. Our system provides high-quality catego-rizations on a flat set of classes without compromising speed. 
In order to represent the documents, we used the vector space model. In the vector space model, it is assumed that each document can be represented as as term vector oftheformZ= (ur,uz,... a,). Each of the terms ai has a weight wi associated with it, where wi denotes the normalized frequency of the word in the vector space. 
A centroid of a set of documents is defined by a concatenation of the documents in the set after damping the word frequencies. The damping function ensures that the repeated presence of a word in a single document does not affect the centroid of the entire cluster excessively. 
A projection of a document is defined by setting the term frequencies (or weights) of some of the terms in the vector representation of the document to zero. These are the terms which are said to be projected out. We will use the process of projection frequently in the course of the supervised clustering algorithm. Each cluster is represented by a seed vector containing only a certain maximum number of projected words. The aim in projection is to isolate a relatively small vocabulary which describes the subject matter of a cluster well. 
Our first phase was to perform the feature selection in such a way so that only the more differentiating words are used in order to perform the clustering. Note that in unsupervised clustering methods, where a pre-existing taxonomy is not used, the feature selection is somewhat rudimentary in that only stop zuords (very commonly-occuring words in the English language) are removed. In our case, we used the gini index of the word in order to pick out relevant words far more aggressively. 
Let there be K classes Cl, C X  X  . . . CK at the lowest level in the original taxonomy. Let fr, f2. . . f~ be the number of occurances of that word in each of the K classes, and let nr . . . nK be total word count for the documents in each of the K classes. Thus, the fractional presence of a word in a particular class is given by j X  X /ni. We define the skew fraction of a word for class i by $;:,*;. We shall denote this skew fraction by pi. 
The normalized gini index of a word with skew fractions pr . . .pK is given by 1 -J--ci=rpi. If the word is distributed evenly across the different classes, then the gini index is 1 -l/a. This is the maximum possible value of the gini index. On the other hand, when the word is highly correlated with particular categories and is very skewed in its distribution, then the normalized gini index is much lower. 
The clustering algorithm uses a seed-based technique in order to create the clusters. Traditional clustering methods have often used seed-based algorithms in order to serve as an anchor point for the creation of the clusters. In other words, seeds form an implicit representation of the cluster partitioning in which each item to be categorized is assigned to its closest seed based on some distance (or similarity) measure. 
In the context of information retrieval, a seed is a meta-document which can be considered as a pseudo-representation of a central point in a given cluster. Most current clustering algorithms use sets of seeds in order to define the implicit partitions. 
Since the focus of the algorithm is on supervised clus-tering, we started off with a set of seeds which are repre-sentative of the classes in the original taxonomy. These representative seeds are constructed by finding the cen-troids of the corresponding classes. This choice of start-ing point (and features picked) ensures the inclusion of supervision information from the old taxonomy, but the subsequent clustering process is independent of any fur-ther supervision. Each seed consists of a vector in which the number of words with a non-zero weight is restricted to a predefined maximum. The algorithm gradually re-duces the projected dimensionality in each iteration, as the clusters get more refined, and a smaller vocabulary is required in order to isolate the subject of the doc-uments in that cluster. This technique of representing clusters by using both the documents and the projected dimensions in order to represent a cluster is referred to as projected clustering [l], and is an effective technique for the creation of clusters for very high dimensional data. The idea of using truncation for speeding up doc-ument clustering has been discussed in [7], though our focus for using projections is different, and is designed 2.1 Categorization Algorithm 
We build a domination matrix on a subset of the the seeds for two nodes in the taxonomy: Business Schools and Law Schools. Recall that our process of projection limits the number of words in each seed to only words which are relevant to the corresponding cate-gories. Some examples of words (with non-zero weights) which could be represented in the seed vector of each of these categories are as follows: (1) Business Schools: business (35) , management (2) Law Schools: law(22), university (ll), school (13), The categories have numerous words in their seeds in common and thus, for example, a document about Law Schools with an unusually high number of mentions of  X  X chool X  may appear close to Business Schools. 
In order to establish the relative closeness of two categories to a given document more accurately, we need to ignore the contributions of the words common to both categories to the cosine measure. This is done by performing a relative seed subtraction operation on the seed vectors of each of the categories. The seed subtraction operation is defined as follows: Let Sr and Ss be two seed vectors. Then, the seed S X  X  -&amp; is obtained by taking the seed Si and setting the weight of all those words which are common to Sr and &amp; to 0. 
We say that the seed 5 X 1 dominates the seed Sz under the following conditions: The (cosine) similarity of Sr to the test document T is larger than the similarity of &amp; to T by at least a predefined threshold referred to as the domination threshold. 
The (cosine) similarity of Sr to T is not larger than the similarity of Ss to T by the predefined threshold, but the similarity of (Sr -Ss) to T is larger than the similarity of (Sz -Sr) to T. 
The use of a domination threshold ensures that it is only possible to reorder seeds whose similarity to the test document are very close together. For each pair of the closest Ic seeds to the test document, we compute the domination matrix, which is the pairwise domination of each seed over the other. In order to rank order the k: candidate seeds, we compute the domination number of each seed. The domination number of a seed is equal to the number of seeds (among the remaining (Ic -1) seeds) that it dominates. The Ic seeds are ranked in closeness based on their domination number; ties are broken in favor of the original ordering based on cosine measure. 
If there are a total of K classes created by the cluster-ing algorithm, then the categorization algorithm needs to perform O(K + k2) cosine similarity calculations. 
Further, since the projected dimensionality of each seed is restricted to a few hundred words, each similarity calculation can be implemented efficiently. Thus, the categorization system is extremely fast because of its simplicity, and scales almost linearly with the number of classes. 
As indicated earlier, we used a scan of the Yahoo! tax-onomy from November, 1996. This taxonomy contained a total of 167,193 Web documents, over a lexicon of ap-proximately 700,000 words. We truncated the Yahoo! tree taxonomy to obtain a set of 1,500 classes corre-sponding to intermediate level nodes. The purpose was to use the lowest level nodes in the taxonomy which contained at least 50 or more documents. 
In our implementation of the supervised clustering algorithm we first calculated the normalized gini index of the different words in the clusters, and removed about 10,000 words with the highest gini index. We also removed the very infrequently occuring words in order to remove misspellings and creative variations on ordinary words. Specifically, we removed all those words which occured in less than 7 documents out of the original training data set of 167,193 Web documents. 
At this stage, we were left with a lexicon of about 77,000 words. The algorithm started with about 500 projected words in each seed, and successively removed words from the seed, until the desired maximum of about 200 words was obtained in each seed. The value of the seed reduction factor 8 was 0.7. The value of the parameter minimum used to decide when to kill a cluster was 8. The value of the merging threshold (the parameter threshold in Figure 1) was 0.95. The algorithm required a total of 3 hours to complete on a 233 MHz AIX machine with 100 MB of memory. We obtained a total of 1,167 categories in a flat taxonomy. We labeled the nodes by examining the constituent Yahoo! categories in this set of newly created clusters. Typically, the clustering did a very excellent job in grouping together documents from very closely related categories in the Yahoo! taxonomy in a creative way. 
The simplicity of the classifier ensured that it was ex-tremely fast in spite of the very large number of classes used. The classifier required about two hours to catego-technique on a real data set such as Yahoo!. to the 1,500 Yahoo! categories described above. For 
Yahoo! for 77% of the documents. Out of this 77%, the (8%:8%) as to whether Yahoo! or our scheme provided 
On only 7% of the questions did a majority of the 
It is important to understand that while our taxon-omy is qualitatively comparable to Yahoo!, it is far more amenable to automated categorization than the manu-ally built Yahoo! taxonomy. Any other categorizer, which trains on the Yahoo! taxonomy, would provide such as Yahoo!, yet at a far lower cost. 
Yahoo! taxonomy. We showed that the supervised 
