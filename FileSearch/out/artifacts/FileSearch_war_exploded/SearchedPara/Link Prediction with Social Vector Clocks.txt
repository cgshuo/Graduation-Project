 State-of-the-art link prediction utilizes combinations of complex features derived from network panel data. We here show that com-putationally less expensive features can achieve the same perfor-mance in the common scenario in which the data is available as a sequence of interactions. Our features are based on social vec-tor clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks. In fact, our experiments suggest that by taking into account the order and spac-ing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date.
 H.2.8 [ Database Management: Database Applications  X  Data Mining ]: social networks, vector clocks, link prediction, online algorithms
Link prediction deals with predicting previously unobserved in-teractions among actors in a network [1]. Predictions are based on the dynamic network of previously observed interactions, which is usually made available in one of two forms: panel data or event data. The former refers to a sequence of complete network snap-shots and typically contains only coarse-grained temporal informa-tion. Event data, on the other hand, consists of a finer-grained se-quence of single, time-stamped relational events, in which the ex-act minute or second of each event is known. Whereas panel data is often collected by means of longitudinal surveys, event data is typ-ically the outcome of automated data collection, such as log files of e-mail, phone, or Twitter communication.

It is possible to convert network event data into (interval-censored) network panel data; indeed, Liben-Nowell and Kleinberg [2] did so in their seminal paper which introduced the link prediction problem for social networks, and the practice has become standard [1, 3]. The conversion is usually carried out by defining a sequence of time slices and aggregating relational events within these time win-dows into static (weighted) networks. At the expense of losing the ordering and spacing of original events, this conversion allows one to employ the large set of tools that have been developed for static and longitudinal network analysis [4, 5, 6].

Whether the conversion from event to panel data is justifiable or not depends crucially on mechanisms which drive tie formation in a given network. For example, Liben-Nowell and Kleinberg [2] conducted experiments on future interactions in large co-authorship networks. In this setting, the exact sequence and spacing of publi-cation dates can hardly be relevant because publications dates are distorted anyway (backlogs, preprints, etc.); aggregation of publi-cation events on a coarser time-scale thus does not appear to be problematic.

In other situations the fine-grained temporal information may be highly relevant, making the conversion from event data to panel data difficult to justify because it may destroy important patterns of interaction; see [7] for a recent review on general temporal network approaches that exploit such information. With regard to the link prediction problem, if we are trying to foresee whether node A will send an email message to B in the near future, for example, then an extremely useful piece of information is whether B has recently sent A a message; if so, it is likely that A will respond to B soon. This response-mechanism is known as reciprocity , and has been observed to be highly relevant for predicting future events in social networks [8]. By aggregating communication events into cross-sectional graphs, traditional link prediction schemes are generally prone to miss such simple and useful mechanisms.

Here, we demonstrate that link predictors can indeed be made more effective and efficient if they operate directly on appropriate time-stamped dyadic communication data, and as a result can take advantage of the information contained in the spacing and ordering of relational events. The approach we introduce is based on keeping track of how out of date a node A is with respect to another node B with respect to time-respecting information flow, and for doing so we employ the concept of vector clocks. Our results confirm that dyadic features that exploit fine-grained temporal information can be highly relevant for predicting which actors will communicate for the first time in the near future, and are not limited to reciprocity.
The outline is as follows: first, we describe the types of data for which we expect fine-grained temporal data to be relevant for link prediction. Next, we review the link prediction problem for this type of data, paying particular attention to supervised link pre-diction, a framework that we employ here. We then specify how a modified version of social vector clocks can be used as a supervised link predictor, and proceed to evaluate this predictor. We conclude with a discussion of our results, as well as possible future work. Figure 1: Illustrative example of social network data containing fine-grained information on dyadic communication events. Indirect information might flow on time-respecting paths, i.e. along labels that respect the ordering of time. Adapted from [9].
In the introduction, we stressed that the ordering and spacing of communication events might contain valuable additional informa-tion over and above the mere number of contacts between a pair of actors. Consider the example in Figure 1, which depicts a series of directed communication events, such as e-mail messages. Let us imagine that the actor represented by node A is in charge of or-ganizing a wellness weekend-trip for a group of friends, and that she keeps changing her mind about when and where to go. She finally settles on a plan on Thursday at noon, and all of the sub-sequent messages she sends out include the final trip details. We can ask: which nodes can possibly know them, given the observed interactions? Clearly, node A communicated with node C after she made up her mind on Thursday, so node C would have received the final information on Friday at 9am. Because node C subsequently sent node B a message, node B could also have received the cor-rect information. On the other hand, nodes D and E could not have received information from node A that is more recent than Wednes-day at 3pm.

Two key and related concepts present in this example are latency and indirect updates . We expect that groups of people who co-ordinate some action, such as a wellness weekend-trip, will need to synchronize their knowledge of certain key information such as departure time and destination. So in some sense (which we will define formally in Section 4) the members of this group have a low latency with respect to each other. Indirect updates, such as the one that made B aware of A X  X  latest trip plans, are an important mech-anism for maintaining these low latencies: even though B did not have any direct message from A after she made up her mind, B still got the latest plans indirectly via C.

This type of indirect communication is common in social sys-tems: consider the case of several adult siblings who communicate with each other rarely, and more often communicate with their par-ents. In this case, the siblings X  information on each other can re-main up to date due to the central role of the parents, who provide the siblings with an indirect means of communication. More gen-erally, we observe that gossip  X  the information exchanged when two people talk about an absent third party  X  is a form of commu-nication prevalent in society and is in essence a form of indirect update.

The motivation behind our approach in this paper is that informa-tion diffusion via indirect updates can be exploited to infer future direct relations. In terms of the example above, we might predict that the siblings are likely to communicate with each other because their latencies with respect to each other remain low. However, the current approach to link prediction throws away much of these temporal clues by first converting the event stream into panel data.
In the datasets we analyze here, we also have reason to believe that fine-grained diffusion patterns are relevant to link prediction. For instance, two of the datasets that we will use during our eval-uation contain sequences of micro-blogging events that come from Twitter. Bakshy et al. [10], e.g., have found that word-of-mouth information in Twitter spreads via many small cascades of tweets, mostly triggered by ordinary individuals. These small chains of diffusion are exemplary of the indirect updates we described above, and by considering the details of how information spread, we may be able to infer which nodes will come into direct contact in the future. Detailed information on the datasets we use in our final evaluation is given in Section 5.1.
In this section, we review the basics of link prediction. In partic-ular, we provide an overview of how machine learning models can be used to combine multiple link predictors  X  a technique called supervised link prediction. Along the lines of its original formulation by Liben-Nowell and Kleinberg [2], we formulate the link prediction problem for dyadic event data as follows: An unsupervised link predictor is a function which, given a dyad (a pair of nodes) and the list of all previously occurring events, returns a score, where a higher score indicates that an edge is more likely to form in the dyad. Common neighbors is an example of an unsuper-vised link predictor: given a dyad (A,B), return the number of con-tacts shared by A and B. Although common neighbors is simple, it is quite effective and many of the most effective unsupervised link predictors (such as the similarity measure originally proposed by Adamic and Adar [11]) are also based on shared neighbors [2, 3].
By running a link predictor on all dyads that are disconnected in the interval [ t 0 ,t 1 ) , one can rank all of the possible new links. To evaluate a link predictor, we compare this ranking with the set of new dyads that actually occur in the period [ t 1 ,t 2 performance on the link-prediction task is often measured using ROC curves or measures based on precision, but in their recent pa-per on evaluation in the link prediction problem, Lichtenwalter and Chawla [1] convincingly argue that due to the extreme class imbal-ance present in the link prediction task, precision-recall curves are a more relevant and less deceptive way to measure performance. For that reason, here we exclusively use precision-recall curves for our evaluation.
As link prediction is fundamentally a binary classification prob-lem, it is natural to use the powerful binary classification models that have been developed in machine learning. The primary advan-tage of this approach is the ability to combine multiple unsuper-vised link predictors into one joint prediction model. We will now provide a brief overview of how supervised link prediction works. For an in-depth discussion of supervised link prediction see [12].
In practice, the specification of a link prediction task involves more details, such as whether directed or undirected dyads are con-sidered; we address these points in Section 5.2.
As is usual in machine learning evaluation, we train and test our classifier on two separate datasets: we must be careful that the classifier is not trained on the same data that is used to evaluate it. For this reason, supervised link prediction requires a train and test framework as depicted in the bottom half of Figure 2. A link prediction classifier is given a set of features related to each discon-nected dyad in the period [ t 0 ,t 1 ) , as well as a label which indicates whether the dyad became connected in the period [ t 1 ,t 2 this information, it learns a model which relates the dyad features to the probability that a previously disconnected dyad becomes con-nected. To measure the accuracy of a link predictor, we then create a set of test dyad features in the interval [ t 0 0 ,t 0 1 score the test labels in the interval [ t 0 1 ,t 0 2 ) . We measure how accu-rately the scored dyads predict the set of test labels using the area under the precision-recall curve (AUPR).

We note that the AUPR of a link predictor can fluctuate greatly: as the behavior of users changes, so does the accuracy of the link predictor. In order to better estimate the typical AUPR attained by a link predictor, we can run this procedure many times; we will refer to each run of the procedure outlined in the bottom panel of Figure 2 as a realization . As shown in the top panel of Figure 2, we shift realizations such that the AUPR of each realization is mea-sured using a distinct set of events.

Lichtenwalter et al. [12] convincingly argue that the link predic-tion problem should be stratified over different geodesic distances (i.e., path lengths). That is, the disconnected dyads with a geodesic distance of N = 2 , 3 , 4 ,... should each be put into different bins, and a separate classifier should be trained on each bin. This strat-ification leads to better performance because the decision bound-ary for each bin can be quite different, with local features (such as common neighbors) being of primary importance at small dis-tances, and global features (such as preferential attachment) be-coming more important at larger distances. Thus, by treating each distance as a separate classification task, not only does performance improve, but one can also gain insight into the particular strengths Figure 2: Framework for performing and evaluating supervised link prediction. Each dataset is split into several realizations ; each real-ization, in turn, is split into intervals to train and test a classifier. Figure 3: The basic idea of vector clocks: each node X  X  vector clock (in grey) keeps track of the most recent information it could have on the other actors in the network. and weaknesses of a predictor. We therefore follow suit and treat each distance as a separate link prediction task.
Having introduced the necessary background on network event data and link prediction, we now explain how fine-grained temporal information can be exploited, using the concept of vector clocks.
Vector clocks were conceptually defined in [13] and [14] as a means to track causality in concurrent systems, but had implic-itly been used before, e.g., in [15], with underlying foundations attributed to [16]; for an introduction to vector clock systems in distributed computing refer to [17]. Kossinets et al. [9] brought the concept to social network analysis by substituting message-exchanging processes with communicating individuals. In this spe-cial setting, the basic motivation of vector clocks is to keep track of the lower bound of how out-of-date a person is with respect to every other person at any time, assuming that information spreads according to a given time-ordered list of communication events. Reconsider the trip-planning example we introduced in Section 2. There we asked: which nodes could possibly know about the most recent details, through either direct or indirect updates? Vector clocks provide a way of answering this question by keeping track of the last possible update that a node could have received from each other node: the vector clock (grey box) next to node E in Figure 3 indicates that E cannot possibly have received information from A more recent than Wednesday at 3pm, that it could have received no information whatsoever from nodes B and C, and so on. Each node is always assumed to have up-to date information on itself.
Formally, let ( t i ,s i ,r i ) , i  X  N , a sequence of (time,sender,re-ceiver) tuples satisfying t i  X  t i +1 and s i 6 = r i . The set of individ-uals is defined implicitly by V = S i  X  N { s i ,r i } . At time t s and receiver r i exchange direct information about themselves, and indirect information about others that result from communica-tion events in the past ( t &lt; t i ). That is, information can not be forwarded instantaneously but with arbitrary small delay. Now, a vector clock is a multivariate function  X  v,t = (  X  v,t ( u ) : u  X  V ) , in which v  X  X  temporal view  X  v,t ( u ) on u at time t is defined as the time-stamp t  X   X  t of the latest information from u that could have possibly reached v (directly or indirectly) until time t . At any time, each actor is up-to-date with respect to itself,  X  v,t ( v ) = t . Tempo-ral views on others can be tracked online as step functions resulting from component-wise maximum calculations of  X  s i ,t i and  X  time-steps t i . Intuitively,  X  s i ,t i is updated if and only if a commu-nication event is mutual (such as telephone conversations or meet- X  = 1  X  = 2  X  =  X  ings), while the update is restricted to  X  r i ,t i if a communication event is directed (such as email-, text-, or Twitter-messages). A major drawback of traditional vector clocks is poor scalability. This results from quadratic space requirements to maintain com-plete temporal views, along with efficiency problems when per-forming linear-size maximum calculations on every event. There-fore, traditional vector clocks are too expensive to maintain and manipulate in large graphs. While quadratic space and linear band-width requirements are necessary to allow for exact calculations in the general case [18], approximate calculations of a limited number of temporal views [19] and less expansive update algorithms in re-stricted settings, such as acyclic communication graphs [20], have been proposed. For suitable topologies, additional data structures can be used to reduce the bandwidth of information to be forwarded [21].
In contrast to those enhancements stemming from the literature on distributed computing, we propose a modification that is tailored to social communication networks. While the original formulation of vector clocks is interesting for social networks because it cap-tures the process of gossip and indirect communication, it does so in an exaggerated and almost clumsy manner. Experiments [22] on the small-world property of social networks [23, 24] and the in-vestigation in [9] suggest that, in the vector clock update algorithm described above, nodes will soon receive huge amounts of infor-mation on people they have never met, and whom even their own contacts have never interacted with directly. Indeed, in our own initial experimentation, we found that most actors quickly attain a non-null temporal view with most other actors in the system, and that single communication events often cause an actor to be updated on nearly all of the other actors.

These global updates are hard to justify because they do not seem to resemble social communication. In other words, while exchanging system-wide information is important in the context of distributed computing, such massive information exchanges do not occur when two people communicate with each other. Rather than updating each other on most of the other actors in the system, the nature of social communication is bounded by cognitive limits; such as the number of acquaintances, which does not scale with the size of the overall population [25].

We observe that when two people meet and talk about third par-ties, they are likely to discuss mutual acquaintances, or at least re-strict the conversation to people at least one of them has met di-rectly. Compared to this circle of acquaintances and mutual ac-quaintances, they are relatively unlikely to talk about any given friend of a friend of a friend, whom neither knows directly. Based on this observation, we propose to bound the reach of indirect up-dates. Our modification adds one parameter  X  to the vector clock framework, which restricts how far information can travel along time-respecting paths; we will also refer to this parameter as the reach of indirect updates. More precisely, the reach of indirect in-formation is bounded by the minimal number of hops it ever took information to travel between a pair of actors on time-respecting paths. Consider the consequence of assigning the following values to  X  :  X  = 1 restricts the creation of temporal views to those pairs of ac- X  = 2 additionally allows the creation of temporal views for dis- X  =  X  corresponds to the classical vector clock algorithm with This modification is straightforward to implement, because using the vector-clock framework, it is trivial to track the length of short-est time-respecting paths: When processing a communication event ( t,s,r ) , the minimum number of hops it ever took information about u to reach r is given by where dist t ( a,b ) refers to the length of the shortest time-respecting ( a,b ) -path until time t . In this way, distances are directed, respect-ing the ordering of events, and decreasing over the course of time. In our implementation, distances are not known to the source of an information chain, but saved together with the vector clock of the target. Once a short information chain has been observed, a corresponding temporal view is established and also allowed to be updated by longer information chains.

Not only does the  X  parameter allow the vector clock update process to more closely approximate how indirect updates actually take place in social communication, but in practice this restriction also substantially reduces the memory used by the algorithm, mak-ing it scale to large sparse social networks with millions of actors and hundreds of millions of communication events. Table 1 shows the memory and CPU runtime requirements of a Java-based imple-mentation of the social vector clocks. The datasets are described below in Section 5.1 (except for the proprietary mobile phone com-munication dataset, which we only use here to demonstrate scala-bility). These results indicate that setting  X  = 2 requires roughly one tenth the memory of traditional vector clocks where  X  =  X  . Further reducing  X  to 1 brings the memory requirements down roughly by another factor of 10.
We have described how social vector clocks can, in real time, keep track of the most recent information that could have possibly traveled between pairs of nodes. While we compute the social vec-tor clocks, we can easily derive several features that may be useful for link prediction. These features can then be combined using a supervised link predictor as we outlined in Section 3.2.

A first feature is immediately derived from the temporal views that are saved in the vector clocks: the current latency is defined as the difference between the current time and the timestamp saved in the temporal view. As second and third features, we track the number of direct updates and indirect updates that occur between a pair of actors as the vector clocks are computed. A fourth feature we calculate is the expected latency between each pair of nodes, which can be thought of as the best guess on how out-of-date an actor is about another at any point in the observation window. See Figure 4 for an illustration of all these features.

Some users of a service like Twitter may be much more active than others. This heterogeneity will mean that some users will have a latency of weeks with their closest contacts, whereas others will typically have a latency of days or hours with their closest contacts. Such heterogeneity may make it hard for a classifier to detect a decision boundary; for this reason, in addition to keeping track of the absolute values of the current and expected latencies, we keep track of the ranks. That is, from the perspective of a given node i , we sort each of i  X  X  temporal views by their current latency, and then rank the corresponding dyads X  X his yields an additional feature for each dyad { i,j } . We do the same for the expected latency.
So far we have described six features for each dyad: the current latency (both absolute value and rank), the expected latency (abso-lute value and rank), the number of direct updates, and the number of indirect updates. All of these features can be kept track of in real time and in practice they add little computational overhead. For Figure 4: Deriving link prediction features from social vector clocks: Example of a dyad with two direct updates (at time t t ; the new latency becomes zero), followed by one indirect up-date (at time t 3 ; the new latency is l 3 &gt; 0 ). The current latency (solid line) is a linear jump function resulting from t  X   X  expected latency (dashed line) is the weighted mean of average la-tencies (dotted horizontal lines) between vector clock updates. each dyad, we can keep track of all six of these features in both di-rections, yielding twelve features. Finally, our definition of social vector clocks included one parameter  X  which bounds how far in-formation can travel. In practice one might not know which value of this parameter will lead to the best results; in such a case, one can simply run multiple instances of the vector clocks in parallel, each with a different value of the reach parameter, and combine all the resulting features. A classifier can then learn which feature set is the most informative. For example, in our evaluation below, we run reach-parameterized vector clocks with three different reach parameters: 1, 2, and  X  , which creates a total of 36 features for each directed dyad.
While most work on link prediction focuses on the panel-data setting, some previous work also exploits fine-grained temporal in-formation, for example [27, 28, 29].

In [27], the authors propose a new dyadic index that exploits two temporal concepts related to those covered here. In particular, for each dyad (A, B), the index incorporates (1) how much time has elapsed since A and B last interacted, and (2) for each common neighbor C, the similarity of A X  X  and B X  X  latency with C. However, the approach we propose here differs from [27] in a couple of key ways. Firstly, our concept of latency between two nodes allows for indirect updates. Secondly, while in [27] several temporal aspects are combined into a single index, our method keeps these sepa-rate and instead produces a feature vector with several dimensions. Given a feature vector with several dimensions, a classifier should have more information to detect the decision boundary.

In [28], an approach based on meta-paths is introduced that is particularly well-suited for heterogeneous information networks, such as can be formed from co-authorship or product recommen-dation data. The authors also define a new formulation of the link prediction problem, which is not based on whether a link will form in the near future, but rather on predicting when it will form. While we do not consider this new formulation here, for certain applica-tions it may be more relevant than the classic formulation.
Two of the datasets we consider come from Twitter. While Twit-ter is often used as a medium for impersonally broadcasting mes-sages to large numbers of followers, it also supports more targeted forms of communication, in which users explicitly refer to each other. This targeted (although public) communication occurs in the form of retweets (in which one user rebroadcasts another users tweet, and attributes the tweet to its source) and user mentions, where the @ symbol is used to explicitly refer to a user. In the Twitter data that we analyze here, we filter Twitter datasets to in-clude only this targeted form of communication (i.e., those with retweets or user mentions). We remove self loops, and if a tweet mentions more than one user, we turn it into as many events as there are users mentioned in the tweet. With this representation, the data corresponds to the basic scenario of dyadic communica-tion event streams underlying our investigation: we are given a sequence ( t i ,s i ,r i ) , i  X  N , of (time,sender,receiver) tuples sat-isfying t i  X  t i +1 and s i 6 = r i .
 Twitter UK Olympics Data The olympics dataset covers Twit-Twitter US Elections Data Similar to the olympics dataset, the StudiVZ Wall posts StudiVZ was created in 2005 as a German UC Irvine Panzarasa et al. [33] introduced an event-based dataset twitter.com/#!/Telegraph2012/london2012 Figure 5: An overview of how we scored the link prediction task. Link predictors are first run on each realization of the experiment. In each realization, precision-recall curves are constructed. The AUPR for each predictor is then measured, and the AUPR of VCLP and the combined link predictor is then divided by the AUPR of HPLP+; this score is the relative performance of each predictor with HPLP+ as the baseline. For each dataset, the average of these scores is then reported; stratified over different geodesic distances.
The High-Performance Link Predictor (HPLP+) introduced in [12] is a state of the art link predictor which combines some of the strongest unsupervised link predictors. In the following experi-ments, HPLP+ acts as the baseline predictor and our objective is to evaluate the performance of the vector clock link predictor (VCLP) described in Section 4.3, and a combined predictor which uses the features from both VCLP and HPLP+. 3
Framing a supervised link prediction task requires several pa-rameters. One important parameter is the choice of classifier: as in [12], we used bagged forests, a technique suited for the ex-tremely imbalanced classes found in link prediction. However, rather than bagging ten random forests, we bag ten Stochastic Gra-dient Boosting classifiers. We use the implementation provided in the scikit-learn python package [35], using 1000 trees in each clas-sifier, setting the learning rate to 0.005, and subsampling rate to 0.5. We use the LPMade link prediction framework to compute HPLP+; this is the author X  X  reference implementation [34]. In each bag we sampled from the positive instances with replace-ment, and undersampled from the negative instances with replace-ment such that the class imbalance ratio was 10 negative for every positive.

Another important experimental parameter is whether the link prediction is directed or undirected. In all of our datasets, edge di-rection is highly relevant X  X or example, I might mention President Obama in a tweet, but Obama mentioning me in a tweet would have a completely different meaning. For this reason, we restrict our evaluation to directed link prediction. As one can see in Table 2, as the directed geodesic distance ( N ) in the link prediction task increases, classes become severely imbalanced, and in the case of olympics , hardly any new links form. In olympics in general, the classifier has very little positively labeled data to train on, which increases the risk of overfitting when extra features are added.
We must also specify some parameters related to the width of the temporal windows used in the evaluation. In principle, we wanted to make the duration of training period long enough so that a clear and stable snapshot of the network has emerged, and then evalu-ate on events that occur just after the end of the training period. Therefore, wherever possible we used a training window of 120 days and a test window of 7 days. (In other words, the width of the red bars in Figure 2 is 120 days and the width of the blue bars is 7 days.) However, given the short duration of the UC Irvine dataset, we use a shorter training period than in the other datasets, and are not able to run as many realizations of our evaluation; we set the training period to 28 days. Furthermore, the small size of olympics meant that in the seven day test period very few new links emerged, leaving the classifier with too little data to train on. Thus, for olympics we set the test width to 14 days.
The number of realizations performed on each dataset is indi-cated in Table 2. For each realization, we record the precision-recall curve of each predictor, leaving us with a sequence of pre-cision-recall plots such as those presented in the upper section of Figure 5. We are interested in how VCLP and the combined predic-tor perform relative to HPLP+, so we summarize them as follows (as outlined in Figure 5): We treat the performance of HPLP+ as the baseline, and in each plot, we measure the area under HPLP+ X  X  precision-recall curve. We then record the area under the precision-recall curve (AUPR) of both VCLP and the combined predictor as a fraction of HPLP+ X  X  AUPR. Thus, if in one realization HPLP+ X  X  AUPR is 0 . 020 and the combined predictor X  X  AUPR is 0 . 024 , then we record the combined predictor X  X  score as 1 . 2 . After recording this score for all realizations, we are left with a distributions of scores as in the histogram in Figure 5. By taking the average of these scores, we can characterize in a single number how much better or worse VCLP and the combined predictor perform than HPLP+. We report these averages for each experiment in Table 3a; see next section for discussion.
In Table 3a, we see that VCLP on its own performs comparably to HPLP+. Considering that HPLP+ combines a broad range of so-phisticated graph features, we were surprised to see VCLP perform similarly. Moreover, all network statistics employed by the pro-posed VCLP can be kept track of online, directly on the list of com-munication events, while many of the statistics included in HPLP+ have to be recalculated whenever new links are added to the net-work. Consequently, our results suggest that link prediction with vector-clock statistics can be performed much more efficiently in Figure 6: The directed dyad (A, D) has a geodesic distance of 3, but the distance of dyad (D, A) is 1. Thus, the dyad (A, D) would be included in the experiments whose results are presented in Ta-ble 3a, but would be excluded from the experiments whose results are presented in Table 3b. any situation, where the model parameters are learned beforehand and applied in real-time on a growing sequence of  X  X est X  events.
When the features of VCLP and HPLP+ are combined, the per-formance increase over HPLP+ is substantial. In general, the per-formance gain is largest when we are predicting links on dyads that have a geodesic distance N = 2 . Performance gain decreases for greater N , suggesting that VCLP features are most useful for predicting local links rather than long-range links ( irvine is an exception to this trend, where N = 3 sees by far the largest boost to performance). The improvement is smallest on olympics , per-haps because the classifier struggles with the small number of pos-itive training examples.

Given that stand-alone VCLP and HPLP+ yield similar predic-tion accuracy, it is interesting to observe the added value of comb-ing both predictors. In other words, there appear to be qualitative differences in the network effects that can be captured in the VCLP and HPLP+ framework.
Imagine we X  X e trying to predict whether a node A will soon send its first message to D. One of the features included in VCLP is D X  X  current latency with A through direct updates  X  in other words, how many seconds have elapsed since D sent a message to A. Given the significance of reciprocity, this feature will be extremely useful for cases where D has just sent a message to A. It could be the case that this feature alone  X  which is trivial to keep track of without vector clocks  X  is responsible for all of the benefit that comes from VCLP. In that case, we could simply keep track of this single feature and forget about vector clocks.

To measure whether this is the case, we run the entire evaluation again, but exclude all dyads where D has had any direct contact with A; see Figure 6. The results presented in Table 3b are in the same units as the results presented in Table 3a. We observe that the performance of VCLP does indeed drop, but that there is still a significant benefit provided by combining the features of VCLP and HPLP+. Again, we stress that the lackluster performance on olympics may be due to the small number of new links that form, which provides very few positive training examples.
The current approach used by state of the art link predictors is to operate in a panel data setting, in which finer-grained temporal information is ignored. In cases where link formation is not driven by cascades of information, such an approach might be appropriate. Regarding co-authorship networks, for instance, precise informa-tion on the sequence and spacing of events may be largely irrelevant or even misleading, and so it may be reasonable to aggregate away information on exact publication dates. However, in some net-works  X  such as the Twitter retweet/mention networks mentioned here  X  information cascades are an important mechanism for driv-ing the formation of edges. In such a setting, the information con-tained in the exact sequence and spacing of events is highly rele-vant, and so the approach commonly employed in link prediction  X  to simply aggregate event data into panel data  X  is highly question-able. For example, the mechanism of reciprocity has been shown to be important in the context of directed link prediction. Thus, if we are trying to predict whether A will send a message to B, then an extremely useful piece of information is whether B has recently sent A a message; if so, it is likely that A will respond to B. By aggregating all events into a static graph, traditional link prediction schemes cannot exploit such simple and useful mechanisms.
Our results suggest that dyadic features that exploit fine-grained temporal information beyond reciprocity are highly relevant for predicting which actors will communicate for the first time in the near future. The approach we introduce here, called the Vector Clock Link Predictor (VCLP), is based on keeping track of the la-tencies between all presumably relevant pairs of actors. The basic idea is to exploit information on how out of date a node A is with respect to another node B, and for doing so we adopt the concept of vector clocks. As an essential modification, we parameterized the traditional vector-clock concept to bound the reach of indirect information. Not only does this make the vector-clock update pro-cess more closely approximate how indirect updates actually take place in social communication, but in practice this restriction also dramatically reduces the memory used by the algorithm, thus mak-ing it applicable to large sparse social networks with millions of actors and billions of communication events.

We have demonstrated that binary classifiers can indeed exploit actor latencies to improve accuracy in link prediction. Even HPLP+, a classifier which utilizes a wide range of graph features based on aggregated panel data, can perform substantially better when provided with additional features based on vector clocks. More-over, VCLP on its own already performs comparably to HPLP+, which allows for much more efficient link prediction in any situ-ation where the model parameters are learned beforehand and ap-plied in real-time on a growing sequence of events.

Both of the supervised link prediction schemes considered here are based on many features, and by adding or removing various features many variations of social vector clocks are conceivable. Even with the intuitive motivation for social vector clocks and their demonstrated performance, we have not necessarily advanced the understanding of the actual mechanisms behind link formation. We are keen to gain more detailed insight into the link prediction prob-lem for specific types of interaction, e.g., by combining feature-selection schemes and more elaborate substantive theories.
We thank Derek Greene for providing us with the Twitter elec-tion datasets; see [30] for details on obtaining these datasets. We thank Ryan Lichtenwalter for providing detailed instructions on us-ing LPmade (described in [34]) and for providing helpful feed-back. This work is supported in part by Deutsche Forschungsge-meinschaft under grant Br 2158/6-1, Science Foundation Ireland under grant no. 08/SRC/I1407 (Clique: Graph and Network Analy-sis Cluster), and the University of Konstanz under grant FP 665/10. [1] R. Lichtenwalter and N. V. Chawla. Link prediction: fair and [2] D. Liben-Nowell and J. Kleinberg. The link prediction [3] L. L X  and T. Zhou. Link prediction in complex networks: A [4] S. Wasserman and K. Faust. Social Network Analysis: [5] U. Brandes and T. Erlebach, editors. Network Analysis: [6] T. A. B. Snijders. Models for Longitudinal Network Data. In [7] P. Holme and J. Saram X ki. Temporal networks. Physics [8] U. Brandes, J. Lerner, and T. A. Snijders. Networks [9] G. Kossinets, J. Kleinberg, and D. Watts. The structure of [10] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts. [11] L. A. Adamic and E. Adar. Friends and neighbors on the [12] R. Lichtenwalter, J. Lussier, and N. V. Chawla. New [13] C. Fidge. Timestamps in message-passing systems that [14] F. Mattern. Virtual time and global states of distributed [15] D. S. Parker, G. J. Popek, G. Rudisin, A. Stoughton, B. J. [16] L. Lamport. Time, clocks, and the ordering of events in a [17] R. Baldoni and M. Raynal. Fundamentals of distributed [18] B. Charron-Bost. Concerning the size of logical clocks in [19] F. J. Torres-Rojas and M. Ahamad. Plausible clocks: [20] S. Meldal, S. Sankar, and J. Vera. Exploiting locality in [21] M. Singhal and A. Kshemkalyani. An efficient [22] J. Kleinberg. The small-world phenomenon: an algorithm [23] S. Milgram. The small world problem. Psychology Today , 1 [24] D. J. Watts and S. H. Strogatz. Collective dynamics of [25] R. Hill and R. Dunbar. Social network size in humans. [26] R. S. Burt. Secondhand brokerage: Evidence on the [27] L. Munasinghe and R. Ichise. Time aware index for link [28] Y. Sun, J. Han, C. C. Aggarwal, and N. V. Chawla. When [29] T. Tylenda, R. Angelova, and S. Bedathur. Towards [30] D. Greene, D. O X  X allaghan, and P. Cunningham. Identifying [31] D. Archambault, D. Greene, and P X draigCunningham.
 [32] C. Lee, T. Scherngell, and M. J. Barber. Investigating an [33] P. Panzarasa, T. Opsahl, and K. Carley. Patterns and [34] R. Lichtenwalter and N. V. Chawla. LPmade: Link [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
