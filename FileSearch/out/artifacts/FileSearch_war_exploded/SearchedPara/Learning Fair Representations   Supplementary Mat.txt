  X  German Credit Dataset. The dataset has 1000 instances which classify the bank account hold-ers into credit class Good or Bad . Each person is described by 20 attributes, which include 13 cate-gorical and 7 numerical attributes. In our experi-ments we consider Age as the sensitive attribute, following earlier papers.  X  Adult Income Dataset. The dataset has 45,222 instances. The target variable indicates whether or not income is larger than 50K dollars, and the sensitive feature is Gender . Each data object is described by 14 attributes which include 8 cate-gorical and 6 numerical attributes. A full descrip-tion can be found at (Kohavi, 1996).  X  Heritage Health Dataset. This dataset was de-rived from the Heritage Health Prize milestone 1 challenge. We used the same features as the winning team, Market Makers. The goal of this dataset is to predict the number of days that a person will spend in the hospital in a given year. To convert this into a binary classification task, we simply predict whether they will spend any days in the hospital that year. We split the pa-tients into two groups based on whether they are older or younger than 65. As a pre-processing step, we binarize the data by quantizing any non-categorical variables.
 For all of the datasets, in order to facilitate compar-ison to the other methods, notably the naive Bayes method which assumes binary variables, we transform all attributes to binary variables by using  X  X ne-hot en-coding X  for each categorical attribute (one variable per value, always exactly one one in the set), and quanti-zation for numerical attributes.
 After the modification, each example in the German Credit Dataset has 61 binary features while the in-dividuals in the Adult Income Dataset are described by 103 binary features, including the sensitive feature. The 139 attributes for the Heritage Health dataset are now encoded in 1157 binary variables.
 For the German Credit Dataset, we optimized each method across five splits, each containing 50% of the data as a training set, 20% as validation and 30% as a test set. The Adult Income Dataset is already divided so that the training set contains two-thirds of the data with the remainder set aside for test. Here we op-timized each method across five splits, each utilizing one-third of the training set as a validation set and the rest for training. For the Health dataset we split the data into five equally sized folds. We train models on three of the folds independently, and test each model using one of the remaining folds for validation and one for testing.
 Here we present the full quantitative results comparing the methods on the various datasets.
 Finally, an aspect of the model that bears scrutiny is its sensitivity to parameter settings and initialization. First, we found that the results of the models were consistent across splits of the data; for example, the variance of the accuracy in the validation sets were 3.12e-06, 5.80e-7 and 1.85e-04 and the variance of the discrimination was 6.38e-05, 3.43e-5 and 8.20e-04 on the Health, Adult and German datasets respectively. We also found that our model obtained fairly simi-lar results across a range of settings for the hyper-parameters, with the expected effect on the learned system. As an example, we show in Table 4 the results of our model as we vary the number of prototypes K , while maintaining the setting of the other hyperpa-rameters ( A x = 0 . 01 , A y = 1 , A z = 50). The trend is clear: adding more prototypes increases the accuracy as it allows finer classification decisions, but also leads to more discrimination as it is more difficult to remove information about membership in the protected set.
