 In many scientific and engineering applications, such as ima ge annotation [28] and web-page clas-aspects, which will be referred to as views . Standard predictive methods, such as support vector of distinct views. These methods would sacrifice the predict ive performance [7] and may also be incapable of performing view-level analysis [12], such as predicting the tags for image annotation been done on exploring multi-view information to alleviate the difficult semi-supervised learning and perform view-level analysis, particularly view-level predictions.
 To discover a subspace representation shared by multi-view data, the unsupervised canonical cor-relation analysis (CCA) [17] and its kernelized version [1] ignore the widely available supervised to finding such a projected subspace. However, this determin istic approach cannot provide view-(e.g., image classification) and view-level predictions (e .g., image annotation). conditional independence is much weaker than the typical as sumption (e.g., in the seminal work of input variables [26]. Therefore, we ground our approach on t he undirected MNs. Undirected latent variable models have shown promising performance in many ap plications [26, 20]. In the multi-data. For example, considering word ordering information c ould improve the quality of discovered latent topics [23] compared to a method (e.g., LDA) solely ba sed on the natural bag-of-word rep-applications [15]. To learn the multi-view latent space MN, we develop a large-margin approach, which jointly maximizes the data likelihood and minimizes t he hinge-loss on training data. The learning and inference problems are efficiently solved with a contrastive divergence method [25]. Our results show that the large-margin approach can achieve significant improvements in terms of prediction performance and discovered latent subspace rep resentations.
 The paper is structured as follows. Sec 2 and Sec 3 present the multi-view latent space MN and concludes. 2 Multi-view Latent Space Markov Networks ... H 1 H K The unsupervised two-view latent space Markov network is sh own in Fig. 1, which consists of two views of input data X := { X and Z := { Z ease of presentation, we assume that the variables on each vi ew are connected via a linear-chain. Extensions to multiple vi ews and more complex structures on each view can be easily done, afte r we have presented the constructive independence assumption that given the latent variables H , the two views X and Z are independent. Graphically, we can see that both the exponential family Har monium (EFH) [26] and its extension of dual-wing Harmonium (DWH) [28] are special cases of multi -view latent space MNs. Therefore, it is not surprising to see that multi-view MNs inherit the wi dely advocated property of EFH that For each view, we consider the first-order Markov network. By the random field theory, we have each component h is: where  X  ( h Next, the joint model distribution is defined by combining th e above components in the log-domain Then, we can directly write the conditional distributions o n each view with shifted parameters, exponential form with a pairwise potential function, which is very similar to conditional random  X  to denote all the parameters (  X , X , X , W , U ) .
 forward-backward message passing scheme [18]. For a genera l model structure, which may contain many loops, approximate inference such as variational meth ods [22] is needed to perform the task. We will provide more details when presenting the learning pr oblem.
 Up to now, we have sticken on unsupervised multi-view latent space MNs, which are of wide use space MNs are defined similarly as above, but with an addition al view of response variables Y . Now, the conditional independence is: X , Z and Y are independent if H is given. As we have stated, this assumption is much weaker than the typical cond itional independence assumption that X and Z are independent given Y . Based on the constructive definition, we only need to specif y consider the discrete case, where y  X  X  1 ,  X  X  X  ,T } , and define others are 0 . Accordingly, V is a stacking parameter vector of T sub-vectors V (1), but with an additional term of V &gt; f ( h ,y ) = V &gt; We note that a supervised version of DWH, which will be denote d by TWH (i.e., triple wing Harmo-nium), was proposed in [29], and the parameter estimation wa s done by maximizing the joint data likelihood. However, the resultant TWH model does not yield improved performance compared to the naive method that combines an unsupervised DWH for disco vering latent representations and of prediction performance and predictiveness of discovere d latent subspace representations. To learn the supervised multi-view latent space MNs, a natur al method is the maximum likelihood in defining a normalized probabilistic model as in Eq. (2), of which the normalization factor can make the inference hard, especially in directed models [24] . Moreover, the standard MLE could re-motivating us to develop a more discriminative learning app roach. An arguably more discriminative large-margin idea into the learning of supervised multi-vi ew latent space MNs for multi-view data analysis, analogous to the development of MedLDA [31], whic h is directed and has single-view. For brevity, we consider the general multi-class classificatio n, as defined above. 3.1 Problem Definition take the expectation over the latent variable H and define the prediction rule as z the expectation of the missed components, as detailed below in Eq. (5).
 loss, as used in SVMs. Given training data D = { ( x rule (3) is where  X  ` true label y that the hinge loss is an upper bound of the empirical loss R principle of regularized risk minimization , we define the learning problem as solving where L ( X ) :=  X  P constants, which can be selected via cross-validation. Not e that R Since problem (4) jointly maximizes the data likelihood and minimizes a training loss, it can be and a prediction model parameter V , which on the one hand tend to predict as accurate as possible on training data, while on the other hand tend to explain the d ata well. 3.2 Optimization technique [16, 25, 26, 28]. Specifically, we derive a variati onal approximation L v ( q negative log-likelihood L ( X ) , that is: where R ( q,p ) is the relative entropy, and q their observed values while q make the structured mean field assumption [27] that 1 q ( x , z , h ) = q ( x ) q ( z ) q ( h ) . Solving the approximate problem : Applying the variational approximation L v in problem (4), we get an approximate objective function L ( X  , V ,q mization method, which iteratively minimizes L ( X  , V ,q q is reconstructed once the optimal q The problem of solving q distribution q (can be q q ( x ) = p ( x | E q ( H ) [ H ]) , q ( z ) = p ( z | E q ( H ) [ H ]) , and q ( h ) = For q efficiently done because of its factorized form. The distrib ution q above updates starting from q models, we can use a message passing scheme [18] to infer thei r marginal distributions, as needed for parameter estimation and view-level prediction (e.g., image annotation), as we shall see. For generally structured models, approximate inference techn iques [22] can be applied. After we have inferred q sub-gradient descent, where the sub-gradient is computed a s: where  X  y tation E If the prediction label y model towards discovering a better representation for pred iction. We have developed the large-margin framework with a generic multi-view latent space MN to model work, in this paper, we concentrate on a simplified but very ri ch case that the data on each view annotation and retrieval. We denote the specialized model b y MMH (max-margin Harmonium). framework, and the only needed change is on the step of inferr ing q in this direction to the full extension of this work.
 and z is a vector of real-valued features (e.g., color histograms ). Each x that denotes whether the i th term of a dictionary appears or not in an image, and each z number that denotes the normalized color histogram of an ima ge. We assume that each real-valued h where W With the above definitions, we can follow exactly the same pro cedure as above to do parameter estimation. For the step of inferring q Therefore, the sub-gradients can be easily computed. Detai ls are deferred to the Appendix. expectation. In this case, we have E Therefore, the classification rule is y  X  = arg max x at its observed values. Then, tags with high probabilities a re selected as annotation. We report empirical results on TRECVID2003 and flickr image d atasets. Our results demonstrate tive subspace representations and the tasks of image classi fication, annotation and retrieval. 5.1 Datasets and Features The first dataset is the TRECVID2003 video dataset [28], whic h contains 1078 manually labeled and a 165-dim vector of HSV color histogram, which is extract ed from the associated keyframe. We WIDE [10], which is a big image dataset constructed from flick r web images. This dataset contains 3411 images about 13 animals, including cat , tiger , etc. See Fig. 6 for example images for each valued features (i.e., 64-dim color histogram, 144-dim col or correlogram, 73-dim edge direction histogram, 128-dim wavelet texture and 225-dim block-wise color moments) and 500-dim bag-of-word representation based on SIFT [19] features. We randoml y select 2054 images for training and use the rest for testing. The online tags are also downloaded for evaluating image annotation. 5.2 Discovering Predictive Latent Subspace Representatio ns We first evaluate the predictive power of the discovered late nt subspace representations. Fig. 2 shows the 2D embedding of the discovered 10-dim latent representations by three models (i.e., MMH, DWH and TWH) on the video data. Here, we use the t-S NE algorithm [21] to find margin based MMH show a strong grouping pattern for the image s belonging to the same category, while images from different categories tend to be separated from each other on the 2D embedding vised DWH and supervised TWH do not show a clear grouping patt ern, except for the first category. margin based latent subspace model can discover more predic tive or discriminative latent subspace representations, which will result in better prediction pe rformance, as we shall see. topics 2 . As shown on the top of each plot in Fig. 2, the large-margin ba sed MMH obtains a much larger average KL-divergence than the other likelihood-ba sed methods. This again suggests that the latent subspace representations discovered by MMH are m ore discriminative or predictive. We topics), where the average KL-divergence scores of 60-topi c MMH, DWH and TWH are 3.23, 2.56 and 0.463, respectively.
 Finally, we examine the predictive power of discovered late nt topics. Fig. 3 shows five example topics discovered by the large-margin MMH on the flickr image data. For each topic H the 5 top-ranked images that yield a high expected value of H not very discriminative between squirrel and wolf . 5.3 Prediction Performance on Image Classification, Retrie val, and Annotation 5.3.1 Classification We first compare the MMH with SVM, DWH, TWH, Gaussian Mixture ( GM-Mix), Gaussian Mix-ture LDA (GM-LDA), and Correspondence LDA (CorrLDA) on the T RECVID data. See [4] for MMH and build an SVM classifier, which uses both the text and co lor histogram features without distinguishing them in different views. For each of the unsu pervised DWH, GM-Mix, GM-LDA and CorrLDA, a downstream SVM is built with the same tool based on the discovered latent represen-because of its too low performance. We can see that the max-ma rgin based multi-view MMH per-forms consistently better than any other competitors. In co ntrast, the likelihood-based TWH does not show any conclusive improvements compared to the unsupe rvised DWH. These results show method. The superior performance of MMH compared to the flat S VM demonstrates the usefulness of modeling multi-view inputs for prediction. The reasons f or the inferior performance of other models (e.g., CorrLDA and GM-Mix) are analyzed in [28, 29].
 MMH only with the best performed DWH, TWH and SVM. For these me thods, we use the 500-dim SIFT and 634-dim real features, which are treated as two v iews of inputs for MMH, DWH and TWH. Also, we compare with the single-view MedLDA [31], w hich uses SIFT features only. To be fair, we also evaluate a version of MMH that uses SIFT fea tures, and denote it by MMH (SIFT). Again, we can see that the large-margin based multi-view MMH performs much better than single-view MMH (SIFT), it performs comparably (slightly b etter than) with the large-margin based MedLDA, which is a directed BN. With the similar large-margi n principle, MMH is an important extension of MedLDA to the undirected latent subspace model s and for multi-view data analysis. 5.3.2 Retrieval For image retrieval, each test image is treated as a query and training images are ranked based on the retrieval results by computing the average precision (A P) score and drawing precision-recall curves. Fig. 4 (c) compares MMH with four other models when th e topic number changes. Here, we show the precision-recall curves when the topic number is set at 15 and 20. We can see that for the AP measure, MMH outperforms all other methods in most cases, and MMH consistently have similar observations. The AP scores of the 60-topic MMH , DWH, and TWH are 0.163, 0.153 and 0.158, respectively. Due to space limitation, we defer t he details to a full extension. Finally, we report the annotation results on the flickr datas et, with a dictionary of 1000 unique tags. The average number of tags per image is about 4 . 5 . We compare MMH with DWH and TWH with two views of inputs X  X for tag and Z for all the 634-dim real-valued features. We also compare with the sLDA annotat ion model [24], which uses SIFT features and tags as inputs. We us e the top-N F1-measure [24], denoted by F 1@ N . With 60 latent topics, the top-N F-measure scores are shown in Fig. 5. We can see that the large-margin based MMH significantly outperforms all the competitors. Fig. 6 shows example images from all the 13 cate gories, where for each category the left image is generally of a good annotation quality and the r ight one is relatively worse. ciently done with contrastive divergence methods. Finally , we concentrate on a specialized model video and web image datasets demonstrate the advantages of l arge-margin learning for both predic-and computer vision [15] applications.
