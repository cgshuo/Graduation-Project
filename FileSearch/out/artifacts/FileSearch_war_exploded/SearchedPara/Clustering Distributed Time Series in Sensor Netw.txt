
Event detection is a critical task in sensor networks, es-pecially for environmental monitoring applications. Tra-ditional solutions to event detection are based on analyz-ing one-shot data points, which might incur a high false alarm rate because sensor data is inherently unreliable and noisy. To address this issue, we propose a novel Dis-tributed Single-pass Incremental Clustering (DSIC) tech-nique to cluster the time series obtained at sensor nodes based on their underlying trends. In order to achieve scala-bility and energy-efficiency, our DSIC technique uses a hier -archical structure of sensor networks as the underlying in-frastructure. The algorithm first compresses the time serie s produced at individual sensor nodes into a compact repre-sentation using Haar wavelet transform, and then, based on dynamic time warping distances, hierarchically groups the approximate time series into a global clustering model in an incremental manner. Experimental results on both real data and synthetic data demonstrate that our DSIC algo-rithm is accurate, energy-efficient and robust with respect to network topology changes.
The advent of wireless sensor networks has fostered growing interest in many important applications for envi-ronmental monitoring. Sensor networks facilitate the pro-cess of monitoring the physical environments and making real-time decisions about events in the environment. In suc h monitoring applications, automatic event detection is an e s-sential task, which aims at identifying emergent physical phenomena of particular concern to the users. When an ab-normal event is detected, the monitoring system will sound an alarm for immediate attention so that prompt actions can be taken to minimize adverse impact of abnormal events. Traditional solutions to event detection can be classified into threshold-based approaches [1] and pattern-based ap-proaches [12, 21]. Threshold-based approaches consider an inherently more complex than traditional clustering tasks [16]. First, sensor networks typically consist of small, battery-powered nodes with limited communication and computational capability. We therefore need to design an energy-efficient technique to cluster the time series obtai ned at sensor nodes. Second, sensor networks are usually de-ployed in a wide area. Thus, the clustering algorithm must be designed to operate in a distributed setting. Third, the communication links among sensor nodes are highly un-reliable subject to constrained energy and communication, making the network topology change over time. This re-quires our clustering technique to be robust to the changes of network topology. Therefore, it is highly desirable to de -sign an energy-efficient , distributed and accurate approach for clustering distributed time series in sensor networks.
In this paper, we propose a novel Distributed Single-pass Incremental Clustering (DSIC) technique to deal with these challenges. In order to achieve scalability and energ y-efficiency, our DSIC technique uses a hierarchical structur e of sensor networks as the underlying infrastructure, where sensor nodes are self-organized into physical clusters with one node selected as a cluster head for each physical clus-ter. Each cluster head is responsible for collecting the dat a from the members and performing most of the computa-tional tasks in its cluster. All the cluster heads form a mult i-hop routing tree back to the gateway. Our DSIC technique works in two phases. In the first phase, the time series data produced at sensor nodes is compressed using Haar wavelet transform [13] and the selected wavelet coefficients are sen t to a cluster head. Upon receiving new data from the mem-bers in its cluster, each cluster head first reconstructs the time series and then incrementally construct a local clus-tering model based on Dynamic Time Warping (DTW) dis-tances. In the second phase, the data clusters are merged across different physical clusters along the routing tree u n-til the gateway obtains a global clustering model. To offset the effect of the data ordering on incremental clustering, w e also devise a new heuristic strategy to ensure the quality of the clustering model. Another advantage of our DSIC technique is that, only a set of cluster representatives nee d to be transmitted across the network during the clustering process, thereby significantly reducing data acquisition a nd transmission costs in sensor networks. Experimental resul ts on both real data and synthetic data have demonstrated that our DSIC algorithm is accurate, energy-efficient and robust with respect to network topology changes.

The remainder of this paper is organized as follows. Sec-tion 2 reviews previous work related to our clustering prob-lem. Section 3 discusses the sensor network infrastructure used in our algorithm. Section 4 describes our proposed DSIC algorithm in detail. Section 5 presents the experimen-tal results on real data and simulation data. Section 6 con-cludes the paper and discusses directions for future work. statistics of the whole cluster.
Distributed clustering assumes that the data to be clus-tered resides on different sites. Previous works on dis-tributed clustering are usually based on the popular k -means algorithms. Instead of transmitting all the data to a centra l site, clustering is performed on two different levels, i.e. , the local level and the global level. At the local level, all site s carry out a local clustering independently from each other. At the global level, the central site is responsible for buil d-ing a global clustering model based on local models. For example, Forman and Zhang [6] proposed a k -Harmonic Means algorithm to cluster homogeneously dis-tributed data. During each iteration, each site computes th e current k centroids based on its own data and broadcasts their centroids to other sites. Once a site has received all the centroids from other sites, it can form its global cen-troids by taking a weighted average over the entire data set. Kargupta et al. [10] developed a collective principal compo -nents analysis (PCA)-based clustering technique for heter o-geneously distributed data. Each local site performs PCA, projects the local data along the principal components and applies a standard clustering algorithm. Having obtained local clusters, each site sends a small set of representativ e data points to a central site. The central site then carries o ut PCA on the collected data and sends the global principal components back to each local site.

However, existing distributed clustering techniques mainly focus on one-shot clustering with respect to in-dividual data points. They are therefore unsuitable for continuously clustering streaming time series in resource s-constrained sensor networks.
To achieve the scalability and energy-efficiency of our clustering algorithm, we adopt a hierarchical organizatio n of sensor networks as the underlying infrastructure. The basic idea is to organize the network into different levels of granularity, ranging from small local areas at the lowest level to the entire network area at the highest level. More specifically, the sensor nodes are self-organized into a set of physical clusters based on available energy resources. Each physical cluster consists of a cluster head (CH) and several cluster members (CMs). The cluster head is responsible for performing most of the computational tasks in each physical cluster. All the cluster heads form a multi-hop routing tree back to the gateway.

As shown in Figure 2, at the lowest level of the routing tree, each cluster member sends its sensor data to the corre-sponding cluster head. Moving up the hierarchy, the cluster the time series obtained at individual sensor nodes. Haar wavelets have been found to be effective in providing good approximation of time series [3, 7]. Haar wavelets are cho-sen because it has a multi-resolution representation of tim e series and it can be computed quickly and easily, requir-ing linear time in the length of the sequence. Haar wavelet transform can be seen as a series of averaging and differenc-ing operations on a discrete time function. Below, we give an example to illustrate the procedure to perform the Haar transform on a time series f ( t ) = (9 , 7 , 6 , 6) .
Table 1. An example of Haar wavelet trans-form
The full resolution of the time series f ( t ) is 4. In resolu-tion 2, (8 , 6) is obtained by taking the average of (9 , 7) and (6 , 6) respectively, at resolution 4. (1 , 0) are the differences of (9 , 7) and (6 , 6) divided by 2, respectively. This process is recursively applied until resolution 1 is reached. Final ly, the Haar transform of the original time series H ( f ( t )) = (7 , 1 , 1 , 0) , which are called wavelet coefficients. The time series can also be reconstructed at different resolutions b y adding differences back to or subtract differences from av-erages. For example, (8 , 6) = (7 + 1 , 7  X  1) , where 7 and 1 are the first and second coefficients, respectively. The moti -vation behind Haar transform is that elements of little vari a-tion in the original data manifest themselves as small or zer o values in the transformed data. Therefore, we can approx-imate each original time series by selecting the k largest Haar coefficients so that the optimal amount of energy can be preserved per time series [20]. 4.1.2 Incremental Clustering Process Let CH physical cluster, and let CM belonging to the cluster. Each cluster member v compresses the time series f ing window of size W using Haar wavelet transform, and transmits the compressed data, consisting of the k largest coefficients along with their positions, to its correspondi ng cluster head CH each cluster head reconstructs the time series by applying an inverse Haar transform to Haar coefficients, and incre-mentally groups the compressed data into a set of data clus-ters.

In order to discover meaningful clusters, we propose to use Dynamic Time Warping (DTW) [3, 23] to measure the limited resources. Therefore, we adopt an accurate approx-imation algorithm FastDTW [19] to scale up the computa-tion, which can run in linear time and space.

Based on the distance measure, each physical cluster head incrementally builds a local clustering model upon receiving new compressed data from its members. Since cluster members may send their compressed time series data to the cluster head in an unsynchronized manner, the data ordering might largely affect the overall quality of th e distance-based clustering model. To address this issue, we propose a two-step heuristic strategy to offset the effect o f data ordering on the clustering quality. In our method, each cluster is represented using four features:  X  Cluster center C : is a vector in which each element  X  Cluster upper bound C u : is a vector in which each ele- X  Cluster lower bound C l : is a vector in which each ele- X  Cluster size | c | : is defined as the number of time series
Now we describe the process of incremental clustering performed for each physical cluster. The cluster head CH starts by initializing a data cluster for its own time series f ter members CM the data clusters as follows: The time series  X  f structed from Haar wavelets coefficients and associated po-sitions. We calculate the distances D (  X  f reconstructed time series and the cluster center C isting clusters c threshold  X  , a new data cluster c M clusters within the threshold  X  , we further test the can-didates by simulating the addition of the new time series to all of them and calculating the variances between the old and new cluster center. If all the variances exceed a pre-defined threshold  X  , that is, the addition of this time se-ries would change the characteristics of all existing clust ers dramatically, a new data cluster c By doing this, the time series  X  f to be merged with other clusters or new time series that comes later. Otherwise, for the cluster candidates whose center variances satisfy the threshold  X  , the time series  X  f is assigned to the cluster c cluster centers. After determining the data cluster c merged with c for each physical cluster. The output of the algorithm is a list of data clusters constructed for each physical cluster .
After a local clustering model is constructed for each physical cluster, the next task is to merge the data clusters across different physical clusters along the routing tree. At intermediate levels of the routing tree, each parent cluste r head collects the cluster representatives from the physica l clusters at the lower level and merges their associated data clusters. The merged data clusters are then communicated to the parent cluster head in an upper level. This process continues until the gateway has a global clustering model.
Considering a data cluster c at the lower level L data cluster c L threshold  X  , we merge the two data clusters together and update the cluster representatives. If there are more than one data clusters c cluster merge criterion as follows: where the terms d ( C dissimilarity of clusters c culated as d ( C a pair of clusters that have small self-similarity. In this w ay, clusters with a larger similarity are retained and the final clusters have larger self-similarity which can better repr e-sent actual events of interest. Finally, the data cluster c merged with the cluster c mized. Accordingly, the cluster representatives for clust er c is updated as follows: where C merged, and C bounds and lower bounds of the two clusters to be merged, respectively.

We now summarize the algorithm procedure in Algo-rithm 2. The MergeClusters procedure (lines 10 X 26) merges the data clusters DataC the data clusters DataC heads CH an AR model at each sensor node, and then, based on a communication graph, starts clustering from a set of nomi-nated root nodes and expands to include other nodes in the network. The second one is called Haar-Centre. This al-gorithm differs from our proposed algorithm in that, after the time series data is compressed using Haar wavelets, a standard clustering algorithm is applied for cluster form-ing and merging, solely based on the Euclidean distance to cluster centers. Our proposed algorithm is referred to as Haar-DSIC in the experiments.
The performance of the clustering algorithms is assessed using the following two evaluation criteria.  X  Clustering quality: The quality of clustering is mea- X  Communication cost: In order to evaluate the com-of data ordering on the clustering quality for Haar-Center and Haar-DSIC. Given a specific set of physical clusters, we randomly generated 50 different orderings of the data to be sent from cluster members to each cluster head. Figure 4 shows clustering accuracy with respect to different num-bers of Haar wavelets coefficients. An interesting observa-tion is that, when too few coefficients are used, the restored time series is too coarse to capture significant information from original time series, and thus, subsequence clusterin g becomes inaccurate. On the other hand, when too many wavelet coefficients are used, the restored time series has a fine granularity, which might retain random noises involved in the original time series. As a result, the performance of the clustering algorithms might degrades as well. We can see that, Haar-Center and Haar-DSIC can achieve the high-est clustering accuracy when the number of wavelet coef-ficients is four. Therefore, we set the number of wavelet coefficients to be four in the following experiments. Figure 4. Clustering quality vs. number of Haar wavelet coefficients
In addition, as the number of wavelet coefficients in-creases, Haar-DSIC can be observed to consistently outper-form Haar-Centre, with small variances in clustering accu-racy. This is because, for Haar-Centre, the data clusters ar e formed solely based on the Euclidean distance to the cluster centers, which makes the clustering accuracy quite sensiti ve to the data ordering. In contrast, Haar-DSIC outperforms Haar-Centre by considering the evolution of cluster center s in the cluster forming process.
 Figure 5 shows the clustering result obtained by the Haar-DSIC algorithm. In total, Haar-DSIC generates three clusters, as marked in the figure. Among them,  X  X luster 1 X  corresponds to the temporal event simulated in our exper-iment, and the other two clusters reflect different lighting conditions in our office.
Experiments were also carried out to compare the robust-ness of the three distributed algorithms (AR-Elink, Haar-Centre and Haar-DSIC) with respect to network topology changes. We used the LEACH protocol to randomly gener-ate 20 different sets of physical clusters by initializing t he number of physical clusters to be five. The three algorithms were then applied on the same data set. Table 3 shows the mean and the standard deviation of Silhouette score. We can see that, based on different configurations of physical clusters, Haar-DSIC can achieve better clustering accurac y with less variance than Haar-Centre and AR-Elink. This indicates that Haar-DSIC is more robust than Haar-Centre with respect to network topology changes over time.
Table 3. Clustering quality vs. network topol-ogy change
To study the scalability of our proposed algorithm, we simulated a sensor network at a square field of 100  X  100 meters. The default number of sensor nodes was set at 100, and the locations of sensor nodes were randomly gener-ated in the field. We simulated the measurements generated by sensor nodes based on the Cylinder-bell-funnel data set. The generation of this data set was proposed in [14]. The data set contains three distinct classes and the time series data has similar local trends in each class. We generated 500 time series at a length of 1000 for each class. The data generated by an individual sensor node was randomly se-lected from the total of 1500 time series. The length of the sliding window was set to be 128 in the experiments.
We first performed experiments to compare the cluster-ing quality of the clustering algorithms. The clustering re -sults are summarized in Table 4. Each value of Silhouette score is the average of 20 trials. In this experiment, for AR-Elink, the order of the AR model is set to be three, and the number of wavelet coefficients is set to be eight for Haar-Center and Haar-DSIC. Again, we can see that, Haar-DSIC outperforms the other two distributed algorithms (AR-Elin k and Haar-Center) to a large margin, and its accuracy is quite close to that of Raw-Centralized.

We then carried out experiments to compare the com-munication cost of the clustering algorithms. We varied the density of sensor nodes from 0.01 sensors/ sq.m. to 0.1 sensors/ sq.m. to study the scalability of the algorithms with perimental results on both real data and synthetic data have demonstrated that DSIC is accurate, energy-efficient and ro -bust with respect to network topology changes.

Our work can be extended in several directions. First, we will implement our DSIC technique in a real event driven sensor network for monitoring soil moisture and evaluate its effectiveness in detecting the wetting front to minimiz e the use of irrigation water. Second, we will explore how to discover the boundaries of homogeneous regions based on our clustering results. Third, we will extend our technique to track the movements of homogeneous regions when the sensor readings change over time.

We would like to thank Siddeswara Guru for providing us with the simulation results of the LEACH protocol and Paul McCarthy for his great help in data collection.
