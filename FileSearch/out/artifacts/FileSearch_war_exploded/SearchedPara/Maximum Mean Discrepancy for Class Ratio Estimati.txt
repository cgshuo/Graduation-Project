 Yahoo! Labs, Bangalore, Karnataka 560071 INDIA IIT Bombay, Powai, Mumbai, Maharashtra 400076 INDIA The goal of this work is to estimate the ratio of classes in any unlabeled test dataset given a labeled training dataset with an arbitrarily different ratio of classes. The closely re-lated problem of creating a classifier with shifted class pri-ors in the training and test set has been extensively studied. In contrast, our end goal is to estimate the ratio of classes and not the labels of individual instances in the unlabeled set. Many real-world applications have emerged that mo-tivate this problem. As an example consider websites that serve user directed content like news, videos, or reviews. Each item (article, video, or product) is associated with many user comments. An analyst wants to estimate the fraction of comments that express positive/negative senti-ments. The polarity of each comment is not of interest. We now describe our problem formally. Let X = { x  X  R d } be the set of all instances and Y = { 0 , 1 ,...,c } be the set of all labels. We are given a labeled dataset D (  X  X  X  Y ) . Our goal is to design an estimator that for any given set U (  X  X  ) can estimate the class ratios  X  = [  X  0 , X  1 ,..., X  where  X  y denotes the fraction of instances with class label y in U . We consider the case where the estimator will have to handle unlabeled data with widely varying values of  X  , which might be different from those in the training data. As in all existing work on the topic (discussed next) we assume that the Pr( x | y ) distribution remains unchanged in the training and test distributions.
 A baseline estimator is to use the labeled data D to train a classifier C : X 7 X  X  using supervised learning techniques and estimate  X  y as  X  n y n the number of instances of U labeled y by C . Since most supervised learning algorithms assume that the training and test data follow the same distribution, this method is un-likely to perform well. Many fixes have been proposed: for example, (Cortes &amp; Mohri, 2004; Cl  X  emenc  X on et al., 2009) propose to maximize the area under the ROC to make the classifier work for all possible class ratios, (Selvaraj et al., 2011) proposes transductive learning, while (Elkan, 2001) and (Lin et al., 2002) assume that the true class ratios are known, and propose to reweight instances or rebalance the decision cutoff. However, the primary goal of these meth-ods is to improve per-instance accuracy, and class ratios are estimated using the same paradigm of aggregating from per-instance predictions.
 Since we are not interested in the labels of individual in-stances, we explore direct methods of estimating the class ratios. There have been three reported attempts for such di-rect estimation. The first is an EM-based approach (Saerens et al., 2002) that alternates between estimating the class ratios from per-instance predictions and  X  X orrecting X  the predictions using the estimated class ratios as priors. The second approach (Plessis &amp; Sugiyama, 2012), proposes to minimize various f -divergence measures between the test distribution and a model distribution parameterized on the unknown  X  s and instance-level ratios of two distributions. This method is developed in a semi-supervised setting and involves solving an elaborate optimization problem for each test set. The third, most recent approach (Zhang et al., 2013), is based on minimizing the maximum mean dis-crepancy (MMD) over functions in a reproducing kernel Hilbert space (RKHS) induced by a kernel K . The MMD-based approach has several advantages: it is applicable to arbitrary domains since it does not assume any parametric density on the data unlike conventional mixture modeling approaches (Titterington, 1983; Woodward et al., 1984). Because of this property, MMD has been successfully de-ployed in other problems including covariance shift (Gret-ton et al., 2009) and two-sample test (Gretton et al., 2012a). When deployed for class ratio estimation, it gives rise to a convex QP over a small number of variables, which is efficiently solvable. However, the approach has not been understood theoretically, empirical comparisons with other class ratio estimation methods are lacking, and kernel se-lection has not been adequately addressed.
 In this paper we address the above limitations of the MMD approach. Specifically, we make these contributions: We theoretically analyze the MMD-based estimator for ar-bitrary number of classes. Under some mild conditions, we show that the estimator is statistically consistent. In addi-tion to asymptotic convergence rates, we derive empirical bounds that involve intuitive geometric quantities and moti-vate a kernel learning method. Analysis of the MMD-based estimator is non-trivial and requires bounding techniques that exploit the nature of the MMD objective in addition to typical concentration inequalities employed in learning theory. We are aware of no work that bounds the error of class ratio estimates by any other method.
 We use the insights obtained from the theoretical analysis, to propose a novel convex formulation for selecting the best kernel to be employed in the MMD-based estimation pro-cedure. Our kernel learning formulation turns out to be an instance of a Semi-Definite Program (SDP) with infinitely many linear constraints.
 Since it is expected that at optimality only a few of the lin-ear constraints are active, we propose a cutting-plane based algorithm for solving this formulation. At every iteration, the SDP restricted to the current constraint-set is solved us-ing a simple projected sub-gradient descent algorithm. We are aware of no prior work on kernel selection for this prob-lem.
 We present an extensive evaluation of several existing methods, both from the direct and per-instance aggregation family, under varying true class ratios and training sizes. We obtain up to 60% reduction in class ratio estimation er-rors over the best existing method.
 Outline: In Section 2, we present an overview of the MMD-based approach and analyze it theoretically. In Sec-tion 3, we provide our formulation for learning a kernel function for improved estimates. In Section 4 we present empirical comparisons and conclude in Section 5. The core idea in maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) is to match two distributions based on the mean of features in the Hilbert space induced by a kernel K . This is justified because when K is universal there is an injection between the space of distributions and the space of mean feature vectors lying in its RKHS. From a practical perspective too, the MMD approach is appealing because unlike other parametric den-sity estimation methods, it can be applied to arbitrary do-mains and to high-dimensional data, and is computationally tractable. This approach was earlier used in the covariance shift problem (Gretton et al., 2009), the two-sample prob-lem (Gretton et al., 2012a), and recently in (Zhang et al., 2013) for estimating class ratios.
 Let K be a universal kernel and H denote the RKHS in-duced by K . Let  X  : X 7 X  H denote the canonical feature map induced by the kernel on the RKHS. We ex-pect the test distribution P U ( x ) to match the distribution Q ( x ) = P y P D ( x | y )  X  y where  X  y denotes the unknown probability of class y instances in U  X  X  distribution and P
D ( x | y ) denotes the training distribution for class y . This holds because as mentioned earlier, we assume that Let  X   X  y and  X   X  u denote the true means of the feature vectors of the y -th class and unlabeled data respectively. That is, The true mean feature vector for the Q ( x ) distribution is then P y  X  X   X  y  X   X  y . To match Q ( x ) and P U ( x ) , the MMD approach minimizes the distance between the two means. This gives rise to the following optimization problem over the unknown  X  s: or, after rewriting using  X  0 = 1  X  P c y =1  X  y as where  X  A = [  X   X  1  X   X   X  0 ...  X   X  c  X   X   X  0 ] and  X  a = [  X  c denotes the new feasibility set {  X  y  X  0 , P c y =1  X  Let  X   X  denote the solution of the above, which is unique because of the following two assumptions: A 2 implies that there is an injection from the space of dis-tributions to the space of mean feature vectors. A 3 is a standard identifiability assumption on  X  without which the class ratio estimation problem is undefined and no algo-rithm can identify the true class ratio.
 However Equation 2 is impossible to solve as  X   X  y and  X  are unknowns. So, we approximate them by substituting sample means from sample U of P U ( x ) , and labeled sam-ple D of P D ( x | y ) calculated as Here, n u denotes the number of instances in U , and n denotes the number of instances labeled y in D . The em-pirical version of the MMD above is: where b A ( n ) = [ b  X  1 ( n 1 )  X  b  X  0 ( n 0 ) ... b  X  and this the MMD estimate b  X  ( n ) . In the paper we sometimes drop the ( n ) argument from b A, b  X  , We can apply the Kernel trick on this objective and rewrite it in kernel form as follows: where an entry yy 0 of [ b A &gt; b A ] = b  X  &gt; y b  X  y 0 b  X  0 b  X  0 and each b  X  [ b
A in terms of kernel. Since the objective is convex in  X  with only the simplex constraints, algorithms such as Mirror De-scent (Beck &amp; Teboulle, 2003) can solve it efficiently. We note here that this MMD formulation is equivalent to Equation 6 in (Zhang et al., 2013) when applied on discrete labels and with a few other minor modifications. However, we are aware of no prior work on the theoretical analysis of this MMD estimate. The analysis of MMD for the co-variance shift problem is very different (Yu &amp; Szepesvari, 2012; Gretton et al., 2009; Cortes et al., 2008) from ours because their objectives and assumptions are different. 2.1. Theoretical Analysis We next discuss results that show the closeness of b  X  ( n ) to the true  X   X  both in the finite sample case and asymptoti-cally. We already assumed that  X   X  is unique. For any formal comparison, b  X  ( n ) is also required to be unique. Our proof makes another mild assumption that makes the ob-jective strongly convex: These assumptions imply the uniqueness of b  X  ( n ) and  X  For example, A 5 holds whenever all labeled and unlabeled data points are distinct and K is universal.
 At this point we note that typical learning theory bounds are derived for knowing how close the objectives of (2) and (3) are, while we desire to know how close their solutions are. Also, (2) and (3) do not have a closed form solution Hence deriving finite sample closeness bounds as well as asymptotic convergence between solutions of (2) and (3) is interesting. To this end, we present the following theorem. Theorem 1. With the notation and strong convexity as-sumptions ( A 4, A 5) presented above, the following hold: 1. With probability at least 1  X   X  , k b  X  ( n )  X   X   X  k 2  X  2. n k b  X  ( n )  X   X   X  k 2 o p  X  0 where mineig ( M ) denotes 2 the minimum eigen value of M and R is the data-spread given by R  X  max x  X  X  k  X ( x ) k .
 The proof has two key steps. The first is a result, given be-low as Lemma 1, that helps to bound the distance between the optimal objectives of (2) and (3). The second is a re-sult, given below as Lemma 2, that bounds the closeness between the solutions of (2) and (3) in terms of closeness between their optimal objectives. We proceed by present-ing the two lemmas and defer their proofs to the end of the section. Lemma 1. with at least probability 1  X   X  .
 Lemma 2. The following two claims hold: 2. With probability 1  X   X  , Now from Lemma 1 and Lemma 2.1, we obtain the first result of the theorem. From Lemma 1, Lemma 2 and union bound, we obtain that with probability 1  X   X  , we have:  X  mineig (  X  A &gt;  X  A )  X  8 R 2 Note that the above bound holds as long as n  X  ( n 0 ,...,n c ,n u ) is high enough such that the denominator in the above expression is positive. Choosing such n is pos-sible because of the assumption A 4, mineig (  X  A &gt;  X  From the above bound, we obtain that n k b  X  ( n )  X   X   X  k 2 0 as n = ( n 0 ,...,n c ,n u )  X   X  . In the following we provide a sketch of proof for the lemmas and postpone the details to the supplementary.
 Proof of Lemma 1 First note that k b A ( n )  X   X   X  k b
A ( n ) b  X  ( n )  X  bound the RHS. Let f ( X 0 ,...,X c ,X u )  X  b A ( n )  X  b a ( n ) = P c y =0  X   X  y b  X  y ( n y )  X  b  X  U ( n u ) , where X note independent samples of size n y ,n u from P D ( x | y ) and P
U ( x ) respectively. Note that || f ( X o ,...,X c ,X u isfies the bounded difference property and applying McDi-armid X  X  inequality we get that with probability at least 1  X   X  , Next, we use E ( k f k ) 2  X  E ( k f k 2 ) and the claim E result.
 Proof of Lemma 2 Let h (  X  )  X k b A ( n )  X   X  h is quadratic in  X  , we then have: Moreover,  X  h ( b  X  ( n )) &gt; (  X   X  b  X  ( n ))  X  0 for any  X   X   X  This is because, b  X  ( n ) is the optimal solution of 3 and hence the gradient at this point  X  h ( b  X  ( n )) should lie in the normal cone of the feasibility set  X  c at b  X  ( n ) . Hence, h (  X   X  )  X  h ( b  X  ( n ))  X  (  X   X   X  b  X  ( n )) &gt; b A ( n ) This, together with assumption A 5, gives Lemma 2.1. To prove Lemma 2.2, we first prove in the supplementary that Let g ( X 0 ,...,X c ) = k  X  A &gt;  X  A  X  b A ( n ) &gt; to verify that E g = 0 . Also, g satisfies the bounded dif-ference property, hence by an application of McDiarmid X  X  inequality, we get that with probability 1  X   X  This completes the proof for Lemma 2.
 Theorem 1 is indeed interesting: first, it shows that our em-pirical estimate is statistically consistent. Second, it shows that the convergence rate of the squared error is at least see the error bound asymptotically going to zero with in-creasing training and test sizes. More importantly, in the finite regime, the theorem provides an upper-bound on the square error in terms of known and intuitive geometric for the two-class case, this bound says that the estimate is accurate whenever the distance between the sample means of feature vectors of the two classes are far apart and the overall data spread ( R ) is small. We illustrate the depen-dence graphically in Figure 1(b), where we plot the bounds for increasing S/R where S = the positive and negative means are sufficiently separated ( S/R  X  0 . 7 ), the error bounds are quite tight. In the fol-lowing section, we present a novel formulation for exploit-ing these bounds for kernel selection. Comparison with other bounds We are aware of no other work that bounds the error of class-ratio estimates via any other method. For regression and under covari-ance shift, (Yu &amp; Szepesvari, 2012) bounds the error of the mean y in an unlabeled set U estimated as P ( x ,y )  X  D  X  where  X   X  ( x ) is estimated using MMD for the covariance shift problem. Even though the setting is different, it is in-teresting to compare their convergence rates (with 0/1 val-ues of y ) with ours for two classes. Their convergence rate constant. Note that these rates are much slower than ours. Our theoretical analysis shows that the universal kernel used in the MMD estimator needs to be carefully chosen to obtain accurate class-ratios. Here, we present a novel con-vex formulation for learning such a kernel. We also present an efficient algorithm for solving this formulation. We assume that we are given a set of base kernels k ,...,k n nation, k w = P n k j =1 w j k j ,w j  X  0  X  j , that makes the esti-mated class-ratios close to the true class-ratios. Posing the problem of kernel learning as that of optimizing the kernel weights in the conic combination is a popular strategy in the kernel learning community (Lanckriet et al., 2002). We first use the bounds in Theorem 1 to increase the accuracy of class ratio estimates: we choose the ker-nel weights such that the upper-bound (5) is minimized. There are two quantities in this upper-bound that de-pend on the kernel weights: i) mineig ( b A ( n ) &gt; mineig ( P n k j =1 w j b A j ( n ) &gt; b A j ( n )) , where term computed using the j th base kernel, and ii) R = k w k 2 . The first term needs to be maximized and the second needs to be minimized. Since we wish to obtain a sparse set of kernel weights, leading to kernel selection, we minimize k w k 1 instead of k w k 2 .
 Our goal of reducing error in the MMD-estimate may not be adequately served by minimizing the upper bound alone because the bound is derived without making any distribu-tional assumptions (other than A 1 X  A 4) and may be overly pessimistic for the given data distribution. We therefore in-clude an empirical term in the objective that reduces the deviation between the estimated and true class-ratios over several datasets. We assume we have a set of datasets { U i } m i =1 (each U i is a set { x i 1 ,..., x in u } ) with known true class-ratios  X   X  i . We discuss later one way of obtain-ing such a dataset from the given labeled set D . Let mmd ( U i ,  X  , w ) denote the value of the MMD objective ( 4) at  X  when applied to U i with kernel k w . One way of minimizing the deviation between the estimate and the true class-ratios is by simply minimizing the deviation between the corresponding mmd () s . We cast this goal in the max-margin framework for structured learning (Tsochantaridis et al., 2005): we want w to be such that for each ( U i ,  X  mmd ( U i ,  X   X  i , w )  X  mmd ( U i ,  X  , w ) for all  X  far from  X  We rewrite mmd ( U i ,  X  , w ) as a linear function of w as: where feature map over the j th kernel calculated on sample U and b  X  j y is also specialized to kernel k j . Let E (  X  measure of the distance between  X   X  i and  X  . We want the margin w &gt; F ( U i ,  X   X  i )  X  w &gt; F ( U i ,  X  )  X  w be large when E (  X   X  i ,  X  ) is large.
 Combining the two bound-related objectives and the em-pirical term we obtain the following convex formulation for learning the kernel weights s.t. w &gt;  X  F i ( U i ,  X  )  X  E (  X   X  i ,  X  )  X   X  i :  X k  X   X   X  where  X  = {  X  i } m i =1 are the slack variables, C,B are the pa-rameters of the optimization problem and &gt; 0 is a user-given tolerance. The above is an instance of a convex pro-gram with infinitely many constraints and hence we resort to solving it using the cutting-plane algorithm (Tsochan-taridis et al., 2005). In Figure 2 we present an overview. The input to the algorithm is a labeled dataset D and can-didate kernels k 1 ,...,k n K . We create the ( U i ,  X   X  pairs required for our training by resampling from available labeled set D . To create m datasets we repeat this process m times: first, sample a value of  X   X  i uniformly randomly from the c + 1 dimensional simplex. If a prior distribution on the test set  X  s is known, one can use it in place of the uniform distribution. We did not assume any such priors in our experiments. Second, form U i as follows. Let T be the expected size of the test set. For each class y , use sampling with replacement to select T X   X  iy instances from 1: Input: D,k 1 ,...,k n k 2: ( U i ,  X   X  i ) = sampled sets from D with varying ratios  X  3: w = Initial parameter w j = 1 4: Initial constraint set S = {} 5: while no convergence do 6:  X  i,  X   X  = argmin i,  X  mmd ( U i ,  X  , w )  X  E (  X   X  8: Solve (11) restricted to S and obtain w ,  X  (Sec 3.2). 9: end while D y = { ( x ,y )  X  D } . We now discuss the two crucial opti-mization problems solved within the loop of the algorithm: (i) the selection of the most violating constraint (step 6), (ii) solving the MKL objective with the finite set of constraints (step 8). We elaborate on how each is solved. 3.1. Finding the most violating constraint For a given value of w we have to find the  X  corresponding to the most violating constraint for ( U i ,  X   X  i ) by solving This optimization problem differs from our original con-vex objective in only the additional E (  X   X  i ,  X  ) term. Since the  X  corresponds to parameters of a multinomial distri-bution, suitable choices for E (  X   X  i ,  X  ) are the L (  X   X  1 ) and KL-divergence both of which are convex in  X  . This makes objective (12) non-convex but since it is the difference of two convex functions, algorithms like CCCP (Yuille &amp; Rangarajan, 2003) can give a local opti-mum. However, for two very apt error measures: the L  X  distance and L 1 we are able to provide an optimal answer. Both of these can be expressed as a max over a small num-ber of linear functions of  X  . For example when E (  X   X  i vectors with exactly one of the c + 1 positions either +1 or -1 and zero for the rest. There are 2 c + 2 such vectors. Now, we can find the most violating constraint by solving these 2 c + 2 MMD-like convex objectives: Similarly, the L 1 distance can be expressed as tors with +1 and -1 over any of the c positions. 3.2. Solving the MKL objective with finite constraints This section focuses on solving (11) with the constraint-set restricted to S. We begin by noting that the formulation in this case can be written as a Semi-Definite Program (SDP): where I is the identity matrix of size c  X  c . As long as c and n k are not high, standard SDP solvers like Mosek or SeDuMi can solve (14). Otherwise, we re-write the above as the following non-differentiable convex problem: The above program can be solved using projected sub-gradient descent 4 . The sub-gradient expression for the first and third terms in the objective is easy, for the sec-h eigenvector corresponding to the maximum eigenvalue of Related work on kernel selection for MMD We are aware of no other work on learning kernels in the context of MMD-based class ratio estimation. For the two-sample test problem, (Gretton et al., 2012b) proposed a kernel learn-ing formulation that minimizes the asymptotic probability of hypothesizing two distributions as same, when they are different (Type II error) for a given bound of the Type I error. In contrast, the novelty of our formulation is that it includes both asymptotic terms and an empirical term that minimizes error under finite data settings. We compare our proposed estimator with several existing methods under various settings. First, we compare differ-ent methods under varying true class ratios. Second, we compare them under varying training sizes. Finally, we compare different kernel selection methods.
 Our chosen kernel family K F is a conic combination of univariate and multivariate Gaussian (RBF) kernels, a pop-ular family in various kernel learning literature (Gretton et al., 2012b; Vishwanathan et al., 2010). We first fix a kernel width (  X  ) based on training data as per (Zhang et al., 2013). For each feature, we have one univariate kernel with this width. We create several multivariate kernels based on bandwidths from this set: [2  X  6 2  X  5 ... 2 6 ]  X   X   X  d 2 d is the number of features.
 Methods: We compare the following methods.
 SMO -MKL : As a first baseline, we estimate class ratio by aggregating from per-instance predictions from a classi-fier. The classifier we used was SMO -MKL 6 (Vishwanathan et al., 2010) which trains a SVM but with the benefit of ker-nel selection from our kernel family K F .
 -DR : As a member of the direct method, we use the PE divergence based class ratio estimation method of (Plessis &amp; Sugiyama, 2012). We thank the authors for providing us with the Matlab code for this method 7 . We exclude results for the method in (Saerens et al., 2002), since it offered no benefit over the baseline SMO -MKL .
 MMD : This is the MMD based approach (Section 2) with a single best kernel chosen from our kernel family K through cross-validation. Recall that (Zhang et al., 2013) X  X  proposal is also a MMD method  X  the only difference is in how kernel parameters are chosen. We found that choosing a single kernel via cross-validation provided much higher accuracy than their formula of kernel width selection. MMD -MKL : Here, we used MMD on a kernel learned as in Section 3. The datasets i.e. { ( U i ,  X   X  i ) } pairs required for this training were sampled from the training data D using the method of Section 3 with m = 110 . The class means b  X  y were estimated from the entire labeled data. The pa-rameters C and B were fixed via cross-validation. Datasets: Table 1 summarizes the datasets we used. The first six are binary datasets comprising five of the six UCI datasets used in (Plessis &amp; Sugiyama, 2012) and a dataset based on YouTube comments that we created based on this 8 collection. The goal in the YouTube dataset is to estimate the fraction of comments that are spams on a YouTube video. The dataset was crawled by tracking 6407 popular YouTube videos over 77 days and comprises of 6,431,471 comments labeled spam or not. The feature set is a normal-ized TF-IDF vector over 1000 words + a comment length feature. The next three are multi-class datasets. Acous-tic is a dataset about classifying military vehicles from geophone recordings and is used in (Plessis &amp; Sugiyama, 2012). Botswana is a dataset about classifying spectral sig-natures into different land cover types and is from (Zhang et al., 2013) and Shuttle is a UCI dataset.
 We created a training set by sampling n y samples from each class y , and series of test sets with n u points each for a given ratio of classes  X   X  . The default values of n y ,n in Table 1. For the binary datasets, all experiments are with varying  X   X  and for the multi-class datasets, the default  X  the class prior skew in the entire labeled data. All numbers are averaged over 10 random seeds. We measure error as the L 1 distance between the true and estimated class ratios normalized by the number of classes.
 Varying class ratios: We perform these experiments on the six binary datasets by varying  X   X  0 as per the Figure 4, we plot the estimation error ( |  X   X  0  X  b  X  0 | ) against true fractions (  X   X  0 ) for the six binary datasets. The follow-ing conclusions can be drawn from the plots: SMO -MKL , the baseline that aggregates per-instance predictions, is in-deed very sensitive to the changes in test prior distribution. For many datasets, we see a  X  X owl X  shaped graph and usu-ally the minimum is when test prior is close to 0.5 which is equal to the training prior. The curves for the direct meth-ods ( PE -DR , MMD , MMD -MKL ) are much flatter showing that they are much less sensitive to the training class ratios. Except for YouTube, both the MMD-based methods pro-vide lower error than PE -DR . MMD -MKL is more accurate than MMD in most cases, and on YouTube we get upto a 33% drop in error.
 Running time: In this paper we skip a detailed comparison on running time, instead we report some specific timings: On YouTube, our largest dataset, MKL training via Matlab takes 20 minutes whereas deployment takes 5 minutes on a desktop class machine. On Botswana, the dataset with 14 classes, MKL training takes 6 minutes whereas deployment takes 0.3 minutes. In comparison PE -DR took 12 minutes on YouTube and 5 minutes on Botswana.
 Increasing training size: For these experiments we vary the value of n y (number of instances per class) in the range [10 30 50 70 90] keeping all other values fixed to their de-fault in Table 1. We selected the three multi-class datasets for these experiments. We observe smooth error reduction in MMD -MKL with increasing training size and consistent improvement over other methods. In contrast, SMO -MKL has inconsistent behavior. The best improvement we get is for the Acoustic dataset where MMD -MKL reduces error from 0.12 to 0.05 with 90 instances per class.
 Comparison of Kernel Selection: Our MKL attempts to jointly minimize the theoretical error bounds and empirical error. We evaluate if any one of them would be adequate by comparing our joint model ( MMD -MKL ) with (1) MKL -BOUND that minimizes only the Eigen and k w k 1 terms in (11), and (2) MKL -EMPIRICAL that drops the Eigen term. In Figure 5 we plot error of these methods averaged over various class ratios. We observe that the joint model pro-vides the highest overall accuracy. In this paper we address a real-world motivated problem of estimating the ratio of classes in an unlabeled set. We investigated the use of the maximum mean discrepancy (MMD) measure as a basis for estimating the class ratios. We present the first ever theoretical analysis of the esti-mator and show that the MMD estimator is consistent un-der mild conditions. We provide empirical error bounds in terms of intuitive quantities like class-separation and data-spread. Combining these bounds and empirical error we propose a novel convex formulation for kernel learning and also design an efficient cutting plane algorithm for solving it. We empirically compare our estimator with many exist-ing methods and obtain up to 60% reduction in error over the best existing method. Further, our method of kernel learning reduces plain MMD error by up to 40%.
 As part of future work, we wish to explore other families of kernel selection, for example directly optimizing the width of the RBF kernel as in (Gehler &amp; Nowozin, 2008; Ar-gyriou et al., 2006).
 We thank the reviewers for their useful comments. This work was partly supported by research grants from Yahoo! Research.
 Argyriou, Andreas, Hauser, Raphael, Micchelli,
Charles A., and Pontil, Massimiliano. A DC-programming algorithm for kernel selection. In ICML , pp. 41 X 48, 2006.
 Beck, Amir and Teboulle, Marc. Mirror descent and non-linear projected subgradient methods for convex opti-mization. Operations Research Letters , 31(3):167  X  175, 2003.
 Cl  X  emenc  X on, St  X  ephan, Vayatis, Nicolas, and Depecker, Ma-rine. AUC optimization and the two-sample problem. In NIPS , 2009.
 Cortes, Corinna and Mohri, Mehryar. AUC optimization vs. error rate minimization. In NIPS , 2004.
 Cortes, Corinna, Mohri, Mehryar, Riley, Michael, and Ros-tamizadeh, Afshin. Sample selection bias correction the-ory. In ALT , 2008.
 Elkan, Charles. The foundations of cost-sensitive learning. In IJCAI , 2001.
 Gehler, Peter Vincent and Nowozin, Sebastian. Infinite ker-nel learning. Technical report, Max Planck Institute for Biological Cybernetics, 2008.
 Gretton, Arthur, Smola, Alexander J., Huang, Jiayuan, Schmittfull, Marcel, Borgwardt, Karsten M., and
Sch  X  olkopf, Bernhard. Covariate shift and local learn-ing by distribution matching , pp. 131 X 160. MIT Press, Cambridge, MA, USA, 2 2009.
 Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Sch  X  olkopf, Bernhard, and Smola, Alexander J. A kernel two-sample test. Journal of Machine Learning Research , 13:723 X 773, 2012a.
 Gretton, Arthur, Sriperumbudur, Bharath K., Sejdinovic, Dino, Strathmann, Heiko, Balakrishnan, Sivaraman,
Pontil, Massimiliano, and Fukumizu, Kenji. Optimal kernel choice for large-scale two-sample tests. In NIPS , 2012b.
 Lanckriet, Gert, Cristianini, Nello, Bartlett, Peter, and
Ghaoui, Laurent El. Learning the kernel matrix with semi-definite programming. JMLR , 2002.
 Lin, Yi, Lee, Yoonkyung, and Wahba, Grace. Support vec-tor machines for classification in nonstandard situations. Machine Learning Journal , 2002.
 Plessis, Marthinus D. and Sugiyama, Masashi. Semi-supervised learning of class balance under class-prior change by distribution matching. In ICML , 2012.
 Saerens, Marco, Latinne, Patrice, and Decaestecker, Chris-tine. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural Computation , 2002.
 Selvaraj, Sathiya Keerthi, Bhar, Bigyan, Sellamanickam, Sundararajan, and Shevade, Shirish. Semi-supervised
SVMs for classification with unknown class proportions and a small labeled dataset. In CIKM , 2011.
 Sriperumbudur, Bharath K., Fukumizu, Kenji, Gretton, Arthur, Lanckriet, Gert R. G., and Sch  X  olkopf, Bernhard.
Kernel choice and classifiability for RKHS embeddings of probability distributions. In NIPS , pp. 1750 X 1758, 2009.
 Titterington, D. M. Minimum distance non-parametric es-timation of mixture proportions. Journal of the Royal
Statistical Society. Series B (Methodological) , 45(1):pp. 37 X 46, 1983.
 Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y.
Large margin methods for structured and interdependent output variables. JMLR , 6:1453 X 1484, 2005.
 Vishwanathan, S. V. N., Sun, Zhaonan, Theera-
Ampornpunt, Nawanol, and Varma, Manik. Multiple kernel learning and the SMO algorithm. In Advances in Neural Information Processing Systems , December 2010.
 Woodward, Wayne A., Parr, William C., Schucany,
William R., and Lindsey, Hildegard. A comparison of minimum distance and maximum likelihood estimation of a mixture proportion. Journal of the American Statis-tical Association , 79(387):590 X 598, 1984.
 Yu, Yaoliang and Szepesvari, Csaba. Analysis of kernel mean matching under covariate shift. In ICML , 2012. Yuille, A. L. and Rangarajan, Anand. The concave-convex procedure. Neural Computation , 15(4):915936, 2003. Zhang, Kun, Sch  X  olkopf, Bernhard, Muandet, Krikamol, and Wang, Zhikun. Domain adaptation under target and
