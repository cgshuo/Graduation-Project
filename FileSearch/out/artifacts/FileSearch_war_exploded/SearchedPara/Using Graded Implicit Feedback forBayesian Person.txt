 In many application domains of recommender systems, ex-plicit rating information is sparse or non-existent. The pref-erences of the current user have therefore to be approxi-mated by interpreting his or her behavior, i.e., the implicit user feedback. In the literature, a number of algorithm pro-posals have been made that rely solely on such implicit feed-back, among them Bayesian Personalized Ranking (BPR).
In the BPR approach, pairwise comparisons between the items are made in the training phase and an item i is con-sidered to be preferred over item j if the user interacted in some form with i but not with j . In real-world applications, however, implicit feedback is not necessarily limited to such binary decisions as there are, e.g., different types of user ac-tions like item views, cart or purchase actions and there can exist several actions for an item over time.

In this paper we show how BPR can be extended to deal with such more fine-granular, graded preference relations. An empirical analysis shows that this extension can help to measurably increase the predictive accuracy of BPR on realistic e-commerce datasets.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Recommender Systems; Implicit Feedback; Evaluation
Most of the published Collaborative Filtering (CF) recom-mendation approaches aim to identify patterns of user pref-erences within a set of explicit item ratings provided by a larger user community. In practice, however, this form of ex-plicit feedback from the users in many application domains is very sparse, limited to dominantly positive or positive-only statements, or even non-existent.

Correspondingly, a number of proposals have been made in the literature to exploit implicit user feedback in the rec-ommendation process, e.g., [3, 7, 8, 10]. In these cases, a user X  X  true preferences are estimated by interpreting his or her behavior. On an online shop, for example, one will typi-cally interpret user actions like an item purchase or viewing an item for some time as a sign of interest in the item.
Bayesian Personalized Ranking (BPR) [10] is a compa-rably recent CF method designed to deal with implicit-only feedback. One particularity of BPR when compared, e.g., to the highly accurate SVD++-based method [3] is that it di-rectly aims to optimize a ranking criterion, whereas SVD++ primarily aims to minimize the prediction error. Therefore, BPR can be considered to fall into the class of  X  X earning-to-rank X  methods [2] which  X  due to their design  X  are typically able to outperform methods designed for error-minimization on rank measures like precision, recall or nDCG.

In the BPR approach, the input consists of a user-item rating matrix, which however only captures positive inter-actions (unary feedback), i.e., there is a  X  X ne X  whenever a user i has interacted with item j . At its core, BPR uses the pairwise preferences derived from the implicit ratings to op-timize an AUC-like ranking criterion and Rendle et al. show how this optimization can be applied to two typical classes of recommendation models (kNN and matrix factorization).
One limitation of the basic BPR model is that it only sup-ports discrete, unary feedback. In practice, however, some types of user interactions might be considered stronger in-dicators of interest than others. A purchase action might be stronger than a view action; multiple clicks on an item might be more indicative than a single click; finally, a recent view action might be more relevant than one in the past. In this work, we propose an extension to BPR (called BPR++ ), which is designed to take various forms of such graded implicit feedback into account. The main idea is to first derive additional pairwise preferences from the data using the available  X  X mportance X  information and  X  in the op-timization phase  X  bias the optimization procedure to draw a certain amount of samples from these additionally available data points. In the next section, we describe the approach in more detail before we present results of an experimental evaluation with different datasets.
In BPR, the first task is to derive pairwise item prefer-ences from the unary feedback. The set of available items is denoted as I . For each user u , we are given a set of items I ` u , containing those items of I for which a (positive) implicit feedback action was observed. User-specific item preferences are triples of the form p u,i,j q , which express the fact that user u prefers item i over item j . The set of derived preference relations D S , i.e., the training data used for optimization later on, is defined as follows.

Using this approach, we can encode single, positive-only statements like non-repeated item purchases or  X  X ike X  state-ments on social platforms. The user actions that can be used to assess preference profiles can however be richer in reality and may include, for example, the following: 1. In the logs of online shops, we typically can find view, add-to-cart, add-to-wishlist, or purchase actions, which can correspond to interest indicators of different strengths. In an even more fine-grained setting, the individual viewing times could be taken into account. 2. Recency of events and temporal aspects could be consid-ered as carrying information about (recent) user preferences. 3. There are specific relationships between the users, e.g., in the form of social network connections [5], or between items that share similar characteristics.

In this paper, we therefore propose a generic extension of BPR regarding the way the set D S is derived. In the original BPR formulation, a preference can only be derived if there is an implicit rating for i and none of j (e.g., be-cause the user only viewed item i ). If, for example, the user viewed i three times and j one time, nothing can be said about the preference. Since the number of views can be an interest indicator, we introduce a preference weight function pweight p u,i q for user actions instead of the binary decision of [10].

We assume that pweight p u,i q returns a positive number if an interaction between u and i exists, and 0 otherwise. A preference tuple p u,i,j q is therefore inferred whenever pweight p u,i q  X  pweight p u,j q holds. Generally, the pweight function can encode arbitrary types of information, includ-ing, e.g., the time or recency aspects as mentioned above. The extended set of preference relations D `` S for each user u is defined as follows: D S :  X  tp u,i,j q| pweight p u,i q  X  pweight p u,j q ,i P I,j P I u
Consider the example in Figure 1. Let us assume that we have information about how often a user viewed an item as well as the time point of the latest view action for each item. User u 1 has both viewed item i 1 and item i 2 . In the original BPR formulation, no preference could be derived. Here, however we could design a function pweight that re-turns a higher value for i 1 because of the higher number of interactions. Likewise, the function could return a slightly higher preference value for item i 1 also for user u 2 because of the more recent view event. A weighted combination of these aspects inside pweight is also possible. For user u both the original and adapted BPR version would derive a preference for i 1 . Generally, how pweight is defined de-pends on the application domain and the characteristics of the available data. Meta-data features or social network in-formation could for example be used as done, e.g., in [5] or [7]. The chosen pweight function in any case however has to be carefully designed and thoroughly evaluated.

In the learning phase of BPR , Rendle et al. proposed to use a random bootstrap stochastic gradient descent ap-proach to choose triples from D S to learn the model, since using all O p| U |  X | I | 2 q triples is in general not feasible. When using the same random sampling in the BPR++ approach, most of the time only triples from the original BPR for-mulation would however be considered. The reason is that the number of additional data tuples p u,i,j q in D `` S can be fairly small when compared to the number of triples in D S because additional tuples are only generated when the user has interacted both with the items i and j .

We therefore extend the training algorithm of BPR++ with a parameter  X  that expresses the probability of taking a sam-ple from the additional tuples in D `` S . Finding a suitable value for  X  is crucial for the effectiveness of the approach and its value has to be determined empirically for a given data set. The modification to the learning procedure of [10] is shown in lines 4-6 in Figure 1. 1: procedure LearnBPR++( D S , X ) 2: initialize  X  3: repeat 4: r = random(0,1) 5: if r  X   X  ^ D `` S z D S  X  H then 6: draw ( u , i , j ) from D `` S z D S 7: else 8: draw ( u , i , j ) from D S 9: end if 11: until convergence 12: return  X   X  13: end procedure Figure 1: Learning algorithm with biased sampling.
To determine to which extent the consideration of different types of feedback helps to improve the prediction accuracy of BPR, we conducted experiments on different data sets and, correspondingly, different types of additional information. The accuracy was measured in terms of precision and recall We used two datasets consisting of navigation log data from real online shops as well as one of the MovieLens datasets. Table 2 shows the basic statistics of the datasets.
Zalando: A dataset from Zalando 2 , a large online retailer for fashion products, that has been sampled from log data in
Additional MRR measurements were in line with these re-sults for all datasets and are therefore not reported here. http://www.zalando.de such a way that no conclusions on visitor data or true busi-ness figures of the company can be drawn. The logged and session-numbered interactions have different types (view, pur-chase, wish, put-in-cart), where the majority of the events are item views. We created a subset for which there are at least 10 interactions per user and item.

Tmall: A similar but smaller anonymized dataset from a shopping portal 3 containing user interactions of different types. For each of the actions, a time stamp is available. MovieLens: We created a subset of  X  X eavy users X  of Movie-Lens using the 10 million ratings dataset as a basis.
Depending on the available data, we used the following comparably simple variants of BPR++ and varied the inter-pretation of pweight p u,i q correspondingly.
 BPR++(T) : Items with which the user interacted more re-cently are considered to be more relevant. pweight p u,i q corresponds to the time stamp or session number of the last interaction of user u with item i .
 BPR++(N) : Items with which the user interacted more often are considered to be preferred. pweight p u,i q corresponds to the number of interactions of user u with item i .

The time-based and interaction-count based versions were tested for the shop datasets. For the MovieLens dataset, we used the rating values as a preference indicator.
 BPR++(R) : Higher rated items are more relevant. pweight p u,i q corresponds to the rating of user u for item i .
 In our evaluation, we compared BPR++ with the original BPR implementation 4 , FunkSVD 5 with 50 latent features, and item-to-item collaborative filtering with the cosine sim-ilarity measure ( Item-KNN ). The number of optimization rounds for the BPR-based methods was varied in the tests to analyze the convergence and possible overfitting behavior of BPR++ . We have empirically evaluated different values for  X  and determined a fixed value  X   X  0 . 75 in all our experi-ments as it led to overall high results and enables compara-bility between the different pweight p u,i q functions 6 .
For the online shop datasets we created one time-based 80/20 train-test split. Since cross-validation is not possible due to the unique time-based ordering, we randomly created ten 90% sub-samples of the splits and repeated the exper-iments to factor out random effects. For the MovieLens dataset we used standard 5-fold cross-validation.
Tmall Rec. Prize 2014 &amp; TianChi Open Data Project
We used matrix factorization, uniform sampling, 100 latent features,  X   X  0 . 05,  X  W  X   X  H `  X  0 . 0025,  X  H  X   X  0 . 00025 http://sifter.org/  X simon/journal/20061211.html
Optimizing  X  for the different datasets and pweight p u,i q functions is beyond the scope of this paper.

Precision@10 and Recall@10 were used as accuracy mea-sures. An item was considered relevant when it was actu-ally purchased (shop data) or rated higher than the user X  X  average (MovieLens). Items that were previously bought or rated were not recommended for the Zalando and movie dataset. Since in the Tmall scenario rather families of items than individual items are recommended, we allowed  X  X e-peated recommendations X  for this dataset.
Zalando: Table 3 shows precision and recall for the Za-lando dataset after n optimization rounds 7 . The time-based variant BPR++(T) helps to increase both precision and recall which indicates that the recency of the interactions is rele-vant in the domain. The BPR++(N) variant in contrast led to results that were worse than the original BPR method, which is possibly caused by recency effects. Since BPR++(N) ignores the interaction time, the focus on items that are no longer relevant for the user might be too strong. Systematic tuning of  X  might help to mitigate this effect.
Tmall: The results for the Tmall shown in Table 4 corrob-orate the findings made for the Zalando dataset. Taking re-cency of events into account ( BPR++(T) ) helps to improve the accuracy both in terms of precision and recall. Considering the number of interactions for each item as preference indi-cators ( BPR++(N) ) leads to faster convergence  X  which can be considered a highly desirable characteristic for large datasets  X  but not to better overall results. Again, Funk-SVD , item-kNN , and popularity-based ranking performed worse.
MovieLens: The results for the MovieLens data are shown in Table 5. The rating-based variant of BPR++ performed best already after only 10 iterations. With an increasing number of iterations, there is however a tendency of overfit-ting 8 .
Results for Funk-SVD , item-kNN , and popularity-based ranking are omitted, since they never exceeded the value 0.001 for precision and recall.
The results obtained for the original BPR are compara-ble to the results reported at http://mymedialite.net/ examples/item_recommendation_datasets.html . item-kNN and popularity-based ranking led to lower results than FunkSVD and are therefore again omitted here. Table 5: Precision@10/Recall@10 for MovieLens
We made additional experiments in which we used the non-filtered MovieLens dataset. In these scenarios, the re-sults of BPR++(R) were however worse than when the original BPR method was used. Overall, our results show that ad-ditional preference indicators and graded forms of implicit feedback can help to increase the prediction accuracy of BPR 9 . The choice of the additional preference signals how-ever depends on the domain. Using frequency information as an indicator for preference strength did for example not lead to better results in the Zalando scenario but actually led to a deterioration of the results.

So far, we have only considered one additional factor at a time to derive pairwise item preferences. On principle, mul-tiple types of information could be incorporated in parallel, which should help us to achieve a higher density of the pref-erence matrix. One particular question here is how to deal with situations when the preference statements that were de-rived from different information sources are conflicting. The design and evaluation of such hybrid methods and the anal-ysis of factors like diversity and potential popularity-biases are part of our ongoing work.
BPR falls into the category of  X  X ne-class collaborative filtering X  approaches and is able to make inferences from positive-only feedback (interpreted user actions). At the same time, with the AUC-like optimization criterion, BPR can be considered as a  X  X earning-to-rank X  approach. An-other learning-to-rang technique is xCLiMF [11] which uses a graded relevance scale and optimizes a rank criterion, in that case the Expected Reciprocal Rank.

In recent years, a number of techniques have been de-veloped to better deal with implicit user feedback, among them the SVD++ method, which integrates both implicit and explicit feedback in a matrix factorization model [3]. In [7], Manzato shows how to integrate additional information about item content into SVD++. In [4], the SVD++ model is extended to be able to deal with temporal dynamics. In our work, we have shown how time and recency information can be incorporated into the BPR method. In contrast to other time-aware CF methods, considering temporal aspects is however only one possible way of incorporating graded relevance feedback. Using interaction counts on items was explored as another possible form of graded feedback in our paper. This is partially similar to the probabilistic method presented in [6], which can use interaction frequency infor-mation when learning a preference profile.

Extensions to the basic BPR scheme have for example been proposed in [1], [5], or [9]. In [1], a strategy that weights
The differences of the best BPR++ results and the BPR results were statistically significant (p &lt; 0.01) for all datasets. the impact of negative items based their global popularity is introduced. In [5], the authors integrate additional informa-tion  X  in that case social network information  X  into BPR. Different to our work, their method directly influences the weight learning approach. In [9], an alternative sampling strategy was proposed that aims to statically or adaptively determine the most informative pairs.
In this work we propose to incorporate implicit feedback signals at a more fine-grained level in the personalized rank-ing process. The approach therefore allows us to differenti-ate between arbitrary types of feedback, e.g., take time and recency aspects into account. An experimental evaluation shows that for the considered domains the retrieval accu-racy can be improved and faster convergence can be achieved when compared to the original BPR approach. In our fu-ture work, we plan to explore additional ways of combining the different types of user feedback in a hybrid approach, further investigate temporal effects and also look at other characteristics of the generated recommendation lists such as diversity and popularity biases. [1] Z. Gantner, L. Drumond, C. Freudenthaler, and [2] A. Karatzoglou, L. Baltrunas, and Y. Shi. Learning to [3] Y. Koren. Factorization meets the neighborhood: a [4] Y. Koren. Collaborative filtering with temporal [5] A. Krohn-Grimberghe, L. Drumond, C. Freudenthaler, [6] H. Ma, C. Liu, I. King, and M. R. Lyu. Probabilistic [7] M. G. Manzato. gSVD++: Supporting implicit [8] D. Oard and J. Kim. Implicit feedback for [9] S. Rendle and C. Freudenthaler. Improving pairwise [10] S. Rendle, C. Freudenthaler, Z. Gantner, and [11] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, and
