 corresponding labels y = ( y d 10 prohibitively expensive, both at training and testing time .
 sparsity , eases the burden of large-scale multi-label learning. of have large support ( e.g. if there is little distinction between several labels). is k -sparse. This is the basis of our approach. solution to predict the final label.
 averaged over the data distribution.
 The main contributions of this work are: some degree of efficiency [11].
 requires training and evaluating exponentially (in d ) fewer predictors. predictor F : X  X  Y with low expected  X  2 errors over all labels) using a set of n training data { ( x non-zero entries. A vector is k -sparse if it has at most k non-zero entries. 3.1 Learning to Predict Compressed Labels A these compressed labels. Since A is linear, we simply represent A  X  R m  X  d as a matrix. Specifically, given a sample { ( x learn a predictor H of E [ Ay | x ] with the objective of minimizing the  X  2 3.2 Predicting Sparse Labels algorithms typically aim to find a sparse vector y such that Ay closely approximates h . ( A h  X  R m , the algorithm R ( k, A, h ) returns an f ( k ) -sparse vector b y satisfying for all y  X  R d . The function f is the output sparsity of R and the constants C factors .
 beforehand.
 We make a few additional remarks on the definition. Concrete examples of valid reconstruction algorithms (alo ng with the associated A given in the next section. functions and reconstruction algorithms in the following s ubsections.
Algorithm 1 Training algorithm parameters sparsity level k , compression input training data S  X  X   X  R d output regressors H = [ h 4.1 Compression Functions isometry property .
 the following [16, 17]. d employing our reduction.
 O (1 /k ) , where ( A ) defined as for ( k,  X  ) -RIP, but the dependence on d is still small. 4.2 Reconstruction Algorithms is valid with respect to the sparsity error given by where y nitude) coefficients of y ).
 sufficient condition for any algorithm to be valid for RIP mat rices. Algorithm 3 Prediction algorithm with R = OMP parameters sparsity level k , compression function A = [ a input regressors H = [ h h  X  [ h 1 ( x ) , . . . , h m ( x )]  X  (predict compressed label vector) b y  X  ~ 0 , J  X  X  X  , r  X  h for i = 1 , . . . , 2 k do end for output b y Theorem 1. Let A b y = R ( k, A, h ) satisfying then it is a valid reconstruction algorithm for A regret factors C Proofs are deferred to Appendix B.
 of columns in each iteration.
 parent from the cited references. For OMP, we give the follow ing guarantee. 0 . 1 /k and has output sparsity f ( k ) = 2 k .  X  algorithms. Basis Pursuit (BP) [14] and its variants are based on finding t he minimum  X  the user to supply the amount of measurement error k Ay  X  h k or multi-stage variants may be valid [21]. 5.1 General Robustness Guarantees valid reconstruction algorithm and linearity of expectati on.
 Theorem 3 (Regret Transform) . Let R be a valid reconstruction algorithm for {A Pick any k  X  N , A  X  A R ( k, A, ) and H , i.e. F ( x ) = R ( k, A, H ( x )) . Then with the reconstruction algorithm meeting the formal speci fications described above. Codes (SECOC) [13], we need to relate E max y  X  X  ( Ay ) i  X  min y ( Ay ) i between induced labels: In can be tuned to yield E we have k Ay k induces exponentially (in d ) fewer subproblems than SECOC.
 suppose E [ y | x ] has small  X  will decrease polynomially in k  X  m/ log d . 5.2 Linear Prediction E [ Ay | x ] : behaved A .
 probability (over the choice of A ), output. options. 6.1 Data players ultimately provide word tags for a diverse set of web images. data for training and half for testing.
 of patches that fall in each cell.
 social bookmarking service in which users assign descripti ve textual tags to web pages. labels on average. Again, we used half the data for training a nd half for testing. combination of frequency thresholding and  X  2 feature ranking. See [11] for details. 6.2 Output Sparsity  X  error of b p and its best k -sparse approximation  X  ( k, b p ( x )) = P d b p (1) ( x )  X  . . .  X  b p ( d ) ( x ) Examining E ( e.g. vectors with small  X  will have to contend with the sparsity error of the target. 6.3 Procedure image data and with  X  not attempt any parameter tuning. these yielded similar but uniformly worse results.
 supp( y ) | /k . Actually, for k  X  6 , we used b y 2 k instead of b y 10 . columns of A . Note that CD is not a valid reconstruction algorithm when m &lt; d . 6.4 Results K least k correct labels.
 squared-error criterion, except when m = 100 . When A has few rows, (1) A  X  A correlated columns and thus perform better in this regime.
 columns need not correspond to accurate label coordinates.
 bustness of the reconstruction algorithms is a key factor in their success. Acknowledgments was completed while the first author was an intern at TTI-C in 2 008. References
