 As clustering methods are often sensitive to parameter tun-ing, obtaining stability in clustering results is an important task. In this work, we aim at improving clustering stability by attempting to diminish the influence of algorithmic incon-sistencies and enhance the signal that comes from the data. We propose a mechanism that takes m clusterings as input and outputs m clusterings of comparable quality, which are in higher agreement with each other. We call our method the Clustering Agreement Process (CAP) . To preserve the clus-tering quality, CAP uses the same optimization procedure as used in clustering. In particular, we study the stability problem of randomized clustering methods (which usually produce different results at each run). We focus on methods that are based on inference in a combinatorial Markov Ran-dom Field (or Comraf , for short) of a simple topology. We instantiate CAP as inference within a more complex, bipar-tite Comraf. We test the resulting system on four datasets, three of which are medium-sized text collections, while the fourth is a large-scale user/movie dataset. First, in all the four cases, our system significantly improves the clustering stability measured in terms of the macro-averaged Jaccard index. Second, in all the four cases our system managed to significantly improve clustering quality as well, achieving the state-of-the-art results. Third, our system significantly improves stability of consensus clustering built on top of the randomized clustering solutions.
 I.5 [ Pattern Recognition ]: Clustering Algorithms Clustering stability, combinatorial MRF, Comraf
Data mining practitioners are often challenged with vaguely stated requests such as:  X  X e have a large data collection X  analyze it for us X . Many data analysis approaches have been developed, among which data clustering X  X artitioning the data collection to semantically coherent groups X  X s one of the most prominent approaches. A large variety of cluster-ing methods are available, each of which has its own advan-tages and drawbacks. The major question that arises after a clustering method has been applied to a data collection is why the data was clustered this way. A data instance x was placed together with a data instance x 2  X  X oes this mean that they are inherently similar? Or they are not so similar but all the other options appear worse? Or our clustering method just made a mistake by associating x 1 with x 2 ?
This question becomes even more acute when taking into account the fact that different clustering methods usually obtain different results on the same data. Even the same method can obtain different results depending on a particu-lar setting of its parameters. Those clustering methods that base on non-convex optimization often employ randomiza-tion to escape local optima, which implies that, with high probability, their results are different from run to run. Hav-ing dozens or hundreds of different clusterings of the same data does not add much clarity to our data analysis task.
Nevertheless, having more than one clustering can be help-ful. Some data instances tend to come together with some others; some instances tend to float around. If all the avail-able clustering methods assign x 1 to the same cluster with x , then this can be a good indicator that x 1 and x 2 have something profound in common. Unfortunately, such sig-nals are usually weak: it is a relatively rare case when one data instance always goes with another. Some clusterings may capture a few patterns like that, whereas others would capture different patterns. An agreement amongst all those clusterings would be much desired.

A somewhat standard approach to achieving an agreement of clusterings is called Cluster Ensembles [22] or, alterna-tively, Consensus Clustering (see, e.g., [13]), which embod-ies a wide variety of averaging schema for the original set of clusterings. The result of a consensus clustering proce-dure is a single clustering that is an optimal combination (in one sense or another) of the original clusterings. We argue that the consensus clustering approach provides only partial solution to the agreement problem: although it does achieve an agreement about a particular clustering, it is still not clear how stable this agreement is with respect to tun-ing the consensus method X  X  parameters. Also, will the same consensus method lead to a similar consensus when applied to different clusterings of the same data? Will a different consensus method lead to a substantially different consen-sus? Moreover, since a consensus method aims at obtaining a single clustering, it cannot provide answers to questions such as: which data instances compose a stable component (i.e. they are most often X  X r always X  X lustered together)? Which stable components tend to be clustered with which? Which data instances do not have a strong pattern of asso-ciation with the others?
In this work, we introduce the Clustering Agreement Pro-cess (CAP) that provides means for improving stability of a clustering ensemble, without shrinking the space down to a single clustering solution, and without compromising the clustering ensemble X  X  quality. In a nutshell, CAP can be described as follows. It operates in the space of all possi-ble partitions of a given data collection. Initialized with the original clusterings, it employs local search that makes the clusterings  X  X et closer X  (i.e. become more similar) to each other. Definitely enough, an arbitrarily chosen local search technique may heavily hurt the quality of the original clus-terings. As a matter of fact, for any pair of clusterings x and x c 2 , there exists a sequence of local updates (i.e. moves of one data instance from one cluster to another), which transfers x c 1 into x c 2 . We overcome this problem by applying the type of local search similar to the one used in clustering for constructing semantically meaningful solutions.
Our underlying clustering method is essentially bi-modal  X  it allows to simultaneously construct two clusterings: one of data instances, another one of their features. We use the term data modality to address a set of data instances, as well as the set of their features. 1 Bi-modal (and, generally, multi-modal) clustering methods are commonly believed to exceed the ordinary, uni-modal clustering methods in the quality of clustering multidimensional data (for a discus-sion, see [2]). A Combinatorial Markov Random Field (or Comraf , see Section 3) has proven itself to be a powerful model for multi-modal clustering [4, 3]. In original Com-raf models, clusterings of data modalities are organized in a Markov Random Field (MRF) with one node per modality, and edges representing interactions between the clusterings. A multi-modal clustering task is subsequently expressed in terms of the Most Probable Explanation (MPE) estimation in the constructed MRF. At a node level, the inference pro-cedure reduces to a local search routine that starts with some initial clustering and stops at a clustering for which the MPE objective reaches its local maximum, representing an agreement between this clustering and its neighbors.
In previously proposed Comraf models, it was not useful to have more than one node per modality. In this paper, however, we show that the clustering agreement process in-troduced above can be easily embedded into the Comraf
From here on, we will intentionally refrain from using the term  X  X eature X , which is somewhat misleading in our con-text. For example, in a document collection, documents are commonly considered X  X ata instances X , while their words are considered  X  X eatures X . However, if the task is to build a meaningful word clustering , then words become  X  X ata in-stances X , while the identities of documents in which the words appear become  X  X eatures X . Accepting the Bag-Of-Words assumption, a document collection can be repre-sented as a contingency table with documents as rows and words as columns. In a contingency table, rows and columns are equivalent up to transposition. framework, with as many nodes as the number of the orig-inal clusterings. We assume that m bi-modal clustering al-gorithms were initially applied to a given data collection, which led to constructing m clusterings of modality X and m clusterings of modality Y . We build a Comraf model with m nodes per one modality and m nodes per another. Each node that corresponds to X is connected to each node that corresponds to Y and vice versa, resulting in an MRF of bipartite topology. An MPE inference is applied to the constructed MRF, which at each node boils down to a local search procedure similar to the one previously proposed. It is initialized with one of the pre-constructed clusterings and aims at finding a clustering that maximizes the agreement with its neighbors in the MRF. Given the bipartite topology, all m new clusterings of X maximize agreement with all m clusterings of Y , which results in improving an agreement between themselves (all details are provided in Section 4).
Note that the algorithms that originally clustered the data do not play any role in the agreement process proposed above, such that no pre-conditions are imposed on them, apart from being bi-modal. However, the problem of choos-ing a clustering ensemble is not in the focus of our current study, and therefore we decided to construct original clus-terings using the same bi-modal Comraf method that under-lies the agreement process. This clustering method employs randomization such that, with high probability, it produces different results from run to run, and thus is suitable for our task. To a large extent, the proposed agreement process al-lows the original clustering method to escape the local max-imum it is stuck in, and continue the clustering optimization process while biasing towards other solutions. This obser-vation brings us to a hypothesis that the proposed process may significantly improve not only the clustering stability, but also the clustering quality (we confirm this in Section 6).
Potentially, such an agreement process can lead to a single solution, i.e. all the local searches that started from the orig-inal clusterings will meet together. Practically, however, this would be quite impossible for the following reasons: (a) the search space is prohibitively large such that finding a com-mon solution of high quality is computationally hard; (b) our non-convex optimization procedure is unable to break ties: if a data instance x 1 equally probably belongs to two clusters, it can be found in either of them. Building m sim-ilar, but not equal clustering solutions opens the door to extensive data analysis. Being similar, the clusterings de-fine relatively large stable components, each of which has a pattern of association with the others. Such a pattern can be easily converted into a distribution over the clusters, which is likely to be peaky enough and therefore easy for comprehension and interpretation. A smoother distribution will then determine a floater. To a certain extent, the pro-posed mechanism can be used for softening a set of hard clustering solutions (i.e. partitions), with a variety of prac-tical applications in which soft clusters are needed.
The proposed agreement process is likely to identify a dense region in the space of possible clusterings. If the user is interested in a single clustering instead of an ensemble, a consensus clustering mechanism can be applied to the con-structed clusterings. Such a consensus method will most probably explore the identified dense region, which would lead to a potentially more stable and therefore more reliable solution compared to the consensus of the original cluster-ings. We justify this empirically in Section 6.1.
An extensive body of work has been published on finding a consensus within a clustering ensemble. As discussed above, we are solving an essentially different problem of improving clustering stability, however consensus clustering is a related task that needs to be reviewed. Cluster ensembles were pro-posed and extensively studied by Strehl and Ghosh [22], who used an information-theoretic framework for discovering a consensus of clusterings. Our MPE inference in the Comraf model also has information-theoretic nature, with its roots in information-theoretic co-clustering [10] and multivariate information bottleneck [21]. Note that Strehl and Ghosh X  X  method is uni-modal while we apply a multi-modal tech-nique. A consensus of bi-modal clustering methods was first investigated by Badea [1]. Fern and Brodley [11] proposed to solve the cluster ensemble problem using bipartite graph partitioning. Our Comraf model also has a bipartite topol-ogy, however our method is not graph-theoretic. Recently, Punera and Ghosh [18] proposed a hard clustering consensus of soft clusterings. To some extent, we address the opposite problem of softening an ensemble of hard clusterings.
In the previously published literature, we were unable to find any work on improving clustering stability, however many works deal with assessing and measuring stability of clustering methods. First works in the area were published in the early 1980 X  X  [19] which were based on an even earlier work by Rand [20] who developed criteria for clustering eval-uation. In the last decade, an effort was done on assessing stability of deterministic clustering algorithms using either Rand X  X  criteria [7], resampling [16], or classification [15], for the tasks of cluster validation and model selection. 2 Later, Ben-David et al. [6] formalized the problem and proposed theoretic criteria for stability of clustering methods, under a few assumptions. Presumably the most relevant work to our current study was done by Kuncheva and Vetrov [14] who evaluated stability of cluster ensembles under random initialization. They proposed to measure an ensemble X  X  sta-bility with a pairwise adjusted Rand index , averaged over all the clustering pairs. We adopt their averaging idea, but instead of using the adjusted Rand index we use the Jaccard index [12] which is easier for interpretation (see Section 5.2).
In this section we will provide all the necessary defini-tions and notation. As discussed in Section 1, we are given m clusterings of a data modality X and m clusterings of a data modality Y , and our goal is to search the solution space around those clusterings in order to come up with new clus-terings that maximize an agreement objective function. To formalize the space of all possible clusterings in which the local search is performed, we define a discrete distribution P over the entire space, such as each possible clustering is as-signed a certain probability mass. We define a combinatorial random variable that is distributed according to P :
Definition 3.1. A combinatorial random variable (or com-binatorial r.v. ) X c is a discrete random variable defined over a combinatorial set X c , which is a finite set whose size is exponential with respect to another finite set X , i.e. log |X c | = O ( |X| ) .
One major model selection problem extensively covered in the literature is the choice of the number of clusters k . In this work, however, we assume that k is given to us. Figure 1: (left) Comraf model used for the original clustering; (right) bipartite Comraf for the cluster-ing agreement process.

For our particular case, the combinatorial set X c is a set of all possible clusterings of the data modality X , and the event of picking one clustering x c 1 out of the set X c has probability P ( X c = x c 1 ). From the theoretical point of view, X c discrete random variable with a finite support. In practice, however, the set X c is extremely large, such that the distri-bution P ( X c ) cannot be explicitly specified. Nevertheless, given two values x c 1 and x c 2 , we can determine which one is more probable than the other. An interaction pattern of a combinatorial random variable with other random variables can be modeled through a combinatorial MRF:
Definition 3.2. A combinatorial Markov Random Field (Comraf) is a Markov Random Field, at least one node of which is a combinatorial random variable.
 From here on, we will discuss only Comraf models each node of which is a combinatorial r.v. Generalizing P into a joint distribution over all combinatorial r.v. X  X  in the Comraf, let us write P down in the form of the Gibbs distribution [8]: where x c is a vector of all combinatorial r.v. X  X  in the Com-raf; C is a clique in the Comraf graph; x c C are combinatorial r.v. X  X  participating in the clique C ; f C is a real-valued func-tion called a log-potential ; and Z f is a normalization factor called a partition function . If we fix the log-potentials f each clique, the partition function Z f becomes a constant. Thus, the Most Probable Explanation (MPE) inference in the Comraf is defined as which now solely depends on the choice of log-potentials. Both for simplicity and for feasibility of our inference algo-rithms, let us consider only cliques of size 2, i.e. graph edges. Denoting by E the set of Comraf X  X  edges, Equation (1) can then be rewritten as:
Let us first describe how the underlying clustering method works. For the task of bi-modal clustering, a very sim-ple Comraf model can be built which contains two nodes Figure 2: The clustering agreement process. First, we fix the values of all Y c j | m j =1 variables (shaded on the picture) and optimize all X c i | m i =1 variables in par-allel. Then we fix new values of all X c i variables (shaded) and optimize all Y c j variables in parallel. connected with an edge. The two nodes are combinatorial r.v. X  X  X c and Y c defined over all possible clusterings of data modalities X and Y , respectively. Over the only edge, we can define a log-potential function that would characterize an agreement between clusterings x c and y c . Assuming that the number of clusters k is given to us and never changes, we follow Dhillon et al. [10] by choosing Mutual Information between the clusterings to be our log-potential function (for a discussion, see [4]). We define a random variable  X  X over all clusters in the clustering x c . We define  X  Y analogously. The only log-potential in the model is then: where the joint distribution p (  X  x i ,  X  y j ) is estimated from the data as p (  X  x i ,  X  y j ) = are obtained through standard marginalization: p (  X  x i ) = P
Our optimization procedure in the resulting model starts with some initial values x c 0 and y c 0 of X c and Y c , respec-tively. First, we fix y c 0 and perform a local search around x 0 that aims at maximizing the objective (3). The specific local search procedure we use is hill climbing, performed by moving a data instance x from its current cluster to another for which the objective is maximal (or keeping x in its cur-rent cluster in case of having no better option). After some while (e.g. when the objective reaches its plateau, or when all the data instances have been tested, etc.), we fix the new clustering x c 1 and switch to optimizing Y c , and then go on over the variables in a round robin fashion. 3 After T such iterations, clustering x c T and y c T come to an agreement with each other. For the sake of our current work on clustering agreement, we build m models like that (for an illustration, see Figure 1, left).

Bekkerman et al. [4, 3] showed that such a bi-modal clus-tering algorithm has a great potential in clustering multidi-mensional data, but adding more modalities in most cases improves the clustering results even more, as it improves the model X  X  regularization and reduces its tendency to over-fitting. We claim that a similar effect can be achieved even
Note that the requirement on the number of clusters k to be fixed during the optimization process does not hold at the transition stage (when we switch between X c and Y c ). At that stage we are free to split or merge clusters, which allows exploring clustering hierarchy. with only two data modalities X  X hen more than one cluster-ing result is available. Since the original bi-modal clustering algorithm is randomized, 4 each of its run is likely to produce a different result that captures different signals coming from the data. We may take the best from each of the constructed models by letting them agree with each other. To initiate such an agreement process, we connect all the X c i | m i =1 of our original models with all the Y c j | m j =1 nodes, which re-sults in a bipartite graph (see Figure 1, right).
Over each edge e ij of the resulting Comraf model, we de-fine a Mutual Information log-potential f ij ( x c i , y c We then derive the new MPE inference process from Equa-tion (2) as follows: ( x To perform this optimization, we apply the Iterative Condi-tional Mode (ICM) inference algorithm [9]. At each iteration of the ICM algorithm, one random variable from the MRF gets chosen, and the values of all the rest of the model get fixed. Next, the chosen variable is optimized with respect to the fixed values of its neighboring random variables. After this optimization step is over, we fix the final value of the chosen variable and move to optimizing another variable X  and so on in a round robin. When only one variable gets optimized at a time, our objective from Equation (4) be-comes linear in m . Suppose that we optimize variable x c ( x c i )  X  = arg max We notice however that rather than optimizing one variable at a time, we can employ parallelization. If we fix all the val-ues of the Y c j | m j =1 variables in our bipartite Comraf, then all the X c i | m i =1 variables become conditionally independent such that we can optimize them in parallel. Once new values of the X c i variables are obtained, we fix them and move to opti-mizing the Y c j variables X  X n parallel again. We can perform this process (illustrated in Figure 2) until its convergence.
Proposition 4.1. The agreement process converges to a local maximum of the objective function from Equation (4).
Proof. The proof of the proposition is fairly straight-forward. It is based on two observations. First, at a node level, the local search optimization does not decrease its ob-jective in Equation (5), because any data instance is moved from its cluster to another only if the objective is increased. Since the global objective from Equation (4) is a sum of these local objectives, then it cannot be decreased either. The second observation is that the global objective remains unchanged at the transition time (when one set of variable values gets fixed and we switch to optimizing the second set of variables). Since our global objective is a sum of Mu-tual Information terms that are bounded from above, and the presented process never decreases its objective, it must reach its local maximum X  X hich will stop the process. The importance of parallelization should not be underesti-mated in the case of our clustering agreement process X  X t allows up to apply our method to large data collections.
The randomization can come from the clustering initializa-tion, as well as from ordering the data instances to test for a potential relocation.
We evaluate our methods on two unsupervised learning tasks: (a) document/word clustering, where the goal is to construct topically related groups of documents, while simul-taneously constructing groups of words X  X hich presumably represent topics; (b) user/movie clustering for the collabo-rative filtering purposes, where the goal is to recommend a movie to a user based on collaborative behavior of the users X  community. On both tasks, we measure the clustering qual-ity as well as the clustering stability (using criteria defined in Section 5.2), as a relative improvement that CAP provides with respect to the original clusterings. We also measure an improvement in stability and quality of a consensus clus-tering method, when applied to the clusterings after CAP was applied on them. For this purpose, we use the Sta-ble Component Consensus Clustering (SCCC) method dis-cussed in Section 5.3. We come up with three experimental setups (CAP, stand-alone SCCC, and SCCC applied after CAP) and test them on four data collections, three of which are document collections, and the fourth is the user/movie dataset (see Section 5.1). We provide technical details on the underlying clustering methods in Section 5.4.
For the task of document clustering, we use three pub-licly available datasets. Two of them (called sanders-r and kitchen-l ) are relatively large email directories of two former Enron employees, derived from the Enron Email Dataset. 5 The sanders-r collection contains 1188 email messages stored in 30 folders. The kitchen-l collection contains 4015 mes-sages stored in 47 folders. 6 The document clustering task on those collections aims at reconstructing the email folders. The third document collection is the benchmark 20 News-groups dataset, which consists of 19997 newsgroup postings, classified into 20 categories. It is known that about 4 . 5% of the postings are duplications of other postings X  X owever, for better replicability, we do not exclude the duplicated post-ings. A particular preprocessing scheme of these datasets is described in Bekkerman et al. [2].

For the collaborative filtering task, we use the Netflix data as it was used for the Netflix KDD X 07 Cup. 7 For this competition, the task was set up as predicting which movies would be rated by which users, without predict-ing the actual ratings. The training data consists of about 100M user/movie pairs (positive instances) for 480,189 users and 17770 movies. The hold-out data consists of 100,000 pairs, 7.8% of which are positive instances. We simultane-ously construct both a clustering of users and a clustering of movies, and then use the resulting clusterings to rank the test instances. Our ranking function r ( u, m ) for a user u and a movie m is composed of the popularity score p ( u ) p ( m ), lifted with the information gained at the clustering process: when the user u was assigned into a cluster  X  u , and the movie m was assigned into a cluster  X  m (for a discussion, see [5]). http://www.cs.cmu.edu/~enron
Preprocessed versions of the sanders-r and kitchen-l datasets can be downloaded from http://www.cs.umass. edu/~ronb/enron_dataset.html . http://cs.uic.edu/~liub/Netflix-KDD-Cup-2007.html
For assessing the quality of document clustering we use the clustering accuracy measure, as used in previous work [10, 2]. Let x c be a clustering of a document collection X . Let T be the set of ground truth categories. For each cluster  X  x of x , let n T (  X  x ) be the maximal number of  X  x  X  X  elements that be-long to one category. Then, precision Prec (  X  x, T ) of the clus-ter  X  x with respect to T is defined as Prec (  X  x, T ) = n The micro-averaged precision of the entire clustering x c where n is the size of the dataset. In words, Prec ( x c , T ) is the portion of documents classified into the dominant cat-egories. Note that this measure is meaningless when the number of clusters k is large. In fact, if k equals the num-ber of data points, the micro-averaged precision is 1. In all our experiments, we choose the number of clusters to be equal to the number of ground truth categories: k = |T| . And when k = |T| , the micro-average precision Prec ( x c equals micro-averaged clustering accuracy , or just clustering accuracy , for short.

For evaluating our collaborative filtering results based on the constructed clusterings of users and movies, we follow Bekkerman and Scholz [5] who compute the Area Under the ROC Curve (or AUC , in short) for a constructed ranking of the user/movie pairs (see Section 5.1 for details on our ranking scheme).

To evaluate the stability of a clustering method, we use the Jaccard index [12] averaged over all m ( m  X  1) 2 pairs of m outcomes of this method. Given a pair of clusterings, x c x , we define a as the number of data instance pairs that belong to the same cluster in x c 1 as well as in x c 2 . We define b as the number of data instance pairs that belong to the same cluster in x c 1 but not in x c 2 . We define c as the number of data instance pairs that belong to the same cluster in x but not in x c 1 . The Jaccard index between x c 1 and x c defined as: Note that the Jaccard index ranges between 0 and 1. Our underlying clustering methods (Section 5.4) are randomized, such that, when applied m times, they produce different results. We compute the averaged Jaccard index over those results, assessing by this the stability of the method. The stability of the SCCC method (see 5.3 below) is assessed by applying it a number of times with different values of its parameter, and computing the averaged Jaccard index over the outcomes. Evaluating the stability of our CAP method is straightforward: it is designed to produce m outcomes, over which we compute the averaged Jaccard index.
One of the applications of our clustering agreement pro-cess is in improving stability and accuracy of consensus clus-tering. A possible scenario for this task would be to come up with the set of original clusterings, let them interact via our clustering agreement mechanism, and then apply a con-sensus clustering method that would presumably be more accurate and more stable than a consensus of the original clusterings. To evaluate this hypothesis, we need to have a consensus clustering algorithm available for experimenta-tion. There are many off-the-shelf consensus clustering algo-rithms (see, e.g., [17]), however, we did not find any existing algorithm that would have the following property: it is de-sired that each cluster in the resulting consensus clustering would be  X  X abeled X  by a large portion of data instances that compose a stable component , i.e. are often, if not always, clustered together.

Let us first provide some intuition. In Section 5.2 we presented the clustering accuracy measure as the micro-averaged accuracy of each cluster, which is computed as a portion of the cluster having the most common label. For example, in document clustering, if a large portion of a clus-ter is classified as, say, sports news , then we assume that the entire cluster is about sports while the cluster mem-bers that were not classified as sports news are considered noise. All the above refers to the evaluation stage, where the ground truth labels are available. At the consensus cluster-ing stage, however, the task remains completely unsuper-vised, i.e. there is no ground truth. Nevertheless, the no-tion of  X  X arge data portions X  still exists. If a large group of data instances is always clustered together, while not hav-ing a strong correlation pattern with other groups, then this group can be a good candidate for seeding a cluster. If all clusters can be seeded with such groups, then these groups will impose  X  X mplicit labels X  on their corresponding clusters, which will be very helpful in further data analysis.
Lacking an existing consensus method that would have such a property, we came up with a method of our own, which is very simple, very efficient, and still effective. We call it Stable Component Consensus Clustering (SCCC) and note that it is somewhat similar to the Iterative Voting Con-sensus (IVC) method proposed by Nguyen and Caruana [17]. The SCCC method operates in two stages: first, it seeds clusters with large stable components that are distant from each other, and second it associates all the other stable com-ponents with one of the seeds. Let us provide more details.
Definition 5.1. Given an ordered list of m clusterings ( x of x as an ordered list of cluster identities to which x belongs in each of the m clusterings: s ( x ) = (  X  x i 1 ,  X  x i
Definition 5.2. A stable component is the largest subset of data instances each of which have the same signature. A distance between two stable components is then defined as a Hamming distance between their signatures. Our SCCC algorithm works as follows: 1. Construct all stable components of the data. Con-2. Traverse the ranked list R and test each component 3. Traverse the ranked list R again, this time assigning We also tested a centroid-based version of the SCCC al-The main advantage of the SCCC algorithm is in its efficiency X  its time complexity is linear in the number of data instances. Its main drawback is that it has a free parameter g , which we call the seed gap parameter. The values of the seed gap parameter are integers in the [0 ..m ] range. We do not at-tempt to set it up in the optimal way, instead, we apply the SCCC algorithm with all possible seed gap values 9 and then average the results. This provides a notion of stability of the SCCC algorithm with respect to tuning its parameter, and as well as a notion of statistical significance of its results.
Other possible limitations of the SCCC algorithm include its instability for very large values of m : if an ensemble consists of very many clusterings, it is highly probable that stable components will be small, and then the seeding pro-cess will be almost random. In our experiments, however, we choose m = 10 , which is small enough to allow many large stable components. Also, the seeding procedure can be almost random if the original clusterings are very differ-ent from each other X  X ut it is exactly the problem that we solve with our clustering agreement process.

It is important to note that in this paper we do not focus on consensus clustering, and therefore do not intend to pro-pose the most effective consensus clustering method. Our goal is to show that CAP can improve consensus clustering, applied on top of CAP X  X  results, as compared to the same consensus clustering method applied without CAP. In our document clustering setup, we use the Multi-way Distributional Clustering (MDC) tool [2, 4] to produce origi-nal clusterings. MDC is an open source tool, 10 which is able to perform simultaneous clustering of multiple modalities, organized in a Comraf model of arbitrary topology. First, we apply it to construct initial clusterings, using the Com-raf topology from Figure 1 (left). MDC allows hierarchical clustering, which is proved to be very effective [2]. One of its advantages is that it is not heavily dependent on the clustering initialization. We perform top-down clustering of words and, simultaneously, bottom-up clustering of docu-ments. The top-down scheme is initialized by assigning all the words in one cluster. The bottom-up scheme is initial-ized by having one cluster per document. At each MDC X  X  iteration, we first split each word cluster to two, and then perform the local search (as described in Section 4) to max-imize the objective (3). Then we merge each two clusters of documents and perform the same optimization over the resulting document clustering. We continue this process un-til we reach the required number of document clusters. At the first iteration of the algorithm, we perform three split-and-optimize procedures over word clusters (ending up with eight clusters), before we start with the merge-and-optimize procedure over the document clusters.

For our clustering agreement process, we apply MDC on the bipartite Comraf from Figure 1 (right). We initialize each clustering node with a clustering obtained at the bi-modal stage, and perform flat (i.e. non-hierarchical) clus-gorithm, in which each cluster X  X  centroid is updated after a stable component is added. The resulting version did not show significantly better results on our test datasets.
In some cases, the seed gap g can be too large such that not all k clusters can be seeded. In those cases, we disregard such a value of g . http://comraf.sourceforge.net results are shown in brackets.
 tering until the changes in the value of the objective (4) are small enough. In practice, it turned out that only a few op-timization iterations are required to reach convergence. For consistency, we fixed the number of iterations to four.
The main disadvantage of the MDC method is that it is inherently sequential and time-consuming. It cannot be applied to the large Netflix dataset. Instead, we apply its parallelized version, called DataLoom [5]. The DataLoom algorithm optimizes the same objective, however, it is not hierarchical, therefore, at the bi-modal stage, we randomly initialize user and movie clusterings. We fix the number of clusters at 800 X  X or both cases. At the CAP stage, how-ever, there is no principal difference between DataLoom and MDC. We performed only two CAP iterations.
Our results on the document clustering task are summa-rized in Table 1. As can be seen from the table, on all the three datasets CAP significantly improves both accu-racy and stability of the original clustering algorithm. The stability improvements are quite impressive. An interesting case is the stability improvement of word clustering on the 20 Newsgroups dataset. CAP managed to increase the aver-aged Jaccard index from about 0.1 to over 0.6. We decided to study this case more closely, and constructed all stable components before and after the CAP application. Even a quick scan through the results showed dramatic changes. For example, one stable component of the post-CAP result consists of the following 15 words: altar materialistic revelation apprentice metaphorical teachings bible narratives theologians gospel philistines tierra inerrant recite unbeliever Checking on these words in the pre-CAP data, we figured out that each of them composed a stable component of size one, i.e. was disconnected from any other. The stability and accuracy improvement per CAP X  X  iteration are shown in Figure 4. As we can see, the first two iterations are most crucial, after which the curves become flatter.
 From the right two columns of Table 1 we can learn that CAP also improves both stability and accuracy of consensus clustering. The improvement on words is always more sub-stantial than the improvement on documents, while the ab-solute values on words are always lower than on documents X  possibly because the word modality is larger than the doc-ument modality, and the number of word clusters is larger than the number of document clusters.

We have to note that, technically, stability of our consen-sus method (with or without CAP) cannot be directly com-pared with the stability of the original clustering (again, with or without CAP), because SCCC is a deterministic method, such that its stability is measured over various val-ues of its parameter g , while the original clustering is ran-domized. Nevertheless, we added the relative improvement values, for consistency with the rest of the table.
In contrast, the accuracy values are comparable between all the listed methods. Although relative improvements do not look as dramatic as for the stability measure, we should point out that in two of the three cases we managed to obtain the state-of-the-art results. Specifically, the 44.8% accuracy obtained by the consensus method with the help of CAP on the kitchen-l dataset, as well as over 75% accuracy shown on the 20 Newsgroups are, to our knowledge, the best results ever obtained on these data collections.

It is interesting to see how the CAP system responds to changes in m  X  X he number of participating clusterings. Fig-ure 3 shows such a result on the 20 Newsgroups . First, it turns out that as few as just three participants can already Table 3: Improving stability of clustering consen-suses on the sanders-r dataset: we show that CAP consistently leads to similar agreements, when ap-plied to different ensembles of clusterings.

Measure Before CAP After CAP Relative
Averaged accuracy . 6771  X  . 0033 . 6870  X  . 0027 +1.5% of doc consensuses
Averaged Jaccard . 5365  X  . 0090 . 6186  X  . 0057 +15.3% of doc consensuses
Averaged Jaccard . 2163  X  . 0018 . 3310  X  . 0028 +53.0% of word consensuses boost the accuracy by 2%. Second, as m goes up, the stan-dard error of clustering accuracy decreases, and the after-CAP curve consistently rises while the before-CAP curve fluctuates.

Our stability and quality results on the Netflix dataset are shown in Table 2. As one can notice, CAP obtains a very modest improvement in the AUC measure. Although being statistically significant, it cannot be considered a suc-cess. However, we should note that improving AUC on this dataset is a very challenging task, and half-percent im-provements are considered a valuable achievement (see [5]). Nonetheless, the stability improvement is substantial, espe-cially taking into account the size of the data.
As we have shown, our clustering agreement process is able to significantly improve stability of clustering methods. An important question still to ask is whether CAP is con-sistent in its behavior, i.e. does it tend to lead to the same (or similar) solutions, independently of the initial cluster-ings? To answer this question, we set up the following ex-periment. Over the same data, we construct M clustering ensembles, and find their M consensuses (using our SCCC method with g = 5 ). Afterwards, we apply CAP to each of the ensembles, and find M consensuses of the after-CAP ver-sions. Then we compute the stability of the two ensembles of consensuses X  X efore CAP and after CAP. The results for the sanders-r dataset (with M = 10 ) are shown in Table 3. To our satisfaction, the after-CAP ensemble of consensuses is much more stable (and, as a matter of fact, is also slightly more accurate). This supports our hypothesis that CAP X  X  behavior is consistent.
In this paper, we investigated a novel machine learning problem, the Improvement of Clustering Stability, which can play a key role in data analysis and other areas of data min-ing. We proposed an effective method for this problem X  X  so-lution, the Clustering Agreement Process (CAP), which is based on inference in a new type of Combinatorial MRFs X  Figure 3: Clustering accuracy on the 20 Newsgroups dataset, as a function of m  X  X he number of cluster-ings which participate in the agreement process. of bipartite topology, with multiple nodes per data modality. We proposed an efficient, parallelized method for performing this inference, and showed its convergence. As a byproduct of this research, we proposed a Stable Component Consen-sus Clustering method, which has a property of  X  X mplicit labeling X  of the constructed clusters. We applied our CAP method to four real-world datasets and obtained significant improvements in clustering stability, as well as in clustering quality, of both initial clusterings and their consensuses.
The authors thank Rajan Lukose for fruitful discussions at the early stages of this project. Ron thanks his wife Anna for her constant support. [1] L. Badea. Clustering and metaclustering with [2] R. Bekkerman, R. El-Yaniv, and A. McCallum. [3] R. Bekkerman and J. Jeon. Multi-modal clustering for [4] R. Bekkerman, M. Sahami, and E. Learned-Miller. [5] R. Bekkerman and M. Scholz. Data weaving: Scaling [6] S. Ben-David, U. von Luxburg, and D. P  X al. A sober the original clustering results, before CAP was applied. [7] A. Ben-Hur, A. Elisseeff, and I. Guyon. A stability [8] J. Besag. Spatial interaction and statistical analysis of [9] J. Besag. On the statistical analysis of dirty pictures. [10] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [11] X. Z. Fern and C. Brodley. Solving cluster ensemble [12] P. Jaccard. The distribution of flora in the alpine [13] A. Kreiger and P. Green. A generalized rand-index [14] L. Kuncheva and D. Vetrov. Evaluation of stability of [15] T. Lange, V. Roth, M. Braun, and J. Buhmann. [16] E. Levine and E. Domany. Resampling method for [17] N. Nguyen and R. Caruana. Consensus clusterings. In [18] K. Punera and J. Ghosh. Consensus-based ensembles [19] V. Raghavan and M. Y. L. Ip. Techniques for [20] W. M. Rand. Objective criteria for the evaluation of [21] N. Slonim, N. Friedman, and N. Tishby. Multivariate [22] A. Strehl and J. Ghosh. Cluster ensembles  X  a
