 In this work we consider learning on a graph. Assume we have an undirected graph of n nodes given by a symmetric weight matrix W . The i th node in the graph has a label y i  X  { 0 , 1 } stored in number of incorrect predictions is || y  X   X  y || 2 .
 problems, graph representations are more natural than feature vector representations. When clas-sifying web pages, for example, edge weights in the graph may incorporate information about hy-perlinks. Even when the original data is represented as feature vectors, transforming the data into a graph (for example using a Gaussian kernel to compute weights between points) can be convenient for exploiting properties of a data set.
 In order to bound prediction error, we assume that the labels are smoothly varying with respect to the underlying graph. The simple smoothness assumption we use is that small. Here || denotes absolute value, but the labels are binary so we can equivalently use squared difference. This smoothness assumption has been used by graph-based semi-supervised learning algorithms which compute  X  y using a labeled set L chosen uniformly at random from V [Blum and Chawla, 2001, Hanneke, 2006, Pelckmans et al., 2007, Bengio et al., 2006] and by online graph la-beling methods that operate on an adversarially ordered stream of vertices [Pelckmans and Suykens, 2008, Brautbar, 2009, Herbster et al., 2008, 2005, Herbster, 2008] In this work we consider methods that make use of the smoothness assumption and structure of the graph in order to both select L as well as make predictions. Our hope is to achieve higher prediction accuracy as compared to random label selection and other methods for choosing L . We are particu-single batch, offline label selection problem is important in many real-world applications because it is often the case that problem constraints make requesting more than one batch of labels very costly. For example, if requesting a label involves a time consuming, expensive experiment (potentially involving human subjects), it may be significantly less costly to run a single batch of experiments in parallel as compared to running experiments in series.
 We give several methods which, under the assumption for previously used methods, and we show improved performance over random label selection and baseline submodular maximization methods on several real world data sets. We first give a simple worst case bound on prediction error in terms of label smoothness using few assumptions about the method used to select labels or make predictions. In fact, the only assumption we make is that the predictions are consistent with the set of labeled points (i.e.  X  y L = y L ). The bound motivates an interesting method for selecting labeled points and provides a new motivation for a standard prediction method Blum and Chawla [2001] when used with arbitrarily selected L . The bound also forms the basis of the other bounds we derive which make additional assumptions. Define the graph cut function  X ( A, B ) , Note this function is different from normalized cut (also called sparsest cut). In this function, the difference is important: computing normalized cut is NP-hard, but we will show  X ( L ) can be com-puted in polynomial time.  X ( L ) measures how easily we can cut a large portion of the graph away from L . If  X ( L ) is small, then we can separate many nodes from L without cutting very many edges. We show that  X ( L ) where L is the set of labeled vertices measures to what extent prediction error can be high relative to label smoothness. This makes intuitive sense because if  X ( L ) is small than there is a large set of unlabeled nodes which are weakly connected to the remainder of the graph (including L ).
 Theorem 1. For any  X  y consistent with a labeled set L || y  X   X  y || 2  X  where  X  is the XOR operator.
 points are incorrectly classified). implies y i = y j . Then The 1 2 term is introduced because the sum double counts edges.
 This bound is tight when the set of incorrectly classified points I is one of the sets minimizing This bound provides an interesting justification for the algorithm in Blum and Chawla [2001] and related methods when used with arbitrarily selected labeled sets. The term involving the predicted labels, algorithm of Blum and Chawla [2001]. When this is used to compute  X  y , the bound simplifies. Lemma 1. If for a labeled set L then Proof. When we choose  X  y in this way follows from Theorem 1.
 Label propagation solves a version of this problem in which  X  y is real valued [Bengio et al., 2006]. The bound also motivates a simple label selection method. In particular, we would like to select a la-beled L set that maximizes  X ( L ) . We first describe how to compute  X ( L ) for a fixed L . Computing  X ( L ) is related to computing with parameter  X  &gt; 0 . The following result is paraphrased from Fujishige [2005] (pages 248-249). and We can compute Equation 1 for all  X  via a parametric maxflow/mincut computation (it is known there are no more than n  X  1 distinct solutions). This gives a polynomial time algorithm for com-puting  X ( L ) . Note this theorem is for unconstrained minimization of T , but restricting T  X  L =  X  does not change the result: this constraint simply removes elements from the ground set. In practice, this constraint can be enforced by contracting the graph used in the flow computations or by giving certain edges infinite capacity.
 As an alternative to solving the parametric flow problem, we can find the desired  X  value through an iterative method [Cunningham, 1985]. The left of Figure 1 shows this approach. The algorithm also know  X   X   X  0 and can conclude  X  =  X  0 and the set T returned achieves this minimum. One can also show the algorithm terminates in at most | V | iterations [Cunningham, 1985].
 Having shown how to compute  X ( L ) , we now consider methods for maximizing it.  X  is neither submodular nor supermodular. This seems to rule out straightforward set function optimization. In current cut. Our heuristic is then to simply add a random element from this cut to L . The right of Figure 1 shows this method.
  X ( L ) , we do not have methods for maximizing it exactly or with guaranteed approximation. Aside from knowing the function is not submodular or supermodular, we also do not know the hardness of the problem. In the next section, we describe a lower bound on the  X  function based on a notion of graph covering. This lower bound can be maximized approximately via a simple algorithm and has a well understood hardness of approximation. Second, we have found in experimenting with our heuristic for maximizing  X ( L ) that the function can be prone to imbalanced cuts; the computed cuts sometimes contain all or most of the unselected points V \ L and other times focus on small sets of outliers. We give a third bound on error which attempts to address some of this sensitivity. The method we consider in this section uses a notion of graph covering. We say a set L  X  -covers the graph if  X  i  X  V either i  X  L or either in L or connected with total weight at least  X  to nodes in L (or both). This is a simple real valued extension of dominating sets. A dominating set is a set L  X  V such that  X  i  X  V either i  X  L or a neighbor of i is in L (or both). This notion of covering is related to the  X  function case that  X ( L )  X   X  . The converse does not hold, however. In other words,  X  is a lower bound on  X ( L ) . Then,  X  can replace  X ( L ) in the bound in the previous section for a looser upper bound on prediction error. Although the bound is looser, compared to maximizing  X ( L ) we better understand the complexity of computing an  X  -cover.
 Corollary 1. For any  X  y consistent with a labeled set L that is an  X  -cover || y  X   X  y || 2  X  where  X  is the XOR operator.
 Similar to Lemma 1, by making additional assumptions concerning the prediction method used we can derive a slightly simpler bound. In particular, for a labeled set L that is an  X  cover, we assume unlabeled nodes are labeled with the weighted majority vote of neighbors in L . In other words, set  X  y = y i for i  X  L , and set  X  y i = y 0 for i /  X  L with y 0 such that With this prediction method we get the following bound.
 Lemma 2. If L is an  X  -cover and V \ L is labeled according to majority vote Proof. The right hand side follows immediately from the middle expression, so we focus on the first satisfies y i 6 = y j  X  j  X  L i , and a unique set of edges with total weight at least  X / 2 included inside the summation in the middle expression.
 In computing an  X  -cover, we want to solve. Where is neither supermodular nor submodular. However, we can still compute an approximately minimal that Also, min( Then, we can replace F 0 with and solve This is a submodular set cover problem. The greedy algorithm has approximation guarantees for this problem for integer valued functions [Krause et al., 2008]. For binary weight graphs the ap-an approximation guarantee. In practice, we apply the greedy algorithm directly.
 As previously mentioned,  X  -covers can be seen as real valued generalizations of dominating sets. In particular, an  X  -cover is a dominating set for binary weight graphs and  X  = 1 . The hardness of approximation results for finding a minimum size dominating set then carry over to the more general  X  -cover problem. The next theorem shows that the  X  -cover problem is NP-hard and in fact the greedy algorithm for computing an  X  -cover is optimal up to constant factors for  X  = 1 and binary weight graphs. It is based on the well known connection between finding a minimum dominating set problem and finding a minimum set cover.
 Theorem 3. Finding the smallest dominating set L in a binary weight graph is NP -complete. Furthermore, if there is some  X  &gt; 0 such that a polynomial time algorithm approximates the smallest dominating set within (1  X   X  ) ln( n/ 2) then NP  X  TIME ( n O (log log n ) ) .
 We have so far discussed computing a small  X  cover for a fixed  X  . If we instead have a fixed label budget and want to maximize  X  , we can do so by performing binary search over  X  . This is the approach used by Krause et al. [2008] and gives a bi-approximation. In this section we consider an algorithm that clusters the data set and replaces the  X  function with a normalized cut value. The normalized cut value for a set T  X  V is In other words, normalized cut is the ratio between the cut value for T and minimum of the size of T and its complement. Computing the minimum normalized cut for a graph is NP-hard.
 Theorem 4. Let S 1 , S 2 , ...S k be a partition of V , and assume we have estimates of the majority according to the estimated majority label for S l then with probability at least 1  X   X  where and Proof. By the union bound, the estimated majority labels for all of the clusters are correct with probability at least 1  X   X  . Let I be the set of incorrectly labeled nodes (errors). We consider the intersection of I with each of the clusters. Let I l , | I  X  S l | . I = since we labeled cluster according to the majority label for the cluster. Then y 6 = y j . The desired result then follows.
 Note in practice we only label the unlabeled nodes in each cluster using the majority label estimates. Using the true labels for the labeled nodes only decreases error, so the theorem still holds. In this bound,  X  is a measure of the density of the clusters. Computing  X  l for a particular cluster is NP-hard, but there are approximation algorithms. However, we are not aware of approximation algorithms for computing a partition such that  X  is maximized. This is different from the standard normalized cut clustering problem; we do not care if clusters are strongly connected to each other only that each cluster is internally dense. In our experiments, we try several standard clustering algorithms and achieve good real world performance, but it remains an interesting open question to design a clustering algorithm for directly maximizing  X  . An approach we have not yet tried is to use the error bound to choose between the results of different clustering algorithms.
 We now consider the problem of estimating the majority class for a cluster. If we uniformly sample decreases exponentially with the number of labels if the fraction of nodes in the minority class is bounded away from 1 / 2 by a constant. We now show that if the labels are sufficiently smooth and the cluster is sufficiently dense then the fraction of nodes in the minority class is small. Theorem 5. The fraction of nodes in the minority class of S is at most where belonging to the other class. Let f be the fraction of nodes in the minority class. If we have an estimate of the smoothness of the labels in a cluster, we can use this bound and an approximation of  X  to determine the number of labels needed to estimate the majority class with high confidence. In our experiments, we simply request a single label per cluster. Table 1: Error rate mean (standard deviation) for different data set, label count, method combina-tions. Figure 2: Left: Points selected by the  X  function maximization method. Right: Points selected by the spectral clustering method. We experimented with a method based on Lemma 1. We use the randomized method for maximizing Theorem 4. We cluster the data then label each cluster according to a single randomly chosen point. We chose the number of clusters to be equal to the number of labeled points observing that if a cluster is split evenly amongst the two classes then we will have a high error rate regardless of how well we estimate the majority class. We tried three clustering algorithms: a spectral clustering method [Ng et al., 2001], the METIS package for graph partitioning [Karypis and Kumar, 1999], and a k -cut approximation algorithm [Saran and Vazirani, 1995, Gusfield, 1990]. As a baseline we use random label selection and prediction using the label propagation method of Bengio et al. [2006] motivated by the graph covering bound, but for lack of space we omit these results.
 We used six benchmark data sets [Chapelle et al., 2006]. We use graphs constructed with a Gaussian kernel with standard deviation chosen to be the average distance to the k 1 th nearest neighbor divided by 3 (a similar heuristic is used by Chapelle et al. [2006]). We then make this graph sparse by k 2 nearest neighbors. We use 10 and 100 labels. We set k 1 and k 2 for each data set and label count to be the parameters which give the lowest average error rate for label propagation averaging over give low error for the baseline method to ensure any bias is in favor of the baseline as opposed to the new methods we propose. We then report average error over 1000 trials in the 10 label case and 100 trials in the 100 label case for each combination of data set and algorithm.
 Table 1 shows these results. We find that the  X  function method does not perform well. We found the points selected are essentially random. However, on the USPS data set and on some synthetic data sets we have tried, we have also observed the opposite behavior where the cuts are very small and seem to focus on small sets of outliers. Figure 2 shows an example of this. The k -cut method also did not perform well. We X  X e found this method has similar problems with outliers. We think these outlier sensitive methods are impractical for graphs constructed from real world data. The results for the spectral clustering and METIS clustering methods, however, are quite encourag-ing. These methods performed well matching or beating the baseline method on the 10 label trials and in some cases significantly improving performance. The METIS method seems particularly ro-bust. On the 100 label trials, performance was not as good. In general, we expect label selection to help more when learning from very few labels. The choice in clustering method seems to be of great practical importance. The clustering methods which work best seem to be methods which minimize normalize cut like objectives. This is not surprising given the presence of the normalized cut term in Theorem 4, but it is an open problem to give a clustering method for directly minimizing the bound. We finally note that the numbers we report for our baseline method are in some cases significantly different than the published numbers [Chapelle et al., 2006]. This seems to be because of a variety of factors including differences in implementation as well as significant differences in experiment set up. We have also experimented with several heuristic modifications to our methods and compared our methods to simple greedy methods. One modification we tried is to use label propagation for prediction in conjunction with our label selection methods. We omit these results for lack of space. Previous work has also used clustering, covering, and other graph properties to guide label selection error to label smoothness for single batch label selection methods. Most previous work on label selection methods for learning on graphs has considered active (i.e. sequential) label selection [Zhu and Lafferty, 2003, Pucci et al., 2007, Zhao et al., 2008, Wang et al., 2007, Afshani et al., 2007]. Afshani et al. [2007] show in this setting O ( c log( n/c )) where c = sufficient and necessary to learn the labeling exactly under some balance assumptions. Without rate. In some cases, our bounds are better despite considering only non sequential label selection. Consider the case where c grows linearly with n so c/n = a for some constant a &gt; 0 . In this case, with the bound of Afshani et al. [2007] the number of labels required to achieve a fixed error rate  X  also grows linearly with n . In comparison, our graph covering bound needs an  X  -cover with  X  = a/ X  . For some graph topologies, the size of such a cover can grow sublinearly with n (for example if the graph contains large, dense clusters). Afshani et al. [2007] also use a kind of dominating set offline setting. Zhao et al. [2008] also use a clustering algorithm to select initial labels. Other work has given generalization error bounds in terms of label smoothness [Pelckmans et al., O (
P graph structure, our bounds can be significantly better. For example, if a binary weight graph con-tains c cliques of size n/c then, we can find an  X  cover of size c X  log( c X  ) giving an error rate of O (
P A line of work has examined mistake bounds in terms of label smoothness for online learning on graphs [Pelckmans and Suykens, 2008, Brautbar, 2009, Herbster et al., 2008, 2005, Herbster, 2008]. These mistake bounds hold no matter how the sequence of vertices are chosen. Herbster [2008] also considers how cluster structure can improve mistake bounds in this setting and gives a mistake bound similar to our graph covering bound on prediction error. Herbster et al. [2005] discusses using an active learning method for the first several steps of an online algorithm. Our work differs from this previous work by considering prediction error bounds for offline learning as opposed to mistake bounds for online learning. The mistake bound setting is significantly different as the prediction method receives feedback after every prediction.
 Acknowledgments This material is based upon work supported by the National Science Foundation under grant IIS-0535100.
