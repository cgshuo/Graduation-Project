 Recently, the dimensionality reduction and matrix factorization techniques have been regarded as a significant machine learning tool for feature extraction and data compres-sion, as both the size and dimensionality of data in most application are continuing to increase rapidly.

A non-trivial issue in applying these techniques to actual problems is how to deal with missing data elements, as the real-world data, e .g., medical testing data, food pref-erence questionnaire data, purchase records, etc. usually contains missing parts. If the missing portion is relatively small, ad hoc treatment such as filling the missing ele-ments with constant values and inferring them from similar data is acceptable. A more sophisticated approach commonly used is to alternately estimate the missing values and conduct dimensionality reduction or matrix fact orization until convergence. The method is known as EM (expectation maximizati on) algorithm in machine learning. However, if the missing portion is very large and has so me structural pattern, these conventional approaches are expected to fail.

Consider the following situation for an example. An observer is wandering around the town, carrying a wireless device (such as tablet PC). The device is assumed to be ca-pable of recording approximate relative dir ections to all detected wireless access points (APs). If the device could always communicate with all APs in the town wherever it is, the observation data could be represented as a complete matrix, whose ( i, j ) -th element is the relative direction to the j -th AP from the i -th observation position. Unfortunately, however, most of the elements are missing, because the wireless communication range is limited and affected by occlusion. Besides, the pattern of missing data is not ran-dom but structured, as whether a measurement is present or absent is dependent on the spatial relationship between the observe r and AP. The conventional approaches are not suitable for this kind of missing data.

In this paper, we propose the locally linear alignment co-embedding (LLACoE) that embeds both row and column vectors of a matrix-form observation data with largely and structurally missing elem ents into low-dimensional latent spaces respectively. A key idea is that a measurement y i,j can be approximated by some linear projection of the state vector of j -th object z j onto the subspace determined by the observer X  X  state x . A remarkable feature of LLACoE is that it does not require iterative computation to estimate the missing values, but is efficiently solved by eigendecomposition or a system of linear equations. Dimensionality reduction is a major topic of machine learning, as well as classification, regression and clustering. Especially, i n the last decade, non-linear dimensionality re-duction (a.k.a. manifold learning) methods such as Isomap[7] and LLE[3] have been developed and become popular. In addition, matr ix factorization or low-rank matrix ap-proximation techniques such a s singular value decomposition (SVD) and non-negative factorization (NMF) have been widely used in a variety of datamining applications.
A practical difficulty is that the real-world data is not only huge and high-dimensional, but also often incomplete due to various reasons. The simplest way of dealing with such incomplete data is to fill the missing parts with some proper constant values, typically by zero. This approach will be reasonable enough, when the values are  X  X issing X  be-cause they are out of measurement ranges. However, the applicability of this method is obviously limited, because not all measurement data have such a property. Besides, it is sometimes nontrivial to find a proper constant value, even when it is applicable.
A more sophisticated and popular approach is to estimate the missing values and conduct dimensionality reduction or matrix factor ization alternately until it converges. In computer vision (CV), PCAMD (PCA with missing data) methods[5] such as alter-nate least squares (ALS) and Wiberg X  X  algorithm[1] have been utilized for the structure from motion (SFM), in which a 3-dimensional surface model of target object is esti-mated from a sequence of 2-dimensional images. In machine learning (ML), this kind of iterative algorithm is generally formalized as the EM algorithm. In fact, it was shown that PPCA (probabilistic PCA) with EM algorithm can deal with incomplete data[4]. Also in NMF, some iterative algorithms that alternately estimating missing values and factorizing a matrix into two low-rank ones h ave been recently developed[6]. While this iterative estimation approach works fine if the missing part is relatively small, the con-vergence property and solution quality become drastically worse as the missing portion becomes larger. Besides, even if the missing data has some pattern or structure which contains information of latent low-dimen sional spaces, it does not have any mechanism to utilize the information. In summary, these conventional approaches implicitly assume small and randomly generated missing elements.

In contrast, our method utilizes only existing elements of the matrix data, which means it is not necessary to fill the absent el ements with constants, nor to estimate them alternately. In addition, it takes advantage of the pattern of missing data, based on the idea that existing (i.e. not missing) elements are roughly linear to their corresponding latent vectors. missing elements, we introduce a set of Boolean indicator variables { q i,j } to specify whether each element is existing or missing. That is to say, Now we pursue two goals at the same time: 1. Obtain a set of n -dimensional row latent vectors X =[ x 1 ,..., x M ] by reducing 2. Obtain a set of m -dimensional column latent vectors Z =[ z 1 ,..., z N ] by reduc-where, n&lt;&lt;N  X  D and m&lt;&lt;M  X  D . It should be noted that our purpose is not to approximate or reconstruct Y by the product of X and Z , but to embed the row and column vectors of Y to low dimensional latent spaces respectively. It can be called as simultaneous dimensionality reduction or co-embedding .

We can give another view to this problem. First, assume that a measurement y i,j is generated by an unknown function of an observer X  X  latent state x i  X  X  n and an item X  X  latent state z j  X  X  m , i.e., where e i,j is the noise. Our goal is to estimate sets of { x i } ( i =1 ,...,M )and { z j } ( j =1 ,...,N ), when a partial set of { y i,j } is given. Note that function g itself is not necessarily estimated.

Now we make an assumption that the presence of an observation y i,j has a locality as to z j . Roughly speaking, this assumption states  X  X f there exists i such that q i,j = q i,j =1 ,then
While this assumption seems to be very restrictive, there are many problems which hold this property in fact. For example, in th e case of mobile wireless device and access points mentioned in section 1, this assumptio n is expected to be valid because the device at a position x i can communicate only with APs in its neighborhood. It is also the case with SLAM (simultaneous localization and mapping) problem[8] in mobile robotics, where x i is the robot X  X  pose and z j is the j -th landmark X  X  position. Another example is the SFM (structure from motion) problem in computer vision, where x i is the relative spatial relationship between the camera and target object, z j is the j -th visual feature X  X  3D coordinates in the body frame, and y i,j is its 2D coordinates on the camera screen. Obviously, if j -th and j -th features are observed at the same time, they are expected to close to each other.

The assumption may be valid even in collaborative filtering. If we consider the Net-user watched two movies, they are likely to be in the same genre. In this section, we introduce the proposed method named LLACoE (locally linear align-ment co-embedding). 4.1 Basic Idea We consider the above assumption  X  X f there exists i such that q i,j = q i,j =1 ,then z j and z j are close to each other X  holds. Then, if q i,j =1 or y i,j is not missing, a linear approximation below is possible in its neighborhood, i.e., coordinates of z j .

Assume that x i implies observer X  X  latent state at time i , while z j implies j -th object X  X  state or position. Then the above approximation states that when observer X  X  state is x i , its observation data is formed by linear projections of all observable objects j  X  X  i into the observation subspace G ( x i ) determined by x i . In other words, each observation data at a time can be regarded as linear projections of a piece (fragment) of the whole world X  X  state into a low-d imensional perception space.
 aligning the pieces of observation data. Intuitive ly, it is similar to jigsaw puzzles or reconstruction of fragmentary fossils. Sin ce the alignment operation of each piece re-flects the observer X  X  state, x i is also expected to be reconstructed. In the remaining of this section, we will explain how to realize this rough idea. 4.2 Unsupervised Locally Linear Alignment Co-embedding First we consider reconstructing the column latent vectors { z j } . The assumption in the previous section means that z j is approximately linear (more strictly, affine) to y i,j if q i,j =1 . We use this local linearity property in a reverse way. That is to say, we think of approximating z j by an affine transformation of y i,j when q i,j =1 : where T i is an alignment transformation matrix common for y i,j ( j =1 ,...,N )as decide the final estimate of z j by averaging all the temporary estimates as: where,  X  q i,j = q i,j / M i =1 q i,j is the normalized observability indicator.
Now our main concern is how we can obtain the optimal set of alignment matri-ces { T i } ( i =1 ,...,M ). A reasonable way is to choose them so that {  X  z i,j } ( i = 1 ,...,M ) X  the estimates of z j for all i coincide with each other. This idea can be realized by minimizing the following cost function  X  aln with respect to { T i } : Although we omit the detailed derivation h ere, by introducing some auxiliary matri-written as: Note that this is a trace of a matrix quadratic form of T ,andthat Z =[ z 1 ,..., z N ] can be obtained as Z = VT .

As the minimization of  X  aln has a trivial solution T = 0 if there are no constraints, we impose a constraint : The solution of this constrained minimization is obtained as T opt =[ u 2 ,..., u m +1 ] where u 2 , u m +1 are the second smallest and ( m +1) -smallest eigenvectors of the generalized eigenvalue problem: Then we obtain  X  Z = VT opt .

Next we consider reconstructing the row latent vectors X =[ x 1 ,..., x M ] .As each alignment transformation matrix T i obtained in the previous step is supposed to characterize the corresponding row latent vector, estimates of { x i } are obtained by reducing the dimension of vec ( T i ) to n ,where vec ( T i ) is a column vector obtained by reshaping the elements of matrix T i . Note that { vec ( T i ) } contain no missing ele-ments, unlike the original observation matrix Y . We employed the simple SVD for the dimensionality reduction this time, while other advanced non-linear methods are also applicable.

The above cost function and the solution of column latent vectors { z j } originate from Verbeek and Roweis X  X  method for non-linear PCA and CCA[9]. However, they did not deal with the missing elements nor simultaneous dimensionality reduction of column and row vectors. Therefore, our method is different from theirs. 4.3 Regularization In actual applications, we can often improve the estimation results by introducing task-specific regularization terms into the original cost function. Especially, when the row latent vector x i corresponds to the observer X  X  state at time i , each pair of x i and x i +1 and corresponding pair of alignment matrices T i and T i +1 are expected to be close to each other. This soft constraint can be reali zed by introducing a regularization term for smoothing successive rows of X expressed as, where S is a matrix that computes the differen ces of pairs of successive elements in T . We minimize the weighted sum of cost functions  X  aln +  X  smo  X   X  smo instead of  X  aln under the same constraint. 4.4 Semi-supervised Co-embedding In some application domains, a semi-supervised problem setting where the partial la-bel information about row and column latent vectors are available beforehand is more natural. For example, in the case of wireless device and access points story, it is no wonder that exact positions of observer are partially available by GPS. LLACoE can be extended to a semi-supervised version in a straightforward way.
 We denote the labeled data of j -th column latent vector z j as z  X  j .Wealsodefinea Boolean variable  X  j to indicate whether the label information is available or not. That is to say, Then we define the cost function for the label information as: as, The whole cost function  X  sem ( T )=  X  aln +  X  smo  X   X  smo +  X  zlb  X   X  zlb can be easily minimized by solving a system of linear equations: Introducing the label information of row latent vectors { x  X  i } is similar to the above discussion, but much simpler. It is a general semi-supervised regression problem, where { vec (  X  T i ) } are input vectors. While there are many advanced methods for the semi-supervised regression, this time we solved it simply by the ridge regression or least-squares linear regression with Tiknov regularization.
 5.1 Experiment 1: Structure from Motion Task First, we applied the proposed co-embedding method to the structure from motion (SFM) task in computer vision domain, and compared it with conventional methods.
Assume that we look at a dodecahedron fro m a randomly chosen direction, identify all visible vertices, then obtain their 2-dimensional coordinates on camera image as the observation data [ y i, 1 ,..., y i,N ] ,where N =20 because a dodecahedron has 20 vertices. We repeat this procedure for M = 100 times, and obtain the observation data Y . The goal of this task is to reconstruct a 3-dimensional model of dodecahedron, or estimate 3-D coordinates { z j } of 20 vertices in the body frame.

For comparison, we first conducted this e xperiment under the condition that all ver-tices are always visible, i.e., Y has no missing elements (Fig.1 (a)). In this case, ordi-nary SVD is applicable. In fact, a perfect 3-D model is reconstructed by SVD as Fig.2 (a). Unsupervised version of the proposed method (LLACoE) also succeeds in recon-structing it as Fig.2 (b).

Next we impose the practical condition that observation elements of occluded ver-tices are lost (Fig.1 (b)). As a result, approx. 30 % of Y  X  X  elements are missing. In this case, we cannot use the ordinary SVD anymo re, because filling the missing elements with some constants is obviously inappropriate. So we applied two PCAMD methods, i.e., alternate least squares (ALS) algorithm and Wiberg X  X  algorithm[1]. The resultant models are shown in Fig.3 (a)-(c). Although al l three methods reconstructed the model successfully, LLACoE is much faster than others because it does not need iterations. 5.2 Experiment 2: Mapping and Localization for Wireless Devices Next we applied LLCoE to a simultaneous localization and mapping (SLAM) problem with wireless devices in a simulated environment.

In this task, we assume that 564 access points ( APs) are distributed in a virtual cam-pus, and a walking observer with a wireless client device records the relative positions of detected APs periodically. Fig.4 illustrates the simulated environment (research cam-pus) and the ground truth map of APs. Some APs X  IDs are indicated for later evaluation. Fig.5 illustrates the ground truth trajector y of the observer and observation points. Num-ber of observation points is 310. In this task, the row latent vector x i ( i =1 ,..., 310 ) is the observer X  X  state (i.e., position and heading direction), whereas the column latent vector z j ( j =1 ,..., 564 ) is each AP X  X  position. Observation data y i ,j is computed from a very noisy bearing and range information. For example, Fig.6 (a) and (b) are a ground truth map and a observed relativ e positions of detected APs at one time. We generated the observation data with: where d i,j is the distance between i -th observation point and j -th AP. As a result, the ratio of missing elements in Y becomes approx. 97 %. Fig.7 shows the distribution of missing (gray) and existing (white) elements in Y .
 Unsupervised Localization and Mapping. First we applied the unsupervised version of LLACoE to estimate X and Z from Y without the smoothing regularization. Al-though the map of APs ( Z ) in Fig.8(a) is largely distorted, we can see the approximate relative relationships with neighbors are reconstructed to some extent. On the other hand, the trajectory of observer ( X ) in Fig.9(a) is reconstructed very well.
Then we added the smoothing regularization term  X  smo described in section 4.3. We set the weight parameter value as  X  smo =0 . 2 here. The results are shown in Fig.8(b) and Fig.9(b). We can see that the reconstruction of X (map of APs) is much improved.
For comparison, we applied latent semantic indexing (LSI) method as in [2] to the data. To do so, we converted the range measurements into signal strengths by a mono-tonically decreasing function. The results are much worse than those of LLACoE as shown in Fig.8(c) and Fig.9(c).
 Semi-supervised Localization and Mapping. We also tested the semi-supervised ver-sion of LLACoE in this experiment. We gave exact positions of 7 APs as the label in-formation z  X  j , which are emphasized by circles in F ig.4. Partial label information of observation points x i were also provided within the  X  X reas X  indicated in Fig.5.
Fig.10 (a) and (b) show the obtained map and trajectory, respectively. Owing to the label information, the absolute accuracy of estimated positions is much improved.
For comparison, we applied Pan X  X  co-localization algorithm based on graph regular-ization [2] to the range measurements. The resultant map and trajectory are shown in Fig.11 (a) and (b). Unfortunately, it completely failed in this experiment. In this paper, we proposed a co-embedding method to embed the row and column vectors of an observation matrix data whose large portion is structurally missing into low-dimensional latent spaces simultane ously. The proposed method outperforms the conventional methods based on EM algorithm and ALS in computational cost and sta-bility, because it is solved by eigendecomposition of a symmetric matrix. We also a semi-supervised version of the proposed co-embedding method, which is solved by a system of linear equations. In the experiment, we evaluated the method on two kinds of tasks, and compared it with other methods. In future, we are going to apply this method to a variety of problems.

