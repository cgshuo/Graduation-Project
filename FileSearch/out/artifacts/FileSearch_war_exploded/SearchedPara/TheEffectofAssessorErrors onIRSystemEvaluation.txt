 Recent efforts in test collection bu ildin g have focused on scaling back the nu mber of necessary relevance judgments and then scaling up the nu mber of search topics. Since the largest source of variation in a Cranfi eld-style experiment comes from the topics, this is a reasonable app roach. How-ever, as topic set sizes grow, and researchers loo k to crowd-sourcing a nd Amazon X  X  Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robu stness of the TRE C Milli on Query track methods when some assessors make significant and systematic errors. We find that while averages are robu st, assessor errors can h ave a large effect on system rankings. Categories and Sub ject Descriptors: H.3.4 [Informa-tion S torage and Retrieval]: Systems and So ftwar e X  X er -formance evaluation , H.3.5 Onl ine Information Services General Terms: Experimentation, Measurement Keywords: assessor error, retrieval test collections
Since TREC began in 19 92 as the first large-scale use of poo ling to create test collections, a g reat deal of research h as focused on examining the quality of poo led test collections in spite of vi olations of the Cranfi eld assumptions [9, 17, 13 , 14 , 15] and on refining poo ling to reduce costs and /o r maximize quality [8, 12 , 11 , 3, 16 , 5]. The TREC Mill ion Query track [1] has emerg ed as a testbed for modern test-collection bu ilding methods, primarily those of Aslam and Pavlu [2] and Carterette et al. [ 6].

A primary motivation in many modern collection-bu ilding methods is to reduce the costs associated with making rel-evance judgments. Relevance assessors can b e expensive to hire, train, and u se, and p articularly in the academic c om-mun ity where fund ing may not be available for collection bu ilding, low-cost ( or zero-cost) methods have broad app eal, if not as yet broad app lication.

Recently, crowdsourcing a nd Amazon X  X  Mechanical Turk (MTurk) 1 have been used as sources of relevance judgments. https://www.mturk.com/mturk/welcome These app roaches have a very low cost per judgment, bu t they may have somewhat higher design costs and require add itional cost and effort to control for assessor error. In particular, if an MTurk worker X  X  only goal is to complete the task, the judgments may not loo k v ery different from ran-dom. Soboroff et al. has s imulated a family of cases of ran-dom assessor errors: the assessors know roughly how many relevant documents there are (give or take a stand ard devi-ation or two), bu t t hey pay little to no a ttention to which documents they are judging relevant [12 ]. Soboroff et al. assigned relevance to documents in the poo l rand omly, cre-ating a pseudo-rels that they used to evaluate systems. Su r-prisingly, evaluation results over the pseudo-rels correla ted significantly to evaluation results over the  X  X rue X  relevance judgments from the assessors.

That work had the relative luxury of deep poo ls of doc-uments, which may have resulted in emergence of patterns. But even an assessor making judgments at rand om would take a fair amoun t of time to make, say, 12 ,000 relevance judgments. Recent work on test collections s uggests that assessing more topics with fewer judgments each is a more cost-effective app roach, since the topics are a larger source of variance than the missing judgments [11, 7]. But t hat work assumes the judgments are in some sense  X  X erfec t X , i .e. that errors made by assessors have inconsequential variance and no bias. This is almost certainly not the case; assessor make mistakes du e to misund erstand ings of the task or documents, fatigue, boredom, and for many other reasons. These mis-takes surely have a larger impact on evaluation when there are only a few relevance judgments to begin with.

Furthermo re, most TRE C collections are bu ilt using trained relevance assessors. Practitioners and researchers using mod-ern collection bu ilding methods s uch as those pioneered in the TREC Milli on Query track may wish to create their own test collections using a vailable sources of labor, such as Amazon X  X  Mechanical Turk or other crowdsourcing meth-ods. Bailey et al. [ 4] found that assessors of differing task and d omain expertise could affect system rankings markedly; Kinn ey et al. [10] found that non-expert assessors judging domain-specific queries make significant errors affecting sys-tem evaluation. When assessors are not closely manag ed or highly trained, mistakes must be common.

Our goa l is to investiga te the effect of assessor errors by simulating different assessor X  X  rchetypes X  X n a low-cost large-scale test collection scenario. We propose several models of how assessors might make errors, then use the models to simulate assessors go ing through the process of making judgments. We focus specifically on the effect in the TRE C Mill ion Query track test collections, where there are many queries with very few judgments each.
We mined a large log o f assessor interaction d ata from the TREC 2009 Mill ion Query (MQ) t rack for patt erns of assessor behavior. The log contains a record for each judg-ment; each record consists of a timestamp, the query nu m-ber, the document ID, the assessor ID, and the judgment itself. We examined the first 32 judgments per topic and ex-cluded inter-ju dgment times in excess of 200 second s. Based on this data, and further on our experience manag ing rele-vance assessors for various projects, we present some types of errors assessor might make, some ways we can model them, and h ypothesized effects on evaluation in a MQ-type setting.
First we identify the following broad trends that can b e used to model assessor behavior. These formed the basis for some of our models below. 1. Time between judgments decreases slightly from the 2. It t akes less time to judge a nonrelevant document 3. Assessors can vary in their judging times (Fig. 1(c)) 4. Measured across non-overlapping bu t large sets of top-
These observations clearly indicate that assessors behave differently (i.e., there is variance du e to the assessor), and moreover that t here is interaction b etween assessor and d oc-ument. There may also be interaction b etween assessor and topic. Because each topic was judged only once we will not find evidence of that in agg regate, bu t we looked for a partic-ular type of interaction: topics that an assessor ga ve less at-tention to than n ormal, possibly du e to unh app iness with the documents they were asked to judge. To do this, we modeled time between judgments as a function of assessor, document judgment, and where in the sequence the judgment fell. We then loo ked for topics for which the actual judgment t imes were lower than those predicted b y the model across most of the sequence, specifically cases where at least 90 % of the judgments were faster than expected. There were about 30 such topics, and for all 30 a ll documents were nonrelevant.
Finally, we loo ked for evidence of autocorrela tion in judg-ments. We calculated the proportion of times an assessor judged a document relev ant cond itional on judging the pre-vious document relevant, and contrasted that with the pro-portion cond itional on judging the previous document non-relevant. We used a sub set of queries with roughly equal pro-portions of relevance so any effect would not be confound ed by differing nu mbers of relevant documents. Assessors are in fact more likely to make the same judgment t wice in a row: P ( j i = 1 | j i  X  1 = 1) = 0 . 22, whil e P ( j i = 1 | j is only 0.18. This difference is significant by a two-sample two-proportion test.

We will simulate different t ypes of assessors using models of systematic errors based on observations above. Given a set of (binary) judgments for a topic, we convert nonrelevant judgments to relevant and vice versa acc ording to a model.
If we are not careful, we can easily inject t oo much b ias into the evaluation. For instance, if we model an assessor that has a tendency to o verrate relevance by changing a judgment of nonrelevance to relevance with fi xed p robabili ty p , the expected effect is to simply increase the nu mber of relevant documents to pn , where n is the nu mber of judged documents. Increasing the nu mber of relevant documents in this way does not really add ress the question; some of those topics may v ery clearly have no relevant documents and even an optimistic assessor would not say they do. We therefo re tied model parameters to the nu mber of known relevant documents for each topic.

To do this, we define a model in terms of  X  X ackground  X  parameters modeling aver age behavior; these parameters are then adjusted by topic. This is easy to do using discrete distributions such as Bernoulli and Poisson d istributions. The parameters of both d istributions have natural conjuga te priors that allow upd ating based on existing judgments.
The Bernoulli distribu tion can b e seen as modeling the probability p that a document is relevant; the parameter p can b e modeled as having a Beta distribu tion. A Beta distri-bu tion is s pecified with two parameters  X ,  X  and h as mean  X / (  X  +  X  ). The posterior of a Beta rand om variable also has a Beta distribu tion (hence it is conjuga te); the posterior parameters are  X  + r,  X  +( n  X  r ), where n is a nu mber of ob-servations and r is the nu mber of positive observations. We can therefore view a Beta prior as a model of an assessor X  X  average probabili ty of judging a document relevant, and a Beta posterior using the nu mber of relevant documents r q among n q judgments for a topic q as a model of an assessor X  X  probability of judging a document relevant t o q .
Similarly, the Poisson d istribution can b e seen as model-ing the nu mber of documents that will be judged relevant in some sequence length based on a rate parameter  X  ;  X  can b e modeled as having a Gamma distribu tion. Like a Beta distribution, a Gamma distribu tion is s pecified with two parameters  X ,  X  , bu t its mean is  X / X  . The posterior of a Gamma rand om variable has a Gamma distribution with parameters  X  + r,  X  + n . We can therefo re view a Gamma prior as a model of an assessor X  X  rate of judging documents relevant, and a Gamma posterior using r q and n q as the rate adjusted for the topic.
Our baseline model is that an assessor makes judgments randomly. Real assessors of course do not make rand om relevance judgments; this model only serves as comparison to the more informed models below. Using the Beta distribu -tion as described above, the probability that a document will be judged relevant t o a topic q will be (  X  + r q ) / (  X  +  X  + n So, for example, an assessor modeled by prior parameters  X  = 2 ,  X  = 8 (expected to judge 20% of documents relevant) confronted b y a topic for which all 32 documents have been judged nonrelevant will have a 2 / 42 = 0 . 05 probability of judging each of those documents relevant. The effect of this is that t he nu mber of simulated relevant documents for each topic can b e expected to be  X  X ear X  the actual nu mber of relevant documents, bu t with enough n oise that evaluation results will change.

Our first realistic model is an unenthusiastic assessor, that is, the assessor is not interested in reading o r und er-stand ing documents and simply wants to complete the job. Judgments by this assessor may be characterized b y a pat-tern such as judging everything nonrelevant, or alternating judgments in some patt ern. Our discovery of topics that were completed faster than usual and with all nonrelevant judgments lends s upp ort to this model. For the un enthusi-astic assessor we do not supp ose that there is any particular probability model. The assessor just follows a fixed pattern, such as judging everything nonrelev ant or alternating be-tween relevant and n onrelevant judgments. This is in some sense the most biased model, i n that the simulated judg-ments have nothing to do with the actual judgments.
Our second model is the optimistic assessor; he or she takes an overly-broad view of the topic and end s up judging things relevant t hat are not. It is well-known from work by Harman [9] and Voorhees [13 ] that assessors do differ rea-sonably in their judgment of relevance; in this model and the one following we presume a view of the topic which most observers would consider beyond reasonable, for ex-ample judging relevance solely by the presence of certain terms or not correctly identifying a spam document as not relevant. We model optimistic assessors as being more likely to judge a nonrelevant document relevant. The model pa-rameters are ag ain Beta parameters  X ,  X  , and the probabili ty that a document will be judged relevant t o a particular topic is (  X  + r q ) / (  X  +  X  + n q ). However, for this model we can only change nonrelevant judgments to relev ant; we will not change any of the relevant documents to nonrelevant.
Conversely, we model a pess imistic assessor as taking an overly-narrow view of the topic and judging documents nonrelevant that should be considered relevant. The model is identical to the optimistic model, except t hat relev ant documents become nonrelevant with p robabili ty (  X  + ( n r )) / (  X  +  X  + n q ) ( which comes from treating a nonrelevant judgment as the  X  X ositive X  outcome).
 Another model we explore we call topic-disgruntled. This assessor chose a query for some reason (interest in topic, seemed easy), bu t t he documents turned out to be something else (different t opic, harder than expected). Dis-gruntled by the topic, the assessor begins to click through rapidly after the first few judgments. Aga in, the presen ce of topics completed faster than u sual l ends s upp ort to this model. The model is time-based. After k judgments, the assessor becomes disgruntled and judges the remaining doc-uments nonrelevant. The parameter is a  X  patience parame-ter X   X  ; after n q  X  judgments (which are identical to the X  X rue X  judgments), the assessor judges everything else nonrelevant. This assessor has a Gamma prior specified by parameters  X ,  X  (resulting in prior patience  X  =  X / X  ), and their poste-rior patience for a g iven topic is based on h ow many relevant documents there are:  X  = (  X  + r q ) / (  X  + n q ).
Similar in execution to the disgruntled model is the lazy/ overfitting assessor. The assessor sees a few very nonrel-evant ( or a few very relevant) documents at t he start of judging a nd un justly assumes all subsequent documents are likely to be the same. He or she begins rapidly entering judgments conforming to his early judgments. The model is implemented roughly the same as the topic-disgruntled model, except t hat it only ki cks in if the first n q  X  judgments are all nonrelevant ( or all relevant).

Another model is that t he assessor is fatigued. The as-sessor starts each d ay alert and att entive, bu t tires as time passes. Judgments become more rand om as a result. This is also a time-based model. We aga in b egin with a Beta prior. For this model, however, there is an assessor model with p a-rameters  X ,  X  as well as s eparate priors for each judgment; we will denote the parameters  X  i ,  X  i . The posterior for a given judgment will be (  X  i + r qi ) / (  X  i +  X  i + i ), where r nu mber of documents judged relevant t o q up to judgment i . For i = 0 we will set  X  i ,  X  i to zero. The first judgment for a topic, then, will be the same. For each subsequent judgment, the Beta parameters will grow w ith assessor pa-rameters  X ,  X  : after each judgment,  X  i and  X  i increase by  X  and  X  respectively. The effect is that after k judgments, the posterior probabili ty of judging a document relevant will be ( k X  + r qk ) / ( k X  + k X  + k ). As k increases, the assessor con-verges to judging every document acc ording to their prior probability  X / (  X  +  X  ).

Our fin al model is Markovian , that is, the assessor X  X  judgments are cond itional on p revious judgments. This could simulate an assessor who X  X eels bad X  X  bout judging too many nonrelevant documents in a row and thus takes a broader view of the topic over time, or one who takes a narrower view after judging many relevant documents in a row. The observation that assessors are more likely to make the same judgment t wice in a row supp orts this model.
To a nalyze the e ffects of particular types of systematic error, we simulated assessors judging documents in a TREC-like setting: an assessor is given a topic description and reads documents to judge whether they are re levant t o the topic. Since we are particularly interested in test collections with very many lightly-judged topics, we used the TRE C 2009 Milli on Query track data a s the starting point for our simulations. Before describing the simulation p rocedu re and results, we briefly describe the track.
The Mill ion Query track was designed to stud y the use of low-cost evaluation methods in the TREC setting. It produ ces test collections that consist of a large nu mber of lightly-judged topics. Judgments in the Mill ion Query track are either not relevant, related (bu t not relev ant), relevant, or highly relevant. In 200 9, 638 topics received a total of 34 ,534 judgments (54 per topic on average), of which 26 % were either relevant or highly relevant. There were 95 topics for which n o relev ant documents were found .

The low-cost methods used by the track attempt to ta r-get judgments that are go ing to be more useful in evalua-tion. The Milli on Query track uses two methods to select documents to judge. One (statAP) is an app roach b ased on statistical sampling, i n which each judged relevant docu-ments is taken to be representative of some popu lation of rel-evant documents in the same X  X egion X  X rom which it was sam-pled [2]. The other (MTC) is an algorithmic app roach that weighs documents acc ording to how inform ative a judgment to them is expected to be; after each judgment, it recom-pu tes the weights given the new inform ation and p resents the top-weighted document for judging [6].

Errors in judging can h ave unp redictable effects in b oth methods. In statAP, an erroneous judgment of relevance can h ave a major impact on the estimated nu mber of rele-vant documents for the topic, particularly if the judgment is to a document t hat has a low probability of being sam-pled. Conversely, an erroneous judgment of nonrelevance will cause the nu mber of relevant documents to be und eres-timated. In MTC, an erroneous judgment can result in the algorithm taking a n entirely differen t path.

The two different app roaches to selecting judgments have differen t app roaches to evaluation that are based on d ifferent assumptions. The sampling a pp roach takes each judgment of relevance as representative of a set of relevant documents; this set is used to calculate an unb iased estimator of aver-ag e precision. The algorithmic app roach can take one of two tacks: it can either bound the differences in average preci-sion b etween pairs of systems or it can compu te a probabili ty that the difference in average precision is less than zero over the space of possible judgments that could be made to un -judged documents. The probabilistic app roach is generally more useful, and it has the advantag e of being a ble to pro-du ce an estimate of average precision called  X  X xpected aver-ag e precision X  (EAP). Unlike the statMAP estimate, EAP is highly biased, bu t because it is meant for pairwise compar-isons the bias can b e expected to cancel out. Plots of EAP typically have very low values compared to plots of statAP. We have decided to limit our focus to the effects on statAP. There are two reasons: first, because statAP estimates X  X ook like X  stand ard average precision, i t is easy to see how er-rorful judgments are causing errors in the estimates. EAP estimates loo k v ery different from average precision, and b e-cause it is already v ery biased, changing the judgments will not necessarily cause obvious differences in their values. The second reason to prefer statAP is that MTC cann ot effec-tively be simulated in the Mill ion Query track data. If one judgment changes, it is likely that some future document selected for judging will be one that we do not already have a judgment on.
Some und erstand ing o f statAP is necessary to und erstand how an errorful judgment affects the estimate. statAP is a method for sampling a set of documents S to be judged, then using those judgments to estimate average precision. The statAP estimate is calculated as: where x d is the relevance of document d (1 for relevant, 0 for not relevant) ,  X  d is an inclusion p robability calculated for sampling, b R is an estimate of the nu mber of relevant documents, and \ prec @ r ( d ) is an estimate of the precision at the rank at which d ocument d app ears. The estimates of precision and the nu mber of relevant documents are: The ratio x d / X  d can b e thought of as the nu mber of relev ant documents that x d is representative of in the same  X  X egion X  of documents with similar inclusion p robabili ties. A lower  X  d gives greater weight to a relevant document, i ncreasing the estimated nu mbers of relevant documents compared to a relevant document with a higher  X  d . (a) Judgments from a rand om assessor give a small (bu t sig-nificant) rank correlation;  X  = 0 . 35. (b) Judgments from an un enthu siastic assessor give a small (bu t significant) rank correlation;  X  = 0 . 33.

The simulation p roceeds as follows: for a g iven topic, we start with the sequence of judgments in the same or-der they were originally made (this information is provided in the  X  X ullrels X  file distributed with the Mill ion Query track data). We alter the judgment acc ording to each of the mod-els above. For those models that involve rand om sampling, we perform 25 trials on each judgment for each topic. We used increasing powers of two for parameter values, i .e.  X  and  X  ranged from 1 to 1024 ind epen dently. When com-plete, we have an errorful  X  X rels X  file that we can u se to evaluate the Milli on Query track systems with statAP.
After re-evaluating Milli on Query track systems, the sim-plest app roach to determining the effect of errors is to mea-sure how w ell the new evaluation correlates to the  X  X rue X  evaluation resulting from using the original relevance judg-ments. Kend all X  X   X  rank correlation is widely-used for this. Kendall X  s  X  is a function of the nu mber of system pairs that swap b etween two rankings. The more swaps, the lower  X  is; when  X  = 1 the rankings are identical.
System evaluation results based on the judgments from the first t wo a ssessor models X  X and om judging with p rior parameters  X  = 1 ,  X  = 8 a nd an un enthusiastic assessor that alternates between nonrelevant and relevant judgments to stay amused X  X re shown in Figure 2. In b oth cases the Kendall X  s  X  correla tion to the official ranking is around 0 . 34, and remains consistently around 0 . 34 no matt er what t he prior parameters are and n o matter what judging pattern is used (among those we tried). This can b e thought of as a baseline for an assessor who is not actively malicious bu t is not interested in making a n effort.

Figure 3 ill ustrates altered system rankings based on the other models for selected parameter values. For models with rand om sampling, error bars indicate the distribution of MAP estimates observed over 25 trials.

The optimistic and p essimistic models give very different results even when the prior parameter give equal probabili ty of changing the judgment. Rank correlations based on op-timistic judgments quickly degrade, while rank correlations based on p essimistic judgments degrade much more slowly. Figures 3(a) and 3(b) demonstrate this for  X  = 1 ,  X  = 16 (for the optimist) and  X  = 16 ,  X  = 1 (for the pessimist). Roughly the same nu mber of j ud gments changed in b oth cases, bu t the effect on p erform ance is much worse when those changes create more re levant documents than when they create more nonrelevant documents. With the pes-simistic model, in fact, the correlation is nearly perfec t; the scores have simply shifted d ownward.

The disgruntled and lazy models are similar to the pes-simistic model in that t hey result in fewer relevant docu-ments than exist in the  X  X rue X  judgments. However, they produ ce worse results in general. In the disgruntled case (Fig. 3(c), despite labeling roughly the same nu mber of doc-uments relevant as the pessimist, the  X  correla tions are on average 10 % lower. The lazy assessor (Fig. 3(d)) actually found many more relevant documents than either the pes-simist or the disgruntled assessor with the same prior pa-rameters, bu t app arently found  X  X orse X  relevant documents than the pessimist, as its  X  correla tion is lower.
The fatigued and Markovian models are rather similar to each other in h ow they rerank systems. The Markovian model produ ces somewhat more pronoun ced effects on some of the systems, resulting in a lower  X  correla tion. Both result in more documents being judged relevant.

One conclusion we draw from these results is that it is generally better to und erestimate relevance than to overes-timate it. The models that result in fewer documents be-ing judged relevant X  X he pessimist, the disgruntled, and the lazy X  X enerally produ ce more accurate rankings of systems than those that result in more documents being judged rel-evant. This sugg ests that low-cost evaluation methods are sensitive to noise in the relevant documents. Among models that result in fewer relevant documents, the pessimist pro-du ces the best rankings overall, though the system scores are strongly biased downward.

Of course, we do not conclud e from this that assessors should b e trained to be pessimists. This is an abstract model; the altered judgments had n o relationship to any properties of the actual documents apart from their original relevance judgments. (a) Optimistic assessor (  X  = 1 ,  X  = 16) judges many more documents relevant.  X  = 0 . 72 (b) Pessimistic assessor (  X  = 16 ,  X  = 1) judges many fewer documents relevant.  X  = 0 . 92 (c) Disgruntled assessor (  X  = 1 ,  X  = 16) gives up early.  X  = 0 . 81 (d) Lazy assessor (  X  = 1 ,  X  = 16) assumes first few judgments indicate the rest.  X  = 0 . 9 (e) Fatigued assessor (  X  = 0 . 05 ,  X  = 1) becomes more rand om over time.  X  = 0 . 9 (f) Markov assessor (  X  = 1 ,  X  = 16) makes each judgment based on the previous one.  X  = 0 . 84 appropriate).
The results presented above represent relatively goo d p a-rameter settings for each of the models. Depending o n the prior parameters, the results can b ecome quite bad. Fig-ure 4 shows contour maps demonstrating the change in  X  with p rior parameters  X  and  X  in the optimistic and p es-simistic models. The optimist is best (ind icated by lighter shading) when  X  is low and  X  is very high, which is when it is least likely to incorrectly judge a document relevant. Its performance quickly degrades from there. The pessimist is best when  X  is low and  X  is high, which is when it is least likely to incorrectly judge a document nonrelevant, bu t it maintains goo d p erformance un til  X  is high and  X  is low. Note that the optimistic is much d arker in much more of the space than the pessimistic, indicating substantially lower  X  correla tions for any parameter settings.

Other models are similar to these two. Those that produ ce more relevant documents than originally existed in the rele-vance judgments tend to exhibit a faster drop-off in perfor-mance when parameters move away from the low-probability regions. Those that produ ce fewer relevant documents than originally existed tend to exhibit a slower drop-off.
The effect of assessor errors is to a dd unp lann ed vari-ance and b ias into the evaluation. This increases the cost indirectly X  X hough the judgments can b e made for the same cost, the cost of the errors they introdu ce add s up . Thus it may be worth expending some extra cost to ensure that errors made by assessors cann ot cause too much d amage in the agg regate. Here we consider some simple approaches to adjust or correct their errors.

Since we are interested in cases where the assessors may be distributed around the world rather than p resent in person, and cases with many more assessors judging fewer topics each, we do not want t o spend too much time on solutions that involve a g reat deal of interaction with the assessors.
One possible solution is to have some documents judged multiple times. The cost clearly depen ds in p art on h ow many rejudgments are made and h ow documents are chosen for rejudging, bu t it also depends on h ow the extra judg-ments are incorporated into the evaluation. Some of the differen ces observed in the extra judgments will be du e to reasonable disagreem ents about relevance rather than errors . While such d ifferences could p ossibly be resolved by adjudi-cation, this essentially add s another assessor X  X ne who must be able to make a decision b ased on confl icting evidence X  X o the process, and that carries significant cost.

One alternative is to use a simple process like majority vote. If rejudgments converge on a particular decision, it is more likely that t he original judgment was in error. This requires more dup licated effort, though, especially since re-judgments themselves are not immun e to error.

Along similar lines, since pessimistic models seem to hu rt performance less, we could require a supermajority of pos-itives to call a rejudged document relevant. Thu s it would take two o f two judgments, or two o f three judgments, being relevant before we are confid ent in concluding that a docu-ment really is relevant. This would only app ly in the cases we actually decide to have a document rejudged; because of that and the add itional cost in dup licated effort, the choice of documents to have rejudged must be made very carefully.
We hypothesize that for statAP evaluation, documents with lower inclusion probabili ties are better cand idates for rejudgment. These documents, if erroneously judged rele-vant, can have a much greater effect on the evaluation than documents with higher inclusion p robabili ties. The simu-lations bear this out: those models that resulted in worse ranking performance had lower inclusion p robabili ties on av-erage among the judgments that changed. For example, the average inclusion p robabili ty among documents that t he op-timist in Figure 3(a) judged relevant was 0.09 , while the av-erage inclusion p robabili ty among documents the pessimist in Figure 3(b) judged relevant was 0.12.

To test the effect of rejudging low probabili ty documents, we ran a second simulation to rejudge a few documents with low inclusion probabili ties from a prior simulation. In this case, the simulated assessor uses the same model as the orig-inal, bu t only judges documents with inclusion p robabili ty less than 0 . 01. The new judgments are then merg ed with the existing judgments using the supermajority app roach: any document t hat has been judged relevant t wice is con-sidered relevant, while the rest are nonrelev ant. Since 90 % of the judgments have inclusion p robabili ties greater than 0 . 01, most will not change, and most of the judged relevant documents will stay relevant. Figure 5: Kendall X  X   X  decreases linearly (within the given error bars) as the number of errorful topics among in an evaluation increases. Average  X  does not fall below 0.9 un til 232 of the original judged topics have bee n replaced with errorful versions.
The e ffect of this on the optimist is a small i mprovement in the  X  correla tion from 0.72 to 0.75 , which may not be worth the cost of the extra judgments. However it seems that this could p otentially be a useful starting point for selecting documents for rejudging and incorporating the rejudgments into the evaluation.
Another app roach to hand ling erroneous errors is to treat relevance judgments as a quality-assurance problem. Given an estimate of the permissible nu mber of badly-judged top-ics, we can sample the topics that have been judged and check whether those seem to be errorful i n order to estimate the total nu mber of problem cases. If the nu mber is above what is permissi ble, we can impose tighter controls for a brief time un til judging seems to be going smoo thly aga in.
We investigated the permissible nu mber of bad topics by starting with the full evaluation over the original judgments and gradu ally replacing topics with their errorful doub les from the models above. The goa l was to see how many  X  X ad X  topics we could inject into the evaluation b efore we reached a  X  correla tion b elow 0.9, the threshold at which we might feel un comfortable with the ranking.

The result is s hown in Figure 5. The decrease in  X  is roughly linear in the nu mber of errorful topics, bu t it does not drop b elow 0 . 9 o n average un til 232  X  X ver 40 % of the total nu mber of topics X  X ave been replaced. This sugg ests that statAP is actually fa irly robu st t o errors in judgments, at least in terms of its abili ty to rank systems. An evaluation could proceed for a fairly long time before tighter controls would n eed to be enforced.
We argue that as test collection construction continu es to take lower-cost routes away from well-trained, manag ed assessors to crowdsourcing o r cheaper, faster assessors, the errors in evaluation estimates will have to be quantified and potentially adjusted for the errors that will almost certainly occ ur in judging. We presented eight models of possible errors and showed how each affects an estimate of average precision. We proposed two possible means to adjust for er-rors: 1) have certain documents selected for rejudging, then use a voting a lgorithm to combine the judgments; 2) esti-mate how many problem cases there seem to be to determine whether judging needs to be more strictly observed.
As a next step, we plan to und ertake a true crowdsourcing experiment using Mechanical Turk to investigate the degree to which the behaviors we posit actually occ ur in that pop-ulation and the effect resulting errors have on evaluation. Beyond that, future work must consider that t hese errors will seldom happ en ind epen dently. Most evaluations will be affected by some mixture of errors, and the parameters of that mixture could have a substantial effect on b oth the evaluation and adjustments.
