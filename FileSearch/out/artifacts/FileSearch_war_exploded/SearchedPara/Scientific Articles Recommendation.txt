 We study the problem of recommending scientific articles to users in an online community and present a novel matrix factorization model, the topic regression Matrix Factorization (tr-MF), to solve the problem. The main idea of tr-MF lies in extending the matrix factorization with a probabilistic topic modeling. Instead of reg-ularizing item factors through the probabilistic topic modeling as in the framework of the CTR model, tr-MF introduces a regres-sion model to regularize user factors through the probabilistic topic modeling under the basic hypothesis that users share the similar preferences if they rate similar sets of items. Consequently, tr-MF provides interpretable latent factors for users and items, and makes accurate predictions for community users. Specifically, it is effec-tive in making predictions for users with only few ratings or even no ratings, and supports tasks that are specific to a certain field, neither of which is addressed in the existing literature. Further, we demonstrate the efficacy of tr-MF on a large subset of the data from CiteULike, a bibliography sharing service dataset. The proposed model outperforms the state-of-the-art matrix factorization models with a significant margin.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.5.1 [ Pattern Recognition ]: Models X  Structural Matrix Factorization, Probabilistic Topic Modeling, Recommender System
With the rapid development of social networks, recent years have witnessed fast booming social network sites for academia services, such as CiteULike 1 and Mendeley 2 . These social network sites http://www.citeulike.org http://www.mendeley.com allow researchers to create their personal libraries for the on-line papers that interest them and to share the libraries with other re-searchers. This makes recommender systems helpful for researchers to find interesting papers. The recommender systems are able to discover and recommend desired scientific articles that researchers have not yet noticed. Thus, it provides a new way for researchers to find interesting articles other than just simple reference tracking or keyword searching.

The traditional approach to recommendation for websites is based on collaborative Filtering (CF), which is a process of evaluating items through the opinions of other users. Among the existing CF algorithms, matrix factorization models play an important role. Given a rating matrix, the main idea of matrix factorization models is to predict the missing entry ( i,j ) with an inner product between latent feature vectors for user i (often referred to as user factors) and for item j (often referred to as item factors). While typically the matrix factorization models suffer from the overfitting problem due to the imbalance of the real-world datasets, a feasible solution to this problem is to regularize the latent feature vectors in the ma-trix factorization models through appropriate priors, e.g., Gaussian prior [22] and other flexible priors [23, 12]. Recent work [1, 24, 28] improves rating predictions through regularizing item factors with item topic proportions which are captured by the topic modeling. For example, [24] uses the topic proportions in place of the item factors, and [28] considers the topic proportions as the prior mean of item factors.
 However, these models suffer from several limitations. First, They have trouble in making accurate predictions for users who have only few ratings, which is also a well-known notorious issue for most existing recommender systems. For most of the matrix factorization models, the latent feature vectors for users with only few ratings are close to the prior mean and, consequently, the pre-dicted ratings for the users are seriously influenced by other users. Thus, inappropriate recommendation is provided by such recom-mender systems, which is typical for scientific article recommender systems. For example, popular articles related to the basic theory of chemistry would be undesirably recommended to computer sci-ence researchers who have no interest at all in chemistry but only had few ratings in history.

This is related to the well-known cold-start problem in recom-mender system literature. In general, there are two types of cold-start problem for recommending scientific articles. The first is for new articles that no one has yet rated. The second is for new users whose preferences are almost unknown for a recommender system. The existing literature [28] for recommending scientific articles fo-cuses on the former when they address the cold-start problem. We focus on the latter, which makes our work different from the exist-ing literature on this problem.
Second, they do not effectively support tasks that are specific to a certain research field. Recent work regularizing item factors through Latent Dirichlet Allocation (LDA) [9] could provide a topic representation of articles, but may not effectively support tasks such as  X  X ecommending papers that my papers should cite X . For exam-ple, for a biological scientist who is interested but has no expertise at all in data mining, she might wish the recommender systems to support the tasks such as  X  X ecommending popular papers on data mining with potential applications to biological science".
To address these difficult issues, we present tr-MF, a novel ma-trix factorization model for the task of recommending scientific ar-ticles to users in an online community. The key idea of tr-MF is to regularize item factors through a Gaussian prior, but also to regular-ize user factors through LDA. Given a bag-of-word representation for article, the idea in LDA is that articles are represented as the topic proportions over the latent topics, where each topic is char-acterized by a distribution over the words of the articles (here it is emphasized that the article topic proportions are just used for the estimation of the user topic proportions and are not direct contri-butions to the item factors). Consequently, we may generate user topic proportions by averaging the rated article topic proportions. Treating user topic proportions as the prior mean of the latent fea-ture vectors for users, we assign similar priors to the latent feature vectors for the users who have rated the similar sets of articles. This special way of regularization on user factors has a strong effect on the users with only few ratings as the predictions for these users are more related to the content of the articles. Furthermore, it also makes this method distinct from the recent work [24, 28] that only regularizes item factors through LDA.

In addition, the topic representation of users in tr-MF provides a better interpretability in helping explain recommendations to users in an application than most of the existing literature. LDA is a well-known dimensionality reduction technique with the proper under-lying generative probabilistic semantics. Thus, user factors directly reflect the interest of the users and provide a good explanation of the recommendations. Further, the topic representation of users helps discover other users whom this user might wish to commu-nicate with due to similar interests. Sometimes, knowing whom to focus on is more important than knowing what to focus on. This also helps the communications within a community.

We conduct the comprehensive evaluations to investigate the per-formance of the tr-MF model. The experimental results on the Ci-teULike dataset demonstrate that tr-MF achieves a significant im-provement over the state-of-the-art methods on the performance evaluations. Moreover, the tr-MF model is applied to discovering the latent topics for a given user and finding the top popular articles for a given topic. The comparison with the state-of-the-art methods demonstrates the promising topic discovery capability of the tr-MF model.
In this paper, we introduce the topic modeling to a matrix factor-ization method and adopt a bag-of-word representation for items. Suppose that we have N users, M items, and integer rating values in { 0 , 1 } . Let R ij represent the rating of user i for item j : R means that user i includes article j in her library; R ij that the article j is not included in user i  X  X  library. Let U  X  IR and V  X  IR K  X  M be the latent feature matrices of users and items, respectively, with column vector U i and V j representing the K -dimensional user-specific and item-specific latent feature vectors, respectively. Specifically, in the field of scientific article recom-mendation, every item represents a specific scientific article and its words form a document. A word is denoted as a unit-basis vector w of size W with exactly one non-zero entry representing the membership to only one word in a dictionary of W words. A document is a collection of P words X  occurrences denoted by w = ( w 1 ,w 2 ,...,w P ) . A training set of M items corresponds to M documents.
In this section, we give a brief review of the matrix factoriza-tion models as well as the probabilistic topic models as part of the related work.
Various matrix factorization (MF) methods haven been proposed for collaborative filtering [27, 3, 18]. In the early work, Singular Value Decomposition (SVD) is used for the low rank approxima-tion based on minimizing the sum-squared distance. It finds the matrix  X  R = U T V of the given rank which minimizes the sum-squared distance to the target matrix R . Furthermore, considering that the rating matrix is very sparse in real-world applications, the weighted schemes are proposed in [26]. The generalized weighted scheme aims at approximating R with U T V and minimizing the objective of a weighted Frobenius loss function as follows. where c ij is a weighting parameter. In general, it is assigned to 1 for the observed entries and to 0 for the unobserved entries.
To prevent overfitting, a regularization term is appended to the above objective function L ( U,V ) : where  X  U and  X  V are the regularization parameters.

This MF for collaborative filtering is a special case of the regu-larized low-rank approximation in [22]. It may be generalized to a probabilistic framework and the generative process is described as follows: 1. For user i , draw U i  X  X  (0 , X   X  1 U I K ) . 2. For item j , draw V j  X  X  (0 , X   X  1 V I K ) . 3. For rating matrix entry ( i,j ) , draw
When the MF models are applied to tackling the one-class col-laborative filtering problems [20, 14], such as page visitation, web-page bookmarking, and news article recommendation, the naive scheme for c ij assigned as "1" to an observed example ( R and as "0" to an unobserved example ( R ij = 0 ) makes mistakes, since the unobserved examples are uncertain. There are two pos-sible explanations for this uncertainty in scientific article recom-mendations. First, an article is of the user X  X  interest but not read by the user; second, an article is read by the user but not of his or her interest. Typically an observed example indicates that user i likes article j .

To address this issue, low weights on the error terms are used in [20]. In this work, we use the same strategy setting with different non-zero weighting parameters c ij for different ratings R where a and b are the tuning parameters satisfying a &gt; b &gt; 0.
Topic models [13, 9, 8] are based upon the idea that documents are mixtures of topics, where each topic is captured by a distribu-tion over words. The topic probabilities provide an explicit low-dimensional representation of a document. Hence, they have been successfully used in many domains, such as text modeling [8, 10], collaborative filtering [24, 2, 28], and content-based image retrieval [4, 29].

The topic model discussed in this work builds on Latent Dirich-let Allocation (LDA) [9], which is a powerful generative frame-work for modeling words in documents. LDA assumes that the dimensionality K of the Dirichlet distribution is known and fixed in advance. The word probabilities are parameterized by  X  = (  X  1 ,..., X  K ) . The generative process of LDA is described as fol-lows: 1. Select a document d m with probability P ( d m ) . 2. Draw topic proportions  X  m  X  Dirichlet (  X  ) . 3. For each word w p in d m
Topic models are powerful generative models for modeling words in documents and allow a document to exhibit characteristics from multiple topics. Thus, given a bag-of-word representation for items or users, it is natural to combine topic models with matrix factor-ization methods to improve rating predictions. Recent work [24, 2, 28] has proven this point. These models improve rating predic-tions through regularizing item factors with item topic proportions which are captured by the topic modeling. For example, [24] uses the topic proportion  X  j in place of the item factors V j considers the topic proportion as the prior mean of item factors. However, these models suffer from several limitations. First, They have troubles in accurate predictions for users who have only few ratings, which is also a common drawback for most existing recommender systems. For most of the matrix factorization mod-els, the latent feature vectors for users with only few ratings are close to the prior mean and, consequently, the predicted ratings for the users are seriously influenced by other users. Thus, inap-propriate recommendation is provided by such recommender sys-tems. This is typical for scientific article recommender systems. For example, popular articles related to the basic theory of chem-istry would be undesirably recommended to computer science re-searchers who have no interest at all in chemistry but only had few ratings in history.

Second, they do not efficiently support tasks that are specific to a certain research field. The above recent work regularizing item factors through LDA could provide a topic representation of arti-cles, but may not efficiently support tasks such as "recommending papers that my papers should cite". For example, for a biological scientist who is interested but has no expertise at all in data mining, she might wish the recommender systems to support the tasks such as "recommending popular papers on data mining with potential applications to biological science".
 To address these difficult issues, we propose the tr-MF model. By regularizing user factors through LDA, users that have rated similar articles have similar prior distributions for their latent fea-ture vectors. This way of regularization has a strong effect on users who have only few ratings. In addition, another advantage of tr-MF is that it provides a topic representation of user factors. Once we obtain the interpretable user topics, it is natural to represent item factors as popularity scores to user topics. By ranking these popu-larity scores for a given topic, tr-MF is capable of supporting tasks that are specific to a certain filed.
Under the assumption that users who have rated similar sets of articles have similar preferences, we introduce a regression model to regularize user factors through LDA.

Our method is based on placing the constraint prior mean on the latent feature vectors for users. In particular, we first obtain the set of item topic proportions  X  = (  X  1 ,..., X  M ) through LDA. Consequently, we generate user topic proportions  X   X  i for user i : where I is the observed indicator matrix with I im taking on value 1 if user i has rated item m and 0 otherwise. Then, we define the latent feature vector for user i as: here Y i is considered as the offset added to  X   X  i to obtain the latent feature vector U i . Specifically, when Y i  X  N (0 , X   X  1 tain the distribution of U i , Thus,  X   X  i now is considered as the prior mean of the latent feature vector U i . Specifically, in the PMF model [22], U i and Y because the prior mean is fixed to zero.

In the process of filtering, we place a zero-mean Gaussian prior to the latent feature vector for item j , We now define the rating that user i gives item j as follows: For an easy reference, the graphical model of tr-MF is shown in Figure 1. Further, the particular generative process of tr-MF is de-scribed in Algorithm 1.

Algorithm 1: Generative process of tr-MF for document m = 1 ,...,M do for item j = 1 ,...,M do for user i = 1 ,...,N do for user i = 1 ,...,N do
The key property in tr-MF lies in how the user factor is gener-ated. It is shown in Eq.(5) that the user factor U i is a mixture of two sources: (1) the user topic proportions  X   X  i which is obtained by averaging the rated item topic proportions; (2) the offset vector Y which reflects the influence from other users X  ratings. This per-spective actually accounts for the process of making a prediction: the recommender system first learns the knowledge from the user X  X  rated items and then combines the suggestions from other users with what it learned through the collaborative filtering to form the accurate prediction. Therefore, the predictions count more on the content of their rated items. Herein, our model recommends the articles concentrating on the area of the interest of the researchers.
We aim at maximizing a posterior of U , V , and  X  given the train-ing data for a precise prediction, which is equivalent to maximizing the log likelihood of U , V ,  X  , and R given the hyper-parameters  X  ,  X  v , and  X  . We adopt the Expectation Maximization (EM) al-gorithm to learn the parameters of tr-MF. The likelihood is denoted as L : L =  X  =  X  X  where indicates the Hadamard product of two matrices, (  X   X  1 ,...,  X   X  N ) , and the matrix C . For simplicity,  X  is assigned to 1 in tr-MF. Notice that C R = R ; setting the derivatives with respect to U and V to zero, we obtain the updating equations for U and V in a vectorization form: where  X  V = I N  X  V ,  X  U = I M  X  U , D 1 = diag ( vec ( C D 2 = diag ( vec ( C )) .  X  is the Kronecker product of two matrices, vec (  X  ) is the vectorization function mapping a matrix to a column vector stacked by the columns of the matrix, for example vec ( U ) = ( U 1 ,...,U T N ) T , and diag (  X  ) is the diagonal function mapping a vector to a diagonal matrix. Since there is no closed form solution in Eq.(9) for updating  X  , we implement an EM framework to learn  X  . We define L  X  as the components in Eq.(9) associated with  X  , and we induce an auxiliary distribution q and a latent variable  X  q ( z mp = k ) to formulate a lower bound Q (  X , X  ) of L  X  Q (  X , X  ) =  X   X  U k U  X   X   X  k 2 F +2 X Therefore, the EM framework is implemented to make the lower bound Q (  X , X  ) approximate to L  X  ,
E-step: From the Bayes X  formula, we obtain a posterior proba-bility of z mp :
M-step: Maximizing Q (  X , X  ) with the constraints that P k 1 and P p  X  k,z mp = 1 , we obtain  X  X  (  X , X  ) where  X  is the Lagrangian multiplier and it is noted that  X  cannot be updated in a closed form; hence, we use the projection gradient to learn  X  .
We apply the updated tr-MF model to computing the likelihood and updating U , V ,  X   X  , and  X  , which can be used to make predic-tions after the iteration converges and the tr-MF model is estab-lished. We predict R ij from its expectation,
For new users with no ratings in the cold-start scenarios, E [ Y 0 and we predict with
We obtain the user topics  X   X  i through the topic analysis of their activities, such as browsing histories, searching histories or com-ments.
We show the effectiveness of tr-MF using a real-world commu-nity dataset. Recall ( b ) K = 200 , and ( c ) K = 300 .
We use a dataset from CiteULike 3 . CiteULike is a social network website for scientific researchers; it allows users to create personal reference libraries for the interesting articles and to capture all the metadata (authors, abstract, keywords, ...) of the articles. In the prior work, Wang and Blei [28] collected a large subset from Ci-teULike to form their dataset 4 . For a fair comparison study, we use the same dataset as the benchmark.

This dataset contains 204,986 pairs of observed ratings with 5551 users and 16,980 articles. However, the sparseness is still quite low, i.e., 0 . 2175% , which is much lower than that of the well-known Movielens dataset 5 with the sparseness 4 . 25% . On average, each user has 37 articles in the library, ranging from 10 to 403, and each article appears in 12 user X  X  libraries, ranging from 1 to 321. For of-word representation. The corpus has 8,000 distinct words after the standard text processing by tf-idf and removing the stop words. These articles are added to CiteULike between 2004 and 2010.
Two metrics, the accuracy and the recall are often used to mea-sure the performance of a recommender system. However, as we discussed earlier, in one-class collaborative filtering, especially for recommending scientific articles, the ratings of R ij = 0 are un-certain and may have two different explanations: the user does not like an article or does not know it. But the ratings of R are known to be true positive. Thus, here we use the recall which only focuses on the true positive examples. Given the number of the recommended items T , the recall@ T is defined as follows: The recall above is user-oriented. To obtain the recall for the entire system, we first compute the recall for each user; we then obtain the overall recall by averaging the recalls from all the users.
We use cross-validation to estimate the performance of different algorithms. The validation dataset is randomly divided into training and test sets with a 80 / 20 splitting ratio. The training set contains 80% known positive examples ( R ij = 1 ) and 80% unknown ex-http://www.citeulike.org/faq/data.adp http://www.cs.princeton.edu/~chongw/citeulike/ http://www.cs.umn.edu/Research/GroupLens amples ( R ij = 0 ). The test set includes the other 20% known positive examples and unknown examples. We form predictive rat-ings for the test set and generate a list of the top T recommended articles.
 Here, we introduce the training settings for the comparison among MF [20], CTR [28], and tr-MF. For the MF model, we empirically specify the parameters as:  X  U =  X  V = 0 . 01 , a = 1 , and b = 0 . 01 . For the CTR model, we set the parameters by referring to [28]:  X 
U = 0 . 01 ,  X  V = 100 , a = 1 , and b = 0 . 01 . For the tr-MF model, we set the parameters:  X  U = 100 ,  X  V = 0 . 01 , a = 1 , and b = 0 . 01 . All the models are compared under the same test set.
Several performance comparisons of all the models are shown in Figure 2 with the varying number of the recommended articles T = { 50 ,..., 300 } . As is shown, tr-MF outperforms the compet-ing models with significant margins in almost all the cases, since the recall achieved by tr-MF is much higher than those achieved by the competing models in most cases. First, the improvement is greater with the increase of T ; this can be explained as that the content restriction for user factors should be more effective when T becomes larger. Second, the performance of tr-MF is much better than those of the competing models with the increase of the latent feature vectors X  dimensionality K ; this can be explained as that the knowledge of the articles is fully discovered through LDA when K becomes large. The above result suggests that it be essential to introduce the topic modeling to enhance the performance of the matrix factorization model.
 Figure 3 shows the comparison study when  X  U varies for tr-MF. We take CTR as our baseline. As is shown, tr-MF outperforms CTR with significant margins in almost all the cases. When  X  is small for tr-MF, the penalty of user-specific latent feature vector U diverging from the user topic proportions  X   X  i is small. When  X  increases,  X   X  i plays a more important role in recommending articles and the performance is improved. When  X  U is too large, however, U i is almost equal to  X   X  i and the performance substantially drops. This effect indicates that the appropriate penalty helps improve the prediction performance.
For the matrix factorization models, the prediction performances for users with different numbers of articles usually vary substan-tially, because the latent feature vectors for users are updated along with different directions during the training process. The user topic proportions of tr-MF serve as an effective constraint to restrict the Figur e 3: Comparison of the recalls under different  X  U dimensionality of the latent feature vectors K is 200. The num-ber of the recommended articles T is 100. Figur e 4: Comparison of the recalls on different users. The di-mensionality of the latent feature vectors K is 200. The number of the recommended articles T is 100. prediction in certain research areas. This constraint has a strong effect on users with only few ratings. Thus, the performance of tr-MF on users with only few ratings is expected to be better than those of the competing models. Figure 4 shows this effect through the performance comparison on users with different numbers of articles. For this study, we set the number of the recommended ar-ticles T = 100 and the dimensionality of the latent feature vectors K = 200 . We group the users by the number of the articles in the user X  X  library.

As we see from Figure 4, for users with the number of articles n  X  [20 , 30] in their libraries, tr-MF outperforms MF with a 3.5% gain and CTR with a 2.3% gain. While for users with the num-ber of articles less than 20, tr-MF outperforms both the competing models with a 2% gain. This is due to the fact that the users with the number of articles n  X  [20 , 30] have more collaborative infor-mation than the users with the number of articles less than 20. In addition, the recalls for users tend to show a decreasing trend with the increase of the number of articles in libraries. The reason for this trend is that more unpopular articles appear in the libraries of those users with a larger number of articles. The predictions for these unpopular articles become difficult. Figur e 5: Comparison of the recalls on the new-user problem. The dimensionality of the latent feature vectors K is 200.
For new users who have not yet rated any scientific article, the traditional CF algorithms cannot make predictions for these users as these algorithms only use the information about the user X  X  ex-isting ratings. In this subsection, we demonstrate that the new-user problem can be tackled with the users X  activities, such as the brows-ing histories, the searching histories, or the comments.

For this study, since the dataset does not have such new-user samples, we elect to generate such new users synthetically on top of the dataset. 100 users are randomly selected from the dataset as the new users, which are not in the training set. We then randomly select the abstract of a rated article for each new user as her search-ing history. Thus, the corresponding user topic proportions can be generated from the searching history for each new user based on the training parameters of tr-MF. Consequently, we make predic-tions for these new users with Eq.(13). Since this is the first work addressing the cold-start problem for new users in recommending scientific articles, there is no existing model in the literature that can be used for a comparison directly. We elect to compare tr-MF with a model equivalent to fixing the per-user latent feature vec-tor U i =  X   X  i in the tr-MF model and using per-article latent feature vector V j to fit to the ratings. This is similar to the method used as a baseline for the cold-start problem in the literature [28]. As this model only uses LDA-Like features (topic proportions), we call this comparing method as LDA-Like.

Figure 5 illustrates that tr-MF outperforms LDA-Like with a 4.8% gain on average. The reason for this is that tr-MF is able to capture the popularities of the articles which are shown in Subsection 6.8. Thus, once the user topic proportions are generated from the user X  X  activity records, tr-MF makes accurate predictions for new users.
In this subsection, we illustrate the interpretable user latent fac-tors learned by tr-MF. For this study, we set the dimensionality of the latent feature vectors K = 200 . Given a user, the top inter-esting topics may be identified by ranking the entries of the latent feature vector U i . Figure 6 illustrates two examples: user A and user B. For both examples, the high topic weight distributions of U and  X   X  i are very similar. This is due to the fact that mean of U i as shown in Eq.(6). This property indicates that the user topic profiling can be nicely visualized through its user factors when tr-MF is used. Figure 6: The comparison between U i and its mean  X   X  i . (a) User A. The 7 topics are obtained by joining the top 5 topics ranked by U ik and another top 5 topics ranked by  X   X  ik , k = 1 ,...,K . (b) User B. It is produced the same way as for (a).

Thus, the learned topic representation of users by tr-MF is ex-pected to provide better explanations for the articles recommended to them than those provided by other models. Table 1 shows this effect through a similar example as used in [28]: user C. It lists the user X  X  top 3 interesting topics (found by k = arg max along with the top 10 preferred articles as predicted by the tr-MF model and the CTR model, respectively. Clearly, tr-MF makes a more accurate prediction (all in the library) than CTR (one out of the library). Moreover, the topics found by tr-MF make more sense in matching the predicted articles than those found by CTR. In ad-dition, since tr-MF is able to discover the users X  latent factors with interpretable meanings as demonstrated in Table 1, as opposed to the users X  latent factors without interpretable meanings for most ex-isting methods, users may take this advantage of tr-MF to connect themselves each other based on their similar interests discovered through this interpretable user topic profiling to facilitate the com-munity communications. For tr-MF, it obtains two different representations of an article. In this section we first illustrate the difference between them; then we show the comparison study on supporting tasks that are specific to a given field. For this study, we set the dimensionality of the latent feature vectors K = 200 .
From the generative process of tr-MF, we see that a scientific ar-ticle plays two different roles: given a bag-of-word representation, article topic proportions are generated through LDA to regularize the latent feature vectors for users; it serves as the same role of item as other matrix factorization models [22, 20]. Consequently, we ob-tain two different representations of article j :  X  j and V is generated by LDA and represents the article topic proportions. Next, we provide an explanation of V j in details. As is seen from Figure 7: Maximum likelihood from incomplete data via the EM algorithm. (a) The topic proportions of this article. The 5 topics are obtained by ranking  X  jk ,k = 1 ,...,K . (b) The pop-ularity score (p-score) distribution on different topics of this ar-ticle. The 5 topics are obtained by ranking V jk ,k = 1 ,...,K . The item factor entry V jk is denoted as the popularity score of article j towards the k th topic. the above section, U i is considered as the topic representation of user i . Specifically, we model the relationship between user i and item j as U T i V j . Thus, V j is represented as the popularity of arti-cle j to those topics. Here we denote V jk as the popularity score (p-score) of article j towards the k th topic. Moreover, we find its top applied topics by ranking the p-scores of article j .
We take an example to showcase the difference between  X  j V . Figure 7 is for the article "Maximum likelihood from incom-plete data via the EM algorithm" [11]. As is shown from Figure 7(a), we obtain the top 5 topics found by k = arg max k  X  lustrates that the content of this article is mainly mapped to the fol-lowing topics: image analysis, maximum likelihood estimation, and classification algorithm . In addition, as is shown from Figure 7(b), we obtain the top 5 topics found by k = arg max k V jk . It shows that this article is mainly applied in the following areas: maximum likelihood estimation, network models study, and statistical biology research . Consequently, an interesting fact is discovered that this article is not a statistical biology paper, but researchers engaging in statistical biology typically like this article.
Another advantage of tr-MF is to support tasks that are specific to a given field, which is not addressed in the existing literature. Since V jk is considered as the popularity score of article j towards the k th topic, we obtain the top popular articles under this topic found by j = arg max j V jk ,j = 1 ,...,M . Consequently, tr-MF is ex-pected to more efficiently support tasks that are specific to a given field. Table 2 shows this effect through two example topics with the corresponding top 5 popular articles predicted by tr-MF and CTR, respectively. As is shown, tr-MF is capable of recommending more popular articles for the same topic than CTR demonstrated by both the overall higher dataset ratings and the overall higher Google ci-tations, whereas most existing recommender systems are unable to offer this capability at all.

Since tr-MF is more capable of discovering the popular and fun-damental papers for a given topic than CTR or other existing meth-ods, it delivers a better service to researchers to facilitate queries such as "recommending papers that my paper should cite" and "rec-ommending papers that may cite my paper". Specifically, tr-MF is more capable of identifying valuable papers that a researcher wishes to reference but is not familiar with. For example, biological science researchers may wish recommender systems to recommend popular papers on a specific topic in data mining area that they in-tend to link their research to but are not familiar with at all. Here tr-MF offers a better solution.
Recommender systems have been extensively studied in the past few years (primarily due to the Netflix Prize). Collaborative fil-tering is popular and widely used for its domain-free characteristic [18]. The two primary methods of collaborative filtering are the neighborhood based methods [5, 17] and the latent factor models [6, 22, 23, 15, 30]. Neighborhood based methods make predictions via computing the relationships between users or between items. Latent factor models try to explain the observed ratings using the latent factors. Among the latent factor models, matrix factorization models perform well in several real-world applications [22, 6, 18].
In addition, several methods have been proposed to improve the performance of collaborative filtering using the content features [19, 16, 31, 25]. In particular, recent work combines the matrix factorization with the topic modeling [9, 7, 8] to achieve a better performance [24, 2, 28, 21]. These models regularize item fac-tors with item topic proportions which are captured by the topic modeling. In this work, we regularize item factors with a Gaussian prior at the same time to regularize user factors with user topic pro-portions through LDA, which distinguishes all existing models in methodology.
In this paper, we have studied the problem of scientific article recommendation. We note that the interest of users is usually con-fined to a certain research area. Based on this unique characteristic, we have developed a new model called tr-MF for recommending scientific articles to users, which is to combine the matrix factor-ization with a probabilistic topic modeling. Compared with the existing literature, tr-MF is particularly effective in recommending articles to users who have only few or even no ratings. Experi-mental results show that the tr-MF model performs well against the state-of-the-art methods, such as MF and CTR.
 Furthermore, tr-MF provides a topic representation for users. The topic representation for users is helpful for connecting simi-lar users so that they may share their libraries. This helps facilitate the community communications. In addition, tr-MF shows promis-ing capability for supporting the task of recommending papers in a specific field. Further, it can be applied to many other areas where a bag-of-word representation for items or users is available. We thank the area chair and reviewers for their constructive com-ments. This work is supported in part by the National Basic Re-search Program of China (2012CB316400), Zhejiang University  X  Alibaba Financial Joint lab, and Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis. ZZ is also supported in part by US NSF (IIS-0812114, CCF-1017828). [1] D. Agarwal and B.-C. Chen. Regression-based latent factor [2] D. Agarwal and B.-C. Chen. flda: matrix factorization [3] P. Arkadiusz. Improving regularized singular value [4] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. M. [5] R. M. Bell and Y. Koren. Scalable collaborative filtering with [6] R. M. Bell, Y. Koren, and C. Volinsky. Modeling [7] D. M. Blei and J. D. Lafferty. Correlated topic models. In [8] D. M. Blei and J. D. McAuliffe. Supervised topic models. In [9] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [10] D. Cai, Q. Mei, J. Han, and C. Zhai. Modeling hidden topics [11] A. P. Dempster, N. Laird, and D. Rubin. Maximum [12] Y. Ge, Q. Liu, H. Xiong, A. Tuzhilin, and J. Chen.
 [13] T. Hofmann. Probabilistic latent semantic indexing. In [14] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for [15] M. Jiang, P. Cui, R. Liu, Q. Yang, F. Wang, W. Zhu, and [16] X. Jin, Y. Zhou, and B. Mobasher. A maximum entropy web [17] Y. Koren. Factorization meets the neighborhood: a last column illustrates whether each article is in the user X  X  library. of citations.
 Model Paper Title tr-MF The structure of collaborative tagging systems 321 102 1687 537 [18] Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization [19] P. Melville, R. J. Mooney, and R. Nagarajan.
 [20] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, [21] S. Purushotham and Y. Liu. Collaborative topic regression [22] R. Salakhutdinov and A. Mnih. Probabilistic matrix [23] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix [24] H. Shan and A. Banerjee. Generalized probabilistic matrix [25] L. Si and R. Jin. Unified filtering by combining collaborative [26] N. Srebro and T. Jaakkola. Weighted low-rank [27] N. Srebro, J. D. M. Rennie, and T. Jaakkola.
 [28] C. Wang and D. M. Blei. Collaborative topic modeling for [29] C. Wang, D. M. Blei, and F.-F. Li. Simultaneous image [30] J. Weston, C. Wang, R. Weiss, and A. Berenzweig. Latent [31] K. Yu, A. Schwaighofer, V. Tresp, W.-Y. Ma, and H. Zhang.
