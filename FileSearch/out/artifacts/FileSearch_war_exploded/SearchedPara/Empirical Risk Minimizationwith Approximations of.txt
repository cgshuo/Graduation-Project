 [7], computer vision [16], computational biology [19] and more recently, in human activity analysis [12]. They are commonly estimated using maximum likelihood estimate or variants. Such estima-tion can be viewed as minimizing empirical risk with the log-loss [21]. The log-loss is not bounded when applied to probabilistic grammars, and that makes it hard to obtain uniform convergence re-sults. Such results would help in deriving sample complexity bounds, that is, bounds on the number of training examples required to obtain accurate estimation.
 To overcome this problem, we derive distribution-dependent uniform convergence results for proba-setting. Based on the notion of bounded approximations [1, 9], we define a sequence of increasingly better approximations for probabilistic grammars, which we call  X  X roper approximations. X  We then derive sample complexity bounds in our framework, for both the supervised case and the unsuper-vised case.
 (number of derivation steps the grammar takes when generating a structure). This means that most of the probability mass for such a distribution is concentrated on a small number of grammatical we believe this is a reasonable assumption.
 The rest of the paper is organized as follows.  X  2 gives an overview of probabilistic grammars.  X  3 concept spaces that permit the derivation of sample complexity bounds for probabilistic grammars. A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process. For example, probabilistic context-free grammars (PCFGs) generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of  X  X hild X  symbols according to a fixed set of production rules. Each rewrite of a PCFG is conditionally independent of previous ones given one PCFG state; this Markov property permits efficient inference for the probability distribution defined by the probabilistic grammar.
 yield( z ) . There may be many derivations z for a given string (perhaps infinitely many for some grammar defines the probability of a grammatical derivation z as:  X  events. We let N = P K k =1 N k denote the total number of derivation event types. D ( G ) denotes event tokens) of the derivation z .
 Parameter estimation for probabilistic grammars means choosing  X  from complete data ( X  X uper-vised X ) or incomplete data ( X  X emi-supervised X  or  X  X nsupervised, X  the latter usually implying that f D ( G ) (note that P does not have to be a probabilistic grammar): | z | X | x | .  X  Exponential decay of derivations: There is a constant r &lt; 1 and a constant L  X  0 such that
P ( z )  X  Lr | z | . of length k in G . Taking r as above, then we assume there exists a constant q &lt; 1 , such that (e.g., as with many PCFGs), but is bounded by ( q/r ) k . finite set. These assumptions also hold in many cases when P itself is a probabilistic grammar. relationship to Tsybakov noise [20, 15]. implying a choice of h  X  H that  X  X grees X  with the training data. MLE chooses h  X   X  H to maximize the likelihood of the data: As shown, this equates to minimizing the empirical risk, or the expected value of a particular loss function known as log-loss. The expected risk, under P , is the (unknowable) quantity Showing convergence of the form sup h  X  H | R emp ,n (  X  log h )  X  R (  X  log h ) |  X  X  X  converges to the minimized expected risk. We assume familiarity with the relevant literature about empirical risk minimization; see [21]. The log-loss is unbounded, so that there is no function F : D ( G )  X  R such that,  X  f  X  F ,  X  z  X  that we can still get consistency for the maximum likelihood estimator, if we bound from below and above the family of probability distributions at hand.
 model according to well-known results about the convergence of stochastic processes. The revision approximates the concept space using a sequence F 1 , F 2 ,... and replaces two-sided uniform con-vergence with convergence on the sequence of concept spaces. The concept spaces in the sequence vary as a function of the number of samples we have. We next construct the sequence of concept spaces, and in  X  5 we return to the learning model. Our approximations are based on the concept of bounded approximations [1, 9].
 as m grows larger, F m becomes a better approximation of the original concept space F . We say that C required to obtain uniform convergence results in the revised model [18]. Note that K m can grow arbitrarily large. The third requirement ensures that our approximation actually converges to the convergence for probabilistic grammars in the supervised setting.
 We note that a good approximation would have K m increasing fast as a function of m and tail ( m ) great effect on the number of samples required to obtain accurate estimation. 4.1 Constructing Proper Approximations for Probabilistic Grammars We now focus on constructing proper approximations for probabilistic grammars. We make an as-sumption about the probabilistic grammar that  X  k,N k = 2 . For most common grammar formalisms, expressed using a grammar that has N k  X  2 . See supplementary material and [6].  X  k =  X   X  k, 1 , X  k, 2  X  in the probabilistic grammar by  X  : { T ( f,m  X  p ) | f  X  F } .
 Proposition 4.1. There exists a constant  X  =  X  ( L,q,p,N ) &gt; 0 such that F m has the boundedness property with K m = pN log 3 m and bound ( m ) = m  X   X  log m .
 requirements on P we have: We show now that F m is tight with respect to F with tail ( m ) = Proposition 4.2. There exists an M such that for any m &gt; M we have: P S f  X  F { z | C m ( f )( z )  X  f ( z )  X  tail ( m ) }  X  tail ( m ) for tail ( m ) = C m ( f ) = T ( f,m  X  p ) .
 Proof. See supplementary material.
 We now have proper approximations for probabilistic grammars. From this point, we use F m to tion 4.1 and Proposition 4.2, and assume that p &gt; 1 is fixed, for the rest of the paper. 4.2 Asymptotic Empirical Risk Minimization risk minimizer (in the log-loss case, this means it converges to the maximum likelihood estimate). As a conclusion to this section about proper approximations, we motivate the three requirements that we posed on proper approximations by showing that this is indeed true. We now unify n , the of the empirical risk over F n ( g n = argmin f  X  F Let D = { z 1 ,...,z n } be a sample from P ( z ) . The operator ( g n =) argmin f  X  F following: Proposition 4.3. Let D = { z 1 ,...,z n } be a sample of derivations for G . Then g n =  X  X ne of z i  X  D is in Z ,n . X  Then if F n properly approximates F then: a proof.) Proof of Proposition 4.3. Let f 0  X  F be the concept that puts uniform weights over  X  , i.e.,  X  k =  X  , 1 2  X  for all k . Note that | E [ R emp ,n ( f  X  n ) | A ,n ] | P ( A ,n ) Let A j,,n for j  X  X  1 ,...,n } be the event  X  z j  X  Z ,n  X . Then A ,n = S j A j,,n . We have that: where Eq. 7 comes from z l being independent and B is the constant from  X  2. Therefore, we have: 1 n
X From the construction of our proper approximations (Proposition 4.2), we know that only derivations of length log 2 n or greater can be in Z ,n . Therefore: |
E A that C n ( f  X  n )  X  F n , and therefore C n ( f  X  n )( z )  X  pN | z | log n . We now give our main sample complexity results for probabilistic grammars. These results hinge on the convergence of sup f  X  F covering numbers for F n do not grow too fast.
 be a distance measure between two functions f,g from G . An -cover is a subset of G , denoted by N ( , G ,d ) is the size of the smallest -cover of G using with respect to the distance measure d .  X  P that describes the data z 1 ,...,z n . Let f,g  X  G . We will use: empirical process sup f  X  F E functions from F n after being truncated by K n . Then for &gt; 0 we have, P sup provided n  X  K 2 n / 4 2 .
 Proof. See [18] (chapter 2, pages 30 X 31). See supplementary material for an explanation. Covering numbers are rather complex combinatorial quantities that are hard to compute directly. Fortunately, they can be bounded by using the pseudo dimension [3], a generalization of VC di-mension for real functions. In the case of our  X  X inomialized X  probabilistic grammars, the pseudo dimension of F n is bounded by N , because we have F n  X  F , and the functions in F are linear with N parameters. Hence, F truncated ,n has also pseudo dimension that is at most N . We have: Lemma 5.2. (From [18, 13].) Let F n be the proper approximations for probabilistic grammars, for any 0 &lt; &lt; K n we have: 5.1 Supervised Case Lemmas 5.1 and 5.2 can be combined to get our main sample complexity result: Theorem 5.3. Let G be a grammar. Let F n be a proper approximation for the corresponding family constant M such that for any 0 &lt;  X  &lt; 1 and 0 &lt; &lt; 1 and any n &gt; M and if then we have where K n = pN log 3 n .
 Proof. Omitted for space.  X  ( L,q,p,N ) is the constant from Proposition 4.1. The proof is based on simple algebraic manipulation of the right side of Eq. 13 while relying on Lemma 5.2. 5.2 Unsupervised Case goal again is to identify grammar parameters  X  from these yields. Our concept classes are now the sets of log marginalized distributions from F n . For each f  X   X  F n , we define f 0  X  as: we define C 0 n ( f 0 )( x ) = P z C n ( f )( x,z ) .
 boundedness property is satisfied with the same K n and the same form of bound ( n ) as in Proposi-on the property of bounded derivation length of P . See the supplementary material for a proof. The following result shows that we have tightness as well: Proposition 5.4. There exists an M such that for any n &gt; M we have: operator C 0 n ( f ) as defined above.
  X  log a i + log b i  X  .
 Sketch of proof of Proposition 5.4. From Utility Lemma 5.5 we have:
P  X   X  proof of Proposition 4.2 and the requirements on P , we know that there exists an  X   X  1 such that where the last inequality happens for some n larger than a fixed M .
 Computing either the covering number or the pseudo dimension of F 0 n is a hard task, because the function in the classes includes the  X  X og-sum-exp. X  In [9], Dasgupta overcomes this problem for F 0 that depends on the covering number of F .
 mar can be arbitrarily large. We overcome this problem using the following restriction. We assume The more samples we have, the more permissive (for large derivation set) the grammar can be. On the other hand, the more accuracy we desire, the more restricted we are in choosing grammars that derivational condition, we can show the following result: Proposition 5.6. (Hidden Variable Rule for Probabilistic Grammars) Under the derivational condi-the unsupervised case, then, we get the following sample complexity result:  X  and | D x ( G ) | &lt; d ( n ) , we have that where K n = pN log 3 n .
 exponential function of n  X  for  X  &lt; 1 , e.g. d ( n ) = 2 criterion as K n increases . . . as d ( n ) increases . . . as p increases . . . tightness of proper approx-imation sample complexity bound degrades degrades degrades d ( n ) is the function that gives the derivational condition, i.e., | D x ( G ) | X  d ( n ) . Our framework can be specialized to improve the two main criteria that have a trade-off: the tight-ness of the proper approximation and the sample complexity. For example, we can improve the tightness of our proper approximations by taking a subsequence of F n . However, this will make the sample complexity bound degrade, because K n will grow faster. Table 1 gives the different trade-offs between parameters in our model and the effectiveness of learning. In general, we would want some t , for small samples), but in that case our sample complexity bounds become trivial. In the supervised case, our result states that the number of samples we require (as an upper bound) example, is a PCFG, then N depends on the total number of rules. When the PCFG is in Chomsky wisdom that lexicalized grammars require much more data for accurate learning.
 The dependence of the bound on N suggests that it is easier to learn models with a smaller grammar size. This may help explain the success of recent advances in supervised parsing [4, 22, 17] that have  X  X oarse X  models (with a much smaller size of nontermimals) as a first pass. Those models are The sample complexity bound for the unsupervised case suggests that we need log d ( n ) times as much data to achieve estimates as good as those for supervised learning. Interestingly, with unsu-pervised grammar learning, available training sentences longer than a maximum length (e.g., 10) are often ignored; see [14].
 We note that sample complexity is not the only measure for the complexity of estimating probabilis-tic grammars. In the unsupervised setting, for example, the computational complexity of ERM is NP hard for PCFGs [5] or probabilistic automata [2]. We presented a framework for learning the parameters of a probabilistic grammar under the log-loss and derived sample complexity bounds for it. We motivated this framework by showing that the empirical risk minimizer for our approximate framework is an asymptotic empirical risk minimizer. Our framework uses a sequence of approximations to a family of probabilistic grammars, which improves as we have more data, to give distribution dependent sample complexity bounds in the supervised and unsupervised settings.
 We thank the anonymous reviewers for their comments and Avrim Blum, Steve Hanneke, and Dan Roth for useful conversations. This research was supported by NSF grant IIS-0915187. [1] N. Abe, J. Takeuchi, and M. Warmuth. Polynomial learnability of probabilistic concepts with [2] N. Abe and M. Warmuth. On the computational complexity of approximating distributions by [3] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations . Cambridge [4] E. Charniak and M. Johnson. Coarse-to-fine n -best parsing and maxent discriminative rerank-[5] S. B. Cohen and N. A. Smith. Viterbi training for PCFGs: Hardness results and competitiveness [6] S. B. Cohen and N. A. Smith. Empirical risk minimization for probabilistic grammars: Sample [7] M. Collins. Head-driven statistical models for natural language processing. Computational [9] S. Dasgupta. The sample complexity of learning fixed-structure Bayesian networks. Machine [10] J. Eisner. Three new probabilistic models for dependency parsing: An exploration. In Proc. of [11] E. M. Gold. Language identification in the limit. Information and Control , 10(5):447 X 474, [12] G. Guerra and Y. Aloimonos. Discovering a language for human activity. In AAAI Workshop [13] D. Haussler. Decision-theoretic generalizations of the PAC model for neural net and other [14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of depen-[15] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. [16] L. Lin, T. Wu, J. Porway, and Z. Xu. A stochastic graph grammar for compositional object [17] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL , [18] D. Pollard. Convergence of Stochastic Processes . New York: Springer-Verlag, 1984. [19] Y. Sakakibara, M. Brown, R. Hughey, S. Mian, K. Sj  X  olander, R. C. Underwood, and D. Haus-[21] V. N. Vapnik. Statistical Learning Theory . Wiley-Interscience, 1998. [22] D. Weiss and B. Taskar. Structured prediction cascades. In Proceedings of AISTATS , 2010.
