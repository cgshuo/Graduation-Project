 Policy gradient methods [18] in Reinforcement Learning have gained popularity, due to the guar-anteed improvement in control performance over iterations (which is often lacking in approximate policy or value iteration) as well as the discovery of more ef fi cient gradient estimation methods. In particular it has been shown that one can replace the true advantage function with a compatible function approximator without affecting the gradient [8,14], and that a natural policy gradient (with respect to Fisher information) can be computed [2,5,11].
 The goal of this paper is to apply policy gradient ideas to the linearly-solvable MDPs (or LMDPs) we have recently-developed [15, 16], as well as to a class of continuous stochastic systems with similar properties [4, 7, 16]. This framework has already produced a number of unique results  X  such as linear Bellman equations, general estimati on-control dualities, compositionality of optimal control laws, path-integral methods for optimal cont rol, etc. The present results with regard to policy gradients are also unique, as summarized in Abstract. While the contribution is mainly theoretical and scaling to large problems is left for future work, we provide simulations demonstrating rapid convergence. The paper is organized in two sections, treating discrete and continuous problems. Since a number of papers on LMDPs have already been published, we will not repeat the general development and motivation here, but instead only summarize the background needed for the present paper. We will then develop the new results regarding policy gradients. 2.1 Background on LMDPs An LMDP is de fi nedbyastatecost  X  (  X  ) over a (discrete for now) state space X , and a transition on in fi nite-horizon average-cost problems where  X  (  X  0 |  X  ) is assumed to be ergodic, i.e. it has a which are ergodic and satisfy  X  (  X  0 |  X  )=0 whenever  X  (  X  0 |  X  )=0 . The cost function is Thus the controller is free to modify the default/passive dynamics in any way it wishes, but incurs a control cost related to the amount of modi fi cation.
 where  X  (  X  ) is de fi ned up to a constant. The optimal  X   X  and  X   X  (  X  ) can be shown to satisfy and the optimal  X   X  (  X  0 |  X  ) can be found in closed form given  X   X  (  X  ) : Exponentiating equation (3) makes it linear in exp (  X   X   X  (  X  )) , although this will not be used here. 2.2 Policy gradient for a general parameterization ditions and O w  X  ,  X  X  X  X  X  w exists for all w  X  R  X  .Let  X  (  X  X  w ) be the corresponding stationary tational clutter we will suppress the dependence on w in most of the paper; keep in mind that all quantities that depend on  X  are functions of w .
 Our objective here is to compute O w  X  . This is done by differentiating the Bellman equation (2) and following the template from [14]. The result (see Supplement) is given by Theorem 1. The LMDP policy gradient for any valid parameterization is Let us now compare (5) to the policy gradient in traditional MDPs [14], which is ing state-action cost-to-go. The general form of (5) and (6) is similar, however the term log (  X  X  X  X  )+  X  in (5) cannot be interpreted as a  X  -function. Indeed it is not clear what a  X  -function means in the LMDP setting. On the other hand, while in traditional MDPs one has to estimate  X  (orratherthe advantage function) in order to compute the policy gradient, it will turn out that in LMDPs it is suf fi cient to estimate  X  . 2.3 A suitable policy parameterization The relation (4) between the optimal policy  X   X  and the optimal cost-to-go  X   X  suggests parameter-izing  X  as a  X  -weighted Gibbs distribution. Since linear function approximators have proven very successful, we will use an energy function (for the Gibbs distribution) which is linear in w : also need the  X  -expectation operator de fi ned for both scalar and vector functions over X . The general result (5) is now specialized as Theorem 2. The LMDP policy gradient for parameterization (7) is Indeed if they are equal the gradient vanishes (the converse is not true). 2.4 Compatible cost-to-go function approximation One of the more remarkable aspects of policy gradient results [8, 14] in traditional MDPs is that, when the true  X  function is replaced with a compatible approximation satisfying certain conditions, the gradient remains unchanged. Key to obtaining such results is making sure that the approximation error is orthogonal to the remaining terms in the expression for the policy gradient. Our goal in this section is to construct a compatible function approximator for LMDPs. The procedure is somewhat elaborate and unusual, so we provide the derivation before stating the result in Theorem 3 below. Given the form of (9), it makes sense to approximate  X  (  X  ) as a linear combination of the same when  X  is replaced with b  X  in (9), the following quantity must be zero: Expanding (10) and using the stationarity of  X  , we can simplify d as One can also incorporate an  X  -dependent baseline in (9), such as  X  (  X  ) which is often used in tradi-tional MDPs. However the baseline vanishes after the simpli fi cation, and the result is again (11). Now we encounter a complication. Suppose we were to fi t b  X  to  X  in a least-squares sense, i.e. minimize the squared error weighted by  X  . Denote the resulting weight vector r  X  X  X  : for r = r  X  X  X  the second term in (11) vanishes, but the fi rst term does not. Indeed we have veri fi ed numerically (on randomly-generated LMDPs) that d ( r  X  X  X  ) 6 =0 .
 If the best fi t is not good enough, what are we to do? Recall that we do not actually need a good fi t, but rather a vector r such that d ( r )=0 .Since d ( r ) and r are linearly related and have the same using the fact that  X  is a linear operator, we have d ( r )=  X  r  X  k where We are not done yet because k still depends on  X  . The goal now is to approximate  X  in such a way that k remains unchanged. To this end we use (2) and express  X  [  X  ] in terms of  X  : Here  X  (  X  ) is shortcut notation for  X  (  X  X   X  (  X |  X  X  w )) . Thus the vector k becomes where the policy-speci fi c auxiliary features g (  X  ) are related to the original features f (  X  ) as The second term in (15) does not depend on  X  ; it only depends on  X  = in (15) involves the projection of  X  on the auxiliary features g . This projection can be computed by Theorem 3. The following procedure yields the exact LMDP policy gradient: 1. fi t e  X  (  X  X  s ) to  X  (  X  ) in a least squares sense, and also compute  X  2. compute  X  from (13), and k from (15) by replacing  X  (  X  ) with e  X  (  X  X  s ) 3. " fi t" b  X  (  X  X  r ) by solving  X  r = k 4. the policy gradient is This is the fi rst policy gradient result with compatible function approximation over the state space rather than the state-action space. The computations involve averaging over  X  , which in practice will be done through sampling (see below). The requirement that  X   X  e  X  be orthogonal to g is somewhat restrictive, however an equivalent requirement arises in traditional MDPs [14]. 2.5 Natural policy gradient When the parameter space has a natural metric  X  ( w ) , optimization algorithms tend to work better if the gradient of the objective function is pre-multiplied by  X  ( w )  X  1 . This yields the so-called natural gradient [1]. In the context of policy gradient methods [5, 11] where w parameterizes a probability density, the natural metric is given by Fisher information (which depends on  X  because w parameterizes the conditional density). Averaging over  X  yields the metric We then have the following result (see Supplement): Theorem 4 . With the vector r computed as in Theorem 3, the LMDP natural policy gradient is Let us compare this result to the natural gradient in traditional MDPs [11], which is In traditional MDPs one maximizes reward while in LMDPs one minimizes cost, thus the sign difference. Recall that in traditional MDPs the policy  X  is parameterized using features over the state-action space while in LMDPs we only need features over the state space. Thus the vectors w  X  r will usually have lower dimensionality in (19) compared to (20).
 Another difference is that in LMDPs the (regular as well as natural) policy gradient vanishes when w = r , which is a sensible fi xed-point condition. In traditional MDPs the policy gradient vanishes when r =0 , which is peculiar because it corresponds to the advantage function approximation being identically 0 . The true advantage function is of course different, but if the policy becomes deterministic and only one action is sampled per state, the resulting data can be fi t with r =0 . Thus any deterministic policy is a local maximum in traditional MDPs. At these local maxima the policy gradient theorem cannot actually be applied because it requires a stochastic policy. When the policy becomes near-deterministic, the number of sampl es needed to obtain accurate estimates increases because of the lack of exploration [6]. These issues do not seem to arise in LMDPs. 2.6 A Gauss-Newton method for approximating the optimal cost-to-go Instead of using policy gradient, we can solve (3) for the optimal  X   X  directly. One option is approx-imate policy iteration  X  which in our context takes on a simple form. Given the policy parameters w biased approximator instead of the compatible approximator given by Theorem 3.
 The other option is approximate value iteration  X  which is a fi xed-point method for solving (3) while replacing  X   X  (  X  ) with w T f (  X  ) . We can actually do better than value iteration here. Since (3) has already been optimized over the controls and is differentiable, we can apply an ef fi cient Gauss-Newton method. Up to an additive constant  X  , the Bellman error from (3) is Interestingly, the gradient of this Bellman error coincides with our auxilliary features g : where  X  and g are the same as in (16, 8). We now linearize:  X  (  X  X  w +  X  w )  X   X  (  X  X  w )+  X  w T g (  X  ) and proceed to minimize (with respect to  X  and  X  w ) the quantity Figure 1: (A) Learning curves for a random LMDP. "resid" is the Gauss-Newton method. The sampling versions use 400 samples per evaluation: 20 trajectories with 20 steps each, starting from the stationary distribution. (B) Cost-to-go functions for the metronome LMDP. The numbers show the average costs obtained. There are 2601 discrete states and 25 features (Gaussians). Convergence was observed in about 10 evaluations (of the objective and the gradient) for both algorithms, exact and sampling versions. The sampling version o f the Gauss-Newton method worked well with 400 samples per evaluation; the natural gradient needed around 2500 samples.
 Normally the density  X  (  X  ) would be fi xed, however we have found empirically that the resulting at each iteration. It is not clear how to guarantee convergence of this algorithm given that the objective function itself is changing over iterations, but in practice we observed that simple damping is suf fi cient to make it convergent (e.g. w  X  w +  X  w  X  2 ).
 It is notable that minimization of (23) is closely related to policy evaluation via Bellman residual minimization. More precisely, using (14, 16) it is easy to see that TD(0) applied to our problem wouldseektominimize The similarity becomes even more apparent if we write  X   X  (  X  X  w ) more explicitly as Thus the only difference from (21) is that one expression has the term w T f (  X  ) at the place where the other expression has the term w T  X  [ f ](  X  ) . Note that the Gauss-Newton method proposed here would be expected to have second-order convergence, even though the amount of computa-tion/sampling per iteration is the same as in a policy gradient method. 2.7 Numerical experiments We compared the natural policy gradient and the Gauss-Newton method, both in exact form and with sampling, on two classes of LMDPs: randomly generated, and a discretization of a continuous "metronome" problem taken from [17]. Fitting the auxiliary approximator e  X  (  X  X  s ) was done using the LSTD(  X  ) algorithm [3]. Note that Theorem 3 guarantees compatibility only for  X  =1 ,however lower values of  X  reduce variance and still provide good descent directions in practice (as one would expect). We ended up using  X  =0  X  2 after some experimentation. The natural gradient was used with the BFGS minimizer "minFunc" [12].
 Figure 1A shows typical learning curves on a random LMDP with 100 states, 20 random features, and random passive dynamics with 50% sparsity. In this case the algorithms had very similar per-formance. On other examples we observed one or the other algorithm being slightly faster or pro-ducing better minima, but overall they were comparable. The average cost of the policies found by the Gauss-Newton method occasionally increased towards the end of the iteration.
 Figure 1B compares the optimal cost-to-go  X   X  , the least-squares fi t to the known  X   X  using our fea-tures (which were a 5-by-5 grid of Gaussians), and the solution of the policy gradient method ini-both algorithms converged in about 10 iterations, although the Gauss-Newton method needed about 5 times fewer samples in order to achieve similar performance to the exact version. Unlike the discrete case where we focused exclusively on LMDPs, here we begin with a very general problem formulation and present interesting new results. These results are then specialized to a narrower class of problems which are continuous (in space and time) but nevertheless have similar properties to LMDPs. 3.1 Policy gradient for general controlled diffusions Consider the controlled Ito diffusion where  X  (  X  ) is a standard multidimensional Brownian motion process, and u is now a traditional optimal control problems. Given a policy u =  X  ( x ) , the average cost  X  and differential cost-to-go  X  ( x ) satisfy the Hamilton-Jacobi-Bellman (HJB) equation where L is the following 2nd-order linear differential operator: In can be shown [10] that L coincides with the in fi nitesimal generator of (26), i.e. it computes the expected directional derivative of  X  along trajectories generated by (26). We will need Lemma 1 . Let L be the in fi nitesimal generator of an Ito diffusion which has a stationary density  X  , and let  X  be a twice-differentiable function. Then Proof: The adjoint L  X  of the in fi nitesimal generator L is known to be the Fokker-Planck operator  X  which computes the time-evolution of a density under the diffusion [10]. Since  X  is the stationary h  X  X  L [  X  ] i . Thus h  X  X  L [  X  ] i =0 .
 This lemma seems important-yet-obvious so we would not be surprised if it was already known, but we have not seen in the literature. Note that many diffusions lack stationary densities. For example the density of Brownian motion initialized at the origin is a zero-mean Gaussian whose covariance grows linearly with time  X  thus there is no stationary density. If however the diffusion is controlled and the policy tends to keep the state within some region, then a stationary density would normally exist. The existence of a stationary density may actually be a sensible de fi nition of stability for stochastic systems (although this point will not be pursued in the present paper). Now consider any policy parameterization u =  X  ( x  X  w ) such that (for the current value of w )the diffusion (26) has a stationary density  X  and O w  X  exists. Differentiating (27), and using the shortcut notation b ( x ) in place of b ( x  X  X  X  ( x  X  w )) and similarly for  X  ( x ) ,wehave Here L [ O w  X  ] is meant component-wise. If we now average over  X  , the last term will vanish due to Lemma 1. This is essential for a policy gradient procedure which seeks to avoid fi nite differencing; indeed O w  X  could not be estimated while sampling from a single policy. Thus we have Theorem 5 . The policy gradient of the controlled diffusion (26) is Unlike most other results in stochastic optimal control, equation (31) does not involve the Hessian O xx  X  , although we can obtain a O xx  X  -dependent term here if we allow  X  to depend on u .Wenow illustrate Theorem 5 on a linear-quadratic-Gaussian (LQG) control problem. Example (LQG) . Consider dynamics  X  X  X  =  X  X  X  X  +  X  X  and cost  X  (  X  X   X  )=  X  2 +  X  2 .Let  X  =  X   X  X  X  be the parameterized policy with  X  X  0 . The differential cost-to-go is known to be in the form and so the policy gradient can be computed directly as O  X   X  =1  X   X  2 +1  X  (  X  ) is a zero-mean Gaussian with variance  X  2 = 1 2  X  . One can now verify that the gradient given by Theorem 5 is identical to the O  X   X  computed above.
 Another interesting aspect of Theorem 5 is that it is a natural generalization of classic results from fi nite-horizon deterministic optimal control [13], even though it cannot be derived from those results. Suppose we have an open-loop control trajectory u (  X  )  X  0  X   X   X   X  , the resulting state trajectory u is O u  X  + O u b T  X  . Now suppose u (  X  ) is parameterized by some vector w .Then
O (closed-loop) deterministic problem. Thus (31) and (32) are very similar. Of course in fi nite-horizon settings there is no stationary density, and inste ad the integral in (32) is over the trajectory. An RL method for estimating O w  X  in deterministic problems was developed in [9].
 Theorem 5 suggests a simple procedure for estimating the policy gradient via sampling: fi t a function approximator b  X  to  X  , and use O x b  X  in (31). Alternatively, a compatible approximation scheme can be O w b ( x ) . This however is not practical because learning targets for O x  X  are dif fi cult to obtain. Ideally we would construct a compatible approximation scheme which involves fi tting b  X  rather than O x b  X  . It is not clear how to do that for general diffusions, but can be done for a restricted problem class as shown next. 3.2 Natural gradient and compatible approxi mation for linearly-solvable diffusions We now focus on a more restricted family of stoch astic optimal control problems which arise in many situations (e.g. most mechanical systems can be described in this form): Such problems have been studied extensively [13]. The optimal control law u  X  and the optimal case we use this relation to motivate the choice of policy parameterization and cost-to-go function approximator. Choosing some features f ( x ) ,wede fi ne b  X  ( x  X  r ) , r T f ( x ) as before, and now substitute these de fi nitions in the general result (31), replace  X  with the approximation b  X  ,and skipping the algebra, obtain the corresponding approximation to the policy gradient: Before addressing the issue of compatibility (i.e. whether e O w  X  = O w  X  ), we seek a natural gradient version of (35). To this end we need to interpret  X  T  X  X  X   X  1  X  T  X  as Fisher information for the (in-fi nitesimal) transition probability density of our para meterized diffusion. We do this by discretizing stochastic dynamics (33) is given by the Gaussian Suppressing the dependence on x , Fisher information becomes Comparing to (35) we see that a natural gradient result is obtained when Assuming (38) is satis fi ed, and de fi ning  X  ( w ) as the average of Fisher information over  X  ( x ) , Condition (38) is rather interesting. Elsewhere we have shown [16] that the same condition is needed to make problem (33) linearly-solvable. More precisely, the exponentiated HJB equation for the optimal  X   X  in problem (33, 38) is linear in exp (  X   X   X  ) . We have also shown [16] that the continuous problem (33, 38) is the limit (when  X   X  0 ) of continuous-state discrete-time LMDPs constructed via Euler discretization as above. The compatible function approximation scheme from Theorem 3 can then be applied to these LMDPs. Recall (8). Since L is the in fi nitesimal generator, for any twice-differentiable function  X  we have Substituting in (13), dividing by  X  and taking the limit  X   X  0 , the matrix  X  and vector k become Compatibility is therefore achieved when the approximation error in  X  is orthogonal to L [ f ] . Thus the auxiliary function approximator is now e  X  ( x  X  s ) , s T L [ f ]( x ) , and we have Theorem 6. The following procedure yields the exact policy gradient for problem (33, 38): 1. fi t e  X  ( x  X  s ) to  X  ( x ) in a least-squares sense, and also compute  X  2. compute  X  and k from (41), replacing  X  ( x ) with e  X  ( x  X  s ) 3. " fi t" b  X  ( x  X  r ) by solving  X  r = k 4. the policy gradient is (35), and the natural policy gradient is (39) This is the fi rst policy gradient result with compatible function approximation for continuous sto-chastic systems. It is very similar to the corresponding results in the discrete case (Theorems 3,4) except it involves the differential operator L rather than the integral operator  X  . Here we developed compatible function approximators and natural policy gradients which only re-quire estimation of the cost-to-go function. This was possible due to the unique properties of the LMDP framework. The resulting approximation scheme is unusual, using policy-speci fi c auxiliary features derived from the primary features. In continuous time we also obtained a new policy gradi-ent result for control problems that are not linearly-solvable, and showed that it generalizes results from deterministic optimal control. We also derived a somewhat heuristic but nevertheless promis-ing Gauss-Newton method for solving for the optimal cost-to-go directly; it appears to be a hybrid between value iteration and policy gradient.
 One might wonder why we need policy gradients here given that the (exponentiated) Bellman equa-tion is linear, and approximating its solution using features is faster than any other procedure in Reinforcement Learning and Approximate Dynami c Programming. The answer is that minimizing Bellman error does not always give the best policy  X  as illustrated in Figure 1B. Indeed a combined approach may be optimal: solve the linear Bell man equation approximately [17], and then use the solution to initialize the policy gradient method. This idea will be explored in future work. Our new methods require a model  X  as do all RL methods that rely on state values rather than state-action values. We do not see this as a shortcoming because, despite all the effort that has gone into model-free RL, the resulting methods do not seem applicable to truly complex optimal control problems. Our methods involve model-based sampling which combines the best of both worlds: computational speed, and grounding in reality (assuming we have a good model of reality). Acknowledgements .
 This work was supported by the US National Science Foundation. Thanks to Guillaume Lajoie and Jan Peters for helpful discussions. [1] S. Amari. Natural gradient works ef fi ciently in learning. Neural Computation , 10:251 X 276, [2] J. Bagnell and J. Schneider. Covariant policy search. In International Joint Conference on [3] J. Boyan. Least-squares temporal difference learning. In International Conference on Machine [4] W. Fleming and S. Mitter. Optimal control and nonlinear fi ltering for nondegenerate diffusion [5] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems , [6] S. Kakade. On the Sample Complexity of Reinforcement Learning . PhD thesis, University [7] H. Kappen. Linear theory for control of nonlinear stochastic systems. Physical Review Letters , [8] V. Konda and J. Tsitsiklis. Actor-critic algorithms. SIAM Journal on Control and Optimization , [9] R. Munos. Policy gradient in continuous time. The Journal of Machine Learning Research , [10] B. Oksendal. Stochastic Differential Equations (4th Ed) . Springer-Verlag, Berlin, 1995. [11] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing , 71:1180 X 1190, 2008. [12] M. Schmidt. minfunc. online material, 2005. [13] R. Stengel. Optimal Control and Estimation . Dover, New York, 1994. [14] R. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement [15] E. Todorov. Linearly-solvable Markov decision problems. Advances in Neural Information [16] E. Todorov. Ef fi cient computation of optimal actions. PNAS , 106:11478 X 11483, 2009. [17] E. Todorov. Eigen-function approximation methods for linearly-solvable optimal control prob-[18] R. Williams. Simple statistical gradient following algorithms for connectionist reinforcement
