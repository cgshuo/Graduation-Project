 Department of Electrical Engineering and Computer Science 1 and Department of Statistics 2 High-dimensional data sets present challenges that are both stat istical and computational in nature. On the statistical side, recent years have witnessed a flu rry of results on consistency and rates for various estimators under high-dimensional scaling, m eaning that the data dimension d and other structural parameters are allowed to grow with the sam ple size n . These results typically involve some assumption regarding the unde rlying structure of the parameter space, including sparse vectors, low-rank matrice s, or structured regression functions, as well as some regularity conditions on the data-gener ating process. On the computational side, many estimators for statistical recovery ar e based on solving convex programs. Examples of such M -estimators include ` 1 -regularized quadratic programming (Lasso), second-order cone programs for sparse non-parame tric regression, and semidefinite programming relaxations for low-rank matrix recovery.
 In parallel, a line of recent work (e.g., [3, 7, 6, 5, 12, 18]) focuses on p olynomial-time algorithms for solving these types of convex programs. Several a uthors [2, 6, 1] have used variants of Nesterov X  X  accelerated gradient method [12] to obtain algorithms with a global sublinear rate of convergence. For the special case of compress ed sensing (sparse regression with incoherent design), some authors have established fast conv ergence rates in a local sense X  X nce the iterates are close enough to the optimum [3, 5]. Oth er authors have studied finite convergence of greedy algorithms (e.g., [18]). If an algorithm id entifies the support set of the optimal solution, the problem is then effectively reduced t o the lower-dimensional subspace, and thus fast convergence can be guaranteed in a loca l sense. Also in application to compressed sensing, Garg and Khandekar [4] showed that a thres holded gradient algorithm converges rapidly up to some tolerance; we discuss this result in mor e detail following our Corollary 2 on this special case of sparse linear models.
 Unfortunately, for general convex programs with only Lipschitz c onditions, the best conver-gence rates in a global sense using first-order methods are sub-lin ear. Much faster global rates X  X n particular, at a linear or geometric rate X  X an be achieved if global regularity condi-tions like strong convexity and smoothness are imposed [11]. Howeve r, a challenging aspect of statistical estimation in high dimensions is that the underlying optim ization problems can never be globally strongly convex when d &gt; n in typical cases (since the d  X  d Hessian matrix is rank-deficient), and global smoothness conditions canno t hold when d/n  X  +  X  . In this paper, we analyze a simple variant of the composite gradient m ethod due to Nesterov [12] in application to the optimization problems that underlie regularized M -estimators. Our main contribution is to establish a form of global geo metric convergence for this algorithm that holds for a broad class of high-dimensional statis tical problems. We do so by leveraging the notion of restricted strong convexity, used in recent work by Negahban et al. [8] to derive various bounds on the statistical error in high-dim ensional estimation. Our analysis consists of two parts. We first establish that for optim ization problems un-derlying such M -estimators, appropriately modified notions of restricted strong convexity (RSC) and smoothness (RSM) suffice to establish global linear conve rgence of a first-order method. Our second contribution is to prove that for the iterates generated by our first-order method, these RSC/RSM assumptions do indeed hold with high p robability for a broad class of statistical models, among them sparse linear regres sion, group-sparse regres-sion, matrix completion, and estimation in generalized linear models. We note in passing that our notion of RSC is related to but slightly different than its prev ious use for bounding statistical error [8], and hence we cannot use these existing result s directly. An interesting aspect of our results is that we establish global geom etric convergence only up to the statistical precision of the problem, meaning the typical Euclidean distance k b  X   X   X   X  k between the true parameter  X   X  and the estimate b  X  obtained by solving the optimization problem. Note that this is very natural from the statistical persp ective, since it is the true parameter  X   X  itself (as opposed to the solution b  X  of the M -estimator) that is of primary interest, and our analysis allows us to approach it as close as is statis tically possible. Over-all, our results reveal an interesting connection between the stat istical and computational properties of M -estimators X  X hat is, the properties of the underlying statistical model that make it favorable for estimation also render it more amenable to optim ization procedures. The remainder of the paper is organized as follows. In the following se ction, we give a precise description of the M -estimators considered here, provide definitions of restricted st rong convexity and smoothness, and their link to the notion of statistica l precision. Section 3 gives a statement of our main result, as well as its corollaries when sp ecialized to various statistical models. Section 4 provides some simulation results that c onfirm the accuracy of our theoretical predictions. Due to space constraints, we refer the reader to the full-length version of our paper for technical details. In this section, we begin by describing the class of regularized M -estimators to which our analysis applies, as well as the optimization algorithms that we analyze . Finally, we describe the assumptions that underlie our main result. A class of regularized M -estimators: Given a random variable Z  X  P taking values in some set Z , let Z n 1 = { Z 1 , . . . , Z n } be a collection of n observations drawn i.i.d. from P . Assuming that P lies within some indexed family { P  X  ,  X   X   X  } , the goal is to recover an estimate of the unknown true parameter  X   X   X   X  generating the data. In order to do so, we consider the regularized M -estimator where L :  X   X Z n 7 X  R is a loss function, and R :  X  7 X  R + is a non-negative regularizer on the parameter space. Throughout this paper, we assume that th e loss function L is convex and differentiable, and that the regularizer R is a norm. In order to assess the quality of an estimate, we measure the error k b  X   X   X  X  ,  X  X  on the parameter space. Typical choices are the standard Euclide an inner product and ` -norm for vectors; the trace inner product and the Frobenius no rm for matrices; and the L ( P ) inner product and norm for non-parametric regression. As desc ribed in more detail in Section 3.2, a variety of estimators X  X mong them the Lasso, stru ctured non-parametric regression in RKHS, and low-rank matrix recovery X  X an be cast in th is form (1). When the data Z n 1 are clear from the context, we frequently use the shorthand L (  X  ) for L (  X  ; Z n 1 ). Composite objective minimization: In general, we expect the loss function L to be differentiable, while the regularizer R can be non-differentiable. Nesterov [12] proposed a simple first-order method to exploit this type of structure, and ou r focus is a slight variant of this procedure. In particular, given some initialization  X  0  X   X , consider the update where  X  u &gt; 0 is a parameter related to the smoothness of the loss function, an d is the ball of radius  X  in the norm defined by the regularizer. The only difference from Nesterov X  X  method is the additional constraint  X   X  B R (  X  ), which is required for control of early iterates in the high-dimensional setting. Parts of our theo ry apply to arbitrary choices of the radius  X  ; for obtaining results that are statistically order-optimal, a settin g  X  =  X ( R (  X   X  )) with  X   X   X  B R (  X  ) is sufficient, so that fairly conservative upper bounds on R (  X   X  ) are adequate.
 Structural conditions in high dimensions: It is known that under global smoothness and strong convexity assumptions, the procedure (2) enjoys a g lobally geometric convergence rate, meaning that there is some  X   X  (0 , 1) such that k  X  t  X  b  X  k = O (  X  t ) for all iterations t = 0 , 1 , 2 , . . . (e.g., see Theorem 5 in Nesterov [12]). Unfortunately, in the high-d imensional setting ( d &gt; n ), it is usually impossible to guarantee strong convexity of the proble m (1) in a global sense. For instance, when the data is drawn i.i.d., the loss fun ction consists of a sum of n terms. The resulting d  X  d Hessian matrix  X  2 L (  X  ; Z n 1 ) is often a sum of n rank-1 terms and hence rank-degenerate whenever n &lt; d . However, as we show in this paper, in order to obtain fast convergence rates for an optimization metho d, it is sufficient that (a) the objective is strongly convex and smooth in a restricted set of d irections, and (b) the algorithm approaches the optimum b  X  only along these directions.
 Let us now formalize this intuition. Consider the first-order Taylor s eries expansion of the loss function around the point  X  0 in the direction of  X  :
Definition 1 ( Restricted strong convexity (RSC)). We say the loss function L satisfies the RSC condition with strictly positive parameters (  X  ` ,  X  ` ,  X  ) if In order to gain intuition for this definition, first consider the degen erate setting  X  =  X  ` = 0. In this case, imposing the condition (5) for all  X   X   X  is equivalent to the usual definition of strong convexity on the optimization set. In contrast, when th e pair (  X ,  X  ` ) are strictly positive, the condition (5) only applies to a limited set of vectors. In p articular, when  X  0 is set equal to the optimum b  X  , and we assume that  X  belongs to the set that is not too close to the optimum b  X  , we are guaranteed strong convexity in the direction  X   X  b  X  .
 We now specify an analogous notion of restricted smoothness:
Definition 2 ( Restricted smoothness (RSM)). We say the loss function L satisfies the RSM condition with strictly positive parameters (  X  u ,  X  u ,  X  ) if Note that the tolerance parameter  X  is the same as that in the definition (5). The additional term  X  u  X  2 is not present in analogous smoothness conditions in the optimization literature, but it is essential in our set-up.
 Loss functions and statistical precision: In order for these definitions to be sensi-ble and of practical interest, it remains to clarify two issues. First, for what types of loss function and regularization pairs can we expect RSC/RSM to hold? Se cond, what is the smallest tolerance  X  with which they can hold? Past work by Negahban et al. [8] has intro-duced the class of decomposable regularizers ; it includes various regularizers frequently used in M -estimation, among them ` 1 -norm regularization, block-sparse regularization, nuclear norm regularization, and various combinations of such norms. Nega hban et al. [8] showed that versions of RSC with respect to  X   X  hold for suitable loss functions combined with a decomposable regularizer. The definition of RSC given here is related but slightly different: instead of control in a neighborhood of the true parameter  X   X  , we need control over the iter-ates of an algorithm approaching the optimum b  X  . Nonetheless, it can be also be shown that our form of RSC (and also RSM) holds with high probability for decompo sable regularizers, and this fact underlies the corollaries stated in Section 3.2.
 With regards to the choice of tolerance parameter  X  , as our results will clarify, it makes little sense to be concerned with choices that are substantially smaller th an the statistical precision of the model. There are various ways in which statistical precision ca n be defined; one natural one is 2 in the data-dependent loss function. 1 The statistical precision of various M -estimators under high-dimensional scaling are now relatively well understood, a nd in the sequel, we will encounter various models for which the RSM/RSC conditions hold w ith tolerance equal to the statistical precision. In this section, we first state the main result of our paper, and disc uss some of its conse-quences. We illustrate its application to several statistical models in Section 3.2. 3.1 Guarantee of geometric convergence Recall that b  X   X  tees that if the RSC/RSM conditions hold with tolerance  X  , then Algorithm (2) is guaranteed to have a geometric rate of convergence up to this tolerance. The theorem statement involves the objective function  X  (  X  ) = L (  X  ) +  X  n R (  X  ).
 Theorem 1 (Geometric convergence) . Suppose that the loss function satisfies conditions (RSC) and (RSM) with a tolerance  X  and parameters (  X  ` ,  X  u ,  X  ` ,  X  u ) . Then the sequence {  X  t }  X  t =0 generated by the updates (2) satisfies Remarks: Note that the bound (7) consists of two terms: the first term dec ays exponen-tially fast with the contraction coefficient  X  := 1  X   X  ` 4  X  offset, which becomes relevant only for t large enough such that k  X  t  X  b  X  k 2 = O (  X  2 ). Thus, the result guarantees a globally geometric rate of convergence up to the tolerance parameter  X  . Previous work has focused primarily on the case of sparse linear re gression. For this spe-cial case, certain methods are known to be globally convergent at s ublinear rates (e.g., [2]), meaning of the type O (1 /t 2 ). The geometric rate of convergence guaranteed by Theorem 1 is exponentially faster. Other work on sparse regression [3, 5] ha s provided geometric rates of convergence that hold once the iterates are close to the optimu m. In contrast, Theorem 1 guarantees geometric convergence if the iterates are not too clo se to the optimum b  X  . In Section 3.2, we describe a number of concrete models for which th e (RSC) and (RSM) conditions hold with  X  stat , which leads to the following result.
 Corollary 1. Suppose that the loss function satisfies conditions (RSC) an d (RSM) with tolerance  X  = O ( stat ) and parameters (  X  ` ,  X  u ,  X  ` ,  X  u ) . Then steps of the updates (2) ensures that k  X  T  X   X   X  k 2 = O ( 2 In the setting of statistical recovery, since the true parameter  X   X  is of primary interest, there is little point to optimizing to a tolerance beyond the statistical precis ion. To the best of our knowledge, this result X  X here fast convergence happens whe n the optimization error is larger than statistical precision X  X s the first of its type, and makes for an interesting contrast with other local convergence results. 3.2 Consequences for specific statistical models We now consider the consequences of Theorem 1 for some specific s tatistical models. In contrast to the previous deterministic results, these corollaries h old with high probability. Sparse linear regression: First, we consider the case of sparse least-squares regression . Given an unknown regression vector  X   X   X  R d , suppose that we make n i.i.d. observations of the form y i =  X  x i ,  X   X   X  + w i , where w i is zero-mean noise. For this model, each observation is of the form Z i = ( x i , y i )  X  R d  X  R . In a variety of applications, it is natural to assume that  X   X  is sparse. For a parameter q  X  [0 , 1] and radius R q &gt; 0, let us define the ` q  X  X all X  Note that q = 0 corresponds to the case of  X  X ard sparsity X , for which any vec tor  X   X  B 0 ( R 0 ) is supported on a set of cardinality at most R 0 . For q  X  (0 , 1], membership in B q ( R q ) enforces a decay rate on the ordered coefficients, thereby mode lling approximate sparsity. In order to estimate the unknown regression vector  X   X   X  B q ( R q ), we consider the usual Lasso program, with the quadratic loss function L (  X  ; Z n 1 ) := 1 2 n ` -norm regularizer R (  X  ) := k  X  k 1 . We consider the Lasso in application to a random design model, in which each predictor vector x i  X  N (0 ,  X ); we assume that max j =1 ,...,d  X  jj  X  1 for standardization, and that the condition number  X  ( X ) is finite.
 Corollary 2 (Sparse vector recovery) . Suppose that the observation noise w i is zero-mean and sub-Gaussian with parameter  X  , and  X   X   X  B q ( R q ) , and we use the Lasso program with  X  n = 2  X  It is worth noting that the form of statistical error stat given in bound (10) is known to be minimax optimal up to constant factors [13]. In related work, Ga rg and Khandekar [4] showed that for the special case of design matrices that satisfy t he restricted isometry prop-erty (RIP), a thresholded gradient method has geometric conver gence up to the tolerance k w k 2 / tistical error stat if n &gt; log d ; moreover, severe conditions like RIP are not needed to ensure fast convergence. In particular, Corollary 2 guarantees guaran tees geometric convergence up to stat for many random matrices that violate RIP. The proof of Corollary 2 involves exploiting some random matrix theory results [14] in order to verify t hat the RSC/RSM conditions hold with high probability (see the full-length version for de tails). Matrix regression with rank constraints: For a pair of matrices A, B  X  R m  X  m , we use  X  X  A, B  X  X  = trace( A T B ) to denote the trace inner product. Suppose that we are given n i.i.d. servation is of the form Z i = ( X i , y i )  X  R m  X  m  X  R . In many contexts, it is natural to assume that  X   X  is exactly or approximately low rank; applications include collaborative filtering and matrix completion [7, 15], compressed sensing [16], and multitask learn ing [19, 10, 17]. In order to model such behavior, we let  X  ( X   X  )  X  R m denote the vector of singular values of  X   X  (padded with zeros as necessary), and impose the constraint  X  ( X   X  )  X  B q ( R q ). We then consider the M -estimator based on the quadratic loss L ( X ; Z n 1 ) = 1 2 n combined with the nuclear norm R ( X ) = k  X  ( X ) k 1 as the regularizer.
 Various problems can be cast within this framework of matrix regres sion:  X  Matrix completion: In this case, observation y i is a noisy version of a randomly selected with one in position ( a ( i ) , b ( i )) and zeros elsewhere.  X  Compressed sensing: In this case, the observation matrices X i are dense, drawn from some random ensemble, with the simplest being X i  X  R m  X  m with i.i.d. N (0 , 1) entries.  X  Multitask regression: In this case, the matrix  X   X  is likely to be non-square, with the column size m 2 corresponding to the dimension of the response variable, and m 1 to the that the regression vectors (or columns of the matrix) lie close to a lower-dimensional subspace. See the papers [10, 17] for more details on reformulatin g this problem as an instance of matrix regression.
 For each of these problems, it is possible to show that suitable forms of the RSC/RSM conditions will hold with high probability. For the case of matrix complet ion, the paper [9] establishes a form of RSC useful for controlling statistical error; this argument can be suit-ably modified to establish related notions of RSC/RSM required for en suring fast algorithmic convergence. Similar statements apply to the settings of compres sed sensing and multi-task regression. For these matrix regression problems, consider the s tatistical precision rates that (up to logarithmic factors) are known to be minimax-opt imal [9, 17]. As dictated by this statistical theory, the regularization parameter should be chosen as  X  n = c X  for matrix completion, and  X  n = c X  stant. The following result applies to matrix regression problems for which the RSC/RSM conditions hold with tolerance  X  = stat .
 Corollary 3 (Low-rank matrix recovery) . Suppose that  X  ( X   X  )  X  B q ( R q ) , and the observa-tion noise is zero-mean  X  -sub-Gaussian. Then there are universal positive constant s c 1 , c 2 , c 3 such that with probability at least 1  X  exp(  X  c 3 n X  2 n ) , the iterates (2) with  X  =  X  mat  X  Here the contraction coefficient  X   X  (0 , 1) is a universal constant, independent of ( n, m, R q ), depending on the parameters (  X  ` ,  X  u ). We refer the reader to the full-length version for specific form taken for different variants of matrix regression. In this section, we provide some experimental results that confirm the accuracy of our theo-retical predictions. In particular, these results verify the predic ted linear rates of convergence under the conditions of Corollaries 2 and 3.
 Sparse regression: We consider a random ensemble of problems, in which each de-sign vector x i  X  R d is generated i.i.d. according to the recursion x (1) = z 1 and tion parameter. The singular values of the resulting covariance mat rix  X  satisfy the bounds  X  for all  X   X  [0 , 1); for  X  = 0, it is the identity, but it becomes ill-conditioned as  X   X  1. We the convergence properties for sample sizes n =  X s log d using different values of  X  . We note that the per iteration cost of our algorithm is n  X  d . All our results are averaged over 10 random trials.
 Our first experiment is based on taking the correlation parameter  X  = 0, and the ` q -ball parameter q = 0, corresponding to exact sparsity. We then measure converge nce rates for  X   X  X  1 , 1 . 25 , 5 , 25 } with d = 40000 and s = (log d ) 2 . As shown in Figure 1(a), the procedure fails to converge for  X  = 1: with this setting, the sample size n is too small for conditions (RSC) and (RSM) to hold, so that a constant step size leads to oscilla tions without these conditions. For  X  sufficiently large to ensure RSC/RSM, we observe a geometric conve rgence of the error k  X  t  X  b  X  k 2 , and the convergence rate is faster for  X  = 25 compared to  X  = 5, since the RSC/RSM constants are better with larger sample size.
 On the other hand, we expect the convergence rates to be slower when the condition number of  X  is worse; in addition to address this issue, we ran the same set of experiments with the correlation parameter  X  = 0 . 5. As shown in Figure 1(b), in sharp contrast to the case  X  = 0, we no longer observe geometric convergence for  X  = 1 . 25, since the conditioning of  X  with  X  = 0 . 5 is much poorer than with the identity matrix. Finally, we also expect o ptimization to be harder as the sparsity parameter q  X  [0 , 1] is increase away from zero. For larger q , larger sample sizes are required to verify the RSC/RSM conditions. F igure 1(c) shows that even with  X  = 0, setting  X  = 5 is required for geometric convergence.
 Low-rank matrices: We also performed experiments with two different versions of low-rank matrix regression, each time with m 2 = 160 2 . The first setting is a version of com-pressed sensing with matrices X i  X  R 160  X  160 with i.i.d. N (0 , 1) entries, and we set q = 0, and formed a matrix  X   X  with rank R 0 = d log m e . We then performed a series of trials with sample size n =  X R 0 m , with the parameter  X   X  { 1 , 5 , 25 } . The per iteration cost in this case is n  X  m 2 . As seen in Figure 2(a), the general behavior of convergence rat es in this problem stays the same as for the sparse linear regression problem : it fails to converge when  X  is too small, and converges geometrically (with a progressively faste r rate) as  X  increases. Figure 2(b) shows matrix completion also enjoys geometric converg ence, for both exactly low-rank ( q = 0) and approximately low-rank matrices. We have shown that even though high-dimensional M -estimators in statistics are neither strongly convex nor smooth, simple first-order methods can still e njoy global guarantees of geometric convergence. The key insight is that strong convexity a nd smoothness need only hold in restricted senses, and moreover, these conditions are sat isfied with high probabil-ity for many statistical models and decomposable regularizers used in practice. Examples include sparse linear regression and ` 1 -regularization, various statistical models with group-sparse regularization, and matrix regression with nuclear norm con straints. Overall, our results highlight that the properties of M -estimators favorable for fast rates in a statistical sense can also be used to establish fast rates for optimization algor ithms.
 Acknowledgements: AA, SN and MJW were partially supported by grants AFOSR-09NL184; SN and MJW acknowledge additional funding from NSF-CDI -0941742. [1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear [2] S. Becker, J. Bobin, and E. J. Candes. Nesta: a fast and accur ate first-order method [3] K. Bredies and D. A. Lorenz. Linear convergence of iterative so ft-thresholding. Journal [4] R. Garg and R. Khandekar. Gradient descent with sparsification : an iterative algorithm [5] E. T. Hale, Y. Wotao, and Y. Zhang. Fixed-point continuation for ` 1 -minimization: [6] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In [7] Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast con vex optimization [8] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for [9] S. Negahban and M. J. Wainwright. Restricted strong convexity and (weighted) matrix [10] S. Negahban and M. J. Wainwright. Estimation of (near) low-ran k matrices with noise [11] Y. Nesterov. Introductory Lectures on Convex Optimization . Kluwer Academic Pub-[12] Y. Nesterov. Gradient methods for minimizing composite object ive function. Techni-[13] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estima tion for high-[14] G. Raskutti, M. J. Wainwright, and B. Yu. Restricted eigenvalue conditions for corre-[15] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning [16] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank so lutions of linear matrix [17] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-r ank matrices. Tech-[18] J. A. Tropp and A. C. Gilbert. Signal recovery from random mea surements via orthog-[19] M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension reduction an d coefficient
