 Baichuan Li  X  Rong-Hua Li  X  Irwin King  X  Michael R. Lyu  X  Jeffrey Xu Yu Abstract In rating systems like Epinions and Amazon X  X  product review systems, users rate items on different topics to yield item scores. Traditionally, item scores are estimated by averagingalltheratingswithequalweights.Toimprovetheaccuracyofestimateditemscores, user reputation [a.k.a., user reputation (UR)] is incorporated. The existing algorithms on UR, however, have underplayed the role of topics in rating systems. In this paper, we first reveal that UR is topic-biased from our empirical investigation. However, existing algorithms cannot capture this characteristic in rating systems. To address this issue, we propose a topic-biased model (TBM) to estimate UR in terms of different topics as well as item scores. With TBM, we develop six topic-biased algorithms, which are subsequently evaluated with experiments using both real-world and synthetic data sets. Results of the experiments demonstrate that the topic-biased algorithms effectively estimate UR across different topics and produce more robust item scores than previous reputation-based algorithms, leading to potentially more robust rating systems.
 Keywords User reputation  X  Topic-biased model  X  Item scores  X  Rating system  X  User study 1 Introduction Since the advent of Web 2.0, there has been an increasing number of rating systems, where users rate items to produce item scores, e.g., App rating system in Apple X  X  App Store, 1 product review in Epinions, 2 voting systems in community question answering services. 3 Item scores are thus utilized for making decisions, such as ranking items and recommending good quality ones. At the very beginning, the ratings of an item are averaged out, with all users having equal weights. However, this averaging activity has poor accuracy when spammers are involved. To minimize the influence of spammers X  ratings, user reputation has been investigated in rating systems. In this connection, a number of studies have proposed Although there is no formal definition for user reputation, a running definition in rating systems is given below: Definition 1.1 ( User reputation (UR) ) In a rating system, user reputation evaluates a user X  X  user ratings are close to the items X   X  X rue X  scores.
 This running definition assumes that an item has a  X  X rue X  score which represents its objective quality. For example, in a product review system like Epinions, a computer X  X   X  X rue X  score etc. Therefore,  X  if a user ratings always deviate from items X   X  X rue X  scores, his/her UR is low;  X  if a user ratings are always close to items X   X  X rue X  scores, his/her UR is high. It is worth noting that in this paper, we only consider rating systems in which items X   X  X rue X  than personal preferences (e.g., in movie/music rating systems, ratings are usually affected by user preference). Therefore, UR only reflects the quality of one X  X  ratings when comparing to  X  X rue X  scores. However,  X  X rue X  scores do not exist in almost all rating systems. Thus, the above reputation-based algorithms estimate items X   X  X rue X  scores from the weighted average of users X  ratings. The higher UR the user has, the more weights the user X  X  ratings are given, and vice versa. Iteratively, UR and item scores are estimated. Due to the introduction of the reputation-based algorithms, the accuracy of estimating item scores has been improved in rating systems [ 16 , 30 ].

However, the existing reputation-based algorithms have not considered one of the most on different topics. Therefore, topic information may have influence on UR due to users X  various profiles, such as educational background, knowledge, and expertise. We validate this assumption on the Epinion data set [ 28 ], which consists of 0.9 million ratings applied to 296,277 items by 22,166 users. In addition, each rating is attached with a helpfulness score (ranges from 1 to 5, the higher the better), which reflects the quality of the rating and UR. In this data set, each item belongs to one category (topic), such as electronics, computer hardware, sports and outdoors. To investigate the influence of topics to users X  ratings, we calculate the mean variance of rater X  X  helpfulness score and observe that:  X  Among all users, the mean variance of rater X  X  helpfulness score is 1.264, while the mean variance of rater X  X  helpfulness score within the same category is 0.542 (decreases 57.1%).  X  Among 1,316 users who rate at least 100 products, the mean variance of rater X  X  helpfulness score is 1.187, while the mean variance of rater X  X  helpfulness score within the same category is 0.745 (decreases 37.2%).
 These observations reveal that in real-world rating systems, UR fluctuate across topics but is more stable within a topic. To a particular user, on some topics his UR is relatively low while in other topics his UR is higher. In other words, users have different UR on various topics. Therefore, it would be better to model UR at topic level. However, existing reputation-based algorithms only utilize a single value to model holistic UR and cannot capture this characteristic in rating systems.
 We use a toy example to illustrate this problem. Consider the following case shown in Table 1 , where five users give ratings to six items, together with these items X   X  X rue X  scores. We estimate UR for the five users using a state-of-the-art reputation-based algorithm L1-AVG [ 18 ]. The results are 0.80, 0.84, 0.92, 0.90, and 0.92 (UR is normalized to [0,1], the higher, the better). The above results only give an overall estimation of UR, from which it is hard to distinguish UR of u 1 and u 2 since they are close to each other. However, when we look into users X  ratings on different topics, we observe that u 1 has good ratings to items in Topic 1 but poor rating to items in Topic 2. Similarly, u 2  X  X  ratings for items in Topic 1 are estimate UR as a whole, we will ignore these fine-grained information and assign inaccurate under-estimated for Topic 1), which furthermore affects the accuracy of estimation on item scores.

To address the above problem, we propose a topic-biased model (TBM) to estimate UR on different topics together with item scores. The model assumes that a user has a series of UR with regard to different topics, and an item has a topic distribution over topics. As such, UR on a topic is estimated from the difference between the user ratings and item scores within the topic. In addition, an item score is derived from a total of weighted average of users X  ratings on those topics to which the item belongs. Iteratively, TBM computes UR and item scores until they converge to fixed values. Based on TBM, we develop six topic-biased algorithms, with four advantages highlighted as follows: 1. Robustness . The six algorithms are robust to unreliable ratings, especially for ratings on 2. Parallelization . The estimation of UR on each topic is independent and parallel compu-3. Adaptability . As the setting of topics is flexible, we can manually encode topics, utilize 4. Single parameter . Every algorithm has only one parameter, which is easy to tune in
To evaluate the performance of the six topic-biased algorithms, we carry out experiments using both real-world data and synthetic data. The experimental results are evaluated using four criteria: effectiveness in estimating UR, accuracy in estimating item scores, robustness to unreliable ratings, and convergence property. To the best of our knowledge, our approach is the first attempt to explore the relationship between UR and topics, and the main contributions of the research are threefold:  X  First, we provide empirical evidence to show that UR in rating systems fluctuates across topics, which is a crucial issues but did not receive much attention in previous reputation-based algorithms;  X  More importantly, we propose a TBM, with six topic-biased algorithms developed, to solve the fluctuation of UR across topics;  X  Finally, we conduct extensive experiments applying both a real-world data set and syn-thetic data sets to demonstrate the advantages of our topic-biased framework in accuracy of estimating UR and item scores, and robustness to unreliable ratings.
 Findings of the research thus bring new insights into building better rating systems. For example, our proposed TBM can be employed widely in various rating systems, including product rating of Amazon, rating tasks in crowdsourcing systems, and academic paper review systems.

This paper proceeds as follows. We briefly introduce the related work in Sect. 2 . Section 3 elaborates on TBM and topic-biased algorithms. Section 4 presents the experimental results using the data collected from a user study. In Sect. 5 , we conduct intensive simulations on synthetic data. Finally, we conclude this work in Sect. 6 . 2 Related work 2.1 Reputation system A rating system is usually embodied in a reputation system [ 11 , 25 , 26 ], which computes item scores (or item reputation) through collecting feedbacks that other entities (users) hold about the items. Common items include services, goods, and entities. There are different approaches to collect feedbacks and compute item scores, such as analyzing the content and Another approach by user-driven reputation systems is to provide a rating system in which users give explicit feedbacks via rating items. For instance, the mobile application rating system in Apple X  X  App Store, Amazon and Epinion X  X  product review systems, voting systems in community question answering services, etc. There is a large body of work for the user-driven reputation systems. A survey can be found in [ 15 ]. 2.2 Reputation-based algorithms for rating systems In recent years, reputation-based methods [ 20 ] for rating systems have attracted increas-ing attention from researchers. Mizzaro [ 22 ] proposes a reputation-based ranking algorithm (Mizz) for the assessment of scholarly papers. In [ 22 ], UR is measured by the root of L1-distance between paper quality and reader rating. Subsequently, Yu et al. [ 30 ]presentan iterative refinement algorithm (YZLM) for bipartite rating systems, with UR being consid-ered. In [ 30 ], UR is measured by the inverse of square L2-distance between item ranking and user rating. More recently, Zhou et al. [ 31 ] propose a similar reputation-based algorithm for rating systems, where the reputation is measured by the correlation coefficient between the user rating and item ranking. However, all of these reputation-based algorithms cannot guarantee convergence, thereby they are very hard to apply in practice. For addressing the concern on convergence, Kerchove et al. [ 8 ] propose a convergence algorithm (dKVD) from an optimization perspective. However, the convergence of the algorithm is severely affected by the parameter settings. In addition, the algorithm suffers when the number of spamming users is relatively large. To address the convergence and spamming issue, Li et al. develop six reputation-based algorithms in [ 18 ], which is mostly related to this work. In this paper, both L 1 and L 2 distances between item ranking and user rating, and three difference aggre-gation approaches (i.e., average, maximum, and minimum) are employed to estimate UR. Comparing with the above algorithm, these algorithms X  convergence is not sensitive to the initializations, and they are more immune to spamming users.

The above reputation-based algorithms estimate item scores and user rating reputation we use r s j to denote the j th estimated item score and c i to denote the i th raters X  UR at as follows: same time, Eq. 2 estimates c i with the divergence between u i  X  X  ratings and estimated item scores. The larger the difference, the lower the rating reputation, and vice versa.
With various divergence measurement functions, we can develop different algorithms. For instance, Kerchove et al. [ 8 ] employ mean square errors, and Li et al. [ 18 ] further explore L 1 and L 2 distances to measure the differences. However, all the above reputation-based algorithms do not consider UR as topic-biased and only employ a single value to model UR, without taking into account UR across different topics. 2.3 Topic-sensitive ranking Topic-sensitive ranking is also relevant to our work, of which the most representative algo-rithm is topic-sensitive PageRank [ 12 ]. Originally, PageRank [ 3 ] computes a single PageRank value for each web page. But the single PageRank value cannot capture the importance of the page on various topics. To address this concern, Haveliwala [ 12 ] proposes the topic-sensitive PageRank algorithm, which computes a set of PageRank values biased toward different topics. The experimental results demonstrate that topic-sensitive PageRank yields more accu-rate searching results. Similar topic-sensitive ranking algorithms have been applied in tag sensitive ranking which works on homogeneous graphs (e.g., web pages), the current study focuses on reputation-based ranking on heterogeneous graphs (e.g., users and items). 3 Model This section describes the topic-biased model and algorithms for estimating UR and item scores.Wefirstpresentourtopic-biasedmodelforaddressingURonvarioustopics(Sect. 3.1 ). Based on this model, we propose six topic-biased algorithms. After that, we employ a toy example to demonstrate the advantages of our algorithms over traditional reputation-based algorithms (Sect. 3.2 ). In the end, we analyze the property of convergence for one represen-tative algorithm (Sect. 3.3 ). Table 2 summarizes the notation used throughout the paper. 3.1 Topic-biased model and algorithms Traditional reputation-based model shown in Eqs. 1 and 2 leverages a single value (i.e., c i ) to model UR. However, it may suffer when users give unreliable ratings to items of some particular topics, while providing reliable ratings to items of other topics, as shown in the example in Sect. 1 .

To better model user rating reputation over different topics, we propose TBM. Utilizing the same notations in the above, we further incorporate a set of topics T and topic distributions of items B (an | O | X | T | matrix with the element b jk representing the degree to which that o |
U | X | T | matrix with c ik representing u i  X  X  rating reputation on t k . Initially, c 0 have: Equation 3 utilizes the sum of productions between UR and topic distributions as the weight-o  X  X  item score. Through decomposing c i to t UR on relevant topics and reduces the influence of UR on irrelevant topics to an item score. It is worth noting that TBM is a generalization of traditional reputation-based algorithms. When b jk = 1 / | T | for all t k , TBM degenerates to the traditional reputation-based models. From the above equations, we find that TBM only contains one parameter (i.e.,  X  ), which makes it feasible to tune in practice. Furthermore, the computation of c ik is independent and can be easily parallelized.

Using different divergence measurement functions ( D k (  X  ) ), we propose the topic-biased versions of the corresponding algorithms. Since the algorithms in [ 18 ] have great advantages than other reputation-based algorithms in convergence and robustness, we adopt them as sample algorithms, as shown in Table 3 . For instance, if D k (  X  ) employs the average L 1 distance, the iterative system of TB-L 1 -AVG algorithm is: where  X   X  ( 0 , 1 ) is a damping factor. It is worth noting that our TBM is not limited to algorithms in [ 18 ]. We can easily extend all the reputation-based algorithms introduced in Sect. 2.2 to topic-biased algorithms using TBM. 3.2 A toy example As a toy example, we estimate the UR of five users on two topics in Table 1 using TB-L 1 -AVG. Table 4 reports the estimated UR of these users, ranging from 0 to 1. As can be seen from Table 4 , L 1 -AVG only produces an overall estimation but fails to identify u 1 and u 2  X  varying rating reputation on topic 1 and topic 2. When examining the results of TB-L 1 -AVG, respectively. 3.3 Convergence property of topic-biased algorithms This section analyzes the convergence property of topic-biased algorithms. As shown in r j is computed based on c data. Thus, it is essential to consider the model X  X  time complexity. From Eqs. 3 and 4 ,wecan easily find that the time complexity of TBM is linear to the number of ratings in each round number of iterations. By using similar approach to [ 18 ], we show that the above topic-biased algorithms converge and the rate of convergence is exponential. In other words, r j and c ik in topic-biased algorithms only change little after a few iterations. In the following, we take TB-L 1 -AVG as an example (the proofs of TB-L 1 -AVG and TB-L 1 -AVG are given in the  X  X ppendix X ) and it is easy to obtain similar results for other algorithms.
 Proof We prove it by induction. If s = 1, then,
Assume when s = t the lemma holds, then we show s = t + 1 the lemma still holds. This completes the proof.
 Theorem 1 r j converges to a unique fixed point in TB-L 1 -AVG.
 there exists N satisfying For any s &gt; t  X  N ,wehave Clearly, r s j is a Cauchy sequence, thus it converges to a fixed point.
 Let X =| r ( 1 )  X   X  r ( 2 )  X  |= max j | r ( 1 ) j  X  r ( 2 ) j | ,then
Since  X   X  X  0 , 1 ) ,weget X &lt; X , which is impossible. Thus, r j converges to a unique fixed point.
 the rate of convergence for TB-L 1 -AVG is exponential.
 Proof We prove it by induction. For t = 1, Assume when t = s the theorem holds, we prove t = s + 1 it still holds. This completes the proof. 4 Experiment 1: Estimating topic-biased UR and item scores in user study The above section introduces TBM and applies one toy example to illustrate the capability of our model in estimating UR on different topics. Now our research focuses on the following three questions: 1. Do the topic-biased algorithms produce accurate estimations of UR in the real data set? 2. Through modeling topic-biased UR, can TBM give more accurate estimations to item 3. Are topic-biased algorithms more robust to unreliable ratings, compared with traditional We answer the three questions by conducting intensive experiments employing both real-world and synthetic data. In this section, we present our experiments with a user study. In the next section (Sect. 5 ), we show our simulations on a real-world data set and several synthetic data sets. In Sect. 4.1 , we present the setup for our user study. We make some observations in Sect. 4.2 . Sections 4.3 and 4.4 report the UR estimation and item score estimation results from our user study. 4.1 Setup The user study involves a rating task and a survey. In the task, the raters are invited to rate the quality of answers crawled from a community question answering (CQA) portal called Sina iAsk. 4 In Sina iAsk, users can post their own questions and answer other users X  questions. In most circumstances, a question receives more than one answer and the asker picks one as the best answer. Besides, the asker can let the community vote for the best answer. We randomly select 200 solved questions from four categories of Sina iAsk (namely, with each question having 3 X 9 answers. A total of 883 QA pairs are used in the rating task. In addition, we resort the answers of each question randomly to eliminate position bias. We educated with a Bachelor X  X  or Master X  X  degree or currently studying in their PhD programs. More importantly, their backgrounds cover a wide range of disciplines in natural sciences, social sciences, and engineering. All raters are first briefly informed of the research design. Afterward, they rate all these QA pairs using a 3-point Likert scale (A, B, and C, with A indicating  X  X he answer completely satisfies the information need, X  B indicating  X  X he answer partially satisfies the information need, X  and C indicating  X  X he answer is irrelevant X  [ 27 ]). We give them sufficient and flexible time but do not allow them to visit Sina iAsk to avoid plagiarism. By using our self-designed user interface, all the 30 raters complete the rating task, with a mean duration of 2h and 38min. After the ratings, raters are also invited to provide feedback or comments in the survey.

The ground truth of UR is defined as follows. Recall that UR measures the degree to which a user ratings are consistent with other users X  on the same items. We follow Mishra et al. X  X  as follows: represent the degree to which that o j belongs to topic k . We set the ground truth of UR on topic k :
We compare our topic-biased algorithms with the corresponding reputation algorithms in [ 18 ] and the averaging rating (AR) approach. The evaluations are made from two aspects: UR and item scores. We evaluate the performance of these algorithms in estimating UR using accuracy and measure their performance in estimating item scores from both accuracy and robustness. Kendall  X  coefficient, which represents the similarity between two rankings, is leveraged in our evaluation. 4.2 Observations We first study the relationships between UR and leaf categories. Since each QA pair only belongs to one particular leaf category, b jk becomes a binary variable. We apply Eq. 3 to calculate each UR on each  X  X opic X  and rank raters in a descending order. If UR is stable across four categories, where the x axis represents categories and the y axis stands for rankings. We observe that user rankings fluctuates on different categories. For instance, among these five raters, Rater 1 ranks second on Computer &amp; Internet , but ranks fifth on Education ; Rater divergence is that raters lack expertise in some questions/answers of certain topics, as one of the raters commented below:  X  X  am not sure about my ratings of answers to some topics, as I have little knowledge of those topics. So, I just rated using my common sense... X 
As the category is manually designed, and may cover various topics, we further study the connections between UR and different topics learned by topic models. We employ the latent Dirichlet allocation (LDA) [ 2 ] to automatically learn topics and topic distributions from the content of QA pairs. In our experiments, we use the tool GibbsLDA++ [ 24 ] with 20 topics and 1,000 iterations. Table 5 presents the top 5 words of some topics generated from LDA. We can see that, compared with categories, these learned topics are more fine-grained. For instance, topic 2 represents NBA games, and topic 8 is related to computer problems. Given refined topics, we examine user reputation across different topics. Figure 2 reports the rankings of the same five raters X  UR across topics. As shown in Fig. 2 , many raters X  rankings fluctuate greatly, like Rater 1, Rater 2, and Rater 3. Taking Rater 3 as an example: among these five raters, Rater 3 ranks second on topics 5, 9, and 15, but takes the last position on topics 8, 19, and 20. The above observations once again indicate that UR is topic-biased and varies across topics. Thus, it is reasonable to model UR taking account of topics.
However, previous reputation-based algorithms overgeneralize UR and use one single value to model it, resulting in an inaccurate estimation of UR and item scores. In the follow-ing two sections (Sects. 4.3 , 4.4 ), we report the results of our topic-biased models on UR estimation and item scores estimation, respectively. 4.3 UR estimation results In this section, we report the performance of various methods in estimating UR. The ground algorithms, and then investigate the influence of the parameter  X  to topic-biased algorithms.
Accuracy. Figure 3 reportstheKendall  X  sof30raters X  X ankingsonvarioustopicsgenerated from TB-L 1 -AVG (TB-Square-AVG) and from Eq. 9 . In most topics, the rankings of TB-L -AVG and TB-Square-AVG are in line with the ground truth (  X &gt; 0 . 75). For other topic-biased algorithms, the Kendall  X  s are all above 0.5 on each topic. These results demonstrate the effectiveness of TBM in estimating UR across various topics.

Effect of  X  . Figure 4 shows the mean Kendall  X  of 30 raters X  rankings generated from six topic-biased algorithms and from Eq. 9 . Across all six algorithms, TB-L 1 -AVG and TB-Square-AVG generate the closest rankings to the ground truth. When  X  = 0 . 1, the rankings of TB-L 1 -AVG and TB-Square-AVG are mostly consistent with the ground truth. With an increase of  X  , Kendall  X  decreases. The setting of  X  has no much impact on the performance TB-L 1 -MAX and TB-Square-MAX. The best rankings of TB-L 1 -MAX and TB-Square-MAX are achieved when  X  = 0 . 7and  X  = 0 . 9, respectively. TB-L 1 -MIN and TB-Square-MIN are not sensitive to the setting of  X  since they give the most conservative estimations on UR: only the minimum differences are taken into account. 4.4 Item score estimation results In this section, we report the performance of various methods in estimating item scores. We rank answers of each question according to their scores generated from AR, traditional reputation-based algorithms, and topic-biased algorithms. For each method, 200 rankings are obtained X  X ne ranking for each question. The best values of  X  are set to each algorithm so that the generated rankings of UR are closest to the ground truth derived from Eqs. 8 and 9 , respectively.

Accuracy. Due to the lack of ground truth, we employ pairwise Kendall  X  between two rankings to measure the accuracy of item scores estimation. Table 6 shows the pairwise mean Kendall  X  between average rating (AR), L 1 -AVG, and TB-L 1 -AVG. We find that the rankings of TB-L 1 -AVG and L 1 -AVG are most consistent. In most cases, the mean Kendall  X  between AR and TB-L 1 -AVG is also higher than the mean Kendall  X  between L 1 -AVG and AR. Other topic-biased algorithms report similar results. From these results, we know that our proposed algorithms generate mediate rankings between AR and traditional reputation-based algorithms although we cannot say which one produces the most accurate rankings.
Robustness. To evaluate the robustness of various methods, we generate spammers who rate randomly from A to C for all QA pairs and calculate the mean Kendall  X  between the rankings of original data and the rankings of data with spammers. Figure 5 shows the results. We can find that the Kendall  X  of TB-L 1 -AVG and TB-Square-AVG is always much higher than that of AR with the number of spammers varying from 1 to 5. In addition, TB-L 1 -AVG and TB-Square-AVG perform slightly better than L 1 -AVG and Square-AVG, respectively. As the topic-biased algorithms consider user reputation across topics, they produce more comprehensive estimations of spammers X  UR across topics, with spammers X  influences on item scores being reduced. 5 Experiment 2: Estimating topic-biased UR and item scores in simulations We undertake intensive simulations in this section. As we have no ground truth for item scores (i.e., answer quality) in our user study, it is hard to measure TBM X  X  accuracy in estimating item scores. Furthermore, the performance of TBM on large-scale data is unknown. Our simulations in this section address these concerns. In Sect. 5.1 , we detail the process of generating simulation data. In Sect. 5.2 , we report the performance of different methods in estimating UR and item scores. Finally, we discuss the algorithm selection and parameter setting in Sect. 5.3 . 5.1 Setup The synthetic data sets generated are ratings for products, each one encompassing 1,000 items, 10,000 users, and 20 categories (topics). The procedures are as follows: 1. We generate true scores of products from a normal distribution with a mean of 5.5 and 2. We generate UR vector ( URV ) for each user. URV has a length of 20, with the value of 3. For each item, we draw a random number of raters from a power law distribution [ 6 ] 4. We generate the topic distribution vector TDV for every item, with each element repre-5. For each item, we randomly select the number of raters generated from step 3 and simulate
We generate 9 data sets with various S sand B s ranging from 0 to 5,000. When S = 0 and B = 0, all users are normal users; when S = 0and B = 0, the data sets contain normal users and topic-biased users; when S = 0and B = 0, the data sets contain normal users and spammers; when S = 0and B = 0, the data sets contain all the above three types of users. We compare the performance of algorithms listed in Table 3 . Each algorithm runs 20 iterations with different settings of  X  ranging from 0.1 to 0.9. In addition, we employ the averaged rating (AR) as our baseline method for estimating item scores. It is worth noting that AR lacks an estimation of UR.

The evaluations are also made from two aspects: UR and item scores. We employ Z test to verify the effectiveness of the estimated UR. Specifically, we first obtain three groups of UR in terms of normal raters, biased raters, and spammers. Then, we conduct Z tests to examine whether the estimated UR values of these groups are significantly different or not. For item scores, we employ mean average error (MAE) and root mean square error (RMSE) to measure each algorithm X  X  accuracy and robustness. 5.2 Results 5.2.1 UR estimation results We present the performance of different methods in estimating UR in this section. Recall that the proposed TBM has the advantage of estimating UR across topics. For verification, we collect the estimated UR for normal users, biased users, and spammers from each topic, and calculate the means and standard deviations. Figure 7 shows error bars of estimated UR of these users using TB-L 1 -AVG and TB-L 1 -MAX. We can see that the estimated UR values of these three types of users are distinct from each other. When there is no spammers, the UR values of normal users are much higher than those of biased users. With the increase of number of spammers, the UR of normal users decreases, while the UR of spammers and biased users increases. When the number of spammer users is large enough, their UR is even higher than that of normal users. Since spammers become the majority, normal users X  ratings are not treated as reliable ones any more. It is worth noting that the estimated UR of constantly give higher ratings. Although the spammers give random ratings, the estimated UR of biased users are still lower than that of spammers. We further conduct Z test for each pair (i.e., NU vs. SP, NU vs. BU, NU vs. (SP+BU), BU vs. SP), with all 150 (25  X  6) tests biased algorithms are able to effectively estimate UR across topics and distinguish unreliable ratings from biased users or spammers, and reliable ratings from normal users. 5.2.2 Item score estimation results Tables 7 and 8 report the MAE and RMSE of various algorithms across different number of biased users and spammers (the best results are in bold). For reputation-based algorithms and topic-biased algorithms, the best settings of  X  are adopted.

Accuracy. The MAE and RMSE of topic-biased algorithms are lower than those of AR and traditional reputation-based algorithms in almost all cases. Among all topic-biased algo-rithms, TB-L 1 -MAX performs the best. This is due to the simulation of biased users: Here, all biased users give the highest ratings. Thus, the max () captures UR best. Among all set-tings, TBM algorithms perform the best except when S = 0and B = 1,000. As there are only a few biased users in this setting, some traditional algorithms like L 1 -MAX and Square-MAX are able to give accurate estimations of UR. With the increasing number of unreliable ratings, however, our TBM algorithms outperform traditional methods. For instance, when B = 5,000, the MAE of TB-L 1 -AVG is 118% lower than that of AR and 27% lower than that of L 1 -AVG; the MAE of TB-L 1 -MAX is 134% lower than that of AR and 45% lower than that of L 1 -MAX. Similar results are observed for other reputation-based and topic-biased algorithms. The RMSEs of TBM algorithms in Table 8 show similar results. The above results demonstrate the effectiveness of TBM: Estimating topic-biased UR and allocating lower weighting to unreliable ratings provide a more accurate estimation of item scores relative to previous algorithms.

Robustness. An increase in B and S leads to a growing number of biased users and spammers, leading to an increase in MAE and RMSE of the three algorithms. In other words, all algorithm X  X  robustness decreases with the increase in proportions of unreliable ratings. However, compared to AR and traditional reputation-based algorithms, topic-biased algorithms are more robust to unreliable ratings due to the slowest growth in MAE and RMSE. For instance, when S = 0, from B = 1,000 to B = 5,000, the MAEs of AR and L -AVG increase by 175% and by 81%, respectively, while the MAE of TB-L 1 -AVG only increases by 43%.
 its influence on the performance of topic-biased algorithms. Figure 8 shows the MAE of TB-L 1 -AVG, TB-L 1 -MAX, and TB-L 1 -MIN under different settings of  X  . The best  X  is 0.8 for TB-L 1 -AVG, and the best  X  is around 0.6 for TB-L 1 -MAX. Like the results of CQA data, TB-L 1 -MIN is less sensitive to the parameter setting.

Rate of convergence. Figures 9 and 10 show the mean and variance of maximum difference of item scores between two consecutive iterations for different topic-biased algorithms when B = 5,000 and S = 4,000. We find that both the difference and the variance decreases rapidly and TBM converges within 15 iterations. These results tally with their good properties of convergence, which enables these topic-biased algorithms to deal with large-scale data. 5.3 Discussion 5.3.1 Algorithm selection From two experiments, we find that topic-biased algorithms usually perform better than corresponding reputation-based algorithms. However, among six topic-biased algorithms, there is no one algorithm which consistently outperforms others. The possible reason is that in performance of each algorithm. AVG-based algorithms capture the mean deviation as the measurement of UR, while MIN-based and MAX-based algorithms use the minimum and maximum deviations. In some cases where the differences between user ratings and true scores within the same topic do not change too much, AVG-based algorithms obtain better performance, such as our user study. In some other cases where the differences between user ratings and true scores within the same topic vary greatly, MAX-based algorithms may perform better, like our simulations. These suggest that, in order to get the best UR and item score estimation, we need to analyze data characteristics and choose the most suitable algorithm. 5.3.2 Parameter setting In our models,  X  is a damping factor which controls the speed of convergence. According to Theorem 2 , smaller value of  X  leads to faster convergence. In addition, we investigate the influence of  X  to different models X  performance in our experiments. Experimental results show that AVG-based and MAX-based algorithms are more sensitive to the setting of  X  , while MIN-based algorithms are insensitive. The reason is that MIN-based algorithms are the most lenient in UR estimation; thus, different parameter settings do not change results too much. For AVG-based and MAX-based algorithms, there is no an optimal  X  for all cases. In other words, the best setting of the damping factor is depended on the data rather than the algorithm. Therefore, in practice a validation set is required to get a good parameter setting for each particular task. 6Conclusion In rating systems, UR plays an essential role in estimating item scores. For this purpose, a numberofstudieshavethereforeemployedURinestimatingitemscores.However,traditional reputation-based approaches neglect the role of topics, resulting in biased ratings. To this end, we first show that UR fluctuates across different topics from the Epinions data set. Based on this, we thereby develop a topic-biased model to estimate UR and item scores simultaneously. Accordingly, we develop six topic-biased algorithms. After analyzing the convergence property mathematically, we subsequently conduct a two-step validation of the topic-biased algorithms. First, we present the effectiveness and robustness of topic-biased algorithms with the data collected from our user study. Second, we conduct large-scale simulations to further examine their performance. Results of the experiments indicate that these topic-biased algorithms are effective in estimating UR on various topics and item scores, and are more robust to unreliable ratings (including biased ratings and spam ratings), in relation to the existing reputation-based algorithms.

Our proposed algorithms, with good convergence property, can contribute to better user reputation estimation and item score estimation. In addition, the proposed algorithms are applicable to most objective rating systems, like product review system and academic paper review system. What X  X  more, it is feasible to incorporate topic modeling methods in the topic-biased model.
 Currently, we apply our topic-biased algorithms to objective ratings like product reviews. For subjective ratings like movie ratings, in which user preference and taste are incorporated in ratings, these algorithms may not be applicable for making recommendations. However, filtering or collaborative filtering. In future studies, we aim to investigate better modeling of UR since the influence of the damping factor (i.e.,  X  ) to current models is data depen-dent. In addition, we plan to collect more data using crowdsoucing systems (e.g., Amazon X  X  Mechanical Turk) to test the performance of our algorithms with different data sets. 7 Appendix In this appendix, we give the proof that Lemma 1 holds for TB-L 1 -MAX and TB-L 1 -MIN. Theorem 1 and 2 for them and other topic-biased algorithms are similar to the proof of TB-L 1 -AVG. 7.1 TB-L 1 -MAX Proof If s = 1, let then
Assume when s = t the lemma holds, then we show s = t + 1 the lemma still holds. Let then | This completes the proof. 7.2 TB-L 1 -MIN Proof If s = 1, let then
Assume when s = t the lemma holds, then we show s = t + 1 the lemma still holds. Let then | This completes the proof. References
