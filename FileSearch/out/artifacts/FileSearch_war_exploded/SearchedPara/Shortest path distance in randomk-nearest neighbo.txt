 Morteza Alamgir 1 morteza@tuebingen.mpg.de Ulrike von Luxburg 1 , 2 ulrike.luxburg@tuebingen.mpg.de
Max Planck Institute for Intelligent Systems, T  X ubingen, Germany Department of Computer Science, University of Hamburg, Germany The shortest path distance is the most fundamental distance function between vertices in a graph, and it is widely used in computer science and machine learning. In this paper we want to understand the geometry in-duced by the shortest path distance in randomly gener-ated geometric graphs like k -nearest neighbor graphs. Consider a neighborhood graph G built from an i.i.d. sample X 1 ,...,X n drawn according to some density p on X  X  R d (for exact definitions see Section 2). As-sume that the sample size n goes to infinity. Two questions arise about the behavior of the shortest path distance between fixed points in this graph: 1. Weight assignment: Given a distance measure D on X , how can we assign edge weights such that the shortest path distance in the graph converges to D ? 2. Limit distance: Given a function h that assigns weights of the form h ( k X i  X  X j k ) to edges in G , what is the limit of the shortest path distance in this weighted graph as n  X  X  X  ? The first question has already been studied in some special cases. Tenenbaum et al. (2000) discuss the case of  X  -and kNN graphs when p is uniform and D is the geodesic distance. Sajama &amp; Orlitsky (2005) extend these results to  X  -graphs from a general density p by introducing edge weights that depend on an explicit es-timate of the underlying density. In a recent preprint, Hwang &amp; Hero (2012) consider completely connected graphs whose vertices come from a general density p and whose edge weights are powers of distances. There is little work regarding the second question. Tenenbaum et al. (2000) answer the question for a very special case with h ( x ) = x and uniform p . Hwang &amp; Hero (2012) study the case h ( x ) = x a , a &gt; 1 for arbi-trary density p .
 We have a more general point of view. In Section 4 we show that depending on properties of the function h ( x ), the shortest path distance operates in different regimes, and we find the limit of the shortest path distance for particular function classes of h ( x ). Our method also reveals a direct way to answer the first question without explicit density estimation.
 An interesting special case is the unweighted kNN graph, which corresponds to the constant weight func-tion h ( x ) = 1. We show that the shortest path dis-tance on unweighted kNN-graphs converges to a limit distance on X that does not conform to the natural intuition and induces a geometry on X that can be detrimental for machine learning applications.
 Our results have implications for many machine learn-ing algorithms, see Section 5 for more discussion. (1) The shortest paths based on unweighted kNN graphs prefer to go through low density regions, and they even accept large detours if this avoids passing through high density regions (see Figure 1 for an illustration). This is exactly the opposite of what we would like to achieve in most applications. (2) For manifold learning algo-rithms like Isomap, unweighted kNN graphs introduce a fundamental bias that leads to huge distortions in the estimated manifold structure (see Figure 2 for an illustration). (3) In the area of semi-supervised learn-ing, a standard approach is to construct a graph on the sample points, then compute a distance between ver-tices of the graph, and finally use a standard distance-based classifier to label the unlabeled points (e.g., Sa-jama &amp; Orlitsky, 2005 and Bijral et al., 2011). The crucial property exploited in this approach is that dis-tances between points should be small if they are in the same high-density region. Shortest path distances in unweighted kNN graphs and their limit distances do exactly the opposite, so they can be misleading for this approach. Consider a closed, connected subset X  X  R d that is endowed with a density function p with respect to the Lebesgue measure. For the ease of presen-tation we assume for the rest of the paper that the density p is Lipschitz continuous with Lipschitz con-stant L and bounded away from 0 by p min &gt; 0. To simplify notation later on, we define the shorthand q ( x ) := ( p ( x )) 1 /d .
 We will consider different metrics on X . A ball with respect to a particular metric d in X will be written as B ( x,r,d ) := { y  X  X | d ( x,y )  X  r } . We denote the Euclidean volume of the unit ball in R d by  X  d . Assume the finite dataset X 1 ,...,X n has been drawn i.i.d according to p . We build a geometric graph G = ( V,E ) that has the data points as vertices and connects vertices that are close. Specifically, for the kNN graph we connect X i with X j if X i is among the k nearest neighbors of X j or vice versa. For the  X  -graph, we connect X i and X j whenever their Euclidean distance satisfies k X i  X  X j k  X   X  . In this paper, all graphs are undirected, but might carry edge weights w ij  X  0. In unweighted graphs, we define the length of a path by its number of edges, in weighted graphs we define the length of a path by the sum of the edge weights along the path. In both cases, the shortest path ( SP ) distance D sp ( x,y ) between two vertices x,y  X  V is the length of the shortest path connecting them.
 Let f be a positive continuous scalar function defined on X . For a given path  X  in X that connects x with y and is parameterized by t , we define the f -length of the path as This expression is also known as the line integral along  X  with respect to f . The f -geodesic path between x and y is the path with minimum f -length.
 The f -length of the geodesic path is called the f -distance between x and y . We denote it by d f ( x,y ). If f ( x ) is a function of the density p at x , then the f -distance is sometimes called a density based distance (Sajama &amp; Orlitsky, 2005).
 The f -distance on X is a metric, and in particular it satisfies the triangle inequality. Another useful prop-erty is that for a point u on the f -geodesic path be-tween x and y we have D f ( x,y ) = D f ( x,u )+ D f ( u,y ). The function f determines the behavior of the f -distance. When f ( x ) is a monotonically decreasing function of density p ( x ), passing through a high den-sity region will cost less than passing through a low density region. It works the other way round when f is a monotonically increasing function of density. A constant function does not impose any preference be-tween low and high density regions.
 The main purpose of this paper is to study the rela-tionship between the SP distance in various geomet-ric graphs and particular f -distances on X . For ex-ample, in Section 3 we show that the SP distance in unweighted kNN graphs converges to the f -distance with f ( x ) = p ( x ) 1 /d .
 In the rest of the paper, all statements refer to points x and y in the interior of X such that their f -geodesic path is bounded away from the boundary of X . In this section we study the behavior of the shortest path distance in the family of unweighted kNN graphs. We show that the rescaled graph SP distance con-verges to the q -distance in the original space X . Theorem 1 ( SP limit in unweighted kNN graphs) Consider the unweighted kNN graph G n based on the i.i.d. sample X 1 ,...,X n  X  X from the density p . Choose  X  and a such that  X   X  Fix two points x = X i and y = X j . Then there definitions) such that with probability at least 1  X  3 e 3 n exp(  X   X  2 k a / 6) we have Moreover if n  X   X  , k  X   X  , k/n  X  0 ,  X   X  0 and  X  k a / log( n )  X   X  , then the probability converges to 1 and ( k/ (  X  d n )) 1 /d D sp ( x,y ) converges to D q probability.
 The convergence conditions on n and k are the ones to be expected for random geometric graphs. The con-dition  X  2 k a / log( n )  X   X  is slightly stronger than the usual k/ log( n )  X  X  X  condition. This condition is sat-isfied as soon as k is of the order a bit larger than log( n ). For example k  X  log( n ) 1+  X  with a small  X  will work. For k smaller than log( n ), the graphs are not connected anyway (see e.g. Penrose, 1999) and are unsuitable for machine learning applications.
 Before proving Theorem 1, we need to state a couple of propositions and lemmas. We start by introducing some ad-hoc notation: Definition 2 (Connectivity parameters) Con-sider a geometric graph based on a fixed set of points X 1 ,...,X n  X  R d . Let r low be a real number such that D f ( X i ,X j )  X  r low implies that X i is connected to X in the graph. Analogously, consider r up to be a real number such that D f ( X i ,X j )  X  r up implies that X i not connected to X j in the graph.
 Definition 3 (Dense sampling assumption) Con-sider a graph G with connectivity parameters r low and r up . We say that it satisfies the dense sampling as-sumption if there exists an  X  &lt; r low / 4 such that for all x  X  X there exists a vertex y in the graph with D f ( x,y )  X   X  .
 Proposition 4 (Bounding D sp by D f ) Consider any unweighted geometric graph based on a fixed set X 1 ,...,X n  X  X   X  R d that satisfies the dense sampling assumption. Fix two vertices x and y of the graph and set Then the following statement holds: Proof. Right hand side . Consider the f -geodesic path  X   X  x,y connecting x to y . Divide  X   X  x,y to segments by u 0 = x,u 1 ,...,u t ,u t +1 = y such that D f ( u i ,u r low  X  2  X  for i = 0 ,...,t  X  1 and D f ( u t ,u t +1 )  X  r (see Figure 3). Because of the dense sampling assump-tion, for all i = 1 ,...,t there exists a vertex v i in the ball B ( u i , X  ; D f ) and we have Applying the triangle inequality gives D f ( v i ,v i +1 )  X  r low , which shows that v i and v i +1 are connected. By summing up along the path we get In step (a) we use the simple fact that if u is on the f -geodesic path from x to y , then Left hand side. Assume that the graph SP between x and y consists of vertices z 0 = x,z 1 ,...,z s = y . By D f ( z i ,z i +1 )  X  r up we can write ( r low  X  2  X  ) D sp ( x,y )  X  The next lemma uses the Lipschitz continuity and boundedness of p to show that q ( x ) k x  X  y k is a good approximation of D q ( x,y ) in small intervals. Lemma 5 (Approximating D q in small balls) Consider any given  X  &lt; 1 . If k x  X  y k X  p min  X /L then the following statements hold: 1. We can approximate p ( y ) by the density at x : 2. We can approximate D q ( x,y ) by q ( x ) k x  X  y k : (1  X   X  ) 1 /d q ( x ) k x  X  y k X  D q ( x,y )  X  (1+  X  ) 1 /d Proof. Part (1). By the Lipschitz continuity of p for k x  X  y k X   X  we have Setting  X  =  X p min /L leads to the result.
 Part (2). The previous part can be written as Denote the q -geodesic path between x and y by  X   X  and the line segment connecting x to y by l . Using the definition of a q -geodesic path, we can write Also, Z Now we are going to show how the quantities r low and r up introduced in Definition 2 can be bounded in ran-dom unweighted kNN graphs and how they are related to the metric D q .
 To this end, we define the kNN q -radii at vertex x as R q,k ( x ) = D q ( x,y ) and the approximated kNN q -radii at vertex x as  X  R q,k ( x ) = q ( x ) k x  X  y k , where y is the k -nearest neighbor of x . The minimum and maximum values of kNN q -radii are defined as Accordingly we define  X  R min q,k and  X  R max p,k for the approx-imated q -radii.
 The following proposition is a direct adaptation of Proposition 31 from von Luxburg et al. (2010).
 Proposition 6 (Bounding R min p,k and R max p,k ) Given  X  &lt; 1 / 2 define r low and r up as r and radius  X  r low and  X  r up as Assume that  X  r up  X   X p 1+1 /d min /L . Then Proof. Consider a ball B x with radius  X  r low /q ( x ) around x . Note that  X  r low /q ( x )  X  p min  X /L , so we can bound the density of points in B x by (1 +  X  ) p ( x ) using Lemma 5. Denote the probability mass of the ball by  X  ( x ), which is bounded by Observe that  X  R q,k ( x )  X   X  r low if and only if there are at least k data points in B x . Let Q  X  Binomial ( n, X  ( x )) and S  X  Binomial ( n, X  max ). By the choice of  X  r low we have E ( S ) = k/ (1 +  X  ). It follows that
P  X  R q,k ( x )  X   X  r low = P Q  X  k  X  P S  X  k Now we apply a concentration inequality for binomial random variables (see Prop. 28 in von Luxburg et al., 2010) and a union bound to get By a similar argument we can prove the analogous statement for R max p,k . Finally, Lemma 5 gives The following proposition shows how the sampling pa-rameter  X  can be chosen to satisfy the dense sampling assumption. Note that we decided to choose  X  in a form that keeps the statements in Theorem 1 simple, rather than optimizing over the parameter  X  to maxi-mize the probability of success.
 Lemma 7 (Sampling lemma) Assume X 1 ,...,X n are sampled i.i.d. from a probability distribution p and a constant a &lt; 1 is given. Set  X  as in Theorem 1, least 1  X  e 3 n exp(  X  k a / 6) , for every x  X  X exists a y  X  X 1 ,...,X n such that D q ( x,y )  X   X  .
 Proof. Define  X  0 = (1 +  X  )  X  1 /d  X  . We prove that for every x  X  X  , there exist a vertex y such that q ( x ) k x  X  y k X   X  0 . Then using Lemma 5 will give the result. The proof idea is a generalization of the covering ar-gument in the proof of the sampling lemma in Tenen-baum et al. (2000). We first construct a covering of X that consists of balls with approximately the same probability mass. The centers of the balls are chosen by an iterative procedure that ensures that no center is contained in any of the balls we have so far. We choose the radius  X  0 /q ( x ) for the ball at point x and call it B q ( x, X  0 ). The probability mass of this ball can be bounded by disjoint. To see this, consider two balls B q ( x, (1  X   X  ) 1 /d  X  0 / 2), B q ( y, (1  X   X  ) 1 /d  X  0 / 2). Observe that We can bound the total number of balls by Now we sample points from the underlying space and apply the same concentration inequality as above. We bound the probability that a ball B q ( u, X  0 ) does not contain any sample point ( X  X s empty X ) by Rewriting and Substituting the value of  X  0 gives
Pr (no ball is empty)  X  1  X  P i Pr ( B i is empty)  X  1  X  Proof of Theorem 1. Set r low and r up as in Proposition 6. The assumption on  X  ensures that  X  r that the statements about r low and r up in Defini-tion 2 both hold for G n with probability at least  X  1 = 1  X  2 n exp(  X   X  2 k/ 6). Set  X  as in Lemma 7 and define the constant a &lt; 1  X  log k 4 d (1 +  X  ) 2 . By this choice we have r low &gt; 4  X  . Lemma 7 shows that the sampling assumption holds in G n for the selected  X  with probability at least  X  2 = 1  X  e 3 n exp(  X  k a / 6). To-gether, all these statements about G n hold with prob-ability at least  X  := 1  X  3 e 3 n exp(  X   X  2 k a / 6). Using Proposition 4 completes the first part of the the-orem. For the convergence we have This shows that e 1  X  1 as  X   X  0 and k  X   X  . For  X   X  0 and k  X   X  we can set a to any constant smaller than 1. Finally it is easy to check that e 2  X  0 and  X /r low  X  0. 2 In this section we discuss both questions from the In-troduction. We also extend our results from the pre-vious section to weighted kNN graphs and  X  -graphs. 4.1. Weight assignment problem Consider a graph based on the i.i.d. sample X 1 ,...,X n  X  X from the density p . We are given a positive scalar function f which is only a function of the density: f ( x ) =  X  f ( p ( x )). We want to assign edge weights such that the graph SP distance converges to the f -distance in X .
 It is well known that the f -length of a curve  X  : [ a,b ]  X  X can be approximated by a Riemann sum over a par-tition of [ a,b ] to subintervals [ x i ,x i +1 ]: As the partition gets finer, the approximation  X  D f, X  converges to D f, X  (cf. Chapter 3 of Gamelin, 2007). This suggests using edge weights However the underlying density p ( x ) is not known in many machine learning applications. Sajama &amp; Orl-itsky (2005) already proved that the plug-in approach using a kernel density estimator  X  p ( x ) for p ( x ) will lead to the convergence of the SP distance to f -distance in  X  -graphs. Our next result hows how to choose edge weights in kNN graphs without estimating the density. It is a corollary from a theorem that will be presented in Section 4.2.
 We use a notational convention to simplify our argu-ments and hide approximation factors that will even-tually go to 1 as the sample size goes to infinity. We say that f is approximately larger than g ( f &lt;  X  g ) if there exists a function e (  X  ) such that f  X  e (  X  ) g and e (  X  )  X  1 as n  X  X  X  and  X   X  0. The symbol 4  X  is de-fined similarly. We use the notation f  X   X  g if f 4  X  g and f &lt;  X  g .
 Corollary 8 (Weight assignment) Consider the kNN graph based on the i.i.d. sample X 1 ,...,X n  X  X with  X  f increasing. We assume that  X  f is Lipschitz con-tinuous and f is bounded away from 0. Set the edge weights Fix two points x = X i and y = X j . Choose  X  and a as in Theorem 9. Then with probability at least 1  X  3 e 3 n exp(  X   X  2 k a / 6) we have D sp ( x,y )  X   X  D f ( x,y ) . 4.2. Limit distance problem Consider a weighted graph based on the i.i.d. sample X 1 ,...,X n  X  X from the density p . We are given a increasing edge weight function h : R +  X  R + which assigns weight h ( k x  X  y k ) to the edge ( x,y ). We are interested in finding the limit of the graph SP distance with respect to edge weight function h as the sample size goes to infinity. In particular we are looking for a distance function f such that the SP distance con-verges to the f -distance.
 Assume we knew the solution f  X  =  X  f  X  ( p ( x )) of this problem. To guarantee the convergence of the dis-tances, f  X  should assign weights of the form of w ij  X   X  f  X  ( p ( X i )) k X i  X  X j k . This would mean which shows that determining  X  f  X  is closely related to finding a density based estimation for k X i  X  X j k . Depending on h , we distinguish two regimes for this problem: subadditive and superadditive. 4.2.1. Subadditive weights A function h ( x ) is called subadditive if  X  x,y  X  0 : h ( x ) + h ( y )  X  h ( x + y ). Common examples of subaddi-tive functions are f ( x ) = x a , a &lt; 1 and f ( x ) = xe For a subadditive h , the SP in the graph will sat-isfy the triangle inequality and it will prefer jumping along distant vertices. Based on this intuition, we come up with the following guess for vertices along the SP : For  X  -graphs we have the approximation k X i  X  X j k  X   X  and f ( x ) = h (  X  ) / X  . For kNN-graphs we have k X i  X  X j k  X  r/q ( X i ) with r = ( k/ ( n X  d )) and We formally prove this statement for kNN graphs in the next theorem. In contrast to Theorem 1, the scal-ing factor is moved into f . The proof for  X  -graphs is much simpler and can be adapted by setting r =  X  , q ( x ) = 1, and r low = r up =  X  .
 Theorem 9 (Limit of SP in weighted graphs) Consider the kNN graph based on the i.i.d. sample X 1 ,...,X n  X  X from the density p . Let h be an increasing, Lipschitz continuous and subadditive func-tion, and define the edge weights w ij = h ( k X i  X  X j k ) . Fix two points x = X i and y = X j . Define r = ( k/ ( n X  d )) 1 /d and set Choose  X  and a such that  X   X  Then with probability at least 1  X  3 e 3 n exp(  X   X  2 we have D sp ( x,y )  X   X  D f ( x,y ) .
 Proof. The essence of the proof is similar to the one in Theorem 1, we present a sketch only. The main step is to adapt Proposition 4 to weighted graphs with weight function h . Adapting Lemma 5 for gen-eral f is straightforward. The lemma states that D f ( x,y )  X   X  f ( x ) k x  X  y k for nearby points. We set r low and  X  as in the sampling lemma and Proposition 6 (these are properties of kNN graphs and hold for any f ). Proposition 6 says that in kNN graphs, x is con-nected to y with high probability iff k x  X  y k 4  X  r/q ( x ). The probabilistic argument and the criteria on choos-ing  X  are similar to Theorem 1.
 First we show that D sp ( x,y ) 4  X  D f ( x,y ). Consider the f -geodesic path  X   X  x,y connecting x to y . Divide  X  x,y into segments u 0 = x,u 1 ,...,u t ,u t +1 = y such that D q ( u i ,u i +1 ) = r low  X  2  X  for i = 0 ,...,t  X  1 and D q ( u t ,u t +1 )  X  r low  X  2  X  (see Figure 3). There exists a vertex v i near to u i such that v i and v i +1 are connected. We show that the length of the path x,v 1 ,...,v t ,y is approximately smaller than D f ( x,y ). From the path construction we have By summing up along the path we get D sp ( x,y )  X  P i h ( k v i  X  v i +1 k ) From the adaptation of Lemma 5 we have D f ( u i ,u i +1 )  X   X  f ( u i ) k u i  X  u i +1 k , which gives
P This shows that D sp ( x,y ) 4  X  D f ( x,y ).
 For the other way round, we use a technique different from Proposition 4. Denote the graph shortest path between x and y by  X  : z 0 = x,z 1 ,...,z s ,z s +1 = y . Consider  X  0 as a continuous path in X correspond-ing to  X  . As in the previous part, divide  X  0 into segments u 0 = x,u 1 ,...,u t ,u t +1 = y (see Figure 3). s &lt;  X  t . Using this and the subadditivity of h we get D sp ( x,y ) = P i h ( k z i  X  z i +1 k ) &lt;  X  P i h ( k u To prove D sp ( x,y ) &lt;  X  D f ( x,y ), we can write P i h ( k u i  X  u i +1 k )  X   X  The proof of Theorem 8 is a direct consequence of this theorem. It follows by choosing h ( t ) = t  X  f ( r (which is subadditive if  X  f is increasing) and setting w ij = h ( k X i  X  X j k ). 4.2.2. Supperadditive weights A function h is called superadditive if  X  x,y  X  0 : h ( x ) + h ( y )  X  h ( x + y ). Examples are f ( x ) = x and f ( x ) = xe x . To get an intuition on the behav-ior of the SP for a superadditive h , take an exam-ple of three vertices x,y,z which are all connected in the graph and sit on a straight line such that k x  X  y k + k y  X  z k = k x  X  z k . By the superadditiv-ity, the SP between x and z will prefer going through y rather than directly jumping to z . More generally, the graph SP will prefer taking many  X  X mall X  edges rather than fewer  X  X ong X  edges. For this reason, we do not expect a big difference between superadditive weighted kNN graphs and  X  -graphs: the long edges in the kNN graph will not be used anyway. However, due to technical problems we did not manage to prove a formal theorem to this effect.
 The special case of the superadditive family h ( x ) = x a a &gt; 1 is treated in Hwang &amp; Hero (2012) by com-pletely different methods. Although their results are presented for complete graphs, we believe that it can be extended to  X  and kNN graphs. We are not aware of any other result for the limit of SP distance in the superadditive regime. In this section we study the consequences of our re-sults on manifold embedding using Isomap and on a particular semi-supervised learning method.
 There are two cases where we do not expect a drastic difference between the SP in weighted and unweighted kNN graph: (1) If the underlying density p is close to uniform. (2) If the intrinsic dimensionality of our data d is high. The latter is because in the q -distance, the underlying density arises in the form of p ( x ) 1 /d , where the exponent flattens the distribution for large d . 5.1. Isomap Isomap is a widely used method for low dimensional manifold embedding (Tenenbaum et al., 2000). The main idea is to use metric multidimensional scaling on the matrix of pairwise geodesic distances. Using the Euclidean length of edges as their weights will lead to the convergence of the SP distance to the geodesic distance. But what would be the effect of applying Isomap to unweighted graphs? Our results of the last section already hint that there is no big difference between unweighted and weighted  X  -graphs for Isomap. However, the case of kNN graphs is different because weighted and unweighted shortest paths measure different quantities. The effect of ap-plying Isomap to unweighted kNN graphs can easily be demonstrated by the following simulation. We sample 2000 points in R 2 from a distribution that has two uni-form high-density squares, surrounded by a uniform low density region. An unweighted kNN graph is con-structed with k = 10, and we apply Isomap with target dimension 2. The result is depicted in Figure 2. We can see that the Isomap embedding heavily distorts the original data: it stretches high density regions and compacts low density regions to make the vertex dis-tribution close to uniform.
 5.2. Semi-supervised learning Our work has close relationship to some of the litera-ture on semi-supervised learning (SSL). In regulariza-tion based approaches, the underlying density is either exploited implicitly as attempted in Laplacian regular-ization (Zhu et al., 2003 but see Nadler et al., 2009; Alamgir &amp; von Luxburg, 2011 and Zhou &amp; Belkin, 2011), or more explicitly as in measure based regu-larization (Bousquet et al., 2004). Alternatively, one defines new distance functions on the data that take the density of the unlabeled points into account. Here, the papers by Sajama &amp; Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both pa-pers suggest different ways to approximate the density based distance from the data. In Sajama &amp; Orlitsky (2005) it is achieved by estimating the underlying den-sity while in Bijral et al. (2011), the authors omit the density estimation and use an approximation.
 Our work shows a simpler way to converge to a similar distance function for a specific family of f -distances, namely constructing a kNN graph and assigning edge weights as in Equation 1. We have seen in this paper that the shortest path dis-tance on unweighted kNN graphs has a very funny limit behavior: it prefers to go through regions of low density and even takes large detours in order to avoid the high density regions. In hindsight, this result seems obvious, but most people are surprised when they first hear about it. In particular, we be-lieve that it is important to spread this insight among machine learning practitioners, who routinely use un-weighted kNN-graphs as a simple, robust alternative to  X  -graphs.
 In some sense, unweighted  X  -graphs and unweighted kNN graphs behave as  X  X uals X  of each other: while degrees in  X  -graphs reflect the underlying density, they are independent of the density in kNN graphs. While the shortest path in  X  -graphs is independent of the underlying density and converges to the Euclidean dis-tance, the shortest paths in kNN graphs take the den-sity into account.
 Current practice is to use  X  and kNN graphs more or less interchangeably in many applications, and the de-cision for one or the other graph is largely driven by robustness or convenience considerations. However, as our results show it is important to be aware of the implicit consequences of this choice. Each graph car-ries different information about the underlying density, and depending on how a particular machine learning algorithms makes use of the graph structures, it might either miss out or benefit from this information. Alamgir, M. and von Luxburg, U. Phase transition in the familiy of p-resistances. In Neural Information Processing Systems (NIPS) , 2011.
 Bijral, A., Ratliff, N., and Srebro, N. Semi-supervised learning with density based distances. In Uncer-tainty in Artificial Intelligence (UAI) , 2011. Bousquet, O., Chapelle, O., and Hein, M. Measure based regularization. In Neural Information Pro-cessing Systems (NIPS) . 2004.
 Gamelin, T. W. Complex Analysis . Springer-Verlag, New York, Inc., 2007. ISBN 0-387-95093-1.
 Hwang, S.J, Damelin S. B. and Hero, A. O. Shortest path through random points. In Preprint available at Arxiv , January 2012. URL http://arxiv.org/ abs/1202.0045 .
 Nadler, B., Srebro, N., and Zhou, X. Semi-supervised learning with the graph Laplacian: The limit of in-finite unlabelled data. In Neural Information Pro-cessing Systems (NIPS) , 2009.
 Penrose, M. A strong law for the longest edge of the minimal spanning tree. Ann. of Prob. , 27(1):246  X  260, 1999.
 Sajama and Orlitsky, A. Estimating and computing density based distance metrics. In International Conference on Machine learning (ICML) , 2005.
 Tenenbaum, J., de Silva, V., and Langford, J. Supple-mentary material to  X  X  Global Geometric Frame-work for Nonlinear Dimensionality Reduction X . Sci-ence , 2000. von Luxburg, U., Hein, M., and Radl, A. Hitting times, commute distances and the spectral gap in large random geometric graphs. In Preprint avail-able at Arxiv , March 2010. URL http://arxiv. org/abs/1003.1266 .
 Zhou, X. and Belkin, M. Semi-supervised learning by higher order regularization. In International Confer-ence on Artificial Intelligence and Statistics (AIS-TATS) , 2011.
 Zhu, X., Ghahramani, Z., and Lafferty, J. Semi-Supervised Learning Using Gaussian Fields and
Harmonic Functions. In International Conference
