 Current popular anomaly detection algorithms are capable of detecting global anomalies but oftentimes fail to distin-guish local anomalies from normal instances. This paper aims to improve unsupervised anomaly detection via the ex-ploration of physics-based diffusion space. Building upon the embedding manifold derived from diffusion maps, we devise Local Anomaly Descriptor (LAD) whose originali-ty results from faithfully preserving intrinsic and informa-tive density-relevant neighborhood information. This robust and effective algorithm is designed with a weighted umbrel-la Laplacian operator to bridge global and local properties. To further enhance the efficacy of our proposed algorithm, we explore the utility of anisotropic Gaussian kernel (AGK) which can offer better manifold-aware affinity information. Comprehensive experiments on both synthetic and UCI real datasets verify that our LAD outperforms existing anomaly detection algorithms.
 H.2.8 [ Database Management ]: Database Application-s X  Data Mining ; I.5.1 [ Pattern Recognition ]: Models X  Unsupervised Anomaly Detection Anomaly detection, diffusion space, LAD
Anomaly detection or outlier detection is of great signifi-cance to many applications [40] [28]. Its primary goal is sim-ilar to that of a classification problem except that it further distinguishes normal instances from a small portion of new or abnormal instances (anomalies) [5] [19] [20]. In many ap-plications, anomalies are sparse and quite diverse, learning with the known anomalies [9] [38] [3] may not be necessarily useful in detecting the unknown ones in unseen data [34]. On the other hand, labeling known datasets can be extremely time-consuming for real-life applications and sometimes even unpractical to detect new types of rare events. Therefore, the key challenge of anomaly detection still lies in its robust ability to quantitatively and unsupervisedly characterize the intrinsic and informative density distribution around every instance.
 Our proposed unsupervised method in this paper, called Local Anomaly Descriptor (LAD), computes a measuremen-t of anomalousness based on physics-based diffusion theory, which is more informative and intrinsic compared with the existing algorithms ([4] [25] [19] [36] [1] etc.). First of all, our algorithm projects origin instances onto a diffusion s-pace. In the diffusion space, distance between anomalies and normal instances will be magnified, which makes the density of anomalies even less and therefore more salient compared with those in the original space. However, the perfect d-iffusion maps are usually unreachable. Oftentimes anoma-lies are hard to be totally distinguished from the normal instances that are not too far away. To better set anoma-lies apart from the nearby normal instances, we innovatively apply a weighted Laplace umbrella operator on the project-ed diffusion space, called Local Anomaly Descriptor (LAD). With the novel LAD which bridges the gap between global and local properties, we can not only obtain intrinsic lo-cal density information, but also take the quantity of sim-ilar instances into consideration. Therefore the representa-tion is more reliable than original attribute distribution, and more informative since it covers a sufficiently large neighbor-hood for each instance. Furthermore, LAD provides reason-ably stable performance as the scaling parameters vary (the neighborhood size k and Gaussian scaling parameter  X  ).
In this paper, we employ heat kernel to build the diffu-sion maps, which offers a statistical description on random walks. However, the pivotal techniques of our algorithm are fundamentally different from the current existing data min-ing research based on heat kernel theory [15] or other similar diffusion methods [6] [30] [29]. Our proposed LAD has dis-tinctive uniqueness on balancing local and global properties, and its advantage on both performance and robustness on real world datasets for anomaly detection.
According to the most classical definition by Hawkins [12], an anomaly is  X  X n observation which deviates so much from the other observations as to arouse suspicion that it was generated by a different mechanism X . However, it is far from trivial to define the quantitative scope of  X  X ther observation-s X . As Figure 1(a) illustrates, global anomalies (in yellow) are those with global minimum neighborhood density, and from normal instances. distinct with respect to (almost) the entire dataset. While local anomalies (in red and green) are those with local min-imum neighborhood density, and distinct with respect to instances in their local neighborhood. Profoundly speaking, local anomalies can be thought of as a generalization of glob-al anomalies, as global anomalies will typically also be local anomalies, but not vice versa [7].

In implementation, kNN-based algorithms such as LOF [4] and LDOF [39] define anomaly if its distance (usually in Euclidean space) to its k -th nearest neighbor (kNN) is large relative to the distances of its neighbors to their own k -th nearest-neighbors. Recent research [7] extended LOF to high-dimensional dataset by using random projection. Two major drawbacks of these approaches are: (1) they tend to miss local anomalies (Figure 1(b)) since it is not peculiar that kNN distances of local anomalies are similar to their normal instance neighbors X ; (2) it is of extreme importance to determine the value of k , because k can not be too small to avoid statistical error. In other words, we need to en-sure that for each instance, especially those forming micro-cluster of anomalies, it does intend to use a neighborhood size which is large enough to include more normal instances than anomalies. However, too large k will lead to miss some genuine anomalies. In Section 6.2 we will show that LOF is statistically vulnerable by analyzing the sensitivity of k .
Instead of detecting anomalies based on average neighbor-hood distance, recent approaches such as IForest [19] [20] and Mass [36] are to separate the anomalies from normal instances with their unique attribute distribution. A rep-resentative anomaly definition [19] in the aforementioned papers states that anomalies should have  X  X ttribute-values that are very different from those of normal instances X , and at the same time are  X  X inority consisting of fewer instances X . Therefore these approaches have the capacity to handle anoma-lies with different attribute distribution compared with nor-mal instances [18]. Nonetheless, these approaches may fail to detect some local anomalies when their attributes have similar distribution with some normal instances X . From Fig-ure 1(c) we can see that, even though IForest did a good job on global anomaly detection, it fails to distinguish lo-cal anomalies (green and red instances in Figure 1(a)) from the  X  X oundary X  instances in the normal instance clusters (blue instances in Figure 1(a)). The reason is that, these anomaly detectors partition instances mainly based on ob-servable attributes, or more precisely, the attribute distribu-tion in original data space. Therefore it will fail miserably when the anomaly distribution becomes far less discrimina-tive by sharing similar attribute range/distribution pattern with parts of the normal instances. In Figure 2 we can see that some anomalies have overlapped distribution on the first four eigenvectors with normal instances in Ionosphere dataset (a popularly used dataset for anomaly detection [19] [13] [23]). Such overlapping also appears at nonclassical mul-tidimensional scaling as well. So, this problem indeed exists in some real world datasets.

A few techniques [1] tried to find an approximation of the data using a combination of attributes that capture the bulk of the variability in the data, and then detect anomalies on the projected space. This kind of approaches adopted by spectral techniques is to determine manifold subspaces in which the anomalous instances can be easily identified [5]. However, the existing algorithms are based on techniques such as isometric feature mapping (ISM) and locally linear embeddings (LLE) [1] which are highly sensitive to density-varying data distribution [16] [37].
Motivated by the aforementioned problems, we re-define anomaly as follows: Definition: Anomalies are those instances with (1) local minimum neighborhood density and (2) small quan-tity compared with normal instances.
 To capture anomalies under such definition, we consider the heat equation in diffusion theory, which has intrinsic rela-tionship with manifold reconstruction and built-in robust-ness of scaling parameters [15]. The reason why we resort to manifold space is that normal instances usually lie on some low dimensional manifold structures in high density regions, moreover, the anomalies deviate from the normal instances which makes them even more discriminative. Diffusion dis-tance is based on Markov matrix which is a stochastic matrix representing random walks on graph [22], it can consider up to t steps out of all the possible paths bridging any two instances, which makes it more robust than Euclidean and geodesic distance [6] [16] [37]. However, different from the current diffusion-related research with the goal of clustering [30] [29] [15], diffusion mapping for anomaly detection has crucial requirement for connecting similar instances and at the same time avoiding excessive-connection. To our best knowledge, this diffusion-based utility on anomaly detection has never been explored yet.

In this paper, we propose Local Anomaly Descriptor (LAD) which offers a natural mechanism to express intrinsic neigh-borhood density information through heat diffusion process. To offer a solution to the inherent problem from the perspec-tive of heat diffusion, LAD provides a meaningful trade-off between local and global manifold awareness by applying a weighted Laplacian umbrella operator. Experiments show that LAD is immediately useful for a wide variety of anoma-ly detection applications.
This paper articulates a novel unsupervised anomaly de-tection algorithm which is intrinsic, informative, and robust to scaling parameters with the following contributions: (1) We quantitatively characterize local density information (2) In order to take the amount of similar instances into (3) We systematically evaluate the proposed algorithm with
Our proposed work is strongly inspired by heat diffusion theory [14] in that it can provide information intimately re-lated to local density. Heat theory can be interpreted as the transition density function of Brownian motion [33], which is the most fundamental continuous time Markov process. Laplace operator is intimately related to heat diffusion, con-necting geometry of a manifold with the properties of the heat flow. Using the discrete Laplace operator, the heat equation can be simplified, and generalized to matrix opera-tion over spaces with an arbitrary number of dimensions. In practice the heat equation is often associated with random walk graph Laplacian [6], L rw . Random walk is a stochastic process which randomly jumps from vertex to vertex. Heat equation therefore can be defined by fold M and t is the time scaling parameter [11]. For L rw  X   X  X  (  X  and  X  are the eigenvectors and eigenvalues of L rw (e) LAD ( t = 1 and k = 10) ), the heat kernel can be re-formulated as follows: where H t ( i,j ) represents the amount of heat being trans-ferred from i to j in time t given a unit heat source at i . The scaling parameter t in heat kernel is used to control the transitive connectivity: small t makes the loosely-connected graph into slightly stronger connection within t connection-s, while large t makes the graph tend to be more strongly-connected.

In 2009, Sun et.al [33] proposed a concise form given by the heat kernel from one instance to itself which is called Heat Kernel Signature (HKS). The physical meaning of HKS is the amount of heat each instance keeps within itself in time t . The property of heat diffusion process states that heat tends to diffuse slower at instances with more sparse neighborhood and faster at instances with denser neighborhood. Therefore HKS can intuitively depict the local density of each instance (the first property in our anomaly defini-tion). Besides, HKS also has the following properties which make it a very lucrative candidate for local density measure-ment: 1) it is intrinsic to the local manifold structure; 2) it is informative since it contains density information about the whole neighborhood in t scale; and 3) the probabilistic in-terpretation of heat diffusion can well support the stableness of HKS against small perturbation in the neighborhood.
However, Heat equation is assumed to build on the un-derlying manifold. But in most applications, the underly-ing manifold is unknown. In practice, HKS is usually built on isotropic Gaussian kernel (IGK) on observed space. Al-though graph Laplacian normalizations [6] based on sim-ple IGK on observed space can recover manifold structure to certain extent, non-uniformly sampled instances tend to show unpreserved density distribution on the reconstructed manifold. HKS on IGK will fail to reveal local density faith-fully in such reconstructions. Figure 3(a) and 3(b) shows the performance of HKS on anomaly detection with t = 1 and t = 10 based on simple IGK and random walk graph Laplacian normalization. When t = 10 (Figure 3(b)) the heat is extremely easy to dissipate, which blends both local and a few global anomalies into normal instances. Mean-while many marginal instances of the two normal instance clusters stand out due to the fact that the HKS on IGK fails to show manifold-aware properties. When t = 1 (Fig-ure 3(a)), although the short period of heat dissipation has salient effect on global anomalies, HKS on IGK still fails to distinguish local anomalies from normal instances on the boundary area of normal clusters. Therefore an alternative way is indispensable to build better manifold-aware affinity matrix. One of the most preferable candidates is anisotropic Gaussian kernel (AGK) [31] [32].
In this section we integrate anisotropic Gaussian kernel (AGK) [31] into HKS to achieve better manifold reconstruc-tion. In Figure 4 we can see the 70 nearest neighbors of red instance when using IGK (Figure 4(a)) and AGK (Fig-ure 4(b)), which shows that the intra-manifold distances are much shorter than the inter-manifold by using AGK. To further support this idea, in Figure 3(c) and 3(d) we show that anomaly detection can directly benefit from the use of AGK. In Figure 3(d) with t = 10, the global anomalies are highlighted even though the local anomalies are latent (com-pared with Figure 3(b)). This is because if the manifold is well reconstructed, global anomalies should be separated far away from normal instances even with large t scale. Further-more, in the small scope of t = 1 (Figure 3(c)), both local and global anomalies can be detected, which illustrates that with the support from AGK, HKS is capable of revealing the density information of the intrinsic manifold structure. Figure 4: 70 nearest neighbors (in green) of red in-stance on IGK (a) and AGK (b), which shows that AGK has better manifold-aware property than IGK.

In the rest of this section we briefly introduce AGK on the observed space Y that approximates the isotropic Gaussian kernel on the underlying manifold X . The idea is to approx-imate the Euclidean distance between instances x ( j ) in the manifold space X using covariance matrix C = JJ T where J is the Jacobian matrix [31] and the instances y j = f ( x in the observable space Y . Let x , be two instances in the manifold space X and y = f ( x ),  X  = f ( ) be their map-ping to the observable space Y . Let g : Y  X  X be the inverse mapping of f : X  X  Y , that is, g ( f ( x )) = x and f ( g ( y )) = y ,  X  x  X  X ,  X  y  X  Y . Expanding the functions x = g ( y ) in a Taylor series at the instance y gives A similar expansion can be built at instance  X  and the av-erage of these two equations can be produced as k  X  x k 2 = given that the Jacobian of the inverse g is the inverse of the Jacobian J (a detailed description of calculation can be referred to [31]). So we can construct the anisotropic Gaussian kernel
AGK ( y i ,y j ) = e { X  where i,j = 1 ,...,N .

AGK has the desired attributes that it is separable, and its first (nontrivial) eigenfunctions are monotonic functions of the independent parameters [32]. It also has been proved that the eigenvectors of AGK reveal the independent com-ponents [31]. HKS, built on such approximation, can better capture the manifold structure of data as shown in Figure 3(c) and 3(d), which is difficult or even impossible to achieve by using IGK or other similar techniques.
Although HKS on AGK has the capability to offer desir-able local density information, it is of importance to select the right time scaling parameter t , which provides a trade-off between the effects of local and global information. However, it is hard to get the  X  X est of both worlds X  with single setting for this parameter. Even with better manifold reconstruc-tion, if t is large the heat is still easy to dissipate regardless of normal instances or local anomalies, although not for global anomalies, which is shown in Figure 3(d). This is because with large t scale, the distance between local anomalies and the normal instances around them would still be close. So local anomalies cannot retain their heat. On the other hand, if t is small, the heat diffusion runs for only a short period of time, and the resulting anomalousness are usefully local, but almost carry the same value for instances with similar density inside a very restrained neighborhood, which is the major reason why it sometimes assimilates local anomalies into some normal instances. In Figure 3(c) we can see HKS assigns similar scores to the local anomalies and some of the boundary normal instances. Intuitively speaking, HKS on AGK still fails to take the amount of similar instances into account with off-the-sweet-spot t setting.

In order to handle the above problem, we propose to use the umbrella operator [35] [8] to consider the quantity of sim-ilar instances in neighborhood by bridging the gap between global and local properties. The main motivation for using this operator is to compute the average difference between a point ( x i ) and its k neighbors ( nb ( i,k )). where W i,j is the weight between x i and x j . If we use AGK ( i,j ) for W i,j , then we may define the Local Anomaly Descriptor (LAD) for a point i as follows:
LAD ( i ) = HKS t ( i )  X  1 The geometric meaning of LAD is illustrated in Figure 5 where we measure the difference between a single HKS t ( i ) and its neighborhood X  X  average HKS t ( j ) value. Note that the heat kernel signature value is always positive and it means the degree of global anomaly level and thus LAD ( i ) indicate the level of both global and local properties.
If an instance is globally anomalous, its HKS would be already high enough to discriminate itself to the other in-stances. While it is locally anomalous, its HKS is likely to be similar to some normal instances X  with similarly sparse Figure 5: Illustration of Local Anomaly Descriptor which calculates weighted average of neighbor differ-ences. It is one of the ways taking into consideration the neighborhood structures [35]. neighborhood. However, the amount of similar instances, if it is taken into account, can serve to recognize the local anomalies from normal instances. Since local anomaly only has a small amount of neighbors with close HKS, but nor-mal instances, on the other hand, have more such neighbors. LAD has a very lucrative property in considering the amount of similar instances (the second property in our anomaly definition): with similar intra-cluster density, AGK tends to assign larger affinity value to two instances inside a larger manifold/cluster, and less value to those inside a smaller manifold/cluster, and much less value to those not inside the same manifold/cluster [31]. So even though k is not large e-nough to include the whole appropriate neighborhood, LAD can still capture the information related to the amount of similar instances.

The benefits of LAD in comparison with HKS can be seen in Figure 3(e), which shows the LAD score of synthetic dataset (Figure 1(a)). Note that HKS with AGK possesses good global properties for long enough time ( t = 10 in Fig-ure 3(d)) and good local properties for small time ( t = 1 in Figure 3(c)), but not both. Nevertheless, Figure 3(e) shows that our proposed LAD has a penetrating awareness on both global and local anomalies primarily because of the power of our proposed umbrella operator.

We now justify the LAD utility by briefly documenting its theoretic connections with a few existing methods, which also lays a solid foundation for LAD X  X  attractive properties in practical use.

Biharmonic Operator. HKS itself is directly derived from the Laplace operator and its eigen-decomposition, so that HKS is intrinsically a second-order property relevant to the Laplace X  X  equation. The aforementioned derivation of LAD can be intuitively related to the biharmonic process, because the Laplace operator is essentially applied twice (to compute both HKS and its umbrella operator of HKS). It provides a good balance in the sense that it decays slowly in small cluster around the source instance and fast enough to be structurally inherent in dense areas. This specific  X  X al-ancing X  is intimately derived from the biharmonic equation with properties such as local support, global informative, and shape-aware [17].

Signal Processing. LAD also has strong connection to signal processing . In lowpass filtering, the divergence of a sample from its average neighborhood is the easiest way to pinpoint those inconsistent instances if the desired signal has significant high frequency content. As in traditional sig-nal processing [35], it is possible for LAD to quantify the frequency response by computing an adjoining sum of the Laplacian operator in its immediate vicinity. As a result, this enables LAD to distinguish between normal instances and inconsistent instances (anomalies) with greater preci-sion. kNN-based Approaches. kNN-based methods [4] [7] [39] approach local density for each instance using its neigh-borhood information. Like LAD, they require scaling pa-rameters to capture a reasonably large neighborhood, and the density information is based on this prescribed local re-gion. However, kNN-based methods has strictly local con-text in that they simply fix the neighborhood size with k . In contrast, LAD employs locally adaptive neighborhood size directly benefited from the physics-inspired properties of heat diffusion. Moreover, Euclidean distance in kNN-based methods is a pair-wise local quantity, while heat k-ernel used in LAD considers all the possible paths between two instances within time t , therefore LAD is more stable than kNN-based methods.

Attribute-based Approaches. Attribute-based meth-ods [19] [20] [36] try to compute local density by adding up a sequence of values from an attribute-based function [36], equivalent to a kernel density function such as heat kernel. The global instance distribution is based on each attribute and how deviated each instance is from the other instances in that specific attribute, which indeed is more informative than kNN-based approaches. However, the strong empha-sis on attribute distribution along its dimension is also a  X  X ouble-sided sword X : on the one hand it is much faster without any distance calculation, on the other hand, such distribution based on attributes still fails to consider local anomalies.

Diffusion-based Clustering. Some recent research [30] [29] [15] proposed the unified probabilistic clustering ap-proach based on diffusion map. By integrating all time s-cales of kernel function into one single term, this kind of techniques completely removes the time scaling parameter of diffusion dissipation, therefore it has the built-in robust-ness to data perturbation and scaling parameter modifica-tion [15]. However, as a side-effect, this process of  X  X ntegra-tion X  assimilates local anomalous instances into normal in-stance clusters since the excessive-diffusion tends to connect everything together. LAD, in sharp contrast, is built upon kernel function with small time scale and weighted umbrella operator instead of integrating all scales together. Therefore it avoids the above-mentioned excessive-connection problem.
After investigating some attractive properties of LAD, it now sets a stage for us to introduce a novel anomaly detec-tion algorithm, which is sensitive to both global and local anomalies. Let X be a matrix of size n  X  m , where n is the number of instances and m the number of dimensions, our framework is detailed in Algorithm 1. This algorithm under-goes a kind of data warping process by using AGK (Step 1) and Laplacian Random Walk normalization (Step 2). Then we perform the eigen-decomposition (Step 3) and construct HKS for each instance (Step 4). Equation 9 is used as the last step to compute Local Anomaly Descriptor for the final measurement of anomalousness.

In Step 4, we adopt the notation in [10] by normalizing the time scale t  X  t/ (2  X  1 ) to achieve scale invariance. Hence-forth, with little abuse of notation, heat diffusion time in Algorithm 1: LocalAnomalyDescriptor( X ,  X  , t , k )
Input : Input data X  X  R n  X  m ;  X  the Gaussian scaling
Output : LAD score for each instance 1 Construct anisotropic Gaussian kernel W using
Equation 7 and  X  ; 2 Construct Laplacian random walk normalization on W ; 3 Compute generalized eigenvectors  X  ( i ) and corresponding eigenvalues  X  i , i = 1 , 2 ,...,n . ; 4 Construct Heat Kernel Signature in time scale t using
Equation 3 ; 5 Compute Local Anomaly Descriptor using Equation 9 with Heat Kernel Signature and anisotropic Gaussian kernel in the k neighborhood for each instance ; our paper will actually denote t/ (2  X  i ), where  X  i is the first non-trivial eigenvalue.

Regarding computational complexity, eigen-decomposition (Step 3) is the most time-consuming step, which will dom-inate our computation. There are many iterative methods to conduct eigenvalue decomposition, but in general finding the eigenvalues reduces to matrix multiplication by comput-ing a symbolic determinant, which gives a running time of O ( n 3 + n 2 log 2 n ) [24]. An alternative way of estimating the heat kernel K t = e  X  1 L rw D  X  1 is to use a partial sum of infi-nite series with This method would be especially attractive for small values of t , since only a few terms would be needed to obtain an accurate estimation of e  X  tL rw [2].
Dataset. To demonstrate the performance of our pro-posed method, we evaluate our algorithm on nine UCI bench-mark datasets including three medical datasets (WDBC, Pima, and Arrhythmia), three biological datasets (Ecoli, Yeast, and Abalone), and three physics datasets (Glass, Ionosphere, and Magic), whose statistics are summarized in Table 1. All these data have been popularly used in anoma-ly detection research (related references for each dataset are listed in Table 1). Anomalies in some of the datasets (WDBC, Pima, Arrhythmia, etc.), although carrying a large number of instances, have scattered and sparse distribution (Figure 6). Therefore the anomalies in these datasets should be treated as a combination of many small anomalous clus-ters instead of one or a few normal clusters with high density [7] [23], which has nothing inconsistent with our definition about anomaly in Section 1.2. Such diverse combination of data is intended for our comprehensive studies. In the data preprocessing step, all nominal (including binary) attributes or attributes with missing value are removed.

Baselines. We choose six states of the art competitors in three categories to show the outstanding performance of our proposed LAD. For kNN-based algorithms, we choose Local Outlier Detection (LOF) [4] and Local Correlation In-tegral (LOCI) [25]. Specially, LOCI provides an automatic, Figure 6: Anomalous instances in green ( 37 . 3% ) are more scattered and sparse than normal instances in blue ( 62 . 7% ) in WDBC dataset (shown with the first three eigenvectors). Therefore these anomalies, al-though have a large amount of instances, should be treated as many small abnormal clusters. data-dictated cut-off to determine whether an instance is an anomaly based on probabilistic reasoning. For attribute-based methods, we include IForest [19] and Mass [36]. For manifold-based methods, we refer readers to two differen-t manifold-based techniques in [1] including locally linear embeddings (LLE), and isometric feature mapping (ISM), followed by LOF to obtain anomalousness measurement.
Evaluation Metrics. Since we have the ground truth of labels for each data, we compare our anomaly detection results with labels. Due to space limitation, AUC (Area un-der Receiver Operating Characteristics Curve) is used as the only listed evaluation metric in this paper because it is com-monly used to evaluate anomaly detectors and it is cut-off independent. Detailed definition of AUC can be referred to [21]. In our paper we also show that our LAD has the most robust and stable performance for all the datasets by using macro paired t-tests [41] against each competitor respective-ly. Note that a score of macro paired t-tests (p-value) should be no more than 0 . 05 to be considered statistically signifi-cant.

Parameters. Our proposed algorithm has three scaling parameters, namely Gaussian kernel scaling parameter  X  , time scaling parameter t , and the size of neighborhood k . We set t = 1 which makes our proposed LAD capable of depicting local minimum density. As the default setting for most of the other algorithms, we fix  X  = 1 and k = 10 for the experiments in Table 2 and 3. But the LAD robustness to the change of these two parameters will be shown in Figure 7. For LOF we try k = 10, 25, 50 for all the datasets. Since in practice, single setting of k for LOF may introduce statistical errors [25]. As for LOCI, we set k = 10 and k = 50 due to its instinctive stability on k which comes from a multi-granularity deviation factor [25]. Radius coefficient is set as  X  = 0 . 5 in LOCI, which is the same to their paper [25]. As for IForest, even though in their paper [19] Liu et al. claimed that a small sub-sampling size  X  provides high AUC and a further increase of  X  is not necessary, in practice when  X  increases, anomalies in-between data groups will become more detectable by IForest [18]. To conduct safe and fair comparison, we set  X  = 4000 and the number of trees nt = 100 since they are the recommended settings in the authors X  technical report [18]. Similarly, in Mass we statistical significance test for performance comparison w.r.t. LAD. WDBC 0.5874 0.6192 0.7784 0.7573 0.8119 0.7974 0.7760 0.8864 (1) Pima 0.4847 0.5270 0.6003 0.5946 0.6089 0.5812 0.6630 0.6936 (1) Arrhythmia 0.7419 0.7547 0.7482 0.7482 0.7483 0.6363 0.7456 0.7558 (1) Ecoli 0.8260 0.8614 0.8454 0.8641 0.8549 0.7699 0.8754 0.8692 (2) Yeast 0.4159 0.6253 0.5986 0.6076 0.6035 0.5712 0.6159 0.6183 (2) Abalone 0.5724 0.6056 0.6525 0.6932 0.7058 0.6923 0.7466 0.7398 (2) Glass 0.7474 0.7008 0.7528 0.7480 0.7593 0.8922 0.6933 0.8612 (2) Ionosphere 0.9064 0.8709 0.7982 0.8512 0.7923 0.8269 0.8467 0.9240 (1) Magic 0.6420 0.6804 0.6970 0.5672 0.6825 0.6984 0.7506 0.7516 (1)
Average 0.6582  X  X  X  0.6939  X  X  X  0.7190  X  X  X  0.7146  X  X  X  0.7297  X  X  X  0.7184  X  X  X  0.7459  X  0.7889 (1) set the number of mass estimation ne = 100 and the sub-sampling size as # instance of dataset. On the other hand, IForest and Mass are based on random sub-sampling which makes their performance very unstable. In an attempt to get more stable results, for each dataset we run 30 times for both IForest and Mass and use the average AUC in the final comparison. For LLE and ISM, we conduct experiments on size of neighborhood k = 10, 50 and 100 with the best number of dimensions d in [2 , 30] respectively, in order to compare their performance and robustness in k .
In this section we evaluate our proposed LAD and the oth-er six anomaly detection algorithms. Table 2 documents the anomaly detection comparison result (in AUC and p-value) of LAD and other four popular algorithms: LOF, LOCI, Mass, and IForest. While the manifold-based methods com-parison including LLE and ISM (all followed by LOF), and our proposed LAD are also listed in Table 3.

In Table 2 LAD shows the best average performance (0 . 7889) across all the datasets. For each dataset LAD has the best or very close to the best performance. Specifically, LAD is the top-ranked one for all the three medical datasets and has al-most unbeatable performance for the three physics datasets. Although LAD ranks the second among all the methods on the three biological datasets. The AUC score are actually very close to the best one (no more than 0 . 008 difference in AUC). As for Glass dataset the AUC score of our LAD (0 . 8612) is still comparable to the best one (0 . 8922) by Mass, meanwhile beats the third best (LOCI with k = 50) for more than 13%. IForest shows the second best (0 . 7459) average performance of real datasets, which supports the argument that it is able to take both global and local contexts into consideration [18]. This is different from kNN-based meth-ods (LOF and LOCI) which only concern with instance-wise local context. Compared with LOF, LOCI performs robust-ly when k varies. It ranks the third (0 . 7297) when k = 50 and fifth (0 . 7146) when k = 10. This moderately stable performance comes from the built-in concept of a multi-granularity deviation factor. LOF, although has the third best score (0 . 7190) when k = 50, shows seriously unstable performance as k changes, which can be explained as follows: LOF is based on a direct normalization of anomaly scores for a very limited neighborhood. Although Mass (0 . 7184) only keeps the fourth record, it has the fastest computation speed compared with the other competitors, especially LAD.
Table 3 shows performance of three different manifold-based algorithms. Generally, LAD evidently outperform-s the other methods with average AUC 0 . 7889. In terms of stability, although LLE has similar average score with k = 10, 50 and 100, it shows fluctuation for some datasets especially Ecoli, Glass and Ionosphere. Part of the reason comes from that LLE assumes the data manifold is suffi-ciently smooth and densely sampled that it is locally ap-proximately linear, while this is not the true story for many real world datasets. Similarly, ISM X  X  AUC score varies as k changes in several datasets (Ecoli, Yeast, Glass etc.). It is because ISM is highly vulnerable to the local data per-turbation since the embedding given by the ISM tends to recovers the geodesic distances between points on the man-0.5464  X  X  X  0.6146  X  X  X  0.6520  X  X  X  0.7889 (1) ifold, which is very locally sensitive compared with random walk [16] [37].
 To systematically manifest the robustness of our proposed LAD on different neighborhood size k and Gaussian scaling parameter  X  , we test our algorithm respectively on a series of k and  X  on seven small datasets: WDBC, Pima, Arrhythmia, Ecoli, Yeast, Glass, and Ionosphere due to limited space, and also with the reason that in theory, datasets with smaller number of instances are more sensitive to the change of k and  X  . Therefore these seven datasets are the more effective choices to show whether our LAD is robust to these two parameters. For k our test range is in [10 , 100]. As for  X  , the test range is in [0 . 1 , 8], with 0 . 05 as step size between 0 . 1 to 1 and 0 . 5 as step size between 1 to 8. From Figure 7(a) we can see that our new LAD algorithm has more stable performance than LOF, LLE and ISM on different k (shown in Table 2 and Table 3). Similarly, Figure 7(b) shows that our proposed LAD retains certain level of robustness as  X  changes. The stability of LAD has an inherent relationship with diffusion maps and random walk.

As for the macro paired t-tests across all the datasets, compared with LOF, LOCI, Mass, LLE and ISM respective-ly, LAD has extremely small p-value (less than 1%). Com-pared with IForest, LAD has p-value less than 5%. This, once again, proves that our LAD has the most stable av-erage performance. Overall, LAD outperforms the selected kNN -based, attribute-based and manifold-based algorithms in that it is more intrinsic, informative, and manifold-aware.
This paper has documented an original unsupervised anoma-ly detection algorithm, called Local Anomaly Descriptor (LAD), which is based on the physics-inspired diffusion s-pace and weighted umbrella operator. Compared with the existing algorithms, our proposed LAD has demonstrated many important properties such as intrinsic, informative to local density, and stable to small parameter perturbation. Together with its more manifold-aware property for the goal of anomaly detection, we expect it to be useful for any type of data distribution. Nonetheless, much more extensive ex-periments are still required to validate this conjecture, which is part of our near-future research. Another direction is to investigate the possible connection with global structure and pattern mining such as clustering and feature classification.
Figure 7: LAD robustness on different k and  X  .
We gratefully thank all the anonymous reviewers for con-structive suggestions toward paper improvement. This re-search is supported in part by NSF grants IIS-0949467, IIS-1047715, and IIS-1049448. It is also supported by United States Department of Energy, Grant No. DE-SC0003361, funded through the American Recovery and Reinvestment Act of 2009. In addition, this project is also supported in part by DOE Systems Biology Knowledgebase (DE-AC02-98CH10886). [1] A. Agovic, A. Banerjee, A. R. Ganguly, and [2] R. Badeau, B. David, and G. Richard. Fast [3] G. Blanchard, G. Lee, and C. Scott. Semi-supervised [4] M. M. Breunig, H. P. Kriegel, R. T. Ng, and [5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly [6] R. R. Coifman and S. Lafon. Diffusion maps. Applied [7] T. de Vires, S. Chawla, and M. Houle. Finding local [8] M. Desbrun, M. Meyer, P. Schrder, and A. Barr. [9] J. Gao, H. Cheng, and P. N. Tan. Semi-supervised [10] F. Goes, S. Goldenstein, and L. Velho. A hierarchical [11] A. Grigoryan. Estimates of heat kernels on riemannian [12] D. Hawkins. Identification of outliers . Chapman and [13] K. Hempstalk, E. Frank, and I. H. Witten. One-class [14] E. Hsu. Stochastic analysis on manifolds. Graduate [15] H. Huang, S. Yoo, H. Qin, and D. Yu. A robust [16] S. Lafon, Y. Keller, and R. R. Coifman. Data fusion [17] Y. Lipman, R. Rustamov, and T. Funkhouser.
 [18] F. T. Liu and K. M. Ting. Can isolation-based [19] F. T. Liu, K. M. Ting, and Z. H. Zhou. Isolation [20] F. T. Liu, K. M. Ting, and Z. H. Zhou. Isolation-based [21] C. Marzban. A comment on the roc curve and the [22] M. Meila and J. Shi. A random walks view of spectral [23] K. Noto, C. E. Brodley, and D. Slonim. Anomaly [24] V. Y. Pan and Z. Q. Chen. The complexity of the [25] S. Papadimitriou, H. Kitagawa, P. B. Gibbons, and [26] D. Pelleg and A. W. Moore. Active learning for [27] C. Plant, C. Bohm, B. Tilg, and C. Baumgartner. [28] B. Pogorelc and M. Gams. Discovery of gait anomalies [29] H. Qiu and E. R. Hancock. Clustering and embedding [30] J. W. Richards, P. E. Freeman, A. B. Lee, and C. M. [31] A. Singer and R. R. Coifman. Non-linear independent [32] A. Singer and R. R. Coifman. Anisotropic diffusion on [33] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and [34] Z. Syed and I. Rubinfeld. Unsupervised risk [35] G. Taubin. A signal processing approach to fair [36] K. M. Ting, G. T. Zhou, F. T. Liu, and J. S. Tan. [37] L. van der Maaten, E. Postma, and J. van der Herik. [38] M. Wu and J. Ye. A small sphere and large margin [39] K. Zhang, M. Hutter, and H. Jin. A new local [40] X. Zhu, X. Wu, and C. Zhang. Vague one-class [41] D. W. Zimmerman. A note on interpretation of the
