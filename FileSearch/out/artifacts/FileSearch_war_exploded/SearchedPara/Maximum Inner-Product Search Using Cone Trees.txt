 The problem of efficiently finding the best match for a query in a given set with respect to the Euclidean distance or the cosine similarity has been extensively studied. However, the closely related problem of efficiently finding the best match with respect to the inner-product has never been explored in the general setting to the best of our knowledge. In this paper we consider this problem and contrast it with the previous problems considered. First, we propose a gen-eral branch-and-bound algorithm based on a (single) tree data structure. Subsequently, we present a dual-tree algo-rithm for the case where there are multiple queries. Our proposed branch-and-bound algorithms are based on novel inner-product bounds. Finally we present a new data struc-ture, the cone tree, for increasing the efficiency of the dual-tree algorithm. We evaluate our proposed algorithms on a variety of data sets from various applications, and exhibit up to five orders of magnitude improvement in query time over the naive search technique in some cases.
 E.1 [ Data Structures ]: Trees; G.4 [ Mathematical Soft-ware ]: Algorithm design and analysis; H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process Algorithms, Design Metric trees, cone trees, dual-tree branch-and-bound
In this paper, we consider the problem of efficiently find-ing the best match for a query from a given set of points with respect to the inner-product similarity. We focus on improv-ing the efficiency of this search. Formally, we consider the following problem: Maximum inner-product search. For a given set of N points S  X  R D and a query q  X  R D , efficiently find a point p  X  S such that: At first glance, this problem appears to be very similar to much existing work in literature. Efficiently finding the best match with respect to the Euclidean (or more generally L ) distance is the widely studied problem of fast nearest-neighbor search in metric spaces [9]. Efficient retrieval of the best match with respect to the cosine similarity has been re-searched in the field of text mining and information retrieval [1]. But as we will explain in the next section, the maximum inner-product search is not only different from these afore-mentioned tasks, but also arguably harder.
An obvious application of maximum inner-product search stems out of the widely successful matrix-factorization frame-work in recommender system challenges like the  X  X etflix prize X  [22, 21, 2]. The matrix-factorization results in accu-rate representation of the available data in terms of user vec-tors and items vectors (examples for items would be movies or music). In this setting, the preference of a user for an item is the inner-product between the corresponding user X  X  vector and the item X  X  vector 1 . The retrieval of recommen-dations for a user is equivalent to maximum inner-product search with the user as the query and the items as the ref-erence set. Linear scan of the items are usually employed to find the best recommendations. An efficient search algo-rithm would make the retrieval of recommendations in the matrix-factorization framework scalable to larger systems.
The usual document retrieval tasks use the cosine similar-ity to match documents. However, in certain settings [11], the documents are represented as (not necessarily normal-ized) vectors and the inner-product between these vectors represent their mutual similarity. In this case, unless the vectors are normalized to have the same length, document matching using the cosine similarity [1] might make the algo-rithm scalable at the cost of returning inaccurate solutions since the inner-product is not the same as the cosine simi-larity (we will discuss this further in Section 2).
There is a similar problem known as the the max-kernel operation: for a given set of points S and a query q and a kernel function K (  X  ,  X  ), the task is to find the point with the maximum value of K ( q,p )overtheset S .This
The preference is actually the inner-product between the user and the item vector plus a item bias term. But this can be reduced to an inner-product by appending the user vector with 1 and the item vector with the item bias. problem is widely used in maximum-a-posteriori inference [20] in machine learning, and for image matching [23] in computer vision. If the kernel function can be explicitly represented in the form a function  X  (  X  ) such that K ( q,p  X  ( q ) , X  ( r ) , then this problem reduces to maximum inner-product search after all the points in the set S and the query q is transformed into the  X  -space.
In this paper, we propose two tree-based branch-and-bound algorithms along with a new data structure to solve this problem. In Section 2, we contrast this problem to the more familiar problems of nearest-neighbor search in metric spaces and best matches with respect to the cosine similarity. In Section 3, we propose a simple branch-and-bound algo-rithm using existing ball tree data structure [28] and a novel bound. In the following section (Section 4), we address the situation where there are multiple queries on the same set of points and propose a dual-tree branch-and-bound algorithm. In Section 5, we present a new data structure, the cone trees , to index the queries for the dual-tree algorithm. These struc-tures take advantage of novel inner-product bounds which are tighter than those that can be achieved with traditional ball trees. The proposed algorithms are evaluated for their efficiency over a variety of data sets in Section 6. Section 7 demonstrates how the proposed algorithms can be applied to the max-kernel operation with general kernel functions without any explicit representation of the points in the  X  space. In the final section, we provide our conclusions along with possible future directions for this work.
Numerous techniques exists for nearest-neighbor search in Euclidean metric space (see surveys like [9]). Large scale best matching algorithms have also been developed for the cosine-similarity measure [1], with a lot of focus on text data. The problem of nearest-neighbor search (in metric space) has been solved approximately with the widely pop-ular locality-sensitive hashing (LSH) [14, 18]. LSH has been extended to other forms of similarity functions (as opposed to the distance as a dissimilarity function) like the cosine similarity [7] 2 . The approximate max-kernel operations can also be solved efficiently with LSH under certain conditions on the kernel function. Dimension reduction [30] and dual-tree algorithms [20] have also been used to solve the approx-imate max-kernel operation efficiently.
Here we explain why the maximum inner-product search is different from these existing search problems. Hence tech-niques applied to these problems (like LSH) cannot be di-rectly applied to this problem.
 Nearest-neighbor search in Euclidean space. This in-volves finding a point p  X  S for a query q such that: Hence, if the norms of all the points in S are normalized to have the same length, then maximum inner-product search is equivalent to nearest-neighbor search in Euclidean metric space. However, without this restriction, the two problems can have potentially very different answers (figure 2). Best-matching with cosine similarity. This constitutes finding a point p  X  S for a query q such that The best match with cosine similarity gives the maximum inner-product only if all the points in the set S are normal-ized to the same length (counter example in figure 2). Locality-sensitive hashing. LSH has been applied to a wide variety of similarity functions. LSH involves construct-ing hashing functions such that each hash function h satisfies the following for any pair of points r, p  X  S : where sim( r, p )  X  [0 , 1] is the similarity function of interest. For our situation, we can normalize our data set such that  X  r  X  S, r  X  1 3 , and assume that the all the data is in the first quadrant (so that none of the inner-products go below zero). In that case, sim( r, p )= r, p  X  [0 , 1] is a valid similarity function of interest.

It is known that for any similarity function to admit a locality sensitive hash function family (as defined in equa-tion 2), the distance function d ( r, p )=1  X  sim( r, p )must satisfy the triangle inequality (Lemma 1 in [7]). However, the distance function d ( r, p )=1  X  r, p does not satisfy the triangle inequality (even when all the points are restricted to the first quadrant) 4 . So LSH cannot be applied to the inner product similarity function even when all the data lies in the first quadrant (which is quite a restrictive condition). Efficient max-kernel operation. Various techniques have been proposed to solve this problem efficiently. For kernel functions with very high (possibly infinite) dimensional ex-plicit representations, Rahimi, et.al., 2007 [30], propose a technique to transform these high-dimensional representa-tions into lower-dimensions while still approximately pre-serving the inner-product to improve scalability. However, the final search still involves a linear scan over the set of points for the maximum inner-product or a fast nearest-neighbor search under the assumption that finding the nearest-neighbor is equivalent to maximizing the inner-product. For translation invariant kernels 5 , a tree-based recursive algo-rithm has been shown to scale to large sets [20]. However, it is not clear how this algorithm can be extended to the gen-eral class of kernels. LSH is widely used for image matching in computer vision [23], but only for kernel functions that admit a locality sensitive hashing function [7]. Hence, none of the existing techniques can be directly applied to max-imum inner-product search without introducing inaccurate results or limiting assumptions.
Inner-products lack a very basic property of generally used similarity functions  X  coincidence. For example, the Eu-clidean distance of a point to itself is 0; the cosine similarity of a point to itself is 1. The inner-product of a point x  X  S to itself is x 2 , which may be high or low depending on the value of the x . Additionally, there can possibly be many other points y  X  S such that y,x &gt; x 2 .

Efficient nearest neighbor search methods typically rely heavily on these properties (triangle inequality and coinci-dence) to achieve their efficiency. Hence, without any added assumptions, this problem of maximum inner-product search is inherently harder than the previously dealt similar prob-lems. This is possibly the reason why there is no existing work for this problem in its general form, to our knowledge.
Trees have been widely used for nearest-neighbor search [13, 4, 29, 8, 32]. Being widely used approach in the nearest neighbor case, we believe it is instructive to review them before considering the maximum inner-product search case.
For exact nearest-neighbor searches, trees can yield great accelerations in anywhere from low-to high-dimensional data, as long as there is low intrinsic dimensionality [4, 10]. Trees can also be easily adapted to the approximate case, with er-ror guarantees of various sorts. These include approximation in the sense of rank , i.e. if the actual best match may not be returned, trees can be used in a way that guarantees that the result is, say, in the top 10 best matches [32]  X  rather than provide a guarantee in terms of potentially less mean-ingful abstract quantities such as distances (as is provided by LSH; it is not clear how to extend LSH to provide rank guarantees). This would appear to make particular sense in many applications such as recommendations. Trees can also be used for another kind of approximate search setting which can be important in practical applications, in which the best possible match is found given a user-defined time limit. This kind of approximation is possible for tree-based branch-and-bound algorithms because they are incremental algorithms. This is not possible with something like LSH  X  LSH provides theoretical error bounds, but there is no way of ensuring the error constraint during the search. Another important advantage of trees is that the trees require a single Algorithm 1 MakeBallTreeSplit(Data S ) Algorithm 2 MakeBallTree(Set of items S ) construction  X  the branch-and-bound algorithm adapts for the different levels of approximate and/or time limitations. Hashing techniques require multiple hashes for different lev-els of approximation. The usual norm is to pre-hash for mul-tiple values of approximation. Trees can also be constructed by learning from the data using techniques from machine learning [6, 25] to provide better accuracy and efficiency.
The significant advantages of the tree-based approach for the nearest-neighbor setting motivate the question of whether they can be brought to the maximum inner-product case.
Ball trees [29, 28] are binary space-partitioning trees that have been widely used for the task of indexing data sets. Every node in the tree represents a set of points and each node is indexed with a center and a ball enclosing all the points in the node. The set of point at a node is divided into two disjoint sets which form the child nodes, partitioning the space into (possibly overlapping) hyper-spheres. The tree is built hierarchically and a node is made a leaf if it contains a set of points of size below a threshold value N 0 .
We use a simple ball tree construction heuristic that ap-proximately picks a pair of pivot points which are farthest apart from each other [28], and splits the data by assigning the points to their closest pivot. The intuition behind this heuristic is that these two points might lie in the principal direction. The splitting and the recursive tree construction algorithm is presented in Algorithms 1 &amp; 2 for completeness.
Ball trees are widely used for the task of nearest neigh-bor search and are known to be fairly scalable to moder-ately high dimensions [28, 26]. The search usually employs Algorithm 3 LinearSearch(Query q , Reference Set S ) Algorithm 4 TreeSearch(Query q ,TreeNode T ) Algorithm 5 ExactMIP(Query set V , Reference Set S ) the depth-first branch-and-bound algorithm  X  a query is an-swered by traversing the tree in a depth-first manner by first going down the node closer to the query and bounding the minimum possible distance to the other branch with the triangle-inequality. If this bound is greater than the distance to the current neighbor candidate for the query, the branch is removed from computation.

An analogous greedy depth-first algorithm can be used for maximum inner-product search. But instead of travers-ing down the node closer to the query, the choice is made on the basis of the maximum possible inner-product between the query and any potential point from the node. The re-cursive depth-first branch and bound algorithm is presented in Algorithm 4. The search algorithm for a query ( q )be-gins at the root of the tree (Alg. 5). At each step, the algorithm is at a tree node ( T ). It checks if the maximum possible inner-product between the query and any point in the node, MIP ( q,T ), is any better than the current best-match for the query ( q. bm). If the check fails, this branch of the tree is not explored any more. Otherwise, the algorithm recursively traverses the tree, exploring the branch with the better potential candidates in a depth-first manner. If the node is a leaf, the algorithm just finds the best-match within the leaf with a linear search (Alg. 3). This algorithm ensures that the exact solution (i.e., the maximum inner-product) is returned by the end of the algorithm.
We present an novel analytical upper bound for the max-imum possible inner product of a given point (in this case, the query q ) with points in a ball. It is important to note that the information about the ball is limited to its cen-ter and its radius. For the rest of this section, we use the notation  X  to denote the  X  2 .

Theorem 3.1. Given a ball B R p p with radius R p and (query) point q , the maximum possible inner product between the point q and the ball B R p p 0 is bounded from above by: max
Proof. Suppose that p  X  is the best possible match in the ball B R p p 0 for the query q and r p be the Euclidean distance between the ball center p 0 and p  X  (by definition, r p  X  R Let  X  p be the angle between the vector p 0 and the vector p p  X  ,  X  and  X  p be the angles made at the origin between the vector p 0 and vectors q and p  X  respectively (see figure 5). The length of p  X  in terms of p 0 and  X  p is: The angle  X  p can be expressed in terms of p 0 and  X  p as: Let  X  q,p  X  be the angle between the vectors q and p  X  .With the triangle inequality of angles, we have: Assuming that the angles lie in the range [  X   X , X  ] (instead of the usual [0 , 2  X  ]), we get: Using this inequality we obtain the following bound for the highest possible inner-product between q and any p  X  X  R p max Byequations4,5&amp;6,wehave max The third inequality comes from the definition of maximum. The following equality comes from maximizing over  X  p .This gives us the optimal value of  X  p =  X  . The final inequality is comes from the fact that r p  X  R p . Simplifying the final inequality gives us equation 3.
 For the tree-search algorithm (Alg. 4), we set the maximum possible inner-product between q and a tree node T as This upper bound can be computed in almost the same time required for a single inner-product (since the norms of the queries can be pre-computed before searching the tree).
For a set of queries, the tree can be traversed separately for each query. However, if the set of queries is very large, a common technique to improve efficiency of querying is to index the queries in the form of a tree as well. The search is performed by traversing both trees simultaneously using the dual-tree algorithm [15]. The basic idea is to amortize the cost of tree-traversal for a set of similar queries. The dual-tree algorithms have been applied to different tree-based al-gorithms like nearest-neighbor search [15] and kernel density estimation [16] with theoretical runtime guarantees [31]. The generic dual-tree algorithm is presented in Alg. 6. Similar to the Alg. 4, the algorithm traverses down the tree on the reference set S ( RTree ). However, the algorithm also traverses down the tree on the set V of queries ( QTree ), re-sulting in a four-way recursion. At each step, the algorithm is at a QTree node Q and a RTree node T. For every Q , the value Q. X  denotes the minimum inner-product between any query in Q and its current best-match candidate. If this value is greater than the maximum possible inner product, MIP ( Q, T ), between any query in Q and any reference point in
T , this part of the recursion is no longer explored. When the algorithm is at the leaf level of both the trees, it obtains the best-matches for each query in the QTree leaf by doing a linear scan over the RTree leaf.

We explore two ways of indexing the queries  X  (1) indexing the queries using the ball-tree ( MakeQueryTree in Alg.7 is Alg. 2) (2) indexing the queries using a novel data structure, the cone-tree ( MakeQueryTree in Alg.7 is Alg. 9). In the fol-lowing subsection, we derive expressions for MIP ( Q, T )for the ball-tree. The expressions for the cone-tree is presented in section 5.
In this subsection, we provide inner-product bounds be-tween two balls with the following theorem:
Theorem 4.1. Given two balls B R p p p and q 0 with radius R p and R q respectively, the maximum possible inner-product with any pair of points p  X  X  R p p
Proof. Consider the pair of point ( p  X  ,q  X  ) ,p  X   X  B R B 0 be such that q  X  ,p  X  Let  X  p be the angle p 0 makes with the vector p 0 p  X  ,and  X  q be the corresponding angle in the query ball. Let  X  p be the angle between the vectors p 0 and p  X  and  X  q be the angle between the vectors q 0 and q  X  .Let r p be the distance between p 0 and p  X  , r q be the distance between q 0 and Let  X  be the angle made between p 0 and q 0 at the origin.
Some facts for the ball B R p p 0 (the facts are analogous for the ball B R q q 0 ): Using the triangle inequality of the angles, we know that: giving us the following: Algorithm 6 DualSearch(QTree Node Q , RTree Node T ) Algorithm 7 ExactMIPDT(Query Set V , Reference Set S ) Replacing  X  p and  X  q with  X  p and  X  q by using the afore-mentioned equalities (similar to the techniques in proof for theorem 3.1), we have: q  X  ,p  X  = q 0 ,p 0 + r p r q cos(  X   X  (  X  p +  X  q )) where the first inequality comes from the definition of max. The second inequality follows from cos(  X  )  X  1 and the final inequality comes from the fact that r p  X  R p , r q  X  R q For the dual-tree search algorithm (Alg. 6), the maximum-possible inner-product between two tree nodes Q and T is: It is interesting to note that this upper bound bound reduces to the bound in theorem 3.1 when the ball containing the queries is reduced to a single point, implying R q =0.
In equation 1, the point p , where the maximum is achieved, is independent of the norm || q || of the query q .Let  X  q,r the angle between the q and r at the origin, then the task of maximum inner-product search is equivalent to finding a point p  X  S such that: This implies that only the directions of the queries affect the solution. Balls provide bounds on the inner-product since they bound the norm of the vector as well as the direction. Since the norms do not matter for the queries, indexing them in balls is not required (and hence bounding their norms) is not necessary. Only the range of their directions need to be bounded. For this reason, we propose the indexing of the queries on the basis of their direction (from the origin) to form a cone tree (figure 8). The queries are hierarchically indexed as (possibly overlapping) open cones. Each cone is represented by a vector, which corresponds to its axis, and an angle, which corresponds to its aperture 6 .
The cone tree construction is very similar to the ball tree construction. The only difference is the use of cosine similar-ity instead of the Euclidean distances for the task of splitting (pseudo-code in Figure 8).
Since the norms of the queries do not affect the solution in equation 10, we assume that all the queries have unit norm.
Theorem 5.1. Given a ball B R p p with radius R p and a cone C  X  q q 0 of queries (normalized to length 1) with the axis of the cone q 0 and aperture of 2 0 , the maximum possible inner-product between any pair of points p  X  X  R p p 0 , q  X  X   X  q q 0 is bounded from above by: where  X  is the angle made between p 0 and q 0 at the origin and the function { x } + =max { x, 0 } . Algorithm 8 MakeConeTreeSplit(Data Q ) Algorithm 9 MakeConeTree(Set of items S )
Proof. There are two cases to consider here: (i) |  X  | &lt; X  q (ii) |  X  | X   X  q For case (i), the center p 0 of the ball B R p p 0 lies within the cone C  X  q q 0 , implying that since there could be some query q  X   X  X   X  q q 0 which is in the same direction as p 0 , giving the maximum possible inner-product.

For case (ii), let us assume that  X   X  0 without loss of gen-erality. Then  X   X   X  q . Continuing with the similar notation as in theorem 3.1 &amp; 4.1 for the best pair of points ( as well as the notation from figure 9, we can say that Since  X  q is fixed, we can say that Expressing p  X  and  X  p in terms of p 0 ,r p and  X  p ,and then subsequently maximizing over  X  p and using the fact that r p  X  R p ,wegetthat Combining case (i) and (ii), we obtain equation 11.
In this section, we evaluate the efficiency of algorithms 5(SB  X  single ball tree) &amp; 7. For the dual-tree algorithm, we use the two variations  X  (i) the set of queries indexed as a ball tree (DBB  X  dual ball-ball), (ii) the set of queries indexed as a cone tree (DBC  X  dual ball-cone). We compare our proposed algorithms to the linear search presented in Alg. 3 (LS  X  linear search). We report the speedup 7 of the proposed algorithms over linear search. For the trees, the leaf size N 0 can be selected by cross-validation. However, for our experiments, we choose a ad hoc value of N 0 =20 for all datasets to demonstrate the gain in efficiency without any expensive cross-validation.
 Datasets. We use a variety of datasets from different fields of data mining. We use the following collaborative filtering datasets: MovieLens [17], Netflix [3] and the Yahoo! Music [12] datasets. For text data, we use the LiveJournal blog moods data set [19]. We also use the MNIST digits dataset [24] for evaluation. Three astronomy datasets, LCDM [27], PSF and SJ2, are also considered. A synthetic data set (U-Rand) of uniformly random points in 20 dimensions is used. The rest of the datasets are widely used machine learning data sets from the UCI machine learning repository [5]. The details of the dataset sizes are presented in Table 1 8 . Tree construction times. The tree-building procedure is extremely efficient. We present the tree construction times in table 6 and contrast them with the runtime of the lin-ear search algorithm. In the last column, we present the ratio of the tree construction times with the runtimes of Alg. 3. For the single ball and dual ball-ball algorithm, the tree construction involves building one and two ball trees respectively. For the dual ball-cone algorithm, the queries are normalized to have unit length for convenience 9 .Fol-lowing the query normalization, two trees are built. The time required for query normalization is included in the con-struction time. This accounts for the significant difference between construction times for the dual ball-ball and the dual ball-cone algorithm.

The numbers in the last column of table 6 (R) show how small the construction times are with respect to the actual linear search. The highest ratio is 0.15 for the OptDigits Table 2: Tree construction time (in seconds) contrasted dataset. This implies that any speedup over 1.18 at search time is enough to compensate for the tree construction time. For most of the datasets, this ratio is much lower. Moreover, this tree building cost is a one time cost. Once the tree is built, it can be used for searching the dataset multiple times. Search efficiency. The speedups over linear search are presented in Table 6. Overall, the speedup numbers vary from as low as 1 . 13 for the OptDigits dataset to over 10 orders of magnitude) for the LCDM and the PSF dataset. An important thing to note here is that for datasets with low speedup (below an order of magnitude) with the single ball tree algorithm, the speedup numbers for all three algorithms were pretty low and fairly comparable. However, even a speedup of 2 is pretty significant in terms of absolute times. For example, for the Yahoo! music dataset, a search speedup of mere 2 with a tree construction time of 120 seconds gives a saving of 19 hours of computation time. For most datasets with a high value of speedup for single ball tree algorithm, the speedups for the dual-tree algorithms are also very high.
There are three important things to note here. Firstly, the dual-tree algorithms (Alg. 7) do not perform very well if the single-tree algorithms (Alg. 5) does not have a high speedup. This is mostly because the tree is unable to find tight bounds and hence has to travel every branch. The dual-tree scheme loosens the bound to amortize the traver-sal cost over multiple queries. But if the bounds are bad for algorithm 5, the bounds for the dual-tree are much worse. Hence, the dual-tree algorithm does not show any significant speedup. Secondly, the dual-tree algorithm (especially dual ball-cone) starts outperforming the single-tree algorithm sig-nificantly when the set of queries is really large. This is a usual behavior for dual-tree algorithms. The query set has to be large enough for the gains from the amortization of query traversal of the reference tree ( RTree ) to outweigh the computational cost of traversing the query-tree ( QTree )it-self. Finally, the dual-tree algorithm with ball trees for the query set is generally significantly slower than the dual-tree with a cone tree for the queries. There are possibly two possible reasons for that  X  (i) The cones provide a tighter indexing of the queries than balls. A single cone can be used to index points in multiple balls which lie in the same di-rection but have varying norms. (ii) The upper bound for MIP ( Q, T ) in equation 10 is fairly loose.

We also consider the general problem of obtaining the points in the set S with the k highest inner-product with the query q . This is analogous to the k -nearest neighbor search problem. We present the speedups of our algorithms over linear search for k =1 , 2 , 5 &amp; 10 in figure 11.
In this section, we provide some discussion of how the proposed algorithms might be applied in an inner-product space without the explicit representation of the points in the inner-product space. The inner-products are defined by a kernel function K ( q,p )=  X  ( q ) , X  ( p ) .

The tree construction has to be modified to work in the inner-product space. For a tree node T with the set of point T.S ,themeanin  X  -space is defined as  X  = 1 However,  X  might not have an explicit representation, but it is possible to compute inner products with  X  as follows: However, this computation is possibly very expensive during search time. Hence we propose picking the point in the  X  -space which is closest to the mean  X  as the new center. So the new center p c is given by: This operation is quadratic in computation time, but is done at the preprocessing phase to provide efficiency during the search phase. Given this new center p c , the radius R p of the ball enclosing the set T.S in  X  -space is given by: With these definitions for the center and the radius, a ball tree can be built in any  X  -space using Algorithm 2. Given aballin  X  -space, the equation 3 in theorem 3.1 becomes: Computing this upper bound is equivalent to a single kernel function evaluation ( K ( q,q ) be pre-computed before search-ing the tree). Using this upper bound, the tree-search algo-rithm (Alg. 5) can be performed in any  X  -space. We will evaluate this method in the longer version of the paper.
Using the same principles, the dual-tree algorithm (Alg. 7) can also be applied to any  X  -space. For the dual-tree with ball-tree for the queries, the upper bound (eq. 7) on the maximum inner-product between queries in node Q and points in node T in theorem 4.1 is modified to:
K ( q c ,p c )+ R p R q + R p K ( q c ,q c )+ R q K ( p c where p c and q c are the chosen ball centers in the  X  -space with radius R p and R q respectively.

For queries indexed in a cone-tree, the central axis of the cone can be the point in the  X  -space making the smallest angle with the mean of the set in the  X  -space. Since the queries are supposed to be normalized in the  X  -space, for a query tree node Q , the mean of the set Q.S is supposed to the cone is given by: Again, this computation is quadratic in the size of the dataset, but provides efficiency during search time. The cosine of half the aperture of the cone is now given by: The upper bound in theorem 5.1 for a cone-tree node Q of queries and a ball-tree node T of reference points becomes: where  X  is defined as: This bound is very efficient to compute as it only requires a single kernel function evaluation (the terms K ( p c ,p K ( q c ,q c ) can be pre-computed and stored in the trees).
We consider the general problem of maximum inner-product search and present three novel methods to solve this prob-lem efficiently. We use the tree data structure and present a branch-and-bound algorithm for maximum inner-product search. We also present a dual tree algorithm for multiple queries. We evaluate the proposed algorithms with a variety of datasets and exhibit their computational efficiency.
A theoretical analyses of these proposed algorithms would give us a better understanding of the computational effi-ciency of these algorithms. A rigorous analysis of the run-time for our algorithm would be part of our future work. [1] R. Bayardo, Y. Ma, and R. Srikant. Scaling Up All [2] R. M. Bell and Y. Koren. Lessons from the Netflix [3] J. Bennett and S. Lanning. The Netflix Prize. In Proc. [4] A. Beygelzimer, S. Kakade, and J. Langford. Cover [5] C. L. Blake and C. J. Merz. UCI Machine Learning [6] L. Cayton and S. Dasgupta. A Learning Framework [7] M. S. Charikar. Similarity Estimation Techniques from [8] P. Ciaccia and M. Patella. PAC Nearest Neighbor [9] K. Clarkson. Nearest-neighbor Searching and Metric [10] S. Dasgupta and Y. Freund. Random projection trees [11] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [12] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [13] J. H. Freidman, J. L. Bentley, and R. A. Finkel. An [14] A. Gionis, P. Indyk, and R. Motwani. Similarity [15] A. G. Gray and A. W. Moore.  X  N -Body X  Problems in [16] A. G. Gray and A. W. Moore. Nonparametric Density [17] GroupLens. MovieLens dataset. [18] P. Indyk and R. Motwani. Approximate Nearest [19] S. Kim, F. Li, G. Lebanon, and I. Essa. Beyond [20] M. Klaas, D. Lang, and N. de Freitas. Fast [21] Y. Koren. The BellKor solution to the Netflix Grand [22] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [23] B. Kulis and K. Grauman. Kernelized [24] Y. LeCun. MNist dataset, 2000. [25] Z. Li, H. Ning, L. Cao, T. Zhang, Y. Gong, and T. S. [26] T. Liu, A. W. Moore, A. G. Gray, and K. Yang. An [27] R. Lupton, J. Gunn, Z. Ivezic, G. Knapp, S. Kent, [28] S. M. Omohundro. Five Balltree Construction [29] F. P. Preparata and M. I. Shamos. Computational [30] A. Rahimi and B. Recht. Random Features for [31] P. Ram, D. Lee, W. March, and A. Gray. Linear-time [32] P. Ram, D. Lee, H. Ouyang, and A. G. Gray.

