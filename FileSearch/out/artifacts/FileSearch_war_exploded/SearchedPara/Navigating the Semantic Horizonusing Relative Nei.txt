 Nearest neighbor search is a fundamental opera-tion in data mining, in which we are interested in finding the closest points to some given reference point. Formally, if we have a reference point r and a set of other points P in a metric space M with some distance function d , the nearest neigh-bor search task is to find the point p  X  P that min-imizes d ( p, r ) . In k -Nearest Neighbor search ( k -NN), we want to find the k closest points to some given reference point. Nearest neighbor search is a well-studied task, and in particular the com-plexity of the task (a linear search has a running time of O ( N i ) where N is the cardinality of P and i the complexity of the distance function d ) has generated a lot of research; suggestions for re-ducing the complexity of linear nearest neighbor searches include using various types of space par-titioning techniques like k -d trees (Bentley, 1975), or various techniques for doing approximate near-est neighbor search (Arya et al., 1998), of which one of the most well-known is locality-sensitive hashing (Indyk and Motwani, 1998).

The problem we are concerned with in this paper is not the complexity of nearest neighbor search, but the question of how to identify the in-ternal structure of neighborhoods defined by the nearest neighbors . The problem with a normal k -NN is that the result  X  a sorted list of the k nearest neighbors  X  does not say anything about the inter-nal structure of the neighborhood. It is quite pos-sible for two neighborhoods with widely different internal structures to produce identical k -NN re-sults. In the context of Distributional Semantic Models (DSMs), which collect and represent co-occurrence statistics in high-dimensional vector spaces, such structural differences may carry sig-nificant semantic information, e.g. about the dif-ferent senses of terms. We argue that the inability of standard k -NN to account for structural prop-erties has been misinterpreted as a shortcoming of the distributional representation (Erk and Pad  X  o, 2010).

We will demonstrate in this paper that this is not a shortcoming of the distributional represen-tation, but of the mode of querying the DSM. We argue that information about the different usages (i.e. senses) of a term is encoded in the structural properties of the nearest neighborhoods, and we propose the use of relative neighborhood graphs for identifying these structural properties. Relative neighborhood graphs may also be used for finding a relevant k for a given reference point, which we refer to as the horizon with respect to the reference point. Collecting and comparing co-occurrence statis-tics for terms in language has become a stan-dard approach for computational semantics, and is now commonly referred to as distributional se-mantics . There are many different types of mod-els that can be used for this purpose, but their common objective is to represent terms as vec-tors that record (some function of) their distri-butional properties. The standard approach for generating such vectors is to collect distributional statistics in a co-occurrence matrix that records co-occurrence counts between terms and contexts. The co-occurrence matrix is then subject to var-ious types of transformations, ranging from the application of simple frequency filters or associ-ation measures to matrix factorization or regres-sion models. The resulting representations are re-ferred to as distributional vectors (or word embed-dings ), which are used to compute similarity be-tween terms.

Given a similarity  X  or distance  X  measure on such distributional vectors, we can perform a near-est neighbor search. This is a particularly impor-tant operation in distributional semantics, since it answers the question  X  X hich other terms are sim-ilar to this one? X  and this is a central question in semantics; lexica and thesauri are built with the main purpose of answering this question. Conse-quently, nearest neighbor search in a DSM could be seen as a compilation step in a distributional lexicon.
 The result of a nearest neighbor search in a DSM is often presented as a list of (the top k ) neighbors, sorted by descending similarity with the target term. Table 1 illustrates typical sorted nearest neighbor lists produced with three dif-ferent DSMs: a standard model based on Point-duced to 2,000 dimensions by applying a Gaus-sian random projection; GloVe, which uses regres-sion to find distributional vectors such that their dot product approximates their log probability of co-occurring (Pennington et al., 2014); and the Skipgram model, which uses stochastic gradient descent and hierarchical softmax combined with negative sampling and subsampling to find dis-tributional vectors that maximize the probability of observed co-occurrence events (Mikolov et al., 2013). We refer to the respective papers for de-tails regarding the various models. The similarity measure used is the cosine similarity: s ( a, b ) =
Table 1 lists the 10 nearest neighbors to suit in these three different DSMs using the entire Wikipedia as data. As can be expected, there are both similarities and dissimilarities between Table 1: Sorted list of the nearest neighbors to  X  X uit X  in three different distributional models. these neighborhoods;  X  X uits X  and  X  X awsuit X  oc-cur among the 10 nearest neighbors to  X  X uit X  in all three models, whereas other terms are spe-cific for one particular model. What is com-mon between the three models is that they all feature neighbors that represent two different us-ages of  X  X uit X : the law -sense ( X  X awsuit X ) and the clothes -sense ( X  X ress X ,  X  X earing X ,  X  X ouble-discernible by merely looking at the list of near-est neighbors; the only information it provides is the ranking of the nearest neighbors in descending order of similarity.

It has been argued that DSMs that represent terms by a single vector cannot adequately handle polysemy, since they conflate several different us-age patterns in one and the same vector (V  X  eronis, 2004; Erk and Pad  X  o, 2010). Examples like the one above is often cited as evidence. We argue that this critique is unfounded and misinformed, and that it is the mode of querying the DSM that can be susceptible to problems with polysemy. As the above example demonstrates, querying DSMs by k -NN conflates different usages of terms. The rea-son for this seems quite obvious: simply ranking the nearest neighbors by similarity (or distance) ignores any local structures of the neighborhood. If  X  X uit X  has as neighbors both  X  X ress X  and  X  X aw-suit X , which represent two distinct types of usages of  X  X uit X , there will be a structural distinction in the neighborhood of  X  X uit X  between these differ-ent neighbors, since they will be mutually unre-lated (i.e. there is a similarity between  X  X uit X  and  X  X ress X  and between  X  X uit X  and  X  X awsuit X , but not between  X  X ress X  and  X  X awsuit X ). k -NN also gives rise to another problem re-lated to polysemy in DSMs. The problem is that the most frequent senses will populate the top of the nearest neighbor list, while the less fre-quent senses will not appear until further down the list, and if we set a too restrictive k , we will only see neighbors relating to the most frequent sense. As an example, consider the two differ-ent senses of  X  X uit X  above. The distributional vec-tor for  X  X uit X  can be thought of as a sum v suit = f v suit | law is an idealized notion of the true dis-tributional vector of  X  X uit X  in the law -sense, and f From there one can easily argue that a similar-ity such as s ( v suit , v clothes ) is actually a weighted composite of the similarities s ( v suit | law , v clothes dominantly in the law -sense in our corpus, the k -NN neighborhood of  X  X uit X  will be dominated by words pertaining to its law -sense, while the less frequent senses might not be present at all. A misguided k may thus obscure any other, less fre-quent, senses of a term. Selecting a relevant k for a given term and group-ing the neighbors according to which senses they represent is an example of Word-Sense Induction (WSI). DSMs are well suited for this task, and there have been a number of different approaches suggested in the literature. One of the earliest ap-proaches is distributional clustering (Pereira et al., 1993), which is based on a probabilistic decompo-sition model that uses maximum likelihood esti-mation to fit the model to observed data. Another example is Clustering By Committee (CBC) (Pan-tel and Lin, 2002), which first uses average-link clustering to recursively cluster the nearest neigh-bors of a term into committees, which are then used to define clusters by iteratively adding com-mittees whose similarity to the term exceeds a cer-tain threshold, and that is not too similar to any other added committee. For each added commit-tee, its features are also removed from the distri-butional representation of the term. This last step ensures that the clusters do not become too similar, and that clusters representing less frequent senses can be discovered.

The idea of iteratively removing features from the distributional vector when a sense cluster as been formed is also present in Dorow and Wid-dows (2003), who use a graph-based clustering method. Another graph-based approach is the HyperLex algorithm (V  X  eronis, 2004), which con-structs a graph connecting all pairs of terms that co-occur in the context of an ambiguous term. The resulting graph contains highly connected compo-nents, which represent the different senses of the term. Agirre et al. (2006) compare HyperLex to PageRank (Brin and Page, 1998) and demonstrates that the two methods perform similarly.

There have also been several attempts to use various types of matrix factorization for WSI. The idea is that the factorization uncovers a set of global senses in the form of the latent factors, and that the sense distribution for a given term can be described as a distribution over these la-tent factors. Examples of factorization methods that have been used include different versions of Latent Dirichlet Allocation ((Brody and Lapata, 2009; S  X  eaghdha and Korhonen, 2011; Yao and Van Durme, 2011; Lau et al., 2012) and non-negative matrix factorization (Dinu and Lapata, 2010; Van de Cruys and Apidianaki, 2011).

Tomuro et al. (2007) argue that clustering ap-proaches like distributional clustering or CBC may produce clusters that are themselves polysemous, which may not be a desirable property of a WSI algorithm, and suggests using feature domain sim-ilarity to solve this problem. The idea is to incor-porate similarities between the features of items rather than the similarity between the items them-selves in a modified version of CBC that enables the algorithm to utilize feature similarities, which inhibit the formation of polysemous clusters.
Koptjevskaja Tamm and Sahlgren (2014) also leverage on the idea of using feature similarity as the basis of sense clustering. The approach, called syntagmatically labeled partitioning , relies on a DSM that encodes sequential as well as sub-stitutable relations. The method essentially sorts the k nearest (substitutable) neighbors according to which sequential connections they share. The resulting partitioning of the nearest distributional neighbors does not only constitute a WSI, but it also provides labels for the induced senses in the form of the sequential connections the neighbors share. Many of the previous WSI approaches operate at a global level, utilizing global structural proper-ties of the semantic spaces, e.g. by matrix fac-torization techniques. We believe this is as ill-advised as setting a global k or radius for the near-est neighbor search, since it is the local structures that are important when analyzing nearest neigh-bors. Other WSI approaches use various forms of clustering techniques. However, previous stud-ies of the intrinsic dimensionality of distributional semantic spaces using fractal dimensions indicate that neighborhoods in semantic space have a fila-mentary rather than clustered structure (Karlgren et al., 2008).

We therefore propose the use of topological models that take the local structure of neighbor-hoods in semantic space into account. The ap-proach discovers different word senses from the local structure of neighborhoods, given nothing but similarities between points. As such it is easy to test on widely different vector models, as long as there exists a well behaved similarity function. The proposed approach not only answers the ques-tion which other terms are similar to a given term, but also how are they similar.

Relative neighborhoods, first proposed in (Tou-ssaint, 1980), are examples of empty region graphs (Cardinal et al., 2009), where points are neighbors if some region between them is empty. For Rela-tive Neighborhood Graphs (RNG) this region be-tween two points a and c is defined as the inter-section of the two spheres with centers in a and c with radius d ( a, c ) . In other words, a point b lies between points a and c if it is closer to both a and c than a and c are to each other. If no such point b exists, a and c are neighbors. Illustrations of this can be seen in Figure 1.
 Figure 1: Example of when point b is between point a and c (left), and when it is not (right).
Such neighborhoods have been argued to better preserve local topology (Bremer et al., 2014), and be more robust to deformations of the data than k -NN neighborhoods (Correa and Lindstrom, 2012) as they in some sense contain information about direction whereas k -NN neighborhoods only con-tain information about distance. Going back to the  X  X uit X  example, we can see that if  X  X uit X  in the law -sense is more similar to the composite  X  X uit X  than to its clothes -sense, and vice versa, then the com-This in turn means that out of those two points, both are relative neighbors to  X  X uit X , and neither of them lies between the other and  X  X uit X .

Formally, the set of points between two points a, c  X  V can be characterized and computed in the following way: btw ( V, a, c ) = { b | b  X  V, b is between a and c } rng-nbh ( V, a ) = { c | c  X  V, btw ( V, a, c ) =  X  X 
E rng ( V ) = { ( a, b ) | a  X  V, b  X  rng-nbh ( V, a ) } where E rng is the undirected edge set of the RNG. The function btw ( V, a, c ) can be straightforwardly translated to an algorithm taking O ( | V | ) time, making the rng-nbh ( V, a ) function take O ( | V | 2 ) time, which in turn makes the computation of the complete graph take O ( | V | 3 ) time. 5 Clearly un-feasible, but we have not found any alternatives
Correa and Lindstrom (2012) note that the inter-section of the RNG and the k -NN graph is a more feasible alternative:
Given a precompiled k -NN lookup, the above takes O ( k 2 ) time, so using a heap-based O ( | V | lg k ) k -NN algorithm results in an algo-rithm taking O ( k 2 + | V | lg k ) time.

The same idea can be used to build a tree struc-ture rooted in a reference word a in the following way: rnbh-tree ( V, a ) = { ( c, arg min which can easily be restricted to the k -nearest neighbors of a in much the same way as above, with the same monotonic behavior.

Computing this for a point a produces a tree where the direct children of a are its relative neigh-bors, and the parent of a point c further down the tree is the point between a and c that is closest to c . This structure, while similar to a minimum spanning tree , differs in some crucial regards: the rnbh-tree ( V, a ) is rooted in a word a . The differ-ence between rnbh-tree ( V, a ) and rnbh-tree ( V, b ) is often quite significant. Furthermore, the re-stricted k-rnbh-tree is monotonic in k . That prop-erty does not hold for a minimum spanning tree of a local neighborhood. To get an intuition of what these neighborhoods look like we present a few examples. The words have been chosen either because they are com-mon examples in similar work  X  e.g.  X  X eart X  and  X  X uit X  from Pantel and Lin (2002)  X  or because they represent different parts-of-speech ( X  X bove X  is a preposition,  X  X ad X  is an adjective, and  X  X er-vice X  is a noun) and disparate kinds of ambiguity ( X  X range X  can be both a fruit and a color).

Figure 2 (next page) illustrates what an RNG looks like for the term  X  X eart X  and its 100 near-est neighbors in the PMI model. Note that the root  X  X eart X  (at the mid-left in the graph) only has two relative neighbors:  X  X ardiac X  and  X  X oul, X  arguably representing a body -sense and a soul -sense of the term. One advantage of using this type of structure for the neighborhood is that it enables us to examine various depths of the tree. Depth one includes only the direct neigh-bors ( X  X ardiac X  and  X  X oul X ), while depth two in-cludes all neighbors two steps away in the graph:  X  X isease, X   X  X oronary, X   X  X ulmonary, X   X  X ardiovascu-lar, X   X  X entricular, X  and  X  X ailure, X  which are all chil-dren to  X  X ardiac. X  This tree structure can be used to identify neighbors that are themselves polyse-mous (c.f. the critique mentioned in Section 3 of clustering-based approaches to word-sense induc-tion that they may produce polysemous clusters ). One example is the neighbor  X  X isease X  at depth two, which has six children that refer to different aspects of disease.

We argue that the RNG can be quite useful for WSI, since the branching structure indicates different usages, and the depth factor enables us to calibrate the granularity of the induced word senses. If we only consider direct neighbors (i.e. depth one), and set k = V (i.e. we do an exhaustive nearest neighbor search), we will ex-tract all terms that have a direct connection to the reference term. We refer to this neighborhood as the semantic horizon . At the most coarse level of analysis, this is the neighborhood that represents the main induced senses of a term. Tables 2 and 3 provide examples of 1,000-RNG neighborhoods of depth one.
 Table 2: RNG for k = 1 , 000 of the words  X  X uit, X   X  X range, X  and  X  X eart X  in three different semantic models. The numbers in parenthesis indicate the k -NN ranks of the neighbors.

These examples demonstrate some interesting similarities and differences between the three models. First of all, there are some direct neigh-bors that are present in all three models:  X  X uit X  has  X  X uits X  and  X  X awsuit X  as direct neighbors in all three models,  X  X eart X  has  X  X earts, X   X  X ervice X  has  X  X ervices, X  and  X  X bove X  has  X  X elow X . Plu-ral forms are of course reasonable neighbors of their singular counterparts in a semantic model, but their usefulness for WSI can perhaps be ques-tioned. Taking  X  X uits X  to indicate the clothes -sense of  X  X uit, X  all three models produce both a clothes -sense and a law -sense. For  X  X range, X  the Skipgram model only represents the color -sense, while the PMI and GloVe models also feature a fruit -sense. For  X  X eart, X  all three models have a disease -sense (represented by the neighbors  X  X ardiac X  in the PMI and GloVe models, and the neighbor  X  X ongestive X  in the Skipgram model), and an organ -sense (rep-resented by the plural form  X  X earts X ).  X  X ervice X  is a comparably vague term that has a number of different senses in the PMI and GloVe models, but only one in the Skipgram model.  X  X ad X  pro-duces both a negativity -sense and a German spa town -sense in all three models, but only the GloVe and Skipgram models have a separate antonym -sense ( X  X ood X  is not a direct neighbor in the PMI model).  X  X bove X  has both the antonym and direct neighbors relating to measurements in all three models.

It is interesting to note that GloVe produces a significant amount of sequential relations;  X  X o-bile suit gundam X ,  X  X heap suit serenaders X ,  X  X r-ange peel X , and  X  X range jumpsuit X  are just some of many examples of sequential relations found in the relative neighborhood of terms in the GloVe model.

The PMI and GloVe models produce the struc-turally most similar RNGs in these examples, with on average a handful of direct neighbors, of which some can be very distant. The Skipgram model on the other hand produces very few direct neigh-bors. This led us to look further into the struc-tural properties of neighborhoods in the Skipgram model. An interesting observation  X  and possi-ble complication  X  is that the neighborhoods in the Skipgram model are highly asymmetric: the first neighbor of  X  X nformation X  is  X  X nformations X , whereas  X  X nformation X  is the 1,829th neighbor of  X  X nformations. X  While such asymmetry occurs in all models, it seems much more prevalent in the Skipgram model. Figure 3 confirms this suspi-cion: each point corresponds to a random word pair ( a, b ) with x corresponding to where b is in the ordered list of a  X  X  neighbor, and y to where a is in the ordered list of b  X  X  neighbors. The figure Skipgram to the right.
 Table 3: k -RNG for k = 1 , 000 of the words  X  X er-vice, X   X  X ad, X  and  X  X bove X  in three different seman-tic models. The numbers in parenthesis indicate the k -NN ranks of the neighbors.
 shows that the local densities vary much more in the Skipgram model than in the others. This is not in itself undesirable, but wild differences in neigh-borhood reciprocity complicates the choice of k in the k -RNG algorithm, as observed by the particu-larly sparse neighborhoods of the Skipgram model above. The standard way to evaluate WSI algorithms is to use one the SemEval WSI test collections (Agirre and Soroa, 2007; Manandhar et al., 2010; Nav-igli and Vannella, 2013; Jurgens and Klapaftis, 2013), which are all designed similarly: systems are expected to first perform WSI and then to as-sign texts to the induced senses (i.e. in effect do-ing a word-sense disambiguation step). We con-sider this type of evaluation to be a less useful for our purposes, since the required disambigua-tion step is a highly non-trivial task in itself. The RNG method proposed in this paper is a pure WSI algorithm, and as such does not offer a solu-tion to the disambiguation problem. We therefore opted to focus solely on the hypothesis that rel-ative neighborhoods cover senses that k -NNs do not. In essence, we investigate whether k -RNG retrieval does a better job at covering different senses than k -NN retrieval . This was done using pseudowords .

Pseudowords are artificially ambiguous words, created by regarding different words as identi-cal. We can, for example, say that the pseu-doword &lt; deadeye &gt; is a composite of the two words marksman and loudspeaker . A corpus with the artificially ambiguous word &lt; deadeye &gt; in it can then be created by replacing all occurrences of the words marksman and loudspeaker with &lt; deadeye &gt; .

Using the pseudowords provided by Pilehvar and Navigli (2013) a corpus with 689 non-overlapping pseudowords was created, based on one on the altered corpus, and one on the unal-tered one. To check whether the neighborhood of a pseudoword contains information about its under-lying senses we compared each underlying sense to the words in the neighborhood, taking the mini-mum of all senses X  maximum similarity as a score, as demonstrated in Table 4. The similarities were calculated using the model trained on the unaltered corpus, as the one based on the altered corpus will not contain the underlying senses of pseudowords.
Working through the example in Table 4, the neighborhood of the pseudoword &lt; deadeye &gt; consists of the three words shooter , stereo , and sport . The pseudoword in itself is made up of the two underlying senses marksman and loud-speaker . The similarities between the words in the neighborhood of the model trained on the unal-tered data and the words of the underlying senses are as presented in Table 4. The closest word to marksman is shooter , with a similarity score of 0.7. The closest word to loudspeaker is stereo , with a score of 0.3. So the scoring would, in total, be 0.3. It should be noted that the upper bound for this score is oftentimes significantly lower than 1: The neighborhood could not possibly contain the words marksman or loudspeaker , as those words are not present in the corpus. This means that the scores are bounded by the similarity of the least similar closest neighbor to the underlying senses. Table 4: Example scoring of a neighborhood of the word &lt; deadeye &gt; . &lt; deadeye &gt; shooter stereo sport max marksman 0.7 0.04 0.4 0.7 loudspeaker 0.01 0.3 0.05 0.3
This score was chosen because of its simplic-ity and intuitive interpretation: a low score im-plies that at least one word sense was not repre-sented in the neighborhood whereas a high score means that all senses are represented in the neigh-borhood. One can then plot these scores for both relative neighborhoods and k -NN neighborhoods for each pseudoword as is done in Figure 4. Each point ( x, y ) represents a pseudoword, with x and y being the score of the k -NN neighborhood and the k -RNG neighborhood respectively.

Figure 5 shows an aggregate of Figure 4, plot-ting the distribution of y  X  x , i.e. the difference be-tween the scores achieved by the k -RNG and the k -NN. As seen in Figure 4, a lot of points lie on the line y = x , meaning both methods achieved the same score. However, when this is not the case, there is a clear bias for the k -RNG to out-perform the k -NN, as demonstrated in Figure 5. Here, using the BNC instead of Wikipedia as train-ing data, the GloVe and Skipgram models yielded sparse relative neighborhoods  X  both with an av-erage of about 8 neighbors  X  but the PMI model produced quite dense neighborhoods averaging 63 neighbors. Since the scoring function does not pe-nalize neighborhood size there is good reason to be skeptical of its viability, and specifically the performance of the PMI-model based on these fig-ures. Figure 5: Distribution of difference between scores for k -RNGs and k -NNs. Positive scores means that the k -RNG scored higher than the k -NN This paper has discussed the question how to query semantic models, which is a question that has been long neglected in research on computa-tional semantics. Nearest neighbor search (or k -NN) is often treated as the only available option, which leads to misunderstandings regarding how semantic models represent and handle vagueness and polysemy. We have argued that the structure  X  or topology  X  of the local neighborhoods in se-mantic models carry useful semantic information regarding the different usages  X  or senses  X  of a term, and that such topological properties there-fore can be used to analyze polysemy and do WSI.
We have introduced relative neighborhood graphs (RNG) as an alternative to standard k -NN, and we have exemplified k -RNG in three differ-ent well-known semantic models. The examples demonstrate that k -RNG manages to retrieve dis-parate and relevant neighbors in all three models, yet the kind of neighbors returned and the nature of the neighborhoods differ. Quantitatively, The k -RNG method consistently outperformed k -NN on underlying sense retrieval.

We have also illustrated how k -RNG can be used as a tool to gain insight into the topological properties of different models. The GloVe model, for example, makes no difference between sequen-tial and substitutable relations, leading to neigh-borhoods that contain n -grams instead of senses. This can clearly be seen in for example Table 2. Skipgram uses more sophisticated tokenization, which alleviates this issue.
 Another interesting result of the paper is that the RNG uncovers otherwise unseen differences be-tween the models, which manifest not as scoring differences but as properties of the word represen-tations themselves. One example is the differences in neighborhood reciprocity observed between the different models.

