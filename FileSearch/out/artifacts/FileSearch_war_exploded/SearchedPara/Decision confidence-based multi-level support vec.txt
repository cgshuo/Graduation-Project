 techniques are available such as cross validation ( Devijver and basis of these techniques is to either penalize an overly complex accuracy of the model.

In this paper we propose a new architecture for support vector of the model. When the complexity of the model is high, the con fi dence measure yields a low value. But de fi ning a con fi dence measure over the output prediction of a learning algorithm is a challenging problem and has not been addressed in depth for
SVM. The main work was based on the notion of algorithmic randomness de fi ned by Kolmogorov (1968) . This notion remained of purely mathematical interest for the simple reason that algo-rithmic measure of randomness is non-computable ( Saunders et al., 1999 ). Hence work was concentrated on fi nding a compu-table approximations to algorithmic randomness and to apply these approximations to benchmark datasets.

Saunders et al. used support vectors (SV) to implement an approximation for the notion of randomness ( Saunders et al., 1999 ). They relied on the fact that in support vector machines a y  X  f  X  not similar to the training points, does not necessarily have low con fi dence. It might be further away from the separating line low value. Therefore, Platt ' s measure is highly dependent on the choice of the training points and their distribution across space.
Con fi dence measure has also been used in development of active learning algorithms. Active learning tries to resolve the problem of choosing the training points that has the most information. In Li and Chen (2006) , Li introduced a con fi dence measure based on the output of the support vector machines but without using a sigmoid function to model the output as the one used by Platt (1999) . A dynamic bin width allocation method was proposed to estimate the sample conditional error for each data neighbor algorithm and is combined with the distance to the hyperplane and integrated with support vector machines to also decide which points to use for training ( Mitra et al., 2004 ).
Another con fi dence approach was done by our team ( Sakr et al., 2010 ). This approach introduced a binary con fi dence measure using the Vapnik and Chervonenkis (VC) dimension for binary  X 
O  X   X 
M this con fi dence was only a binary con fi dence measure and the cutoff between con fi dent and non-con fi dent decision was not de fi ned. Hence, this method did not allow the comparison possible to know which one has a higher con fi dence than the other.

In this paper we present a new architecture for SVM that is built using a new con fi dence measure. We also present practical postal dataset, the Wisconsin breast cancer dataset and the agitation detection dataset.

In the next section, a concise introduction of the theory of support vector machines (SVM) is presented, this section also output. Then the architecture is introduced and applied on the testing datasets. 2. Multi-level architecture design
The multi-level SVM architecture is a two-class classi fi er based on cascading N SVMs. The algorithm to develop the architecture is by point. If the con fi dence level over a point is greater than a number of SVMs created before the stopping criterion is met, is called the size of the architecture and is denoted by N . dimensional feature space. This task is achieved using various nonlinear mapping functions: polynomial, sigmoid and radial basis functions (RBF) such as gaussian RBF. In the higher dimen-sion feature space the SVM algorithm separates the data using a linear hyperplane. Another common approach to separate non-meter C . It is very common, in many fi elds, to classify non-separable data using a linear classi fi er by simply regularizing using the C -parameter. Lowering the C value reduces model complexity at the expense of having some errors on the training data. Not like other techniques, probability model and probability density functions do not need to be known apriori. This is very important for generalization purposes, as in practical situations, there is not enough information about the underlying probability laws and distributions between the inputs and the outputs.
In the case of linearly separable data the approach is to fi nd among all the separating hyperplanes the one that maximizes the margin. Clearly, any other hyperplane will have a greater expected risk than this hyperplane.

During the learning stage the machine uses the training data to fi d  X  x ; w ; b  X  given by d  X  x ; w ; b  X  X  w T x  X  b  X   X  n the testing phase, an unseen vector z , will produce an output y according to the following indicator function: y  X  sign  X  d  X  z ; w ; b  X  X  X  3  X  class (  X  1).

The weight vector and the bias are obtained by minimizing the following equation:
L  X   X 
 X  X  0 : 5  X  T H  X   X  f T  X   X  4  X  subject to the following constraints: y
T  X   X  0 ;  X   X  0 : optimization problem will be suf fi cient to determine the weight vector and the bias using the following equations: w  X  and the bias is given by b  X  1 l where l represents the number of support vectors.

The linear classi fi er presented above has limited capabilities since it is only used with linearly separable data while in most
The nonlinear data has to be mapped to a new feature space of
From the theory of reproducing kernel Hilbert spaces ( Aronszajn, 1950 ), which is beyond the scope of this paper, a kernel function is de fi ned to be
K  X  x ; x j  X  X   X   X  x i  X  T  X   X  x j  X  :  X  7  X  function of the VC dimension, and thus a trade-off should be made between the empirical error and the generalization error.
Although it is extremely dif fi cult and sometimes impossible to compute the VC dimension of a certain algorithm, a bound on the
VC dimension has been established and will be very useful in building the con fi dence and credibility measures. Vapnik states that a bound on the VC dimension is given by
VC o VC max  X   X  w  X  2 D 2  X  12  X  where D is the minimum radius of the sphere that contains all the is minimizing.  X  w  X  2 is given by Vapnik (1998) :  X  w This bound is important in two ways: it is easy to compute, and
In particular he showed that, in most of the cases, the true minimum of the VC dimension is obtained when this bound is subscript and denote it by VC. 2.3. Con fi dence: complexity level approximation de as  X  1. The optimization problem yields the Lagrange multipliers optimization is done using z labeled as + 1. The optimization to by Eq. (13) . It remains to fi nd D . Since D is the radius of the smallest sphere englobing all training points then it is indepen-solved by Vapnik (1998) and in more details by Scholkopf and
Smola (2002) . They established that when the training points  X  x by the mapping function  X  , the center O of the sphere that englobes all training points is given by
O  X   X  n where  X  i is given by the following quadratic minimization equa-tion:
L  X   X  n subject to the constraints  X   X  i 4 0  X 
O  X  is the center of the sphere of smallest radius that contains all training points, thus it is possible to deduce the radius of the and any training point x k :
D  X  max as con fi dence reference in what follows. In order to fi nd this reference value of the con fi dence consider any two support which is de fi ned by v  X  v  X   X  t  X  v  X   X  v  X   X  where t  X   X  0 ; 1 . This point is a variable point on the segment connecting the two support vectors as shown in Fig. 1 . When computing the con fi dence over v t the following analysis holds.
As t increases v t moves from v  X  to v  X  . To compute the con fi dence, v t is added to the training set fi rst as of class (  X  1). support vector of class (  X  1). Indeed, since v  X  and v  X  are two hyperplane passes between them. When v t is added to the training
Hence the hyperplane is now passing between v t and v  X  because
Thus v  X  is not a support vector anymore. Hence the Lagrange matrix will have two identical lines with each line representing a since C is their upper limit then  X  t  X   X   X   X  C .

Next v t is trained as class ( + 1). This will remove v  X  from the support vector set for the same reason stated above except that now v t is in class ( + 1). Hence the Lagrange multiplier  X   X  is now until  X  t reaches 0 when t  X  1 and  X   X  has its original value. Fig. 2  X  (  X 
The objective is to fi nd the lowest con fi dence while moving  X  1, hence the con fi dence criterion is  X   X  d  X  z  X  X  VC  X  1  X  VC 1  X  X  VC 1  X  VC  X  1
Minimizing  X  minimizes the con fi dence. Hence to minimize the con fi dence, the point that maximizes VC  X  1 and minimizes VC 1 , and  X  t change values and all the other  X  i exhibit a negligible
Between all support vectors from different classes, the two support vectors that yield a point on the hyperplane that has the the two vectors that correspond to the highest increasing rate of  X  where the intersection point of  X   X  t with the hyperplane must be In order to achieve it the gradient of the curves must be high.
To fi nd the reference values of both regions, one can use the exhaustive search method and try all possible combinations of support vectors starting from the negative region toward the positive region, then repeat this procedure starting from positive number of support vectors of negative type is N n and of positive following two propositions this number can be reduced to N n N p . on the negative side of the hyperplane and T  X  max be the point the con fi dence between these two points is related by
C
Proof. The criterion for t max on the negative side is given by Thus
C hyperplane from the negative side between any 2 support vectors of given by
C
C
Proof. Because of the discontinuity of the con fi dence on the hyperplane if a hyperplane has a high con fi dence on one side it the minimal con fi dence achieved on the hyperplane from the negative side between any 2 support vectors of different classes, the highest con fi dence achieved on the hyperplane on the nega-from the other side hence it is the reference con fi dence for the positive region. &amp; .

From the analysis made above we can also deduce that the con fi dence for a training point is 1. Hence the threshold T can and 1.

De
C ref be the two reference points for both classes. The the cut-off maintaining the same accuracy over the training set, this demon-that of the single SVM. 3. Simulations and results
This section presents the results of the proposed architecture compared to the accuracy of the traditional single SVM trained with the same training set. The architecture is tested for binary classi fi cation over 3 different datasets as well as multi-class classi fi cation for digit recognition. 3.1. US postal dataset
The US postal handwritten digit recognition is a well known dataset that was used by Le Cun et al. (1990) . The numbers are represented using 16 16 pixels that represent the normalized gray scale of each pixel. Each number has multiple samples coming from different people ' s handwriting. Our architecture is and  X  8  X  . These numbers were chosen because the literature shows that these are the most dif fi cult digits to classify. 3.1.1. Binary classi fi cation
In total 1720 samples for both numbers are studied. The K-Fold cross validation technique is used to evaluate the performance of the proposed con fi dence measure. The 1720 samples are divided into 10 groups of 172 samples each. Each group contains samples set and the remaining 9 folds as testing set. One kernel function that satis fi es the RBF property is the Gaussian kernel function de fi ned as
K  X  x ; x j  X  X  exp  X  1 2 s 2  X  x i  X  x j  X  T  X  x i  X  x j  X  as described in Section 2.5 . A leave one out cross-validation was conducted where 1 = s 2 and C were varied over a large range. The the 2 groups, were chosen in the larger 10-fold cross-validation procedure. The values found are s  X  noting that no training point was used at the same time for the accuracy of the testing phase. Next we proceeded by fi nding the cut-off thresholds as described in Section 2.4 . Since at each off thresholds and its own architecture. It happened that for all
For each iteration, the accuracy of the traditional SVM as well as the accuracy of our architecture were computed. Fig. 5 shows both accuracies with respect to the corresponding fold. The fi gure shows that the accuracy of the multi-level architecture is always greater or equal to that of the traditional SVM. And on average, with a standard deviation of 0.87% while the average accuracy of the proposed architecture is 96.8% with a standard deviation of 0.62%. By noting that the training accuracies for both the tradi-testing accuracy of the architecture in comparison with the traditional SVM shows that the overall complexity of the model .
 accuracy while the multi-level architecture had a 91.4 % accuracy with a standard deviation of 0.61%. 3.2. The Wisconsin breast cancer dataset
This dataset has 569 samples and uses 10 features to predict if a tumour is malignant or benign. The features are computed from a digitized image of a fi ne needle aspirate (FNA) of a breast mass.
They describe characteristics of the cell nuclei present in the image. More information on this dataset and a full description of the features can be found in Street et al. (1992) . The same cross validation procedure as described previously was also followed on this dataset. The best parameters were found to be s  X  31 and was 91.4% with a standard deviation of 2.41% over all the folds.
Note that in this case we have 30 folds from which 1 was used for training and the remaining for testing. Fig. 9 shows both accura-traditional SVM for 26 folds out of 30.

Table 3 shows the change of accuracy with different values of s with C having a fi xed value of 50. Note the almost constant performance with the increase of complexity of the feature space (as s decreases) indicating reduced over fi tting. 4. Conclusion and future work
In this paper we presented a new SVM multi-level architecture presented design we assume a Gaussian kernel function is used to train the SVM as well as the general assumption of any machine learning problem that the training data are independent and identically distributed. The architecture is based on a new con-fi dence measure that is computed using the VC dimension. The classi the Wisconsin breast cancer dataset and the agitation detection dataset. The results showed that the architecture ' s accuracy was better than the traditional SVM for the three datasets. On average the architecture always outperformed the traditional SVM.
Although this improvement was 1.5% on average, this improve-ment is very signi fi cant as it occurred on datasets where the achieved accuracies with the traditional SVM were already in the 90s, hence there was not much room for improvement anymore.
As for the future work the authors intend to develop a theoretical framework to make the architecture described in this paper faster to implement. The idea behind this framework is to fi nd closed form equations to use for calculating the VC after adding the testing point to the training set. This would eliminate the time complexity needed by the numerical algorithm to fi nd the VC. Acknowledgments This research was funded by the American University of Beirut University Research Board, Dar Al-Handassah (Shair &amp; Partners) Research Fund, the Rathman (Kadifa) Fund, and the National Council for Scienti fi c Research (NCSR).
 References
