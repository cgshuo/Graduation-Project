 Over the years, people have often held the hypothesis that negative feedback should be very useful for largely improving the performance of information filtering systems; however, we have not obtained very effective models to support this hypothesis. This paper, proposes an effective model that use negative relevance feedback based on a pattern mining approach to improve extracted features. This study focuses on two main issues of using negative relevance feedback: the selection of constructive negative examples to reduce the space of negative examples; and the revision of existing fea-tures based on the selected negative examples. The former selects some offender documents, where offender documents are negative documents that are most likely to be classi-fied in the positive group. The later groups the extracted features into three groups: the positive specific category, general category and negative specific category to easily up-date the weight. An iterative algorithm is also proposed to implement this approach on RCV1 data collections, and substantial experiments show that the proposed approach achieves encouraging performance.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms Information Filtering, Text Mining, Negative feedback, Pat-tern Mining
A phrase (or pattern) based approach can be used to overcome the limitations of the term-based approaches and gories: positive specific terms, general terms, and negative specific terms.

The remainder of this paper is organized as follows. Sec-tion 2 reviews the concepts of pattern taxonomy mining. Section 3 describes the proposed method of using negative feedback. Empirical results and discussion are reported in section 4, and the last section contains the concluding re-marks.
We use PTM as the basic model in this study and improve it in order to use negative relevance feedback to significantly improve the performance of IF systems. For PTM, we as-sumed that each document d is split into a set of paragraphs PS ( d ). Let D be a training set of documents, which con-sists of a set of positive documents, D + and a set of negative documents, D  X  . Let T = { t 1 , t 2 , . . . , t m } be a set of terms (or keywords) that are extracted from the set of positive documents, D + .
 ally say s 1 is a sub-pattern of s 2 , and s 2 is a super-pattern of s 1 . In the following, we simply say patterns for sequential patterns.

Given a pattern (an ordered termset ) X in document d , p X q is still used to denote the covering set of X , which includes all paragraphs ps  X  PS ( d ) such that X v ps , i.e., p X q = { ps | ps  X  PS ( d ) , X v ps } . Its absolute support is the number of occurrences of X in PS ( d ), that is sup a ( X ) = | p X q | . Its relative support is the fraction of the paragraphs
A sequential pattern X is called frequent pattern if its absolute support  X  min sup , a minimum support. The property of closed patterns can be used to define closed sequential patterns. A frequent sequential pattern X is called closed if not  X  any super-pattern X 1 of X such that sup a ( X 1 ) = sup a ( X ). Patterns can be structured into a taxonomy by using the is-a (or subset ) relation and closed patterns.

The evaluation of term supports (weights) is different to the term-based approaches. In the term based approaches, the evaluation of a given term X  X  weight is based on its ap-pearance in documents. In pattern mining, terms are weighted according to their appearance in discovered patterns [11].
To improve the effectiveness of the pattern taxonomy min-ing, an algorithm, SPMining ( PS ( d ) , min sup ), was pro-posed in [12] to find all closed sequential patterns, which used the well-known Apriori property in order to reduce the searching space. For every positive document d , the SPMin-ing algorithm discovered a set of closed sequential patterns based the min sup .

Let SP 1 , SP 2 , ..., SP | D + | are the sets of discovered closed sequential patterns for all documents in D + . For a given term, its support in discovered patterns from D + can be described as follows:
Extracting patterns first then deploying them on the term NFMining ( D ) Input: A training set, { D + , D  X  } , parameter  X  =  X  1; Output: Updated term set T and function weight .
 Method: 1: GT =  X  , T + =  X  , T  X  =  X  , loop = 0 ; 2: foreach t  X  T do 3: weight ( t ) = support ( t, D + ); 4: foreach d  X  D  X  do 7: D  X  3 = { d i | d i  X  D  X  , j  X  i &lt; d | D + | 3 e + j } ; 8: DP  X  = SPMining ( D  X  3 , min sup ); //find negative patterns 9: T 0 = { t  X  p | p  X  DP  X  } ; // all terms in negative patterns 10: foreach t  X  ( T 0  X  T ) do 11: if ( loop = 0 ) then weight ( t ) =  X   X  support ( t, D  X  3 ) 12: T  X  = T  X   X  ( T 0  X  T ), loop + + ; 13: if loop &lt; 3 then goto step 4; 14: foreach t  X  T do //term partition 15: if ( t  X  T  X  ) then GT = GT  X  X  t } 16: foreach t  X  T + do 18: T = T  X  T  X  ; set. The offender X  X  document is selected based on the ex-tracted features from the positive documents. Features in-cluding both terms and patterns, will be extracts from the selected negative documents using the same pattern mining technique used for feature extraction in the positive docu-ments. In addition, this process revises the initial features and obtains revised features. The process can be repeated for several times as follows: selecting negative documents, extracting negative features and revising revised features.
Algorithm NFMining ( D ) describes the details of the strate-gies of the revision, where we assume that the number of negative documents is greater than the number of positive documents. For a given training set D = { D + , D  X  } , we as-sume that the initial features, &lt; T, DP + , DP  X  &gt; , have been extracted from positive documents D + before we start the algorithm, where we let DP  X  =  X  . We also let the experi-mental parameter  X  =  X  1 that will be used for calculating weights of terms in negative patterns.
Four baseline models are used: the classic Rocchio model, a BM25 based IF model, a SVM based model, and PTM model. In this paper, our new model is called Negative Model (N-PTM).

The Rocchio algorithm has been widely adopted in the areas of text categorization and information filtering. It can be used to build the profile for representing the concept of a topic which consists of a set of relevant (positive) and irrelevant (negative) documents. The empirical parameters  X  = 1.0 and  X  = 1.0 shows the best result in RCV1 data collection.
 Table 2: Results of assessor topics where % chg is the percentage change over the best term-based model.
Generally, negative is a term that can be defined as any-thing except something positive. It is obvious that not all negative feedback is suitable to be selected as an offender, where offenders are the most useful negative documents that can help balance the weight of general terms because they are closer to the user X  X  interested subtopic. Figure 3 shows the difference between using all negative documents and using offenders for all the assessor topics. The proposed method for offender selection is shown to meet the design objectives.

To review the weight of extracted features, the proposed method classifies extracted terms into general terms and spe-cific terms, which is a distinct advantage compared with other methods. Specific terms are generally considered to be more interesting than general terms for a given topic. However, general terms are still important because they fre-quently appear in positive documents. The problem for gen-eral terms is that they may also frequently appear in some negative documents, probably because negative documents describe some extent to what users need. Before revision in the top 10 topic more than 72% weights are distributed to general terms, although the percentage of general terms is 31% for all extracted terms in the positive documents.
To reduce the side effects of using general terms in the ex-tracted features, the proposed method adds negative specific terms into the extracted features. However, adding negative specific terms to balance the negative documents will also affect the positive document because they share the same general terms. For this problem, the proposed method only increases the weights of positive specific terms when it con-ducts the revision using negative documents. After revi-sion, the percentage of general terms weight drooped into 59%, because about 175 negative specific terms added in each topic. Figure 3 shows the proposed model in differ-ent stages. Compared with the best state-of-the-art models, the proposed approach achieves excellent performance with 11 . 08% average change for all four measures.
Negative feedback contains information that helps to im-prove feature selection and balance the extracted term weights. However, one of the common problems for negative feedback is that negative has no clear defined boundary. As a result, it is important to carefully select offender documents in order to reduce the space of negative documents. In this paper, we proposed a new approach to use both positive and neg-ative feedback to improve PTM effectiveness. The results compared with several baseline models, including Rocchio, SVM, and BM25. The experimental results on RCV1 col-lections and TREC topics shows that the proposed method achieves exciting performance with 11 . 08% average percent-age change for all four measures. This research would be a significant contribution to information filtering for using negative relevance feedback.

