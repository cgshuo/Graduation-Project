 Current web search engines perform well for navigational queries which are used to acquire particular web pages that a user has in mind. However, due to their use of simple conjunctive Boolean filters [4], such engines perform poorly for informational queries, which are used to acquire information about a certain topic where the information may be on one or more web pages. To better support informational queries, such engines should incorporate an information retrieval (IR) model [8], such as the term frequency-inverse document frequency (TF-IDF) or BM25, along with a combination of enhancement techniques [8], such as query expansion and relevan ce feedback. Moreover, highly optimized techniques are needed for efficiently evaluating ex panded queries because the sizes of the expanded queries may often result in fifty or more.

There has been a large amount of work on optimization techniques including index compression and caching [12], result caching [7], and top-k query process-ing [1], [2], [3] [5], [6], [9], [10]. In this paper, we focus on top-k query processing techniques, which are used to find the correct top-k documents without pro-cessing the entire posting list for each query term. This a pproach is especially efficient in the case of large-scale IR sys tems such as web search engines, where k is small and the posting lists can be overwhelmingly long. Much of the previ-ous work on this approach mainly dealt with relatively short queries, not longer queries. We have developed a novel extension of an existing top-k query process-ing technique that enables it to efficiently evaluate informational queries using an IR model even when the queries have been expanded. It uses a simple data structure called a  X  X erm-document binary matrix, X  which indicates which docu-ment contains which query term. To the best of our knowledge, there have been no reports on integrating such a data structure into top-k query processing to increase the efficiency. 1. We describe the integration of the term-document binary matrix into the best 2. We describe our experimental evaluation of the extended technique using The paper is organized as follows. Section 2 discusses related work on top-k query processing. Section 3 pres ents the model and algorithm that are used. Section 4 describes our approach. Section 5 explai ns the experimental evaluation done using the TREC GOV2 collection, describes the key results, and presents our conclusions. The key points are summa rized and future work is mentioned in Section 6. Top-k query processing is an efficient m ethod to retrieve top-k documents by combining the values from sorted posting lists for query terms without process-ing the entire lists. There has been considerable work on top-k query processing in the IR and database communities. The IR community has a long history of research on efficient evaluation of vector space queries. Earlier work includes [3] and [9]. Although Buckley and Lewit [3] and Persin et al. [9] dealt with longer queries, their techniques are intended for relatively small collections. There has been recent work, e.g., Anh and Moffat [1], aimed at efficient evaluation for top-k queries for large collections, but the techniques reported mainly focus on relatively short queries consisting of at most ten or so terms. In the database community, seminal work has been done by Fagin [5] who introduced a family of threshold algorithms for top-k query processing. He introduced the notion of instance optimality and showed that his family of threshold algorithms satisfy this notion. Inspired by his work, many researchers in both the IR and database communities extended Fagin X  X  threshold algorithms. Among them, those most relevant to our work are those using upper level information of sorted posting lists such as intersections of such lists [6], and those using lower level informa-tion of sorted posting lists, such as histograms of value distributions in such lists and/or co-occurrence statistics bet ween such lists [2]. For the former type, Kumar et al. [6] generalized Fagin X  X  threshold algorithms to the case in which pre-aggregated intersection lists of different sorted posting lists are available in addition to the original sorted posting lists. Although these ideas are relevant to our approach, which uses upper level information in a term-document binary matrix, they deal with conjunctive queries. In contrast, we focus on disjunctive queries, which are traditionally studied in the IR community. Schenkel et al. [10] developed an efficient top-k query processing technique using upper level information such as indexes for pairs of terms in each document. However, their work aims at the case in which term proximity, i.e., the distance between term occurrences in a document, is integrated into the IR model used. Therefore their work is orthogonal and complementary to our work. Among the techniques using lower level information of sorted posting lists, Bast et al. [2] proposed integrating into Fagin X  X  threshold algorithms a nove l access scheduling based on statistics on such lists, such as histograms of per-term score distributions in such lists and co-occurrence statistics between su ch lists. While they did not report the instance optimality of the resulting algorithms, they demonstrated significant performance improvements in evaluating shorter disjunctive queries. The ap-proach of Bast et al. [2] is also orthogonal and complementary to our approach. 3.1 Model We describe the underlying model that is used.

Queries. We assume that a query q contains m terms t 1 ,...,t m and that the where score is a given scoring function and w ( d, t i ) is the per-term score of d for aterm t i .

Indexes. We assume that a posting list L i is maintained for each term t i ,that each posting in each L i has a unique document ID and a p er-term score, i.e., each posting is of the form d, w ( d, t i ) , and that the postings in each L i are sorted in descending order by the per-term score, w ( d, t i ), of d for t i .AsinFagin[5],we consider two modes of access to the sorted posting lists. The first mode is sorted access in which the query processor obtai ns the per-term score of a document for t in L i by proceeding through L i sequentially from the top. The second mode is random access in which the query proce ssor obtains the per-term score of a document for t i in L i in one random access. In addition, we assume that we have a list for each term which contains only the IDs of documents containing that term. These document-ID-only lists ar e used for creating the term-document binary matrices presented in the next section. 3.2 Combined Algorithm We extended CA, the best known top-k query processing technique and one of the threshold algorithms of Fagin [5]. C A combines sorted access with random access and is appropriate w hen random access is expen sive relative to sorted access as in the case of IR systems. The outline of CA is following: 1. CA begins by doing sorted access in par allel for each of the sorted posting 2. At each step of the execution of the algorithm, the query processor main-The characteristics of CA include early pruning and early termination, which are described next.

Early pruning. Let score LB ( d ) be the lower bound score of document d seen so far: and let score UB ( d ) be the upper bound score of document d seen so far: where a ( d )= { t i 1 ,...,t i l } X  q = { t 1 ,...,t m } contains already known query est) per-term score obtained thr ough sorted access in sorted list L i .Let mink be the lower bound score of the rank-k document in T k . The query processor safely prunes off d when the score UB ( d ) is no longer larger than mink .Thus,CAcan keep bookkeeping cost small. Early pruning is very important for the efficient execution of the algorithm.

Early termination. Let S be the set of documents seen so far left outside T k and U be the set of documents which have not been seen so far. Let score UB ( S ) be the maximum upper bound score of documents in S : And let score UB ( U ) be the maximum upper bound score of documents in U : The query processor halts, yielding the correct top-k documents, when both score UB ( S )and score UB ( U ) are no longer larger than mink .Thus,CAcan stop without processing the entire sorted posting list for each query term. Early termination is especially efficient in the case of large-scale IR systems such as web search engines where k is small and the posting lists can be overwhelmingly long. 4.1 The Key Idea As reported by Bast et al. [2], CA shows po or efficiency when used for retrieving top-k documents for longer queries for a number of reasons. First, the upper bound score, score UB ( d ), computed using Equation ( 2), becomes looser (larger) when the number of query terms is increased because of the increase in un-known query terms. Looser upper bound scores restrict early pruning, which sig-nificantly increases bookkeeping overhead. Second, the maximum upper bound scores, score UB ( S )and score UB ( U ), computed using Equation (3) and (4) re-spectively, also become looser (larger) when the number of query terms is in-creased. This restricts the possibility of early termination. Thus, CA is not effec-tive in this scenario. To overcome these shortcomings of CA for longer queries, we propose integrating a simple data structure B q , which depends on query q , into CA. The ( i, j )entryof B q is 1 if query term t i is contained in document d , and 0 if not. We call this matrix a  X  X erm-document binary matrix. X  We re-estimate the upper bound score, score UB ( d ), and the maximum upper bound scores, score UB ( S )and score UB ( U ), more tightly by using the term-document binary matrix. When d = d j , referring to the j-th column of the term-document binary matrix, Thus, score UB ( S ) is re-estimated using Equation (3) and (5). score UB ( U )is re-estimated using where w i is ith largest value of w 1 ,...,w m and m is the maximum co-occurrence of query terms in documents in U , i.e., m =max d j  X  U m i =1 B q ( i, j ). The m can be efficiently computed by calulating the co-occurrence statistics for all documents in a collection based on B q at the begining of the algorithm and updating the statistics each time an unknown document is obtained during the execution of the algorithm. CA using these tighter upper bounds leads to better early pruning and early termination. 4.2 Term-Document-Binary-Matrix-Based Combined Algorithm We call our version of the CA algorithm  X  X MCA, X  short for  X  X erm-document-binary-matrix-based Combined Algorithm. X  1. Create B q for q = { t 1 ,...,t m } from the document-ID-only lists of t 1 ,...,t m . 2. Choose sorted posting lists and perform sorted accesses: 3. Call document d viable if score UB ( d ) &gt;mink .Every  X  sorted accesses (that 4. Compute score UB ( S )and score UB ( U ). If the following conditions are satis-This algorithm consists of two phases. The first phase of this algorithm (that is, when score UB ( U ) &gt;mink , which means that some document in U might make it into the final top-k documents.), is to find all the documents which could qualify for the final top-k documents, and the second phase of this algorithm (that is, when score UB ( U )  X  mink , which means that no document in U could make it into the final top-k documents.), is to choose the final top-k documents from documents found in the first phase. The algorithm optimizes the execution of the sorted and random access, which is described next.

Optimizing Sorted Access. In the first phase, the algorithm decreases score UB ( U ) by performing as few sorted accesses enough to decrease score UB ( U ) as possible based on Equation (6), not performing sorted accesses for all the sorted posting lists as in CA. This redu ces sorted accesses. In the second phase, the algorithm identifies and ignores sorted posting lists based on B q ,fromeach of which any missing per-term nonzero s core for any query term in any not-yet-pruned document in T k or S can not be obtained. This avoids useless sorted accesses.

Optimizing Random Access. By Equation (5), which estimates score UB ( d ) more tightly than Equation (2) of CA, the algorithm chooses more viable docu-ments for random accesses than does CA. This leads to an increase in mink and thereby better early pruning and early termination. 5.1 Setup We used the TREC GOV2 collection. This collection contains about 25 million web documents crawled from the gov domain during early 2004. The uncom-pressed size of this collection is 426GB. To evaluate the performance for longer queries, we created expanded queries from the title fields of TREC topics 701 X 850 using the query expansion feature of the Indri search engine. 1 We used 32-and 64-term expanded queries. In the experiments in described here, no stopwords were removed. We implemented CA and BMCA on the Zettair search engine. 2 Because the Zettair search engine provides a standard technique for processing the entire posting list for each query ter m for the pivoted cosine model [11], we implemented this model both in CA and BMCA. For handling sorted access, both CA and BMCA were implemented such that they obtained a partition of 64KB in each sorted posting list per sorted access. This partition contained about 13,000 postings. For handling random access, we integrated the additional data structure which assigned each do cument to a document vector containing all terms in that document with nonzero p er-term scores along with their scores. Thus, both CA and BMCA were implemented such that they obtained a doc-ument vector for each random access. 3 For a term-document binary matrix, we created a matrix as an array of 32-or 64-bit integers for a 32-or 64-term ex-panded query from document-ID-only lists corresponding query terms. All runs were performed on a single core of a 2.50-GHz Intel(R) Xeon CPU. For BMCA, the runs were performed on condition that the term-document binary matrices were created beforehand and cached. 5.2 Results We compared the performance of our algorithm, BMCA, with that of a standard technique, in which the entire posting list for each query term is processed, and that of the original algorithm, CA. We measured the average execution time per query. Figure 1 shows the average execution times for 32-and 64-term queries for various values of k for the TREC GOV2 data set. BMCA significantly out-performed the standard technique and CA for every query size and every k. In particular, while the performance of both CA and BMCA deteriorated as k increased for both query sizes, the CA degration was much greater, and its per-formance actually became worse than that of the standard technique for query size = 64 and k = 100. BMCA showed high performance, yielding performance gains of up to a factor of 2.7 over CA. This is because our term-document-binary-matrix-based algorithm leads to better early pruning and early termination even for expanded queries. Figure 2 illustrates the effects of the better early pruning and early termination property of BMCA. It shows the number of documents retained at each iteration during the execution of both algorithms for a 32-term expanded query from the title field of TREC TOPIC 741 and k = 100. BMCA pruned more documents than CA by an order of magnitude and terminated after about three fifths of the number of iterations performed by CA. That is, BMCA performed about three fifths of sorted and random accesses performed by CA. We statistically analyzed this property of BMCA. Figure 3 shows the average total number of sorted and random accesses performed by CA and BMCA, and the average maximum number of documents retained by both algorithms, for 32-term queries for various values of k for TREC GOV2 data set. 4 The results shown in these figures indicate that, for longer queries, CA incurs substantial I/O overhead due to more sorted and ra ndom accesses along with considerable CPU overhead due to the exp losive increase of retained documents, resulting in performance degradation for longer queries. In contrast, BMCA incurs much less I/O overhead due to efficient early pruning and early termination based on the term-document binary matrices along with much less CPU overhead due to a reduced number of retained documents, even for longer queries. As a result, BMCA has significantly better performance than CA. Our integration of a simple data structure, the  X  X erm-document binary matrix, X  into the CA algorithm resulted in efficient evaluation of top-k disjunctive queries, even ones expanded using enhancement t echniques such as query expansion and relevance feedback. Experimental evaluation using the TREC GOV2 data set and expanded versions of the evaluation queries attached to this data set showed that the resulting algorithm performed significantly better than CA. In addition, we have studied the instance optimality of the resulting algorithm, but we will discuss on this issue at the next chance.

The evaluation was done under the assumtion that the term-document bi-nary matrices were created beforehan d and cached because the creation from the document-ID-only lists imposes relatively large overhead due to our farily simple implementation of the functionality. However, the term-document binary matrices should be created a t runtime in the case of large-scale IR systems such as web search engines because it is not re alistic for such systems to create be-forehand and cache the term-document binary matrices for a large number of queries. We plan to re-implement the functionality using state-of-the-art tech-niques, e.g., [12], which should greately reduce the overhead. We also plan to investigate the combination of our approach with that of Bast et al. [2] to achive a more sophisticated random access scheduling.

