 Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursinsap A dataset is considered to be imbalanced if one of target classes has a tiny number of majority class. The minority class includes a few positive instances, and the majority class includes a lot of negative instances. 
In many real-world domains, analysts encounter many class imbalanced problems, such as the detection of unknown and known network intrusions [8], and the detection of oil spills in satellite radar images [13]. In these domains, standard classifiers need to accurately predict a minority class, which is important and rare, but the usual clas-sifiers seldom predict this minority class. 
Strategies for dealing with the class imbalanced problem can be grouped into two costs to correctly classified instances or classifications errors [7], [9], [16]. illustrated in Table 1. The rows of the table are the actual class label of an instance, and the columns of the table are the predicte d class label of an instance. Typically, the class label of a minority class set as positive, and that of a majority class set as nega-Negative, respectively. From Table 1, the six performance measures on classification; (1)-(6). 
The objective of a classifier needs to aim for high prediction performance on a mi-ance of a classifier. lated works for handling the class imbalanced problem. Section 3 describes the details of our over-sampling technique, Safe-Level-SMOTE. Section 4 shows the experimen-tal results by comparing Safe-Level-SMOTE to SMOTE and Borderline-SMOTE. Section 5 summarizes the paper and points out our future works. balanced dataset until it is nearly balanced, before feeding it into any classifiers. The simplest re-sampling techniques are a random over-sampling technique [14] and a random under-sampling technique [14]. The former randomly duplicates positive approximately equally represented. However, the random over-sampling technique may cause the overfitting problem [19] because the technique may create the decision regions smaller and more specific. The random under-sampling technique encounters follows. Chawla, N., Bowyer, K., Hall, L., Kegelmeyer, W. (2002) designed the State of the Art over-sampling technique, namely SMOTE, Synthetic Minority Over-sampling TEchnique [4]. It over-samples a minority class by taking each positive instance and generating synthetic instances along a line segments joining their k nearest neighbours create larger and less specific decision regions, rather than smaller and more specific regions. More general regions are now learned for positive instances, rather than those generalize better. However, SMOTE encounters the overgeneralization problem. It blindly generalizes the region of a minority class without considering a majority class. class, thus resulting in a greater chance of class mixture. Han, H., Wang, W., Mao, B. (2005) designed the improvement of SMOTE, namely SMOTE uses the same over-sampling technique as SMOTE but it over-samples only the class like SMOTE does. Unfortunately, considering two positive instances those n instances are not obviously difference but they are divided into the different regions; noise and borderline. The first instance is declined but the second instance is selected for over-sampling. Based on SMOTE, Safe-Level-SMOTE, Safe-Level-Synthetic Minority Over-level so all synthetic instances are generated only in safe regions. generate synthetic instances. safe level ( sl ) = the number of a positive stances in k nearest neighbours . (7) showed in the lines 12 to 28 of Fig. 1. 
The first case showed in the lines 12 to 14 of Fig. 1. The safe level ratio is equal to  X  want to emphasize the important of noise regions. 
The second case showed in the lines 17 to 19 of Fig. 1. The safe level ratio is equal to curs, a synthetic instance will be generated far from noise instance n by duplicating p because the algorithm want to avoid the noise instance n . 
The third case showed in the lines 20 to 22 of Fig. 1. The safe level ratio is equal to synthetic instance will be generated in the range [0, 1 / safe level ratio ]. instance will be generated in the range [1 -safe level ratio , 1]. and then s will be added to D' . 
After the algorithm terminates, it returns a set of all synthetic instances D' . The al-instances in D , and t is the number of instances that satisfy the first case. Algorithm: Safe-Level-SMOTE Input: a set of all original positive instances D 
Output: a set of all synthetic positive instances D' 1. D' =  X  2. for each positive instance p in D { 3. compute k nearest neighbours for p in D and randomly select one from the k nearest neighbours, call it n 4. sl p = the number of positive stances in k nearest neighbours for p in D 5. sl n = the number of positive stances in k nearest neighbours for n in D 6. if (sl n  X  0) { ; sl is safe level. 7. sl_ratio = sl p / sl n ; sl_ratio is safe level ratio. 8. } 9. else { 10. sl_ratio =  X  11. } 12. if (sl_ratio =  X  AND sl p = 0) { ; the 1 st case 13. does not generate positive synthetic instance 14. } 15. else { 16. for (atti = 1 to numattrs) { ; numattrs is the number of attributes. 17. if (sl_ratio =  X  AND sl p  X  0) { ; the 2 nd case 18. gap = 0 19. } 20. else if (sl_ratio = 1) { ; the 3 rd case 21. random a number between 0 and 1, call it gap 22. } 23. else if (sl_ratio &gt; 1) { ; the 4 th case 24. random a number between 0 and 1/sl_ratio, call it gap 25. } 26. else if (sl_ratio &lt; 1) { ; the 5 th case 27. random a number between 1-sl_ratio and 1, call it gap 28. } 29. dif = n[atti] -p[atti] 30. s[atti] = p[atti] + gap X dif 31. } 32. D' = D'  X  {s} 33. } 34. } 35. return D' and AUC, for evaluating the performance of three over-sampling techniques; Safe-Level-SMOTE, SMOTE, and Borderline-SMOTE. The value of  X  in F-value is set to trees C4.5 [18], Na X ve Bayes [12], and support vector machines (SVMs) [6], are ap-Repository of Machine Learning Databases [1]; Satimage and Haberman, illustrated in Table 3. The first to last column of the table represents the dataset name, the num-ber of instances, the number of attributes, the number of positive instances, the num-ber of negative instances, and the percent of a minority class, respectively. 
The experimental results on the two datasets are illustrated in Fig. 2. The x-axis in these figures represents the over-sampling percent on a minority class. The y-axis in BORD, and SAFE are the label of the original dataset, SMOTE, Borderline-SMOTE, and Safe-Level-SMOTE, respectively. Fig. 2 (c). It is apparent that F-value is improved when over-sampling percent on the minority class is increased. Moreover, Safe-Level-SMOTE achieved higher F-value than SMOTE and Borderline-SMOTE. The results on recall using Na X ve Bayes are performance on recall , while Safe-Level-SMOTE comes second. For Haberman dataset, the minority class is about one quarter of the whole dataset. performance of Safe-Level-SMOTE is the best performance on precision . The results SMOTE and SMOTE show similar performance on AUC. In addition, Borderline-SMOTE shows poor performance on higher percent. 
For all experimental results, Safe-Level-SMOTE obviously achieve higher per-formance on precision and F-value than SMOTE and Borderline-SMOTE when deci-improvement in all over-sampling techniques. Theses causes by the convex regions of SVMs are indistinguishable. The class imbalanced problem has got more attentions among data miners. There are many techniques for handling such problem. However, traditional data mining tech-SMOTE to handle this class imbalanced problem. 
The experiments show that the performance of Safe-Level-SMOTE evaluated by Level-SMOTE carefully over-samples a dataset. Each synthetic instance is generated and Borderline-SMOTE may generate synthetic instances in unsuitable locations, stances generated in safe positions can improve prediction performance of classifiers on the minority class. 
Although the experimental results have provided evidence that Safe-Level-SMOTE can be successful classified numeric datasets in the class imbalanced problem, there are to assign safe level would be valuable. Second, additional methods to classify datasets which have nominal attributes are useful. Third, automatic determination of the amount of synthetic instances generated by Safe-Level-SMOTE should be addressed. 
