 Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this pa-per, we propose an evolutionary multi-branch tree clustering method for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likeli-hood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate his-torical tree information and is more e ffi cient and e ff traditional evolutionary hierarchical clustering algorithm. I.2.6 [ Learning ]: Knowledge acquisition; G.3 [ Probability and Statistics ]: [Time series analysis] Time Series Data, Multi-Branch Tree, Topic Evolution, Visualiza-tion, Clustering
With an increasingly large number of textual documents (e.g., news, blogs) published on the Web every day, there is an increasing need to better understand the topics in a text stream. In many applications, topics are naturally organized in a hierarchy and the hierarchy often evolves over time [9]. Consequently, there have been some initial e ff orts to model such evolving hierarchies. The state-of-the-art approach, evolutionary hierarchical clustering [9], aims to generate evolving binary trees to organize the topics at di times. However, they may fail to provide interpretable topic results since most of the topic trees in real world applications are not binary [8]. It is therefore important to e ff ectively learn an evolving multi-branch tree representation, providing users a coherent view of content transitions. S. Liu is the correspondence author.

In this paper, we define and study the problem of mining evolv-ing multi-branch topic trees inside a text stream, as well as their evolution patterns over time. Specifically, we take a news dataset as an example to illustrate the basic idea. Fig. 1(a) shows part of the evolving topics and their hierarchical structures extracted from this dataset. The three labeled topics are  X  X box X (A),  X  X indows X (B), and  X  X ales and earnings X (C) from Jan. 8 to Mar. 11, 2012. We align the correlated topics across di ff erent trees according to their content similarity. From the alignment edges, we can see the three topics are quite stable during this time period, with a few splitting relationships between them over time. With such evolutionary trees and their visual representation in Fig. 1(a), users can easily ex-amine: 1) the evolution of multi-branch trees and their content alignment over time; 2) the topics of interest and their evolving patterns (e.g., splitting / merging) at di ff erent levels of these trees.
To better understand the evolving patterns of user-selected topics, we leverage a dynamic topic visualization technique, TextFlow [10], to illustrate the topic merging / splitting patterns. In this visualiza-tion, a river flow metaphor is adopted to illustrate topic evolution over time (Fig. 1(b)). Each colored layer represents a topic. The varying layer height along the horizontal axis represents the number of documents for the topic at each time point. Like a river flow in the real world, the topic flow can either be split into several branches when the corresponding topic splits, or combined with several other branches into one layer when the corresponding topics merge together. Fig. 1(b) shows the splitting / merging patterns of topics  X  X box, X   X  X indows, X  and  X  X ales and earnings X  from Jan. 8 to 28. Topics  X  X indows X  and  X  X ales and earnings X  merge in the week of Jan. 15 when Microsoft reported its quarterly revenue. To discover more information about the merging, we browse the related news. Some news items report on the  X  X ales and earnings X  of  X  X indows. X  For example, one of the articles has the title  X  X indows sales slowdown as Microsoft reports Q2 revenue up 5%. X  These two topics split in the next week as the association becomes weaker. Another interesting pattern is that part of the  X  X indows X  topic splits itself from the main topic in the first week and then joins  X  X box X  in the next week. The major reason is that Dave Culter, the father of Windows NT, shifted his focus to Xbox and was working  X  X o extend xbox beyond its status as a gaming platform. X 
Motivated by this example, we aim to generate a sequence of coherent multi-branch topic trees. Each tree in the sequence should be similar to the one at the previous time point (smoothness). It also needs to well describe the document distribution at that time point (fitness). However, it is quite challenging to achieve the desired results. First, it is not trivial to generate evolving multi-branch tree representations as well as to model their evolution patterns over time. Although the state-of-the-art multi-branch clustering methods [8, 16] can generate a topic tree with a high fitness value, they cannot guarantee the smoothness between topic trees. One way to solve this problem is to minimize the tree distance di ff erence between two correlated node pairs at two consecutive time points. This method can improve the smoothness between trees to some extent. However, it may fail to reconstruct an optimized tree for the current time point since the parent-child relationships are lost. Second, online docu-ments (e.g., news articles) arrive regularly and thus they are usually large in number. Since the complexity of tree-based algorithms are non-linear to the data number, it is therefore very time-consuming to generate a sequence of topic trees that well balances the fitness of each tree and the smoothness between adjacent trees.

To tackle the above challenges, we propose an algorithm, an evo-lutionary Bayesian rose tree ( EvoBRT ), to automatically learn tree evolutionary patterns. In our work, a Bayesian rose tree ( BRT ) [8] is adopted to handle the multi-branch tree construction problem, as well as to maintain a high fitness for human understanding. To preserve the smoothness between adjacent trees, we formulate the evolutionary clustering problem as a Bayesian online filtering al-gorithm inspired by the method in [5]. A tree prior is introduced in the BRT learning framework, which is formulated as a Markov random field ( MRF ). The key here is to define the energy function of the MRF model to measure the smoothness cost. A previous study [19] has shown that a tree can be uniquely defined by a set of triples and fans. A triple is a sub-tree with three leaf nodes and two internal nodes (Fig. 3(a)), while a fan is a sub-tree with three leaf nodes and one internal node (Fig. 3(b)). We argue that, in order to create a sequence of coherent topic trees, we need to keep as many sub-trees as possible when generating a new tree. To this end, we define the smoothness cost as proportional to the number of violated triples / fans between the adjacent trees. Our experiments show that the triple-and fan-based measures work better than the evolution-ary hierarchical clustering algorithm [9] in preserving smoothness between trees.

The computational complexity of our algorithm is mainly deter-mined by two parts: the construction of a Bayesian rose tree and the parametrization of the tree prior given a pre-defined tree. We leverage our previous work [16] to generate the Bayesian rose tree, which reduces the complexity from O ( n 2 log ( n )) to O ( n log ( n )). The complexity of the tree prior parametrization is mainly caused by calculating the number of violated triples / fans. Directly computing the number usually takes  X  ( n 3 ) time, which is very time consuming. To solve this problem, we build a constraint tree from all the triples and fans. By leveraging this tree index, we reduce the calculation time to O ( n log( n )).
In the area of data mining, researchers have developed various approaches to perform constrained hierarchical clustering. Based on the constraint type, they can be classified into two categories: pair-wise approaches and triplewise approaches. Pairwise approaches incorporate the constraints in the form of must-links and cannot-links, which indicate that two samples must or cannot be in the same cluster [11, 18]. Since must-links and cannot-links do not consider hierarchical information, these methods may fail to characterize the hierarchical document distribution.
Triplewise approaches incorporate triple constraints (e.g., two samples must be combined before the other sample is combined with either of them) among the data to generate clusters. Existing methods consider two di ff erent ways to use triple constraints, metric-based approaches and instance-based approaches [4]. Metric-based approaches learn a distance or similarity metric from the constraints and then embed the metric in the clustering process [4, 13, 21, 30]. Instance-based approaches follow all triplewise constraints in the bottom-up merging process and will fail to generate a hierarchy if one of them is violated [4, 15, 29].

In contrast to the above algorithms, which are designed for build-ing a static binary tree, our algorithm aims to generate evolving multi-branch hierarchies. A multi-branch tree contains both triples and fans, so the existing approaches do not work since they cannot handle fans. In addition, to model evolving patterns, our algorithm automatically generates constraints for the tree at time t based on the tree structure at t  X  1, while the constraints in constrained hierarchical clustering are predefined.
Recent e ff orts in topic analysis have focused on developing ad-vanced machine learning algorithms to extract evolving topics, such as dynamic latent Dirichlet allocation and its variations [1, 6], and hierarchical Dirichlet processes [2, 3, 25, 26, 28]. The evolving topics may be correlated with others by various relationships over time. The most intuitive relationships are topic correlation [23] and common topics [24]. Recently, TextFlow was developed to help users analyze topic evolution patterns, including topic birth, death, splitting, and merging, in text data [10, 12]. However, none of the above methods focus on mining evolving trees.

A previous study has shown that it is very useful for information understanding and consumption if users are provided with hierar-chical topic information over time [27]. However, how to e and e ff ectively mine the hierarchical topics as well as their evolving patterns has not been solved yet. The most related method to ours is the evolutionary hierarchical clustering algorithm [9]. It measures the di ff erence between trees by the average distance between all node pairs. However the tree distance metric is not su ffi reconstruct a tree and measure the smoothness between trees since the parent-child relationships are lost. In contrast to evolutionary hierarchical clustering, we introduce two constraints, triples and fans, into the model to guarantee high smoothness between topic trees. In addition, we also choose the Bayesian rose tree [8] as our base representation to discover multi-branch structures in text data instead of binary tree structures.
To perform evolutionary hierarchical clustering, we adopt the static multi-branch tree as the base representation. In this section, we briefly introduce its definition and construction method.
Given a set of text documents D = { x 1 , x 2 ,..., x n } , where each document is represented as a feature vector x  X  X  |V| and |V| is the vocabulary size of the corpus, a multi-branch tree either consists of a single leaf x  X  D , or consists of a set of sub-trees T whose parent is the root node T . Each sub-tree T i is also defined in the same way. Here n T can be larger than two.

To infer the multi-branch tree structure, we follow the Bayesian rose tree ( BRT ) [8] approach, which greedily estimates the tree structure based on probability P ( D| T ). Initially, each document is regarded as an individual tree on its own: T i = { x i algorithm repeatedly selects two trees T i and T j and combines them into a new tree T m by a join , absorb , or collapse operation (Fig. 2), aiming to maximize the ratio of probability: where p ( D m | T m ) is the likelihood of D m given the tree T D  X  X  j represents all the data under T m . p ( D m | T m ) is defined as: where f ( D m ) is the marginal probability of D m , and children ( T is the child set of T m .  X  T m is the prior probability that all the data in T m is kept in one cluster. Specifically, it is defined as: where n T m is the child number of T m , and 0  X   X   X  1 is the hyper-parameter to control the partition granularity.

To represent the marginal distribution f ( D ), we use the DCM distribution [16, 17] parameter that controls the Dirichlet distribution, which is the prior of the multinomial distribution of each cluster.
In this section, we first introduce the overall procedure of evo-lutionary tree construction. Then we illustrate the formulation of constraint modeling and related operations.
In our model, we assume text data comes in a sequential and continuous way. At each time t , we have a set of documents, which ments coming at that time point. We assume there is an underlying tree T t that organizes the documents at t . To capture the tree struc-ture changes behind the data, we formulate the learning procedure as a Bayesian online filtering process: With this formulation, the new tree T t depends on both the likelihood of the current data p ( D t | T t ) and conditional prior p ( T t | T t measures the di ff erence between T t and T t  X  1 . Accordingly, our model considers both the fitness and historical smoothness costs as in the evolutionary hierarchical clustering algorithm [9]. For simplicity, here we only use one-step historical smoothness. It is also  X   X   X  (a) Triple: ab | c easy to incorporate more than one historical tree in the conditional
Similar to BRT , directly maximizing p ( D t | T t ) p ( T t | T t tractable, since there are a super-exponential number of candidate trees for T t . We then follow the greedy construction method of BRT [8] to select two sub-trees and one of the three types of combi-nations (join, absorb, and collapse) to construct a larger (sub-)tree. The selection aims to maximize the following posterior test ratio: where T t i and T t j are two candidate sub-trees that are considered for
By defining the energy function, which measures the smoothness we can formulate the conditional tree prior in a recursive way: where and  X  is the constraint weight that balances the importance of smoothness and tree likelihood. Inspired by a simpler Markov random field ( MRF ) defined on flat clusters in [5], we regard the conditional prior p ( T t | T t  X  1 ) as a Gibbs distribution based on a re-cursively defined MRF . The energy function of the MRF can be parameterized as a sum of a set of V T t  X  1 ( { T t i , T t parametrization, we rewrite Eq. (6) as: Since the first term corresponds to the BRT algorithm (Eq. (1)), the key is then to calculate the second term, the smoothness cost V
The major goal of the smoothness cost is to preserve as many common tree structures as possible. To this end, we first introduce the triple-and fan-based constraints and use them to measure the cost. Then a constraint tree is built for e ffi cient computation. Finally, we illustrate how to gradually modify the constraint tree for better measuring the cost.
We first introduce some preliminary definitions that are useful for subsequent discussions.

D efinition 1. A triple (Fig. 3(a)) is a sub-tree with three leaf nodes and two internal nodes. We denote it as ab | c where a and b are the two closest leaf nodes and c is the third leaf node.
D efinition 2. A fan (Fig. 3(b)) is a sub-tree with three leaf nodes and one internal node. We denote it as ( abc ) where a , b , and c are the three leaf nodes.

Binary trees only contain triples, while multi-branch trees contain both triples and fans. The example in Fig. 3(c) illustrates the relation-ship between a multi-branch tree and its corresponding triples This tree contains nine triples and one fan.

The following lemma illustrates the relationship between a multi-branch tree and its related triples / fans, which was proposed by Ng et al. [19].

L emma 1. A multi-branch tree T can be uniquely defined by a set of triples and fans.
 This lemma indicates that triples and fans contain all the hierarchical information of a multi-branch tree. Since a tree can be uniquely re-constructed by a set of triples and fans, we measure the smoothness cost by the violated triples / fans between adjacent trees. A tree with n leaves contains C 3 n triples and / or fans. Thus, directly computing the violation number usually takes O ( n 3 ) memory and  X  which is very time-and memory-consuming. To solve this problem, we build a tree to organize all the related triples and fans. We call it a constraint tree .

D efinition 3. A constraint tree  X  T t hierarchically organizes the triples / fans inferred from D t . It is initialized based on the previ-ous tree T t  X  1 , and modified as the corresponding triples violated.
 In the next three sub-sections, we illustrate the constraint tree ini-tialization and modification.
The basic idea of initializing the constraint tree  X  T t is to map ment that does not belong to any topic in T t  X  1 , a new topic is then generated at t . In our implementation, we propose two alternative is based on the cosine similarity between documents:
An alternative measure, the prediction measure, is based on the
We adopt a greedy method to map the document x t i to the sub-trees, which searches T t  X  1 in a top-down manner. We compare the similarity value of a parent with those of its children by leveraging one of the above two similarity measures. If the similarity value of the parent is larger than any of its children, as well as a pre-defined threshold s 0 , we stop the search process and set the parent as the most relevant topic; otherwise we treat the child with the highest relevance value as the new parent and repeat the process. tree. With this initial constraint tree, one way to compute the smooth-ness cost is as follows: When building a new tree T t , we compute how many triples and fans are violated when combining two candi-date sub-trees T k i and T k j by leveraging the constraint tree. Although this method is intuitive, it has one major problem. This is due to the fact that we ignore conflicting constraints. For example, given a constraint states will be introduced. The first type is the violated constraints. In this example, three triples / fans are violated. Here we take the triple ab | d as an example. This triple indicates that a and b should be combined first and then they are combined with d . However, if a and d are combined first, this triple is violated. The second type is the conflicting constraints, which are the triples that cannot co-exist in the constraint tree. Considering fan ( abc ), if a and d are combined, ( abc ) conflicts with the triple bc | d since they cannot both be in this tree.

Besides the violated constraints, the conflicting constraints also influence the smoothness cost. However, it is not easy to compute the cost caused by conflicting constraints since the relationships between them are complicated. For example, one constraint may conflict with multiple constraints. Moreover, besides the pairwise conflicts, there are triplewise and multiple conflicts. As shown in Fig. 4, if a and d are combined, then ( abc ), ac | e , and de | b conflict even though any two of them do not conflict with each other. As a result, it is hard to measure the corresponding cost even if we list all the conflicting constraints. To tackle this issue, we introduce two basic operations, merge and split , to the constraint tree.
To better measure the cost introduced by conflicting constraints, we define two basic operations, merge and split . Fig. 5 briefly illustrates the two operations.

D efinition 4. merge : a sub-tree  X  T k forwards the data of its own and its children to its parent  X  T l . Then  X  T k itself is removed from the constraint tree.

D efinition 5. split : some children of a sub-tree  X  T 0 parent to a newly generated child  X  T k of  X  T 0 l . Figure 5: merge and split operations on the constraint tree.
Next, we illustrate the major reason why these two operations are enough to remove the conflicts from a constraint tree and make it consistent. For simplicity, we take a two-level tree as an example to illustrate the basic idea. As shown in Fig. 2, BRT provides three types of combinations, a join , an absorb , and a collapse . If sub-trees T i and T j are combined with a BRT operation that is di from the one in the constraint tree, conflicting constraints will be introduced. The basic idea of avoiding conflicts is to update the con-straint tree to make it consistent with the current data organization. For example, assume sub-trees T i and T j are combined together by an absorb operation (Fig. 2A). If they appear in the constraint tree as shown in Fig. 6B, conflicting constraints are introduced. To solve them, we change the constraint tree into the one in Fig. 6C using a merge operation. More examples of using the merge operation(s) to modify the constraint tree are shown in Fig. 6, which demonstrate that the changes between the constraint trees that cor-respond to the BRT operations can all be handled with the merge operation(s). Consequently, they are enough to maintain a consistent constraint tree.
With merge / split , we can then measure the cost from conflicting constraints by counting the violated constraints in the two opera-tions. The calculation method is given by the following theorems. We take the merge / split operation in Fig. 5 as an example in the following discussions.

T heorem 1. In a merge operation, only triples may be violated, and violated triples become fans. The number of violated triples is where |  X  D k | is the number of leaves in the tree  X  T
P roof . Given a (sub-)tree T , the number of fans is where F T is the set of fans in T , and |F T | is the number of fans in the set.

Since all the triples in  X  T 0 l are contained in  X  T l only violates triples and changes them to fans. Thus the cost of the merge operation is the di ff erence in fan number between Similarly, we can prove the following theorem on the split
T heorem 2. In a split operation, only fans may be violated, and the violated fans become triples. The number of violated fans is:
In Sec. 4.2.3, we assume the sub-trees to be combined are adja-cent to each other. In real applications, however, the two sub-trees may not be adjacent. In such a case, we first move them to the closest common ancestor by a sequence of merge operations. Once they are under the same ancestor, we perform the merge / split ation(s) to make the related constraint tree consistent. As a result, the smoothness cost introduced by the conflicts can be calculated by summing up the violation costs of all merge / split operations: where opt o is a merge operation or a split operation.
The complexity of EvoBRT is determined by three parts: con-struction of the Bayesian rose tree, constraint tree initialization, and computation of the violated constraints. The complexity results are summarized in Table 1.

Several methods have been proposed to implement BRT [16]. In this paper, we leverage them to build the multi-branch tree. We use EvoBRT to represent the implementation based on the original BRT [8], and K NN-EvoBRT and SpillTree-EvoBRT to represent the ones based on the two approximation algorithms, K NN-BRT and SpillTree-BRT [16]. For simplicity, we call these two algorithms the evolutionary approximation algorithms. The related complexity results are shown in Table 1. Interested readers are referred to [16] for a detailed analysis.
 To build the initial constraint tree  X  T t , we map each document in D t to a proper node in T t  X  1 . Since we search for the best node in a top-down manner, the complexity is O ( nC V log n ).

The complexity of calculating the violated constraints is mainly caused by two parts: counting the violated constraints and updating the posterior test ratios. The first part has O ( n 2 h ) time complexity in EvoBRT , and O ( nKh ) time complexity in the evolutionary approx-imation algorithms. Here h is the depth of tree  X  T t , and ber of nearest neighbors. When the constraint tree is modified, some posterior test ratio values will be changed. Since the posterior test ratios are stored in a sorted list, the complexity of the second part is therefore caused by updating this list. The complexity is O ( n for EvoBRT and O ( nKh log n ) for the evolutionary approximation algorithms. The log n factor is due to updating one value in the sorted list, while n 2 h and nKh are the number of maximum updated values for EvoBRT and the evolutionary approximation algorithms. Table 1: Time Complexity. C V is the number of non-zero ele-ments in the vector. The time for initializing  X  T t is O ( nC
To demonstrate the performance of our algorithm, we have con-ducted a series of experiments on several real datasets. In this section, we first introduce the baseline algorithm. Then the e tiveness of our constraint model is demonstrated by using the 20 newsgroups dataset 1 . Next, we illustrate how our algorithm can well preserve both fitness (likelihood) and smoothness. Finally, we show that our EvoBRT algorithm is as e ffi cient as the BRT algo-rithm and its variations in [16]. The results show that our algorithm outperforms the baseline in all aspects that we have compared. http: // qwone.com / jason / 20Newsgroups /
In our experiments, we exploited K NN-EvoBRT ( K = 50) due to its e ffi ciency and relative stable performance [16]. For each experiment, the rose tree parameters with the highest likelihood value were selected through a grid search.
We implemented a baseline algorithm based on the evolutionary hierarchical clustering algorithm [9]. Since this method focuses on binary hierarchies, we only compared its constraint model. More exactly, we incorporated its smoothness function into our framework. The smoothness is defined as where d T ( r , s ) is the tree distance between r and s , and  X  is the constraint weight that balances smoothness and tree likelihood.
Here we choose the squared heuristic [9] to maximize the smooth-ness since it can be easily adapted to the multi-branch scenario. With is given by
V The baseline was implemented by substituting V T t  X  1 ( { T t
The major goal of this experiment is to evaluate: 1) the e ness of the two similarity measures for constraint tree initialization and check which one is better; 2) the clustering quality of the tree built by our constraint model.
In this experiment, we used the 20 newsgroups dataset. This dataset has a ground-truth hierarchy with two levels of clusters (7 clusters at the first level and 20 clusters at the second level). We removed the clusters with only one child and got a hierarchy with 4 and 17 clusters at the first and second levels, respectively. In each trial, we randomly sampled 2,000 documents to form a ground-truth labeled tree. We treated it as the tree at t  X  1. We then sampled 2,000 documents again and mapped them to the ground-truth labeled tree, which is the initial constraint tree in our model. The second dataset of 2,000 documents can be sampled such that it has a specific percentage of overlap with the first dataset. In our experiment, we considered 5 overlapping ratios that vary from 0 to 1. For each given overlapping ratio, we sampled the two datasets 5 times and the results of each ratio were computed by averaging the results of these 5 trials. The Bayesian rose tree parameter  X  was set at 0.1, and  X  ( i ) = 0 . 01( i = 1 ,..., |V| ) (Eq. (4)).
We evaluated the tree clustering quality by two criteria: Normal-ized Mutual Information ( NMI ) and Cluster Number Error ( CNE ). NMI is the most commonly used metric to measure the clustering quality [22], but may fail in certain instances, especially when the cluster number di ff erence is large. For example, the ground-truth contains 50 clusters, each of which consists of 20 data samples. If the algorithm builds 1,000 clusters, each of which only contain 1 data sample, then the NMI value is 0.75. To solve this problem, we introduced CNE, which measures the cluster number di ff between the generated tree and the ground-truth tree at each level. The larger the CNE value, the worse the tree clustering quality. In our experiments, we evaluated the clustering quality by averaging (a) Overlapping ratio vs. NMI. Figure 7: Clustering quality of the constraint trees initialized by di ff erent measures with di ff erent overlapping ratios. the NMI / CNE values at di ff erent levels. A clustering result with a larger NMI value and smaller CNE value has better quality.
First, we evaluated which similarity measure was better for map-ping documents in constraint tree initialization. We used Cosine to represent the mapping method with the cosine similarity (Eq. (11)), and Prediction to represent the mapping method with the predic-tion measure (Eq. (12)). Fig. 7 shows how the quality of the initial constraint tree changed with di ff erent data overlapping ratios and dif-ferent measures. The Prediction measure outperformed the Cosine measure on both criteria. For example, even when the overlapping ratio was 0, the NMI value was 0.7. We speculate that this may be due to the fact that the probability-based prediction measure is more consistent with the tree building probability in our model.
Second, we compared our algorithm with the baseline method to demonstrate its e ff ectiveness in building high-quality tree clustering results. In each experiment, we also compared the results with the overlapping ratios 0, 0.5, and 1. Fig. 8 shows how the tree clustering quality changed with di ff erent constraint weight  X  (Eq. (9)) for both algorithms with di ff erent overlapping ratios. As illustrated by the results, our algorithm is much more e ff ective than the tree distance-based baseline algorithm. When the data overlapping ratio is 1, we can reconstruct the ground-truth labeled tree. Even if the overlap-ping ratio is 0, our algorithm still maintains a larger NMI (0.7) and smaller CNE (near 0), while the baseline has a smaller NMI (0.5) and much larger CNE (650). In our algorithm, both NMI and CNE become better as the constraint weight increases. In the baseline, the NMI becomes better with the increase of the constraint weight, while the CNE gets worse with the increase of the constraint weight.
In this experiment, we aimed to demonstrate that our algorithm well preserved both fitness and smoothness. In this experiment, we used New York Times news articles (from Jan 2006 to Jun 2007) 2 . This dataset contains 12,798 articles on art, style, travel, business, and sports. We grouped the data into nine segments, each of which contained 2 months of articles. We randomly sampled 1,000 documents from each time segment. To eliminate the randomness caused by sampling, we sampled the data 5 times and ran the experiment 5 times. The results were computed by averaging the results of the 5 trials. Bayesian rose tree parameters
In our experiment, we used likelihood to measure the fitness of a tree. We also introduced three metrics to measure the smoothness between adjacent trees.

Tree Distance Smoothness ( S Dist ): This metric is defined based on the smoothness cost of the baseline algorithm. It measures the tree structure di ff erence by aggregating the tree distance di between two correlated leaf pairs of the adjacent trees: S
Tree Order Smoothness ( S Order ): This metric is defined based on the smoothness cost of our algorithm. It is the negative value of the violated triples / fans compared with the previous tree: S log( p ( T t | T t  X  1 )), where p ( T t | T t  X  1 ) is defined in Eq. (8).
Robinson-Foulds Smoothness ( S RF ): This metric is based on the widely used Robinson-Foulds distance metric for phylogenetic trees [20].
 where T t  X  1 ( D t ) represents the constraint tree built on the given tree T t  X  1 and data D t . Then S RF is defined as the average distance of version of Robinson-Foulds distance given in [14].
Two 5-depth trees were built by the baseline and our algorithm, respectively. Fig.9 shows how the smoothness scores changed with likelihood. We did a grid search of eight constraint weights for both the baseline and our algorithm. The constraint weights for the baseline and our algorithm were { 1 e  X  25 , 1 e  X  20 ,..., { 3 e more emphasis was put on the smoothness factor. In each of the figures, we connected the points in the order of increasing constraint weights. Based on an analysis of the results, we draw the following conclusions.

First, our method can generate a much smoother structure than the baseline while maintaining a larger likelihood. Besides having very good performance under the metric of S Order , our algorithm also achieved a comparable performance under the metrics of S Dist S
RF . This demonstrates that triples and fans contain all the hierar-chical information of a multi-branch tree and thus the cost function based on them is very e ff ective at preserving both smoothness and fitness. The baseline algorithm achieved a reasonable performance under the metric of S Dist . However, it failed to get a good result under the metrics of S Order and S RF . This is due to the fact that tree distance constraints do not consider the hierarchical information (e.g., parent-child relationships) of a tree. http: // nytimes.com
Second, the smoothness of our algorithm increases consistently with the increase of the constraint weight, while the likelihood in-creases at first and then decreases. This indicates that incorporating a certain amount of historical information actually helps to increase fitness. This is because the highly reliable triples / fans are kept in the current tree and they can help the greedy algorithm find a better solu-tion. However the baseline does not exhibit a similar pattern. Even if the constraint weight increases, the smoothness between trees is not guaranteed. This is because the baseline does not well consider multi-branch structures and does not handle conflicting constraints.
Next, we found that our algorithm can well preserve both smooth-ness and likelihood at each time point. The result was the average values of the eight trails with di ff erent constraint weights. As shown in Fig. 10(b), Fig. 10(c), and Fig. 10(d), our algorithm outperformed the baseline on smoothness at almost every time point. Furthermore, the baseline also worked better at preserving smoothness than the method with no constraint (denoted as  X  X o constraint X  in Fig. 10), which is equivalent to performing BRT at each time point. As for the likelihood, our algorithm and the baseline were as good as the one with no constraint (Fig. 10(a)). This demonstrates again that our al-gorithm can preserve smoothness between trees without sacrificing the likelihood of each tree.
In this section, we first demonstrate that our algorithm is more e ffi cient than the baseline. Then we compare the running time of our algorithm with di ff erent constraint weights. Experimental results show that K NN-EvoBRT and SpillTree-EvoBRT is as e ffi cient as K NN-BRT and SpillTree-BRT . We randomly sampled 100,000 documents from the New York Times corpus, and evaluated the e ffi ciency of our model based on K NN-BRT and SpillTree-BRT . We classified the data into two groups. The first was used for constructing the constraint trees and the second was used for building a new tree based on the constraint tree. The vocabulary size used in this experiment was 665,261.
As shown in Fig. 11(a), our algorithm outperformed the baseline in terms of e ffi ciency (based on K NN-BRT ). Its performance was also comparable to K NN-BRT . Fig. 11(b) and Fig. 11(c) demon-strate the running time of K NN-EvoBRT and SpillTree-EvoBRT , respectively. Here, we also compare the implementations with dif-ferent constraint weights. The results demonstrate again that the performance of our algorithm is comparable to that of the two ap-proximation algorithms of BRT . In this example, when the constraint weight was small (i.e., 1e-10), our algorithm was similar to the one with no constraint in terms of e ffi ciency. When the constraint weight was large, our algorithm was even faster for the corpus with a larger number of documents. After checking the results with the faster running time, we found their constraint trees were quite balanced. A balanced constraint tree leads to a balanced tree structure. Typically, BRT and its variations can build a balanced tree much faster. Due Figure 11: E ffi ciency comparison: (a) comparison of di to the complexity of our algorithm, it can also handle larger scale datasets in a reasonable amount of time.
In this paper, we present an evolutionary multi-branch hierarchi-cal clustering algorithm, EvoBRT , to automatically learn dynamic tree structures over time. We leverage a Bayesian online filtering framework to formulate our evolutionary clustering problem. To build multi-branch trees, we adopt the state-of-the-art multi-branch tree clustering method, Bayesian rose trees. To preserve tree smooth-ness over time, we use the conditional prior over tree structures to keep the information from previous trees. Particularly, we introduce the concepts of triples and fans, which can uniquely represent a multi-branch tree. To compute the tree structure di ff erences e ciently, we define a constraint tree from triples and fans, as well as the corresponding operations to make it consistent over time. Our experiments show that our algorithm outperforms the traditional evolutionary hierarchical clustering algorithm both in tree clustering quality and construction e ffi ciency. The complexity analysis demon-strates that our algorithm can be applied to large-scale datasets. The authors would like to thank S. Lin for proofreading, as well as H. Wei and C. Blundell for their insightful discussions.
