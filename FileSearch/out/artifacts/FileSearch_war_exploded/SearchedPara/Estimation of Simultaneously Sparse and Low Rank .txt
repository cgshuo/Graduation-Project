 Emile Richard emile.richard@cmla.ens-cachan.fr CMLA UMR CNRS 8536, ENS Cachan &amp; 1000mercis, France Pierre-Andr  X e Savalle pierre-andre.savalle@ecp.fr Ecole Centrale Paris, France Nicolas Vayatis nicolas.vayatis@cmla.ens-cachan.fr CMLA UMR CNRS 8536, ENS Cachan, France Matrix estimation is at the center of many modern applications and theoretical advances in the field of high dimensional statistics. The key element which differentiates this problem from standard high dimen-sional vector estimation lies in the structural assump-tions which are formulated in this context. Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Sre-bro, 2004; Cai et al., 2008)). In this paper, we argue that being low-rank is not only an equivalent of spar-sity for matrices but that being low-rank and sparse can actually be seen as two orthogonal concepts. The underlying structure we have in mind is that of a block diagonal matrix. This situation occurs for in-stance in covariance matrix estimation in the case of groups of highly correlated variables or when denois-ing/clustering social graphs.
 Efficient procedures developed in the context of sparse model estimation mostly rely on the use of ` 1 -norm regularization (Tibshirani, 1996). Natural extensions include cases where subsets of related variables are known to be active simultaneously (Yuan &amp; Lin, 2006). These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien &amp; Tibshirani, 2010) and graph-ical model structure learning (Banerjee et al., 2007; Friedman et al., 2008). In the low-rank matrix comple-tion problem, the standard relaxation approach leads to the use of the trace norm as the main regular-izer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative prox-imal solutions (Combettes &amp; Pesquet, 2011; Beck &amp; Teboulle, 2009) (for general classes of losses). How-ever, solutions of low-rank estimation problems are in general not sparse at all, while denoising and variable selection on matrix-valued data are blind to the global structure of the matrix and process each variable in-dependently.
 In this paper, we study the benefits of using the sum of ` 1 and trace-norms as regularizer. This sum of penalties on the same object allows to benefit from the virtues of both of them, in the same way as the elastic-net (Zou &amp; Hastie, 2005) combines the sparsity-inducing property of the ` 1 norm with the smoothness of the quadratic regularizer. Trace norm and ` 1 penal-ties have already been combined in a different context. In Robust PCA (Candes et al., 2009) and related lit-erature, the signal S is assumed to have an additive decomposition S = X + Y where X is sparse and Y low-rank. Note that S is not in general sparse nor low-rank and that this decomposition is subject to identi-fiability issues, as analyzed, e.g., in (Chandrasekaran et al., 2011). The decomposition is recovered by using ` -norm regularization over X and trace norm regu-larization over Y . This technique has been success-fully applied to background substraction in image se-quences, to graph clustering (Jalali et al., 2011) and covariance estimation (Luo, 2011).
 Here, we consider the different situation where the ma-trix S is sparse and low-rank at the same time. We demonstrate the applicability of our mixed penalty on different problems. We develop proximal methods to solve these convex optimization problems and we provide numerical evidence as well as theoretical ar-guments which illustrate the trade-off which can be achieved with the suggested method.
 The remainder of the paper is organized as follows. In Section 2, we present the setup and motivations. Sec-tions 3 and 4 are devoted to theoretical results on the interplay between sparse and low-rank effects. Sec-tion 5 presents algorithms used for resolution of the optimization problem and Section 6 is devoted to nu-merical experiments. The last Section explores related topics. 2.1. Problem formulation and notations We first set some notations. For a matrix S = ( S i,j ) i,j we set the following matrix norms: k S k 1 = P i,j | S i,j values of S and rank( S ) is the rank of S . We consider the following setup. Let A  X  R n  X  n be a fixed matrix and ` a loss function over matrices. We introduce the following optimization problem: for some convex admissible set S  X  R n  X  n and nonneg-ative regularization parameters  X  ,  X  .
 In the sequel, the projection of a matrix Z onto S is denoted by P S ( Z ). The matrix ( M ) + is the compo-nentwise positive part of the matrix M, and sgn( M ) is the sign matrix associated to M with the conven-tion sgn(0) = 0. The component wise product of matrices is denoted by  X  . The class S + n of matri-ces is the convex cone of positive semidefinite matri-ces in R n  X  n . The sparsity index of M is || M || |{ M i,j 6 = 0 }| and the Frobenius norm of a matrix M is defined by k M k 2 F = P i,j M 2 i,j . In Section 3, we shall also use k M k op = sup x : k x k k M k  X  = max | M i,j | . 2.2. Main examples The underlying assumption in this work is that the unknown matrix to be recovered has a block-diagonal structure. We now describe the main modeling choices through the following motivating examples:  X  Covariance matrix estimation -the matrix A rep- X  Graph denoising -the matrix A is the adjacency  X  Link prediction -the matrix A is the adjacency The next result shows how matrix recovery is governed by the trade-off between the rank and the sparsity in-dex of the unknown target matrix, or by their convex surrogates: the trace norm and the ` 1 -norm. Proposition 1. Let S 0  X  R n  X  n and A = S 0 + with  X  R n  X  n having i.i.d. entries with zero mean. Assume for some  X   X  [0; 1] that  X   X  2  X  k k op and  X   X  2(1  X   X  ) k k  X  . Let and b S = arg min S  X  X  L ( S ) . Then k b S  X  S 0 k 2 F  X  inf and k b
S  X  S 0 k 2 F  X  min 2  X  k S 0 k  X  + 2  X  k S 0 k 1 , The techniques used in the proof (see the Appendix) are very similar to those introduced in (Koltchinskii et al., 2011). Note that the upper bound interpolates between the results known for trace-norm penalization and Lasso. In fact, for  X  = 0,  X  can be set to zero, and we get a sharp bound for Lasso, while the trace-norm regression bounds of (Koltchinskii et al., 2011) are obtained for  X  = 1. We dwell for a moment on the task of link prediction in order to illustrate how rank and sparsity constraints can help in this setting. Given a subset E of observed edges from a graph adjacency matrix A  X  { 0 , 1 } n  X  n , we set out to predict unobserved links by finding a sparse rank r predictor S  X  R n  X  n with small zero-one loss ` ( S,A ) = by minimizing the empirical zero-one loss The objective of a generalization bound is to relate ` ( S,A ) with ` E ( S,A ). In the case of the sole rank constraint, (Srebro, 2004) remarked that all low-rank matrices with the same sign pattern are equivalent in terms of loss and applied a standard argument for gen-eralization in classes of finite cardinality. In the work of Srebro, a beautiful argument is used to upper bound the number of distinct sign configurations for predic-tors of rank r leading to the following generalization performance: for  X  &gt; 0, A  X  X  0 , 1 } n  X  n and with probability 1  X   X  over choosing a subset E of entries in { 1 ,...,n } 2 uniformly among all subsets of | E | entries, we have for any matrix S of rank at most r and  X ( n,r ) = 8 en r 2 nr We consider the class of sparse rank r predictors M ( n,r,s ) = { UV T | U,V  X  R n  X  r , || U || 0 + || V || and let s splr ( n,r,s ) be the number of sign configura-tions for the set M ( n,r,s ). By upper bounding the number of sign configurations for a fixed sparsity pat-tern in ( U,V ) using an argument similar to (Srebro, 2004), a union bound gives Using the same notations as previously, we deduce from this result the following generalization bound: with probability 1  X   X  and for all S  X  X  ( n,r,s ), In general, bound (2) is tighter than (1) for sufficiently large values of n as shown in the next proposition. The two bounds coincide when s = 2 nr , that is, when ( U,V ) is dense and there is no sparsity constraint. Proposition 2. For r n = n X  with  X   X  ]0 , 1] and s n = n X  with  X   X  2  X  , which diverges when n goes to infinity.
 Proof. The result follows from the application of Stir-ling X  X  formula.
 By considering a predictor class of lower complexity than low-rank matrices, we can thus achieve better generalization performances. We now present how to solve the optimization problem with mixed penalties presented in Section 2. We con-sider a loss function ` ( S,A ) convex and differentiable in S , and assume that its gradient is Lipschitz with constant L and can be efficiently computed. This is, in particular, the case for the squared Frobenius norm previously mentioned and for other classical choices such as the hinge loss.
 5.1. Proximal operators We encode the presence of a constraint set S using the indicator function 1 S ( S ) that is zero when S  X  X  and +  X  otherwise, leading to  X  S = arg min This formulation involves a sum of a convex differen-tiable loss and of convex non differentiable regularizers which renders the problem non trivial. A string of al-gorithms have been developed for the case where the optimal solution is easy to compute when each regu-larizer is considered in isolation. Formally, this corre-sponds to cases where the proximal operator defined for a convex regularizer R : R n  X  n  X  R at a point Z by is easy to compute for each regularizer taken sepa-rately. See (Combettes &amp; Pesquet, 2011) for a broad overview of proximal methods.
 The proximal operator of the indicator function is sim-ply the projection onto S , which justifies the alter-nate denomination of generalized projection operator for prox R . The proximal operator for the trace norm is given by the shrinkage operation as follows (Beck &amp; Teboulle, 2009). If Z = U diag(  X  1 ,  X  X  X  , X  n ) V T is the singular value decomposition of Z , Similarly, the proximal operator for the ` 1 -norm is the soft thresholding operator 5.2. Generalized Forward-Backward splitting The family of Forward-Backward splitting methods are iterative algorithms applicable when there is only one non differentiable regularizer. These methods alter-nate a gradient step and and a proximal step, leading to updates of the form In particular, this corresponds to projected gradient descent when R is the indicator function of a convex set. On the other hand, Douglas-Rachford splitting tackles the case of q  X  2 terms but does not ben-efits from differentiability. A generalization of these two setups has been recently proposed in (Raguet et al., 2011) under the name of Generalized Forward-Backward, which we specialize to our problem in Al-gorithm 1. The proximal operators are applied in par-allel, and the resulting ( Z 1 ,Z 2 ,Z 3 ) is projected onto the constraint that Z 1 = Z 2 = Z 3 which is given by the mean. The auxiliary variable Z 3 can be simply dropped when S = R n  X  n . The algorithm converges under very mild conditions when the step size  X  is smaller than 2 L .
 Algorithm 1 Generalized Forward-Backward
Initialize S,Z 1 ,Z 2 ,Z 3 = A , q = 3 repeat until convergence return S 5.3. Incremental Proximal Descent Although Algorithm 1 performs well in practice, the O ( n 2 ) memory footprint with a large leading constant due to the parallel updates can be a drawback in some cases. As a consequence, we mention a matching se-rial algorithm (Algorithm 2) introduced in (Bertsekas, 2011) that has a flavor similar to multi-pass stochas-tic gradient descent. We present here a version where updates are performed according to a cyclic order, al-though random selection of the order of the updates is also possible.
 Algorithm 2 Incremental Proximal Descent
Initialize S = A repeat until convergence return S 5.4. PSD constraint For any positive semidefinite matrix, we have || Z ||  X  = Tr( Z ). The simple form of the trace norm allows to take into account the positive semidefinite con-straint at no additional cost, as the shrinkage oper-ation and the projection onto the convex cone of posi-tive semidefinite matrices can be combined into a sin-gle operation.
 Lemma 1. For  X   X  0 and S  X  R n  X  n , We present numerical experiments to highlight the benefits of our method. For efficiency reasons, we use the serial proximal descent algorithm (Algorithm 2). 6.1. Synthetic data Covariance matrix estimation. We draw N vectors x i  X  N (0 ,  X ) for a block diagonal covariance matrix  X   X  R n  X  n . We use r blocks of random sizes and of the form vv &gt; where the entries of v are drawn i.i.d. from the uniform distribution on [  X  1 , 1]. Finally, we add gaussian noise N (0 , X  2 ) on each entry. In our experi-ments r = 5 , N = 20 , n = 100 ,  X  = 0 . 6. We apply our method (SPLR), as well as trace norm regularization (LR) and ` 1 norm regularization (SP) to the empirical covariance matrix, and report average results over ten runs. Figure 1 shows the RMSE normalized by the norm of  X  for different values of  X  and  X  . Note that the effect of the mixed penalty is visible as the min-imum RMSE is reached inside the (  X , X  ) region. We perform, on the same data, separate cross-validations on (  X , X  ) for SPLR, on  X  for LR and on  X  for SP. We show in Figure 2 the supports recovered by each al-gorithm, the output matrix of LR being thresholded in absolute value. The support recovery demonstrates how our approach discovers the underlying patterns despite the noise and the small number of observa-tions.
 6.2. Real data sets Protein Interactions. We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are scored by strength in [0 , 2]. The data suggested that interactions between two proteins are governed by a small set of factors, such as surface ac-cessible amino acid side chains (Bock &amp; Gough, 2001), which motivates the estimation of a low-rank represen-tation. Representing the data as a weighted graph, we filter to retain only the 10% of all 4394 proteins that exhibit the most interactions as measured by weighted degree. We corrupt 10% of entries of the adjacency matrix selected uniformly at random by uniform noise in [0 , X  ]. Parameters are selected by cross-validation and algorithms are evaluated using mean RMSE be-tween estimated and original adjacency matrices over 25 runs. RMSE scores are shown in Table 1 and show the empirical superiority of our approach (SPLR). Social Networks. We have performed experiments with the Facebook100 data set analyzed by (Traud et al., 2011). The data set comprises all friendship relations between students affiliated to a specific university, for a selection of one hundred universities. We select a single university with 41554 users and filter as in the previous case to keep only the 10% users with highest degrees. In this case, entries are corrupted by impulse noise: a fixed fraction  X  of randomly chosen edges are flipped, thus introducing noisy friendship relations and masking some existing relations. The task is to dis-cover the noisy relations and recover masked relations. We compare our method to standard baselines in link prediction (Liben-Nowell &amp; Kleinberg, 2007). Near-est Neighbors (NN) relies on the number of common friends between each pair of users, which is given by A 2 when A is the noisy graph adjacency matrix. Katz X  X  coefficient connects a pair of nodes according to a score based on the number of paths connecting them, em-phasizing short paths. Results are reported in Table 2 using the area under the ROC curve (AUC). SPLR outperforms LR but also NN and Katz which do not directly seek a low-rank representation. Other loss functions. The methods presented in this paper can be seamlessly extended to non-square ma-trices, which can arise, for instance, from adjacency matrices of bipartite graphs. Our work also applies to a wide range of other losses. A useful example that links our work to the matrix completion framework is when linear measurements of the target matrix or graph are available, or can be predicted as in (Richard et al., 2010). In this case, the loss can be defined in the feature space. Due to the low-rank assump-tion, our method does not directly apply to the esti-mation of precision matrices often used for gaussian graphical model structure learning (Friedman et al., 2008), and the applications of conditional indepen-dence structures generated by low-rank and possibly sparse models is to be discussed. Note that the trace norm constraint is vacuous for some special classes of positive semi-definite matrices. For instance, it is not useful for estimating a correlation matrix as, in this case, the trace is always equal to the dimension. Matrix factorizations. A related and popular task is finding low-rank factorizations of matrices of the form UV T (see, e.g., (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U,V  X  R n  X  r loss functions of the form ` (( U,V ) ,A ) = || UV T  X  A || 2 F for some tar-get maximum rank r . This implicitly encodes the low-rank constraint which leads to efficient optimization schemes, and allows for interpretability as estimated ( U,V ) pairs can be considered as latent factors. Non-negative Matrix Factorization (NMF) (Lee et al., 1999) imposes non negativity constraints on the coefficients of U and V to enhance interpretability by allowing only for additive effects and tends to produce sparse factor matrices U,V , although this a rather indirect effect. There is no strong guarantee on the sparsity achieved by NMF nor is it easy to set the target spar-sity and different methods for sparse NMF have been proposed in (Hoyer, 2004; Kim &amp; Park, 2008). Sparse matrix factorizations have also been proposed with-out the positivity constraint. Most work along this line is motivated by extending the classical PCA and finding sparse directions that maximize the variance of the projection. Most methods give up orthogo-nality between the components and can thus be seen as sparse matrix factorization techniques. SPCA pro-posed in (Zou et al., 2004) penalizes the ` 1 norm of the principal components and can be reduced to solving independent elastic-nets. A different formulation us-ing SDP programming is introduced in (D X  X spremont et al., 2007) with good empirical results. In spite of good empirical performances, all these methods based on matrix factorization suffer from a significant draw-back. Although formulations are usually convex in U or V, they are not in general jointly convex and opti-mization procedures can get stuck in local minima. Regularization parameters. We showed how to empir-ically select using cross-validation the hyper parame-ters  X  and  X  for a specific application. From a theoret-ical point of view, Proposition 1 provides us with per-formance guarantees when the regularization parame-ters are large enough. We know from random matrix theory that the operator norm of a random gaussian matrix concentrates around gent constraint on  X  for  X   X  2  X  || || op to hold with high probability. Similarly, the  X  -norm k k  X  can be bounded by k k op or using the multivariate Tcheby-cheff inequality of (Olkin &amp; Pratt, 1958) which implies that the condition  X   X  2(1  X   X  ) k k  X  is satisfied with probability 1  X   X  when  X  =  X  (1  X   X  ) 2 n X   X  . In prac-tice,  X  should not exceed the order of magnitude of the entries of the matrix, as this leads to a trivial zero solution. Asymptotically, to keep the sparsity regu-larization parameter  X  of the order of magnitude of elements of the observation matrix A , the free param-eter  X  must be chosen so that 1  X   X  n  X  n 1 n . This gives the same asymptotic behavior in O ( bound on  X  as in matrix completion.
 Optimization. Other optimization techniques can be considered for future work. A trace norm constraint alone can be taken into account without projection or relaxation into a penalized form by casting the prob-lem as a SDP as proposed in (Jaggi, 2011). The spe-cial form of this SDP can be leveraged to use the ef-ficient resolution technique from (Hazan, 2008). This method applies to a differentiable objective whose cur-vature determines the performances. Extending these methods with projection onto the ` 1 ball or a sparsity-inducing penalty could lead to interesting develop-ments.
 For any S in S and by optimality of b S ,  X  2  X  b S  X  S,S 0  X  X  X  X  2  X  b S  X  S,S 0  X  + L ( S )  X  X  ( b S )  X  2  X  k b S  X  S k  X  k k op + 2(1  X   X  ) k b S  X  S k 1 k k  X  for any  X   X  [0; 1]. The assumptions on  X , X  and trian-gular inequality lead to the first bound.
 Let r = rank( S ), k = k S k 0 , S = P r j =1  X  j u j v &gt; SVD of S , S =  X   X | S | , where  X  = sgn( S ), and  X   X   X  { 0 , 1 } n  X  n the complementary sparsity pattern. We use P onto the orthogonal of the left (resp. right) singular space of S . We also note P S ( X ) = X  X  P S  X  such that X = P S ( X ) + P S  X  Any element V of the subgradient of the convex func-tion S 7 X   X  k S k  X  +  X  k S k 1 can be decomposed as V =  X  for W 1 ,W  X  with k W  X  k op  X  1, k W 1 k  X   X  1, which can be chosen such that  X  V, b S  X  S  X  =  X   X  By monotonicity of the subdifferential and optimality conditions, 2  X  b S  X  S 0 , b S  X  S  X  Decompose =  X  P S ( ) + P S  X  Using results on dual norms, we have for all M 1 ,M 2  X  R n  X  n and hence, Using leads for  X   X  2  X  k k op and  X   X  2(1  X   X  ) k k  X  to k b
S  X  S 0 k 2 F + k b S  X  S k 2 F Using  X x  X  x 2  X   X  2 k b
S  X  S 0 k 2 F  X k S  X  S 0 k 2 F + and setting S = S 0 gives the result.
 Banerjee, O., El Ghaoui, L., and d X  X spremont, A.
Model selection through sparse maximum likelihood estimation. Machine Learning Research 101 , 2007. Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal of Imaging Sciences , 2(1):183 X 202, 2009.
 Bertsekas, D.P. Incremental gradient, subgradient, and proximal methods for convex optimization: a survey. Optimization for Machine Learning , pp. 85, 2011.
 Bien, J. and Tibshirani, R. Sparse estimation of a covariance matrix. Biometrika , 2010.
 Bock, J.R. and Gough, D.A. Predicting protein X  protein interactions from primary structure. Bioin-formatics , 17(5):455 X 460, 2001.
 Cai, J.F., Candes, E.J., and Shen, Z. A singular value thresholding algorithm for matrix completion. Arxiv preprint Arxiv:0810.3286 , 2008.
 Candes, E.J., Li, X., Ma, Y., and Wright, J. Ro-bust principal component analysis? Arxiv preprint ArXiv:0912.3599 , 2009.
 Chandrasekaran, V., Sanghavi, S., Parrilo, P.A., and
Willsky, A.S. Rank-sparsity incoherence for matrix decomposition. SIAM J. Opt. , 21:572 X 596, 2011. Combettes, P.L. and Pesquet, J.C. Proximal split-ting methods in signal processing. Fixed-Point Al-gorithms for Inverse Problems in Science and Engi-neering , pp. 185 X 212, 2011.
 D X  X spremont, A., El Ghaoui, L., Jordan, M.I., and
Lanckriet, G.R.G. A direct formulation for sparse pca using semidefinite programming. SIAM review , 49(3):434 X 448, 2007.
 El Karoui, N. Operator norm consistent estimation of large-dimensional sparse covariance matrices. An-nals of Statistics , 2009.
 Friedman, J., Hastie, T., and Tibshirani, R. Sparse in-verse covariance estimation with the graphical lasso. Biostatistics , 9(3):432, 2008.
 Hazan, E. Sparse approximate solutions to semidefi-nite programs. In Proceedings of the 8th Latin Amer-ican conference on Theoretical informatics , pp. 306 X  316. Springer-Verlag, 2008.
 Hoyer, P. O. Non-negative Matrix Factorization with
Sparseness Constraints. Journal of Machine Learn-ing Research , 5:1457 X 1469, 2004.
 Hu, P., Janga, S.C., Babu, M., D  X  X az-Mej  X  X a, J.J., But-land, G., Yang, W., Pogoutse, O., Guo, X., Phanse,
S., Wong, P., et al. Global functional atlas of es-cherichia coli encompassing previously uncharacter-ized proteins. PLoS biology , 7(4):e1000096, 2009. Jaggi, M. Convex optimization without projection steps. Arxiv preprint arXiv:1108.1170 , 2011.
 Jalali, A.and Chen, Y., Sanghavi, S., and Xu, H. Clus-tering partially observed graphs via convex opti-mization. ICML  X 11, 2011.
 Kim, J. and Park, H. Sparse nonnegative matrix fac-torization for clustering. Technical report, Georgia Institute of Technology, 2008.
 Koltchinskii, V., Lounici, K., and Tsybakov, A. Nu-clear norm penalization and optimal rates for noisy matrix completion. Annals of Statistics , 2011. Lee, D.D., Seung, H.S., et al. Learning the parts of ob-jects by non-negative matrix factorization. Nature , 401(6755):788 X 791, 1999.
 Liben-Nowell, D. and Kleinberg, J. The link-prediction problem for social networks. Journal of the Ameri-can society for information science and technology , 58(7):1019 X 1031, 2007.
 Luo, X. High dimensional low rank and sparse co-variance matrix estimation via convex minimization. Arxiv preprint arXiv:1111.1133 , 2011.
 Olkin, I. and Pratt, J.W. A multivariate tchebycheff inequality. The Annals of Mathematical Statistics , 29(1):226 X 234, 1958.
 Raguet, H., Fadili, J., and Peyr  X e, G. General-ized forward-backward splitting. Arxiv preprint arXiv:1108.4404 , 2011.
 Richard, E., Baskiotis, N., Evgeniou, Th., and Vayatis,
N. Link discovery using graph feature tracking. Pro-ceedings of Neural Information Processing Systems (NIPS) , 2010.
 Srebro, N. Learning with matrix factorizations . PhD thesis, MIT, 2004.
 Srebro, N., Rennie, J., and Jaakkola, T. Maximum-margin matrix factorization. Advances in Neural In-formation Processing Systems , 17:1329 X 1336, 2005. Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society , 58:267 X 288, 1996.
 Traud, A. L., Mucha, P. J., and Porter, M. A. Social structure of facebook networks. arXiv:1102.2166, 2011.
 Yuan, M. and Lin, Y. Model selection and estima-tion in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 68(1):49 X 67, 2006.
 Zou, H., Hastie, T., and Tibshirani, R. Sparse princi-pal component analysis. Journal of Computational and Graphical Statistics , pp. 1 X 30, 2004.
 Zou, Hui and Hastie, Trevor. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B , 67:301 X 320,
