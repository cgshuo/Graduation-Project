 It is well known that a great number of query X  X ocument features which significantly improve the quality of ranking for popular queries, however, do not provide any benefit for new or rare queries since there is typically not enough data associated with those queries that is required to reli-ably compute the values of those features. It is a common practice to propagate the values of such features from pop-ular to tail queries, if the queries are similar according to some predefined query similarity functions. In this paper, we propose new algorithms that facilitate and increase the effectiveness of this propagation. Given a query similarity function and a query X  X ocument relevance feature, we in-troduce two different approaches (linear weighting approach and tree-based approach) to learn a function of values of the similarity function and values of the feature for the similar queries w.r.t. the given document. The propagated value of the feature equals the value of the obtained function for the given query X  X ocument pair. For the purpose of finding the most effective method of propagating query X  X ocument fea-tures, we measure the effectiveness of different approaches to features propagation by performing experiments on a large dataset of labeled queries.
Modern web search engines use a large number of rele-vance signals that are typically combined in a complex fash-ion via learning a ranking function. Using these features, popular commercial search engines can generally respond to most frequently asked queries (or head queries ) with excel-lent results. However, providing answers of similarly high quality for tail queries, infrequently issued or previously un-seen, is more difficult due the fact that the quality of some features [1] highly depends on the amount of the data avail-able for calculation of those features that in turn is influ-enced by the query popularity. Previous studies [36] even re-ported that the quality of web page ranking for head queries is nearly the same for different search engines, while tail c  X  queries contribute to the major differences among search en-gines. Therefore, improving the performance of these queries by overcoming the sparseness of the data associated with them is one of the most critical challenges faced by search engines.

This problem is usually solved by propagating statistics between query X  X ocument pairs sharing the same document on the basis of query similarity , which can be determined via a similarity function [2, 5, 13, 21, 32, 34, 35, 38]. There is more than one way to define query similarity, which can be based on characteristics of user behavior ([13, 21, 26, 28, 32, 35]), semantic and syntactic properties of the queries [5, 17, 34, 38], and values of different features for different doc-uments with respect to the considered queries [32]. Our motivation for this work is to understand how different sim-ilarities and other properties of queries can be incorporated in the most optimal way for solving the problem of propa-gating query X  X ocument features. For this purpose, we pro-pose a general framework for learning a web page ranking function based on propagation of features via queries X  simi-larities. The learning part of our framework allows to com-pare different propagating methods by evaluating ranking quality of trained rankers. Given two propagating methods, a set of regular query X  X ocument features, a set of query X  document features that are computed by propagation, and an algorithm for learning a ranking function, we learn two different ranking functions, which are trained by using the same learning algorithm. The learning process differs only in the sets of propagated features that, in each process, are computed with a different propagation method (both sets of features contain the considered regular features). The first propagating method outperforms the second one, if the first ranking function outperforms the second one in terms of a considered measure of retrieval quality.

The effectiveness of propagating query X  X ocument features, as we demonstrate in this paper, depends on a variety of as-pects that need to be considered by a propagating method, but that were overlooked by the state-of-the-art methods. Assume we are provided with a collection of queries Q , val-ues of a feature f for query X  X ocument pairs ( q,d ) over all q from Q , and a collection of query similarity functions. The common approach to obtain the propagated value of the fea-ture f for a target query X  X ocument pair ( q,d ) is the follow-ing [2, 13, 35]. The propagated feature value  X  f ( q,d ) equals taken over the queries  X  q which are similar to query q . The weights w (  X  q,q ) and the set Q q of similar queries are defined in some way on the basis of one of the existing query simi-larity functions s (  X  q,q ).

So far, the values of the query similarity functions (some-times, normalized) were used directly as the weights in Equa-tion 1, without regard to their different nature and scale. Besides, there have been no comparison of the weighted mean as a function to aggregate the feature values for sim-ilar queries with any other function that could be taken over them. Also the state-of-the-art propagating method described by Equation 1 was not previously compared [2, 13, 35] with methods utilizing some more statistics about the distribution of the feature values over similar queries besides its linearly weighted mean.
 The major contributions of this paper are the following.
First, we investigate different approaches to learn a func-tion of similarity values which is used to compute the weights in the linear combination of the feature values in Equation 1. Moreover, we consider another approach to propagate (ag-gregate) query X  X ocument feature values based on the deci-sion trees instead of taking the weighted sum and compare the performances of the approaches.

Second, when using different propagating methods, some important data associated with similar queries  X  q is currently neglected. We show that some properties of similar queries can help the ranker to use those similar queries more ef-fectively for propagating. Our learning framework allows utilizing any query features to increase the contribution of propagated features to the quality of the ranking function.
To the best of our knowledge, we are the first to ad-dress the above-mentioned aspects of propagating query X  document relevance features of a ranking model via a col-lection of query similarities of different types.

Third, we provide experiments on a large data set of la-beled queries, compare performances of the state-of-the-art and proposed approaches and distinguish their combination that gives the best quality and advances the state-of-the-art. Moreover, our experimental results show that the best method of propagation significantly outperforms the state-of-the-art even if the features values are propagated in the case of the queries of high or moderate frequency. However, the relative quality of the final ranking functions is much greater for the tail queries.

The remainder of the paper is organized as follows. Sec-tion 2 contains a review of the methods of propagating query X  document features and the ways of defining queries X  similar-ity functions. In Section 3, we propose a classification of similarity functions. We describe our framework of propa-gating the features in Section 4. In Section 5, we introduce a method of exploiting some statistics about the distribu-tion of the feature values over similar queries to use similar queries more effectively for the propagation algorithms from Section 4. The experimental results are reported in Sec-tion 6. In Section 7, we summarize the outcomes of our study, discuss its potential applications and directions of fu-ture work.
The problem of query X  X ocument features (especially, click-through data) sparseness has been reported in many studies [1, 11, 13]. The lack of implicit feedback for tail (new or rare) queries affects the quality of web search, and is known [36] to affect the quality of results for tail queries most nega-tively. The common approach to this problem is to propa-gate features from head to tail queries. It is ordinarily done by considering a query similarity function [2, 13], which, in general case, can be defined in different ways [5, 21, 32, 34, 35, 38].

Evaluating a high-quality query similarity function itself is an important task in web search, because it is helpful in such problems as query suggestion [20, 21], query re-formulation [19, 31], and query expansion [30, 33]. We distinguish the following four types of similarity functions: text similarity functions (semantic and syntactic similari-ties), query X  X ocument features-based similarity functions, query-level user-based similarity functions and session-level user-based similarity functions.

Semantic similarity functions are typically computed based on the following intuition. Each query is associated with its n -gram vector. The closer two query n -gram vectors in the n -gram space are to each other (in accordance with some metrics), the more similar the queries are. This intuition was used to define a query similarity in [34, 38]. Syntactic similarity functions measure the distance between queries as term sequences. The most common syntactic function is the Levenshtein distance [5, 17, 18]. Query-document features-based similarities measure the distance between vectors of query X  X ocument features values for a pair of queries [32]. A query-level user-based similarity is computed on a click graph [13, 21, 32, 35]. Usually, a few steps of a random walk is used for this computation [13, 21, 26, 35]. A session-level user-based similarity is computed by using the information about co-occurrence of different queries in user search ses-sions [5]. In [26, 28], a random walk on a query-flow graph is exploited for similarity computation.

Query X  X ocument feature values can be propagated among queries (usually, from head to tail queries, but not always) after one or several similarity functions are defined. Despite the obvious practical importance of the research on the prop-agation of query X  X ocument relevance features by using simi-larity functions, its history is rather short. Given a similarity function, the most common approach is to define propagated feature values by Equation 1 (see [2, 13, 35]). For instance, Aktolga and Allan [2] use a query similarity function for the purpose of click-through data propagating. The resulting ranking score of a document with respect to a query q in their framework equals to a linear combination of the de-fault ranking score, the original value of the click-through feature and the propagated value of the feature. The latter propagated value is the weighted sum of the values of the feature over all queries similar to the query q as in Equa-tion 1 (the weights are equal to the respective outcomes of the similarity function). In their experiments, they consider only a small set of 2400 popular queries, among which they propagate click-through data (a query is considered popular, if it has at least 500 clicks).

In [13], the authors propagate click-through feature val-ues as well. A query similarity function is used to expand the clickthrough streams of a document. The propagated features values are computed on the basis of these expanded clickthrough streams. The authors constrain the set of sim-ilar queries utilized to propagate click-through features by tuning a threshold on the value of the similarity function (for the purpose of propagating they use only those similar queries whose similarity value w.r.t. the target query ex-ceeds the threshold). For any propagated feature  X  f from the paper there exists a query X  X ocument feature f such that for any query X  X ocument pair ( q,d ) and a clickthrough stream w of a document d the feature value  X  f ( q,d ) equals sum of f (  X  q,d ) over all  X  q in w . Therefore, the propagated values in this method equal the linear combination of feature val-ues with the binary weights (so, essentially are defined by Equation 1).

Unfortunately, both papers do not propose any method for optimizing the weights in the linear combinations of the feature values given ad-hoc similarity functions that de-fine those weights. Our work elaborates the idea of query-document feature value propagation via query similarity func-tions proposed in these papers further. We experiment with several implementations of that idea and, most importantly, advance it by solving the problems posed in Section 1, thus aiming to make this approach worth taking more seriously for search engine development.
In this section, we introduce our classification of query similarity functions investigated in previous papers. Essen-tially, such a function should be computed on the basis of the following intuition: similar queries (in terms of this func-tion) represent similar information needs. The similarity of these needs can be evaluated using the statistics of different nature. In particular, the computation of similarity might utilize some a priori data (lexical properties of the queries) or a posteriori data (shows of documents to the queries or clicks on documents shown to the queries). Therefore, our classifi-cation is based on the type of the data utilized for the com-putation of similarity functions. Specifically, we distinguish the following 4 classes: query X  X ocument features-based sim-ilarity (depending on the feature type it utilizes either a priori data or a posteriori data), query-level user-based sim-ilarity (utilizes a posteriori data), session-level user-based similarity (utilizes a posteriori data) and text similarity (se-mantic and syntactic similarity), which utilizes a priori data. We list several similarity functions of each class, which were commonly used in previous studies.

In what follows, we consider a set of queries Q and define all the similarities for a pair of queries from this set. 3.1. Features-based similarity. The computation of a query X  X ocument features-based similarity s of queries q 1 q [32] utilizes any type of data, which is used for measuring relevance of documents with respect to the queries. Let f be any nonnegative feature of a query X  X ocument pair. Let D = { d j } N j =1 be a collection of documents. Consider two feature values for all the documents with respect to queries q and q 2 respectively.

Consider a function g : R N +  X  R N +  X  R + chosen in such a way that, if two queries q 1 and q 2 are naturally simi-Query X  X ocument features-based similarity s between q 1 and q equals s ( q 1 ,q 2 ) = g ( f q 1 , f q 2 ). We choose g from the set of two functions, scalar product and cosine, which provide the following two similarity functions:
SFS f ( q 1 ,q 2 ) =  X  f q 1 , f q 2  X  , CFS f ( q 1 ,q 2 for the given feature f .

Popular search engines store the anonymized information describing user behavior (visited pages, times of visiting, submitted queries etc.) in the query logs. In the next two sections, we consider queries X  similarities mined from user search behavior data from the query logs. 3.2. Query-level user-based similarity. The similar-ity of the second type is based on properties of one of the bipartite graphs constructed by using user search behavior data and described in what follows. It is a common prac-tice to compute such similarities (which are called query-level user-based similarities) using random walks on these graphs (see [13, 21, 27, 35]), because random walks model the user behavior. We consider two graphs: a click graph and a graph of shows. Click graph is a bipartite unoriented graph G ( Q,D,E ), where Q is a set of queries, D is a set of documents and an edge { q,d } X  E is weighted by the num-ber cl qd of clicks on document d shown to query q . Graph of shows is defined in a similar way, the weight of an edge { q,d } in the graph of shows equals the number sh qd of shows of document d to query q . Consider a random walk on a bi-partite graph G ( Q,D,E ) (either a click graph or a graph of shows), where the probability of transition by an edge { q,d } is proportional to its weight w qd (either w qd or w qd = sh qd ). Let q 1 ,q 2  X  Q , k  X  N . Usually, query-level user-based similarities QUS cl k ( q 1 ,q 2 ) and QUS sh k are defined as the probabilities of arriving at query q 2 2 k steps of the random walks on the click graph and the graph of shows respectively, which have started from query q . 3.3. Session-level user-based similarity. User search sessions extracted from the query logs can serve as another source of information about similar queries. Indeed, users of-ten reformulate queries in the same search session. Such re-formulations often represent similar information needs. We call a similarity function session-level user-based similarity , if the input data for computing this function is extracted from the query log.

Particularly, we consider the similarity functions that were proposed in [5]. Using sessions extracted from query logs, the authors get the estimations P ( q ) and P ( q 1 ,q 2 ), the prob-ability of a query q to appear in an arbitrary search ses-sion and the probability of queries q 1 ,q 2 to appear in one search session respectively, which are utilized for comput-ing four similarity functions: PMI, PMIJ, PMIS and PMIG. Apart from these functions, we consider a simple similarity function, which estimates the probability of the query q 1 appear in a session under the condition that the query q appears in that session: CP( q 1 ,q 2 ) = P ( q 1 ,q 2 ) / P ( q 3.4. Text similarities (semantic and syntactic).
 There can be too little important data for computing simi-larities of new or rare queries which exploit some noise-prone query X  X ocument features or are based on users search be-havior. Instead, the texts of such queries can be used for the estimation of their similarity. So, both semantic and syntactic similarities are computed by utilizing texts of the queries only. Therefore, we do not divide this class into two different subclasses.

Each query q can be represented by a vector v q in the n -gram vector space [10, 34, 38], where each dimension cor-responds to an n -gram (a sequence of n consecutive terms of a query from Q ). To define v q , one first defines nonneg-ative weights of the n -grams occurring in the query. These weights are the components of v q that correspond to the n -grams of the query. The other components of the vector are zero.

Similarly to the feature-based approach, we consider two functions, scalar product and cosine. Note that these func-tions are nonnegative-valued, because their arguments are nonnegative. Thereby, two semantic similarities ,
SSS n ( q 1 ,q 2 ) =  X  v q 1 ,v q 2  X  , CSS n ( q 1 ,q 2 ) = cos( v are considered for a given value of n chosen for construction of the n -gram space.

As it was done in previous studies [34, 38], for any n -gram, term frequency tf or the feature tf  X  idf (see [6], we compute these features on the queries X  corpus) are considered to be the weight w of the n -gram.

For any two queries q 1 , q 2 , syntactic similarity measures the distance between the queries q 1 and q 2 as sequences of terms. The most common syntactic similarity function is the Levenshtein similarity s ( q 1 ,q 2 ), which is defined by using the Levenshtein distance dis( q 1 ,q 2 ) [18] with the deletion, the insertion and the substitution costs which are equal to 1. The Levenshtein similarity equals 1  X  dis( q 1 ,q 2 ) | q | is the number of terms in q . In [5], the deletion and the insertion costs equal 1, and the substitution cost of x for x 2 equals the normalized original Levenshtein distance as sequences of characters. We call the similarity function between queries based on this modified Levenshtein distance weighted Levenshtein similarity (WLS). The value of this function depends on the orders of the terms in the queries. Alternatively, the authors of [5] introduce the weighted Lev-enshtein similarity, which avoids this effect. In order to calculate this similarity, we should sort the terms in each query in the alphabetical order. Then the weighted Leven-shtein similarity computed for the reordered term sequences is called ordered weighted Levenshtein similarity (OWLS).
In this section, we introduce the state-of-the-art and a number of novel methods of propagating query X  X ocument features via query similarity functions. We consider two different propagating principles: linear weighting and tree-based. The first method is a generalization of the state-of-the-art propagating method which was proposed in the previous studies [2, 13, 35].

Let Q and D be a set of queries and a set of documents respectively. We consider a query X  X ocument feature f : Q  X  D  X  R and similarity functions s 1 ,...,s m : Q  X  Q  X  R + We assume that, for any query q and any j  X  { 1 ,...,m } , a set Q q = Q j q of queries, which are similar to q , can be obtained on the basis of the similarity function s = s j (  X  ,q ). Sections 4.1 X 4.3 focus on propagating feature f to queries q based on Q q and s .
We start with a description of the standard way to propa-gate feature values across similar queries [2, 35]. It is based on the equation which defines the propagated feature value  X  f ( q,d ) as a weighted average of the feature values over similar queries  X  q  X  Q The weights w (  X  q,q ) reflect the importance of queries  X  q for query q and are defined on the basis of query similarities s (  X  q,q ). In the state-of-the-art approach [2, 35], the weight function w is defined by which is theoretically unfounded and may be suboptimal in practice.

Intuitively, we assume that the actual purpose of Equa-tion 2 is to obtain a (more reliable) estimate of unreliable or noisy feature value f ( q,d ). In contrast to Equation 2 which, in fact, defines  X  f w ( q,d ) in an ad-hoc unsupervised way, we propose to learn the weight function w = w (  X  q,q ) in order to optimize the quality of estimation, i.e we directly minimize the mean squared deviation of the right-hand side of Equation 4 from its left-hand side:
Appendix A provides a theoretical foundation of Equa-tion 4 and optimization Problem 5 in more precise and de-tailed terms. In Section 4.2, we propose novel methods of learning the weights w (  X  q,q ) in the described optimization setting. In Section 4.3, we consider another approach to propagate query X  X ocument features as an alternative to the linear weighting approach defined by Equation 2.
In this section, we introduce a method of learning the weighting function w (  X  q,q ) used in Equation 2. We look for a solution w to Problem 5 of the form where g belongs to a parameterized class of nonnegative val-ued functions G = { g  X  : R  X  R ,  X   X   X  } , and  X  denotes the parameter space 1 .

We consider two different approaches to define class G further in this paper: 1) to choose a particular family of functions; 2) to divide segment [0 , 1] into N disjoint subsets D ,...,D N and set G to be the space of step functions that are constant on each D i , i.e. g  X  ( x ) = P N i =1  X  i I N is a fixed natural number,  X  = (  X  1 ,..., X  N )  X  R N is the set of parameters, and I D i is the indicator function of D
In the described setting, the optimization Problem 5 can be expressed as follows: where D is a training set of query X  X ocument pairs and h = h ( x,y ) (particularly, we consider the squared loss function h ( x,y ) = ( x  X  y ) 2 ). 4.2.1. Parametric family of functions. Let G be any parameterized class of functions g  X  w.r.t. the parameter (a vector of parameters)  X  . For many classes of functions g
In our experiments, we consider  X   X  R N . (e.g., G = {  X  x ,  X   X  R } ), Problem 7 does not have a closed-form solution. Consider an alternative optimization problem
X which is obtained by interchanging the order of taking g and summation over Q q in the optimization Problem 7. It is easy to see that, for some classes G , Problem 8 has a closed-form solution (e.g., a solution for a polynomial regression model with G = { a k x k + ... + a 1 x + a 0 ,  X  = ( a 0 ,a R k +1 } , h = ( x  X  y ) 2 ). Obviously, if g  X  (  X  ) is not a linear ho-mogeneous function, then Problems 7 and 8 are not equiv-alent or necessarily have similar solutions, as long as the values of g  X  1 ( q ) = g  X  ( P  X  q  X  Q P for any ( q,d )  X  D , we replace the values f (  X  q,d ) with their normalized modifications f  X  q (  X  q,d ) := f (  X  q,d ) / P in g  X  1 ( q ) and g  X  2 ( q ), then the obtained values (we denote them by g  X  1 ( q ) and g  X  2 ( q )) are close and hence we can arrive at the solution of the modification of Problem 7 by solving the modification of Problem 8 Indeed, for linear functions g ( a 0 ,a 1 ) ( x ) = a 0 + a equality g  X  1 ( q ) = g  X  2 ( q ) holds. For a convex function g g ( q ) is slightly smaller than g  X  2 ( q ). For a concave func-tion g  X  , g  X  1 ( q ) is slightly larger than g  X  2 ( q ). If g convex nor concave differentiable function, then, intuitively, the difference between is g  X  1 ( q ) and g  X  2 ( q ) is far less. More-over, in Appendix B, for two classes G , polynomial functions and power functions (the two classes we consider in our ex-periments, see Section 6.3) and Q q large enough, we prove that
Therefore, in order to consider modifications of Problems 7 and 8 that have similar solutions, we, first, normalize the values of f and consider the optimization Problem 9 instead of Problem 7. Under our assumptions, Problem 9 reduces to Problem 10. Moreover, in Section 6.3, we show that the propagating method which is based on solving Problem 10 outperforms the state-of-the-art propagating method.
As we mentioned above, in Section 6.3, we consider a class of polynomial functions with degree 3 and a class of power functions. In the first case, Problem 10 with h ( x,y ) = ( x  X  y ) 2 is solved by ordinary least squares estimation for a polynomial regression model E ( y | x ), where P  X  q  X  Q f (  X  q,d ) is the explanatory variable x and f  X  q ( q,d ) is the re-sponse variable y . In the second case, we change the loss function h , because, for the class of power functions, Prob-lem 10 cannot be solved by ordinary least squares estima-tion (the iterative Gauss-Newton algorithm for a non-linear least squares optimization problem [14] can be used, but we avoid it because of its computational complexity). To motivate a choice of function h in this case, we study the dependency of f  X  q ( q,d ) on P  X  q  X  Q tion 6.3. As we show, the dependency of log f  X  q fore, in this particular case we choose h ( x,y ) = (log x  X  log y ) 2 . 4.2.2. Step weighting functions. Assumption 11 is well-reasoned far not for any class of functions G and distri-butions ( f  X  q (  X  q,d ) ,  X  q  X  Q q ). Fortunately, we do not need it in the case of the second approach to define G , where g  X  are step functions and D 1 ,...,D N  X  [0 , 1] are disjoint subsets. We learn pa-rameters  X  = (  X  1 ,..., X  N )  X  R N by minimizing the mean squared error (MSE) (Problem 7, where h ( x,y ) = ( x  X  y ) exploiting the linear regression algorithm: f = ( f ( q,d ) , ( q,d )  X  D ) T is the vector of the values of the feature f , which are ordered in the same way as the elements of each column of matrix F .
In Sections 4.1 and 4.2, for a given query X  X ocument pair ( q,d ), we define the propagated feature value  X  f ( q,d ) by Equation 2. It defines a function, which linearly depends on the values of f (  X  q,d ) with coefficients w (  X  q,q ) calculated on the basis of similarities s  X  (  X  q,q ). Certainly, this linear prediction method might be not the best performing among all the prediction principles, as linear learning models are known to be suboptimal in a variety of learning tasks. So, in the general case, a predicted feature value  X  f ( q,d ) equals a value of some function of feature values f (  X  q,d ) and simi-larity values s  X  (  X  q,q ) (  X  q  X  Q q ). By this reason, we compare the linear approach described in Section 4.2 with the  X  X ree-based approach X , were a function of the same arguments trees [12, 23, 29]). In this section, we consider this alterna-tive approach to propagate query X  X ocument feature values.
For a query X  X ocument pair ( q,d ), the input of a tree-based model is a feature vector X 1 ( q,d ) ,...,X N ( q,d ) of dimension N , which is a parameter of the algorithm. We consider two ways to define features X 1 ,...,X N : 1. We simply take ( f (  X  q,d ) ,s  X  (  X  q,d ) |  X  q  X  Q 2. Given a partition D 1 ,...,D N of [0 , 1], we set X i
The second approach is motivated by the second opti-mization algorithm from Subsection 4.2.2, where we learn, in fact, a linear model predicting  X  f ( q,d ) using the same fea-The components of feature vectors ( f (  X  q,d ) ,s  X  (  X  q,q ) |  X  q  X  Q ) in the first approach should be ordered on the basis of some principle common across different pairs ( q,d ). We use the following ordering: where Q q = { q 1 ,q 2 ,... } is ordered in such a way that s ( q s ( q 2 ,q ) &gt; ... . Here we set f ( q i ,d ) = s  X  ( q i  X  X | Q q | + 1 , | Q q | + 2 ,...,N/ 2 } , if | Q q | &lt; N/ 2.
Our goal is to learn a function g : R N  X  R such that the predicted values are close to the target values f ( q,d ) for ( q,d )  X  D , where D is a training set of query X  X ocument pairs (as in the pre-vious section). We solve this problem by minimizing the squared loss function P ( q,d )  X  X  (  X  f ( q,d )  X  f ( q,d )) dient boosted decision trees [12]. We use the obtained func-tion g to compute the propagated feature value  X  f ( q,d ) de-fined by Equation 14.
In Section 1 we ask, if some statistics of the distributions of the feature values over similar queries besides the average value can help the ranker to use those similar queries more effectively. To answer this question, we use quantiles of four different distributions: the distribution of query frequen-cies, the distribution of the numbers of characters in queries ( character lengths of queries), the distribution of the num-bers of terms in queries ( term lengths of queries), and the distribution of values of the similarity function in the follow-ing way. For each similarity s and query q  X  Q , we divide the set Q q of queries similar to q (which is obtained on the basis of the similarity function s ), into 5 subsets Q q (1) ,...,Q (see Equation 15) with respect to the values of quantiles and calculate the average value of query X  X ocument feature CTR (that we exploit in our experiments, see Section 6.2) over queries  X  q  X  Q q ( i ) and all pairs ( q,d ). We represent the dependency of the obtained values on quantile i in Fig-ure 1 for 4 different similarities, one from each similarity type (see Section 3) and two types of distributions whose quantiles define Q q ( i ): the distribution of query frequencies and the distribution of the values of the similarity function. For most of these similarities, the dependencies are mono-tone. This means that the values of the feature for similar queries is lesser, on average, for greater quantiles of query frequency (similarity to initial query q ). This observation suggests that the average values of the feature over queries from each Q q ( i ) may carry more information than the aver-age value of the feature over all queries from Q q alone. In our experiments, we show that these values provide useful information for improving the quality of ranking (see Sec-tion 6.3). In what follows, we describe how we use quantiles in detail.

As in Section 4, we consider a query X  X ocument feature f , a similarity function s := s j , j  X  { 1 ,...,m } , and, for each q  X  Q , a set Q q := Q j q of queries in Q , which are similar to q .
 Let P be a distribution on R + . Consider a sample X 1 ,..., X n of the distribution P and an increasing sequence of num-bers  X  1 ,..., X  r  X  [0 , 1]. Let  X  1 ,..., X  r be the empirical  X  ,..., X  r  X  X uantiles of distribution P respectively. We con-struct the sample X 1 , ..., X n in two different ways: either it depends on q or it does not. The first way is to build the sam-Figure 1: The dependencies of an average value of ple by using all the queries from Q q for the frequency distri-bution and the distributions of lengths (i.e., ( X 1 ,...,X distribution (i.e., ( X 1 ,...,X n ) = ( s (  X  q,q ) ,  X  q  X  Q quantiles depend on q  X  Q . The second way is to take all the queries from Q for the frequency distribution and the distributions of lengths and all the pairs from Q  X  Q for the similarity distribution (the quantiles are the same for all the queries q  X  Q ).

We use the quantiles  X  1 ,..., X  r for dividing queries into disjoint subsets and propagating the values of feature f for each subset in the following way. For each i  X  { 1 ,...,r } , we define the propagated feature value  X  f i ( q,d ) for the pair ( q,d )  X  Q  X  D either by Equation 2 in the linear weighting approach or by Equation 14 in the tree-based approach using instead of Q q for learning the function g , where  X  0 =  X  X  X  and X l (  X  q ) is the element of the sample which corresponds either to the query  X  q (for the frequency distribution and the distributions of lengths) or to the pair (  X  q,q ) (for the similarity distribution).
We compare the performance of different propagating tech-niques described in Section 4 by learning ranking functions using propagated query X  X ocument features. In the next sec-tion, we describe the details of learning to rank framework. In Section 6.1 and Section 6.2, we describe the dataset and the similarity functions we exploit in our experiments. Fi-nally, we compare the performances of different propagating methods in Sections 6.3 and 6.4. First, we compute average values of ranking quality metrics on a test set of queries. Second, as we expect that our methods are especially use-ful for tail queries, Section 6.4 presents a closer look a the methods X  performance on a subset of infrequent queries.
We consider the classical problem of ranking documents according to their relevance to a query [8]. It was shown that regression with gradient boosted decision trees, while com-putationally more efficient, is only marginally worse than the most advanced list-wise learning to rank methods [8], so, given the amount of calculations in our experiments and the fact that our methods are agnostic w.r.t. to a specific ranking method, we solve the ranking problem by minimiz-ing MSE using a proprietary implementation of gradient boosted decision trees [12].
For ranking evaluation, a random sample of queries Q issued by the users of the European country under study was selected from the query logs of one of the most popular commercial web search engines. We selected 66 . 6% of the queries as our training set Q 1 , 16 . 7% as the validation set Q 2 and the remaining 16 . 7% as the test set Q 3 . The set Q was divided into two subsets Q 1 1 and Q 2 1 of equal size, the first one is for learning to propagate query X  X ocument fea-tures, the second one is for learning to rank (see Table 1). For each query q  X  Q 0 \ Q 1 1 (we do not need relevance labels for learning to propagate features), the relevance of each of the top ranked documents was judged by professional asses-sors hired by the search engine (we denote the set of these documents by D ( q )). In our experiments, we consider only documents from the union D := S q  X  Q documents over all queries in the dataset. The relevance labels were assigned using a popular graded relevance scale with labels: perfect, excellent, good, fair, bad . The data we use contains  X  1 . 9M query X  X ocument pairs and  X  90K unique queries (around 21 judged documents per query). We compare the methods on the test set in terms of average NDCG@3 and NDCG@5 metrics (see [7]), which were the primary measures in [2, 13].

While our framework is not specific to a particular type of query-document features, in this paper, we experiment with click-through features for the sake of consistency with the previous papers [2, 11, 13]. One of the most widely used feature of that kind is CTR, another one is nCTR proposed in [2]. As the reliability of click-through feature values for query X  X ocument pairs ( q,p ) depends on the number of shows of p in response to q and the numbers of clicks on p shown to q , we construct a set Q = Q 0  X   X  Q of queries (see Table 1), which we use to propagate query X  X ocument features from in the following way. We add a query q to  X  Q , if either the total number of shows of documents to q recorded in the query log (all the records were made from 1 January 2014 to 31 March 2014) is greater than 70 or the number of clicks on documents shown in response to q is greater than 6. For each labeled document d from the data set, we create top-1000 list of queries from Q 0  X   X  Q , which have the largest number of shows of document d in response to them. We remove a query from  X  Q , if it is not in such a top-1000 list for at least one document.The final set Q contains  X  52 . 7M queries.
For the purpose of evaluating features-based and query-level user-based similarities and for propagating click-through features, we consider all query X  X ocument pairs (  X  254 . 2M pairs) from the query logs, where all queries are from Q and the documents are from D .
In our experiments, we consider 30 query similarity func-tions. We exploit 6 features-based similarities (see Sec-tion 3.1): SFS f , CFS f , where f is one of the features BM25, CTR, nCTR (normalized CTR [2]); 9 query-level user-based similarities (see Section 3.2): QUS cl 1 , ..., QUS cl 6 and QUS QUS sh 2 , QUS sh 3 ; 5 session-level user-based similarities (see Section 3.3): PMI, PMIJ, PMIS, PMIG, CP, 8 seman-tic similarities (see Section 3.4): SSS 2 , CSS 2 , SSS 3 where w is either tf or tf  X  idf ; 2 syntactic similarities (see Section 3.4): WLS and OWLS.

For each query q from the set Q 0 , each similarity type y  X  { 1 ,..., 4 } (we enumerate them in the same way as in Section 3), and each similarity function s of type y , we define a set of similar queries Q q ( s )  X  Q by removing from the set Q queries  X  q with small values of similarity s (  X  q,q ) (to avoid too outlying feature values for these queries  X  q w.r.t. a given document, which are used for propagation) in the following way. First, independently on q , we define a threshold t ( s ) on the similarity values in such a way that there are 10% of the values of s which are less than t ( s ) and, for the query q , remove from Q all queries  X  q such that s (  X  q,q ) 6 t ( s ). Sec-ond, we define a threshold n ( y ) on the size of a set of top queries in set Q q ( s ) ranked by their similarity to q . The value of n ( y ) was chosen from segment [50 , 2000] with step 50 by maximizing NDCG@3 on the part Q 2 1 of the training set (we exploit 5 regular query X  X ocument relevance features PageRank, BM25, Query Frequency, CTR, nCTR (apart from the features obtained via propagation) and 60 prop-agated CTR and nCTR by the linear weighting approach (each of these two features is propagated by using each of the considered similarity functions), where the weights for propagating are defined by Equation 3). It was also com-pared with the quality of the ranking function, which was learned without the threshold n ( y ): the quality of the func-tion which was learned by utilizing the threshold was better for all the similarity types. From this point, we consider only top n ( y ) queries from Q q ( s ) for each similarity type y .
Finally, we filter out those similarity functions from the initial set, which do not influence the effectiveness of the propagation significantly (which is expected as many of them strongly correlated with each other). Let y  X  { 1 , 2 , 3 , 4 } be a similarity type, s y, 1 ,...,s y,m ( y ) be all the considered simi-larities of type y (we denote by m ( y ) the number of similarity functions of type y ). First, we train the ranking functions on the set Q 2 1 with 5 regular features and two sets of features  X  f ,...,  X  f m ( y ) defined by Equation 2 and Equation 3, where f is either CTR or nCTR. Further, for each i  X  X  1 ,...,m ( y ) } , we remove two features  X  f i (one for CTR and one for nCTR) at a time and train the ranking function. If the quality of the ranking function does not change statistically significantly (according to paired t-test with p &lt; 0 . 05) on the validation set, we no longer exploit the similarity s y,i in our experi-ments.

After this procedure, the following 15 similarity functions remain: all the 6 features-based similarities, 3 query-level user-based similarities: QUS cl 2 , QUS cl 6 and QUS sh 1 level user-based similarities (PMIJ, CP), 2 semantic similar-ities (SSS 3 , CSS 3 , where w is tf  X  idf), 2 syntactic similarities (WLS and OWLS).
The maximal portion of queries which can be removed without a significant change in the ranking quality.
All the ranking functions in this section were trained on the set Q 1 . The meta-parameters for each ranking function were tuned by maximizing the respective quality measures on the set Q 2 . We trained the ranking functions by ex-ploiting the set of 5 regular features. Moreover, we learn 8 ranking functions by exploiting different sets of features which are described in what follows (see Table 2).
 Table 2: Performances of the ranking functions for dif-For learning functions g used by propagation methods of Section 4.2 (Equation 6) and Section 4.3 (Equation 12) with each of the considered similarity functions, we set Q = Q We construct the set D in the following way: pair ( q,d ) is in D if and only if the number of shows sh qd of the document d to query q is greater than 25 and the number of shows of all documents to the query q is greater than 500. Let s be any of the considered similarity functions.

First, we assume that the continuous function g for the linear weighting propagating method (see Section 4.2.1) is a power function and study the dependency of f  X  q ( q,d ) on  X  f that q  X  Q 1 1 . To prove the choice of the function h (see Section 4.2.1), we interpret this dependency in the follow-ing way. For each value of f  X  q ( q,d ), we find the mean value E  X  f  X  q ( q,d ) of  X  f  X  q 0 ( q 0 ,d 0 ) over all pairs ( q  X  f poses, we consider only two similarities here), we represent the dependency of f  X  q ( q,d ) on E  X  f  X  q ( q,d ) and approximation f  X  = b E  X  f  X  + a using linear regression on the log-log scale plot (red line). As the dependency is approximately linear, we suppose that G = { ax b ,  X  = ( a,b )  X   X  } ,  X  = h ( x,y ) = (log x  X  log y ) 2 . We find the optimal  X  in Prob-lem 10 using the linear regression algorithm in the log-log scale for each similarity function (Figure 2, blue line) and, therefore, obtain 30 propagated features. Finally, we train the ranking function on the set Q 2 1 with 5 baseline features and the obtained 30 propagated features (see Table 2).
Second, we suppose that g is a polynomial function with degree 3 (see footnote 2), i.e. G = { a 3 x 3 + a 2 x a ,  X  = ( a 3 ,a 2 ,a 1 ,a 0 )  X   X  } ,  X  = R 4 , and h ( x,y ) = ( x  X  y ) We find the optimal  X  in Equation 10 using the linear re-gression algorithm for each similarity function, obtain 30 propagated features and train the ranking function on the set Q 2 1 with 5 baseline features and the obtained 30 propa-gated features (see Table 2).
 For learning the discrete function g we set N = 100 (see Section 4.2) 3 . We divide the segment [0 , 1] into N intervals D ent ways. Firstly, we exploit the uniform division: d i = i  X  { 0 ,..., 99 } , learn the parameters  X  1 ,..., X  100 (see Sec-tion 4.2) and obtain 30 propagated features (see Table 2). Secondly, we choose a random sample  X  Q 1 1 of 20% queries from Q 1 1 and let d 1 ,...,d 99 be the empirical 0 . 01 ,..., 0 . 99 X  quantiles of distribution of s when the second argument is in Q . For learning the parameters  X  1 ,..., X  100 we remove the sample  X  Q 1 1 from the set Q 1 1 . As the result, we also obtain 30 propagated features (see Table 2).

For each of the two ways of constructing sets of features in the tree-based approach (see Section 4.3) we consider three choices of the parameter M ( M = 100, M = 200 and M = 400). The optimal values of M were chosen by maximizing NDCG@3 on Q 2 1 (see Section 6.1) 4 .

We compare the effectiveness of all the methods of prop-agating in each class of propagating methods on the test set: continuous weights-based, discrete weights-based and tree-based (we consider 2 ranking functions from each class of methods) in Table 2. The best methods are Power in the continuous weighting class, Equivalently discrete in the discrete weights-based and Interval-based tree-based in the tree-based class. The best method of propagating is Equiv-alently discrete . The NDCG@3 and NDCG@5 gains of this method in comparison with State-of-the-art equal  X  0 . 6% and  X  0 . 5% respectively. The NDCG@3 and NDCG@5
The value of N was chosen from { 10 , 100 , 1000 } by maxi-mizing NDCG@3 on Q 2 1 (see Section 6.1). For the point-based tree-based ranking function we chose M = 200 and for the interval-based tree-based ranking func-tion we chose M = 100. gains of this method in comparison with Regular equal  X  3% and  X  3 . 5% respectively. All the differences in metrics val-ues are significant according to Paired Bootstrap Hypothesis Test [25] with p &lt; 0 . 01.

For the best propagating method ( Equivalently discrete ), we use the quantiles (see Section 5) for dividing similar-ity scores into intervals and propagating the features for each interval. We consider 5 quantiles,  X  1 = 0 . 2,  X  2  X  3 = 0 . 6,  X  4 = 0 . 8,  X  5 = 1 (see Section 5), for 4 different dis-tributions (the distribution of frequencies, the distribution of character lengths, the distribution of term lengths and distribution of values of the respective similarity function). Moreover, we evaluate the quantiles by constructing sam-ples by two methods (either each sample is constructed for each query or one sample is constructed for all the pairs of queries, see Section 5). We compare the performance of the obtained 8 ranking functions. Quantiles for the similarity distributions with one sample which is constructed for all the pairs of queries outperforms all the other propagating meth-ods on the validation set with both NDCG@3 and NDCG@5 metrics. The NDCG@3 and NDCG@5 gains of Equivalently discrete ranking function with quantiles in comparison with simple Equivalently discrete ranking function equal  X  0 . 4% for both ranking metrics on the test data (see Table 2). The NDCG@3 and NDCG@5 gains of this ranking func-tion in comparison with State-of-the-art equal  X  1 . 1% and  X  0 . 9% respectively. The NDCG@3 and NDCG@5 gains of this method in comparison with Regular equal  X  3 . 4% and  X  3 . 9% respectively. All the differences of qualities are sig-nificant according to Paired Bootstrap Hypothesis Test with p &lt; 0 . 01.
We test our algorithm on the set of tail queries (see Fig-ure 3). For the queries infrequently issued by users (with frequencies at most 5) the NDCG@3 and NDCG@5 gains of the Equivalently discrete ranking function with quantiles in comparison with State-of-the-art ( Regular ) equals  X  3 . 9% (  X  13 . 7%) and  X  3 . 9% (  X  13 . 8%) respectively. These gains decrease up to  X  1 . 9% (  X  6 . 2%) and  X  2 . 2% (  X  7 . 4%) re-spectively as frequencies grow up to 30. Figure 3: Comparison of our algorithm, State-of-the-art
In this paper, we consider different methods of propagat-ing query X  X ocument relevance features for the purpose of improving the quality of page ranking. Given a similarity function and a query X  X ocument feature, we investigate dif-ferent approaches to learn a function of similarity values and values of the feature for similar queries w.r.t. a given document. This function is further used to compute the propagated values of the features.

We also consider the statistics associated with distribu-tions of character and term lengths of queries, queries X  fre-quencies and queries X  similarities and prove that it helps the ranker to use similar queries more effectively. Moreover, we choose the most effective type of the distribution and the most effective way of exploiting it.

Our experimental results show that the proposed propa-gating algorithms significantly outperform the state-of-the-art propagating algorithm. Moreover, as expected, the effec-tiveness of propagation by our method in comparison with the state-of-the-art approach depends heavily on the fre-quencies of queries which are supplemented with the features obtained via propagation. We observe that our methods are especially useful for rare (tail) queries.

It would be interesting to continue to study the properties and applicability of the propagating methods. We plan to determine in more detail, which properties of queries define whether they would benefit from propagation by different methods. Moreover, we are going to apply the propagating methods for different tasks besides web ranking.
We rely on a hypotheses that f (  X  q,d ) are independent ob-servations of P f | q = q and on an additional hypothesis that s (  X  q,d ) are independent observations of some probability dis-tribution P s | q = q as well. Let  X  1 ,..., X  n be independent non-negative and identically distributed random variables such that for all t  X  Z the inequality E  X  t 1 &lt;  X  holds. By the law of large numbers and Markov X  X  inequality, for any  X ,, X  &gt; 0 and n large enough, n P (  X  1 &gt; n ( E  X  1 +  X  )) + P (  X  1 + ... +  X  n &gt; ( E  X  ( P (  X  1 &gt; n  X  ( E  X  1  X   X  ) , X  1 + ... +  X  n &gt; ( E  X  So, if k is the exponent of a power function g  X  , then g | Q then g  X  i ( q ) = a 0 (1 + | Q q |  X  1+ o (1) ) , i  X  { 1 , 2 } . If g a
