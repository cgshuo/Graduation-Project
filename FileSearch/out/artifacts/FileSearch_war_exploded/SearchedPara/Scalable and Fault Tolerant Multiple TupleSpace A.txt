 Considering the cooperating characteristic of mobile agents, it is obvious that the mobile agent based systems require efficient communication architecture. For this purpose, there have been research efforts about the mobile multi agent com-munication such as message based middlewares, remote procedure call (RPC) based methods, shared memory etc. Of them, virtual shared memory is fre-quently adopted because of its simplicity, flexibility, and versatility. Moreover, its model is powerful enough to meet the needs of many coordination challenges. agent systems is a tuple space [1] which behaves like a shared memory where each agent in a system can put or retrieve information to communicate. Partic-ularly, JavaSpace [2], a new realization of the tuple space in Java language, has been developed, and employed as the backbone of the agent communication in many systems.
 are the main topic of this paper. In the case of a single tuple space, the failure of the tuple space may bring disastrous results such as the system halt and the data loss. Even during the normal operation, the tuple space may turn out to be a bottleneck to the agent communication. To overcome these shortcomings of the single tuple space architecture, there have been research investigations to organize multiple tuple spaces in a system. However, the multiple tuple space architectures introduce other complexities that are not within the scope of the single tuple space, e.g. how to isolate the failures of tuple spaces, how to deal with replicated tuples, etc.
 tecture consisting of multiple tuple spaces. Besides being able to overcome the limitations of the single tuple space architecture, e.g. one X  X oint failure and the bottleneck, the federation is able to detect and isolate the failures of the tuple spaces in it, and keep the consistency among replicated tuples. For the fault detection of the tuple spaces, we extend the Adaptive Distributed System level Diagnosis (ADSD) algorithm [3], which was originally devised for the fault di-agnosis in arbitrary networks. For the consistency of duplicated copies of tuples, a global tuple , a new tuple type is proposed.
 of the tuple space and the ADSD algorithm. We then discuss the fault tolerance capability of the federation as well as the tuple replication schemes in Section 3. Section 4 presents the numerical analysis of the proposed federation, and Section 5 concludes this paper. 2.1 Tuple Space for Agent Communication Tuple space is a shared and associative virtual memory designed for commu-nication and synchronization of distributed processes [1]. It allows processes to communicate by writing tuples into tuple spaces and retrieving them as specified by templates . Tuples and templates are ordered sets of typed fields that can be either actual or formal . An actual field has a specific value, while a formal field represents a set of values. Tuples and templates do not have restrictions on how fields are composed.
 is to put a tuple into a tuple space; the read is to obtain a copy of a tuple that matches a specified template. The take is to extract a matched tuple from the tuple space, removing the matched tuple. In the tuple space, associativity is a key concept that enables to retrieve partly matched tuples.
 paces [2], and TSpaces [5]. These implementations differ in the variety of the extensions they provided. There have been research investigations to extend the tuple space model with multiple tuple spaces. Hierarchical arrangement of tuple spaces has been suggested by [6] [7]. The combination of flat and hierarchical arrangements has been described in [8]. However, these research efforts have not contemplated the issues how the failures of tuple spaces are tolerated and isolated, and how the replicated copies of tuples are controlled to meet the con-sistency requirement as well as the scalability. These shortcomings of the past research are addressed in this paper. 2.2 Adaptive Distributed System Level Diagnosis Algorithm This section introduces Adaptive Distributed System level Diagnosis (ADSD) algorithm[9] which was developed for detecting and diagnosing faulty nodes in an arbitrary network. In the ADSD algorithm, monitoring of a distributed system which consists of a set of nodes is executed in two separate steps; detection and dissemination , both of which are executed in a distributed way. First, in the detection step, nodes of a system test neighbor nodes one another periodically and then, in the dissemination step, pass on the test results to other nodes in the system.
 in which vertices correspond to nodes and directed edges represent testing rela-tionship, i.e. who tests whom [9]. The testing results are either faulty or fault-free and this information is dispersed through a dissemination tree , a virtual path which is dynamically formed with the system nodes. The root of this tree is a node that initiates the information dissemination. On receiving the information, nodes determine whether to update their local databases with this information and whether to relay it to next level nodes depending on the recentness of the disseminated information.
 ized schemes in terms of dynamic reconfigurability, parallel processing, and no need of synchronization, it has the limitations in dealing with the complexities introduced by the node failure or join during the dissemination, which will be addressed by the extensions suggested by this paper. We propose a federation , a scalable and fault tolerant multiple tuple space archi-tecture for mobile agent communication. As shown in Figure 1, the federation is a set of tuple spaces which provide mobile agents with the logical view of one large tuple space, mainly enabled by the replication of tuples and the management of failures of tuple spaces.
 spaces in it, thus being able to host a large number of mobile agents. The global tuple is a new type of tuple allowing its replication without loss of consistency. Moreover, with the global tuples, the federation can balance loads on tuple spaces; it can distribute frequently read X  X ccessed tuples to all other tuple spaces by declaring them as global tuples.
 spaces diagnose the failures of other tuple spaces by testing one another in a distributed way, and pass on the test results to neighbor tuple spaces without the need of a central observer, thus avoiding the vulnerability to the single point failure. The federation is flexible; tuple spaces can join and leave the federation at will and at arbitrary moments in time.
 messages are classified, depending on its purpose, into four categories: monitor-ing, dissemination, reconfiguration for monitoring, and reconfiguration for join. The details of the messages will be discussed in corresponding sections. provides large enough bandwidth to exchange messages among the tuple spaces without unbearable delay. Also, a tuple space can send and receive messages to/from any tuple spaces directly in a federation.
 3.1 Fault Tolerance of the Federation Tuple spaces in a federation test one another periodically to diagnose failures of tuple spaces and share the test results, which are faulty or fault X  X ree . Based upon these test outcomes, they determine how to manage the federation con-figuration and replicated tuples. These operations are executed by each tuple space in three discrete steps: detection , dissemination , and reconfiguration .To devise these steps, we modify the ADSD algorithm [3] and also make some ex-tensions so that it can cope with complexities which were not considered in the original work.
 Failure Detection. Failure detection in a federation is accomplished by each tuple space; a tuple space periodically tests exactly one other tuple space by sending test-msg , and also it is tested by only one another tuple space. It decides the failure by timing out on test-ack , a response to the test. The testing relationship among tuple spaces is designed to form a ring. To construct the ring topology, we assign each tuple space with a federation X  X ide unique identifier which can be also ordered.
 Reconfiguration. Whenever a tuple space is removed from, or added to the federation, its ring topology for testing is reconfigured to maintain its circular structure. This reconfiguration process is initiated by an orphan , a tuple space with no neighbor tuple space testing it. A tuple space becomes the orphan: i) when the tuple space testing it becomes faulty, resulting in being unable to test, ii) when its tester quits monitoring in order to test another tuple space, and iii) by default, a new or repaired tuple space remains an orphan until it joins the federation successfully.
 reconfiguration process by which it is able to join the federation. This process is started when the orphans send request-monitoring or request-join to any of fault X  X ree tuple spaces in the federation; the former message is used by the ones of which tester became faulty or changed its monitoring target, whereas the second one is used by new or repaired tuple spaces.
 mines whether to allow the join by executing the reconfiguration algorithm. To make this decision, the tuple space compares T req , the identifier of the tuple space that sent the request with T id and T tested , where T id is its own identi-fier, T tested is that of the tuple space it is testing at the moment. The order relationship between these identifiers allows the join of the tuple space T req . By this relationship, the ring topology can be maintained after the join of the tuple space T req .
 Dissemination. Dissemination enables tuple spaces in a federation to share among them the events such as failures, joins, etc. As shown in Figure 2, this dissemination is performed through a binary tree representing a virtual channel through which the event is spread to all the tuple spaces. The binary tree is composed dynamically on the spot. The root of the tree becomes the first tuple space that detects an event. The dissemination continues along the binary tree until all terminal nodes receive the event. In Figure 2, the tuple space TS 1is the one which has an event to disseminate.
 of tuple spaces, ii) joins of new or repaired tuple spaces, and iii) updates on global tuples, e.g. addition of new global tuples, deletion of existing ones. The global tuples will be discussed later.
 At first, a tuple space that initiates the dissemination makes List all , a list of all fault X  X ree tuple spaces at the moment. Then it splits List all into two approxi-mately same-size sub X  X ists, List lef t and List right , corresponding to the left and right sub X  X rees respectively. From each sub-list, one tuple space is picked as the root of the sub X  X rees, in this case T lef t and T right . Then info-msg conveying the event information is transmitted only to T lef t and T right . Of the parameters composing the info-msg , there are three parameters related with the dissemi-nation: i) the event, ii) List lef t or List right depending on whether this message is sent to T lef t or T right , iii) the list of all fault X  X ree tuple spaces to which this event should be delivered. On receiving the info-msg , T lef t and T right relay the message to others by repeating the above steps except that, in this repetition, List all is replaced by List lef t in the case of T lef t ,or List right if it is T right . Local Database Update. On receiving events through the dissemination, tu-ple spaces determine if it is necessary to update their own local database with the events. In the local database, each tuple space keeps the status of other tuple spaces as entries each of which represents one corresponding tuple space. An en-try is composed of two fields: ID ts , a tuple space identifier and Int status , status integer , which has a positive-value and will be discussed shortly. The update is determined if the following two conditions are met: i) the event is about a failure or join and ii) it is not yet reflected on the databases.
 Odd values of the status integer represent the fault X  X ree status of an associated tuple space, whereas even values mean the faulty status. The status integer of a tuple space is initialized to 1 when the tuple space joins a federation for the first time, and increments by one whenever the tuple space changes its status: faulty or fault-free. By the ever increasing nature of the status integer, it is also feasible to determine the recentness of events by comparing the status integers, i.e. the event with bigger status integer is more recent than the other. a failure or join event carried by Msg ,an info-msg . Of the fields in the Msg , there are two fields, ID ts and Int status , describing this event, the same format as used by the database entries. The update process starts by retrieving from the Msg , T id the tuple space identifier stored in the ID ts field. Then a database entry matching T id is searched and, if found, the associated Status db the status integer, is fetched. Finally, by comparing Status db with Status msg a status integer from the Msg , the received event is classified into new , same ,or old for the update decision of the database.
 the corresponding entry, or no database entry matching the event is found. For this new event, tuple spaces update the database entry with the event, or cre-ate a new entry if no entry exists. After the update, they continue to relay this event to next level along the binary tree by calling DISSEMINATE described in Figure 3. corresponding entry. With this same event, tuple spaces need not update the entry, however continue to pass on the event to next level by DISSEMINATE . matching entry. By this old event, tuple spaces neither update the database entry nor continue the dissemination. They just discard the event without relaying it. 3.2 Extensions to the ADSD Algorithm The original ADSD algorithm is not able to deal with neither the failures nor joins during the dissemination. In this section, we introduce late acknowledge-ment and auditor mechanisms to remedy these drawbacks of the original work. Late Acknowledgement. It ensures that, once event dissemination is started, all the fault-free tuple spaces along the binary tree receive the events even if some of the tuple spaces fail during the dissemination. Moreover, the late ac-knowledgement is able to detect these failures by timeout.
 parent in the binary tree in order to notify that it received an event relayed from the parent. The late acknowledgement is that a tuple space sends this info-ack late , i.e. after it confirms that its children tuple spaces in the binary tree received the event. This confirmation is accomplished by the info-ack from the children. The opposite to the late acknowledgement is early acknowledgement by which a tuple space acknowledges its parent without this confirmation, i.e. on receiving an event, it first sends info-ack to its parent, and then relays the event to next level children.
 problem of the early acknowledgement. In Figure 2, the tuple space TS 1 begins to disseminate an event to two sub X  X rees each of which consists of ( TS 2 ,TS 3) and ( TS 4 ,TS 5 ,TS 6) respectively. With the early acknowledgement, if the tuple space TS 2 fails at the moment which is after sending info-ack to TS 1 but before relaying the event to TS 3, this failure leads to the situation where TS 3 never receives the event, however such inconsistency is never detected. late acknowledgement is able to prevent this inconsistency. Note that, by us-ing the late acknowledgement in Figure 2, TS 2 waits until it receives info-ack from TS 3 before sending the acknowledgement to TS 1. In the diagram, all pos-sible moments at which tuple space TS 2 may fail are labeled from F ailure 1 to F ailure 5 .
 incur the inconsistency. First of all, F ailure 1 does not affect the dissemination because it is prior to the event reception. F ailure 2 to F ailure 4 which occur in the middle of the dissemination are also safe because these moments are before tuple space TS 2 sends info-ack to TS 1, thus TS 1 is able to notice the dissemination failure by timing out the acknowledgement expected from TS 2. Finally, F ailure 5 turns out to be harmless because it is after the completion of the dissemination, i.e. after TS 3 received the event and acknowledged it. Auditor. Another inconsistency prevention mechanism is auditor , which is de-vised to ensure the consistency of newly joining tuple spaces. We first illustrate the inconsistency problem that may incur when the auditor mechanism is not in use. In Figure 2, we assume a scenario where a new tuple space TS new asks TS 1 to allow the join right after TS 1 began to disseminate an event. Then TS new is never able to receive or recognize this event because TS 1 had no information about the existence of TS new at the moment of building the binary tree for the event dissemination.
 works as follows; whenever a tuple space receives an event through the dissem-ination, it ensures that T tested the tuple space that it is testing at the moment receives the same event. To accomplish this, it searches the identifier of T tested in the third parameter, a list of all fault X  X ree tuple spaces to which the event is intended to be delivered. This list is one of the fields of info-msg as described in Figure 3.
 similar way to the proof of the late acknowledgement by using a space X  X ime diagram. However, the diagram and the proof are omitted in this paper because of the length restriction. 3.3 Scalability and Fault Tolerance of Tuples Concerning the consistency problem associated with the tuple replication, we first introduce global tuples , a new type of tuples which are allowed to be dupli-cated, while we use the term local tuple for the tuples that are not replicated. Once a tuple is declared as the global tuple, it can be duplicated to all other fault X  X ree tuple spaces in a federation. The duplication proceeds in the same way as the event dissemination; a global tuple is delivered as an event to tuple spaces along the binary tree.
 eration, but also new tuple spaces. Once joining a federation, these new tuple spaces populate their spaces with the global tuples which are provided by the tuple spaces that permitted their join.
 are replicated on multiple tuple spaces, any change on a global tuple should be reflected to all of its copies. We approach this consistency complexity by assigning mobile agents or tuple spaces with exclusive authority of modifying global tuples. We classify the global tuples into two subtypes, agent X  X xclusive and tuplespace X  X xclusive , based upon which entity has the exclusive right to modify those tuples. The agent X  X xclusive tuple is the one that only one associated agent is allowed to modify, whereas the read is allowed to all agents. In a similar way, the tuplespace X  X xclusive ones can be modified only at a designated tuple space, while read X  X ccess is possible at any tuple spaces. The changes on global tuples, either agent X  X xclusive or tuplespace X  X xclusive, are disseminated as events to all other tuple spaces in the same way as the dissemination.
 Delegation of Exclusiveness. The exclusive authority for tuple modifica-tion can be delegated; an agent can become entitled with authority for agent X  exclusive tuples, and so does a tuple space with tuplespace X  X xclusive ones. This delegation capability is necessary not only for the case of failures, but also other cases, e.g. an agent which soon terminates may request that the global tuples authorized to it should be delegated to another agent. Another example is that a tuple space authorized with a large number of global tuples may request the delegation to reduce the load, which is primarily incurred by the modification requests on those tuples.
 or tuple spaces possessing those authorities. Particularly, for the delegation in the case of failures, one of fault X  X ree agents in a federation is selected to initiate the delegation of exclusive rights owned by the failed agents or tuple spaces. We compare the performance of the federation numerically with the correspond-ing results of a traditional central observer scheme in terms of the maximum number of monitoring messages on one tuple space, required time for dissemina-tion, and the maximum elapsed time from fault detection until the completion of its dissemination. The central observer scheme is that all the operations, i.e. detection, dissemination, and reconfiguration, are managed in a centralized way by one designated object, called central observer .
 ing messages per tuple space as the group size increases. In the case of federation, the number of monitoring messages remains as constant regardless of the group size, whereas the central observer needs the maximum 2  X  N message exchange to monitor all tuple spaces in a group, where N is the group size. increases. In the case of the federation, the binary tree enables the dissemination to become parallel gradually as it proceeds down the tree, thus the time is logarithmical proportion to the group size. With u the time for one message transmission, and N the group size, the central observer requires the time 2  X  N  X  u until the dissemination finishes, while the federation needs the time 2  X  logN  X  u . detection to its completion of the dissemination. Since the central observer tests one tuple space at a time, in the worst case, it can take the time N  X  m to detect a failure, where m is the monitoring interval. Thus the maximum elapsed time becomes N  X  m +2  X  ( N  X  1)  X  u , where 2  X  ( N  X  1)  X  u is the time for dissemination as described above, resulting in the time complexity of O ( n 2 ). In the case of the federation, regardless of the group size, the worst case time for a failure detection becomes the time m and the dissemination takes the time 2  X  log ( N  X  1)  X  u , thus the total elapsed time becoming m +2  X  log ( N  X  1)  X  u , with its time complexity O ( logN ). This paper proposed the federation, a scalable and fault tolerant architecture consisting of multiple tuple spaces, for the communication of mobile agents. We improved the original ADSD algorithm with the mechanisms such as the late acknowledgement and the auditor; with the former mechanism, failures during the dissemination can be tolerated, whereas the latter one allows tuple spaces to join a federation without the loss of consistency. For the scalability, the federation allows tuples to be replicated to all the tuple spaces in it by the use of the global tuple concept without loss of consistency. The results of the numerical analysis of the proposed federation show that the performance of the federation is superior to the conventional central observer model.

