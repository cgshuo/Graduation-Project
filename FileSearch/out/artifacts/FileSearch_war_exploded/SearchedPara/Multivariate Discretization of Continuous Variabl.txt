 Man y algorithms in data mining can b e formulated as a set mining problem where the goal is to nd conjunctions (or disjunctions) of terms that meet user sp eci ed constraints. Set mining techniques have b een largely designed for cat-egorical or discrete data where variables can only tak eon a xed num berofv alues. However, many data sets also contain continuous variables and a common metho d of deal-ing with these is to discretize them by breaking them into ranges. Most discretization metho ds are univ ariate and con-sider only a single feature at a time (sometimes in con-junction with the class variable). W e argue that this is a sub-optimal approach for knowledge discovery as univari-ate discretization can destroy hidden patterns in data. Dis-cretization should consider the e ects on all variables in the analysis and that two regions X and Y should only b e in the same cell after discretization if the instances in those regions have similar multivariate distributio ns ( F variables and combinations of variables. W e presen tabot-tom up merging algorithm to discretize continuous variables based on this rule. Our exp eriments indicate that the ap-proach is feasible, that it do es not destroy hidden patterns and that it generates meaningful intervals.
 H.2.8 [ Database Managemen t ]: Database Applications| In set mining the goal is nd conjunctions (or disjunctions) of terms that meet all user sp eci ed constraints. For ex-ample, in Asso ciation Rule Mining [1] a common step is to nd all itemsets that have supp ort greater than a thresh-old. Set mining is a fundamental op eration of data mining. In addition to asso ciation rule mining, many other large classes of algorithms can b e formulated as set mining such as classi catio n rules [14] where the goal is to nd sets of attribute-value (A-V) pairs with high predictiv epo wer, or contrast set mining [4, 5] where the goal is to nd sets that represent large di erences in the probabili ty distributions of two or more groups.
 There has b een muc hw ork devoted to sp eeding up search in set mining [6, 19] and there are many ecient algorithms when all of the data is discrete or categorical. The problem is that data is not always discrete and is typically a mix of discrete and continuous variables. A central problem for set mining and one that we address in this pap er is \How should continuous values b e handled?" The most common approach is to discretize them into disjoint regions and then apply the set mining algorithm.
 Past work on discretization has usually b een done in a clas-si cation context where the goal is to maximize predictive accuracy. For example, discretizing continuous attributes for the naiv eBa yesian classi er can greatly improve accu-racy over a normal approximation [8]. In Knowledge Dis-covery we often analyze the data in an exploratory fashion where the emphasis is not on predictive accuracy but rather on nding previously unknown and insightful patterns in the data. Thus we feel that the criteria for cho osing intervals should b e di erent from this predictive context as follows: In addition, there is the obvious requirement that the metho d should b e fast enough to handle large databases of interest. W e feel that one metho d of addressing these p oints is to
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 use multivariate as opp osed to univariate discretization . In m ultiv ariate discretization one considers ho w all the v ari-ables in teract b efore deciding on discretized in terv als. In con trast, univ ariate approac hes only consider a single v ari-able at a time (sometimes in conjunction with the class). W e presen t a simple motiv ating example. Consider the prob-lem of set mining on X OR data as in Figure 1a. Clearly one should discretize the data as in Figure 1b whic h is the re-sult of the metho d w e are prop osing in this pap er. Ho w ev er algorithms that do not consider more than one feature will fail. F or example, F a yy ad and Irani's recursiv e minim um en trop y approac h [9] will not realize that there is an in ter-action b et w een X X Our approac h to this problem is to nely partition eac h con-tin uous attribute in to n basic regions and then to iterativ ely merge adjacen tin terv als only when the instances in those in-terv als ha v e similar distributions. That is, giv en in terv als X and Y w e merge them if F test of di erences to c hec k this. Merging allo ws us to deal with the resolution problem and it automatically determines the n um ber of in terv als. Our m ultiv ariate test means that w e will only merge cells with similar distributio ns so hidden patterns are not destro y ed and the regions are coheren t. The literature on discretization is v ast but most algorithms are univariate in that they consider eac h feature indep en-den tly (or only join tly with a class v ariable) and do not con-sider in teractions with other features. F or example, F a yy ad and Irani [9] recursiv ely split an attribute to minimize the class en trop y . They use a minim um description length cri-terion to determine when to stop. Doughert y , Koha vi and Sahami [8] pro vide a go o d o v erview of man y algorithms in this category .Asw e men tioned previously , these approac hes can miss in teractions of sev eral v ariables.
 Srik an t and Agra w al [17] prop osed an approac h that w ould a v oid this limitation. They nely divide eac h attribute in to n basic in terv als and then attempt to consider all p ossi-ble com binations of consecutiv e basic in terv als. Ho w ev er, this creates t w o problems they refer to as Exe cTime and ManyR ules . The ExecTime problem is that since there are O ( n 2 ) com binations of in terv als for eac h feature the com-plexit y will \blo w up" esp ecially when w e consider the in-teractions with other features. The Man yRules problem is also related to the n um b er of com binations. If an in terv al meets the minim um supp ort requiremen t sodoesan y range con taining the in terv al; e.g., if age[20,30] meets the mini-m um supp ort constrain ts then so will age[20,31], age[20,40] and so on. This can result in a h uge n um b er of rules. Miller and Y ang [13] note that Srik an t and Agra w al's solu-tion ma y com bine ranges that are not meaningful and can result in unin tuitiv e groupings. They presen t an alternativ e approac h based on clustering the data and then building asso ciation rules treating the clusters as frequen t itemsets. Am ultiv ariate test of di erences tak es as input instances dra wn from t w o probabilit y distributio ns and determines if the distribution s are equiv alen t. In statistical terms the n ull h yp othesis H is that the t w o distributions are di eren t F section, w e review past approac hes and discuss wh y they are inappropriate for our application . W e argue for a new test based on recen tw ork in con trast set mining [4, 5]. With a single dimension, w e can use the Kolmogoro v-Smirnov (K-S) t w o sample test or the W ald-W olfo witz (W-W) runs test [7] to c hec k for di erences. These metho ds sort the ex-amples and compute statistics based on the ranks of sorted mem b ers in the list. F or example, the K-S test lo oks at the maxim um absolute di erence in the cum ulativ e distribution functions. The W ald-W olfo witz test uses the total n um ber of runs R , where a run is a set of consecutiv e instances with iden tical lab els. H The problem with these metho ds is that the notion of a sorted list do es not apply in m ultiv ariate data. Th us in their basic form the K-S and W-W tests are not useful for our problems. Ho w ev er, F riedman and Rafsky [10] generalized the notion of a sorted list b y using a minim um spanning tree (MST). They use order information in the MST to calculate m ultiv ariate generalizations of K-S and the W-W tests. F or the K-S v arian t, they use a heigh t directed preorder tra v ersal of the tree (visit subtrees in ascending order of their heigh t) to de ne a total order on no des in the tree. F or the W-W test the m ultiv ariate generalization is to remo v e all edges in the tree that ha v e di eren t lab els for the de ning no des and let R b e the n um b er of disjoin t subtrees.
 Unfortunately , the MST based test has a n um b er of disad-v an tages: First, the generation of the MST requires pairwise distance measures b et w een all instances. In data mining, v ariables can b e b oth con tin uous and discrete th us dev el-oping a distance metric is not straigh tforw ard. An y MST dev elop ed will b e sensitiv e to the metric used. Second, the MST is exp ensiv e to nd. Using Prim's algorithm it is O ( N 2 ) [16] for N data instances. F or our data sets N is usually v ery large th us making the complexit y prohibitive. Finally , the ab o v e tests w ere designed to measure signi -cance and ha v e no meaningful in terpretation as a measure of the size of the di erences b et w een the t w o distribution s. F or example, one cannot relate c hanges in the test statistic (i.e. di erence in cum ulativ e distributio n function, distribu-tion of runs) to meaningful di erences in underlying analysis v ariables suc h as age or o ccupation. Additionall y , signif-icance b y itself is not sucien t [2] b ecause as N !1 all di erences, no matter ho w small, will sho w up as signi can t. W e prop ose using an alternate test of di erences based on Con trast Set miners suc h as STUCCO [4, 5]. STUCCO at-tempts to nd large di erences b et w een t w o probabilit y dis-tributions based on observ ational data. F or example, giv en census data if w e compare PhD and Bac helor's degree hold-ers, STUCCO w ould return di erences b et w een their distri-butions suc h as: P( o ccupation = sales j PhD ) = 2.7%, while P( o ccupation = sales j Bachelo r ) = 15.8%.
 The mining ob jectiv es of STUCCO can b e stated as follo ws: Giv en t w o groups of instances G tions of attribute v alue pairs C (con trast sets) suc h that: Supp ort is a frequency measuremen t and is the p ercen tage of examples where C is true for the giv en group. Equation 1 is a signi cance criterion and ensures that the di erences w e nd could not b e explained b y uctuations in random sampling. W e test this b y using a c hi-square test whic hm ust reject indep endence of group mem b ership and probabilit y of C . Equation 2 is a size criterion and estimates ho w big the di erence is b et w een t w o distributions . W e require the minim um di erence in supp ort to b e greater than . STUCCO nds these con trast sets using searc h. It uses a set en umeration tree [15] to organize the searc h and it uses man y of the tec hniques in [6, 19] suc h as dynamic ordering of searc h op erators, candidate groups and supp ort b ounds in conjunction with pruning rules geared for nding supp ort di erences. STUCCO also carefully con trols error caused b y m ultiple h yp othesis testing (i.e., false p ositiv es). W e use STUCCO as a m ultiv ariate test of di erences as fol-lo ws. If STUCCO nds an y C that meets b oth constrain ts then w esa y that F erwise w esa y that F Giv en our test from the previous section, w eno w presen t our algorithm for MultiV ariate Discretization (MVD): 1. Finely partition all con tin uous attributes in to n basic 2. Select t w o adjacen tin terv als X and Y that ha v e the 3. If F 4. If there are no eligibl e in terv als stop. Otherwise go to W e test if F that fall in X and Y form the t w o groups whose distri-butions w e compare. STUCCO requires that w e sp ecify whic h represen ts ho w big a di erence w e are willing to tolerate b et w een t w o distributions . This allo ws us to con-trol the merging pro cess: small means more in terv als and large means few er in terv als. W e set adaptiv ely accord-ing to the supp ort of X and Y so that an y di erence b e-t w een the t w o cells m ust b e larger than a xed p ercen tage of the en tire dataset. F or example, if w e tolerate di er-ences of size up to 1% of the en tire distribution then w e set =0 : 01 = min f sup ( X ) ; sup ( Y ) g . F or eac h con tin uous feature, MVD ma y call STUCCO up to n 1 times where n is the n um b er of basic in terv als. Eac h in v o cation of STUCCO ma y require an ev aluation of an ex-p onen tial n um b er of candidates (i.e. all com binations of attribute-v alue pairs) and up to j A j passes through database. This b egs the question of ho ww e can implemen t MVD e-cien tly when it calls a p oten tially exp ensiv e mining routine. W e b eliev e that it will run ecien tly b ecause of the follo w-ing reasons: First, although the w orst case running time is exp onen tial, in practice STUCCO runs ecien tly on man y datasets [5]. Second, the problems passed o to STUCCO are often easier than that faced b y the main mining pro-gram. STUCCO only needs to consider the examples that fall in to the t w o ranges that are b eing tested for merging. This can b e only a small fraction of the data set. Third, STUCCO only needs to nd a single di erence b et w een the groups and then it can exit. It do es not need to nd all di erences. Finally , calling STUCCO rep eatedly will result in man y passes o v er the database. Ho w ev er the limiting factor for mining algorithms is the exp onen tial n um ber of candidates that need to b e considered, not passes through database (e.g. [6]). W e test the abilit y of MVD to prop erly discretize data with hidden patterns in high dimensional data b y de ning a prob-lem called P arit y R + I . This is a con tin uous v ersion of the parit y problem where there are R con tin uous v ariables rang-ing from [-0.5,0.5] with a uniform distributio n, one con tin-uous irrelev an tv ariable also ranging from [-0.5,0.5], and a b o olean class v ariable. If an ev en n um b er of the rst R features are p ositiv e then the class v ariable is 1; 0 other-wise. W e then added 25% class noise and generated 10000 examples from this distribution .
 W e used MVD with equal frequency partitionin g (100 exam-ples p er division) on the P arit y5+ I problem. This problem is dicult b ecause there is a hidden 6 dimensional relation-ship and with our initial partitioning of features w eha v e only 10000 instances to b e divided in to 100 6 2 p ossible cells. W e ran v e trials and T able 1 sho ws the cutp oin ts w e found for eac h feature (MVD found at most 1 cutp oin tper feature). The true solution is [0,0,0,0,0,ignore]. MVD did v ery w ell at iden tifying the relationshi p b et w een F1, ::: ,F5 and the class. Although it did not exactly repro duce the desired cutp oin ts, it came reasonably close, and a set miner should still b e able to iden tify the parit y relationshi p. MVD failed only once out of the v e trials and it alw a ys managed to iden tify the irrelev an tv ariable. In con trast, univ ariate discretizers will only b e able to solv eP arit y1+ I problems. In this section, w e sho w that our approac h is ecien t and can b e used in set mining without a large o v erhead on the en tire pro cess. W e also demonstrate b y example that our approac h generates meaningful results.
 F or our exp erimen ts w e compared MVD with F a yy ad and Irani's recursiv e minim um en trop y approac h with the MDL stopping criteria. W e refer to this as ME-MDL and w e used the MLC++ [12] implemen tation of this discretizer. P ast w ork has sho wn that ME-MDL is one of the b est metho ds for classi cation [8, 11]. W e also compared our execution times with Apriori to giv e an indication of ho wm uc h time discretization tak es relativ e to the set-mining pro cess. W e used C. Borgelt's implemen tation of Apriori (in C). 1 W e ran exp erimen ts on v e databases whic h are summarized in T able 2. The rst four databases are publicall y a v ail-able from the UCI KDD Arc hiv e [3]. The UCI Admissions dataset represen ts all undergraduate studen t application s to UCI for the y ears 1993-1999. The data con tains v ariables suc h as ethnicit y ,sc ho ol (e.g. Arts, Engineering, etc.), if an o er of admission w as made, GP A, SA T, etc.
 W e ran all exp erimen ts on a Sun Ultra-5 with 128 MB of RAM. W e used the follo wing parameter settings: The ba-sic in terv als w ere set with equal frequency partitioning with 100 instances p er in terv al for Adult, SatImage, and Sh uttle, 2000 p er in terv al for UCI Admissions, and 10000 p er in-terv al for Census-Income. W e required di erences b et w een adjacen t cells to b e at least 1% of N . ME-MDL requires a class v ariable and for Adult, Census-Income, SatImage, and Sh uttle w e used the class v ariable that had b een used in previous analyses. F or UCI Admissions w e used Admit = f y es,no g (i.e., w as the studen t admitted to UCI). T able 3 sho ws the discretization time for MVD, ME-MDL and the time tak en b y Apriori to p erform frequen t set mining on MVD's discretizations. MVD's time w as generally com-parable to ME-MDL. Both discretization pro cesses usually to ok longer than than Apriori but they w ere not excessiv ely slo w. Census-Income w as exceptionall y dicult for Apriori whic h ran out of memory and could not mine frequen t item-sets at 10% supp ort. W e tried mining with 30% supp ort but ev en at this lev el Apriori could not complete mining in reasonable time and w e stopp ed it after 10 CPU hours. a v ailable from http://fuzzy .cs.Uni -Magdeburg.de/ bo rgel t/ W e b eliev e that our approac h of com bining ranges only when they ha v e similar distributio ns will lead to seman tically mean-ingful in terv als. W e demonstrate this b y comparing the in-terv als formed b y MVD with ME-MDL.
 F or sev eral v ariables MVD and ME-MDL obtained similar discretizations. F or example, Figures 2a and 2b sho w the discretization b oundaries for the age v ariable in the Adult dataset sup erimp osed on top of it's histogram. The in terv als for b oth MVD and ME-MDL are narro waty ounger age ranges and wider at older ranges. This mak es in tuitiv e sense as the data represen ts man y emplo ymen t related v ariables and p eople's careers tend to c hange less as they age. Ho w ev er, MVD and ME-MDL di ered signi can tl y on man y other v ariables. Figures 3a and 3b sho w the discretiza-tions b oundaries found for paren tal income on UCI Admis-sions data. Note that w e plotted the logarithm of the in-come for visualizatio n purp oses only; w e did not transform the v ariables b efore discretization. The MVD cutp oin ts o ccur at meaningful lo cations: f  X  17000,  X  30000,  X  51760, 75000 g .W e can easily relate these to our notions of p o v ert y and w ealth. The ME-MDL discretization is not meaningful. Consider that it group ed ev eryb o dy with paren tal income bet w een  X  36K and  X  200K together. Additional ly , ME-MDL had man y cutp oin ts distinguis hi ng applican ts at the upp er range of paren tal income (i.e. o v er  X  400K).
 Another problem with using ME-MDL in a disco v ery con-text is that it requires a class v ariable and the discr etization is sensitive to this . While b eing sensitiv e to the class v ari-able is probably go o d in a classi cation con text, it is not for disco v ery . Stable results are essen tial for domain users to accept the disco v ered kno wledge [18]. W e discretized UCI Admissions data with ME-MDL using sex and y ear as alter-nate class v ariables and w e found wildly di eren t cutp oin ts. Using sex pro duced income cutp oin ts at f  X  55K,  X  162K, 390K g and using y ear pro duced cutp oin ts at f  X  13K,  X  Figures 2c and 2d sho w the discretization cutp oin ts found for Capital-Loss on Adult. In this case, most data analysts w ould probably agree with MVD as opp osed to ME-MDL. W e ran Apriori using b oth MVD and ME-MDL's b oundaries for capital-loss (using MVD's discretization for all other v ari-ables) and found that the p o or cutp oin ts c hosen b y ME-MDL can hide imp ortan t asso ciation rules. F or example, with MVD's cutp oin tw ew ere able to nd rules suc has capital-loss  X  155 ! salary &gt;  X  50K (supp ort 2.3%, con -dence 50.1%). This rule states that declaring a capital-loss is strongly predictiv eofha ving a high salary . The base rate for salary &gt;  X  50K is 24% so the rule iden ti es a group with t wice this probabilit y .W e did not nd an y similar rules with ME-MDL's discretization b ecause it used to o man y parti-tions making it dicult for Apriori to infer relationships. Income and Capital-Loss are v ery sk ew ed v ariables, ho w ev er, ME-MDL can also pro duce unin tuitiv e b oundary p oin ts for more normal data. Figures 3c and 3d sho w the discretization b oundaries found for GP A on UCI Admissions.
 These results suggest that p erhaps ME-MDL is not appro-priate for kno wledge disco v ery when the goal is understand-ing the data. ME-MDL can generate to o man yin terv als that are close together and are at o dd lo cations. In MVD our similarit y test prev en ts this from happ ening b ecause it requires that adjacen tin terv als not only b e di eren t but also b e di eren tb y a certain minim um size. W e presen ted a m ultiv ariate discretization algorithm that nely partitions con tin uous v ariables and then merges adja-cen tin terv als only if their instances ha v e similar m ultiv ari-ate distributions . Merging allo ws us to automatically deter-mine an appropriate resolution to quan tize the data. Our m ultiv ariate test ensures that only similar distributio ns are joined. Our exp erimen tal results on syn thetic data indicate that our algorithm can detect high dimensional in teractions bet w een features and discretize the data appropriately .On real data our algorithm ran in time comparable to a p opu-lar univ ariate recursiv e approac h and pro duced sensible dis-cretization cutp oin ts. This researc hw as funded in part b y the National Science F oundation gran t IRI-9713990. [1] R. Agra w al, T. Imielinski, and A. Sw ami. Mining [2] D. Bak an. The test of signi cance in psyc hological [3] S. D. Ba y . The UCI KDD arc hiv e. Irvine, CA: [4] S. D. Ba y and M. J. P azzani. Detecting c hange in [5] S. D. Ba y and M. J. P azzani. Detecting group [6] R. J. Ba y ardo. Ecien tly mining long patterns from [7] W. J. Cono v er. Pr actic al Nonp ar ametric Statistics . [8] J. Doughert y , R. Koha vi, and M. Sahami. Sup ervised [9] U. M. F a yy ad and K. B. Irani. Multi-in terv al [10] J. H. F riedman and L. C. Rafsky . Multiv ariate [11] R. Koha vi and M. Sahami. Error-based and [12] R. Koha vi, D. Sommer eld, and J. Doughert y . Data [13] R. J. Miller and Y. Y ang. Asso ciation rules o v er [14] J. R. Quinlan. C4.5 pr o gr ams for machine le arning . [15] R. Rymon. Searc h through systematic set [16] R. Sedgewic k. A lgorithms in C . Addison-W esley , 1990. [17] R. Srik an t and R. Agra w al. Mining quan titativ e [18] P .T urney .T ec hnical note: Bias and the quan ti cation [19] G. I. W ebb. OPUS: An ecien t admissible algorithm
