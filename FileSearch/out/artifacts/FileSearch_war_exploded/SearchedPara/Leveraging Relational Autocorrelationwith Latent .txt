 The presence of autocorrelation provides strong motivation for us-ing relational techniques for learning and inference. Autocorrela-tion is a statistical dependency between the values of the same vari-able on related entities and is a nearly ubiquitous characteristic of relational data sets. Recent research has explored the use of collec-tive inference techniques to exploit this phenomenon. These tech-niques achieve significant performance gains by modeling observed correlations among class labels of related instances, but the mod-els fail to capture a frequent cause of autocorrelation X  X he presence of underlying groups that influence the attributes on a set of enti-ties. We propose a latent group model (LGM) for relational data, which discovers and exploits the hidden structures responsible for the observed autocorrelation among class labels. Modeling the la-tent group structure improves model performance, increases infer-ence efficiency, and enhances our understanding of the datasets. We evaluate performance on three relational classification tasks and show that LGM outperforms models that ignore latent group struc-ture, particularly when there is little information with which to seed inference.
Autocorrelation is a statistical dependency between the values of the same variable on related entities, which is a nearly ubiq-uitous characteristic of relational datasets. For example, hyper-linked web pages are more likely to share the same topic than ran-domly selected pages [21], and movies made by the same studio are more likely to have similar box-office returns than randomly selected movies [7]. More formally, autocorrelation is defined with respect to a set of related instance pairs ( z i , z j )  X  Z relation between the values of a variable X on the instance pairs ( z .x, z j .x ).

Despite the challenges to learning, the presence of autocorrela-tion offers a unique opportunity to improve model performance, as autocorrelation enables inferences about one object to be used to improve inferences about related objects. Indeed, recent work in relational domains has shown that collective inference over an en-tire dataset results in more accurate predictions than conditional in-ference over each instance independently [2, 21] and that the gains over conditional models widen as autocorrelation increases [8].
Collective inference techniques exploit autocorrelation by rea-soning with collective models that represent the dependencies among class labels of related instances. Collective models have lower variance than models that represent autocorrelation dependencies indirectly through dependencies on other attributes of related in-stances [8]. Collective inference techniques use these low variance models to propagate information throughout the dataset and im-prove the overall set of predictions.

Modeling the correlations among class labels directly, however, fails to capture a frequent cause of autocorrelation X  X he presence of underlying groups, conditions, or events that influence the at-tributes on a set of entities. For example, in the cinematic domain, it is likely that studios cause the observed autocorrelation among movie returns. Movie-goers are unlikely to choose movies based on the success of other movies from the same studio. It is more likely that movie success is influenced by some unobserved prop-erties of the studio (e.g., advertising budget). In this case, the class labels of movies are conditionally independent given studio type (e.g., high-budget studio). Latent-variable models that represent the correlation of unobserved group properties (e.g., studio type) with attribute values (e.g., movie returns) may be able to express density functions more accurately and compactly than approaches that directly model the observed autocorrelation.

When the groups are observable (e.g., movies made by the same studio), we can model the latent structure with a relatively sim-ple application of the expectation-maximization (EM) algorithm. A similar approach is used in information retrieval where latent-unigram models represent each document as a group of word oc-currences 1 that are conditionally independent given the topic of the document [1]. In many relational datasets, however, group membership is unobserved and must be inferred from the relations and attributes. For example, the World Wide Web contains com-munities  X  X roups of hyperlinked pages with similar topics. Al-though we can not directly observe which community a web page belongs to, intra-community citations are more frequent than inter-community citations so hyperlinks are evidence of the underlying community structure. To continue the document retrieval metaphor, it is as if we have word occurrence information (attribute values) and noisy indicators of word co-occurrence within documents (link information) but we do not know the document boundaries or the topic distributions. In these situations, a joint model of attributes domains X  X e generally have less than 10 class values, whereas documents have thousands of unique words. and links is needed to recover group membership and infer latent group properties.

In this work, we propose a latent group model (LGM) for rela-tional data, which is a joint model of links, attributes, and groups for unipartite relational graphs. The model addresses a number of weaknesses of current collective models. First, collective models, which model autocorrelation dependencies directly, generally re-quire computationally-complex, approximate inference techniques because inference is over a large, cyclic graphical model. If, how-ever, the instances are conditionally independent given the under-lying group structure, then exact inference is not only feasible but much more efficient. Second, collective models typically restrict their representation to the dependencies among instances that are restriction constrains the space of possible dependencies to a rea-sonable size and prevents useful information from being drowned out by noise. However, modeling the dependencies among neigh-boring but unlinked instances (e.g., transitive relationships) allows information to propagate in a more elaborate manner during in-ference. Group models are a natural way to extend the represen-tation to improve model performance without fully representing the O ( N 2 ) dependencies between all pairs of instances. Finally, latent-variable models recover the underlying groups and identify their associated density functions. This attempt to model the true cause of autocorrelation will improve domain understanding and motivate development of additional modeling techniques.

Our initial evaluation of LGMs is on out-of-sample classifica-tion tasks. More specifically, we aim to learn LGMs of datasets where the attributes and links are fully observed, and group struc-ture is unobserved, and then apply the model to classify instances in new datasets, where the attributes are partially observed, links are fully observed and the groups are again unobserved. This approach is suited for domains with large, nearly disconnected graph struc-tures. For example, in gene prediction tasks, models of proteins and how they interact to perform certain functions in the cell can be learned in one genome and then applied to classify the proteins in new genomes. In addition, this approach is suited for dynamic network domains, where groups emerge and and disband over time. For example, fraud detection efforts usually analyze a single dataset that is evolving over time. The data contain demographic informa-tion about individuals and transactional links (e.g., bank deposits, telephone calls) that can indicate the underlying organizations. In these domains, LGMs could be used to detect group formation and use a few hand-labeled examples to seed inference about the clas-sifications of new group members.

In the remainder of the paper, we outline LGM, our initial al-gorithms for learning and inference, and related work in statistical relational learning. We present empirical evaluation on three classi-fication tasks to demonstrate the capabilities of the model, showing that LGMs perform better than models that ignore latent groups when there is little known information with which to seed infer-ence. Finally, we conclude with directions for future work.
Latent group models specify a generative probabilistic model for the attributes and link structure of a relational dataset. The model posits groups of objects in the data of various type. Membership in these groups influences the observed attributes of objects, as well as the existence of relationships (links) among objects.

For this initial investigation of LGMs, we make several simpli-of the O ( N 2 ) possible dependencies. fying assumptions about the data. More specifically, we assume a unipartite relational data graph (single object type) with binary, undirected links, and at most one link between any pair of objects. We also assume the number of objects, groups, and group types are fixed and known. However, it is relatively straightforward to extend the model to accommodate deviations from these assumptions.
The model assumes the following generative process for a dataset with N O objects and N G groups: 1. For each group g , 1  X  g  X  N G : 2. For each object i , 1  X  i  X  N O : 3. For each object j , 1  X  j  X  N O :
This generative model specifies that attribute values and link ex-istence are conditionally independent given group membership and type information. More specifically, the class labels of objects are conditionally independent.

The joint distribution of the dataset D , with groups N G N
O , and links L , is thus given by:
Y
See figure 1 for a graphical model representation of LGM. The plates represent replicates: N G groups; N O objects, each with M attributes; and ` N O 2  X  = N O ( N O  X  1) 2 possible binary links. The shaded nodes are observed variables: C is the class label, ject attributes, E is a binary variable indicating link existence. The unshaded nodes are unobserved variables: T is the group X  X  type, is the object X  X  group membership. The conditions on the arcs con-strain the manner in which the model is rolled out for inference 3  X  each E will be influenced by two G and two T variables and each C will be influenced by a single T variable in the unrolled Bayes net. The LGM model is similar to hierarchical Bayesian models but extended to a relational domain where the generative process is re-sponsible for generating both attributes and links. More specifically the model is a form of probabilistic relational model (PRM) [4] that combines a directed relational Bayesian network, link existence un-certainty, and hierarchical latent variables. 3 We use contingent Bayesian network [13] notation to denote context-specific independence.

Learning an LGM consists of learning the parameters of the dis-tribution and inferring the latent variables on both objects (group membership) and groups (group type). Ideally, we could learn the model using a straightforward application of the EM algorithm X  iterating between inferring the latent variables (E-step) and estimat-ing the parameters (M-step). Unfortunately, there are difficulties with this approach. First, there are N O latent group variables with N
G possible values and N G latent type variables with k possible values. When the average group size is small ( N G = O ( N expect that EM will be very sensitive to the start state. Further-more, the E-step requires that we run inference over a large, com-plex, rolled-out Bayes net where objects X  group memberships are all interdependent given the link observations. When group mem-bership is unknown, each of the C and E variables depends on T variables X  X here is no longer context-specific independence to exploit. Exact inference in this situation is impractical, although approximate inference techniques such as loopy belief propaga-tion or variational methods may allow accurate inference. How-ever, given the number of latent variables and their dependency on sparse link information ( L ` N O have many local (suboptimal) maxima and we expect that EM will not converge to a reasonable solution. Because collective models propagate information only on existing links, we expect the auto-correlated groups will have more intra-group links than inter-group links. We exploit this characteristic to decouple the group discov-ery from the remainder of the estimation process and propose the following approximate learning algorithm: 1. Hypothesize group membership for objects based on the ob-2. Use EM to infer group types and estimate the remaining pa-
A hard clustering approach, which assigns each object to a single group, greatly simplifies the estimation problem X  X e only need to estimate the latent group type variables and parameters of p ( C | T ) , and p ( A | C ) . To this end, we employ a recursive spectral decomposition algorithm with a norm-cut objective function [19] to cluster the objects into groups with high intra-group and low inter-group linkage.

Spectral clustering techniques partition data into disjoint clus-ters using the eigenstructure of a similarity matrix. We use the divisive, hierarchical clustering algorithm of [19] on the relational graph formed by the links in the data. The algorithm recursively partitions the graph as follows: Let E N  X  N = [ E ( i, j )] adjacency matrix and let D be an N  X  N diagonal matrix with d = P j  X  V E ( i, j ) . Solve the eigensystem ( D  X  E ) x =  X  Dx the eigenvector x 1 associated with the 2 nd smallest eigenvalue Consider m uniform values between the minimum and maximum value in x 1 . For each value m : bipartition the nodes into such that  X  v a  X  A x 1 a &lt; m , and calculate the NCut value for Partition the graph into the ( A, B ) with minimum NCut . If stabil-ity ( A, B )  X  c , recursively repartition A and B . 4
As we will show in section 4, the spectral clustering approach appears to work well in practice. However, refinements that iterate the clustering and EM steps, or incorporate soft clusterings, may improve results even further.
There are two ways to apply LGMs to relational data. First, given a dataset with observed attributes and links, we can use the model to cluster objects into groups with similar attribute values and pat-terns of linkage. Second, we can apply the model to a new unseen dataset with partially observed attributes and/or links to infer both the group memberships and the unobserved attributes/links. This approach exploits the underlying group structure to improve pre-dictions by jointly inferring the latent groups, their types, and the unknown attributes/links. Our initial investigation of LGMs has fo-cused primarily on this latter task to enable an objective comparison of LGMs with current, alternative techniques. In the experiments reported below, we learn the model on a dataset with fully observed attributes and links, then we apply the model to a new dataset with unknown class labels and fully observed links to jointly infer the group memberships and class labels. We designed our learning al-gorithm with this out-of-sample classification task in mind. When classifying a new dataset with partially observed attributes, we can cluster the objects into groups using the observed links and then use the learned model to jointly infer the group types and the unob-served attributes. There are two types of statistical relational models related to LGMs: those that represent joint distributions of attributes condi-tioned on link structure, and those that cluster objects into groups based on link and attribute structure.
The first category consists of models that represent a joint distri-bution of the attributes of set of instances. Relational Markov net-works (RMNs) [21] and relational dependency networks (RDNs) [14] model autocorrelation in a procedural fashion. RMNs use clique templates to model the pairwise correlations among class labels of related instances, whereas RDNs use aggregated features. These techniques model the autocorrelation dependencies at a global level X  X he autocorrelation dependencies are assumed to be uniform across each link in the data and parameters of features are tied across the entire dataset. As such, these models will not be able to distinguish among regions with varying levels of autocorrelation.
Probabilistic relational models (PRMs) [4] are also able to model autocorrelation relationships, but only if the autocorrelation can be structured to be acyclic (e.g., with temporal constraints). The use of latent variables in PRMs has been explored in limited set-tings where the groups are known and represented as objects in the data [22]. For example, in the cinematic domain we could use a PRM with a latent variable on studios to model the autocorrelation of movie returns. However, latent groups are not posited by the model and group variables are not conditioned on the observed link structure.
The second category consists of models that cluster relational data into groups. Popescul and Ungar [18] cluster relational database tables and use the cluster IDs as features in individual classification models that reason about each instance independently. This ap-proach has been shown to improve classification performance, but it can only be employed in situations where the test set instances link into the clusters used during training because the features use the identity of the clusters rather than generalizing over the proper-ties of the groups.

Kubica, Moore, Schneider and Yang [10] use a latent variable model that is a special case of our proposed model to cluster ob-jects into groups based on their attribute values and link structure. Their approach is geared toward clustering data with multiple trans-actional links (e.g., phone calls, email) where the links patterns are homogeneous with respect to the groups. In other words, it is assumed that all groups have the same distribution of intra-and inter-group linkage. A situation where the patterns of linkage dif-fer among groups is, however, easy to imagine. For example, con-sider machine learning papers: Reinforcement learning papers tend to cite papers in optimization, operations research, and theory, but genetic algorithm papers cite primarily other genetic algorithm pa-pers. Allowing the link probabilities to vary among groups will be important for modeling group structures in large heterogeneous domains.
Consider the case where there are k group types, | C | class val-ues, and each object has a latent variable. There is a spectrum of group models ranging from k = | C | to k = N G . RMNs can rea-son at one end of the spectrum ( k = | C | ) by tying the parameters across all links and creating a feature for each class. Techniques that cluster the data for features to use in conditional models (e.g., [18]), can reason at the other end of the spectrum ( k = N ing the identity of each cluster. The approach of [10] uses in the sense that it ties the parameters of intra-and inter-group link probabilities across all groups.

When group size is large there may be enough data to reason about each group independently, setting k = N G . For example, in the cinematic domain, once a studio has made a sufficient number of movies, we can reason about the likely returns of its next movie independent of the rest of the data. However, when group size is small, assuming that all groups are drawn from the same distribu-tion, and setting k = | C | , will offset the limited data available for each group. A model that can vary k may be thought of as a back-off model, which smoothes to the background signal when there is not enough data to estimate about a group X  X  type in isolation. LGMs offer a principled framework within which to explore this spectrum.

One of the primary advantages of LGMs is that influence can propagate between pairs of objects that are not directly linked but are close in graph space (e.g., in the same group). In RMNs and RDNs, the features of an object specify its Markov blanket. This limits influence propagation because features are generally con-structed over the attributes of objects one or at most two links away in the data. Influence can only propagate farther by influencing the probability estimates of attribute values on each object in a path sequentially. An obvious way to address this issue is to model the O ( N 2 O ) dependencies among all pairs of objects in the data, but dataset size and sparse link information makes this approach infea-sible for most datasets. PRMs with existence uncertainty [5] are the only current models that consider the full range of dependen-cies and their influence on observed attributes. LGMs are a natural way to expand current representations while limiting the number of dependencies to model. LGMs can aggregate influence over a local neighborhood, instead of only passing on autocorrelation informa-tion through changes to the probability distributions of each object in a path sequentially.
The experiments in this section demonstrate the utility of la-tent group models in relational domains. Using three classification tasks, we evaluate whether LGMs can leverage autocorrelation to improve model accuracy and illustrate the conditions under which LGMs will perform well.
 We present results for two LGM variations. The first variation, LGM-k, sets the number of group types to the number of class label values, k = | C | (e.g., for binary tasks, k = 2 ond variation, LGM-2k, sets k = 2 | C | . We compare LGM to four alternative models. The first two are individual classification models that reason about each instance independently and do not use the class labels of related instances: the relational probability tree (RPT) model [15] is a decision tree model and the relational Bayesian classifier (RBC) model [16] is a naive Bayes model. The third model is a relational dependency network (RDN) [14] that reasons about networks of instances collectively. The fourth model (RDN-ceil) is a probabilistic ceiling for the RDN model, where we allow the true labels of related instances to be used during infer-ence. This model shows the level of performance possible if the RDN model could infer the true labels of related instances with perfect accuracy.

To limit the confounding effects of feature construction and model selection, we first consider the restricted task of predicting class la-bels using only the class labels of related instances and/or the group membership. For the RPT and RBC models, we clustered the train-ing and test sets together and used cluster ID as the sole attribute in the model. The performance of these baseline models illustrates the baseline utility of clustering without typing the groups and serves as a comparison to previous work [18], which clusters the data to generate additional features for classification. For the LGM, RDN and RDN-ceil, we used the class label of related instances as the sole attribute available for modeling. We used exact inference for all models expect the RDN, which requires an approximate infer-ence technique. In the RDN experiments, we used Gibbs sampling with chains of length 500 and burn-in of 100 . (At this length, ac-ccuracy and AUC had converged.) During inference we varied the number of known class labels available to seed the inference pro-cess. We expect performance to be similar when other informa-tion serves to seed the inference process X  X ither when some labels can be inferred from intrinsic attributes, or when weak predictions about many related instances serve to constrain the system.
A second set of experiments includes object attributes in each of the models to explore the effects of intrinsic attribute information on performance.
The first data set was collected by the WebKB Project [3]. The data consist of a set of 3,877 web pages from four computer science AUC
IMDb departments, manually labeled with the categories: course, faculty, staff, student, research project, or other. We considered the unipar-tite co-citation web graph. The classification task was to predict page category. As in previous work on this dataset, we do not try to predict the category Other ; we remove them from the data after creating the co-citation graph.

The second data set is drawn from the Internet Movie Database (www.imdb.com). We used a sample of 1,382 movies released in the U.S. between 1996 and 2001. The binary classification task was to predict movie opening weekend returns ( &gt; $ 2million ). Based on past work that showed movie receipts to be autocorrelated through studios [7], we considered a unipartite graph of movies, where links indicate movies that are made by a common studio.

The third data set is drawn from Cora, a database of computer science research papers extracted automatically from the Web using machine learning techniques [12]. We considered the unipartite co-citation graph of 4,330 machine-learning papers. The classification task was to predict paper topic. There are seven topics (e.g., Neural Networks ).
Figure 2 shows area under the ROC curve (AUC) results for each of the models on the three classification tasks when the models do not use attributes. The graph shows AUC for the most prevalent cross-validation by department, learning on three departments and testing on the fourth. For IMDb, we used snowball sampling to bipartition the data into five training/test samples. For Cora, we used temporal samples where we learned the model on one year and applied the model to the subsequent year. For each training/test split we ran 10 trials at each level of labeling, except the RDNs where we ran 5 trials due to relative inefficiency of RDN inference. The error bars indicate the standard error of the AUC estimates for a single training/test split, averaged across the training/test splits. This illustrates the variability of performance within a particular sample, given the random initial labeling.

The results show that LGM performance quickly reaches per-formance levels comparable to RDN-ceil. (Note that RDNs and LGMs cannot be expected to do better than random at 0% labeled.) On WebKB and IMDb, LGM performance asymptotes at less than 40% known labels, indicating that the model is able to exploit group structure when there is enough information to accurately infer the group type. The RDN doesn X  X  converge as quickly to the ceiling level of performance. There are two explanations for this effect. First, when there are few constraints on the labeling space (e.g., fewer known labels), RDN inference may not be able to fully explore the space of labelings. Although we saw perfor-mance plateau for Gibbs chains of length 500-2000, it is possible that longer chains, or alternative inference techniques, could fur-ther improve RDN performance [11]. The second explanation is that joint models are disadvantaged by the data X  X  sparse linkage. When there are few labeled instances, influence may not be able to propagate to distant objects over the existing links in the data. A group model that allow influences to propagate in more elaborate ways may be able to exploit the seed information more success-fully. Future work will attempt to quantify the amount of error due to each of these sources.

RPT performance is near random on all three datasets. This is because the RPT algorithm uses feature selection and there is sig-nificant correlation between only a few cluster IDs and the class label. This indicates that there is little evidence to support general-ization about cluster identities themselves. The RBC, on the other hand, does not do any feature selection, it simply uses cluster IDs without regard to their support in data. On Cora, the RBC signifi-cantly outperforms all other models. Cora is the one dataset where the test set instances link into the training set. In the other datasets, where the training and test sets are nearly disjoint, the RBC does no better than random.

LGM performance does not reach that of RDN-ceil in Cora. Al-though the LGM outperforms the RDN when there is little know in-formation, eventually the RDN takes over as it converges to RDN-ceil performance. We conjecture that this effect is due to the qual-ity of the clusters recovered in Cora. To explore this, we repeated the LGM experiments on a number of different clusterings, vary-shows average AUC over the five samples as the stability threshold is varied in the range [0.02,0.40]. The average number of clusters and average cluster size is reported in parentheses for each thresh-old value. As the threshold is decreased, the clustering algorithm returns fewer, larger-sized clusters. When we use a more conser-Figure 3: Model performance as clustering stopping threshold is varied. vative threshold (e.g., 0 . 06 ), the clusters are larger and the model is able exploit group structure to make accurate predictions with fewer labeled instances. However, when we use a less conservative threshold (e.g., 0 . 2 ), the clusters are smaller. In this case the model has the potential to make more accurate predictions but only when there is a large number of labeled instances in order to identify the group types (i.e., 90% labeled). A technique the allows informa-tion to propagate outside the clusters (e.g., soft clustering) may be more robust in this situation.

Figure 4 shows average AUC results for each of the three clas-sification tasks when we include attributes in the models. For the WebKB task, we included three page attributes: school, url-server, url-text ; for IMDb, we included eight movie-genre attributes; for Cora, we included three paper attributes: type, year, month . In all three dataset, the attributes improve LGM performance when there are fewer known labels. This is most pronounced in the IMDb, where LGM achieves ceiling performance with no class label in-formation, indicating that movie genre is predictive of group type. In contrast, the RDN models are not able to exploit the attribute in-formation as fully. In particular, in the WebKB task, the attributes significantly impair RDN performance. This is due to poor feature selection, which selects irrelevant page attributes over the pairwise autocorrelation features. However, in the other two tasks where this is not a problem, the RDN does not propagate the attribute infor-mation as efffectively as the LGM when there are less than known class labels.

For a subjective evaluation of the clustering abilities of LGM, ta-ble 1 lists the studios associated with the IMDb clusters. We group the clusters by their (inferred) type values and present a sample of the associated studios and the estimated probability distribution of movie returns.
This paper presents a latent group model that reasons jointly about attribute information and link structure to improve reason-ing in relational domains. To date, work on statistical relational models has focused on models of attributes conditioned on the link structure (e.g., [21]), or on models of link structure conditioned on the attributes (e.g., [5]). These restrictions to the model space make learning and inference more tractable but limit the manner in which influence can propagate in the data. However, as our ini-tial investigation has shown, modeling the interaction among links and attributes promises to improve model generalization and inter-pretability.

Latent group models are a natural means to model the attribute and link structure simultaneously. The groups decouple the link and attribute structure, thereby offering a way to learn joint mod-els tractably. Preliminary investigations of latent variable models in the social networks community for link prediction [17, 6], and in the relational learning community for clustering [10, 9] and aug-mentation of classification models [22] show promise. However, the power and range of applicability of these models is yet to be fully explored. Our analysis has shown that group models outper-form collective models when there is little information to seed in-ference. This is likely because a smaller amount of information is needed to infer group type than is needed to propagate information throughout sparse relational graphs. This suggests active inference as an interesting new research direction X  X here techniques choose which instances to label based on estimated improvement to the collective predictions.

Latent group models extend the manner in which collective mod-els exploit autocorrelation to improve model performance. One of the reasons collective inference approaches work is that the class labels are at the  X  X ight X  level of abstraction X  X hey summarize the attribute information that is relevant to related objects. Group mod-els also summarize the information but at higher level of abstrac-tion (e.g., group membership and type). Positing the existence of groups decouples the search space into a set of biased abstractions and could be considered a form of predicate invention [20]. This al-lows the model to consider a wider range of dependencies to reduce bias while limiting potential increases in variance and promises to unleash the full power of statistical relational models. Indeed, the results we report for LGMs using only the class labels and the link information achieve nearly the same level of performance reported by relational models in the recent literature. The authors acknowledge helpful comments from S. Macskassy, C. Perlich, F. Provost, and the participants of the Dagstuhl work-shop: Probabilistic, Logical and Relational Learning X  X owards a Synthesis .

This effort is supported by DARPA and NSF under contract num-bers IIS0326249 and HR0011-04-1-0013. The U.S. Government is authorized to reproduce and distribute reprints for governmen-tal purposes notwithstanding any copyright notation hereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied of DARPA, NSF, or the U.S. Government.
 AUC
IMDb [1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. [2] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced hypertext [3] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, [4] L. Getoor, N. Friedman, D. Koller, and A. Pfeffer. Learning [5] L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning [6] P. Hoff, A. Raftery, and M. Handcock. Latent space [7] D. Jensen and J. Neville. Linkage and autocorrelation cause [8] D. Jensen, J. Neville, and B. Gallagher. Why collective [9] C. Kemp, T. Griffiths, and J. Tenenbaum. Discovering latent [10] J. Kubica, A. Moore, J. Schneider, and Y. Yang. Stochastic [11] S. Macskassy and F. Provost. Classification in networked [12] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. A [13] B. Milch, B. Marthi, D. Sontag, S. Russell, D. Ong, and [14] J. Neville and D. Jensen. Dependency networks for relational [15] J. Neville, D. Jensen, L. Friedland, and M. Hay. Learning [16] J. Neville, D. Jensen, and B. Gallagher. Simple estimators for [17] K. Nowicki and T. Snijders. Estimation and prediction for [18] A. Popescul and L. Ungar. Cluster-based concept invention [19] J. Shi and J. Malik. Normalized cuts and image [20] I. Stahl. Predicate invention in inductive logic programming. [21] B. Taskar, P. Abbeel, and D. Koller. Discriminative [22] B. Taskar, E. Segal, and D. Koller. Probabilistic classification
