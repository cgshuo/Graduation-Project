 Federated text search provides a unified search int erface for multiple search engines of distributed text informa tion sources. Resource selection is an important component for fe derated text search, which selects a small number of information sources that contain the largest number of relevant documents fo r a user query. Most prior research of resource selection focused o n selecting information sources by analyzing static information of available information sources that is sampled in the offline manner. On the other hand, most prior research ignored a large amo unt of valuable information like the results from past que ries. This paper proposes a new resource selection techni que (which is estimating the utilities of available information s ources for a specific user query. Experiment results demonstrate the effectiveness of the new resource selection algorit hm. H.3.3 [ Information Search and Retrieval]: Algorithms Past Queries, Resource Selection, Federated Search The proliferation of searchable text information so urces on local area networks and the Internet creates a problem of finding information that is distributed among many text inf ormation federated search is resource selection or collection selection , which is the task for selecting a small number of t ext information sources for search for a specific user query [3,5,6 ,7]. Most prior research work of resource selection focu sed on selecting information sources by analyzing the stat ic information which is sampled from available information sources in an offline manner. For example, the ReDDE (Relevant Document Distribution Estimation [7]) algorithm first tries to build a centralized sampled database based on query-based-s ampling, and then uses the information of the centralized sample d database to estimate the distribution of relevant documents for resource selection. However, such approaches ignored a large amount of valuable information like the resource selection re sults of past queries. For instance, in a real world search engin e, there are many similar or even duplicated queries every day. The results from those similar past queries are very valuable t o guide the resource selection decision of a current user query . In this paper, we propose a new resource selection technique (i.e., qSim ) that utilizes the search results of past queries for estimating query. The new algorithm calculates the query simil arities between a specific query and all past queries, and then estimates the utilities of available information sources by t he weighted combination of search results of past queries with respect to the query similarities. The new resource selection algo rithm is practical as it does not require relevance judgment of past queries, and it only utilizes regression based results mergi ng method to rank the results of past queries. This section briefly surveys related work in resour ce selection. There is a large body of prior research on resource selection (e.g., [3,5,6,7]). Space limitations preclude discussing i t all, so we recently in prior research.
 A large body of resource selection algorithms follo ws the  X  X ig Document X  approach by representing each information source as a single big document. The similarities between the  X  Big Documents X  and a user query are calculated to rank available information sources. Particularly, the CORI resourc e selection algorithm [1] has been shown to be one of the most stable and effective  X  X ig Document X  resource selection algorit hms in prior studies (e.g, [3]). However, the  X  X ig Document X  app roach does not consider information source sizes, which is a b ig problem for searching in the environment of a mixture of  X  X mall  X  and  X  X ery large X  information sources [7]. ReDDE [7] resource selection algorithm was proposed to address the above issue. It explicitly estimates the distri bution of relevant documents among available information sources for r esource selection. ReDDE utilizes database size estimation and a centralized sample database (CSDB) that consists of the documents obtained by query based sampling. The CSD B is a representative subset of the centralized complete d atabase (CCDB) which is the union of all the documents in a vailable information sources. Since the CCDB is not availabl e in the federated search environment, ReDDE uses the CSDB t o simulate the property of CCDB. Details of ReDDE algorithm ca n be found in [7]. However, all the above prior research of resource s election utilized only static information of available infor mation sources and ignored a large amount of valuable information like the results from past queries. In most real world appli cations, there is often substantial similarity between users X  queries ; and even many of them are duplicates of each other. This fact mot ivates the research to make use of the results of past queries . Voorhees et al. X  X  work considered using results of past queries [9]. However, it is not exactly resource selection since it retrieves documents from all information sources. Furthermore their methods require relevance judgment. Since there tends to be many similar queries in a r eal world federated search system, the valuable information o f past queries can help us provide better resource selection resul ts. In this section, we propose a novel algorithm, which is cal led qSim , to utilize the valuable information to guide the decis ion of resource selection. Assume that there exists a set of past queries, whi ch is denoted by = { , , ... , } , where represents the i th past query. For an online user query , assume that we have a specific method to measure the similarity between and (we will further discuss this in section 3.2), let us denote the similarity by ( | ) . Denote the set of information sources by = { , , ... , } . For a query , we use ( | ) to represent how likely it is for the information source to be relevant to the query , or what (estimated) percentage of the relevant documents fo r the query is in . In qSim, we can estimate the value of ) for all past queries. For an online query , the task is to estimate ( | ) based on the information of ) and ( | ) . Any resource selection algorithm can be applied to estimate the value of ) . If a resource selection algorithm can explicitly give scores for all information sources, we can directly map those scores to ) . Here, we present a regression based approach to get a much more precise ) estimation by utilizing the search results from past queries, no matter whether a prior resource selection has been made fo r or not. First, assume that a set of sources (denoted by "# $ for . For the case that no prior resource selection has been mode, we can have "# $ Second, we use the regression based Semi-Supervised Learning method [5] to merge the search results from those s ources in "# $ Third, we assign scores for the information sources by the following way: If the source is not in "# $ where ' is a pre-defined number, and The value ' can be very small like 20; and we set it to 20. Please note that when we build the regression model s for merging the search results, some documents need to be downl oaded for each past query. To evaluate the resource selection approach of learning from past queries in a strict manner, we d o not allow it to obtain documents in the centralized sample database as training documents to build regression models. On the other side, we download the top 3 documents to build the regressio n models for results merging in a similar manner as the minimum downloading method of the Semi-Supervised Learning results merg ing algorithm [5]. There are many ways to measure the similarities bet ween queries, such as term-based approach (edit distance, latent semantic analysis, etc), selection-based approach and result -based approach. In our experiments, we use a result-based approach to calculate the similarities. Assume that there is a database which contains rele vant information to our queries. One way to construct su ch a database is to index the documents that are downloaded when we build regression models for merging search results [8] fo r past queries. Let $ we search (or ) in the downloaded documents database. We measure the similarity of with respect to by where the score function for a matching document is /01 20/ , $ In practice, we cut each search results ( $ focusing on the top 20% returned documents in their rank lists. Finally, we normalize the values of ( | ) according to the following formulas. ?01&gt;K2 ( | )  X 
L For an online query , we predict ( | ) based on estimated relevant information from similar past queries. It is calculated by the following formula In practice, we only consider the Y most similar past queries when we calculate the above summation. In our experiment s, Y is set to 5. The last step is to simply rank the information sou rces according to the values of ( | ) . A larger value of ( | ) means information with respect to . For a past query , the amount of search results will affect the quality of the estimated ) . The amount of results is measured by the number of information sources selec ted and searched for generating search results. If more sou rces have been searched, the amount of search results for the past query is more comprehensive. We call the qSim algorithm by  X  qSim-Cut-X  X  where the number of sources selected and searched for is C , or in other words |"# $ Z = 100 information sources, the typical number of sources that are chosen for the search task is about C = 5, 10 or at most 20 . In this work, X is chosen to be 10 for efficiency p urposes. An extensive set of experiments was designed in res earch environments to simulate real world applications. Experiments were carried out on Trec123 and Trec4 t estbeds. Details about Trec123 and Trec4 datasets are given in Table 1. Trec123-100col-bysource (Trec123): 100 information sources were created from TREC CDs 1, 2 and 3. They are org anized by source and publication date. Trec4-bysource (Trec4): 100 information sources were created from TREC4 according to the sources of the document s in TREC4. For Trec123 and Trec4 testbeds, 50 queries were cre ated from the title fields of TREC topics 51-100 and from the des cription fields of TREC topics 201-250 respectively. These queries will be used as the set of test/online queries. The detailed sta tistics about the test queries can be found in [7]. All the 100 information sources were assigned one o f three types of retrieval algorithms as INQUERY [2], a unigram l anguage model with linear smoothing [4] and a TFIDF retriev al algorithm with the  X  X tc X  weighing schema [8]. All the algorit hms were implemented with the lemur toolkit [4]. In a real world federated search system, there ofte n exist many similar queries. However in Trec123 and Trec4 testb eds, we don X  X  have many similar queries. Therefore we use two dif ferent approaches to generate two different sets of past q ueries using the test queries that are available in Trec123 and Trec 4 datasets. Then the set of real queries that are available from Tre c123 and Trec4 datasets are used as the set of test/online queries and either one of the two sets of generated past queries are used as the set of past queries. The first approach of generating the set of past qu eries samples queries for each test/online query by extracting th e titles of some top ranking documents in the search results of that particular test query. The past queries generated by this approach will be referred as sampled past queries . One set of sampled past queries is generated for Trec123, which is called Trec123_T 20. The past queries in Trec123_T20 were generated in the follow ing way: 
A new Trec123 database called Sub_Trec123 is constr ucted, which consists of the documents that exist in the o riginal Trec123 database and that do not exist in the CSDB of the ReDDE algorithm. 
For each test query, 20 most similar documents that have a title (not all the documents have a title) are retrieved from 
Sub_Trec123 database. The titles of these TOPX docu ments construct the set of 20 sampled queries for this te st query. For 50 test queries, the above sampling process is repeated and a total of 1000 past queries are acquired for Trec123 _T20. One set of sampled past queries is also generated for Trec4 testbed in the same way described above. The average number of wor ds in Trec123_T20 and Trec4_T20 query sets are 8.98 and 8 .53 respectively. The second approach of generating the set of past q ueries creates the past queries from test queries by randomly remo ving some terms from the test queries. The past queries gener ated by this approach will be referred as simulated past queries. One set of past queries is generated for each of Trec123 and T rec4 testbeds, which are called Trec123_R1 and Trec4_R2. The past queries in Trec123_R1 and Trec4_R2 were generated in the follo wing way: 
For each test query, 1 (for Trec123_R1) or 2 (for T rec4_R2) term(s) is (are) removed to generate a simulated pa st query; 
For each simulated past query at least 2 (for Trec1 23_R1) or 3 (for Trec4_R2) terms are kept to make sure the simu lated past query is not too short to be meaningful. In the experiments that used the simulated past que ries as the set of past queries; the 50 real queries are treated as test/online user queries, and the 50 simulated queries (i.e. in Trec 123_R1 or Trec4_R2) are treated as past queries. The average number of words in Trec123_R1 and Trec4_R2 are 2.60 and 6.92 respectively. Since the processes of generating the simulating queries include randomness, each set of simulated q ueries (e.g., Trec123_R1) has been generated for 15 times. Accord ingly, the resource selection experiments that use simulated p ast queries are run 15 times for each set of simulated queries and the average results of all of these runs are reported for evalu ation. 
Table 1: Summary statistics for Trec123 and Trec4 distributed IR testbeds.
 Testbed Trec123 3.2 0.7 10.8 39.7 28 32 42 
Trec4 2.0 5.6 5.6 5.6 4 20 138 In the experiments, we use ReDDE to do resource sel ection for past queries (i.e. to determine "# $ resource selection algorithm to compare with qSim. For ReDDE, we build a centralized sampled database, which cont ains 150 documents per source by query-based sampling. All o ther parameters in ReDDE are the same as that in [7]. The recall metric ] has been commonly used to evaluate resource selection algorithms [1,7]. It measures wh at percentage the difference is between the estimated ranking and the most desirable ranking. Therefore, at a fixed @ , a larger indicates a better ranking. Experiments are conducted to address how good qSim is in comparison to the state-of-the-art ReDDE algorithm.
 Table 2 and Table 3 show the results of qSim and Re DDE on the Trec123 and Trec4 testbeds with Trec123_T20 and Tre c4_T20 sampled query sets and Trec123_R1 and Trec4_R2 simu lated query sets. The performance is evaluated by the Rec all metric R defined in section 4.4, where  X # Selected Sources X  in the tables is the k in R k . The percentages within the parentheses are the re lative improvements of qSim over ReDDE. The overall improv ement is calculated by taking the average of the improvement percentages when 1,2,...,10 sources are selected. As shown from Table 2 and Table 3, qSim-Cut-10 alwa ys generates much better results than ReDDE. Thus, qSi m can be considered as a very effective algorithm for resour ce selection. Resource selection is an important component for a federated text search system. Prior research of resource selection ignored a large amount of information of the results from past quer ies. This paper proposes a new resource selection approa ch called qSim to utilize the search results of past queries for estimating the utilities of available information sources for a sp ecific user query. For a user query, the algorithm first calculates th e similarity measurements between the current user query and the past queries. The qSim algorithm then estimates the util ities of available information sources by the weighted combi nation of search results of past queries with respect to the query similarity measurements. The qSim algorithm does not require r elevance queries, which are generated by regression based re sults merging method. Experiment results have demonstrated the ad vantage of qSim to achieve more effective results in compariso n to the ReDDE algorithm. This research was partially supported by the NSF gr ant IIS-0746830. Any opinions, findings, conclusions, or recommendations expressed in this paper are the aut hors', and do not necessarily reflect those of the sponsor. [1] J. Callan. (2000). Distributed information retrieva l. In W.B. [2] J. Callan, W.B. Croft, and J. Broglio. (1995). TREC and [3] N. Craswell. (2000). Methods for distributed inform ation [4] The lemur toolkit. http://www.cs.cmu.edu/~lemur [5] A.L. Powell, J.C. French, J. Callan, M. Connell, an d C.L. [6] M. Shokouhi and J. Zobel. (2007). Federated text re trieval [7] L. Si and J. Callan. (2003). Relevant document dist ribution [8] L. Si and J. Callan. (2003). A Semi-Supervised lear ning [9] E. Voorhees, N. K. Gupta, and B. J. Laird. (1995). Learning Table 2: Comparison between ReDDE with qSim@Cut@10 on 
Trec123 dataset with Trec123_T20 sampled past query set and with Trec123_R1 simulated past query set. Table 3: Comparison between ReDDE with qSim@Cut@10 on Trec4 dataset with Trec4_T20 sampled past query set and Trec4_R2 simulated past query set. # Selected Sources # Selected Sources 
