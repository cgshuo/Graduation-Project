 Configuring enterprise database management systems is a notoriously hard problem. The combinatorial parameter space makes it intractable to run and observe the DBMS behavior in all scenarios. Thus, the database administrator has the difficult task of choosing DBMS configurations that potentially lead to critical incidents, thus hindering its avail-ability or performance. We propose using machine learning to understand how configuring a DBMS can lead to such high risk incidents. We collect historical data from three IT environments that run both IBM DB2 and Oracle DBMS. Then, we implement several linear and non-linear multivari-ate models to identify and learn from high risk configura-tions. We analyze their performance, in terms of accuracy, cost, generalization and interpretability. Results show that high risk configurations can be identified with extremely high accuracy and that the database administrator can po-tentially benefit from the rules extracted to reconfigure in order to prevent incidents.
 H.2.7 [ Database Management ]: Database Administra-tion; I.5.2 [ Design Methodology ]: Classifier Design and Evaluation Database configuration; multivariate analysis
Configuring enterprise relational databases is a notori-ously hard problem [14, 4, 22]. The large number of config-uration parameters that come with enterprise DBMS, like IBM DB2 1 [13] or Oracle 2 [21], and their respective widely varying ranges prohibit database administrators to identify and apply the best configurations for specific workloads. Ide-ally, one would use full factorial design to find the optimal c  X  configurations, that is run all possible combinations of con-figurations. However, as the number of parameters increases linearly, the number of combinations explodes, making this approach intractable in practice. In fact, beyond generat-ing combinations for a handful of parameters [24, 20, 5], the configuration space is rather unknown to us.

Our research is guided by several observations. On the one hand, databases are complex black boxes and apply-ing a top-down approach [25, 15] to reconfigure them on a need basis is difficult. Even the built-in cost-based optimiz-ers, which are actively used by the DBMS advisors, fail to accurately estimate how well a DBMS will perform for spe-cific workloads and configurations [14]. On the other hand, a bottom-up approach requires empirical evidence on the DBMS behavior across various scenarios.

We advocate the idea that instead of employing heuristic algorithms to find approximate solutions for the NP-hard problem at hand, machine learning  X  specifically multivari-ate analysis  X  can be better suited in the context of DBMS (re)configuration . Not only it can be successfully used to explore and understand the huge configuration space, but it can generalize beyond specific DBMS scenarios. The tech-niques we look into vary from linear, such as generalized linear (GLM), to non-linear methods, like additive (GAM) models [11, 29], decision trees or random forests [2].
The goals of this paper are two-fold: (1) to use machine learning techniques for identifying databases with high risk of failure or performance degradation incidents, and (2) to recommend cost effective reconfigurations that can be used by DBMS administrators to mitigate some of the risk, as well as to potentially improve the DBMS availability and performance.

We approach the problem of DBMS (re)configuration in several steps: 1. We gather broad configuration and workload informa-2. We collect and classify vast amounts of incidents span-3. We apply the afore-mentioned models on the collected 4. We use the same models to learn what constitutes 5. We evaluate the linear and non-linear models on the
Our findings show that high risk configurations can be identified with extremely high accuracy, 91% for IBM DB2 and 96% for Oracle. This is particularly the case for ran-dom forest and generalized additive models with interac-tions, since they have the advantage of exploiting the intri-cate relationships between the DBMS X  configuration param-eters. These models also generalize well on new DBMS con-figurations, achieving 75-80% classification accuracy. How-ever, high accuracy comes at the expense of computational cost and output interpretability. Generalized linear models and decision trees excel at providing rules that can be eas-ily understood and applied by database administrators to reconfigure DBMS. We advocate that some of these models can be used in combination to achieve high accuracy, gener-alization and interpretability.

The rest of the paper is organized as follows. In Sec-tion II we review the state of the art. Section III describes the methodology and provides brief overviews of the mod-els implemented. In Section IV we present the experimental setup, while in Section V we discuss our results and conclude in Section VI.
Ganek and Corbi [10] were among the first to motivate the push for autonomic computing and devised four im-portant attributes that define it: (1) self-configuring (i.e., systems adapt automatically to dynamic environments), (2) self-healing (i.e., systems discover and react to malfunc-tions), (3) self-optimizing (i.e., systems monitor their per-formance and tune automatically), and (4) self-protecting (i.e., systems predict and protect themselves from negative impact events). Substantial work has been done along all these directions for DBMS, with a special focus on provid-ing them with self-optimizing and self-configuring capabili-ties. The research on self-optimizing/self-configuring DBMS can be divided into two categories, namely the physical or logical tuning of the DBMS, and the tuning of the config-uration parameters. The problem of self-optimization and self-configuration is a challenging one. For instance, tun-ing the buffer pool or the memory allocation for queries is quite disparate from selecting or building the right indexes. Each problem requires different workload abstractions and has different constraints on the optimal solution.

In the context of physical or logical tuning, automatic index tuning has received most of the attention. This is at-tributed to two reasons: (1) the high impact index selection has on DBMS performance, (2) the difficulty of the opti-mal index creation problem. Generally, the objective is the performance maximization of the materialized indices given an expected workload, under resource constraints. Various solutions have been proposed in [25, 3, 4, 1, 27].
In configuration parameter analysis, the approaches that account for all parameters are rather few. Instead, attempts have been made in finding optimal values for subsets of con-figuration parameters [24, 20] or even for only one configu-ration parameter as in [5], where the authors single out the buffer pool as the most important configuration parameter. However, given our observations on the existence of non-trivial interactions between parameters, there is no clear way to merge the insight obtained through separate mod-els into a comprehensive standalone model that can aid in optimizing the DBMS performance.

Given that DBMS come with tens or hundreds of configu-ration parameters, whose ranges can vary widely, the explo-ration space of the problem is combinatorial. Models that account for all parameters have been proposed in [6, 15, 1]. In [6], the authors propose a model to aid in finding config-uration settings that deliver best performance for a certain workload. First, they model a response surface of the DBMS performance with respect to the parameter values. Second, they devise a feedback-driven model of a response surface, which they further use to infer the performance expecta-tion of a combination of parameters. The biggest draw-backs of the solution are the large temporal and computa-tional requirements in order to provide a configuration that exhibits a significant margin on performance improvement. IBM DB2 X  X  Configuration Advisor [15] recommends param-eter settings based on answers given by users to a set of high-level questions (e.g., the approximate number of users who will connect to the DBMS) and information collected automatically. However, if the recommended configuration is still unsatisfactory, the tool does not refine its recommen-dation. The authors in [1] devise a statistical model that performs parameter ranking based on the workload impact. However, in contrast to our approach in parameter ranking, they conduct a limited set of experiments in which they con-sider only extreme values (minimum or maximum) for each of the configuration parameters and consider only low-order interactions (maximum two factor interactions).

Given the problem at hand and the other approaches en-countered in the literature in solving it (e.g., control theory, Gaussian processes, decision trees), we can conclude that the selection of optimal DBMS configurations is bound to build on mathematical models and their correct integration into the system components [26].
Let D = { ( X i ,Y i ) } N 1 represent a set of size N , where X = ( x i 1 ,...,x in ) is a feature vector with n features and Y the target parameter. In this paper, X i consists of a mix of DBMS-specific configuration parameters associated to a particular DBMS instance, DBMS i , and the properties of the server on which it is hosted. We call X i the configura-tion vector . Let I t i denote the set of incidents corresponding to DBMS i over the period t . This set can either be empty (i.e., if no incidents occurred on DBMS i ) or not. Thus, the configuration vector X i associates to I t i . The output, Y , represents whether the corresponding configuration X i is problematic or non-problematic. Any DBMS configura-tion with no incidents is non-problematic and problematic, otherwise.
We want to be able to establish whether a configuration X is problematic or not and model this as a binary classification problem. We want to predict the value of Y i  X  0,1, with Y = 0 corresponding to non-problematic configurations and Y = 1 to problematic ones. Given a model M , let Y = F ( X be the prediction of the model for configuration X goal is to minimize the expected value of some loss function L ( Y ,Y )  X  [0,1]. In our case, the loss function can only take 2 values, 0 if Y = Y or 1 if Y 6 = Y .

In the second step, we consider the classified configu-rations X i N 1 and for each x i define its set of single rules R i = { R ij } J 1 , where J is its cardinality. Then, we extract general rules r : Condition  X  Y , where the condition is expressed as a join of rules specific to individual configu-ration parameters, Condition = R 1 j 1 R 2 j ... 1 R kj , with k  X  [1 ,n ] and j  X  [1 ,J ]. Note that a generalized rule r can contain an arbitrary number of single rules. In the simplest case, only one rule can determine the output ( r : R kj  X  Y ), whereas in the most complex scenario all input parameters contribute to predict the target ( r : R 1 j 1 R 2 j Our goal in extracting generalized rules is two-fold. First, we favor simpler rules, as in general predicting the output with fewer input parameters ensures a higher coverage of the data set. Second, we aim for easily generalizable rules, that can achieve high prediction accuracies when applied on unseen data sets.

Next, we briefly review the concepts behind linear, addi-tive and non-linear models used.
Generalized linear models are defined as: where  X  i n 1 are the computed coefficients for the individual configuration parameters,  X  0 is the intercept, e is the error and g is the link function. The  X  i n 1 coefficients are estimated using maximum likelihood. GLMs are the simplest models to build, since the contribution of each input variable x Y is a linear expression  X  i x i . One can easily understand the contribution in predicting the output and whether it is positively (  X  i &gt; 0) or negatively (  X  i &lt; 0) correlated to Y . However, they suffer from two major drawbacks which hin-der their accuracy. First, they do not capture non-linear relationships between individual parameters and the out-put. Second, they do not model any interactions between these parameters and assume they are independent. As such, GLMs are not a particularly good fit for classifying DBMS configurations, since incidents mostly occur due to poorly configured combinations of parameters.

To reduce model overfitting, we apply regularization.Thus, instead of minimizing L ( Y ,Y ), we minimize L ( Y ,Y ) +  X  || w || , where || w || is the complexity penalty and  X  is the tuning parameter. We implement and evaluate GLMs with L1 (Lasso), L2 (Ridge) and L1/L2 (ElasticNet) regularization.
Generalized additive models [11, 29] are defined as: where f i are the shape functions of configuration param-eters x i . GAMs explicitly decompose a complex function into a sum of one-dimensional components, its shape func-tions. While the expression is simple, these functions can be non-linear, which means they capture non-linear rela-tionships between individual configuration parameters and the output. Thus, GAMs can achieve significantly higher accuracy than GLMs, while retaining much of their com-prehensibility. As such, one can visualize the relationship between parameters and the output through plots f i ( x i x . However, they do not model any interactions between features, as they assume they are all independent. This as-sumption not only limits the accuracy GAMs can achieve, but it does not apply to the problem of DBMS configura-tion. For example, increasing logbufsz for IBM DB2 implies that dbheap has enough space to sustain such a reconfigura-tion. This can potentially lead to a prerequisite increase in dbheap .

In [17], the authors present two methods of modeling shape functions  X  regression splines and tree ensembles  X  and two techniques for fitting  X  least squares and gradient boosting , which we use for building several GAMs. We briefly summarize them below for completeness.
Regression splines  X  Splines are piecewise-polynomial func-tions with high degrees of smoothness. The degree of a spline is given by the highest order of the polynomials. We imple-ment spline-based GAMs with 3 well-known spline types, namely cubic , thin plate [28] and P [7].

Tree ensembles  X  Tree ensembles are sets of binary bal-anced trees, where the largest variance reduction is used as the split selection method. We use boosted bagged trees [8], since they outperform both boosted and bagged trees, as shown in [17]. The idea behind is that bagged ensembles are used in every step of stochastic gradient boosting, such that each successive tree predicts the overall residual from the preceding trees.
Least squares  X  The method is used for fitting regression splines. It minimizes the sum of squared residuals, by learn-ing the coefficients of the piecewise polynomials. For classifi-cation, this is equivalent to fitting a logistic regression using penalized reweighted least squares.

Gradient boosting  X  We briefly outline the gradient boost-ing algorithm presented in [17]. In each iteration m M 1 cycle through all configuration parameters to learn their re-spective shape functions. First, all functions are set to zero. Second, over the M iterations and all n parameters, the re-sponse are computed by Third, in every iteration m M 1 , for a parameter x j , the algo-rithm learns the one-dimensional function to predict Y i , as a tree with K leaf nodes using ( X ij , Y i ) as training set. This function is, then, added to the shape function:
GAMx2 [18] enhance GAMs by adding pairwise interac-tions. Ideally, improvements of GAMs would consider in-teractions between any i parameters, where i  X  [2 ,n ]. Al-though any interactions beyond pairs quickly diminish the model interpretability, pairwise interactions are still intelli-gible and can be visualized as heatmaps of f ij ( x i ,x j x ,x j space. Therefore, [18] proposes GAMx2 of the form:
The main challenge in building accurate GAMx2s is the large number of parameter pairs to consider. Since not any 2 features truly interact, there is no point to include shape functions for all pairs. Detecting true interactions is a well researched topic. One class of approaches measures the in-teraction and additive effects [9, 16, 29]. A major draw-back of these methods is that false interactions may be re-ported over low-density regions [12]. Another approach is to measure the performance drop of the model if a particular interaction is left out [23]. Although straightforward, this method is extremely computationally expensive.
 Keeping the need for performance in mind, we use the FAST algorithm [18] for quick pairwise interaction detec-tion. The extensive implementation details of FAST are beyond the scope of this paper, but we briefly outline the execution pipeline. First, the standard GAM F is built us-ing only one-dimensional features. Second, x i , x j pairs are ranked in ascending order of their interaction strength. To compute the strength, one needs to look only at the resid-ual sum of squares (RSS) for f ij , computed from individual RSS for f i and f j . Instead of actually computing f ij algorithm prunes the x i ,x j space to find the interaction pre-dictor T ij with the lowest RSS, which is a measure of the pair X  X  strength of interaction. Finally, the shape functions for the top-k ranked pairs are computed and added to F .
In our scenario, the interior nodes of a decision tree cor-respond to one of the configuration parameters. Each leaf represents one of the two output classes, problematic or non-problematic, given the values of the parameters composing the path from the root to the leaf. The tree is grown by bi-nary recursive partitioning until the terminal nodes are too small or too few to split.

The biggest advantage of decision trees is the high intelli-gibility by means of rules. In our scenario, each branch can be formulated as a single conjunctive rule of the form:
If cond 1 ( x 1 ) and ... and cond k ( x j ) then Y = label (6) where x j n 1 are configuration parameters, cond l k 1 are condi-tions involving single parameters and label is either prob-lematic or nonproblematic. However, one major issue with decision trees is the optimal size of the final tree. On the one hand, a tree that is too large risks overfitting the train-ing data and poorly generalizing on unseen sets. On the other hand, a small tree might lose important structural in-formation. We address this problem by first, limiting the minimum number of configurations per node to 0.02 * N and second, by pruning the decision tree.
Random forests [2] are ensembles of decision trees. For every configuration vector X i , each tree provides a classi-fication into problematic or nonproblematic and votes for that class. The forest, then, chooses the classification hav-ing the most votes over all trees. Each tree is grown using DB2 11915 914  X   X   X  28 Oracle 10844 802 38% 21% 41% 45 bagging. With the training set D train , we randomly sample |D train | configurations from D , with replacement and use them to grow the tree. The left out configurations are used to get a running unbiased estimate of the classification er-ror as trees are added to the forest. This technique leads to better performance because it decreases the overall variance. This means that while the predictions of a single tree can be highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated.
The main benefits of using random forests are their high performance, robustness against outliers and the ability to capture nonlinear relations between input and output vari-ables. However, these advantages come at the expense of a small increase in the bias, loss of interpretability and poten-tially high computational cost. We collect configuration and workload data for the IBM DB2 and Oracle DBMS from three IT environments over several snapshots in 2014. These snapshots show that the configurations of a DBMS are largely static over time. We use the workload information to cluster configurations in 3 classes: R + W corresponds to over 80% reads (i.e, DSS), RW + to over 80% writes (i.e., OLTP) and RW represents mixed workloads. Table 1 summarizes the 2 datasets, in terms of volume of relevant incidents in 2014, size, number of input configuration parameters, as well as the distribution of configurations across workload types. 41% of the IBM DB2 configurations and 36% of the Oracle ones are problematic. For IBM DB2, we are not able to collect detailed workload information, therefore all models are workload-agnostic. In the case of Oracle, however, 38% of configurations are R + 21% are RW and 41% are RW + . For each of these subsets, the percentages of problematic configurations are 40%, 24% and 38%, respectively.

To identify the problematic and non-problematic config-urations, we collect incident tickets (i.e., description, reso-lution, severity) from database service line teams spanning 2014. We classify them into 5 categories: backup , DB error , disk space , diagnostics and other . Backup incidents refer to backups missing or crashed, backups too old, inconsistent backups, etc. DB error refers to database crashes, inability of running transactions, slow performance, too many users connected, deadlocks, etc. We classify incidents as disk space only if the low disk space utilization affects the database per-formance (i.e., otherwise, we exclude them since they can be solved either by removing files to reduce the utilization or increasing the disk capacity). Diagnostics refer to transient incidents, that are monitored over extended periods of time, but do not require any action from the service team, and other contains any non-database related incidents. Only the backup , DB error and disk space incidents are relevant and are usually associated with high severities, which means they will be resolved first. From all these incidents, we extract the server and database unique identifiers by exact string matching. This step allows us to link incidents to the actual databases on which they occur and classify their configura-tions into problematic and non-problematic. We consider a configuration as being problematic, if it is associated with at least one incident throughout the time range. The number of configuration parameters used in both the Oracle and IBM DB2 models is not extremely large. How-ever, even so, not all of them have high predictive strength and importance. In fact, those with low importance only overcomplicate the models and cause overfitting, without increasing the overall accuracy and should be pruned out. To select the important parameters, we apply feature selec-tion and validate using expert knowledge. We ensure that the importance of each predictor is computed in a model-agnostic fashion, using a filter approach. The output is the list of all configuration parameters for both DB2 and Oracle in decreasing order of their importance.

Figure 1 shows the importance for all parameters and their cumulative prediction strength. For IBM DB2, the top 17 server and database parameters explain over 80% of the dataset variance. The top features in decreasing order of their importance are: server age , dbheap , logpri-mary , logbufsz , OS version , server architecture , logsecond , util heap sz , sortheap , locktimeout , DBMS version , logfilsiz , stmtheap , avg appls , num ioservers , applheapsz and buffpage .
For Oracle, only 15 out of 45 parameters are sufficient to explain over 80% of the data variance. These are in de-creasing order: database size , memory target , sga max size , server architecture , dml locks , control file record keep time , shared pool size , parallel max servers , log buffer , server age , dbblock checking , session cached cursors , dbblock checksum , java pool size and db keep cache size . We use these param-eters as input to all models.

Note that configuration parameters like db mem thresh , backup pending , restore pending or rollforward for IBM DB2 have little or no predictive strength for our dataset. This is due to the fact that the same settings are used through the entire dataset, thus presenting no variation from which models can learn. The same observation applies for several Oracle configuration parameters.
Table 2 provides an overview of the models and their de-nominations, in ascending order of their complexity. GLM-L1, GLM-L2 and GLM-L12 denote GLMs with L1, L2 and L1/L2 regularization, respectively, and are implemented with the vglm R package. GAM-csNI, GAM-tpsNI, GAM-psNI and GAM-teNI are GAMs implemented with cubic splines (CS), thin plate splines (TPS), P-splines (PS) ( mgcv R pack-age) or bagged trees (Java mltk library [19]), and model no inter-feature interactions. GAM-csI, GAM-tpsI, GAM-psI and GAM-teI use the same implementations, but also in-clude the top 20 pairwise interactions. For GAM-teNI and GAM-teI, we use 3 leaves for each tree, as indicated in [17]. DT denotes decision tree, while RF stands for the random forest model. We use the rpart and randomForest R pack-ages, respectively, to implement the models.

In the context of our classification problem, we split both datasets into the train set D train and the test set D test random sampling. 75% of the IBM DB2 and Oracle con-figurations, respectively, are assigned to D train , and the re-maining 25% to D test . We train all models on D train and evaluate their performance on D test through several rele-vant measures: (1) balanced accuracy ( ACC ), (2) accuracies over the problematic ( ACC  X  ) and nonproblematic ( ACC + configurations, respectively, as well as (3) F 1 score. These metrics are defined as follows: where TP = true positives (i.e., correctly classified prob-lematic configurations), TN = true negatives (i.e., correctly classified nonproblematic configurations), FP = false posi-tives and FN = false negatives. |D  X  test | and |D + test the number of actual problematic and nonproblematic con-figurations in the test set, respectively, such that D D test = D test . We report averages of the metrics over 10-fold cross validation for all models.
The classification results of all models, when workload is not considered, are shown in Figure 2, for both the IBM DB2 (a-d) and Oracle (e-h) datasets. Each graph is laid out left to right, as follows: The first 3 bars correspond to the gen-eralized linear models, GLM-L1, GLM-L2 and GLM-L12, respectively. The next 4 bars correspond to the generalized additive models modeled with splines, GAM-csNI, GAM-tpsNI and GAM-psNI, as well as with bagged trees, GAM-teNI. The same order is kept for the following 4 bars, that correspond to the generalized additive models with pairwise interactions, GAM-csI, GAM-tpsI, GAM-psI and GAM-teI. Finally, the last 2 bars correspond to the decision tree, DT, and the random forest model, RF.

Several clear patterns emerge for both datasets. (i) There is a clear gap between linear methods that do not support feature shaping (GLMs) and all other models. This is mostly visible for the IBM DB2 dataset, where all GLMs reach an overall accuracy between 65 and 70%, while the rest exceed 80%. Applying L1/L2 regularization slightly increases the model performance, but only by 1-2%. Even more important is the low accuracy in correctly classify-ing problematic configurations, around 50%. For Oracle, while still having the lowest performance among all models, GLMs already reach quite high accuracies, over 87%. How-ever, they still suffer from a lower classification accuracy of problematic configurations, between 70 and 78%. (ii) The tree-based GAM models are in general more ac-curate than the spline-based ones, by 3-7% for IBM DB2 and 2-4% for Oracle. For IBM DB2, using splines lowers the accuracy of correctly classifying nonproblematic configura-tions, while the opposite effect is observed for Oracle. (iii) The GAMx2 models are more accurate than GAMs, which is expected since they model pairwise interactions in addition to feature shaping. For instance, the difference in the overall accuracy and the accuracy in classifying nonprob-lematic configurations between GAM-teI and GAM-psNI is 11% and 34% for IBM DB2. (iii) The tree-based GAMs generally perform better than DT. For IBM DB2, GAM-teI is the closest to RF in terms of classification accuracy, with ACC = 85%, ACC  X  = 82% and ACC + = 88%. The same observation holds for Ora-cle as well, where GAM-teI matches RF, with ACC = 95%, ACC  X  = 95% and ACC + = 96%. This model is quite robust in providing high accuracies for correctly classifying prob-lematic, as well as nonproblematic configurations, therefore minimizing both false positives and false negatives. While DT also performs excellent, especially for Oracle, we note a lower ACC  X  than GAM-teI, by 2-6%. (iv) Full-complexity models, such as RF, perform the best, with 91% overall accuracy for IBM DB2 and 97% for Oracle. RF reaches higher accuracy than GAMs, because it is able to model intricate feature interactions, which linear models of shaped features cannot capture. It also performs better F igure 3: Distributions of selected parameters across prob-lematic and nonproblematic configurations for Oracle. than decision trees, because it trains and fits hundreds of single trees, as compared to a single one.

While the accuracy increase from GLMs to RF is over 25% for DB2, we note only an 8% increase for Oracle. More-over, for Oracle all GAM models and DT are comparable to RF. This similar performance is caused by the nature of the dataset, more precisely by the strong correlation of particu-lar parameter values or ranges to either problematic or non-problematic configurations. Examples for selected features  X  session c ached cu rsors and control file r ecord ke ep ti me  X  are shown in Figure 3. 98% of configurations where ses-sion c ached c ursors &lt; 100 and control fi le rec ord k eep t ime &lt; 8 are nonproblematic, whereas all configurations for which these parameters are greater than 150 and 15, respectively, are problematic. These clear trends are easy to detect even for the simple GLMs, therefore explaining the comparable accuracies across all models.
Next, we compare all models in terms of their required running time. Since both Oracle and IBM DB2 sets are similar in size, we use them to build several synthetic sets with 30 input features each and 1000, 5000, 10000 or 25000 distinct configurations. Figure 4 summarizes the results for all models, except GLM-L1 and GLM-L2, which we exclude due to their similar performance to GLM-L12. The short-est running time corresponds to GLM-L12 when run on the smallest set. Thus, we use it as the baseline and for the remaining models we report the increase factor in running time as compared to the baseline. This allows us to ob-serve the increase in execution time relative to the set size increase. As expected, the most expensive model is RF by 150x on the 25000 configurations set, although its trend is linear to the set size.

The GAM-csNI, GAM-tpsNI and GAM-psNI are the fastest relative to the baseline, the largest increase factor being 10 for GAM-psNI when run on the largest set. When adding pairwise interactions, all GAM models become significantly slower. In particular, this is noted for the spline models on larger datasets. For instance, GAM-psI is 100x slower than the baseline on the 25000 configurations set, as compared to GAM-teI (64x), whereas for the smallest set GAM-psI is faster than GAM-teI by 50%. This may be attributed to the fact that gradient boosting converges faster since in each iteration it adds a delta to the existing predictor pool.
Finally, DT is the fastest out of the non-linear models, being 58 times slower than the baseline. Although DT is Figure 4: Computational cost of models across datasets con-sisting of 1000, 5000, 10000 and 25000 configurations. even faster than GAM-teI, we note a steeper increasing trend with the set size (i.e., polynomial), which leads us to expect slower fitting times already at 50000 configurations.
Next, we evaluate the models X  ability to generalize what they learn during fitting on unseen data in three scenarios.
Case #1  X  We train the models on two IT environments and use the third ITE X  X  database configurations for test-ing. Note that this is an extreme scenario, especially since the held-out set contains configurations that are very dif-ferent from the training configurations, thus making it dif-ficult for the models to reach high accuracies. We can only perform the experiment on the IBM DB2 dataset, as the Oracle dataset has several limitations: (1) either the train-ing set contains only nonproblematic configurations, thus not being able to learn problematic ones, or (2) the train set becomes very small, namely less than 45% of the entire set. For IBM DB2, the test set contains 170 configurations, out of which 47% are problematic. The overall accuracy of the models is shown in Figure 5a. As before, RF has the highest accuracy (72%), although it drops 19% compared to the case when all three IT environments contribute to both the train and test sets. The closest model in perfor-mance to RF is GAM-teI, with 67% accuracy. We note two surprising results. First, DT has the lowest accuracy from all models, being outranked even by the GLMs. Sec-ond, the GLMs, and especially GLM-L12, is comparable in performance to all GAMs that do not support feature in-teractions. For brevity, we do not show the models X  ACC  X  and ACC + , but we summarize the results next. All models achieve lower accuracies in correctly identifying the prob-lematic configurations and the best performance is reached by RF with 62%. The other models range between 50% and 60%, which means they reach higher ACC + . Indeed, with the exception of the GLMs and DT, which do not exceed 65% accuracy, all others correctly classify at least 73% of the nonproblematic configurations.

Case #1a  X  We add 50% of the third IT environment X  X  data to the training set used in Case#1 and leave the re-maining 50% for testing. Since the models can learn some of the configurations held-out in Case#1, we expect to ob-tain higher accuracies. Overall accuracy in Figure 5b shows an increase in performance compared to Case#1 by 4% for GLMs, 3-10% for GAMs, 6-10% for GAMs with interactions, 13% for DT and 11% for RF. GLMs and RF have the lowest, respectively the highest performance, with 64% and 83% ac-curacy. DT and GAMs are comparable and correctly classify 69-74% of the configurations, with GAM-csI outperforming GAM-teI by 3%.

Case #2  X  All three IT environments X  configurations are used in both training and testing, but in a temporal manner. More specifically, we train the models on data collected be-tween January and August 2014 and test on the remaining four months. As mentioned before, DBMS are not reconfig-ured often. However, throughout a year, we note at least 2-3 configuration changes, which means the test data con-tains configurations that have not been seen before by the models. The overall accuracy for both IBM DB2 and Oracle is shown in Figures 5c and 5d, respectively.

For Oracle, 14% of configurations in the test set are prob-lematic. RF reaches 84% accuracy, and the closest mod-els in performance are GAM-teI, GAM-teNI and GAM-psI with 81%, 80% and 80% correctly classified configurations, respectively. The gap between RF and GLMs is 11-14%, higher than the 8% observed before, which indicates that GLMs do not generalize as well as more complex models. All models obtain high ACC  X  , above 80% for GLMs and greater than 95% for the remaining, but perform worse in  X  x 2  X  4 . 5  X  x 3  X  2 . 1  X  10  X  2  X  x 4 + 2 . 9  X  10  X  1  X  x  X  x  X  1 . 85  X  10  X  5  X  x 7 + 6 . 1  X  x  X  1 . 2  X  x 3 + 4 . 8  X  10  X  4  X  x 4  X  2 . 4  X  10  X  10  X  x + 1 . 4  X  10  X  8  X  x 7  X  2 . 7  X  10  X  2  X  x 8 &gt; 6.1 and x 3 &gt; 12 years, then problematic 4 &gt; 59000 pages and x 5 &gt; 13000 pages and x 6 &gt; 10, &lt; 8000 pages and x 4 &lt; 5488 pages and x 5 &lt; 12, &lt; 0.54, then nonproblematic 2 &lt; 0.69, then nonproblematic  X  4644 and x 4  X  37, then nonproblematic identifying the nonproblematic configurations (i.e., ACC is in the 70-80% range). 34% of the IBM DB2 configurations in the test set are problematic. RF classifies correctly 77% of configurations, with 14% less than before. Unlike in the initial experiment, all models except GAM-teI have similar performance, be-tween 65% and 67%. As in the previous scenarios, the mod-els achieve lower accuracies in identifying the problematic configurations. Since the same models have an opposite be-havior on the Oracle set, we attribute this trend to the IBM DB2 data. In fact, more than 60% of the problematic config-urations used in the test phase are significantly different than the training ones, thus explaining the lower performance. One of the main reasons to use simpler models, such as GLMs or decision trees, is intelligibility. We discuss this in the context of rule extraction and show examples in Table 3.
GLMs  X  For each input configuration parameter the model provides a coefficient, which can be either positive or nega-tive. This indicates whether the output correlates positively or negatively to the respective parameter. Generalized equa-tions, rather than rules, can be easily built, as the examples shown in Table 3, lines 1 X 2. The output of the equation is the classification probability P . If P &lt; 0.5, then the configu-ration is classified as nonproblematic, else it is problematic. For DB2, parameters x 1 to x 7 are util heap sz , locktime-out , normalized OS version , avg appls , server age , stmtheap F igure 6: Shapes of selected features for GAM-psI (top) and GAM-teI (bottom) on the IBM DB2 dataset. and buffpage . For Oracle, x 1 to x 8 correspond to log size , parallel max servers , server age , dml locks , memory target , sga max size , shared pool size and session cached cursors . Note that the DB2 equation also includes an intercept value of 6.1, whereas for Oracle its value is negligible.
GAMs  X  We show shape functions for selected configu-ration parameters as a result of GAM-psI and GAM-teI in Figure 6. The solid line is the predicted value of the depen-dent variable as a function of parameter x i . For brevity, we focus on the IBM DB2 dataset, although similar shape func-tions are observed for Oracle as well. The selected features are non-linear. The interpretation of the top-left plot, for example, is that a configuration is most likely to be problem-atic if logsecond is on either extreme (e.g., -1 or above 240). Similarly, util heap sz &gt; 20000 pages is strongly correlated with nonproblematic configurations. GAM-psI may appear easier to interpret, due to the smoothness of the shape func-tions. However, there is structure in GAM-teI  X  captured by sharp turns  X  which is either less noticeable or missing in the GAM-psI plots, that is important and justifying the higher accuracy of tree-based GAMs. For instance, the models do not agree on a partial slope for util heap sz and the entire slope for logsecond .

DT  X  Decision trees reveal the basis of the predictive model through the output rules. Examples of such rules are provided in Table 3, lines 3 X 5. For DB2 (lines 3 X 4), pa-rameters x 1 to x 6 are util heap sz , AIX version , server age , stmtheap , buffpage and logsecond . For Oracle (line 5), x to x 4 correspond to dbblock checksum , server age , log buffer and sga max size . We make several observations. First, most of the parameters captured in the GLM equations are present in the example rules for DT. However, for DB2, logsecond is not considered statistically significant in GLMs, which partially explains why these models perform worse in general. Second, typically we observe between 20 and 35 rules. Third, each rule can be constructed from an arbitrary number of parameters, but typically rules with more than 8 to 10 input features are rare. Last, rules are able to capture ranges for continuous parameters and discrete set of values (e.g., dbblock checksum for Oracle), otherwise.

RF  X  Beyond the classification accuracy, the RF output is extremely difficult to interpret, as it is not expressed as either equations or rules. One alternative is to look at the rules applied by each fitted tree. However, since RF gener-ally uses hundreds of such trees, this can be a tedious task. Other solutions involve either building new learning algo-rithms on top or using a DT model, which extracts rules from the RF trained set. To illustrate the second approach, we use the RF-trained data in DT to extract rules. For brevity, we only show example rules for DB2 in Table 3, lines 6 X 7. Parameters x 1 to x 6 correspond to server age , log-primary , dbheap , util heap sz , logsecond and stmtheap . On the one hand, by training with RF, the second and third top-ranked parameters ( dbheap and logprimary ) appear to significantly contribute in classifying problematic and non-problematic configurations, whereas with DT they are omit-ted from the rules. On the other hand, for the configuration parameters where both DT and RF match, the disassociat-ing ranges are either identical or extremely similar.
We split the Oracle dataset by workload type, as show in Table 1. All models perform similarly on the three sets, achieving over 93% accuracy. However, the top-ranked con-figuration parameters differ from workload to workload.
For R + W , x 1 = parallel max servers , x 2 = memory target , x = log buffer and x 4 = DBMS size are the solely most impor-tant features, whereas for RW , x 1 = dbblock checksum and x = memory target are enough to discriminate between prob-lematic and nonproblematic configurations.

Last, x 1 = memory target , x 2 = DBMS size , x 3 = dml locks , x = log buffer and x 5 = server age are ranked highest for the RW + type. This difference in the top parameters implies variation in the rules extracted as well. We train DT on the three subsets and build rules as the examples shown in Table 3, lines 8 X 13. We make several observations. First, the workload type significantly impacts which configuration parameters correlate with incidents. This is expected, since workloads and database behaviors are intrinsically linked. Second, we expect the models to achieve different classifi-cation accuracies depending on the ratio of problematic to nonproblematic configurations in the subsets. The fact that all models perform similarly for Oracle is a limitation of our dataset. Finally, we see that dml locks only matters for RW + workloads, which is expected since it represents the lock obtained on a table undergoing an insert, update or delete operation.
Table 4 summarizes our findings on how the models per-form relative to classification accuracy, computational cost, intelligibility and generalization. There is no clear front-runner, as all models have pros and cons.

Classification accuracy  X  For both IBM DB2 and Ora-cle datasets, the full complexity model, RF, has the high-est accuracy in identifying both problematic and nonprob-lematic configurations, whereas GLMs experience an over-all 25% drop in accuracy and 40% decrease in classifying problematic databases. GAMs with pairwise interactions (GAM-I) reach higher performance than the vanilla GAMs and GAMs implemented with bagged boosted trees perform better than the spline-based ones. Moreover, although it does not support interactions beyond pairwise, GAM-teI outperforms DT, placing itself immediately below RF.
Computational cost  X  High accuracy comes at a high com-putational cost. When compared to GLMs across different set sizes, the remaining models show relatively stable in-crease factors within one order of magnitude: (1) RF is 30-32 times slower, (2) DT is more expensive by 6-12x, but follows a polynomial trend, (3) GAM-I are 14 to 20 times slower with GAM-teI being faster than the spline-based models, and (4) GAMs are slower by 2-7x. These results also show that DT and GAM-I models are 50% faster than RF.

Generalization  X  Models that capture non-linear interac-tions between features have the potential to generalize bet-ter on unseen sets. This is the case for both GAM-I and RF, but not for DT. Not surprisingly, DT overfits on the training data and achieves decreased accuracies on classify-ing configurations that are significantly different than the ones trained. Across all models, we note a somewhat lower accuracy for problematic configurations and a near 100% ac-curacy for nonproblematic ones. This is due to the similar-ities/differences between the train and test configurations, and because we optimize for overall accuracy in all models (i.e., one could optimize for balanced accuracy or F1-score. Table 4: Summary of models comparison ( X  +  X  = worst out-come and  X  ++++  X  = best outcome for each measure). GAM-NI ++ ++++ ++ +++
Intelligibility  X  GLMs and DT are easy to understand, since they automatically output either generalized equations or rules learned. RF models are nearly impossible to inter-pret, unless another learning algorithm (e.g., DT) is applied after training to facilitate rule extraction. GAMs are some-what more intelligible than RF, especially since one can plot the shape functions of the input features. However, as inter-actions are added to increase accuracy, even the visualization of pairwise interactions becomes cumbersome, not to men-tion if interactions between at least 3 features are modeled.
Finally, workload types influence not only the rankings of configuration parameters, but also the actual rules ex-tracted. Considering the scenario in which a database ad-ministrator would need to run a model periodically, as the balance of reads and writes changes, two possible approaches arise. One could use GAM-teI, since it has proven to be highly accurate, and moderately expensive, generalizable and interpretable. Or, to achieve high performance and in-telligibility, but at the cost of computation, RF could be used for training, followed by DT for rule extraction.
We propose using machine learning to understand how configuring a DBMS can lead to high risk incidents. We collect historical data from over 1700 DBMS IBM DB2 and Oracle DBMS belonging to three IT environments over 2014. Then, we implement several linear and non-linear multivari-ate models to identify and learn from high risk configura-tions. We analyze their performance, in terms of accuracy, computational cost, generalization and interpretability. Re-sults show that random forest and GAMs with pairwise in-teractions identify high risk configurations with extremely high accuracy, 91% for IBM DB2 and 96% for Oracle, and generalize well on unseen DBMS configurations (i.e., 75-80% accuracy). However, high accuracy comes at the expense of computational cost and output interpretability. All non-linear models are at least an order of magnitude slower than GLMs. In terms of interpretability, GLMs and decision trees excel at providing rules that can be easily understood and applied by database administrators to reconfigure DBMS. We advocate that some of these models, such as random for-est and decision trees, can be used in combination to achieve high accuracy, generalization and interpretability.
