 Human language technologies are playing an in-creasingly important role in the science and prac-tice of language learning. For example, intelligent Computer Assisted Language Learning (CALL) sys-tems are being developed that can automatically tai-lor lessons and questions to the needs of individual students (Heilman et al., 2006). One critical task that language tutors, word learning experiments, and related applications have in common is assessing the learning progress of the student or experiment sub-ject during the course of the session.

When the task is learning new vocabulary, a vari-ety of tests have been developed to measure word learning progress. Some tests, such as multiple-choice selection of a correct synonym or cloze com-pletion, are relatively passive. In production tests, on the other hand, students are asked to write or say a short phrase or sentence that uses the word being learned, called the target word , in a specified way.
In one important type of production test, called a definition production test, the subject is asked to de-scribe the meaning of the target word, as they under-stand it at that point in the session. The use of such tests has typically required a teacher or researcher to manually score each response by judging its sim-ilarity in meaning to the reference definition of the target word. The resulting scores can then be used to analyze how a person X  X  learning of the word re-sponded to different stimuli, such as seeing the word used in context. A sample target word and its ref-erence definition, along with examples of human-judged responses, are given in Sections 3.3 and 4.1.
However, manual scoring of the definition re-sponses has several drawbacks. First, it is time-consuming and must be done by trained experts. Moreover, if the researcher wanted to test a new hy-pothesis by examining the responses with respect to a different but related definition, the entire set of re-sponses would have to be manually re-scored against the new target. Second, manual scoring can often be limited in its ability to detect when partial learn-ing has taken place. This is due to the basic trade-off between the sophistication of the graded scoring scale, and the ease and consistency with which hu-man judges can use the scale. For example, it may be that the subject did not learn the complete mean-ing of a particular target word, but did learn that this target word had negative connotations. The usual binary or ternary score would provide no or little indication of such effects. Finally, because manual scoring almost always must be done off-line after the end of the session, it presents an obstacle to our goal of creating learning systems that can adapt quickly, within a single learning session.

This study describes an effective automated method for assessing word learning by scoring free responses to definition production tests. The method is flexible: it can be used to analyze a response with respect to whatever reference target(s) the teacher or researcher chooses. Such a test represents a pow-erful new tool for language learning research. It is also a compelling application of human language technologies research on semantic similarity, and we review related work for that area in Section 2. Our probabilistic model for computing text seman-tic similarity, described in Section 3, can use both corpus-based and knowledge-based resources. In Section 4 we describe a new dataset of human def-inition judgments and use it to measure the effec-tiveness of the model against other measures of text similarity. Finally, in Section 5 we discuss further directions and applications of our work. The problem of judging a subject response against a target definition is a type of text similarity problem. Moreover, it is a text semantic similarity task, since we require more than measuring direct word overlap between the two text fragments. For example, if the definition of the target word ameliorate is to improve something and the subject response is make it better , the response clearly indicates that the subject knows the meaning of the word, and thus should receive a high score, even though the response and the target definition have no words in common.

Because most responses are short (1  X  10 words) our task falls somewhere between word-word simi-larity and passage similarity. There is a broad field of existing work in estimating the semantic similar-ity of individual words. This field may be roughly divided into two groups. First, there are corpus-based measures, which use statistics or models de-rived from a large training collection. These require little or no human effort to construct, but are limited in the richness of the features they can reliably repre-sent. Second, there are knowledge-based measures, which rely on specialized resources such as dictio-naries, thesauri, experimental data, WordNet, and so on. Knowledge-based measures tend to be comple-mentary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006).
Because of the fundamental nature of the se-mantic similarity problem, there are close connec-tions with other areas of human language tech-nologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine transla-tion (Jayaraman and Lavie, 2005), text summariza-tion (Mani and Maybury, 1999), and textual co-herence (Foltz et al., 1998). Educational applica-tions include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of short-answer free-response items (Burstein et al., 1999).
As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a lan-guage modeling framework, applied to information retrieval. Our graph-based approach may be viewed as a probabilistic variation on the spreading activa-tion concept, originally proposed for word-word se-mantic similarity by (Quillian, 1967).

Finally, (Mihalcea et al., 2006) describe a text se-mantic similarity measure that combines word-word similarities between the passages being compared. Due to limitations in the knowledge-based similarity measures used, semantic similarity is only estimated between words with the same part-of-speech. Our graph-based approach can relate words of different types and does not have this limitation. (Mihalcea et al., 2006) also evaluate their method in terms of paraphrase recognition using binary judgments. We view our task as somewhat different than paraphrase recognition. First, our task is not symmetric: we do not expect the target definition to be a paraphrase of the subject X  X  free response. Second, because we seek sensitive measures of learning, we want to dis-tinguish a range of semantic differences beyond a binary yes/no decision. We start by describing relations between pairs of terms using a general probability distribution. These pairs can then combine into a graph, which we can apply to define a semantic distance between terms. 3.1 Relations between individual words One way to model word-to-word relationships is us-ing a mixture of links, where each link defines a par-ticular type of relationship. In a graph, this may be represented by a pair of nodes being joined by mul-tiple weighted edges, with each edge correspond-ing to a different link type. Our link-based model is partially based on one defined by (Toutanova et al., 2004) for prepositional attachment. We allow directed edges because some relationships such as hypernyms may be asymmetric. The following are examples of different types of links. 1. Stemming : Two words are based on common 2. Synonyms and near-synonyms : Two words 3. Co-occurrence . Both words tend to appear to-4. Hyper-and hyponyms : Relations such as  X  X 5. Free association : A relation defined by the fact
We denote link functions using  X  summarize different types of interactions between words. Each  X  of lexical or semantic relation or constraint between w weight  X  between w
Our goal is to predict the likelihood of a target definition D given a test response R consisting of terms { w lary V . We are thus interested in the conditional dis-tribution p ( D|R ) . We start by defining a simple model that can combine the link functions in a gen-eral purpose way to produce the conditional distribu-tion p ( w use a log-linear model of the general form In the next sections we show how to combine the estimate of individual pairs p ( w graph of term relations, which will enable us to cal-culate the desired p ( D|R ) . 3.2 Combining term relations using graphs Graphs provide one rich model for representing mul-tiple word relationships. They can be directed or undirected, and typically use nodes of words, with word labels at the vertices, and edges denoting word relationships. In this model, the dependency be-tween two words represents a single inference step in which the label of the destination word is inferred from the source word. Multiple inference steps may then be chained together to perform longer-range in-ference about word relations. In this way, we can in-fer the similarity of two terms without requiring di-rect evidence for the relations between that specific pair. Using the link functions defined in Section 3.1, we imagine a generative process where an author A creates a short text of N words as follows.
 Step 0: Choose an initial word w Step i : Given we have chosen w
This conditional probability may be interpreted as a mixture model in which a particular link type  X  m ( . ) i . Note that the mixture is allowed to change at each timestep. For simplicity, we limit the number of such changes by grouping the timesteps of the walk into three stages: early, middle and final. The func-tion  X ( i ) defines how timestep i maps to stage s , where s  X  X  0 , 1 , 2 } , and we now refer to  X  stead of  X 
Suppose we have a definition D consisting of terms { d transition matrix C ( D , m ) based on the definition D . The reason D influences the transition matrix is that some link types, such as proximity and co-occurrence, are context-specific. Each stage s has an overall transition matrix C ( D , s ) as the mixture of the individual C ( D , m ) , as follows.

Combining the stages over k steps into a single transition matrix, which we denote C k , we have We denote the ( i, j ) entry of a matrix A k by A k Then for a particular term d chain reaches d where we identify w and d indices into the vocabulary V . The overall probabil-ity p ( d a word w is therefore P ( d i | w ) =
The walk continuation probability  X  can be viewed as a penalty for long chains of inference. In practice, to perform the random walk steps we re-place the infinite sum of Eq. 6 with a small number of steps (up to 5) on a sparse representation of the adjacency graph. We obtained effective link weights  X  m ( i ) ity we assume that the same  X  is used across all link types, but further improvement may be possible by extending the model to use link-specific decays  X  Fine-tuning these parameter estimation methods is a subject of future work. 3.3 Using the model for definition scoring In our study the reference definition for the target word consisted of the target word, a rare synonym, a more frequent synonym, and a short glossary-like definition phrase. For example, the reference defini-tion for abscond was abscond; absquatulate; escape; to leave quickly and secretly and hide oneself, often to avoid arrest
In general, we define the score of a response R with respect to a definition D as the probability that the definition is generated by the response, or p ( D|R ) . Equivalently, we can score by log p ( D|R ) since the log function is monotonic. So making the simplifying assumption that the terms d exchangable (the bag-of-words assumption), and taking logarithms, we have: log p ( D|R ) = log Y
Suppose that the response to be scored is run from the cops . In practical terms, Eq. 7 means that for our example, we  X  X ight up X  the nodes in the graph cor-responding to run, from, the and cops by assigning some initial probability, and the graph is then  X  X un X  using the transition matrix C according to Eq. 7. In this study, the initial node probabilities are set to val-ues proportional to the idf values of the correspond-ing term, so that P ( d the probabilities at the nodes for each term in the reference definition R are read off, and their log-arithms summed. Similar to an AND calculation, we calculate a product of sums over the graph, so that responses reflecting multiple aspects of the tar-get definition are rewarded more highly than a very strong prediction for only a single definition term. We first describe our corpus of gold standard human judgments. We then explain the different text sim-ilarity methods and baselines we computed on the corpus responses. Finally, we give an analysis and discussion of the results. 4.1 Corpus We obtained a set of 734 responses to definition pro-duction tests from a word learning experiment at the University of Pittsburgh (Bolger et al., 2006). In total, 72 target words, selected by the same group, were used in the experiment. In this experiment, subjects were asked to learn the meaning of target words after seeing them used in a series of context sentences. We set aside 70 responses for training, leaving 664 responses in the final test dataset.
Each response instance was coded using the scale shown in Table 1, and a sample set of subject re-sponses and scores is shown in Table 2. The target word was treated as having several key aspects of meaning. The coders were instructed to judge a re-sponse according to how well it covered the various aspects of the target definition. If the response cov-ered all aspects of the target definition, but also in-cluded extra irrelevant information, this was treated as a partial match at the discretion of the coders. We obtained three codings of the final dataset. The first two codings were obtained using an in-dependent group, the QDAP Center at the Univer-sity of Pittsburgh. Initially, five human coders, with varying degrees of general coding experience, were Table 1: Scale for human definition judgements. Table 2: Examples of human scores of responses for the target word abscond . trained by the authors using one set of 10 example instances and two training sessions of 30 instances each. Between the two training sessions, one of the authors met with the coders to discuss the ratings and refine the rating guidelines. After training, the authors selected the two coders who had the best inter-coder agreement on the 60 training instances. These two coders then labeled the final test set of 664 instances. Our third coding was obtained from an initial coding created by an expert in the Univer-sity of Pittsburgh Psychology department and then adjusted by one of the authors to resolve a small number of internal inconsistencies, such as when the same response to the same target had been given a different score.

Inter-coder agreement was measured using lin-ear weighted kappa, a standard technique for or-dinal scales. Weighted kappa scores for all three coder pairs are shown in Table 3. Overall, agree-ment ranged from moderate (0.64) to good (0.72). 4.2 Baseline Methods We computed three baselines as reference points for lower and upper performance bounds.

Random. The response items were assigned la-bels randomly. Table 3: Weighted kappa inter-rater reliability for three human coders on our definition response dataset (664 items).
 Table 4: Ability of methods to match human ranking of responses, as measured by Spearman rank corre-lation (corrected for ties).

Human choice of label. We include a method that, given an item and a human label from one of the coders, simply returns a label of the same item from a different coder, with results repeated and averaged over all coders. This gives an indication of an upper bound based on human performance.

Cosine similarity using tf.idf weighting. Cosine similarity is a widely-used text similarity method for tasks where the passages being compared of-ten have significant direct word overlap. We repre-sent response items and reference definitions as vec-tors of terms using tf.idf weighting, a standard tech-nique from information retrieval (Salton and Buck-ley, 1997) that combines term frequency ( tf ) with term specificity ( idf ). A good summary of arguments for using idf can be found in (Robertson, 2004). To compute idf , we used frequencies from a standard 100-million-word corpus of written and spoken En-ity component by applying Porter stemming (Porter, 1980) on terms. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses.
Markov chains ( X  X arkov X ). This is the method described in Section 3. A maximum of 5 random walk steps were used, with a walk continuation probability of 0.8. Each walk step used a mixture of synonym, stem, co-occurrence, and free-association links. The link weights were trained on a small set of held-out data.

Latent Semantic Analysis (LSA). LSA (Lan-dauer et al., 1998) is a corpus-based unsupervised technique that uses dimensionality reduction to clus-ter terms according to multi-order co-occurrence re-lations. In these experiments, we obtained LSA-based similarity scores between responses and target definitions using the software running on the Univer-sity of Colorado LSA Web site (LSA site, 2006). We used the pairwise text passage comparison facility, using the maximum 300 latent factors and a general English corpus (Grade 1  X  first-year college).
Although LSA and the Markov chain approach are based on different principles, we chose to ap-ply LSA to this new response-scoring task and cor-pus because LSA has been widely used as a text se-mantic similarity measure for other tasks and shown good performance (Foltz et al., 1998).

LSA+Markov. To test the effectiveness of com-bining two different  X  and possibly complemen-tary  X  approaches to response scoring, we created a normalized, weighted linear combination of the LSA and Markov scores, with the model combina-tion weight being derived from cross-validation on a held-out dataset. 4.4 Results We measured the effectiveness of each scoring method from two perspectives: ranking quality, and label accuracy.

First, we measured how well each scoring method was able to rank response items by similarity to the target definition. To do this, we calculated the Spear-man Rank Correlation (corrected for ties) between the ranking based on the scoring method and the ranking based on the human-assigned scores, aver-aged over all sets of target word responses.
Table 4 summarizes the ranking results. For Table 5: Root mean squared error (RMSE) of la-bel(s) for top-ranked item, and top-three items for all 77 words in the dataset. overall quality of ranking, the Markov method had significantly better performance than the other au-tomated methods ( p &lt; 2 . 38 e  X  5 ). LSA gave a small, but not significant, improvement in overall ple combination of LSA and Markov resulted in a slightly higher but statistically insignificant differ-ence ( p &lt; 0 . 253 ).

Second, we examined the ability of each method to find the most accurate responses  X  that is, the re-sponses with the highest human label on average  X  for a given target word. To do this, we calculated the Root Mean Squared Error (RMSE) of the label as-signed to the top item, and the top three items. The results are shown in Table 5. For top-item detec-tion, our Markov model had the lowest RMS error (0.7222) of the automated methods, but the differ-ences from Cosine and LSA were not statistically significant, while differences for all three from Ran-dom and Human baselines were significant. For the top three items, the difference between Markov (0.7968) and LSA (0.9965) was significant at the p &lt; 0 . 03 level.

Comparing the overall rank accuracy with top-item accuracy, the combined LSA + Markov method was significantly worse at finding the three best-quality responses (RMSE of 1.0650) than Markov (0.7968) or LSA (0.9965) alone. The reasons for this require further study. Even though definition scoring may seem more straightforward than other automated learning as-sessment problems, human performance was still significantly above the best automated methods in our study, for both ranking and label accuracy. There are certain additions to our model which seem likely to result in further improvement.

One of the most important is the ability to identify phrases or colloquial expressions. Given the short length of a response, these seem critical to handle properly. For example, to get away with something is commonly understood to mean secretly guilty , not a physical action. Yet the near-identical phrase to get away from something means something very dif-ferent when phrases and idioms are considered.
Despite the gap between human and automated performance, the current level of accuracy of the Markov chain approach has already led to some promising early results in word learning research. For example, in a separate study of incremental word learning (Frishkoff et al., 2006), we used our measure to track increments in word knowledge across multiple trials. Each trial consisted of a sin-gle passage that was either supportive  X  containing clues to the meaning of unfamiliar words  X  or not supportive. In this separate study, broad learning ef-fects identified by our measure were consistent with effects found using manually-scored pre-and post-tests. Our automated method also revealed a pre-viously unknown interaction between trial spacing, the proportion of supportive contexts per word, and reader skill.

In future applications, we envision using our auto-mated measure to allow a form of feedback for intel-ligent language tutors, so that the system can auto-matically adapt its behavior based on the student X  X  test responses. With some adjustments, the same scoring model described in this study may also be applied to the problem of finding supportive contexts for students. We presented results for both automated and hu-man performance of an important task for language learning applications: scoring definition responses. We described a probabilistic model of text seman-tic similarity that uses Markov chains on a graph of term relations to perform a kind of semantic smooth-ing. This model incorporated both corpus-based and knowledge-based resources to compute text seman-tic similarity. We measured the effectiveness of both our method and LSA compared to cosine and ran-dom baselines, using a new corpus of human judg-ments on definition responses from a language learn-ing experiment. Our method outperformed the tf.idf cosine similarity baseline in ranking quality and in ability to find high-scoring definitions. Because LSA and our Markov chain method are based on different approaches and resources, it is difficult to draw definitive conclusions about performance dif-ferences between the two methods.

Looking beyond definition scoring, we believe au-tomated methods for assessing word learning have great potential as a new scientific tool for language learning researchers, and as a key component of in-telligent tutoring systems that can adapt to students. We thank D.J. Bolger and C. Perfetti for the use of their definition response data, Stuart Shulman for his guidance of the human coding effort, and the anonymous reviewers for their comments. This work supported by U.S. Dept. of Education grant R305G03123. Any opinions, findings, and conclu-sions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.

