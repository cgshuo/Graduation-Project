 Martin Jaggi  X  CMAP,  X  Ecole Polytechnique, Palaiseau, France Binary SVMs are amongst the most popular classifica-tion methods, and this has motivated substantial in-terest in optimization solvers that are tailored to their specific problem structure. However, despite their wider applicability, there has been much less work on solving the optimization problem associated with structural SVMs, which are the generalization of SVMs to structured outputs like graphs and other combinato-rial objects (Taskar et al., 2003; Tsochantaridis et al., 2005). This seems to be due to the difficulty of dealing with the exponential number of constraints in the pri-mal problem, or the exponential number of variables in the dual problem. Indeed, because they achieve an  X  O (1 / X  ) convergence rate while only requiring a single call to the so-called maximization oracle on each it-eration, basic stochastic subgradient methods are still widely used for training structural SVMs (Ratliff et al., 2007; Shalev-Shwartz et al., 2010a). However, these methods are often frustrating to use for practitioners, because their performance is very sensitive to the se-quence of step sizes, and because it is difficult to decide when to terminate the iterations.
 To solve the dual structural SVM problem, in this paper we consider the Frank-Wolfe (1956) algorithm, which has seen a recent surge of interest in machine learning and signal processing (Mangasarian, 1995; Clarkson, 2010; Jaggi, 2011; 2013; Bach et al., 2012), including in the context of binary SVMs (G  X artner &amp; Jaggi, 2009; Ouyang &amp; Gray, 2010). A key advan-tage of this algorithm is that the iterates are sparse , and we show that this allows us to efficiently apply it to the dual structural SVM objective even though there are an exponential number of variables. A sec-ond key advantage of this algorithm is that the it-erations only require optimizing linear functions over the constrained domain, and we show that this is equivalent to the maximization oracle used by sub-gradient and cutting-plane methods (Joachims et al., 2009; Teo et al., 2010). Thus, the Frank-Wolfe al-gorithm has the same wide applicability as subgradi-ent methods, and can be applied to problems such as low-treewidth graphical models (Taskar et al., 2003), graph matchings (Caetano et al., 2009), and associa-tive Markov networks (Taskar, 2004). In contrast, other approaches must use more expensive (and poten-tially intractable) oracles such as computing marginals over labels (Collins et al., 2008; Zhang et al., 2011) or doing a Bregman projection onto the space of struc-tures (Taskar et al., 2006). Interestingly, for struc-tural SVMs we also show that existing batch subgra-dient and cutting-plane methods are special cases of Frank-Wolfe algorithms, and this leads to stronger and simpler O (1 / X  ) convergence rate guarantees for these existing algorithms.
 As in other batch structural SVM solvers like cutting-plane methods (Joachims et al., 2009; Teo et al., 2010) and the excessive gap technique (Zhang et al., 2011) (see Table 1 at the end for an overview), each Frank-Wolfe iteration unfortunately requires calling the ap-propriate oracle once for all training examples, unlike the single oracle call needed by stochastic subgradient methods. This can be prohibitive for data sets with a large number of training examples. To reduce this cost, we propose a novel randomized block-coordinate version of the Frank-Wolfe algorithm for problems with block-separable constraints. We show that this al-gorithm still achieves the O (1 / X  ) convergence rate of the full Frank-Wolfe algorithm, and in the context of structural SVMs, it only requires a single call to the maximization oracle. Although the stochastic subgra-dient and the novel block-coordinate Frank-Wolfe al-gorithms have a similar iteration cost and theoretical convergence rate for solving the structural SVM prob-lem, the new algorithm has several important advan-tages for practitioners:  X  The optimal step-size can be efficiently computed  X  The algorithm yields a duality gap guarantee, and  X  The convergence rate holds even when using ap-Further, our experimental results show that the op-timal step-size leads to a significant advantage during the first few passes through the data, and a systematic (but smaller) advantage in later passes. We first briefly review the standard convex optimiza-tion setup for structural SVMs (Taskar et al., 2003; Tsochantaridis et al., 2005). In structured predic-tion, the goal is to predict a structured object y  X  Y ( x ) (such as a sequence of tags) for a given in-put x  X  X . In the standard approach, a structured feature map  X  : X  X  Y  X  R d encodes the rele-vant information for input/output pairs, and a lin-ear classifier with parameter w is defined by h w ( x ) = set D = { ( x i , y i ) } n i =1 , w is estimated by solving min s.t.  X  w ,  X  i ( y )  X  X  X  L ( y i , y )  X   X  i  X  i,  X  y  X  where  X  i ( y ) :=  X  ( x i , y i )  X   X  ( x i , y ), and L L ( y i , y ) denotes the task-dependent structured error of predicting output y instead of the observed output y i (typically a Hamming distance between the two la-bels). The slack variable  X  i measures the surrogate loss for the i -th datapoint and  X  is the regularization parameter. The convex problem (1) is what Joachims et al. (2009, Optimization Problem 2) call the n -slack structural SVM with margin-rescaling. A variant with slack-rescaling was proposed by Tsochantaridis et al. (2005), which is equivalent to our setting if we replace all vectors  X  i ( y ) by L i ( y )  X  i ( y ).
 Loss-Augmented Decoding. Unfortunately, the above problem can have an exponential number of con-straints due to the combinatorial nature of Y . We can replace the linear ones by defining the structured hinge-loss: The constraints in (1) can thus be replaced with the non-linear ones  X  i  X   X  H i ( w ). The computation of the structured hinge-loss for each i amounts to finding the most  X  X iolating X  output y for a given input x i , a task which can be carried out efficiently in many structured prediction settings (see the introduction). This prob-lem is called the loss-augmented decoding subproblem. In this paper, we only assume access to an efficient solver for this subproblem, and we call such a solver a maximization oracle . The equivalent non-smooth un-constrained formulation of (1) is: Having a maximization oracle allows us to apply subgradient methods to this problem (Ratliff et al., 2007), as a subgradient of  X  H i ( w ) with respect to w is  X   X  i ( y  X  i ), where y  X  i is any maximizer of the loss-augmented decoding subproblem (2).
 The Dual. The Lagrange dual of the above n -slack-formulation (1) has m :=  X  X upport vectors X . Writing  X  i ( y ) for the dual variable associated with the training example i and potential output y  X  X  i , the dual problem is given by where the matrix A  X  R d  X  m consists of the m columns A := 1 dual variable vector  X  , we can use the Karush-Kuhn-Tucker optimality conditions to obtain the correspond-ing primal variables w = A  X  = see Appendix E. The gradient of f then takes the sim-ple form  X  f (  X  ) =  X A T A  X   X  b =  X A T w  X  b ; its ( i, y )-th component is  X  1 note that the domain M  X  R m of (4) is the product of n probability simplices, M :=  X  |Y We consider the convex optimization problem min  X   X  X  f (  X  ), where the convex feasible set M is compact and the convex objective f is continuously differentiable . The Frank-Wolfe algorithm (1956) (shown in Algorithm 1) is an iterative optimiza-tion algorithm for such problems that only requires optimizing linear functions over M , and thus has wider applicability than projected gradient algorithms, which require optimizing a quadratic function over M . At every iteration, a feasible search corner s is first found by minimizing over M the linearization of f at the current iterate  X  (see picture in inset). The next iterate is then obtained as a convex combina-tion of s and the previous iterate, with step-size  X  .
 These simple up-dates yield two interesting proper-ties. First, every the starting point  X  (0) and the search corners s found representation, which makes the algorithm suitable even for cases where the dimensionality of  X  is exponential. Second, since f is convex, the minimum of the linearization of f over M immediately gives a lower bound on the value of the yet unknown optimal solution f (  X   X  ). Every step of the algorithm thus computes for free the following  X  X inearization duality gap X  defined for any feasible point  X   X  M (which is in fact a special case of the Fenchel duality gap as Algorithm 1 Frank-Wolfe on a Compact Domain
Let  X  (0)  X  X  for k = 0 ...K do explained in Appendix D): g (  X  ) := max As g (  X  )  X  f (  X  )  X  f (  X   X  ) by the above argument, s thus readily gives at each iteration the current dual-ity gap as a certificate for the current approximation quality (Jaggi, 2011; 2013), allowing us to monitor the convergence, and more importantly to choose the the-oretically sound stopping criterion g (  X  ( k ) )  X   X  . In terms of convergence, it is known that after O (1 / X  ) iterations, Algorithm 1 obtains an  X  -approximate solu-tion (Frank &amp; Wolfe, 1956; Dunn &amp; Harshbarger, 1978) as well as a guaranteed  X  -small duality gap (Clarkson, 2010; Jaggi, 2013), along with a certificate to (5). For the convergence results to hold, the internal linear sub-problem does not need to be solved exactly, but only to some error. We review and generalize the conver-gence proof in Appendix C. The constant hidden in the O (1 / X  ) notation is the curvature constant C f , an affine invariant quantity measuring the maximum de-viation of f from its linear approximation over M (it yields a weaker form of Lipschitz assumption on the gradient, see e.g. Appendix A for a formal definition). Note that classical algorithms like the projected gra-dient method cannot be tractably applied to the dual of the structural SVM problem (4), due to the large number of dual variables. In this section, we explain how the Frank-Wolfe method (Algorithm 1) can be efficiently applied to this dual problem, and discuss its relationship to other algorithms. The main in-sight here is to notice that the linear subproblem em-ployed by Frank-Wolfe is actually directly equivalent to the loss-augmented decoding subproblem (2) for each datapoint, which can be solved efficiently (see Appendix B.1 for details). Recall that the optimiza-tion domain for the dual variables  X  is the product Algorithm 2 Batch Primal-Dual Frank-Wolfe Algo-rithm for the Structural SVM
Let w (0) := 0 , ` (0) := 0 for k = 0 ...K do of n simplices, M =  X  |Y simplex consists of a potentially exponential number |Y i | of dual variables, we cannot maintain a dense vec-tor  X  during the algorithm. However, as mentioned in rithm is a sparse convex combination of the previously visited corners s and the starting point  X  (0) , and so we only need to maintain the list of previously seen solutions to the loss-augmented decoding subproblems to keep track of the non-zero coordinates of  X  , avoid-ing the problem of its exponential size. Alternately, if we do not use kernels, we can avoid the quadratic explosion of the number of operations needed in the A Primal-Dual Frank-Wolfe Algorithm for the Structural SVM Dual. Applying Algorithm 1 with line search to the dual of the structural SVM (4), but only maintaining the corresponding primal primal that the Frank-Wolfe search corner s = ( e y  X  1 ,..., e y which is obtained by solving the loss-augmented sub-problems, yields the update w s = A s . We use the nat-w (0) = 0 as  X  i ( y i ) = 0  X  i .
 The Duality Gap. The duality gap (5) for our structural SVM dual formulation (4) is given by where s is an exact minimizer of the linearized prob-lem given at the point  X  . This (Fenchel) duality gap turns out to be the same as the Lagrangian duality gap here (see Appendix B.2), and gives a direct handle on Using w s := A s and ` s := b T s , we observe that the gap is efficient to compute given the primal variables w := A  X  and ` := b T  X  , which are maintained dur-ing the run of Algorithm 2. Therefore, we can use the duality gap g (  X  ( k ) )  X   X  as a proper stopping criterion. Implementing the Line-Search. Because the ob-jective of the structural SVM dual (4) is sim-ply a quadratic function in  X  , the optimal step-size for any given candidate search point s  X  M can be obtained analytically . Namely,  X  LS := argmin  X   X  [0 , 1] f  X  +  X  s  X   X  is obtained by setting the derivative of this univariate quadratic function in  X  to zero, which here (before restricting to [0 , 1]) gives  X  and 4).
 Convergence Proof and Running Time. In the following, we write R for the maximal length of a dif-and we write the maximum error as L max := max i, y L i ( y ). By bounding the curvature constant C for the dual SVM objective (4), we can now directly apply the known convergence results for the standard Frank-Wolfe algorithm to obtain the following primal-dual rate (proof in Appendix B.3): Theorem 1. Algorithm 2 obtains an  X  -approximate solution to the structural SVM dual problem (4) and duality gap g (  X  ( k ) )  X   X  after at most O R 2  X  X  itera-tions, where each iteration costs n oracle calls. Since we have proved that the duality gap is smaller than  X  , this implies that the original SVM primal ob-jective (3) is actually solved to accuracy  X  as well. Relationship with the Batch Subgradient Method in the Primal. Surprisingly, the batch Frank-Wolfe method (Algorithm 2) is equivalent to the batch subgradient method in the primal, though Frank-Wolfe allows a more clever choice of step-size, since line-search can be used in the dual. To see the equivalence, notice that a subgradient of (3) is given by d sub =  X  w  X  1 y i and w s are as defined in Algorithm 2. Hence, for a step-size of  X  , the subgradient method update becomes (1  X   X  X  ) w ( k ) +  X  X  w s . Comparing this with Algo-rithm 2, we see that each Frank-Wolfe step on the dual problem (4) with step-size  X  is equivalent to a batch subgradient step in the primal with a step-size of  X  =  X / X  , and thus our convergence results also ap-ply to it. This seems to generalize the equivalence be-tween Frank-Wolfe and the subgradient method for a quadratic objective with identity Hessian as observed by Bach et al. (2012, Section 4.1).
 Relationship with Cutting Plane Algorithms. In each iteration, the cutting plane algorithm of Joachims et al. (2009) and the Frank-Wolfe method (Algorithm 2) solve the loss-augmented decoding prob-lem for each datapoint, selecting the same new  X  X ctive X  coordinates to add to the dual problem. The only dif-ference is that instead of just moving towards the cor-ner s , as in classical Frank-Wolfe, the cutting plane al-gorithm re-optimizes over all the previously added  X  X c-tive X  dual variables (this task is a quadratic program). This shows that the method is exactly equivalent to the  X  X ully corrective X  variant of Frank-Wolfe, which in each iteration re-optimizes over all previously visited corners (Clarkson, 2010; Shalev-Shwartz et al., 2010b). Note that the convergence results for the  X  X ully correc-Algorithm 3 Block-Coordinate Frank-Wolfe Algo-rithm on Product Domain for k = 0 ...K do tive X  variant directly follow from the ones for Frank-Wolfe (by inclusion), thus our convergence results ap-ply to the cutting plane algorithm of Joachims et al. (2009), significantly simplifying its analysis. A major disadvantage of the standard Frank-Wolfe al-gorithm when applied to the structural SVM problem is that each iteration requires a full pass through the data, resulting in n calls to the maximization oracle. In this section, we present the main new contribution of the paper: a block-coordinate generalization of the Frank-Wolfe algorithm that maintains all appealing properties of Frank-Wolfe, but yields much cheaper iterations, requiring only one call to the maximization oracle in the context of structural SVMs. The new method is given in Algorithm 3, and applies to any constrained convex optimization problem of the form where the domain has the structure of a Cartesian product M = M (1)  X  ...  X M ( n )  X  R m over n  X  1 blocks. The main idea of the method is to per-form cheaper update steps that only affect a single ously. This is motivated by coordinate descent meth-ods, which have a very successful history when ap-plied to large scale optimization. Here we assume that each factor M ( i )  X  R m i is convex and compact , with m = block of coordinates of a vector  X   X  R m . In each step, Algorithm 3 picks one of the n blocks uniformly at ran-dom, and leaves all other blocks unchanged. If there is only one block ( n = 1), then Algorithm 3 becomes the standard Frank-Wolfe Algorithm 1. The algorithm can be interpreted as a simplification of Nesterov X  X   X  X uge-scale X  uniform coordinate descent method (Nesterov, 2012, Section 4). Here, instead of computing a pro-jection operator on a block (which is intractable for structural SVMs), we only need to solve one linear sub-problem in each iteration, which for structural SVMs is equivalent to a call to the maximization oracle. Algorithm 4 Block-Coordinate Primal-Dual Frank-Wolfe Algorithm for the Structural SVM for k = 0 ...K do Convergence Results. The following main theo-rem shows that after O (1 / X  ) many iterations, Algo-rithm 3 obtains an  X  -approximate solution to (6), and guaranteed  X  -small duality gap (proof in Appendix C). Here the constant C  X  f := (partial) curvature constants of f with respect to the schitz assumption on the gradient in more details in Appendix A, where we compute the constant precisely for the structural SVM and obtain C  X  f = C f /n , where C f is the classical Frank-Wolfe curvature.
 Theorem 2. For each k  X  0 , the iterate  X  ( k ) of Algorithm 3 (either using the predefined step-sizes, or using line-search) satisfies E f (  X  ( k ) )  X  f (  X   X  )  X  the starting point of the algorithm, and the expectation is over the random choice of the block i in the steps of the algorithm.
 Furthermore, if Algorithm 3 is run for K  X  0 itera-tions, then it has an iterate  X  (  X  k ) , 0  X   X  k  X  K , with duality gap bounded by E g (  X  (  X  k ) )  X  6 n K +1 C  X  f Application to the Structural SVM. Algo-rithm 4 applies the block-coordinate Frank-Wolfe al-gorithm with line-search to the structural SVM dual problem (4), maintaining only the primal variables w . We see that Algorithm 4 is equivalent to Algorithm 3, by observing that the corresponding primal updates Note that Algorithm 4 has a primal parameter vector w i (= A  X  [ i ] ) for each datapoint i , but that this does not significantly increase the storage cost of the algo-rithm since each w i has a sparsity pattern that is the union of the corresponding  X  i ( y  X  i ) vectors. If the fea-ture vectors are not sparse, it might be more efficient to work directly in the dual instead (see the kernel-ized version below). The line-search is analogous to the batch Frank-Wolfe case discussed above, and for-malized in Appendix B.4.
 By applying Theorem 2 to the SVM case where C  X  f = C /n = 4 R 2 / X n (in the worst case), we get that the number of iterations needed for our new block-wise Al-gorithm 4 to obtain a specific accuracy  X  is the same as for the batch version in Algorithm 2 (under the assumption that the initial error h 0 is smaller than 4 R 2 / X n ), even though each iteration takes n times fewer oracle calls. If h 0 &gt; 4 R 2 / X n , we can use the fact that Algorithm 4 is using line-search to get a weaker dependence on h 0 in the rate (Theorem C.4). We summarize the overall rate as follows (proof in Ap-pendix B.3): rithm 4 obtains an  X  -approximate solution to the struc-tural SVM dual problem (4) and expected duality gap each iteration costs a single oracle call.
 (constant in  X  ) number of O n log  X nL max R 2 steps to get the same error and duality gap guarantees. In terms of  X  , the O (1 / X  ) convergence rate above is similar to existing stochastic subgradient and cutting-plane methods. However, unlike stochastic sub-gradient methods, the block-coordinate Frank-Wolfe method allows us to compute the optimal step-size at each iteration (while for an additional pass through the data we can evaluate the duality gap (5) to allow us to decide when to terminate the algorithm in practice). Further, unlike cutting-plane methods which require n oracle calls per iteration, this rate is achieved  X  X nline X , using only a single oracle call per iteration. Approximate Subproblems and Decoding. In-terestingly, we can show that all the convergence re-sults presented in this paper also hold if only approx-imate minimizers of the linear subproblems are used instead of exact minimizers. If we are using an ap-proximate oracle giving candidate directions s ( i ) in Al-gorithm 3 (or s in Algorithm 1) with a multiplicative accuracy  X   X  (0 , 1] (with respect to the the duality gap (5) on the current block), then the above conver-gence bounds from Theorem 2 still apply. The only change is that the convergence is slowed by a factor of 1 / X  2 . We prove this generalization in the Theo-rems of Appendix C. For structural SVMs, this signif-icantly improves the applicability to large-scale prob-lems, where exact decoding is often too costly but ap-proximate loss-augmented decoding may be possible. Kernelized Algorithms. Both Algorithms 2 and 4 can directly be used with kernels by maintaining the implicitly as a sparse combination of the correspond-ing kernel functions, i.e. w = A  X  . Using our Algo-rithm 4, we obtain the currently best known bound on the number of support vectors , i.e. a guaranteed  X  -approximation with only O ( R 2 In comparison, the standard cutting plane method (Joachims et al., 2009) adds n support vectors  X  i ( y  X  i at each iteration. More details on the kernelized vari-ant of Algorithm 4 are discussed in Appendix B.5. We compare our novel Frank-Wolfe approach to ex-isting algorithms for training structural SVMs on the OCR dataset ( n = 6251 ,d = 4028) from Taskar et al. (2003) and the CoNLL dataset ( n = 8936 ,d = 1643026) from Sang &amp; Buchholz (2000). Both datasets are se-quence labeling tasks, where the loss-augmented de-coding problem can be solved exactly by the Viterbi algorithm. Our third application is a word alignment problem between sentences in different languages in the setting of Taskar et al. (2006) ( n = 5000 ,d = 82) . Here, the structured labels are bipartite matchings, for which computing marginals over labels as required by the methods of Collins et al. (2008); Zhang et al. (2011) is intractable, but loss-augmented decoding can be done efficiently by solving a min-cost flow problem. We compare Algorithms 2 and 4, the batch Frank-Wolfe method ( FW ) 1 and our novel block-coordinate Frank-Wolfe method ( BCFW ), to the cutting plane al-gorithm implemented in SVMstruct (Joachims et al., 2009) with its default options, the online exponen-tiated gradient ( online-EG ) method of Collins et al. (2008), and the stochastic subgradient method ( SSG ) with step-size chosen as in the  X  X egasos X  version of Shalev-Shwartz et al. (2010a). We also include the erates from SSG (called SSG-wavg ) which was recently shown to converge at the faster rate of O (1 /k ) instead of O ((log k ) /k ) (Lacoste-Julien et al., 2012; Shamir &amp; Zhang, 2013). Analogously, we average the iterates from BCFW the same way to obtain the BCFW-wavg method (implemented efficiently with the optional line in Algorithm 4), which also has a provable O (1 /k ) convergence rate (Theorem C.3). The performance of the different algorithms according to several criteria is visualized in Figure 1. The results are discussed in the caption, while additional experiments can be found in Appendix F. In most of the experiments, the BCFW-wavg method dominates all competitors. The superiority is especially striking for the first few itera-tions, and when using a small regularization strength  X  , which is often needed in practice. In term of test error, a peculiar observation is that the weighted av-erage of the iterates seems to help both methods sig-nificantly: SSG-wavg sometimes slightly outperforms BCFW-wavg despite having the worst objective value amongst all methods. This phenomenon is worth fur-ther investigation. There has been substantial work on dual coordinate descent for SVMs, including the original sequential minimal optimization (SMO) algorithm. The SMO al-gorithm was generalized to structural SVMs (Taskar, 2004, Chapter 6), but its convergence rate scales badly with the size of the output space: it was estimated as O ( n |Y| / X  X  ) in Zhang et al. (2011). Further, this method requires an expectation oracle to work with its factored dual parameterization. As in our algo-rithm, Rousu et al. (2006) propose updating one train-ing example at a time, but using multiple Frank-Wolfe updates to optimize along the subspace. However, they do not obtain any rate guarantees and their algo-rithm is less general because it again requires an ex-pectation oracle. In the degenerate binary SVM case, our block-coordinate Frank-Wolfe algorithm is actu-ally equivalent to the method of Hsieh et al. (2008), where because each datapoint has a unique dual vari-able, exact coordinate optimization can be accom-plished by the line-search step of our algorithm. Hsieh et al. (2008) show a local linear convergence rate in the dual, and our results complement theirs by providing a global primal convergence guarantee for their algo-rithm of O (1 / X  ). After our paper had appeared on arXiv, Shalev-Shwartz &amp; Zhang (2012) have proposed a generalization of dual coordinate descent applicable to several regularized losses, including the structural SVM objective. Despite being motivated from a differ-ent perspective, a version of their algorithm (Option II of Figure 1) gives the exact same step-size and update direction as BCFW with line-search, and their Corol-lary 3 gives a similar convergence rate as our Theo-rem 3. Balamurugan et al. (2011) propose to approx-imately solve a quadratic problem on each example using SMO , but they do not provide any rate guar-antees. The online-EG method implements a variant of dual coordinate descent, but it requires an expecta-tion oracle and Collins et al. (2008) estimate its primal convergence at only O 1 / X  2 .
 Besides coordinate descent methods, a variety of other algorithms have been proposed for structural SVMs. We summarize a few of the most popular in Table 1, with their convergence rates quoted in number of ora-cle calls to reach an accuracy of  X  . However, we note that almost no guarantees are given for the optimiza-tion of structural SVMs with approximate oracles. A regret analysis in the context of online optimization was considered by Ratliff et al. (2007), but they do not analyze the effect of this on solving the optimization problem. The cutting plane algorithm of Tsochan-taridis et al. (2005) was considered with approximate maximization by Finley &amp; Joachims (2008), though the dependence of the running time on the the approx-imation error was left unclear. In contrast, we pro-vide guarantees for batch subgradient, cutting plane, and block-coordinate Frank-Wolfe, for achieving an  X  -approximate solution as long as the error of the oracle is appropriately bounded. This work proposes a novel randomized block-coordinate generalization of the classic Frank-Wolfe algorithm for optimization with block-separable con-straints. Despite its potentially much lower iteration cost, the new algorithm achieves a similar convergence rate in the duality gap as the full Frank-Wolfe method. For the dual structural SVM optimization problem, it leads to a simple online algorithm that yields a solu-tion to an issue that is notoriously difficult to address for stochastic algorithms: no step-size sequence needs to be tuned since the optimal step-size can be effi-ciently computed in closed-form. Further, at the cost of an additional pass through the data (which could be done alongside a full Frank-Wolfe iteration), it al-lows us to compute a duality gap guarantee that can be used to decide when to terminate the algorithm. Our experiments indicate that empirically it converges faster than other stochastic algorithms for the struc-tural SVM problem, especially in the realistic setting where only a few passes through the data are possible. Although our structural SVM experiments use an exact maximization oracle, the duality gap guaran-tees, the optimal step-size, and a computable bound on the duality gap are all still available when only an appropriate approximate maximization oracle is used. Finally, although the structural SVM problem is what motivated this work, we expect that the block-coordinate Frank-Wolfe algorithm may be useful for other problems in machine learning where a complex objective with block-separable constraints arises. Acknowledgements. We thank Francis Bach, Bernd G  X artner and Ronny Luss for helpful discussions, and Robert Carnecky for the 3D illustration. MJ is supported by the ERC Project SIPA, and by the Swiss National Science Foundation. SLJ and MS are partly supported by the ERC (SIERRA-ERC-239993). SLJ is supported by a Research in Paris fellowship. MS is supported by a NSERC postdoctoral fellowship.

