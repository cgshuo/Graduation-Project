 ways by presentation order. One pervasive phenomenon is the spacing effect, manifested in the presenting them in an interleaved manner (e.g., A in consecutive blocks (e.g., A various types of learning in other species, including rats a nd Aplysia [4][5]. Standard statistical methods based on summary information are unable to deal with order effects, including the performance difference between spaced and ma ssed conditions. From a computa-Bayesian sequential models have been successfully applied to model causal learning and animal of the parameters for each category given all the observatio ns x However, given that both the mean and the variance of a catego ry are random variables, standard is not warranted in the current application.
 approach can be generalized to efficiently perform model sel ection.
 of the model. Section 3 and 4 demonstrate how to develop super vised and unsupervised learning models, which can be compared with human performance. We dra w general conclusions in section 5. model in which learning is assumed to be a Markov process with unobserved states. The exemplars unknown parameters, means and variances. These two unknown parameters need to be learned from a limited number of exemplars (e.g., less than ten exemplars ).
 can change over time and is denoted by m prior distribution P ( m the temporal prior P ( m Bayes-Kalman [11] gives iterative equations to determine t he posterior P ( m of observations X correction: Intuitively, the Bayes-Kalman first predicts the distribution P ( m prior to correct for the new observation x Note that the temporal prior P ( m 2.1 Conjugate priors The distributions P ( m bution P ( m tributions. The distributions are specified in terms of Gamm a and Gaussian distributions: We specify the prior P ( m Gaussian distribution with parameters  X ,  X ,  X ,  X  .
 The likelihood function and temporal prior are both Gaussia ns: where  X ,  X  are constants.
 The conjugacy of the distributions ensures that the posteri or distribution P ( m a Gamma-Gaussian distribution with parameters  X  rameters are specified in the next section. 2.2 Update rules for the model parameters Kalman equations 1, 2. We sketch how these update rules are ob tained assuming that P ( m is a Gamma-Gaussian with parameters  X  new observation x 2.3 Model evidence We also need to compute the probability of the observation se quence X be used later for model selection). This can be expressed rec ursively as: tion (9) can be expressed as P ( x integrals can be calculated analytically yielding: predictions with human data in psychological experiments. 3.1 Two-category learning with supervision We first conduct a synthetic experiment with two categories u nder supervision. We generate six conditions are included, a massed condition with the data pr esentation order of AAABBB and a spaced condition with the order of ABABAB.
 updates the parameters corresponding to the category that p roduced the observation based on the supervision (i.e., known category membership), following equation (8).
 moment-moment variance of the states is higher for faster ti mescales (p. 779). will update the  X  in which  X  Figure 1: Posterior distributions of means P ( m for the massed condition (i.e., AAABBB), and the bottom pane l shows the results for the spaced save space. See section 3.1.
 Figure (1) shows the change of posterior distributions of th e two unknown category parameters, means P ( m resentation in the form of the posterior distribution of P ( x two-category supervised learning. Figure 2: Posterior distribution of each category, P ( x category supervised learning. Same conventions as in figure (1). See section 3.1. 3.2 Modeling the spacing effect in six-category learning decreased with more test blocks.
 ances for each category. To compare with human performance r eported by Kornell and Bjork, the that the model predictions match human performance well. Both humans and animals can learn without supervision. For e xample, in the animal conditioning preexposure to two stimuli A and B (massed or spaced) determi nes the degree to which backward Figure 3: Posterior distribution of each category, P ( x category supervised learning. Same conventions as in figure (1). See section 3.2. of presentation training conditions (massed and spaced) an d test block. See section 3.2. importance of supervision in training by comparing perform ance after unsupervised learning with that after supervised learning.
 We consider a model with two hidden categories. Each categor y can be represented as a Gaussian provided, We specify prior distributions and temporal priors as befor e: The joint posterior distribution P ( m 1 tained by applying the Bayes-Kalman update rules to the join t distribution  X  i.e., replace ( m whether the new observation x assignments at time t . This can be performed efficiently in a recursive manner. Let A of possible assignments at time t where each assignment is a string ( a for P ( m 1
P ( m 1 t , r 1 , m 2 t , r 2 | X t ) = X where  X  i for observation sequence ( a At t = 0 there is no observation sequence and P ( m 1 which corresponds to A The prediction stage updates the  X  component of ~ X  i ( a We define  X  i ( a the generic prior described in section 3.1.
 two categories. This gives a new set A posterior: where we compute ~ X  i  X  and we compute P ( a where The model selection can, as before, be expressed as P ( x We can now address the problem posed by Balleine et. al. X  X  pre exposure experiments [4]  X  why equations (9,22), for the two cases AAABBB (massed) and ABAB AB (spaced). We use the same data as described in section (3.1) but without providing category membership for evidence for the one-category model with model evidence for the two-category model. A greater agree with with Balleine et. al. X  X  findings.
 Right, comparison of supervised and unsupervised learning in terms of accuracy. See section 4.3. To assess the influence of supervision on learning, we compar e performance between supervised or unknown category membership (unsupervised) for each tra ining observation. Accuracy measured learning. for humans and other animals.
 pattern of power-law forgetting that is fairly universal in human memory [14] a pruning strategy to keep the complexity practical.
 Acknowledgement This research was supported a grant from Air Force FA 9550-08 -1-0489.
