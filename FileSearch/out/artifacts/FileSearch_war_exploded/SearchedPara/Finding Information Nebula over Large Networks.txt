 Social and information networks have been extensively studied over years. In this paper, we concentrate ourselves on a large informa-tion network that is composed of entities and relationships, where entities are associated with sets of keyword terms (kterms) to spec-ify what they are, and relationships describe the link structure among entities which can be very complex. Our work is motivated but is different from the existing works that find a best subgraph to de-scribe how user-specified entitie s are connected. We compute in-formation nebula (cloud) which is a set of top-K kterms P that are most correlated to a set of user-specified kterms Q , over a large in-formation network. Our goal is to find how kterms are correlated given the complex information network among entities. The infor-mation nebula computing requests us to take all possible kterms into consideration for the top-K kterms selection, and needs to measure the similarity between kterms by considering all possible subgraphs that connect them instead of the best single one. In this work, we compute information nebula using a global structural-context similarity, and our similarity measure is independent of connection subgraphs. To the best of our knowledge, among the link-based similarity methods, none of the existing work considers similarity between two sets of nodes or two kterms. We propose new algorithms to find top-K kterms P for a given set of kterms Q based on the global structural-context similarity, without com-puting all the similarity scores of kterms in the large information network. We performed extensive performance studies using large real datasets, and confirmed the effectiveness and efficiency of our approach.
 H.3 [ Information Storage and Retrieval ]: Miscellaneous Design, Management, Performance
Social and information systems analysis plays an important role in modern knowledge discovery [12, 16, 14] to extract information and/or discover knowledge from large information networks [10]. In an information network, the two primary concepts are entities and relationships, where an entity is associated with information to describe the entity in a form of label, attributes, or a short descrip-tion with a set of terms, and a relationship (or link) shows how two entities are related and the relationships among the entities can be very complex. Examples of information networks include the pub-lication network [17], knowledge base [24], social networks [9], etc. In the literature, the existing work over information networks includes clustering, ranking, classification, data quality and search, as discussed in [10].

In this paper, we focus on an information network, G ,whereen-tities are associated with a short description in a form of a set of terms, which we call keyword terms or simply kterms in short, that best describe the entities. We study a new problem on identifying a set of top-K kterms, P , that are most correlated to a set of user-specified kterms, Q ,for P  X  Q =  X  , based on a goodness function such that every kterm in P is similar to every kterm in Q and the similarity is measured by a structural similarity over the compli-cated link structures among the entities in the large information network G .

Our study is motivated by two reported studies [25, 14]. Tong and Faloutsos [25] and Kasneci et al. [14] find an informative con-nection subgraph ( CSG ) which best explains the relationships among a set of user-specified entities in an information network the approaches effectively identify such informative connection sub-graph following a goodness function. The identified subgraph is the best of many possible subgraphs to explain the relationships among user-specified entities, and the information (e.g. the kterms in our context) associated with entitie s is simply ignored. In our work, we study the relationships between kterms at the kterm level rather than at the entity level. Instead of finding a singl e best subgraph to explain the connectivities among entities, we find a set of top-K kterms P by exploring all possible subgraphs among entities that can possibly connect every kterm in P to every kterm in Q .Itis identify the subgraph to explain t he set of entities. In our problem, the kterms are given, there exist a large number of entities that may use some of kterms to describe themselves, and such entities may appear anywhere in the large information network G . Our approach can be a complement of [25, 14].

Consider the semantic knowledge base YA G O ( http://www. mpi-inf.mpg.de/yago-naga/yago/ ), which is a hetero-geneous information network [10], an excerpt is shown in Fig. 1. Each node represents an entity, such as, a person, an organization, a country, etc. Based on the link s tructure among entities, suppose a user wants to know what the best is to connect {  X  X lbert_Einstein X ,  X  X alther_Bothe X  } .The CSG approach returns a subgraph induced by the five nodes,  X  X lbert_Einstein X ,  X  X alther_Bothe X ,  X  X ermany X ,  X  X ax_Planck_Medal X , and  X  X obel_Prize_in_Physics X , which in-dicates that both  X  X lbert_Einstein X  and  X  X alther_Bothe X  are from  X  X ermany X , and they are both Max Planck Medal and Nobel Prize winners. However, suppose a user wants to find similar scien-tists like Q = {  X  X lbert_Einstein X ,  X  X alther_Bothe X  } ,the CSG approach cannot find  X  X olfgang_Ketterle X  and  X  X ax_von_Laue X , since the CSG is designed to identify the best subgraph to explain given entities. We consider {  X  X lbert_Einstein X ,  X  X alther_Bothe X  as kterms to describe two entities, and attempt to find the top-K kterms that are similar to {  X  X lbert_Einstein X ,  X  X alther_Bothe X  which include  X  X olfgang_Ketterle X  and  X  X ax_von_Laue X . Note that in this example, since all entities have unique labels (kterms), there are no obvious differences between entities and kterms. In other words, in this example, our goal can be seen as to find most re-lated entities connected to {  X  X lbert_Einstein X ,  X  X alther_Bothe X 
Consider an information network, DBLP-CP , which is an in-duced subgraph of the DBLP dataset ( http://www.informatik. uni-trier.de/~ley/db/ ). In DBLP-CP , a node represents a paper, and each edge reflects a citation relationship between two pa-pers. A subgraph of DBLP-CP is shown in Fig. 2, e.g., p 1 represents 18 papers, and the edge between p 2 and p 3 indicates that paper p 3 is cited by paper p 2 . Each node is described using a set of kterms, e.g.,  X  X istogram X  and  X  X avelet X  as shown in Fig. 2(a), and the node p 11 contains kterms  X  X electivity X  and  X  X istogram X . Sup-pose a new graduate student wants to study a research topic related to data compression by submitting a query Q = {  X  X istogram X ,  X  X avelet X  } , which are known as two of the basic methods in data compression, and hopes to identify highly correlated kterms to Q . Under this scenario, each kterm in Q appears in several nodes. The top-3 kterm to be identified are {  X  X electivity X , X  X racticality X ,  X  X ptimality X  } . It is worth noting that there are a large number of occurrence of kterms including  X  X electivity X ,  X  X racticality X ,  X  X pti-mality X , as well as others. All such occurrences may be related to Q = {  X  X istogram X ,  X  X avelet X  } via possible connections in the underneath information network. It is almost impossible to enu-merate all such possible subgraphs with all possible combination of kterms, in order to compute the top-K kterms. For this example, the two approaches [25, 14] cannot be effectively used since the entities are not explicitly specified.

The main contributions of this paper are summarized below. First, we study a new problem of identifying top-K correlated kterms, P , in an information network G for a set of user-specified kterms Q . All possible kterms exhibited in G are taken into consideration for the top-K selection. We compute the top-K correlated kterms P regarding Q based on a global structural-context similarity between nodes in G without enumerating connection subgraphs. It is impor-tant to note that our approach is to measure correlated kterms using a global structural-context similarity but is independent of connec-tion subgraphs. Among the link-based similarity measures [11, 13, 23], none of them consider similarity between two sets of nodes like we do in this work. Second, we propose two new top-K al-gorithms using the framework of the threshold algorithm [4, 5]. We study how to obtain sorted lists in different ways and how to optimize the threshold algorithms. Our algorithms avoid comput-ing all pairs similarities, which is hard in large networks. Third, we propose an algorithm to build index for computing correlation scores, which optimizes the existing algorithm in both index con-struction time and index size. And we give a block-based index which reduces the I/O cost and memory requirement to hold the index in memory. Finally, we conducted extensive performance studies using large real datasets, and confirmed the effectiveness and efficiency of our approach.

The remainder of the paper is organized as follows. We discuss our problem definition in Section 2. In Section 3, we give a so-lution overview. We discuss an extended network representation, and a naive algorithm to find correlated kterms for a query Q over extended network. We also give a 2-step solution with two algo-rithms, called CL-T OPK and IL-T OPK .TheCL-T OPK algorithm is given in Section 4. The IL-T OPK is discussed in Section 5. We conducted experimental studies and discuss our findings in Sec-tion 6. The related works are discussed in Section 7. We conclude our paper in Section 8.
We consider an information network, G =( V , E , K ) . K is the corpus of keyword terms (or simply kterms) contained in G entities are described by kterms which are a small number yet im-portant terms to descr ibe entitie s. For a node v denote the set of kterms associated with v .Then K =  X  v  X  X  For a kterm in the corpus, t  X  X  ,let K  X  1 ( t ) denote the set of nodes in G that contain t , i.e., K  X  1 () is the inverse of that, |K  X  1 ( t ) | X  1 for any t  X  X  ,and |K  X  1 ( t ) | =1 if t is an entity. For a pair of nodes u, v  X  X  , there may exist a directed edge from u to v , i.e., u, v  X  X  .Let I ( v ) denote the set of in-neighbors of v , i.e., I ( v )=  X  u,v  X  X  { u } ,and O ( v ) denote the set of out-neighbors of v , i.e., O ( v )=  X  v,u  X  X  { u } .
 Example 2.1: Fig. 3(a) shows an information network G ( V where V = { p 1 ,p 2 ,  X  X  X  ,p 6 } . There are kterms contained in nodes p ,p 5 and p 6 , e.g., K ( p 4 )= { t 1 ,t 3 } ,and K ( p 5 kterm corpus is K = { t 1 ,t 2 ,t 3 ,t 4 } . The set of nodes that contain t is K  X  1 ( t 3 )= { p 4 ,p 6 } . The in-neighbors of p { p 2 ,p 3 } . The out-neighbors of p 2 is O ( p 2 )= { p 4 ,p
We also consider entity names as one kind of kterms if they are explicitly given. For example, in Fig. 1, the entity names are listed, they are also treated as kterms, i.e., K = {  X  X lbert_Einstein X ,  X  X er-many X ,  X  X  X } .For DBLP-CP , the kterms are explicitly given instead of entity names, i.e., K = {  X  X istogram X ,  X  X stimators X ,  X  X  X } following, we consider our problem in a kterm level instead of en-tity level.
 Problem Statement: Givenaninformationnetwork G ( V , E , K query Q consists of l kterms from K , { k 1 ,  X  X  X  ,k l } ,and l weights corresponding to the kterms, { w 1 ,  X  X  X  ,w l } , i.e., Q =  X  X  X  , ( k l ,w l ) } . Here, a weight is a positive value default. The problem is to find an information nebula (cloud) P related to Q , which is a list of top-K kterms from K with P  X  .

In order to identify the top kterms similar to Q , we define a cor-relation score, kcorr ( Q, t ) , for each kterm t  X  X  , which quantifies the similarity of t to Q in the information network G . where kcorr ( t 1 ,t 2 ) is a correlation score between two kterms t and t 2 computed in G .

In an information network, each kterm appears at nodes K  X  1 Thus, the similarity between two kterms t 1 and t 2 , kcorr ( t should consider the similarity between the set of nodes containing t and the set of nodes containing t 2 . This is because that each simi-larity between one node from K  X  1 ( t 1 ) and one node from is a witness of the correlation between t 1 and t 2 . Among the link-based similarity measures [11, 13, 23], none of them considers sim-ilarity between two sets of nodes. In SimRank [13], it defines the similarity between two nodes as an average of the similarities be-tween their in-neighbors, which are two sets of nodes. Similarly, we define, Here, K  X  1 ( t ) denotes all nodes that contain the kterm t and denotes the i -th element in K  X  1 ( t ) .Also, |K  X  1 ( t ) of
K  X  1 ( t ) .Let sim ( u, v ) denote a structural correlation measure of two nodes in a network, which will be discussed shortly. Eq. (2) computes the correlation between two kterms t 1 and t 2 by comput-ing the average structural correlation ( sim ( , ) ) between every node that contains t 1 and every node that contains t 2 in the network In this paper, for the similarity between two nodes u, v in sim ( u, v ) in Eq. (2), we adopt a structural-context similarity mea-sure in the litera ture, SimRank [13]. SimRank is a popularly used measurement, which quantifies the similarity of a pair of nodes based on the link structures around these two nodes in the network. Let sim ( u, v ) denote the similarity between two nodes u and v . u is always similar to itself, i.e., sim ( u, u )=1 .Otherwise,itis defined in a recursive fashion as follows, where I i ( u ) means the i -th element in I ( u ) , |I ( u ) of elements in I ( u ) ,and C is a constant between 0 and 1 . When the in-neighbor of u or v is empty, i.e., I ( u )=  X  or I ( v )= is not defined, and it is assigned with value 0 . The SimRank value will always be between 0 and 1 , i.e., 0  X  sim ( u, v )  X  V . The SimRank values for node pairs in the network in Fig. 3(a) are shown in Fig. 3(b), where C is chosen as 0 . 8 .

The similarity sim ( u, v ) (see Eq. (3)) between two nodes in a network is defined as an average of the similarities of combinations of in-neighbors of u and in-neighbors of v . Meanwhile, our cor-relation score between two kterms, kcorr ( t 1 ,t 2 ) (see Eq. (2)), is also defined as an average of the similarities of combinations of nodes containing t 1 and nodes containing t 2 . Thus, we can con-sider kterms in K as kterm nodes , and treat the nodes containing t  X  X  , K  X  1 , as in-neighbors of the kterm node t . Based on this, we can use the techniques of computing sim ( , ) to compute our correlation score kcorr ( , ) .Wedefinethe extended network . Extended Network: An extended network G e =( V e , E e , K the information network G =( V , E , K ) extended by kterm nodes and edges between kterm nodes and other nodes, i.e., V e = and E e = E X  X  v,t | t  X  X  X  v  X  X   X  1 ( t ) } . For example, Fig. 4(a) shows the extended network of Fig. 3(a).

Although, in the extended network, each kterm in the query Q corresponds to exactly one node, the method used in [25] to com-pute a connection subgraph for a set of user-given nodes, can-not be applied to our extended network. The reasons are twofold. First, the methods in [25] work only on undirected graphs. How-ever, if we treat the extended network as undirected, the similar-ity score between node-pairs may change dramatically. For ex-ample, in Fig. 4, the similarity of t 1 and t 2 is the lowest among all the kterm node-pairs. Based on the random walk with restart measure [25], sim ( t 1 ,t 2 ) will be higher than sim ( t sim ( t 2 ,t 4 ) ,since p 4 and p 6 are now very similar to each other by the connection through t 3 . Second, in order to find the global top-K correlated kterms, all connection subgraphs are needed, while [25] only finds one best connection subgraph.

In the extended network, we can compute sim ( , ) for pairs of kterm nodes as shown in Fig. 4(b), using the techniques of comput-ing SimRank. From Eq. (3) and Eq. (2), we know that sim ( t C  X  kcorr ( t 1 ,t 2 ) for any pair of kterm nodes. Then the correlation 112 (a) Fingerprint Tree score of a kterm t to query Q can be rewritten as
With the extended network and Eq. (4), our approach finds top-K kterms based on sim ( , ) .
 A Naive Algorithm : A naive algorithm, N AIVE , to compute top-K correlated kterms for a user-given query Q , can be designed to work as follows. It first gets the kterm corpus, K ,of G , which consists of all the kterms appeared at nodes in G . Then, it builds the extended network G e as defined above. For each kterm t in K but not in the query Q , we compute a correlation score kcorr ( Q, t ) . Finally, the top-K correlated kterms can be identified.

In N AIVE , the most expensive computational cost comes from the O ( l  X |K| ) sim ( k i ,t ) values computation, where l is the number of kterms in query Q .However, |K| can be of the same magnitude of |V| , and it is impossible to compute the O ( l  X |K| ) sim ( k values online, even for an information network with modest size, i.e., n  X  10 , 000 .
The problem of computing SimRank for node-pairs in large net-works is hard in general [21]. It i s impractical to compute SimRank value for so many node-pairs on-line as illustrated in the N gorithm. We propose a two-step approach which makes use of the properties of fingerprint index [7]. In the first step, we process (or more precisely fingerprint index of G ) to generate a set of sorted lists, L = { L 1 ,L 2 ,  X  X  X } , where the definition of a list and the num-ber of lists are determined by the specific algorithm. In the second step, we adopt the idea of threshold algorithm [4, 5] to compute top-K kterms over the set of sorted lists L . The main issues are, how to define and obtain the set of lists L , and how to optimize the threshold algorithm in our problem situation. We discuss the properties of fingerprint index in the following, and propose our algorithms in the next two sections. Note that the extended net-work is only used to illustrate the ideas, and does not need to be materialized.
In the random surfer-pairs model, sim ( u, v ) is equal to the ex-pected first meeting distance of two independent random walks traveling backwardly starting from u and v respectively [13]. Let  X  u,v denote the random variable that is equal to the first meeting time of the random walks starting from u and v .  X  u,v =0 if u = v , and  X  u,v =  X  if the random walks do not meet. It shows in [13, 7] that, the equation E ( C  X  u,v )= sim ( u, v ) holds.

Fogaras and Racz [7] generate a set of coalescing walks where each pair of walks will follow the same path after their first meet-ing time. With these coalescing walks, the equation E ( C  X  u,v )= sim ( u, v ) still holds, since any pair of walks are independent un-til they first meet. A fingerprint index is designed to encode and store the set of random walks efficiently. Initially, a path of length zero is generated for each node in G . Then it goes for L iterations to generate random walks up to length L . At each iteration i ,all the paths travel one step backward by randomly choosing one of its in-neighbors. For all the paths that end at the same node, e.g., the paths starting from u 1 ,  X  X  X  ,u k , a directed edge u weight i is added to the fingerprint index for 2  X  j  X  k ,andthe paths starting from u 2 ,  X  X  X  ,u k are discarded. As an example, one fingerprint index for the network shown in Fig. 3(a) is shown in Fig. 5(a), the labels of the edges are their weights.

Let FPG denote the final graph obtained, it is a forest of rooted trees with edges towards the roots. FPG is of size 2 n ,where n = |V| . The FPG compactly encodes all-pairs first meeting time  X   X  ,  X  ,and  X  u,v can be computed using FPG. If u and v are in different trees in FPG, then  X  u,v =  X  .Otherwise, u and v has one lowest common ancestor in FPG, let it be x ,asshown in Fig. 5(b). Let p ( u, x ) be the directed path from u to x ,then  X  the maximum weight among all the edges in the path from u or v to Note that, PSimRank [7], as a variant of SimRank, modifies Sim-Rank by allowing random walks t o meet with higher probability when they are close to each other, i.e., a pair of reverse random walks at u and v will advance to the same node in the next step with PSimRank also has a recursive formula similar to Eq. (3) [7]. In our approach, we can use either SimRank or PSimRank as sim ( , ) .
From Eq. (4), we can see that the correlation score of a kterm t to query Q , kcorr ( Q, t ) , is a linear combination of sim ( k sim ( k l ,t ) , and it is a monotonic function. Therefore, the general idea of threshold algorithm [4, 5] can be adopted here to compute top-K correlated kterms for a user-given query. We define a list for each query kterm k i  X  Q , L k i = { ( t, sim ( k i ,t )) consists of kterms t  X  X  and their correlation scores to k this kterm-based lists , as one list is defined for each query kterm. There are total l lists, L = { L k 1 ,  X  X  X  ,L k l } .The l lists should be computed in advance or on-demand. In order to adopt the idea of threshold algorithm, two interfaces are needed for each list. One is NEXT ( L k i ) which returns the next entry ( t, sim ( k i the highest sim ( k i ,t ) value, and it corresponds to the sorted access of a list in the threshold algorithm. The other one is GET which retrieves the sim ( k i ,t ) value for kterm t , and it corresponds to the random access of a list in the threshold algorithm. Given l lists, and a sorted access interface ( NEXT ) and a random access interface ( GET ) for each list, the threshold algorithm [4, 5] can be used to find top-K correlated kterms for a user-given query.
CL-T OPK is shown in Alg. 1. Here, a priority queue Q is used to store potential results, each element in it is a kterm in correlation score to the query Q as the key, i.e., ( t, sim ( Q, t )) ,and Q is a minimum queue so that the top element has the minimum key value. For simplicity, we access the l lists in a round-robin fashion, and each list has both sorted access interface ( NEXT ) and random access interface ( GET ). An upper bound of the correlation score of unseen kterm is maintained to terminate the algorithm earlier, denoted as threshold . We access the l lists in decreasing order, and an upper bound value of sim ( k i ,t ) is maintained to be the last retrieved entry from L k i through interface NEXT , denoted as L Then, the upper bound correlation score for any unseen kterms in K is bounded by the sum of L u k i , i.e., threshold = l 1 L each seen kterm t  X  X  , its correlation score is computed based on Algorithm 1 CL-T OPK ( G ,Q ) 2: Initialize a priority queue Q to be empty; 3: Initialize threshold =  X  ; 7: for j  X  1 to l and j = i do 9: Compute kcorr ( Q,t ) based on Eq. (4); 10: Update Q with ( t, kcorr ( Q, t )) ; 12: Output the kterms in Q ; Eq.(4),where sim ( k i ,t ) for all l lists are retrieved through inter-face GET . The priority queue Q maintains the current top-K kterms among all the seen kterms. If the lowest correlation score in less than threshold , which means that none of the unseen kterms can have a correlation score larger than the lowest correlation score in
Q , then the K kterms stored in Q are guaranteed to be the top-K kterms in K .

Note that, in CL-T OPK , it first generates l lists L , then it works on
L and discards the network G . For each list L k i  X  X  , the entries are accessed through two interfaces: NEXT and GET . To implement NEXT ( L k i ) and GET ( L k i ,t ) , the entries ( t, sim ( k ordered in non-increasing order according to sim ( k i ,t ) .Foranin-formation network with millions of nodes, no scalable algorithm in the literature can compute sim ( k i ,t ) exactly for millions of pairs of nodes in practical time. Because the query kterms are contained in the kterm corpus, i.e., Q  X  X  , one may think of generating a list L t for each t  X  X  off-line. However, it is impractical to gen-erate and store the lists { L t | t  X  X } , because it will take space O ( |K| 2 ) ,where K contains millions of kterms.

To generate the l lists, L = { L k 1 ,  X  X  X  ,L k l } , efficiently on-line, we use an approximate algorithm to compute sim ( , ) ,whichis based on the fingerprint index as discussed in Section 3.1. It is worth noting that sim ( , ) values computed using the fingerprint in-dex are approximate values. Let N be the number of fingerprint indices (based on different random walks) used to improve the ac-curacy, denoted as FPG = { FPG 1 ,  X  X  X  , FPG N } ,and sim ( , ) can be computed as, where sim j ( u, v ) is the sim ( , ) value computed on FPG
In order to generate the lists, for each kterm t  X  X  and k Q ,CL-T OPK needs to compute sim ( k i ,t )= N j =1 sim j which needs to access all the N copies of FPG S , thus it needs a lot of I/Os. In this section, we propose another formulation of lists and an optimized top-k algorithm to efficiently find top-K corre-lated kterms directly from FPG.

In order to develop an efficient algorithm, we rewrite Eq. (4) by combining Eq. (5) as follows, Algorithm 2 IL-T OPK (FPG ,Q ) 2: Initialize a minimum priority queue Q to be empty; 3: Initialize threshold =  X  ; 4: Initialize a sorted list ub to be empty; 5: for i  X  1 to N do 10: for j  X  1 to N and j = i do 13: Update Q with ( t, kcorr ( Q,t )) ; 14: Update threshold with kcorr i ( Q, t ) ; 16: Output the kterms in Q ; If we define a correlation score based on each FPG j ,suchas, then, kcorr ( Q, t )= N j =1 kcorr j ( Q, t ) . Here, kcorr ( Q, t ) is re-garded as the global correlation score, and kcorr j ( Q, t ) (computed on FPG j ) is regarded as the local correlation score. We define a list for each FPG j , denote as L F j = { ( t, kcorr K} , which consists of kterms t and their local correlation scores kcorr j ( Q, t ) computed on FPG j . We call this index-based lists , as one list is generated for each fingerprint index FPG j total N lists, i.e., L = { L F 1 ,  X  X  X  ,L F N } . In each list L entries ( t, kcorr j ( Q, t )) are ordered according to kcorr is worth noting that, for each list L F j , only those kterms t with non-zero kcorr j ( Q, t ) values are of interest, the size of which usually is not large (e.g., in the order of hundreds to thousands). The interfaces for sorted access and random access of list L are NEXT ( L F j ) and GET ( L F j ,t ) , respectively, where gets the next entry ( t, kcorr j ( Q, t )) from L F j ,and retrieves the local correlation score of t computed on FPG
The algorithm to compute top-K correlated kterms for a user-given query is shown in Alg. 2 (IL-T OPK ). In IL-T 1, P ROCESS ( FPG ,Q ) generates N sorted lists, L = { L F 1 L
F N } which will be discussed shortly. A sorted list ub is used to store the upper bound kcorr i ( Q, t ) for each list L F i ub consists of two fields, id and key ,where id denote the list id and key denote the upper bound of the corresponding list. Entries in ub are sorted in deceasing order of key . Here, ub is used to determine the list on which the next sorted access should be. Different from CL-T OPK which accesses the l kterm-based lists in a round-robin fashion, we access the index-based list with highest upper bound in IL-T OPK , i.e., ub. FIRST () .id . This is based on the fact that there are N lists in IL-T OPK , which is much larger than that of l lists in CL-T OPK , i.e., N l . Thus, we can access the list that is more likelytohaveanswers.
 Theorem 5.1: Let m denote the number of distinct kterms pro-cessed, i.e., it is the number of calls of NEXT , the time complexity of IL-T OPK is O ( m  X  ( N +log K )) ; the space complexity of IL-T OPK is O ( K + N ) .
 Proof Sketch: For each kterm processed (Lines 9-12), it takes O ( N ) time to compute kcorr ( Q, t ) . Here, we assume a hash index Algorithm 3 O PT F INGER P RINT ( G ,N,L ) 1: for i  X  1 to N do 3: for each t  X  X  do 4: PathEnd [ t ]  X  t ; 5: for j  X  1 to L do 6: Generate a random permutation  X  with the n nodes in V ; 7: for each t  X  X  with PathEnd [ t ] =  X  X topped X  do 9: for each set of nodes with the same PathEnd , u 1 ,  X  X  X  is used to implement GET . Updating Q (Line 13) takes O (log K ) time, because the size of Q is at most K . Updating ub (Line 15) takes O ( N ) time, because ub is of size N . So the total time com-plexity is O ( m  X  ( N +log K )) .
 For the space complexity, IL-T OPK maintains a priority queue of size at most K , and a priority queue ub of size N . So it totally takes space O ( K + N ) .
In the following, we discuss some IL-T OPK implementation de-tails. First, we show an optimization of the fingerprint index for our problem, which reduces both the index construction time and index size. We propose a general index structure to effectively store the N copies of the fingerprint index on disk. We give an algorithm to generate the N lists L F i efficiently based on our optimized finger-print index.
 Optimized Fingerprint : In PSimRank, to extend the random walks one step backwardly, instead of choosing one of its in-neighbors independently for all the paths, it first generates an independent random permutation  X  on the nodes of G . Then, the random in-neighbor for v is chosen as argmin u  X  X  ( v )  X  ( u ) . We show an algo-rithm, called O PT F INGER P RINT , that directly generates the FPG on
G efficiently in Alg. 3. Initially, FPG i is initialized as lated nodes (Line 2). To expand the paths one step backwardly (Lines 7-8), a random permutation is generated first (Line 6). O GER P RINT generates only |K| random walks, while a naive gener-ation of FPG on an extended network needs to generate |K| random walks. With Alg. 3, the FPG generated is of size 2 is smaller than the size of FPG on G e ,whichis 2  X  ( |K| FPG generated by O PT F INGER P RINT contains all the information needed for our algorithm.
 A New Block-Based Index Structure : With N copies of FPG the total index size is O ( N |K| ) ,where |K| is of same magnitude of |V| . So it can not completely reside in main memory, we show data structures to store them on disk so that it can be efficiently retrieved and processed by the following query answering algorithms. Recall that, each FPG is a forest of rooted trees. We call a connected tree in a FPG as a block , denote as B , since the nodes in it should be stored in consecutive blocks on disk as they will be retrieved as a unit by any processing algorithm. Let the set of FPG FPG 1 ,  X  X  X  , FPG N . The index structure consists of Global index , where there exists only one for the whole index, and In-block index , where there exists one for each block, see Table 1.

For the Global index , it has two parts: a mapping between kterms and kterm ids ( kid ); and an inverted list of block ids ( bid ) for each kterm. Because kterms are stri ngs, and they are more costly to Algorithm 4 P ROCESS (FPG ,Q ) 1: Find and load l inverted lists kid  X  bid s into main memory; 2: for i  X  1 to N do 5: for each bid  X  X  do 6: Load the block of FPG i that has block id bid into main memory, 7: for each t  X  B do 10: Initialize a pointer for each list to point to the first entry; store and process than integers, a unique kid is used to identify each kterm. In the beginning of query processing, all the input kterms are transformed to their kid s. After the processing, the re-sulting kid s are transformed to the corresponding kterms and out-put to users. Usually, the kid s are from the corpus { 1 , 2 , where |K| is the size of the kterm corpus. For example, in Table 1, the kterm  X  X lgebra X  has kid 47 , and the kterm  X  X lgorithm X  has kid 48 .

In order to fast retrieve the corresponding block in each FPG that contains a specific kterm, an inverted list is built for each kid . Note that, a kid is contained in exactly one block in each FPG only the kterms in these blocks have non-zero correlation scores with it. For example, in Table 1, the kterm  X  X lgebra X  with kid 47 has an inverted list, { (1 , 10) , (2 , 15) ,  X  X  X  , ( N, 6) that the block id ( bid ) in each FPG i that contains kterm  X  X lgebra X  are, bid 10 in FPG 1 , bid 15 in FPG 2 ,and bid 6 in FPG N
For each block B  X  N i =1 FPG i , we build an in-block index so that it can be processed efficiently. Recall that, each block is a re-versed tree that all edges direct to the root. Furthermore, in a block B , the kterms contained are not with consecutive kid s. In order to store and process a tree in memory effectively and efficiently, the nodes should be mapped to consecutive integers starting from 1 . So we store a mapping between kterm ids ( kid s) and in-block ids ( iid s). Then, each edge is stored as iid of the start node , iid of the end node, and its weight. For example, in Table 1, kid 47 has iid 10 ,and kid 1280 has iid 138 ; the edge 8 , 10 has weight 3 ,which means that the parent of kid 47 (with iid 10 ) is the node with iid 8 . List Sorting :P ROCESS used in IL-T OPK is shown in Alg. 4. Given FPG and Q , it generates N lists, L = { L F 1 ,  X  X  X  ,L F N loads the l inverted lists kid  X  bid s (see Table 1), one for each user-specified query kterm, into main memory (Line 1). For each kterm, the bid s in the inverted lists are those blocks in the corre-sponding FPG i that contain it. Then, a sorted list L F i each FPG i (Lines 3-9). Let B be the set of bid s belonging to FPG which is found from the l inverted lists (Line 3). Note that, the size of
B can be smaller than l , i.e., there can be more than one user-specified kterms in one block. Then L F i can be constructed from those blocks indicated by B . For each block in B (Line 5), we first load the whole block into main memory (Line 6). For each kterm t in the block B (Line 7), kcorr i ( Q, t ) can be computed based on B ,andthepair ( t, kcorr i ( Q, t )) is inserted into list L As the kterms are inserted into L F i in random order, we have to sort it so that the kterms are ranked in non-increasing order with respect to the local correlation scores kcorr i ( Q, t ) (Line 9). Note that, after processing a block B (Lines 5-8), block B is discarded from memory.

After processing FPG, there are N lists L , and they can reside in main memory since each list usually is not large. Also, only those N lists should be in memory. In order to call entry that should be returned when NEXT is called next time, and it Each time when NEXT ( L F i ) is called, the entry pointed by the pointer is returned, and the pointer moves to the next entry in the list. GET ( L F i ,t ) gets the local correlation score kcorr kterm t , an in-memory index should be built to retrieve it efficiently, because the kid s in a list L F i usually are not with consecutive inte-ger ids.
 Theorem 5.2: Let n i denote the size of list L F i , the time complexity of query kterms in Q and L is the maximum length of random walks in FPG ; the space complexity of P ROCESS is O ( N i =1 n Due to space limit, we omit the proof.
 Kterm-based lists vs Index-based lists: Recall that, kterm-based lists, { L k 1 ,  X  X  X  ,L k l } ,isusedinCL-T OPK , which generates a list for each k i  X  Q , and index-based lists, { L F 1 ,  X  X  X  ,L in IL-T OPK , which generates a list for each FPG i  X  FPG. The number of lists in index-based lists is N , which is much larger than that of l kterm-based lists. However, there are advantages of the index-based lists. (1) The generation of index-based lists is more efficient. To generate a list L F i , only the related blocks, which is less than l , from FPG i are needed. The related blocks from FPG usually are stored closely on disk, which is called the localization of index-based lists. (2) In generation of index-based lists, a block is loaded from disk at most once to compute correlation scores. While in kterm-based lists, a block will be loaded from disk multi-ple times, once for each query kterm k i  X  Q . (3) In order to enable sorted access on lists, entries in a list should be sorted. The size of a list in index-based lists is much smaller than that in kterm-based lists. Therefore, it is much more efficient for the index-based lists to be sorted than that of kterm-based lists.
We conducted extensive performance studies to test the algo-rithms proposed in this paper. We implemented our algorithms: CL-RR, IL-RR, and IL-BF. Here, CL-RR is for Alg. 1 which is kterm-list based, IL-RR is Alg. 2 with round robin access of the sorted lists, and IL-BF is for Alg. 2 which is index-list based. All algorithms were implemented in Visual C++ 2003, and all tests were conducted on a 2.8GHz CPU and 2GB memory PC running Windows XP.

We used two large real datasets, DBLP 1 and YA G O 2 [24] for testing. For DBLP , the network consists of two types of nodes, Author and Paper . Each node of type Author has a text at-tribute which stores the author name, and each node of type Paper has a text attribute which stores the title of the paper and the con-ference name of the paper. The kterms are author names and terms contained in paper titles. There are two types of relationships (edges) in DBLP , namely, author-write-paper relationship and paper-cite-paper relationship. After processing, the DBLP network consists of 2,120,264 nodes and 3,515,292 edges. The total number of distinct kterms and the average number of kterms per node are 445,314 and 5.05, respectively. We also extracted a subgraph from the DBLP network, denoted as DBLP-CP . It considers only paper-cite-paper relationship, and the papers from some top-ranked conferences are considered. The resulting DBLP-CP network consists of 6,705 nodes and 30,559 edges. The total number of distinct kterms and the average number of kterms per node are 4,723 and 6.0, respec-tively.

YA G O is a huge semantic knowledge base. Each node represents an entity, such as, a person, an organization, a city, etc. The text attribute of a node stores its name , and kterms are entity names. Each edge corresponds to a fact in YA G O knowledge base, such as, a person works at which university, a city belongs to which country. After processing, the YA G O network we used consists of 314,025 nodes and 398,257 edges. The total number of distinct kterms and the average number of kterms per node are 314,066 and 1.0, respectively.
To test the effectiveness of our algorithm, we used the imple-mentation of IL-BF algorithm. We also tested the Center-Piece algorithm proposed in [25], denote as CePS 3 , which returns a con-nection subgraph for a query Q which consists of entities. Since CePS does not scale to large graphs, in order to run first extract a subgraph with 3k-7k nodes around the query entities. As the input graphs of CePS are undirected, we treat each edge as undirected in order to run CePS . (Q1) { X  X lbert_Einstein X ,  X  X alther_Bothe X  X  on YA G O . The con-nection subgraph obtained by CePS is shown in Fig. 6(a), which in-dicates that both  X  X lbert_Einstein X  and  X  X alther_Bothe X  are  X  X er-many X , and they both have won  X  X obel_Prize_in_Physics X  and  X  X ax_Planck_Medal X . However, CePS cannot find the persons in YA G O with similar relations as the two querying persons. The top-8 correlated persons returned by our approach are shown in Table 2. We also show in Fig. 6(b) the connection subgraph returned by CePS by adding the first two persons into Q . We know that both  X  X olfgang_Ketterle X  and  X  X ax_von_Laue X  are from  X  X ermany X  and have won  X  X obel_Prize_in_Physics X , and  X  X ax_von_Laue X  and  X  X alther_Bothe X  have the same academic advisor  X  X ax_Planck X . (Q2) { X  X sabel_Sanford X ,  X  X athan_Lane X  X  on YA G O . The top-8 correlated actors to the two actors in query are shown in Table 2. These returned actors have won similar awards with actors in Q , such as they have won  X  X mmy_Award X ,  X  X ollywood_Boulevard X , and/or  X  X ollywood_Walk_of_Fame X , as shown in Fig. 7. However, the CePS approach can only find the awards that  X  X sabel_Sanford X  and  X  X athan_Lane X  have won, not actors with similar awards. (Q3) { X  X istogram X ,  X  X avelet X  X  on DBLP-CP . The top-ranked kterms  X  X ynopses X  denotes that both  X  X istogram X  and  X  X avelet X  are meth-http://www.informatik.uni-trier.de/~ley/db/ http://www.mpi-inf.mpg.de/yago-naga/yago/ http://www.cs.cmu.edu/~htong/work/CePS.zip (b) CePS for Q = { AE, WB, WK, ML } ods to build synopses.  X  X ectangle X  and  X  X edians X  are approxima-tion methods used in generating histograms and wavelets.  X  X urve X   X  X itting X  sometimes is used together with  X  X istogram X  and  X  X avelet X  to get better results. (Q4) { X  X requent X ,  X  X attern X ,  X  X ining X  X  on DBLP-CP . The top cor-related kterms for this query are shown in Table 2.  X  X iscovering X  has the same meaning as  X  X ining X , which is to discover frequent  X  X temsets X  or frequent patterns from a database, or even to derive  X  X ssociation X   X  X ules X  from frequent itemsets. There are also some research papers about finding frequent  X  X pisode X  patterns from se-quential data. (Q5) { X  X ergey_Brin X ,  X  X arry_Page X  X  on DBLP , which is to dis-cover the authors who have writte n papers with those two Google co-founders, or whose papers have similar citation patterns. The top authors are shown in Table 2. Among these eight authors, only  X  X raig_Silverstein X ,  X  X ay-Wei_Chang X , and  X  X rian_Milch X  are coauthors of  X  X ergey_Brin X , while the other authors have writ-ten papers with similar citation patterns with the papers written by  X  X ergey_Brin X  and  X  X arry_Page X .
For the efficiency testings, we report the query processing time and peak memory consumption for each test case. The query pro-cessing time of CL-RR, IL-RR, and IL-BF includes the time of processing the FPG index to build sorted lists and the time to find top-K kterms, and the time to build FPG index is the index con-struction time.

For each dataset, we select representative queries with different kterm frequencies. After removing all the stop words, let  X  denote the maximum kterm frequency among all the kterms, the kterms are divided into five categories depends on their frequencies which are evenly divided between 0 and  X  . For simplicity, we say a kterm Table 4: Query kterms used in Efficiency Testing (# is the kterms frequency) between ( p  X  1)  X   X / 5 and p  X   X / 5 .

For all the testings, we vary three parameters, namely, the kterm frequency ( p ), the query kterm number ( l ), and the number of kterms returned ( K ). The corresponding values for the three parameters are shown in Table 3 together with their default values. When varying the value of one parameter, the other two parameters take their default values. The query kterms selected for the datasets are shown in Table 4. When varying query kterm number l ,thefirst l kterms from the default frequency category are selected.
 Exp-1(Testingon DBLP dataset): The time to construct indices for CL-RR and IL-BF algorithms are 32 minutes and 21 minutes, respectively. For DBLP , we test three algorithms: CL-RR, IL-RR, and IL-BF, where IL-RR has the same FPG index as IL-BF. The testing results are shown in Fig. 8. From Fig. 8(a) and Fig. 8(b), we know that the query processing time and peak memory consump-tion of all the three algorithms are not affected too much by the kterm frequency. Because the factors that affect the running time of IL-BF and CL-RR are the number of FPG indices (i.e., N )and the number of nodes in the index blocks that contain user-given kterms. Consistently, IL-BF takes less CPU time than IL-RR, and IL-RR takes less CPU time than CL-RR. IL-BF and IL-RR have the same index, and the same preprocessing algorithm. But, IL-BF accesses the N sorted lists in a best first manner, so it can stop ear-lier with less access of the lists. IL-RR outperforms CL-RR based on the fact that, the preprocessing algorithm for IL-RR is more efficient because it retrieves each block from disk only once. The memory consumption of CL-RR is less than that of IL-RR and IL-BF, which have almost the same memory consumption. This is because that the main memory consumption for CL-RR and IL-BF algorithms are the l sorted lists and the N ( &gt;l ) sorted lists, respec-tively. When increasing the query kterm number, the CPU time and peak memory consumption of all the three algorithms increase, be-cause the size of the sorted lists increases, as shown in Fig. 8(c) and Fig. 8(d). Fig. 8(e) shows that, when the top-K value increases, the CPU time of IL-BF increases while that of CL-RR and IL-RR almost remain the same. This is because that, the best first access makes IL-BF stop earlier when fewer results are desired. It also shows that round robin is inefficient when only a few results are desired.
 Exp-2 (Testing on YA G O dataset): For this dataset, the time to construct indices for CL-RR and IL-BF algorithms are 4 . 2 min-utes and 2 . 4 minutes, respectively. The testing results for IL-BF, IL-RR, and CL-RR are shown in Fig. 9, which have similar trends as that for DBLP dataset.
Our work is the first to find correlated kterms for a set of query kterms. It differs from other works in two aspects. First, one kterm can appear at many nodes in a graph. Second, we consider a global structural-context similarity between two sets of nodes in our cor-relation measure of two kterms. In the literatures, there are lots of work trying to find the best connection subgraph(s) to describe how the query terms are connected. Among them, query terms in [6, 25, 14] are entity names which corre spond to unique nodes in a graph, while query terms in [3, 8, 2, 15] are kterms as our work. Instead of finding connection subgraphs, complementally, we find the cor-related kterms surrounding the query kterms which is measured by a structural correlation score.
 Finding Connection Subgraphs: Given an Entity-Relationship graph and a set of entities, the works in the literature return one (or a set of) connection subgraph ( CSG ), which is a subgraph describes the connection between query entitie s. Faloutsos et al. [6] proposed a delivered current based method to find a connection subgraph be-tween two query nodes. To handle more than two query nodes, Tong and Faloutsos [25] proposed to find center-piece subgraph (
CePS ), which defines the proximity of a node t to node s as the probability that a random walk with restart from s will end at t .The center-piece subgraph consists of nodes whose proximity measures to all query nodes are high, and paths connecting to those query nodes. Kasneci et al. [14] first find a steiner tree between the query nodes, and then add nodes based on the probab ility that random walks from a node in the steiner tree to another node in the steiner tree will pass through it. In the above mentioned works, each query entity corresponds to a unique node in the graph.
 Keyword Search: Keyword search in graphs finds small subgraphs which describe the connection between user-given keywords, where a keyword can appear at many nodes. The existing approaches de-fine the cost of a subgraph based on the weights of edges. In [3, 8, 2, 15], they find a set of steiner trees with minimum costs. Ex-act [3] and approximate algorithms [8, 15] were studied. Dalvi et al. [2] considered external memory graphs. Qin et al. [22] return subgraphs (communities) for a keyword query. All these works aim at finding connection subgraph for a set of terms, based on the weights of edges. Our work is to find similar kterms for a set of user-given kterms based on the similarity between sets of nodes containing the corresponding kterms.
 Link-Based Similarity Measures: One fundamental problem in graphs is to determine the similarity between node-pairs, which has a wide range of applications, such as link predication, graph clus-tering, etc. Liben-Nowell and Kleinberg [20] used similarity scores to predicate links between un-linked node-pairs, and reported that ensemble of paths performs the best. Examples of similarity mea-sures of using the ensembles of paths are, personalized PageR-ank [11], SimRank [13], etc. In the personalized PageRank, the basic idea is to start a random walk from a node v , and at any setp the walk moves to a random neighbor with probability 1  X   X  and is reset to the start node v with probability  X  . One characteristic of the personalized PageRank is that nodes close to the starting node will have higher stationary probability [23], and this is used to find connection subgraph for query nodes [25] and find relevant nodes for query keywords [12].
 SimRank: SimRank was first proposed by Jeh et al. [13] to mea-sure the similarity of two objects in a relationship graph based on the link structure. Iterative algorithms was proposed in [13]. Li-zorkin et al. [21] gave an accuracy estimation for the number of iter-ations. Optimization techniques were also studied in [21] to reduce the computational cost for each iteration. A non-iterative method was proposed by Li et al. in [18]. Exact SimRank computation based on these techniques can only scale to graphs with thousands of nodes. Fogaras and Racz [7] proposed an index to scale the Sim-Rank computation to graphs with millions of nodes, while the Sim-Rank values computed are approximated values. Also, PSimRank was proposed in [7] to improve the quality of measuring node-pair similarity. As the data graph in our situation can have a size up to millions of nodes, we adopt the index proposed in [7]. How-ever, the query in our work is different from that of [7], and the query answering techniques used in [7] can not be applied in our situation. A technique to efficiently compute SimRank value for a single-pair of nodes was proposed by Li et al. [19]. However, the technique can not be applied to our situation, because it has to compute SimRank values for millions of pairs of nodes. Variants of SimRank were proposed in the literature [26, 1]. P-R ANK was pro-posed by Zhao et al. [26], which takes both in-and out-neighbors into consideration while SimRank only considers in-neighbors. It-erative algorithms were proposed to compute P-R ANK values, so it does not scale to graphs with millions of nodes. Antonellis et al. [1] proposed SimRank++ for a w eighted bipartite graph, which is an adaptation of SimRank to the special case of weighted bipar-tite graph.
In this paper, we studied knowledge discover from information networks which is composed of entities and relationships. Here, we consider the kterms associated with entities, and study the problem in the kterm level rather than entity level. We compute information nebula (cloud) which is a set of top-K kterms P that are most cor-related to a set of user-specified kterms Q over a large information network. A global structural-context similarity between two sets of nodes is considered in our correlation measure of two kterms. We present efficient algorithms to find top-K kterms without comput-ing correlation scores for all kterms in the information network. We conducted extensive performance studies using large real datasets, and confirmed the effectiveness and efficiency of our approach. Acknowledgment: The work was supported by grants of the Re-search Grants Council of the Hong Kong SAR, China No. 419008 and 419109, and Microsoft Research Asia FY12-RES-OPP-002.
