 Frequent itemset ( FI ) mining is fundamental to many important data mining tasks. Recently, the increasing promine nce of data streams has led to the study of online mining of FIs [5]. Due to the constraints on both memory consumption and processing efficiency of stream processi ng, together with the exploratory nature of FI mining, research studies have sought to approximate FIs over streams.
Existing approximation techniques for mining FIs are mainly false-positive [5,4,1,2]. These approaches use an error parameter , , to control the quality of the approximation. However, the use of leads to a dilemma. A smaller gives a more accurate mining result. Unfortunately, a smaller also results in an enormously larger number of itemsets to be maintained, thereby drastically increasing the memory consumption and lowering processing efficiency. A false-negative approach [6] is proposed recently t o address this dilemma. However, the method focuses on the entire history of a stream and does not distinguish recent itemsets from old ones.
 We propose a false-negative approach to mine FIs over high-speed data streams. Our method places greater i mportance on recent data by adopting a sliding win-dow model. To tackle the problem introduced by the use of ,weconsider as a relaxed minimum support threshold and propose to progressively increase the value of for an itemset as it is kept longer in a window. In this way, the number of itemsets to be maintained is greatly r educed, thereby saving both memory and processing power. We design a progressively increasing minimum support function and devise an algorithm to mine FIs over a sliding window. Our experiments show that our approach obtains highly accurate mining results even with a large ,so that the mining efficiency is significantly improved. In most cases, our algorithm runs significantly faster and consumes less memory than do the state-of-the-art algorithms [5, 2], while attains the same level of accuracy. Let I = { x 1 ,x 2 ,...,x m } be a set of items. An itemset is a subset of I .A trans-action , X ,isanitemsetand X supports an itemset, Y ,if X  X  Y .A transaction data stream is a continuous sequence of transactions. We denote a time unit in the stream as t i , within which a variable number of transactions may arrive. A window or a time interval in the stream is a set of successive time units, denoted as T = t i ,...,t j ,where i  X  j ,orsimply T = t i if i = j .A sliding window in the stream is a window that slides forward for every time unit. The window at each slide has a fixed number, w , of time units and w is called the size of the window. In this paper, we use t  X  to denote the current time unit .Thus,the current window is W = t  X   X  w +1 ,...,t  X  .

We define trans ( T ) as the set of transactions that arrive on the stream in atimeinterval T and | trans ( T ) | as the number of transactions in trans ( T ). The support of an itemset X over T , denoted as sup ( X, T ), is the number of transactions in trans ( T ) that support X . Given a predefined Minimum Support Threshold (MST) ,  X  (0  X   X   X  1), we say that X is a frequent itemset ( FI )over T if sup ( X, T )  X   X  | trans ( T ) | .

Given a transaction data stream and an MST  X  , the problem of FI mining over a sliding window is to find the set of all FIs over the window at each slide . Existing approaches [5, 4, 2] use an error parameter , , to control the mining accuracy, which leads to a dilemma. We t ackle this problem by considering = r X  as a relaxed MST , where r (0  X  r  X  1) is the relaxation rate ,tominethesetof FIs over each time unit t in the sliding window. Since all itemsets whose support is less than r X  | trans ( t ) | are discarded, we define the computed support as follows. Definition 1 (Computed Support). The computed support of an itemset X over a time unit t is defined as follows: The computed support of X over a time interval T = t j ,...,t l is defined as Based on the computed support of an itemset, we apply a progressively increasing MST function to define a semi-frequent itemset .
 Definition 2 (Semi-Frequent Itemset). Let W = t  X   X  w +1 ,...,t  X  be a win-dow of size w and T k = t  X   X  k +1 ,...,t  X  ,where1  X  k  X  w , be the most recent k time units in W . We define a progressively increasing function where m k =  X  | trans ( T k ) | and r k =( 1  X  r w )( k  X  1) + r .

An itemset X is a semi-frequent itemset ( semi-FI )over W if sup ( X, T k )  X  minsup ( k ), where k =  X   X  o +1 and t o is the oldest time unit such that sup ( X, t o ) &gt; 0.
 The first term m k in the minsup function in Definition 2 is the minimum support required for an FI over T k , while the second term r k progressively increases the relaxed MST r X  at the rate of ((1  X  r ) /w ) for each older time unit in the window. We keep X in the window only if its computed support over T k is no less than minsup ( k ), where T k is the time interval starting from the time unit t o ,inwhich the support of X is computed, up to the current time unit t  X  . We use a prefix tree to keep the semi-FIs. A node in the prefix tree represents an itemset, X , and has three fields: (1) item which is the last item of X ;(2) uid ( X ) which is the ID of the time unit, t uid ( X ) ,inwhich X is inserted into the prefix tree; (3) sup ( X ) which is the computed support of X since t uid ( X ) . The algorithm for mining FIs over a sliding window, MineSW , is given in Algorithm 1, which is self-explanatory.
 Algorithm 1 (MineSW) We run our experiments on a Sun Ultra-SPARC III with 900 MHz CPU and 4GB RAM. We compare our algorithm MineSW with a variant of the Lossy Counting algorithm [5] applied in the sliding window model, denoted as LCSW . We remark that LCSW, which updates a batch of incoming/expiring transactions at each window slide, is different from the algorithm proposed by Chang and Lee [2], which updates on each incoming/expiring transaction. We implement both algorithms and find that the algorithm by Chang and Lee is much slower than LCSW and runs out of our 4GB memory. We generate two types of data streams, t10i4 and t15i6, using a generator [3] that modifies the IBM data generator.
We first find (see details in [3]) that when r increases from 0.1 to 1, the precision of LCSW ( = r X  in LCSW) drops from 98% to around 10%, while the recall of MineSW only drops from 99% to around 90%. This result reveals that the estimation mechanism of the Lossy Counting algorithm relies on to control the mining accuracy, wh ile our progressively increasing minsup function maintains a high accuracy which is only slightly affected by the change in r . Since increasing r means faster mining process and less memory consumption, we can use a larger r to obtain highly accurate mining results at much faster speed and less memory consumption.

We test r =0 . 1and r =0 . 5 for MineSW. According to Lossy Counting [5], a good choice of is 0 . 1  X  and hence we set r =0 . 1 for LCSW. Fig. 1 (a) and (b) show that for all  X  , the precision of LCSW is over 94% and the recall of MineSW is over 96% (mostly over 99%). The recall of MineSW ( r =0 . 5) is only slightly lower than that of MineSW ( r =0 . 1). However, Fig. 2 (a) and (b) show that MineSW ( r =0 . 5) is significantly faster than MineSW ( r =0 . 1) and LCSW, especially when  X  is small. Fig. 3 (a) and (b) show the memory consumption of the algorithms in terms of the number of itemsets maintained at the end of each slide. The number of itemsets kept by MineSW ( r =0 . 1) is about 1.5 times less than that of LCSW, while that kept by MineSW ( r =0 . 5) is less than that of LCSW by up to several orders of magnitude. We propose a progressively increasing minimum support function, which allows us to increase at the expense of only slightly degraded accuracy, but signif-icantly improves the mining efficiency and saves memory usage. We verify, by extensive experiments, that our algorithm is significantly faster and consumes less memory than existing algorithms, while attains the same level of accuracy. When applications require highly accurate mining results, our experiments show that by setting =0 . 1  X  (a rule-of-thumb choice of in Lossy Counting [5]), our algorithm attains 100% precision and over 99.99% recall.

