 Retrieval in a question and answer archive involves finding good answers for a user X  X  question. In contrast to typical document retrieval, a retrieval model for this task can ex-ploit question similarity as well as ranking the associated an-swers. In this paper, we propose a retrieval model that com-bines a translation-based language model for the question part with a query likelihood approach for the answer part. The proposed model incorporates word-to-word translation probabilities learned through exploiting different sources of information. Experiments show that the proposed transla-tion based language model for the question part outperforms baseline methods significantly. By combining with the query likelihood language model for the answer part, substantial additional effectiveness improvements are obtained. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Question and Answer Retrieval, Translation Model, Lan-guage Model, Information Retrieval
Large scale question and answer (Q&amp;A) archives have be-come an important information resource on the Web. These include the FAQ archives constructed by companies for their products and the archives generated from Web services such as Yahoo Answers! and Live QnA, where people answer  X 
The contributions of this author were done during graduate studies at UMass Amherst.
 questions posed by other people. The retrieval task in a Q&amp;A archive is to find relevant question-answer pairs for new questions posed by the user [6]. Q&amp;A retrieval has several advantages over Web search. First, the user can use natural language instead of only keywords as a query, and thus can potentially express his/her information need more clearly. Second, the system returns several possible answers directly instead of a long list of ranked documents, and can therefore increase the efficiency of finding the re-quired answers. Q&amp;A retrieval can also be considered as an alternative solution to the general Question Answering (QA) problem. Since the answers for each question in the Q&amp;A archive are generated by humans, the difficult QA task of extracting a correct answer is transformed to the Q&amp;A retrieval task.

The major challenge for Q&amp;A retrieval, as for most in-formation retrieval tasks, is the word mismatch between the user X  X  question and the question-answer pairs in the archive. For example,  X  X hat is francis scott key best known for? X  and  X  X ho wrote the star spangle banner? X  are two very similar questions, but they have no words in common. This problem is more serious for Q&amp;A retrieva l, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording.
To solve the word mismatch problem, many different ap-proaches have been proposed. In this paper, we focus on translation-based approaches since the relationships between words can be explicitly modeled through word-to-word trans-lation probabilities.

Berger and Lafferty [2] proposed using the classic IBM translation model 1 for information retrieval tasks 1 .How-ever, because of various fundamental differences between machine translation and information retrieval, the pure IBM model performs worse than other state of the art retrieval al-gorithms. We explain the reasons for the poor performance of the pure IBM model in the comparison with the query likelihood language model. This comparison also gives us insights that enable us to address problems with the IBM model. We propose a mixed model that leverages the benefit of both approaches.

Besides designing the translation based retrieval model, another important problem is how to learn good word-to-word translation probabilities. In a Q&amp;A archive, since the asker and the answerer may express similar meanings with different words, it is natural to use the question-answer pairs as the  X  X arallel corpus X  that is used for estimation in machine
IBM model 1 will be described in the following section. translation. Since the question part and answer part are written in the same language, the word-to-word translation probabilities can be learned with either part as the source language and the other part as the target. Intuitively, the same word will be related to different sets of words whichever part it appears in. Thus, combining the word-to-word trans-lation probabilities learned with different source and target configurations is beneficial. We propose two combination techniques to improve the translation probability estimates.
Question-answer pairs can also be viewed as documents with different fields, and that probabilities associated with these fields may be estimated in different ways. If we as-sume a language model approach, we can consider how to estimate probabilities of generating queries. Given the word mismatch problem between the user question and questions in the archive is particularly acute, for the question part, the query is generated by our proposed translation-based language model. For the answer part, the query is simply generated by the query likelihood language model. Our fi-nal model for Q&amp;A retrieval is a combination of the above models. Experiments show that our proposed translation-based language model for the question part outperforms three types of representative baseline methods significantly. After combining with query likelihood language model for the answer part, further i mprovement is observed.
Most previous studies on translation-based information retrieval did not recognize the weakness of the original trans-lation model and adopted the IBM model  X  X s is X . They also suffered from low-quality word to word translation probabil-ity estimates. In this paper, we overcome these drawbacks and successfully propose a translation-based language model to solve the word mismatch problem in Q&amp;A retrieval.
A typical Q&amp;A archive consists of a huge number of question-answer pairs. Here, C denotes the whole archive, C = { ( tions in C , Q = { q 1 ,q 2 , ..., q M } and A denotes the set of all answers in C , A = { a 1 ,a 2 , ..., a N } .Foreach( q, a q  X  Q and a  X  A .Notethat M  X  L and N  X  L ,since the same question can be provided with different answers and the same answer can correspond to different questions. Given the user question q 2 , the task of Q&amp;A retrieval is to rank ( q, a ) i according to score ( q , ( q, a ) i ). Under the lan-guage modeling framework, this score can be modeled by the probability that q is generated by ( q, a ) i .Thus,the following parts of this section focus on how to calculate P ( q | ( q, a ) i ).
Both the IBM model and the query likelihood language model are generative models: the former for translation, the latter for information retrieval. Although they were pro-posed for different purposes, they share many common as-pects and assumptions. In this subsection, we compare these two approaches and propose a new model that combines ad-vantages of both approaches 3 .

The language modeling approach to information retrieval
In this paper, the user question has the same meaning as the user query.
This part was first published in Jeon X  X  Ph.D. thesis [4]. [12] has been successfully applied to many different applica-tions because of its flexibility and theoretically solid back-ground. The probab ilities of sampling (or generating) the query from document language models are used to rank doc-uments. Typically, unigram language models with the max-imum likelihood estimator are used to estimate document language models that are smoothed by background collec-tions with the Dirichlet smoothing technique [16].
IBM model 1 [3] does not require any linguistic knowl-edge of the source or the target language and treats every possible word alignment equally. Because of its simplicity and proven performance, this model has been widely used for many translation tasks. Berger and Lafferty [2] proposed to directly use this model for retrieval tasks.

The ranking function for the query likelihood language model with Dirichlet smoothing and IBM model 1 are com-pared in Table 1.

Here, q is the query, D is the document, C is the back-ground collection,  X  is the smoothing parameter, | D | and are the lengths of D and C , respectively. #( t, D ) denotes the frequency of term t in D . P ( w | null ) is the probabil-ity that the term w is translated (generated) from the null term. P ( w | t ) is the the translation probability from word to word w .

From Table 1, it is easy to recognize that the equations used to describe the query likelihood language model and the IBM model look similar to each other. There are three different parts in the equations, which can be compared as follows:
P (
However, we cannot simply choose the sampling method used in the IBM model because of the self translation prob-lem. Since the target and the source languages are the same, every word has some probability to translate into itself. In some cases, low self-translation probabilities reduce retrieval performance by giving very low weights to the matching terms. In the opposite case, very high self-translation proba-bilities do not exploit the merits of the translation approach.
To overcome this problem, a few different approaches have been proposed. Murdock and Croft [10] use the translation based sampling method only when the target term does not exist in the document and use the maximum likelihood es-timator if the target term is present in the document. This method does not fully exploit the power of the translation method. Jeon et al. [6] set P ( w | w ) = 1 for all w while maintaining other word translation probabilities unchanged. This approach produces inconsistent probability estimates and makes the model unstable. Jin et al. [8] force other terms to have lower translation probabilities than self trans-lations: P ( w | w )  X  P ( w = w | w ). This constraint can reduce the problem but very low or ver y high self translations are still possible. All these improvised modifications gave sig-nificant improvements over the original translation model.
Instead of using makeshift solutions, here, we propose to linearly mix two different estimations: maximum likelihood estimation and translation based estimation. Our final rank-ing function is given as,
P mx ( w | D )=(1  X   X  ) P ml ( w | D )+  X 
In our translation based language model (TransLM), we can control the impact of the translation component by  X  .If we set a small value for  X  , the model behaves like the query likelihood model and the importance of matching terms is emphasized. This is similar to increasing the self translation probability. Therefore, we can control the amounts of self translation using  X  . The background smoothing is adjusted using  X  .

In the situation of Q&amp;A retrieval, after applying our trans-lation based language model to the question part, the above equations are changed as follows.

Note that in Eq. 6, P mx ( w | ( q, a )) is only calculated based on the question part and the answer part of a question-answer pair is not considered. Although it has been shown that doing Q&amp;A retrieval based solely on the answer part does not perform well [6], the answer part should provide ad-ditional evidence about relevance and, therefore, it should be combined with the estimation based on the question part. We use the query likelihood language model for the an-swer part, which is combined with the translation-based lan-guage model for the question part to form the final retrieval model. In this combined model, P ( q | ( q, a )) and P ( w | are estimated with Eq. 4 and Eq. 5. The estimation for P mx ( w | ( q, a )), however, is changed to the following: P mx ( w | ( q, a )) =  X P ml ( w | q )+  X  where  X  +  X  +  X  =1.

In Eq. 7, the generation probability of the question part is modeled by  X P ( w | q )+  X  ation probability of the answer part is modeled by  X P ( w | a The relative importance of these components is adjusted through  X  ,  X  and  X  .When  X  = 0, the retrieval model is based only on the translation-based language model for the question part. When  X  = 1, the retrieval model is based on the query likelihood language model for the answer part. In addition, when  X  = 0, the retrieval model becomes a combi-nation model which combines the language model estimated from different fields [11].
The performance of the proposed retrieval model heavily depends on the quality of the learned word-to-word transla-tion probabilities. In this section, techniques for estimating word-to-word translation probabilities are discussed in de-tail.
IBM translation model 1 incorporated an EM-based algo-rithm to learn the word-to-word translation probabilities. Suppose there is a parallel corpus consisting of English-French sentence pairs, S = { ( e 1 , f 1 ) , ( e 2 , f 2 ) The translation probability from an English word e to an French word f is calculated as: c ( f | e ; f i , e i )= Here,  X  e = to make the sum of translation probabilities for the word equal to 1. { e 1 , ..., e l } are English words that appear in e #( f, f i )and#( e, e i ) are the number of times the French word f appears in f i and the number of times the English word e appears in e i .

Given the initial value of P ( f | e ), Eq. 8 and Eq. 9 are used to calculate the updated P ( f | e ) repeatedly until the probability converges. Brown et. al. [3] showed that this process converges to the same final probability no matter what initial values are set.
In a Q&amp;A archive, question-answer pairs can be consid-ered as a type of parallel corpus, which is used for estimating word-to-word translation probabilities. In IBM translation model 1, English is the source language and French is the target language. Since the questions and answers in a Q&amp;A archive are written in the same language, the word-to-word translation probability can be calculated through setting ei-ther as the source and the other as the target. P ( A | Q is used to denote the word-to-word translation probability with the question as the source and the answer as the target. P ( Q | A ) is used to denote the opposite configuration.
For a given word, the related words differ when it ap-pears in the question or in the answer. For example, when the word  X  X heat X  appears in the question part, words such as  X  X rust X ,  X  X orgive X ,  X  X ump X  and  X  X eave X  usually appear in the corresponding answer part. These words represent the answerer X  X  suggestion when the asker poses some question about how to react to cheating behaviors. On the other hand, when the word  X  X heat X  appears in the answer, words such as  X  X usband X  and  X  X oyfriend X  will be observed in the question, which implies most cheating related questions are about the asker X  X  husband and boyfriend. Clearly, all these words are useful to attack the word mismatch problem, thus it is reasonable to combine P ( Q | A )and P ( A | Q ) instead of choosing just one of them.

In addition, the correspondence of words in the question-answer pair is not as strong as in the English-French sen-tence pair, thus noise will be inevitably introduced for both P (
Q | A )and P ( A | Q ). Suppose a word w 2 appears in the corresponding answer or question part whenever the word w 1 appears in the question or answer part. Another word w 3 only appears in the corresponding answer part when w 1 appears in the question part. Intuitively, w 2 should be more similar to w 1 than w 3 . This intuition will be considered im-plicitly by combining P ( Q | A )and P ( A | Q ), since P will get contributions from both P ( Q | A )and P ( A | Q P ( w 3 | w 1 ) only gets the contribution from P ( A | Q ). Two methods are used to combine P ( Q | A )and P ( A | Q ). These two methods differ in the stage that the combination occurs. The first method linearly combines the trained word-to-word translation probabilities, which is shown as follows:
P lin ( w i | w j )=(1  X   X  ) P ( w i ,Q | w j ,A )+  X P ( w i
The second method first pools the question-answer pairs used for learning P ( A | Q ) and the answer-question pairs used for learning P ( Q | A ) together, and then uses Eq. 8 and Eq. 9 to learn the combined word-to-word translation probabil-ities. Suppose we use the collection { ( q, a ) 1 , ..., learn P ( A | Q ) and use the collection { ( a, q ) 1 , ..., learn P ( Q | A ), then { ( q, a ) 1 , ..., ( q, a ) n , ( used here to learn the combination translation probability P
Table 2 shows some example word-to-word translations learned from the Wondir dataset 4 , using three different ways of estimating the translation probabilities. It can be seen that most top target words are semantically related to the source word, regardless of the estimation method.
To clarify the differences between using questions and an-swers as sources and targets, consider the word  X  X verest X . When this word appears in the question part, the words  X 29,035 X ,  X 8,850 X ,  X  X eet X  and  X  X eight X  often appears in the corresponding answers, as shown by the P ( A | Q ) column, since the user often asks about the height of the moun-tain everest. On the other hand, if this word appears in the answer part, the corresponding question part often con-tains words such as  X  X allest X ,  X  X ighest X , and  X  X ountain X  as shown by the P ( Q | A ) column, because  X  X verest X  is used as the answer to the questions such as  X  X hat is the highest mountain? X . Furthermore, P pool shows that after combining P (
A | Q )and P ( Q | A ),wecanobtainboththeseimportant words.

It is also interesting to note that for the source term  X  X p X , the rank of  X  X rive X  is higher than  X  X indow X  according to P (
A | Q ). However, P pool assigns the opposite order for these two words, since  X  X indow X  is also among the top words ac-cording to P ( Q | A ) but  X  X rive X  is not. Intuitively,  X  X indow X  should be more similar to  X  X p X  than  X  X rive X . This intuition supports our assumption that two words are more similar when they are related with different source-target config-urations. Also, our proposed combination method indeed boosts such words implicitly.
In this section, experiments are conducted on a real Q&amp;A archive to demonstrate the effect of our proposed retrieval model.
The details will be introduced in the following part. Source everest xp search
TTable P ( A | Q ) P ( Q | A ) P pool P ( A | Q ) P (
Q | A ) P pool P ( A | Q ) P ( Q | A ) P pool
The Wondir collection consists of roughly 1 million ques-tion and answer pairs collected from a community based question answering service run by Wondir 5 . The collection has been used in previous research on question and answer services (e.g., [9]). Topics for questions are very diverse, ranging from restaurant recommendations to rocket science. The average length for the question part and the answer part is 27 words and 28 words respectively. Spelling errors are very common in this collection, which makes the word mismatch problem very serious.

For a practical Q&amp;A retrieval system, the search results should be presented to the user in a hierarchical way, where a list of questions is first presented, and after the user selects a specific question the corresponding answers are presented. This hierarchical strategy is more effective for the user than directly presenting a list of question-answer pairs, since this result list could easily be overwhelmed a single question with many answers. Since the relevance of the answer to its corre-sponding question is usually guaranteed (with the exception of  X  X pam X  answers [5]), the retrieval performance of a system can be measured by the rank of relevant questions it returns. Thus, in our experiments, relevance judgments are based on questions. It is easy to transform a question-answer pair rank into a question rank by taking the highest rank among a group of question-answer pairs for the same question. In all the following experiments, ranking algorithms first out-put question-answer pair ranks that are then transformed into question ranks. 50 questions from the TREC-9 QA track 6 are used for testing. These questions are selected from search engine logs collected from Excite and Encarta. A pooling technique was used to find candidate questions for each query. After being manually judged, 220 semantically similar questions are found in total for 50 queries.

Mean Average Precision (MAP) and Precision at 10 (P@10) are used as the performance measures. A two-sided paired t-test is used for significance testing.

Three types of baselines are used to compare with our proposed retrieval model, which are summarized as follows: http://www.wondir.com http://trec.nist.gov/data/qa/t9_qadata.html
A preliminary experiment was conducted to show the im-portance of the question part and the answer part for Q&amp;A retrieval. The query likelihood retrieval model was used with the question parts, the answer parts, and the question-answer pairs, respectively. Table 3 shows the retrieval per-formance with these different fields.

Table 3 shows that on Wondir dataset the question part is more important than the answer part for Q&amp;A retrieval, which supports the observation of previous research [6, 7]. As expected, the performance of using the question-answer pair is better than using each field alone. Thus, it is rea-sonable for Type I baselines to work on the question-answer pair instead of any field.

Our proposed translation-bas ed retrieval model based on the question part is compared with the state-of-the-art re-trieval systems (Type I), the combination technique for doc-uments with different fields (Type II), and other translation-based language models based on the question part (Type III). For each method, the best performance after parameter tuning is reported. For translation-based methods, P ( Q | A and P ( A | Q ) are both used. The results are summarized in Table 4. Note that, because of the many baselines and es-timation methods, we present a comparison of all retrieval runs using significance tests after reporting all results.
Table 4 shows that our proposed TransLM model per-Table 4: Comparisons with three types of baselines. formsbetterthanboththestate-of-the-art retrieval systems and the combination technique for documents with different fields. Compared with other translation-based approaches, TransLM shows improvement no matter what kind of word-to-word translation probability is used. In addition, it seems that P ( A | Q )ismoreeffectivethan P ( Q | A ), which can be explained as the question source being more important than the answer source for generating the user question. Overall, with P ( A | Q ) our proposed TransLM model outperforms any baseline method noticeably.
 Fig. 1 shows the influence of  X  for the performance of TransLM with P ( A | Q ). The RM, LM-Comb, Murdock and Jeon methods are used for reference. Fig. 1 shows that, for MAP, TransLM performs better than the baseline methods when  X  is between 0.6 and 0.9, whereas for P@10, TransLM performs better than the baselines methods when  X  is be-tween 0.2 and 0.9. In both cases, a relatively broad set of good parameter values is observed.
 Although P ( A | Q ) shows good performance when used for TransLM, we carried out experiments to see whether the performance of TransLM can be further improved with the word-to-word translation probability combination techniques proposed in Section 3.

Table 5 compares the effect of P lin and P pool with P ( A | Q and P ( Q | A ) when used with TransLM. Here, the best perfor-mance of P lin is reported after tuning its linear interpolation parameter  X  . Table 5 shows that both combined translation probability estimates are better than P ( Q | A )and P ( A | Q The method P pool performs slightly better than P lin .Fig.2 shows the influence of the parameter  X  on the performance of
After exploring translation based language models and methods for learning word-to-word translation probabilities, Table 5: Results for the combined word-to-word translation probability.
 Table 6: Results for the combined retrieval model. we then tested the performance of our retrieval model for question-answer pairs that incorporates the answer part (Eq. 7). This model is called TransLM+QL. Table 6 compares TransLM+QL with TransLM and LM-Combine. P pool is used as the method for estimating translation probabilities.
Table 6 shows that by incorporating the query likelihood language model from the answer part, TransLM+QL fur-ther improves TransLM significantly, even though the latter showed significant improvement over other baseline meth-ods. This observation can be explained as follows. Some-times, different wording to the concept of the question can be directly observed in the corresponding answer, thus the query likelihood language model for the answer can be con-sidered as a good complement to the translation-based lan-guage model for the question.

Fig. 3 shows the influence of  X  and  X  on the perfor-mance of TransLM+QL. Different colors are used to denote the parameter configurations where the performance is in-ferior to LM-Comb, the performance is between LM-Comb and TransLM and the performance is better than TransLM. No color areas denote the invalid parameter configurations. Clearly, better performance can be observed when  X  is small and  X  is relatively big.
 A comparison using the paired t-test is conducted for TransLM( P ( Q | A )), TransLM(( P ( A | Q )), TransLM( P TransLM( P pool )andTransLM( P pool )+QL over baseline meth-ods. The results are summarized in Table 7, which clearly shows that our proposed retrieval model TransLM( P pool )+QL outperforms all baseline methods significantly.

Finally, two retrieval examples are shown in Table 8. These examples indicate that the questions retrieved by our models are more reasonable than the language model results 7 .
Since the LM is based on question-answer pairs, it is possi-on the performance of P lin Figure 3: The influence of  X  and  X  on the perfor-mance of TransLM+QL
There has been research related to our approach in ques-tion and answer retrieval, FAQ retrieval, and translation-based retrieval. Apart from Jeon X  X  previous work ([6, 5, 4], there has been other work on retrieval from FAQ data, which is very similar to Q&amp;A data. Berger et al. [1] re-ported some of the earliest work using statistical retrieval models, including translation-based approaches, with FAQ data. This work used the  X  X ure X  IBM model 1 to find rele-vant answers among multiple candidate answers for call cen-ter users. Their experiments were done with small data sets that consisted of only a few thousand Q&amp;A pairs. Riezler et al. [13] also demonstrated the potential advantages of a translation-based approach to retrieval with FAQ data us-ing a more sophisticated translation model trained on a large amount of data extracted from FAQ pages on the Web. Sori-cut and Brill [15] used one million FAQs collected from the web to train their answer passage retrieval system. They also used the original IBM model 1 without any modifica-tion. Other work on FAQ retrieval has used simpler retrieval models. For example, FAQ Finder [14] used the conventional vector space model to calculate the statistical similarities between questions and WordNet to help calculate the se-mantical similarities. These two types of similarities were combined heuristically to rank FAQs.

Recently, Jijkoun and Rijke [7] automatically collected ap-proximately three million FAQs from the web and imple-mented a retrieval system for the collected FAQ collection. Their retrieval system was con structed based on the vector space model. Several combinat ions of scores for different fields were attempted in their experiments and the results showed the importance of the question part. ble that some questions without query words are retrieved.
There are two main issues with applying translation meth-ods. The first is to modify the model to make it suitable for monolingual transformation rather than translation, and the second is the estimation of translation probabilities. In sec-tion 2, we reviewed some previous attempts [6, 10, 8] to address these problems.
Q&amp;A retrieval has become an important issue due to the popularity of Q&amp;A archives on the web. In this paper, we propose a novel translation-based language model to solve this problem. Our approach combines the translation-based language model estimated using the question part and the query likelihood language model estimated using the answer part. A new technique was described for using different con-figurations of question-answer pairs to improve the quality of the translation probability estimates. The retrieval exper-iments demonstrated the effectiveness of both the retrieval model and the estimation technique.

Our experiments were conducted on one type of Q&amp;A archive, which was collected from a web service where people answer questions posed by other people. Further work will focus on testing the effect of the proposed retrieval model on FAQ archives. We also plan to work on data from Ya-hoo! Answers, which is potentially a much larger collection than Wondir. The new experiments will use questions de-rived from this data. Other techniques for combining the models estimated from the question and answer parts will be investigated. In addition, phrase-based machine transla-tion models have shown superior performance compared to word-based translation models in translation applications. We plan to study the effectiveness of these models in the Q&amp;A setting.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by NSF grant #IIS-0711348. Any o pinions, findi ngs and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] A. Berger, R. Caruana, D. Cohn, D. Freitag, and
Table 7: T-Test result summary. m and p denote p-value &lt; , p m , p m , p [2] A. Berger and J. Laffery. Information retrieval as [3] P.F.Brown,V.J.D.Pietra,S.A.D.Pietra,and [4] J. Jeon. Searching Question and Answer Archives .IR, [5] J. Jeon, W. B. Croft, J. Lee, and S. Part. A [6] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [7] V. Jijkoun and M. de Rijke. Retrieving answers from [8] R. Jin, A. G. Hauptmann, and C. Zhai. Title language [9] X. Liu, W. B. Croft, and M. Koll. Finding experts in [10] V. Murdock and W. B. Croft. A statistical model for [11] O. Paul and J. Callan. Language models and [12] J. M. Ponte and W. B. Croft. A language modeling [13] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, [14] R. D. Rurke, K. J. Hammond, V. A. Kulyukin, S. L. [15] R. Soricut and E. Brill. Automatic question [16] C. Zhai and J. Lafferty. A study of smoothing
