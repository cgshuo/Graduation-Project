
Yong Zhuang, Wei-Sheng Chin, Yu-Chin Juan, and Chih-Jen Lin known that a distributed training algorithm may involve expensive communi-cation cost between machines. The aim of this work is to construct a scalable distributed training algorithm for large-scale logistic regression. Logistic regression is a binary classifier that has achieved a great success in many fields. Given a data set with l instances ( y i , x y  X  X  X  1 , 1 } is the label and x obtain the model w . In this paper, we design a distributed Newton method for logistic regression. Many algorithmic and implementation issues are addressed in order to reduce the communication cost and speed up the computation. For example, we investigate different ways to conduct parallel matrix-vector products and discuss data for-mats for storing feature values. Our resulting implementation is experimentally shown to be competitive with or faster than ADMM and VW, which were con-implement a variant of our approach on Spark 1 through private communication, but they mainly focus on the efficient use of Spark. distributed Newton method. Experiments are in Section 4 while we conclude in Section 5 . Many distributed algorithms have been proposed for linear classification. ADMM has recently emerged as a popular method for distributed convex optimization. Although ADMM is a known optimization method for decades, only recently methods. Besides, parallel coordinate descent methods have been considered a stochastic gradient method in the beginning, then switches to parallel LBFGS [ 13 ] for a faster final convergence.
 considered state-of-the-art and therefore are involved in our experiments. 2.1 ADMM for Logistic Regression Zhang et al. [ 19 ] apply ADMM on linear support vector machine with squared hinge loss. Here we modify it for logistic regression. Assume data indices are partitioned to J sets M 1 ,...,M J to indicate data on rewrite the problem ( 1 ) to the following equivalent form. ( 1 ). ADMM repeatedly performs ( 3 )-( 5 ) to update primal variables Lagrangian dual variable  X  using the following rules. where  X &gt; 0 is a chosen penalty parameter. Depending on local data, the local w ,  X  j, so an O ( n ) amount of local data from each machine is communicated across the network. The iterative procedure ensures that under some assump-tions, as k  X  X  X  , { z k } approaches an optimum of ( 1 ).
 Recall that the communication in ( 4 ) involves O ( n ) data per machine. Obvi-ously the cost is high for a data set with a huge number of instances (i.e., can transform the scale of the communicated data to O ( l have
J machines, the data matrix X is partitioned to X However, the optimization process becomes different from ( 2 )-( 5 )[ 3 ]. 2.2 VW for Logistic Regression by using only local data at each machine, it applies stochastic gradient method edly averages the model as the initial solution for the subsequent quasi New-ton method [ 13 ] on the whole data. The stage of applying stochastic gradient sub-problem. Newton method applied is LBFGS, which uses m vectors to approximate inverse Hessian ( m is a user-specific parameter). To support both numerical and string lookup. That is, it applies a hash function on the feature index to generate a new index for that feature value.
 We discuss the communication cost of VW, which supports only the instance-no communication until VW weightedly averages local w j ,  X  j on each machine must be aggregated. For LBFGS, it collects each machine to calculate the function value and the gradient. Therefore, the communication cost per LBFGS iteration is similar to that of each ADMM iteration under the instance-wise data split. method.
 3.1 Newton Methods method updates the current model w by where s , the Newton direction, is obtained by minimizing ing linear system instead. For data with a huge number of features,  X  2 f ( w ) becomes too large to be we note that where D is a diagonal matrix with ficient to finish the Hessian-vector product. Because  X  2 formed, the memory difficulty is alleviated.
  X  in ( 10 ) to ensure the reduction of the function value. If  X  is not large enough, then s is rejected and w is kept. Otherwise, by ( 6 ). Then, the radius  X  is adjusted based on  X  [ 11 ].
 operations such as function and gradient evaluations are also important. For example, the function value in ( 1 ) requires the calculation of discussion, we can conclude that a scalable distributed Newton method relies will discuss more details in the rest of this section. 3.2 Instance-Wise and Feature-Wise Data Splits X or X fw, j represents the j th segment of data stored in the we use  X  X W/iw X  and  X  X W/fw X  to denote instance-wise and feature-wise splits. To discuss the distributed operations easily, we use vector notation to rewrite ( 1 )as where X and Y are defined in Section 2.1 , log(  X  )and  X  ( applied to a vector, and  X   X   X  stands for a dot product. In ( 11 ), vector of all ones. Then the gradient can be represented as Next we discuss details of distributed operations under the two different data splits.
 dient values, and Hessian-vector products can be written as and matrix of Y , e ,and D corresponding to instances in the j and redistributes the sum to them. For example, J j =1 log( that each machine calculates its own log(  X  ( Y j X iw, j summation is performed.
 Feature-wise Split: We notice that where w j is a sub-vector of w corresponding to features stored in the machine. Therefore, in contrast to IW, each machine maintains only a sub-vector of w . The situation is similar to other vectors such as s . However, to calculate the function value for checking the sufficient decrease ( 10 ), summed and then distributed to all machines. Therefore, the function value is calculated by Similarly, each machine must calculate part of the gradient and the Hessian-vector product: and where, like w , only a sub-vector v p of v is needed at the Newton methods. For example, to evaluate the value in ( 7 ) we must obtain where  X  f ( w ) fw, j and s j are respectively the sub-vectors of J values rather than vectors in ( 16 ). Another difference from the IW approach is that the label vector y is stored in every machine because of the diagonal Analysis: To compare the communication cost between IW and FW, in Table 1 we show the number of operations at each machine and the amount of data sent to all others. From Table 1 , to minimize the communication cost, IW and FW should be used for l n and n l , respectively. We will confirm this property through experiments in Section 4 . 3.3 Other Implementation Techniques Load Balancing: The parallel matrix-vector product requires that all machines machines have a similar computational load. Now the computational cost is related to the number of non-zero elements, so we split data in a way such that each machine contains data of a similar number of non-zero values. Data Format: Lin et al. [ 11 ] discuss two approaches to store the sparse data matrix X in an implementation of Newton methods: compressed sparse row (CSR) and compressed sparse column (CSC). They conclude that because of the possibility of storing a whole row or column into a higher-level memory (e.g., cache), CSR and CSC are more suitable for l n and n l Because we split data so that each machine has a similar number of non-zero elements, the number of columns/rows of the sub-matrix may vary. In our imple-mentation, we dynamically decide the sparse format based on if the sub-matrix X  X  number of rows is greater than columns.
 Speeding Up Hessian-vector Product: Instead of sequentially calculating X v , D ( X v ), and X T ( D ( X v )) in ( 9 ), we can re-write Then the data matrix X is accessed only once rather than twice. However, this technique can only be applied for instance-wisely split data in the CSR format. If the data is split by features, then calculating each x operation. The l all-reduce summations in ( 20 ) cause too high communication cost in practice. In this section, we begin with describing a specific Newton method used for our implementation. Then we evaluate techniques proposed in Section 3 , followed by a detailed comparison between ADMM, VW, and the distributed Newton method on objective function values and test accuracy. 4.1 Truncated Newton Method In Section 3.1 , using CG to find the direction s may struggle with too many that employs CG to approximately minimize ( 7 ) under the constraint of in the trust region. With the Newton direction s obtained approximately, the settings for our distributed implementation. 4.2 Experimental Settings 1. Data Sets: We use five data sets listed in Table 2 for experiments, and 2. Platform: We use 32 machines in a local cluster. Each machine has an 8-3. Parameters: For C in ( 1 ), we tried different values in 4.3 IW versus FW This subsection presents a comparison between two different data splits. We run the Newton method until the following stopping condition is satisfied: where =10  X  6 and pos/neg are the number of positive and negative data. are consistent with our analysis in Table 1 . For example, IW is better than FW for epsilon , which has l n . On the contrary, FW is superior for because l n . This experiment reflects that a suitable data split strategy can significantly reduce the communication cost.
 the time for computation and synchronization. It is only similar but not the same for IW and FW because of the variance of the cluster X  X  run-time behavior and the algorithmic differences. 4.4 Comparison Between State-of-the-art Methods on Function Values We include the following methods for the comparison.  X  the distributed trust-region Newton method (TRON) : It is imple-mented in C++ with OpenMPI [ 7 ].  X  ADMM : We implement ADMM using C++ with OpenMPI [ 7 ]. Following descent method [ 17 ] with a fixed number of iterations, where the number of iterations is decided by a validation procedure. The parameter the setting in [ 19 ].  X  VW (version 7.6) : The package uses sockets for communications and syn-chronization. Because VW uses the hashing trick for the features indices, hash collisions may cause not only different training/test sets but also worse accuracy. To reduce the possibility of hash collisions, we set the size of the hashing table larger than the original data sets. 3 Although the data set may be slightly different from the original one, we will see that the conclusion of our experiments is not affected. Besides, we observe that the default setting of running a stochastic gradient method and then LBFGS is very slow on webspam , so for this problem, we apply only LBFGS. 4 Except of features hashing, we use the default parameters in VW.
 In Figure 2 , we compare VW, ADMM, and the distributed Newton method the current function value to the optimum: The reference optimal w  X  is obtained approximately by running the Newton method with a very small stopping tolerance. 5 We present results of ADMM and distributed Newton using both instance-wise and feature-wise splits. For the sparse format in Section 3.3 , the distributed Newton method dynamically chooses CSC or CSR, while ADMM uses CSC because of the requirement of the coordinate descent method for the sub-problem ( 3 ).
 Results in Figure 2 indicate that with suitable data splits, the distributed Newton method converges faster than ADMM and VW. Regarding IW versus FW, when l n , an instance-wise split is more suitable for the distributed Newton method, while a feature-wise split is better for n l consistent with Table 2 . However, the same result does not hold for ADMM. One of the distributed Newton method are exactly the same. In contrast, ADMM X  X  optimization processes are different under IW and FW strategies [ 3 ]. the Newton-method implementation of the software LIBLINEAR a model having similar prediction capability to the optimal solution has been obtained. In Figure 2 , ADMM can quickly reach the horizontal line for some problems, but is slow for others. 4.5 Comparison Between State-of-the-art Methods on Test Accuracy We present in Figure 3 the relation between the training time and where best accuracy is the best final accuracy obtained by all methods. In the early stage ADMM is better than the other two, while the distributed Newton method gets the final stable performance more quickly on all problems except 4.6 Speedup Following Agarwal et al. [ 1 ], we compare the speedup of ADMM, VW, and the distributed Newton method for obtaining a fix test accuracy by varying the number of machines. Results are in Figure 4 .
 have l n and n l , respectively. For ADMM and the distributed Newton method, we use the data split strategy leading to the better convergence in Figure 2 . Figure 4 shows that for epsilon ( l n ), VW and the distributed Newton method yield a better speedup than ADMM as the number of machines increases, while for webspam ( n l ), the distributed Newton method is better than the other two.
 and problems in [ 1 ], so we conduct some investigation. For [ 1 ], they have l n . Thus we suspect that because n l for considers an instance-wise split, VW suffers from high communication cost. To property of the problem webspam is that it actually has only 680,715 non-zero feature columns, although its feature indices are up to 16 million. We generate a new set by removing zero columns and rerun VW. 6 The result, indicated as VW* in Figure 4 (b), clearly shows that the speedup is significantly improved. Because of parallelizing only the matrix-vector products, the numbers of itera-tions in VW and the distributed Newton method are independent of the number of machines. In contrast, ADMM X  X  number of iterations may significantly vary, 5 , we present the relation between the number of ADMM iterations and the rel-ative difference to the optimum. It can be seen that as the number of machines increases, the higher number of iterations comes with more computational and communication costs. Thus the speedup of ADMM in Figure 4 is not satisfactory. To the best of our knowledge, this work is the first comprehensive study on the distributed Newton method for regularized logistic regression. We carefully address important issues for distributed computation including communication and memory locality. An advantage of the proposed distributed Newton method and VW over ADMM is that the optimization processes are independent of the distributed configuration. That is, the number of iterations remains the same regardless of the number of machines. Our experiment shows that in a practi-cal distributed environment, the distributed Newton method is faster and more scalable than ADMM and VW that are considered state-of-the-art for real-world problems.
 However, because of requiring differentiability, Newton methods are more restrictive than ADMM. For example, if we consider L1-regularization by replac-ing w 2 / 2in( 1 ) with w tiable, so Newton methods cannot be directly applied. 7 Our experimental code is available at (removed for the blind review requirements).

