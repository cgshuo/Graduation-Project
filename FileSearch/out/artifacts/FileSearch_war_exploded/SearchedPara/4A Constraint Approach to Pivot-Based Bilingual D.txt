 MAIRIDAN WUSHOUER, DONGHUI LIN, and TORU ISHIDA , Department of Social Bilingual dictionaries, machine-readable resources used to translate a word or phrase from one language to another, are essential for many tasks in Natural Language Pro-cessing (NLP), such as machine translation [Brown et al. 1990] and cross-lingual in-formation retrieval [Nie et al. 1999]. However, high-quality bilingual dictionaries are only available for well-resourced language pairs, such as English X  X rench or English X  Chinese; they remain sparse, dated, or simply unavailable for less resourced language pairs like Uyghur and Kazakh. Hence, researchers have investigated the issue of automatic creation of a bilingual dictionary. For example, a bilingual dictionary has been induced from a large-scale parallel corpus using subsentential alignment techniques [Wu and Xia 1994]. More recently, the use of comparable corpora (e.g., Wikipedia) has drawn increasing attention [Dou and Knight 2012; Yu and Tsujii 2009; Haghighi et al. 2008] since the Internet era has made monolingual data readily avail-able 1 while parallel corpus remains scarce.

From the viewpoint of the etymological closeness of languages, some studies directly tackled the creation of dictionaries for closely related language pairs such as Spanish and Portuguese [Schulz et al. 2004] by using specific heuristics such as spelling. These studies, however, describe techniques that are not language transparent. Another well-known approach, pivot-based induction, uses a widespread language as a bridge be-tween less resourced language pairs. Its naive implementation proceeds as follows. For each word in language A , take its translations to the pivot language using bilingual dictionary A -B , and then for each such pivot translation, take its translations to the language C using B -C . This implementation yields an extremely noisy bilingual dictio-nary containing incorrect translation pairs as lexicons are generally intransitive. This intransitivity stems from polysemic and ambiguous words in the pivot language. Take Uyghur X  X nglish X  X azakh as an example. The English word tear is the translation of Uyghur word yash , but only in the sense of liquid from the eyes. Further translating tear into Kazakh yields both the correct translation jash and an incorrect one, jirtiw (to rip).
To cope with the issue of divergence, previous studies attempted to select correct translation pairs by using semantic distances from the structures of the input dic-tionaries [Tanaka and Umemura 1994] or by using additional resources such as part of speech [Bond and Ogura 2008], WordNet [Istv  X  an and Shoichi 2009], comparable corpora [Kaji et al. 2008; Shezaf and Rappoport 2010], and descriptions present in dic-tionary entities [Sjobergh 2005]. Although the technique of adding resources to pivot-based induction is promising for improving performance [Shezaf and Rappoport 2010], a basic method that uses the structures of the input dictionaries must be developed because (1) it is essential for inadequately resourced languages, (2) it is compatible with other approaches and so can be combined [Mairidan et al. 2013; Saralegi et al. 2012], and (3) there is a potential for improving quality by considering the missing meanings [Saralegi et al. 2011].

There has been growing interest in using constraint optimization problem for-malism for ideally describing and solving many problems in NLP and Web Service Composition [Matsuno and Ishida 2011; Ravi and Knight 2008; Hassine et al. 2006], because these problems are (or could be) reformed as combinatorial problems that can be represented by a set of variables connected upon constraints. For instance, the word sense ambiguity in machine translation has been resolved efficiently by a proposal of a consistent word selection method based on constraint optimization [Matsuno and Ishida 2011], in which the authors considered constraints between words in the document based on their semantic relation and contextual distance. Moreover, Ravi and Knight [2008] presented an application of optimization by solving substitution ciphers using low-order letter n-gram models, where the authors enforced global constraints using integer programming [Wolsey 1998] and guaranteed that no decipherment key is overlooked.

In this article, we propose a constraint-based method for pivot-based dictionary induction to promote the quality of output dictionary A -C , where A and C are closely related languages (intrafamily), while pivot language B is distant. 2 More precisely, we try to obtain semantic distance by constraining the types of connection in the structures of the input dictionaries based on a one-to-one assumption of intrafamily language lexicons. Furthermore, instances of pivot-based dictionary induction are represented by graphs, to which weighted edges are added to represent missing meanings. In this context, the Weighted Partial Max-SAT framework (WPMax-SAT), an optimization extension of Boolean satisfiability, is used to encode the graphs that will generate the optimally correctly output dictionary. Meanwhile, we discuss an alternative formalization within the 0-1 Integer Linear Programming framework (0-1 ILP) and its computation performance over WPMax-SAT as a comparison study. The reasons for using the WPMax-SAT framework as a primary formalization are that (1) the hidden facts such as whether a word pair is a correct translation or whether a meaning of pivot word is missing for a dictionary have binary states when they are unknown to the machine; (2) automatic detection of correct translation pairs and missing meanings whose states are bounded by certain weights can be seen as an optimization problem, which is to find the most reliably correct translation pairs, while adding the most probable missing meaning(s); and (3) the constraints inferred from language similarity can easily be transformed into propositional expressions. In other words, a new bilingual dictionary is created in the following steps:
First, we make an assumption: lexicons of intrafamily languages are in one-to-one relation , which allows any word in language A to have a unique translation equivalent in language C , or vice versa. Such a word pair is called a one-to-one translation pair (or one-to-one pair ) in this article.

Second, to incorporate the input bilingual dictionaries, we use graphs, in which vertex is a word, and an edge is the indication of shared meaning. Following Mausam et al. [2009], we call these transgraph , which is defined to be the maximum scope for discovering the one-to-one pairs. Moreover, we treat transgraphs as being incomplete because some translations might not have been covered (missing) in input bilingual dictionaries when they were created. Hence, we allow for the automatic addition of edges to transgraphs, but only with certain costs, the chance that a particular edge is NOT missing (endpoint words are not a translation pair). This value is obtained by analyzing the structures of the transgraphs.
 Third, the one-to-one assumption is used to constrain the word pair candidates of A and C languages in the transgraphs, and the candidates are recognized as one-to-one pairs only if they satisfy these constraints. Each transgraph is encoded as an optimization problem formulated within the Weighted Partial Max-SAT framework. Finally, an iterative algorithm is created to extract one-to-one pairs by evaluating CNF formulas, in which, at each iteration, the CNF formula corresponding to a trans-graph is evaluated to extract only a single one-to-one pair from the optimal assignment. This CNF formula is modified for the next iteration with reference to the awareness of the availability of one-to-one pair(s) accumulated from the previous iteration.
We designed a tool to implement the proposal using an open-source SAT library 3 as the default solver. With this tool, we evaluated our approach by inducing a Uyghur X  Kazakh bilingual dictionary from Chinese X  X yghur and Chinese X  X azakh dictionaries; Uyghur and Kazakh are members of the Turkic language family, while Chinese is a Sino-Tibetan language. The evaluation result revealed the efficiency of our proposal in case of a related language pair; details can be found in Section 7.
The rest of the article is organized as follows: In Section 2, we discuss existing works on dictionary creation. Section 3 briefly introduces dictionary induction and the pivot-based technique. Sections 4 and 5 describe the proposed method with Max-SAT formalization, while in Section 6, an alternative formulization is discussed. An experiment and analyses are covered in Section 7. Finally, we end with the conclusion. A very early attempt to create bilingual dictionaries from existing dictionaries was by Tanaka and Iwasaki [1996], who used a pivot language. They used Inverse Consultation (IC) to tackle lexical intransitivity divergence. IC tries to measure the intersection of two pivot word sets: the set of pivot translations of a word w in language A ,andthe set of pivot translations of each word in language C , a candidate for a translation of w . The number of elements in the intersection indicates the nearness of the original word and its candidate. IC generally requires that the intersection contain at least two synonymous words. For example, the intersection of the English translations of French printemps and Spanish resorte contains only a single word, spring . The intersection for the correct translation pair printemps and primavera will include two synonymous words, spring and springtime . If only one pivot word is present in the intersection, the equivalent candidate is considered to be inadequate and could be discarded if the consultation limit is equal to one. If it is two, further consultation could be conducted: Spanish translation set of the French resorte is generated through English pivot words, and how many times the Spanish resorte is repeated in this set is determined. The French printemps will be discarded, or subjected to further ranking if it is repeated more than once. In the IC approach, inverse consultation can be conducted n times. A weakness of the IC method is that it relies on synonymous words to identify correct translations. In the previous example, if the relatively rare springtime did not exist or was missing from the input bilingual dictionaries, IC would not have been able to detect that primavera is a correct translation, which may result in low recall.
With the assumption that using more than one pivot language could provide more information to evaluate semantic distance of the cross-lingual pairs in the output dictionary, one method uses multiple input bilingual dictionaries [Mausam et al. 2009]. They represent the input bilingual dictionaries as an undirected graph, where vertices represent the words from all the input dictionaries, and edges represent translation pairs. The new translation pairs are induced based on cycles in the undirected graph, where a cycle indicates that there are multiple paths between a pair of words in different languages. In the previous example, if both English and German are used as pivots, printemps and primavera would be accepted as a correct translation pair because they are linked by both spring in English and Fruehling in German, while printemps and resorte are not linked by any German pivot. This multiple-pivot idea is similar to the IC method, but its use of multiple pivot languages eliminates IC X  X  dependency on synonym-rich input bilingual dictionaries to some extent. However, the new problem is the need to find suitable multiple input dictionaries.

One work [Skoumalova 2001] presented a method for inducing a bilingual dictio-nary of cross-family languages based on transduction models of cognate pairs via bridge languages, where the bilingual dictionaries within language families are in-duced using probabilistic string edit distance models. Bilingual dictionaries for arbi-trary distant language pairs are then generated by a combination of these intrafamily translation models and one or more cross-family online dictionaries. They reported rel-atively high precision when generating English X  X ortuguese, English X  X orwegian, and English X  X krainian bilingual dictionaries.
 Another line of research is extracting a bilingual dictionary from a large amount of parallel corpus or comparable corpora. This has been generally done in two ways: (1) Using cross-lingual co-occurrences obtained from parallel corpus or monolingual corpora to improve the bilingual dictionaries generated by using pivot language [Kaji and Aizono 1996; Bond et al. 2001; Ahn and Frampton 2006; Saralegi et al. 2011; Tanaka and Iwasaki 1996]. For example, Schafer and Yarowsky [2002] created bilin-gual dictionaries between English and a target local language (e.g., Gujarati) using a related language (e.g., Hindi) as the pivot. In their case, English pivot words were used in conjunction with the cognates of two closely related languages. Cross-lingual co-occurrences were used to remove errors, together with other cues such as edit distance and IDF scores. (2) Extraction from parallel corpus or monolingual corpora without using the pivot language [Nerima and Wehrli 2008; Sahlgren and Karlgren 2005].
Using parallel corpus to obtain word-oriented machine-readable bilingual dictionar-ies is a mature and very promising approach if adequate amounts of data are available. The general implementation aligns the texts to each other at the chunk or word level. The alignment is usually evaluated by consistency (source words should be translated to a small number of target words relative to the entire corpus) and minimum shift-ing (the source must be aligned to a nearby translation). However, parallel corpus is expensive and generally not available for every domain.

Relying on readily available monolingual corpora is an alternative with growing research interest. In this context, most researches were inspired by Fung [1998] and Rapp [1999]. Their main assumption is that the term and its translation share similar contexts. These methods consist of two steps: modeling of contexts and measuring the similarity between the contexts of two languages using a seed dictionary. The majority of approaches follow the bag-of-words paradigm and represent contexts as weighted collections of words using LL [Ismail and Manandhar 2010], TF-IDF [Fung 1998], or PMI [Shezaf and Rappoport 2010]. Furthermore, Haghighi et al. [2008] characterized word types in each language by multiple monolingual features, such as context counts and orthographic substrings. The translations are induced using a generative model based on canonical correlation analysis. Such settings have been considered in other works, most notably in Koehn and Knight [2002] and Fung [1995], but Haghighi et al. [2008] was the first to use a probabilistic model and present results across a variety of language pairs and data conditions. In their experiment to create an English X  X panish dictionary, relatively high precision (89.0%) but a very low recall (33%) were reported. However, this approach was revealed to have low efficiency with distant language pairs (such as English X  X hinese) due to its heavy dependence on orthographic features of languages. One work [Bergsma and Van Durme 2011], considering the fact that a large number of annotated images are added to sites like Facebook and Flickr every month, efficiently eases the dependence on orthographic similarities while improving the performance of bilingual dictionary extraction by using labeled web images: cross-lingual pairs of words are proposed as translations if their corresponding images have similar visual features. They concluded that visual similarity provides substantial gains over orthographic similarity alone.

Recently, there has been growing interest in decipherment work [Dou and Knight 2012; Ravi and Knight 2008; Nuhn et al. 2012], in which researchers treat training and statistical-based translation without parallel data as a deciphering problem. It is worth mentioning these works because they produce bilingual dictionaries (more precisely, a probabilistic translation table) as a byproduct by training statistical machine trans-lation with monolingual corpora. Dou and Knight [2012] used decipherment to learn a domain-specific translation table and use it to improve out-of-domain translations, because although it is hard to find parallel corpora for any specific domain, it is rela-tively easy to find domain-specific monolingual corpora. Their probabilistic decipher-ment model treats Spanish words as a simple word substitution for its French counter-parts and learns a Spanish X  X rench word-to-word lexicon via decipherment on domain-specific monolingual corpora, although their decipherment model does not assume deterministic relation and is able to discover multiple translations. Meanwhile, a sim-ilar work [Nuhn et al. 2012] took a big step toward large-scale and large-vocabulary unsupervised training of statistical translation models with large-scale monolingual corpora, breaking the constraints in vocabulary or data size that previous approaches faced.

Most methods for bilingual dictionary creation are promising when extra language resources are available for the given language pair to acquire word sense or to evaluate semantic distance between cross-lingual word pairs. The sole work to try to create bilingual dictionaries purely from two input dictionaries [Tanaka and Umemura 1994] utilizes pivot synonymous words as the only information supporting semantic distance (hence, it is often seen as a baseline method in evaluations). In our work, we also focus on creating a bilingual dictionary from just two input dictionaries, since there are still many world languages that lack useful languages resources. Our approach, modeling complete structures of input dictionaries as a constraint optimization problem to handle the incompleteness of input dictionaries to some extent, performs well when the target languages are closely related. Moreover, as language resources are gradually being accumulated for inadequately resourced language pairs, methods to combine many language resources to produce a bilingual dictionary are becoming promising [Mairidan et al. 2013]. In this sense, our approach can also be adopted as a useful heuristic for extracting semantic information from the bilingual dictionary resources available. Let D l in one language to one or more words in the other language. The creation of a bilingual dictionary can be done manually or automatically. If the latter, it is the process of determining whether a word in one language has the same meaning as a word in the another language.

Using a pivot language is well known in research on machine translation [Tanaka et al. 2009] and service computing since a large number of language resources are being accumulated as web services, and the recent service computing technologies allow us to utilize existing services to create new composite services [Ishida 2011]. However, in this context, the pivot-based induction is used to induce new dictionary D A -C from they have the same translation in B . Such a D A -C may include both correct and incorrect w relation of translation pairs in two languages (see Figure 1(a) and Figure 1(d)). Assume that we are seeking translations of words in language A to those in language C using D w
Note that this deduction is not correct because it does not take into account the word in input bilingual dictionaries [Saralegi et al. 2011]. Taking Uyghur X  X nglish X  X azakh as an example, the English word tear is the translation of the Uyghur word yash ,but only in the sense of liquid from the eyes. Further translating tear into Kazakh yields both the correct translation jash and an incorrect one, jirtiw (to rip). Identifying such an incorrect translation is challenging (see Figure 1), because, unfortunately, most dictionaries lack comparable information about senses in their entries. So it is not possible to map entries and translation equivalents according to their corresponding senses. As an alternative, most previous studies try to guide this mapping according to semantic distances extracted from the dictionaries themselves or external resources. One can create a dictionary of two languages just by propagating their lexicons. This dictionary would have the highest recall and lowest precision. It is important to note that the basic pivot approach is often the first and easiest step to increasing the precision of such a dictionary. In many cases, the precision obtained from the first step is so low that the resulting dictionary is impractical. Merging two input bilingual dictionaries D A -B and D B -C via language B forms a big graph whose vertices are words and whose edges are the indication of common meaning be-tween endpoint words. Such a graph has at least one connected component [Hopcroft and Tarjan 1973] X  X  subgraph in which any two vertices are connected to each other, and which is connected to no additional vertices in the supergraph X  X uch as the one shown in Figure 1(a). We, following Mausam et al. [2009], call each connected compo-nent a transgraph .

Definition 1 . A transgraph is defined as an undirected graph G ={ V , E } ,inwhich e ( transgraph ) in language A , B , and C , respectively. For further use, V l 2 w l 1 meanings of w l 1 i in language l 2 .
 It should be noted that although D A -B and D B -C are usually directional, for example, D -B was made with the intention of translating words in language A to language B , ignoring directionality is possible, because not only is this in accordance with the reversibility principle found in lexicographic literature [Tomaszczyk 1986] but also the initial noisy dictionary, D A -C , would provide the most complete candidate set possible. However, one is allowed to merge D l the coverage of the output dictionary. However, doing so apparently creates additional ambiguities. The small transgraph in Figure 1(a) is used as an example in this article. The key characteristic of intrafamily languages is that their lexicons are similar and share a significant number of cognates X  X ords that are derived from the same origin and are similar in both spelling and meaning (e.g., neveu [Fr.] and nephew [Eng.]). Most cognate pairs have direct translations, 4 meaning that they correspond one to one. A classical lexicostatistical study of 15 Turkic languages, 5 mostly used in central Asia, indicated that cognate pairs shared among members of the Turkic language family scales from 44% to 94% of their lexicons, and the majority of noncognates tend to be nouns. Although there are some studies on constructing a bilingual dictionary for intrafamily languages by recognizing these cognate pairs, they often turned out to be language-pair dependent since subsequent separate phonetic development of languages has made many cognates not identical in spelling. Our work, however, aims at generating a relatively complete bilingual dictionary of intrafamily language pairs.
Taking account of such facts, we make the following assumption: lexicons of intrafam-ily languages offer one-to-one relation . That is, if A and C are intrafamily, for any w A i there exists a unique w C j , such that they have exactly the same meaning. Such a pair logically, the state of one-to-one relation. We sometimes use the term one-to-one pair candidate to refer to a pair of words whose state of one-to-one relation has yet to be determined.

Although such an assumption may be too strong for the general case, we consider it reasonable for the case of intrafamily languages, although the evolution of languages has different situations and is reached in different states. This may result in a less accurate dictionary in applying the proposed algorithm in other language pairs that are closely related. However, utilizing a one-to-one assumption can also be found in existing studies. For example, Melamed [1997] made a similar assumption for any language pair in trying to create a word-to-word model of translation. They presented a fast method for inducing accurate translation lexicons from a parallel corpus. This method assumes that words are translated one to one. They claim that such an assumption reduces the explanatory power of their model in comparison to the IBM models and also helps them to avoid indirect associations, a major source of errors in translation models. Koehn and Knight [2002] also made a similar assumption when they extracted an English X  X erman dictionary from monolingual corpora. Another relevant line of using the one-to-one criterion is extracting cognates of closely related languages and using them to adapt resources (such as parallel corpus) from one language to another. For example, Hana et al. adapted Spanish resources to Brazilian Portuguese to train a part-of-speech tagger [Hana et al. 2006]. Moreover, creating and using one-to-one translation equivalents are often tackled in translation between dialects of the same language, for example, between Cantonese and Mandarin [Zhang 1998], or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic [Bakr et al. 2008; Sawaf 2010; Salloum and Habash 2011].

In conclusion, our goal is not to create bilingual dictionaries with word-to-word re-lation but accurate bilingual dictionaries of closely related language pairs (such as Turkic languages), so that the result might be of use in translation systems like Aper-tium (an open-source machine translation platform at http://www.apertium.org/), which uses bilingual dictionaries and manual rules to translate between related languages, including Spanish X  X atalan, Spanish X  X alician, Occitan X  X atalan, and Macedonian X  Bulgarian. The one-to-one assumption in our proposal is a tool whose aim is higher precision while preventing significant drop in recall. However, it is important to handle exceptions by proper postprocessing, which we try to explore in later works.
The one-to-one assumption is realized as constraints that are imposed on pair ( w A i , w ) in a transgraph; all constraints must be satisfied before the pair is recognized as a one-to-one pair. The next section details the constraints. In a transgraph , the one-to-one assumption is realized with two constraints, one of which demands symmetric connection entitles while the other guarantees their unique-ness. Actually, selecting candidates of one-to-one pairs also can be seen as a constraint, which is independently defined in this article. 4.2.1. One-to-One Pair Candidate. Theoretically, any word w A i can be a one-to-one equiv-alent to any w C j in a transgraph (or even in the lexicons) when it is unknown to the machine. As the initial step of pivot-based techniques, the possible translation pairs are selected to generate a noisy D A -C based on the structures of the input dictionaries. In our work, we also take such a step, so that whether word pair ( w A i ,w C j ) can be a one-to-one pair candidate is decided by the following constraint.

Constraint 1 ( Candidate Existence ) : A pair of words, w A i and w C j , in a transgraph can be a one-to-one pair candidate if they are connected via at least one pivot word .
That is, a word pair is taken to be a candidate and subjected to further evaluation only if they share at least one word in the pivot language. For instance, in Figure 1(a), This constraint may raise doubts on the potential one-to-one pair candidates that are hidden because of data incompleteness (missing pivot words or meanings). However, and w C j in language B , respectively. This constraint can be expressed mathematically have no member in common, then w A i and w C j should never be taken to be a one-to-one pair candidate: 4.2.2. Symmetry. A one-to-one pair is a pair of words that carry exactly the same meanings. This allows us to define the following constraint on one-to-one pairs.
Constraint 2 ( Symmetry ) : Given a pair of words, w A i and w C j , in a transgraph, if they are a one-to-one pair, then they should be symmetrically connected through pivot word(s) .

In other words, a one-to-one pair must share the same words in pivot language; the number of edges between w A i and pivot words should equal the number of edges between w j and pivot words. Note that a path through a pivot word might maintain at least one common word sense along the edges. This constraint is written in the following propositional expression: the transgraph, where e ( w C 1 ,w B 3 ) is, indeed, not present. We consider that such an edge may be missing, which means that the corresponding translation might not have been included in the input dictionary when it was built. 4.2.3. Uniqueness. Another consequence of one-to-one assumption is that the transla-tion pairs of intrafamily languages should be unique, meaning any w A i can have only a single one-to-one equivalent in language C , and vice versa. It is possible that syn-onymous words in a language can share exactly the same meaning(s) and can be used as alternates in the translation. Such synonymous words apparently can be one-to-one translation equivalent to the same word, but since we are looking for a single equiv-alent under the one-to-one assumption, we need to prevent the selection of multiple equivalents. This needs a constrain, which can be stated as follows.

Constraint 3 ( Uniqueness ) : Given a pair of words, w A i and w C j , in a transgraph, if they are a one-to-one pair, then they should be unique, such that all other candidates involving w A i or w C j are not one-to-one pairs .

For example, in Figure 1(a), if ( w A 1 ,w C 1 ) is a one-to-one pair, then we assert that ( w the following propositional expression: where the first AND operation iterates over the words w C h (excluding the given w C j )that are one-to-one candidates of w A i , and, likewise, the last AND operation iterates over the words w A p (excluding the given w A i ) that are one-to-one candidates of w C j . The completeness of input dictionaries is seldom guaranteed: (1) a pivot word is missing so that some translation pair for D A -C is not identified, (2) a nonpivot word is missing ( w
Apparently, the first two problems cannot be resolved without additional resources, so they are not considered further in this article. The third one, however, is vital because any missing edge may break a symmetric connection between w A i and w C j ,so that O ( w A i ,w C j ) could not be detected as a one-to-one pair. This could harm the quality of induction. Moreover, missing edges are hard to avoid since the input dictionaries are usually independently created, and their completeness is seldom guaranteed. However, adding missing edges into the transgraph makes it complete and, thus, makes induction more accurate. This is why D l for either direction [Tanaka and Umemura 1994]. We assign a weight to missing edge e , indicating the chance of it being incorrectly missed.

Although many methods are proposed for this calculation, we employ a simple statis-tical method [Nakov and Ng 2012] for the sake of simplicity; see Formula 5. 6 However, one can extend our method by adopting a different formula or even using external knowledge to gain more accurate weights. Notice that the weight of an existing edge (which exists when the transgraph is formed) is predefined as 1, which means that the its weight equals the chance of a word pair ( w A i ,w C j )  X  X  ( w A i ,w C i pair whose value is the maximum among all pairs, { ( w A i ,w C i e ( be identified as a one-to-one pair). The same calculation is applicable for a possible missing edge e ( w C j ,w B h ): where the likelihood of a pair ( w A i ,w C j ) to be a one-to-one pair is calculated by the following equation: w here
P represents a maximum weight value of w A i  X  V A or w C j  X  V C having its one-to-one equivalent in transgraph; h is the index of pivot words shared by w A i and w C j .Figure2 shows the edges that are considered to be missing in the sample transgraph (given in Figure 1(a)), and an example of their weights is calculated by Formula 5. A transgraph is the maximum scope of extracting one-to-one pairs. In a transgraph, however, as mentioned in the previous section, a missing edge can make a one-to-one pair undetectable. For instance, in Figure 1(a), assume that w A 1 and w C 1 are a one-to-one pair, but this fails to be detected because any automatic attempt to classify it as a one-to-one pair needs to ignore the absence of the edge e ( w C 1 ,w B 3 ) in that transgraph, which is actually a violation of Constraint 2.

Therefore, an edge is allowed to be added to the transgraph if it has nonzero proba-bility, p , of having been missed. If it is added, then a certain cost, 1  X  p ,istobepaid.We define the process of extracting one-to-one pairs from a transgraph as an optimization problem; the objective is to extract as many one-to-one pairs as possible while mini-mizing the cost of edge addition, where the cost is the penalty for adding an edge that should not exist.

We used a Boolean optimization framework, WPMax-SAT, to formulate the induction to generate the optimally correct one-to-one pair set, since the facts that whether a pair has one-to-one relation and whether an edge is actually missing, and constraints can be easily represented by Boolean variables and expressions.
 In the next section, we will describe how we formalize this problem within the WPMax-SAT framework and then evaluate CNF (Conjunctive Normal Form) formulas to generate one-to-one pairs. This section describes our proposal of formalizing the problem within the WPMax-SAT framework and process of extracting one-to-one pairs from a transgraph in detail. Boolean Satisfiability (SAT) is the problem of finding, if it exists, an assignment to the set of Boolean variables V that satisfies the Boolean formula expressed in CNF (Conjunctive Normal Form) [Biere et al. 2009]. A literal is a Boolean variable  X  or its clause consists of ORed literals. A CNF  X  is the conjunction (logical AND) of m clauses c ,..., c such that all c i  X   X  evaluate to 1.
 There are several extensions to the SAT problem. One such extension of interest is Weighted partial Max-SAT (WPMax-SAT) [Fu and Malik 2006], which aims to satisfy a partial set of clauses. In a WPMax-SAT problem, clauses are assigned weights (natural number in most cases, though real numbers are also widely used) and are separated into hard and soft types. Hard clauses have maximum weights (represented by infinity  X  ) and all must be satisfied, while soft clauses need to be satisfied such that the sum of the weights of the satisfied soft clauses is maximized or the sum of the weights of the unsatisfied (falsified) is minimized.

Formally, a WPMax-SAT is a multiset of weighted clauses  X  ={ ( c 1 , X  1 ) ,..., ( c m , X  m ) , ( c are hard. WCNF formula (weighted extension of CNF)  X  is the problem of finding an assignment to the set of Boolean variables V that minimizes the cost of the assignment on  X  . If the cost is infinity, it means that we must falsify a hard clause and say that the multiset is unsatisfiable.
 As a first step of casting the problem in WPMax-SAT form, we apparently need a variable to denote whether a given word pair is a one-to-one pair. Moreover, another variable is also needed to represent whether an edge is missing, since the identification of a one-to-one pair requires the existence of particular edges. Overall, we use x and y to denote one-to-one pair candidates and edges in the transgraph, respectively: otherwise. (It is easily estimated that the number of x variables of a transgraph never exceeds | V A | X | V C | ). X denotes a set of x variables in a given problem instance. respectively. Figure 3 illustrates how variables are created for a transgraph. Before evaluating a WPMax-SAT problem by using a solver, it must be encoded to CNF. There are several ways of encoding most problems [Biere et al. 2009], yet the choice of encoding can be as important as the choice of search algorithm. However, for our problem, we use a resolution approach based on simple Boolean algebra rules such encode the constraints in our problem within the WPMax-SAT framework.

We use hard clauses to encode all the constraints that must be satisfied, and an apparent constraint: an existing edge cannot be deleted (this constraint is added because preexisting edges need to be protected from being deleted, since they are assumed to be created by humans). Meanwhile, the missing edges are encoded with soft clauses since adding an edge is not mandatory. In the following clause formulations,  X   X  indicates hard, while  X  + indicates soft. CNF formula  X  =  X  +  X   X   X  1  X   X   X  2  X   X   X  3 can be evaluated by a Max-SAT solver to output an optimal variable assignment (solution). However, any satisfiable assignment on  X  ends up with minimum cost, equally zero, because no hard clause in  X  requires x variables to evaluate to TRUE (doing so may need edge addition that eventually increases the cost of assignment). However, we resolve this by adding a new hard clause whose constraint is that at least one x variable must evaluate to TRUE . This clause is simply the disjunction of all the x variables.

Constraint 4 : In a transgraph, at least one one-to-one pair should be extracted .Itis encoded as follows: an optimal assignment with minimum cost, 7 which equals 0 when no edge is added or exceeds 0 when the most probable missing edge(s) is added.

An optimal assignment can have a single variable x m , k  X  X evaluated to TRUE , while considered to be the most reliably correct one-to-one pair. We add it into output dic-encoded by a new hard clause ( x m , k ,  X  ). The regenerated  X  is again evaluated by the solver to identify one more one-to-one pair. The same process is iterated until  X  becomes unsatisfiable, at which point the extraction of the output dictionary completes (as in Algorithm 1).
 ALGORITHM 1: Extracting one-to-one pairs from a transgraph
We describe how two one-to-one pairs are extracted from the example transgraph in Figure 1(a) after three iterations (as illustrated in Figure 4). Before solving the x (1)  X  is evaluated: an optimal solution is found, where x 1 , 1 is assigned to TRUE ,since (2)  X  is evaluated: an optimal solution is found, where the variable x 2 , 3 is assigned (3)  X  is evaluated: no solution is found (problem is unsatisfiable) because, in this case, Cardinality constraints X  X xpressing numerical bounds on discrete quantities X  X rise frequently out of the encoding of real-world problems. Due to the progress made over the last years in solving propositional satisfiability instances, interest has increased in tackling problems that include cardinality constraints using SAT solvers. This, how-ever, requires the encoding of cardinality constraints in the language of purely propo-sitional logic or, more specifically, in CNF. However, Boolean cardinality constraints put numerical restrictions on the number of propositional variables that are allowed to be TRUE at the same time. Expressing such constraints by pure CNF leads to more complex SAT instances [Aloul et al. 2002]. Its typical expression is that not more than k outofthe n Boolean variables x 1 ,..., x n are allowed to be TRUE , and the common way of converting such a constraint using purely propositional logic is to explicitly exclude all possible combinations of k + 1 variables being simultaneously TRUE ,which to O (2 n / n / 2) clauses [Sinz 2005].

For instance, in the example transgraph of this article, there are three one-to-one pair them is allowed to be TRUE due to the Uniqueness constraint. The propositional logic  X  (  X 
Our approach to bilingual dictionary induction involves a large amount of cardinal-ity constraints, and unfortunately, the weakness of SAT in handling them negatively affects the computation performance of the proposal (details are given in Experiment section). Therefore, we consider that it is reasonable to discuss some alternative for-malizations with the goal of improving performance.
 Integer Linear Programming 8 (ILP) handles such constraints efficiently (but generic ILP solvers may ignore the Boolean nature of 0-1 variables) [Aloul et al. 2002]. For example, in the previous case, the cardinality can be tackled by the single inequality x candidate. In this section, we briefly compare Max-SAT and ILP formalizations. A full comparison of computation performance is given in Section 7.4. An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings, the term refers to ILP, in which the objective function and the constraints (other than the integer constraints) are linear. Integer programming is NP-hard, while its special case, 0-1 Integer Linear Programming, in which unknowns are binary, is an NP-complete problem [Wolsey 1998]. Moreover, 0-1 techniques tend to outperform generic ILP on Boolean optimization problems [Aloul et al. 2002].

SAT (as well as its Max-SAT extension) problem can be easily transformed to its ILP equivalent [Li et al. 2004]. This provides an alternative tool for solving SAT by using ILP. On the other hand, given an ILP problem, we can also transform it to an SAT problem in polynomial time by the NP-complete theory [Cook 1971].

We formalize each constraint imposed by one-to-one assumption in both Max-SAT and 0-1 ILP as a comparison study. ized as follows: where Y is a set of variables corresponding to the symmetric edges (both existing and missing) that connect w A i and w C j to V B i , j . With these formulas, WPMax-SAT pro-duces | V B i , j | number of clauses when encoded in CNF. By contrast, ILP handles the constraint with just a single inequality. For example, given the pair ( w A 1 ,w C 1 )and variables; if the solver assigns 0 to x 1 , 1 , then there will be no restriction on the values of y variables to make this inequality valid. 6.1.3. Encoding Uniqueness Constraint. Given a set of pairs where all items include a com-mon word w A i or w C j ,let X denote the Boolean variable set corresponding to these pairs, while X denotes a set of Boolean variables representing one-to-one pair candidates generated from G . The Uniqueness constraint is written as follows: 6.1.4. Complete Formalization. Given transgraph G ,let X denote a set of Boolean vari-ables representing one-to-one pair candidates generated from G ; the problem of extract-ing one-to-one pairs from G is formalized, within the 0-1 ILP framework, as follows:
Notice that (1) in the case of ILP formalization, the weight of missing edges is used in an objective function, while in Max-SAT, it is assigned to soft clauses; and (2) it is pos-sible to balance precision against recall of the output dictionary to some extent by adjusting coefficients  X  1 and  X  2 in the expression of the objective function. How-ever, in our experiment, in order to prevent Max-SAT and ILP formalizations from yielding different optimal solutions, we set  X  1 to a certain number so that it can be guaranteed for any x i , j that the value of  X  1  X  x i , j should be greater than  X  be detected because assigning their corresponding variables TRUE may decrease the objective function X  X  value due to excessive edge addition costs. We designed a tool (see a screenshot in Figure 5) to implement the proposal using Sat4j 9 as the default solver due to its flexibility in integration with third-party software. With this tool, we evaluated our approach by inducing D ug -kk from D zh -ug and D zh -kk (see Table II for details), where ug (Uyghur) and kk (Kazakh) are Turkic languages, while zh (Chinese) belongs to the Sino-Tibetan language family. The highlights of the tool X  X  main features are as follows:  X  X rovides many options for preprocessing the input dictionaries  X  X isplays transgraphs using dynamic graph components (see Figure 5) so that users can easily observe the induction process and permits interaction with transgraphs to manually modify their structure (e.g., annotating known one-to-one pairs or adding missing edges)  X  X roduces comprehensive statistics of the structures of input dictionaries, trans-graphs, CNF encoding, solutions, and some other details such as computation per-formance  X  X upports two different problem-solving frameworks: Max-SAT and ILP (is expected to support more)  X  X ilingual human experts can use it to evaluate automatically selected sample pairs easily Table II shows structural information yielded by preprocessed D zh -ug and D zh -kk .Con-necting them resulted in 12,393 transgraphs. Among them, we selected only 1,184, each of which involves at least two pivot words (see Table III). In theory, our approach does not make sense to others (all the possible assignments always have equal cost, so a random selection is valid). However, these 1,184 transgraphs involve 52,218 ug and 73,093 kk words, which make up 73.5% and 71.4% of total ug an kk words in the input dictionaries, respectively. This means that our approach affects the majority part of input data. For others, approaches that consider the orthographic similarity may work [Haghighi et al. 2008], which is left as a future work.

One of them, #1,184, is remarkably large; it contains 69% of all vertices and 82% of all edges, while the rest are distributed to 1,183 smaller transgraphs (see Figure 6 for details). We examined why there was such a large difference in the percentage of input dictionaries. Figure 7 shows the distribution of a number of meanings of 35,235 pivot words contained in the transgraph #1,184, from which we can draw a rough conclusion that the large number of polysemic words in the pivot language explains the difference. However, different language pairs will need to be examined to see whether this imbalance in transgraphs is common.

Encoding this large transgraph resulted in a CNF formula with 16,879,348 variables and 46,059,686 clauses. We were unable to evaluate it using the Sat4j solver in our experimental hardware environment 10 due to its high computation complexity. Hence, for experimental purposes, we partitioned transgraph #1,184 into 150 smaller subgraphs (see Figure 6 for their distribution) using a graph partitioning algo-rithm [Dhillon et al. 2005]. Since the goal of graph partitioning is to minimize the number of edges that cross from one subgroup of vertices to another, we consider adopting such an algorithm reasonable. However, as one can implement our proposal with more efficient SAT solvers and rerun the experiment using stronger hardware, the step of partitioning may not be needed. Also, since graph partitioning is not a focus in our work, we have not made any comparison study on relevant algorithms with our data. Instead, one that offered easy integration with our tool was preferred.
We independently processed these two groups of transgraphs (1,183 in Group I and 150 in Group II) and evaluated the induction result of each group. The overall val-ues were also calculated by averaging. To measure the recall, we set an upper-bound value that represents the maximum number of possible one-to-one pairs available in a A and | V C | in language C . Moreover, if there are n transgraphs, g 1 ,..., g n , the overall upper-bound value should equal n i = 1 Min ( | V A g dividing this value by the maximum number of possible one-to-one pairs. To evaluate precision, samples were evaluated by bilingual human experts. Figure 8 illustrates the distribution of maximum expected and actual extracted one-to-one pairs from transgraphs in each group; we can observe extraction with relatively high coverage in almost every transgraph. Overall, however, 84.2% of maximum ex-pected one-to-one pairs were extracted as the details show in Table IV.

In order to evaluate the precision of the extracted one-to-one pairs, we randomly selected 3  X  100 samples from the sets of one-to-one pairs extracted from each group, receptively, and asked a ug -kk bilingual human to judge whether they are indeed correctly mapped as one to one. As a result, 237 (79%) out of 300 for Group I and 251 (84%) out of 300 for Group II were determined to be correct. Thus, our method roughly yielded 70.5% overall recall as shown in Table IV. Notice that we did not ask a ug -kk bilingual human to judge whether the sample pairs are unique (in other words, to check whether a word in a one-to-one pair has an alternative translation equivalent that is also in one-to-one relation), because it is possible in practical translations. However, in our work, uniqueness represents a feature of output dictionary that it consists of an equal number of distinct ug and kk words that are in word-to-word relation.

Nonetheless, it is not reasonable to directly compare these numbers with those in related works and reach a conclusion on the efficiency of our approach, since the ex-perimental language pairs and resources chosen in each similar research work are not quite the same. In response, we processed our 1,183+150 transgraphs with the IC method as it is used as a baseline in related works [Shezaf and Rappoport 2010]. It is a well-known approach to creating a new dictionary from just two input dictionaries with no extra information.

IC examines the two pivot word sets: the set of pivot translations of word w A i and the set of pivot translations of each w C j word that is a candidate for being a translation to w A i . The more closely they match, the better the candidate is. Since the IC has no one-to-one constraint on translation pairs, it allows multiple translations for a word through induction setting. However, in our implementation of IC, we only leave a top-ranked translation candidate in language C for each word in language A to make the two methods consistent. This does not harm the performance of IC as long as the top-ranked candidate is selected. As a result, output of the IC method was roughly 10.5% lower than the result of our proposal with similar recall, 72%.

Recall that in our proposal we did not allow one-to-many translations in the out-put dictionary in accordance with the one-to-one assumption, because relaxing the one-to-one constraint resulted in relatively higher recall but the loss in precision was remarkable. We consider such an output dictionary less useful than a more accurate dictionary with lower coverage. However, we acknowledge that being high in recall has the potential to overcome the precision problem if further processing is made, such as using parallel or comparable corpora to eliminate wrong translations [Nerima and Wehrli 2008; Otero and Campos 2010] and considering spelling similarity [Schulz et al. 2004]. Moreover, imposing the one-to-one restriction well controlled the runtime ad-dition of possibly missing edges in the transgraphs, while relaxing it gives the solver to add more edges, which indeed further escalates the reduction in precision. How-ever, we consider that allowing one-to-many translation by disabling the uniqueness constraint is a word try for controlling the balance between recall and precision of an automatically created dictionary. The computation performance might be another concern when implementing our method or using our tool to create one X  X  own bilingual dictionaries. Therefore, in order to evaluate the speed of processing a transgraph and the memory space required to store CNF expressions, we recorded relevant data during the experiment.

The computation environment used was selected to suit ordinary users. Two experi-ments were conducted to compare Max-SAT and ILP. For ILP, the CPLEX 11 is utilized as the solver (it is widely used in the public domain due to its stability and high computation efficiency).

Figure 9 illustrates the distribution of completion time (time spent on evaluating transgraph by solver) among 150 transgraphs in Group II. With Max-SAT formaliza-tion, roughly 6 hours 12 are spent to finish solving all the transgraphs (running with four threads and 100% CPU utilization), while with ILP, only 116 seconds were spent to produce the same output dictionary, a remarkably speedy enhancement. As for the memory space used, which largely depends on the number of clauses in CNF in the case of Max-SAT and linear constraints in the case of ILP, 523MB CNF encodes were produced, while about 400MB were used to store linear constraints. More specifically, 10,354,815 clauses and 3,488,206 linear constraints were generated to encode 150 transgraphs.

The main reason for this big difference in completion time is the iterative mechanism of our SAT-based algorithm: in order to extract n number of one-to-one pairs from a transgraph , the corresponding problem instance needs to be evaluated n times by the SAT solver, while a single evaluation is enough to produce the same result in the case of ILP. Moreover, although the experiments were done in the same hardware environ-ment, there might be many other factors, from underlining algorithms to programming implementation in software, that contribute to this big difference in completion time. For example, the IBM Cplex ILP solver, written in C language, is a commercial software that is designed to select a best-match algorithm based on the problem X  X  features and size dynamically during the solution; it offers strong parallelism and can well utilize whatever number of CPU cores are available. It is hard for us to identify the effects of such factors in our experiment, and there is a need to do a more detailed comparison using different types of solvers to make a precise conclusion. As new open-source SAT solvers are emerging 13 each year with improved performance, it is possible that the performance of the SAT implementation of our proposal could be largely improved.
Notice that theoretic calculation of clauses used to encoding a transgraph is hard to formulate, since its value largely depends on graph structure. However, a maximum possible number of clauses can be obtained by Formula 7.4 for any given transgraph assuming that it constrains | V B | X  ( | V A |+| V C | ) edges.
 where | V A |= a , | V B |= b , and | V C |= c , respectively. Bilingual dictionaries have yet to be created for many languages. Such work is chal-lenging because many language pairs lack useful language resources like a parallel corpus and even comparable corpora. To provide an efficient, robust, and accurate dic-tionary creation method for poorly resourced language pairs, we presented a constraint approach to pivot-based dictionary induction, where a new dictionary of closely related language pairs is induced from two existing dictionaries using a distant language as a pivot. In our approach, the lexical intransitivity divergence is tackled by modeling an instance of induction as an optimization problem, where the new dictionary is pro-duced as the solution of the problem. We also considered data incompleteness to some extent. An experiment showed the feasibility of our approach. However, we note the following points: (1) The problem may also be tackled by maximum weighted bipartite matching [Cheng et al. 1996] as well as other optimization frameworks other than Max-SAT and Integer Linear Programming. This is left as a future work, as we will continue to explore more efficient modeling approaches and algorithms for dictionary induction. (2) There is the potential for including spelling as additional information. (3) More comparisons are expected to find whether the method can indeed rely purely on the structure and still outperform the methods that utilize cheap external resources such as monolingual data. (4) The one-to-one assumption may be too strong for the general case, but we consider it reasonable for the case of intrafamily languages as it greatly reduces the complexity of the problem. (5) Applying the proposal to extrafamily language pairs is also promising and should be explored.

