
Joakim Nivre (V  X  axj  X  o University) Dordrecht: Springer (Text, speech, and language technology series, edited by Nancy
Ide and Jean V  X  eronis, volume 34), 2006, xi+216 pp; hardbound, ISBN 1-4020-4888-2, $119.00,  X  89.95 Reviewed by Christer Samuelsson Columbia University ments de Syntaxe Structurale (Tesni ` ere 1959). For any serious dependency parsing stu-dent or professional, it would have been worth learning Swedish, just to read Joakim
Nivre X  X  Inductive Dependency Parsing if Nivre had not done the world the immense heroes.
 and dependency parsing clearly and succinctly to a wide audience. His overviews of dependency grammars (Section 3.1) and dependency parsing (Section 3.2) should be approach, often in a single sentence, and those wishing to learn more can rely on his extensive collection of references.
 parser X  X he book X  X  main innovation X  X y formalizing the dependency parsing task and specifying a particular type of deterministic shift-reduce parsing algorithm. Here, the stituent parsing, and it is assumed that there is a one-to-one correspondence between dependency analyses and valid, complete sequences of parser actions. Nivre proves that the parsing algorithm correctly performs the formalized dependency parsing task, producing an acyclic, single-headed, projective dependency graph.
 dependency parser, stated in Chapter 2: 1. Robustness: The parser must assign at least one dependency analysis to 2. Disambiguation: The parser must assign at most one dependency analysis 3. Accuracy: The parser must assign the correct dependency analysis to each 4. Efficiency: The parser must process all given sentences in all texts in time Nivre draws the inevitable conclusion from the first two criteria: of an Oracle that always provides the correct parser action. In later chapters, he reverts to more traditional measures of Accuracy, while leaving the other three criteria intact.
The Efficiency criterion is achieved by the deterministic and greedy (non-ambiguity-packing) nature of the shift-reduce parser. One could argue that even at a word level, deterministic, non-packing parsing is not always possible, but Nivre sticks to his guns.
In the first step, the parser instead chooses the locally most advantageous parser action a according to some evaluation function g ( a ):
This preserves the determinism and greediness of the parser, at the cost of potentially ultimately resulting in the reference dependency analysis of the training or testing data. is learned from training data, using some machine-learning scheme. Thus the parser instead chooses the parser action  X  a  X  : which may very well deviate from the locally best parser action a globally best and the gold-standard parser actions. Nivre relates this learning scheme to maximizing likelihood estimates and conditional likelihood estimates, as well as to standard parser action to the dependency analyses of the reference data. strengths of this book. Many followers take the book this far as a point of departure for alternative explorations into inductive dependency parsing.
 on to his choice of machine-learning framework X  X emory-based learning (MBL) (Daelemans and van den Bosch 2005) X  X ltimately leading to the proposed inductive shift-reduce parser: the MALT parser.
 parsing task. This is arguably a waste of time as: the Penn Treebank is a constituency even worse; and the rules for retrieving corresponding dependency analyses are, at best, inspired hacks.
 have replaced MBL with some other machine-learning technique, such as conditional random fields. About half of the entrants of the dependency-parsing shared task of the 268 2006 Conference on Computational Natural Language Learning (CoNLL X) (Marquez book.
 versions of Nivre X  X  inductive shift-reduce parsing scheme, in the vein of Masaru Tomita X  X  generalization of (natural language) LR parsing (Tomita 1987).
 alone is worth it, constituting a good ten percent of the book.
 References
