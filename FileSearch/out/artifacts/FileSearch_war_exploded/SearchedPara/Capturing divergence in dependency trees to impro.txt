 Abstract Obtaining syntactic parses is an important step in many NLP pipelines. However, most of the world X  X  languages do not have a large amount of syntactically annotated data available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when syntactic structures for some sentence pairs in the two languages look quite different. In this paper, we investigate the use of small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. We then use these detected patterns to improve projection algorithms and dependency parsers, allowing for better performing NLP tools for resource-poor languages, particularly those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of syntactic projection and parsing.
 Keywords Multilingualism Translation divergence Syntactic projection 1 Introduction When it comes to resources for natural language processing, a small handful of languages account for the vast majority of available resources. Out of the resources listed by the Language Resource and Evaluation (LRE) Map (Calzolari et al. 2012 ), English accounts for 30 % of all recorded resources, and the ten most resourced languages account for 62 %. A broad variety of tools are available for these resource-rich languages, since the time and effort spent to annotate resources for them allows for state-of-the-art systems to be built utilizing supervised and semi-supervised methods.

The availability of such resources does not come at a low cost; they are the result of a large investment over many years on a per-language-basis. Because creating high-quality annotation is expensive and labor intensive, the vast majority of the world X  X  languages lack such resources and, likewise, high-performance NLP tools. To address this issue, recent studies (Lewis and Xia 2008 ; Benajiba and Zitouni 2010 ; Georgi et al. 2012 ) have investigated using bitexts in which one half of the bitext is a resource-rich language. In this paradigm, existing tools for the resource-rich language can be used to process one side and project the information to the other (the resource-poor language) via word alignments.

While projecting annotation from one language to another is a promising method for adding annotation to languages using automated methods, it relies on the assumption that simple word alignments between languages are sufficient to represent analogous meanings and structures between the languages. For reasons we will discuss in the following sections, this assumption is useful, but often erroneous.
Finding out whether and when this assumption fails for a given language pair is not easy without knowledge about the two languages. It would be useful if, given a small set of seed data, a language pair could be analyzed to find where and in what ways the languages diverge, and use these detected patterns as corrective guidelines for improving projection on other sentences for the language pair.

In this paper, we propose a method for analyzing a language pair and determining the degree and types of divergence between two dependency trees in the two languages. We then use this systematic identification of divergence types to inform and correct the trees produced by syntactic projection. Using these improved trees, we are able to boost the performance of a dependency parser by adding new features extracted from projected trees. 2 Background While there is a growing body of work on projection methods as a means to bootstrap resources for one language from another, there are not many studies on how to handle the issue of linguistic divergence between these languages. In this section, we provide a brief review of work on divergence and projection algorithms. We will also introduce interlinear glossed text (IGT), a common format used by linguists to represent language examples (Lewis 2006 ). 2.1 Projection methods Projection algorithms have been the target of a fair amount of research in the last decade, as attempts have been made to utilize statistical alignment methods to match words between languages with parallel data and  X  X  X roject X  X  annotations between them. Figure 1 shows an example bitext in the form of an IGT, while Fig. 2 shows how this data may be used to project a dependency tree from English to Hindi.

Some of the initial research on the subject of projecting word-level annotation from one language to another was published in Yarowsky and Ngai ( 2001 ). Here, the authors used IBM Model 3 (Brown et al. 1990 ) to align large parallel corpora in English X  X hinese and English X  X rench. A part-of-speech (POS) tagger was trained for French using projections from English, and noun phrase (NP) bracketers were trained similarly for both French and Chinese. The authors identified noisy statistical alignment and 1-to-many alignments as two main issues affecting the performance of projection. The first of these issues is a difficult problem for resource-poor languages, as high-quality statistical word alignment often requires much more bitext than might be available for the language. While it is not a full solution to the problem, many of the language pairs we use in this work are drawn from collection of IGT instances, as shown in Fig. 1 , which provide unique shortcuts for obtaining word alignments with a small amount of data. IGT will be discussed further in Sect. 2.2 .

The second issue, 1-to-many alignments, may be the result of linguistic divergence in a language pair where the language being projected from is morphologically richer than the other. In cases such as this, finding common patterns of conflation can be useful for generalizing a projection to new data. For instance, Fig. 3 shows a very simple but common case of conflation in the SMULTRON corpus (Volk et al. 2010 ), where a single German word aligns to multiple English words. Using this one-to-many alignment, the same POS tag would be projected to both English tokens. In this case, using a universal tagset such as more complex cases, learning the pattern would be more critical.

Hwa et al. ( 2004 ) investigated the issues in using projection between languages in order to develop and train syntactic parsers, and described the Direct Correspondence Assumption (DCA), the assumption made in projection algorithms that the target language tree should be homomorphic with the source language tree. While useful, this assumption often does not hold, as the authors pointed out. In order to fix some of the errors made by faulty projections, Hwa et al. used an approach that applies post-projection correction rules. For projection from English to Spanish, the accuracy of the projected structures increased from 36.8 to 70.3 %. The accuracy of the English to Chinese projection increased from 38.1 to 67.3 %.
While these language-specific rewrite rules are promising, they still require language-specific knowledge. What we seek to accomplish in this paper is a general framework for automatically detecting this divergence, both in specific language pairs and its frequency throughout a large number of languages. With the use of this automated divergence detection, it may be possible to learn these rewrite rules from a small annotated corpus and use them to improve projection algorithms.
 2.2 Interlinear glossed text As mentioned in the preceding section, much of the data for our experiments was drawn from the unique IGT data type. IGT instances are a common way for linguists to give illustrative examples for a language being studied. Figure 1 shows an instance of IGT for Hindi. As with this example, an IGT instance typically has three lines: a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line. The translation line is typically in English, the language of the research paper from which the IGT is extracted. Of special interest in IGT instances is the middle gloss line, which gives a word-by-word gloss of the original language. By design, the alignment between the language and gloss lines is monotonic and one-to-one, thus providing easy matches between these two lines. The matching of words between the gloss and translation can be utilized to obtain high-quality, automatic word alignment between the sentences in the language pair without the need for the much larger amounts of data typically required by statistical alignment algorithms.

In Lewis and Xia ( 2010 ), IGT data for seven language pairs was automatically aligned, projection performed, then finally hand-corrected to create gold standards with minimal manual intervention. They showed the potential for using IGT as a resource for languages for which finding resources would otherwise be extremely difficult or impossible to obtain. We will use this data for the current work. A breakdown of the language pairs can be seen in Sect. 4.1 .

Lewis and Xia ( 2008 ) used projected phrase structures to determine the basic word order for 97 languages using a database of IGT instances. By using the alignment method described above and projecting phrase structures from English to the foreign language line, the word order in the foreign language could be inferred. For languages with just 10 X 39 IGT instances, the accuracy of predicting basic word order was 79 %; with more than 40 instances, the accuracy jumped to 99 %. Figure 4 gives a high-level view of a basic syntactic projection system that uses IGT as the source of projection. Using the IGT as a source, we extract sentences in the foreign language (F) and English (E), as well as word alignment between F and E via the gloss line. Then the E sentence is parsed and the parsed tree is projected to the F side via word alignment and heuristics. Section 3.5 will discuss learning and applying corrections to projected trees, while Sect. 3.6 will describe a system that bootstraps a parser using the projected systems.
 2.3 Linguistic divergence The previously mentioned studies illustrate the promise of projection for bootstrapping new tools for resource-poor languages, but one limitation is their reliance on the assumption that syntactic structures of the two sentences in a given sentence pair are similar. While Hwa et al.  X  X  DCA describes the assumption made for projection, Dorr ( 1994 ) makes a deeper analysis of divergence in languages. Dorr outlined lexical conceptual structures (LCS) that provide a general framework to describe these exceptions to the DCA. This framework is capable of representing divergence stemming from syntactic, lexical, or semantic differences, but for the purposes of this paper we will focus primarily on those that are lexical and syntactic.
Our goal in this work is to create a methodology by which these common types of divergences can be detected automatically from bitexts in order to improve the performance of existing structural projection methods. 3 Methodology In our approach to automatically detecting divergent structures between language pairs, we first propose a metric to measure the degree of matched edges between trees in a language pair (Sect. 3.2 ). Second, we define three operations on trees in order to capture three common types of divergence (Sect. 3.3 ). Third, we apply the operations on a tree pair and show how the operations could affect the degree of tree match (Sect. 3.4 ).
Next, we address how the detected patterns can be used to apply tree modification rules to improve the projection algorithm (Sect. 3.5 ) and help in training a dependency parser (Sect. 3.6 ). Finally, we will explain the relationship of our operations to Dorr X  X  divergence types (Sect. 3.7 ). 3.1 Definitions In the following sections, as we describe the projection algorithm and trees, we will assume that the resource-rich language being projected from is typically English, E ( 1 ), found in the translation line of IGT instances, and the resource-poor language is the foreign language F ( 2 ). Each sentence will be represented by a pair of words and language tree, with W E ( 5 ) and W F ( 6 ) referring to the individual words in the respective sentences. Each tree edge will be represented as a pair of words to be 3.2 Comparing dependency trees Defining a metric for comparing dependency trees cross-linguistically proved to be a crucial component of our method, as most existing tree similarity measures are intended to compare tree representations with the same number of tokens. Comparing across languages, however, means that the number of tokens can vary. We instead look for a method to determine similarity by means of matched edges in the tree, as shown in Fig. 5 .

Given an IGT instance, and the sentences F and E as defined above, A is the word alignment between the sentence pair, where f i and f j are words in W F and e k and e l denoted as e i , not f i . The alignment A is a set of word pairs: We call an  X  F ; E ; A  X  tuple an aligned tree pair. A corpus, C , in our experiments, is a is aligned to e i and f k is aligned to e k . Because the alignment between a sentence pair can be many-to-many, we define the following functions, which map a word from one sentence to the set of words in the other sentence.
 We then define the boolean function match , as follows: That is, an edge  X  f i ; f j  X  in F matches some edge in E according to A if there exists edge in E .

Given an aligned tree pair  X  F ; E ; A  X  , we define SentMatch  X  F ; E ; A  X  as the percentage of edges in F that match some edge in E . Given a corpus C , we define CorpusMatch F ! E  X  C  X  as the percentage of edges in the T F trees that match some edges in the corresponding T E trees. Similarly, CorpusMatch E ! F  X  C  X  is the percentage of edges in the T E trees that match some edges in the corresponding T F trees.
 Algorithm 1 : Calculating the percentage of matched edges in corpus C.
 3.3 Defining Tree Operations When an edge  X  f i ; f k  X  in T F does not match any edge in T E , it may be caused by one of the following cases: C1 f i or f k are spontaneous (they do not align with any words in the other tree). C2 f i and f k are both aligned with the same node e i in the other tree (Fig. 5 b). C3 f i and f k are both aligned with nodes in the other tree, e k and e i , but in a C4 There are some other structural differences not caused by C3.3 X  X 3.3.
 The first three cases are common. To capture them, we define three operations on a tree X  remove , merge , and swap . 3.3.1 O1: Remove The remove operation is used to remove spontaneous words. As shown in Fig. 6 a, removal of the node l is accomplished by removing the link between node l and its parent j , and adding links between the parent and the removed node X  X  children.
This result of this operation can be seen in Fig. 6 a, using the relation Children , which maps a word to the set of all its children in the tree. 3.3.2 O2: Merge The merge operation is used when a node and some or all of its children in one tree align to the same node(s) in the other tree, as can be seen in Fig. 5 b. The parent j and child l are collapsed into a merged node, as indicated by l ? j in Fig. 6 b, and the children of l are promoted to become children of the new node l ? j . The result can be seen in Fig. 6 b. 3.3.3 O3: Swap The swap operation is used when two nodes in one tree are aligned to two nodes in the other tree, but in a reciprocal relationship, as shown in Fig. 5 c. This operation can be used to handle certain divergence types such as demotional and promotional divergence, which will be discussed in more detail in Sect. 3.7 .

Figure 6 c illustrates how the swap operation takes place by swapping nodes l and j .Node j , the former parent, is demoted , keeping its attachment to its children. Node l , the former child, is promoted , and its children become siblings of node j , the result of which can be seen in Fig. 6 c. Note that the swap operation does affect multiple edges simultaneously, and thus can create a mismatch on one edge while fixing that of another. We allow for this possibility since trees that exhibit such b ehavior are rare, and will not be easily reconciled. 3.4 Calculating tree matches after applying operations The operations O1 X  X 3 are proposed to handle common divergence cases in C1 X  X 3. To measure how common C1 X  X 3 is in a language pair, we designed an algorithm that transforms a tree pair based on a word alignment.
The algorithm takes a tree pair  X  F ; E  X  and a word alignment A as input and creates a modified tree pair ( F 0 , E 0 ) and an updated word alignment A 0 as output. It has several steps. First, spontaneous nodes (nodes that do not align to any node on the other tree) are removed from each tree. Next, if a node and its parent align to the same node on the other tree, they are merged and the word alignment is changed accordingly. Finally, the swap operation is applied to a node f i and its parent f p in one tree if they align to e i and e p respectively and e p is a child of e i in the other tree. The pseudocode of the algorithm is shown in Algorithm 5.

Now given a corpus C and word alignment between each sentence pair, we can measure the impact of C1 X  X 3 by comparing CorpusMatch E ! F  X  C  X  scores before and after applying operations O1 X  X 3. This process can also reveal some patterns of divergence (e.g., what types of nodes are often merged), and the patterns can later be used to enhance existing projection algorithms.
 3.5 Improving projection algorithms With the tree operations described above, we can detect potential post-processing rules automatically. In Georgi et al. ( 2013 ), we showed that by coupling the divergent remove , merge , and swap cases C1 X  X 3 with corresponding operations O1 X  X 3, we are able to keep statistics on the affected nodes, and then use these statistics to make the following corrections to the projection algorithm: 1. Spontaneous: better informed attachment of spontaneous words. 2. Merge: better informed choice for head for multiply-aligned words. 3. Swap: post-projection correction of frequently swapped word pairs.

Figure 7 shows flowcharts for the learning and applying of the correction patterns described in the following sections. Compared to the basic projection algorithm (System 1) illustrated in Fig. 4 , the improved projection algorithm has two stages: in the training stage, correction rules are learned by comparing the projected trees produced by System 1 with the gold standard trees for the F sentences. In the test stage, those rules are applied to the projected trees produced by System 1. 3.5.1 Spontaneous reattachment The analog to the Remove operation in modifying the projection algorithm is determining how to reattach spontaneous (unaligned) words. Given that they are unaligned, no information from E is projected to them, so a lexicalized approach is used. First, we note all lexical items in the training trees and the relative position of their head (left/right). Second, we select the attachment direction for every word in the training data as noted and the attachment direction for the language as a whole. At test time, if the spontaneous word appears in the training data, we use either the word X  X  preference based on the training data to make a left or right local attachment, otherwise we use the language X  X  overall attachment direction as a backoff. 3.5.2 Merge correction As shown in Fig. 5 b,  X  X  X erged X  X  alignments are those for which there are multiple words in W F aligned to a single word in W E . The difficulty facing projection words should be made the head, and which the dependent.

In order to correct the projection at runtime, we would like to be able to know which of the multiply aligned words should be selected as the head. By keeping statistics on whether the multiply aligned words for a given POS tag tend toward the left or the right, we can then use the POS tag that is present at runtime to select the headedness for attachment.

Figure 8 a illustrates a detected merge case, while Figure 8 b demonstrates the  X  X  X eft X  X  direction of the multiply aligned dependency between the two words. Finally, Figure 8 c shows an example set of rules for a given English POS tag learned by this method. At projection time, the direction of the merge is chosen by the most likely rule learned by the analysis, or by the language X  X  overall headedness preference as a backoff.

These preferences can easily be learned by examining the attachments for each word in the corpus, and finding the proportion of those tokens that attach to words to the left versus those to the right, then using the majority preference at testing time. 3.5.3 Swap correction Swapped alignments, as illustrated in Fig. 9 a, are not patterns which would be able to be corrected in projection without some previous training, since it would require the foreign-language tree T F to already exist. Unfortunately, as we will discuss later with Hindi, these swaps can be frequent enough to cause serious performance problems in projection.

In order to correct for the swapped elements, we analyze the edges for which the swap operation was triggered, similar to the merge operation above. However, rather than keeping track of only a single part of speech tag, we instead keep corresponds to a swap operation in T F . Based on the collected counts, we keep only the pairs that occur in at least of 10 % of the training sentences, and a frequency of at least 70 % 1 .

To apply the rules, after projection, the POS tag pairs that meet the given requirements are swapped using the Swap operation defined by O3 in Sect. 3.3 . The results of applying these post-processing rules will be discussed in Sect. 4.5 . 3.6 Bootstrapping a parser Given that the analysis described here uses a small amount of training data to build these rules, one can also train a parser using the same data. Figure 10 illustrates a dependency parser trained on the small amount of monolingual data available for the given language, produced either by projection or by gold standard data. However, with such little data such a system is often outperformed by even the basic projection method, as noted in Georgi et al. ( 2012 ), cf. S3-1 in Table 5 versus S1-R/ L in Table 4 . In this previous paper, we used the edges of the projected trees as a feature to extend the MST Dependency Parser (McDonald et al. 2006 ) and the experiments showed an increase in performance over both the baseline parser and the basic projection algorithm in many cases. Here, we take the baseline parser and add a number of features based on projected trees provided at testing time (Fig. 11 ).
As previously defined, let E  X  X  W E ; T E  X  and F  X  X  W F ; T F  X  , the latter being the same way as Eqs. ( 8 ) and ( 9 ) in Sect. 3.2 . Now we can define the projection-based features in Eqs. ( 12 ) and ( 13 ). The ProjBool feature, as defined in ( 12 ) is the basic feature used for marking agreement between an edge in the projected tree and an edge being considered by the parser. This feature simply takes a TRUE value if the edge being considered by the parser also occurs in the projection. The AlignType ( 13 ), on the other hand, is actually a group of binary features used to subdivide agreement with the projection based upon the type of alignment exhibited by the word e i to words in T F . AlignType has four sub-features based on possible alignment types, which are illustrated in Fig. 12 . IS_SINGLE (Fig. 12 a) is TRUE when a token f i aligns to only one word in T . IS_UNALIGNED (Fig. 12 b) is triggered when f i is a spontaneous word; that is, it does not align to any word in T E .

IS_MATCH is TRUE in the case where the edge  X  f i ; f j  X  is being considered by the parser for the edge and the parent and child align with words e i , e j in T E , and have the same parent/child relationship, as seen in Fig. 12 c. IS_MERGE is TRUE when the foreign language word f i is one of multiple foreign words aligned with a single English word e i (Fig. 12 d).

Adding these features to the original feature set used by the MST Parser results extracted from the E trees and projected trees. Those features are added to the standard feature set used by the MST parser, and the expanded feature set is used to train the MST parser.

In the test stage, the Match/Aln features are extracted from projected F trees, and they are added to the standard features. Because of the addition of the Match/Aln features, unlike System 3, System 4 requires the test F sentences to have an aligned English sentence.

Note that for the projected F trees used to extract Match/Aln features can come from System 1, System 2, or the gold standard. The same is true for the standard features. In Sect. 4.6 we will compare the parsing performance when different combinations of F trees are used (see Table 5 a, b). 3.7 Relationship to Dorr (1994) Dorr ( 1994 ) lists seven types of divergence for language pairs. While our analysis method is more coarse-grained than the LCS that Dorr proposes, it is nonetheless able to capture some of the same cases.

For instance, Fig. 13 a illustrates an example of what Dorr identified as  X  X  X romotional X  X  divergence, where usually , a dependent of the verb goes in English, is  X  X  X romoted X  X  to become the main verb, suele in Spanish. In this case, the direction of the dependency between usually and goes is reversed in Spanish, and thus the swap operation can be applied to the English tree and result in a tree that looks very much like the Spanish tree.

A similar operation is performed for demotional divergence cases, such as aligning  X  X  X  like eating X  X  with the German translation  X  X  X ch esse gern X  X  ( X  X  X  eat likingly X  X ). Here, the main verb in English ( X  X  X ike X  X ) is demoted to an adverbial modifier in German ( X  X  gern  X  X ). The swap operation is applicable to both types of divergence and treats them equivalently, and so it essentially can handle a superset of promotional and demotional divergence, namely, X  X  X ead-swapping. X  X 
Another type of divergence that can be captured by our approach is Dorr  X  X   X  X  X tructural X  X  divergence type, as illustrated in Fig. 13 b. The difference between the English and Spanish structures in this case is the form of the argument that the verb takes. In English, it is a NP; in Spanish, it is a prepositional phrase. While the tree operations defined previously do not explicitly recognize this difference in syntactic labels, the divergence can be handled by the remove operation, where the spontaneous  X  X  X n X  X  in the Spanish side is removed.
Next, Dorr X  X  description of conflational divergence lines up well with the merge operation (see Fig. 6 b). Figure 14 illustrates an example for English and Hindi, where both sides have spontaneous words (e.g., to and a in English) and a causative verb in Hindi corresponds to multiple verbs in English. Figure 14 b shows the original tree pair, Fig. 14 c demonstrates the altered tree pair after removing spontaneous words from both sides, while Fig. 14 d shows the tree pairs after the English verbs are merged into a single node. It is clear that the remove and merge operations make the Hindi and English trees much more similar to each other.
In addition to the four divergence types mentioned above, additional operations could be added to handle other divergence types. For instance, if dependency types (e.g., patient, agent) are given in the dependency structure, we can define a new operation that changes the dependency type of an edge to account for thematic divergence, where thematic roles are switched as in  X  X  X  like Mary X  X  in English vs.  X  X  X ar X a me gusta a m X  X  X  (Mary pleases me) in Spanish. Similarly, an operation that changes the POS tag of a word can be added to cover categorial divergence where words representing the same semantic content have different word categories in the two languages, such as in  X  X  X  am hungry X  X  in English versus  X  X  X ch habe Hunger X  X  (I have hunger) in German.

Compared to Dorr X  X  divergence types, whose identification requires knowledge about the language pairs, our operations on the dependency structure relies on word alignment and tree pairs and can be applied automatically. 4 Experiments For evaluation, we ran our systems on a total of eleven language pairs, using the corpora described in Table 1 . We will describe the data used for our experiments in Sect. 4.1 . Section 4.2 details how the numbers of matches across corpus pairs are counted, using the CorpusMatch metric described in Algorithm 1 in Sect. 3.2 . Section 4.3 will look at the cases in which the match percentage still doesn X  X  reach 100 % after applying all the tree operations. Section 4.4 will show some of the patterns discovered by breaking down the analysis by POS. Finally, Sect. 4.5 will discuss the results of using the automatically discovered patterns to improve the baseline projection algorithm, while Sect. 4.6 shows the result of using this improved projection algorithm to bootstrap a dependency parser. 4.1 Data Our work utilizes three corpora for a total of eleven language pairs. The three corpora used are the SMULTRON treebank (Volk et al. 2010 ), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al. 2009 ), and several sets of IGT data as used in Lewis and Xia ( 2010 ). The statistics of the corpora are shown in Table 1 . Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus.
In the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and we converted the phrase structures into dependency trees using a head percolation table (Collins 1999 ).

From the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser (Marneffe et al. 2006 ) and then we hand corrected the structures. Word alignment is initially derived from the IGT instances using heuristic alignment following Lewis and Xia ( 2010 ), and later hand-corrected. The IGT data from Lewis and Xia ( 2010 ) was obtained in the manually corrected dependency forms as described in Sect. 2.2 . 4.2 Match results By running Algorithm 1, we can calculate the CorpusMatch E ! F and CorpusMatch F ! E before and after each operation and see how the operation affects the percentage of matched edges in the corpus. As the operations are applied, the percentage of matches between the trees should increase until all the divergence cases that can be handled by operations O1 X  X 3 have been resolved. At this point, the final match percentage can be seen as an estimate of the upper-bound on performance of a simple projection algorithm, if C1-C3 can be identified and handled by O1-O3. Table 2 a shows the full results of this process for the Hindi-English pair, while Table 2 b shows a summary for the results in the remaining ten languages.
 The results in Table 2 a show that the trees start out very dissimilar between English and Hindi, having only 47.7 % of the edges in the English trees matching those in the Hindi trees initially. After removing words that are not aligned between English and Hindi, still only 66.1 % are aligned. While merging multiply-aligned words improves this match by 3.4 %, applying swaps in the English trees results by increasing matches by a large 20.8 %. The reason for this large increase in this language pair can be attributed to the way in which prepositions and postpositions are represented in Hindi, which is explained further in Sect. 4.4 .

Between Hindi in Table 2 a and the other languages in Table 2 b, the application of the operations increases the match percentage, but never reaches 100 %. The match percentage after all operations are applied can be seen as an upper bound on the tree similarities between the language pair for a given corpus.
 4.3 Remaining cases After applying three operations, there may still be unmatched edges. An example is given in Fig. 15 . 2 The dependency edge (in, America) can be reversed by the swap operation to match the Hindi counterpart. The mismatch in this sentence is that the adverb mentally in English corresponds to the noun mana (mind) in Hindi. If the word alignment includes the three word pairs as indicated by the dotted lines, one potential way to handle this kind of divergence is to extend the definition of merge to allow edges to be merged on both sides simultaneously X  X n this case, merging am and mentally in the English side, and hE (is) and mana (mind) on the Hindi side. 4.4 Operation breakdown by POS After performing the operations as seen in Sect. 4.2 , we can get further insight into what precisely is happening within each language by breaking down the operations by the POS tags on which the operations apply. Table 3 shows some of these POS tag breakdowns for a number of languages, and the frequency with which the given operation applies to the POS tag or POS tag pair out of all the times it is seen in that language. For example, the results in Row 1 shows that when a modal (MD) depends on a verb (VB) in English, 43.9 % of the time in the training data, the two words align to the same word in Hindi. Table 3 shows expected phenomena from the language pairs. For instance, Rows 5 and 6 show the English ! German pair merging many nouns as multiple English words are expressed as compounds in German. In another case, Row 8 shows that all Hindi nouns undergo swap with prepositions, as Hindi uses postpositions. Noticing the regularity with which NN and IN swap leads us to the next experiment, where we examine how such regularly-occurring rules might be harnessed to improve projection. 4.5 Analyzing trees for post-processing rules Table 4 compares the projection accuracy of the basic projection algorithm (System 1 as in Fig. 4 ) and the improved projection algorithm (System 2 as in Fig. 7 ). For all the experiments, we use tenfold cross validation with a 9:1 training/test split.
In this table, System 1 (S1) serves as a baseline. In System 1, when a merge alignment was detected or a spontaneous word needed reattaching, the algorithm simply attached rightward (S1-R) or leftward (S1-L). The results are in the last two rows of Table 4 b. The numbers in the table are Unlabeled Attachment Scores (UAS). In contrast, System 2 (S2) uses the training data to learn which direction the attachment is more common. In addition to the merge rules, System 2 can also apply rules for swap, or remove. These results are shown in the first three rows of Table 4 b. The table shows that applying automatically learned rules improves projection accuracy significantly, the average accuracy increasing from 83.22 % (S1-L) or 83.58 % (S1-R) to 88.95 % (S2-1). 4.6 Parsing experiments In Sect. 3.6 , we described two parsing approaches: System 3 and System 4. In System 3 (see Fig. 10 in Sect. 3.6 ), the MST parser uses the standard feature set without using Match/Aln features. It is trained with the projected trees produced by System 1, System 2, or gold standard F trees (As shown in the  X  X  X rain Source X  X  Column in Table 5 a. For the feature set, we can use either the word features only or add POS tag features in the F sentences (as indicated by the checkmark in the  X  X  X rojected POS Feat X  X  column). If only the word features are used, the input are F sents with words only. If POS tag features are used, for both training and test data, we use the POS tags projected from the English side. The parsing results are shown rows labeled S3-1 to S3-5 in Table 5 .
In System 4 (see Fig. 11 in Sect. 3.6 ), Match/Aln features are added to the standard feature set used by the MST parser. Standard features can be extracted from System 1, System 2, or gold F trees. The same is true for the Match/Aln features. The sources of the trees are indicated in the  X  X  X rain Sources X  X  column in Table 5 a. At the test stage, standard features are extracted from test F sentences with projected POS tags, and the source of the Match/Aln features is indicated in the  X  X  X est Source X  X  column. The  X  X  X racle X  X  system is the system in which Gold F trees are used both for training and testing. S4-1 through S4-3 are a few variants of System 4 when different combinations of train and test sources are used.

There are several observations: First, adding the Match/Aln features improves performance significantly, from 89.32 % in S4-1 versus 79.16 % in S3-1. Second, using the improved projection algorithm improves parsing results compared to using the basic projection algorithm for both System 3 (row S3-2 vs. S3-3 and S3-4) and System 4 (row S4-1 vs. S4-2 and S4-3). A further discussion and comparison of the average scores of the system can be found in Sect. 5.3 . 5 Discussion of results The results of the experiments above show that the match scoring that we have introduced here has the potential to address many interesting issues arising between language pairs. In this section, we highlight some observations based on the experimental results.
 5.1 Match scores The results of Table 2 a and b compare similarity both across languages and across corpora. For instance, in the scores for the baseline ODIN data, we see that the baseline for matches between English and German is the highest out of all the pairs at 76.7 %. Scots Gaelic and Welsh are 72 and 75.4 %, respectively. Hausa, Malagasy, Korean, and Yaqui all show baseline scores between 54 X 57 %. This seems in line with what we would expect, with German and the Celtic languages being closely related to English, and the others being unrelated.

Another stark contrast can be seen between all the languages in the ODIN data and the languages in the SMULTRON corpus. While the ODIN sentences tend to be short sentences used primarily for illustrative purposes, the SMULTRON corpus consists of economic, literary, and weather domains. As Table 1 shows, the SMULTRON sentences are much longer on average. A closer look at the SMULTRON translations also shows them to be much freer translations than those found in the ODIN data. While the size of the data sets used here are very small, and the ODIN IGT data may be biased towards illustrative purposes (described as the  X  X  X GT Bias X  X  in Lewis and highlights two types of differences among the corpora. First, by comparing baselines match results among comparable corpora, basic similarities between languages appear to pattern as expected. Second, the freer translations in the SMULTRON data appear with lower match scores across all instances.
 One final item of interest from the match results can be seen in the Hindi data in Table 2 a. Here, there appears to be a large increase in match percentage after the swap operation has been performed. As previously noted, knowing this is the inspiration for automatically inferring the swap rules in Sect. 4.5 . 5.2 POS breakdowns The breakdown of the operations by language and POS in Table 3 provides a good opportunity to check that the defined operations conform with expectations for specific languages.
 For instance, Row 1 in Table 3 shows Modals (MD) merging with a parent (VB). This is in line with instances such as Fig. 14 c where Hindi combines aspect with a verb that is typically expressed as a separate word in English. This does not appear to be a very frequent occurrence, however, as it only occurs for 42.9 % of MD ! VB dependencies.

Row 3, going from Hindi to English shows the case where auxiliary verbs VAUX merge with main verbs VM. These cases typically represent those where Hindi expresses tense as an auxiliary verb, whereas English tense is expressed by inflection on the verb.

With regard to spontaneous words in English and Hindi, Row 14 shows that 69.8 % of case markers (PSP) were removed from Hindi that were either absent in English or applied as inflections to the noun, while 86 % of determiners in English were removed, as Hindi does not have definite or indefinite determiners (Row 12).
Examining the English and German data in Table 3 , we first see in Row 5 that 66.7 % of NN-NNS dependencies in English merge. This, along with the 65.4 % of NN-NN dependencies merging, is something we would expect to see in German, as it compounds nouns with far more frequency than English. Interestingly, as Row 7 shows, a plural noun child never merges with a parent noun.

Finally, looking more closely at the swaps, we see a 100 % of NN ! IN dependencies are swapped in Hindi, giving further impetus for the rules as described in Sect. 4.5 . 5.3 Performance summary In this study, we proposed four systems for parsing F sentences: basic projection (System 1), improved projection (System 2), original MST parser trained with projected F trees (System 3), and MST parser with additional Match/Aln features (System 4). The results are in Tables 4 and 5 . A summary of the most important results of these tables and the comparison of the systems are in Table 6 a, and the error reduction of some system pairs are in Table 6 b.

As Table 6 b shows, in average across the eight languages, the reduction in error in using the parser trained with match and alignment features (S4-1) has a 48.76 % reduction in error over the parser trained on the gold standard trees with projected POS tags (S3-1). In the projection systems alone, the improved projection system S2-1 reduces error over the baseline system S1-R by 32.73 %. Finally, even without the Match/Aln features, a monolingual parser trained on projected dependency trees (S3-2) shows 19.27 % fewer errors than the system trained on the basic projections (S3-3).

While the improvement over the modified projection algorithm is modest, a dependency parser such as S4-1 does have the advantage of being more noise-robust. For instance, given an English sentence where the head of the English sentence is not aligned to any of the words in the language line, projection algorithms will not produce a tree structure, whereas the MST Parser will produce a tree structure based on other features when the alignment is not available. This advantage makes such an approach appealing for extending to larger corpora, which is something we will address in Sect. 6 . 5.4 Remaining issues Two large issues that our methodology faces are data sparsity and translation quality of the sentence pairs in the data sets. The former is somewhat inevitable given the task X  X  reasonable amount of annotated data is not always likely to exist for languages with scarce electronic resources, and guaranteeing coverage is difficult. As with the Hindi data, however, using IGT as a resource has convenience in both covering wide varieties of phenomena in a language, and providing a gloss that assists in creating word-level alignments. Creating dependency annotation on a small set of data from a source like ODIN (Lewis and Xia 2010 ) can get a lot of mileage with a small amount of investment.
Perhaps the more challenging issue is determining whether divergence in a language pair is caused by fundamental differences between the languages, or simply stylistic choices in translation. The latter of these scenarios appeared to be common in portions of the SMULTRON data, where translations appeared to be geared toward naturalness in the target language; in contrast, the translations in the Hindi guideline sentences were intended to be as literal as possible. Again, IGT provides a good possible solution, as such examples are often intended specifically for illustrative purposes. 6 Conclusion and future work In this paper, we have demonstrated a generalizable approach to detecting patterns of structural divergence across language pairs using simple tree operations based on word alignment. We have shown that this methodology can be used to detect similarities between languages on a coarse level, as well as serve as a general measure of similarity between dependency corpora. Finally, we establish that harnessing these detection methods improves standard projection algorithms and informs dependency parsing with little to no expert involvement.

For future work, we plan to focus on two areas. The first is that of adapting these techniques to larger data sets. In particular, use of the high-quality alignments derived from IGT to bootstrap a statistical aligner may allow for reasonable performance on languages for which the amount of parallel data may not be sufficient for building a high-quality statistical word aligner. Secondly, while this paper explores the utility of IGT in terms of word alignment and projection, we are currently looking into the ways in which the additional morphemic and lexicosyn-tactic information in the gloss lines may be used to perform more complex automated linguistic analysis.

The techniques described here are promising for maximizing the effectiveness of existing resources such as IGT for languages where such resources are limited. While access to electronic resources continues to increase globally, many of these resource-poor languages are still left behind in terms of NLP tools. Though projection techniques may not ultimately be full replacements for large treebank projects, the ability of these techniques to be rapidly deployed is extremely useful for researchers seeking to experiment with new languages at minimal cost. References
 Abstract Obtaining syntactic parses is an important step in many NLP pipelines. However, most of the world X  X  languages do not have a large amount of syntactically annotated data available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when syntactic structures for some sentence pairs in the two languages look quite different. In this paper, we investigate the use of small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. We then use these detected patterns to improve projection algorithms and dependency parsers, allowing for better performing NLP tools for resource-poor languages, particularly those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of syntactic projection and parsing.
 Keywords Multilingualism Translation divergence Syntactic projection 1 Introduction When it comes to resources for natural language processing, a small handful of languages account for the vast majority of available resources. Out of the resources listed by the Language Resource and Evaluation (LRE) Map (Calzolari et al. 2012 ), English accounts for 30 % of all recorded resources, and the ten most resourced languages account for 62 %. A broad variety of tools are available for these resource-rich languages, since the time and effort spent to annotate resources for them allows for state-of-the-art systems to be built utilizing supervised and semi-supervised methods.

The availability of such resources does not come at a low cost; they are the result of a large investment over many years on a per-language-basis. Because creating high-quality annotation is expensive and labor intensive, the vast majority of the world X  X  languages lack such resources and, likewise, high-performance NLP tools. To address this issue, recent studies (Lewis and Xia 2008 ; Benajiba and Zitouni 2010 ; Georgi et al. 2012 ) have investigated using bitexts in which one half of the bitext is a resource-rich language. In this paradigm, existing tools for the resource-rich language can be used to process one side and project the information to the other (the resource-poor language) via word alignments.

While projecting annotation from one language to another is a promising method for adding annotation to languages using automated methods, it relies on the assumption that simple word alignments between languages are sufficient to represent analogous meanings and structures between the languages. For reasons we will discuss in the following sections, this assumption is useful, but often erroneous.
Finding out whether and when this assumption fails for a given language pair is not easy without knowledge about the two languages. It would be useful if, given a small set of seed data, a language pair could be analyzed to find where and in what ways the languages diverge, and use these detected patterns as corrective guidelines for improving projection on other sentences for the language pair.

In this paper, we propose a method for analyzing a language pair and determining the degree and types of divergence between two dependency trees in the two languages. We then use this systematic identification of divergence types to inform and correct the trees produced by syntactic projection. Using these improved trees, we are able to boost the performance of a dependency parser by adding new features extracted from projected trees. 2 Background While there is a growing body of work on projection methods as a means to bootstrap resources for one language from another, there are not many studies on how to handle the issue of linguistic divergence between these languages. In this section, we provide a brief review of work on divergence and projection algorithms. We will also introduce interlinear glossed text (IGT), a common format used by linguists to represent language examples (Lewis 2006 ). 2.1 Projection methods Projection algorithms have been the target of a fair amount of research in the last decade, as attempts have been made to utilize statistical alignment methods to match words between languages with parallel data and  X  X  X roject X  X  annotations between them. Figure 1 shows an example bitext in the form of an IGT, while Fig. 2 shows how this data may be used to project a dependency tree from English to Hindi.

Some of the initial research on the subject of projecting word-level annotation from one language to another was published in Yarowsky and Ngai ( 2001 ). Here, the authors used IBM Model 3 (Brown et al. 1990 ) to align large parallel corpora in English X  X hinese and English X  X rench. A part-of-speech (POS) tagger was trained for French using projections from English, and noun phrase (NP) bracketers were trained similarly for both French and Chinese. The authors identified noisy statistical alignment and 1-to-many alignments as two main issues affecting the performance of projection. The first of these issues is a difficult problem for resource-poor languages, as high-quality statistical word alignment often requires much more bitext than might be available for the language. While it is not a full solution to the problem, many of the language pairs we use in this work are drawn from collection of IGT instances, as shown in Fig. 1 , which provide unique shortcuts for obtaining word alignments with a small amount of data. IGT will be discussed further in Sect. 2.2 .

The second issue, 1-to-many alignments, may be the result of linguistic divergence in a language pair where the language being projected from is morphologically richer than the other. In cases such as this, finding common patterns of conflation can be useful for generalizing a projection to new data. For instance, Fig. 3 shows a very simple but common case of conflation in the SMULTRON corpus (Volk et al. 2010 ), where a single German word aligns to multiple English words. Using this one-to-many alignment, the same POS tag would be projected to both English tokens. In this case, using a universal tagset such as more complex cases, learning the pattern would be more critical.

Hwa et al. ( 2004 ) investigated the issues in using projection between languages in order to develop and train syntactic parsers, and described the Direct Correspondence Assumption (DCA), the assumption made in projection algorithms that the target language tree should be homomorphic with the source language tree. While useful, this assumption often does not hold, as the authors pointed out. In order to fix some of the errors made by faulty projections, Hwa et al. used an approach that applies post-projection correction rules. For projection from English to Spanish, the accuracy of the projected structures increased from 36.8 to 70.3 %. The accuracy of the English to Chinese projection increased from 38.1 to 67.3 %.
While these language-specific rewrite rules are promising, they still require language-specific knowledge. What we seek to accomplish in this paper is a general framework for automatically detecting this divergence, both in specific language pairs and its frequency throughout a large number of languages. With the use of this automated divergence detection, it may be possible to learn these rewrite rules from a small annotated corpus and use them to improve projection algorithms.
 2.2 Interlinear glossed text As mentioned in the preceding section, much of the data for our experiments was drawn from the unique IGT data type. IGT instances are a common way for linguists to give illustrative examples for a language being studied. Figure 1 shows an instance of IGT for Hindi. As with this example, an IGT instance typically has three lines: a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line. The translation line is typically in English, the language of the research paper from which the IGT is extracted. Of special interest in IGT instances is the middle gloss line, which gives a word-by-word gloss of the original language. By design, the alignment between the language and gloss lines is monotonic and one-to-one, thus providing easy matches between these two lines. The matching of words between the gloss and translation can be utilized to obtain high-quality, automatic word alignment between the sentences in the language pair without the need for the much larger amounts of data typically required by statistical alignment algorithms.

In Lewis and Xia ( 2010 ), IGT data for seven language pairs was automatically aligned, projection performed, then finally hand-corrected to create gold standards with minimal manual intervention. They showed the potential for using IGT as a resource for languages for which finding resources would otherwise be extremely difficult or impossible to obtain. We will use this data for the current work. A breakdown of the language pairs can be seen in Sect. 4.1 .

Lewis and Xia ( 2008 ) used projected phrase structures to determine the basic word order for 97 languages using a database of IGT instances. By using the alignment method described above and projecting phrase structures from English to the foreign language line, the word order in the foreign language could be inferred. For languages with just 10 X 39 IGT instances, the accuracy of predicting basic word order was 79 %; with more than 40 instances, the accuracy jumped to 99 %. Figure 4 gives a high-level view of a basic syntactic projection system that uses IGT as the source of projection. Using the IGT as a source, we extract sentences in the foreign language (F) and English (E), as well as word alignment between F and E via the gloss line. Then the E sentence is parsed and the parsed tree is projected to the F side via word alignment and heuristics. Section 3.5 will discuss learning and applying corrections to projected trees, while Sect. 3.6 will describe a system that bootstraps a parser using the projected systems.
 2.3 Linguistic divergence The previously mentioned studies illustrate the promise of projection for bootstrapping new tools for resource-poor languages, but one limitation is their reliance on the assumption that syntactic structures of the two sentences in a given sentence pair are similar. While Hwa et al.  X  X  DCA describes the assumption made for projection, Dorr ( 1994 ) makes a deeper analysis of divergence in languages. Dorr outlined lexical conceptual structures (LCS) that provide a general framework to describe these exceptions to the DCA. This framework is capable of representing divergence stemming from syntactic, lexical, or semantic differences, but for the purposes of this paper we will focus primarily on those that are lexical and syntactic.
Our goal in this work is to create a methodology by which these common types of divergences can be detected automatically from bitexts in order to improve the performance of existing structural projection methods. 3 Methodology In our approach to automatically detecting divergent structures between language pairs, we first propose a metric to measure the degree of matched edges between trees in a language pair (Sect. 3.2 ). Second, we define three operations on trees in order to capture three common types of divergence (Sect. 3.3 ). Third, we apply the operations on a tree pair and show how the operations could affect the degree of tree match (Sect. 3.4 ).
Next, we address how the detected patterns can be used to apply tree modification rules to improve the projection algorithm (Sect. 3.5 ) and help in training a dependency parser (Sect. 3.6 ). Finally, we will explain the relationship of our operations to Dorr X  X  divergence types (Sect. 3.7 ). 3.1 Definitions In the following sections, as we describe the projection algorithm and trees, we will assume that the resource-rich language being projected from is typically English, E ( 1 ), found in the translation line of IGT instances, and the resource-poor language is the foreign language F ( 2 ). Each sentence will be represented by a pair of words and language tree, with W E ( 5 ) and W F ( 6 ) referring to the individual words in the respective sentences. Each tree edge will be represented as a pair of words to be 3.2 Comparing dependency trees Defining a metric for comparing dependency trees cross-linguistically proved to be a crucial component of our method, as most existing tree similarity measures are intended to compare tree representations with the same number of tokens. Comparing across languages, however, means that the number of tokens can vary. We instead look for a method to determine similarity by means of matched edges in the tree, as shown in Fig. 5 .

Given an IGT instance, and the sentences F and E as defined above, A is the word alignment between the sentence pair, where f i and f j are words in W F and e k and e l denoted as e i , not f i . The alignment A is a set of word pairs: We call an  X  F ; E ; A  X  tuple an aligned tree pair. A corpus, C , in our experiments, is a is aligned to e i and f k is aligned to e k . Because the alignment between a sentence pair can be many-to-many, we define the following functions, which map a word from one sentence to the set of words in the other sentence.
 We then define the boolean function match , as follows: That is, an edge  X  f i ; f j  X  in F matches some edge in E according to A if there exists edge in E .

Given an aligned tree pair  X  F ; E ; A  X  , we define SentMatch  X  F ; E ; A  X  as the percentage of edges in F that match some edge in E . Given a corpus C , we define CorpusMatch F ! E  X  C  X  as the percentage of edges in the T F trees that match some edges in the corresponding T E trees. Similarly, CorpusMatch E ! F  X  C  X  is the percentage of edges in the T E trees that match some edges in the corresponding T F trees.
 Algorithm 1 : Calculating the percentage of matched edges in corpus C.
 3.3 Defining Tree Operations When an edge  X  f i ; f k  X  in T F does not match any edge in T E , it may be caused by one of the following cases: C1 f i or f k are spontaneous (they do not align with any words in the other tree). C2 f i and f k are both aligned with the same node e i in the other tree (Fig. 5 b). C3 f i and f k are both aligned with nodes in the other tree, e k and e i , but in a C4 There are some other structural differences not caused by C3.3 X  X 3.3.
 The first three cases are common. To capture them, we define three operations on a tree X  remove , merge , and swap . 3.3.1 O1: Remove The remove operation is used to remove spontaneous words. As shown in Fig. 6 a, removal of the node l is accomplished by removing the link between node l and its parent j , and adding links between the parent and the removed node X  X  children.
This result of this operation can be seen in Fig. 6 a, using the relation Children , which maps a word to the set of all its children in the tree. 3.3.2 O2: Merge The merge operation is used when a node and some or all of its children in one tree align to the same node(s) in the other tree, as can be seen in Fig. 5 b. The parent j and child l are collapsed into a merged node, as indicated by l ? j in Fig. 6 b, and the children of l are promoted to become children of the new node l ? j . The result can be seen in Fig. 6 b. 3.3.3 O3: Swap The swap operation is used when two nodes in one tree are aligned to two nodes in the other tree, but in a reciprocal relationship, as shown in Fig. 5 c. This operation can be used to handle certain divergence types such as demotional and promotional divergence, which will be discussed in more detail in Sect. 3.7 .

Figure 6 c illustrates how the swap operation takes place by swapping nodes l and j .Node j , the former parent, is demoted , keeping its attachment to its children. Node l , the former child, is promoted , and its children become siblings of node j , the result of which can be seen in Fig. 6 c. Note that the swap operation does affect multiple edges simultaneously, and thus can create a mismatch on one edge while fixing that of another. We allow for this possibility since trees that exhibit such b ehavior are rare, and will not be easily reconciled. 3.4 Calculating tree matches after applying operations The operations O1 X  X 3 are proposed to handle common divergence cases in C1 X  X 3. To measure how common C1 X  X 3 is in a language pair, we designed an algorithm that transforms a tree pair based on a word alignment.
The algorithm takes a tree pair  X  F ; E  X  and a word alignment A as input and creates a modified tree pair ( F 0 , E 0 ) and an updated word alignment A 0 as output. It has several steps. First, spontaneous nodes (nodes that do not align to any node on the other tree) are removed from each tree. Next, if a node and its parent align to the same node on the other tree, they are merged and the word alignment is changed accordingly. Finally, the swap operation is applied to a node f i and its parent f p in one tree if they align to e i and e p respectively and e p is a child of e i in the other tree. The pseudocode of the algorithm is shown in Algorithm 5.

Now given a corpus C and word alignment between each sentence pair, we can measure the impact of C1 X  X 3 by comparing CorpusMatch E ! F  X  C  X  scores before and after applying operations O1 X  X 3. This process can also reveal some patterns of divergence (e.g., what types of nodes are often merged), and the patterns can later be used to enhance existing projection algorithms.
 3.5 Improving projection algorithms With the tree operations described above, we can detect potential post-processing rules automatically. In Georgi et al. ( 2013 ), we showed that by coupling the divergent remove , merge , and swap cases C1 X  X 3 with corresponding operations O1 X  X 3, we are able to keep statistics on the affected nodes, and then use these statistics to make the following corrections to the projection algorithm: 1. Spontaneous: better informed attachment of spontaneous words. 2. Merge: better informed choice for head for multiply-aligned words. 3. Swap: post-projection correction of frequently swapped word pairs.

Figure 7 shows flowcharts for the learning and applying of the correction patterns described in the following sections. Compared to the basic projection algorithm (System 1) illustrated in Fig. 4 , the improved projection algorithm has two stages: in the training stage, correction rules are learned by comparing the projected trees produced by System 1 with the gold standard trees for the F sentences. In the test stage, those rules are applied to the projected trees produced by System 1. 3.5.1 Spontaneous reattachment The analog to the Remove operation in modifying the projection algorithm is determining how to reattach spontaneous (unaligned) words. Given that they are unaligned, no information from E is projected to them, so a lexicalized approach is used. First, we note all lexical items in the training trees and the relative position of their head (left/right). Second, we select the attachment direction for every word in the training data as noted and the attachment direction for the language as a whole. At test time, if the spontaneous word appears in the training data, we use either the word X  X  preference based on the training data to make a left or right local attachment, otherwise we use the language X  X  overall attachment direction as a backoff. 3.5.2 Merge correction As shown in Fig. 5 b,  X  X  X erged X  X  alignments are those for which there are multiple words in W F aligned to a single word in W E . The difficulty facing projection words should be made the head, and which the dependent.

In order to correct the projection at runtime, we would like to be able to know which of the multiply aligned words should be selected as the head. By keeping statistics on whether the multiply aligned words for a given POS tag tend toward the left or the right, we can then use the POS tag that is present at runtime to select the headedness for attachment.

Figure 8 a illustrates a detected merge case, while Figure 8 b demonstrates the  X  X  X eft X  X  direction of the multiply aligned dependency between the two words. Finally, Figure 8 c shows an example set of rules for a given English POS tag learned by this method. At projection time, the direction of the merge is chosen by the most likely rule learned by the analysis, or by the language X  X  overall headedness preference as a backoff.

These preferences can easily be learned by examining the attachments for each word in the corpus, and finding the proportion of those tokens that attach to words to the left versus those to the right, then using the majority preference at testing time. 3.5.3 Swap correction Swapped alignments, as illustrated in Fig. 9 a, are not patterns which would be able to be corrected in projection without some previous training, since it would require the foreign-language tree T F to already exist. Unfortunately, as we will discuss later with Hindi, these swaps can be frequent enough to cause serious performance problems in projection.

In order to correct for the swapped elements, we analyze the edges for which the swap operation was triggered, similar to the merge operation above. However, rather than keeping track of only a single part of speech tag, we instead keep corresponds to a swap operation in T F . Based on the collected counts, we keep only the pairs that occur in at least of 10 % of the training sentences, and a frequency of at least 70 % 1 .

To apply the rules, after projection, the POS tag pairs that meet the given requirements are swapped using the Swap operation defined by O3 in Sect. 3.3 . The results of applying these post-processing rules will be discussed in Sect. 4.5 . 3.6 Bootstrapping a parser Given that the analysis described here uses a small amount of training data to build these rules, one can also train a parser using the same data. Figure 10 illustrates a dependency parser trained on the small amount of monolingual data available for the given language, produced either by projection or by gold standard data. However, with such little data such a system is often outperformed by even the basic projection method, as noted in Georgi et al. ( 2012 ), cf. S3-1 in Table 5 versus S1-R/ L in Table 4 . In this previous paper, we used the edges of the projected trees as a feature to extend the MST Dependency Parser (McDonald et al. 2006 ) and the experiments showed an increase in performance over both the baseline parser and the basic projection algorithm in many cases. Here, we take the baseline parser and add a number of features based on projected trees provided at testing time (Fig. 11 ).
As previously defined, let E  X  X  W E ; T E  X  and F  X  X  W F ; T F  X  , the latter being the same way as Eqs. ( 8 ) and ( 9 ) in Sect. 3.2 . Now we can define the projection-based features in Eqs. ( 12 ) and ( 13 ). The ProjBool feature, as defined in ( 12 ) is the basic feature used for marking agreement between an edge in the projected tree and an edge being considered by the parser. This feature simply takes a TRUE value if the edge being considered by the parser also occurs in the projection. The AlignType ( 13 ), on the other hand, is actually a group of binary features used to subdivide agreement with the projection based upon the type of alignment exhibited by the word e i to words in T F . AlignType has four sub-features based on possible alignment types, which are illustrated in Fig. 12 . IS_SINGLE (Fig. 12 a) is TRUE when a token f i aligns to only one word in T . IS_UNALIGNED (Fig. 12 b) is triggered when f i is a spontaneous word; that is, it does not align to any word in T E .

IS_MATCH is TRUE in the case where the edge  X  f i ; f j  X  is being considered by the parser for the edge and the parent and child align with words e i , e j in T E , and have the same parent/child relationship, as seen in Fig. 12 c. IS_MERGE is TRUE when the foreign language word f i is one of multiple foreign words aligned with a single English word e i (Fig. 12 d).

Adding these features to the original feature set used by the MST Parser results extracted from the E trees and projected trees. Those features are added to the standard feature set used by the MST parser, and the expanded feature set is used to train the MST parser.

In the test stage, the Match/Aln features are extracted from projected F trees, and they are added to the standard features. Because of the addition of the Match/Aln features, unlike System 3, System 4 requires the test F sentences to have an aligned English sentence.

Note that for the projected F trees used to extract Match/Aln features can come from System 1, System 2, or the gold standard. The same is true for the standard features. In Sect. 4.6 we will compare the parsing performance when different combinations of F trees are used (see Table 5 a, b). 3.7 Relationship to Dorr (1994) Dorr ( 1994 ) lists seven types of divergence for language pairs. While our analysis method is more coarse-grained than the LCS that Dorr proposes, it is nonetheless able to capture some of the same cases.

For instance, Fig. 13 a illustrates an example of what Dorr identified as  X  X  X romotional X  X  divergence, where usually , a dependent of the verb goes in English, is  X  X  X romoted X  X  to become the main verb, suele in Spanish. In this case, the direction of the dependency between usually and goes is reversed in Spanish, and thus the swap operation can be applied to the English tree and result in a tree that looks very much like the Spanish tree.

A similar operation is performed for demotional divergence cases, such as aligning  X  X  X  like eating X  X  with the German translation  X  X  X ch esse gern X  X  ( X  X  X  eat likingly X  X ). Here, the main verb in English ( X  X  X ike X  X ) is demoted to an adverbial modifier in German ( X  X  gern  X  X ). The swap operation is applicable to both types of divergence and treats them equivalently, and so it essentially can handle a superset of promotional and demotional divergence, namely, X  X  X ead-swapping. X  X 
Another type of divergence that can be captured by our approach is Dorr  X  X   X  X  X tructural X  X  divergence type, as illustrated in Fig. 13 b. The difference between the English and Spanish structures in this case is the form of the argument that the verb takes. In English, it is a NP; in Spanish, it is a prepositional phrase. While the tree operations defined previously do not explicitly recognize this difference in syntactic labels, the divergence can be handled by the remove operation, where the spontaneous  X  X  X n X  X  in the Spanish side is removed.
Next, Dorr X  X  description of conflational divergence lines up well with the merge operation (see Fig. 6 b). Figure 14 illustrates an example for English and Hindi, where both sides have spontaneous words (e.g., to and a in English) and a causative verb in Hindi corresponds to multiple verbs in English. Figure 14 b shows the original tree pair, Fig. 14 c demonstrates the altered tree pair after removing spontaneous words from both sides, while Fig. 14 d shows the tree pairs after the English verbs are merged into a single node. It is clear that the remove and merge operations make the Hindi and English trees much more similar to each other.
In addition to the four divergence types mentioned above, additional operations could be added to handle other divergence types. For instance, if dependency types (e.g., patient, agent) are given in the dependency structure, we can define a new operation that changes the dependency type of an edge to account for thematic divergence, where thematic roles are switched as in  X  X  X  like Mary X  X  in English vs.  X  X  X ar X a me gusta a m X  X  X  (Mary pleases me) in Spanish. Similarly, an operation that changes the POS tag of a word can be added to cover categorial divergence where words representing the same semantic content have different word categories in the two languages, such as in  X  X  X  am hungry X  X  in English versus  X  X  X ch habe Hunger X  X  (I have hunger) in German.

Compared to Dorr X  X  divergence types, whose identification requires knowledge about the language pairs, our operations on the dependency structure relies on word alignment and tree pairs and can be applied automatically. 4 Experiments For evaluation, we ran our systems on a total of eleven language pairs, using the corpora described in Table 1 . We will describe the data used for our experiments in Sect. 4.1 . Section 4.2 details how the numbers of matches across corpus pairs are counted, using the CorpusMatch metric described in Algorithm 1 in Sect. 3.2 . Section 4.3 will look at the cases in which the match percentage still doesn X  X  reach 100 % after applying all the tree operations. Section 4.4 will show some of the patterns discovered by breaking down the analysis by POS. Finally, Sect. 4.5 will discuss the results of using the automatically discovered patterns to improve the baseline projection algorithm, while Sect. 4.6 shows the result of using this improved projection algorithm to bootstrap a dependency parser. 4.1 Data Our work utilizes three corpora for a total of eleven language pairs. The three corpora used are the SMULTRON treebank (Volk et al. 2010 ), the guideline sentences in IGT form from the Hindi treebank (Bhatt et al. 2009 ), and several sets of IGT data as used in Lewis and Xia ( 2010 ). The statistics of the corpora are shown in Table 1 . Ten of the language pairs use English as one side of the language, while the eleventh uses the pair of German and Swedish from the SMULTRON corpus.
In the SMULTRON Treebank, the German and Swedish phrase trees are marked for head children, allowing for the automatic extraction of dependency trees. The English side of the phrase structures do not contain edge labels and we converted the phrase structures into dependency trees using a head percolation table (Collins 1999 ).

From the Hindi Treebank guidelines, we extracted example sentences in the form of IGT (i.e., Hindi sentences, English gloss, and English translation) and the Hindi dependency structures manually created by the guideline designers. We obtained dependency structures for the English translation by running the Stanford dependency parser (Marneffe et al. 2006 ) and then we hand corrected the structures. Word alignment is initially derived from the IGT instances using heuristic alignment following Lewis and Xia ( 2010 ), and later hand-corrected. The IGT data from Lewis and Xia ( 2010 ) was obtained in the manually corrected dependency forms as described in Sect. 2.2 . 4.2 Match results By running Algorithm 1, we can calculate the CorpusMatch E ! F and CorpusMatch F ! E before and after each operation and see how the operation affects the percentage of matched edges in the corpus. As the operations are applied, the percentage of matches between the trees should increase until all the divergence cases that can be handled by operations O1 X  X 3 have been resolved. At this point, the final match percentage can be seen as an estimate of the upper-bound on performance of a simple projection algorithm, if C1-C3 can be identified and handled by O1-O3. Table 2 a shows the full results of this process for the Hindi-English pair, while Table 2 b shows a summary for the results in the remaining ten languages.
 The results in Table 2 a show that the trees start out very dissimilar between English and Hindi, having only 47.7 % of the edges in the English trees matching those in the Hindi trees initially. After removing words that are not aligned between English and Hindi, still only 66.1 % are aligned. While merging multiply-aligned words improves this match by 3.4 %, applying swaps in the English trees results by increasing matches by a large 20.8 %. The reason for this large increase in this language pair can be attributed to the way in which prepositions and postpositions are represented in Hindi, which is explained further in Sect. 4.4 .

Between Hindi in Table 2 a and the other languages in Table 2 b, the application of the operations increases the match percentage, but never reaches 100 %. The match percentage after all operations are applied can be seen as an upper bound on the tree similarities between the language pair for a given corpus.
 4.3 Remaining cases After applying three operations, there may still be unmatched edges. An example is given in Fig. 15 . 2 The dependency edge (in, America) can be reversed by the swap operation to match the Hindi counterpart. The mismatch in this sentence is that the adverb mentally in English corresponds to the noun mana (mind) in Hindi. If the word alignment includes the three word pairs as indicated by the dotted lines, one potential way to handle this kind of divergence is to extend the definition of merge to allow edges to be merged on both sides simultaneously X  X n this case, merging am and mentally in the English side, and hE (is) and mana (mind) on the Hindi side. 4.4 Operation breakdown by POS After performing the operations as seen in Sect. 4.2 , we can get further insight into what precisely is happening within each language by breaking down the operations by the POS tags on which the operations apply. Table 3 shows some of these POS tag breakdowns for a number of languages, and the frequency with which the given operation applies to the POS tag or POS tag pair out of all the times it is seen in that language. For example, the results in Row 1 shows that when a modal (MD) depends on a verb (VB) in English, 43.9 % of the time in the training data, the two words align to the same word in Hindi. Table 3 shows expected phenomena from the language pairs. For instance, Rows 5 and 6 show the English ! German pair merging many nouns as multiple English words are expressed as compounds in German. In another case, Row 8 shows that all Hindi nouns undergo swap with prepositions, as Hindi uses postpositions. Noticing the regularity with which NN and IN swap leads us to the next experiment, where we examine how such regularly-occurring rules might be harnessed to improve projection. 4.5 Analyzing trees for post-processing rules Table 4 compares the projection accuracy of the basic projection algorithm (System 1 as in Fig. 4 ) and the improved projection algorithm (System 2 as in Fig. 7 ). For all the experiments, we use tenfold cross validation with a 9:1 training/test split.
In this table, System 1 (S1) serves as a baseline. In System 1, when a merge alignment was detected or a spontaneous word needed reattaching, the algorithm simply attached rightward (S1-R) or leftward (S1-L). The results are in the last two rows of Table 4 b. The numbers in the table are Unlabeled Attachment Scores (UAS). In contrast, System 2 (S2) uses the training data to learn which direction the attachment is more common. In addition to the merge rules, System 2 can also apply rules for swap, or remove. These results are shown in the first three rows of Table 4 b. The table shows that applying automatically learned rules improves projection accuracy significantly, the average accuracy increasing from 83.22 % (S1-L) or 83.58 % (S1-R) to 88.95 % (S2-1). 4.6 Parsing experiments In Sect. 3.6 , we described two parsing approaches: System 3 and System 4. In System 3 (see Fig. 10 in Sect. 3.6 ), the MST parser uses the standard feature set without using Match/Aln features. It is trained with the projected trees produced by System 1, System 2, or gold standard F trees (As shown in the  X  X  X rain Source X  X  Column in Table 5 a. For the feature set, we can use either the word features only or add POS tag features in the F sentences (as indicated by the checkmark in the  X  X  X rojected POS Feat X  X  column). If only the word features are used, the input are F sents with words only. If POS tag features are used, for both training and test data, we use the POS tags projected from the English side. The parsing results are shown rows labeled S3-1 to S3-5 in Table 5 .
In System 4 (see Fig. 11 in Sect. 3.6 ), Match/Aln features are added to the standard feature set used by the MST parser. Standard features can be extracted from System 1, System 2, or gold F trees. The same is true for the Match/Aln features. The sources of the trees are indicated in the  X  X  X rain Sources X  X  column in Table 5 a. At the test stage, standard features are extracted from test F sentences with projected POS tags, and the source of the Match/Aln features is indicated in the  X  X  X est Source X  X  column. The  X  X  X racle X  X  system is the system in which Gold F trees are used both for training and testing. S4-1 through S4-3 are a few variants of System 4 when different combinations of train and test sources are used.

There are several observations: First, adding the Match/Aln features improves performance significantly, from 89.32 % in S4-1 versus 79.16 % in S3-1. Second, using the improved projection algorithm improves parsing results compared to using the basic projection algorithm for both System 3 (row S3-2 vs. S3-3 and S3-4) and System 4 (row S4-1 vs. S4-2 and S4-3). A further discussion and comparison of the average scores of the system can be found in Sect. 5.3 . 5 Discussion of results The results of the experiments above show that the match scoring that we have introduced here has the potential to address many interesting issues arising between language pairs. In this section, we highlight some observations based on the experimental results.
 5.1 Match scores The results of Table 2 a and b compare similarity both across languages and across corpora. For instance, in the scores for the baseline ODIN data, we see that the baseline for matches between English and German is the highest out of all the pairs at 76.7 %. Scots Gaelic and Welsh are 72 and 75.4 %, respectively. Hausa, Malagasy, Korean, and Yaqui all show baseline scores between 54 X 57 %. This seems in line with what we would expect, with German and the Celtic languages being closely related to English, and the others being unrelated.

Another stark contrast can be seen between all the languages in the ODIN data and the languages in the SMULTRON corpus. While the ODIN sentences tend to be short sentences used primarily for illustrative purposes, the SMULTRON corpus consists of economic, literary, and weather domains. As Table 1 shows, the SMULTRON sentences are much longer on average. A closer look at the SMULTRON translations also shows them to be much freer translations than those found in the ODIN data. While the size of the data sets used here are very small, and the ODIN IGT data may be biased towards illustrative purposes (described as the  X  X  X GT Bias X  X  in Lewis and highlights two types of differences among the corpora. First, by comparing baselines match results among comparable corpora, basic similarities between languages appear to pattern as expected. Second, the freer translations in the SMULTRON data appear with lower match scores across all instances.
 One final item of interest from the match results can be seen in the Hindi data in Table 2 a. Here, there appears to be a large increase in match percentage after the swap operation has been performed. As previously noted, knowing this is the inspiration for automatically inferring the swap rules in Sect. 4.5 . 5.2 POS breakdowns The breakdown of the operations by language and POS in Table 3 provides a good opportunity to check that the defined operations conform with expectations for specific languages.
 For instance, Row 1 in Table 3 shows Modals (MD) merging with a parent (VB). This is in line with instances such as Fig. 14 c where Hindi combines aspect with a verb that is typically expressed as a separate word in English. This does not appear to be a very frequent occurrence, however, as it only occurs for 42.9 % of MD ! VB dependencies.

Row 3, going from Hindi to English shows the case where auxiliary verbs VAUX merge with main verbs VM. These cases typically represent those where Hindi expresses tense as an auxiliary verb, whereas English tense is expressed by inflection on the verb.

With regard to spontaneous words in English and Hindi, Row 14 shows that 69.8 % of case markers (PSP) were removed from Hindi that were either absent in English or applied as inflections to the noun, while 86 % of determiners in English were removed, as Hindi does not have definite or indefinite determiners (Row 12).
Examining the English and German data in Table 3 , we first see in Row 5 that 66.7 % of NN-NNS dependencies in English merge. This, along with the 65.4 % of NN-NN dependencies merging, is something we would expect to see in German, as it compounds nouns with far more frequency than English. Interestingly, as Row 7 shows, a plural noun child never merges with a parent noun.

Finally, looking more closely at the swaps, we see a 100 % of NN ! IN dependencies are swapped in Hindi, giving further impetus for the rules as described in Sect. 4.5 . 5.3 Performance summary In this study, we proposed four systems for parsing F sentences: basic projection (System 1), improved projection (System 2), original MST parser trained with projected F trees (System 3), and MST parser with additional Match/Aln features (System 4). The results are in Tables 4 and 5 . A summary of the most important results of these tables and the comparison of the systems are in Table 6 a, and the error reduction of some system pairs are in Table 6 b.

As Table 6 b shows, in average across the eight languages, the reduction in error in using the parser trained with match and alignment features (S4-1) has a 48.76 % reduction in error over the parser trained on the gold standard trees with projected POS tags (S3-1). In the projection systems alone, the improved projection system S2-1 reduces error over the baseline system S1-R by 32.73 %. Finally, even without the Match/Aln features, a monolingual parser trained on projected dependency trees (S3-2) shows 19.27 % fewer errors than the system trained on the basic projections (S3-3).

While the improvement over the modified projection algorithm is modest, a dependency parser such as S4-1 does have the advantage of being more noise-robust. For instance, given an English sentence where the head of the English sentence is not aligned to any of the words in the language line, projection algorithms will not produce a tree structure, whereas the MST Parser will produce a tree structure based on other features when the alignment is not available. This advantage makes such an approach appealing for extending to larger corpora, which is something we will address in Sect. 6 . 5.4 Remaining issues Two large issues that our methodology faces are data sparsity and translation quality of the sentence pairs in the data sets. The former is somewhat inevitable given the task X  X  reasonable amount of annotated data is not always likely to exist for languages with scarce electronic resources, and guaranteeing coverage is difficult. As with the Hindi data, however, using IGT as a resource has convenience in both covering wide varieties of phenomena in a language, and providing a gloss that assists in creating word-level alignments. Creating dependency annotation on a small set of data from a source like ODIN (Lewis and Xia 2010 ) can get a lot of mileage with a small amount of investment.
Perhaps the more challenging issue is determining whether divergence in a language pair is caused by fundamental differences between the languages, or simply stylistic choices in translation. The latter of these scenarios appeared to be common in portions of the SMULTRON data, where translations appeared to be geared toward naturalness in the target language; in contrast, the translations in the Hindi guideline sentences were intended to be as literal as possible. Again, IGT provides a good possible solution, as such examples are often intended specifically for illustrative purposes. 6 Conclusion and future work In this paper, we have demonstrated a generalizable approach to detecting patterns of structural divergence across language pairs using simple tree operations based on word alignment. We have shown that this methodology can be used to detect similarities between languages on a coarse level, as well as serve as a general measure of similarity between dependency corpora. Finally, we establish that harnessing these detection methods improves standard projection algorithms and informs dependency parsing with little to no expert involvement.

For future work, we plan to focus on two areas. The first is that of adapting these techniques to larger data sets. In particular, use of the high-quality alignments derived from IGT to bootstrap a statistical aligner may allow for reasonable performance on languages for which the amount of parallel data may not be sufficient for building a high-quality statistical word aligner. Secondly, while this paper explores the utility of IGT in terms of word alignment and projection, we are currently looking into the ways in which the additional morphemic and lexicosyn-tactic information in the gloss lines may be used to perform more complex automated linguistic analysis.

The techniques described here are promising for maximizing the effectiveness of existing resources such as IGT for languages where such resources are limited. While access to electronic resources continues to increase globally, many of these resource-poor languages are still left behind in terms of NLP tools. Though projection techniques may not ultimately be full replacements for large treebank projects, the ability of these techniques to be rapidly deployed is extremely useful for researchers seeking to experiment with new languages at minimal cost. References
