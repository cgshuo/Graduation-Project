 Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often ove rwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspec tive toward the problem, based on reinforcement learning. Unlik e other recommender systems, our system does not use the sta tic patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing con cepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly inte racts with the user and learns from her behavior. Our experime ntal evaluations support our claims and demonstrate how th is approach can improve the quality of web recommendations. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information Filtering. I.2.6 [ Artificial Intelligence ]: Learning. H.2.8 [ Database Management ]: Applications  X  Data mining. Algorithms, Performance, De sign, Experimentation Recommender systems, Personalization, Machine Learning, Reinforcement Learning, Web Usage Mining The amount of information availa ble on-line is increasing rapidly with the explosive growth of the World Wide Web and the advent of e-Commerce. Although this surely provides users with more options, at the same time makes it more difficult to find the  X  X ight X  or  X  X nteresting X  information from this great pool of information, the problem co mmonly known as information overload. To address these probl ems, recommender systems have been introduced [14]. They can be defined as the personalized information technology used to predict a user evaluation of a particular item [3] or more gene rally as any system that guides users toward interesting or useful objects in a large space of possible options [1]. Recommender systems have been us ed in various applications ranging from predicting the products a customer is likely to buy [16], movies, music or news that mi ght interest the user [8,22] and web pages that the user is likely to seek[2,4,7,11], which is also the focus of this paper. Web page recommendation is considered a user modeling or web personalizati on task. One research area that has recently contributed greatly to this problem is web mining. Most of the systems developed in this field are based on web usage mining [17] which is the process of applying data mining techniques to the discovery of usage patterns form web data. These systems are mainly concerned with analyzing web usage logs, discovering patterns fro m this data and making recommendations based on the ex tracted knowledge [4,11,15,21]. One important characteristic of these systems is that unlike traditional recommender systems, which mainly base their decisions on user ratings on diffe rent items or other explicit feedbacks provided by the user [3,6] these techniques discover user preferences from their implicit feedbacks, namely the web pages they have visited. More recently, systems that take advantage of a combination of content, usage and even structure information of the web have been introduced [9,12,13] and shown superior results in the web page recommendation problem. We propose a different machine l earning perspective toward the problem, which we believe is suita ble to the nature of web page recommendation problem and has so me intrinsic advantages over previous methods. Our system makes recommendations primarily based on web usage logs. We m odel the recommendation process as a Reinforcement Learning problem (RL) [20] or more specifically a Q-Learning problem. For this purpose we devise state and action definitions and rewarding policies, considering common concepts and techniques used in the web usage mining domain. Then we train the system using web usage logs available as the training set. During the training, the system learns to make recommendations; this is somehow different from the previous methods in which the purpose wa s to find explicit and static patterns or rules from the data. We X  X l explain this matter further in the coming sections. The choice of reinforcement learning was due to several reasons: It seems a ppropriate for the nature of web page recommendation problem as is discussed in section 3 and as evaluation results show; Due to the characteristics of this type of learning and the fact that we are not making decisions explicitly from the patterns discovered from th e data, it provides us with a system which is constantly in the learning process; Does not need periodic updates; can easily adapt itself to changes in website structure and content and new trends in user behavior. The organization of the paper is as follows: in section 2 we overview the related work done in recommender systems, focusing more on recent systems and on the application of reinforcement learning in these systems. We introduce our solution including modeling the problem as a Q-Learning one and the training procedure in section 3. We evaluate the proposed system in section 4. The conclusi on of the paper comes in section 5 along with some recommendations for future work. Recommender systems have been developed using various approaches and can be categorized in various ways [1]. Collaborative techniques [6] are the most successful and the most widely used techniques employed in these systems [3,8,21]. Recently, Web mining and especially web usage mining techniques have been used wide ly in web recommender systems [2,4,11,21,12]. Common approach in these systems is to extract navigational patterns from usage da ta by data mining techniques such as association rules and clustering, and making recommendations based on them. These approaches differ fundamentally from our method in which no static pattern is extracted from data. RL has been previously used for recommendations in several applications. WebWatcher [7], e xploits Q-Learning to guide users to their desired pages. Pages co rrespond to states and hyperlinks to actions, rewards are computed based on the similarity of the page content and user profile ke ywords. In most other systems reinforcement learning is used to reflect user feedback and update current state of recommendati ons. A general framework is presented in [5], which consists of a database of recommendations generated by various models a nd a learning module that updates the weight of each recommendation by user feedback. In [18] a travel recommendation agent is introduced which considers various attributes for trips and customers, computes each trip X  X  value with a linear function and updates function coefficients after receiving each user feedback. RL is used for information filtering in [22] which maintains a profile for each user containing keywords of interests and update s each word X  X  weight according to the implicit and explicit feedbacks received from the user. In [16] the recommendation problem is modeled as an MDP. The system X  X  states correspond to use r X  X  previous purchases, rewards are based on the profit achieved by selling the items and the recommendations are made using the theory of MDP and their novel state-transition function. To the best of our knowledge our method differs from previous work, as none of them used reinforcement learning to train a system in making web site recommendations merely from web usage data . The specific problem which our syst em is supposed to solve, can be summarized as follows: the syst em has, as input data, the log file of users X  past visits to th e website, these log files are assumed to be in any standard log format, containing records each with a user ID, the sequence of pages the user visited during a session and typically the time of each page request. A user enters our website and begins requesting we b pages. Considering the pages this user has requested so far the system has to predict in what other pages the user is probabl y interested and recommend them to her. Table 1 illustrates a sample scenario. Predictions are considered successful if the user chooses to visit those pages in the remaining of that session, e.g. page c recommended in the first step in table 1. Obviously the goal of the system would be to make the most successful recommendations. Table 1: A sample user session and system recommendations Visited Navigation System 
Prediction {c,g} {d,m} {e,d} {s,r} {f,b} {h} Reinforcement learning [20] is primarily known in machine learning research as a framework in which agents learn to choose the optimal action in each situation or state they are in. The agent is supposed to be in a specific state s , in each step it performs some action and transits to another state. After each transition the agent receives a reward R(s) . The goal of the agent is to learn which actions to perform in each state to receive the greatest accumulative reward, in its path to the goal state. The set of actions chosen in each state is called the agent X  X  policy. One variation of this method is Q-Learning in which the agent does not compute explicit values for each state and instead computes a value function Q(s,a) which indicates value of performing action a in state s. Formally the value of Q(s,a) is the discounted sum of future rewards that will be obtained by doing action a in s and subsequently choosing optimal ac tions. In order to solve the problem with Q-Learning we need to make appropriate definitions for our states and actions, consider a reward function suiting the problem and devise a procedure to train the system using web logs available to us. In order to better represent our approach toward the problem we try to use the notion of a game. In a typical scenario a web user visits pages sequentially from a we b site, let X  X  say the sequence a user u requested is composed of pages a, b, c and d . Each page the user requests can be considered a step or move in our game. make a move. The system X  X  purpos e is to predict user X  X  next move(s) with the knowledge of his previous moves. Whenever the user makes a move (requests a page), if the system has previously predicted the move, it will receive positive points and otherwise it will receive none or negative points. For example predicting a above example yields in positive points for the system. The ultimate goal of the system would be to gather as much points as possible during a game or actually during a user visit from the web site. Some important issues can be inferred from this simple analogy: first of all, we can see the problem certainly has a stochastic nature and like most games, the next state cannot be computed deterministically from our current state and the action the system performs due to the fact that the user can choose from a great number of moves. This must be considered in our learning algorithm and our update rules for Q values; the second issue is what the system actions should be, as they are what we ultimately expect the system to perform. Actions will be prediction or recommendation of web pages by the system in each state. Regarding the information each state must contain, by considering our definition of actions, we can deduct that each state should at least show the history of pages vis ited by the user so far. This way we X  X l have the least information needed to make the recommendations. This analogy also determines the basics of rewarding policy. In its simplest form it shall consider that an action should be rewarded positively if it recommends a page that will be visited in one of the c onsequent states, of course not necessarily the immediate next state. One last issue which is worth noting about the analogy is that it cannot be categorized as a typical 2-player game in which opponents try to defeat each other, as in this game clearly the user has no intention to mislead the system and prevent the system from gathering points. It might be more suitable to consider the problem as a competition for different recommender systems to gather more points, than a 2-player game. Because of this intrinsic difference, we cannot use self-play, a typical technique used in training RL systems [20], to train our system and we need the actual web usage data for training. Considering the above observations we begin the definitions. We tend to keep our states as simple as possible, at least in order to keep their number manageable. Regarding the states, we can see keeping only the user trail can be insufficient. With that definition it won X  X  be possible to reflect the effect of an action a performed in state S i , in any consequent state S i+n where n&gt;1 . This means the system would only learn actions th at predict the immediate next page which is not the purpose of our system. Another issue we should take into account is the number of possible states: if we allow the states to contain any given sequence of page visits clearly we X  X l be potentially faced by an infinite number of states. What we chose to do was to limit the page visit sequences to a constant number. For this purpose we adopted the notion of N-Grams which is commonly applie d in similar personalization systems based on web usage mining [11,12,19]. In this model we put a sliding window of size w on us er X  X  page visits, resulting in states containing only the last w pages requested by the user. The assumption behind this model is that knowing only the last w page visits of the user, gives us enough information to predict his future page requests. The same problem rises when considering the recommended pages X  sequence in the states, for which we take the same approach of considering w' last recommendations. Regarding the actions, we chose si mplicity. Our action consists of a single page recommendation in each state. Considering multiple page recommendations might have shown us the effect of the combination of recommended pages on the user, in the expense of making our state space and rewarding policy much more complicated. The corresponding stat es and actions of the user session of Table 1 are presented in Figure 1 (straight arrows represent the actions performed in each state). The basis of reinforcement learning lies in the rewards the agent receives, and how it updates state a nd action values. As with most stochastic environments, we shoul d reward the actions performed in each state with respect to the consequent state resulted both from the agent X  X  action and other factor X  X  in the environment on which we might not have control. These consequent states are sometimes called the after-states [20] . Here this factor is the page the user actually chooses to vis it. We certainly do not have a predetermined function R(s,a) or even a state transition function current state s and performed action a. state and more specifically on the intersection of previously recommended pages in each state and current page sequence of the state. If we consider each state s consists of two sequences V, R indicating the sequence of visited and previously recommended pages respectively: indicates the i th recommended page in the state s . Reward for next state. One tricky issue worth considering is that though tempting, we should not base on rewards on will cause extra credit for a singl e correct move. Considering the above example a recommendation of page b in the first state shall be rewarded only in the transiti on to the second state where user goes to page b, while it will also be present in our recommendation list in the third state. To avoid this, we simply consider only the occurrence of the last page visited in the recommended pages list in state s  X  to reward the action performed in the previous sate s . To complete our rewarding procedure we take into account co mmon metrics used in web page recommender systems. One issue is considering when the page was predicted by the system and when the user actually visited the page. According to the goal of the system this might influence our rewarding. If we consider s hortening user navigation as a sign of successful guidance of user to his required information, as is the most common case in recommender systems [11,9] we should consider a greater reward for pages predicted sooner in the user X  X  navigation path and vice vers a. Another factor commonly considered in theses systems [22,11,17] is the time the user spends on a page, assuming the more time the user spends on a page the more interested he pr obably has been in that page. Taking this into account we s hould reward a successful page recommendation in accordance with the time the user spends on the page. The rewarding can be summarized as follows:  X  Assume s a s  X  = ) , (  X   X   X  If p  X   X  recommended pages list and Time(p w v ) indicates the time user has spent on the last page of the stat e. Here Reward is the function combining these values to calculate r(s,a) . We chose a simple linear combination of th ese values as follows: factor according to the maximum values dist and time can take. why reinforcement learning might be a good candidate for the recommendation problem: it does not rely on any previous assumptions regarding the probability distribution of visiting a page after having visited a seque nce of pages, which makes it general enough for diverse usage pa tterns as this distribution can take different shapes for different sequences. the nature of the problem matches perfectly with th e notion of delayed reward or what is commonly known as tempor al difference. The value of performing an action/recommendation might not be revealed to us in the immediate next state and sequence of actions might have led to a successful recommendation for which we must credit rewards. What the system learns is directly what it should perform, though it is possible to extract rules from the learned policy model, its decisions are not based on explicitly extracted rules or patterns from the data. One issue commonly faced in systems based on patterns extracted from training data is the need to periodically update these patterns in order to make sure they still reflect the trends residing in user behavior or the changes of the site structure or content. With reinforcement learning the system is intrinsically learning even when performing in real world, as the recommendations are the actions the system performs, and it is commonplace for the learning procedure to take place during the interaction of system with its environment . We chose Q-Learning as our learning algorithm. This method is primarily concerned with estima ting an evaluation of performing specific actions in each state, know n as Q-values. In this setting we are not concerned with evaluating each state in the sense of the accumulative rewards reachable from this state, which with respect to our system X  X  goal can be useful only if we can estimate the probability of visiting the following states by performing each action. On the other hand Q-Learni ng provides us with a structure that can be used directly in the recommendation problem, as recommendations in fact are the actions and the value of each recommendation/action shows an estimation of how successful that prediction can be. Another decision is the update rule for Q values. Because of the non-determin istic nature of this problem we use the following update rule [20]: With This rule takes into account the fact that doing the same action can yield different rewards each time it is performed in the same converge and decreases the impact of changing reward values as the training continues. What remains about the training phase is how we actually train the system using web usage logs available. As mentioned before these logs consist of previous user sessions in the web site. Comparing to the analogy of the ga me they can be considered as a set of opponent X  X  previous game s and the moves he tends to make. We are actually provided with a set of actual episodes occurred in the environment, of c ourse with the difference that no recommendations were actually made during these episodes. The training process can be summarized as the following:  X  initial values of Q(s,a) for each pair s,a are set to zero  X  Repeat until convergence The Choice of  X  -greedy action selection is quite important for this specific problem as the exploration especially in the beginning phases of training, is vital. The Q values will converge if each episode, or more precisely each state-action pair is visited infinitely. In our implementation of the problem convergence was reached after a few thousand (between 3000 and 5000) visits of each episode. This definition of the learning algorithm completely follows a TD(0) off-policy learning procedure, as we take an estimation of future reward accessible from each state after performing each action by considering the maximum Q value in the next state. The last modification we experi mented was changing our reward function. We noticed as we put a sliding window on our sequence of previously recommended pages, practically we had limited the After training the system using th is definition, the system was mostly successful in recommending pages visited around w' st eps ahead. Although this might be quite acceptable while choosing an appropriate value for w' , it tends to limit system X  X  prediction ability as large numbers of w' make our state space enormous. To overcome this problem we devised a rather simple modification in our reward function: what we needed was to reward recommendation of a page if it is likely to be visited an unknown number of states ahead. Fortunately our definition of states and actions gives us just the informa tion we need and ironically this information is stored in Q values of each state. The basic idea is that when an action/recommendation is appropriate in state S indicating the recommended page is likely to occur in the following states, it should also be considered appropriate in state S and the actions in that state that frequently lead to S Following this recursive procedure we can propagate the value of performing a specific action beyond the limits imposed by w' . This change is easily reflected in our learning system by considering value of Q(s',a) in computation of r(s,a) with a coefficient like  X  . It should be taken into account that the effect of this modification in our reward function must certainly be limited as in its most extreme case where we only take this next Q value into account we X  X e practically encouraging recommendation of pages that tend to occur mostly in the end of user sessions. We evaluated system performance in the different settings described above. We used simulate d log files generated by a web traffic simulator [10] to tune our rewarding functions. The log files were simulated for a webs ite containing 700 web pages. We pruned user sessions with a le ngth smaller than 5 and were provided with 16000 user sessions with average length of eight. As our evaluation data set we us ed the web logs of the Depaul University website, made available by the author of [12]. This dataset contains 13745 sessions a nd 687 pages. 70% of the data set was used as the training set and the remaining was used to test the system. For our evaluation we presented each user session to the system, and recorded the recommendations it made after seeing each page the user had visited. The system was allowed to make r recommendations in each step with r&lt;10 and recommendations is adopted from [9]. To evaluate the recommendations we use the metrics presented in [9] because of the similarity of the settings in both systems and the fact that we believe these metrics can reveal the true performance of the system more clearly than simpler metrics. Recommendation Accuracy and Coverage are two metrics quite similar to the precision and recall metrics commonly used in information retrieval literature. Recommendation accuracy measur es the ratio of correct recommendations among all recommendations, where correct recommendations are the ones that appear in the remaining of the session s after considering each page p the system generates a set of recommendations R(p) . To compute the accuracy, R(p) is compared with the rest of the session T(p) as follows: Recommendation coverage on the ot her hand shows the ratio of the pages in the user session that the system is able to predict before the user visits them: As is the case with precision and recall, these metrics can be useful indicators of the system performance only when used in accordance to each other and lose their credibility when used individually. As an example, consider a system that recommends all the pages in each step, this system will gain 100% coverage, of course in the price of very low accuracy. Another metric used for evaluati on is called the shortcut gain which measures how many page-v isits users can save if they follow the recommendations. If we call the shortened session S' , the shortcut gain for each se ssion is measured as follows: In the first set of experiments we tested the effect of different decisions regarding state definiti on, rewarding function, and the learning algorithm on the system behavior. Afterwards we compared the system performance to the other common techniques used in recommendation systems. In our state definition, we used the notion of N-Grams by putting a sliding window on user navigati on paths. The implication of using a sliding window of size w is that we base the prediction of user future visits on his w past visits. The choice of this sliding window size can affect the system in several ways. A large sliding window seems to provide the system a longer memory while on the other hand causing a larger state space with sequences that occur less frequently in the usage logs. We trained our system with different window sizes on user trail and evaluated its performance as seen in Figure 3. In these experiments we used a fixed window size of 3 on recommendation history. As our experiments show the best results are achieved when using a window of size 3. It can be inferred form this diagram that a window of size 1 which considers onl y the user X  X  last page visit does not hold enough information in memory to make the recommendation, the accuracy of recommendations improve with increasing the window size and the best results are achieved with a window size of 3. Using a window size larger than 3 results in weaker performance, it seems to be due to the fact that, as mentioned above, in these models , states contain sequences of page visits that occur less freque ntly in web usage logs, causing the system to make decisions based on weaker evidence. In our evaluation of the short cut gain th ere was a slight difference when using different window sizes. In the next step we performed si milar experiments, this time using a constant sliding window of size 3 on user trail and changing size of active window on recommendations history. As this window size was increased, rather intere sting results was achieved as shown in Figure 4. In evaluating system accuracy, we observed improvement up to a window of size 3, after that increasing the window size caused no improvement while resulting in larger number of states. This increase in the number of states is more intense than when the window size on user trail was increas ed. This is manly due to the fact that the system is exploring and makes any combination of recommendations to learn the good ones. The model consisting of this great number of states is in no way efficient, as in our experiments on the test data onl y 25% of these states were actually visited. In the sense of s hortcut gain the system achieved, it was observed that shortcut gain increased almost constantly with increase in window size, wh ich seems a natural consequence as described in section 3. Next we changed the effect of parameters constituting our reward function. First we began by not considering the Dist parameter, described in section 3, in our re wards. We gradually increased it X  X  coefficient in steps of 5% and r ecorded the results as shown in Table 2. These results show that increasing the impact of this parameter in our rewards up to 15% of total reward can result both in higher accuracy and higher shortcut gain. Using values greater than 15% has a slight negative effect on accuracy with a slight positive effect on shortcut gain and keeping it almost constant. This seems a natural consequence since although we X  X e paying more attention to pages that tend to appear later in the user sessions, the system X  X  vision into the future is bounded by the size of window on recommendations. This limited vision also explains why our accuracy is not decreasing as expected.
 The next set of experiments tested system performance with the reward function that considers next state Q-value of each action in rewarding the action performed in th e previous state, as described in section 3. We began by increasi ng the coefficient of this factor (  X  ) in the reward function the same way we did for the Dist parameter as shown in Table 1. In the beginning increasing this value, lead to higher accuracy and shortcut gains. After reaching an upper bound, the accuracy began to drop. In these settings, recommendations w ith higher values were those targeted toward the pages that occurred more frequently in the end of user sessions. These recommended pages, if recommended correctly, were only successful in predicting the last few pages in the user sessions. As expected, s hortcut gain increased steadily recommendations became so inaccurate that rarely happened anywhere in the user sessions. 
Table 2: System performance with varying  X  in the reward 
Table 3: System performance with next state Q-Values in the Finally we observed our system performance in comparison with two other methods: association ru les, one of the most common approaches in web mining based recommender systems [2,11,17,15], and collaborative filtering which is commonly known as one of the most successful approaches for recommendations. We chose item-based collaborative filtering with probabilistic similarity measure [3], as a baseline for comparison because of the promising results it had shown. In Figure 5 you can see the performance of these systems in the sense of their accuracy and shortc ut gain in different coverage values. At lower coverage values we can see although our system still has superior results especially over association rules, accuracy and shortcut gain values are rather close. As the coverage increases, naturally accuracy decreases in all systems, but our system gains much better results than the other two systems. It can be seen the rate in which accuracy decreases in our system is lower than other two systems; at lower coverage values where the systems made their most promising recommendati ons (those with higher values), pages recommended were mostly th e next immediate page and as can be seen had an acceptable accuracy. At lower coverage rates, where recommendations with lower values had to be made our system began recommending pages occurring in the session some steps ahead, while the other approaches also achieved greater shortcut gains, as the results show their lower valued recommendations were not as accurate and their performance declined more intensely. 
Figure 5: Comparing our syst em X  X  performance with two We presented a new web page recommendation system based on reinforcement learning in this paper. This system learns to make recommendations from web usage data as the actions it performs rather than discovering explicit patterns from the data and inherits the intrinsic characteristic of reinforcement learning which is being in a constant learning process. We modeled web page recommendation as a Q-Learning problem and trained the system with common web usage logs. System performance was evaluated under different settings and in comparison with other methods. Our experiments showed promising results achieved by exploiting reinforcement learning in web recommendation based on web usage logs. However, there are other alternatives that can potentially improve the system and constitute our future work. Regarding our evaluation, we tend to use an on-line evaluation of users instead of the off-line method based on web usage logs. In the case of the reward function used, various implicit feedbacks from the user rather than just the fact that the user had visited the page can be used, such as thos e proposed in [22]. Another option is using a more complicated reward function rather than the linear combination of factors; a learning structure such as neural networks is an alternative. Finally , in a broader sense, instead of making recommendations merely on web usage, it can be beneficial to take into account evidence from other sources of information, such as web content and structure, as has been the trend in recent web recommendation systems. [1] Burke, R. Hybrid recommender systems: Survey and [2] Cooley, R., Tan, P. N., Srivastava, J. WebSIFT: The Web [3] Deshpande, M., Karypis, G. Item-based top-N [4] Fu, X., Budzik, J., Hammond, K. J. Mining navigation [5] Golovin, N., Rahm, E. Reinforcement Learning Architecture [6] Herlocker, J., Konstan, J., Brochers, A., Riedel, J. An [7] Joachims, T., Freitag, D., Mitchell, T. M. WebWatcher: A [8] Konstan, J., Riedl, J., Borchers, A., Herlocker, J. [9] Li, J., Zaiane, O. R. Combining Usage, Content and [10] Liu, J., Zhang, S., Yang, J. Characterizing Web usage [11] Mobasher, B., Cooley, R., Srivastava, J. Automatic [12] Mobasher, B., Dai, H., Luo, T., Sun, Y., Zhu, J. Integrating [13] Nakagawa M., Mobasher, B. A Hybrid Web Personalization [14] Resnick, P., Varian, H.R. Recommender Systems. [15] Shahabi, C., M. Zarkesh, A., Abidi, J., Shah, V. Knowledge [16] Shany, G., Heckerman, D., Barfman, R. An MDP-Based [17] Srivastava, J., Cooley, R., De shpande, M., Tan, P.N. Web [18] Srivihok, A., Sukonmanee, V. E-commerce intelligent agent: [19] Su, Z., Yang, Q., Lu, Y., Zhang, H. What next: A prediction [20] Sutton, R.S., Barto, A.G. Reinforcement Learning: An [21] Wasfi, A. M. Collecting User Access Patterns for Building [22] Zhang, B., Seo, Y. Personalized web-document filtering 
