 Co-clustering is based on the duality between data points (e.g. documents) and features (e.g. words), i.e. data points can be grouped based on their distribution on features, while features can be grouped based on their distribution on the data points. In the past decade, several co-clustering algo-rithms have been proposed and shown to be superior to tra-ditional one-side clustering. However, existing co-clustering algorithms fail to consider the geometric structure in the data, which is essential for clustering data on manifold. To address this problem, in this paper, we propose a Dual Regularized Co-Clustering (DRCC) method based on semi-nonnegative matrix tri-factorization. We deem that not only the data points, but also the features are sampled from some manifolds, namely data manifold and feature manifold re-spectively. As a result, we construct two graphs, i.e. data graph and feature graph, to explore the geometric struc-ture of data manifold and feature manifold. Then our co-clustering method is formulated as semi-nonnegative matrix tri-factorization with two graph regularizers, requiring that the cluster labels of data points are smooth with respect to the data manifold, while the cluster labels of features are smooth with respect to the feature manifold. We will show that DRCC can be solved via alternating minimization, and its convergence is theoretically guaranteed. Experiments of clustering on many benchmark data sets demonstrate that the proposed method outperforms many state of the art clus-tering methods.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recognition ]: Clustering Algorithms, Experimentations Co-clustering, Data manifold, Feature manifold, Graph reg-ularization, Semi-nonnegative matrix tri-factorization
Clustering is one of the most fundamental topics in un-supervised machine learning and has been widely applied in data mining, computer vision, biology and so on. From a traditional view, clustering aims to divide the unlabeled data set into groups of similar data points. From a geometrical view, a data set can be seen as a set of discrete samplings on continuous manifold, and clustering aims at finding intrinsic structures of the manifold.

Many clustering methods have been proposed up to now, e.g. Kmeans [1], spectral clustering [21] [18] [15] and Non-negative Matrix Factorization (NMF) [13] [23]. It is worth noting there is close connection between Kmeans, spectral clustering and NMF [24] [7] [10] [14].

However, the methods mentioned above focus on one-side clustering, i.e. clustering the data side based on the similari-ties along the feature side. Motivated by the duality between data points (e.g. documents) and features (e.g. words), i.e. data points can be grouped based on their distribution on features, while features can be grouped based on their distribution on the data points, several co-clustering algo-rithms have been proposed in the past decade and shown to be superior to traditional one-side clustering. For instance, [6] proposed a bipartite spectral graph partition approach to co-cluster words and documents. However, it requires that each document cluster is associated with a word clus-ter, which is a very tough restriction. [8] proposed an infor-mation theoretic co-clustering algorithm, which can be seen as the extension of information bottleneck method [20] to two-side clustering. [11] proposed an orthogonal nonnega-tive matrix tri-factorization (ONMTF) to co-cluster words and documents, which owns an elegant mathematical form and encouraging performance.

Recent studies show that many real world data are ac-tually sampled from a nonlinear low dimensional manifold which is embedded in the high dimensional ambient space [17] [16]. Yet existing co-clustering algorithms [6] [8] [11] fail to consider the geometric structure in the data which is essential for clustering data on manifold. This greatly limits the application of co-clustering for the data lying on manifold.
 To address this problem, in this paper, we propose a Dual Regularized Co-Clustering (DRCC) method based on semi-nonnegative matrix tri-factorization, which inherits the ad-vantages of ONMTF [11]. We deem that not only the data points but also the features are discrete samplings from some manifolds, namely data manifold and feature manifold re-spectively. Thus, we construct two graphs, i.e. data graph and feature graph, to explore the geometric structure of data manifold as well as feature manifold. We require that the cluster labels of data points are smooth with respect to the intrinsic data manifold, while the cluster labels of features are smooth with respect to the intrinsic feature manifold. This is achieved by graph regularization. Then DRCC is for-mulated as semi-nonnegative matrix tri-factorization with two graph regularizers. As a result, DRCC takes into ac-count the geometric information of the data points and fea-tures, and is suitable for clustering data on manifold. We will show that DRCC can be optimized by iterative mul-tiplicative updating algorithm and its convergence is the-oretically guaranteed. Experiments of clustering on many benchmark data sets demonstrate that the proposed method outperforms many state of the art clustering methods. The remainder of this paper is organized as follows. In Section 2 we will propose dual regularized co-clustering (DRCC) method, along with the optimization algorithm, followed with the proof of the convergence of the algorithm. In Sec-tion 3, we discuss several related works. The experiments on benchmark data sets are demonstrated in Section 4. Fi-nally, we draw a conclusion and point out the future work in Section 5.
In this section, we first briefly introduce the formulation of co-clustering, and some notations frequently used in this paper. Then we present the graph regularization on both the data side and the feature side, followed which we present the dual regularized co-clustering (DRCC) method and its optimization algorithm. Finally, we prove the convergence of the algorithm. In the setting of co-clustering, we are given a data set X = { x  X  1 , . . . , x  X  n }  X  R d . The goal is to group the data points { x  X  1 , . . . , x  X  n } into c clusters {C j } c the features { x 1  X  , . . . , x d  X  } into m clusters {W
We use a partition matrix F  X  X  0 , 1 } n  X  c to represent the clustering result of data points, such that F ij = 1 if x longs to cluster C j and F ij = 0 otherwise. This is also known as hard clustering, i.e. the cluster assignment is binary. Sim-ilarly, we use another partition matrix G  X  { 0 , 1 } d  X  m represent the clustering result of features.

For convenience, we present in Table 1 the important no-tations used in the rest of this paper.
As we have mentioned above, recent researches show that many real world data distribute on low-dimensional mani-fold embedded in the high-dimensional ambient space [17] [16]. However, existing co-clustering algorithms [6] [8] [11] fail to consider the geometric structure which is essential for clustering data on manifold. A natural treatment for the data sampled from a manifold is to construct a graph to dis-cretely approximate the manifold, whose vertices correspond to the data samples, while the edge weight represents the affinity between the data points. One common assumption Table 1: Important notations used in this paper.
 about the affinity between data points is Cluster Assump-tion [4], which says if two samples are close to each other in the input space, then their labels (or embeddings) are also close to each other. This assumption has been widely used in spectral clustering [21] [18] [15], dimensionality reduction [16] [12] and semi-supervised learning [4] [25]. Furthermore, we deem that not only the data points are sampled from a manifold, namely data manifold, but also from the dual view, the features are discrete samplings from another mani-fold, namely feature manifold. As a result, we construct two graphs, i.e. data graph and feature graph, to explore the geometric structure of data manifold and feature manifold. In the following, we will introduce the construction of data graph and feature graph respectively.
We construct a data graph G F whose vertices correspond points x  X  i and x  X  j are close to each other, then their cluster labels f i  X  and f j  X  should be close as well. This is formulated as follows, where W F ij is the affinity measuring how close f i  X  and f be.

For simplicity, we define the data affinity matrix W F as follows, where N ( x  X  i ) denotes the k -nearest neighbor of x  X  i the advantage that there is no parameter to be tuned except the neighborhood size, i.e. k . Other kinds of affinity can also be adopted, e.g. heat kernel [12].
Eq.(1) can be further rewritten as where D F ii = L
F = D F  X  W F is the graph Laplacian [5] of the data graph G F . Eq.(3) reflects the label smoothness of the data points. The smoother the data labels are with respect to the underlying data manifold, the smaller the value of the data graph regularization in Eq.(3) will be.
Similar with the construction of the data graph G F , we construct a feature graph G G whose vertices correspond to { x 1  X  , . . . , x d  X  } . According to Cluster Assumption again, if features x i  X  and x j  X  are near, then their cluster labels g and g j  X  should be near as well. This is formulated as follows where W G ij is the affinity measuring how close g i  X  and g will be.
 For simplicity, we also define the feature affinity matrix W G as follows, where N ( x i  X  ) denotes the k -nearest neighbor of x i  X 
Eq.(4) can be further rewritten as where D G ii = W
G is the graph Laplacian of the feature graph G G . Eq.(6) reflects the label smoothness of the features. The smoother the feature labels are with respect to the underlying feature manifold, the smaller the value of the feature graph regular-ization in Eq.(6) will be.
Based on the two graph regularizers presented in Eq.(3) and Eq.(6), we propose a new co-clustering method, mini-mizing the following objective, J DRCC = || X  X  GSF T || 2 F +  X  tr( F T L F F ) +  X  tr( G T where  X ,  X   X  0 are regularization parameters balancing the reconstruction error of co-clustering in the first term and the label smoothness of the data points and features in the second and third terms. Since there are two graph regu-larizers in the objective, we call Eq.(7) Dual Regularized Co-Clustering (DRCC). When letting  X  =  X  = 0, DRCC degenerates to ordinary co-clustering method.

By its definition, the elements in F and G can only take binary values, which makes the minimization in Eq.(7) very difficult, therefore we relax F and G into continuous nonneg-ative domain. Then DRCC in Eq.(7) turns out to minimize, J DRCC = || X  X  GSF T || 2 F +  X  tr( F T L F F ) +  X  tr( G T where S is a matrix whose entries can take any signs. Note that Eq.(8) is a Dual Regularized Semi-Nonnegative Matrix Tri-Factorization (DRSNMTF). To make the objective in Eq.(8) lower bounded, we use L 2 normalization on columns of F and G in the optimization, and compensate the norms of F and G to S .
In the following, we will give the solution to Eq.(8). As we see, minimizing Eq.(8) is with respect to S , F and G , and we cannot give a closed-form solution. We will present an alternating scheme to optimize the objective. In other words, we will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.
Optimizing Eq.(8) with respect to S is equivalent to opti-mizing Setting  X  X  1  X  S = 0 leads to the following updating formula
Optimizing Eq.(8) with respect to F is equivalent to op-timizing
For the constraint F  X  0, we cannot get a closed-form solution of F . In the following, we will present an itera-tive multiplicative updating solution. We introduce the La-grangian multiplier  X   X  R n  X  c , thus the Lagrangian function is L ( F ) = || X  X  GSF T || 2 F +  X  tr( F T L F F )  X  tr(  X  F where A = X T GS and B = S T G T GS .

Using the Karush-Kuhn-Tucker condition [2]  X  ij F ij = 0, we get Introduce L F = L + F  X  L  X  F , A = A +  X  A  X  and B = B + where A + ij = ( | A ij | + A ij ) / 2 and A  X  ij = ( | A we obtain [  X  L + F F  X   X  L  X  F F  X  A + + A  X  + FB +  X  FB  X  ] ij F ij Eq.(15) leads to the following updating formula
Optimizing Eq.(8) with respect to G is equivalent to op-timizing
Similar with the computation of F , since G  X  0, we in-troduce the Lagrangian multiplier  X   X  R d  X  m , thus the La-grangian function is L ( G ) = || X  X  GSF T || 2 F +  X  tr( G T L G G )  X  tr(  X  G where P = XFS T and Q = SF T FS T .

Using the Karush-Kuhn-Tucker complementarity condi-tion [2]  X  ij G ij = 0, we get
Introduce L G = L + G  X  L  X  G , P = P +  X  P  X  and Q = Q + Q  X  , we obtain [  X  L + G G  X   X  L  X  G G  X  P + + P  X  + GQ +  X  GQ  X  ] ij G ij Eq.(21) leads to the following updating formula
In summary, we present the iterative multiplicative up-dating algorithm of optimizing Eq.(8) in Algorithm 1. Algorithm 1 Dual Regularized Co-Clustering
Input:X , the number of data clusters c , the number of feature clusters m , regularization parameters  X ,  X  , maxi-mum number of iterations T ; Output: Partitions F  X  R n  X  c ;
Initialize F and G using K-means; while not convergent and t  X  T do end while
In this section, we will investigate the convergence of Al-gorithm 1.

We use the auxiliary function approach [13] to prove the convergence of the algorithm. Here we first introduce the definition of auxiliary function [13].
 Definition 2.1 [13] Z ( h, h 0 ) is an auxiliary function for F ( h ) if the conditions are satisfied.
 Lemma 2.2 [13] If Z is an auxiliary function for F , then F is non-increasing under the update Lemma 2.3 [9] For any nonnegative matrices A  X  R n  X  n , B  X  R k  X  k , S  X  R n  X  k , S 0  X  R n  X  k , and A , B are symmetric, then the following inequality holds
In the following, we will present 4 theorems, which guar-antee the convergence of Algorithm 1.
 Theorem 2.4 Let Then the following function =  X   X  2 + is an auxiliary function for J ( F ) . Furthermore, it is a con-vex function in F and its global minimum is Proof. See Appendix A.
 Theorem 2.5 Updating F using Eq.(16) will monotonically decrease the value of the objective in Eq.(8), hence it con-verges.
 Proof. By Lemma 2.2 and Theorem 2.4, we can get that monotonically decreasing. Since J ( F ) is obviously bounded below, we prove this theorem.
 Theorem 2.6 Let Then the following function =  X   X  2 + is an auxiliary function for J ( G ) . Furthermore, it is a con-vex function in G and its global minimum is Proof. See Appendix B.
 Theorem 2.7 Updating G using Eq.(22) will monotonically decrease the value of the objective in Eq.(8), hence it con-verges.

Proof. By Lemma 2.2 and Theorem 2.6, we can get that J ( G 0 ) = Z ( G 0 , G 0 )  X  Z ( G 1 , G 0 )  X  J ( G J ( G ) is monotonically decreasing. Since J ( G ) is obviously bounded below, we prove this theorem.

According to Theorem 2.5 and Theorem 2.7, Algorithm 1 is guaranteed to converge. Note that there is no guarantee that Algorithm 1 will converge to global optimum.
In this section, we will review several works related with ours, and compare our method with them.
 Given a nonnegative data matrix X = [ x 1 , . . . , x n ]  X  R + , NMF [13] aims to find two nonnegative matrices S  X  R + and F  X  R Note that in Eq.(27) X is a nonnegative constant matrix. This limits the application of NMF for general data with mixed signs. [9] proposed a Semi-NMF, which relaxes the nonnegative constraint S  X  0 in Eq.(27) and hence is suitable for general data. It minimizes the following objective Note that in Eq.(28) X is a constant matrix whose entries can take any signs.
 The most related works with ours is [3] and [11].
In [3], the authors proposed a graph regularized NMF (GNMF), which adds an additional graph regularizer on NMF, imposing Cluster Assumption on the data points. It minimizes the following objective Hence GNMF can take into account the geometric informa-tion of the data.

In [11], the authors proposed an Orthogonal Nonnega-tive Matrix Tri-Factorization (ONMTF) to co-cluster words and documents, aiming to find three nonnegative matrices G  X  R d  X  m , S  X  R m  X  c and F  X  R n  X  c which minimizes the following objective where I m  X  R m  X  m and I c  X  R c  X  c are identity matrices.
DRCC not only considers the geometric structure in the data points as in GNMF, but also takes into account the ge-ometric information in the features. In addition, our method relaxes the nonnegative constraint on S which is imposed in GNMF and ONMTF. As a result, DRCC applies for general data, while both GNMF and ONMTF are restricted to non-negative data. Furthermore, the orthogonality constraints on F and G which are imposed in ONMTF are omitted in our method, since we use L 2 normalization on columns of F and G in the optimization, and compensate the norms of F and G to S .
In this section, we will evaluate the performance of the proposed method. We compare our method with Kmeans, Normalized Cut (NCut) [18], NMF [13], Semi-NMF (SNMF) [9], ONMTF [11] and GNMF [3]. In order to verify our assumption that features also lie on a manifold, we test a special case of the proposed method with  X  = 0, denoted by RCC, and compare it with DRCC.
To evaluate the clustering results, we adopt the perfor-mance measures used in [3]. These performance measures are the standard measures widely used for clustering.
Clustering Accuracy Clustering Accuracy discovers the one-to-one relationship between clusters and classes and mea-sures the extent to which each cluster contained data points from the corresponding class. Clustering Accuracy is defined as follows: where r i denotes the cluster label of x i , and l i denotes the true class label, n is the total number of documents,  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map ( r i ) is the permutation mapping function that maps each cluster label r i to the equivalent label from the data set.

Normalized Mutual Information The second measure is the Normalized Mutual Information (NMI), which is used for determining the quality of clusters. Given a clustering result, the NMI is estimated by where n i denotes the number of data contained in the cluster C (1  X  i  X  c ),  X  n j is the number of data belonging to the L (1  X  j  X  c ), and n i,j denotes the number of data that are in the intersection between the cluster C i and the class L The larger the NMI is, the better the clustering result will be.
In our experiment, we use 6 data sets which are widely used as benchmark data sets in clustering literature [3] [11].
Coil20 1 This data set contains 32  X  32 gray scale images of 20 3D objects viewed from varying angles. For each object there are 72 images. http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php
PIE The CMU PIE face database [19] contains 68 indi-viduals with 41368 face images as a whole. The face images were captured by 13 synchronized cameras and 21 flashes, under varying pose, illumination and expression. All the images were also resized to 32  X  32.

CSTR This is the data set of the abstracts of technical reports published in the Department of Computer Science at a university. The data set contained 476 abstracts, which were divided into four research areas: Natural Language Processing (NLP), Robotics/Vision, Systems and Theory.
Newsgroup4 The Newsgroup4 data set used in our ex-periments is selected from the famous 20-newsgroups data set 2 . The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828. The Newsgroup4 data set contains 3970 documents.

WebKB4 The WebKB dataset contains webpages gath-ered from university computer science departments. There are about 8280 documents and they are divided into 7 cat-egories: student, faculty, staff, course, project, department and other, among which student, faculty, course and project are four most populous entity-representing categories.
WebACE The data set contains 2340 documents consist-ing of news articles from Reuters new service via the Web in October 1997. These documents are divided into 20 classes.
Table.2 summarizes the characteristics of the data sets used in this experiment.

Since each clustering algorithm has one or more param-eters to be tuned, in order to compare these algorithms fairly, we run these algorithms under different parameter settings, and select the best average result to compare with each other. We set the number of clusters equal to the true number of classes for all the data sets and clustering algo-rithms.

For NCut [18], the scale parameter of Gaussian kernel for constructing adjacency matrix is set by the grid { 10  X  3 10  X  1 , 1 , 10 , 10 2 , 10 3 } .

For ONMTF, the number of word clusters is set to be the same as the number of document clusters, i.e. the true number of classes in our experiment, according to [11].
For GNMF, the neighborhood size to construct the graph 100 , 500 , 1000 } .

For DRCC, the number of data clusters is set the same as the number of feature clusters, i.e. the true number of classes, as in ONMTF. And for simplicity, the neighborhood size of the data graph is set to be the same as that of the feature graph, i.e. k , which is tuned by searching the grid http://people.csail.mit.edu/jrennie/20Newsgroups/ DRCC is tuned roughly. Better parameter tuning would achieve better clustering performance than that reported in this paper.

The parameter setting of RCC is the same as DRCC, ex-cept keeping  X  = 0.
 Note that no parameter selection is needed for Kmeans, NMF and Semi-NMF, given the number of clusters.

Under each parameter setting of each method mentioned above, we repeat clustering 20 times, and the average result is computed. And we report the best average result for each method. The best average results are shown in Table 3 and Table 4. Table 3 shows the clustering accuracy of all the algorithms on all the data sets, while Table 4 shows the normalized mutual information.

We can see that DRCC outperforms the other clustering methods on all the data sets. The superiority of DRCC arises in the following two aspects: (1) co-clustering the fea-tures and data points together, and the clustering of features can lead to improvement in the clustering of data points; (2) exploration of the geometric structure in the data points as well as in the features, which is essential for clustering data on manifold. In addition, DRCC outperforms RCC on all the data sets except CSTR. This indicates considering the geometric structure in the features can further improve the clustering results at most cases, and verifies our assump-tion that features also lie on a manifold. Besides, ONMTF and GNMF usually achieve encouraging results, which fur-ther strengthens the advantages of co-clustering features and data points simultaneously, and considering the geometric structure in the data. Note that DRCC owns all these ad-vantages.
In this subsection, we will investigate the sensitivity with respect to the neighborhood size k . When we vary the value of k , we keep the other parameters fixed at the optimal value. We plot the clustering accuracy with respect to k in Figure 1.

As we can see, DRCC is a little sensitive to the neigh-borhood size of the graph. Fortunately, it usually achieves good result when the neighborhood size is large enough, e.g. k = 10 in our experiments.
Next, we will investigate the sensitivity with respect to the regularization parameter  X  (=  X  ). When we vary the value of  X  , we keep the other parameters fixed at the optimal value. We plot the clustering accuracy with respect to  X  in Figure 2.

We can see that DRCC is very stable with respect to the regularization parameter. It achieves consistent good result with the regularization parameter varying from 100 to 1000.
In summary, we may set k = 10 and  X  =  X  = 500 in application for simplicity.
In this paper, we propose a Dual Regularized Co-Clustering (DRCC) method based on semi-nonnegative matrix tri-factorization with two graph regularizers, requiring that the cluster labels of data points are smooth with respect to the intrinsic data manifold, while the cluster labels of features are smooth with respect to the intrinsic feature manifold. DRCC is solved via alternating minimization, and its convergence is theo-retically guaranteed. Experiments of clustering on many benchmark data sets demonstrate that the proposed method outperforms many state of the art clustering methods.
In our future work, we will investigate other kind of affin-ity in the graph regularization, e.g. Local Learning Assump-tion [22], which says the cluster label of each sample can be predicted by the samples in its neighborhood.
This work was supported by the National Natural Sci-ence Foundation of China (No.60721003, No.60673106 and No.60573062) and the Specialized Research Fund for the Doctoral Program of Higher Education. We thank the anony-mous reviewers for their helpful comments. [1] C. M. Bishop. Pattern Recognition and Machine [2] S. Boyd and L. Vandenberghe. Convex optimization . [3] D. Cai, X. He, X. Wu, and J. Han. Non-negative [4] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. [5] F. R. K. Chung. Spectral Graph Theory . American [6] I. S. Dhillon. Co-clustering documents and words [7] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: [8] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [9] C. H. Ding, T. Li, and M. I. Jordan. Convex and [10] C. H. Q. Ding and X. He. On the equivalence of [11] C. H. Q. Ding, T. Li, W. Peng, and H. Park.
 [12] X. He and P. Niyogi. Locality preserving projections. [13] D. D. Lee and H. S. Seung. Algorithms for [14] T. Li and C. H. Q. Ding. The relationships among [15] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [16] P. Niyogi. Laplacian eigenmaps for dimensionality [17] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [18] J. Shi and J. Malik. Normalized cuts and image [19] T. Sim, S. Baker, and M. Bsat. The cmu pose, [20] N. Tishby, F. C. Pereira, and W. Bialek. The [21] U. von Luxburg. A tutorial on spectral clustering. [22] M. Wu and B. Sch  X  olkopf. A local learning approach [23] Xu, Wei, Liu, Xin, and Gong, Yihong. Document [24] H. Zha, X. He, C. H. Q. Ding, M. Gu, and H. D. [25] X. Zhu. Semi-supervised learning literature survey. Proof. We rewrite Eq.(23) as By applying Lemma 2.3, we have To obtain the lower bound for the remaining terms, we use the inequality that z  X  1 + log z,  X  z &gt; 0, then By summing over all the bounds, we can get Z ( F , F 0 ), which J
To find the minimum of Z ( F , F 0 ), we take and the Hessian matrix of Z ( F , F 0 ) is a diagonal matrix with positive diagonal elements.
Thus Z ( F , F 0 ) is a convex function of F . Therefore, we can obtain the global minimum of Z ( F , F 0 ) by setting  X  X  ( F , F 0 and solving for F , from which we can get Eq.(24). Proof. We rewrite Eq.(25) as By applying Lemma 2.3, we have To obtain the lower bound for the remaining terms, we use the inequality that z  X  1 + log z,  X  z &gt; 0, then tr( GQ  X  G T )  X  By summing over all the bounds, we can get Z ( G , G 0 ), which J
To find the minimum of Z ( G , G 0 ), we take and the Hessian matrix of Z ( G , G 0 )  X  2 Z ( G , G 0 ) is a diagonal matrix with positive diagonal elements.
Thus Z ( G , G 0 ) is a convex function of G . Therefore, we can obtain the global minimum of Z ( G , G 0 ) by setting  X  G ij = 0 and solving for G , from which we can get Eq.(26).
