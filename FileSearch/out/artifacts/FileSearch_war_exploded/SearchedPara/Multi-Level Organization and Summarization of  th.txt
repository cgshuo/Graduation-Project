 Many existing data mining techniques often produce a large number of rules, which make it very difficult for manual inspection of the rules to identify those interesting ones. This problem represents a major gap between the results of data mining and the understanding and use of the mining results. In this paper, we argue that the key problem is not with the large number of rules because if there are indeed many rules that exist in data, they should be discovered. The main problem is with our inability to organize, summarize and present the rules in such a way that they can be easily analyzed by the user. In this paper, we propose a technique to intuitively organize and summarize the discovered rules. With this organization, the discovered rules can be presented to the user in the way as we think and talk about knowledge in our daily lives. This organization also allows the user to view the discovered rules at different levels of details, and to focus his/her attention on those interesting aspects. This paper presents this technique and uses it to organize, summarize and present the knowledge embedded in a decision tree, and a set of association rules. Experiment results and practical applications show that the technique is both intuitive and effective. Producing too many rules has been regarded as a major problem with many data mining techniques [12, 14, 23, 21, 5]. The large number of rules makes it very hard for manual inspection of the rules to gain insight of the domain. This paper argues that the key problem is not with too many rules, but with our inability to organize and to present the rules in such a way that is easily analyzed by the user. rules from a database with the knowledge contained in a conventional scientific book. This is reasonable b ecause discovered rules are a form of knowledge. Although a book can contain a huge amount of knowledge in a few hundred pages, people seldom complain that  X this book is too thick and contains too much information, and I cannot read and use it X . In most cases, a thick book with a comprehensive coverage is of great value. However, we do often hear people complain that a book or an article is badly written and badly organized. A good book is easy to read because the author has put a great deal of effort to organize and present the contents of the book in such a way that is able to constantly draw the reader X s attention and enables him/her to follow and to understand the contents easily. hierarchy. In such an organization, the contents are divided into topics, and presented at different levels of details. The main advantage of the hierarchical organization is that it allows the reader to manage the complexity of knowledge, and to view it from general to specific, and to focus his/her attention on those interesting aspects. For a hierarchical organization to work effectively, summarization is crucial. In a book, chapter and section headings, and abstract and/or summaries are used to tell the reader what to expect from each chapter or each section. From the summaries, the reader can obtain an overall view of what the chapter or section is about and decide whether to read the details. rules like a well organized book so that they can be easily browsed and used by human users? X  This paper shows this is possible. Till now, little research has been done in this direction. In this paper, we propose a novel technique, called general rules, summaries &amp; exceptions (GSE), to organize, summarize and present the discovered rules. education, medical and other domains. In these domains, users always find it very hard to browse through the discovered rules (even when the set is not very large) to gain a good understanding of the domain. Although there exist techniques to deal with the problem of too many rules, they are still not satisfactory. Through extensive interactions with our users, we discovered that our users always talk about knowledge in a certain way, i.e., in terms of general patterns and special cases . For example, when we tried to profile the students of an educational institution, our users would make the following comment: In general, our good students have certain profiles. However, in some situations, these may not be true. In the medical applications, the situation is similar. The doctors always say that people with certain characteristics tend to have a particular disease. However, in some special situations, they may not develop the disease. We began to realize that the individual rule based representation of knowledge employed in current data mining techniques is not intuitive for human users. This inspired us to design the proposed technique, which use general rules to give the user a big picture of the domain and exceptions to point out the special cases. The summaries are used to summarize those non-essential rules, which will be clear later. 
Permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 with no further organization. This is like writing a book or a paper by randomly listing out all the facts and formulas. The resulting book will be difficult for anyone to read b ecause one ca nnot see any relationships among the facts and formulas. In data mining applications, a list of rules presents the following two problems: 1. It obscures the essential relationships and the special cases or 2. It represents the discovered knowledge only at a single level Let us use an example to illustrate. Our example dataset has two attributes (A1 and A2), 500 data tuples and two classes,  X  and O . The domain of A1 is {a, b, c, d}, and the domain of A2 is {x, y}. Assume a rule miner found the following rules ( support and confidence have their usual meanings [2]): By simply looking at the 10 rules above, it is hard for us to obtain a good overall picture of the relationships in the data. Figure 1. The two numbers within the brackets next to each junction (or value combination) are the number of  X  class tuples, and the number of O class tuples respectively. Only the majority class symbol is drawn for each value combination. rule, summary &amp; exceptions (GSE) patterns, the picture becomes clear and much easier to understand. Let us ignore the summary component for the time being. GSE-1: A2 = x  X   X  (sup = 41%, conf = 77%) GSE-2: A2 = y  X  O (sup = 38%, conf = 83%) The first part of a GSE pattern is a general rule, and the second part (after  X Except X ) is a set of exceptions to the general rule. From these two parts of the GSE patterns, we see the essential relationships of the domain, the two general rules, and the special cases, the exceptions (R5 and R7). This representation is much simpler than the 10 rules because R2-R4 and R8-R10 are not used. These rules (R2-R4 and R8-R10) fragment the knowledge in the data. They are non-essential, and worse still they make the discovered knowledge hard to interpret. completely useless. The fragmented rules may be interesting due to their higher confidences (e.g., 97% and 98%). We use the summary component to summarize these fragmented rules. Here, we use the highest confidence of the rules as the summary (see Section 3). Thus, the complete GSE-1 and GSE-2 are as follows: GSE-1: A2 = x  X   X  (sup = 41%, conf = 77%) GSE-2: A2 = y  X  O (sup = 38%, conf = 83%) two key advantages: Simplification : It simplifies the discovered rules using general Organization : It organizes the discovered knowledge in an In Section 3, we will see that the GSE representation also naturally introduces a hierarchical (multi-level) organization of the discovered rules. organize, summarize and present the knowledge embedded in a decision tree and a set of association rules. Since decision trees and association rules embed somewhat different types of knowledge, the proposed technique is applied differently depending on the context. Experiment results and practical applications show that the proposed technique is very effective and efficient. In the past few years, a number of techniques were proposed to deal with the problem of too many rules [21, 23, 12, 14, 9, 19, 1]. The main idea of these techniques is to use the user X s knowledge or statistical measures to remove those uninteresting rules. Our work is different. We aim to organize and summarize the rules so that the user can browse through them easily and effectively. rules. This approach first asks the user to specify what rules he/she wants. The system then finds those matching rules. The technique assumes that the user knows exactly what he/she is looking for. However, in many situations, the user does not know what to find exactly. His/her needs can only be met by incorporating browsing as well as template-based search in the system. It is well known that humans are better able to recognize something than to generate a description of it [11]. approach to find interesting rules. In this approach, the user is also required to input his/her existing knowledge about the domain, and the system then finds those conforming and unexpected rules. This approach is more advanced than the template-based approach because it not only finds those rules that satisfy the templates but also the unexpected rules. However, since it requires input knowledge from the user, it suffers from the problem of knowledge acquisition. That is, one may know a great deal, but it is very difficult for one to tell what he/she knows [14]. user in association rule mining to generate only the relevant rules. This approach also requires the user to know precisely what he/she wants or does not want. [14] presents a technique to summarize the discovered setting rules (DS rules), to summarize the discovered rules. DS rules represent the essential relationships of the domain. For example, we have the following discovered rules from a loan application domain: R1: Job = yes  X  Loan = Approved (sup = 40%, conf = 70%) R2: Own_house = yes  X  Loan = Approved If these two rules are known, then the following association rule is not so surprising to us: R3: Job = yes, Own_house = yes  X  Loan = Approved because it intuitively follows R1 and R2. R1 and R2 (the DS rules) are used to provide a summary of the three rules. R3 is non-essential. This technique is proven useful in our real-life applications. However, some shortcomings are also exposed. Firstly, it takes at least two rules to summarize another rule. However, the user only sees one rule at a time. Thus, when the user just sees a particular DS rule, it is not easy to imagine those rules that have been summarized because he/she may not have seen the other DS rules. Secondly, comparing to GSE representation, DS rules are less intuitive as DS rules do not directly reflect the way that we think about knowledge. Finally, the technique cannot be applied to summarize the rules in a decision tree because it requires at least two rules to summarize another. However, one of the rules may not be in the tree as a decision tree typical only discovers a subset of the rules in data. without the summary component, to represent decision trees. The GE representation is less effective than GSE as the information on the fragmented rules is lost. [15] also does not study organization, summarization and presentation of association rules, which presents a different set of problems. given some general rules. Our work is different. We aim to organize and summarize the mining results. We do not report another technique to mine exceptions from the data. exceptions, for knowledge acquisition in AI. This confirms that general rules and exceptions are intuitive to human experts. We now present the GSE representation. In this paper, we are interested in rules generated from a relational table D , which consists of a set N of data tuples described by m distinctive attributes A . Rule mining in such a dataset is typically targeted at a specific attribute because the user normally wants to know how have many values) [13, 3]. This target attribute is often called the we define the GSE pattern in this context. Definition 1 (GSE patterns) : A GSE pattern consists of three Except E 1 ,  X , E n where: We now discuss the intuitive meanings of the three components of the GSE pattern: 1. The general rule gives the user a general relationship or a 2.  X Summary X  highlights some key information of the rules 3. Exception rules are unexpected rules with respect to the Since each exception is also a GSE pattern, the GSE pattern can naturally represent knowledge in a hierarchical fashion. That is, a general rule can have exceptions, and an exception can also have its own exceptions, and so on. However, the depth (or the number of levels ) of the hierarchy should not be too large, otherwise it also becomes difficult to understand. In an actual implementation, the user can decide when the system should stop using GSE patterns. This will be explained in Section 4, which is also applicable to Section 5. rule) here because for different data mining tasks it may be defined differently (see Section 4 and 5). Decision tree construction [22] is a popular method for building classification models. In this section, we use GSE patterns to organize and summarize the knowledge embedded in a decision tree. The GSE representation can also be expressed as a tree, which we call a GSE tree . converting it to a GSE tree. The resulting GSE tree can still be used for classification to produce the same result as the original decision tree. ( internal node ) and leaf nodes . A decision node specifies some test to be carried out on an attribute value. A leaf node indicates a class. A decision tree basically represents a partitioning of the data space. A serial of tests (or cuts) from the root node to a leaf represents a hyper-rectangular region. The leaf node gives the class of the region. For example, the five regions in Figure 2(A) are produced by the tree in Figure 2(B) (ignore the numbers of rules. For example, the leaf node 6 in Figure 2(B) can be expressed with the rule, Figure 2. A partitioning of the data space and its decision tree Before presenting the detailed algorithm, we first define a general rule or an exception rule in the context of a decision tree. Definition 2 (general or exception rules) : A general (or an The significance of a rule can be measured in various ways. In this work, we measure it using chi-square (  X  2 ) test/fisher's exact test [6]. The way that we measure the significance of a rule is the same as that in [14]. We will not discuss it further in this paper. class because a classification system always outputs the majority class in prediction. In the GSE representation, we aim to keep the original spirit of the underlying data mining task. decision tree to a GSE tree. The algorithm consists of two steps (summarization is done in both steps, see the algorithm later): 1. Find top-level general rules: We descend down the decision 2. Find exceptions: We go down from each top-level general rule Figure 2(B), the following GSE patterns can be formed (for simplicity, we omit the support and confidence information): (which are formed by node 2 &amp; 3 in Figure 2(B)). An exception rule (with its exception) is formed at node 7 ( according to formula 2(A) that the area within (A1 &gt; 7, A2  X  2.5) is a general area for class g . This gives us a 3-level hierarchical representation of knowledge. Figure 3(A) shows the corresponding GSE tree, which has the complexity of 4 as it has 4 leaves. An internal node with a represents the summary at the node. For simplicity, we do not list the content of the summary in the GSE tree. Those fragmented tree leaves or rules are omitted. In our implementation, these leaves or rules can be accessed by clicking on the summary. 6 2.5 1.8 0 
A2 too deep, the knowledge embedded in it becomes harder to understand. An alternative representation of GSE-2 is GSE-2 X , which stops using the GSE representation after node 3. The hierarchy of GSE-2 X  has only two levels. This may be easier to understand. The user can obtain the desired output by specifying the maximum depth of the hierarchy. After this depth, no further split of general rules and exceptions will be performed. Figure 3(B) gives the GSE tree for GSE-1 and GSE-2 X . Notice that finding general rules and finding exceptions rules are essentially the same because exception rules are general rules for their own exceptions. Thus, the two intuitive steps can be performed with a single procedure, i.e., the findExcepts procedure in Figure 4. findGSE calls two procedures, countLeavesAndSummarize, and findExcepts. As discussed above, to decide whether a node below a general rule should form an exception rule or not, we need the number of leaves of each class below the node (formula (4.1)). The countLeavesAndSummarize procedure performs this task and also summarizes the tree leaves (or rules) below each internal node, which in our implementation records the highest confidence of the leaves of each class below the node. This procedure is fairly straightforward and is not discussed here. In the case of Figure 2(B), countLeavesAndSummarize produces the numbers of leaves of different classes below each internal node as shown within "()" in Figure 2(B). The highest confidence of the rules of each class below each node is not shown. The first number within () is the number of g class leaves below the node, and the second number is the number of O class leaves. The findExcepts procedure traverses down the tree from a rule. Note that when starting from the root node, the second input database) to indicate that there is no general rule at the root node. Thus, the first exception found along each path of the tree is a top-different from the general rule's class c i , it is reported as an exception. Otherwise, the procedure goes down to its children to find exceptions (line 5). If the conditions in line 7 are satisfied, it means that node child can form an exception rule. n i is the number this node to form an exception rule and recursively goes down (line 8 and 9). For example, in Figure 2(B), node 7 forms an exception rule because the c onditions in line 7 are met (assume the rule is significant). If the conditions are not met, the procedure goes down further (line 10). For the tree in Figure 2(B), we obtain the GSE tree in Figure 3(A), with two top-level general rules and two exception rules. A GSE tree can be constructed very efficiently because the algorithm traverses the decision tree at most twice, once by countLeavesAndSummarize, and once by findExcepts. Algorithm findGSE ( RootNode ) 1 countLeavesAndSummarize( RootNode ); 2 findExcepts( RootNode , 0) findExcepts ( Node , c i ) 1 if Node is a leaf then 2 if the class of the leaf is different from c i then 3 mark it as an exception 4 else delete the node /* it is a fragmented leaf or rule */ 5 else for each child node in Node.children do 6 c j = majority class of node child ; 7 if c j  X  c i AND the rule formed from root node to 8 mark it as an exception rule; 9 findExcepts( child , c j ) 10 else findExcepts( child , c i ) 12 end We now discuss how to represent association rules using GSE patterns. Since association rule mining is not aimed at producing a classification model but to find all significant rules, we need to apply the proposed technique differently. These will become clear later. The nature of association rule mining also presents some special problems that make it hard to find real exceptions because of the minimum support and rule overlapping (the same data tuple may be covered by many rules). In this section, we first present the basic framework of using GSE patterns to organize association rules. We then present and deal with the special problems. The association rule mining model is stated as follows [2]: Let I = { i ,  X , i r } be a set of items , and D be a set of data cases (transactions). Each data case consists of a subset of items in I . An association rule is an implication of the form X  X  Y , where X  X  I , Y  X  I , and X  X  Y =  X  . The rule X  X  Y holds in D with confidence c if c % of data cases in D that support X also support Y . The rule The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support and minimum confidence. In this work, we focus on association rule mining from a relational table, which is described by a number of attributes. An item is an attribute value pair, i.e., (attribute = value) (numeric attributes are discretized). We also have a target or class attribute, which can have a number of values C (or classes). With the class attribute, we only mine rules of the form: where c i  X  C is a class, and X is a set of items from the rest of the attributes. We say a rule is large if it meets the minimum support. We will not discuss the algorithm for mining such rules as the existing algorithm in [2] can be easily modified for this purpose (see also [13]). Rule pruning: It is well known that many discovered associations are redundant or minor variations of others. Their existence may simply be due to chance rather than true correlation. Those insignificant rules should be removed. [14] studies the pruning of association rules. Its basic idea can be shown with the following two rules from a loan application data. 
R : Job = yes  X  Loan = Approved (sup = 60%, conf = 90%) r : Job = yes, Credit_history = good  X  Loan = Approved If we know R , then r is of limited use because it gives little extra information. Its slightly higher confidence is more likely due to chance than to true correlation. subset of data tuples covered by R , r is not significant. The significance (or positive correlation) of a rule is measured by chi-done as follows:  X  Given a rule r , we try to prune r using each ancestor rule R See [14] for more details about pruning. Note that we do not use minimum confidence in our framework. Minimum confidence (a user-specified threshold) does not reflect the underlying relationships of the domain [3, 14]. Instead we use statistical significance as the basis for finding rules that represent the fundamental relations of the domain. Since association rule mining aims to find all rules in data, it often generates a huge number of rules. Even after pruning, the number of rules left can still be very large, in the hundred or thousands (see Section 6). Clearly, such a large number of rules are very difficult, if not impossible, to be analyzed by a human user. present the discovered associations so that the user can browse through them and focus on the interesting subset of the rules. The key issue still is how to determine whether a rule should form an exception rule. In the case of a decision tree, we make the significance still applies (after pruning every association rule is statistically significant). However, we do not require that an exception rule X s class be the majority class. Since the aim of association rule mining is to find all significant rules, thus we cannot require each rule to use the majority class. we find a significant rule and we do not use it as an exception decision tree, the original rules for classification are formed only at the leaf level. Thus, if an internal node does not form an exception rule, we do not lose any information. significance to determine whether a rule should form an exception (of course, its class must be different from its general rule). Since every rule is significant after pruning, we do not need any more significance test. use a different technique. It is not time and space efficient to build the whole structure like the GSE tree for association rules. The reason is that this structure could be too large due to duplicate rules appearing in different parts of the structure, i.e., the same rule may appear in many places as exceptions. (A2 = b  X  C1). Assume that we also have the rule, (A1 = a, A2 = b  X  C3). Then, this rule will appear as an exception rule of A1 = a  X  C1 and as an exception rule of A2 = b  X  C1. In the case of a decision tree, no rule will be an exception rule for more than one general rule because rules in a decision tree are disjoint. This is not the case for association rules as the above example shows. rule as an exception only once? X  We could not do this because the new technique aims to allow the user to browse and explore the rules effectively. If we have 2000 rules, obviously the user is unlikely to see all of them. GSE patterns organize the rules in an intuitive way so that the user can focus his/her attention on only a subset of interesting rules. However, we will not know where the user will choose to focus on beforehand. Thus, we are unable to decide whether the rule (A1 = a, A2 = b  X  C3) should appear as an exception of (A1 = a  X  C1) or (A2 = b  X  C1). the exceptions of a general rule when the user requests it. This approach is reasonable because of two reasons: 1. the computation can be done very efficiently without the user 2. even if we display all the information, the user will not be able summarize all the rules below each exception rule. This summary highlights the main characteristics of the rules below the exception rule. From the summary, the user can decide whether to go down further to see more details. If he/she decides to do so, he/she can click and the system will compute the exceptions of this exception rule (which now behaves as a general rule).  X  The coverage of each class: This shows how big a population  X  The highest confidence of the rules in each class: This gives  X  The number of rules in each class: This allows the user to The set of characteristics can be further expanded when the need arises. However, in our applications, we found that these together with the exception rules are sufficient for the user to decide where to focus on. rules to GSE patterns, we first show how the rules are stored in memory. In essence, they are represented as a tree, called a rule tree. A rule tree is different from a decision tree because every representing some rules, and the branches of the tree at each level do not partition the data. Each tree node contains three pi eces of information: 1. An item, which is represented by an integer number. 2. A support count, which is the number of data tuples that fall 3. A class distribution list, which stores the number of data An example rule tree is given in Figure 5, which uses 4 items (1, 2, 3, 4). The class distribution list and the support counts are not shown. Note that the items (represented in the tree) are in an ascending order either horizontally or vertically. This is inherited from association rule mining. This ordering greatly facilitates our computation later. represented by the root node rootNode of a rule tree and the class grClass of the general rule. We will see later that a tree is constructed dynamically for each general rule, which is used to find its exceptions. The tree for a general rule contains only those rules (called superset rules below) that are below the general rule. The top-level general rules can also be seen as exception rules that do not have a general rule above them. 
Algorithm findGSE ( rootNode , grClass ) 1 exceptionR = {}; 2 findErules( rootNode , grClass); 3 for each er  X  exceptionR do 4 supersetR = {}; 5 findSupersetR( er.cond , er.class , rootNode ); 6 summarize( supersetR ); 7 buildTree(cond( er ), supersetR ) 8 endfor exception rules of a general rule. The findErules procedure (line 2) finds all the exception rules starting from rootNode . From line 3-8, for each exception rule er , we find all the rules below it, i.e., those rules whose conditions are a superset of the conditions of er summarizes the superset rules of each class. This procedure is simple to design, and thus will not be discussed further. In line 7, we build a new tree using all rules in supersetR with all the conditions in er removed from each rule. This tree will be used to find exceptions below er ( er will behave as a general rule). That is why the algorithm findGSE always starts from the root node of a tree. The buildtree procedure is omitted, as it is straightforward. from a root node representing a general rule with its class grClass . For finding top-level general rules, the same procedure is used. In this case, grClass is a special value (we use 0) that is not a class in the data. rules from each child node. If an exception rule is found from a child (line 3), it is recorded (see getER procedure in Figure 8). Otherwise, the procedure traverses further down to find exception rules. The procedure (getER) for finding exception rules within a node is given in Figure 8. findErules ( rootNode , grClass ) 1 if rootNode . children  X  NIL then 2 for each child in rootNode.children do 3 if not(getER( child , grClass )) then 4 findErules( child , grClass ); of the node (in node.classDistr ) to see whether any rules are formed here. If c = 0, it means that this class does not form a rule. found (line 4-5). The variable found indicates whether any exception rules are found. 1 found = FALSE 2 for each c in node . classDistr do 3 if c  X  0 then 4 if c  X  grClass then /* we have found an exception rule */ 6 found = TRUE 7 endif 8 endif 9 endfor 10 return( found ) Figure 9 gives the findSupersetR procedure that finds all superset rules of an exception rule. These rules will be summarized to the user. Assume the set of conditions of an exception rule is S = { s  X , s k }, which is a set of numbers sorted in the ascending order. findSupersetR starts off from the root node of the current tree and traverses down every possible path of the tree to find all superset rules. Note that we cannot simply traverse down the tree from the exception rule to find its superset rules because some of its superset rules may appear in other parts of the tree as well. which shows how general rules, summaries and exceptions are organized and presented. The rules are generated from a loan application data. The classes in the data are Approved and Not_Approved , indicating whether a loan is approved or not. Lines 1, 5, 7 and 11 show 4 general rules. Lines 2-3, line 6, lines 8-9, and lines 12-13 summarize the rules below the 4 general rules respectively.  X Except  X  X  indicates that there are exceptions, which are not shown. The user needs to click on it to see the exception rules, which calls the findGSE procedure. The user can also click on the class Approved or Not_Approved in the summary field to see all rules of the class below each general rule. findSupersetR ( S , node ) findFirst(hd( S ), tail( S ), child ) findFirst ( first , rest , node ) getRules ( node ) goDown( node ) goDown ( node ) We now discuss three problems in using the GSE representation to organize and summarize association rules. The solution to these problems is also presented. 1. Due to the minimum support constraint some exceptions for a 2. Due to pruning in rule generation, some exceptions may be We deal with these problems using two methods. Lower down the minimum support : When the user is interested Integrate with a decision tree algorithm : Extract the data tuples We now evaluate the effectiveness and efficiency of the proposed technique. We first discuss the experiment results with decision trees, and then the experiment results with association rules. Finally, we discuss our application experiences. We applied the proposed technique to the decision trees produced by C4.5 [22] using 20 datasets in the UCI Machine learning repository [17]. Our objective here is to see how much the GSE representation is able to simplify the decision tree. Simplification, however, is not the only objective of the proposed technique. Another important aspect is the organization of the discovered knowledge. Unfortunately, organization is hard to quantify. We use our application experiences to show that users like the proposed technique due to its intuitive representation. [15] for the detailed results. The first column gives the average number of leaves in the decision trees produced by C4.5 (after pruning) for the 20 datasets. The second column gives the average number of leaves in the GSE trees for the 20 datasets. in the GSE tree is substantially smaller. On average, over the 20 datasets, the number of leaves in the GSE tree is only 36% of that of the original decision tree. This shows that GSE trees significantly simplify decision trees. The execution times are not reported as the algorithm runs so fast that they cannot be logged. 95% is used to measure the significance of a rule (this is a commonly used level [6]). When an expected frequency is below 5 for chi-square test, fisher's exact test is employed instead. For the experiments with association rules, it is not appropriate to use the percentage of reduction to assess the effectiveness because there are duplicate exception rules in many places and our implementation is based on a lazy approach. The system only finds exception rules of a general rule when requested by the user. In this situation, efficiency is important because we ca nnot let the user wait for too long to obtain the results. We show the execution time taken to compute the exceptions of a general rule, and to find and summarize all their superset rules. The datasets used here are the same as those used in the experiments with decision trees. The experiments are done on Pentium-II 350 with 128MB of memory. prune those insignificant rules (without generating the DS rules). The minimum support is set to 1% as it is shown in [13] that for these datasets, rules with this support threshold are sufficiently predictive. Pruning of association rules also uses the  X  significance level of 95%. Many datasets we use contain numeric attributes. Since association rule mining does not handle numeric attributes. We discretize these attributes into intervals using the target (class) attribute. We use the entropy-based method in [7]. The code is taken from MLC++ [10]. general rules and their superset rules (including summarization) for each dataset. We only use the top-level general rules to give an indication of efficiency because the number of top-level general rules is normally larger than the number of exceptions of a general rule below them. They also tend to have more superset rules as the number of conditions in a top-level general rule is small. 2 gives the number of rules found after pruning for each dataset. not present a big problem any more as we will see in Section 6.3. Column 3 shows the number of top-level general rules found for each dataset. Column 4 gives the execution time. We can see that the execution is very fast. The user can hardly notice that the computation is done on the fly. 
Table 2: Execution time for finding top-level general rules We built two systems in this work, one for converting a decision tree to a GSE tree (we call this system GSEtree) and one for organizing, summarizing and presenting association rules (we call this system GSErule). GSEtree is also integrated into GSErule, i.e., GSErule can call GSEtree when needed (see Section 5.3). The two systems have been used in a number of applications for two educational institutions, and a medical center. tasks. For example, in a medical application, we needed to build a classifier to predict whether a person has a particular kind of disease. We used the C4.5 system, which produced a decision tree with 35 leaves. Our GSE tree had only 10 leaves or rules. For 44, the chance of having the disease is small with 3 exceptions. However, the corresponding sub-decision tree has 14 leaves (or rules). Another general rule says that if the patient's age is greater than 62, the chance of having the disease is very high with only one exception. However, C4.5 produced 5 fragmented rules. The doctors said that they could not obtain an overall picture of the domain from the fragmented decision tree leaves (or rules). The GSE tree gives them exactly the types of knowledge they want. In the education application, the situation is similar. Our users agree that the GSE representation is simple and more intuitive. users find it more effective than our existing systems. Before this work, we had two previous systems. The first system is based on user expectations or beliefs [12]. In this system, the user first inputs some existing knowledge, and the system then finds those conforming and unexpected rules. The main problem with this system is that the users found it difficult to give existing domain knowledge. Very quickly they gave up and asked us to do it for them. The second system is the DS rule system [14]. DS rules are the essential rules in the domain. Since the number of DS rules is not very large in these applications, less than one hundred, they can be browsed manually. However, it is still hard to gain a good overall view of the domain using the DS rule system since the DS rules are just a list of rules with no organization. The expectation system and the DS rule system are also hard to understand. It often takes many sessions for a student to explain to our users what these systems do. The GSErule system, on the other hand, is much easier to understand and to use b ecause it is very intuitive. In fact, our users had inspired us to design the new representation. With the GSE representation and the GSErule system, browsing through the rules is no longer so difficult and boring as the exception (unexpected) rules always draw the users X  attention. need to identify interesting rules for our users in some applications. Our users could only explain to us the types of rules that may be interesting to them. Using the GSErule system, we are able to identify interesting rules easily. We can also present the rules to our users in the way (using the GSE representation) that they can easily understand and appreciate. rules after pruning. The large number of rules does not present a major problem as the users always have some tasks in mind and thus are only interested in certain aspects of the discovered rules. The GSE representation makes it convenient to focus on these interesting aspects and to obtain a good overall picture of them. It is also quite interesting to note that even those less interesting aspects are often explored because exceptions (unexpected rules) can be quite tempting, i.e., people are always curious about unexpected things. This paper proposed a novel technique to organize, summarize and present the discovered rules. The technique is based on the representation and organization of knowledge using general rules, summaries and exceptions, i.e., GSE patterns. The GSE pattern is very intuitive as it is closely related to the way that we think and talk about knowledge in our daily lives. It allows the user to see an overall picture of the domain first and then the special cases. It manages the complexity of the discovered rules in a hierarchical fashion, which enables the user to focus his/her attention on the interesting aspects of the rules. In this paper, we applied the representation to organize and summarize the knowledge embedded in a decision tree and a set of association rules. Our practical applications show that the representation is simple and more intuitive than a list of individual rules. Acknowledgement: We thank Shuik-Ming Lee, Shanta C Emmanuel, Paul Goh, Jonathan Phang, Hing-Yan Lee and King-Hee Ho, for providing us the data and giving us feedbacks. The project is funded by National Science and Technology Board, and National University of Singapore under RP3981678. [1] Adomavicius, G. and Tuzhilin, A.  X User profiling in [2] Agrawal, R. and Srikant, R.  X Fast algorithms for mining [3] Bayardo, R., Agrawal, R, and Gunopulos, D. "Constraint-[4] Compton, P. and Jansen, R. Knowledge in context: a [5] Dong, G. and Li, J.  X Interestingness of discovered [6] Everitt, B. S. The analysis of contingency tables . Chapman [7] Fayyad, U. M. and Irani, K. B.  X Multi-interval [8] Han, J. and Fu, Y.  X Discovery of multiple-level association [9] Klemetinen, M., Mannila, H., Ronkainen, P., Toivonen, [10] Kohavi, R., John, G., Long, R., Manley, D., and Pfleger, K. [11] Large, A. Tedd, L. &amp; Hartley, R Information seeking in [12] Liu, B., Hsu, W.  X Post-analysis of learnt rules X  AAAI-96 . [13] Liu, B., Hsu, W. and Ma, Y.  X Integrating classification and [14] Liu, B., Hsu, W and Ma, Y.  X Pruning and Summarizing the [15] Liu, B., Hu. M., and Hsu, W.  X Intuitive representation of [16] Liu, H., Lu, H., Feng, F and Hussain, F.  X Efficient search of [17] Merz, C. J. &amp; Murphy, P. UCI repository of ML databases, [18] Ng. R. T. Lakshmanan, L., and Han, J.  X Exploratory mining [19] Padmanabhan, B., and Tuzhilin, A.  X A belief-driven [20] Pazzani, M., Mani, S. and Shankle, W. R.  X Bey ond concise [21] Piatesky-Shapiro, G., and Matheus, C.  X The interestingness [22] Quinlan, R. C4.5 : program for machine learning. Morgan [23] Silberschatz, A., and Tuzhilin, A.  X What makes patterns [24] Srikant, R., Vu, Q. and Agrawal, R.  X Mining association [25] Suzuki, E.  X Autonomous discovery of reliable exception 
