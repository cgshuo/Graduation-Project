 Deterministic parsing methods achieve both effec-tive time complexity and accuracy not far from those of the most accurate methods. One such deterministic method is Nivre X  X  method, an incre-mental parsing method whose time complexity is linear in the number of words (Nivre, 2003). Still, deterministic methods can be improved. As a spe-cific example, Nivre X  X  model greedily decides the parsing action only from two words and their lo-cally relational words, which can lead to errors. In the field of Japanese dependency parsing, Iwatate et al. (2008) proposed a tournament model that takes all head candidates into account in judg-ing dependency relations. This method assumes backward parsing because the Japanese depen-dency structure has a head-final constraint, so that any word X  X  head is located to its right.

Here, we propose a tree-based model, applica-ble to any projective language, which can be con-sidered as a kind of generalization of Iwatate X  X  idea. Instead of selecting a parsing action for two words, as in Nivre X  X  model, our tree-based model first chooses the most probable head can-didate from among the trees through a tournament and then decides the parsing action between two trees.

Global-optimization parsing methods are an-other common approach (Eisner, 1996; McDon-ald et al., 2005). Koo et al. (2008) studied semi-supervised learning with this approach. Hy-brid systems have improved parsing by integrat-ing outputs obtained from different parsing mod-els (Zhang and Clark, 2008).

Our proposal can be situated among global-optimization parsing methods as follows. The pro-posed tree-based model is deterministic but takes a step towards global optimization by widening the search space to include all necessary words con-nected by previously judged head-dependent rela-tions, thus achieving a higher accuracy yet largely retaining the speed of deterministic parsing. 2.1 Dependency Parsing A dependency parser receives an input sentence x = w 1 , w 2 , . . . , w n and computes a dependency graph G =( W, A ) . The set of nodes W = { sentence, and the node w the set of arcs ( w dependency relation where w is the dependent .

In this paper, we assume that the resulting de-pendency graph for a sentence is well-formed and projective (Nivre, 2008). G is well-formed if and only if it satisfies the following three conditions of being single-headed , acyclic , and rooted . 2.2 Nivre X  X  Method An incremental dependency parsing algorithm was first proposed by (Covington, 2001). After studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto, 2003), and (Nivre, 2003), the deterministic incre-mental parser was generalized to a state transition system in (Nivre, 2008).

Nivre X  X  method applying an arc-eager algorithm works by using a stack of words denoted as  X  , for a buffer  X  initially containing the sentence x . Pars-ing is formulated as a quadruple ( S, T where each component is defined as follows:  X  S is a set of states, each of which is denoted  X  T  X  s  X  S Syntactic analysis generates a sequence of optimal transitions t applied to a target consisting of the stack X  X  top ele-ment w oracle is constructed as a classifier trained on tree-bank data. Each transition is defined in the upper block of Table 1 and explained as follows: Left-Arc Make w Right-Arc Make w Reduce Pop w Shift Push the word w
The method explained thus far has the following drawbacks.
 Locality of Parsing Action Selection The dependency relations are greedily determined, so when the transition Right-Arc adds a depen-dency arc ( w located in the stack is disregarded as a candidate. Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words w words. These words are not sufficient to select Re-duce, because this action means that w pendency relation with any word in the stack. Preconditions When the classifier selects a transition, the result-ing graph satisfies well-formedness and projectiv-ity only under the preconditions listed in Table 1. Even though the parsing seems to be formulated as a four-class classifier problem, it is in fact formed of two types of three-class classifiers.

Solving these problems and selecting a more suitable dependency relation requires a parser that considers more global dependency relations. 3.1 Overall Procedure Tree-based parsing uses trees as the procedural el-ements instead of words. This allows enhance-ment of previously proposed deterministic mod-els such as (Covington, 2001; Yamada and Mat-sumoto, 2003). In this paper, we show the applica-tion of tree-based parsing to Nivre X  X  method. The parser is formulated as a state transition system and  X  for a state s =(  X ,  X , A )  X  S denote a stack of trees and a buffer of trees, respectively. A tree t  X  T is defined as the tree rooted by the word w and the initial state is s which is formed from the input sentence x .
The state transitions T following two steps. 1. Select the most probable head candidate 2. Select a transition: Choose a transition, These transitions correspond to three possibilities for the relation between t is a dependent of a word of t dependent of a word of t not related.

The formulations of these transitions in the lower block of Table 1 correspond to Nivre X  X  tran-sitions of the same name, except that here a tran-sition is applied to a tree. This enhancement from words to trees allows removal of both the Reduce transition and certain preconditions. 3.2 Selection of Most Probable Head By using mphc ( t (the head of t didate in t number of errors resulting from greedy decision considering only a few candidates.

Various procedures can be considered for im-plementing mphc ( t tournament procedure to the words in t nament procedure was originally introduced for parsing methods in Japanese by (Iwatate et al., 2008). Since the Japanese language has the head-final property, the tournament model itself consti-tutes parsing, whereas for parsing a general pro-jective language, the tournament model can only be used as part of a parsing algorithm.

Figure 1 shows a tournament for the example of  X  X ith, X  where the word  X  X atched X  finally wins. Although only the words on the left-hand side of tree t tree-based method considers only one side of a de-pendency relation. For example, when we apply the tree-based parsing to Yamada X  X  method, the search problems on both sides are solved.

To implement mphc ( t is built to judge which of two given words is more appropriate as the head for another input word. This classifier concerns three words, namely, the two words l (left) and r (right) in t propriateness as the head is compared for the de-pendent w compared repeatedly in a  X  X ournament, X  and the survivor is regarded as the MPHC of w
The classifier is generated through learning of training examples for all t of which generates examples comparing the true head and other (inappropriate) heads in t ble 2 lists the features used in the classifier. Here, lex( X ) and pos( X ) mean the surface form and part of speech of X , respectively. X lef t means the dependents of X located on the left-hand side of X , while X right means those on the right. Also, X head means the head of X . The feature design concerns three additional words occurring after w 3.3 Transition Selection A transition is selected by a three-class classifier after deciding the MPHC, as explained in  X  3.1. Table 1 lists the three transitions and one precon-dition. The transition Shift indicates that the tar-get trees t The transition Right-Arc indicates generation of the dependent-head relation between w result of mphc ( t ure 2 shows an example of this transition. The transition Left-Arc indicates generation of the de-pendency relation in which w While Right-Arc requires searching for the MPHC in
The key to obtaining an accurate tree-based parsing model is to extend the search space while at the same time providing ways to narrow down the space and find important information, such as the MPHC, for proper judgment of transitions.
The three-class classifier is constructed as fol-lows. The dependency relation between the target trees is represented by the three words w and w corporate these words, their relational words, and the three words next to w set of features used in this work. Since this transi-tion selection procedure presumes selection of the MPHC, the result of mphc ( t rated among the features. 4.1 Data and Experimental Setting In our experimental evaluation, we used Yamada X  X  head rule to extract unlabeled dependencies from the Wall Street Journal section of a Penn Treebank. Sections 2-21 were used as the training data, and section 23 was used as the test data. This test data was used in several other previous works, enabling mutual comparison with the methods reported in those works.
 support vector machine classifiers. The binary classifier for MPHC selection and the three-class classifier for transition selection were built using a cubic polynomial kernel. The parsing speed was evaluated on a Core2Duo (2.53 GHz) machine. 4.2 Parsing Accuracy We measured the ratio of words assigned correct heads to all words (accuracy), and the ratio of sen-tences with completely correct dependency graphs to all sentences (complete match). In the evalua-tion, we consistently excluded punctuation marks.
Table 4 compares our results for the proposed method with those reported in some previous works using equivalent training and test data. The first column lists the four previous methods and our method, while the second through fourth columns list the accuracy, complete match accu-racy, and time complexity, respectively, for each method. Here, we obtained the scores for the pre-vious works from the corresponding articles listed in the first column. Note that every method used different features, which depend on the method.
The proposed method achieved higher accuracy than did the previous deterministic models. Al-though the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is de-terministic. These results show the capability of the tree-based approach in effectively extending the search space. 4.3 Parsing Time Such extension of the search space also concerns the speed of the method. Here, we compare its computational time with that of Nivre X  X  method. We re-implemented Nivre X  X  method to use SVMs with cubic polynomial kernel, similarly to our method. Figure 3 shows plots of the parsing times for all sentences in the test data. The average pars-ing time for our method was 8.9 sec, whereas that for Nivre X  X  method was 7.9 sec.
 Although the worst-case time complexity for Nivre X  X  method is O ( n ) and that for our method is O ( n 2 ) , worst-case situations (e.g., all words hav-ing heads on their left) did not appear frequently. This can be seen from the sparse appearance of the upper bound in the second figure. We have proposed a tree-based model that decides head-dependency relations between trees instead of between words. This extends the search space to obtain the best head for a word within a deter-ministic model. The tree-based idea is potentially applicable to various previous parsing methods; in this paper, we have applied it to enhance Nivre X  X  method.

Our tree-based model outperformed various de-terministic parsing methods reported previously. Although the worst-case time complexity of our method is O ( n 2 ) , the average parsing time is not much slower than O ( n ) .

