 Multi-document summarization aims at cr eating a compressed version of a given document set on the same or similar topic. A well organized summary can greatly help users to reduce information overl oad. In general, document summarization can be classified into two categories: abst ractive summarization and extractive summarization. Abstractive summarization produces summaries by arranging new sentences from the original documen ts, while the more widely used extrac-tive summarization generates summaries directly by selecting salient sentences from the original documents. In this paper, we focus on the extractive multi-document summarization.

Data reconstruction, from the matrix view, can be seen as finding a lower dimensional matrix to well approximate the high dimensional matrix [5]. In a recent paper, He et al.[11] proposed a new s ummarization framework called Doc-ument Summarization based on Data Reconstruction(DSDR). They argued that a good summary should consist of those se ntences that can best reconstruct the original document. The document set in the DSDR is represented in the form of sentence-by-term matrix. It aims to find a linear combination of a small sen-tence vector set (summary) to reconstruct t he original senten ce-by-term matrix. However, the number of sentences in the summary produced by the DSDR is rel-atively large which approximates to the original sentence number. Moreover, the quality of the summary produced by the DSDR is relatively poor. The main rea-son is that the DSDR algorithm bases on the sentence-by-term matrix which is very sparse and can X  X  capture the semati c similarity. For example, two sentences with small difference are regarded a s two totally different sentences.
A document generally contains a varie ty of information centered around a main topic and covers different aspects o f the main topics [12]. Recently, prob-abilistic topic models [3][13] have been successfully introduced into document summarization [1][2]. In this paper we propose a novel model that combines data reconstruction and topic decomposit ion to summarize the documents, named TopicDSDR, which can not only best reconstruct the original documents but also capture the semantic similarity. We point out that it is not appropriate to perform data reconstruction directly on word space, since it can X  X  discriminate words from different topics and is time consuming due to high dimension of word reconstruct the original text. Here we discuss two kinds of reconstructions: topic linear reconstruction, which reconstructs the document by linear combinations of the selected sentences; topic nonnegat ive reconstruction, which allows only additive, not subtractive, combinatio ns among the selected sentences [11]. For each reconstruction, we use the KL-divergence as the loss function to evaluate the quality of summary and develop effect ive algorithms to solve them. For the topic linear reconstruction, we introduce a greedy algorithm to minimize the loss function by forward stepwise selection that choose a sub-optimal sentence set to reconstruct the original documents. For the topic nonnegative linear reconstruc-tion, we design a multiplicative updating algorithm which guarantees converging monotonically at least locally optimal solution.

The rest of the paper is organized as follows. We first introduce the related work in section 2. In section 3, we give a brief introduction of the DSDR frame-work. In section 4, we discuss our TopicDSDR in detail. Experiment results and analysis are presented in section 5, and finally section 6 concludes the paper. Document summarization has been well st udied in literatures. Earlier studies conducted by [7][19] were based on simple features like term frequency, keyword, word position and etc.
 In recent years, graph-based ranking algorithms such as PageRank [4] and HITS [14] have been proposed for multi-document summarization in [8][21]. The graph-based algorithms used nodes to represent the sentences and edges to repre-sent the similarity among the sentences an d then selected the sentences according to their PageRank or HITS scores.

Topic decomposition has been intro duced into document summarization to help select the salient sentences. Arora et al. [1] used Latent Dirichlet Alloca-tion(LDA) [3] to capture the topics being covered by the documents and using the topic distribution as the basis of choosing the sentences to form the summary. HierSum proposed by [10] utilized a hierarchical LDA-style model to represent content specificity as a hierarchy of topic vocabulary distributions.
Matrix factorization methods have al so been employed to summarize docu-ments. Gong et al. [9] applied the Latent Sematic Analysis which used the Sin-gular Value Decomposition(SVD) to discover semantically important sentences for summarization. Arora et al. [2] combined the LDA with SVD to capture the latent topics and the orthogonal representation of topics in the documents. They first used the LDA to break down the documents into topics. For each topic, a sentence-by-term matrix was generated with the probabilities between the sen-tence and the terms, then applied the SVD decomposition to select the salient Matrix Factorization(NMF) [15] to se lect sentences for summary. The NMF can get a sparse weight vector of the semanti c feature vectors. The relevant score of a sentence was calculated based on the weight matrix and then chose the top-k scores to generate the summary.

Finding the optimal extractive summary can be seen as a combinatorial opti-mization problem which is NP-hard to solve [18]. One of the well-know methods is Maximal Marginal Relevance(MMR) [6] that used a greedy algorithm to select the most relevant sentences while avoid the redundancy by removing sentences similar to the already selected sentences . Global inference algorithms for docu-ment summarization have been studied in r ecent years [20][22]. They treated the summary task as an Integer Linear Programming (ILP) problem. The optimal algorithms they described are different f rom ours, since we directly optimize the feature space without similarity computing. He et al. [11] proposed the document summarization framework based on data reconstruction. We first introduce some definitions used in DSDR. Given a sen-term frequency vector of a sentence. N is the sentence number of the corpus and D is the vocabulary size of the corpus. X =[ x 1 ,x 2 ,...,x M ] T  X  V is the sum-mary sentences selected by DSDR and M&lt;N .

The DSDR X  X  generation of a summary ca n be described in two parts: (1) For any sentence v i in the document, DSDR selects the related sentences X from the candidate set to reconstruct the given sentence v i by learning a reconstruction function for the sentence v i : (2) For the entire document, DSDR attempts to find an optimal set of represen-tative sentences X to approximate the entire document V , by minimizing the reconstruction error with the following objective function: || . || is the L reconstruction and nonnegative linear reconstruction. 3.1 Linear Reconstruction The reconstruction function f i ( X ; a i ) can be defined as a linear function: Then we get the objective function of linear DSDR with regularization: A greedy algorithm has been pr oposed, see [11] for detail. 3.2 Nonnegative Linear Reconstruction The parameter a ij in linear reconstruction may be negative value, which means redundant information needs to be removed from the summary sentence set X . They tackled this problem by restricting a ij to be nonnegative. The optimal objective function can be defined as : || . || the j -th candidate is not selected. A global optimal solution was proposed in [11]. In this section, we first introduce the topic decomposition and the loss function based on KL divergence. Then we discuss two kinds of TopicDSDR algorithms: topic linear reconstruction and topic nonnegative reconstruction. 4.1 Topic Decomposition via Latent Dirichlet Allocation LDA proposed by [3] is a generative three-level hierarchical bayesian probability model for discrete data such as documents. It models documents using a latent topic layer. In LDA, for each document d , first a multinomial distribution  X  ( d ) over topics is sampled from a Dirichlet prior distribution with hyper parameter  X  . Then, for each word w di in document d ,atopic z di is sampled from the over words  X  ( z ) with a specific topic z . Therefore, the probability a word w is generated from the given document d is : In our work, we treat each sentence as a document. LDA is utilized to detect the topic distribution of sentences. We apply GibbsLDA++ [23] which using the Gibbs sampling to do the topic decomposition. After iteration, we can get two probability distributions: the topic distribution of sentences  X  and the word distribution of topics  X  . We use the topic distribution of sentences  X  as the sentence-by-topic matrix V , each row of V is sum to one. Our TopicDSDR is conducted on this matrix. 4.2 Loss Function of TopicDSDR In order to find a reconstruction function that can best reconstruct the original documents, we first need to define the loss function to evaluate the quality of the reconstruction. He et al. [11] employed the square of Euclidean distance as the loss function. Since every sentenc e is represented by topic distribution, we use the generalized Kullback-Leibler divergence to measure the divergence between the summary sentences and the original documents. The loss function of TopicDSDR can be defined: where V =[ v 1 ,v 2 ,...,v N ] T  X  IR N  X  K , K is the topic number. We set K =10 for TopicDSDR. The objective function of TopicDSDR can be defined as: 4.3 Topic Linear Reconstruction For the topic linear reconstruction, the optimal objective function can be defined as follow: where  X  is the regular parameter.

The optimization problem in Eq.(9) is NP-hard since we need to choose M sentences from N sentences, more detail discussion can be found in [11]. However, the Topic linear reconstruction is more complicated than linear reconstruction in [11], since it is difficult to eliminate the parameter A by matrix transformation using the Transductive Experiment Design(TED) [24]. Since we only need to select a small number of sentences (e.g. 10 to 20) from the original documents, we develop a greedy select algorithm to get a sub-optimal solution. We begin with the definition of an auxiliary function [15]: ditions are satisfied.
 Lemma 1. If G is an auxiliary function of F ,then F is non-increasing under the update Suppose we have already selected M  X  1 sentences as the summary, now we want to select the M -th sentence from the rest candidate set C to form M sentences as the summary that best reconstruct the original text. We need to minimize Eq.(9) and we have the following lemma: Lemma 2. Function is an auxiliary function for F ( a ) : F ( a )= F ( a ). We use the convexity of the log function which holds for all nonnegative  X  m that sum to unity. Setting then we have : From this inequality, it follows that G ( a, a ( t ) )  X  F ( a ) . " # Now the derivative of G ( a, a ( t ) ) with respect to a im is : where x m is a probability distribution and K k =1 x mk =1. sign ( . )isthesign function.

By setting the above derivative to be zero, we get the updating formula of a Algorithm 1 describes the TopicDSDR wi th linear reconstruction. In step(3) and (6), we select a sentence as the seed sentence that minimizes Eq.(9) with a ij = 1 N , which means every sentence has the equal chance to be selected as the summary. Then in the m -th iteration, for every candidate v i in the rest of set C , we put it in position m , using above updating formula to obtain the weight a v i . In step (15), we choose a sentence with maximum column weight score as the best candidate: score( SumColumn ( A  X  j )), SumColumn means the sum of column j in A . Finally, we get the summary sentences set with M sentences. The main cost of algorithm 1 is from (8) to (17). O ( NNK )forstep(3)to(5). O ( TNMK ) for step (12) to (16), where T is maximum iteration. Then we have O ( NNK + MN ( TNMK )) = O ( KN 2 + KTM 2 N 2 ) overall, where K and M are relatively small.
 4.4 Topic Nonnegative Linear Reconstruction For the topic nonnegative reconstruction, we have the following objective func-tion with regularization: auxiliary function for the topic nonnegative reconstruction: Lemma 3. Function is an auxiliary function for F ( a ) : Similar prove can be seen in Lemma 2. By fixing a i and setting the derivative of G with respect to  X  to be zero, we obtain the minimum solution of  X  : The derivative of G with respect to A with  X  fixing is : By setting the above derivative to be zero, we get the updating formula of a ij : Under the above updating rule, we can find at least a locally optimal solution, more detail can found in [15]. Algorithm 2 describes the TopicDSDR with non-according to Eq.(22). Then by fixing  X  ,weupdate a ij according to Eq.(24) until and the nonnegative values of  X  are chosen as the summary for the document set. Suppose the maximum number of iterations in step (4) and (6) are T 1 and T 2 respectively, the complexity of algorithm 2 is O ( T 1 N + T 2 KN 2 ). Algorithm 1. TopicDSDR with linear reconstruction Algorithm 2. TopicDSDR with nonnegative linear reconstruction 5.1 Data Sets and Evaluation System To evaluate our multi-document summarization algorithms, we use the DUC2006 and DUC2007 data sets from Document Understanding Conference(DUC) for generic automatic summarization evaluation. DUC2006 and DUC2007 have 50 and 45 document sets respectively, each with 25 news articles.

We use the ROUGE(Recall-Oriented Understudy for Gisting Evaluation) toolkit[17] to evaluate the proposed methods, which has been widely applied by DUC for performance evaluation. It measures the quality of a summary by counting the unit overlaps between the c andidate summary and a set of references summaries. In this experiment results, we show four F measures of ROUGE met-rics: ROUGE-1(unigram-based), ROUGE-2(bigram-based), ROUGE-SU4(the skip-bigram co-occurren ces statistics) and ROUGE-L( the longest common sub-sequence). 5.2 Implemented Systems and Result Analysis To compare with our proposed algorithms, we implement the following document summarization methods as the baseline systems.(1)LDA-based [1]. (2)LDA-SVD original DSDR using the Euclidean distance proposed in [11]. We evaluate it on two feature spaces. The first is word space: LinDSDR-Word and NonDSDR-Word. The second is topic probability space: LinDSDR-Prob and NonDSDR-Prob. (5)Top-icDSDR. Our TopicDSDR using the KL divergence are named LinTopicDSDR and NonTopicDSDR respectively.

Table 1 and Table 2 show the experiment results between TopicDSDR and other methods. From the results, we have the following observations: (1) The NonDSDR-Word based on term frequency has the worst performance, since the number of summary sentences generated by NonDSDR-Word is rel-atively large and the final  X  j values are almost the same for each candidate summary sentence. (2) The score of LinDSDR-Prob and NonDSDR-Prob are very closed. The LinDSDR-Prob does not have the weight parameter A to be learned. It takes few seconds to generate a summary for a document set while the NonDSDR-Prob may take half an hour. While our LinTopicDSDR is rela-tively poor since we need to learn the parameter A . (3) Our NonTopicDSDR outperforms the original DSDR and the other implemented systems. This is be-cause the extracted summary based on word space would be dominated by the frequency of the words. The topic decomposition can distinguish words from dif-ferent topics. What X  X  more, our model reduces word space to topic space which is more efficient. The number of sentences generated by nonnegative DSDR and our TopicDSDR can be seen in Table 3, which shows that our approach is more closed to the natural form of summary. (4) The NMF outperforms the LDA-SVD and LDA-based. The NMF with nonnegative weight and feature vectors, which is similar to the nonnegative reconstruction. In this paper, we propose a novel summarization model by incorporating the topic decomposition with data reconstruction, named TopicDSDR. We develop an optimization frame work based on KL divergence. The experiment results demonstrated TopicDSDR can not only best reconstruct the original documents but also capture the semantics similarity, since it directly optimizing the topic space rather than the word space. TopicDSDR is more efficient than the original DSDR model and more closed to the natural form of summary. It is interesting to develop effective solutions for linear TopicDSDR by eliminating the parameter A in our future work.
 Acknowledgments. We thank the anonymous reviewers for their valuable and constructive comments. This work is financially supported by NSFC Grant 61272340.

