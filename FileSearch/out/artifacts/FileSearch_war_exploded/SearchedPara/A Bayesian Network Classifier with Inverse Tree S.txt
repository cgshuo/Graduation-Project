 We propose a B ayesian-n etwork c lassifier with i nverse-t ree structure (BNCIT) for joint classification and variable selec-tion. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve clas-sifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C ; thus, the classifier-training process returns a parent set for C , which is a subset of the Markov blanket of C .BNCIT uses voxels in the parent set, and voxels that are probabilis-tically equivalent to them, as variables for classification of new image data. Since the data set has a limited num-ber of samples, we use the jackknife method to determine whether the classifier generat ed by BNCIT is a statistical artifact. In order to enhance stability and improve classifi-cation accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an ef-ficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.
 G.3 [ Probability and Statistics ]: Multivariate statistics; J.3 [ Life and Medical Sciences ]: Medical information systems Algorithms a Bayesian-network classifier with inverse-tree structure. We also describe a validation method to determine whether the classifier generated from the training data is artifactual.
To capture the associations among probabilistically equiv-alent voxels, we employ a latent-variable model, in which the latent variable represents the state of these variables. We use the iterated conditional modes (ICM) algorithm [2] to find the maximum of the posterior marginal estimate for the latent variable. However, inferring the state of the latent variable is very computationally demanding. We propose a novel method that, in practice, dramatically reduces the computational requirements for computing the state of the latent variable.
There are two different approaches to variable selection [25, 19]. The filter method considers variable selection to be a preprocessing step, and therefore independent of clas-sification. For example, we can calculate the pairwise corre-lation coefficients between V i and C ; then sort variables by correlation coefficient; and finally select the highest-ranked variables for constructing a classifier. Regardless of the clas-sification model, variable selection is independent of the classifier-generation process. On the other hand, the em-bedded approach integrates variable selection and classifier training, and therefore simultaneously selects variables and trains the classifier.

The classifier that we propose in this paper is based on a Bayesian-network representation. There exist different Bayesian-network models, such as na  X   X ve-Bayes [15, 28, 14], tree augmented na  X   X ve-Bayes (TAN) [16], BN augmented na  X   X ve-Bayes (BAN) [7], and the general BN classifier (GBN) [7, 8]. The primary difference among these classifiers centers on various restrictions on allowed BN structures. The details of these classifiers are presented in Section 2.2.
The remainder of this paper is organized as follows. Sec-tion 2 provides an introduction to Bayesian networks and Bayesian-network classifiers. The details of the BNCIT al-gorithm are presented in Section 3. Section 4 provides ex-perimental results to verify the accuracy of BNCIT. We con-clude in Section 5.
A Bayesian Network (BN) [29, 24, 5, 20] is a probabilistic graphical model defined by a pair B =( G , X ). G =( V , E ) is a directed acyclic graph (DAG) called the structure of aBN. X  X  X  represents a random variable in the problem domain and V is the set of variables. The edge set E denotes probabilistic associations among nodes. For a given node X  X  X  ,aparentof X isanodefromwhichthereexistsa directed edge to X . We denote the set of parents of X as pa ( X ).

The conditional probability  X  ijk = P ( X i = k | pa ( X i )= j ) is the probability that variable X i assumes state k when pa ( X i ) assume states j .If X i has no parents, then  X  ijk corresponds to the marginal probability of X i .Wedenote the distribution of X i with fixed parent states j by  X  ij ,and we denote the conditional-probability table (CPT) of X i by  X  . X ,thesetofall  X  ijk , represents the parameters of a BN.
We can use a BN with C as one of its nodes for classifica-tion [15, 16]. After we have trained a classifier, classification is performed by using Bayes X  rule. The value of C can be determined from C  X  =max P ( C | V ). P ( C | V )canbecal-culated using BN-inference algorithms, such as the junction-tree algorithm [11]. For the case in which all variables in new instances assume known states, as is often the case for image data, only nodes in the Markov blanket mb ( C )of C will affect classification, so we can simplify the classifier to include only nodes in mb ( C ). Let I ( X ; Y | Z )representthe statement that X is conditionally independent of Y given Z . The Markov blanket of node X is defined as the smallest set such that I ( X ; Y | mb ( X )) for all Y  X  V \{ Y, mb ( X ) } [29, 26]. In a DAG, the union of X  X  X  parents, its children, and the parents of its children, forms mb ( X ).

A BN classifier that is generated from data based on the approach described above, differs from other classification algorithms, in that it does not directly minimize some type of classification error. Instead, this BN classifier models the joint distribution of the data.

Figure 1 illustrates the structures of different BN classi-fiers. Na  X   X ve Bayes (Figure 1 (a)) assumes that predictive variables { V i } are conditionally independent of each other given C . This assumption seems unrealistic in many appli-cations, yet often results in accurate classification in prac-tice [16, 30]. In addition, Domingos and Pazzani [14] have shown that Na  X   X ve Bayes may be optimal under zero-one loss, even when this assumption does not hold. Generating na  X   X ve Bayes classifiers, and performing inference on them, are of-ten computationally tractable in practice, even for large data sets. TAN (Figure 1 (b)) is a BN classifier that may improve the classification performance of na  X   X ve Bayes by relaxing the conditional-independence assumption. It allows a tree struc-ture among the child nodes of C . BAN (Figure 1 (c)) also relaxes the conditional-independence assumption of na  X   X ve Bayes, in this case by allowing an unrestricted BN struc-ture among the child nodes. TAN and BAN are both na  X   X ve Bayes-based classifiers. As opposed to them, GBN (Figure 1 (d)) uses the Markov blanket of C for classification. In at least some domains, GBN performs better than na  X   X ve Bayes-based classifiers, achieving lower classification error rates [7], at the cost of greater computational demands.
As an example of a complex data set, we describe the analysis of a study of cerebral-volume changes among dif-ferent groups over time. We have MR images for two time points, t 1and t 2, along with a class variable, C ,foreach subject in our study. These MR images are skull-stripped to remove the skull and non-brain tissues. The images are then segmented into gray matter, white matter, and cere-brospinal fluid [18]. In order to compare brain volumes ac-quired from different subjects and time points, we normalize these images by registering them to a standard coordinate system. For each subject, this registration process gener-ates a RAVENS map, which is a density map defined on a stereotaxic canonical space, whose voxel values represent re-gional volumetric measurements for that image [13, 12, 31]; this RAVENS map forms the voxel-wise morphometric data for each subject. To correct fo r registration error, we apply an isotropic Gaussian smoothing kernel to each RAVENS map. Subsequently, we generate volumetric difference maps
BNCIT is a method that integrates the selection of a sub-set of variables, and the construction of a classifier based on these variables. The subset of Bayesian networks gen-erated by the BNCIT algorithm is shown in Figure 1 (e). First, the classifier-training process generates a parent set for C ; we then use the ML or MAP method to estimate the parameters. Variables in pa ( C ) naturally form a subset of the variables in mb ( C ), which are most informative about C . BNCIT X  X  BN-construction process is illustrated in Algo-rithm 1. Let R be pa ( C ). BNCIT starts with an empty R . Iteratively, variables are increasingly incorporated into R . R i = R i  X  1  X  X  R i } ,where R i istheparentnodeaddedin iteration i and R i the parent set after that iteration. Con-sider  X  ( V ), which is the difference between the BDeu met-ric after adding edge V  X  C to the current BN structure, from that for the network without this edge. If  X  ( V ) &gt; 0, the BN with this edge is more likely to have generated the data than the BN without this edge. We search for R i that maximizes  X  ( R ), and add R i  X  C to R . If there is no R such that  X  ( R ) &gt; 0, then the network-generation process stops. Generally, D is noisy due to the small number of samples. Furthermore, using a single voxel, such as R i ,for classification makes the classifier unstable. To correct these problems, we identify a set of variables that are probabilis-tically equivalent to each R i , in the hope that the classifier will be stabilized by using these voxels jointly. Toward this end, we define variable V to be probabilistically equivalent to variable R if a similarity measure s ( V, R ) is large. For binary V and R , one measure of probabilistic equivalence is P ( V =0 ,R =1)  X  0and P ( V =1 ,R =0)  X  0. We can then use Bayesian thresholding [23] or belief-map learning [6] to find variables probabilistically equivalent to R i  X  1 .
An important property of this variable-selection process is that R is a subset of mb ( C ). Let D be a collection of in-dependent and identically distributed samples from a prob-ability distribution P .Let G be a DAG and G be G with adding X i  X  X j .Ascoringmetric S ( G , D ) is called locally true. The BDeu score is locally consistent [9]. In BNCIT, if a variable outside mb ( C ) were to be added to R ,voxels inside mb ( C ) would be added before it, because of local con-sistency of the BDeu score. Fur thermore, if all variables in mb ( C )werein R , they would prevent other variables from being added. Thus all variables in R are inside mb ( C ). BNCIT may not find the complete set mb ( C ), but it will not add variables outside mb ( C )to R . Therefore, R con-sists of variables that are predictive of C .

The BDeu score incorporates a tradeoff between goodness of fit and model complexity. A complicated structure that fits the data well, but has poor predictive power, will not maximize the BDeu score. The number of variables in R is referred to as the the dimensionality of BNCIT. The BDeu score guarantees that BNCIT will generate a BN with low model order; however, we can also set an upper bound on model order to ensure that BNCIT generates a parsimonious model. As we show in section 4, BNCIT X  X  training and clas-sification processes are efficient because of the compactness of the BN model generated by BNCIT.
In a MR image study, D usually contains only dozens or perhaps 100 samples. In a machine-learning problem that {
R i , E i } , we introduce a binary latent variable L i to repre-sent the state of cluster i , as shown in figure 2. We have and we try to find L that maximizes p ( L, R, E ). Again, p ( L, R, E ) can be computed using BDeu. Let Q be the data set with k instances containing only { L, R, E } .Weobserve the states of { R j , E j } for instances j =1 , 2 ,...,k .The goal is to infer the state of the latent variable L for each instance in Q . There does not exist a generally applicable optimization technique that solves this problem, even for small k . The maximum of the posterior marginal (MPM) estimate of L is defined as where L j is the state of L for instance j .

There are different techniques of obtaining the MPM es-timate. Gibbs sampling is a stochastic relaxation technique that can return a globally optimal solution if an appropri-ate temperature schema is chosen [17]. In contrast, ICM is a deterministic greedy-search algorithm [2], which will often return a locally optimum result. Furthermore, both of these approaches are based on computation of BDeu ( L \ L j ,R, E ), and are subsumed under the generalized-EM framework. Because Gibbs sampling is very computationally intensive, we have implemented ICM in BNCIT to obtain L MPM .In each iteration, ICM sequentially updates L j ,bycalculating BDeu ( L \ L j ,R, E ) for all possible states of L j ,andthen selecting the state that maximizes this metric.

To infer the hidden state of L , BNCIT computes the BDeu score 2 k times during each iteration, which requires a great amount of time. Consider a particular iteration. Let Q j u be the data set with L j = u . The difference between Q and Q u is only one instance. If we can derive an equation for  X  j = BDeu ( Q j u )  X  BDeu ( Q ), then an incremental com-putation algorithm for BDeu score follows: (1) compute BDeu ( Q ) at the begin of each iteration; (2) use  X  u j to com-pute BDeu ( Q j u ); (3) find L j MPM based on BDeu ( Q j u )for each instance. A fter iteration i ,weupdate Q by L MPM and proceed to the next iteration.

We present here the equation for  X  u i . InEquation(1),the computation of the BDeu metric is carried out variable by variable. Each instance has an entry index in the SST for each variable. For a fixed binary variable X ,whenupdating the current instance A with a new instance B, there are two different cases, as shown in figure 3 and 4. We consider first what we call case 1 (figure 3), in which the indices of instances A and B ( I A and I B ) are in the same row in the SST. Note that if I A = I B , there is no change in the BDeu score. Let a and b be the sufficient statistics for I A and I B , respectively. The logarithm of the BDeu score of a row in log- X  is approximately 3 times that of  X . The reduction of computational cost of the incremental-updating algorithm relative to the whole-sample method is 1 3  X  4 rqk .Forasmall data set with r =2 ,q =1 , X  = 20, this is on the order of 10
A typical application of using BNCIT to analyze MR im-ages is as follows: a training data set is provided, containing a subject X  X  MR images and a class variable, such as whether a subject has Alzheimer X  X  disease; we train a classifier by applying BNCIT to the training data set; we then use the classifier to predict the probability of a new subject X  X  having Alzheimer X  X  disease, based on the MR images for this new subject.

To use BNCIT, we first preprocess the MR images in the training data set using the protocol described in section 2.3. The output of this step is a set of difference maps that are registered to a standard coordinate space. BNCIT then se-lects a set of variables using the variable selection method described in Section 3.1.1 and the validation method in Sec-tion 3.1.2. These variables can be divided into different clus-ters. Each cluster consists of a variable in R and its prob-abilistically equivalent variables. BNCIT then introduces a latent variable, to represent the state of a cluster. The algorithm described in Section 3.1.3 infers the unobserved values of the latent variables. BNCIT then generates a BN classifier, with a structure such that latent variables are the parents of the class variable. This classifier is used to label new instances. Figure 5 shows an overview of this process.
When we have MR images for a new subject, after pre-processing, choosing the selected variables, and inferring the states of latent variables, the probability that this subject is in a particular class can be predicted by the BN classifier generated by BNCIT from the training data.
We present our evaluation of BNCIT in this section. The data sets we used for evaluation contain associations be-tween cerebral morphology and age or sex, in 119 subjects; data for each subject consists of 128  X  128  X  94 = 1 , 540 , 096 voxel variables and one class variable. Age= { + ,  X  X  rep-resents a subject X  X  age in [60 , 70] and [70 , 80], respectively. Sex= { + ,  X  X  represents male or female sex, respectively. We obtained the data used in this experiment from normal el-derly subjects of the Baltimore Longitudinal Study of Aging [1]. We refer to the data set GM-Age as that consisting of gray-matter morphological changes associated with the class variable age; similarly, we refer to the data set GM-Sex as that consisting of gray-matter morphological changes associ-ated with sex, WM-Age consisting of white-matter morpho-logical changes associated with age, and WM-Sex consisting of white-matter morphological changes associated with sex. All variables in these data sets are binary; each voxel as-sumes the state 1 if there is volume loss in the corresponding RAVENS-map voxel location, or 0 if there is no volume loss.
First we define metrics for evaluating a classifier X  X  accu-racy. Let { Y, N } represent the positive and negative clas-sifications produced by a classifier, respectively. The true-positive rate (TPR) and false-positive rate (FPR) are de-Leave-one-out 0.88 0.90 0.87 0.88 Table 2: Classification accuracy for GM-Age, GM-Sex, WM-Age and WM-Sex. aggregated ROC graphs for all data sets for five-fold cross-validation. As expected, when  X   X  1, BNCIT has a very low FPR and a low TPR; when  X   X  0, BNCIT has a low false-negative rate; and when  X  is in the middle of this range, BNCIT balances TPR and FPR. The areas under the ROC curves (AUCs) when applying BNCIT to the GM-Age, GM-Sex, WM-Age, and WM-Sex data sets are 0.87, 0.94, 0.90, and 0.89, respectively. Figure 6: The aggregated ROC graph of CV(5). (a) GM-Age. (b) GM-Sex. (c) WM-Age. (d) WM-Sex.

In terms of the computational cost of BNCIT, our im-plementation on a Silicon Graphics (Mountain View, CA) Origin workstation required approximately an hour for vari-able selection, and 20 hours for inferring the latent variables. The computation time is related to the subject number, the voxel number of MR images , and the dimensionality of the model. The time to label a new instance is approximately 30 minutes.

In addition, we compared the accuracies  X  (Equation 9) of multi-layer perceptron (MLP), logistic regression, decision tree, prism rule set, na  X   X ve Bayes, support vector classifier, and bagging, with that of BNCIT based on leave-one-out cross-validation on all data sets, with the selected variables. As shown in Table 3, BNCIT X  X  accuracy is greater than those of the other classifiers, for all data sets. Note that we did not compare the AUCs of these classifier because some classifiers had deterministic classification outputs and did not have ROC graphs.
 Figure 7: Validation results for all data sets. (a) f ( R ) of GM-Age; (b) f ( E ) of GM-Age; (c) f ( R ) of GM-Sex; (d) f ( E ) of GM-Sex; (e) f ( R ) of WM-Age; (f) f ( E ) of WM-Age; (g) f ( R ) of WM-Sex; (h) f ( E ) of WM-Sex. believe that this result is explained by BNCIT X  X  joint vari-able selection and classifier generation. For other types of classifiers, such as na  X   X ve Bayes, accuracy could be im-proved by adopting a variable-selection method that is de-signed particularly for na  X   X ve Bayes, if this is possible for data with such high dimensionality. Regardless of which variable-selection method we use, we need to know whether the difference in classification accuracy is statistically sig-nificant, or is due to random artifact. Towards this end, we can use resampling methods, such as the bootstrap, to resample the data, and compare overall performance. We plan to implement bootstrap resampling for BNCIT.
 This work was supported by The Human Brain Project, National Institutes of Health grant R01 AG13743, which is funded by the National Institute of Aging, the National [19] I. Guyon and A. Elisseeff. An introduction to variable [20] D. Heckerman. Bayesian networks for knowledge. In [21] D. Heckerman. A tutorial on learning with Bayesian [22] E. H. Herskovits. Computer-based probabilistic-network [23] E. H. Herskovits, H. Peng, and C. Davatzikos. A [24] F. Jensen. An Introduction to Bayesian Networks . [25] R. Kohavi and G. H. John. Wrappers for feature [26] D. Koller and M. Sahami. Towards optimal feature
