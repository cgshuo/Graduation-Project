 Recommender systems provide consumers with ratings of items. These ratings are based on a set of ratings that were obtained from a wide scope of users. Predicting the ratings can be formulated as a regression problem. Ensemble regres-sion methods are effective tools that improve the results of simple regression algorithms by iteratively applying the sim-ple algorithm to a diverse set of inputs. The present paper describes a simple and effective ensemble regressor for the prediction of missing ratings in recommender systems. The ensemble method is an adaptation of the AdaBoost regres-sion algorithm for recommendation tasks. In all iterations, interpolation weights for all nearest neighbors are simulta-neously derived by minimizing the root mean squared er-ror. From iteration to iterati on instances that are hard to predict are reinforced by manipulating their weights in the goal function that needs to be minimized. The experimen-tal evaluation demonstrates that the ensemble methodology significantly improves the predictive performance of single neighborhood-based collaborative filtering.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Ensemble Methods, Collaborative Filtering, Neighborhood based Collaborative Filtering
In recent years we witness an ever growing increase in the utilization of recommender systems in various retailing settings since these systems prove to be an effective mar-keting tool which increases sales. Recommender systems are an integral part of many online stores such as Ama-zon.com, Buy.com, etc. One of the most famous examples of a recommender system is Netflix [1]. This system contains movie ratings for over 17 , 000 movies provided by more than 480 , 000 users. A contest whose purpose is to improve the re-sults of the current Netflix re commender system is currently taking place.

The present paper focuses on collaborative filtering (CF) techniques in which customers grade the products they pur-chased. Given the supplied ratings, the system recommends other products that might be of interest to the users. At the core of the technique lies a prediction algorithm whose purpose is to approximate for every user the ratings of items they have not rated. These predictions are based on items that the user has rated and ratings provided by other users. According to the predicted ratings, the system recommends items which might interest the users. Usually, the system will recommend the items with the highest predicted ratings.
A predominant approach to collaborative filtering is neigh-borhood based (kNN -k -nearest neighbors). A user-item-preference rating is interpolated from ratings of similar items and/or users. While past kNN methods relate items (or users) by an arbitrary similarity function, modern kNN meth-ods discover the interpolation weights by using a global op-timization of a cost function pertaining to all weights simul-taneously [6].

The main idea of the ensemble methodology is to combine a set of models, each of which solves the same original task, in order to obtain a better composite global model, with more accurate and reliable estimates or decisions than those produced by using a single model [7]. Ensemble classifiers can be classified into two types: homogeneous and heteroge-nous. Homogeneous ensembles utilize different versions of the same core model while the heterogenous approach com-bines models of different types. In the homogeneous ap-proach, it is important to choose a simple and basic ( weak ) model (for example, in case of classification, one should use an inducer that is just a little better than the random one) in order to obtain an effective ensemble. Choosing a strong model will produce very little improvement since it does not provide enough diversity -one of the key principles of the ensemble methodology.

Experimental results show that ensembles can produce better results than any single model. Bell et al. [5], for in-stance, used a combination of 107 different models in their progress-prize winning solution to the Netflix challenge. In order to fuse the results from the ensembles they used a lin-ear regression approach. Different base algorithms for cre-ating the models were employed. Their findings show that it is better to use substantially different approaches than to refine a particular method. In [3], a boosting algorithm that is based on the AdaBoost algorithm, is proposed in order to provide recommendations in the form of ranking .Theal-gorithm combines many weak rankings -each of them may have only a weak correlation to the target ranking. The output of ranking algorithms is a list of recommendations that are sorted according to their relevance. This is differ-ent from the output of our proposed algorithm in which the rating of every item is approximated.

The present paper introduces a homogeneous ensemble algorithm which is a modified version of the AdaBoost.RT ensemble regressor [8]. As a homogeneous ensemble, it in-corporates a simple and effective regression algorithm which minimizes the prediction error by solving the gradient of an error cost-function. The AdaBoost algorithm, along with its different versions, is an effective ensemble method that is used to improve the results of a given inducer or regressor where the underlying principal is to reinforce instances that are harder to predict (produce a relatively high prediction er-ror). The amount of reinforcem ent is determined according to a weight that is assigned to every instance and is itera-tively updated. In each iteration, the regressor (or inducer) is applied to the instances and the current weights. The weights of the problematic instances are increased. Techni-cally, the same effect is achieved by reducing the weights of the instances that are easy to predict and normalizing all the weights to sum up to 1. Initially, the same weight is assigned to all instances.

The contribution of the proposed algorithm is two-fold: first, it demonstrates how ensemble methods can be utilized in order to improve the performance of a single CF method; second, a novel adaptive data-driven instance-enforcement criterion is introduced into the ensemble regressor.
The rest of the paper is organized as follows: A detailed description of the proposed algorithm is given in Section 2. In Section 3 we provide experimental results to demonstrate the effectiveness of the proposed method. Conclusions are in Section 4.
Formally, the ratings in a recommender system can be represented as a set of m rating triplets ( u, i, r ui ) 1 ,...,m where u  X  U is a user, i  X  I is an item and r ui is the rating that user u gave item i .Wedenoteby | U | the number users and by | I | the number of items. Usually, m | U | X | I | since the number of items, | I | ,isveryhigh and users rate only the items they tried out. It is rare, if impossible, to find even a small subset of users that tried out all the items and also provided ratings for all of them. To make things worse, even if a user tried out an item, he or she might not provide their rating due to lack of motivation. The prediction algorithm produces a set of predicted ratings ( u, i, r ui ) k ,k =1 ,..., | U | X | I | .

In each iteration of the proposed ensemble regression al-gorithm, the missing ratings are approximated by solving a minimization problem.

Let  X  r ui be the approximation of the rating of the i -th item by the u -th user. We set  X  r ui to be where u =1 ,...,U ; i =1 ,...,I and: N ( u ) defines a neighborhood for a user u . The neighbor-hood includes the users whose ratings are similar to those of user u . The similarity is determined according to the Pearson correlation. Specifically, the users that are included in
N ( u ) are those who produce the top ranking (highest) correlations.

The initial approximation b ui is defined as where  X  is the average of all the provided ratings by all the users, b u is the average of the provided ratings of user and b i is the average of all the provided rating of item Formally, given the indicator function these quantities are given by: and
We can now formalize the minimization problem. Define the total error of the approximation by the function where  X  ui is a weight that is assigned to every rating. These weights are determined by the ensemble algorithm that is described in Section 1.
 The approximation is obtained by the minimization of E ( W ) which is found by solving  X  E ( W ) = 0. We assume that w uv is not necessarily equal to w vu i.e. the ratings of user u do not influence the rating of user v by the same amount as the ratings of user v influence the rating of user u . Accordingly, the gradient of E ( W )isgivenby where N ( u )and R ( u ) were defined above. Thus, for every similarity weight, we obtain a system of | N ( u ) | equations in | N ( u ) | variables. Adding the constraint results in a system of N ( u ) + 1 linear equations in | N variables.

Before solving any of the obtained systems of equations, one must first incorporate the following adjustments: 1. In case r vi does not exist, it is set to be b vi .This 2. When users u and v have the same rating values and 3. When user u and v have the same rating values with After the predictions are obtained, ratings that are above and below the rating range are truncated to the maximal and minimal ratings, respectively.
The ratings obtained by the proposed algorithm are en-hanced by using ensemble regression. Specifically, we use a modified version of the AdaBoost.RT algorithm [8]. The AdaBoost-based algorithms improve the results of a classi-fier (or regressor) via an iterative process which reinforces instances that are harder to predict (or approximate). The AdaBoost.RT algorithm incorporates a relative-error param-eter  X  in the weight reinforcement criterion. Only instances whose relative errors exceed  X  are reinforced. Both  X  and the number of iterations are given as parameters. Our proposed scheme differs from the AdaBoost.RT algorithm by the rein-forcement criterion that it uses. Rather than using a relative error  X , a deviation factor  X  isused. Ineachiterationwe calculate the mean and standard deviation of the prediction errors. Only instances whose p rediction errors exceed the mean by a given factor  X  multiplied by the standard devia-tion are reinforced. Although the algorithm still requires a parameter, the advantage in employing this scheme is two-fold: first, the reinforcement threshold is dynamic, adapting itself at each iteration to the obtained errors; and second, the reinforcement is determined according to the error statistics rather than a predefined threshold.

Bothourproposedmethodandthe AdaBoost.RT algo-rithm [8] use the same reinforcement factor for all the hard-to-predict instances i.e., their weights are multiplied by the same factor. This is contrary to the AdaBoost.R2 algo-rithm [2] where the reinforced weight of each individual in-stance is determined according to the magnitude of its cor-responding prediction error. Technically, the reinforcement effect is achieved by reducing the weight of instances that are easy to approximate (achieve a low prediction error) and normalization of the resulting weights so their sum will be equal to 1.

The proposed ensemble algorithm is described in Algo-rithm 1. The input to the algorithm is a set of m rating triplets as defined above. The algorithm produces a set of predicted ratings ( u, i, r ui ) k ,k =1 ,..., | U | X | I | the number users and | I | is the number of items.
Theproposedalgorithmwastestedonthe MovieLens rat-ing database. The database contains 3900 distinct movies and 6040 users. The predictiv e model is induc ed for 100 ran-domly selected users. The training set was constructed from 70% of the ratings of the selected users. The algorithm was used to predict the remaining 30% ratings. For example, if a user rated 20 movies, 14 of them were used for the training and the other 6 were included in the test set. Although, the predictive model refers to only 100 users, the users that were included in N ( v ) were selected from the entire database.
Thesizeof N ( v ) was set to 50. The effectiveness of each iteration is measured accordin g to the root mean square er-ror (RMSE). Figure 1 shows the decrease in the RMSE with each iteration demonstrating the improvement that is ob-tained by using the proposed ensemble algorithm. In ten iterations of the ensemble algorithm, the RMSE decreased by 41.86% where the most dramatic improvement -21.03% -was achieved after the first iteration. We compared our results to those obtained by the SVD++ [4] algorithm.
An ensemble regression algorithm for the approximation of rating values in a k-NN recommender system was intro-duced. The regression algorithm, which is at the heart of every ensemble iteration, minimizes the root mean squared prediction error by directly solving the gradient of a cost function. This is achieved via overdetermined systems of lin-Algorithm 1 Modified AdaBoost.RT ensemble regressor 1. Asetof m rating triplets ( u, i, r ui ) k ,k =1 ,...,m 2. The rating prediction algorithm from the beginning of Section 2. 3. Integer T specifying the number of iterations. 4. Error factor  X  (default  X  =1).
 1. Set iteration number t =1. 2. Set a weight to every triplet according to a uniform dis-tribution  X  t k = 1 / m .
 Iterate. While t&lt;T 1. Call the rating prediction algorithm to obtain the pre-dicted ratings  X  r t ui . 2. Calculate the mean,  X   X  t , and the standard deviation, of the errors  X  r t ui  X  r t ui . 3. Calculate the error rate where Q = ( u, i ):  X  r t ui  X  r t ui  X   X   X  t &gt; X   X   X  t 4. Set the weights of the instances for the next iteration according to where N t is a normalization factor whose value is chosen such that  X  t +1 k form a distribution. 5. Set t = t +1 Figure 1: A plot of the root mean square predic-tion error as a function of the iteration number. An improvement is clearly noticed. The total improve-ment that is achieved by the ensemble algorithm is 41.86%. ear equations. Occasionally, these systems cannot be solved due to similar equations whose free terms differ. Currently, this situation is resolved by arbitrarily choosing one of the equations. In some cases, the linear system is underdeter-mined. At present, the algorithm chooses one of the possible solutions. An improvement to both of these situations is to choose the solution which minimizes the RMSE. Further-more, the method is going to be extended and tested on larger datasets such as the Netflix [1] dataset.

The proposed scheme achieves good results which render ensemble regressors that are based on a weak collaborative filtering algorithm as a legitimate tool for recommender sys-tems. However, further research should be done in order to extend the proposed approach so that it achieves the results obtained by stronger collaborative filtering algorithms such as those based on SVD matrix-factorization.
 The authors are grateful to Dr. Yehuda Koren for his in-valuable guidelines in the implementation of his SVD++ algorithm. [1] Netflix, inc.  X  X etflix prize X , www.netflixprize.com. [2] H. Drucker. Improving re gressor using boosting. In [3] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram [4] Yehuda Koren. Factorization meets the neighborhood: [5] Y. Koren R. M. Bell and C. Volinsky. The bellkor [6] Y. Koren R. M. Bell and C. Volinsky. Modeling [7] Lior Rokach. Taxonomy for characterizing ensemble [8] D. P. Solomatine and D. L. Shrestha. Adaboost.rt: A
