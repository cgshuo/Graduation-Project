 Data warehousing (DW) and On-line analytical processing (OLAP) consist of essen-tial elements of decision support, increasingly becoming a focus of the database in-dustry. Deviating from the queries to transactional databases, the nature of the queries to DWs is more complex, longer lasting and read-intensive, but less attributes to touch. Since column-store database only retrieves attributes required in the query, it can optimize cache performance. Thus columnar storage is more suitable for DW in this sense, and now the Microsoft also adds columnar storage in SQL server 2012 to improve performance on data warehousing queries [5]. With the increment of RAM density and reduction of its price there are a number of in-memory column-oriented database systems such as MonetDB [1, 2] and C-Store [3] emerging for better per-formance. These systems offer order-of-magnitude gains against traditional, row-oriented databases on certain workloads, particularly on read-intensive analytical processing workloads, such as those encountered in DWs. In this paper, we propose a framework for OLAP called CDDTA-MMDB (Columnar Direct Dimensional Tuple Access-Main Memory Database) to efficiently process the query on DWs modeled with star scheme. This work is on the basis of our prior work to develop one-pass join algorithm [7] which is variant of invisible join [10]. The following is our focus inno-vations:  X 
The Storage Model. In CDDTA-MMDB we employ the decomposed storage model (DSM) [4] which vertically partitions the tables and maintain a collection of triplets &lt; surrogate, key, value &gt; tables. In addition to make use of CPU cache line more efficiently, this representation is one of crucial measures for our goal to gain light-weight materialization and LWMJoin algorithm.  X 
Materialization Strategy in Column-Store Database. We propose a new materia-lization strategy called light-weight materialization (LWM) which adopts notion of late materialization (LM) to push the materialization as late as possible. Compared to the LM, LWM tries to avoid producing intermediate data structure such as bit-map and reduce random memory access during the whole query plan. Experiments show that CDDTA-Join [7] equipping with LWM (CDDTA-LWMJoin, in the fol-lowing we contract it to LWMJoin) outp erforms the state-of-art join algorithm such as radix-cluster join [8], invisible join [10] in the context of DWs modeled with star schema. To the best of our knowledge, this is first time to propose such materialization for column-store database. storage, materialization and join algorithms is described. Section 3 gives the light-weight materialization techniques. In Section 4, system overview is presented. Sec-tion 5 shows representative experimental results. Finally, conclusion is drawn in section 6. Storage . The decomposition storage Model (DSM) [4] reduces each n -attribute rela-tion to n separate relations. DSM organizes values from same attribute close in physi-cal. Supposed the record reconstruction cost is low, cache performance of main-memory database systems can be improved by DSM. As mentioned to the detail of representation of DSM, MonetDB proposed the Binary Units (or BUNs) [2] to con-struct the decomposed table in the form of &lt; OID , value &gt; pairs. Materialization and Join Algorithm in Column-Store. Column-store modifies the physical data organization in database. In logical, the applications treat column-store the same as row-store. At some point in a query plan, a column-oriented DBMS must gies of materializing are divided into two categories: early materialization (EM) and late materialization (LM). A recent work on materialization in column-store is in [14]. It used a dynamic and cache-friendly data structure, called cracker maps which provide direct mapping between the pairs of attributes in the query, to reduce the ran-dom data access. In [6], trade-offs between different strategies are systematically explored and advices are given for choosing a strategy for a particular query. 
The na X ve join algorithm using LM strategy (na X ve LM join) in column-store data-base is about 2x slower than the join algorithm using EM strategy (EM join) [9]. The main reason is that in na X ve LM join the access to inner table is out-of-order, causing the high cache miss. The state-of-art LM join algorithms in column store such as radix hash join [8], MCJoin[13] and invisible join [10] all try to avoid the random memory access to inner table. Radix-clustered hash join [8] partitions the input relation into H clusters. By controlling the number of clustering bits each pass, the H clusters gener-ated can be kept under the count of Translation Lookaside Bu ff er (TLB) entries and cache lines available on that CPU. The random access only happens in storage block. can reduce the random memory accesses. LWMJoin tries to avoid producing the in-termediate data structure and further reduces memory access compared to invisible join [10] and CDDTA-Join [7]. Experiments show that LWMJoin runs about 4x faster than invisible join and 2x faster than radix-cluster join on average. Column-store is physical modification to database. In terms of logical and view level it is identical to row-store. The applications involved with database, row-oriented or column-oriented, treat the interface as ro w-oriented. So a column-oriented DBMS must glue multiple attributes, at some time, from the same logical tuple into a physi-materialization (EM) and late materialization (LM). At each point when a column is needed, it is read into CPU and added to intermediate tuple representation. This policy to reconstruct the tuple is called early materialization . It is not surprising that EM is not always the best strategy for a column-store [9], because some CPU works are useless. Consider this scenario: a column-s tore DBMS place all columns of a table in separate files and sort them in the same order; a query has two selection operators 1  X  higher selective than 2  X  . Under the EM policy, the operations are as follows: read in a block of R .a and a block of R .b; glue them together to form the rows and apply 1  X  and 2  X  on the rows in turn. Then, send the tuples for aggregation. A more efficient way for this process is that: keep two intermediate position lists; scan the block of R .a and R .b, meanwhile apply 1  X  and 2  X  on them respectively; use the lists to record whether attributes in the corresponding position in column satisfy the prediction; then use position-wise AND to intersect the two position lists. According to intersected result, the block of R .a is re-accessed for aggregation. The latter strategy is called late materialization . Light-weight materialization (LWM) adopts notion of LM to push the materialization as far as possible in query plan, but try to reduce the memory access and avoid producing intermediate data structure. Instead of re-accessing the block of R .a to get record value for GROUP BY, LWM just lets the key of triplet pro-duced by scanning ( R .a, R .b) go through whole query plan. Furthermore, LWM doesn X  X  need to be assisted by the intermediate data structure except triplets (in some case such as the attributes that are not used any more but just serve as filters, the trip-lets can be replaced by bitmap to save space). Considering the above scenario again, we don X  X  need to keep two intermediate position lists and use position-wise AND to and let it go through query plan. So, the LWM can accelerate the system speed, and it greatly facilitates aggregation operation. The sub-queries on fact table and dimension tables are handled separately in CDDTA-MMDB. When a query is received, it is broken off. Then, sub-queries on dimension table(s) are rewritten in order to produce triplets, while the sub-query on fact table is those tasks. Based on the triplets and information from parsing sub-query on fact strates the overview of the system. 4.1 Details of CDDTA-LWMJoin Algorithm CDDTA-LWMJoin is derived from invisible join [10]. Invisible join is also divided rows that pass the prediction to build hash table. Second, scan the fact table; mean-while fetch the foreign key to probe the hash table. The bitmaps on dimension tables according to final bitmap scan the fact table again to construct result relation. If the i -th value is set to true in the final bitmap, the i -th record in the fact table is retrieved. From above analysis, the time t used by invisible join can be expressed as t 0 + 2 t 1 + t 2, where t 0 is spent on producing hash table; t 1 on scanning the fact table; t 2 on produc-t + 3 t 1 . 
Compared with invisible join, LWMJ oin algorithm just utilizes the key information in triplet which was produced in first phase and thus avoids scanning fact table twice. The following is the algorithm procedure. First, decomposed queries involved with dimension table are rewritten; then we get the result sets from dimension tables. The last step of this phase is to produce the &lt; surrogate, key, value &gt; triplets. For those sub-queries such as RQ 1, bitmap is enough to represent them; for the other case, the &lt; sur-LWMJoin. Second, Fact table is scanned sequ entially; foreign keys are used to probe the key vectors producing in first phase, deciding whether the row satisfies the predic-wise, a physical tuple is constructed. Now, the operators such as GROUP BY can be applied on rows passing qualification.The benefits brought by LWM are obvious: dynamically allocating the memory to build the temporary position lists is no need; it accelerate the aggregation especially when the fact table is big and result sets are big because the records don X  need to be fetched again for aggregation. In general, the key of triplet serves as grouper in star schema and in such case we don X  X  need to re-access attributes in dimension table for aggregation. Therefore, random memory access can be further reduced. Figure 2 is the illustration of CDDTA-MMDB handling Q4.1. First apply the rewritten SQL RQ 1, RQ 2, RQ 3, and RQ 4 on corresponding dimension probe key /OID arrays. In this example, there are four tuples passing the prediction, and result set contains two groups. Key/ OID vectors with broken line are used in struct tuple directly). The time t used by LWMJoin is expressed as: t 0 + t 1 , where the time t 0 is used to produce the triplets and t 1 is used to scan the fact table. So, the time spent by invisible join is almost three times more than LWMJoin. The following ex-periments will show that the consumed time by invisible join is more than three times compared with time used by LWMJoin, which accords with above analysis. In this section, we will evaluate CDDTA-MMDB using SSBM. We generated 100G participating in experiment are loaded into memory, and all dimension tables are resi-dent in memory. The experiment environment is as following: 48GB memory, two Intel X  xeon X  processors (each runs at 2.4GHz and has six superscalar processor cores that support simultaneous multi-threading with two hardware threads per core), 1TB disk space and 64-bit Ubuntu 10.10 OS (linux kernel version 3.0.0-19, gcc ver-sion 4.6.1). We focus the benefits LWM brings, the performance of CDDTA-MMDB and the scalability of the system. 5.1 Aggregation Time Comparison Query 1.x doesn X  X  have GROUP BY, so we omit them in the experiment. We simulate state-of-art LM join algorithms such as in [7, 10] that after join phase the data is re-trieved for aggregation (LMJoin). Then we compare aggregation time used by LMJoin with that used by LWMJoin. To our surprise, as is shown in Figure 3 time consumed by aggregation in LMJoin even exceeds the time by join phase on Q3.1. On Q3.1 the time took up by aggregation is up to 61% of total time, followed by 35% on Q4.1, 21% on Q2.1 and 17% in Q4.1. Whereas, in LWMJoin the time took by aggre-gation is reduced to 28%, 13%, 8% and 5% respectively. The difference of aggrega-tion time between LMJoin and LWMJoin lies in the random memory access. After join phase, LMJoin fetches the tuple for aggregation. This out-of-order memory access will incur cache miss. So the aggregation time in LMJoin is much more than that in LWMJoin, especially when the selectivity is high. 5.2 CDDTA-MMDB vs. MonetDB and Invisible Join CDDTA-MMDB and invisible join implemented by us can automatically detect parameters of the host CPU, and then horizontally partitions the fact table evenly according to CPU cores. Figure 4 gives the performance comparison of CDDTA-MMDB, MonetDB (v11.7.9) and invisible join . We implement invisible join using the C++ Boost Library 1.48. All systems run on the thirteen queries several times such that the experiment is done on  X  X ot X  data set. CDDTA-MMDB has best performance among them. The time consumed by invisible join is almost four times more than the time by CDDTA-MMDB, which accords with the above analysis. Our system has worse performance than MonetDB on Q.2.2 and Q2.3, but it outperforms MonetDB greatly on Q1.x and Q4.x. On Q3.x, our system is slightly better than MonetDB. Overall, the total time to execute all queries by our system is about 22532ms, and total time by MonetDB is about 50311ms. The main contribution of this paper is that we propose a framework which introduces LWMJoin to efficiently deal with the queries on DW modeled with star scheme. LWMJoin is the combination of CDDTA-Join [7] and LWM. Compared to the state-of-art materialization in columnar database, LWM reduces production of intermediate data structure and can cut down memory access greatly. Experiments show that CDDTA-MMDB has good scalability and can be 2x faster than MonetDB and 4x faster than invisible join . In future, we will add triplet representation and LWM strat-egy into open source columnar database such as MonetDB. Acknowledgement. This work is supported by the Important National Science &amp; Technology Specific Projects of China ("HGJ" Projects, Grant No.2010ZX01042-001-002), the National Natural Science Foundation of China (Grant No.61070054). 
