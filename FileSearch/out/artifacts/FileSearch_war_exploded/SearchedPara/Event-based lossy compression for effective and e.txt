 1. Introduction 1.1. Preliminaries search challenges that make traditional DBMS technology (e.g., RDBMS) inadequate to the goal of dealing with the un-data stream query processing algorithms inspired by conventional database query processing algorithms. stream mining algorithms. 1.2. Data stream compression foundations [52,46] ), wavelets [47], sketches (e.g., [45,31,81] ), quantile (e.g., [51]) and frequency moments (e.g., [75]). 1.3. OLAPing data streams the term OLAPing data streams methodologies.
 wards the accomplishment of an efficient multidimensional data stream query processing engine.
To achieve high performance, an innovative approximate query answering technique is proposed in [26]. This technique ries over (compressed) data streams.
 compression model [26] for data streams in great detail. 1.4. Event-based compression of data streams an adaptive data stream compression approach .
 results over most-recent and past readings that refer to expired significant events . tion scenarios.
 posalstudieshowthetworesearchfieldscanbeefficientlymadesynergicintopowerfulcomputationmodelsforcomplextime-depending systems, like data stream processing systems. The results of this research effort have been synthesized in Stream [61], a four stage integration model for data streams and events. In more detail, tween data stream and event processing along with MavEStream
Following the main intuition deriving from the integration of streams and events, in this paper we introduce a novel event-based data stream lossy compression paradigm and we propose a formal model, called 1.5. Paper organization processing models, by putting in evidence how the two processing models can be integrated in order to achieve more search proposed in this paper, we furnish a comprehensive overview of the linear compression model for data streams of a comprehensive experimental assessment and analysis of the data stream lossy compression algorithm embedded in deriving from the proposed compression approach, and, in addition to this, the reliability of new directions for further research developments and extensions in the investigated scientific field. 2. Synergistic integration of data stream and event processing attention by recent research efforts.
 Although the data stream and event processing integration component is orthogonal to neous solutions can be exploited to this end, in our research we propose making use of data stream layer of ECM-DS . This due to the specific characteristics of ligent information systems, including data stream application scenarios. As a consequence, the ECM-DS reference architecture incorporates data streams (e.g., [33]), and so forth, where specialized event processing components may be required. ities and differences, and then we describe principles, functionalities and architecture of event processing integration component embedded in the ECM-DS 2.1. Seamless integration of data streams and events: motivations
Event processing [86,29,43,42,39,69,12,16,55,34,30] and lately data stream processing [7,1,20,74,65,77,5,13,26,31, ferent emphasis.
 more powerful model that combines the strengths of each one.
 road benchmark [89].
 2.1.1. Example  X  car accident detection and notification upstream for up to 20 min by the accident will not be tolled.
 naturally modeled as a data stream. The format of such a car location data stream is implemented by the object populating a reference object-oriented DBMS. The structure of object field of the object is intuitive thus it does not deserve further details. Upon the object CarLocStr , the input car segment data stream statement like the one shown in Fig. 2 .
 In particular, in the running example the effective input stream of the car (modeled by the object CarLocStr ) by the segment corresponding to the location. consecutive time units. This can be computed via the CQ over the stream els. In the remaining part this Section, we will illustrate how this can be done. 2.2. Data stream processing models and event processing models: foundations and discussion. 2.2.1. Data stream processing models data stream model are read-only operations as there is no update operations on data streams. However, computation in tions are not precluded.

The main computation required in a general query processing model over data streams is to incrementally compute que-stream processing model due to the potential unbounded input.

Due to the unique requirements of data streams, data stream processing has received widespread attention, and there efficiently. 2.2.2. Event processing models [39,69,78,79] . A comprehensive introduction and description about most of these systems can be found in [91,83] . which the method is invoked by an object. On the other hand, composite or complex events are composed of more than detected based on the event operator semantics when all of its constituent events occur. uent event in an event expression as the time of occurrence for the entire event expression. to other approaches based on computation and storage requirements for detecting events. scalable nature of complex data stream applications and systems. events (or event operators). Event occurrences flow in a bottom-up fashion. Fig. 4 shows a composite event AND with two events E 1 and E 2 . Whenever there is an occurrence of event E event occurrences while events are propagated. For example, there can be a mask on event E of unnecessary events up the event graph. 2.2.2.5. Event consumption modes. The AND event shown in Fig. 4 is detected when both the events E [16,17] , and REACH [12]. 2.3. Comparative analysis of data stream and event processing models and 2.2.2 , respectively, by putting emphasis on their similarities and differences.
There are a number of similarities and differences between data stream and event processing models. Both models em-cessing, and most of the functionalities provided by these two models are complementary to each other. all converging towards an effective definition and an efficient implementation of DSMS.
On the other hand, the event processing model focuses on detecting composite events and processing rules under the ingful events compared with techniques provided by the event processing model. 2.4.
 MavEStream : principles, functionalities and architecture
Inspired by these considerations, [65] proposes MavEStream semantic window condition [65]. Basically, semantic windows extend traditional time windows with the difference that of ing phases over the target data stream.

The reference architecture implementing MavEStream is shown in Fig. 5 . It consists of the following four stages: (1) CQ Processing , which is in charge of evaluating CQ over data streams. (2) Event Generation / Stream Modifiers , which is in charge of generating interesting as a result of CQ processing. (3) Event Processing , which is in charge of detecting events with/without masks. As specifically regards the  X  X  X ustomization X  property mentioned before, it should be noted that, in the context of collection of QTW that models the compressed data stream (see Section 1).
The seamless nature of the MavEStream integrated model is due to the compatibility of the chosen event processing/ both the processing models with respect to the goals of MavEStream based on attributes and not solely on timestamps; (iii) specification of events/event-expressions, rules and CQ. In order to efficiently support an effective and efficient integration of data stream processing, data stream processing model, the following improvements are instead achieved in the expressiveness and computational efficiency of CQ, and the creation of more meaningful windows. 3. The linear compression model for data streams In Cuzzocrea et al. [26] the linear compression model for data streams is proposed. Since
First, we provide the formal notion of data stream we use throughout the paper. A data stream S can be defined as an unbounded sequence of stream readings , S  X h r 0 ; r 1 ; r r  X h ID k ; V k ; t k i , where (i) ID k is the absolute identifier of r r stamp, i.e. 8 r k ^ r h 2 S with k &lt; h ; t k &lt; t h .
 parameter for our model.
 Section 1), such that R S ; A models a range on the stream dimension, and respectively. In our formal framework, R S ; A is defined as follows: such that N models the number of stream sources of the target system, whereas such that: (i) t 0 models the starting time instant observed by the target system; (ii) k models an integer indexing generic two-dimensional range of stream sources and time A By denoting as D S A the width of R S ; A (i.e., D S A  X  S tively, the volume of A , denoted as k A k , is given by the following formula: From Section 1, given a two-dimensional range of stream sources and time aggregating summarized readings, in a bottom-up manner. At the lowest level of aggregation, each leaf node n resents the range h S i ; D t j i , and stores the sum of all the readings produced by the stream source S
D t properties of A above are inherited by the QTW built on top of it, hence the following equalities hold: sented by the QTW ; (iii) D S QTW denotes the width of R S ; QTW straightforwardly reduced to more simple square ranges, via appropriately aliasing the model parameter N . Furthermore, for each QTW the total number of nodes, denoted by N such that P QTW models the depth of the QTW .
 particular data stream application domain considered.
 h X  S instance, node n .1 represents the range: whose aggregate value is 1778, while node n .2.3 represents the range: data-oriented representation of the QTW .
 -653 -560 -537=616.
 estimation [11].

QTW encompass other nice properties that are extremely useful to represent compressed multidimensional data. It discovery/mining tasks principles provided in Section 1.
 is not enough to represent new arrivals, the oldest QTW is selected, said QTW scribed process is iteratively repeated until the storage space B  X  B 0 &lt; B  X  . When the root node of T old is reached, the  X  X  X ext X  sub-tree in QTW
If the root node of QTW old is reached, then the whole QTW root) whose aggregate value is referred to all the leaf nodes of QTW is compressed according to the same scheme.

The above-described compressed data stream representation model is particularly suitable to answer range-SUM queries efficiently [26]. A range-SUM query Q on summarized data streams is defined in terms of two ranges, namely respectively, such that R S ; Q denotes a range on the stream dimension, and respectively. Formally: such that S k and S u denote the starting and the ending stream source involved by Q , respectively, and t starting and ending timestamp involved by Q , respectively.

By denoting as D S Q the width of R S ; Q (i.e., D S Q  X  S the selectivity of Q , denoted as k Q k , is given by the following formula:
The approximate answer to Q is obtained by applying the aggregation operator SUM on the summarized readings produced by stream sources belonging to the range R S ; Q during the time interval defined by the range
QTW nodes), assuming that stream readings are distributed uniformly. We denote as
Q . To give an example, Fig. 8 shows a range-SUM query Q  X h X  S [26], we show how approximate answers retrieved by the MRDS are characterized by a provable query error, which makes queries over data streams.
 stream sources and time A , we introduce the Percentage Relative Error (PRE) e  X  Q  X  , defined as follows: such that A ( Q ) is the exact answer to Q , i.e. the answer to Q evaluated against the answer to Q evaluated against the QTW built on top of of the query layer primitives above as baseline operations.

Besides the SUM-based WQ above, the linear compression model [26] is suitable to answer SUM-based CQ as well. Let us tinuous range-SUM query Q C on summarized data streams is defined as the following tuple: such that: (i)  X  S k : S u is a fixed range of stream sources; (ii)  X  t the temporal range  X  t start : t end moves forward along the temporal dimension of summarized data streams; (iv) n query frequency that determines the temporal period by which Q h X  S stream of ( approximate ) answers . 4. The event-based compression model for data streams event-based lossy compression model for data streams ECM-DS 4.1. Preliminaries ciated to them (see Section 1).

Under the non-linear compression scheme, the MRDS is compressed in dependence on a given degree of approximation d , which is retrieved from the event processing layer implemented by to be released in order to represent new arrivals. From Section 3, recall that B the fact that the degree of approximation d must be now considered along with B of approximation which is at least equal to d , having released the required storage space B clearly follows that d models indeed a lower-bound parameter for
In order to retrieve the degree of approximation d , MavEStream up phase of the target data stream application/system built on top of the the relation between interesting events and d plays a critical role for and must be considered carefully. From Section 2.2.2, it follows that, in turn, this critical task is related to in a transparent-for-the-data-stream-processing-layer (implemented by the MRDS ) manner. For this reason, d must be separation abstraction between the data stream processing layer (i.e., MRDS ) and the event processing layer (i.e., MavEStream )of ECM-DS , thus moving the attention towards interesting adaptive and reliability features of
ECM-DS , and discuss the non-linear compression approach accordingly. 4.2. Modeling and annotating the degree of interestingness of events the following kind: such that: (i) E k denotes the actual event; (ii) t E k ; start denotes the ending timestamp in which the event E k expired; (iv) d as h X  S 0 : S N 1 ;  X  t E k ; start : t E k ; end i the portion of aggregate values related to readings E d k can be reasonably expressed as a percentage value, whose semantics is as follows: d k  X  100 % : this value of d means that the portion of aggregate values h X  S this value of d does not originate any compression in QTW d k  X  0 % : this value of d means that the portion of aggregate values h X  S pletely removed by saving the whole aggregate value it represents in a singleton node (in other words, E relevance for the actual data stream application scenario)  X  this value of d originates a full compression of QTW d k 2 0 ; 100  X  % : this value of d means that the portion of aggregate values h X  S  X  this value of d originates a partial compression of QTW Section 4.1, recall that compression tasks above (triggered by different d values) involve QTW h X  S : S N 1 ;  X  t E k ; start : t E k ; end i for which the required degree of approximation is d compression task still does not cause a full compression of QTW limit condition represented by d k  X  0 % is reached on QTW pression of other portions of aggregate values of the MRDS different from h X  S
QTW E (from Section 4.1, recall that d is a lower-bound parameter for adigm above delineates a best-effort strategy embedded in the compression approach of critical aspect of our research in Section 4.4.
 ware component implementing such a scheme to be easily deployed within the over large-in-size and high-dimensional data streams. 4.3. Event-based data stream compression: models and theoretical results In this Section, we focus the attention on specific properties and theoretical results of discussed in Section 4.4.

From Section 4.2, note that, for an uncompressed QTW , d k to the approximate evaluation of range-SUM queries). In the second case (i.e., d queries (see Section 3).
 effective compression of a QTW? they refer to) hence they do not introduce query error and, instead, tend to make approximate answers as more as possible.

To become convinced of the property illustrated above, consider the following running example. Fig. 9 (a) shows a
Now, consider Fig. 11 , where two range-SUM queries, namely Q sources and time (Fig. 11 (a)) and the QTW (Fig. 11 (b)) of Fig. 9 , respectively, are depicted.
Q . Also, note that the answer to Q 0 evaluated against the range of Fig. 9 (a) is exact (i.e., A  X  Q  X  X  1054), as Q corresponding to the grid defined on the range, whereas the answer to Q (i.e., e A  X  Q 1  X  X  655), as Q 1 overlaps the grid defined on the range (see Section 3). When Q
Fig. 9 (b) (which, in turn, is built on the range of Fig. 9 (a)), the following approximate values are retrieved: and e A  X  Q 1  X  X  550 : 20. Focus the attention on the PRE (Eq. (6)) of both retrieved answers. For e
A  X  Q 1  X  ; e  X  Q 0  X  X  16 % , respectively. It follows that e  X  Q
Q is lower than the aggregation level of leaf nodes overlapped by Q of approximate answers to range-SUM queries, according to the considerations discussed above. Let us focus the attention on some properties and theoretical results of g QTW p , having a number of nodes N g pressed QTW (see Section 4.2), denoted by g QTW f , namely N by g QTW u , namely N g i.e. (from Eq. (6)): such that P g
QTW , denoted by d QTW  X  Q  X  , which is defined as follows: such that W is a function that captures the (inversely proportional) dependency between d tion between d QTW  X  Q  X  and e  X  Q  X  , and we left that study as future work. Furthermore, given a QTW and a population of range-SUM queries of
Q over the QTW , denoted by d QTW  X  Q  X  , which is defined as follows: such that: W k models a function capturing the dependency between d in the most general case, a different W k for each query Q
Now, let us focus on the theorem mentioned above. Given a partially-compressed QTW SUM queries Q , it follows that the degree of approximation of of nodes of g QTW p , N g such that U models a function capturing the linear dependency between d nodes on the accuracy of approximate answers to range-SUM queries.
 First corollary states what follows. Given a QTW having N
Q , the following property holds: such that g QTW f denotes the full-compressed QTW .
 Second corollary states what follows. Given a QTW having N ries Q , the following property holds: such that g QTW u denotes the uncompressed QTW and P QTW denotes the depth of the QTW . bound (corollary (20)) to the complexity due to evaluating range-SUM queries against the MRDS . 4.4. Non-linear compression of QTW needed to represent new arrivals (see Sections 4.1, 4.2, and 4.3 ).

The relation between the degree of approximation d and the QTW compression process, and the influence of QTW leaf degree of approximation of a QTW .
 of queries in QWL . Formally, given a QTW and the following three input customizable parameters: D S D S generating all the synthetic queries Q l ; k having selectivity equal to D S
Q l ; k 2 QWL , the two-dimensional range associated to Q l ; k n by an overlapping region that is at least equal to the r l easily obtained via ranging D S Q l and D T Q l along the ranges D S gree of approximation of QWL over the QTW , denoted by d QTW such that: (i) d QTW  X  Q l ; k  X  models the degree of approximation of Q due to evaluating Q l ; k against the QTW (Eq. (6)); (iii) W that, in the query-driven error metrics model proposed above, by ranging the input parameters D S sible to obtain more or less  X  X  X ich X  QWL , hence more or less accurate estimates for d in dependence on a degree of approximation d k and the storage space B tained. Then, d QTW  X  QWL  X  is compared with the required d ends as the QTW cannot be compressed while ensuring a (final) degree of approximation which is at least equal to d the settings defined by the actual model parameters D S Q d
QTW  X  QWL  X  &gt; d k , then the non-linear compression algorithm can execute. On the basis of d If d k  X  100 % , then the QTW must be maintained uncompressed (in fact, in this case, the condition d stores the aggregate value of all the (erased) leaf nodes of the QTW . In this case, B d 2 0 ; 100  X  % , then the QTW have to be partially-compressed in dependence on d most interesting case to treat.
 approximation at least equal to d k is achieved, having released the required storage space B target data stream, beyond to the main requirement of releasing the storage space B searching strategy like the previous compression model (see Section 3).
Looking at technical details, in the case d QTW  X  QWL  X  &gt; d that aims at simultaneously accomplishing the requirements related to d L the QTW node space via the reverse breadth-first searching strategy. This originates the partially-compressed QTW , We then check the condition: d g the released space due to this compression task is equal to: B related to d k cannot be accomplished. In addition to this, if B well. Both requirements are thus not satisfied on the actual QTW locally, but the main requirement related to B satisfied on the whole MRDS globally (from Section 4.2, recall that in the latter case, if the condition d g the compression algorithm ends, with the achievement of both the two requirements (i.e., the one related to d such that the constraint B 00 P B 0 has a higher priority over the constraint d dled in a very efficient way, as follows. Let QWL j 1 and QWL N
QTW j denote the number of nodes of the QTW at the  X  j 1  X  th and  X  j  X  th iteration of the algorithm, respectively ( N
QTW j 1 &lt; N QTW j ). It is easy to understand that QWL iterations j 1 and j of the algorithm), as follows: such that: where Q  X  n L iterations of the compression algorithm .
 reduced via somewhat optimization. In order to lower the effect of this complexity, in during the non-linear compression of QTW .

Finally, algorithm compressQTW (see Fig. 12 ) implements the proposed non-linear compression approach for QTW . The tion above, so that they do not deserve further details. 4.5. Non-linear compression of the MRDS the MRDS by means of tuples of kind: h E k ; t E k ; start
MRDS adheres to a best-effort strategy, whose main goal consists in releasing the storage space B arrivals.
 h E ; t E k ; start ; t E k ; end ; d k i , the related portion of aggregate values h X  S h X  S h X  S approximation is required for h X  S 0 : S N 1 ;  X  t E k ; start amount of storage space B 00 QTW released from portions of aggregate values h X  S 0 : S N 1 amount of storage space released from the whole MRDS due to the non-linear compression process, denoted by B tained as follows: egy (see Section 4.2):
Eq. (26) states that, similarly to d k (see Section 4.1), even B erty of B 0 is more intuitive than the equivalent one for d released from compressing QTW E during the compression of QTW E to B
MRDS . Overall, every MRDS compression process can possibly generate an amount of storage space that exceeds the re-quired one, B 0 , denoted by B E MRDS . B E MRDS is defined as follows: B cation scenario.

Second, having sorted the event annotation tuples by descendent values of d ing towards those having low values of d k , until the storage space B
As regards more practical issues, given an event annotation tuple h E number of QTW embedded in the MRDS can be contained within the time interval  X  t  X  t
E , we make use of conventional containment/overlap relations among temporal ranges of QTW of the MRDS and  X  t ; start : t E k ; end . After that, we run algorithm compressQTW set above.

The meaning and the semantics of both main algorithm and its sub-routines are completely intuitive from the detailed description above, so that they do not deserve further details. 5. Experimental assessment and analysis In order to assess the performance of the event-based data stream compression algorithm embedded in synthesized in a more reliable experimental evaluation and subsequent analysis of the performance of our proposed algorithm. 5.1. Setting of the experimental framework based data stream compression algorithm embedded in ECM-DS readings whose values are uniformly distributed over a given interval  X  L over a given interval  X  Z 0 : Z 1 , such that Z 0 &gt; 0 ; Z data sets of kind F 1 F 2 F 3 where the parameters F 1 ; F ciated to the topology of the stream sources (e.g., connected by a network); (ii) F Z above. The probability distribution associated to M , denoted by f where: (i) f i denotes the i th probability distribution, with i 2f U ; G ; Z g ; (ii) p that x is randomly extracted by the i th probability distribution, with i 2f U ; G ; Z g ; (iii) p  X  X  X lobal X  best performance is achieved when the following parameter setting is considered: h F the sake of brevity, here we omit the results obtained from our experimental assessment when the parameter setting h F  X  U ; F 2 ; F 3  X  U i is considered.
 more reliable assessment of the performance of the event-based data stream compression algorithm embedded in
DS ranging from hydrology to meteorology and telecommunication networks. In more detail, in our experimental framework we made use of the following real-life data sets: (i) BAYDU in Canada, during the interval of time [1953:1981] [59]; (ii)
Morristown Research and Engineering Facility [71]; (iii) LBL-PKT Berkeley National Laboratory and the rest of the world, during one-hour interval [71]; (iv)
As regards the query layer of our experimental framework, both SUM-based window and continuous queries (see Sec-queries, denoted by Q W , which has been obtained as follows. Let and the temporal dimension of the generic query Q w 2 Q W such that D S Q w &gt; 0 and D T Q w &gt; 0, which model the width of R Queries Q w 2 Q W have all the same selectivity D S Q w D T sional range h R S ; MRDS ; R T ; MRDS i by means of the fixed two-dimensional range h ners all the two-dimensional entries h I i ; I j i in the two-dimensional space associated to the range h this process generates a large number of queries in Q W , hence the cardinality of W the so-generated  X  X  X aximal X  instance of experimental framework we introduce the so-called sampling factor s that determines a sample of in the experimental campaign.
 With instead regard to CQ, we fix both the range on the stream dimension R c of the generic query Q c , respectively, and move forward the two-dimensional range h
WQ (see Section 3), along the temporal dimension of the MRDS by one step equal to D t parameter of the formal definition of CQ (Eq. (12)). Still according to (Eq. (12)), Q inating a stream of approximate answers.

Now, focus the attention on the metrics of our experimental framework. Here, we consider a query-based error metrics continuous).
 Given a population of synthetic WQ Q W , we introduce the error metrics defined by the Average PRE e such that e  X  Q w ; k  X  is the PRE due to evaluating Q w ; k answer to Q c ; e A  X  Q c  X  , from the one of the exact answer to Q rithm for data-intensive applications and systems.
 accurate and reliable experimental assessment of the compression algorithm embedded in elaborating stream readings within semantic windows. In our experimental framework, we conceived an event processing layer capable of producing random synthetic interesting events E by a given time interval  X  T min : T max uniformly, where T
T min &lt; T max . In turn, synthetic approximation degrees d ing layer have values uniformly distributed over a interval  X  d d &lt; d max .
 Other important experimental parameters modeled in our framework are the following: (i) D S parameters. In our experimental evaluation, the values D S
MRDS . 5.2. Experimental results and discussion dence of the nature of input queries (window, or continuous).
 For WQ, given the population Q W , we observed the variation of e B available to house the MRDS . Fig. 14 shows the experimental results for WQ over the synthetic data set details, Fig. 14 (a) shows the variation of e Q W when ranging the query selectivity by varying both D S when ranging the compression ratio r , for different different combinations of D S tion to this, Figs. 18 X 21 show the same experiment on the real-life data sets
For CQ, given a  X  X  X mall X  CQ Q L c and a  X  X  X arge X  CQ Q H c to Q L c (respectively, Q H c ), e A  X  Q L c  X  (respectively, Q shows the variation of e A  X  Q L c  X  (respectively, e A  X  Q On the other hand, Fig. 22 (b) shows the variation of the corresponding PRE e  X  Q the same experiment on the synthetic data sets UGU , UZU and same experiment on the real-life data sets BAYDU , BC , LBL-PKT event-based data stream compression algorithm embedded in special feature plays a critical role in real-life data stream applications and systems [5]. 6. Conclusions and future work so-called event-based data stream lossy compression paradigm.
 streams has been addressed by means of a set of complex data stream compression methodologies where the knowledge on target OLAP application scenario. As a secondary contribution of our research, we have provided a comprehensive experimental assessment and analysis of the proposed data stream lossy compression algorithm embedded in ECM-DS , along its reliability in the context of advanced data stream applications and systems. algorithms able to make ECM-DS dealing with more complex OLAP aggregations/queries over data streams, like those described in [31], beyond the simple SUM-based ones considered in this research; (ii) investigating how integrated with novel Grid architectures [36,35] in order to embed into that will allow us to efficiently process high-rate, high-dimensional, massive data streams. Acknowledgments experimental part of this research.

References
