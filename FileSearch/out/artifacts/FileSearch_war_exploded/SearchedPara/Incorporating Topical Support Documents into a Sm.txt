 This paper explores the incorporation of topical support documents into a training set as a means of compensating for a support topical representation, our method applies a simple transformation to documents, i. e., making new documents from existing positive documents by squaring a conventional term weight. The topical support documents thus created not only are expected to preserve the topic, but even improve the topical representation by emphasizing terms with higher weights. Experiments with support vect or machines showed the effectiveness on RCV1 collection with a small number of positive training data. Our topical support representation achieved 52.01% and 8.83% improvements for 33 and 56 categories of RCV1 Topic in micro-averaged F1 with less than 100 and 300 positive documents in learning, respectively. Result analyses based on robustness indicate that topical support documents contribute to a steady and stable improvement. H.3.3 [ Information Storage and Retrieval ] Information filtering Experimentation, Performance Text Categorization, Support V ector Machine, Topical support Representation, Transformation Typical text categorization methods such as k-nearest neighbors, a na X ve Bayesian classifier, and support vector machines have focused on the scoring algorithms which assign relevance scores to each document-category pair [2]. There has been a research on the thresholding algorithms to overcome problem in existing scoring approaches [7]. When the available training data for categories are limited, however, text categorization systems by the scoring and thresholding methods have difficulty in estimating proper decisions. 
To deal with the limitation of a training set, there has been interest in statistical re-sampli ng approaches such as bagging and boosting by manipulating a training set and a co-training approach using a large amount of unlabe lled data. Sassano [5] generated virtual documents by randomly a dding or deleting a small number of words. There is a closely related work, a virtual relevant document technique [3], which artificially generates virtual relevant documents using pair-wise documents. Recent research on web-query classification [6] also deals with the limitation of the training data by query enrichment. The open problem about the incorporation of transformed examples is to what extent transformations can be safely app lied to the training examples, as some transformations can produce worse results [1]. 
In this paper, we propose a topical support document (TSD) method to incorporate transformed documents by applying a simple transformation to positive documents, i.e., making a new document from an existing positive document by squaring a available training data. This topical representation has the effects on emphasizing topic terms and de -emphasizing general terms in an actual document. To support topical representati on for a category with a small number of positive training data, we artificially generate topical support documents by a simple transformation using positive examples in a training set. 
The simple transformation to generate a topical support document S is to square weight of each term in a vector D and then apply cosine normalization to the vector S . The weight for each term in a document is calculated by ltc weighting scheme. weight(x, S) = weight(x, D)  X  weight(x, D) (1) where x is a term in a document D . Note that a new document S has the same terms as a document D . 
The effect of squaring weights is to emphasize terms in higher weights and de-emphasize terms in lower weights. When the number of positive examples is N , the same number of topical support documents are generated. 
Our hypothesis for topical support documents is that new documents contribute to a centroi d of the relevant documents group far from the non-relevant documents group. We have evaluated the TSD method on RCV1-v2 (Reuters Corpus Volume 1) collection [4] which in cludes three categories: Topic, Industry and Region. Training and test data consist of 23,149 and 781,265 documents. We used the SVM light [2] with linear kernel. values. 
We have compared our method with the baseline method and a virtual relevant document method [3]. z baseTR: performance of SVM for the original training set. z VRDsv: performance of SVM by incorporating virtual z TSDsv: performance of SVM by incorporating TSDs 
The procedure to train a SVM to the incorporation of TSDs into the original training set is as follows: (a) Training an SVM for the training set for a topic; thus (b) Generating TSDs using the pos itive SVs extracted from the (c) Training another SVM by incor porating TSDs generated at base system (baseTR) and the second SV set is used for the proposed method (TSDsv) in experiments. 
To see the effectiveness of our TSD method, we performed experiments according to the size of positive training documents for categories which have at l east one positive document and less than 10, 20, 50, 100, and 300 positive documents. 
As shown in Table 1, the TSDsv achieved significant improvements on Topic categories. When the number of positive training documents increases VRDsv shows worse performance than even the baseline. When th e number of positive documents is less than 100, the TSD method achieved 52.0%, 53.6% and 5.5% improvements over the baseline on Topic, Industry and Region categories, respectively. (The results for Industry and Region categories are not shown.) We have analyzed classification robustness. Robustness is defined as the number of categories w hose performance is improved or hurt compared to the baseline as the result of applying the TSD method. As shown in Figure 1, our TSD method shows strong robustness on Topic and Industry categories. For the RCV1 Topic category, the TSD method improves 46 categories and hurts 6 among 56 categories. For the RCV1 Industry category, the TSD method improves 215 categories and hurts 3 categories. Incorporating the topical support doc uments into a training set in the learning phase is effective for text categorization. The topical representation showed substantia l gains. The improvements are documents are produced simply and the incorporation of the topical support documents contribut ed to the steady improvement. In classification robustness, our method showed strong robustness for each category. The results show that the suitable term weight could be manipulated for learning within the limitation of a training set. [1] DeCoste, D. and Sch X lkopf, B. 2002. Training invariant [2] Joachims, T. 1998. Text Categorization with Support Vector [3] Lee, K-S., Kageura, K. 2007. Vi rtual relevant documents in [4] Lewis, D.D., Yang, Y., Rose, T., and Li, F. 2004. RCV1: A [5] Sassano, M. 2003. Virtual Exampl es for Text Classification [6] Shen, D., Pan, R., Sun, J.-T., Pan, J., Wu, K., Yin, J., and [7] Yang, Y. 2001. A study on thres holding strategies for text Table 1. Performan ce comparison on Topic categories. Figure 1. Robustness on RCV1 Topic and Industry. 
