 An increasingly important problem for data mining (DM) users is the manage-ment of large sets of models [9]. One particular problem is the selection of the best algorithm and corresponding parameters for a particular dataset. When de-veloping hundreds or thousands of models, common trial-and-error approaches are too expensive in terms of computatio nal and human resources. These issues are of particular concern to DM software vendors, who are beginning to integrate functionalities to address them into their tools. 1
One approach to the problem of algorithm selection is metalearning [3]. It consists of applying learning algorithms on metadata , i.e., data that describe the performance of learning algorithms on previously processed datasets. Thus, the metamodel obtained maps characteristics of those datasets to the performance of the algorithms. Besides providing knowledge that can be useful to understand those algorithms better, it can be used to provide recommendation (e.g., a rank-ing of those algorithms) for new datasets [4], thus reducing the effort required to obtain satisfactory models for them.
One of the most important problems in using metalearning for algorithm selec-tion is the availability of a number of datasets that is sufficient to enable reliable (meta-)induction. The UCI Repository (UCI-R) [1] is the most common source of examples for metalearning and it contai ns slightly over 100 datasets. Given that each dataset represents one meta-example, most metal earning research is based on approximately 100 examples. This is a small number to ensure that highly reliable models are obtained, particularly in such a complex application such as metalearning. Two common approaches to increase the amount of datasets used for metalearning is the generation of synthetic datasets or the manipulation of existing ones [2,6,7]. In this paper, we propose a new method to generate new datasets from existing ones, referred to as datasetoids , by applying very simple manipulations, which addresses the shortcomings of existing methods. We test the new method on datasets from the UCI-R and use the datasetoids obtained to (meta)learn when to use pruning learning in decision trees.

In the next section we summarize related work on the generation of datasets by manipulating existing ones and on metalearning. In Section 3 we describe the method to generate datasetoids. The metalearning application that illustrates the usefulness of datasetoids is given in Section 4. A discussion of potential anomalies of datasetoids and how to deal with them is given in Section 5 and we close with conclusions. Manipulation of Datasets. The manipulation of existing datasets to obtain new ones is not a novel idea (e.g., [6]). Typical approaches apply changes separately to independent (e.g., adding redundant attributes) and dependent variables (e.g., adding noise to the target attribute). The metaknowledge that can be obtained from such datasets is focuse d on a certain aspect of the behavior of algorithms (e.g., resilience to redundant a ttributes). Although useful, it is too specific to provide much insight on the general performance of algorithms, which is the focus of this paper. What ultimately affects the performance of algorithms is the joint distribution between the dependent variables and the target. The changes that can be forced on the joint distribution of a given dataset are either random or made according to some model . The former case can be reduced to the case of adding noise to the data. In the latter case, the model used in the lat-ter entails a bias, which will, naturally, favor some algorithms relative to others. This restricts the generality of the metaknowledge that can be obtained.
Similar drawbacks apply to methods that generate artificial datasets. How-ever, given that no data is available to start with, the joint distribution must be defined apriori . If it is random, the data is mostly useless.Otherwise, some kind of bias is favored.

An important issue when generating datasets with the aim of obtaining gen-eral metaknowledge is relevance. The que stion is whether the g enerated datasets are representative of the population of real world datasets, or an interesting part of it, at least. Metaknowledge generated from a set of datasets is only useful in practice if they are representative of real world problems. Two approaches that can be followed to obtain empirical metaknowledge is by carrying out ex-perimental studies or by metalearning, which is the approach followed in this work.
 Metalearning for Algorithm Selection. Metalearning can be defined as the use of learning algorithms to obtain knowledge of the behavior of learning algorithms [3]. One particular application of metalearning is to provide models that can predict the (relative) performance of algorithms on new datasets and can, thus, be used to support the user in selecting which algorithm to use. Metalearning for algorithm selection involves three steps: (1) the generation of metadata ;(2) induction of a metamodel by applying a learning algorithm on the metadata; and (3) application of the metamodel to support the selection of the algorithms to be used in new datasets. These steps are summarized next, but for a more thorough description, the reader is referred to [3].

In this context, metadata are data that d escribe the (relative) performance of the selected algorithms on a set of dataset s, which were already processed with those algorithms. They consist of a set of meta-examples , each one representing one dataset. Each meta-example consists of a set of attributes and a target.
The attributes, which are also known as metafeatures , are measures that char-acterize the datasets. These measures represent general properties of the data which are expected to affect the perform ance of the algorithms. A few examples of commonly used metafeatures are the number of examples, the proportion of symbolic attributes, class entropy and the mean correlation between attributes.
The target of the metalearning problem, or metatarget , represents the relative performance of the algorithms on the dataset. Here, it is represented as the algorithm that is expected to perform best, thus transforming the metalearning problem into a classification task
By applying a learning algorithm to the metadata, we obtain a metamodel that relates the characteristics of datasets (given by the metafeatures) with the relative performance of the algorithms (metatarget). In this work, standard clas-sification algorithms can be used for metalearning.

The metamodel can then be used to support the data miner in selecting the algorithm to use on a new dataset. To do this, it is rst necessary to compute the metafeatures that are used by the metamodel to generate the prediction. The method proposed here also creates new datasets by manipulating existing ones. It simply consists of generating a new dataset for each symbolic attribute of a given dataset, obtained by switching the selected attribute with the target variable (Figure 1). We call datasetoids to the the newly generated datasets because, although they are sets of data, they probably do not represent a learning application which is interesting in the corresponding domain (e.g., predicting the product purchased, given the customer and the amount purchased).
Given that there are approximately 3000 symbolic attributes in the UCI-R datasets, this method enables us to create up to that number of datasetoids. In practice, however, some restrictions apply, as will be discussed below. Still, a considerable number of new datasets can be created. Here, we used 64 datasets from the UCI-R, obtaining a total of 983 datasetoids. Our goal in this paper is to investigate whether it is possible to obtain useful metaknowledge from datasetoids. To achieve this, we chose a simple metalearning setting, namely in terms of the algorithm se lection problem, the metafeatures and the meta-level algorithm. The algorithm selection problem consists of predicting, a priori, if pruning a decision tree will improve the quality of the model or not. The implementation of the algorithm for induction of decision tress we used is provided by the rpart library included in the R statistical package [8]. Pruned decision trees were obtained using the defa ult parameters of this implementation and unpruned trees were obtained by setting the complexity parameter ( cp )to 0, which means that a split can make only a very small ( &gt; 0) improvement to the model fit. The measure of performance used was classification accuracy,estimated using 10-fold cross-validation.

As mentioned earlier, each meta-example represents one problem (i.e., a dataset or a datasetoid). The class of each meta-example is based on the re-sults of the experiments on the corresponding problem. The class values are p , u or t , meaning, respectively, the winner is the pruned tree, the unpruned tree or that they are tied. Table 1 presents the class distribution, both in the metadata obtained from the datasets as in the metadata from the datasetoids. The table shows that the class distributions are different, which indicates that there may be some underlying differences between datasetoids and datasets. However, we also note that the differences are not so large and that the relative proportions are the same (i.e., ties are the most common, followed by the case when pruning improves accuracy).

An important issue in metalearning is the choice of metafeatures. Our goal is not to maximize metalearning results but to assess whether datasetoids provide useful information for metalearning, so we use simple measures. As the induction of decision trees uses entropy, we select ed two measures that are based on this concept: the entropy of the class and the average entropy of the attributes [4]. To estimate the accuracy of the metal earning approach, we use the original UCI-R datasets as test set. To ensure independence between the training and test sets, we must guarantee that the datasetoids generated from a given dataset are not used in the training set used to make a prediction for that dataset. Therefore, we used a leave-one-out (LOO) approach. A model is induced for each dataset using all the datasetoids except the ones for the test dataset. For metalearning, we use several algorithms from the R package [8], namely decision trees ( dt ), linear discriminant ( ld ), random forest ( rf ), support vector machines ( svm )and multi-layer perceptron ( nn ), all with default parameters (which, in the case of svm and nn imply that some optimization of the parameters is carried out). The measure of metalearning per formance is also the classification accuracy, i.e., the proportion of datasets for which a correct decision was prediction was made, in terms of whether it is best to prune or not, or if there is a tie.

As a baseline, we compare the same metalearning approach using only UCI-R datasets. In this case, LOO consists of inducing a model for each dataset using all the other datasets for training. Finally, we have tested a combination of datasets and datasetoids. In this case, the training data for a given test dataset includes all the other datasets plus the datasetoids that were not generated from the test dataset (UCI++).

In the results presented in Table 2, we observe that using a limited number of datasets is not sufficient to predict when to prune a decision tree, at least in the metalearning setting considered in this work. The accuracy obtained is not higher than the default accuracy, except for rf and nn . However, by using datasetoids as training data, we obtain systematic gains for all algorithms, ranging from 6% to 15%. These results clearly show that datasetoids contain useful information for predicting whether it is worthwhile to do pruning or not. Note that the level of accuracy achieved (in the 50%-60% rang e) can still be regarded as relatively low. However, as mentioned earlier, we have used a simple metalearning context with plenty of room for improvement. Additionally, we observe that the im-pact of combining datasets and datasetoids (column  X  X CI++ X ) is much smaller (probably not significant from a statistic al perspective). This could be expected because there are far less datasets than datasetoids. We start by discussing a number of potential anomalies in datasetoids and meth-ods to overcome them. Then, we briefly dis cuss the relevance of datasets, par-ticularly in terms of the application domain to which they belong. Finally, we discuss further extensions to this approach.
 Potential Anomalies. Datasetoids may contain some anomalies which must be detected and corrected.

The first case is the existence of missing values in the symbolic attribute used as the target attribute in the datasetoid. In the pure supervised classification setting we are assuming, this is not possibl e. One possible correction is to elim-inate the lines containing the missing class values. This is one of the possible causes of the second type of anomaly, wh ich is an insufficient number of cases. One possible correction is to eliminate datasetoids which have less than a given number of examples.

When the new target attribute is very skewed, it is difficult, without proper handling (e.g., different missclassification costs), to induce a classification model other than simply predicting the default class. One possible solution is to discard datasets for which the proportion of examples with the majority class is higher than a given level. Note that skewed targets may be interesting for other pur-poses (e.g., outlier detection, problems with different missclassification costs). Conversely, the new attribute may be very sparse, meaning that there are not enough cases for each different class value to enable any reliable generalization to be done. Again, one solution is to set a maximum threshold on the number of classes or a minimum threshold on the proportion of examples from each class.
It may also be the case that a datasetoid is random because the corresponding target attribute is completely unrelated to all the other attributes and, therefore, the corresponding concept cannot be learned from the available data. A dataset with the same characteris tics can also be generated when dealing with a real problem, if the data analyst has extremely poor knowledge engineering skills or there are no predictive attributes available at all. This is, however, much more improbable with real datasets than with datasetoids. One possiblity to identify datasetoids which do not contain a learnable problem is to run several algorithms and eliminate those for which none of the algorithms obtains a satisfactory result.
Finally, it may be the case that two or more attributes represent exactly the same property or that one attribute is com pletely determined by another one. An example of the latter are the zip code and the city, where the former determines the latter. Therefore, the datasetoids using any of those attributes are target will be trivial. To determine these situations, we can execute several algorithms, as in the previous case, and identify the ones with an accuracy very close to 100%. The models obtained by those algorithms can then be analyzed to check if there are such attributes. Again, we may remove the corresponding datasetoids or, a less radical solution is to remove the dependent attribute.
 Relevance. One may question the relevance of the learning problems that a datasetoid represents. According to [5], there are three types of classification datasets in terms of the type of class. The class can be (1) a label, (2) a random variable or (3) determined by a partition of the attribute space. Given that this distinction also applies to the independent attributes in a dataset, new datasets of all the types are generated by the method described. Analyzing the symbolic attributes of the soybean (large) dataset, for instance, we observe that the date of the observation, the air temperature and condition of the stem will generate datasetoids of each of t he three types, respectively.

From the point of view of the application domain of the data, most datase-toids are pure nonsense. For instance, predicting the product purchased, given the customer and the amount purchased is probably not an interesting applica-tion. However, validity from the point of view of the original domain is important if the goal is to compare the performance of algorithms on the corresponding ap-plication. However, this is not true in the kind of problem we are concerned with, where the goal is to obtain metaknowledge, i.e., to relate properties (metafea-tures) of the data with the (relative) performance of algorithms.
 Future Work. The good results reported in this paper, make a range of similar approaches equally appealing. For instance, datasetoids can also be generated by discretizing continuous attributes. This approach has the very interesting pos-sibility of controlling the number of classes and their distribution. This method could also be used to generate classificat ion problems from regression problems.
Finally, it seems that is possible to generate datasetoids for all kinds of super-vised learning problems. For instance, re gression datasetoids can be generated by using continuous attributes as target. Additionally, it may be possible to generate datasetoids from any kind of dataset and not necessarily from learning datasets. If these expectations are met, a huge body of highly reliable metaknowledge can be generated. We proposed a method to generate new le arning datasets from existing ones, simply consisting of switching the target attribute with each of the independent ones. The new datasets, referred to as datasetoids , are not interesting as applica-tions per se . But, as shown by our experiments on the problem of metalearning when to prune a decision tree, they can be u seful in situations where it is re-quired to carry out experiments on a group of datasets to obtain metaknowledge. This metasknowledge may consist of models relating the results obtained with abstract properties of the data or may simply be a general assessment of the competitiveness of one or more methods, as is usual in published machine learn-ing research. A larger amount of dataset s available to perform these studies also decreases the possibility of drawing wrong conclusions due to results obtained by chance.

We identified a number of anomalies that may occur in the new data sets, due to the artificial nature of the problems they represent. Methods to identify and correct those anomalies have been suggested.

The successful results obtained, opens a wide range of extensions to this method, therefore enabling the generation of new datasets that could solve cur-rent shortcomings in the production of metaknowledge.
 Acknowledgements. Thanks to Al  X   X pio Jorge for the inspiration for naming the new datasets as datasetoids. This work was partially funded by FCT (Pro-grama de Financiamento Plurianual de Unidades de I&amp;D and project Rank! -PTDC/EIA/81178/2006).

