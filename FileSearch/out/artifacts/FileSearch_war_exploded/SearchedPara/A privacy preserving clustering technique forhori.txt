 School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran 1. Introduction
Privacy protection is one of the main requirements of people in their daily life and is an important issue, which has attracted many researchers. Data security and privacy are two concepts that often are have been developed for them [21].
  X  X he rights of individuals to determine for themselves when, how, and what the information about them is used for different purposes X  [3].

On the other hand,people need to communicate,cooperate and collaborate with each other. Sometimes, they need to share their private and sensitive data with un-trusted parties. In these situations, some techniques are required to ensure preservation of privacy of the shared sensitive data.
Data mining is introduced as an important technique for extraction of useful and unknown knowledge-and freedom [21]. This has led to some backlashes against data mining.

According to the Congressional Research Service report on data mining [41], privacy is one of the four pro fi ling Computer Assisted Passenger Screening Program was canceled due to an outcry from civil liberty groups avoiding the sharing of private passenger data with government agencies and other data mining companies [6].

Efforts to resolve this challenging problem has led to developing privacy preserving data mining (PPDM) techniques by the research community working on security and knowledge discovery. The main goal of PPDM is to develop algorithms and techniques for modifying the original data in some way, so that the private data and knowledge remain private even after the mining process [46]. PPDM has become increasingly popular because it allows sharing of sensitive data for analysis purposes.
PPDM has been introduced in two articles [2,29] in 2000. In these articles, two main methods of PPDM, including random perturbation and secure multiparty computation (SMC) were introduced. Since 2000 till now, different PPDM m ethods have been developed for different purposes, such as data hiding [1, 2,5,7,15,17,26,43,48,51], knowledge hiding [4,13,36,39,40], distributed PPDM [14,23,28,29,54], and privacy-aware knowledge sharing [20,24,37] in different data mining tasks. The detailed description of different PPDM methods can be found in [21].

After a deep study on existing PPDM methods and especially on privacy preserving clustering methods and their features, we concluded that to introduce an appropriate new PPC method we should fi nd a data distortion method with the following properties:  X  Applying the method on original dataset should hide the original data (i.e. data hiding),  X  The distance between original data records should be preserved in the transformed dataset, and  X  The proposed method should possibly have better clustering quality, privacy and performance
Looking for an appropriate transformation method to meet the above requirements, we decided to use wavelet transform (HWT), to solve the problems of privacy preserving clustering (PPC). In addition, we have used scaling data perturbation (SDP) [35] in addition to HWT to provide more privacy.
The acceptable results of using HWT for centralized datasets [51,55,56], motivated us to extend the proposed technique for distributed datasets. In [ 51,55,56], we have introdu ced the initial idea and the results of using HWT in combination with SDP to preserve the privacy of centralized datasets for clustering task. In this paper, we present two new algorithms using HWT and SDP for horizontally and vertically distributed datasets.

In this paper, HWT and SDP are used together to mask the underlying numerical attribute values subjected to clustering and protecting them from being revealed. Our proposed techniques bene fi t from the fact that HWT alters the data from the original domain to a different domain by preserving the distance between the original data records.

The remainder of this paper is organized as follows. Section 2 presents the related works. Section 3, some motivations of the distributed privacy preserving clustering are mentioned. The proposed technique is introduced in Section 5. Section 6 presents the experimental evaluation results of the proposed technique. Finally, some concluding remarks are mentioned in Section 7.
 2. Related works
As discussed in the previous section, a lot of results have been published on PPDM for different purposes and different data mining tasks, some of which are reported in [1,2,4,5,7,13 X 15,17,20,23,24, 26,28,29,36,37,39,40,43,48,51]. One of the most important PPDM methods is for data hiding purpose.
Depending on the type of data privacy problems being addressed, there are the following two important categories of data hiding techniques: 2.1. Data perturbation techniques miners. The techniques presented in [1,2,15,17,25,26,29,31,45] belong to this category. 2.2. Data anonymization techniques
These techniques focus on anonymization of publishing data to hide the identity of entities. The techniques presented in [5,7,42,43,48,51] belong to this category.

Several techniques have been proposed for both categories of data hiding techniques for different data for privacy preserving clustering over distributed datasets.
 Two main approaches of PPDM are random perturbatio n and secure multiparty computation (SMC). The random perturbation approach is based on a ddition or multip lication of random noises from a normal distribution with mean zero to the main data, such that the individual data values are changed, but the original data distribution can be reconstructed with appropriate degree of accuracy [2,26]. One as vectors in the original data space is not preserved well. Thus, random perturbation technique is inappropriate for clustering.

The SMC technique has been used in a wide range of clustering and other data mining algorithms to address the privacy issues in distributed environment, where the data are vertically or horizontally distributed across multiple parties.

A secure scalar product protocol based on cryptographic primitives is applied in privacy preserving k-uses pseudo-random numbers for privacy preserving expectation-maximization clustering (PPEMC) [11, 28].
 Model sharing and aggregation is non-cryptographic SMC approaches used in PPC. Merugu and Ghosh [31] and Klusch et al. [27] proposed model-based solutions for the PPC problem. Data holder a global model from these local models.

Using SMC protocols which are based on cryptography or sharing intermediate results of clustering algorithm often required changing the clustering algorithms. Therefore, they are dependent to the clustering algorithm and often dif fi cult to generalize them to other clustering algorithms [32].
A number of special methods and techniques for PPC applicable for distributed datasets exist in the literature. These methods are focused on using dif ferent data distortion t echniques for data hiding, instead of cryptographic methods. A proposed method for PPC is random projection (RP) [34]. In this method the number of dimensions reduces from m to k by multiplying the data matrix to the random matrix , where every entry of the random matrix is independent and identically chosen from some normal distribution with mean zero and variance one. Experimental results have shown that the Euclidean in each execution. Also, for large datasets, the evaluation time of this method is high. RP can also be presented in [34,54], both have multiplied the origin al data matrix in a random matrix. However, there are some minor differences in generation of the random matrix used in these two methods. A privacy preserving technique for Euclidean distance based mining algorithms is proposed in [32]. This method uses discrete cosine transform (DCT) to address both problems of data reduction and privacy preservation. This method can also be used with distributed datasets.

Wavelet transform is widely used in signal processing [12] and noise suppression [38]. They have also mining preprocessing steps [44].

Wavelet transform has also been used in the context of PPDM in [6] for privacy preserving in classi fi cation mining over centralized datasets. However, in this paper in addition to using wavelet different criteria for stopping wavelet transforms) and different algorithms are developed. Besides, in this paper HWT and SDP have been used together for distributed datasets. 3. Background In this section, we brie fl y review the background knowledge required for the remainder of this paper. Section 3.1 provides an overview of clustering. Section 3.2 gives further details about SDP method. Section 3.3 describes the basic concepts and properties of wavelet transforms. Section 3.4 discusses HWT and its computation. An overview of the technique for centralized datasets is also presented in Section 3.5. 3.1. Clustering
The main goal of clustering as an important data mining task is to group data records such that the clusters.

Clustering is widely used in many applications such as grouping related documents for browsing, customer behaviors analysis, targeted marketing, and many others.

There are many similarity measures for clustering; one of them is Euclidean distance. We have selected it due to the following reasons: 1. The Euclidean distance is the most popular distance measure. 2. It has simple distance function calculation. 3. There exist some algorithms, such as [47], that can accurately map arbitrary distance functions 4. Other important works on PPC have also chosen Euclidean distance, so we have selected it in order
If i = ( x distance between i and j is computed by:
In general, d each other, and becomes larger when they differ. 3.2. Scaling data perturbation
In [35], authors have proposed a family of geometric data transformation methods (GDTMs) that and have a sound mathematical foundation. GDTMs change all records of one attribute equally, so the Euclidean distance is preserved between data records to a great extent.

Scaling data perturbation (SDP) is one of GDTMs. In SDP, the values of con fi dential attributes in data records are perturbed using a multiplicative noise perturbation .The noise term e applied to each con fi dential attribute is different and can be either positive or negative.
 Among all GDTMs, SDP provide better tradeoff between privacy and clustering quality. Therefore, SDP with the noise term e generated from a uniform distribution is used in this paper. 3.3. Wavelet transforms
The wavelet transform is a synthesis of ideas that emerged over many years from different domains, such as mathematics and signal processing [44]. Fourier transform consists of breaking up a signal into view of a signal.

Wavelets are de fi ned by the wavelet function  X  (also called mother wavelet) and scaling function  X  and L 2 ( R ) is the set of signals of fi nite energy, a wavelet family  X  set of functions generated by dilations and translations of a unique mother wavelet. A function  X   X  L 2 ( R ) is an orthogonal wavelet if the family  X   X  such that &lt; X , X &gt; = 0(i.e.,  X  is orthogonal to  X  ) and satis fi es  X  detailed description of wavelets can be found in [8,10].

Wavelets also have many important properties such as ef fi cient computation, vanishing moments, play an important role in data mining and provide more effective solutions in different domains in data mining such as data management, data preprocessing and data mining tasks. In fact with respect to above de fi nition, a wavelet transform converts data from the original domain to a wavelet domain by expanding the row data in an orthogonal basis generated by dilation and translation of a mother and father wavelet [10]. After this transformation the structure of the original data is preserved.
Two important properties of wavelet transform serving as the backbone of our proposed technique are as follows: 3.3.1. Parseval X  X  theorem
In wavelet transform domain, Parseval X  X  theorem [10] is de fi ned as: any function e  X  L 2 ( R ) can be represented in terms of the following orthogonal basis as: and the c of  X  j,k ( t ) and e ) . The Parseval X  X  theorem states the following property of wavelet transform: the energy of function e .

Parseval X  X  theorem indicates that the energy is preserved under the orthogonal wavelet transform. If e be the Euclidean distance function, this theory also states that e will not change by using orthogonal wavelet transform. 3.3.2. Ef fi cient computation
The computation of wavelet transform is also very ef fi cient. The run time complexity of the wavelet mation, where n is the maximum level number of wavelet or Fourier decompositions [44]. Thus, data analysts may prefer the wavelet transform methods that are fast in dealing with very large datasets.
In wavelet transform, often speak about approximation coef fi cients (approximations) and wavelet the signal. The wavelet coef fi cients are the low-scale, high-fre quency components. The approximation
As shown in Fig. 1, the low-pass fi lter is associated with the scaling function and high-pass fi lter is associated with the wavelet (mother) function. In other words, the wavelet function and the scaling wavelet decomposition. 3.4. Haar wavelet transform
Haar wavelets are the most commonly used orthogonal wavelets in database and computer science because they are easy to understand and fast to compute.

In the process of HWT the length of each input data row is restricted to an integer power of 2. If this end of each data row. The HWT has the following mother function [8]: and the following scaling function:
In a dataset with n attributes, each data record is de fi ned as vector Y = { y 1 , y 2 , ... , y wavelet coef fi cient with a speci fi cstage r are decomposed from
The i th element of
Thus by using HWT, the original dataset could be replaced by the approximation coef fi cient dataset is reduced by removing wavelet coef fi cient parts in each stage. Each column of the approximation coef fi cient dataset is simply called a coef fi cient.
 record continues till the last (zero) stage, only one element approximation coef fi cient dataset will have only one column. Figure 2 shows an example of using HWT for one sample data record: The reconstruction algorithm for reconstruct Y data record is the reverse process of decomposition. The
It is clear that reconstruction of the exact original data record values is possible if and only if the to fi nd the second last approximation part and this process can repeat until reconstruction Y . wavelet decomposition continues until the last stage) is equal to m *2( n -1). 3.5. An overview of the technique for centralized datasets
In this section, we brie fl y review the HWT-S-CDS technique proposed in [51,55] for centralized datasets. For more information about this technique, including the experimental results and privacy analysis, please see [51,55]. 3.5.1. Stopping deco mposition for centralized datasets
An important question in using HWT in the context of PPC is selecting an appropriate stage for stopping decomposition. Improper continuation of decomposition will lead to lower number of dimensions and is done by a data mining server. Another solution to the selection of proper stage is based on loss of transformed records, the clustering accuracy will also be high. The loss of distance can be measured by computing the stress function [34]: where, d record i and j in transformed dataset. However, computing stress function is expensive, especially for large datasets. Also, c omputation must be done in each step of d ecomposition.

Zhang et al. [50] has used sum of squared errors (square of Euclidean) as the dissimilarity measure between an original data record and its transforms data record. Given a data record Y  X  R n ,if  X  Y  X  R n specify any transformed of Y , the sum of squared errors (SSE) between Y and  X  Y is de fi ned as SSE ( Y, with HWT. Given a data record Y  X  R n , the energy of Y is de fi ned as E ( Y )= n following inequality holds ( R = log 2 n ) :
Then, removing the wavelet coef fi cient parts of all records in stage r + 1 or above will not change the similarity between the original dataset and the approximation coef fi cient dataset too much and no information will be lost during transformation and the decomposition steps can be continued.
We use this metric for selecting automatic and a ppropriate stage for stoppi ng decompos ition without expensivecomputing and need any p arameter setting by user in the pr oposedPPC technique in centralized number of dimensions in the approximation coef fi cient dataset is equal to or less than n /2. 3.5.2. The solution for enhancing privacy of centralized datasets category of breaching algorithms to reconstruct the original data values from the transformed dataset. Breaching algorithms can exploit the knowledge of the distribution of the transformed or the original data in order to guess appropriate values for the original data. In HWT, breaching algorithms require guessing two values for each transformed value. In [6], two categories of breaching algorithms are proposed for this purpose: (1) independent-sequence breaching and (2) dependent-sequence breaching. For independent-sequencebreaching, the transformed data must be broken down into the two components should be regenerated by two or more transformed data values.

Both breaching algorithms are Monte Carlo-based Bayesian reconstructions on empirically derived cumulative proba bility distributions (CDF) for the transformed da tasets [6]. In HWT, because a trans-formed value ( y values must be less and the other may be greater than y rence of y other may be greater than e the original values. This estimated value is one of the reconstructed values. For HWT, if e randomly selected from a uniform distribution, an e selected from a uniform distribution an e on the cumulative probability distribution, the appropriate reconstructed value ( x e (2 j ) .After fi nding x i (2 j ) , use it along with y ij value to fi nd x i (2 j  X  1) .
Based on the above explanations, if the third party knows that the wavelet transform is used and also the number of these steps is not known by the third party. This lack of information about the number of steps of HWT will make more dif fi cult the usage of sequence breaching algorithms by third party to third party for any reason, by having the number of attributes of the original dataset and the number of attributes of the transformed data, he/she will be aware of the number of steps of HWT. When the third party knows the number of steps of HWT, if the number of steps is more than one, the third party should use the sequence breaching algorithm more than one time for reconstruction of the original data. By noting to the estimating nature of these algorithms, by execution of them more than one time, the of the correct original data values is higher. Execution of SDP method and permute the attributes in a secret random permutation order on the datasets r esulting from HWT, not only will cause generation of resulting from HWT, the su ccess ratio of the sequence breachin g algorithms will reduce more. By of reconstruction of the exact original data values using breaching algorithms will more decreased and even will be equal to zero. Therefore, execution of SDP, especially in the situations where the HWT is algorithms.

By noting to the above explanations, we offer a solution to enhance the privacy by the following two steps: 2. Apply the SDP method on approximation coef fi cient dataset to enhance the data distortion and 3. Do not divulge the number of original attributes to data mining server. 3.5.3. The algorithm for centralized datasets The pseudocode of the algorithm for centralized datasets from [55] is presented in Fig. 3.
With respect to the time complexity of HWT as discussed before, the time complexity of the proposed algorithm is the sum of the time complexity of the different steps bounded by O( mn ) [55]. 4. Motivations of individuals would be protected. Meanwhile, after transformation, the distances between data records should be preserved or marginally changed. Therefore, a PPC technique must fi nd an optimum in the tradeoff between clustering quality and data privacy.

The problem of PPC can be divided into three groups: (1) PPC over centralized datasets, (2) PPC consider the second and third problems in detail.

In PPC techniques that belong to the second group, k parties ( k 2), a trusted third party (TTP) and common n attributes. The data mining server is responsible for performing clustering over horizontal joint datasets of k parties. In a PPC technique that belongs to the third group, k parties ( k 2), a for a common set of data records. The data mining server is responsible to do clustering over vertical joint datasets of k parties. In both vertical and horizontal cases, the TTP that may be one of the k the proposed technique. This will lead to increasing the privacy and security of the proposed technique the data mining server concealing the data record values of each party. Therefore, before moving their attribute values. However, transforming the dataset of each party should be done such that the results vertically/horizontally joint of all original datasets.

In this paper we propose two different algorithms using HWT and SDP for two groups of PPC problems discussed above. In our proposed algorithms, all parties must collaborate together and with the TTP to datasets. 5. The proposed techniques for distributed datasets
In this section, we present the detailed procedures of the proposed techniques for PPC of distributed are presented in Sections 5.4, and 5.5, respectively. 5.1. Assumptions
The matrix representation is one of the most popular ways to specify the record/attribute relationships columns and all secret records represented by rows. In such a matrix, we assume that every entity is fi xed and numerical (integer or real numbers) and no missing entity is allowed. 5.2. Stopping decom position for distributed datasets In horizontally distributed datasets, the metric f or stopping decom position of HWT that described in Section 3.5.1 should be used such that the following three properties are provided: 1. The information lost during Haar wavelet d ecompositio n for each party should be minimum, 2. The distance between data records in horizontal joint of the approximation coef fi cient datasets of 3. The number of dimensions (coef fi cients) of the approximation coef fi cient datasets of all parties For this purpose, all parties should continue the Haar wavelet decomposition steps if the relation Hence, after stopping the Haar wavelet decompositi on, the number of dimensions of all approximation preserved well.

In vertically distributed datasets, the metric should be used such that the number of coef fi cients of approximation coef fi cient datasets belonging to each of the k parties is maximized, to provide the following two properties: 1. The information lost during Haar wavelet d ecompositio n for each party should be minimum,
For this purpose, the decomposition steps should be s topping the same as the hor izontally distributed dataset using the relation Eq. (11).
 experimental evaluation section we will demonstrate that using this metric offer a good balance between both centralized and distributed datasets. 5.3. A solution for enhancing privacy of distributed datasets
In [53], two models are de fi ned: a malicious model and a semi-honest model that are needed to formulate the behaviors of the participants, which must be considered as an assumption in the design of dataset of other parties).

For applying a solution for enhancing privacy, introduced in Section 5.3.2, for vertically and horizon-on security and privacy are done by the TTP. The main tasks are as follows: (1) determination of a proper stage for stopping Haar wavelet decomposition, (2) generation of a random seed for execution of SDP, (3) generation of a secret random permutation order, and (4) zero padding operations (as will be transformed datasets. The TTP and all k parties use cryptographic tools such as public key encryption to send and receive information between TTP and other parties. The TTP have a public key that all distribution problem, (2) the data mining server will not become a bottleneck, and (3) the data mining server will not be aware of any important information.

With respect to the step 2 of an introduced solution for enhancing privacy, in horizontally distributed datasets, all parties must apply SDP on their local approximation coef fi cient datasets, however the random noises that are generated for each common attribute of all parties must be the same to preserve value to generate each noise term e for each common attribute from the same uniform distribution. Also, in the same secret random permutation order and without divulge speci fi ed random permutation order to condition, all parties must cooperatively generate the same secret random permutation order and apply it to their local approximation coef fi cient datasets.

Generating a common secret random seed and secret random permutations order for coef fi cients can be done securely by using cryptographic tools such as public key infrastructure (PKI). By using a PKI, each party registers a public key and has its own private key. The TTP selects a random permutation order of coef fi cients and a random seed. Then he sends the random permutation order and seed to the seed using their own private key. Note that the sensitive information is never sent to the data mining server. Thus the data mining server cannot access the sensitive information.

In vertically distributed datasets, to execute the zero padding instructions that must be done by each with TTP and in a secure manner obtain the values required for zero padding instructions, without any disclosure to the data mining server according to the step 3 of an introduced solution for enhancing attribute X  X  number using the public key of TTP and sends the encrypted value to the TTP. Then, the TTP data mining server cannot access to sensitive information. 5.4. The proposed algorithm for horizontally distributed datasets
Let we have k parties, the TTP and Y = { Y 1 , Y 2 , ... , Y between k parties that contain m 1 , m 2 , ... , m pseudocode of this algorithm is presented in Fig. 4.

At the step 10, the TTP select pr oper stopping decompos ition stage based on received values that is described before in Section 5.2. The steps 17 through 22 are also accordance with the solution for enhancing privacy described completely in Section 5.3.

With respect to the time complexity of HWT as discussed before, all parties take O ( mn ) to compute the transformed data matrix Y to the TTP during this algorithm is &lt;x&lt;R ).

Now, we will investigate the information revealed during the execution of the proposed algorithm to show that revealing this information will not endanger the privacy of local datasets of all k parties. 1. In the step 8 of this algorithm, each party sends the value 2. In the step 17 of the algorithm, all k parties and TTP cooperatively generate a secret random 3. In the step 20 of the algorithm, all k parties and TTP cooperatively generate a secret random seed. 5.5. The proposed algorithm for vertically distributed datasets
The algorithm is based on the linearity property [8] of wavelet related transforms, i.e. given two vertically distributed datasets X and Y ,
Let we have k parties, the TTP and Y = { Y 1 , Y 2 , ... , Y between k parties contain n 1 , n 2 , ... , n The pseudocode of this algorithm is presented in Fig. 5.

In the proposed algorithm for applying HWT over data records with different attributes belonging to For running zero padding instructions, each party needs to know the sum of the attribute numbers of the before in Section 5.3.

The following is a small example of a dataset with 3 partitions for demonstrating how zero padding instructions (steps 4 through 8) are run by each party: have: Next, all parties run HWT on their local padded data matrix, according to the step 10 of the algorithm. Because the total information sent from all parties to the TTP during this algorithm is step 15 that is repeatedly sending in each stage x (0 &lt;x&lt;R ).

The linearity property of wavelet transform shows that it is also possible to use wavelet transform However, the important problem in the proposed algorithm is to select an appropriate step for stopping the stopping measure that proposed in Section 5.2 is used. However, since in vertically distributed party to be less than the energy computed in previ ous stage (i.e. this means that decomposition should obtained. However, stoppi ng wavelet decompos ition according to t he step 17 of algorithm that stated comparing to centralized datasets. Because, obtaining correct results are more important than earlier cause to obtain incorrect and very weaker results than the centralized dataset.

Now, we will investigate the information revealed during the execution of the proposed algorithm to show that revealing this information will not endanger the privacy of local datasets of all k parties. 1. In the step 15 of this algorithm, all parties send the value
Finally, it is worth to mention that the disclosure of the information, which is needed for execution of both proposed distributed algorithms, is divided between all k parties, TTP and data mining server However, even in worst case and when the protocols are broken, due to the suppression of wavelet distributed datasets, the degree of privacy will be preserved in an acceptable level. 6. Experimental evaluations
In this section, we present the experimental evaluations of the proposed technique. We have used k-means , which is one of the best, commonly used and scalable clustering algorithms. It is also one of the most popular Euclidean distance-based mining algorithms.

The experiments are done using a prototype tool that we have developed for this purpose. We have executed the tool on a machine with Intel Core TM 2, 2.0 GHz CPU, 1.0 GB of RAM, and under Windows XP Professional. All of the algorithms and evaluation methods were implemented using Microsoft Visual Studio 2008 with C# programming languageand using extreme optimization component[18] as statistical utility. We have used R SA-1024 for public key encryption.

Section 6.1 describes the datasets used in our experiments. Section 6.2 describes the PPDM techniques that are implemented and compared with our technique. The evaluation approach and metrics used to validate our technique and compare it to the other privacy preserving techniques are presented in and 6.5, respectively. 6.1. Datasets
The experiments are run on the following datasets:  X  Synthetic Control Chart Time Series (SCCTS) and Pendigits obtained from UCI Repository of  X  Pumsb and Mushroom obtained from Frequent Itemset Mining Dataset Repository [19].
We have selected the above datasets considering two important measures: (1) being real life datasets, (2) containing numerical attribute values without missing values and with various distributions. For as [32,34,54]. Besides, we intended to choose various datasets having enough and different number of attributes and records to demonstrate the performance of the proposed method.

A summary of the information about the selected datasets are shown in Table 1. 6.2. PPDM techniques
We have developed a pr ototype tool, n amed HWT-S, based on HWT-S-HDDS and HWT-S-VDDS, privacy will enhance more.

The following two existing techniques applicable for distributed PPC are also implemented to compare with HWT-S:  X  DCT-H . This technique, which is proposed in [32], maps the original n -dimensional data points to 6.3. Evaluation measures 6.3.1. Clustering quality measure
When using PPC techniques, the clusters in the transformed dataset must match the corresponding the corresponding records in the transformed dataset should also be grouped together. In this paper, we present the performance evaluation results for clustering quality based on overall F-measure (OF) that is already used to evaluate PPC techniques in [32,34].
 the matrix of frequencies shown in Table 2.

In [34] such a matrix is referred to as the clustering membership matrix (CMM). In CMM, the rows represent the clusters in the original dataset, the columns represent the clusters in the transformed dataset, and freq After computing the frequencies freq F-measure for each cluster c using the following formulas: where | c the highest F-measure among all the c X  c , fi nally the overall F-measure ( OF ) is computed as follows:
The higher the OF value, the higher the clustering quality. 6.3.2. Privacy measure
As mentioned before, in our proposed technique, the third party cannot automatically reconstruct the original data values exactly, because wavelet coef fi cient parts are removed in each stage of Haar wavelet decomposition. Besides, privacy can be enhanced to a great degree with a proposed solution for enhancing privacy. Moreover, we need some measures to compute the degree of privacy provided by the proposed technique.
 interval [2], (2) using information theory [1], (3) based on the notion of privacy breach [6,16,17,52], and (4) some privacy measure that only depend on original dataset and its corresponding transformed is not appropriate for clustering because the Euclidean distance is based on individual data values, while the information theory considers only the distribution of values. Meanwhile information theory Privacy breach-based methods that were discussed more in Section 3.5.2 consider the worse cases. In fourth group, some privac y measure are proposed that are based on the fact that traditionally, degree of privacy provided by a perturbation technique has been measured as the variance between the actual that belongs to fourth group, VD, RP, RK, CP and CK, as de fi ned in [49].

In this paper, with respect to the four different groups of privacy measures and by noting to the existing breaching algorithms for HWT, we have used breach-based methods for measuring privacy, which considers the worst case (as we mentioned earlier). For HWT-S, this paper considers the worst cases for measuring privacy: In the worst case, we assume that the third party not only knows that the wavelet transform is being used, but also knows its type and number of decomposition steps (i.e. the the value we have measured in worst case. Because, the third party don X  X  know the number of original attributes and randomly guesses the number of attrib utes (the number of wavelet decomposition steps) and reconstructs the original data by this random assumption and using sequence breaching algorithm. HWT that may occur due to few number of attributes or other reasons), the degree of privacy is provided in an acceptable level.

In the worst case, the privacy of HWT-S is computed by comparing the original dataset and the reconstructed dataset generated by independent-sequence breaching algorithm and knowledge of correct number of attributes (the number of wavelet decomposition steps). 6.4. Evaluation results for HWT-S-HDDS algorithm
This section presents the results of k-means clustering for the horizontally distributed datasets. In data holder parties as TTP. Tables 3 through 6 sho w the number of decompos ition steps, the number provided with fi rst algorithm for the Pendigits, SCCTS, Pumsb and Mushroom dataset when the number of sites varies from 1 to 4. For all datasets the data are distributed uniformly across these sites. is 1) and OF values remained almost unchanged when the number of sites increases. The degree of privacy using HWT-S for horizontally distributed datasets is almost the same as the degree of privacy of the HWT.
We have selected k = 5 considering the fact that this value is a common value, not too low and not too reported.

For SCCTS dataset the number of attributes in the transformed datasets that are generated by second algorithm is equal to 30, for Pendigits is equal to 4, for Pumsb is equal to 37 and for Mushroom is decomposition. To compression HWT-S with RP and DCT-H, both of which belong to the PPC techniques and can be applied to PPC in horizontally distributed dataset, we also show the results of RP and DCT-H in Fig. 6. Figure 6 shows the OF value of clustering using HWT-S, RP, and DCT-H for the Pendigits dataset when the number of sites varies from 1 to 4.

Figure 6 shows that the quality of clustering of HWT-S is much higher than the quality of clustering achieved by RP and DCT-H in horizontally distributed datasets. As mentioned above, for Pendigits dataset the number of attributes in the transformed datasets generated by second algorithm is equal to 4 in all cases. So, it is clear that the percentage of dimensionally reduction is about 75% for each site. Meanwhile the OF values reported in Fig. 6 for DCT-H and RP is for six attributes in transformed datasets that generated with these methods. Thus it is clear that HWT-S provide much higher OF values a good balance between clustering quality, data privacy, and data reduction and provides better results comparing to the other techniques for horizontally distributed datasets. 6.5. Evaluation results for HWT-S-VDDS algorithm
This section presents the results of k-means clustering for the vertically distributed datasets. In our holder parties as TTP. Tables 11 through 14 show th e number of decompositio n steps, the number of SCCTS, Pumsb and Mushroom dataset when the number of sites varies from 1 to 4.
 is 1) and clustering quality not decrease much as the number of sites increases. The degree of privacy using HWT-S for vertically distributed datasets is the same as the degree of privacy for centralized cases for different number of sites is due to the difference in the number of steps of the HWT.
The number of attributes in the merged dataset that generated with third algorithm varies for different number of sites, with respect to selection of proper stage for stopping Haar wavelet decomposition.
To compression HWT-S with RP and DCT-H, both of which belong to PPC techniques and can be applied to PPC in vertically distributed dataset, we also show the results for RP and DCT-H in Fig. 7. Figure 7 shows the OF value of clustering using HWT-S, RP, and DCT-H for the Pendigits dataset when the number of sites varies from 1 to 4.

Figure 7 shows that the quality of HWT-S is much higher than the quality achieved by DCT-H and RP increases from one to four. The number of attributes in the merged dataset that generated with HWT-S for Pendigits dataset is 4, 4, 9, and 4 for 1, 2, 3, and 4 sites, respectively. Meanwhile the OF values that generated by these techniques. Thus it is clear that HWT-S provide much higher OF values even for higher percentage of data reduction.

It is also obvious that the second algorithm provides a good balance between clustering quality, data privacy, and data reduction and provides better results comparing to the other existing techniques for vertically distributed datasets. 7. Conclusions
This paper is the result of research in the context of privacy preserving data mining (PPDM). Privacy preservation is an important issue in many domains such as data mining. We have used the HWT and results have shown that the proposed technique provides a proper degree of privacy, data reduction and clustering quality beyond its fast running in comparison with other privacy preserving clustering (PPC) techniques having similar functionality.

Some major features of the proposed technique are as follows:  X  Preservation of the Euclidean distance between data records to provide accurate clustering,  X  Changing the original data values to preserve privacy,  X  Generation of different transformed datasets even from the same original datasets,  X  Prevention of reconstructing the original data values,  X  Provision of dimensionally reduction for data reduction purposes,  X  Independence of the clustering algorithms,  X  Fast running,  X  Finding a proper tradeoff between clustering quality, data privacy, and data reduction automatically  X  Applicable to both centralized and distributed datasets.

Our major contributions in this paper can be summarized as follows: 1. We have used HWT in order to decompose each data record into an approximation coef fi cient part 2. We have used sum of squared errors for selecting automatic and appropriate stage for stopping Haar 4. We have proposed two algorithms using HWT and SDP, one for a horizontally distributed, and the 5. According to our experimental results, the proposed algorithms provide proper results with respect
In future, we intend to do more research on applying the proposed technique on privacy preservation of non-numeric data values. Furthermore, we have planned to investigate on other kinds of wavelet transforms and different ways to apply them to different PPDM tasks and compare the results in order to achieve to a complete solution.
 Acknowledgement
Authors are grateful to Madjid Sadeghi Alavijeh for taking part in implementation of some parts of a prototype tool used for experimental evaluations presented in this paper.
 References
