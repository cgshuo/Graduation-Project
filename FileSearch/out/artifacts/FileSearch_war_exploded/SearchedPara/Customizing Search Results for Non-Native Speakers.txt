 Blog posts, news articles and other webpages are present on the web in multiple languages. Standard search engines eval-uate the relevance of the candidate documents to the given query. However, when considering documents with overlap-ping content, many of them written in a foreign language other than the user X  X  own native tongue, it is beneficial to promote documents that are easy enough for the user to read. Here, we show how to rank a collection of foreign doc-uments based on both: a) relevance to the query, and b) the comprehension difficulty of the document. We design effective ranking operators that evaluate the difficulty of a foreign document with respect to the user X  X  native language. We show that existing search engines can easily augment their scoring function by incorporating the proposed com-prehensibility metrics. Finally, we provide extensive exper-imental evidence that the comprehensibility-aware ranking model significantly improves the standard relevance-based ranking paradigm.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search Process ; I.2.7 [ Artificial In-telligence ]: Natural Language Processing X  Text analysis multilingual document search, document comprehensibility
Large numbers of texts discussing the same topic can nowadays be retrieved from Web sources around the world (e.g. news portals, reviews, blogs, RSS feeds, etc.). As a result, a typical web search may return similar documents in multiple languages. The question that we are addressing in this work is how to build an engine that delivers not only the most relevant documents, but also the ones that best match the user X  X  comprehension level of a foreign language. Intuitively, foreign documents that are easier to read and un-derstand should be ranked higher than more advanced texts with the same coverage of the topic. Given a collection of foreign documents (e.g. books, articles, news articles), we provide a structured methodology to effectively and accu-rately rank them based on their estimated comprehensibil-ity . We then use this mechanism to build a search engine that considers both relevance and comprehensibility when evaluating candidate documents. To the best of our knowl-edge, this is the first approach that examines the problem of document ranking through the prism of foreign language difficulty, using a completely unsupervised approach.
The problem is challenging because it lies at the conflu-ence of fields as diverse as linguistics, information retrieval and machine learning. Our approach combines both struc-tural and linguistic features, exploring the different aspects of document comprehensibility. An additional dimension that we consider when estimating the reading difficulty of a foreign document, is the native language of the reader. For example, for a native Portuguese speaker, it can be signifi-cantly easier to comprehend Spanish documents rather than documents written in Greek or German. This is mainly due to the presence of cognates , i.e., words that are similar in both meaning and form in two languages. Such visual sim-ilarities between words can significantly ease the task of a reader. We incorporate the identification of such word in-stances in our methodology.

We envision numerous applications where our methodol-ogy can be of use: 1) Customization of search results . Given the user X  X  personal linguistic skills, our methodology enables the multi-lingual personalization of a search session, by evaluating and ranking foreign documents based on their comprehensibility. 2) Language learning. Studies have suggested that learning a foreign language is more effective when study-ing texts that match one X  X  comprehension level [1]. Our work can be used to recommend the most suitable reading material to foreign language students. 3) Machine Translation. By estimating the compre-hensibility of a given document, we can determine whether it falls within the language skills of the reader, or whether a translation should be attempted.
We estimate the comprehensibility of foreign documents based on two primary factors: readability , which assesses the structural features of a given document, and f amiliar-ity , which focuses on the vocabulary. Each of these two components captures a different aspect of comprehensibili-ty. An illustration of our mechanism is shown in Figure 1. The comprehensibility of a document d with respect to a l anguage L is defined as a linear combination of readability and familiarity. Formally: where fam ( d, L ) denotes the familiarity of document d to user who is native (or proficient) in language L , and rd ( d ) denotes the readability of d . Notice that familiarity (and hence comprehensibility) is defined as a function of the tar-get language L . For example, a German document is ex-pected to have higher comprehensibility when read by Dutch people rather than by Italian people, due to the higher lin-guistic similarity between German and Dutch. Finally, the two non-negative weights w 1 and w 2 are used to tune the impact of each factor.
The familiarity of a document assesses how likely it is that its vocabulary is known to the user. We define the measure as a function of two indicators: popularity and cognativity . Popularity attempts to captures the general prevalence (i.e frequency) of terms in the language. Intuitively, rare terms are less likely to be familiar to the user. Cognativity is a language-dependent measure. Its use is to capture the degree to which a document X  X  terms are similar in the user X  X  own native language; normally, such terms would be easier to understand. Next, we discuss further these two factors.
Intuitively, when reading a foreign document, a non-native speaker is more likely to recognize a very popular token than one which is rarely used. In a broader context, a document consisting of commonly used tokens is much easier to com-prehend than another that uses more esoteric and unfamiliar vocabulary. In order to capture this  X  X rior frequency X  of a given a token t , we utilize the collective knowledge of the we-b. Today, most search engines provide the number of pages that the query appears in. We use this information as an estimate of term popularity. In particular, we use the page count from Google. An added advantage of using search en-gines instead of pre-existing text corpora, is the fact that online texts capture newly used terms, which is importan-t since languages constitute an evolving organism. Finally, search engines allows us to focus on a particular language for the documents to be retrieved.
 Formally, popularity is defined as:
Definition 1 (Popularity). The popularity of a ter-m t is computed as the fraction: where count ( t ) returns the number of appearances of a given token t in the entire document collection D , and V is the vocabulary of all the distinct tokens in D . The popularity of t is thus defined as the percentage of tokens in V that have fewer appearances in D than t .

In addition to having a clear interpretation, this formula is robust to outliers (i.e., tokens with very low or very high frequencies) and serves as an intuitive and parameter-free way to smooth the obtained counts. Alternative smoothing techniques have been proposed in the literature [2]. Consider the following sentence:  X  X in Experte kam die Maschine zu reparieren X . A person proficient in English can easily deduce that this sentence translates to  X  X n expert came to repair the machine X , even if one is only a novice in German. The inherent familiarity of this sentence is due to the existence of cognates . Cognates are words in differ-ent languages that exhibit both orthographic and semantic affinity. In our work, we spot cognates by exploiting interlin-gual homography. Our approach is based on the well-known problem of finding the Longest Common Subsequence (LCS) of two strings. In particular, given a term t , let tr ( t, L ) be its translation in the native language L of the user. Then, we define their similarity sim as follows: where | X | represents the length of a given string. Clearly, the measure assumes values in [0 , 1], evaluating the visual simi-larity between the term and its translation. Naturally, due to polysemy issues, we need to evaluate the term X  X  similari-ty with all possible translations in the target language, and retain the best score. Let T ( t, L ) contain all translations of t in language L . Then, we define the cognativity of the term t with respect to L as: We consider a word as a cognate if its cognativity value is greater than a cutoff threshold value  X  . In our experi-ments, we set  X  = 0 . 45 which yielded the best results across languages. Terms identified as cognates are assigned the maximum possible familiarity (i.e. 1). The familiarity of non-cognates is equal to their popularity. Formally, we de-fine the familiarity of a term t with respect to a language L as follows:
Equation 3 gives us the familiarity of a single term. We define the aggregate familiarity of an entire document by where c ount ( t, d ) is the total number of appearances of term t in document d , and | d | = P t  X  d count ( t, d ) is the total number of terms in d .
Several languages such as German, Dutch or Swedish, are known as compounding languages , because they allow the creation of new complex words by merging together sim-pler ones. Schiller identified more than 40% of the words in a large German newspaper corpus as compounds [16]. As an example, the German compound word  X  X edizindoktor X  (=medical doctor) cannot be found in a dictionary and po-tentially also has few occurrences in texts or the web; howev-er, its meaning is easily discernible given its building blocks. The splitting of a compound word in its basic parts is called decompounding . Our methodology is equipped with an ef-fective algorithm for identifying 2-and 3-compounds. For ease of exposition, we provide next a method for the detec-tion of 2-compounds. This method can be easily extended to identify 3-compounds.

Given a term t of length n , let sub (  X , i, j ) return the sub-string of  X  that begins at position i (inclusive) and ends at position j (inclusive). If i &gt; j , the function returns  X  . We define the decompounded familiarity of t with respect to a language L as follows: where 1  X  i  X  n and fam (  X  ) = 0. Therefore, the above for-mula discovers the split point that maximizes the popularity of the two sub-components.
The notion of document readability has been a well-studied topic, particularly for English documents [18]. Many read-ability formulas have been proposed, all attempting to as-sign a single numerical readability score to a document. The most popular example is the Flesch Reading Ease (FRE) measure [10], which consists of a linear function of the mean number of syllables per word and the mean number of words per sentence in the document. The measure has been adapt-ed to several languages, including English, French, Spanish, Italian and German 1 . For example, the formalization of the measure for German documents is: where words ( d ) , sents ( d ) and syllables ( d ) denote the num-ber of words, sentences and syllables in d , respectively. The weights on the above formula have been derived by means of regression on training data. The Flesch Reading Ease yields numbers from 0 to 100, expressing the range from  X  X ery dif-ficult X  to  X  X ery easy X , and is meant to be used for measuring the readability of texts addressed to adult language users. We choose FRE as a measure of readability, because of its popularity and widespread use as a readability yardstick in many organizations (e.g., U.S. Department of Defense).
Finally, we define the readability rd ( d ) of a document d as the normalized version of F RE ( d ), taken by dividing the h ttp://www.ideosity.com/ideosphere/ seo-information/readability-tests score with the maximum F RE (  X  ) observed over our entire collection D . Formally:
Next, we discuss how comprehensibility can be combined with relevance, toward a complete search engine for foreign-document retrieval. In our work, we define the relevance rel ( d ) of a given document d via a combination of the Boolean Model and the Vector Space Model, as implemented in the popular Lucene search engine 2 . We further normalize the relevance values by dividing the score of each document with the maximum value observed over the entire corpus.
A document d can be represented by a two-dimensional the document X  X  comprehensibility (given the user X  X  native language L ) and relevance, respectively. We say that docu-ment d 1 , dominates a document d 2 if d 1 is both more com-prehensible and more relevant to the given query. Docu-ments that are not dominated by any other document com-pose the skyline S  X  D of the entire corpus D .
 Figure 2: Navigating on the skyline of top-rated results
T he skyline serves as an intuitive way to browse the promis-ing documents of the search results. A good starting point is the skyline-document d  X  that maximizes ( rel ( d  X  )+ C ( d annotated as the  X  X iddle X  document in Figure 2. If the start-ing point is not comprehensible enough, the user is presented the next document to the right on the skyline. Note that the next point will be less relevant, otherwise it would dominate the point before it. Similarly, if the document not relevant enough, the next document to the left on the skyline is con-sidered. Since the entire navigation process focuses on the skyline points, it is guaranteed to lead to document that is both comprehensible and relevant enough, if such a docu-ment exists. Further, this interface can assist in the evalu-ation of the relative comprehensibility and relevance of any document on the search result, based on its distance from the skyline points. We refer to this approach as LingoRank .
In this section we illustrate the ability of our methodology to capture the inherent comprehensibility of foreign textual content. We also demonstrate how our LingoRank approach is superior to standard relevance-based techniques in the context of foreign-document retrieval.
I nitially, we want to estimate how well our comprehensi-bility measure approximates the ranking provided by human annotators. We have assembled documents that address the same general topic but examine different aspects of it and possibly addressing different audiences. The topic we have focused on is the financial crisis in Greece (2010-2012). In order to include texts of variable comprehensibility, we have selected texts from sources with consistent language levels: 3 segments from financial websites (sophisticated and for-mal language with technical terms), 3 segments from main-stream news portals (edited, well-structured content with an average level of sophistication), and 3 segments from rele-vant comments posted in public forums (simpler, informal language). Nine German texts were given to eight human annotators who are native (or proficient) in English, but possess only a basic command of the German language. The same process was repeated for nine English texts, which were given to eight annotators native in German, and have a ba-sic command of the English language. The annotators were asked to rank the texts from easiest to most difficult. We also computed the scores for each text, using the developed com-prehensibility formula. For this study, we used equal weights for familiarity and readability. The results are shown in Fig-ure 6.1. The first column of each table shows the rank of each text based on the scores assigned by our method, the second and third columns hold the average rating and the standard deviation assigned from the annotators, respectively. Figure 3: User Study on German and English texts: texts a re ranked by our technique, as well as by human annotators. The results of the study are very encouraging. For both German and English texts, the rank given by our method is consistently close to the average human rating. Our method-ology was successful in ranking the texts by comprehensi-bility, illustrating its potential usefulness in the context of foreign document retrieval. For German documents, the ob-served standard deviation on the ratings was low, indicat-ing a strong consensus among the annotators. The elevated deviation for English suggests that the same task on the English texts was more challenging. Nonetheless, our com-prehensibility formula was still able to capture the average consensus rating of the annotators.
In this experiment, we use data from the educational web-site CourseInfo.com , which hosts essays on a variety of topics, including foreign languages. On the website, es-says are grouped into 3 levels of increasing difficulty: GCSE (300 essays for high school students), A-level (150 essays for pre-college preparation) and University-level (50 essays for Bachelor-level students). We use all available essays from the  X  X erman Essays X  category.

First, we measure the comprehensibility of each essay. We tune the weights of familiarity and readability by minimizing the Minimum Squared Error (MSE) over the annotations of our user study. Specifically, the weights for familiarity and readability were set to 0.65 and 0.35, respectively. As men-tioned above, each essay belongs to one of three difficulty levels: A-Level , GCSE or University . Given two different levels, we define the error to be the fraction of essay-pairs that contain an essay from each level, such that the essay from the easier level received a lower comprehensibility s-core than the one from the higher level. The values for all possible level combinations are the following:
Observe that for GCSE and University (the two levels that differ the most in terms of difficulty) the observed error was very low (3.1%). A small error was also observed for the GCSE and A-Level pair, indicating that our approach can consistently distinguish GCSE essays. An inspection of the erroneous pairs for the A-Level / University experiment re-vealed that deducing the true level of difficulty was an am-biguous task, even for a human annotator. Still, as shown in the table, such pairs made up for less than a third of the total corpus.
Here, we compare our LingoRank approach to standard relevance-based techniques. Specifically, we show that our approach allows the user to consider significantly fewer doc-uments, before locating one that is both comprehensible and relevant to the given query. For this experiment, we collect-ed a total of 1,002,394 articles from Google News, written in four different languages: German, Italian, Spanish, French. The corpus spans the timframe between August and Novem-ber of 2011.

The experiment is performed independently for each lan-guage. First, we compose a list of 10 (foreign language) queries for each language, pertaining to major events that happened in the time-frame of the news articles. For this, we consult the list of important events of 2011, as reported in Wikipedia 3 . For each query, we retrieve the top-100 rel-evant documents, using Lucene X  X  search functionality. For each document d in the top-100, we set d as the target . The target X  X  comprehensibility and relevance values determine the lower bounds of the search session: any document sat-isfying both bounds is considered a match. An approach is then evaluated based on how many documents it needs to present to the user until a match is found. We report the average number of examined documents for each approach (out of the 100). Since the process is repeated independent-ly for each of the 10 queries, for each of the four languages, and for each of the documents in the top-100, we simulate a total of 4  X  10  X  100 = 4000 search sessions. We use Lucene X  X  relevance-based engine as a baseline. Starting from the most relevant document, we traverse downward until a compre-hensible document is reached. We then report the number h ttp://en.wikipedia.org/wiki/2011 of documents that had to be examined. We refer to this a pproach as RelSort .
 Figure 4: Number of documents that are considered by L in-goRank and RelSort until a match is found.

The results are shown in Figure 4. The y-axis shows the number of documents that had to considered until a match was found. The figure shows that our approach outper-formed RelSort , consistently considering fewer documents across all four languages. In fact, the average value observed for LingoRank was around one (1), suggesting that the start-ing point chosen by our approach (i.e., the document with the best comprehensibility/relevance mixture) was often the only one that needed to be considered.
Our work is related to the approaches for evaluating tex-tual readability [6], which can be broadly categorized into supervised and unsupervised . Unsupervised approaches rely on two aspects of text: the familiarity of the reader with its semantic units (words or phrases) and the complexity of its syntax. In order to define a metric for the former, linguistic resources ranging from manually compiled lists of words [3] to language models [5] have been employed. For syntactic complexity, the average sentence length is widely used, since it has been found to be strongly correlated with comprehensibility [7, 19, 9]. Supervised approaches exploit the availability of training data in order to derive statistical language models of readability for a particular language [14, 4, 15, 13]. Finally, approaches such as the Lexile framework for document readability [11], do not consider the native language of the reader and ignore the effect of cognativity. Other relevant papers include studies on cognativity [8, 17], a concept that is a part of our own methodology. Finally, the impact of readability in the context of education and language learning has been explored by Ott [12].

The problem we explore in this work is far more rich and challenging, since assessing the comprehensibility of a for-eign document, which depends on both structural, linguis-tic, and content-based features. In addition, the above works make no effort to utilize their concept to improve foreign-document search, which is the main focus of our work.
In this work we described a search engine for foreign-document retrieval. The novelty of our engine lies with the consideration of a document X  X  comprehensibility, in addition to its relevance to the given query. Our experimental evalua-tion verified the efficacy of our approach, and demonstrated its advantage when compared with standard techniques that focus exclusively on relevance. [1] T. Bell. Extensive reading: speed and comprehension. [2] T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. [3] J. Chall and E. Dale. Readability revisited: The New [4] K. Collins-Thompson and J. Callan. Predicting [5] K. Collins-Thompson and J. Callan. A language [6] W. DuBay. The principles of readability. In Impact [7] R. Flesch. A new readability yardstick. In J Appl [8] B. M. Friel and S. M. Kennison. Identifying [9] M. Heilman, K. Collins-Thompson, and M. Eskenazi. [10] J. Kincaid, R. Fishburne, R. Rogers, and B. Chissom. [11] C. Lennon and H. Burdick. The lexile framework as an [12] N. Ott. Information retrieval for language learning: [13] S. E. Petersen. Natural language processing tools for [14] S. E. Petersen and M. Ostendorf. A machine learning [15] E. Pitler and A. Nenkova. Revisiting readability: a [16] A. Schiller. German compound analysis with wfsc. In [17] S. Schulz, K. Mark  X o, E. Sbrissia, P. Nohama, and [18] L. A. Sherman. Analytics Of Literature: A Manual [19] A. Stenner. Measuring reading comprehension with
