
Data mining is an active research area (Elmasri and Navathe 2000). Clustering, asso-ciation rules and decision trees are some of the most popular data mining techniques.
This work focuses on clustering. There is no universal definition of clustering, but most authors agree on the following. Clustering algorithms partition a set of points into disjoint groups such that points in t he same group are similar to each other and points across groups are different from each other (Duda and Hart 1973). et al. 1998; Xu and Jordan 1996; Zhang et al. 1996), but there are some algo-rithms that can cluster categorical data (Ganti et al. 1999; Guha et al. 1999; Huang 1998; Ordonez et al. 2001). Some important data mining clustering approaches in-clude (Aggarwal and Yu 2000; Agrawal et al. 1998; Bradley et al. 1998; Breunig et al. 2001; Hinneburg and Keim 1999; Nanopoulos et al. 2001; Zhang et al. 1996).
Several aspects make clustering a challenging problem. These aspects include data set size (Zhang et al. 1996; Bradley et al. 1998), high dimensionality (Aggarwal and
Yu 2000; Hinneburg and Keim 1999), data sparsity (Aggarwal and Yu 2000; Guha et al. 1998) and noise (Agrawal et al. 1998; Bradley et al. 1998; Breunig et al. 2001;
Hinneburg and Keim 1999). This article focuses on the well-known expectation-maximization (EM) algorithm.
Based on previous work related to maximum likelihood estimation, EM was formal-ized as a fundamental statistical algorithm in the seminal article (Dempster et al. 1977), and there has been significant research on it thereafter (Bradley et al. 1999; Ordonez and Cereghini 2000; Roweis and Ghahramani 1999; Xu and Jordan 1996;
Yuille et al. 1994; Ordonez and Omieci nski 2002). A good reference on EM can be found in MacLachlan and Krishnan (1997). The EM algorithm can be used for tering. The EM algorithm has many desirable features (Xu and Jordan 1996; Jor-dan and Jacobs 1994). It has a strong statistical basis, theoretical guarantees about optimality (Dempster et al. 1977), easily expl ainable results (Bradley et al. 1999), robustness to noise (Roweis and Ghahramani 1999) and the ability to handle miss-ing information (Dempster et al. 1977). Nevertheless, the classical EM algorithm also has several limitations. The quality of the final solution heavily depends on converge to a poor locally optimal solution. It needs an undetermined number of iterations to converge. Therefore, it is not possible to have guaranteed performance.
In the case of clustering computations, EM gets unstable when variances approach zero. This problem commonly arises when d ata sets have binary coded categorical values, many outliers or missing information. Probabilities vanish for outliers, mak-ing computations undefined or unstable. It may also produce inaccurate results with high-dimensional data sets, which are known to be difficult to cluster (Aggarwal and Yu 2000; Beyer et al. 1999).

There has been work on accelerating and improving EM to work with large data sets, like incremental EM (Neal and Hinton 1993), on-line EM (Sato and Ishii 2000; Xu and Jordan 1996), EM for high-dimensional binary data (Ordonez et al. 2001),
EM programmed in SQL (Ordonez and Cereghini 2000) and scalable EM (Bradley et al. 2000), but no solution solves all problems listed above. Related work on EM will be discussed in more detail later.
This work proposes several improvements for the EM algorithm to accelerate con-vergence and to find higher quality solutions. Cluster centroids are initialised with a small perturbation of the global mean. Sufficient statistics are combined with peri-odic M steps to accelerate convergence. Res eeding of low-weight clusters and split-ting of high-weight clusters help in finding higher quality solutions. A fraction of the global variance is added to cluster vari ance matrices to solve zero-variance prob-lems. Mahalanobis distance is used instead of probability to manage outliers. All the improvements are put together in an algorithm called FREM, which stands for fast and robust EM. A preliminary version of FREM was introduced in Ordonez and
Omiecinski (2002), and here we provide a revised version, studying its properties in more depth.
 tion of the EM algorithm. Section 3 presen ts improvements to accelerate conver-gence and to find higher quality solutions , and it introduces the FREM algorithm. Section 4 presents experiments to justify FREM X  X  parameter settings and compares
FREM against the classical EM algorithm and the on-line EM algorithm. Section 5 discusses related work. Section 6 concludes the article.
EM estimates the statistical parameters of a mixture of normal distributions. The multivariate normal (also known as Gaussian) density function for a vector y on d -dimensional space for cluster j , j  X  X  1 ,..., k } ,is: where C j is called the mean vector and R j is called the variance matrix; C d -dimensional vector and R j is a d  X  d diagonal matrix (zeroes off the diagonal). The probability of the mixture is computed as 1973) instead of Euclidean distance. The basic difference between them is that R is used to scale each dimensio n for distance computation. This is particularly use-ful for dimensions having different scales and clusters having different spatial sizes (area of the ellipse they represent). The squared Mahalanobis distance from point y to cluster j is computed using { y trices C , R , W (containing the k means, k variances and k weights, respectively), a measure of model quality L (  X  ) (explained below) and a soft partition of Y into k subsets. Matrices C and R are d  X  k , whereas matrix W is k matrices, we use the following convention for subscripts. For point number, we use i ; i  X  X  1 , 2 ,..., n } . For cluster number, we use j ; j one dimension, we use l ; l  X  X  1 , 2 ,..., d } . To refer to one column of C or R we use the j subscript (i.e., C j , R j ). So C j refers to the j th cluster centroid and R has its corresponding variances. Each R j is a diagonal covariance matrix that can be manipulated as a vector. To refer to the probability of y we use the notation P ( y i | j ) = P ( y i ; C i , R j ) parameters are used as a single set called  X  , i.e., as the mixture model. The quality of a clustering solution is measured by a quantity called average log likelihood (the closer to zero the better), that is computed as
This is a brief overview of the EM clustering algorithm. EM starts by initialis-ing  X  . Initialisation will be described in more detail later. It has two major steps: the
E step and the M step. EM iteratively executes the E step and the M step as long as the change in L (  X  ) is greater than an accuracy threshold or as long as a maximum number of iterations has not been reached. The E step computes P
P ( y ;  X  ) . The M step updates  X  based on the probabilities computed in the E step.
EM is theoretically guaranteed to monotonically increase L to converge to a locally optimal and stable solution (Dempster et al. 1977; Xu and
Jordan 1996). Due to lack of space, we do not include a more detailed exposition of the EM algorithm, but pseudocode for the classical EM algorithm and a more detailed description can be found in Ordonez and Omiecinski (2002) and Ordonez and Cereghini (2000).
This section contains this article X  X  contributions. First, improvements to the EM clus-tering algorithm are introduced in two major groups: improvements to increase speed and improvements to find higher quality solutions. Second, the FREM clustering al-gorithm is presented assembling all improvements together.
EM improvements are presented in two groups: improvements to increase speed and improvements to increase the quality of solutions. These groups are orthogonal in the sense that they can be used independently if needed.
 ficient statistics are summaries of groups of points (Mood et al. 1974) (in this case clusters), represented by Y 1 ,..., Y k . The idea of using sufficient statistics is not new (Bradley et al. 1998; Neal and Hinton 1993; Zhang et al. 1996), but the way we use them is. The sufficient statistics we are about to present are based on two import-ant assumptions: dimensions are independent (to simplify computations) and they induce a hard partition on Y . Sufficient statistics are stored in matrices M ( M and Q are d  X  k and N is k  X  1). Matrix M stores k sums of points ( k vectors with d dimensions), matrix Q has k sums of squared points ( k diagonal matrices with d variances on the diagonal) and vector N counts the number of points per cluster: zero because several computations become undefined (e.g. means that points belonging to one or more clusters have the same value on some dimensions. We solved t his problem by adding to each dimension a constant multiplied by the global variance matrix (  X  ) when variances are updated. This con-stant is chosen small enough to avoid altering the real variance values. This im-provement will be used when the M step updates R from sufficient statistics, mak-ing FREM numerically stable. Then the update formulas based on sufficient statistics used by FREM are: an incremental fashion ba sed on the assumption that n is large. The E step must be run for every point, i.e., n times per scan. The E step updates M tive way after each point is read, assigning it to the cluster with highest probability.
Sufficient statistics reduce I/O time by decreasing the number of iterations and by allowing parameter estimation periodica lly as points are being read. Cluster mem-bership is determined in the E step and C , R , W are updated in the M step. By using sufficient statistics, FREM can run the M step at different times while scanning Y .
At one extreme, we could have an on-line EM version (Xu and Jordan 1996) that runs the M step after every point. That would be a bad choice. It would be sensitive to the order of points and, more importantly, it would not reach the global optimal solution (Bradley et al. 1998; Xu and Jordan 1996) (although in some cases, it can get close); this will be shown in the experimental section. On-line EM (Sato and
Ishii 2000; Xu and Jordan 1996) makes only one iteration and executes the M step after every E step for each point. It represent s the fastest version that can be derived from the EM algorithm but without guarantees about optimality or stability of the solution. At the other extreme, we could have a version that runs the M step after all n points are read. This would reduce it to a standard version of EM with a hard partition and little performance improvement would be achieved. Nevertheless, this latter version would not have any sensitivity to the order of points, would compute the correct value for L (  X  ) and would have the potential of reaching the global op-timal solution. Therefore, it is desirable to choose a point somewhere in the middle, but closer to the last scenario. That is, running the M step as few times as pos-sible. When a good approximation to the solution has been reached, then normal
EM iterations can be executed to make the algorithm converge. It is important to observe that each new M step combines pa st estimations from previous M steps with the estimation based on the latest subset of points. In other words, FREM does not forget estimations from previous M steps and it uses sufficient statistics M a memory of previous estimations. In general, it would be incorrect to reinitialise
M ,
Q , N at each M step because FREM would tend to base its estimation on the last points read.

To control the frequency of M steps, we introduce  X  , which will be an important parameter for FREM. The M step is periodically executed iterations and only once in subsequent iterations. To some extent, total number of iterations when each subset of points is similar t o a previous subset in the sense that the clusters in one subset are similar to those in another subset. We believe that, for large n , this is a reasonable assumption. Based on (subset of Y ) considered per M step will have size equal to n  X   X  n , FREM approximates on-line EM, and as  X   X  1, FREM approximates a hard partition EM. Higher n implies more redundancy in the dataset, larger sample sizes and therefore more opportunity to conver ge to a good solution with less work. Lower n implies less redundancy, fewer M steps on the first two iterations and subsequently more iterations to converge. FREM makes at least three iterations. The first iteration gets a decent solution, the second one tune s it and further iterations make FREM converge. The default value for  X  will be  X  = n ; this setting will be justified in Sect. 4.

Like many other numerical optimiza tion methods, EM depends on a good initiali-sation. Initialisation is based on the global statistics and dimensionality of Y .The global mean  X  and the global covariance matrix  X  can be computed in one scan over the data via sufficient statistics, the equations of which are as follows:
Initialisation is then done as follows: W j = 1 / k , R  X  represents standard deviations per dimension,  X  controls how far seeds are from the global mean and r is a random number in [ 0 , 1 must be noted that, as d  X  X  X  , C j  X   X  (seeds get closer to centroid seeds are small perturbations of  X  . On the other hand, at the beginning of each iteration, sufficient statistics M , Q , N are initialised to zero. Notice that FREM could also be initialised using a sample of k different points, but ours was better.
As discussed in Sect. 1 EM is often criticized for finding a suboptimal solu-tion (Bradley et al. 1999, 2000; Roweis and Ghahramani 1999), a common problem with clustering algorithms (Dubes and Ja in 1980; Duda and Hart 1973) and optimiza-tion methods. Most of the times, suboptimality involves several clusters incorrectly grouped together as one while other clusters have almost no points or are empty. So we propose splitting heavy clusters to reach higher quality results. We introduce a minimum weight threshold  X  to control splitting and reseed low-weight clusters.
This will be an important parameter for FREM. Let a be the index of the weight of some cluster s.t.
 vect [ R j ] and C b + vect [ R j ] are the new centroids, where the right terms repre-sent one standard deviation. The old value of C a disappears and its points change membership to heavier clusters. Then index a is incremented and b is decremented to analyse the next potential low-weight cluster to reseed. Splitting gets repeated until there are no more clusters below  X / k or a  X  b . Cluster splitting will be done iteration, FREM will not split clusters in order to avoid incorrectly splitting clusters.
This is based on the fact that FREM will execute frequent M steps on the second iteration and will iterate giving points the chance to change membership to the best and further iterations will regroup its points. This improvement will lead to higher quality solutions in most cases, but it is not guaranteed that the global optimum will be reached.
 are far from any cluster centroid C j and then P ( y i ;  X  )  X  probability of belonging to any cluster. The solution we propose is simple; we use the closest cluster based on Mahalanobis distance to update sufficient statistics. FREM does not multiply the probability approximation by cluster weights. Also, almost-null probabilities make EM numerically unstable, in this case due to numerical precision.
So FREM has a lower threshold  X  for probabilities. If P are updated using the nearest neighbor of y i basedonEq.(3).
 high dimensionality or datasets where t he set of points considered at each M step is significantly different from previous subs ets, it is necessary to adjust covariance matrices so that the spatial size of some cluster is not so large so as to absorb points that may belong to another cluster and not so small so as to reject most points. At the end of the first and second iterations, all covariance matrices R (tied) to be the same, as follows: R j = k J = 1 W J R J for j assures, among other things, that covariance matrices of heavier clusters have more influence, all clusters have the same spatial size at the beginning of the second and third iterations and that each variance pr eserves the scale of each dimension. With this adjustment, FREM proceeds from a numerically stable approximation to the other hand, centroids C j do not need to be adjusted because they will be close to the final solution and then can be considered initial points at the beginning of the third iteration. When FREM iterates, making one M step after doing n partial E steps for all points (a full E step), each R j will quickly get more accurate.
In this section, all improvements are put together in one place. The FREM algorithm pseudocode is shown in Fig. 1. The input is a set of points Y having d dimensions and k , the desired number of clusters. The output will be the mixture model  X  ={ C , R , W } (centroids, varian ces and weights) and L ure of model quality) as defined in Sect. 2. Variable x probability that point y i belongs to cluster j in a soft partition; even though FREM produces a hard partition, these numbers can be used to individually judge cluster membership in a soft assignment. FREM X  X  iterations can be summarised as follows:
The first iteration accelerates convergenc e and increases quality of solutions by split-ting clusters. The second iteration accelerat es convergence and regroups points when clusters were incorrectly split. The third and further iterations eliminate sensitivity to the order of points (if any), compute an accurate value for L converge to a stable solution. The mixture parameters of the model one point.
 clustering solution. The first iteration splits those clusters that have too many points and reseeds those that are almost empty. Th e first two iterations are somewhat sensi-tive to the order of points and their L (  X  ) computation may not be accurate because
 X  is updated at every M step and L (  X  ) is not recomputed for old points. Therefore, further iterations are needed to have consistency between fourth and subsequent iterations represen t an execution of EM based on sufficient the M step looking at the entire data set Y , are essential to compute an accurate value for L (  X  ) like classical EM, to reduce sensitivity to the order of points and to deal with clusters having high overlap. Therefore, after the second iteration, FREM monotonically increases L (  X  ) and converges to a locally optimal solution, like clas-sical EM. The FREM algorithm has time complexity O step is executed n times per iteration, computing cluster membership probabilities (Eq. 1), probability of the mixture (Eq. 2) and updating sufficient statistics based on
Eqs. (5), (6) and (7) in time O ( dk ) . The mixture parameters C in the M step using Eqs. (8), (9) and (10) taking O analysis of FREM X  X  time complexity consult Ordonez and Omiecinski (2002).
To evaluate quality of results and performance, we tested FREM with real and syn-thetic data sets. FREM is compared against the classical EM clustering algorithm (Dempster et al. 1977; Roweis and Ghahramani 1999; Xu and Jordan 1996) and against the on-line EM algorithm (Xu and Jordan 1996; Bradley et al. 2000). Be-cause  X ,  X  are computed only once per data set, that time is not included in the total time. All algorithms were implemented in the C ++ language. Data sets were stored as plain text files and were never maintai ned or cached in memory. All experiments were run on a Sun server running at 800 MHz, having several GB of disk space and 128 MB on main memory. The implementation used text files as input. Some time improvement could be expected if bi nary files were used because no parsing would have to be done; we include some experiments about this aspect. Another ob-servation is that the FREM, EM and on-line EM implementations use dynamically sized matrices, and these require heavy pointer manipulation. Using fixed-sized ma-trices without pointers would also improve times because memory allocation would be easier and accessing arrays by subscripts would be fas ter. In short, these per-formance experiments were performed under pessimistic conditions without making optimizations that would affect usability.
 to study FREM X  X  parameters. The first section presents extensive experiments with real data sets to justify parameter setti ngs and to compare FREM against classical
EM and on-line EM. The second section compares FREM, EM and on-line EM with synthetic data sets assessing correctness, sensitivity to changing characteristics in the input data and performance.
In this section, we evaluate quality of results and convergence speed with real data sets from different domains. These data sets cover a wide spectrum, including small, medium and large data sets; low, medium and high dimensionality; different scales for each dimension and dimens ions with missing information. Astronomy is a large data set containing information about stars from an observatory. The dimensions of stars include their position in the sky as ( x , y ness; for this data set, n = 368,891 and d = 4. This data set presented highly skewed data, significant cluster overlap an d big differences in dimension scale. The basket data set contained a sample of aggregated information of transactions from a large data warehouse of a grocery retailer. The basket dimensions are total sales amount, total cost, number of items bought, number of distinct articles and number of distinct departments visited. Measurements about the basket were clean and there were little zero-variance problems, no missing information and no highly skewed data. That is, this was a well-behaved data set. For this data set, n d = 5. The medical data set contains information for a set of patients being treated for heart disease. The numeric dimensions include four artery disease (blockage) percentages in the [0 X 100] range and nine numbers describing perfusion measure-ments in the [  X  1,1] range corresponding to nine specific regions of the heart; for this data set, n = 655 and d = 13. This data set presented serious problems with zero variances in most clusters, significan t cluster overlap and a significant portion of outliers, reflecting the variety of m edical conditions of each person. The coil data set contains several measurements about European rivers. Categorical values were coded as binary dimensions. Dimensions had very different scales and some columns had the same value in 90% of the rows, causing variances to be 0. This data set was small, with n = 200 and d = 18. The U.S. census data set contains demographic data from the 1990 U.S. census. Record identifiers were removed and all remaining numbers were treated as clustering dimensions. This data set was high dimensional and sparse, having 0 values on most dimensions, making it hard to cluster. In this case, n = 8 , 000 and d = 68.

As explained above, FREM has defaults for its parameters. This section justifies those settings. These defaults will be used to compare FREM, classical EM and on-line EM with real and synthetic data sets. The most important parameters are of M steps on the first and second iterations, and weight clusters. Parameters  X  and  X  are used just to make FREM numerically stable and they have settings that introduce a minimum change to the clustering problem.
So we did not conduct experiments varying them. We did not include experiments varying or the maximum number of iterations because FREM and classical EM are always stopped at a locally optimal and stable solution. The maximum number of iterations was set at 500 so that FREM always stops at a stable solution but there was no case reaching this limit. FREM was run ten times for each different parameter setting to cluster each real data set. We decided to set k the clustering problem more challenging. We will later show experiments with lower k values to compare FREM aga inst classical EM and on-line EM. In general, the default settings for FREM X  X  parameters were  X  = and  X  = 1 . 0e-300.
 used initialisation for centroid-based iterative algorithms (EM and K means). Our initialisation uses small changes in  X  to initialise C . The standard deviation factor for seeding was set at  X  = 1 /( dk ) ; in general, this number worked well. We found a wide range for  X  that went from 1 / k to 1 /( 10 dk ) that produced soluti ons of comparable quality. We decided to set  X  to 1 /( dk ) by default. Due to lack of space, we do not include experiments varying  X  . The alternative initialisation uses k different points randomly selected from Y . This initialisation, known as the Forgy method, is proven to produce good results (Hammerly and Elkan 2002; Bradley et al. 1998). For this set of tests, FREM is run without acceleration or reseeding. From the table, we can see that, in general, the mean-based initialisation ( quality solutions for both the best and the average case. The drawback is that the number of iterations is sometimes increased . These results indicate that a mean-based initialisation is better for higher dimensional data, which is the case of the medical and the U.S. census data sets; in those two cases, L (  X  ) in the other three cases.
 text and binary files. These experiments attempt to quantify how much performance cases, FREM is faster than EM. For text files, FREM takes about one third the time compared with EM. For binary files, FREM takes about one fourth the time EM needs. Clearly, both algorithms are affected by the parsing that needs to be done with text files. However, FREM is more i mpacted than EM with text files because times with binary files decrease by almost 50% while those for EM decrease by 30%. A close code inspection reveals that classical EM performs many more com-putations than FREM per iteration becau se it produces a soft partition. The compu-tations that mainly make EM slower are those required to estimate soft-partitioned sufficient statistics for C and R , which require dk computations to be updated. For more details, this can be verified in the E step in the EM pseudocode in Ordonez and Omiecinski (2002). These experiments show that using binary files would benefit FREM more than EM.

Table 3 is one of the most interesting experiment summaries showing how FREM is accelerated. This table shows what happens when extreme, doing only one M step per iteration, to the other extreme, doing n Msteps per iteration. The table shows averages; results for the best run are similar, but due to lack of space, they are omitted. FREM used  X  in the first two iterations as proposed in Sect. 3 and did not reseed low-weight clusters. Then FREM iterates until there is a minimal change in L (  X  ) . Because there are several selected they are proportional to n . We conjecture that  X  should be directly proportional to n .
The pattern is not perfect, but it can be observed that, in general, the lowest number of iterations shows up in log ( n ) or appear more on the left side of the table. Another outstanding feature is that, in most cases, the highest quality of solutions appears in either log  X  = n (on-line version) produced better results than cases. Therefore, this table shows that  X  helps achieving faster convergence but also reaching higher quality solutions. FREM e scapes local maxima by making periodic
M steps. It is noteworthy that  X  = log ( n ) produced better results in two cases, but we chose  X  = of M steps and sample size.
 ters and splitting of high-weight clusters. Tables 4 and 5 show the best and average run varying  X  , the threshold to reseed low-weight clusters and split high-weight clus-ters. Recall that clusters s.t. W j  X   X / k are reseeded on the first iteration after every
M step. At one extreme,  X  = X  1 represents a conservative no-reseeding strategy. At the other extreme,  X  = 1 represents an aggressive res eeding strategy because clusters are always reseeded. When  X  = 0, clusters are reseeded only when there are empty clusters and thus it represents a conservative strategy. Finally, represent a moderate strategy. From both tables, it can be seen that higher quality solutions appear on the right side, where reseeding/splitting is more aggressive. In general,  X  = X  1 produced the lowest quality solutions. In some cases, solutions were slightly better when  X  = 0. In all cases, the best solutions were found at  X  = 0 . 5. To our surprise, the most aggressive reseeding found better solutions than  X  = 0 . 2 in some cases. The second important fact is that the number of iterations required for  X  = 0 . 2or  X  = 0 . 5 were less or equal than those required for FREM without reseeding. In other words, reseedin g accelerated convergence in some cases.
For the experiments to be presented later, we chose but  X  = 0 . 5 could have worked as well. We preferred conservative. Summarising, reseeding low-weight clusters and splitting high-weight clusters is needed to reach higher quality solutions. The number of iterations may required without reseeding/splitting.
We now proceed to compare FREM against classical EM, which is known to pro-duce high-quality solutions and on-line EM, which is very fast. The same type of initialisation was used to make a fair comparison. All algorithms are initialised using the global mean as explained in Sect. 3. Thi s initialisation produced slightly higher quality solutions than initialising with a sample of k points as shown before. FREM and EM are stopped until they converge with the same tolerance
This setting assured both algorithms converged to a stable and locally optimal so-lution. FREM X  X  most important parameters are  X  , the number of periodic M steps, and  X  , the threshold to reseed low-weight clusters. Based on the experiments pre-sented before, their default settings were  X  = real data sets, especially high-dimensional ones, the regularisation constant required because of 0 variances, which was  X  = 1 . 0e-2 to introduce a negligible change in the real variance values. The default value for close to the smallest number in double precision. The standard deviation factor for seeding was set to  X  = 1 /( dk ) . In general, k , the desired number of clusters, was the only input parameter.

Results for the best run out of ten runs are summarised in Table 6 and results for their average are summarised in Table 7. The three algorithms were run ten times with k = 5 and ten times with k = 10. Times are given in seconds. Because the globally optimal solution cannot be known, we can only use L of results. The closer L (  X  ) is to 0, the better. For FREM, we include measurements at the end of the third scan (FREM-3scan s) and after it has converged (FREM-converge). EM is always measured after i t converges. A second scan is needed to compute an accurate value for L (  X  ) for on-line EM, but that time is not included in the table.
 than classical EM in eight cases. In one case, it found a worse solution ( Astronomy k = 10), and in one case, they were tied ( Astronomy k 3scans found better solutions than EM but in general it came behind. On-line EM was always in last place. Performance-wis e, on-line EM was the fastest followed by FREM-3scans. FREM-converge came in third place and classical EM came in fourth place. Now we summarise Table 7. For the average run, classical EM fares better than in the best run. In two cases ( Astronomy k it finds higher quality solutions than FREM-converge and in four cases it is better than FREM-3scans. The winner is FREM-co nverge, which finds the best solution in eight cases. On-line EM is again the worst. Performance-wise, results are similar to the best case with FREM being faster than classical EM and on-line EM being the stop FREM after the third iteration.
This section presents experiments with synthetic data. Each experiment was run ten times unless otherwise indicated. The times and quality of results are averaged and such average is reported. We used a synthetic data generator that created mixtures of normal distributions. In general, real data sets have missing information, skewed distributions, varied scales, high dimensionality and noise. With that in mind, we created synthetic data sets varying n (size), d (dimensionality), k (embedded number of clusters),  X  (a noise level) and  X  (a scale for each dimensi on). For a more detailed discussion on how we generated synthetic data, consult (Ordonez and Omiecinski 2002). We now proceed to analyse how FREM, classical EM and on-line EM behave as these parameters vary.

Noise, dimensionality and a high number of clusters have been shown to be trou-blesome aspects for clustering algorithms. In the following set of experiments, we analyse the algorithm behaviour when we vary them. The set of experiments is not complete, as we do not show what happens when we vary combinations of these parameters, but we tried to choose values that are common in a data mining envi-ronment. These experiments were essential to test algorithm correctness.
Accuracy is the main quality concern for clustering. Because we are clustering synthetic data, we already know the true clusters. In our case, one cluster is con-sidered accurate if there is no more than error in its centroid and weight. This is computed as follows: Let c j ,w j be the correct mean and weight, respectively (as given by the data generator), of cluster j having estimated C algorithm, and let e be the estimation error. Then cluster j is considered accurate if ( 1 /
This is a high requirement because noise and data skew can easily distort estima-tions. For the experiments below, we set e = 0 . 1. That is, we will consider a cluster to be correct if it differs by no more than 10% of its true mean and weight. The process to check clusters for accuracy is as follows: Clusters are generated in a ran-dom order and clustering results may also appear in any order given initialisation on random seeds. So there are many permutations in which clusters may correspond to each other ( k ! ). To simplify checking, we build a lis t of pairs (estimated, generated) clusters. Estimated cluste rs are sorted in descending or der by weight. Then each es-timated cluster is matched with the closest synthetic cluster and both are eliminated from further consideration. For each pair, we compute its accuracy error. If it is be-low e , it will be considered accurate. Any unmat ched estimated cluster is considered inaccurate.

Figures 2 and 3 compare FREM vs. EM and on-line EM. Algorithms are ini-tialised in the same manner and are run with k + 1( k is the actual number of embed-ded clusters and the extra cluster accounts for noise). The three graphs in Fig. 2 show the average percentage of correctly found clusters increasing noise level, dimension-ality and number of clusters. In all cases, FREM clearly performs best. Noise affects both, but it is noteworthy that EM performs almost equally better at the highest level of noise. Observe that FREM and on-line EM are minimally affected by dimension-ality. Figure 3 compares algorithms in the best run out of ten runs. In this case, results for EM are better than the average EM case above, but FREM finds better solutions anyway. On-line EM finds lower quality models than FREM and EM for increasing noise and number of clusters. It is interesting to observe that on-line EM is as good as FREM for higher dimensionality. The difference in quality becomes more significant as the number of clusters and dimensionality increase. The differ-
L (  X  ) , but it is always higher in the mixture models computed by FREM, consistent with the shown graphs.

Time varying, dimensionality and desired number of clusters is shown in Fig. 4. In general, on-line EM (with one scan) takes between one third and one half the time
FREM (with three scans) takes to cluster a synthetic data set; those times are not shown. The synthetic data sets defaults were n = 10 k , d exhibit linear behaviour and FREM outperforms EM by one order of magnitude. Because these data sets are synthetic and clu sters are well separated, it is easier for
FREM to find a good solution and it converges in just three iterations. However, in order for EM to have equivalent performance, it would need to converge to an acceptable solution in only three itera tions. In general, this is unlikely.
FREM solves several problems related to clustering with the EM algorithm, but it is not the final answer. Its most important limitations are the following: Like classical
EM, it still needs to read all the n points at each iteration. Even though we showed experimental evidence about the default settings for  X  and ing problems that require careful tuning. The regularisation constant deal with common 0-variance problems, but i t introduces a small variation in actual variance values. Computing probabilities in high dimensionality is difficult. Given our outlier handling approach, points may be assigned to the closest (and not most probable) cluster, like K means. There exist many scalable clustering algorithms. Some well-known examples include
BIRCH (Zhang et al. 1996), SKM (Bradley et al. 1998), OPTI-GRID (Hinneburg and Keim 1999), CLIQUE (Agrawal et al. 1998), CURE (Guha et al. 1998), PRO-
CLUS (Aggarwal and Yu 2000) and DBSCAN (Ng and Han 1994). There is work on accelerating K means using trees (Pelleg and Moore 1999). We did not compare FREM with any of these clustering al gorithms because FREM and EM compute a statistical model and optimize log likelihood (Eq. 4), whereas these algorithms compute clusters based on distance or density. That is, they have different optimiza-tion goals.
 previous work comes from the machine learning community. Neal and Hinton study authors analyse several variants of EM viewing the log-likelihood optimization as an optimization of a physics energy function. One of the variants they present is incremental and the key idea is to use sufficient statistics. They describe a simple mixture problem with d = 1and k = 2 and show some experimental evidence of the superiority of the incremental approach. Their study is not complete, as it does not address the problem of clustering large data sets with high dimensionality. The problem of minimising disk acces s is not analysed. Problem s related to numerical sta-bility, proper initialisation and outlier handling are not addressed in their work either. sets was introduced in Bradley et al. (1998, 1999, 2000). A scalable K means is pre-sented in Bradley et al. (1998), and this algorithm is used as a framework to build a very similar scalable EM algorithm in Bradley et al. (1999, 2000). In Bradley et al. (1999, 2000), the authors present a heuristic scalable EM algorithm (SEM) that works in memory and also summarises points through their sufficient statistics.
This summarisation is done by making compression in two phases. To avoid locally optimal solutions, they reseed empty clust ers and they estimate several models con-currently, sharing information across models. They require the user to specify what fraction the working buffer should be from the entire database. The buffer size is a critical parameter and its typical value is 1%. The authors cannot explain why this value gives the best results. This wor k is different from ours in several aspects.
Our initialisation is based on global statistics, whereas theirs is based on sampling, although FREM and SEM would work with either one. They use K means to cluster points in main memory, whereas we use EM its elf. Our reseeding strategy is differ-ent, as we split high-weight clusters. They do not address the problem of handling outliers or zero variances. FREM does not estimate several models concurrently, as that is expensive. FREM requires several passes over the data, whereas theirs requires only one. However, EM is a CPU-bound algorithm and SEM makes it even more CPU intensive, rendering a slow algorithm as explained in Fanstrom et al. (2000).
Their log-likelihood computation is inaccu rate because it is based on an approxima-tion with sufficient statistic s; an accurate computation would require a second scan.
Finally, they do not show SEM converges to a stable solution. They assume SEM converges based on K means convergence (which is known to be an inferior algo-rithm) on compressed point sets, but it is never shown that the solutions found by SEM are stable and locally optimal when making a full iteration with the E and the clustering algorithm is not faster than K m eans and, in some cases, it is even slower.
Also, they show that the quality of solutions is about the same. In short, their version is not consistently better than K means. For the reasons described above, we believe a direct comparison with classical EM and on-line EM was more appropriate.
The idea of helping EM clustering deal with 0 variances has been studied before in Ueda et al. (2000). The authors use Bayesian regularisation techniques to deal with 0 covariances. Their approach is similar to ours, but it is different in two aspects: we use regularisation together combined with sufficient statistics and we scale the reg-ularisation constant by the global covariance matrix so that Mahalanobis distance is properly computed for dimensions with different scales. Moving cluster centroids to better locations in K-means clustering is presented in Fritzke (1997). This approach is different from ours in several aspects. It is based on Euclidean distance and only clusters in one M step and the splitting criterion is more complex. Other convergence acceleration methods are described in MacLachlan and Krishnan (1997).
This work proposed several improvement s to the EM clustering algorithm to accel-erate convergence and to find higher quality solutions. These improvements were incorporated into a new algorithm called FREM. The most important improvements include mean-based initialisation, periodic M steps based on sufficient statistics and low-weight cluster reseeding combined with cluster splitting. FREM has two main parameters, the number of periodic M steps per iteration and the threshold to reseed low-weight clusters. These and other additi onal parameters have defaults. In general, the desired number of clusters is the only input parameter. In the experimental sec-tion, the proposed improvements were studied in isolation and the default parameter settings were justified. Experiments show the quality of solutions and speed of the algorithm with both real and synthetic data sets. FREM can produce a reasonable solution in just three iterations or it can be run until it converges to a locally optimal solution like EM. In general, FREM finds solutions of higher quality than EM in fewer iterations. Compared with on-line EM, running FREM with three iterations is only three times slower, but it always finds better solutions.
 statistics and there is a way to move bad solutions to new locations. Other data mining problems solved with EM include factor analysis or time series where an incremental learning approach may be feas ible. We want to provide theoretical ev-idence about FREM X  X  speed of convergence and the quality of solutions it finds.
Clustering data with categorical attribut es (Guha et al. 1999), finding outliers (Knorr and Ng 1999) or finding clusters in projections of high dimensional data (Aggarwal and Yu 2000) are also interesting problems for further research.

