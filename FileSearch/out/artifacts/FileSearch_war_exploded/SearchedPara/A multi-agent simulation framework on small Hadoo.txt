 1. Introduction
Multi-agent simulation is an important research field in today X  X  scenario and analyzing emergent behaviors in such simulations largely depend on the number of agents involved. More the number of agents involved with detailing of agent decision making and communication, closer is the result obtained to the real world.
A classic example would be simulating traffic of a city with millions of human agents and thousands of commuting agents like trains, buses. Due to large number of agents, time taken for such simulations becomes large and so we resort to a distributed computing solution.

In simulations involving millions of agents, the running time for each simulation cycle can be of several seconds or even minutes; and when run for a large number of cycles, the total simulation time can be of several hours or days. If some of the machines fail during the run-time, then the entire simul ation needs to be restarted. If we can somehow dynamically re-balance the work load on the remain-ing number of machines and maintain logs of the simulation progress, we can continue the simulation from the point of failure.
So, we require a fault-tolerant and failure-resilient framework which is also easily extensible to run on a large number of processors and agents.

Hadoop ( http://wiki.apache.org/hadoop ) is a promising option in this respect. It takes care of non-functional requirements, like scalability, fault-tolerance, load balancing, and the framework developer only needs to develop a layer for agent-based simula-tion on top of it. If some systems in the distributed environment fail, the simulation does not stop. Hadoop automatically reba-lances the work load on remaining systems and continues to run the simulation. Further, Hadoop facilitates dynamic addition of new nodes in a running simulation. In this paper, we present a design of an agent-based simulation framework implemented on Hadoop cloud. Being developed on top of Hadoop, it inherits
Hadoop X  X  afore-mentioned advantages. 1.1. Related work
Developing tools for multi-agent simulations has always been an active area of research ( Railsback et al., 2006 ), with emphasis being laid on different aspects  X  architecture, scalability, effi-ciency, fault-tolerance and effectiveness of the system. A number of frameworks have been developed such as Netlogo ( Tisue, 2004 ), ZASE ( Yamamoto et al., 2007 ), DMASF ( Rao et al., 2007 ) and MASON ( Luke et al., 2005 ).

Tisue (2004) introduced a novel programming model for imple-menting agent-based simulations, which eased the development of complex agent-models and scenarios. It manages all the agents in a single thread of execution, switching execution between different agents, deterministically and not randomly, after each agent has done some minimal amount of work (simulated parallelism). The simulated parallelism provides deterministic reproducibility of the agent-based simulation every time it is run with same seed for random number generator; which was one of the implementation goals of Netlogo. Further, a visualization module provides 2D/3D visuals of the ongoing simulation. However, Netlogo is not able to distribute the computation on a cluster of computers and hence is not scalable.

MASON ( Luke et al., 2005 ), developed in Java, provides a platform for running massive simulations over a cluster of computers. It has a layered architecture with separate layers for agent-modeling and visualization, which makes decoupling the visualization part easier. It has the capability to support millions of agents (without visualization). Checkpoints of agent data are created on disk for offline visualization.

ZASE ( Yamamoto et al., 2007 ) (developed in Java) is another scalable platform for running billions of agents. It divides the simulation into several smaller agent-runtimes, with each runtime controlling hundreds of thousands of agents and running on a separate machine. It keeps all agents in main memory without the need to access the disk. A thread-pool architecture is followed with several agents sharing a single thread of execution.

DMASF ( Rao et al., 2007 ) (developed in Python) has an architecture similar to MASON and ZASE. Like ZASE, it divides simulation into several smaller runtimes executing on different computers and same thread is shared by several agents. But it uses MySQL database for providing scalability with the help of secondary storage rather than getting bounded by the limited main memory. Similar to MASON, it has a modular architecture separating agent-modeling and visualization. Further, it dynami-cally balances the agent execution load on different machines.
However, the ability to handle hardware failures is lacking in all the three (MASON, ZASE, DMASF). If some of the systems using these frameworks fail during the simulation run, then the simula-tion needs to be restarted from the beginning.

SWARM ( Minar et al., 1996 ), RePast ( Collier, 2001 ) and JAS ( Sonnessa, 2003 ) are some of the other widely used frameworks for studying emergent agent-behaviors through agent-based social simulations. However, they lack the capability to manage more than one system and hence are not scalable. 1.2. Contribution and organization
Our proposed framework developed on Hadoop provides three major advances to the current state of art: (i) Dynamic addition of new computing nodes while the simulation is running; (ii) Hand-ling node failures without affecting the ongoing simulation by redistributing the failed tasks on working systems; (iii) Allowing simulations to run on machines running different operating systems. Further, the framework incorporates several optimization techniques: (i) clustering of frequently communicating agents (for reducing inter-processor communication); (ii) caching of results (for improving performance) that are run on Hadoop cloud.
Section 2 presents the architecture of Hadoop. Section 3 gives the framework architecture built on top of Hadoop. Issues faced in developing the framework and our proposed solutions for overcoming them are presented in Section 4. In Section 5, we present some performance results for few example scenarios. Finally, in Section 7 we present some conclusions. 2. Hadoop architecture and map-reduce model
Hadoop ( http://wiki.apache.org/hadoop ) is an Apache project which develops open-source software for reliable and scalable distributed computing. It maintains a distributed file system, Hadoop Distributed File System (HDFS) ( http://hadoop.apache.org/ hdfs/ ) for data storage and processing. Hadoop uses classic Map-Reduce programming paradigm to process data. This paradigm easily fits a large number of problems (http://code.google.com/ edu/parallel/mapreduce-tutorial.html) . Hadoop consists of a single master system (known as namenode ) along with several slave systems (known as datanodes ). For failure resilience purposes, it has a secondary namenode which replicates the data of namenode at regular intervals. 2.1. Hadoop distributed file system (HDFS)
HDFS ( http://hadoop.apache.org/hdfs/ ) is a block-structured file system: individual files are broken into blocks of a fixed size (default size is 64 MB), which are distributed across a cluster of one or more machines ( datanodes ); thus all the blocks of a single file may not be stored on the same machine. Thus, access to a file may require access to multiple machines, in which case a file could be rendered unavailable by the loss of any one of those machines. HDFS solves this problem by replicating each block across a number of machines (three, by default). The metadata information consists of division of the files into blocks and the distribution of these blocks on different datanodes . This metadata information is stored on namenode . 2.2. Map-reduce paradigm
The MapReduce paradigm transforms a list of (key, value) pairs into a list of values. The transformation is done using two functions: Map and Reduce. Map function takes an input (key1, value1) pair and produces a set of intermediate (key2, value2) pairs. The Map output can have multiple entries with the same key2 . The MapReduce framework sorts the Map output according to intermediate key2 and groups together all intermediate value2  X  X  associated with the same intermediate key2 . The Reduce function accepts an intermediate key2 and the set of corresponding value2  X  X  for that key2 , and produces one or more output value3  X  X . (i) map(key1 , value1) -4 list / ( key2 , value2 ) S (ii) reduce(key2 , list / value2 S )-4 list / value3 S The intermediate values are supplied to the Reduce function via an iterator ( http://download.oracle.com/javase/6/docs/api/java/util/ Iterator.html ). This allows handling lists of values that are too large to fit in memory. The MapReduce framework calls the Reduce function once for each unique key in sorted order. Due to this the final output list generated by the framework is sorted according to the key of Reduce function.

For example, consider the standard problem of counting the number of occurrences of each word in a large collection of documents ( Dean and Ghemawat, 2004 )( Table 1 ). This problem can be solved by using the Map and Reduce functions. The Map function emits each word along with an associated count of occurrences (just  X 1 X  in this simple example). The Reduce function sums together all counts emitted for a particular word. 2.3. MapReduce job execution by hadoop
In Hadoop terminology, by  X  X ob X  we mean execution of a Mapper and Reducer across a data set and by  X  X ask X  we mean an execution of a Mapper or a Reducer on a slice of data. Hadoop distributes the Map invocations across multiple machines by automatically partitioning the input data into a set of M independent splits. These input splits are processed in parallel by MMapper tasks on different machines.
Hadoop invokes a RecordReader method on the input split to read one record per line until the entire input split has been consumed.
Each invocation of the RecordReader leads to another call to the Map function of the Mapper. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function. One more thing to mention here is that some problems require a series of MapReduce steps to accomplish their goals:
Map 1-4 Reduce 1-4 Map 2-4 Reduce 2-4 Map 3 ... and soon
Hadoop supports this by allowing us to chain MapReduce jobs by writing multiple driver methods, one for each job, using ChainMapper classes (see Section 4.1.2).

The namenode is responsible for scheduling various MapReduce tasks on different datanodes . First the Map tasks are scheduled and then the Reduce tasks. It gets periodic updates from the datanodes about the work load on each. It computes the average time taken by MapReduce tasks on each datanode and then does the distribu-tion of tasks in a way that the faster datanodes get more number of tasks to execute and the slower ones get less number of tasks to execute. 3. Multi-agent simulation framework on hadoop
Each agent has three essential properties: a unique identifier ,a time-stamp associated with the state of that agent and type of agent. The identifiers and time-stamps (current time) are gener-ated by the framework itself, whereas agent-type is provided by the user. User can also specify additional properties. State of an agent at a particular timestamp refers to the set of its property-values at that timestamp. Likewise, an update in the state of an agent refers to changes in these property-values. User needs to provide an update-agent-state function for each type of agent, which is a programmatic model for incorporating agent strategy.
Hadoop requires problems to be solved using MapReduce model. This constraint is required in order to make the scheduling of MapReduce tasks (done by Hadoop) independent of the problem being solved. Further, by default Hadoop invokes one
Map task for each input file. Therefore, we model a multi-agent simulation as a series of MapReduce jobs with each job represent-ing one iteration in the simulation and we model each agent as a separate input file to these jobs. This leads a particular job to invoke multiple Map tasks, one for each agent, executing in parallel. And therefore, the function, update-agent-state , which is responsible for updating the state of an agent in each iteration, is written as Map task. Reduce tasks are responsible for writing the data back into the file associated with each agent.

Each agent-state is modeled as a separate flat file on HDFS with file name being the agent identifier. This agent file contains t most recent states of the agent, where t is a user specified parameter (specified through getIterationParameters() in CreateEnviron class described later), each state is distinguished by a timestamp. This agent file is the input for map task initiated to update the state of an agent (one map task for each input file/agent). Current state of an agent is the one having most recent timestamp. A separate message file is associated with each agent that stores recent messages received from other agents. One MapReduce task is invoked for each agent.

Each iteration in the simulation corresponds to one MapReduce job invoked with one MapReduce task corresponding to every single agent. The framework implements two classes Agent and AgentAPI ( Table 2 ). Class Agent contains two classes: Map and Reduce corresponding to the MapReduce task.

Method Map.map : It reads data from associated input file of the agent into a Java map / object, object S , say agent _ data , mapping agent properties to their values. Agent state is updated by execution of user-supplied Update method (in the AgentUserCode ( Table 3 ) class described later). Timestamp is also updated to the current time. All the properties are concatenated as a string, and it is passed as a value in the (key, value) tuple to the Reducer , key being the agent identifier.

Method Reduce.reduce : It writes the agent X  X  properties-conca-tenated string obtained as input from Map.map in the correspond-ing agent X  X  flat file.

Method Main : It creates the simulation world with the help of class CreateEnviron for which the code is supplied by user.
Method CreateEnviron.getIterationParameters : It gets the user specified inputs such as number of iterations the simulation is intended to run and the number of most recent timestamps for which the agent data is retained in the agent specific flat file.
Method CreateEnviron.createWorld : It creates agents and initi-alizes the world. Further, the framework supplies code for the AgentAPI class.

Method AgentAPI.createAgent : It creates a flat file in HDFS with name as agent identifier and writes the initial state of agent.
Method AgentAPI.getAgents : It returns all those agents which satisfy the given set of filters. Several keywords help in this respect, for example, if all the agents are required, then user can pass agent identifier as  X  ALL  X  in filters map.

Method AgentAPI.sendMessage : It sends a message from one agent to another and writes the current timestamp and identifiers of the two agents involved in communication in message files associated with them.

Finally the user needs to implement the strategies of different types of agents. 3.1. Handling failures
System failures are a common case when number of systems involved are large. Information about the namenode and secondary namenode is present on all the datanodes .The namenode sends a heart-beat message to datanodes at regular intervals (by default 600 s; can be configured by user). Each datanode sends an acknowl-edgement message along with the information regarding the status of various tasks running on it. This information includes the number of completed MapReduce tasks (for the current MapReduce job) after the last heart-beat message received and the total time taken to complete them. It also includes number of active MapRe-duce tasks and number of MapReduce tasks in queue, waiting to be scheduled.

When a particular Map (or Reduce ) task fails (and in cases when a slower Map task is becoming a bottleneck for rest of the processes), Hadoop spawns a new process to carry out its job, and may also use idle processes to do its task (the ones which have completed their Map/Reduce task ). When one of the several pro-cesses spawned to complete the failed task finishes, rest of them are aborted ( Speculative execution ). Thus, the simulation enters into next iteration only when all Map tasks in the current iteration are completed. Hence Map tasks is used for updating an agent X  X  state in a particular iteration. 3.1.1. Namenode failure
If the datanodes do not receive heart-beat message from the namenode for more than two time inte rvals (1200 s), then the namenode is considered to have failed. The namenode data has already been replicated on secondary namenode at regular intervals. Therefore, failure of namenode does not cause any loss of data. Datanodes (on detecting namenode failure) at once declare the secondary namenode as the new namenode . All the responsibilities of namenode like job scheduling are now taken up by this new namenode .Further,the datanode which is physically nearest to the new namenode (for faster namenode data replication) is selected as the new secondary namenode . A special case occurs when the secondary namenode turns out to have failed at the instant when namenode is detected as failed. In this case, the simulation is aborted. 3.1.2. Secondary namenode failure
The namenode detects secondary namenode failure if it does not receive any acknowledgement for the heart-beat message. Since, secondary namenode only had replica of namenode data, therefore its failure is handled simply by electing a new secondary name-node from the current datanodes ( datanode physically nearest to the namenode is selected). 3.1.3. Datanode failure
Namenode detects a datanode failure if it does not receive an acknowledgement of the heart-beat message from the datanode . Data of each node is replicated on three other nodes in the distributed system. As such, when a datanode fails its data can be recovered easily using its replica. However, it might be running several MapReduce tasks when it failed. These MapReduce tasks need to be rescheduled on some other datanode. Two cases arise for a failed MapReduce task: (i) failure occurred while running the Map task; (ii) failure occurred while running the Reduce task. When the datanode fails while running its Map task, the entire MapReduce task needs to be rescheduled on some other datanode and the complete taskneedstobedoneagain.Ifa datanode failure occurs while running a Reduce task, then optimally the Map task should not be redone. For achieving this, output of Map task is replicated (as soon as it is finished) on those datanodes which contain the data replicas for the failed datanode . Thus, if a datanode fails when it is running Reduce task, then only the Reduce task is rescheduled and redone on some other node.

Thus, even if some of the machines fail in the Hadoop cluster, the simulation does not stop. 3.2. Dynamic addition of new nodes
Namenode maintains a file containing the details of IP addresses of different machines ( datanodes ). It sends heart-beat messages to the systems mentioned in this file. If a new system needs to be added in the Hadoop cluster, then information about it simply needs to be added in this file. When the namenode finds a new entry in this file, it immediately grants access to HDFS to the new datanode and invokes MapReduce tasks on this system, rebalancing the total work load. Since, heart-beat messages are sent every 600 s, therefore the newly added datanode can be idle at most for this period. Further, the simulation need not be stopped for achieving this addition. 4. Implementation issues for MAS framework on hadoop
Our model faces run-time challenges which needs to be addressed. As the number of agents becomes large (of the order 10 7 agents on 100 machines), overhead increases significantly due to generation of large number of MapReduce tasks. Further, the way in which the agents are distributed on different datanodes may increase the execution time. We present below the challenges faced with above model and solutions for the same. 4.1. Agent communication
Agent simulation requires communication between different agents. In the present model, agent-communication occurs by fetching the state of other agent from their corresponding agent files which reside on different datanodes. This can be a time consuming factor in such simulations. Hence, the number of accesses to files residing on remote datanodes need to be reduced. This is achieved by placing the agents which communicate with each other frequently on the same data node.

We use clustering for redistributing the agents. Further, we need to cluster while the simulation is running. Hence, we need an algorithm that does not incur too much overhead on execution time of the simulation. The following algorithm achieves the above requirements. Further, this greedy algorithm can itself be distributed via Map/Reduce. 4.1.1. Agent clustering algorithm based on agent-communication between these agents, the problem is to redistribute the agents on these sites in such a manner that communication between agents on same site (intra-site communication) is maximized and that between agents on different sites (inter-site communication) is minimized. The complexity of solution needs to be of the order of number of unique communication links among the agents ( Table 4 ).

R m ( a k ) denotes the map value for agent a k on site m .The algorithm begins by placing the agents randomly on given sites.
Their communication links with each other for the first iteration is noted and based on these links they are grouped together. Step 2 brings together the agents who communicated with each other in the same group. To avoid redundancy, if two agents communicated with each other, then it is considered as the agent having the lower agent identifier communicated with the other agent and not the other way round (Step 2(b)); and we refer this agent having lower identifier as representative agent. Step 3 combines the resulting groups formed on different sites in Step 2 and merges them using the criteria given in the algorithm. This step is required, because same agent a i may communicate with several agents on different sites and hence having different values for R j ( a i ). Next, consider a case when agent a i is occurring on two sites m and n .Onsite m , a a representative agent and having R m ( a i )  X  a i .Onsite n ,itisnota representative agent and has R n ( a i )lowerthan a i . In such a case,
R( a )  X  R n ( a i ). All the elements which initially mapped to a have to be re-mapped to R n ( a i ). This justifies Step 4 of the above algorithm.

As an example, let the distribution of agents and communica-tion links between them be as shown in Fig. 1 .

Step 2 (corresponding to the algorithm): R 1 ( a 1 )  X  a
R ( a )  X  a 3 ; R 1 ( a 7 )  X  a 1 ; R 2 ( a 2 )  X  a 1 ; R 2 ( a
R ( a )  X  a 3 ; R 3 ( a 5 )  X  a 4
Step 3: R ( a 1 )  X  a 1 ; R ( a 3 )  X  a 3 ; R ( a 7 )  X  a
R ( a )  X  a 3 ; R ( a 5 )  X  a 4
Step 4: R ( a 1 )  X  a 1 ; R ( a 3 )  X  a 3 ; R ( a 7 )  X  a
R ( a )  X  a 3 ; R ( a 5 )  X  a 3 Step 5:-Group 1: a 1 , a 2 , a 7
Group 2: a 3 , a 4 , a 5 , a 6 So finally the agents communicating with each other are grouped together. In the presented example it is the best solution. However, the algorithm may not always give the best solution, but for the example scenarios tested it gave reasonable results with O(M/S) complexity, where M is the number of unique communication links between different agents and S is the number of sites. 4.1.2. Implementing greedy agent-redistribution algorithm
Since we already have a Hadoop cloud setup, we use this cloud to run the above algorithm and compute the clusters. So, we reframed the algorithm into a Chained MapReduce model and executed it on Hadoop. Execution is carried out in two chained
MapReduce jobs. Output of first MapReduce job becomes the input to the Map phase of second MapReduce job. Finally, clusters of agents are obtained as output from the second MapReduce job.
Step 1 in the above algorithm corresponds to MAP-1 , Step 2 corresponds to REDUCE-1 . Further, Step 3 is executed as MAP-2 and Step 4 as REDUCE-2 .
 In MAP-1 phase :
Input Agent and the list of agents, it communicated with. This is Output ( Key , Value ) pairs ( x i , x 1 )if x 1 o  X  x i . In REDUCE-1 phase : Input The output from MAP-1 .

Output Different values corresponding to the same key are In MAP-2 phase : Input (Key, Value) pairs from REDUCE-1 output and the file Output For each Key , the corresponding Value is mapped to In REDUCE-2 phase : Input (Key, Value) pairs from MAP-2 .
 Output Values corresponding to same key are grouped together in
These clusters/groups contains agents who communicate with each other. 4.1.3. Allocating sites to the agents
Finally the clusters obtained are allocated on different sites such that all agents belonging to the same cluster occur together on same site as far as possible. We follow procedure listed in Table 5 for site allocation, where, size(c) denote the size of cluster c , allocation(site) gives the current number of agents being allocated to this site , and capacity(site) gives the maximum number of agents which this site can hold. 4.2. Small-files-problem
Every file, directory and block in HDFS is represented as an object in the namenode X  X  memory, each of which occupies about 150 bytes. So if we have 10 million agents running then we need about 3 GB (assuming each file has one block) of memory for the namenode. Furthermore, HDFS is not geared up to efficiently access small files: it is primarily designed for streaming access of large files. Reading through small files normally causes lots of seeks and lots of hopping from datanode to datanode to retrieve each small file, all of which is inefficient. Moreover, map tasks usually process a block of input at a time. If the file is very small and there are lots of them, then each map task processes very little input, and there are a lot more map tasks running, each of which imposes extra book-keeping overhead. 4.3. Queries in agent-state updates
An agent, to decide its next state, needs to know about the state of other agents. For knowing state of other agents, we implemented the method getAgents(filters) in class AgentAPI . Execution of this method is significantly time consuming as it needs to access a large number of files and most of them residing on different datanodes. For reducing access time, we cache results of recent queries in the HDFS.

It is common that same type of agents issue similar queries. So, if another agent issues a query whose result has already been computed and cached, then we return the cached result to it. Another improve-ment in this respect would be to find out intersection of the filters specified by different agents, and compute and cache the result of this out result for the superset query filters. For example, for the queries 4 60 , x 4 40 and x  X  90 , the superset would be x 4 40 .Another example would be, for the queries x o 40 &amp;&amp; y o 50 o 60 &amp;&amp; z 4 40 , the superset would be y o 60 .

The cached result files are physically replicated on each datanode with the help of DistributedCache class of Hadoop. This provides a rapid access of the cached results to the datanodes. Name of the cache file indicates the query, along with the iteration number and timestamp during which it was created. Cache files older than a predefined threshold are regularly deleted.
Adding to the solution for the above problem, we built an index of agents based on the frequent query fields (attributes), and updated this index at regular intervals. This allowed a faster lookup for the potential candidate agents for the query. 5. Experimental results
In our experiments, we took multi-agent simulation problems of diverse nature, so as to test the overhead of execution of the two optimization algorithms  X  clustering of agents and caching of intermediate results  X  running on top of Hadoop framework. Further, we studied the speed-up provided by Hadoop. We set up a Hadoop cluster with 10 Unix machines, each having 1.67 GHz processor with 2 GB RAM. Our experiments involved 20,000 agents distributed on these 10 machines and interacting with each other for 100 iterations. Code for the framework is available at ( https://sourceforge.net/projects/hmasf/ ). Some of the impor-tant results obtained are presented here. (i) Circle simulation : In this problem, agents are scattered randomly on a 2-D plane. Their goal is to position themselves in a circle, and then move around it. The strategy involved computation of arithmetic mean of locations of all the agents. A frequent query is executed to compute this mean to get the locat ions of all agents. Accessing the agent files on different systems, everytime (once in each agent update function) an agent requested was avoided by caching locations of all the agents once and then using this cached value for future reference. This cache was updated in every iteration. Average execution time for one iteration reduced from 124 to 58 s (see Fig. 2 (a)). (ii) Standing ovation problem (SOP) : The basic SOP can be stated as a brilliant economics lecture ends and the audience begins to applaud. The applause builds and tentatively, a few audience mem-bers may or may not decide to stand. Does a standing ovation ensue or does the enthusiasm fizzle? The authors present a simulation model for the above problem in Miller and Page (2004) .Wemodeled the auditorium as a 100 200 grid, and agents were randomly assigned a seat. Agents in this simulation communicated with almost a constant set of neighboring agents. Hence the clustering algorithm proposed earlier showed marked reduction in number of inter-site messages and showed major improve ments in execution time. Time for each iteration was almost halved (see Fig. 2 (b), (c)). The slight increase in iteration-time during the third iteration is due to the overhead of clustering algorithm. It took approximately 35 sforthe clustering algorithm to run for 20,000 agents. (iii) Sand-pile simulation : In this problem, grain particles are dropped from a height into a beaker through a funnel, and they finally settle down in the beaker after colliding with each other and with the walls of beaker and funnel. A detailed description and solution of the problem is given by Breton et al. (2001) and for
The queries generated in this problem were generic enough to give good results on caching intermediate results and were similar to circle simulation and hence omitted. However, the set of agents with which a particular agent interacted, changed too frequently. Hence, the number of internode messages varied from iteration-to-iteration. The results show a 100 iteration run with regular agent-clustering done after every 30th iteration (see Fig. 2 (d)). It is observed that agent-clustering reduces the 10 3 10 4 10 5 10 6 in N X  X h iteration 100 120 140 160 in seconds 100 105 110 115 120 125 130 135 140 in seconds in 60 seconds execution time for following iterations but due to frequent changes in communication links of agents, the affect of clustering reduces rapidly. The sharp rise at the 33rd, 63rd and 93rd iterations are due to the overhead of clustering algorithm. (iv) KP simulation : This simulation is done to test messaging efficiency of the framework. K denotes the number of agents in a particular run of the simulation. P is the number of messages sent by each agent. Results obtained for different values of K and P are shown in Fig. 2 (e). Results indicate that internode communication incurs major cost in execution time. As the number of agents increase, resulting in involvement of larger number of datanodes, the cost for messaging increases. This is indicated by the two pairs of values  X  K  X  1000 , P  X  5000  X  , time taken 11 s and  X  K  X  5000 , P  X  1000  X  , time taken 36 s for the same number of total messages flowing in the system (5000 1000). (v) Dynamic nodes addition : We tested the ability of Hadoop to redistribute agents when new datanodes are dynamically added to the Hadoop cluster and when some hardware failures occur. We ran sand-pile simulation with Hadoop cloud consisting of 10 datanodes. At 30th iteration, we failed two datanodes. Finally we added one datanode at 60th iteration. Results show an inverse relation between execution time and the number of active datanodes shown in Fig. 2 (f). The average execution time for one iteration increased from 99 to 121 s on reduction of nodes from 10 to 8 and then decreased again to 106 s when nine datanodes became active. (vi) Scalability tests : For testing the scalability of framework, we conducted two experiments. In the first experiment, we ran the circle simulation with 20,000 agents and varied the number of active datanodes from 1 to 10 and noted the average time taken for one iteration in each case. Results show run-time being inversely proportional to the number of machines ( Fig. 2 (g)). Time taken for one iteration reduced from 489 s (on one machine) to 58 s when executed on 10 systems. In the second experiment, we ran the circle simulation for 60 s and noted the number of agents updated with one machine in the Hadoop cloud. Then, we varied the number of machines from 1 to 10 and in each case noted the number of agents being updated in 60 s. Almost linear increase in the number of updated-agents was observed ( Fig. 2 (h)). For one system, 2681 agents were updated in 60 s for one system which increased to 20,012 when 10 systems were put to use in same amount of time. (viii) Execution time comparison with MASON :WerantheSchel-ling Segregation ( Schelling, 1971 ) simulation on MASON ( Luke et al., 2005 ) on a single machine having 2 GB RAM and 1.67 GHz processor. For 50,000 agents it took 0.25 s to execute one iteration. On further increasing the number of agents, the simulation crashed.
Hadoop provides dynamic load bala ncing, failure-recovery and dynamic addition of new nodes whic h definitely incur processing overheads in terms of heart-beat messages and data replication. It is a generic framework to solve diverse problems and is not specifically intended for multi-agent systems. Hence, the execution time for different simulations obtained with Hadoop are definitely not the best when compared with other simulation frameworks. The execu-tion time for 50,000 agents on the presented framework took as much as 63 s. By grouping 2000 agents in a single file, we were able to speed-up the simulation and run as many as 2 million agents with an iteration-time of 331 s. Therefore, we still need to devise better approaches for efficient multi-agent simulation on Hadoop. 6. Future work
The presented framework lags with respect to the execution time standards existing in the current state of art. Therefore, more efficient agent-models need to be implemented.

Visualization module can be easily implemented. The simula-tion data is stored in HDFS. A separate directory for each iteration is created, each containing the agents-state data for that iteration. This data can be used for offline/online visualization of the simulation. For online analysis, scene visualization module can be triggered at the end of a single map-reduce job (which is equivalent to a single iteration in the multi-agent simulation) and the updated scene can then be rendered using Java APIs. 7. Conclusion
Cloud computing is a recent advancement in the field of solving larger problems. Multi-agent simulations when scaled up to a large number of agents require a framework to run them. Hadoop provides a novel framework for running applications involving thousands of nodes and petabytes of data. It allows a developer to focus on agent model and their optimization without getting involved in fault-tolerance issues. Extensibility of hardware on which framework is running is made easy by Hadoop, by allowing dynamic addition of new nodes and by allowing heterogeneity between operating systems which the different nodes are running. Therefore, it provides a strong backbone for implementing large scale agent-based simulation framework.
 Using cached results is a major optimization in the framework. Developing better heuristics for caching results and to determine appropriate cache sites for faster access of the results are some of the challenging tasks and work is going on in this direction. A faster lookup for agents is achieved by indexing them on fre-quently queried agent attributes.
 References
