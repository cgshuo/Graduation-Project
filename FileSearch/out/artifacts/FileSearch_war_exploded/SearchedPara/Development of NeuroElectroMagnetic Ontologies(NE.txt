 Event-related potentials (ERP) are brain electrophysiolog-ical patterns created by averaging electroencephalographic (EEG) data, time-locking to events of interest (e.g., stimu-lus or response onset). In this paper, we propose a generic framework for mining and developing domain ontologies and apply it to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined accord-ing to the following steps: pattern decomposition, extrac-tion of summary metrics for concept candidates, hierarchical clustering of patterns for classes and class taxonomies, and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128-channel) ERP datasets. Results suggest good correspondence between mined con-cepts and rules, on the one hand, and patterns and rules that were independently formulated by domain experts, on the other. Data mining resul ts also suggest ways in which expert-defined rules might be refined to improve ontology representation and classification results. The next goal of our ERP ontology mining framework is to address some long-standing challenges in co nducting large-scale compar-ison and integration of results across ERP paradigms and laboratories. In a more general context, this work illus-trates the promise of an interdisciplinary research program, which combines data mining, neuroinformatics and ontology engineering to address real-world problems.
 H.2.8 [ Database applications ]: Data mining; J.3 [ Life and Medical Science ]: Neuroscience; I.2.4 [ Knowledge Representation Formalism and Methods ]: Ontology Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Theory, Design Ontology Mining, Clustering-based Classification, Temporal PCA, ERP, Semantic Web
Research in cognitive and clinical neuroscience has given rise to a wealth of data over the past several decades. It is becoming increasingly clear that management and distribu-tion of these data will require advanced tools for data rep-resentation, mining, and integration. In this paper, we pro-pose a generic framework for mining and developing domain ontologies and apply it to mine brainwave (ERP) ontologies. Techniques are applied to several dense-array (128-channel) datasets acquired during studies of visual word comprehen-sion. Development of these ERP ontologies will support future work on semantic mapping discovering, multi-modal data integration, and cross-laboratory data sharing.
Electroencephalography (EEG ) is a widespread, noninva-sive method for imaging brain activity. EEG data are ac-quired by placing sensors on the head to measure electrical signals that are generated in the cortex and conducted to the scalp surface. Compared with other noninvasive imaging techniques, such as Positron Emission Tomography (PET) and functional Magnetic Resonance Imaging (fMRI), EEG methods have two advantages: first, they provide a direct measure of neuronal activity (PET and fMRI measure the hemodynamic response, which is closely linked with neu-ronal activity), and second, they have excellent temporal resolution -on the order of milliseconds, compared with 6 seconds or more for hemodynamic measures. Given that most sensory-motor and cognitive processing takes place within a few hundred milliseconds, fine-grained representa-tion of the time course of brain activity is extremely impor-tant. In addition, with the advent of dense-array methodolo-gies, modern EEG methods are now characterized by high spatial (scalp topographic), as well as high temporal, di-mensionality. With the application of tools for anatomical source localization, dense-array EEG can be used for non-invasive brain functional mapping, supporting a wide range of clinical and basic re search applications.

Event-related potentials (ERPs) are derived by averaging across segments of EEG data, time-locking to events of inter-est (e.g., onset of a visual or audi tory stimulus). Signals that are not event-related tend towards zero as the number of av-eraged trials increase. In this way, averaging increases the signal-to-noise ratio (SNR) and provides measures of electri-cal activity that are specifically linked to stimulus processing (e.g., Figure 1(A)).

At each time point, many parts of the brain may be si-multaneously active, contributing overlapping (or  X  X uper-posed X ) patterns to the measured signal. ERP research aims to separate and classify these patterns (or  X  X omponents X ) and to relate them to specific brain and cognitive functions. Distinct patterns are characteri zed by their time course (e.g., early or late), polarity (positive or negative), and scalp dis-tribution, or topography. For example, as illustrated in Fig-ure 1, the  X  X 100 component, X  which was extracted from the superposed data (A) using Principal Components Anal-ysis [14] has a peak latency of approximately 100ms (B) and is positive over occipital areas of the scalp (C). Figure 1: (A)128-channel EEG waveplot; positive volt-
Although there is general agreement on how to character-ize ERP patterns or  X  X omponents, X  in reality, such patterns can be difficult to identify, and definitions vary across re-search labs. Furthermore, methods for ERP data summary and analysis differ widely acro ss research sites. This vari-ability can make it hard to compare results across experi-ments and across laboratories, limiting the generalizability of research results and, therefore, the ability to generate high-level integration and interpretation of patterns.
To address these issues, we have proposed a new frame-work, called  X  X eural ElectroMagnetic Ontologies, X  or NEMO. The NEMO project proposes to develop ontologies to sup-port ERP data representation and integration.

In general, an ontology can be defined as the formal spec-ification of a vocabulary of concepts and the relationships among them in a specific domain. In traditional knowl-edge engineering and in emerging Semantic Web research, ontologies play an important role in defining the semantics of data. The adoption of domain ontologies in biomedical research has enabled several major scientific advances [22], which are exemplified in projects such as the Gene ontol-ogy [19], UMLS [24] and the National Center of Biomedi-cal Ontology [8]. Most biomedical ontologies are developed through a top-down or knowledge driven approach, i.e., do-main experts define the concepts and relationships based on their domain knowledge with the help of ontology engineers. Currently there are no formal ERP ontologies, and in fact there is little neuroinformatics research in this important area, although there are a variety of statistical techniques that are emerging for analysis of spatiotemporal patterns in EEG and ERP research [17]. The reason for this gap may be linked to the absence of robust methods for identi-fication of ERP patterns (concepts). Perhaps the greatest challenge, at this stage of the ontology development, is to develop and test a framework for separating and classify-ing complex spatiotemporal patterns that are superposed in measured EEG. In this paper, we describe some general concepts (i.e., patterns) and rul es (i.e., the high-level pat-tern representations) that have emerged from our prior work on neuroscience, and detail the methodology used to mine and develop initial ERP ontologies which formally represent and store these concepts, their taxonomies and high-level representations (i.e., rules).

There are several popular ontology languages which are basedondifferentlogics,suchastheWebOntologyLan-guage (OWL [6]) based on description logic, KIF [20] based on first order logic and OKBC [5] (i.e., the protocol used by Protege-frames) based on frame-logic. In general, the vocabulary and relationships in an ontology can be roughly divided into several categories:
The objective of ontology mining is to mine domain spe-cific (i.e., real world) data to acquire a vocabulary of con-cepts, to establish a concept taxonomy, and to discover the relationships among the concepts. In this paper, we will mainly use OWL to describe the classes, properties and ax-ioms to be mined, since OWL has become the W3C stan-dard web ontology language. We note that most concepts describedinOWLcanalsoberepresentedinKIForOKBC.

The reminder of the paper is organized as follows. In Sec-tion 2, we will first give a brief overview of related work on ontology mining and ERP data analysis. In Section 3, we describe our generic framework for ontology mining which includes a sophisticated combination of (hierarchical) clus-tering, classification and association rule mining. Specif-ically, we apply our framework to mine an ERP domain ontology. In Section 4, we present results for ERP ontol-ogy mining based on data collected from three EEG experi-ments on visual word comprehension. The input to our on-tology mining framework are extracted from spatiotemporal ERP data using temporal Principal Components Analysis (PCA). The mined ERP ontology is conceptually transpar-ent for domain experts, and in particular, the mined rules (axioms) correspond closely wi th the labeling classification rules that were independently defined by domain experts. In Section 5, we discuss more robust data preprocessing, ontol-ogy mining, and future data integration directions for the NEMO project. Finally, in Section 6, we draw some gen-eral conclusions about our contribution regarding state-of-the-art techniques for ERP ontology mining, representation, and integration.
ERP data consist of time series, representing temporal fluctuations in the EEG that are time-locked to events of interest (e.g., word or picture stimuli). In dense-array EEG and ERP research, these time series are measured across multiple locations on the scalp surface. A variety of tools are available for ERP preprocessing and pattern analysis. For example, Net Station [4] is a suite of tools, which in-cludes data cleaning, statistical extraction and visualization techniques. EEGLAB [2] is a Matlab toolbox that provides advanced statistical methods for EEG/MEG and ERP pro-cessing, including independe nt component analysis (ICA) and joint time-frequency analysis (TFA). APECS is a Mat-lab toolbox that contains tools for data cleaning (ICA and related techniques) and evaluation of data decomposition results [17]. The Dien PCA Toolbox [1] includes Princi-pal Component Analysis (PCA) tools that are optimized for ERP data decomposition.

Ontology mining is a process for learning an ontology, in-cluding classes, class taxonomy, properties and axioms. In the existing work, researchers mainly focus on mining the ontologies from text documents (e.g., web content) [25] or other web data (web usage, web structure and web user pro-files) [23]. In [28], clustering is used to discover the concepts in the ontology. Association rule mining has been adopted to discover the relationships be tween different concepts [26]. The NetAffx Gene ontology mining tool [11] is an interactive platform for visualizing and analyzing microarray data.
In this paper, we propose a generic framework for develop-ing and mining domain ontologies, with specific application to the development of a first-generation ERP ontology. The target data type consists of spatiotemporal data (ERPs), and summary statistics (e.g., the  X  X atent X  or principal com-ponents that emerge from statistical analysis of ERP data). In addition to identifying classes, a hierarchy of classes and part-of relations of classes, our approach includes classifica-tion methods for mining properties and axioms (rules). This is also an important extension from our previous work [29], which focuses only on ERP pattern mining. In this paper, we first use the previous ERP pattern mining results (data from Experiment 1-2) to develop ERP classes. Furthermore, we adopt hierarchical clustering methods to generate class taxonomies and association rules to discover the property relations respectively from a new dataset (Experiment 3).
Based on existing ontology mining approaches and our previous work for mining ERP patterns [29], we summarize and propose the following four general procedures for mining the concepts and their relatio nships in domain ontologies: 1. Classes  X  Clustering-based Classification :Ifthereex-2. Class Taxonomy  X  Hierarchical Clustering :Moregran-3. Properties  X  Classification : The classification process 4. Axioms  X  Association Mining and Classification :The
All of the above four procedures and their interactions are shown in Figure 2 and the outputs (i.e., classes, class hierarchy, properties, axioms) are put together into a do-main ontology. It is a semi-automatic framework because we need  X  X xpert labeling X  to give meaningful names for classes. The input data are put into some semi-structured formats, such as the spreadsheet, after data preprocessing. Other-wise, some statistical or text processing step needs to be done as a part of data preprocessing.

To further explain why our ontology mining framework based on the four general procedures makes sense, we first suppose there exists a domain ontology (i.e., semantics of data) for a set of data instances in some specific domain (e.g., ERP). Our goal is to find what classes, properties Figure 2: A semi-automatic framework for mining do-and axioms can be mined to compose that domain ontology. From a machine learning point of view, the domain ontology is the target function to be learned and it includes several components such as classes, class hierarchy, properties and axioms. A reasonable assumpti on is that the data instances which belong to the same class must be similar by sharing some properties, the data instances which belong to differ-ent classes must be dissimilar. Therefore, determining what and how many classes should be included in an ontology is typically a clustering problem. It is a natural extension that finding the hierarchy of classes (clusters) is a hierarchial clustering problem. On the other hand, what properties and values the data instances in the same class should share is a typical classification problem. The selection of attributes for classification (e.g., information gain selection) can be used for property selection in ontology mining. The classification rules can also be treated as the relationships (axioms) of properties and classes. The association rules between dif-ferent properties can be treated as relationships (axioms) of two or multiple properties themselves, which will be a good complementary for the ontology.

In summary, our generic framework includes data prepro-cessing, clustering, hierarchical clustering, clustering-based classification and association mining. The data input into four procedures are in some semi-structured format (e.g., spreadsheet) after data preprocessing. The outputs (i.e., classes, class hierarchy, properties, axioms) will be used to compose a domain ontology. We will elaborate the details of our framework with the description of experiments of mining ERP ontologies in the following Section 4.
In this paper, we analyzed data collected in three studies of neural activity during visual word comprehension (Ex-periment 1 -3). Data were acquired using a 128-channel EEG sensor net [3]. Sampling rate was 250hz. The EEG were segmented into 1,500ms epochs, beginning 500ms be-fore stimulus onset (total number of samples = 375).
The data from Experiment 1 and Experiment 2 comprise 89 subjects and 6 experimental conditions (number of obser-vations = 534). A description of the experiment paradigm, behavioral measures, scalp ERPs, and cortical (source) wave-forms can be found in [18]. For cross-validation of our pat-tern classification and labeling procedures, subjects were randomly assigned to one of two groups, resulting in 20-24 subjects per subgroup. Subgroups were matched in propor-tion of males to females and in mean age and handedness. Data from a new experiment ( Experiment 3) dataset con-sist of 36 subjects and 4 experiment conditions (number of observations = 144).
ERP data represent a mixture of  X  X ignal X  (functional brain patterns) and  X  X oise X  (extracerebral artifacts and brain ac-tivity that is not related to the events of interest). Data decomposition methods can help separate signal from noise and disentangle overlapping patterns. A variety of statisti-cal decomposition methods have been applied to ERP data in the past few decades, such as Independence Component Analysis (ICA), wavelets and P rincipal Component Analysis (PCA). In this paper, Principal Component Analysis [13] is used to decompose the ERP data. PCA belongs to a family of dimension reduction procedures. It projects the data into a new space of lower dimension.

In the present study, we used temporal PCA, as imple-mented in the Dien PCA Toolbox [1]. The dataset used as input to the PCA is organized with the variables correspond-ing to time points. The number of variables is equal to the number of samples (number = 375). The waveforms vary across subjects, channels (number = 128) and experimental conditions. PCA extracts as many factors as there are vari-ables. After rotation of extracted factors, a small subset of the factors are retained for further analysis. In this exper-iment, we retained the first 15 PCA factors, accounting for most of the variance ( &gt; 75%). The remaining factors are assumed to contain  X  X oise. X  Th is assumption is verified by visual inspection of the time course and topographic projec-tion of each factor.
For each PCA factor, we extracted summary metrics rep-resenting spatial, temporal and functional dimensions of the ERP patterns of interest. After preprocessing, the Exper-iment 1 and Experiment 2 datasets consist of vectors con-taining 25 spatial, temporal and functional attributes de-rived from the automated measure generation. Thus, the data represent the individual PCA factors of each subject and condition as points in a 25 dimensional attribute space. For the Experiment 3 dataset, we increase the number of attributes to 31 by adding more intensity attributes, such as  X  X seudo-Known X  (Difference in mean intensity over re-gion of interest at time of peak latency (Nonwords-Words)). Attribute Description Table 1: Intensity, spatial, temporal and functional met-Table 1 lists some common attributes that are used for all the datasets. The datasets were put into spreadsheets and each column corresponds to an attribute. Domain experts labeled 3 kinds of pattern factors for Experiment 1 group 1 data, 4 for Experiment 1 group 2 data and 8 for Experiment 3 data. For example, four spatiotemporal patterns relat-ing to visual object processing are: the P100 (an occipital positivity, peaking at 100ms), N100 (an occipital negativity, peaking at 180ms), N2 (a left temporal pattern, peaking at 250ms), and P300 (a parietal positivity from 300 to 700ms).
Traditionally, ERP patterns are identified through visual inspection of grand-averaged ERP data. However, the pre-cise definition of a target pattern, its operationalization, and measurement across individual subjects, can vary consid-erably across research groups. In our framework, we use clustering to automatically separate ERP patterns, as they are distributed across  X  X atent X  (PCA) factors. The fac-tors extracted through PCA are weighted across individual subjects and experiment conditions. Summary metrics ex-tracted from each observation (subject and condition) are then input into clustering tool. Observations that belong to the same pattern are expected to map to the same cluster using this method. The larger aim is to develop an au-tomatic pattern classification method, which can support robust ERP pattern definitions.
The Expectation-Maximization (EM) algorithm [12] is of-ten used to approximate distributions using mixture mod-els. It is an iterative procedure that circles around the ex-pectation and maximization steps (i.e., E-step and M-step). EM clustering can assign each object to a cluster accord-ing to a weight representing the probability of membership. The goal is to  X  X aximize X  the likelihood of the distributions given the data. We also tried other classical clustering al-gorithms, such as K-Means and K-Medoids. It seems EM works better than others, especially in the scenario that the number of clusters (e.g., ERP patterns) is indefinite. Table 2: EM Clustering Results for Experiment 1 group Table 3: EM Clustering Results for Experiment 1 group
In the E-step for clustering, t he algorithm calculates the posterior probability that a data instance (e.g., a data tu-ple with 25 attributes in our ERP experiment) belongs to a cluster. In the M-step, EM algorithm searches for optimal parameters that maximize the sum of log-likelihood prob-abilities. EM algorithm automatically selects the number of clusters by maximizing the logarithm of the likelihood of future data. The detailed implementation of EM clustering can be found at [21]. And we use EM clustering algorithm in WEKA [9] in the experiments.
For each of the experimental datasets, we applied EM clustering to the summary metrics described previously in Table 1. In the current study, the data in each cluster were compared with the human labeling result (which are gen-erated with the rules defined by domain experts) to de-termine the distribution of the pre-defined ERP patterns amongst the clusters. The number of clusters was set equal to the number of patterns that were identified by domain experts. Observations were then assigned to clusters using this semi-automatic approach. Table 2, 3 and 4 show the clustering results for Experiment 1 group 1, Experiment 1 group 2 and Experiment 3 data. The resulting assignment of observations to clusters corresponded closely with the pat-tern labeling results based on expert judgments. Compared with our generic framework shown in Figure 2, domain ex-perts actually did  X  X xpert labeling X  for all data instances before the clustering step. However, we did not input the lateN1/N2 0 0 0 0 49 4 79 0 Table 4: EM Clustering Results for Experiment 3 Pat-labels into the EM clustering. Instead, we only use them to compare with clustering resul t and replace arbitrary clus-ter (class) names by corresponding patter names. In more general cases, we believe that  X  X xpert labeling X  can only happen with the help of discovered classification rules. The data instances with labels do not always exist before the clustering step. On the othe r hand, there was not a strict one-to-one mapping between clusters and labeled patterns. Rather, the results showed some pattern  X  X plitting, X  where observations belonging to a target pattern were assigned to more than one cluster. The proper diagnosis and interpre-tation of such results will require careful system evaluation to determine the source of this  X  X isallocation of variance. X 
Based on the clustering result, we can generate the fol-lowing OWL classes:
The Experiment 1 and Experiment 2 only include 3 or 4 patterns (classes) and it is hard to show how to mine another important component, class taxonomy, for an ERP ontology. To show that, we analyzed the Experiment 3 data. Accord-ing to prior ERP research, we would expect to find about 8 patterns between 100 to 700ms after presentation of a vi-sual word stimulus. These patterns include the P100, N100, lateN1/N2, N3, MFN, N400, P1r, and P300. We expect the hierarchical clustering to discover these patterns automati-cally, and also to generate a taxonomy of these patterns.
We have applied EM clustering in a hierarchical way to discover the class taxonomy, using both divisive and agglom-erative strategies. In the divisive approach, we first put all the data from 8 ERP patterns (classes) into one cluster. Our goals is to sub-divide this cluster into 2 clusters by setting up the number of clusters. Then we repeatedly sub-divide each cluster until the majority of data instances from each pattern forms a cluster.

In each step, the data instances of a particular pattern (class) labeled by domain experts may go to different clus-ters. We always keep the majority of data instances for each pattern in one cluster but take out those in other clusters. In agglomerative approach, we first put the data instances into 8 clusters to reflect expert hypotheses regarding the number of distinct patterns. Then we try to merge them into 7 clusters by setting up the number of clusters. Again, we only keep the majority data instances for each pattern. We continue to merge them into fewer clusters until all data instances can be put into one cluster, if possible.
For the eight patterns our neuroscientists want to discover from the data, both divisive and agglomerative clustering approaches result in the same hierarchy shown in Figure 3. Figure 3: The hierarchy graph of 8 ERP patterns (classes), where  X  X 3 X  means P300,  X  X 2 X  means lateN1/N2,  X  X 3 X  means N300,  X  X 4 X  means N400.
 The hierarchy in Figure 3 shows that the MFN and N4 pat-terns belong to the same cluster (class). Likewise, the N3 and P1r patterns belong to the same cluster. These results suggest one of two possibilities. First, it is possible that patterns previously assigned distinct labels in the ERP lit-erature reflect one and the same underlying process. Second, it is possible that these patterns are in fact distinct, but our analyses failed to separate them. In this second case, it will be important to refine human labeling steps to capture fine-grained distinctions between spatiotemporal patterns. Finally, it will be critical to include data collected across a range of experiment paradigms, to provide a broader range of functional data that can be used for pattern analysis.
The discovered hierarchy (class taxonomy) can be repre-sented in OWL and added into the ERP ontology like:
The EM clustering process can partition pattern factors into several clusters (i.e., OWL classes) in a hierarchical way, such that each cluster is mainly comprised of one or several categories of pattern factors. Our next goal is to discover the axioms (rules), which specify the properties and their relationships with defined classes. After EM clustering and hierarchical clustering, we use C4.5 classification algorithm [27] to build a decision tree to classify factors in each cluster. C4.5 is a standard decision learning algorithm which works well for continuous values. Some ERP attributes have continuous values. On the other hand, the classification rules derived from the decision tree are meaningful to human experts. The discovered rules will be used for defining the axioms in the ERP ontology which specify the properties and their relationships with defined classes.

Considering the number of clusters needs to be referred to the labeling efforts of domain experts, the current process is semi-automatic for mining ERP ontologies. Once the data mining process becomes more robust, we will not need the data labeling effort from domain experts. But we will need domain experts to give meaningful names for classes based on the classification rules.

We use J48 in WEKA, which is an implementation of the C4.5 algorithm, to classify the data. The input of the decision tree classifier is the pattern factor metrics vector and their arbitrary cluster names are used as classification labels. We built decision trees based on the clusters discov-ered from Experiment 1 and Experiment 2 data, and also the cluster hierarchy from Experiment 3 data. We use infor-mation gain [21] as an attribute selection measure in build-ing the decision tree. It always chooses the attribute that is most capable of differentiating different classes of data at each level of the tree. Those selected attributes will be considered as properties in ERP ontology.
Figure 4 shows the decision tree learner trained on Exper-iment 1 group 1 data. It achieves a precision of 97 . 44% on the training data.
 From Figure 4, we can see that although 25 attributes are input to the learning process, only 6 of them are used in the final decision tree classifier. For instance, Table 5 is a ... ... ...
 table of information gain of attributes. In the rules provided by domain experts, only TI-max and IN-mean (ROI) are used. However, information gains of the attributes show that IN-mean (ROCC) is also important in the classification of patterns.

Therefore, we consider the top attributes in Table 1 as the candidate properties. The information gain selection help us (including domain experts) to determine which properties should be in the ERP ontology. For example, in OWL, a subset of properties can be represented as Table 6: Expert-defined rules vs. Decision tree gener-
One advantage of using decision tree is that we can gen-erate rules from decision tree and compare them with the ones that are defined by domain experts. It can help domain experts to determine (i.e., X  X xpert labeling X ) the names of clusters if their names were arbitrarily assigned. Table 6 compares the rules for N100 and lateN1/N2 patterns, where cluster0 and cluster1 correspond to N100 and lateN1/N2 re-spectively. From it, we can see that decision tree uses more attributes. The values for EVENT and MODALITY are the same for all the data in Experiment 1 group 1. Therefore EVENT and MODALITY are not selected by decision tree classifier. On the other hand, the attribute values of deci-sion tree rules can not be exactly the same as domain expert rules, although they are basica lly consistent. We believe the attributes that are used in the decision tree and their values can be a good reference for domain experts to refine their rules.
The classification rules derived from the decision tree will be used to determine what axioms in the ontology can de-scribe the relationships between properties and classes. How-ever, what will be a standard logic language for rules is still an open question in the Semantic Web research. SWRL [7] can be a choice and it is a subset of first order logic. For example, the decision tree rule related to cluster1 (corre-sponding to lateN1/N2) in Table 6 can be represented in SWRL like:
Table 7: Association rules from Experiment 3 dataset
In this paper, to save the space, we may just use general first order logic axioms to represent the above SWRL rule. It looks like:
Clustering and classification methods induce the axioms to distinguish different pattern s (classes) using properties. Relationships between properties themselves is also of inter-ests of domain experts and can be put as axioms into the ERP ontology. In this paper, we use association rule mining to discover the relationships between different properties.
Association rule mining aims at finding frequent patterns in certain data sets. In our case, association rule mining is used to seek the properties that frequently co-occur for the specific ERP pattern factors. After decision tree classifica-tion, we quantify the values of each attribute by using their splitting point value in the tree. This converts the numeric values of each attribute to categorical values. Then, we ap-plied the well-used Apriori algorithm [10] in Weka [9] to find association rules of these attributes. Table 7 lists a subset of the association rules we generated. We only selected those association rules with high confidence (i.e., &gt; 90%) and put them into the ERP ontology.
 Association rules should also be represented as logic (e.g., SWRL) axioms in the ERP ontology. For example, the first and fourth association rules in Table 7 can be represented in general first order axioms:
As the number of ERP patterns and attributes increases, the decision tree generated by classification step expands. The path from root node to the leaf node in a tree, which corresponds to one rule for a cluster, becomes longer. How-ever, we can use association rules to trim the classification rules. An important inference rule [30] in logic is:
It can be applied when the parameters (e.g., A and B ) in the condition of a classification rule (e.g., B  X  A  X  C are closely related to an association rule (e.g., A  X  B or B  X  A ). For instance, there is one rule for cluster4, which corresponds to N3 pattern, from the Experiment 3 dataset: We can use the fourth association rule in Table 7 to op-timize the above classification rule to: We have built an ontology inference engine, OntoEngine, which can be extended to implement this kind of optimiza-tion (transformation) for axio ms [16]. The inference process will continue until no rule can be trimmed anymore.
In this paper, we have outlined a new framework for min-ing ERP ontologies based on clustering, classification and association rule mining. Our first-generation ERP ontology consists of 16 classes, 57 properties and 23 axioms. We show a partial view of this preliminary ERP ontology in Figure 5. Figure 5: A partial view of a mined ERP ontology.
Figure 5 shows 10 classes, i.e., factor and pattern taxon-omy. Patterns have temporal, spatial and functional at-tributes (some of which are listed in the graph, such as Event, Modality etc.) which are represented as properties of the  X  X attern X  class in the ERP ontology. TI-max, IN-mean(ROI) etc are properties of  X  X actor X  which have num-ber values.  X  X actor X  relates to  X  X attern X  by a  X  X abeled as X  property.

As described here, this approach can be highly informative when applied to PCA-based metrics generated from high-density ERP data. Part of the ongoing work is focused on further refinements to our clustering process. For exam-ple, in the present set of experiments, some patterns  X  X plit X  across (were assigned to) more than one cluster. Inspec-tion of temporal PCA results suggested that refinements to the data decomposition process, as well as additional met-rics that capture temporal and spatial attributes more accu-rately, may reduce this  X  X isallocation X  of pattern variance. To achieve accuracy in system evaluation, we will compare system results with a  X  X old standard, X  which will be estab-lished by expert labeling of early visual-evoked ERP pat-terns (e.g., P100v, N100v, and N2v).

To show our ontology mining framework is generic and robust, we will apply a variety of data preprocessing tech-niques besides PCA decomposition. It will also be inter-esting to try our ontology mining framework in a range of experimental paradigms, including auditory as well as visual stimulus processing, and nonlinguistic as well as language-related paradigms.

Another important aim of the NEMO project is to store high-level pattern descriptions in an ERP ontology database , which is automatically modeled based on the semantics of an ERP ontology. The next phase of the NEMO project will be focused on development of an ontology-based inte-gration system which will facilitate the representation and dissemination of ERP data across studies and labs. Differ-ent labs will create their own ERP ontologies and ontology databases. Ontology-based integration in NEMO will study semantic mapping rules between different ERP ontologies. Given the mapping rules, once the user query comes in, var-ious ERP ontology databases can be searched for answers to the query. We reported an efficient ontology-based data inte-gration system (OntoGrate) in [15], which will be extended to support NEMO. We will implement the data exchange and query answering components through the inference en-gine by reasoning with ERP ontologies and mapping rules.
In general, we expect that this ontology-based methodol-ogy can be extended for integrating other types of neuro-science data (e.g., fMRI data) and support other biomedical data sharing efforts (e.g., the Gene Ontology).
In this paper, we introduce a generic framework for mining domain ontologies and present some results of our work on development of a first-generation ERP ontology. This work aims at exploring methods for differentiating different ERP pattern factors and selecting important concepts and rules for ontology definition.
 The framework works well for ERP ontology definition. The raw ERP data can be preprocessed to ERP factors by temporal Principal Compone nt Analysis. EM-based clus-tering and hierarchical clustering can cluster ERP factors to different groups (i.e., classes and class taxonomy in the ontology). Then we use classification method (C4.5 decision tree learning) to get ERP pattern rules automatically after labeling the factors with the result from clustering. The classification rules are consistent with domain experts X  rules and can be used as references for further human refinement. The classification rules are used to define the axioms be-tween properties and classes in the ERP ontology. Associa-tion rule mining are used to define axioms among properties and to help rule optimization.

The future work of the NEMO project is to make our sys-tem more robust in different experimental paradigms. The design of ontology databases and the discovery of their map-pings will support data integration across different ERP lab-oratories. We expect that our NEMO framework will be ex-tended to other types of neuroscience data and to support other biomedical ontology-based data sharing efforts. We thank Paea LePendu for his input to the final version of this paper. We also thank the anonymous reviewers for their helpful comments. [1] Dien, J. PCA Toolbox (version 1.7). [2] EEG lab. [3] Electrical Geodesics, Inc. http://www.egi.com . [4] NetStation Technical Manual. [5] Open Knowledge Base Connectivity (OKBC). [6] OWL Web Ontology Language. [7] SWRL: A Semantic Web Rule Language Combining [8] The National Center for Biomedical Ontology. [9] Weka 3: Data Mining Software in Java. [10] R. Agrawal and R. Srikant. Fast Algorithms for [11] J. Cheng, S. Sun, A. Tracy, E. Hubbell, J. Morris, [12] A. Dempster, N. Laird, and D. Rubin. Maximum [13] J. Dien. Addressing misallocation of variance in [14] J. Dien and G. Frishkoff. Introduction to principal [15] D. Dou, P. LePendu, S. Kim, and P. Qi. Integrating [16] D. Dou and D. McDermott. Deriving Axioms Across [17] R. M. Frank and G. A. Frishkoff. Automated protocol [18] G. A. Frishkoff. Hemispheric differences in strong [19] T. Gene Ontology Consortium. Creating the Gene [20] M. R. Genesereth. Knowledge Interchange Format [21] J. Han and M. Kamber. Data Mining: Concepts and [22] D.B.Keator,S.Gadde,J.S.Grethe,D.V.Taylor, [23] Y. Li and N. Zhong. Mining ontology for [24] D. Lindberg, B. Humphries, and A. McCray. The [25] A. Maedche and S. Staab. Mining ontologies from [26] A. Maedche and S. Staab. Ontology learning for the [27] J. Quinlan. C4.5: Programs for Machine Learning . [28] M.-L. Reinberger, P. Spyns, W. Daelemans, and [29] J. Rong, D. Dou, G. Frishkoff, R. Frank, A. Malony, [30] S. Russell and P. Norvig. Artificial Intelligence: A
