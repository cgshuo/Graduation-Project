 Discretization is probably one of the most broadly used pre-processing techniques in machine learning and data mining [6,13] with various applications, such as solar images [2] and mobile market [14]. By using discretization algorithms on continuous variables, it replaces the real distribution of the data with a mixture of uniform distributions. Generally, discretization is a process that transforms the values of continuous attributes into a finite number of intervals, where each interval is associated with a discrete val ue. Alternatively, this process can be also viewed as a method to reduce data size fro m huge spectrum of numeric variables to a much smaller subset of discrete values.

The necessity of applying discretization on the input data can be due to dif-ferent reasons. The most critical one is that many machine learning and data mining algorithms are known to produce better models by discretizing continu-ous attributes, or only applicable to discrete data. For instance, rule extraction techniques with numeric attributes often lead to build rather poor sets of rules [1]; it is not always realistic to presume normal distribution for the continuous values to enable the Naive Bayes classifier to estimate the frequency probabilities [13]; decision tree algorithms cannot handle numeric features in tolerable time directly, and only carry out a selection of nominal attributes [9]; and attribute reduction algorithms in rough set theory can only apply to the categorical val-ues [10]. However, real-world data sets predominantly consist of continuous or quantitative attributes. One solution to this problem is to partition numeric do-mains into a number of intervals with corresponding breakpoints. As we know, the number of different ways to discretize a continuous feature is huge [6], in-cluding binning-based, chi-based, fuzzy-based [2], and entropy-based methods [13], etc. But in general, the goal of discretization is to find a set of breakpoints to partition the continuous range into a small number of intervals with high distribution stability and consistency, and then to obtain a high classification accuracy. Thus, different discretization algorithms are evaluated in terms of four measures: simplicity [5], stability [3], consistency [6], and accuracy [5,6].
Of all the discretization methods, the entropy-based algorithms are the most popular due to both their high efficiency and effectiveness [1,6], including ID3 , D2 ,and MDLP , etc. However, this group of algor ithms only concern the decrease of uncertainty level by means of information attribute dependency in a decision table [5], which is not rather convincing. From an alternative perspective, we propose to improve the discretization quality by increasing the certainty degree of a decision table in terms of deterministic attribute relationship, which is re-vealed by the positive domain ratio in rough set theory [10]. Furthermore, based on the rationales presented in [8,12], w e take into account both the decrement of uncertainty level and increment of certainty degree to induce a Coupled Dis-cretization ( CD ) algorithm. This algorithm selects the best breakpoint according to the importance function composed of the information entropy and positive domain ratio in each run. The key contributions are as follows: -Consider the information and deterministic feature dependencies to induce -Evaluate our proposed algorithm with existing classical discretization meth--Develop a way to define the importance of breakpoints flexibly with our -Summarize a measurement system, including simplicity , stability , consis-The paper is organized as follows. Section 2 briefly reviews the related work. In Section 3, we describe the problem of d iscretization within a decision table. Discretization algorithm based on information entropy is specified in Section 4. In Section 5, we propose the discretization algorithm based on positive domain. Coupled discretization algorithm is presented in Section 6. We conduct extensive experiments in Section 7. Finally, we end this paper in Section 8. In earlier days, simple methods such as Equal Width ( EW ) and Equal Frequency ( EF ) [6] are used to discretize continuous values. Afterwards, the technology for discretization develops rapidly due to the great need for effective and efficient machine learning and data mining methods. From different perspectives, dis-cretization methods can be classified into distinct categories. A global method uses the entire instance space to discretize, including Chi2 and ChiM [6], etc.; while a local one partitions the localized region of the instance space [5], for instance, 1R . Supervised discretization considers label information such as 1R and MDLP [1]; however, unsupervised method does not, e.g., EW , EF . Splitting method such as MDLP proceeds by keeping on adding breakpoints, whereas the merging approach by removing breakpoints obtains bigger intervals, e.g., Chi2 and ChiM . The discretization method can also be viewed as dynamic or static by considering whether a classifier is incorporated during discretization, for example, C4.5 [6] is a dynamic way to discretize continuous values when building the classifier. The last dichotomy is direct vs. incremental, while di-rect method needs the pre-defined number of intervals, including EW and EF ; incremental approach requires an addition al criterion to stop t he discretization process, such as MDLP and ChiM [3]. In fact, our proposed method CD is a global-supervised-splitting-incremental algorithm, and comparisons with the aforementioned classical methods are conducted in Section 7. In this section, we formalize the discretization problem within a decision table, in which a large number of data objects with t he same feature set can be organized.
A Decision Table is an information and knowledge system which consists of four tuples ( U, C C = { c 1 ,  X  X  X  ,c n } and D are condition attribute set and decision attribute set, respectively. V C is a set of condition feature values, V D is a set of decision attribute values, and the whole value set is V = V C is an information function which assigns every attribute value to each object. D =  X  if there is at least one decision feature d  X  D .Theentry x ij is the value of continuous feature c j (1  X  j  X  n ) for object u i (1  X  i  X  m ). If all the condition attributes are continuous, then we call it a Continuous Decision Table .
Let S =( U, C V  X  ,f  X  )isthe Discretized Decision Table when adding breakpoint set P ,where C  X  is the discretized co ndition attribute, V  X  is the attribute value set composed of discretized values V  X  C and decision value V D ,and f  X  : U  X  ( C  X  the discretized information function. For simplicity, we consider only one decision attribute d  X  D . Below, a consistent discrete decision table is defined: Definition 1. A discrete decision table S ( P )=( U, C  X  tent if and only if any two objects have identical decision attribute value when they have the same condition attribute values.
 In fact, the discretization of a continuous decision table S is the search of a proper breakpoint set P , which makes discretized decision table S ( P ) consistent. In this process, different algorithms result in distinct breakpoint sets, thus correspond to various discretization results. Chmielewski and Grzymala-Busse [5] suggest three guidelines to ensure successful discretiza tion, that is complete process, simplest result and high consistency. Thus, among all the breakpoints, we strive to obtain the smallest set of breakpoints which make the least loss on information during discretization. In this section, we present a discretization method which uses class information entropy to evaluate candidate breakpoints in order to select boundaries [6]. The discretization algorithm based on entropy ( IE ) is associated with the information gain of objects divided by breakpoint s to measure the importance of them. Definition 2. Let W  X  U be the subset of objects which contains | W | objects. k t denotes the number of the objects whose decision attribute values are y t (1 t  X | d | ) ,where | d | is the number of distinct decision values. Then the class information entropy of W is defined as follows: Note that H ( W )  X  0. Smaller H ( X ) corresponds to lower uncertainty level of the decision table [5,6], since some certa in decision attribute values play the leading role in object subset W .Inparticular, H ( W ) = 0 if and only if all the objects in subset W have the same decision attribute value.

For a discretized decision table S ( P ), let W 1 ,W 2 ,  X  X  X  ,W r be the sets of equivalence classes based on the identical condition attribute values. Then, the class information entropy of the discretized decision table S ( P ) is defined as between entropy and consistency as follows. The proof is shown in the Appendix. Theorem 1. A discretized decision table S ( P ) is consistent if H ( S ( P )) = 0 . After the initial partition, H ( S ( P )) is usually not equal to 0, which means S ( P ) is not consistent. Accordingly, we need to select breakpoints from candidate set Q = { q 1 ,q 2 ,  X  X  X  ,q l } , and it is necessary to measure the importance of every el-ement of Q to determine which one to choose in the next step. Let S ( P be the discretized decision table w hen inserting the breakpoint set P the continuous decision table S , and the corresponding class information en-tropy is H ( S ( P breakpoint q i is defined as: Note that the greater the decrease H ( q i ) of entropy, the more important the breakpoint q i .Since H ( S ( P )) is a constant value for every q i (1  X  i  X  l ), then the smaller the entropy H ( S ( P will be chosen.
 Alternatively, we propose another discretization method incorporated with rough set theory to select breakpoints to partition the continuous values. The dis-cretization algorithm based on positive domain ( PD ) is built upon the indiscerni-bility relations induced by the equivalence classes to evaluate the significance of the breakpoints. Firstly, we recall the relevant concept in rough set theory [10]. Definition 3. Let U be a universe, P,Q are the equivalence relations over set U , then the Q positive domain (or positive region )of P is defined as: where W  X  U/Q is the equivalence class based on relation Q , [ u ] P is the equiva-lence class of u based on relation P .
 In the discretized decision table S ( P )=( U, C  X  equivalence relation of  X  X wo objects have the same condition attribute values X , let D denote the equivalence relation of  X  X wo object have the same decision attribute value X . Then, the positive domain ratio of the decision table S ( P ) is R ( S ( P )) = | POS C  X  ( D ) | / | U | .Notethat | U | is the number of objects, and 0  X  R ( S ( P ))  X  1. The greater the ratio R ( S ( P )), the higher the certainty level of discretized decision table [8,10]. Below, we reveal the consistency condition for the PD algorithm. The proof is also shown in the Appendix.
 Theorem 2. A discretized decision table S ( P ) is consistent if R ( S ( P )) = 1 . Similarly, we usually have R ( S ( P )) =1,thatistosay, S ( P )isnotconsistent after initialization. Thus, it is necessary to choose breakpoints from candidate set Q = { q 1 ,q 2 ,  X  X  X  ,q l } according to the significance order of all the candidate breakpoints for the next insertion. Let R ( S ( P main ratio of the discretized decision table S ( P the importance of breakpoint q i as: Note that the larger the increase R ( q i ) of ratio, the greater importance of the breakpoint q i .Since R ( S ( P )) is a constant for each candidate q i (1  X  i  X  l ), therefore, the larger the ratio R ( S ( P point q i . Discretization algorithms are considered in terms of information entropy and positive domain in Section 4 and Section 5 , respectively. In a discretized de-cision table, the information entropy m easures the uncertainty degree from the perspective of information attribute relationship, while the positive domain ratio reveals the certainty level with respect to the deterministic feature dependency [8]. In this Section, we focus on both the information and deterministic attribute dependencies to derive the coupled discretization ( CD ) algorithm.

Theoretically, Wang et. al [12] compared algebra viewpoint in rough set and information viewpoint in entropy theory. Later on, Chen and Wang [4] applied the aggregation of them to the hybrid space clustering. Similarly, by taking into account both the incremen t of certainty level and th e decrement of uncertainty degree in a decision table, we consider to combine the PD and IE based methods together to get the CD algorithm. This algorithm measures the importance of breakpoints comprehensively and reasonably by aggregating the positive domain ratio function R (  X  ) and the class information entropy function H (  X  ) together. Alternatively, we propose one option to quantify the coupled importance: Definition 4. For a discretized decision table S ( P ) , we have the coupled im-portance of breakpoint set P be: where R ( q i ) and H ( q i ) are the importance functions of breakpoint q i according to (5.2) and (4.2), respectively; k 1 ,k 2  X  [0 , 1] are the corresponding weights. For every condition attribute c j  X  C in the continuous decision table S = ( U, C
The process of the discretization algorithm based on the coupling of positive domain and information entropy is designed as follows. The algorithm below clearly shows that its computational complexity is O ( m 2 n 2 ) based on the loops. In this section, several experiments ar e performed on extensive UCI data sets to show the effectiveness of our proposed coupled discretization algorithm. All the experiments are conducted on a Dell Optiplex 960 equipped with an Intel Core 2 Duo CPU with a clock speed of 2.99 GHz and 3.25 GB of RAM running Microsoft Windows XP. For simplicity, we just assign the weights k 1 = k 2 =0 . 5 in Definition 4 and Algorithm 1.

To the best of our knowledge, there are mainly four dimensions to evaluate the quality [3,5,6] of discretization algorithms as follows: -Stability: How to measure the overall spread of the values in each interval. -Simplicity: The fewer the break points, the better the discretization result. -Consistency: The inconsistencies caused by discretization should not be large. -Accuracy: How discretization helps i mprove the classification accuracy. Discretization methods that adhere to internal criterion assign the best score to the algorithm that produces break points with high stability and low simplicity ; Algorithm 1. Coupled Algorithm for Discretization while discretization approaches that adhere to external criterion compare the results of the algorithm against some external benchmark, such as predefined classes or labels indicated by consistency and accuracy . From these two perspec-tives, the experiments here are divided into two categories according to different evaluation standards: internal criteria ( stability , simplicity ) and external criteria ( consistency , accuracy ), as shown in Section 7.1 and Section 7.2, respectively. 7.1 Internal Criteria Comparison With respect to the internal criterion, i.e., stability and simplicity, the goal in this set of experiments is to show the superiori ty of our proposed coupled discretiza-tion ( CD ) algorithm against some classic methods [6] such as Equal Frequency ( EF ), 1R , MDLP , Chi2 , and Information Entropy-based ( IE ) algorithms.
Specifically, simplicity measure is described as the total number of intervals ( NOI ) for all the discretized attributes. More complicatedly, the stability mea-sures are constructed from a series of estimated probability distributions for the individual intervals constructed by incorporating the method of Parzen windows [3]. As one of the induced measure, Attribute Stability Index ( ASI j )iscon-structed from the weighted sum of the Stability Index ( SI jk ), which describes the value distribution for each interval I k of attribute c j . The measure SI jk fol-the interval I k , while SI jk is close to 1 when its values are near the center of the interval I k .Furthermore,wehave0 &lt;ASI j &lt; 1, and the larger the ASI j value, the more stable and better the discretization method. Here, we adapt this mea-sure to be the Average Attribute Stability Index ( AASI ), which is the weighted sum of ASI j for all the attributes c j (1  X  j  X  n ): AASI = n j =1 ASI j /n .
The break points and intervals produced by the aforementioned six discretiza-tion methods are then analyzed on 15 UCI d ata sets in different scales, ranging from 106 to 1484 (number of objects). The results are reported in Table 1. As discussed, larger AASI , smaller NOI indicate more stable and simpler character-ization of the interval partition capability, which further corresponds to a better discretization algorithm. The values in bold are the best relevant indexes for each data. From Table 1, we observe that with the exception of only few items (in italic), the other indexes all show that our proposed CD algorithm is better than the other five classical approaches ( EF , 1R , MDLP , Chi2 , IE ) in most cases from the perspectives of stability and simplicity . It is also worth noting that our proposed CD always outperforms the IE algorithm presented in Section 4 in terms of stability , which verifies the benefit of aggregating the positive domain. 7.2 External Criterion Comparison In this part of our experiments, we focus on the other two aspects of evaluation measures: consistency and accuracy . Two independent groups of experiments are conducted with extensive data sets based on machine learning applications.
According to Liu et al. [6], consistency is defined by having the least pattern inconsistency count which is calculated as the number of times this pattern appears in the data minus the largest number of corresponding class labels. Thus, the fewer the inconsistency count, the better the discretization quality. Based on the discretization results in Section 7.1, we compute the sum of all the pattern inconsistency counts for all possible patterns of the original continuous feature subset. Consistency evaluation is conducted on nine data sets with different number of objects, ranging from 132 (Echo) to 768 (Pima) in an increasing order. We also consider the other seven d iscretization methods for comparison, i.e., Equal Frequency ( EW ), EF , 1R , MDLP , ChiM , Chi2 ,and IE .

As shown in Fig. 1, the total inconsistency counts of IE and our proposed CD are always 0 on all the data sets, because the stopping criteria are the consistency conditions presented in Theorem 1 and Theorem 2. However, MDLP seems to perform the worst in terms of the consistency index, and the inconsistency counts of the other five algorithms fall in the intervals between those of MDLP and CD for all the data sets. These observati ons reveal the fact that algorithms IE and CD are the most consistent candidates for discretization. While IE and AD both indicate a surprisingly high consistency , in general, CD produces higher stability (larger AASI ) and lower simplicity (smaller NOI ), as presented in Table 1.
How does discretization affect the cla ssification learning accuracy? As Liu et al. [6] indicate, accuracy is usually obtained by running a classifier in cross validation mode. In this group of experiments, two classification algorithms are taken into account. i.e., Naive-Bayes, and Decision Tree (C4.5). A Naive Bayes ( NB ) classifier is a simple probabilistic classifier based on applying Bayes X  the-orem with strong (naive) independence assumptions [13]. C4.5 is an algorithm used to generate a decision tree ( DT ) for classification. As pointed out in Section 1, the continuous attributes take too many different values for the NB classifier to estimate frequencies; DT algorithm can only carry out a selection process of nominal features [9]. Thus, discretization is rather critical for the task of classification learning. Here, we evaluate the discretization methods with the classification accuracies induced by NB and DT ( C4.5 ), respectively.
Fig. 2 reports the results on 9 data sets with distinct data sizes, which vary from 150 to 1484 in terms of the number of objects. As can be clearly seen from this figure, the classification algorithms with CD ,whether NB or DT ,mostly outperform those with other discretization methods (i.e., EW , EF , 1R , IE )from the perspective of average accuracy . That is to say, discretization algorithm CD is better than others on classification qualities. Though for the data set Yeast , the average accuracy measures induced by NB with CD are slightly smaller than that with IE ,the stability measures shown in Table 1 indicate that CD is better than IE . Therefore, our proposed discretization algorithm CD is better than other candidates with respect to the classification accuracy measure.
Besides, we lead a comparison among the algorithms presented in Section 4 ( IE ), Section 5 ( PD ), and Section 6 ( CD ). Due to space limitations, only simplic-ity and accuracy measures are considered to evaluate these three discretization algorithms. Here, we take advantage of the k-nearest neighbor algorithm ( k-NN ) [7], which is a method for classifying objects based on closest training examples in the feature space. After discretizatio n, five data sets are used for classification with both 1-NN and 3-NN , in which 70% of the data is randomly chosen for training with the rest 30% for testing. As indicated in Table 2, our proposed CD method generally outperforms the existing IE algorithm and proposed PD algorithm. Specifically for 3-NN , the average accuracy improving rate ranges from 2 . 35% (Iris) to 27 . 06% (Glass) when compared CD with IE . With regard to 1-NN , this rate falls within  X  1 . 58% (Glass) and 1 . 96% (Austra) between CD and PD . However, by considering both simplicity and accuracy , we find out that CD is the best one since it takes the aggregation of the other two candidates.
Consequently, we draw the following conclusion: our proposed Coupled Dis-cretization algorithm generally outperforms the other classical candidates in terms of all the four measures: stability , simplicity , consistency ,and accuracy . Discretization algorithm plays an important role in the applications of machine learning and data mining. In this paper, we propose a new global-supervised-splitting-incremental algorithm CD based on the coupling of positive domain and information entropy. This method measures the importance of breakpoints in a comprehensive and reasonable way . Experimental results show that our proposed algorithm can effectively improve the distribution stability and clas-sification accuracy , optimize the simplicity and reduce the total inconsistency counts. We are currently applying the CD algorithm to the estimation of web site quality with flexible weights k 1 ,k 2 and stopping criteria, and we also con-sider the aggregation of the CD algorithm with coupled nominal similarity [11] to induce coupled numeric similarity and clustering ensemble applications. Acknowledgment. This work is sponsored by Australian Research Council Grants (DP1096218, DP0988016, LP100200774, LP0989721), and Tianjin Re-search Project (10JCYBJC07500).
 Proof.  X  X  Theorem 1 ]Since H ( S ( P )) = 0 , then Because we have H ( W )  X  0,then H ( W 1 )= H ( W 2 )=  X  X  X  = H ( W r )=0 . According to the definition of class information entropy of W i ( i =1 , 2 ,  X  X  X  ,r ), p indicates that the decision attribute values of W i ( i =1 , 2 ,  X  X  X  ,r ) are all equal. That is to say, the discretized decision table is consistent.
 Proof.  X  X  Theorem 2 ] Let the equivalence class of the objects that have the same decision attribute value be denoted as Y = { Y 1 ,Y 2 ,  X  X  X  ,Y s } ,andthe equivalence class of the objects that have identical condition attribute value be denoted as X = { X 1 ,X 2 ,  X  X  X  ,X t } .
 Since we have R ( S ( P )) = 1, then | POC C  X  | = | U | holds. As we know POC C  X  ( D )  X  U , then we further obtain that POS C  X  ( D )= U . According to the Definition 6, for each Y j  X  Y ,wethenhaveatleastone X i  X  X ,tosatisfy X " one Y j  X  Y ,sothat X i  X  Y j . Hence, when the objects have identical condition attribute value, their decision attribute values are the same, which means the
