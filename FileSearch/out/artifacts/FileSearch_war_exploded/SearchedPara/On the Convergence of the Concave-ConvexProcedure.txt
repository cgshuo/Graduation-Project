 The concave-convex procedure (CCCP) [30] is a majorization-minimization algorithm [15] that is popularly used to solve d.c. (difference of convex functions) programs of the form, where f ( x ) = u ( x )  X  v ( x ) with u, v and c i being real-valued convex functions, d j being an affine function, all defined on R n . Here, [ m ] := { 1 , . . . , m } . Suppose v is differentiable. The CCCP algorithm is an iterative procedure that solves the following sequence of convex programs, around a solution obtained in the current iterate so that u ( x )  X  x T  X  v ( x ( l ) ) is convex in x , and therefore the non-convex program in (1) is solved as a sequence of convex programs as shown in (2). The original formulation of CCCP by Yuille and Rangarajan [30] deals with unconstrained and linearly constrained problems. However, the same formulation can be extended to handle any constraints (both convex and non-convex). CCCP has been extensively used in solving many non-convex programs (of the form in (1)) that appear in machine learning. For example, [3] proposed a successive linear approximation (SLA) algorithm for feature selection in support vector machines, which can be seen as a special case of CCCP. Other applications where CCCP has been used include sparse principal component analysis [27], transductive SVMs [11, 5, 28], feature selection in SVMs [22], structured estimation [10], missing data problems in Gaussian processes and SVMs [26], etc. The algorithm in (2) starts at some random point x (0)  X  { x : c i ( x )  X  0 , i  X  [ m ]; d j ( x ) = 0 , j  X  is to study the convergence of { x ( l ) }  X  l =0 : (i) When does CCCP find a local minimum or a stationary point 1 of the program in (1)? (ii) Does { x ( l ) }  X  l =0 converge? If so, to what and under what conditions? From a practical perspective, these questions are highly relevant, given that CCCP is widely applied in machine learning.
 In their original CCCP paper, Yuille and Rangarajan [30, Theorem 2] analyzed its convergence, but of { x ( l ) }  X  l =0 to a minimum or saddle point of the program in (1). However, a rigorous proof is not provided, to ensure that their claim holds for all u , v , { c i } and { d j } . Answering the previous questions, however, requires a rigorous proof of the convergence of CCCP that explicitly mentions the conditions under which it can happen.
 In the d.c. programming literature, Pham Dinh and Hoai An [8] proposed a primal-dual subd-ifferential method called DCA (d.c. algorithm) for solving a general d.c. program of the form min { u ( x )  X  v ( x ) : x  X  R n } , where it is assumed that u and v are proper lower semi-continuous convex functions, which form a larger class of functions than the class of differentiable functions. It can be shown that if v is differentiable, then DCA exactly reduces to CCCP. Unlike in CCCP, DCA involves constructing two sets of convex programs (called the primal and dual programs) and solving them iteratively in succession such that the solution of the primal is the initialization to the dual and vice-versa. See [8] for details. [8, Theorem 3] proves the convergence of DCA for gen-eral d.c. programs. The proof is specialized and technical. It fundamentally relies on d.c. duality, however, outlining the proof in any more detail requires a substantial discussion which would lead us too far here. In this work, we follow a fundamentally different approach and show that the con-vergence of CCCP, specifically, can be analyzed in a more simple and elegant way, by relying on Zangwill X  X  global convergence theory of iterative algorithms. We make some simple assumptions on the functions involved in (1), which are not too restrictive and therefore applicable to many practical situations. The tools employed in our proof are of completely different flavor than the ones used in the proof of DCA convergence: DCA convergence analysis exploits d.c. duality while we use the notion of point-to-set maps as introduced by Zangwill. Zangwill X  X  theory is a powerful and general framework to deal with the convergence issues of iterative algorithms. It has also been used to prove the convergence of the expectation-maximation (EM) algorithm [29], generalized alternating mini-mization algorithms [12], multiplicative updates in non-negative quadratic programming [25], etc. and is therefore a natural framework to analyze the convergence of CCCP in a more direct way. The paper is organized as follows. In Section 2, we provide a brief introduction to majorization-minimization (MM) algorithms and show that CCCP is obtained as a particular form of majorization-minimization. The goal of this section is also to establish the literature on MM algorithms and show where CCCP fits in it. In Section 3, we present Zangwill X  X  theory of global convergence, which is a general framework to analyze the convergence behavior of iterative algorithms. This theory is used to address the global convergence of CCCP in Section 4. This involves analyzing the fixed points of the CCCP algorithm in (2) and then showing that the fixed points are the stationary points of the program in (1). The results in Section 4 are extended in Section 4.1 to analyze the convergence of the constrained concave-convex procedure that was proposed by [26] to deal with d.c. programs with d.c. constraints. We briefly discuss the local convergence issues of CCCP in Section 5 and conclude the section with an open question. MM algorithms can be thought of as a generalization of the well-known EM algorithm [7]. The general principle behind MM algorithms was first enunciated by the numerical analysts, Ortega and Rheinboldt [23] in the context of line search methods. The MM principle appears in many places in statistical computation, including multidimensional scaling [6], robust regression [14], correspondence analysis [13], variable selection [16], sparse signal recovery [4], etc. We refer the interested reader to a tutorial on MM algorithms [15] and the references therein.
 The general idea of MM algorithms is as follows. Suppose we want to minimize f over  X   X  R n . The idea is to construct a majorization function g over  X   X   X  such that Thus, g as a function of x is an upper bound on f and coincides with f at y . The majorization algorithm corresponding with this majorization function g updates x at iteration l by jorization function, g is usually constructed by using Jensen X  X  inequality for convex functions, the first-order Taylor approximation or the quadratic upper bound principle [1]. However, any other method can also be used to construct g as long as it satisfies (3). It is easy to show that the above iterative scheme decreases the value of f monotonically in each iteration, i.e., where the first inequality and the last equality follow from (3) while the sandwiched inequality follows from (4).
 Note that MM algorithms can be applied equally well to the maximization of f by simply reversing the inequality sign in (3) and changing the  X  X in X  to  X  X ax X  in (4). In this case, the word MM refers to minorization-maximization, where the function g is called the minorization function. To put things in perspective, the EM algorithm can be obtained by constructing the minorization function g using Jensen X  X  inequality for concave functions. The construction of such a g is referred to as the E-step, while (4) with the  X  X in X  replaced by  X  X ax X  is referred to as the M-step. The algorithm in (3) and (4) is also referred to as the auxiliary function method , e.g., for non-negative matrix factorization [18]. [17] studied this algorithm under the name optimization transfer while [19] referred to it as the SM algorithm, where  X  X  X  stands for the surrogate step (same as the majorization/minorization step) and  X  X  X  stands for the minimization/maximization step depending on the problem at hand. g is called the surrogate function. In the following example, we show that CCCP is an MM algorithm for a particular choice of the majorization function, g .
 Example 1 (Linear Majorization) . Let us consider the optimization problem, min x  X   X  f ( x ) where f = u  X  v , with u and v both real-valued, convex, defined on R n and v differentiable. Since v is convex, we have v ( x )  X  v ( y ) + ( x  X  y ) T  X  v ( y ) ,  X  x, y  X   X  . Therefore, It is easy to verify that g is a majorization function of f . Therefore, we have If  X  is a convex set, then the above procedure reduces to CCCP, which solves a sequence of convex programs. As mentioned before, CCCP is proposed for unconstrained and linearly constrained non-convex programs. This example shows that the same idea can be extended to any constraint set. The first strict inequality follows from (6). The strict convexity of u leads to the strict convexity of g For an iterative procedure like CCCP to be useful, it must converge to a local optimum or a stationary point from all or at least a significant number of initialization states and not exhibit other nonlinear system behaviors, such as divergence or oscillation. This behavior can be analyzed by using the global convergence theory of iterative algorithms developed by Zangwill [31]. Note that the word  X  X lobal convergence X  is a misnomer. We will clarify it below and also introduce some notation and terminology.
 To understand the convergence of an iterative procedure like CCCP, we need to understand the notion of a set-valued mapping , or point-to-set mapping , which is central to the theory of global convergence. 2 A point-to-set map  X  from a set X into a set Y is defined as  X  : X  X  P ( Y ) , which assigns a subset of Y to each point of X , where P ( Y ) denotes the power set of Y . We introduce few definitions related to the properties of point-to-set maps that will be used later. Suppose X and Y are two topological spaces. A point-to-set map  X  is said to be closed at x 0  X  X if x k  X  x 0 as k  X   X  , x k  X  X and y k  X  y 0 as k  X   X  , y k  X   X ( x k ) , imply y 0  X   X ( x 0 ) . This concept of closure generalizes the concept of continuity for ordinary point-to-point mappings. A point-to-set map  X  is said to be closed on S  X  X if it is closed at every point of S . A fixed point of the map  X  : X  X  P ( X ) is a point x for which { x } =  X ( x ) , whereas a generalized fixed point of  X  is a point for which x  X   X ( x ) .  X  is said to be uniformly compact on X if there exists a compact set H independent of x such that  X ( x )  X  H for all x  X  X . Note that if X is compact, then  X  is uniformly compact on X . Let  X  : X  X  R be a continuous function.  X  is said to be monotonic with respect to  X  whenever y  X   X ( x ) implies that  X  ( y )  X   X  ( x ) . If, in addition, y  X   X ( x ) and  X  ( y ) =  X  ( x ) imply that y = x , then we say that  X  is strictly monotonic .
 Many iterative algorithms in mathematical programming can be described using the notion of point-to-set maps. Let X be a set and x 0  X  X a given point. Then an algorithm , A , with initial point x 0 is a point-to-set map A : X  X  P ( X ) which generates a sequence { x k }  X  k =1 via the rule x k +1  X  A ( x k ) , k = 0 , 1 , . . . . A is said to be globally convergent if for any chosen initial point x 0 , the sequence { x k }  X  k =0 generated by x k +1  X  A ( x k ) (or a subsequence) converges to a point for which a necessary condition of optimality holds. The property of global convergence expresses, in a sense, the certainty that the algorithm works. It is very important to stress the fact that it does not imply (contrary to what the term might suggest) convergence to a global optimum for all initial points x 0 . With the above mentioned concepts, we now state Zangwill X  X  global convergence theorem [31, Con-vergence theorem A, page 91].
 Theorem 2 ([31]) . Let A : X  X  P ( X ) be a point-to-set map (an algorithm) that given a point x 0  X  X generates a sequence { x k }  X  k =0 through the iteration x k +1  X  A ( x k ) . Also let a solution set  X   X  X be given. Suppose Then the limit of any convergent subsequence of { x k }  X  k =0 is in  X  . Furthermore, lim k  X  X  X   X  ( x k ) =  X  ( x  X  ) for all limit points x  X  .
 The general idea in showing the global convergence of an algorithm, A is to invoke Theorem 2 by appropriately defining  X  and  X  . For an algorithm A that solves the minimization problem, min { f ( x ) : x  X   X  } , the solution set,  X  is usually chosen to be the set of corresponding station-ary points and  X  can be chosen to be the objective function itself, i.e., f , if f is continuous. In Theorem 2, the convergence of  X  ( x k ) to  X  ( x  X  ) does not automatically imply the convergence of x k to x  X  . However, if A is strictly monotone with respect to  X  , then Theorem 2 can be strengthened by using the following result due to Meyer [20, Theorem 3.1, Corollary 3.2].
 Theorem 3 ([20]) . Let A : X  X  P ( X ) be a point-to-set map such that A is uniformly compact, closed and strictly monotone on X , where X is a closed subset of R n . If { x k }  X  k =0 is any sequence generated by A , then all limit points will be fixed points of A ,  X  ( x k )  X   X  ( x  X  ) =:  X   X  as k  X   X  , where x  X  is a fixed point, k x k +1  X  x k k X  0 , and either { x k }  X  k =0 converges or the set of limit points of { x k }  X  k =0 is connected. Define F ( a ) := { x  X  F :  X  ( x ) = a } where F is the set of fixed points of A . If F (  X   X  ) is finite, then any sequence { x k }  X  k =0 generated by A converges to some x  X  in F (  X   X  ) . Both these results just use basic facts of analysis and are simple to prove and understand. Using these results on the global convergence of algorithms, [29] has studied the convergence properties of the EM algorithm, while [12] analyzed the convergence of generalized alternating minimization procedures. In the following section, we use these results to analyze the convergence of CCCP. Let us consider the CCCP algorithm in (2) pertaining to the d.c. program in (1). Let A cccp be the where  X  := { x : c i ( x )  X  0 , i  X  [ m ] , d j ( x ) = 0 , j  X  [ p ] } . Let us assume that { c i } are dif-ferentiable convex functions defined on R n . We now present the global convergence theorem for CCCP.
 Theorem 4 (Global convergence of CCCP  X  I) . Let u and v be real-valued differentiable convex functions defined on R n . Suppose  X  v is continuous. Let { x ( l ) }  X  l =0 be any sequence generated by A cccp defined by (9). Suppose A cccp is uniformly compact 3 on  X  and A cccp ( x ) is nonempty for where x  X  is some stationary point of A cccp .
 Before we proceed with the proof of Theorem 4, we need a few additional results. The idea of the proof is to show that any generalized fixed point of A cccp is a stationary point of (1), which is shown below in Lemma 5, and then use Theorem 2 to analyze the generalized fixed points.
 Lemma 5. Suppose x  X  is a generalized fixed point of A cccp and assume that constraints in (9) are qualified at x  X  . Then, x  X  is a stationary point of the program in (1).
 Proof. We have x  X   X  A cccp ( x  X  ) and the constraints in (9) are qualified at x  X  . Then, there exists Lagrange multipliers {  X   X  i } m i =1  X  R + and {  X   X  j } p j =1  X  R such that the following KKT conditions hold:  X  is a stationary point of (1). Before proving Theorem 4, we need a result to test the closure of A cccp . The following result from [12, Proposition 7] shows that the minimization of a continuous function forms a closed point-to-set map. A similar sufficient condition is also provided in [29, Equation 10].
 Lemma 6 ([12]) . Given a real-valued continuous function h on X  X  Y , define the point-to-set map  X  : X  X  P ( Y ) by Then,  X  is closed at x if  X ( x ) is nonempty.
 We are now ready to prove Theorem 4.
 Proof of Theorem 4. The assumption of A cccp being uniformly compact on  X  ensures that condition (1) in Theorem 2 is satisfied. Let  X  be the set of all generalized fixed points of A cccp and let  X  = f = u  X  v . Because of the descent property in (5), condition (2) in Theorem 2 is satisfied. By our assumption on u and v , we have g ( x, y ) = u ( x )  X  v ( y )  X  ( x  X  y ) T  X  v ( y ) is continuous in x and y . Therefore, by Lemma 6, the assumption of non-emptiness of A cccp ( x ) for any x  X   X  ensures that A cccp is closed on  X  and so satisfies condition (3) in Theorem 2. Therefore, by Theorem 2, v ( x ( l ) )) = u ( x  X  )  X  v ( x  X  ) , where x  X  is some generalized fixed point of A cccp . By Lemma 5, since the generalized fixed points of A cccp are stationary points of (1), the result follows. Remark 7. If  X  is compact, then A cccp is uniformly compact on  X  . In addition, since u is continuous on  X  , by the Weierstrass theorem 4 [21], it is clear that A cccp ( x ) is nonempty for any x  X   X  and therefore is also closed on  X  . This means, when  X  is compact, the result in Theorem 4 follows trivially from Theorem 2.
 In Theorem 4, we considered the generalized fixed points of A cccp . The disadvantage with this case is that it does not rule out  X  X scillatory X  behavior [20]. To elaborate, we considered { x  X  }  X  A with the convergent subsequences converging to the generalized fixed points x 1 and x 2 . Such an oscillatory behavior can be avoided if we allow A cccp to have fixed points instead of generalized fixed points. With appropriate assumptions on u and v , the following stronger result can be obtained on the convergence of CCCP through Theorem 3.
 Theorem 8 (Global convergence of CCCP  X  II) . Let u and v be strictly convex, differentiable func-tions defined on R n . Also assume  X  v be continuous. Let { x ( l ) }  X  l =0 be any sequence generated by A cccp defined by (9). Suppose A cccp is uniformly compact on  X  and A cccp ( x ) is nonempty for as l  X   X  , for some stationary point x  X  , k x ( l +1)  X  x ( l ) k  X  0 , and either { x ( l ) }  X  l =0 con-S ( a ) := { x  X  S : u ( x )  X  v ( x ) = a } and S is the set of stationary points of (1). If S ( f  X  ) is Proof. Since u and v are strictly convex, the strict descent property in (8) holds and therefore A cccp is strictly monotonic with respect to f . Under the assumptions made about A cccp , Theorem 3 can converge or form a connected compact set. From Lemma 5, the set of fixed points of A cccp are already in the set of stationary points of (1) and the desired result follows from Theorem 3. Theorems 4 and 8 answer the questions that we raised in Section 1. These results explicitly provide sufficient conditions on u , v , { c i } and { d j } under which the CCCP algorithm finds a stationary point of (1) along with the convergence of the sequence generated by the algorithm. From Theorem 8, it x ( l ) to x  X  . The convergence in the latter sense requires more stringent conditions like the finiteness of the set of stationary points of (1) that assume the value of f  X  . 4.1 Extensions So far, we have considered d.c. programs where the constraint set is convex. Let us consider a general d.c. program given by where { u i } , { v i } are real-valued convex and differentiable functions defined on R n . While dealing with kernel methods for missing variables, [26] encountered a problem of the form in (12) for which they proposed a constrained concave-convex procedure given by in (13) is a sequence of convex programs. Though [26, Theorem 1] have provided a convergence analysis for the algorithm in (13), it is however not complete due to the fact that the convergence approach similar to what we did for CCCP by considering a point-to-set map, B ccp associated with convergence result for the constrained concave-convex procedure, which is an equivalent version of Theorem 4 for CCCP. We do not provide the stronger version of the result as in Theorem 8 as it can be obtained by assuming strict convexity of u 0 and v 0 . Before proving Theorem 10, we need an equivalent version of Lemma 5 which we provide below.
 Lemma 9. Suppose x  X  is a generalized fixed point of B ccp and assume that constraints in (13) are qualified at x  X  . Then, x  X  is a stationary point of the program in (12).
 Proof. Based on the assumptions x  X   X  B ccp ( x  X  ) and the constraint qualification at x  X  in (13), there exist Lagrange multipliers {  X   X  i } m i =1  X  R + (for simplicity, we assume all the constraints to be inequality constraints) such that the following KKT conditions hold: which is exactly the KKT conditions for (12) satisfied by ( x  X  , {  X   X  i } ) and therefore, x  X  is a stationary point of (12).
 Theorem 10 (Global convergence of constrained CCP) . Let { u i } , { v i } be real-valued differentiable convex functions on R n . Assume  X  v 0 to be continuous. Let { x ( l ) }  X  l =0 be any sequence generated by B ccp defined in (13). Suppose B ccp is uniformly compact on  X  := { x : u i ( x )  X  v i ( x )  X  0 , i  X  [ m ] } and B ccp ( x ) is nonempty for any x  X   X  . Then, assuming suitable constraint qualification, Proof. The proof is very similar to that of Theorem 4 wherein we check whether B ccp satisfies the conditions of Theorem 2 and then invoke Lemma 9. The assumptions mentioned in the statement of the theorem ensure that conditions (1) and (3) in Theorem 2 are satisfied. [26, Theorem 1] has proved the descent property, similar to that of (5), which simply follows from the linear majorization idea and therefore the descent property in condition (2) of Theorem 2 holds. Therefore, the result follows from Theorem 2 and Lemma 9. The study so far has been devoted to the global convergence analysis of CCCP and the constrained concave-convex procedure. As mentioned before, we say an algorithm is globally convergent if for any chosen starting point, x 0 , the sequence { x k }  X  k =0 generated by x k +1  X  A ( x k ) converges to a point for which a necessary condition of optimality holds. In the results so far, we have shown that all the limit points of any sequence generated by CCCP ( resp. its constrained version) are the stationary points (local extrema or saddle points) of the program in (1) ( resp. (12)). Suppose, if x 0 is chosen such that it lies in an  X  -neighborhood around a local minima, x  X  , then will the CCCP sequence converge to x  X  ? If so, what is the rate of convergence? This is the question of local convergence that needs to be addressed. [24] has studied the local convergence of bound optimization algorithms (of which CCCP is an example) to compare the rate of convergence of such methods to that of gradient and second-order methods. In their work, they considered the unconstrained version of CCCP with A cccp to be a point-to-point map that is differentiable. They showed that depending on the curvature of u and v , CCCP will exhibit either quasi-Newton behavior with fast, typically superlinear convergence or extremely slow, first-order convergence behavior. However, extending these results to the constrained setup as in (2) is not obvious. The following result due to Ostrowski which can be found in [23, Theorem 10.1.3] provides a way to study the local convergence of iterative algorithms.
 Proposition 11 (Ostrowski) . Suppose that  X  : U  X  R n  X  R n has a fixed point x  X   X  int ( U ) and is sufficiently close to x  X  , then the iterates { x k } defined by x k +1 =  X ( x k ) all lie in U and converge to x  X  .
 Few remarks are in place regarding the usage of Proposition 11 to study the local convergence of CCCP. Note that Proposition 11 treats  X  as a point-to-point map which can be obtained by choosing u and v to be strictly convex so that x ( l +1) is the unique minimizer of (2). x  X  in Proposition 11 can be chosen to be a local minimum. Therefore, the desired result of local convergence with at least linear rate of convergence is obtained if we show that  X  ( X  0 ( x  X  )) &lt; 1 . However, currently we are not aware of a way to compute the differential of  X  and, moreover, to impose conditions on the functions in (2) so that  X  is a differentiable map. This is an open question coming out of this work. On the other hand, the local convergence behavior of DCA has been proved for two important classes of d.c. programs: (i) the trust region subproblem [9] (minimization of a quadratic function over a Euclidean ball) and (ii) nonconvex quadratic programs [8]. We are not aware of local optimality results for general d.c. programs using DCA. The concave-convex procedure (CCCP) is widely used in machine learning. In this work, we analyze its global convergence behavior by using results from the global convergence theory of iterative algorithms. We explicitly mention the conditions under which any sequence generated by CCCP converges to a stationary point of a d.c. program with convex constraints. The proposed approach allows an elegant and direct proof and is fundamentally different from the highly technical proof for the convergence of DCA, which implies convergence for CCCP. It illustrates the power and generality of Zangwill X  X  global convergence theory as a framework for proving the convergence of iterative algorithms. We also briefly discuss the local convergence of CCCP and present an open question, the settlement of which would address the local convergence behavior of CCCP.
 Acknowledgments The authors thank anonymous reviewers for their constructive comments. They wish to acknowl-edge support from the National Science Foundation (grant DMS-MSPA 0625409), the Fair Isaac Corporation and the University of California MICRO program.

