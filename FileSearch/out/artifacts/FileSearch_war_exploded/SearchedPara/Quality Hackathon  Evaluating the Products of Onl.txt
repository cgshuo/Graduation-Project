 This full -day workshop focuses on building Big Social Data research competencies for scholars interested in issues of contribution quality and contributor performance in online co -production systems that generate value through contributions by volunteers. The workshop is design ed to engage discussion and promote co -working through a hackathon format to stimulate productive conversation and learning, using shared data sets to provide a common focus for participants to engage questions of contribution quality and contributor perfo rmance with multiple disciplinary, theoretical, and analytical backgrounds.
 K.4.3 [ Computers and Society ]: Organizational impacts  X  computer -supported cooperative work.
 Human Factors , Measurement, Performanc e Contribution quality; performance; user -generated content; online communities; peer production; data quality; hackathon As the contexts of online co -production continue to expand, from software in open source systems, to consumer ta stes in recommender systems, to encoding knowledge in Wikipedia, to research data in citizen science, questions continue to resurface around contributor performance, contribution quality, and collective productivity. How do we evaluate productivity, charac terize performance, or rate the quality of contributions from specific individuals ? How and to what extent are concepts, measures, or frameworks identifiable and reusable across contexts? The quality of co -production is an important factor in the broader adoption and use of these systems, and also for evaluation of overall success in goal -oriented communities [ 7]. In all cases, the products of these communities are considered suspect until accepted evaluative measures can be applied to verify the value of contributions [ 2], both at the level of the individual and collectively. Yet naive measures of contribution volume and quality are rarely transferable across contexts and the approaches used to evaluate conv entional products do not always translate to co -production environments with large -scale contributor bases and atomized or uneven individual contributions. As a result, both research and adoption of online co -production platforms is slowed by the need to redevelop contribution assessment for each new context. Simplistic heuristics, such as  X  X ho contributes the most X  and user -generated rankings have typically proven unsatisfactory in isolation [9]. Free/libre and open source software development was one of the earliest contexts for understanding the nuances of co -production, with complex, multifaceted frameworks developed to evaluate software quality outside of proprietary environments [ 10,15,13 ]. Substantial progress has been made in Wikipedia research, wh ich has taken several different approaches to evaluate editors, their productivity, and the quality of their contributions [ 1,6,14 ]. In question -answering communities, numerous factors impact the evaluation of responses, including whether there is a bount y offered, the diversity of the community, and personal characteristics of the asker [ 3,5,11 ]. In other domains, such as citizen science, quality control strateg ies for water quality projects ( e.g., [12 ]) are inapplicable to projects that involve contribut ors in gathering species occurrence data [ 4], and projects focused on image classification take altogether different strategies [ 8]. Due to wide variability in the nature of the data that can be extracted to evaluate contributions and performance, the degr ee to which concepts and findings from one context might apply to another remains unclear, and there are few examples of cross -context studies employing similar measures in different communities. In short, evaluating contribution quality an d contributor performance in co -production communities remains a critical question, but discussions of how to approach this challenge are limited by the context of participation and specifics of the available data. Transcending these constraints and sharin g knowledge around the conceptual and practical issues of conceptualizing and evaluating contribution quality and contributor performance will help stimulate a higher level of research discourse across multiple contexts of participation and academic discip lines. The primary goals of the proposed workshop are to facilitate productive discussions among individuals whose work focuses on different contexts of participation, foster the development of a network of researchers with shared interests in evaluating contribution quality and contributor performance, and provide opportunities for developing skill and collaborative research relationships through hands -on engagement. The conceptual focus of the workshop will be on identifying concepts, measures, tools, and analyses that can advance our understanding of contribution quality and contributor performance in large online communities. The audience for the workshop will be researchers and practitioners with interests related to these t hemes, and those seeking opportunity to develop collaborative relationships with others in the Group community. Although participants will have experience with diverse research contexts and approaches, applying such knowledge and skills to a new context pr ovides a good opportunity for learning, synthesizing, and generating ideas. Particip ants will apply with a short (~ 1 page) document describing their interest in studying contribution quality and contributor performance with a short summary of their applic able skills, theories, tools, and ideas. These documents will be summarized, aggregated, and distributed to all participants in advance to seed the day X  X  activities. We expect that the participants will bring a broad mix of theoretical, methodological, tec hnical, and analytical assets to the workshop, and providing background on participants X  intellectual diversity helps set expectations for flexibility around specific hackathon activities as well as team composition. Despite its heterogeneity, the advanced technical knowledge and experience of the Group community should generate an adequate distribution of skills to allow each team to have a productive experience. Workshop participants will be provided advance access to the data; we anticipate us ing one or two data sets. Examples of candidate data sets, pending evaluation of Terms of Use, include the Yelp Dataset Challenge data, Reddit images and comments, Stack Exchange data, and FLOSSmole data. An agenda with workshop plans and expectations will also be distributed in advance. Participants will be encouraged to examine the data, consider which other participants with whom they may wish to work, and prepare questions or ideas to work on during the workshop.
 The workshop will open with a brief over view of the format, goals, and plans for the day (10 minutes). Each participant will provide a brief introduction to their work, their applicable skills, and specific interest in the workshop themes (30 minutes). We will then engage a brief brainstorming p rocess, eliciting the questions and ideas attendees have considered in advance and iterating on these ideas (30 minutes). The participants will then be able to self -select into small groups of 3 -4 individuals to pursue data -driven hacking oriented toward developing and implementing one or more measures of content quality or contributor performance in the shared dataset. The workshop organizers will join in with groups as needed.
 Before lunch, we will break briefly to give two -minute status updates for each group, both to acknowledge the work completed to that point and to seed meal time conversation. Following lunch, the organizers will lead a short activity for the full group intended to bring awareness to the spectrum of emerging challenges and perhaps fos ter a new set of connection points for participants to work together. Participants will be encouraged to change groups or to form larger clusters as appropriate. We will reserve the last 90 minutes of the session for full-group discussion during which each team will present a short debrief on their progress over the course of the day and share any datasets, visualizations, and analyses they have produced.
 While the primary goal of the hackathon is to produce a functional analysis of contribution quality and contributor performance, we will endeavor to create a low -pressure environment for exploratory learning and data play. The hackathon workshop model, while loosely structured and lightly managed, was highly successful at CSCW 2014 and we anticipate it will work effectively for Group 2014 as well. There are a variety of potential outcomes from a hackathon-style workshop focused on data analysis. By creating the opportunity for participants to work together with tools and theories focused o n shared data sets, we expect the workshop will create an environment suitable for professional development gains for participants: introduction to new research skills and theoretical perspectives, refinement of ideas and questions for research, and develo ping new collaborative relationships. New research projects and publications could also emerge from the starting point provided by co -working at the workshop. In addition, participants will gain experience with the hackathon model of peer production in a r esearch -oriented context, which was a popular feature of prior related workshops. The workshop will benefit from having the following equipment available: one projector, 1 -2 flip charts with markers, and snacks. We can accommodate up to 20 partic ipants. Our co -organizers are experienced in facilitating and participating in workshops and hackathons, familiar with theories and tools suitable for addressing the problem of evaluating contribution quality in online production systems, and pl ay well with others.
 Andrea Wiggins co -organized the successful CSCW 2014 Online Communities Data (OCData) Hackathon and has participated in several community infrastructure development hackathons. She has expertise in the topics of human computation, open source systems, and large -scale collaboration; her methodological skills include operationalizing theory using digital trace data, measure development, and social network analysis.
 David Gurzick participated in the CSCW 2014 OCData Hackathon and has exper ience working with a variety of related data sets. His expertise includes research in the design, evaluation, and operation of online communities and the development and configuration of open-participation systems. His methodological backgrou nd includes th e application of mixed -method analysis to sociotechnical data and the use of design science techniques. He has strong software development skills and extensive professional experience in developing online systems and applications with user -centered methodo logies. Sean Goggins co-organized the CSCW 2014 OCData Hackathon and led a similar 50 -person event at the 2013 iConference; he has also organized several smaller production -driven hackathons to develop community resources. These events were all considered very successful despite differences in focus and widely varying audiences and needs. Goggins X  subject matter expertise includes computer -supported cooperative learning and learning analytics, with methodological skills in social network analysis and comput ational linguistic analysis. Brian Butler co -organized the CSCW 2014 OCData Hackathon, directed the Digital Societies and Social Technologies 2014 Summer Institute, and has led various doctoral/junior faculty workshops. In both cases, the events focused on the goals of building relationships within an interdisciplinary community, developing the capabilities of individual researchers, and catalyzing theory -oriented, data -enabled collaborative projects. He has worked with general online communities, Wikipedia , Q&amp;A forums, and other co -production platforms. His prior work has focused on using empirical and simulation models to develop and test generalizable models of contribution and participation dynamics in online communities. This work was supported in part by U.S. National Science Foundation Grant IIS 1449209. [1] Blumenstock, J. E. (2008). Size matters: word count as a [2] Duguid, P. (2006). Limits of self -organization: Peer production [3] Harper, F. M., Raban, D., Rafaeli, S., &amp; Konstan, J. A. (2008). [4] Hochachka, W. M., Fink, D., Hutchinson, R. A., Sheldon, D., [5] Jurczyk, P ., &amp; Agichtein, E. (2007). Discovering authorities in [6] Kittur, A., &amp; Kraut, R. E. (2008). Harnessing the wisdom of [7] Preece, J. (2001). Sociability and usability in online [8] Prestopnik, N. R., Crowston K., &amp; Wang J. (2014). Exploring [9] Riedl, C., Blohm, I., Leimeister, J. M., &amp; Krcmar, H. (2010). [10] Samoladas, I., Gousios, G., Spinellis, D., &amp; Stamelos, I. [11] Shah, C., &amp; Pomerantz, J. (2010). Evaluating and predicting [12] Sheppard, S. A., &amp; Terveen, L. (2011). Quality is a verb: The [13] Spinellis, D., Gousios, G., Karakoidas, V., Louridas, P., [14] Suzuki, Y., &amp; Yoshika wa, M. (2012). QualityRank: assessing [15] Taibi, D., Lavazza, L., &amp; Morasca, S. (2007). OpenBQR: a 
