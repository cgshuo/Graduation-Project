 Microblogging services, such as Twitter, have become pop-ular channels for people to express their opinions towards a broad range of topics. Twitter generates a huge volume of instant messages (i.e. tweets) carrying users' sentiments and attitudes every minute, which both necessitates auto-matic opinion summarization and poses great challenges to the summarization system. In this paper, we study the prob-lem of opinion summarization for entities, such as celebri-ties and brands, in Twitter. We propose an entity-centric topic-based opinion summarization framework, which aims to produce opinion summaries in accordance with topics and remarkably emphasizing the insight behind the opin-ions. To this end, we rst mine topics from #hashtags, the human-annotated semantic tags in tweets. We integrate the #hashtags as weakly supervised information into topic modeling algorithms to obtain better interpretation and rep-resentation for calculating the similarity among them, and adopt Anity Propagation algorithm to group #hashtags into coherent topics. Subsequently, we use templates gen-eralized from paraphrasing to identify tweets with deep in-sights, which reveal reasons, express demands or re ect view-points. Afterwards, we develop a target (i.e. entity) de-pendent sentiment classi cation approach to identifying the opinion towards a given target (i.e. entity) of tweets. Fi-nally, the opinion summary is generated through integrating information from dimensions of topic, opinion and insight, as well as other factors (e.g. topic relevancy, redundancy and language styles) in an uni ed optimization framework. We conduct extensive experiments on a real-life data set to evaluate the performance of individual opinion summariza-tion modules as well as the quality of the produced summary. The promising experiment results show the e ectiveness of the proposed framework and algorithms.
 Con tribution during internship at Microsoft Research Asia. I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Lan-guage Processing| Text analysis Algorithms, Experimentation Opinion summarization, sentiment analysis, topic analysis, #hashtag, Twitter
Twitter 1 is a microblogging service that allows people to publish short messages (i.e. tweets) up to 140 characters to share with others what is happening, what is interesting, as well as their feelings and opinions. In the recent several years, Twitter has become an extremely popular site. It has been reported that about 200 million 2 tweets are posted on June 30, 2011. In these tweets, people not only share their daily update information or personal conversation, but also exchange their opinions towards a broad range of topics. For example, they may talk about the newly released electronic products, express their feelings to celebrities, and deliver their comments on politicians or global events, etc. Conse-quently, we can use Twitter to survey public opinions. In this paper, we are particularly interested in the sentiment to-wards certain entities , such as celebrities, brands and prod-ucts. These opinions are extremely useful in many real-life applications. For instance, politicians can study opinions in Twitter in order to know their public image, and company can study opinions in Twitter to obtain customer feedbacks.
The huge volume of tweets carrying sentiments neces-sitates automatic summarization techniques, which assists users in consuming these opinions. Ideally, we expect the opinion summary to convey the overview of the opinions for a speci c entity from the public perspective. However, peo-ple may express opinions towards di erent aspects, or topics, of an entity. Take the US president Obama as an example, people may criticize (in a negative manner) his economic revitalization plans, but rmly support (in a positive atti-tude) Obama's counter-terrorism polices. This suggests a ne-grained summary, in terms of the di erent topics, in-stead of the general summary that mixes up the opinions h ttp://twitter.com http://blog.twitter.com/2011/06/200-million-tweets-per-day.html fro m all aspects regarding Obama. Furthermore, in Twitter, many tweets simply reveal the sentiment information. For example, \ I love Obama! #libya ". These tweets certainly account for accumulating the overall sentiment for the tar-get, but do not deliver much information for the underlying insight behind the opinions, which are even more important for users. On the other hand, there are tweets in Twitter that explicitly state the factors leading to the positive or negative opinions. We show a typical example: \ I think it is time to say that Barack Obama deserves credit for backing up his words with action on #Libya despite domestic opposi-tion. " We can clearly understand the reasons underlying the sentiment from the tweet. To summarize, the opinion sum-mary should include three key elements, which are topic , sentiment and insight .

In this paper, we study the problem of opinion summa-rization for entities in Twitter. In particular, we propose an entity-centric topic-based opinion summarization frame-work, which aims to produce opinion summaries in accor-dance with topics and remarkably emphasizing the insight behind the opinions. It has been well-recognized that tweets are short, noisy and ungrammatical, which issues tough chal-lenges for topic analysis, sentiment analysis as well as iden-tifying tweets with insights in Twitter. As for topics anal-ysis, we leverage the human-annotated semantic tags, i.e. #hashtags to assist identifying topics from tweets. #hash-tags are created organically by Twitter users as a way to categorize messages and to highlight topics, which is done by simply pre xing a word or a phrase with a symbol # to mark keywords or topics in tweets, such as \#hashtag". For example, in the tweet \ Neelie Kroes, Vice President of European Commission, Digital Agenda Commissioner, will be Keynote speaker at #www2012 : http://www2012.org/ ", the author marked up the topic keyword \#www2012" as a #hashtag. Besides, in Twitter, people often group several words together to form a #hashtag, for example, #IraqWar and #HealthCare. Those #hashtags have clear boundaries and often can be viewed as semantic concepts, hence are more suitable as topics than n-gram phrases. The extensive use of #hashtags makes Twitter more expressive and wel-comed by people. We measure on a data set with around 0.2 million tweets and nd that around 23% of them have at least one #hashtag in each. Among all unique #hashtags, we sample 100 #hashtags and nd 75% are useful for topic extraction. The statistics show a great potential for mining topics from #hashtags in Twitter.

Consequently, we integrate #hashtags as weakly super-vised information into topic modeling algorithms to obtain better interpretation and representation for calculating the similarity among them, and then adopt Anity Propaga-tion algorithm to group #hashtags into coherent topics. We develop a target (i.e. entity) dependent sentiment classi -cation approach to identifying the opinion towards a given target (i.e. entity) of tweets. We incorporate the depen-dency relationship between the sentiment lexicons and the target (i.e. entity) into the lexicon based sentiment classi -cation framework. Subsequently, we use template methods to identify insightful tweets that revealing reasons, express-ing demands or re ecting viewpoints to embed deep insights into the opinion summary. However, template methods may su er from the problem of low recall, we thus employ para-phrasing approach to conduct template expansion. Finally, the opinion summary is generated through integrating in-formation from the above-mentioned dimensions as well as other factors (e.g. redundancy and language styles) in an uni ed optimization framework.

We conduct extensive experiments on a real-life data set to evaluate the performance of individual opinion summariza-tion modules as well as the quality of the produced summary. The promising experiment results show the e ectiveness of the proposed framework and algorithms. To the best of our knowledge, the work in this paper is the rst attempt on opinion summarization in Twitter.
Opinion summarization is a broad and diversi ed research topic. Most existing work focuses on summarizing opinions from user generated content such as product reviews [11, 12], movie reviews [28] or hotel reviews [26]. They follow the aspect-based opinion summarization paradigm which ana-lyzes sentiment on ne-grained features or aspects of a prod-uct. The features can be the functionality of the product, like camera functionality of a mobile phone, or a part of the product, like the screen of a mobile phone. In these work, aspects are extracted from reviews by association rule min-ing [11, 12] or aspect-sentiment topic models [18, 25, 19], extensions of topic models. Then, the opinion towards each aspect is extracted and summarized. In the simplest case, the summary is a positive or negative label aggregated over the user generated content. Di erent methods are proposed to assign the sentiment labels for aspects. Some of them leverage existing sentiment classi er, while others directly model aspect-sentiment relation using the aspect-sentiment models [19]. The interested readers are referred to [22] and [16] for a general introduction to opinion summariza-tion, and [14] for a more recent survey on opinion summa-rization. The above-mentioned work can be viewed as opin-ion summarization on restricted domain (product reviews, movie reviews, etc.), unlike our approach, which is applied in Twitter, a general domain opinion source.

Sentiment analysis on microblogging services like Twit-ter is also receiving popularity. Barbosa and Feng conduct sentiment classi cation on tweets via two-stage SVM clas-si er [1]. They focus on selecting features and combining di erent label sources in order to remove noises in Twit-ter. Davidov et al. incorporate #hashtags and smileys from tweets as sentiment labels [8]. O'Connor et al. de-termine the sentiments by subjective Lexicon [21] . Jiang et al. study target-dependent Twitter sentiment classi ca-tion [13]. Wang et al. [27] propose to conduct sentiment classi cation on #hashtags, which are coarsely regarded as topics in their work. These work mainly focuses on senti-ment classi cation, which apparently di ers from the work described in this paper.
We start this section by a formal de nition of the task of entity-centric topic-oriented opinion summarization in Twit-ter and then present an overview of the proposed approach.
Given a set of tweets T mentioning an entity e , we aim to produce a opinion summary O = f O 1 ; O 2 ; : : : ; O N g
We refer a tweet t i 2f t g as an insightful opinionated tweet to indicate a tweet not only conveys opinions but also pro-vides insight. We show an example of insightful opinionated tweet as follows.

The author not only expresses his/her positive attitude to-wards President Obama, but also explicitly states that this attitude is the result of Obama's actions on Libya. This type of insightful opinionated tweets conveys the insight be-hind the opinions, which is especially important for opinion summarization.

Accordingly, we divide the system into two main parts, namely topic extraction and opinion summarization. Firstly, we extract topics for an entity from tweets containing the entity. Secondly, we identify the opinionated tweets with insight information to compose the opinion summary and organize the summary in accordance with topic extraction results.
In this paper, we propose to mine topics from #hash-tags. #hashtags are human annotated tags for providing additional context and metadata to tweets, which are used to categorize messages and to highlight topics. We use #hashtags as candidate topics. We conduct a comprehen-sive study on #hashtags and categorize #hashtags accord-ing to their usage in Table 1. In this paper, we focus on category keywords , entities as well as events and is-sues . We develop a rule-based classi er to identify the #hashtags of these types. Speci cally, for category key-words #hashtags, we collect a category dictionary from the Open Directory Project (http://www.dmoz.org/). For per-son/location #hashtags, we collect a dictionary from Free-base (http://www.freebase.com/). For events/issues #hash-tags, we rst use a bi-gram language model segmenter 3 to split #hashtags into multiple words. And then we check if some of these words are in our person/location dictionary. If one or more words are in our dictionary, this #hashtag tends to be an event or issue, #LondonRiot and #Occupy-Chicago for instance. For the remaining #hashtags, we use tagness 4 to determine if they are candidate topics. Tagness for a #hashtag is de ned as the occurrences of this #hashtag divided by the total occurrences of its content, i.e. without # symbol. For #hashtag may contain multiple words, when computing tagness, we also segment the words and count the occurrences of the segmented word sequences. When the tagness of a #hashtag is smaller than a threshold we set empirically(0.85 in this paper), it will be one of our candi-dates topic #hashtags. With tagness measurement, we can remove many #hashtags in the other 4 categories, such as #fb and #tcot, since they are always used as #hashtags.
In this paper, we model the task of extracting topics as clustering #hashtags into coherent groups. Particularly, we h ttp://norvig.com/ngrams/ http://energy.twex.poeschko.com/metrics/ create a weighted undirected graph G = &lt; H ; E ; f &gt; . the node set, and each node in the graph is a #hashtag; E the edge set, and f ( e = ( h i ; h j ) 2E ) !R , 1 i; j j the weight function to measure the relatedness between the two nodes (i.e. #hashtags), h i and h j . Then, we run Anity Propagation [9], a state-of-the-art clustering algorithm on G . The input of the Anity Propagation clustering algorithm is the #hashtags pairwise relatedness matrix, and the output are the #hashtags clusters and the centroids of clusters.
As for the #hashtags relatedness, we consider three kinds of metrics,namely co-occurrences, context similarity and dis-tributional similarity via weakly supervised topic models. f (( h i ; h j ) 2 e ) =  X  X  ( h ) is the centroid context vectors of #hashtags h , which can be obtained from the output of the Anity Propagation algorithm.

Co-occurrences metric discards words that are not #hash-tags; context similarity metric assigns equal importance to every word and every #hashtag occurred in a tweet. In order to obtain better representation and interpretation of #hash-tags, we use Labeled LDA [23] to learn #hashtags-words correspondence, which associates individual word in a tweet to appropriate #hashtags. We model the #hashtag(s) in a tweet as the label(s) of the tweet and other words as the doc-ument for Labeled LDA. We can thus obtain the #hashtag-word distribution, which is similar to topic-word distribution in LDA. Then, we use negative symmetric KL divergence of distribution over words as the similarity between #hashtags. We use Labeled LDA instead of supervised topic model [3], since Labeled LDA can handle multiple-label cases. where p ( W h i ) and p ( W h j ) are the word distributions of h and h j , respectively.
Once we obtain the topics from the previous step, we la-bel each topic with a representative #hashtag. We use the topic centroids output by Anity Propagation as the topic labels. We assign one or more suitable topics extracted to each tweet. Firstly, for a tweet with #hashtag(s), we assign it the topic(s) corresponding to every #hashtag in the tweet. Hence we may assign multiple topics to a tweet. Secondly, for a tweet without #hashtags, we predict its topic using a SVM classi er trained on tweets that have been assigned topics in the rst step. We use bag-of-words features for the classi er.
We use the insightful opinionated tweets to compose the opinion summary. In the following sections, we rst de-scribe insightful and opinionated tweets classi cation and then present an optimization framework to integrate topic , sentiment and insight to generate the opinion summary.
It is dicult to come to an universal and formal de nition of insightful tweets. For our application, we de ne 7 types of insightful tweets and list them in Table 2, based on the reason that all these types of tweets provide deeper insights and are more useful to the readers.

As a baseline, we use a binary classi cation approach to select the insightful tweets. In particular, we use Lib-SVM [5] with bag-of-words features and linear kernel. How-ever, based on our observation of the tweets dataset, only about 10% of the tweets can be regarded as insightful opin-ionated tweets, which indicates this is an imbalance dataset. In order to alleviate the data imbalance problem, we increase the weight of positive instance [2], but the classi er's per-formance does not become better.

Alternatively, we can use the patterns de ned in Table 2 to identify insightful tweet. A naive pattern matching method is to manually create a pattern list and match the pattern strings against the candidate tweets, and then we predict the matched tweets as insightful. However, this straightfor-ward pattern matching method causes two problems. First, given a tweet containing multiple entities, it makes the same prediction for di erent entities, irrespective of the syntactic relations between the pattern and the entities. Second, the coverage of manually created pattern list is low.
Therefore, instead of straightforward pattern matching, we use a syntactic-constrained pattern matching approach. We rst use Stanford Parser 5 to parse pattern phrases to obtain pattern syntax trees. And then we parse tweets to obtain tweets syntax trees. Finally we match the pattern syntax trees against the pattern syntax trees. To eciently create a high coverage pattern set without hurting preci-sion too much, we adopt an automatic pattern generaliza-tion paradigm. We rst de ne an initial set of patterns and then generate new patterns for each pattern in the initial pattern set. For example, given a pattern \that is why", we generate two new patterns \which is why" and \this is why" and they share the same meaning. To avoid gener-ating grammatically incompatible patterns, we also enforce the syntax constraint, i.e. the pattern generated should have the same syntax tree as the original, in order to ensure the pattern and the new pattern(s) mutually substitutable in the syntax tree. Therefore we use a paraphrase generation h ttp://nlp.stanford.edu/software/lex-parser.shtml algorithm [4], which output patterns having the same syn-tactic structures to the input patterns by applying syntactic constraints to the underlying phrase extractions and para-phrasing substitution approaches, to generate new patterns.
The opinion summary consists of opinionated tweets, i.e. tweets containing positive or negative sentiment regarding the entity. We thus need an entity (also called target) depen-dent sentiment classi er to identify the sentiment orientation (positive ( P ), negative ( N ) or neutral ( O )) of a tweet. In this paper we build the tweet-level sentiment classi er based on sentiment lexicon.

Our lexicon-based sentiment classi er relies on sentiment dictionary matching, or in other words, sentiment lexicon words counting. Speci cally, given a sentiment lexicon D , each word in D is annotated with a prior sentiment orienta-tion, either positive or negative. The lexicon based approach counts the occurrences of the positive ( c p ) and negative ( c words and determines the sentiment orientation of a sen-tence ( s ) simply by aggregating c p and c n . However, many named entities contain sentiment words. As a result, we conduct named entity recognition (NER) on sentence, and sentiment words contained in any entity will not be counted.
Here, SO ( s ) denotes the sentiment orientations of the tweet s . We should pay additional attention to the negation phenomenon in lexicon-based sentiment analysis. Negation expressions in sentences might cause sentiment negation, i.e. reverse the sentiment orientation to its opposite side. Ba-sically, we adopt the categorization for negation as intro-duced in [7], which categorizes the negation words into two classes: content negation words (such as \not", \never") and function negation words (such as \eliminate", \reduce"). Ac-cording to both [20] and [7], to incorporate the two types of negators will bene t the accuracy of sentiment classi cation greatly. We take a simple and heuristic approach to tackling the negation problem in our lexicon-based sentiment analy-sis. We invert the local sentiment orientation of a sentiment word w 2D to its opposite orientation whenever a nega-tion expression neg is found preceding w and the distance in words between neg and w is smaller than a prede ned threshold (5 in our experiments).

It should be emphasized that we are working on target-dependent sentiment classi cation. For this purpose, be-fore we count the positive and negative sentiment words, we conduct a classi cation to determine whether the sentiment word ( w ) is used to depict the target ( e ). This is achieved Ca tegory Example Example Patterns
Rea soning tweets \And that is why I voted for President Obama. No
Ap peal tweets \Please re-elect President Obama. Dont let these
Ca usal tweets \Obamacare was, is, and always will be a fatally
Co mparative tweets \Pinocchio is looking like an honest guy compared
Vi ewpoint tweets \I wonder how many followers Obama will have after by a binary SVM classi er which predicts whether the sen-timent of e should be dependent on w or not. We design a set of features to facilitate the classi cation as follows.
For a given entity e , we only consider the sentiment words w which are classi ed as dependent to e in the lexicon based sentiment analysis approach as described in Equation 4.
By using the techniques described in previous sections, we can already obtain insightful opinionated tweets for each topic towards the given entity. However, these tweets can not be directly output as the opinion summary. First, Many of them are dicult to understand. Unlike news report and other formal text, people use slangs, abbreviations and emoticons extensively in Twitter. Tweets containing too many informal symbols and words are unreadable and should not be included in the opinion summary. Second, they con-tain highly redundant information, since retweeting is pop-ular in Twitter, which make many tweets extremely similar or even identical.

Consequently, we use an optimization framework to se-lect the most relevant, representative and readable tweets as the nal opinion summary. The framework considers three factors simultaneously, namely, relevancy, redundancy and readability of tweets. Intuitively, we model this problem as h ttp://www.ryanmcd.com/MSTParser/MSTParser.html selecting a subset of tweets from all tweets, so as to minimize the information loss in discarding other tweets. Formally, we de ne the tweets selection problem as selecting a subset of tweets P from tweet set T k for topic k to where D ( t; p ) is the cost function of representing tweet t with tweet p . This is an NP-hard problem in general, but POPSTAR [6, 24] algorithm can be used to solve this prob-lem approximately. We de ne the cost function for opinion summarization as where t i and t j are two tweets and k is the topic index. L ( t i ) is the language style score of t i . We favor the more readable tweet.
 T ( t i ) is the topic relevance score of t i , de ned as KL diver-gence between term distribution of t i and topic label l k Re ( t i ; t j ) is the redundancy score between tweet t i de ned as KL divergence between word distribution of t i and t j .
 The nal output from our optimization framework are sev-eral representative opinionated insightful tweets per topic. Note that we use a asymmetrical cost function here, D k ( t  X  = D k ( t j ; t i ). As a result, the optimization procedure will au-tomatically select the direction with less cost, hence it tends to keep the more readable and relevant tweets.
In this section, we present the evaluation results on a real-life data set. We rst provide thorough experiments on each component, i.e. topic extraction, insightful tweet classi -cation as well as opinionated tweet classi cation, and then we present the evaluation results on the nal opinion sum-maries.
We collect the evaluation data set from 15th September 2011 to 20th October 2011 using the Twitter API 7 . We mainly focus on two types of entities, people (e.g. politi-cians and celebrities) and brands (e.g. electronic devices company and software company). We select 6 entities, 3 for people and 3 for brands. The entities include, \Obama", \Lady Gaga", \David Cameron", \Microsoft", \Apple", and \Nokia". We use these entities as queries and continuingly crawled tweet mentioning them to build the evaluation cor-pus. We do not use entity normalization techniques here be-cause the amount of tweets is enormous and we can collect enough data simply through string matching. We collected 201, 234 tweets in total. Table 3 shows the statistics of our data set.

As a preliminary study, we investigate the e ectiveness of using #hashtags as topics. We count the number and the percentage of tweets containing zero, one or more #hash-tags, and we nd out that about 23% of tweets contain #hashtags, and 8% of tweets contain more than 1 #hash-tags. These #hashtags can cover a large proportion of the topics in Twitter.

Furthermore, we examine the feasibility of using #hash-tags to extract topics. We obtain 15,865 unique #hashtags and 83, 713 #hashtags from the evaluation corpus. Then, we sample 100 #hashtags from the set of the 15,865 #hash-tags and manually examine the tags. We discover that 75 #hashtags can be used to represent topics, which supports our proposal of extracting topics from #hashtags.
The aim of the experiments in this section is to evaluate the e ectiveness of topic extraction from tweets using the graph-based methods. In particular, we evaluate the perfor-mance of topic clustering and topic assignment.

The evaluation of topic extraction is challenging because it is dicult to collect the \gold standard" data set. Though it is possible to identify and label topics from tweets manu-ally, the time cost is prohibitive. Therefore we conduct our evaluation by comparing the topics generated by our meth-ods and baselines.
Here we evaluate the e ectiveness of topic clustering. The aim is to assess the coherency of the produced #hashtags topics. We conduct the tokenization on the evaluation cor-pus with ark-tweet-nlp [10], and then remove stopwords, numbers (1.2, 100, $5 etc.), URL and words starting with \@" (accounts in Twitter). Finally we use Porter stemmer 8 to stem the words and #hashtags.

We run the Labeled LDA implementation in the Stanford h ttps://dev.twitter.com/ http://tartarus.org/martin/PorterStemmer/ Topic Modeling Toolbox 9 with the default settings to ob-tain the #hashtag-word distribution from the preprocessed evaluation corpus. Then we feed the distribution into the Anity Propagation algorithm 10 and produce the #hash-tags clusters.

We evaluate the clustering result by purity [17]. Table 4 shows the result. From the table, we can clearly nd that Co-occurrence and Cosine method produce more clusters and smaller clusters than the Labeled LDA method. This indicates that Co-occurrence method and Cosine method can not e ectively model the relation between #hashtags and consequently are not capable of nding strong relation information to group similar #hashtags. Moreover, the dis-tributional similarity approach (Labeled LDA) can greatly improve the performance.
We evaluate the e ectiveness of our topic assignment ap-proach in this section. We sample 100 tweets for 3 topics per entity as the evaluation data set, and collect a corpus of 1,800 tweets in total.

We report the topic assignment accuracy, the number of tweets correctly assigned divided by the total number of tweets, in Table 5. The topics are generated from the Label LDA approach, which gives the best topic extraction results as we have stated in Section 6.2.1. We notice that the ac-curacy for the singer Lady Gaga is substantially lower than that of politicians. Examining the dataset, we nd that the tweets about Lady Gaga contain more noises.
In this section, we present the evaluation results of opinion summarization. First, we evaluate the performance of iden-tifying insightful tweets and opinionated tweet classi cation. Then, we assess the quality of opinion summary.
To evaluate the e ectiveness of insightful tweets classi -cation, we annotate 300 tweets per entity and obtain a test set of 1,800 tweets.

We conduct experiment on both the SVM baseline and the pattern matching approach. We train the SVM baseline with 100 tweets as training data (14 are insightful tweets). We experiment two setting of our pattern approach. In the rst setting, we pick several pattern phrases for each type of insightful tweets and in the end we collect a pattern set of 20 patterns phrases. In the second setting, we use the same pattern phrase set but use the syntactic-constrained para-phrase method to conduct pattern expansion. After pattern expansion, our pattern set expands from 20 to 187. We eval-uate the performance of pattern approach without general-ization (Pattern man ) and pattern approach with generaliza-tion (Pattern gen ) respectively. We report the performance of these three methods in Table 6. The precision of baseline classi cation is very low, owing to the data imbalance prob-lem. Meanwhile, patterns selected by human tend to have high precision but very low recall. After pattern expansion, recall increases by 13% while precision only decreases by 4%. This indicates that our pattern expansion method is e ective. h ttp://nlp.stanford.edu/software/tmt/tmt-0.3/ http://scikit-learn.sourceforge.net T able 6: Performance of insightful tweets classi ca-tion T able 7: Performance of the target-lexicon depen-dency classi cation T able 8: Performance of the opinionated tweet clas-si cation
In this section, we evaluate the performance of opinion-ated tweet classi cation as described in Section 5.2. We also present the evaluation result for the dependency classi ca-tion between target and sentiment word.

In our experiments, we employ the sentiment lexicon from the General Inquirer 11 . We manually annotate 500 tweets which are sampled from the tweets containing the 6 entities as mentioned in Section 6.1. We ask two annotators to con-duct the annotation. Each annotator is asked to annotate each tweet as positive, negative or neutral, given the speci c target (entity). We also ask the annotators to annotate the dependency between the sentiment word and the target in the tweet. Finally, we obtain 420 tweets, each of which is annotated with the same sentiment label by the two annota-tors. This indicates the inter-agreement of the annotators is 84%. We use the 420 tweets as our evaluation set for opinion-ated tweets classi cation. The evaluation set contains 203 positive tweets, 126 negative tweets and 91 neutral tweets.
Table 7 shows the precision, recall, and F1-Score on 5-fold cross valuation of the target-lexicon dependency classi-cation. As we can see, it yields very encouraging results. h ttp://www.wjh.harvard.edu/ inquirer/ Table 8 shows the results for target-dependent tweet-level sentiment analysis results. Since the evaluation data set is also used in the target-lexicon dependency classi cation, we report the results using the same 5-fold cross validation strategy here. The system and baseline method in Table 8 indicates whether we incorporate the target-lexicon depen-dency classi cation into the lexicon based sentiment clas-si cation approach or not. As can be seen from the table, the target dependent sentiment classi cation can greatly im-prove the performance. Especially, the precision has been signi cantly improved, which is very important in the con-text of our work because there will always be huge amount of tweets available and thus we favor precision over recall.
Here we evaluate the e ectiveness of our method to gen-erate opinion summary in Twitter. To build the evaluation data set, we assume that the topics given by the topic ex-traction procedure are accurate and select 3 topics for each entity, and then for each topic we select 100 tweets as the corpus for opinion summarization. Then we manually se-lect 5 insightful opinionated tweets from those 100 tweets for each topic as the \ground truth" opinion summary of that topic. Totally we have an evaluation set of 18 opinion summaries on 1800 tweets.

We adopt ROUGE-SU4 measure to evaluate the nal sum-mary produced [15]. For comparison, we also experiment a setting of our approach which ignores the language style, i.e. L ( t i ) always equals to 1. We compute the average precision, recall and F1-Score of 3 summaries of each entity and report results for both settings in Table 9. The results indicate that employing linguistic knowledge improves the opinion sum-mary quality. We conclude that language style is vital for opinion summarization in Twitter.
To provide deeper insight into the advantages of topic-oriented insight-based opinion summary, we use the opinion summary towards \Obama" as a case study. Table 10 shows the topic clusters extracted from the tweets, along with their topic labels. We select the #hashtags by its frequency in the clusters. From this table, we observe that the topic labels directly suggest the topics, and other #hashtags in the clus-ter are also tightly related to the topic. For example, the label of the rst topic is \#occupywallst", which immedi-ately suggests that it is a topic about the OccupyWallStreet movement. And other words in this cluster provide fur-
T opic label T opic #o ccupywallst #w allstreet #occupydenv # reobama #i ran #mi ddleeast #israel #nuclear #islam #o bama2012 #h ermancain #rickperri #energi #vote T able 10: Topic cluster for \Obama" with our method
T able 11: Topic cluster for \Obama" with LDA ther information such as the event location (#wallstreet), the slogan (# reobama). The topics extracted are easy to interpret and understand.

For a comparison with popular topic extraction methods, we also run LDA on the same dataset and generate Table 11 with K = 12, the same as the number of topics found by our method. We use the LDA implementation in Stanford Topic Modeling Toolbox with the default setting. From the clus-ters we select 3 example clusters that describe approximately the same topic produced by our method. Topic 1 conveys the same amount of information with more words. In partic-ular, they describe the location with two words, \wall" and \street". Topic 2 in LDA contains many verbs, while the corresponding topic 2 in our approach just contain nouns and/or noun phrases and are easier to interpret. Compar-ison of Topic 3 demonstrates superior performance of our approach. Topic 3 in LDA contains some common words such as \becaus", \last" and \fast", while our approach select readable topic #hashtags. Moreover, our approach directly provides a topic label to represent the main idea of the topic, which is not available when using LDA.

In Table 12, we present the opinion summaries, consisting of insightful opinionated tweets for each topic. This sum-mary is a tight integration of topic, sentiment and insight. The patterns and clue words for identifying the insightful tweets are highlighted using the bold font. We select 4 rep-resentative topics for Obama, namely Libya issue, Occupy-WallStreet, HealthCare and jobs plan, covering both domes-tic and foreign a airs. We can gain many insights from this table. First, the 4 topic labels correctly and concisely sum-marize the 4 hot debated topics regarding president Obama. Second, on the topic of Libya issue, Obama is praised for saving American soldiers' lives for his intervention policy, though at the same time he is disputed on how he deals with the violence issue in Libya and United States. Third, for the other three topics, Obama is facing criticisms for his ine ective policy. Finally, it can be concluded that Obama receives more supports on foreign a airs than on domestic a airs. We also discover that people tend to provide more insights behind the opinions in their negative tweets.
From this case study, we see that the integration of topic, sentiment and insight provide a powerful way for summariz-ing opinions regarding entities.
In this paper, we present an entity-centric topic-oriented opinion summarization framework, which is capable of pro-ducing opinion summaries in accordance with topics and re-markably emphasizing the insight behind the opinions in Twitter. We decompose the opinion summarization into three dimensions, namely topic, opinion and insight; the opinion summary is generated by integrating these three dimensions as well as other factors (e.g. redundancy and language styles) in an uni ed optimization framework. We develop a template based method to e ectively identify the insightful tweets with high precision; we use target depen-dent sentiment classi cation to identify opinionated tweets regarding the entities. Extensive experiments are conducted to evaluate the performance of both the individual summa-rization components and the overall summarization results. We also present a case study of the produced opinion sum-mary, which further demonstrates the e ectiveness of the proposed opinion summarization framework and algorithms.
In the future, we will further study the semantics un-derlying #hashtags, which we can make use of to extract more comprehensive and interesting topics. We believe topic based opinions summarization will bene t from the deeper understanding of #hashtags.
This research was partly supported by National High Tech-nology Research and Development Program of China (863 Program) (No. 2012AA011101) and National Natural Sci-ence Foundation of China (No.91024009, No.60973053) [1] L. Barbosa and J. Feng. Robust sentiment detection [2] A. Ben-Hur and J. Weston. A user's guide to support [3] D. M. Blei and J. McAuli e. Supervised topic models. [4] C. Callison-Burch. Syntactic constraints on [5] C. Chang and C. Lin. Libsvm: a library for support [6] J. Cheung, G. Carenini, and R. Ng.
 [7] Y. Choi and C. Cardie. Learning with compositional [8] D. Davidov, O. Tsur, and A. Rappoport. Enhanced [9] B. Frey and D. Dueck. Clustering by passing messages [10] K. Gimpel, N. Schneider, B. O'Connor, D. Das, [11] M. Hu and B. Liu. Mining and summarizing customer [12] M. Hu and B. Liu. Mining opinion features in [13] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. [14] H. Kim, k. Ganesan, P. Sondhi, and C. Zhai. [15] C. Lin. ROUGE: A package for automatic evaluation [16] B. Liu. Opinion mining. In Encyclopedia of Database [17] C. Manning, P. Raghavan, and H. Sch  X  utze. [18] R. McDonald, K. Hannan, T. Neylon, M. Wells, and [19] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [20] T. Nakagawa, K. Inui, and S. Kurohashi. Dependency [21] B. O'Connor, R. Balasubramanyan, B. Routledge, and [22] B. Pang and L. Lee. Opinion mining and sentiment [23] D. Ramage, D. Hall, R. Nallapati, and C. Manning. [24] M. Resende and R. Werneck. A hybrid heuristic for [25] I. Titov and R. McDonald. A joint model of text and [26] I. Titov and R. McDonald. Modeling online reviews [27] X. Wang, F. Wei, X. Liu, M. Zhou, and M. Zhang. [28] L. Zhuang, F. Jing, and X. Zhu. Movie review mining
