 We investigate latent aspect mining problem that aims at automatically discovering aspect information from a collec-tion of review texts in a domain in an unsupervised manner. One goal is to discover a set of aspects which are previously unknown for the domain, and predict the user X  X  ratings on each aspect for each review. Another goal is to detect key terms for each aspect. Existing works on predicting aspec-t ratings fail to handle the aspect sparsity problem in the review texts leading to unreliable prediction. We propose a new generative model to tackle the latent aspect mining problem in an unsupervised manner. By considering the user and item side information of review texts, we introduce two latent variables, namely, user intrinsic aspect interest and item intrinsic aspect quality facilitating better modeling of aspect generation leading to improvement on the accuracy and reliability of predicted aspect ratings. Furthermore, we provide an analytical investigation on the Maximum A Pos-terior (MAP) optimization problem used in our proposed model and develop a new block coordinate gradient descent algorithm to efficiently solve the optimization with closed-form updating formulas. We also study its convergence anal-ysis. Experimental results on the two real-world product review corpora demonstrate that our proposed model out-performs existing state-of-the-art models.
 H.3.3 [ Information Search and Retrieval ]: Text Mining Topic Model; Sparse Coding; Aspect Mining  X 
There has been much research effort on extracting and mining information from review texts, such as sentimen-t analysis [24, 16], opinion summarization and identifica-tion [6, 3, 11]. But most of these models just target for the general sentiment analysis of review texts. In order to provide users more effective detailed insights of different re-views, it is necessary to detect more fine-grained information of the items. To address this task, aspect-based sentimen-t analysis has been conducted [22, 28, 7, 14] and it led to useful opinion summarization. Aspects are the common at-tributes or components of an item in a particular domain as exemplified by the aspects such as  X  X oom X ,  X  X ervice X  and  X  X ocation X  of the hotel. On the E-commerce web sites such as Amazon and eBay, users usually write a piece of review text and provide an overall rating for the reviewed item. However, we do not know user X  X  ratings on each aspect, i.e. aspect ratings.

The latent aspect mining task investigated in this paper takes as input a collection of review texts in a particular do-main together with a numerical overall rating of each review. The goal is to discover a set of aspects and predict ratings on each aspect for each review in an unsupervised manner. Also, the key terms for each aspect are detected. Note that the aspects are previously unknown and only the number of aspects is required. Figure 1 illustrates a sample hotel review. Such kind of review consists of some text content and an overall rating (e.g. 2-star). Suppose that a collection of such reviews and ratings information is available, and the number of aspects is provided. The aim of latent aspect mining is to discover the aspects including  X  X oom X ,  X  X alue X ,  X  X ocation X , etc. and predict user X  X  ratings on each aspect for each review, e.g. 1-star for the Room aspect and 2-star for the Value aspect. Some key terms such as  X  X tandard X ,  X  X win X  for the Room aspect,  X  X emote X ,  X  X ccessible X  for the Location aspect, etc. can also be detected. Recently, Wang et al. [26, 25] have proposed a model called Latent Aspect Rating Analysis Model (LARAM) that can tackle the laten-t aspect mining problem. They adopted the classical topic model Latent Dirichlet Allocation (LDA) [1] to model the generation of words in online reviews, and determine the aspect rating based on a rating regression component.
One limitation of probabilistic topic models such as LDA-based models is that they are ineffective when dealing with aspect sparsity in texts [31]. Aspect sparsity refers to the observation that the text content of most reviews only covers some aspects, rather than mentioning all aspects. In fact, it is quite common that real-world reviews exhibit aspect sparsity issue. For example, let us consider the hotel do-main with a set of aspects such as Value, Room, Location, Cleanliness, Food, Service, etc. For a particular review, a user typically comments on some aspects and not necessar-ily all the aspects. Another example refers to a particular hotel which is famous for its delicious food. It is more like-ly that a typical review of this hotel contains comments on its the Food aspect while some other aspects such as Value and Room. are not mentioned especially for short reviews. The main obstacle for traditional probabilistic topic models such as LDA in LARAM mentioned above to handle aspec-t sparsity is that topic or aspect proportions are modeled as normalized distributions, namely, the sum of each aspect proportion should be one, so applying a sparsity inducing l -regularizer as in lasso [20] is not helpful. As a result, some of the aspect ratings (e.g ratings on the Room aspect and the Value aspect in the example above) predicted by the probabilistic topic model may not be reliable.
Recently, non-probabilistic sparse coding techniques, such as the Sparse Topical Coding (STC) model proposed by Zhu et al. [31], can tackle the above sparsity issue. STC does not require the aspect proportion be the normalized distribution, so it is able to employ a theoretically sound l 1 -regularizer to control the aspect sparsity. However, one limitation is that it cannot be directly applied to tackle the latent aspect mining problem since we need to consider more latent variables such as aspect ratings. Incorporating these additional variables into the model may prohibit a closed-form updating formula, compromising computational efficiency especially for large data sets. Another issue of the STC model is that there is no convergence analysis reported for the block coordinate descent algorithm commonly used in Maximum A Posterior (MAP) estimation adopted by the model.

Another observation is that in practical situations, we can easily collect side information of the reviews such as user and item information. For example, it is easy to obtain all the reviews written by a particular user, or all the reviews asso-ciated with the same item. Such user and item information can be exploited to improve the latent aspect mining prob-lem. Existing models for this problem do not explore such information.

We investigate the latent aspect mining problem. The in-put data of this problem consists of a collection of reviews in a particular domain with a numerical overall rating associat-ed with each review. One goal is to discover a set of aspects which are previously unknown for the domain, and predict the aspect ratings for each review. Another goal is to detec-t key terms for each aspect. We propose a new generative model that can tackle the latent aspect mining problem in an unsupervised manner. It is capable of alleviating the aspect sparsity issue when predicting aspect ratings. Our proposed model, known as Sparse Aspect Coding Model (SACM), is a new model employing l 1 -regularizer to control the sparsi-ty on the aspect proportions. In addition, we consider user and item side information of review texts. Such information can facilitate better modeling of aspect generation leading to improvement on the accuracy and reliability of predicted aspect ratings. Specifically, we introduce two notions, name-ly, user intrinsic aspect interest and item intrinsic aspect quality , which are modeled as latent variables in our model. User intrinsic aspect interest captures the intrinsic interest for each aspect of a particular user. Item intrinsic aspect quality represents the intrinsic quality for each aspect of a particular item. In addition to aspect rating prediction, our proposed model is able to detect key terms for each aspect.
We make use of MAP technique to find the solution. In-stead of directly applying block coordinate descent algo-rithms as in STC, we first conduct analytical investigation on the MAP optimization problem and develop a new al-gorithm called block coordinate gradient descent algorithm with a closed-form formula to iteratively update the solu-tion. We also study its convergence analysis. This new al-gorithm allows our model to process the text data efficiently.
Experimental results on two different real-world product review corpora demonstrate that our proposed model out-performs existing state-of-the-art models.

Our contributions in this paper can be summarized as follows:
There have been much efforts on sentiment analysis for on-line reviews. One category is to determine whether a review is positive or negative [24, 17, 3]. Another category is to classify online reviews into multi-point sentiment scale [16]. However, all the above models above just conduct overall sentiment analysis and do not explore fine-grained aspects.
There have been some works on extracting aspect terms from review texts. Titov et al. [22] proposed a model called MG-LDA to automatically extract the ratable aspects. Muk-herjee et al. [15] applied the user provided seed words of a few aspect categories to jointly extract and cluster aspect terms by a semi-supervised model. Chen et al. [2] exploited the prior domain knowledge to generate coherent aspects. Some research efforts have been conducted on aspect-level s entimental opinion mining. Mei et al. [13] introduced sen-timent in discovering the facets and also positive/negative opinions. Later, Titov and McDonald [21] extended their multi-grain topic model to extract aspect-specific topics. Lin et al. [10] proposed a sentiment/topic joint model called JST to extract the aspect and its corresponding sentiment polar-ity. However, it is still not informative enough to identify the sentiment orientation or predict ratings on each topi-cal aspect of a particular item, especially for large review corpora. In [26], Wang et al. aimed at inferring the us-er X  X  ratings and also relative weights on each aspect based on the review text and overall ratings. To tackle this prob-lem, they have proposed two models. One model, known as Latent Rating Regression (LRR), models the overall rat-ing by applying two-fold linear regression model for aspect rating and aspect weight, based on the user specified seed terms for each aspect. Their second model, known as La-tent Aspect Rating Analysis Model (LARAM), is a unified generative model and it does not need to predefine the as-pect seed terms. Nevertheless, the above models based on probabilistic topic models fail to handle the aspect sparsity issue.

The sparsity-enhanced models have been widely used in different applications. Yang et al. [29] extended the popular spatial pyramid matching model and proposed a linear SP-M kernel based on SIFT sparse codes. Shashanka et al. [19] applied an entropic prior in Maximum A Posterior estima-tion to enforce sparsity based on the Probabilistic Latent Semantic Analysis. Zhu et al. [31] improved the traditional probabilistic models by incorporating the sparse coding idea to discover sparse latent representations for each document. Later, Zhu et al. [30] presented another model called Con-ditional Topical Coding which is enhanced by incorporating rich language features in text.

Recently, there have been some works on considering the user and item side information to conduct sentiment analysis for online reviews. Li et al. [9] explored the reviewer and product information to predict the overall rating of each review. Wang et al. [27] proposed a supervised topic model to label the prediction for each review with consideration of user and item information.
We provide the problem definition for the latent aspect mining problem investigated in this paper. The input of the latent aspect mining problem consists of a collection of review texts in a particular domain. For each review text, it is also associated with a numerical overall rating. One goal is to discover the set of previously unknown aspects for the domain and predict the ratings on each aspect for each review. It only requires to specify the total number of aspects. Another goal is to detect key terms for each aspect.
Reviews are written by users to share opinions about their reviewed product items. For a particular domain, the input review corpus is represented as D = { d 1 , d 2 , ..., d | D | use U = { 1 , ..., U } and H = { 1 , ..., H } to denote the user collection and item collection. Typically, we assume that the review d  X  D is written by the user u d  X  U for the item h d  X  H . Also, the overall rating, denoted by Y d  X  R + , is given by the user to express his overall satisfaction for the reviewed item. Normally, this rating value is a numerical integer value and it commonly ranges from 1 to 5 star.
An aspect represents the common attributes or compo-nents of the product item in a particular domain. For ex-ample,  X  X ervice X ,  X  X oom X  X spects for the hotel domain,  X  X la-vor X ,  X  X ocation X  for the restaurant domain, etc.. Let K be the total number of aspects in a particular domain. We use A = { 1 , 2 , ..., K } to denote the set of aspects that has been commented in the review corpus. Each aspect is denoted by k  X  A . An aspect rating is the user X  X  fine-grained rating on each aspect of the reviewed item, e.g.  X 3-star Service X  and  X 5-star Room X  for a hotel. For the review d  X  D , the aspect ratings are represented by a K  X  dimensional vector Y
We propose a new generative model that can tackle the latent aspect mining problem in an unsupervised manner. This model is capable of alleviating the aspect sparsity is-sue when predicting the aspect ratings. The aspect sparsity issue has been discussed in Section 1. Our proposed model, known as SACM, controls the sparsity of aspect proportions by means of l 1 -regularizer, and generates the aspect ratings by considering user and item intrinsic information. We in-troduce two notions, namely, user intrinsic aspect interest and item intrinsic aspect quality , which are modeled as la-tent variables in our model. User intrinsic aspect interest denotes the intrinsic interest for each aspect of a particu-lar user. This notion is different from the notion of aspect weight defined in [26]. Specifically, aspect weight represents the user X  X  emphasis placed on each aspect when the user de-cides the overall rating. It varies with different review texts. User intrinsic aspect interest is not item dependent while aspect weight is item dependent. For example, consider a foodie user who has a great interest on the Food aspect in the hotel domain. This user X  X  reviews will mainly comment on this aspect no matter for which hotel. Likewise, if a user has no interest on the Food aspect, his/her reviews do not likely mention this aspect. Item intrinsic aspect quality rep-resents the intrinsic quality for each aspect of a particular item, and it is not user dependent. For example, for a five star hotel, the intrinsic quality for most of its aspects will be superior than that of lower star hotels except for the price.
One characteristic of our proposed SACM is that the s-parsity of aspect proportion in a particular review can be handled more effectively via the modeling of user intrinsic aspect interest and item intrinsic aspect quality. In gen-eral, it can be observed that if a particular aspect is not mentioned in a review, it is essentially due to two main rea-sons. The first reason is that the user has no interest on that aspect. The second reason is that the concerned aspect of a particular item is not so distinctive that such aspect is normally ignored when a user writes the review for that particular item. Another characteristic of SACM is that the aspect rating is modeled by a Gaussian distribution with the mean related to item intrinsic aspect quality, and the variance related to user intrinsic aspect interest. It can be observed that the aspect rating of a particular item from a large number of users should attain an average value deter-mined by the item intrinsic aspect quality, and the variance of such aspect rating is related to the user intrinsic aspect i nterest. For example, in the hotel domain, when a foodie user, who is sensitive to the Food aspect, has written re-views for a number of different hotels, his ratings on the Food aspect of each hotel should exhibit some variations. The degree of sensitivity depends on his/her intrinsic inter-est. On the contrary, a user, who never cares about food, would exhibit much less variations in aspect ratings on the Food aspect in this user X  X  reviews. Thus, the variance of aspect rating is related to user intrinsic aspect interest.
In addition to aspect rating prediction, our proposed mod-el can also detect the key terms for each aspect. The learned (aspect) dictionary within our proposed model contains terms that are associated with each aspect together with the asso-ciation strength. Recently, Zhu et al. [31] have proposed a Sparse Topical Coding (STC) model for discovering the hidden topic repre-sentations from a collection of documents. Unlike traditional topic models, it can directly control the sparsity of the in-ferred representations by sparsity-inducing regularizers such as l 1 regularizer. Figure 2 depicts the graphical model of STC. Each circle represents a variable. The shaded circles represent the observed variables and non-shaded circles are the hidden variables to be inferred. The inner rectangle plate denotes the replication for the word in each document, and the outer rectangle plate is the replication for a documen-t. The arrows capture the dependency among the variables. STC models the observed text words in each document by latent variables including word code s , document code  X  and the dictionary  X  .

For a particular document d  X  D , the document code  X   X  R K + is a K -dimensional vector, where the component  X  represents the document X  X  association strength regarding the topic k . For example, the sample document code (0.5, 2.3, 1.4, 3.4, 0.0) indicates that this document mainly focuses on the second, third and fourth topic but hardly mention the first and fifth topic. Different from the topic distribution in traditional probabilistic topic models, the sum of each component of the document code does not require to be one, so l 1 regularizer can be applied to enforce sparsity for the document code, i.e. some components of the document code equal zero. Similarly, the word code s dn is also a K -dimensional vector, and the k th component s dnk captures the association strength on the topic k for the word n in the document d . The sum of the component in the word code does not need to be one as well. Hence, this word code for each word is also different from the topic assignment. In traditional probabilistic topic models, topic assignment just assigns each word to one of the predefined topics while word code can let each word belong to multiple topics with varying degrees.  X   X  R K  X  N + is a dictionary 1 with K bases
N ote that  X  is called a dictionary in the sparse coding area historically. It is different from the concept of dictionary in and the vocabulary size of N . It is a global matrix and document independent. Each row  X  k  X  represents an topical basis with a unigram distribution over the vocabulary V . In other words,  X  k  X  belongs to a ( N -1)-simplex. Essentially, the document d is projected to a semantic space spanned by the topical bases in the dictionary  X  .

Assume that V = { 1 , ..., N } is the vocabulary with N words. We model each document d  X  D as a vector ( w d 1 , ..., w dn d ) T , where n d = | I d | . I d is the index set of the appearing words in the document d and w dn , where n  X  I d , denotes the number of occurrences, namely the word count, of the word n in the document d . The basic generative process for the words in the document d  X  D is as follows: we first sample the document code from the prior p (  X  d ), and sample the word code s dn from p ( s dn |  X  d ) for each observed word n , where n is the word index in vocabulary. Finally, we sample the observed word count w dn from a distribution with s T as the mean, where  X   X  n represents the n -th column of  X  . The joint distribution is defined as: p (  X  d , s d , { w dn } n  X  I d |  X  ) = p (  X  d ) X Normally, the word count in each document is assumed to be sampled from a Poisson distribution. For the sparsity of  X  d and s dn , the document code is induced by the Laplace prior, and the word code is drawn from the supergaussian. The specific formulations are shown in Section 5.1.
STC employs the Maximum A Posterior (MAP) estima-tion method to infer these set of hidden variables. We repre-sent the collection of document code and word code as  X  and S , respectively, i.e.  X  = {  X  d } d  X  X  , S = { s dn } d  X  X  ,n  X  I hidden variable set can be represented as  X  = {  X  , S ,  X  } . The observed data is the text words { w dn } d  X  X  ,n  X  I d goal is to infer hidden variable set  X  conditioned on the ob-served data. The MAP objective function of STC can be formulated as follows: The block coordinate descent algorithm is usually employed to solve the objective function above.
As mentioned in Section 4, our proposed model, known as Sparse Aspect Coding Model (SACM), incorporates two latent variables, namely, user intrinsic aspect interest t item intrinsic aspect quality q h when modeling the observed review text and overall rating. User intrinsic aspect interest t for the user u  X  U represents this user X  X  intrinsic interest for each aspect. Item intrinsic aspect quality q h denotes the intrinsic quality of the item h  X  H for each aspect, which is user independent. More description for these two notion-s can be found in Section 4. The generative process is as follows: One would first choose the subset of all aspects for giving comments and decide the text proportion for describ-ing each aspect based on the user intrinsic aspect interest t and item intrinsic aspect quality q h . Then, some terms including opinionated words would be selected to form the review content. The details of the generation process of a the IR community. To avoid confusion, we call it  X  X spect d ictionary X  instead of  X  X ictionary X  in this paper. word will be described below. Next, the sentimental ori-e ntation for each aspect characterized by the aspect rating is determined. Finally, the observed overall rating given by this user will be based on the weighted sum of aspect ratings. The graphical model of SACM is depicted in Figure 3. The outer rectangle plate represents the replication for a review. The inner rectangle plate captures each word in each review. There are two components in this model. The first component shown on the lower left is related to the review text content component including  X  d , s dn and w dn The second component shown on the upper right is related to the rating mining component.

We first describe the review text content component which uses a variant of STC mentioned in Section 4.2 to generate the observed words. For a particular review d  X  D written by the user u d  X  U for the item h d  X  H , the document code  X  d is modeled as the Hadamard product between the user intrinsic aspect interest t u d and the item intrinsic as-pect quality q h d instead of Laplace prior. Precisely, the k th element of the document code  X  dk represents the association strength on the aspect k . Also, the more the word occur-rence over the k th aspect, the higher the value of  X  dk is. Specifically, the dominated aspect proportions in a review mainly depend on the corresponding t u d and q h d . For in-stance, in the hotel domain, a user who likes delicious food will have high t u d k where the aspect k is the Food aspec-t. This user likely provides opinions on food in detail in his/her reviews leading to a high value of  X  dk . Addition-ally, a hotel possessing distinctive environment, i.e. q hk high where k is the Environment aspect, is likely to draw attention from users by its environment. Thus, it tends to attract some comments on this aspect. As a result, the cor-responding  X  dk also has a high value. The above examples show us that both t u d and q h d contributes to  X  d . Based on the above motivation, we use Eq. (3) below to generate the aspect proportion, which is modeled by the document code  X  for review d , where the operator  X  is the Hadamard product, which is defined as the entry-wise product between the vector t u d and the vector q h d .

It is reasonable that the user intrinsic aspect interest t u  X  U is drawn from the Laplace prior, i.e. p ( t u )  X  exp (  X   X  k t ).
 Specifically, a user usually will not be interested in all pos-sible aspects of a particular item. Then, we use the STC model to generate the observed review text. After obtaining the document code  X  d , we sample the word code s dn from p ( s dn |  X  d ) for each observed word n , where n is the word index in vocabulary, and sample the observed word count w dn from a distribution with s T dn  X   X  n as the mean, where  X  represents the n -th column of  X  . Unlike the multinomial dis-tribution adopted in traditional probabilistic topic models, for the sparsity of word code, s dn is drawn from the super-gaussian as shown below. The l 1 -norm within them tends to find sparse codes.
Then, the word count in each document is sampled from the Poisson distribution p ( w dn | s dn ,  X  ) = P oiss ( w
In the rating mining component, we define the aspect weight represents the user X  X  relative weight placed on each aspect when the user decides the overall rating for a partic-u lar review. For the review d , we assume that aspect weight  X  d  X  R K ++ is generated by the document code  X  d , which denotes the aspect strength in each aspect. After normal-ization, we have each element of  X  d as follows:
For a review d written by the user u d for the item h d , we assume that the k -th element of the aspect rating Y A drawn from a Gaussian distribution. The mean and variance are assumed to be q h d k and  X  2 t 2 u a positive scaler.
 Consequently, the ratings on the k th aspect Y A dk from all reviews for a particular item h d should attain the average value determined by the intrinsic aspect quality q h d of this item h d . For a particular user u , the variance for his/her aspect ratings should be related to this user X  X  intrinsic as-pect interest t u . For example, in the hotel domain, a foodie person is likely to write more about the Food aspect in the reviews, and this user would be more sensitive about the variation of the Food aspect in different hotels. Thus, he would give ratings on the Food aspect with higher variance. Another example is that a thrifty person would be more sen-sitive to the Price aspect and tends to provide a wider range of ratings for the Price aspect for different hotels. But for other aspects, this user does not care much and the ratings on them would exhibit much less variance.

Finally, as the generative process mentioned above, we assume that the overall rating Y d of the review d is drawn from a Gaussian distribution. The weighted sum of aspect ratings  X  T d Y A d is the mean and c 2 is a fixed variance, i.e. Y Since the user intrinsic aspect interest is modeled by a Laplace prior, we employ the Maximum A Posterior (MAP) to estimate all the latent variables in this model. Let T and Q be the collection of user intrinsic aspect interest and item intrinsic aspect quality respectively, i.e. T = { t u } u  X  X  { q h } h  X  H , and we represent the collection of word codes and aspect ratings as S = { s dn } d  X  D,n  X  I d and Y = { Y A respectively. Our goal is to infer the latent variable set  X  where  X  = { Y , S , T , Q ,  X ,  X  } . The objective function is the negative logarithm of the posterior p ( X  |{ w dn , Y d } d  X  X  ,n  X  I Combining (3) to (6), and the review text content compo-nent, the optimization problem based on MAP estimation is given as follows: min s.t. t u  X  0 , q h  X  0 , s dn  X  0 ,  X  dk = exp (  X  dk ) where S ( N  X  1) represents the ( N -1)-simplex.
We utilize the dictionary  X  to detect the key terms for each aspect. For a particular aspect k , each row  X  k  X  represents the association strength of each term for the aspect k . We can rank the terms based on their association strength and treat the top terms as the representative key terms for the aspect k . In contrast with the STC model, our proposed model incorporates the overall ratings associated with each review text as input, so our learned aspect dictionary can be more informative.
We investigate the optimization technique for finding the solution for MAP estimation in (7). Note the such problem could be written in the following form, where k T k 1 = P u k t u k 1 , k S k 1 = P d P n  X  I f ( Y , S , T , Q ,  X ,  X  ) denotes the function that unifies all the other terms in the objective of problem (7).

A popular approach for solving optimization problem of form (8) is the block coordinate descent (BCD) method. At each iteration of BCD, a single block (subset) of the whole set of variables is chosen to be optimized while fixing the re-maining variables, as used in STC [31]. However, our model is more complex such that for each subproblem of BCD, we are unable to find a closed-form solution. In other words, in our model, solving each subproblem of BCD would be of high computational cost. To remedy this issue, we introduce the block coordinate gradient descent (BCGD) method. Like BCD, BCGD is also an iterative algorithm starting with a specified initial point x 0 = ( Y 0 , S 0 , T 0 , Q 0 ,  X  iteration of BCGD, it first chooses an block B to be up-dated (in (8), B  X  { Y , S , T , Q ,  X ,  X  } ). Then it calculates a descent direction at the current point x = ( Y , S , T , Q ,  X ,  X  ) Algorithm 1 A lgorithm for Our Proposed Model SACM 2: repeat 3: for d = 1 to | D | do 7: end for 8: end for 9: for u = 1 to | U | do 11: end for 12: for h = 1 to | H | do 14: end for 15: Optimize over  X  : solve the gradient  X   X  f to obtain 16: Optimize over  X  : set the gradient  X   X  f to zero, we up-17: until certain convergence criterion is met with respect to the block B , denoted by d ( x ; B ). After the descent direction d ( x ; B ) is obtained, we update the vari-able by x new = x +  X  B d ( x ; B ). Here  X  B is the step size that could be determined by various searching rule, e.g. Armijo rule. Now the remaining question of BCGD is how to cal-culate the descent direction d ( x ; B ). Mathematically, it is given as follows: d ( x ; B ) := arg min  X  f ( x ) T d + 1 2 k d k 2 2 + r ( x + d ) where r ( x ) =  X  k T k 1 +  X  k S k 1 and F denotes the whole feasible region in (8). Though it seems complicated, the following proposition ensures that for problem (8), the de-scent direction d ( x ; B ) admits closed-form solutions for B  X  { Y , S , T , Q ,  X  } .

Proposition 1. Suppose v and x are given vectors in R n , then the optimal solution of the following optimization prob-lem is given by
Let  X  B f denote the partial derivative of function f with respect to block B . Then it is obvious that the descen-t directions d ( x ; T ) , d ( x ; S ) are obtained by solving opti-mization problem of form (10) with v =  X  T f ,  X  =  X  and v =  X  S f ,  X  =  X  , respectively. Moreover, the optimization for descent directions d ( x ; Y ), d ( x ; Q ) and d ( x ;  X  ) are all of form (10) with v =  X  Y f, v =  X  Q f, v =  X   X  f respectively and  X  = 0. Thus by Proposition 1, the updating scheme of block B  X  { Y , S , T , Q ,  X  } are all with simple implementa-tion and of low computational cost.

For the aspect dictionary block  X  , since its feasible re-gion is a simplex, we could not hope for a closed-form of its update. Instead, we apply the projected gradient descen-t method for solving (9) and use a linear algorithm [5] to perform the projection to the simplex.

Thus, we summarize our Block Coordinate Gradient De-scent for solving (7) in Algorithm 1. Our goal is to solve each latent variables including Y , S , T , Q ,  X  , and  X  separately assuming that the other variables are fixed in an alternate manner.

Moreover, the convergence of BCGD has been extensively studied in the optimization community. Specifically, for op-timization problems with the property that all non-smooth parts are of a block-separable structure, such as (8), both the objective value and the iterates generated by Algorithm 1 are guaranteed to converge to a critical point. We summa-rize the result in the following theorem and its proof could be found in [23].
 Theorem 1. Suppose { x k } is the sequence generated by Algorithm 1, and the step sizes are chosen by the Armijo rule bounded away from 0. Then the value of the objective function is nonincreasing and every cluster point of { x k a stationary point of Problem (8) . We carry out some experiments on two review corpora. One is the beer review corpus from a beer-rating web site BeerAdvocate 2 , which has been used in [12]. Another is the hotel review corpus crawled from TripAdvisor 3 , and orig-inally used in [26] and [25]. In the beer corpus, for each review, in addition to review texts, ratings are given on 4 aspects including Appearance, Aroma, Palate, and Taste. Furthermore, there is an overall rating for each review. All ratings range from 1 to 5 stars. In the hotel corpus, users are allowed to rate hotels on 7 predefined aspects in each re-view: Value, Room, Location, Cleanliness, Check In/Front Desk, Service, and Business Service, as well as an overall rating. All ratings range from 1 to 5 stars. In some reviews, there are several aspects not being rated and they are rep-resented by  X -1 X  instead of 1 to 5 stars. We call such kind of aspect rating as a non-existent aspect rating , and its corresponding aspect is non-existent aspect . Very often, a review text may contain only some and not necessarily all aspects. This issue is known as aspect sparsity as mentioned in Section 1. Some previous works such as [25] filter out such kind of reviews in their experiments since their models can-not handle aspect sparsity. In contrast, we retain such kind of reviews without removing them and form two data sets in our experiments. They are called  X  X eer X  and  X  X otel X  data set. Table 1 depicts the statistics of these data sets. The Sparse Ratio is defined as the fraction of non-existent aspect h ttp://beeradvocate.com/ http://www.tripadvisor.com/ ratings.
 where g d denotes the number of non-existent aspect ratings in the review d . D and K are the number of reviews and the number of predefined aspects, respectively.

By controlling the weight of l 1 regularizer, our model can also be applicable for data sets without non-existent aspect ratings. Therefore, in order to further investigate the ef-ficacy of our model, we also prepare two additional data sets deriving from the beer and hotel corpora without as-pect sparsity by removing reviews containing non-existent aspect ratings similar to some previous works such as [26, 25]. These additional data sets are called  X  X eer-nonsparse X  and  X  X otel-nonsparse X  as depicted in Table 1.

We perform pre-processing on these data sets including : (1) removing the punctuations, stop words from a stan-dard stop word list as in [8], and the terms whose count frequency is less than 5. (2) converting the words into lower cases. (3) stemming each word to its root form using Porter Stemmer [18].

We carry out the experiment on predicting the aspect rat-ings for each review to conduct quantitative evaluation. The numerical aspect ratings can be used as the ground-truth for the task of aspect rating prediction. Note that if a certain aspect rating exists but its corresponding aspect has not been mentioned in the review text content, then the rating cannot been regarded as a valid ground-truth information for the evaluation of aspect rating prediction. To ensure the validity of the ground-truth aspect ratings, we employ the Aspect Segmentation algorithm in [26] to segment each re-view. Aspect ratings which are not supported by the text content segments will be treated as non-existent aspect rat-ings. Besides, in order to align with the predefined aspects, we use a set of seed words for each aspect (e.g.  X  X riend X  and  X  X oncierg X  for the Service aspect) in the beer and hotel do-main as a prior to guide the text content component in our model, which has been conducted similarly in [25].
We initialize each word code s by the prior seed words, and uniformly initialize  X  , t , q and  X  . The aspect ratings Y d are initialized by its corresponding overall rating Y d For the parameter setting, we manually set  X  = 1 . 0, c = 0 . 1,  X  = 5 e  X  4 , and search for the most appropriate  X  and  X  both in the range of [0 . 1 , 1 . 0]. The number of aspects for the Beer and Beer-nonsparse data sets is fixed as 4 while we fixed the number of aspects to 7 for the Hotel and Hotel-nonsparse data sets.

Our quantitative experiments are conducted in three tri-als. In the first trial, all the models will be evaluated on the  X  X eer X  and  X  X otel X  data sets. The Beer data set is much larger than the Hotel data set. In the second trial, we e-valuate all the models on the  X  X eer-nonsparse X  and  X  X otel-nonsparse X  data sets. Note that our proposed model can be easily configured to generate numerical aspect ratings w ithout non-existent aspect rating by means of controlling the weight of l 1 -regularizer. In the third trial, we examine the performance of non-existent aspect identification for our model. Finally, we also perform some qualitative experiment on user and item characterization.
Similar to previous works such as [25], we make use of several metrics to measure the performance of our proposed model and all the comparing methods. Specifically, we use three groups of metrics to conduct quantitative evaluation. The first group of metrics evaluate the performance on all the reviews based on the aspect, including: (1) Mean Square Error (MSE) between the predicted aspect ratings and the ground-truth aspect ratings. It can evaluate the prediction accuracy. For the data set involving the aspect sparsity, we need to adjust the MSE metric as follows. Suppose that we successfully predict the non-existent aspect rating meaning that both the predicted aspect rating and the ground-truth aspect rating are  X  X on-existent X , MSE will be zero. On the other hand, if a model fails to detect  X  X on-existent X  aspect rating, the MSE will be penalized by our specified constan-t. In our experiment, this constant is 1.0. Note that if there is no non-existent aspect rating in the review data set, then this MSE is exactly the same as the standard MSE. (2) Pearson correlation of all the reviews (  X  a ). For an in-dividual review, this metric can evaluate the performance on preserving the relative order of aspect ratings; (3) Per-centage of failing to detect the best and worst aspect with-in reviews ( Mis a ); (4) nDCG of aspect ranking in all the reviews ( nDCG a ). For each review, we regard the ground-truth aspect ratings as the graded relevance to calculate the nDCG. Each of the first group of metrics is calculated by the average value over all the reviews. The second group of metrics evaluate the performance based on items, including: (1) Pearson correlation across all the items (  X  h ) measures the performance on maintaining the ranking order of aspect ratings for all items. Based on all the reviews comment-ing on each item, we calculate the average predicted aspect ratings and the ground-truth aspect ratings for each item to calculate  X  h ; (2) Mean Average Precision ( MAP h @10) measures the ranking accuracy for items. If each aspect is a query, after ranking by the ground-truth aspect ratings, we regard the top 10% of the items as the relevant answers. MAP h @10 evaluates whether we are able to preserve their top ranking positions if using the predicted aspect rating to rank them. Each of the second group of metrics is calcu-lated by the average value over all the items. For Beer and Hotel data set, when we calculate metrics except MSE, each non-existent predicted or ground-truth aspect ratings will be replaced by the mean determined by the existing aspect ratings in the same review.

The third group of metrics are used to evaluate the perfor-mance on the non-existent aspect identification, including: (1) Precision is the fraction of predicted non-existent aspect-s that are predicted correctly. (2) Recall is the fraction of non-existent aspects having being predicted correctly. (3) F1 score is the harmonic mean of precision and recall.
Note that for MSE and Mis a , the lower the value is, the better the performance is. For the remaining metrics, the higher the value is, the better the performance is. Table 2: Aspect rating prediction performance on t he data sets with aspect sparsity. For MSE and Mis a , the lower the value is, the better the perfor-mance is. For other metrics, the higher the value is, the better the performance is.
 Table 3: Non-existent aspect identification perfor-m ance on the data sets with aspect sparsity. For all the metrics, the higher the value is, the better the performance is.
 We conduct the aspect rating prediction of our model SACM comparing with LRR [26] and LARAM [25], which are two state-of-the-art models to do latent aspect mining problem. Since LRR model needs to apply topic models to identify aspects, in order to conduct fair comparision, sL-DA [4] model which is able to consider the overall rating is employed to identify aspects for each review in LRR.
The aspect rating prediction performance of different mod-els on the data sets with aspect sparsity is illustrated in Table 2, where we highlight the best performance for each metric. In general, for both Beer and Hotel data sets, our proposed model SACM outperforms two comparing meth-ods in all measures. In the first group of metrics, MSE denotes that SACM can achieve better prediction accuracy.  X  , Mis a and nDCG a are the aspect-based ranking metrics. The results show that SACM is able to better preserve the relative order of the aspect ratings within a review. In other words, our model can better answer the questions such as  X  X hat is this user X  X  favourite aspect? X  and  X  X oes this user prefer the Service than the Room of this hotel? X . In addition,  X  is relatively low for all the methods because our predicted aspect ratings are real values while the ground truth aspect ratings are all integers, leading to an over-penalty for the  X  metric. Instead, nDCG a is able to alleviate this bias and handles the integer tie cases well. In the second group of metrics,  X  h and MAP h @10 indicate that the performance of LRR and LARAM on the ranking of items is inferior in comparision with that of SACM.

Table 4 depicts the result of the second trial experiment on  X  X eer-nonsparse X  and  X  X otel-nonsparse X  data sets. It can be observed that our proposed model still outperforms the LRR and the LARAM in all measures.
For the data sets  X  X eer X  and  X  X otel X  with aspect sparsity, our model SACM is capable of identifying the non-existent aspect indicated by the user intrinsic aspect interest t . To evaluate the identification performance, we compare SACM with two different methods: Sparse Topical Coding (STC) a nd its supervised version MedSTC. These existing models only conduct non-existent aspect identification but cannot predict aspect ratings. STC can take a collection of reviews as input and identify the non-existent aspects of each re-view by the low association strength in the corresponding document code  X  . MedSTC improves the STC model by taking advantage of the overall rating associated with each review, and identifies the non-existent aspects similar with STC. Note that we assume if the association strength or user intrinsic aspect interest value of a certain aspect is less than 0.005, then we regard the aspect as a non-existent aspect. Table 4: Aspect rating prediction performance on t he data sets without aspect sparstiy. For MSE and Mis a , the lower the value is, the better the perfor-mance is. For other metrics, the higher the value is, the better the performance is.

With the same parameters setting mentioned in Section 6.2, we show the non-existent aspect identification performance measured by precision, recall and F1 score in Table 3. We can observe that SACM shows superior performance on non-existent aspect identification than all the comparing meth-ods, which is benefit from considering the user and item information into the modeling of aspect ratings and review texts. STC achieves a poor performance because of ignoring the valuable overall rating associated with each review. Be-sides, it can be observed that all the methods perform better on the Beer data set than that on the Hotel data set. It is mainly due to the reason that the fraction of non-existent aspect ratings (i.e. Sparse Ratio) of the Beer data set is less than that of Hotel data set implying that the users in BeerAdvocate are more willing to share detailed experience with others. Table 5: The detected key terms of some aspects in t he Hotel data set
Table 5 shows the detected key term lists of some aspect-Figure 4: Result of user characterization.  X  X ppear-a nce X  is denoted by  X  X pp. X  s, including Room, Cleanliness, Check In/Front Desk (i.e. CI/FD), Service, and Business service (i.e. BS), for the Ho-tel data set by SACM. It can be observed that each key term list can express the basic idea of its corresponding aspect. For example,  X  X ingle X ,  X  X oor X ,  X  X oom X ,  X  X tandard X , and  X  X indow X  appear quite common in the reviews provid-ing comments on the Room aspect. These terms are quite indicative to the Room aspect.
As we discussed before, the output user intrinsic aspect interest t u and item intrinsic aspect quality q h in the SACM can be used to characterize different types of users and items. Specifically, we apply the k-means clustering on the user intrinsic aspect interest t u for all the users u  X  U on the Beer data set and perform the same procedures for item intrinsic aspect quality q h , h  X  H .
For the clustering of user intrinsic aspect interest, we spec-ify the number of cluster as 6, and the average normalized user intrinsic aspect interest of each cluster is depicted in Figure 4. There are six types of users. Users in type (b) and type (c) represent the groups who have no obvious prefer-ence for different aspects of beers. When writing the reviews, these types of users always write detailed experience for each aspect. But other four types of users, namely, (a), (d), (e) and (f) have their own taste. For example, users in type (d) appear to be hardly interested in the Appearance of beer. When this type of user wants to buy a bottle of beer, he/she would not care about the appearance of the beers no matter how beautiful the appearance design is. On the other hand, from the perspective of beer merchants, they can provide personalized beer sales strategy for different types of users based on the result of user characterization.
For the clustering of item intrinsic aspect quality, we spec-ify the number of cluster as 5, and we show the average item intrinsic aspect quality of each cluster in Figure 5. It can be observed that the top item group possesses relatively bet-ter average aspect quality on each aspect than that of lower i tem group. We name them  X 5-star X  item to  X 1-star X  item. Users can make use of the result of item characterization to know the difference between different items at the aspect level, and choose the most appropriate item based on their own aspect interest. For example, based on Figure 5, when a user in type (a), who has no interest in the Taste aspec-t of beer, wants to buy a bottle of beer, he/she is able to see the main advantage of  X 5-star X  beer due to its Taste, and the quality of other aspects has little difference with  X 4-star X  beer. Hence, he would buy the  X 4-star X  beer for saving money. On the other hand, merchants can know the reasons why the items they sold are inferior than other items. For example, based on Figure 5, a  X 4-star X  beer seller can find that its main weakness is the Taste aspect in contrast with  X 5-star X  beers.
We propose a generative model to tackle the latent aspec-t mining problem. Our proposed model SACM can handle the aspect sparsity when predict the aspect ratings from a review text corpus. SACM applies l 1 -regularizer to control the sparsity on the aspect proportion and also takes user and item intrinsic information into consideration. Moreover, we conduct the analytical investigation for the Maximum A Posterior (MAP) problem used in our proposed model and develop a new block coordinate gradient descent algorith-m to effectively find the solution with closed-form updating formulas. Our experimental results on two real-world re-view corpora demonstrate that our proposed model SACM outperforms the state-of-the-art models.
