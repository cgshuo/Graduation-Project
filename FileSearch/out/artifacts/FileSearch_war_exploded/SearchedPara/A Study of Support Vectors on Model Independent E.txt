 the extent to which SVM training behaves like a model common machine-learning training databases, we compare fication task that may be solved using example-based learning methods. When developing classifiers using duce risk, the learning process can itself get computa-ident today, because there are complex classification problems waiting to be solved in many domains, where large amounts of training data are already available [9]. community [6, 5, ll] have therefore been trying to scale up classical inductive learning algorithms to handle ex-tremely large data sets. 
One popular approach for dealing with the in-tractability problem of learning from huge databases is to select small data subsets for training. Training with a small data sample from a large database requires less computation and memory resource. On the other hand, a small training set can omit important informa-tion captured in the original large data set, and hence lead to significantly poorer prediction results. Vari-ous data sampling methods have been studied [3] such as windowing [13], random sampling, stratified sam-pling, peepholing [5], and information-theoretic peep-holing [lo]. Catlett X  X  study [S] showed that certain sam-pling strategies helped in speed-ups, and improving the accuracy of classifiers in noise free domains. However, he also acknowledged that naive sampling methods are often not suitable for real world domains with noisy training data, where prediction results can degrade un-predictably and significantly. 
Boundary hunting methods like IB2 [l] seek out training examples near class boundaries because these examples tend to be useful for locating class boundaries precisely. Assuming only a small fraction of training examples in a large database lie along class boundaries, selecting small but highly informative training data subsets. However, examples selected by a particular boundary hunting method may be specifically tuned for use with an implicit classifier architecture, and hence may not work well for training other classifier types. 
An interesting theoretical issue to address is the extent to which one can select small but highly informative training data subsets from large databases in a model independent fashion. Model independence means selecting data that trains well with a wide range of different classifier types, such that prediction accuracy for each type is comparable to results obtained from training with the entire data set. From a practical standpoint, model independent example selection can be extremely useful for efficient data storage, because one can treat omitted data points as being redundant in a very broad sense, and can hence be safely and permanently discard from the database. 
Support Vector Machines (SVMs) are a recently de-veloped class of statistical learning architectures de-rived from the structural risk minimization principle [16, 41. Over the past few years, researchers have re-ported experimental results suggesting that the SVM training procedure exhibits certain promising model independent example selection characteristics for pat-tern classification. Specifically: 1) SVM training in-volves selecting a small subset of critical data points (known as support vectors) from the original training database. This critical data subset fully and succinctly defines the classification task at hand. 2) SVMs can be implemented with different kernel functions to sim-ulate a variety of standard classifier architectures (e.g., polynomial, Gaussian RBF and Neural Network clas-sifiers). SVMs trained with different kernel functions on the same database have been observed to select al-most identical data subsets as support vectors [15]. 3) 
SVMs trained with different kernel functions on the same database have given rise to similar high predic-tion rates on novel data [7]. 
It is thus tempting to treat SVM training as a form of model independent example selection procedure. This paper attempts to verify the model independent char-acteristics of support vectors, obtained via batch mode 
SVM training over the entire training database. Notice that in order for SVM training to be useful as an exam-ple selection procedure for extremely large databases, one also has to evaluate the model independent nature of data selected by incrementally applying and combin-ing SVM training methods on parts of the full database (refer to our other paper in these proceedings). 
Tjsing several common machine learning databases as benchmarks, we conducted the following two sets of experiments. For a few representative classifier types, compare the difference in prediction results of: training each classifier type with a full database versus training the same classifier with SVM selected examples. This study reveals how well support vectors preserve relevant class information in a model independent fashion. training each classifier type with SVM selected support vectors versus training the same classifier with data selected by two other selection methods: random sampling and IB2. This study compares the information-retaining quality of SVM selected data against data selected by other methods. 
Section 2 describes SVM theory and properties rel-evant to example selection. The detailed experimental procedure is given in Section 3, with observations and explanations discussed in Section 4. 
Support Vector Machines (SVMs) are a general class of statistical learning architectures that perform structural risk minimization on a nested set structure of separating hyper-planes [16]. For a classification problem, given 1 data points {(xl, yr), . . . , (xl, yl)}, SVM training involves solving a quadratic programming problem and the optimal solution gives rise to a decision function of the following form: 
To allow for decision surfaces other than hyper-planes, one can first non-linearly transform the set of input training vectors x1,. . . , xl into a higher-dimensional feature space using a map @(xi) ++ zi, and then do a linear separation there. Often, only a small fraction of the ai coefficients are non-zero. The corresponding pairs of xi entries (known as support vectors) and yi output labels fully define the decision function. Together with the Eli coefficients, they are preserved for use in the classification procedure. All remaining training examples may be redundant. 
We now discuss the SVM example selection proce-dure. Given a large input database, run the SV learn-ing algorithm using an appropriate kernel function I(, and keep the resulting support vectors as selected exam-ples. It is tempting to expect support vectors to exhibit the following characteristics desirable of sampled data: 1) High information content. Because the optimal decision function from SVM training depends only on examples that are support vectors, one may view the support vector set as a comprehensive data sample that fully defines the classification task at hand. 2) Rep-resentative of available and unseen data. This follows from the argument that SVM training is based on the structural risk minimization principle which at-tempts to minimize an actual risk error measure. 3) Model/platform independence. Fairly platform in-dependent, since different kernels have been reported to give similar support vectors. Examples selected should be good training data for different classifier architec-tures. Also implies choice of K may not be critical. The experiments in the next section attempt to quan-tify the extent to which all this is true. 
We are interested in studying the effect of using a subset of examples to train a classifier on its predictive accuracy. If the change in performance of various classifiers, when trained with a selected subset of examples is comparable, i.e. the increase or drop in performance of various classifiers is similar, then we will have empirical evidence to confirm that the selected subset is indeed model independent. This will in turn mean that the selection strategy used to obtained the subset is model independent. It might be that the selection strategy is model independent, but on using the selected subset, the predictive accuracy of various classifiers drops drastically, equally among all Table 1: The datasets used in experiments. 
Figure 1: Diagrammatic representation of the experi-mental procedure the classifiers. In such a situation, the selection strategy is of not much use either. So to provide evidence for model independence of any example selection strategy, we consider the following two criteria: [Cl]: The change in performance of classifiers should be comparable when trained with the the selected examples against when the classifiers are trained with all the examples for a given selection strategy 5 X . [C2]: If the predictive accuracy of any classifier drops upon using only the selected examples for training, such a drop in accuracy should not be significant. 
If we can obtain such a method, then it will be very valuable for scaling up classification algorithms: any classifier will work just as well with the selected examples, and the subset can be used to train various inductive learning algorithms efficiently and quickly. 
To verify the model independence of the selected ex-amples, and to study the effect on the accuracy of the classifiers trained with these examples, the following procedure was adopted (Figure 1 shows the steps): 1. A representative set of classifiers is selected, repre-senting various learning architectures. 2. These classifiers are trained with all the data using lo-fold cross validation method, and the average classi-fication accuracy on the test set, PA,ll is recorded. 3. Next, within the lo-fold cross validation method ex-ample selection is carried out on the the training set, separately using SVM, IB2, and random sampling, and the selected subsets are stored. 4. Finally, the classifiers were again trained with the selected examples, and the average predictive accuracy on the test set PA,,l is recorded. We chose to use the following classifiers as a representa-tive set (a) Multi-Layer Perceptron (MLP) [14, 21 -connectionist learning architecture. Simple backprop-agation algorithm was used to train the Neural Net-work.(b) Nearest Neighbor (l-NN) [8] -instance-based learning architecture. (c) C4.5 [12] -tree based classifier. (d) SVM [16] -optimal margin classifier. 
The datasets used for carrying out the empirical studies are listed in Table 1. The datasets are standard among the machine learning community, and obtained from the UCI-machine learning repository l. We also used SVM X  X ht 2 and IBL 3 in our experiment. 
Table 2 provides predictive accuracy of various classifiers (including SVM) when trained with all the training vectors. The column labeled  X  X ernel function X  shows the kernel used for the SVM training:  X  X  X  for gaussian, and L for linear. The numbers in the brackets show the number of examples saved by the classifier for classification stage. For SVM, it means the number of support vectors, for l-NN and IB2 the number of saved prototypes. Since l-NN stores all the examples from the training set, the number in the brackets under this column shows the average number of examples in the training dataset. In the case of C4.5 and MLP, they do not store the training vectors, and so there are no such brackets under their column. 
The support vectors selected by SVMs after training were stored as the selected subset. Similarly the prototypes by IB2 training were saved after training. 
The example selection process was also carried out using random sampling. For selecting examples by random sampling, the number of examples sampled was equal to the maximum of the number of examples selected by SVM and IB2. After doing example selection using the three strategies, the classifiers were again trained using only the selected examples, with lo-fold cross validation, and the average predictive accuracy on the test set PA,,1 was recorded. This predictive accuracy is shown in Table 3 for the classifiers. Since we are interested in the change in performance, it will be more instructive to look at difference in predictive accuracy (PA,,1 -P&amp;d. Table 4 shows these values, the negative numbers show a drop in performance on using just the selected examples for training, and the positive numbers would mean an increase in predictive accuracy. 
The SVM selection strategy performs poorly as a model independent example selector, whereas the random sampling strategy appears to provide best results -the average decrease in predictive accuracy is the least. The following can be observed from Table 4. 1. Which selection strategy is the best? (a) The random sampling selection strategy is the best in terms of model independence. (b) In spite of better performance in general, because of its random nature, random sampling can sometimes cause catastrophic decrease in accuracy, as can be seen the performance of MLP on the mushroom data when it is trained with randomly sampled subset. (c) Overall, among the three example selection strategies, SVM selected examples perform the poorest, and random sampling still holds the edge over other techniques. 2. Which learning algorithm is the most robust in using the selected examples? (a) MLP in general is the most robust of all the classifiers, irrespective of the selection strategy used to select its training subset. (b) C4.5 actually performs very well with training set selected using random sampling. In many cases, its predictive accuracy improves when trained with the randomly sampled training subset. However, its performance drops in large number of cases on using SVM, and IB2 selected examples for training. (c) For the majority of cases l-NN performs reasonably well when trained with IB2 selected, and randomly sampled examples, but its predictive accuracy drops drastically, when it is trained using SVM selected examples. (d) Conversely, SVM also performs very badly when trained with IB2 selected examples. Further it performs badly even when trained with randomly sampled examples. 
Valuable lessons are learned from these observations: (1) More often than not the random sampling strategy is a good choice for model independent example selec-tion. It is the most robust among the three strategies, yet simplest and fastest. (2) In spite of nice claimed properties of SVM, the use of SVMs for model indepen-dent example selection is not a feasible idea in general. (3) Although the instance based learning algorithms and SVMs hunt boundary data points, they vary vastly in their learning bias. As such using one as an example selection method to get a training set for training the other will often not work. (4) The observation (l.(b)) suggests a need for caution when using random sam-pling as a data reduction method. Sometimes, it could cause an adverse effect as was also found in [6]. 
The above evidence suggests that the model indepen-dence of the support vectors is limited to SVM formu-lation. In fact, the bias of the SVM formulation seems to be very different from the bias of the algorithms cor-responding to the SVM kernels. For random sampling, its very randomness lands it with advantage in gain-ing model independence. Its lack of bias means that in many cases, randomly sampled subsets can still give us a reasonable performance. On the other hand, this randomness also lends itself to being unreliable because there could be cases when the randomly selected exam-ples cause catastrophic loss in predictive accuracy. Starting from the verified claims about SVMs, we set to search for model independent example selection via SVMs. In a nut shell, our endeavor leads us to many valuable findings but we fail to achieve the original objective -a selection strategy that is better than random sampling. We ask where the discrepancy occurs between the previous findings about SVMs and our findings reported here. One sensible explanation lies in what is minimized in different learning algorithms. SVMs minimize the structural risk and the other algorithms only minimize the empirical risk. When one tries to minimize the structural risk, it is more likely that different SVMs end up having the similar support vector set for the same data set. When one only minimizes the empirical risk, many bias coming with various algorithms, influence the induction results. As such, examples selected by some other learning algorithms as the most suitable, may be different from the support vector set. 
In conclusion, the salient finding of this study is to realize the limits of SVMs and therefore we are one step closer towards our goal of model independent example selection, and able to suggest a general guideline for example selection facing a suite of selection strategies. 
PI [61 181 PI 
PO1 [Ill 
PI [161 
