 The World Wide Web has many doc ument repositories that can act as valuable sources of additional data for various machine learning tasks. In this paper, we pr opose a method of improving text classification accuracy by using such an additional corpus that can easily be obtained from the web. This additional corpus can be unlabeled and independent of th e given classification task. The method proposed here uses topic mode ling to extract a set of topics from the additional corpus. Those extracted topics then act as evaluation on the RCV1 dataset shows significant improvement over a baseline method. H.3.3 [Information Search and Retrieval]: Information Filtering Algorithms, Experimentation Text Classification, Topic Mode ling, Semi-Supervised Learning Nowadays one can easily obtain a document corpus discussing various real world topics from the internet. For example, Wikipedia (http://wikipedia.org/) and the web directories contain huge collection of documents discussing va rious topics of the world. In this paper, we show a method of utilizing such a corpus to improve the accuracy of common text classification tasks. Such a corpus is created independent of the given classification task. Traditional semi-supervised learning techni ques mostly assume the same distribution or model of generati on for the labeled and unlabeled data. Therefore, those techniques are unlikely to perform well when the unlabeled corpus is obtained independently. The method proposed here first obtai ns a subset of documents from from the unlabeled corpus using a topic modeling technique called Latent Dirichlet Allocation (LDA) [1]. These steps are independent of the classification tasks to be performed and therefore can be done once and offline. In our method, those extracted topics acts as additional features of the documents of the given classification task. That is, the documents of the given task are represented in a feature space consisting of the usual features (e.g. bag of words) and the topics extracted from the unlabeled corpus. The values of these topic features can be obtained by inferring the alignments of the documents towards those topics. Any off-the-shelf classification algorithm can then be used on this expanded feature space. The proposed method is based on the following hypothesis. The topics extracted by the LDA will resemble the real world topics discussed in the unlabeled corpus. Most text classification tasks are to classify the documents under r eal world topics (e.g. sports, business etc in news domain). Therefore, the alignment of the documents of the given classification task towards the extracted topics can provide useful hints to the classifier. The proposed method has two other major advantages in compared to many other techniques of incorporating an unlabeled corpus in a classification task. Firstly, since the extraction of the topics is offline and the inference of topic alignmen ts for a new document is not that costly, the online computation overhead is low. Although this method expands the feature space, as the number of extracted topics our experiment we used 200 topics which is almost negligible in compared to the tens of thousands of word features a text collection typically has. Secondly, the topics are used as additional features of the data. If those additional features are useless then a state of the art machine learning algorithm will most probably ignore them. This somewhat alleviates one of the concern often sited in the semi-supervised learning literature that incorporating unlabeled data can sometime hurt the performance. Earlier Gabrilovich et al [2] observed that expanding the feature space using Wikipedia can help in improving text classification accuracy. For a given document they retrieve set of pages from additional features of the corresponding document. The method requires a large encyclopedia corpus and expands the feature space in an ad-hoc and unrestricted ma nner. Also, given the successful application of LDA on images [4] our method can possibly be extended beyond text as well. Ne xt we briefly introduce LDA and then describe our experiments and the results. Latent Dirichlet Allocation (LDA) [1] is a generative model and assumes the existence of K latent class (topic) variables in the process of generation of the data. A ccording to this model a word in a document is generated by first choosing a topic from a multinomial distribution of the topics given the document (P(z |d); z generated from a multinomial distri bution of words given the topic (P(w |z)). The learning part of the model invol ves estimating the parameters of the model from a collection of doc uments. Given a new document, that is not in the collection, the multinomial topic distribution for the document is estimated using an inference technique. LDA provides systematic methods of learni ng and inference by assuming a Dirichlet prior for the topic distri bution of the documents. We used Gibbs sampling method [3] for lear ning and inference in the LDA. The advantage of the Gibbs sampling method is it is easy to implement and the inference for a new document quite fast. An iteration during the inference usi ng Gibbs sampling method involves computation of the order of the number of words in the new document and K. Typically 50 iterations are good enough for our purpose. In our method, the parameters are learnt from the unlabeled corpus. Then while performing a classification task, for each given serve as K new features for the document and the P(z=topic the value of the i th new feature. The unlabeled corpus in our e xperiment was constructed by taking 10000 random pages from Wi kipedia. The choice of Wikipedia was driven by the fact that the entire Wikipedia corpus can be downloaded as an XML. As no Wikipedia specific features are here used we believe that a corpus like a web directory can also be used as the unlabeled corpus. We used K=200, i.e. the number of topics in the unlabeled corpus is assumed to be 200. The evaluation was performed on the RCV1 [5] corpus. The corpus contains more than 800,000 news articles produced over 365 days. To keep the experiment time manageable we used 23,307 news articles of the first month (1996-08-20 to 1996-08-31). The evaluation was done on the binary classification tasks of 30 most common categories in the corpus, from CCAT ~47% down to C11 ~3%. The baseline method here uses standard TF*IDF based term vector representation of the documents. That is, each term (word) is a feature of the documents. Standard stop word removal and porter stemming we re also applied. In our method, 200 additional topic features are added to the documents. Note that these features are added to the training as well as test documents. The SVM implementation of the Weka library (http://www.cs.waikato.ac. nz/ml/weka/) is used as the base classifier that performs the classification task using document representation created by the baseline or our method. Figure 1 shows the comparison of the F-measures between the baseline and our method for the 30 categories. Here first 10% news articles are used as the training data and remaining 90% as the test data. In the Figure 1, for each category an arrow is drawn from the F-measure obtained by the baseline to the F-measure obtained by our method. The F-measure of our method here is the average of 10 runs with 10 different unlabeled corpora randomly created from Wikipedia. It can be seen that for most categories the F-measure has improved by our method. For some categorie s (e.g. GSPO, GPOL etc) the improvement is huge. Also, our method has no negative impact (no downward arrow). Figure 1 Baseline vs. Our Method on 30 RCV1 categories Figure 2 shows how the F-measure changes as we change the training data size from 10% to 50%. The F-measure here is the average F-measure of the 30 categories (and the 10 runs for our method). The interesting observation here is the improvement by our method is more when the training data size is less. Here we show a method of improving text classification accuracy using LDA over an additional co rpus. The method does not have any rigid assumption about the distribution or the model of generation of the additional corpus and therefore can make use of some corpora easily available on the web. Evaluation on several text classification tasks on the RCV1 corpus shows the usefulness of our method. Presently we are i nvestigating the impact of type, size and the number of assumed t opic variables of the additional corpus on the accuracy of a text classification task. [1] Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent [2] Gabrilovich, E., Markovitch, S. 2006. Overcoming the [3] Griffiths, T. L., and Steyve rs, M. 2004. Finding Scientific [4] Fei-Fei, L. and Perona, P. 2005. A Bayesian Hierarchical [5] Lewis, D., Yang, Y., Rose, T., and Li, F. 2004. RCV1: A 
