 In recent years, we have witnessed the wi dely recognized phenomenon of high speed data streams. A data stream is a massive real-time continuous sequence of data elements. The typical applications include sensor network, stock tickers, network traffic measurement, click str eams and telecom call records. The main challenge of these applications is that the data element arrives continuously and the volume of the data is so large that they can hardly be stored in the main memory (even on the local disk) for online processing, and sometimes the system has to drop some data elements due to the high arriving speed. The data in the traditional database applications are organized on the hard disk by the Database Management System(DBMS) so the queries from the users can be answered by scanning the indices or the whole data set. Considering of the characteristics of the stream applications, it is not feasible to simply load the arriving data elements onto the DBMS and operate on them because the traditional DBMS X  X  are not designed for rapid and continuous loading of individual data element and they do not directly support continuous queries that are typical of data stream applications [6]. Therefore, in order to support the emerging data stream applications, many works on data stream systems and related algorithms have been done by researchers i n various communities and it still remains an active research area nowadays.

As mentioned in [6], following characteristics make data streams different from the conventional relational models :  X  The data elements in the stream arrive online and the system has no control  X  Data streams are potentionally unbounded in size. The stream elements are  X  Once an element from data streams ha s been processed, it is discarded or there has been tremendo us progress in building Data Stream Manage Sys-tems(DSMSs) as evidenced by many emerging DSMSs like NiagaraCQ [17], STREAM [86, 6], statStream [98], Gougar [94], Aurora [14], Telegraph [59], Bo-realis [2], Gigascope [32] etc.

Besides the works of building Data Stream Manage Systems (DSMS X  X ), vari-ous data stream algorithms have been proposed from various communities like database, network, computation theory and multimedia. Among various compu-tations over data streams, the aggregate computation plays an important role in many realistic applications such as network traffic management system and sensor network. Following are some important aggregate queries studied in the network traffic management system:  X  How much HTTP traffic went on a link today from a given range of IP  X  Find out the number of flows ( with or without aggregation ) which exceed  X  What are the 1  X   X  ,1  X   X  2 ,1  X   X  3 ...1  X   X  k (0 &lt; X &lt; 1) quantiles of the  X  Identifying the superspreaders in the network. A superspreader is defined
The rest of the paper is organized as follows. Section 2 presents the compu-tational model of the data stream algorithms. Then three important aggregate computations over data stream are introduced in sections 3,4 and 5. Particularly, section 3 and section 4 give survey on the frequency mo ment and frequency count computation over data str eams respectively. In section 5, we first introduce the rank computation over data stream with uniform and relative error metrics. Then the top-k computation follows. Some future work on the aggregate computation over data streams are proposed in section 6. Because of the unique characteristics of data stream applications, following issues are critical for data stream algorithms: 2.1 Processing Time In many data stream applications, data elements arrive at a high speed so it is essential for the system to reduce the per record processing time. Otherwise the system might get congested and many el ements will be dropped without being processed since usually there is no enoug h space to keep all of the elements. The arriving rate might burst in some applications, so the consideration of buffering and load-shedding is also required. Likewise, the query response time is another critical issue as a short response time is one of key requirements in many real time data stream applications like network monitoring and stock data analysis. 2.2 Space Usage Since the size of the data stream is potentially unbounded, it is infeasible to keep all of the stream data elements. M oreover, although the processing time of secondary storage device has been sign ificantly improved in recent years, it might be unacceptable even the system simply keeps every incoming stream element. So many steam algorithms are confined to the main memory without accessing the disk. Consequently, only a synopsis of the data stream can be kept to support user queries. Usually the space used is at most poly-logarithmic in the data size. Sampling, histogram, wavelet and sketch are widely used techniques to summarize the stream data. 2.3 Accuracy It has been shown that in order to get exact answers for some important com-plex statistics like median and the number of distinct value, a linear space is required. As the synopsis maintained by the system must be very small in size, usually poly-logarithmic in the size of the data stream, the approximation is a key ingredient for stream algorithms. In many applications the exact answer is not crucial, so an approximate answer is sufficient. The system needs to make a trade-off between accuracy and storage space. Hopefully, the algorithm X  X  perfor-mance in terms of accuracy will decrease gracefully when there is less memory available. Suppose a data stream consists of elements { a 1 ,a 2 ,...,a m } which arrive se-quentially and a j is a member of U = { 1 , 2 ,...,n } .Let f i denote the number of occurrences of i in the data stream. The k -th frequency mome nt of the data set, denoted by F k , is defined by n i =1 f k i . Frequency moments play an important role in many applications as they can capture the demographic information of the data set. Particularly, F 0 is the number of distinct elements appearing in the data sequence and F 1 is the length of the sequence. While F 2 is the self-join size (also called surprise index) of the data set and F  X  is the maximal f i .In[4], Alon et al. present the seminal work to study the problem of frequency moment computation against data streams. It is shown that the exact computation of F k ( k = 1 ) needs space linear to the data se t size. A general frame work is proposed to estimate the F k in [4] and many works [21,62,47] have been done in the literature to improve the space(time) efficiency and theoretical bounds. The range efficient computation of frequency moment is studied in [11] and their result is improved by Buriol et al. in [13]. The problem of frequency moment computation over sliding window is studied in [35].

Compared with other F k s, much more attention has been given to the com-putation of F 0 because of its wide applications. Flajolet and Martin [46] develop the well known FM algorithm to estimate the F 0 of the dataset with one scan. A new algorithm is proposed by Durand and Flajolet [37] to improve the space complexity. As algorithms in [46, 37] assume the existence of hash functions with some ideal properties which are hard to construct in practise, Alon et al. build on similar technique but only require random pairwise independent hash functions. An adaptive distinct samp ling technique is developed by Gibbons et al. in [49, 48]. In [10], three new algorithms are proposed to improve the space and time efficiency of previous work. In the context of graphic theoretic applica-tions, Cohen [18] develops a size-estimation framework to explore the closure and reachability of a graph. The linear counting algorithm is proposed by Whang et al. in[92]basedonthe bitmap counting technique to estimate the cardinality in the database applications. The result is further improved in [41] based on an adaptive bitmap technique. Moreover, range effici ent algorithm for estimating F 0 on stream data is proposed by Yossef et al. in [11]. Recently, [1] improves the time efficiency of the algorithm. And the same problem is investigated under sliding window model by [35] and [50]. Frequency counting is one of the most basic statistics of a data set because it can mark the most influential part of elements, especially in the skewed data distribution. It has many applications including network monitoring, traffic man-agement, click stream monitoring, telephone call recording and sensor readings. Ideally, we would like to keep track of the top k elements with the highest fre-quency for desired value of k (top-k elements) or the elements with frequency exceeding a pre-given threshold ( heavy hitters ). For simplicity, we call them fre-quent elements. Exact computation of frequent elements against data stream in a small space (sub-linear to N ) is infeasible. Rather, various approximate algorithms are proposed to address this problem in the context of data streams.
Misra et al. [78] present the first deterministic algorithm for finding -approximate frequent elements, which uses O ( 1 )spaceand O (1) amortised processing time. Recently [36] and [65] i mprove the algorithm by reducing the processing time to O (1) in the worst case. Their algorithms guarantee to find all of the frequent candidates in the first pass, but the second pass is required to identify real frequent ones from the candidates. In many data stream applica-tions, it is infeasible to rescan the data. By combining the hashing and sampling techniques, Estan and Verghese [40] present a novel algorithm to identify flows which exceed a pre-defined threshold. In [ 74], a deterministic algorithm called lossy counting is presented for -approximate frequency elements. Only one pass is required in their algorithm and the worst working space used is O ( 1 log( N )). An adaptive sampling algorithm called sticky sampling is also proposed in [74]. Although stick sampling only uses O ( 1 ) space in the worst case, it is shown in [74] that lossy counting is more space efficient in practise. Metwally et al. [77] present space saving algorithm which can support approximation of top-k ele-ments and heavy hitters with a unified data structure called Stream Summary .
Recently, [53] investigates how to efficiently compute the frequent elements in the data stream with the graphics processors. And Bandi et al. [9] study the problem under a special networking architectures called Network Processing Units(NPUs). Base on the lossy counting [74] and space saving [77] techniques, two TCAM-conscious algorithms are proposed to provide efficient solutions.
The algorithms above can not work under Turnstile Model . Consequently some sketch based algorithms are proposed to address this problem. As a variance of AMS sketch in [4], the count sketch technique is introduced by Charikar et al. [16] to find k elements whose frequencies are at least (1  X  ) times the frequency of the k -th most frequent elements, with probability at 1  X   X  and space O ( 1 2 log n  X  ). In [28], a novel algorithm called group test is proposed by Cormode et al. to further reduce the space requirement by re-examining the previous algorithm in [16]. Based on the main idea of the bloom filter , which is a classical data structure to support membership queries with certain probabilistic guarantees, [39, 19] extend the bloom filter to find frequent elements in the data stream. The Minimal Increase and Recurring Miminum techniques are introduced to improve the accuracy of their algorit hm. Recently, following the basic idea of previous algorithms, a new count min sketch is presented by Cormode et al. in [29]. Their algorithm significantly improve theoretical space efficiency of the previous results by reducing a factor of O ( 1 ). Their essential idea is to estimate the frequency of each element in the doma in by maintaining hash based multiple counters. A similar sketch called hCount is independently developed by Jin et al. in [63].

The problem has been studied in various applications and many new algo-rithms are introduced to s upport different scenarios.  X  As the hierarchy structure is widely employed in various online applications  X  [52] proposes an efficient algorithm to find the frequent elements against the  X  With the development of the sensor net work, various efficient algorithms  X  In some applications, it is desirable to find the distinct frequent elements .For Among various statistics, order statistics computation is one of the most chal-lenging, and is employed in many real applications, such as web ranking ag-gregation and log mining [3, 38], sensor data analysis [55], trends and fleeting opportunities detection in stock markets [6, 71], and load balanced data parti-tioning for distributed computation [76, 84].

In this section, we will introduce existing works on two kinds of order statistic oriented queries: ra nk queries and top-k ranked queries. Although the top-k ranked query can be regarded as a special case of rank query where the rank is limited between 1 and k , they have different focuses. For the rank query, we can only provide approximate solution in the context of data stream while the exact solution is required for the top-k ranked query. Moreover, the rank function is pre-given for the rank query problem while usually we need to support ad-hoc rank function for the later problem. 5.1 Rank Query A rank query is essentially to find a data element with a given rank against a monotonic order specified on data elements. And it has several equivalent variations [57, 30]. Rank queries over data streams have been investigated in the form of quantile computation. A  X  -quantile (  X   X  (0 , 1]) of an ordered set of N data elements is the element with rank  X N .

Rank and quantile queries have many applications including query optimiza-tion, finding association rule, monitoring high speed networks, trends and fleeting opportunities detection in the stock markets, sensor data analysis, webranking aggregation, log mining and query visualisation etc. Simple statistics such as the mean and variance are both insufficiently descriptive and highly sensitive to data anomalies in real world data distributions, while quantiles can summarize the distribution of massive data more robustly. Several applications employ quantile algorithms as a foundation, like counting inversions [57] and maintaining reverse nearest neighbour aggregates [67] in the context of data streams.

It has been shown in [80] that the space required for any algorithm to compute the exact rank query with p passes is  X  ( N 1 p ), where N is number of elements. Clearly, it is infeasible to do the exact rank computation in data stream appli-cations where data is massive in size an d fast in arriving speed. Consequently, approximate computation o f rank queries over data stream has receive a great attentions in the recent years [80].
In this subsection, we will introduce the space and time efficient techniques of continuously maintaining data summaries to support the rank(quantile) queries in various data stream models.

Suppose an element x may be augmented to ( x, v )where v = f ( x ) (called  X  X alue X ) is to rank elements a ccording to a monotonic order v and f is a pre-defined function. Without loss of generality, we assume v&gt; 0 and the monotonic order is always an increasing order. We study the following rank queries over a data stream S .
 Rank Query: given a rank r, find the rank r element in S .

Suppose that r is the given rank in a rank query and r is the rank of an approximate solution. We could use the constant-based absolute error metric, say | r  X  r | X  for any given . It is immediate that such an absolute error precision guarantee will lead to the space requirement  X  ( N )evenforthean offline computation where N = | S | . So two kinds of error metrics have been used in the recent works.
 Uniform Error. r  X  r N  X  .
 Relative Error. r  X  r r  X  .
 An answer to a rank query regarding r is uniform -approximate if its rank r has the precision | r  X  r | X  N . And it is relative -approximate if its rank r has the precision | r  X  r | X  r . In the following part, we will introduce the techniques of continuously maintaining a synopsis over data stream S such that at any time, the synopsis can provide a (relative or uniform) -approximate answer for a given rank query. The focus of the techniques is to minimize the maximal memory space required in such a continuous synopsis maintenance procedure. The processing time per element and query response time are also important issues.
 Uniform Error Techniques. In [80], Munro and Paterson present a one pass algorithm to provide the uniform N approximate answer for quantile query. A binary tree structure is employed in their paper and the work space required is O ( 1 log 2 ( N )). Manku et al. [75] improve the previous algorithm in terms of the working space. They reduce the constant factor significantly by applying a more sophisticated merge strat egy. Then they propose a space efficient randomized algorithm in [76] to further reduce the space bound to O ( 1 log 2 1  X  ) by applying an adaptive sampling approach. Then with probability at least 1  X   X  ,theiral-gorithm can achieve N approximation. Moreover, their algorithm can compute the quantiles without the advanced knowledge of the length of the data stream. They also show that further space deduction can be achieved by feeding the sample set to any deterministic quantile algorithm.

Greenwald and Khanna [54] propose the best known deterministic quantile algorithm, called GK algorithm, for the Cash Register Model ,with O ( 1 log( N )) working space in worst case. Following the space reduction framework of [76], a randomized algorithm with space O ( 1 log( 1  X  )) can be immediately developed. The GK algorithm has been widely employed as a building block in many quan-tile related works [67, 69, 5, 55].

As to the Turnstile Model , Gilbert et al. [51] propose the first algorithm to -approximate the rank query with probability at least 1  X   X  . Their algorithm is based on estimating range-sums of the data stream over dyadic intervals with The data structure they used for estimating range-sums of the data stream is called Random subset sums sketch , which can be directly replaced by the Count-Min sketch proposed in [29]. Then an immediate improvement over the known space bound in the Turnstile model . Applications of their algorithms include the telecommunication transaction monitoring and query optimization in the DBMS.

Lin et al. [69] propose the first space and time efficient algorithm to contin-uously maintain order statistics against the count-based sliding window model. Their techniques are based on a combination of GK-algorithm [54] and expo-nential histogram technique in [35]; They considered the rank queries over fixed sliding windows as well as variable sliding windows . And their space bound is O ( log 2 N + 1 2 )and( 1 2 log 2 ( N )) for fixed sliding windows and variable sliding windows respectively. Based on a more sophisticated interval-tree like structure, Arasu and Manku [5] improve the space bound in [69].

With the development of the sensor network, various statistic computation algorithms on the sensor network have been developed by various communities. Greenwald and Khanna [55] study the problem of power-conserving computation of order statistics in sensor networks. They show that the tree model of the sensor network model is at least as hard as stream model. Their algorithm enforces that the largest load difference between any two nodes will not exceed O (log( N )) in ordertoachieve -approximation. The maximal load for each node is bounded by O ( log 2 n ). Shrivastava et al. [85] improve the maximal load to O ( log n )based on a novel Q-digest data structure.

Instead of answering rank queries against a snapshot of the data set like [55, 85], Cormode et al. [22] investigate the problem of continuous track-ing of complex aggregates (e.g quantile) and data-distribution summaries over collections of distributed streams. In order to achieve highly communication-and space-efficient solutions, they combine a local tracking at remote sites and simple prediction models for local site behaviour.

In [70], novel techniques are propose d to efficiently process a massive set of continuous rank queries where the Continuous Queries are issued and run continuously to update the query results along with the updates of the underlying datasets.

In [56], Guha et al. investigate the importance of the ordering of a data stream, without any assumptions about the actual distribution of the data. The quantile computation is used as a sample application. They prove some theoretical space bounds for the quantile algorithm over the data streams with adversary and completely random order. And their space efficient technique enforces a finer rank error guarantee | r  X  r | = O ( r 0 . 5+ ). [53] shows how to efficiently compute the quatiles over the data stream with the graphics processors. Relative Error Techniques. Using the relative error metric to measure ap-proximation is not only of theoretical interest but also very useful in many ap-plications. For instance, as shown in [26], finer error guarantees at higher ranks are often desired in network management. This is because IP traffic data of-ten exhibits skew towards the tail and it is exactly in the most skewed region where user wants relative rank error guarantees, to get more precise informa-tion about changes in values. Relative error is also motivated by the problem of approximately counting inversions of a data stream [57].

The problem of finding approximate quantiles with relative error guarantee is first studied by Gupta and Zane [57], who develop a one-scan randomized technique with O ( 1 3 log 2 N ) space requirement for a pproximately counting in-versions, by maintaining an order sketch with the relative rank error guarantee . However, their technique requires advanced knowledge of (an upper bound on) N to do one-scan sampling. This potentially limits its applications. Cormode et al. [26] study the related problem of computing biased quantiles ,thatis,the set of quantiles  X  = {  X  i =  X  i 0 :1  X  i  X  k } ,forafixed k and some  X  0 ,which are estimated with precision  X  i N . [26] gives an algorithm to approximate such biased quantiles with deterministic e rror guarantees which performs very well against many real data sets. While the problem of computing biased quantiles focuses on the relative rank error guarantee bounded by a minimum quantile  X 
N , the rank query addresses relative error guarantees at all ranks, no matter how small  X  is. As shown in [26], the application of their technique to the arbi-trary rank queries leads to a linear space requirement  X  ( N )intheworstcase; this can render the deterministic technique impracticable in applications where small space usage is imperative.

In [96], We developed a novel, one-sca n randomized algorithm ( X  X R X ) which guarantees the precision of relative rank errors with confidence 1  X   X  and requires O ( 1 2 log 2  X  log 2 N ) space. We also develop an effectiv e one-scan space compression technique. Combined with the above one-scan randomized technique, it leads to a more space-efficient one-s can randomized algorithm ( X  X RC X ) which guarantees worst case space requirement remains O ( 1 2 log 2  X  log 2 N ). Recently, Cormode et al. [27] develop a novel deterministic algorithm to approximate the rank queries with relative error. The Q-digest structure in [85] is extended in their work, and the space required by their algorithm is O ( log | U | log N ). As shown in [27], their algorithm outperforms the randomized algorithms. However, their solution is re-stricted to a fixed value domain U . The space efficient deterministic algorithm with relative error guarantee remains open for the applications where the domain size of the data elements is unlimited.
 Duplicate-insensitive. In many data stream applications, duplicates may often occur due to the projection on a subspace if elements have multiple attributes. For example, in the stock market a deal with r espect to a particula r stock is recorded by the transaction ID (TID), volume ( vol ), and average price ( av ) per share. To study purchase trends, it is important to estimate the number of different types of deals (i.e. deals with the same vol and the same av are regarded as the same type of deal) with their total prices (i.e. vol * av ) higher (or lower) than a given value. It is also interesting to know the total price (of a deal) ranked as a median, or 25th per-centile, or 10th, or 5th percentile, etc. amo ng all different types of deals. These two types of rank queries are equivalent [27,57]; To accommodate such queries, we need to project each deal transaction (TID, vol , av )on( vol , av ) and then summarize the distribution of distinct ( vol , av )s according to a decreasing (or increasing) order of vol * av . In this application, the data elements to be summarized are mapped from (TID, vol , av )to( vol , av ) by the projection. Consequently, any generated dupli-cates ( vol , av ) must be removed to process such rank queries. Moreover, relative (or biased) rank error metrics need to be used to provide more accurate results to-wards heads (or tails depending on which monotonic order is adopted). Note that the generality of rank queries (quantiles) remains unchanged in this application since two different types of deals (i.e., ( vol , av )s) may also have the same values of vol * av . The unique challenge is to detect and remove the effect of duplicates without keeping every element.

Duplicates may also occur when data e lements are observed and recorded multiple times at different data sites. For instance, as shown in [26, 30] the same packet may be seen at many tap points within an IP network depending on how the packet is routed; thus it is important to discount those duplicates while summarising data distributions by rank queries (quantiles). Moreover, to deal with possible communication loss TCP retransmits lost packets and leads to the same packet being seen even at a given mo nitor more than once. Similarly, in order to achieve high fault-tolerance against communication errors in a sensor network a popular mechanism is to send data items by multi-paths [20, 72, 82] which will create duplicates.

In such distributed applications, continuously maintaining order sketches for processing rank queries may be conducted either centrally at one site or at a set of coordinating sites depending on the computing environment and the availability of software and hardware devices. Neverth eless, in those situations a crucial issue is to efficiently and continuously maintain a small space sketch with a precision guarantee at a single site, by discounting duplicates.

The FM technique [46] has been first applied in [12, 20, 82] to develop duplicate-insensitive techniques for approximate computing sum , count (num-ber of sensor nodes), average to achieve high communication fault-tolerance.
In [82], Nath et al. present a duplicate-insensitive in-network quantile com-putation algorithm to cope with multi-path communication protocol. For each element, a random number between [0 , 1] is drawn, which determines if the el-ement will remain in the quantile sample; this combines with the element ID to remove duplicates generated by the multipass communication. As the uni-form sampling technique does not guarantee to draw the same random number for the duplicated element, the technique in [82] can only handle the duplicates generated in communication rather than duplicates in data streams.

In [72], Manjhi, Nath and Gibbons propose an effective adaption paradigm for in-network aggregates computation over stream data with the aim to minimize communication costs and to achieve high fau lt-tolerance. A duplicate-insensitive technique for approximately computing quantiles can be immediately obtained by a combination of their tree-based approximation technique and the existing distinct counting technique in [10]. It can be immediately applied to a single site, where a data stream has duplicated elements, with the uniform precision guarantee | r  X  r | X  n by confidence 1  X   X  and space O (1 / 3 log 1 / X  log m ), where m is the maximal possible number of distinct elements.

In [30], Cormode and Muthukrishnan present a Distinct range sums technique by applying the FM [46] technique on the top of their count-min sketch [29]. The technique can be immediately used to approximately process rank query with the uniform precision guarantee | r  X  r | X  n , confidence 1  X   X  ,andspace O ( 1 3 log 1  X  log 2 m ). Independently, Hadjieleftheriou, Byers and Kollios [58] de-velop two novel duplicate-insensitive techniques based on [85] and [29] to ap-proximately compute quantiles in a distributed environment. Applying their techniques to a single site immediately leads the uniform precision guarantee | r  X  r | X  n by confidence 1  X   X  and space O ( 1 3 log 1  X  log m ).

Very recently, we develop the first space-and time-efficient, duplicate-insensitive algorithms [97] to continuously maintain a sketch of order statis-tics over data stream to enforce relative -approximation. They not only im-prove the existing precision guarantee ( from uniform -approximation to rel-ative -approximation ) but also reduce the space from O ( 1 3 log 1  X  log m )to O ( 1 3 log 1  X  log m )where m is the element domain size. 5.2 Top-k Ranked Query Instead of finding records with arbitrary rank, in many applications users are typically interested in the k records with highest ranks, where k N and N is the number of records. Moreover, the ranking( preference ) function might be proposed by users at query time. Providing efficient answers to such top-k ranked queries has been a quite active topic and has many important applications involving multi-criteria decision making.

In many applications the volume of the dataset is extremely large while users are usually only interested in a limited number of answers regarding to their preference functions, so it becomes n ecessary to pre-pro cess the data to speed up the performance. Many related works have been done in the literature, and they can be classified into three categories: distributed index [42,43,44,45], view based index [61,95,34] and minimal space index [15,90,87,93,64,79,89]. However, only a few work [79,88,33] investigate the problem in the context of data streams.
In [79], Mouratidis el al. study the problem of continuous monitoring of top-k queries over sliding windows. Based on the concept of K -skyband introduced in [83], it is shown that only the tuples in the K -skyband can be answers for any top-k ranked query with monotonic preference function where k&lt;K .In[79], elements are indexed by a regular grid in main memory. Two algorithms, TMA and SMA , are proposed to continuously monitor the top-k answers for those queries. In [79], a tuple can be regarded as a two dimensional data point. One dimension is the score of the tuple (the rank function is pre-given) and an-other is its timestamp. The SMA algorithm is proposed to continuously main-tain the K -skyband against the stream data. The K -skyband of a dataset is the points which can be dominated by at most K -1 other points, Clearly skyline is a special instance of skyband with K =1. The basic idea of SMA is to maintain a dominance count ( DC ) for each tuple t where the DC is the number of tuples which dominate t , One tuple can be immediat ely discarded once its DC exceeds K since it will not be touched by any top-k query with k  X  K .

In [88], Tao et al. show how to continuously maintain the K -skyband against multidimensional data indexed by the R Tree, and the concept of dominance count is also employed in [89] as well. With a branch-and-bound search strategy proposed in [88], [89] can efficiently retrieve answers for the top-k ranked queries.
Recently, based on the novel concept of the geometric arrangement, Das et al. [33] further improve the efficiency of the top-k algorithms over data streams. Instead of continuously maintaining the K -skyband of the stream data, new tuple pruning algorithm is proposed in the paper such that the cost of minimal candidate set maintenance is significantly reduced. Although there are still many problems remaining open for the data stream algorithms, recently a great attention is given to the aggregate computation over probabilistic data streams. In many important applications such as envi-ronmental surveillance, market analysis and quantitative economics research, uncertainty is inherent because of various factors including data randomness and incompleteness, limitations of measuring equipments, delayed data updates, etc. Meanwhile, those data are created rapidly so it is worthwhile to investigate various computations over the probabilistic data streams. The main challenge is to design space and time efficient algorithms to handle the uncertain data which might arrive rapidly.

