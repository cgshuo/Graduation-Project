 One goal of text mining is to provide readers with automatic methods for quickly finding the key ideas in individual doc-uments and whole corpora. To this effect, we propose a statistically well-founded method for identifying the origi-nal ideas that a document contributes to a corpus, focusing on self-referential diachronic corpora such as research pub-lications, blogs, email, and news articles. Our statistical model of passage impact defines (interesting) original con-tent through a combination of impact and novelty, and it can be used to identify the most original passages in a doc-ument. Unlike heuristic approaches, this statistical model is extensible and open to analysis. We evaluate the approach on both synthetic and real data, showing that the passage impact model outperforms a heuristic baseline method. H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experimentation Language Modeling, Topic Modeling, Original Contributions
For diachronic text corpora like research publications, web discussions, and news, identifying the origin and flow of ideas is a crucial step towards understanding their content and structure. To automatically detect the origin of ideas, we propose a generative probabilistic model that allows infer-ence for originality in an unsupervised way. Unlike methods for Topic Detection and Tracking [2], our model provides an operational definition of originality by combining novelty and impact, and we show how it can find text passages that best summarize the original contribution of a document.
We define original ideas as being both novel and having impact. Anybody can write novel nonsense, but adding im-pact focuses on new ideas that interest and are important to many people. Applications range from automatically ex-tracting snippets to summarize important developments in news corpora, to attributing ideas to their originators in web discussions, and to making the ideas in the large body of re-search literature easily accessible to humans by summarizing the original contribution of each document.

Our originality-detection methods are text-based and do not require hyperlinks. While heuristic approaches for iden-tifying important words or passages exist (e.g., based on TFIDF [4]), the statistical methods we propose have a con-cise probabilistic semantic which affords easy analysis and extensibility. Furthermore, we show that using the language-modeling approach empirically outperforms simple heuris-tics for novelty detection on a discussion-group corpus.
We take a language modeling approach and define a gener-ative model for diachronic corpora. An author writes a new document using a mixture of novel ideas and ideas  X  X opied X  from earlier documents. An idea has impact, if it is copied (i.e., discussed, elaborated on) by future documents. This picture is one of idea flows, originating in documents with impact, and  X  X lowing X  to documents based on idea devel-opment. We directly model idea flows between documents, without an extra level of the topic as in topic models [3]. Identifying the original contribution of a document means separating novel ideas from old ideas, and simultaneously as-sessing impact. We assume that documents generally con-tain a key paragraph or sentence(s) that succinctly sum-marize the new idea, and we aim to identify this piece of original text as a summary. This task differs from summa-rization, however, because our method focuses on originality [1]: While all documents can be summarized, some do not contribute original ideas (e.g., a textbook). The task also differs from the TREC novelty track [6], since we detect im-pact, not relevance. The following gives more detail on our probabilistic model and inference method.
We propose a generative model of a diachronic corpus that extends the model in [5]. A document D ( i ) of length n i is modeled as a vector of n i random variables W ( i ) = into two sets: Z ( i )  X  X  1 ...n i } for the word indices in D that are original and have impact, while  X  Z ( i ) = { 1 ...n Z ( i ) contains the rest of D ( i ) . Words in the original portion Z ( i ) are drawn from a unigram language model with word probabilities  X  ( i ) . The remaining words  X  Z ( i ) come from a mixture of a novel unigram model  X   X  ( i ) (new, but without im-pact) and words copied from the original sections of existing documents. Words are copied uniformly and independently Table 1: Percentage of misranked non-original pas-sages. Passage length L = 100 ,  X  = 0 . 2 ,  X  ( i ) n = 0 . 5 ,  X  i = 0 . 05 , and  X  k
F used in inference. One standard error is shown. Table 2: Percentage of misranked non-original pas-sages for k F = 2 , L = 100 ,  X  ( i ) n = 0 . 5 ,  X  ( l )  X  n = 0 . 6 . One standard error is shown. so that copying uses a unigram model with parameters  X   X  ( k ) for each prior document D ( k ) . The document-specific mix-Formally, we model a diachronic corpus as follows: Model 1. (Passage Impact Model) A corpus C = ( D (1) ...D ( n ) ) of temporally-sorted documents D has probability P ( C ) = Q n i =1 P ( D ( i ) | D (1) ...D and where  X   X  ( k ) w is the probability of uniformly drawing word w from the words in the original section z ( k ) of document D
Using the PIM, we want to infer the section Z ( i ) in D ( i ) that most succinctly summarizes the original idea. Only document text is observed. After a few justifiable approxi-mations, we use a MAP inference procedure based on Model 1 to infer Z ( i ) by maximizing P ( D ( i ) ...D ( n ) | D sequence of convex programs that can be solved efficiently.
We tested this method on synthetically-generated data based on the Neural Information Processing Systems pro-ceedings and on web documents from Slashdot discussions. Synthetic data is based on language models  X  ( i ) from the MLEs of NIPS documents. Document d ( i ) has 20 passages. One is Z ( i ) , and the others form  X  Z ( i ) . Since real documents may not have exactly one Z ( i ) passage, we test robustness by diffusing  X  original content through  X  Z ( i ) . Future documents are d ( l ) . The evaluation measure is the average percentage of  X 
Z ( i ) passages that have a higher likelihood than Z ( i ) Table 1 shows that impact is critical to define originality. Using just one future document can differentiate real original ideas and novel noise. Table 2 shows that the method is robust for diffused original content, up to  X  = 0 . 3.
We also evaluate on Slashdot, where users post and dis-cuss articles. Sometimes the first post links to and directly Table 3: Prec@2 and Rec@10 use the predicted sen-tence ranking by likelihood and TFIDF sum. Orig-inal sentences are those quoted word-for-word from the article. Results are for  X  ( i ) o = 0 . 2 and  X  ( l ) quotes a web document. We collect such linked-to web doc-uments and discussions, treating human-selected direct quo-tations as labeled original contributions. For inference, we rank all sentences in the linked-to web document d ( i ) by like-lihood. The documents d (1) ...d ( i  X  1) are the ones that have been linked to in earlier discussions. The future  X  X ocument X  d ( i +1) is the user discussion of d ( i ) , except with direct arti-cle quotations removed. We evaluated the method on iden-tifying human-selected sentences on 61 articles (7 months) matching these criteria from the Games subtopic. The base-line uses TFIDF, with sentences scored by the sum its words X  IDF values. Evaluation is by Precision and Recall. The Prec@2 statistics in Table 3 show that the Passage Impact Model outperforms the TFIDF heuristic baseline for predicting original sentences at the top of the ranking. In addition, Rec@10 shows that the PIM is also significantly better when trying to find a large subset of original content.
We proposed a generative model for diachronic corpora and an inference procedure to identify original ideas. This method significantly beats a heuristic baseline for selecting a document section to summarize its original contribution. We acknowledge Adam Siepel, Art Munson, Yisong Yue, Nikos Karampatziakis, and the ML Discussion Group. This work was supported in part by NSF Grant IIS-0812091. [1] Document understanding conferences. [2] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and [3] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [4] G. Salton and C. Buckley. Term weighting approaches [5] B. Shaparenko and T. Joachims. Information [6] I. Soboroff and D. Harman. Overview of the TREC
