 While most methods for learning-to-rank documents only consider relevance scores as features, better results can often be obtained by taking into account the latent topic structure of the document collection. Existing approaches that con-sider latent topics follow a two-stage approach, in which top-ics are discovered in an unsupervised way, as usual, and then used as features for the learning-to-rank task. In contrast, we propose a learning-to-rank framework which integrates the supervised learning of a maximum margin classifier with the discovery of a suitable probabilistic topic model. In this way, the labelled data that is available for the learning-to-rank task can be exploited to identify the most appropriate topics. To this end, we use a unified constrained optimiza-tion framework, which can dynamically compute the latent topic similarity score between the query and the document. Our experimental results show a consistent improvement over the state-of-the-art learning-to-rank models. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering, Retrieval models, Search process Learning-to-rank; Topic models; Maximum margin learning
T he work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414) and the Direct Grant of the Faculty of Engineering, CUHK (Project Code: 4055034). This work is also affiliated with the CUHK MoE-Microsoft Key Lab-oratory of Human-centric Computing and Interface Tech-nologies. Steven Schockaert has been supported by ERC Starting Grant 637277. This work was done when the first author was a Postdoctoral Fellow at The Chinese University of Hong Kong.
 c
The learning-to-rank (LTR) paradigm for information re-trieval (IR) consists in the use of machine learning tech-niques for constructing suitable document ranking functions. Documents in this context are represented as vectors of fea-tures, capturing relevance w.r.t. the given query as well as query-independent statistics such as PageRank. LTR ap-proaches can be distinguished in how they approach the ranking problem. In particular, pointwise (e.g. [22]), pair-wise (e.g. [7]), and listwise (e.g. [8, 19, 31]) approaches are commonly considered, which respectively interpret ranking as a regression problem, a binary classification problem and an optimization problem.
 In this paper, we will extend a maximum margin based LTR model, which is known to perform well on this task [22, 14, 1, 7], with information derived from a latent topic model, which has already proven beneficial in many IR tasks [30, 32, 10]. Several authors have already considered the use of latent topics for LTR. A common approach to do this is to adopt a two-stage  X  X ownstream X  method [34], in which an existing topic model such as Latent Dirichlet Allocation ( LDA ) [6] is used to find the latent topics in the documents and queries. One can then compute a topic-based similar-ity between the query and a document, e.g. using cosine similarity or a likelihood based score [30]. By adding the resulting similarity scores as an additional feature, existing LTR models can be used to learn a ranking function for these topic-enriched feature vectors. For example, such a two-stage method has been used in [27] as a comparative method. However, since the mechanisms behind discover-ing the topics and learning the ranking are completely de-coupled, this approach is inherently sub-optimal. Indeed, similar approaches have already been shown to perform un-satisfactorily in other prediction tasks [34, 27], as errors from one stage are propagated to the next.

The mechanism we propose in this paper is significantly different from these existing two-stage  X  X ownstream X  ap-proaches, as it is based on a tight coupling of latent topic detection and maximum margin classification. Specifically, we propose a unified constrained optimization framework, which is used to find a regularized posterior distribution of the predictive function in a space defined by the pairwise maximum margin constraints. The topic model component of the composite objective function is used to find the latent dimensions of the dataset, whereas the maximum margin component is used for label prediction. The advantage of this approach is that latent topic information can be chosen so as to aid the classification of data points around the de-cision boundary of a standard pairwise classifier, and more g enerally to prevent misclassifications. In particular, the proposed framework is capable of obtaining additional la-tent topic information to obtain a more discriminative rep-resentation of these hard data points. In this way, a more effective pairwise-based ranking classifier can be obtained, taking into account hidden topics that are automatically de-tected as part of the training process. After presenting the details of our model, we discuss a technique for solving the resulting optimization problem. Finally, we conduct exten-sive experiments on several benchmark datasets and show that our proposed model consistently improves the state-of-the-art approaches. We also present results related to the query-wise performance of our model in comparison to the comparative models and show that our model performs much better.
Maximum margin learning has already been successfully applied to LTR in a number of different ways [1, 14], in both pointwise [22] and pairwise [7] LTR models. For exam-ple, the latter approach uses RankSVM with a pairwise hinge loss function which is specifically adapted to the LTR task. Note that while [5] discusses the use of query topics in a maximum margin setting, it does not rely on a topic model for inferring these query topics. A large number of other learning-to-rank models have recently been proposed. For example, Gao et al. [13] presented a novel semi-supervised listwise LTR model to deal with domains for which no train-ing data is available. A sparse learning-to-rank model for in-formation retrieval has been proposed in [17]. Finally, Dang et al. [9] proposed a two-stage LTR framework to improve the performance in cases where many relevant documents are excluded from the ranking list by bag-of-words retrieval models. None of the above works consider latent topics for tackling the LTR problem.
Unsupervised topic models such as Latent Dirichlet Allo-cation ( LDA ) [6] have proven very effective for ad-hoc infor-mation retrieval [30, 32, 10]. The usefulness of topic models in this context stems from the fact that latent topics can be used to better estimate the similarity between queries and documents when the overlap in actual content words is low [18, 12]. Existing approaches do not follow the LTR paradigm, but rather use the low-dimensional topic space to conduct traditional document retrieval under an unsuper-vised setting, without considering any other features.
Supervised models based on a latent semantic space have also been considered [18]. In [4], the authors have pro-posed a discriminative model, called supervised semantic in-dexing, which can compute query-document and document-document similarity in a semantic space. While the authors state that their model can easily be extended to an LTR setting, they have not developed such an extension. Gao et al. have proposed topic models which jointly consider the query and the title of a document, with the aim of improv-ing document retrieval in a language modeling framework [12, 15]. Even though they also use posterior regularization, there are major differences with our approach; for instance, our model is designed for the LTR task. In addition, we introduce a novel interpretation to the optimization frame-work of the posterior distribution obtained using LDA , using a convex optimization technique, which leads to a novel for-mulation and inference algorithm.

There is another line of work that uses probabilistic topic modeling for document classification, which takes into ac-count labeled training data during parameter estimation. One example is the supervised topic model from [21], which introduces a response variable in the topic modeling frame-work. The maximum margin entropy discrimination model, known as MedLDA [34], discriminatively learns a topic model for binary and multi-class document classification. A differ-ence between our work and such maximum margin super-vised topic models is that our model is designed for solving LTR tasks whereas the aforementioned models only consider document classification, which leads to a different optimiza-tion problem.

Note that both our model and the models proposed in [34, 35] are learned by solving a regularized Bayesian infer-ence task. However, MedLDA and infinite latent SVM solve a different optimization problem. In particular, these models use the same input as text classification models, and use a latent linear discriminant function with a random weight vector that only encapsulates the topic feature weights, lead-ing to an expected classifier with an effective discriminant function. Due to the use of a random weight vector, these models cannot directly take into account the non-random pre-computed features that are traditionally considered in the LTR task. Supervised text classification models also face problems when dealing with unbalanced data [22], where one class tends to dominate the training set, as is commonly the case for the class of non-relevant documents in information retrieval. Such unbalanced data leads to a classifier where the minority class is totally ignored by the text classification model [22]. Therefore, novel methodologies and algorithms are needed in order to solve the LTR task using topic models with pairwise maximum margin constraints.
Some probabilistic models have been proposed which make use of posterior inference with regularization, although la-tent topics have not previously been considered in such mod-els. For example, in [11, 29] the authors proposed a prob-abilistic regularization framework for structured weakly su-pervised learning. They showed that by directly imposing decomposable regularization on the posterior moments of latent variables, the computational efficiency of the uncon-strained model can be retained while ensuring that desired constraints hold in expectation. Supervised topic models discussed above such as [34, 16] also conduct posterior reg-ularization.
The learning-to-rank (LTR) paradigm aims to learn an optimal ranking function for a given set of available rele-vance features. Specifically, let a query set Q and document collection D be given, and assume that each query q u from Q is associated with r u documents. The learning algorithm is given x queries as training examples. The corresponding documents are typically retrieved from a search engine and will be denoted as D u = { d u 1 ,d u 2 , ,d u r u  X  D } . Further-more assume that in the training data, a discrete relevance l abel h u i is associated with each document d u i . Our objec-tive is to learn a ranking function from this training data that can be used to rank documents for previously unseen queries.

As is typical in LTR settings, we assume that we have access to a mapping  X  : Q  X  D  X  R n that maps each query-document pair to an n-dimensional feature vector. The training algorithm is then given labeled examples of the form T = ( T 1 ,T 2 , ,T x ), with each T u of the form T then need to learn a real-valued function f : R n  X  R such that f (  X  ( q,d i )) &gt; f (  X  ( q,d k )) if document d vant to query q than document d k .
We will learn a linear ranking function, parametrized by a weight vector  X   X  R n : Our framework follows the pairwise approach to LTR. In other words, we treat LTR as a binary classification problem by constructing for each pair of documents ( d u i ,d u k ated with a given query q u , the vector (  X  u i  X   X  u k ). The latter vector is assigned the class label y u ik defined as follows: Let the set B u of document index pairs, given a query q u be defined as follows: In other words, ( i,k )  X  B u if d u i is more relevant than d
Our framework is based on a maximum margin classifier model, similar to Support Vector Machines ( SVM ), which re-quires us to solve the following optimization problem: where C is a regularization parameter and  X  ik are the non-negative slack variables. It can be shown that solving this optimization problem is equivalent to minimizing the empir-ical hinge loss function, defined as follows: In our framework, we will consider a modified hinge loss function, which takes into account the difference in rele-vance degree and the different number of document pairs for different queries: Advantages of considering a query-level loss function have been discussed in [25, 1], where the relevance levels of dif-ferent documents were also taken into account.
The topic modeling component of our framework is based on Latent Dirichlet Allocation ( LDA ). The aim of LDA is to represent the meaning of each document as a probability dis-tribution over a set of latent topics. It assumes a generative process, in which each word in a document is generated by sampling a topic and then sampling a word. LDA assigns each word in the document collection to one of the latent topics (initially at random), and uses this assignment to estimate both the probability distribution over topics associated with each document and the probability distribution over words associated with each topics. These probability distributions are then used to improve the topic assignments of the words, and the whole process is repeated until convergence.
To apply LDA in our setting, we expand the document col-lection D with the set of queries from the training data Q , i.e. queries are treated as short documents to construct an aggregated document collection, denoted as D . For any doc-ument d in D , we let N d denote the number of words in document d . Furthermore, we let w d = { w d n } N d n =1 words appearing in the each document and W = { w d } D d =1 the words in the entire aggregated document set. Let z d = { z ment d and let Z = { z d } D d =1 denote the topic assignment of all words in D . Let  X  = {  X  d } D d =1 be the topic distri-butions of all documents in D . Let the number of topics be K . Let  X  = {  X  1 , X  2 , , X  K } be the V  X  K matrix of topic distribution parameters, where each  X  k parameterizes a topic-specific multinomial word distribution. V denotes the number of words in the vocabulary. The posterior dis-tribution is then given by:
P (  X  , Z ,  X  | W ,  X  ,  X  ) = P 0 where P 0 (  X  , Z ,  X  |  X  ,  X  ) is the prior probability, with  X  the parameter of the Dirichlet prior on the per-document topic distributions and  X  the parameter of the Dirichlet prior on the per-topic word distributions.

Zellner in [33] has extended Bayes X  rule so that it can be used as a learning model. Specifically, Zellner has showed that Bayes X  rule can be transformed into an optimization problem. In this way, it can be shown that the values for  X  , Z and  X  which maximize the posterior distribution (7) can be found by solving the following optimization problem: where P is the probability distribution space, KL( P || P the Kullback-Leibler divergence from P to P 0 , and E is the expected value operator. This interpretation of Bayes X  theo-rem will be useful for designing our pairwise LTR model, which will be based on an extension of (8). Note that P ( W |  X  ,  X  ) has been omitted because it does not depend on  X  , Z ,  X  .
The key characteristic of our framework is that maximum margin learning is tightly integrated with topic discovery. To this end, in Section 4.1, we extend the optimization problem from (4) with a latent topic model, similar to Latent Dirich-l et Allocation ( LDA ). As we discuss in Section 4.2, solving the resulting optimization problem requires us to alternate between topic discovery (informed by the parameters of the maximum margin classifier) and maximum margin label pre-diction (informed by the latent topic structure).
The optimization view of Zellner X  X  interpretation of Bayes X  rule in (8) can be extended to incorporate posterior con-straints in Bayesian inference, by adding a convex function to (8). This leads to the introduction of some auxiliary free parameters, which can be slack variables. We will use the following formulation: where P new (  X  ) is the subspace of probability distributions satisfying the constraints that arise out of the optimization framework. The convex function M (  X  ) in our case will be based on the relevance-weighted pairwise query-level hinge loss defined in (6). Note that many other interesting exten-sions to Zellner X  X  equation have been proposed in the past such as the MedLDA model [34].

To consider latent topics during maximum margin learn-ing, the loss function in (6) needs to take into account the topic similarity between the query q u and document d u i we denote this similarity by  X  u i , we can consider the follow-ing discriminant function: where  X  t is a parameter encoding the relative importance of the topic similarity  X  u i , which is calculated based on the topic document distribution  X  . Recall that in this distri-bution, a document (or query) d is represented as a K  X  1 vector ~ d u i , encoding for each latent topic z the correspond-ing probability P ( z | d ). For document d u i associated with a query q u , we define  X  u i as the cosine similarity between and ~q u . Other metrics such as KL-divergence could also be used. The overall optimization problem we end up with is as follows: Note that the latent topic similarities  X  u i and  X  u k are com-puted in the regularized topic space arising from the opti-mization component, which differentiates our approach from the two-stage heuristic methods described in Section 1. By directly regularizing the posterior distribution with the max-imum margin constraint, we obtain a more powerful model, catered specifically to the pairwise LTR task. The useful-ness of this approach stems from the fact that latent topics can be chosen specifically to help the maximum margin clas-sifier, e.g. by preventing instances from being located near the margin. As solving (11) exactly is intractable, we resort to a Monte Carlo method which alternates between two steps. In the first step, our goal is to find a maximum margin separation of the points, i.e. we determine  X  and  X  t given P (  X  , Z ,  X  | W ,  X  ,  X  ). As in the existing RankSVM algorithm, the optimum solution can be found by adopting the Lagrangian method. In the second step, we estimate P (  X  , Z ,  X  | W ,  X  ,  X  ) given (  X  , X  t ), as explained below. Both steps are repeated for a given number of iterations or until the sampler converges to a steady state. As in traditional topic modeling, the procedure starts with a random initialization of the topic assignments. Note that this mechanism closely resembles the Expectation-Maximization (EM) algorithm. However, while EM maximizes the expected log-likelihood under the marginal distribution of the latent variables, we are mini-mizing the regularized loss.

To estimate P (  X  , Z ,  X  | W ,  X  ,  X  ), we use collapsed Gibbs sampling, where our goal is to estimate the model parame-ters  X  and  X  . We present a brief derivation of the collapsed Gibbs sampler with maximum margin constraints below. Let n zw denote the number of times a word w is assigned to a topic z and let p dz denote the number of times a word from document d has been assigned to topic z . We can start with the joint distribution of the model, which can be expressed as: When we incorporate the maximum margin framework in (12), we obtain the following model: where  X  is the normalization constant. The first factor in the right-hand side of (13) adopts the collapsed Gibbs sampling formulation. In this sampling scheme, we also compute the transition probabilities, which are used to iteratively find the word-topic and document-topic latent topic distributions. The second factor corresponds to the regularization effects of the pairwise maximum margin classifier. Let  X  denote the Gamma function where  X ( x ) = ( x  X  1)!. By integrating out  X  and  X  , we get the marginalized posterior distribution: P ( Z |  X  )  X  where  X  and  X  are defined as follows with  X  u dk and  X   X  u  X  d  X  k the Lagrange multipliers, satisfying 0  X   X  dk  X  C x  X  | B u | for all d , k and u . The above formulation can be used to derive the following updating rule for the Gibbs sampler, for one variable z d n given the others:
P ( z d n = t | Z  X  n ,w = v, W  X  n ,  X  ,  X  ) (17) I n the above equation, the current word is excluded in the counts n zw and p dz which we denote with the symbol  X  . The parameters in the model can be estimated based on the following formulations:
To apply our LTR model on unseen data, we have to de-termine the latent dimensions of the unseen data using the regularized topic space that was learned from the training data. In addition, the model has to project the unseen data into the learned ranking space of the pairwise maximum margin classifier for label prediction. To this end, we use the point estimate of the topics computation procedure from the training data. After the Markov chain has reached a certain number of iterations, we draw J samples from it. Specif-ically, using the maximum a posteriori probability (MAP) estimation scheme, we obtain a new set of topic distribu-tions  X  , which we write as  X   X  . Using our collapsed Gibbs sampler, an estimate of  X   X  can be obtained as follows: The latent dimensions of an unseen document  X  can then be computed as follows: The above formulations can be used to separately compute the latent dimensions for the query and the documents. Let  X  denote a feature vector for  X  , containing the same fea-tures as those considered in the training set. The prediction formula for the pairwise maximum margin classifier can be expressed as:
F (  X  ) = In the right-hand side of (22), the expression P x u =1 P  X  ik [(  X  u i  X   X  u k ) + ( X  u i  X   X  u k )] computes the feature weights for the new vector  X  . The recipe is to alternately run, until convergence is reached, the topic prediction model depicted in (21) using the parameters of the trained regularized topic space, and then use the pairwise maximum margin formula-tions depicted in (22), which provides the pairwise regular-ization effect to the latent dimensions computed in the pre-vious step, for predicting the labels. When (21) and (22) are combined together following the same paradigm as depicted in (13), the new documents are  X  X olded-in X  in the previously trained regularized topic space.
Many benchmark LTR collections have been released in the past, such as LETOR [24] and the Yahoo! LTR Chal-lenge dataset. These collections contain pre-computed query-dependent and query-independent features but do not pro-vide access to the corresponding documents, which means that the latent topic features cannot be computed. As a result, such existing public benchmark LTR collections can-not be used in our experiments. One exception is the LE-TOR OHSUMED [24] benchmark collection, whose text and queries are freely available 1 . In addition to LETOR OHSU-MED, we will use three new collections, which we have built based on well-known TREC datasets. First, we have used AQUAINT, which is used in the TREC-HARD [2] track and contains a total of 50 queries 2 . The queries along with the corresponding annotations are provided on the TREC HARD disk. Second, we have used the WT2G dataset 3 , consisting of 50 TREC queries 4 . Finally, we have used the ClueWeb09 Category B English documents collection. In particular, we used a list of 91 features from [3], where the authors have considered 150 TREC Web Track queries from 2009 to 2011.

Recently, Terrier v4.0 has introduced LTR in its distri-bution 5 . We used this open source search engine to cre-ate training, testing and validation sets for our two new datasets (AQUAINT and WT2G). We created the corre-sponding LTR datasets using all query-dependent and query-independent models currently available in Terrier v4.0. Note that Terrier v4.0 uses the  X  X itle X  field from the TREC top-ics file to retrieve documents. The created datasets com-prise 39 normalized features, which form a subset of the features described in [20]; all considered features fall in the WM and WMP classes described in [20]. Terrier v4.0 can be set to use its default BM25 implementation to retrieve the top 1000 documents, which corresponds to what is used in the official LETOR [24] collections. We separately cre-ated the LTR dataset for ClueWeb09 because of two rea-sons. First, this dataset is extremely large for Terrier to handle on a standalone machine. Second, Terrier X  X  imple-mentation currently cannot compute the 91 considered fea-tures for the ClueWeb09 dataset. In all text collections, h ttp://ir.ohsu.edu/ohsumed/
Note that we use the term queries here whereas in TREC usually the term topics is used. http://ir.dcs.gla.ac.uk/test collections/access to data.html h ttp://trec.nist.gov/data/t8.web.html http://terrier.org/docs/current/learning.html we used stemming (Porter stemmer) and stopword removal, a s implemented in Terrier v4.0. Subsequently, we created five folds. Each fold has approx. 60% query-document pairs for training, approx. 20% query-document pairs for testing and the rest for validation. Note that the training, test-ing and validation sets do not share any queries. One can thus notice that we have followed the standard LTR dataset creation procedure as reported in the LETOR dataset doc-umentation [24]. In our experiments, 42681 unique docu-ments were retrieved by Terrier for the 50 queries in the AQUAINT collection. For the WT2G dataset, we retrieved 40773 unique documents, whereas the ClueWeb09 dataset consists of 42044 unique documents. Note that our imple-mentation of topic modeling considers the complete docu-ments, irrespective of different fields (e.g. TITLE) that may be present.

The publicly available LETOR OHSUMED dataset con-tains training (approx. 60% of query-document pairs), test-ing (approx. 20% of query-document pairs) and validation (approx. 20% of query-document pairs) splits in five folds. It consists of 14430 unique documents. More information about this dataset can be obtained from [1]. Each feature vector in the LETOR OHSUMED also contains the docu-ment identifier. This information can help us relate a doc-ument from the downloaded OHSUMED text documents with its corresponding feature vector in LETOR OHSU-MED. LETOR OHSUMED also contains meta-information which allows us to relate text queries with their correspond-ing query identifiers. Many popular models have been implemented in RankLib 6 : Listnet , AdaRank-MAP , AdaRank-NDCG , Coordinate Ascent , LambdaMART and MART , which belong to the listwise class of LTR models, as well as the pairwise models RankNet , Rank-Boost and LambdaRank , and the pointwise model Linear Re-gression -L2 Norm and Random Forests . Note that mod-els implemented in RankLib have been used as strong state-of-the-art baselines in many recent works such as [26, 23]. We will compare our method against the aforementioned methods, as well as a number of other publicly available im-plementations: SVMMAP 7 , a listwise model, RankSVM-Struct a pairwise model, and the recently proposed DirectRank .
Finally, we will also compare our approach with a seman-tic search model similar to the one proposed in [4], which we have slightly modified to handle the feature instances needed for the LTR task; we will refer to this model as Se-mantic Search . Note that while the authors of [4] mention that their model can be adapted to incorporate LTR fea-tures, they have not included such an evaluation in their work. This model bears some resemblance to our proposed framework, in that both models are capable of generating latent topic similarity features dynamically. In particular, Semantic Search can be made to generate topic similar-ity with minor changes to the original model. However, a crucial difference is that we compute topic similarity in the regularized space, whereas Semantic Search computes topic similarity in a so-called  X  X oncept space X , which is very sim-h ttp://people.cs.umass.edu/  X  vdang/ranklib.html http://projects.yisongyue.com/svmmap/ http://research.microsoft.com/en-us/um/beijing/projects/letor/letor4.0/baselines/ ranksvm-struct.html ilar to the space obtained by the latent semantic indexing ( LSI ) model. We have also experimented with a variant of Semantic Search , proposed in [12], but found its perfor-mance on LTR datasets to be very similar to, but slightly weaker than that of the model from [4]. Finally, note that we have not considered any unsupervised topic models for comparison, as such models cannot make use of relevance judgements during the training process, and can therefore not be competitive with LTR models in this context.
Hyperparameter Values: We use the following symmet-ric hyperparameter values in the LDA topic model:  X  = 50 K a nd  X  = 0 . 01, where K is the number of topics. This value is used throughout all experiments, for both our model and other approaches that use latent topics. These values have also been used e.g. in LDA for document retrieval [30]. We have experimented with different hyperparameter val-ues, but did not obtain any significantly different results. We used the validation set for model selection, which is com-mon for LETOR baselines [26, 19]. All NDCG results have been averaged across five folds.

Parameter tuning for comparative models: The parame-ters of the comparative LTR models in each fold were tuned using the validation set. To replicate the results for LETOR OHSUMED, we followed the official guidelines 9 , which state:  X  The validation set can only be used for model selection (set-ting hyper-parameters and model structure), but cannot be used for learning. Most baselines released in LETOR web-site use MAP on the validation set for model selection; you are encouraged to use the same strategy and should indicate if you use a different one.  X  For consistency, we used the same rules for model selection in the three other datasets. Parameters were tuned for each fold separately, as is usual for the LTR setting.

Parameter tuning for our model: We need to tune the reg-ularization parameter C , the learning rate and the number of topics K for each fold. In each fold, we varied C from 0.1 to 1 in steps of 0.1, and K from 10 to 1000 in steps of 10. The learning rate values that we experimented with are from 10 to 200 in steps of 10. The number of iterations of the Markov chain was set to 1000. Using these ranges for the pa-rameters, as for the comparative models, we searched for the configuration that produced the best mean average precision (MAP) score on the validation set. Note that because we use regularized Bayesian inference, overfitting in our model is unlikely, despite using 1000 iterations for the Markov chain. Moreover, there is evidence that models based on Bayesian inference in general tend not to overfit [28].

Description of the experiments: We have conducted two types of experiments. First, we have compared our approach with the comparative models, using their default feature sets, as described in Section 5.1. The results of this exper-iment are discussed in Section 5.4.1. While this allows us to find out whether our approach can outperform the exist-ing state-of-the-art LTR models, it should be noted that we are using a latent topic similarity feature, which other mod-els do not have access to, and it might be that our model is performing better simply because it has access to better features. Therefore, in a second experiment, we have added h ttp://research.microsoft.com/en-us/um/beijing/projects/letor//letor3baseline.aspx the query-document latent topic similarity as an additional f eature for the comparative models. In particular, we have used LDA , following the procedure described in [30] to in-dependently compute query-document topic similarity. We also experimented with the cosine similarity for comparing queries and documents in the latent topic space, but the re-sults were inferior to those of the method from [30]. As for our model, the number of latent topics K was automatically determined using the validation set by varying the number of topics from 10 to 200 in steps of 10. The number of it-erations in the Gibbs sampler for computing the topic simi-larity using the method from [30] is 1000. This experiment will allow us to assess whether our integrated model has any benefits over models that simply use latent topics as one of the features. The results are presented in Section 5.4.2.
Evaluation metric: We have used the PERL evaluation tool available in LETOR 3.0 for evaluating all the models Our evaluation metric is NDCG which is popular for LTR evaluation; we refer to [1] for a definition of this metric. We will use NDCG@k to present our main results, where k = 1 , 3 , 5 , 8 , 10. We have tested for statistical significance using the paired t-test, as is usual for LTR experiments. Figure 1 presents the results for the considered datasets. As can be seen from the figure, our model consistently out-performs all the comparative models, for each of the datasets and NDCG cut-offs. The improvements against each of the comparative methods are statistically significant according to the paired t-test with p &lt; 0 . 05 . For the AQUAINT dataset, DirectRank , Semantic Search , Coordinate Ascent , and LambdaMART do much better than the other comparative models. For the WT2G dataset, LambdaRank performs very poorly, with our model being the only one that clearly dif-fers. The results for ClueWeb09 and LETOR OHSUMED are consistent with the results for AQUAINT. In both cases, our model considerably outperforms the other, while Direc-tRank also shows good performance among the comparative models. These results confirm our intuition that integrating latent topic information with maximum margin learning is capable of outperforming the state-of-the-art approaches.
To better understand the differences in performance across different queries, Table 1 shows a winning number compari-son. We refer to [19] for a detailed description of this metric, which has also been used in e.g. [26]. As we can see from the table, our model substantially outperforms all of the com-parative models, with DirectRank again the best performer among the comparative methods.
We present the results for the second experiment in Fig-ure 2. As we can see from this figure, the performance of many comparative methods has slightly improved or has re-mained same as in the previous result. For example, in the AQUAINT dataset, performance of many comparative mod-els has improved and none of the results have deteriorated. Therefore, in the majority of cases latent topic information leads to a slightly improved performance. However, even h ttp://research.microsoft.com/en-us/um/beijing/projects/letor/letor3download.aspx Table 2: Winning number comparison when the la-t ent topic similarity feature is incorporated in the comparative methods. with the latent topic feature added, none of the compara-tive methods can outperform our approach. The improve-ment is still statistically significant for all datasets and all models, according to the paired t-test with p &lt; 0 . 05, except at NDCG@8 in WT2G dataset where Coordinate Ascent model performs equally well. For the LETOR OHSUMED dataset, our model performs strictly better than each of the comparative methods, except at NDCG@8 where Co-ordinate Ascent performs equally well. Linear Regres-sion model also shows good performance in this experiment. We can see that the performance gap between the compara-tive models and our model has slightly decreased compared to Figure 1. Table 2 presents the winning number results. Comparing this table with Table 1 clearly shows that the relative performance of the comparative methods has im-proved. This demonstrates that latent topic information is helpful, even when used as a standard feature in other LTR models. However, our approach still performs consistently better, confirming the benefit of using an integrated model.
When looking in more detail at the per-query performance in each dataset, we note that our model especially outper-forms the comparative methods for long queries, e.g. con-sisting of two words or more. In Table 3, we present this comparison in terms of the percentage of winning numbers, Table 3: Query level performance in terms of the p ercentage of winning numbers for different query lengths. which shows that the relative performance also improves with longer queries. These comparisons are done by consid-ering the latent topic feature in the comparative methods. This result is expected, as longer queries make it easier to assign meaningful topics to the query.
We have presented a novel LTR model that combines la-tent topic information with maximum margin learning in a unified way. In our model the latent topic representation is directly regularized with a pairwise maximum margin con-straint, which leads to more informative latent topics. We have conducted extensive experiments using benchmark col-lections and have shown clear improvements over the state-of-the-art comparative methods. The main strength of our model stems from the fact that topic similarity is computed in the regularized latent topic space. This allows for a direct interplay between the pairwise maximum margin classifier and the topic model, which ensures that the topics which are learned are maximally informative for the prediction of la-bels. As our experiments show, such an integrated approach outperforms the existing two-stage de-coupled approach to incorporating latent topics for in LTR models. One inter-esting line for future work would be to look at different LTR tasks such as temporal LTR, for which the use of temporal topic models seems promising.
