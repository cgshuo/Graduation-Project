
Singapore University of Technology and Design University of Cambridge are considered: Combinatory Categorial Grammar (CCG) and dependency grammar. Given the search, and investigate several alternative training algorithms.
 system. 1. Introduction
Word ordering is a fundamental problem in natural language generation (NLG, Reiter or lemmas, as input, the task is to generate a fluent and grammatical sentence using those words. Additional annotation may also be provided with the input X  X or example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007;
Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word order-ing. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (transla-tion) and fluency (linearization), and can potentially improve target grammaticality for alone word ordering component can in principle be applied to a wide range of text generation tasks, including transfer-based machine translation (Chang and Toutanova 2007).
 at controling local fluency. Syntax-based language models, in particular dependency language models (Xu, Chelba, and Jelinek 2002), are sometimes used in an attempt to improve global fluency through the capturing of long-range dependencies. In this article, we take a syntax-based approach and consider two grammar formalisms: Com-binatory Categorial Grammar (CCG) and dependency grammar. Our system also em-ploys a discriminative model. Coupled with heuristic search, a strength of the model is that arbitrary features can be defined to capture complex syntactic patterns in output hypotheses. The discriminative model is trained using syntactically annotated data.
 lem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the trav-eling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn, tion of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial-time inference is feasible. In fluency improvement (Blackwood, de Gispert, and Byrne fixed, so that word ordering elsewhere is strictly local.
 ular, we treat syntax-based word ordering as a structured prediction problem, for which the input is a multi-set (bag) of words and the output is an ordered sentence, together with its syntactic analysis (either CCG derivation or dependency tree, depending on the grammar formalism being used). Given an input, our system searches for the highest-scored output, according to a syntax-based discriminative model. One advantage of this formulation of the reordering problem, which can perhaps be thought of as a  X  X ure X  text realization task, is that systems for solving it are easily evaluated, because all that is required is a set of sentences for reordering and a standard evaluation metric such as BLEU (Papineni et al. 2002). However, one potential criticism of the  X  X ure X  (e.g., in statistical machine translation systems) the input does provide constraints on the possible output orderings. Our general formulation still allows task-specific contraints to be added if appropriate. Hence as a test of the flexibility of our system, 504 and a demonstration of the applicability of the system to more realistic text gener-ation scenarios, we consider two further tasks for the dependency-based realization system.
 tem, determined by two parameters. The first is whether POS information is provided for each word in the input multi-set. The second is whether syntactic dependencies be-tween the words are provided. The extreme case is when all dependencies are provided, in which case the problem reduces to the tree linearization problem (Filippova and
Strube 2009; He et al. 2009). However, the input can also lie between the two extremes of no-and full-dependency information.
 scenario, in that lemmas, rather than inflected words, are provided as input. Hence some modifications are required to our system in order that it can perform some word inflection, as well as deciding on the ordering. The shared task data also uses labeled, rather than unlabeled, syntactic dependencies, and so the system was modified to incorporate labels. The final result is that our system gives competitive BLEU scores, compared to the best-performing systems on the shared task.
 use of syntax, and the search for a sentence together with a single CCG derivation or dependency tree, the search space is exponentially larger than the n -gram word permutation problem. No efficient algorithm exists for finding the optimal solution.
Kay (1996) recognized the computational difficulty of chart-based generation, which has many similarities to the problem we address in his seminal paper. We tackle the high complexity by using learning-guided best-first search, exploring a small path in the whole search space, which contains the most likely structures according to the dis-criminative model. One of the contributions of this article is to introduce, and provide a discriminative solution to, this difficult structured prediction problem, which is an interesting machine learning problem in its own right.
 and Clark 2011; Zhang, Blackwood, and Clark 2012; Zhang 2013). It includes a more detailed description and discussion of our guided-search approach to syntax-based word ordering, bringing together the CCG-and dependency-based systems under one unified framework. In addition, we discuss the limitations of our previous work, and show that a better model can be developed through scaling of the feature vectors. The resulting model allows fair comparison of constituents of different sizes, and enables the learning algorithms to expand negative examples during training, which leads to significantly improved results over our previous work. The competitive results on the
NLG 2011 shared task data are new for this article, and demonstrate the applicability of our system to more realistic text realization scenarios.
 2. Overview of the Search and Training Algorithms
In this section, the CCG-based system is used to describe the search and training algorithms. However, the same approach can be used for the dependency-based system, as described in Section 4: Instead of building hypotheses by applying CCG rules in a bottom-up manner, the dependency-based system creates dependency links between words.
 a plausible CCG derivation. The search space of the decoding problem consists of all possible CCG derivations for all possible word permutations, and the search goal is to find the highest-scored derivation in the search space. This is an NP-hard problem, as mentioned in the Introduction. We apply learning-guided search to address the high complexity. The intuition is that, because the whole search space cannot be exhausted in order to find the optimal solution, we choose to explore a small area in the search search space containing the most plausible hypotheses is explored.
 agenda to order hypotheses, and expands the highest-scored hypothesis on the agenda at each step. The resulting hypotheses after each hypothesis expansion are put back on the agenda, and the process repeats until a goal hypothesis (a full sentence) is found. This search process is guided by the current scores of the hypotheses, and the search path will contain the most plausible hypotheses if they are scored higher than implausible ones. An alternative to best-first search is A of a heuristic function to estimate future scores. A  X  can potentially be more efficient given an effective heuristic function; however, it is not straightforward to define an admissible and accurate estimate of future scores for our problem, and we leave this research question to future work.
 sentence together with its CCG derivation. Hypotheses are constructed bottom X  X p: starting from single words, smaller phrases are combined into larger ones according to CCG rules. To allow the combination of hypotheses, we use an additional struc-ture to store a set of hypotheses that have been expanded, which we call accepted 506 hypotheses . When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses. The data and Charniak 1998), and we adopt the term chart for this structure. However, note there are important differences to the parsing problem. First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling. Second, although we use the term chart , the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. In our previous papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT decoding (Koehn 2010). However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used. We will also investigate the possibility of applying dynamic-programming-style pruning to the chart.
 speed and accuracy of the resulting decoder. CCGBank (Hockenmaier and Steedman 2007) is used to train the model. For each training sentence, the corresponding CCGBank
All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses. From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both gram-matical and fluent. Nevertheless, it is the most feasible choice given the training data available.
 because the agenda is ordered according to the hypothesis scores. Hence, a better model standard hypotheses are scored higher than all non-gold hypotheses, and therefore only gold-standard hypotheses are expanded before the gold-standard goal hypothesis is found. In this case, the minimum number of hypotheses is expanded and the output is correct. The best-first search decoder is optimal not only with respect to accuracy but of the training algorithm: to find a model that scores gold-standard hypotheses higher than non-gold ones.
 model than standard structured prediction problems, for example, CKY parsing for
CCG (Clark and Curran 2007b). If we take gold-standard hypotheses as positive training examples, and non-gold hypotheses as negative examples, then the training goal is to find a large separating margin between the scores of all positive examples and all negative examples. For CKY parsing, the highest-scored negative example can be found via optimal Viterbi decoding, according to the current model, and this negative example can be used in place of all negative examples during the updating of parameters. In a good model has already been trained, and therefore we cannot run the decoding algorithm in the standard way during training. In our previous papers (Zhang and
Clark 2011; Zhang, Blackwood, and Clark 2012), we proposed an approximate online training algorithm, which forces positive examples to be kept in the hypothesis space without being discarded, and prevents the expansion of negative examples during the training process (so that the hypothesis space does not get too large). This algorithm ensures training efficiency, but greatly limits the space of negative examples that is to expand negative examples, which leads to improved performance.
 a stronger model for learning-guided search than for dynamic programming (DP) X  based search, such as CKY decoding. For CKY decoding, the model is used to compare hypotheses within each chart cell, which cover the same input words. In contrast, for the best-first search decoder, the model is used to order hypotheses on the agenda, which can cover different numbers of words. It needs much stronger discriminating power, so that it can determine whether a single-word phrase is better than, say, a 40-word sentence. In this article we use scaling of the hypothesis scores by size, so that hypotheses of different sizes can be fairly compared. We also find that, with this new approach, negative examples can be expanded during training and a single beam applied to the chart, resulting in a conceptually simpler and more effective training algorithm and decoder. 3. CCG-Based Word Ordering 3.1 The CCG Grammar
We were motivated to use CCG as one of the grammar formalisms for our syntax-based realization system because of its successful application to a number of related tasks, such as wide-coverage parsing (Hockenmaier 2003; Clark and Curran 2007b; Auli and
Lopez 2011), semantic parsing (Zettlemoyer and Collins 2005), wide-coverage semantic analysis (Bos et al. 2004), and generation itself (Espinosa, White, and Mehay 2008).
The grammar formalism has been described in detail in those papers, and so here we provide only a short description.
 with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorization information. During CCG parsing, and during our search procedure, categories are combined using CCG X  X  combinatory rules. For example, a verb phrase in English ( S \ NP ) can combine with an NP to its left, in this case using the combinatory rule of (backward) function application: operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase:
Such a type-raised category can then combine with a transitive verb type using the rule of forward composition: stances directly from the derivations in CCGbank (Hockenmaier and Steedman 2007), rather than defining the combinatory rule schema manually as in Clark and Curran 508 (2007b). Hence the grammar we use can be thought of as a context-free approximation to the mildly content sensitive grammar arising from the use of generalized composition rules (Weir 1988). Hockenmaier (2003) contains a detailed description of the grammar 3.2 The Edge Data Structure algorithm. An edge corresponds to a sentence or phrase with a CCG derivation. Edges are built bottom X  X p, starting from leaf edges , which are constructed by assigning possible lexical categories to input words. Each leaf edge corresponds to an input word with a particular lexical category. Two existing edges can be combined if there exists a
CCG rule (extracted from CCGbank, as described earlier) that combines their category labels, and if they do not contain the same input word more times than its total count in the input. The resulting edge is assigned a category label according to the CCG rule, and covers the concatenated surface strings of the two sub-edges in their order of combination. New edges can also be built by applying unary rules to a single existing edge. We define a goal edge as an edge that covers all input words.
 derivations. Edge equivalence is used for comparison with gold-standard edges. Two edges are DP-equivalent when they have the same DP-signature . Based on the feature templates in Table 1, we define the DP-signature of an edge as the CCG category at the root of its derivation, the head word associated with the root category, and the multi-set of words it contains, together with the word and POS bigrams on either side of its surface string. 3.3 The Scoring of Edges
Edges are built bottom X  X p from input words or existing edges. If we treat the assign-ment of lexical categories to input words and the application of unary and binary CCG recursively as the sub-structure resulting from its top action plus the structure of its sub-edges (if any), as shown in Figure 1. Here the top action of an edge refers to the most recent action that has been applied to build the edge.
 score of an edge e is defined as: ( e ) represents the feature vector of e and  X   X  is the parameter vector of the model.
In this equation, e s  X  e represents a sub-edge of e . Leaf edges do not have any sub-edges. Unary-branching edges have one sub-edge, and binary-branching edges have feature templates in Table 1. Example instances of the feature templates are given according to the example string and CCG derivation in Figure 2. For leaf edges,  X  ( e ) includes information from the unary rule; for binary-branching edges,  X  ( e ) includes information from the binary rule, and additionally the token, POS, and lexical cat-egory bigrams and trigrams that result from the surface string concatenation of its sub-edges.
 510 as e is built during the decoding process: f ( e s ) (for all e s different sizes during decoding. The size of an edge can be measured in terms of the number of words it contains, or the number of syntax rules in its structure. We define the size of an edge as the number of recursive sub-edges in the edge plus one (e.g., the assignment for leaf edges, and rule application for unary/binary edges) that have been numbers of features, which can make the training of a discriminative linear model more difficult. Note that it is common in structured prediction problems for feature vectors to have slightly different sizes because of variant feature instantiation conditions. In
CKY parsing, for example, constituents with different numbers of unary rules can be kept in the same chart cell and compared with each other, provided that they cover the same span in the input. In our case, however, the sizes of two feature vectors under comparison can be very different indeed, since a leaf edge with one word can be compared with an edge over the entire input sentence.
 this linear model, and obtained competitive results. However, as explained in Section 2, only positive examples were expanded during training, and the expansion of negative examples led to non-convergence and made online training infeasible. In this article, in order to increase the discriminating power of the model and to make use of negative examples during training, we apply length normalization to the scoring function, so linear model score by the number of recursive sub-edges in the edge plus one. For a given edge e , the new score  X  f ( e ) is defined as: the score  X  f ( e ) represents an averaged value of  X  ( e the number of recursive sub-edges plus one (i.e., the total actions), and is independent needs to be adjusted correspondingly, which will be discussed subsequently. 3.4 The Decoding Algorithm
The decoding algorithm takes a multi-set of input words, turns them into a set of leaf edges, and searches for a goal edge by repeated expansion of existing edges. For best-first decoding, an agenda and a chart are used. The agenda is a priority queue on which size beam used to record a limited number of accepted edges. During initialization, as the output, and the decoding finishes. Otherwise it is extended with unary rules, and combined with existing edges in the chart, using binary rules to produce new edges.
The resulting edges are scored and put on the agenda, and the original edge is put into by size, and the largest is taken as the current default output. Then the sorted list is traversed, with an attempt to greedily concatenate the current edges in the list to the contain some input words more times than its count in the input), the current edge is discarded. Otherwise, the current default output is updated.
 number of highest-scored edges that cover a particular number of words. This structure is similar to the chart used for phrase-based SMT decoding. The main reason for the multiple beams is the non-comparability of edges in different beams, which can have gle beam structure containing the top-scored accepted edges. This simple data structure is enabled by the use of the scaled linear model, and leads to comparable accuracies agenda-based search, because edges of different sizes will ultimately be compared with each other on the agenda. 512 among those that have the same DP-signature. During decoding, before a newly con-off the chart and compared with the newly constructed edge e , with the higher scored edge  X  e being put into the chart and the lower scored edge e newly constructed edge e is not discarded, then we expand e to generate new edges. e can have DP-equivalent edges in the agenda or the chart, which had been generated by expansion of its DP-equivalent predecessor e  X  = e 0 . Putting such new edges on the agenda will result in the system keeping multiple edges with the same signature. How-ever, because applying DP-style pruning to the agenda requires updating the whole agenda, and is computationally expensive, we choose to tolerate such DP-equivalent duplications in the agenda.
 initialized agenda with all leaf edges. I NIT C HART returns a cleared chart. T
Algorithm 1 The decoding algorithm. c  X  I NIT C HART ( ) while not T IME O UT ( ) do end while returns true if the timeout limit has been reached, and false otherwise. P the top edge from the agenda and returns the edge. G OAL T returns true if and only if the edge is a goal edge. DPC HART e exists, it is popped off the chart and compared with e , with the lower scored edge e being discarded, and the higher scored edge  X  e being put into the chart. The function returns the pair e  X  and  X  e . C AN C OMBINE checks whether two edges can be combined in a given order. Two edges can be combined if they do not contain an overlapping categories can be combined according to the CCG grammar. A to its score, and, in the latter case, the lowest scored edge in the beam is pruned when the chart is full. 3.5 The Learning Algorithm We begin by introducing the training algorithm of our previous papers, shown in
Algorithm 2, which has the same fundamental structure as the training algorithm of this article but is simpler. The algorithm is based on the decoder, where an agenda is used as a priority queue of edges to be expanded, and a set of accepted edges is kept in a fixed-size chart. The functions I NIT A GENDA , I NIT G those used in the decoding algorithm. G OLD S TANDARD takes an edge and returns true if and only if it is a gold-standard edge. M IN G OLD returns the lowest scored gold-standard edge in the agenda. U PDATE P ARAMETERS represents the parameter update algorithm. R ECOMPUTE S CORES updates the scores of edges in the agenda and chart after the model is updated.
 edges. During each step, the edge e on top of the agenda is popped off. If it is a gold-standard edge, it is expanded in exactly the same way as in the decoder, with the newly a gold-standard edge, we take it as a negative example e  X  gold-standard edge on the agenda e + as a positive example, in order to make an update which can be proved by contradiction. 1 For example, they may not cover the same words, or even the same number of words.
This is different from online training for CKY parsing, for which both positive and negative examples used to adjust parameter vectors reside in the same chart cell, and cover the same sequence of words. The training goal of a typical CKY parser (Clark and Curran 2007a, 2007b) is to find a large separation margin between feature vectors of different derivations of the same sentence, which have comparable sizes. Our goal is to score all gold-standard edges higher than all non-gold edges regardless of their size, which is a more challenging goal. After updating the parameters, the scores of the agenda edges above and including e  X  , together with all chart edges, are updated, and e is discarded before the start of the next processing step. 514
Algorithm 2 The training algorithm of our previous papers. 3: while not T IME O UT ( ) do 4: new  X  [] 6: if G OLD S TANDARD ( e ) and G OAL T EST ( e ) then 7: return e 8: end if 9: if not G OLD S TANDARD ( e ) then 14: continue 15: end if 16: ( e  X  ,  X  e )  X  DPC HART P RUNE ( c , e ) 17: if e  X  is e then 18: continue 19: end if 20: for e  X   X  U NARY ( e , grammar ) do 22: end for 23: for  X  e  X  c do 24: if C AN C OMBINE ( e ,  X  e , grammar ) then 25: e  X   X  B INARY ( e ,  X  e , grammar ) 27: end if 28: if C AN C OMBINE (  X  e , e , grammar ) then 29: e  X   X  B INARY (  X  e , e , grammar ) 31: end if 32: end for 33: for e  X   X  new do 35: end for 36: A DD ( c , e ) 37: end while wards the top of the agenda, and, crucially, pushes them above non-gold edges (Zhang and Clark 2011). Given a positive example e + and a negative example e style update is used to penalize the score for  X  ( e  X  ) and reward the score of  X  ( e
This method proved effective empirically (Zhang and Clark 2011), but it did not converge well when an n -gram language model was integrated into the system (Zhang, Blackwood, and Clark 2012).
 tive than the perceptron update and enabled the incorporation of a large-scale language model (Zhang, Blackwood, and Clark 2012). This method treats parameter update as finding a separation between gold-standard and non-gold edges. Given a positive example e + and a negative example e  X  , we make a minimum update to the parameters so that the score of e + is higher than that of e  X  by a margin of 1:
The update is similar to the parameter update of online large-margin learning algo-rithms, such as 1-best MIRA (Crammer et al. 2006), and has a closed-form solution: results when allowing the expansion of negative examples during training, which can potentially improve the discriminative model (since expanding negative examples can result in a more representative sample of the search space). We address this issue by introducing a scaled linear model in this article, which, when combined with the expansion of negative examples, significantly improves performance. We apply the same online large-margin training principle; however, the parameter update has to be between  X  f ( e + ) and  X  f ( e  X  ) instead of f ( e + ) and f ( e sponding to the parameter update becomes: where  X   X  0 and  X   X  represent the parameter vectors before and after the update, respec-tively. The equation has a closed-form solution: where M AX N ON G OLD returns the highest-scored non-gold edge in the chart. In ad-dition to the aforementioned difference in parameter updates, new code is added to perform additional updates when gold-standard edges are removed from the chart.
In our previous work, parameter updates happen only when the top edge from the agenda is not a gold-standard edge. In this article, the expansion of negative training examples will lead to negative examples being put into the chart during training, and hence the possibility of gold-standard edges being removed from the chart. There are 516
Algorithm 3 The training algorithm of this article. 3: while not T IME O UT ( ) do 4: new  X  [] 6: if G OLD S TANDARD ( e ) and G OAL T EST ( e ) then 7: return e 8: end if 9: if not G OLD S TANDARD ( e ) then 14: end if 15: ( e  X  ,  X  e )  X  DPC HART P RUNE ( c , e ) 18: R EMOVE ( c ,  X  e ) 20: else 21: if e  X  is e then 22: continue 23: end if 24: end if 25: for e  X   X  U NARY ( e , grammar ) do 27: end for 28: for  X  e  X  c do 29: if C AN C OMBINE ( e ,  X  e , grammar ) then 30: e  X   X  B INARY ( e ,  X  e , grammar ) 32: end if 33: if C AN C OMBINE (  X  e , e , grammar ) then 34: e  X   X  B INARY (  X  e , e , grammar ) 36: end if 37: end for 38: for e  X   X  new do 40: end for 41: e  X   X  A DD ( c , e ) 43:  X  e  X  M AX N ON G OLD ( c ) 46: end if 47: end while and there exists a gold-standard edge in the chart with the same DP-signature but a lower score, the gold-standard edge will be removed from the chart because of DP-style pruning (since only the highest-scored edge with the same DP-signature is kept in the chart).
 chart is a fixed-size beam), then the lowest scored edge on the chart will be removed. edge is pruned as the result of the expansion of a negative example. On the other hand, in order for the gold-standard goal edge to be constructed, all gold-standard edges that have been expanded must remain in the chart. As a result, our training algorithm triggers a parameter update whenever a gold-standard edge is removed from the chart, the scores of all chart edges are updated, and the original pruned gold edge is returned to the chart. The original pruned gold-standard edge is treated as the positive example for the update. For the first situation, the newly inserted non-gold edge with the same
DP-signature is taken as the negative example, and will be discarded after the parameter update (with a new score that is lower than the new score of the corresponding gold-as the negative example, and removed from the chart after the update.
 line 14 in Algorithm 2, which skips the expansion of negative examples, is removed in
Algorithm 3. Second, lines 16 X 20 and 42 X 46 are added in Algorithm 3, which correspond to the updating of parameters when a gold-standard edge is removed from the chart.
In addition, the definitions of U PDATE P ARAMETERS are different for the perceptron training algorithm (Zhang and Clark 2011), the large-margin training algorithm (Zhang,
Blackwood, and Clark 2012), and the large-margin algorithm of this article, as explained earlier. 4. Dependency-Based Word Ordering and Tree Linearization
As well as CCG, the same approach can be applied to the word ordering problem using other grammar formalisms. In this section, we present a dependency-based word ordering system, where the input is again a multi-set of words with gold-standard POS, and the output is an ordered sentence together with its dependency parse. Except for necessary changes to the edge data structure and edge expansion, the same algorithm can be applied to this task.
 more informed, dependency-based word ordering task: tree linearization (Filippova and Strube 2009; He et al. 2009), a task that is very similar to abstract word ordering from a computational perspective. Both tasks involve the permutation of a set of input dependency trees are given as input. As a result, the output word permutations are more constrained (under the projectivity assumption), and more information is available for search disambiguation.
 mar constraint is applied such that the output sentence has to be consistent with the input tree. There is a spectrum of grammar constraints between abstract word ordering might consist of a set of dependency relations between input words, but which do not form a complete unordered spanning tree. We call this word ordering task the partial-tree linearization problem, a task that is perhaps closer to NLP applications 518 sense that NLG pipelines might provide some syntactic relations between words for the linearization step, but not the full spanning tree.
 we extend by using the technique of expanding negative training examples (one of the overall contributions of this article). 4.1 Full-and Partial-Tree Linearization
Given a multi-set of input words W and a set of head-dependent relations H between and a dependency tree that contains all the relations in H . If each word in W is given given fixed POS tags. In all cases, a word either has exactly one (gold) POS tag, or no
POS tags. 4.2 The Edge Data Structure
Similar to the CCG case, edge refers to the data structure for a hypothesis in the decoding algorithm. Here a leaf edge refers to an input word with a POS tag, and a non-leaf edge refers to a phrase or sentence with its dependency tree. Edges are con-structed bottom X  X p, by recursively joining two existing edges and adding an unlabeled dependency link between their head words.

Table 2 shows the feature templates we use, which are inspired by the rich feature templates used for dependency parsing (Koo and Collins 2010; Zhang and Nivre 2011). edge, where h and m refer to the head and dependent of the newly constructed arc, s refers to the nearest sibling of m (on the same side of h ), and h to the left and rightmost dependents of h and m , respectively.

RVAL are maps from indices to word forms, POS , left valencies, and right valencies of words, respectively. Example feature instances extracted from the sentence in Figure 3 are shown in the example column. Because of the non-local nature of some of the feature templates we define, we do not apply DP-style pruning for dependency-based tree-linearization. 4.3 The Decoding Algorithm
The decoding algorithm is similar to that of the CCG system, where an agenda is a edges. During initialization, input words are assigned possible POS tags, resulting in a set of leaf edges that are put onto the agenda. For words with POS constraints, only 520 the allowed POS tag is assigned. For unconstrained words, we assign all possible POS tags according to a tag dictionary compiled from the training data, following standard practice for POS-tagging (Ratnaparkhi 1996).
 ways to generate new edges. Two edges can be combined by concatenation of the surface strings in both orders and, in each case, constructing a dependency link between their heads in two ways (corresponding to the two options for the head of the new link). When there is a head constraint on the dependent word, a dependency link can be constructed only if it is consistent with the constraint. This algorithm implements alized word ordering tasks X  X n a unified method.
 have the same definition as for Algorithm 1: I NIT A GENDA P
OP B EST , G OAL T EST , A DD . C AN C OMBINE checks whether two edges do not contain an overlapping word (i.e., they do not contain a word more times than its count in the input); unlike the CCG case, all pairs of words are allowed to combine according to the dependency model. C OMBINE creates a dependency link between two words, with the word order determined by the order in which the arguments are supplied to the function, and the head coming from either the first ( HeadLeft ) or second ( HeadRight ) argument (so there are four combinations considered and C in Algorithm 4). 4.4 The Learning Algorithm
As for the CCG system, an online large-margin learning algorithm based on the decod-ing process is used to train the model. At each step, the expanded edge e is compared with the gold standard. If it is a gold edge, decoding continues; otherwise e is taken as a negative example e  X  and the lowest-scored gold edge in the agenda is taken
Algorithm 4 The decoding algorithm for partial-tree linearization. c  X  I NIT C HART ( ) while not T IME O UT ( ) do end while as a positive example e + , and a parameter update is executed (repeated here from
Section 3.4):
The training process is essentially the same as in Algorithm 3, but with the CCG grammar and model replaced with the dependency-based grammar and model.
 system (Zhang 2013), the decoding step is finished immediately after the parameter update; in this article we expand the negative example, as in Algorithm3, putting it part containing negative examples). Our later experiments show that this method yields improved results, consistent with the CCG system. 522 5. Experiments We use CCGBank (Hockenmaier and Steedman 2007) and the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) for CCG and dependency data, respectively. CCGbank is the CCG version of the Penn Treebank. Standard splits were used for both: Table 3 gives statistics for the Penn Treebank.
 bags of words, with sequence information removed, and passed to our system as input data. The system outputs are compared to the original sentences for evaluation.
Following Wan et al. (2009), we use the BLEU metric (Papineni et al. 2002) for string comparison. Although BLEU is not the perfect measure of fluency or grammaticality, being based on n -gram precision, it is currently widely used for automatic evaluation and allows us to compare directly with existing work (Wan et al. 2009). Note also that one criticism of BLEU for evaluating machine translation systems (i.e., that it can only register exact matches between the same words in the system and reference translation), does not apply here, because the system output always contains the same words as the original reference sentence. For the dependency-based experiments, gold-standard dependency trees were derived from bracketed sentences in the treebank using the Penn2Malt tool. 2 preparing the input. Wan et al. used base NPs from the Penn Treebank annotation, and we follow this practice for the dependency-based experiments. For the CCG experi-ments we extract base NPs from CCGBbank by taking as base NPs those NPs that do not recursively contain other NPs. These base NPs mostly correspond to the base NPs from the Penn Treebank: In the training data, there are 242,813 Penn Treebank base NPs with an average size of 1.09, and 216,670 CCGBank base NPs with an average size of 1.19. 5.1 Convergence of Training
The plots in Figure 4 show the development test scores of three CCG models by the number of training iterations. The three curves represent the scaled model of this article, the online large-margin model from Zhang, Blackwood, and Clark (2012), and the perceptron model from Zhang and Clark (2011), respectively. For each curve, the
BLEU score generally increases as the number of training iterations increases, until it reaches its maximum at a particular iteration. We use the number of training iterations that gives the best development test scores for the training of our model when testing on the test data.
 the large-margin and the scaled CCG models are shown in Figure 5. For each model, the reflecting the convergence of learning-guided search. When the model gets better, fewer non-gold hypotheses are expanded before gold hypotheses, and hence it takes less time for the decoder to find the gold goal edge. Figure 6 shows the corresponding curve for dependency-based word ordering, with similar observations.
 time to train than those of our previous conference papers. However, the convergence 524 rate is also faster when negative training examples are expanded, as demonstrated by the rate of speed improvement as the number of training iterations increases. The train-ing times of the perceptron algorithm are close to those of the large-margin algorithm, and hence are omitted from Figures 5 and 6. The new model gives the best development innovations of this article: use of negative examples during training and the scaling of the model by hypothesis size. 5.2 The Effect of the Scaled Model and Negative Examples
Table 4 shows a set of CCG development experiments to measure the effect of the scaled model and the expansion of negative examples during training. With the standard linear model (Zhang, Blackwood, and Clark 2012) and no expansions of negative examples, our system obtained a BLEU score of 39.04. The scaled model improved the BLEU score by 1.41 BLEU points to 40.45, and the expansion of negative examples gave a further improvement of 3.02 BLEU points.
 ples during training is an important factor in achieving good performance. When no negative examples are expanded, the higher score of the scaled linear model demon-it is a more important advantage of the scaled linear model that it allows the expansion of negative examples during training, which was not possible with the standard linear model. In the latter case, training failed to converge when negative examples were expanded, reflecting the limitations of the standard linear model in separating the training data. Similar results were found for dependency-based word ordering, where the best development BLEU score improved from 44.71 (Zhang 2013) to 46.44 with the expansion of negative training examples. 5.3 The Effect of Search Time
Figure 7 shows the BLEU scores for the CCG system on the development data when at test time. The scaled model with negative training examples was used for this set of experiments, and the same model was used for all timeout settings. The results demonstrate that better outputs can be recovered given more search time, which is expected for a time-constrained best-first search framework. Recall that output is created greedily by combining the largest available edges, when the system times out.
Similar results were obtained with the dependency-based system of Zhang (2013), where the development BLEU scores improved from 42.89 to 43.42, 43.58, and 43.72 when the timeout limit increased from 5 sec to 10 sec, 30 sec, and 60 sec, respectively.
The scaled dependency-based model without expansion of negative examples was used in this set of experiments. 5.4 Example Outputs
Example output for sentences in the development set is shown in Tables 5 and 6, grouped by sentence length. The CCG systems of our previous conference papers and this article are compared, all with the timeout value set to 5 sec. All three systems perform relatively better with smaller sentences. For longer sentences, the fluency of 526 the output is significantly reduced. One source of errors is confusion between differ-ent noun phrases, and where they should be positioned, which becomes more severe with increased sentence length and adds to the difficulty in reading the outputs. The system of this article gave observably improved outputs compared with the two other systems. 528 5.5 Partial-Tree Linearization and the assumption was made that the input to the system would be a bag of words, with no constraints on the output structure. This somewhat artificial assumption allows a standardized evaluation but, as discussed previously, text generation applications are unlikely to satisfy this assumption and, in practice, the realization problem is likely to be easier compared with our previous set-up. In this section, we simulate practical situations in dependency-based pipelines by measuring the performance of our sys-tem using randomly chosen input POS tags and dependency relations. For maximum flexibility, so that the same system can be applied to different input scenarios, our system is trained without input POS tags or dependencies. However, if POS tags and dependencies are made available during testing, they will be used to provide hard constraints on the output (i.e., the output sentence with POS tags and dependencies must contain those in the input). From the perspective of search, input POS tags and dependencies greatly constrain the search space and lead to an easier search problem, with correspondingly improved outputs.
 dependency information in the input. For each test, we randomly sampled a percentage of words for which the gold-standard POS tags or dependencies are given in the input.
As can be seen from the table, increased amounts of POS and dependency information in the input lead to higher BLEU scores, and dependencies were more effective than POS tags in determining the word order in the output. When all POS tags and dependencies are given, our constraint-enabled system gave a BLEU score of 76.28. with different input settings. These examples illustrate the positive effect of input dependencies in specifying the outputs. Consider the second sentence as an example.
When only input words are given, the output of the system is largely grammatical but nonsensical. With increasing amounts of dependency relations, the output begins to look more fluent, sometimes with the system reproducing the original sentence when all dependencies are given. 5.6 Final Results
Table 9 shows the test results of various systems. For the system of this article, we take the optimal setting from the development tests, using the scaled linear model and 530 expansion of negative examples during training. For direct comparison with previous outperforms all previous systems and achieves the best published BLEU score on this task. It is worth noting that our systems without a language model outperform the system of our 2012 paper using a large-scale language model.
 systems of this article. One of the main reasons is that the CCG systems generated shorter outputs by not finding full spanning derivations for a larger proportion of be combined with the hypothesis being expanded, leading to an increased likelihood of full spanning derivations being unreachable. Overall, the CCG system recovered 93.98% of the input words in the test set, and the dependency system recovered 97.71%. 5.7 Shared Task Evaluation
The previous sections report evaluations on the task of word ordering, an abstract yet fundamental problem in text generation. One question that is not addressed by these which more considerations need to be taken into account in addition to word ordering.
We investigate this question using the 2011 Generation Challenge shared task data, which provide a common-ground for the evaluation of text generation systems (Belz et al. 2011).
 which consist of selected sections of the Penn WSJ Treebank, converted to syntactic dependencies via the LTH tool (Johansson and Nugues 2007). Sections 2 X 21 are used for training, Section 24 for development, and Section 23 for testing. A small number of sentences from the original WSJ sections are not included in this set. The input format of the shared task is an unordered syntactic dependency tree, with nodes being lemmas, and dependency relations on the arcs. Named entities and hyphenated words are broken into individual nodes, and special dependency links are used to mark them.
Information on coarse-grained POS, number, tense, and participle features is given to each node where applicable. The output is a fully ordered and inflected sentence. the core component being the dependency-based word ordering system of Section 4. In addition to minor engineering details that were required to adapt the system to this new task, one additional task that the generation system needs to carry out is morphological generation X  X inding the appropriate inflected form for each input lemma. Our approach is to perform joint word ordering and inflection using the learning-guided search of ambiguous lemmas. For a lemma, we generate one or more candidate inflections by generated according to the lemma itself and its input attributes, such as the number and tense. Some lemmas are unambiguous, which are inflected before being passed to the word ordering system. For the other lemmas, more than one candidate X  X  inflections are passed as input words to the word ordering system. To ensure that each lemma occurs only once in the output, a unique ID is given to all the inflections of the same lemma, making them mutually exclusive.
 or n X  X  ), for which the best inflection can be decided only when n -gram information is available. For these lemmas, we pass all possible inflections to the search module. For singular, plural) of a lemma is given as an attribute of the input node, and comparative and superlative adjectives have specific parts of speech. For those cases where the over to the search module for further disambiguation. The most ambiguous lemma types are verbs, which can be further divided into be and other verbs. The uniqueness node. In addition, for verbs in the present tense, the subject needs to be determined in order to differentiate between third-person singular verbs and others. This can be straightforward when the subject is a noun or pronoun, but can be ambiguous when the from the dependency tree. We leave all possible inflections of be and other verbs to the word ordering system whenever the ambiguity is not directly solvable from the subject dependency link. Overall, the pre-processing step generates 1.15 inflections for each lemma on average.
 the feature templates of Table 2 are used with additional labeled dependency features described subsequently. The main reason that the dependency-based word ordering algorithm can perform joint morphological disambiguation is that it uses rich syntac-tic and n -gram features to score candidate hypotheses, which can also differentiate between correct and incorrect inflections under particular contexts. For example, an honest person and a honest person can be differentiated by n -gram features, while Tom and Sally is and Tom and Sally are can be differentiated by higher-order dependency features.
 and the word ordering problem solved by Algorithm 4 is that the former uses labeled dependencies whereas Algorithm 4 constructs unlabeled dependency trees. We address this issue by assigning dependency labels in the construction of dependency links, and applying an extra set of features. The new features are defined by making a duplicate of the new copy with a dependency label.
 while references in the shared task data are raw sentences. We perform a pre-processing 532 step to obtain gold-standard training data by matching the input lemmas to the ref-erence sentence in order to obtain their gold-standard order. More specifically, given a training instance, we generate all candidate inflections for each lemma, resulting in an exponential set of possible mappings between the input tree and the reference sentence. We then prune these mappings bottom X  X p, assuming that the dependency tree is projective, and therefore that each word dominates a continuous span in the reference. After such pruning, only one correct mapping is found for the majority of the cases. For the cases where more than one mapping is found, we randomly choose one as the gold-standard. There are also instances for which no correct ordering can be found, and these are mostly due to non-projectivity in the shared task data, with a few cases being due to conflicts between our morphological generation system and the shared flicting instances are discarded, resulting in 36.2K gold-standard ordered dependency trees.
 the shared task. Our system outperforms the STUMABA system by 0.5 BLEU points, and the DCU system by 3.8 BLEU points. More evaluation of the system was published in Song et al. (2014). 6. Related Work tion of dependency structures (Barzilay and McKeown 2005; Filippova and Strube 2007, 2009; He et al. 2009; Bohnet et al. 2010; Guo, Hogan, and van Genabith 2011). On the other hand, Wan et al. (2009) study the ordering of a bag of words without any depen-dency information given. We generalize the word ordering problem, and formulate it as a task of ordering a multi-set of words, regardless of input syntactic constraints. on chart-based realization (Kay 1996; Carroll et al. 1999; White 2004, 2006; Carroll and
Oepen 2005). Kay (1996) first proposed the concept of chart realization, drawing analo-gies between realization and parsing of free order languages. He discussed efficiency improvement has been further investigated (Carroll et al. 1999; Carroll and Oepen 2005).
The inputs to these systems are logical forms, which form natural constraints on the interaction between edges. In our case, one constraint that has been leveraged in the dependency system is a projectivity assumption X  X e assume that the dependents of a word must all have been attached before the word is attached to its head word, and that spans do not cross during combination. In addition, we assume that the right dependents of a word must have been attached before a left dependent of the word is attached. This constraint avoids spurious ambiguities. The projectivity assumption is an important basis for the feasibility of the dependency system; it is similar to the chunking constraints of White (2006) for CCG-based realization.
 search. The search process of our algorithm is similar to that work, but the input is different: logical forms in the case of White (2004) and bags of words in our case.
Further along this line, Espinosa, White, and Mehay (2008) also describe a CCG-based realization system, applying  X  X ypertagging X  X  X  form of supertagging X  X o logical forms in order to make use of CCG lexical categories in the realization process. White and Rajkumar (2009) further use perceptron reranking on n -best realization output to improve the quality.
 2005). LaSO is a general framework for various search strategies. Our learning algorithm is similar to LaSO with best-first inference, but the parameter updates are different.
In particular, LaSO updates parameters when all correct hypotheses are lost, but our algorithm makes an update as soon as the top item from the agenda is incorrect.
Our algorithm updates the parameters using a stronger precondition, because of the large search space. Given an incorrect hypothesis, LaSO finds the corresponding gold hypothesis for a perceptron update by constructing its correct sibling. In contrast, our algorithm takes the lowest scored gold hypothesis currently in the agenda to avoid updating parameters for hypotheses that may have not been constructed.
 the easy-first algorithm of Shen, Satta, and Joshi (2007), which maintains a queue of hypotheses during search, and performs learning to ensure that the highest-scored hypothesis in the queue is correct. However, in easy-first search, hypotheses from the queue are ranked by the score of their next action, rather than the hypothesis score.
Moreover, Shen, Satta, and Joshi use aggressive learning and regenerate the queue after each update, but we perform non-aggressive learning, which is faster and is more fea-sible for our large and complex search space. Similar methods to Shen, Satta, and Joshi (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010).
Langford, and Marcu 2009), which addresses structured prediction problems that can be transformed into a series of simple classification tasks. The transformation is akin to greedy search in the sense that the complex structure is constructed by sequential classification decisions. The key problem that SEARN addresses is how to learn the t th structure is minimized. Similar to our framework, SEARN allows arbitrary features.
However, SEARN is more oriented to greedy search, optimizing local decisions. In contrast, our framework is oriented to best-first search, optimizing global structures. and beam-search framework for incremental structured prediction (Zhang and Clark 2011). In this framework, an output is constructed incrementally by a sequence of tran-training is performed based on the search process, with the objective function being the margin between correct and incorrect structures. The method involves an early-update strategy, which stops search and updates parameters immediately when the
Roark (2004) for incremental parsing, and later gained popularity in the investigations of many NLP tasks, including POS-tagging (Zhang and Clark 2010), transition-based dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010), and machine 534 translation (Liu 2013). Huang, Fayong, and Guo (2012) propose a theoretical analysis to the early-update training strategy, pointing out that it is a type of training method is lower than that of a non-gold structure, a violation exists. Our parameter udpate strategy in this article can also be treated as a mechanism for violation fixing. 7. Conclusion
We investigated the general task of syntax-based word ordering, which is a fundamental problem for text generation, and a computationally very expensive search task. We provide a principled solution to this problem using learning-guided search, a frame-work that is applicable to other NLP problems with complex search spaces. We com-pared different methods for parameter updates, and showed that a scaled linear model gave the best results by allowing better comparisons between phrases of different sizes, increasing the separability of hypotheses and enabling the expansion of negative examples during training.
 specificity, from  X  X ure X  word ordering without any syntactic information to fully-informed word ordering with a complete unordered dependency tree given. Experi-ments show that our proposed method can effectively use available input constraints in generating output sentences.
 cessfully applied to a more realistic application scenario, in particular one where some dependency constraints are provided in the input and word inflection is required as well as word ordering. Additional tasks that may be required in a practical text generation scenario include word selection, including the determination of content words and generation of function words. The joint modeling solution that we have proposed for word ordering and inflection could also be adopted for word selection, although the search space is greatly increased when the words themselves need deciding, particularly content words.
 Acknowledgments 536
