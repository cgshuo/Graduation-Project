 document retrieval. Ranking SVM is a typical method of learning to rank. We point out that there are two factors one must consider when applying Ranking SVM, in general a  X  X earning to rank X  method, to document retrieval. Fi rst, correctly ranking documents on the top of the result list is crucial for an Information Retrieval system. One must conduct training in a way that such ranked results are accurate. Second, the number of relevant documents can vary from query to query. One must avoid training a model biased toward queries with a la rge number of relevant documents. Previously, when existing methods that include Ranking SVM modifications in conventional Ra nking SVM, so it can be better used for document retrieval. Speci fically, we modify the  X  X inge Loss X  function in Ranking SVM to deal with the problems described above. We employ two methods to conduct programming. Experime ntal results show that our method, referred to as Ranking SVM for IR, can outperform the conventional Ranking SVM and other existing methods for document retrieval on two datasets. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Retrieval Models Algorithms, Experimentation, Theory Information retrieval, loss function, Ranking SVM number of features (e.g., term frequency, inversed document frequency, and document length) , which makes it possible to empirically tune ranking parameters [20]. Recently, however, a and anchor text, and query-indepe ndent features (e.g., PageRank and URL length) have proved usef ul in document retrieval while empirical tuning of ranking func tions has become increasingly difficult. 
Fortunately, in recent years more and more human-judged document retrieval results have become available. This makes it possible to employ supervised learning methodologies in the tuning of ranking functions. Many such efforts have been made using these approaches. 
In one of such effort, documen t retrieval is formalized as classification of documents into two categories: relevant and retrieval as binary classification and solves the classification problem using Support Vector Machines (SVM) and Maximum Entropy (ME). 
In another approach, document re trieval is formalized as a  X  X earning to rank X  problem in which documents are mapped into several ordered categories (ranks ). OHSUMED [9] is a data basis of SVM and apply their me thod to document retrieval. We refer to their method as Ranki ng SVM (or conventional Ranking SVM) in this paper. Specifically, Ranking SVM formalizes two categories (correctly ranked and incorrectly ranked). Other methods within this approach have also been proposed [1, 19, 24]. 
We explore the problem of applying learning to rank to document retrieval and propose a new learning method on the basis of Ranking SVM. We refe r to the method as Ranking SVM for IR. consideration when applying Ra nking SVM in a learning method for ranking documents being retrieved. Unfortunately, they are ignored in the existing methods, such as Ranking SVM. (1) To have high accuracy on top-ranked documents is crucial for an IR system. Analysis on click-through data from search engines shows that users usually click on top-ranked documents among returned search results [16, 17, 18]. The Normalized Discounted Cumulated Gain (NDCG) measure [10] used in evaluation of document retrieval also reflects this preference. Therefore, it is necessary to perform training so that the top-learning methods such as Ranking SVM, the losses (penalties) of incorrect ranking between highe r ranks and lower ranks and incorrect ranking among lower ranks are defined the same. (2) In reality some queries may have many relevant documents, while others may have a few. If we treat training data from different queries as equal, then th e model trained is biased toward queries with many relevant docum ents. Therefore, it is necessary to put higher weight on data fro m queries with fewer relevant documents. However, in existi ng learning methods such as Ranking SVM, the losses (penalties) from incorrect rankings from different queries are defined the same. 
To deal with the problems above, we propose using a new loss function for Ranking SVM used in document retrieval. The loss function is a modification of th e Hinge Loss function in Ranking SVM. Thus our method can be viewed as an adaptation of Ranking SVM to document retrieval. Specifically, we set varying losses for misclassification of instance pairs between different instance pairs from multiple queries. To reduce errors on top rankings, the loss function heavily pe nalizes errors with regard to the highest ranked documents. To increase influences of queries with fewer relevant documents, the loss function heavily penalizes errors from queries. We propose two learning methods to optimize the cost function: gradient descent and quadratic programming. 
Experimental results indicate that Ranking SVM for IR can outperform existing methods, including Ranking SVM on two datasets. One dataset is OHSUMED [9], and the other is from a commercial web search engine. One key question in document retr ieval is how to rank documents been placed on the development of ranking functions. 
Traditionally, document retrieva l methods only use a small number of features (e.g., term frequency, inversed document frequency, and document length). Thus, it is possible to empirically tune the parameters of ranking functions [20]. In that sense, the methods are unsupervised. Okapi BM25 [14] and language models for information retrieval (LMIR) [13, 21] are such methods. 
Currently, additional features ha ve proved useful for document retrieval, including structural features (e.g., title, anchor text, and URL) and query-independent feat ures (e.g., PageRank and URL length). This increase in featur es makes empirical tuning of parameters difficult. The paradi gm of employing supervised learning in construction of docum ent retrieval models has drawn recent attention. For instance, document retrieval is formalized as classification. Documents are judged within tw o categories: relevant and classification and solves it us ing SVM and Maximum Entropy. creating a ranking model. 
For another example, document retr ieval is regarded as learning to rank. Herbrich et al. [8] propose addressing the issue by means method as Ranking SVM in this pape r. Joachims [11] also applies the method to document retrieval. He utilizes click-through data to deduce pair-wise training data for learning Ranking SVM models. Burges et al. [1] propose employing relative entropy as a loss function and gradient descent as an algorithm to train a neural network model for document retrieval. are provided. Each instance is represented by a feature vector, and categories. They are referred to in this paper as  X  X oint-wise training X  and  X  X air-wise training X . 
In point-wise training, each instance (and its rank) is used as an independent training example. The goal of learning is to correctly map instances into intervals. 
For instance, Crammer et al. [4] propose a Perceptron-based learning algorithm called PRank. PRank trains a Perceptron model retaining not only a weight vector w vector c Perceptron model that successfully projects all the instances into k subintervals defined by c
In pair-wise training each instance pair is used as a training differences between ranks of instance pairs. 
For instance, the Herbrich et al. Ranking SVM [8] is such a method. That model formalizes learning to rank as learning for issue by using SVM. See also [1, 15, 19]. 
For other work on learning to rank, refer to[22,23,24,25,26,27]. (categories) represented by labels } , , , { order between the ranks preference relationship. A set of ranking functions and each of them can determine the preference relations between instances: 
Suppose that we are given a set of ranked instances = r from the space Y X  X  . The task here is to select with respect to the given ranked instances. 
Herbrich et al. [8] propose formalizing the above learning problem as that of learning for classification on pairs of instances. 
First, we assume that f is a linear function. where w product. Plugging (2) into (1) we obtain 
Note that the relation is expressed by a new vector pair and their relation to create a new vector and a new label. Let r and (2) x r denote the first and second instances, and let y y (2) denote their ranks, then we have 
From the given training data set S, we create a new training data set S' containing l labeled vectors. 
Next, we take S X  as classification data and construct a SVM model that can assign either positive label z = +1 or negative label z = -1 to any vector ) 2 ( ) 1 ( x x
Constructing the SVM model is equivalent to solving the following Quadratic Optimization problem: min ( ) subject to 0, , 1 1, , regularizer. In Figure 2, the solid line denotes the Hinge Loss function of Ranking SVM. Suppose that * w r is the weights in the SVM solution. ranking instances. 
When Ranking SVM is applied to document retrieval, an instance (feature vector) is created from one query-document pair [1]. Each feature is defined as a function of query and document. number of times in which the query term occurs in the document. The instances from all queries are then combined in training. There is no difference in treatments toward the instances from different queries. Furthermore, th ere is no difference in treatments between instance pairs from different rank pairs. learning to rank to IR. Let us take Ranking SVM as example and use data from the OHSUMED collec tion to look at the problems. OHSUMED [9] is data collection on document retrieval, as will be further explained in Section 6. 
In learning of Ranking SVM for IR, each instance x generated from one query-document pair and is labeled with one rank. There are three possible ranks: definitely relevant ( r observe its tendencies. Specifically, we conduct principle component analysis (PCA) on the data and display its first and second principle components. 
Figure 1 (a) shows the results for two randomly selected queries (query number 12 and 50). The horizontal axis and empty points represent the instances of query 12 and query 50. It is a problem that Ranking SVM doe s not take into consideration. Figure 1 (b) shows the ranking models between pairs of ranks: a b c r SVM). It also gives the ranking model of Ranking SVM. Because Ranking SVM treats the instance pa irs from all rank pairs equally and there are more instances in the ranks of r 2 and r model of Ranking SVM tends to be close to that of r not desirable, since it mean s that Ranking SVM emphasizes ranking on the middle and bottom. Actually, it should do the penalty of misclassification of instance pairs between different rank pairs. problem which Ranking SVM does not consider. Figure 1 (c) shows the ranking models of Ranking SVM created individually ranking model created when the data of the two queries are (and thus more instance pairs), although it should not have such a data from queries with varied relevant documents. explanation. In general, we obser ve that the same tendencies exist for all queries in OSUMED. 
One may argue that the characteristics of data depend on the features used and with more features available the problems happen when more features are available. In our current data analysis, we use all the conventi onal features utilized in document retrieval and our experimental results indicate that, with the features, we can achieve the same performance as those of state-standard feature set, we obser ve the phenomena explained above. 
We also note that other learning to rank [1, 19, 24] methods also do not take into consideration these two factors. To deal with the two problems desc ribed in Section 4, we define a new loss function on the basis of Hinge Loss. In the loss function between rank pairs and add query parameters  X   X  X  to adjust the bias across queries. We re-form alize the learning problem of Ranking SVM for IR as that of minimizing the following loss function. min ( ) 1 , , where k ( i ) denotes the type of ranks of instance pair i , i , and of  X  and misclassification of input instances into different classes has been proposed for other problems [28]. To the best of our knowledge, this is the first attempt of it in document retrieval. 
Figure 2 plots the shapes of diffe rent Hinge Loss functions with as that in Ranking SVM. 
Figure 2: Hinge Loss functions with di fferent penalty parameters We then consider how to de termine the values of the parameters  X   X  X  and  X   X  X . 
For  X   X  X , we propose a heuristic method for estimating the parameters on the basis of si mulation. Suppose that NDCG is used in evaluation. (In principle, any other measure can be used). We take the averaged drops in terms of NDCG as the parameter values when randomly changing the positions of documents in value of the parameter for a specific rank pair, we repeat the following processes. For each query, we first find its perfect ranking (NDCG@1 of the ranking is 1.0). We then randomly select one document from each of the ranks and reverse their positions. In this way we obtain a new ranking and we can calculate its NDCG@1. Usually there is a drop in NDCG@1. We dataset. Finally, we take the average performance drop as the value of rank parameter for the rank pair. As for  X   X  X , we define it as follows: With such a parameter, we can more penalize the errors on the queries with fewer instance pairs. As a result, the training will be conducted equally over all queries. function (9). The two methods are gradient descent and quadratic programming. The loss function in (9) can be rewritten as 
By differentiating (11) with respect to parameters w v , we obtain ww where  X  X  X  rr
We then define the iteration equa tions of gradient descent as 
Upon each iteration we reduce the cost function along its descent direction, as in (12). To determine the step size of iteration, we conduct a line search along the descent direction, as described in (13). In practice, instead of calculating each make all  X   X  X  fixed. Figure 3 provides the algorithm. In our method, instead of directly solving (9), we solve the equivalent Quadratic Optimization problem as described below. min ( ) subject to 0, , 1 1, , This is because the following theorem holds. = .

We will give the proof in the full version of the paper. It turns out that the method creates a SVM model as solution. 
For the optimization problem in (14), the Lagrange Function can be written as 
Then, the objective is to minimize (15) with respect to w . Setting the respective derivatives to zero, we get as well as the positive constraints ,, 1,, Substituting (16) and (17) into (15), we obtain the Lagrange dual function. Lzzxxxx (18) 
It gives a lower bound on the objective function (10). We maximize L D subject to the constraints The objective function here is si milar to that in Ranking SVM. The only difference lies on the use of the box constraints on parameter  X  v (19). For Ranking SVM, the upper bounds of all l , , 1 ,  X   X   X  = i  X  are the same ( l , , 1 , 0  X   X   X  =  X   X  i C the upper bounds vary according to different types of pairs. A larger C i corresponds to a more important rank pair or a pair from a query with fewer instances. As one measure for evaluating th e results of ranking methods, we used Normalized Discounted Cumulative Gain (NDCG) [10]. NDCG is a measure commonly used in IR, when there are more than two categories in relevance ranking. Given a query q NDCG score at the position m in the ranking of documents provided by a retrieval method is defined as where r ( j ) is the rating of the j th document and n constant. n i is chosen so that a perfect ranking X  X  NDCG score is 1. For queries whose returned documents are less than m , the NDCG score is only calculated for th e returned documents. In our experiments we measured NDCG at the positions of 1, 3, and 5. 
We also used Mean Average Precision (MAP) as evaluation measure for evaluating ranking met hods. MAP is widely used in instances (documents). MAP calculates the mean of average precisions over a set of queries. Given a query q precision is defined as the aver age of precision after each positive (relevant) instance is retrieved. Given a query q precision (AvgP i ) is calculated as: where j is the rank, M is the number of instances retrieved, pos( j ) is a binary function to indicates whether the instance in the rank j is positive (relevant), and P ( j ) is the precision at the given cut-off rank j : 
Pj We denote our method as Ranking SVM for IR (RSVM-IR). We further denote RSVM-IR with the two optimization options, gradient descent, and quadra tic programming, as RSVM-IR-GD and RSVM-IR-QP. 
Obviously, we can have a method in which we only consider the first factor in Section 4. That is, we set different rank parameters as 1.0. We denote it as RSVM-IR-Rank. Similarly, we can have a method of RSVM-IR-Query. In RSVM-IR-Rank and RSVM-IR-Query, we only employ quadratic programming as our optimization method. denotes the number of terms in th e query; |.| denotes the size of function; and idf (.) denotes the inverse document frequency. 1 log( ( , ) 1) 3 log( ( )) 7 log( 25 ) BM score RSVM-IR-QP 2.44E-13 2.28E-12 0.0391 Top 5 ranked doc p d d p n d p d n p Figure 5: Distribution of queries over numbers of instance pairs 
We use Ranking SVM (RSVM) as a baseline method in all the Ranking SVM for IR (RSVM-IR) when all the  X   X   X  values equals 1.0. We also compare our methods with BM25 [14] and language model for information re trieval (LMIR) [21]. For both BM25 and LMIR, we used the tool Lemur 1 (Language Models Toolkits for Information Retrieval). In the experiment, we made use of the OHSUMED collection [9]. consisting of 348,566 references and 106 queries. There are a total of 16,140 query-document pairs upon which relevance judgments where made. The rele vance judgments are either d data have been used in many e xperiments in IR, for example, the TREC-9 filtering track [14]. 
Each instance consists of a vector of features, determined by a query and a document. We adopted the standard features used in document retrieval [12]. Table 1 shows all the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and their co mbinations are features. BM25 score is another feature, which is calculated using the ranking method of BM25 [14]. We took log on the feature values in order tendencies of the results, based on preliminary experiments. Stop words are removed and stemming is conducted in indexing and retrieval. 
In MAP calculation, we define the Category d as positive (relevant) and the other two cate gories as negative (irrelevant). 
We conducted 4-fold cross-valida tion. We tuned the parameters for LMIR and BM25 with one of the trials and applied them to the other trials directly. The results reported in Figure 4 are the averaged of four trials. From Figure 4, we see our methods (both RSVM-IR-QP and RSVM-IR-GD) outperform Ranking SVM, BM25 and LMIR in terms of all measures. Furthermore, RSVM-IR-QP and RSVM-IR-GD are better than RSVM-IR-Rank and RSVM-IR-Query. The results i ndicate our method improves the baseline methods. We effectivel y deal with the two problems from which Ranking SVM suffers. We conducted a Sign Test on the improvements of RSVM-IR-QP over BM25, LMIR, and RSVM in terms of NDCG@1. Th e results shown in Table 2 indicate that the improvements are statistically significant (p-value &lt; 0.05) in terms of NDCG@1. . 
Statistics on the distribution of instances in the OHSUMED dataset include 2,252 query-documen ts labeled as d, 2,585 labeled our observation in Figure 1(b). It explains why RSVM-IR-Rank performs better than Ranking SVM (RSVM). We also analyze the highest ranks than Ranking SVM. For example, for query 9 ( X  X -cell lymphoma associated with au toimmune symptoms X ). The top five documents returned by ranking SVM and our method are listed in Table 3 (the scores of NDCG@1 are also given). We note that both the ranking methods incorrectly rank the two document pairs. Two (d, p) pairs reversed in the ranking by Ranking SVM and one (d, p) and one (p, n) pairs are reversed in the ranking by our method. However, the errors in our method are less problematic, since they are not at the top. The top-ranked results by Ranking SVM contain mo re errors than those of our method, because our method is trained to better perform at the top. 
We also provide statistics on th e number of instance pairs per query, as shown in Figure 5. The queries are grouped into different categories based on the ranges of the number of instance pairs. For example, all the queries in the category 1000-1999 should have instance pairs in between 1,000 and 1,999. We can see that the numbers of instance pairs may vary from query to query. As discussed in Section 4, we should consider a way to adjust the discrepancy. The statistics also explain why RSVM-IR-Query performs better than Ranking SVM. 
In the example data in Section 4 (queries 12 and 50), we also applied Ranking SVM and our met hods to it. Figure 6 shows the observe that our method outpe rforms Ranking SVM, indicating that it effectively deals with the problems Ranking SVM suffers from. 
Figure 6: NDCG curves with respect to queries 12 and 50 In the experiment, we made us e of a data collection from a commercial web search engine. Th e collection consists of 2,198 queries. Human judges have ma de relevance judgments on documents related to all queries. There are six ranks:  X  X efinitive X ,  X  X xcellent X ,  X  X ood X ,  X  X air X ,  X  X ad X , and  X  X etrimental X . In total, there are 74,276 query-document pairs (instances). Feature vectors are also generated from the query-document pairs. There are 426 features including those desc ribed in Section 6.3, as well as those made from hyperlink, anc hor text, URL, and PageRank. 
We randomly split the data into training set and test set. The not use LMIR as the baseline. 
For MAP calculation we used the top ranks  X  X efinitive X , negative. 
Figure 7 shows the performances on the test set. We see that our methods (RSVM-IR-QP and RSVM-IR-GD) outperform all the baselines in all measures. This indicates again that our approach is effective for improvi ng real IR problems. (We were not able to apply LMIR to the data because we obtained the data for BM25 because BM25 score exists as one of the features.) 
In the collection, 8,990 query-documents are labeled as  X  X efinitive X , 4,403 as  X  X xcellent  X , 3,735 labeled as  X  X ood X , 20463  X  X etrimental X . RSVM-IR-Rank ma kes larger improvements over Ranking SVM. 
In contrast, we can also see that RSVM-IR-Query does not make much improvement. In Figur e 8, the statistics on the dataset suggest the reason. Most of the queries fall into the range of 0-99 dataset. Figure 8: Distribution of queries over numbers of instance pairs In the paper, we have propos ed a new method for document retrieval on the basis of learning to rank. consideration when applying learning to rank to document with many relevant documents. Existing learning to rank methods for document retrieval, in cluding Ranking SVM, does not consider the two factors. We propose using a new loss function to deal with ranking problems. The new loss function naturally incorporates the two factors into the Hinge Loss function used in Ranking SVM, with two types of additional parameters. We employ gradient descent and quadratic programming to optimize significantly outperform Ranki ng SVM and other existing methods in document retrieval. We hope to explore future work in several areas, such as conducting theoretical analysis on the proposed method, applying more suitable for document retrieval. [1] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. [2] W. Chu and Z. Ghahramani. Ga ussian Process for Ordinal [3] W. Chu and S. Keerthi. New Approaches to Support Vector [4] K. Crammer and Y. Singer. Pranking with Ranking. [5] J. Gao, H. Qi, X. Xia, and J. Nie. Linear Discriminant Model [6] E. F. Harrington. Online Ranking/Collaborative filtering [7] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of [8] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin [9] W. R. Hersh, C. Buckley, T. J. Leone, and D. H. Hickam. [10] K. Jarvelin and J. Kekalain en. IR evaluation methods for [11] T. Joachims. Optimizing Sear ch Engines Using Clickthrough [12] R. Nallapati. Discriminative models for information [13] J. Ponte and W. B. Croft. A language model approach to [14] S. Robertson and D. A. Hull. The TREC-9 Filtering Track [15] A. Shashua and A. Levin. Taxonomy of Large Margin [16] C. Silverstein, M. Henzinger, H. Marais, and M. Moricz. [17] A. Spink, B. J. Jansen, D. Wolfram, and T. Saracevic. From [18] A. Spink, D. Wolfram, B. J. Jansen, and T. Saracevic. [19] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [20] G. Salton. The SMART Retrieval System: Experiments in [21] J. Lafferty and C. Zhai Document Language Models, Query [22] D. Grangier and S. Bengio. Exploiting Hyperlinks to Learn [23] S. Rajaram and S. Agarwal Generalization Bounds for k-[24] C. Burges. Ranking as Learning Structured Outputs. [25] N. Usunier, V. Truong, R. A. Massih, and P. Gallinari, [26] W. Chu and Z. Ghahramani Extensions of Gaussian [27] S. Yu, K. Yu, and V. Tresp, Collaborative Ordinal [28] K. Morik, P. Brockhausen, and T. Joachims. Combining 
