 } Record deduplication is the task of merging database records that refer to the same underlying entity. In relational data-bases, accurate deduplication for records of one type is of-ten dependent on the decisions made for records of other types. Whereas nearly all previous approaches have merged records of different types independently, this work models these inter-dependencies explicitly to collectively dedupli-cate records of multiple types. We construct a conditional random field model of deduplication that captures these re-lational dependencies, and then employ a novel relational partitioning algorithm to jointly deduplicate records. For two citation matching datasets, we show that collectively deduplicating paper and venue records results in up to a 30% error reduction in venue deduplication, and up to a 20% error reduction in paper deduplication.
 Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval ]: Information Search and Retrieval X  clustering General Terms: Algorithms, Performance Keywords: record linkage, deduplication, conditional ran-dom fields
A common prerequisite for knowledge discovery is accu-rately combining data from multiple, heterogeneous sources into a unified, mineable database. An important step in cre-ating such a database is record deduplication : consolidating multiple records that refer to the same abstract entity.
Most historical approaches have framed the deduplication problem as a set of independent, pairwise decisions. More recently, McCallum and Wellner [5] and Parag and Domin-gos [6] have demonstrated that making multiple dedupli-cation decisions collectively can provide better results than historical approaches. These models are types of conditional random fields (CRFs) [3], where the observed nodes are men-tions, and the predicted nodes are the deduplication deci-sions for each pair of nodes. The models are  X  X ollective X  in the sense that mentions are clustered based not only on their distance to each other, but also on their distance to all other mentions. By treating deduplication decisions in dependent relation to each other, inconsistencies and noise in the similarity metric may be overcome.

We extend this work to the case of relational databases, where the identity of a record often depends on the iden-tities of related records . For example, consider a database of research papers, where records can be of type paper or venue . If two paper records are labeled as duplicates, then it follows that the venue records corresponding to those pa-pers should also be labeled as duplicates. The converse is more subtly true: if two venues are duplicates, then this may slightly increase the probability that their correspond-ing papers are duplicates. We propose a CRF model that leverages these subtle interdependencies to make deduplica-tion decisions collectively across multiple record types, and we validate its performance on two real-world datasets.
The model is an instance of a conditional random field that jointly models the conditional probability of multiple deduplication decisions given an observed relational database.
CRFs [3] are undirected graphical models encoding the conditional probability of a set of output variables Y given a set of evidence variables X . Let X be a collection of random variables representing observed record mentions in a database. For clarity, assume there are only two types of records, X = ( X a , X b ), where X a = ( X a 1 ,...,X a n ( X 1 ,...,X b m ). The goal of deduplication is to partition X into clusters of records that refer to the same abstract entity.
To this end, we define a collection of binary random vari-ables Y = ( Y a , Y b ) indicating whether or not two records are duplicates. For example, Y a ij indicates whether or not records X a i and X a j are coreferent. We also define the bi-nary random variables R , where R ab ij indicates whether some relation R holds between record mentions X a i and X b j .
For example, in a research paper database, X a represents the set of paper records, X b represents the set venue records, Y ij indicates whether X a i and X a j are duplicates, and R indicates whether paper X a i was published at venue X b j model the conditional distribution P ( Y a , Y b | X , R ). records and their corresponding venue records. To capture the dependence between y a ij and y b ij , we factorize the poten-tial functions to consider them jointly, resulting in: p ( y a , y b | x , r ) = 1 where f l are feature functions, f  X  are consistency checking functions used to enforce transitivity among deduplication decisions, and Z x is a normalizer. The parameters  X  are learned by maximizing a product of local marginals [5, 4].
MAP inference in this model corresponds to finding the solution to y  X  = ( y a  X  , y b  X  ) = argmax y p  X  ( y a , y that is, finding the most probable deduplication decisions y  X  given x a , x b , r and the learned parameters  X .
Because exact inference here is intractable, we follow re-cent work which finds an equivalence between graph parti-tioning algorithms and inference in certain undirected graph-ical models [1, 5]. We first transform our graph to a weighted, undirected graph that only contains vertices for variables x and has edges weighted by the (log) clique potential for each pair of vertices. The value on these edges depends on which type of records they join.

For paper edges, we define the weight and similar weights w b ij for venue edges: It can be shown that the optimal partitioning of this graph corresponds to the optimal configuration y  X  in the original undirected graph-ical model. Here, the number of partitions is unknown, as it corresponds to the number of unique records.

Because traditional partitioning algorithms do not account for the known relations between clusters that exist in our data, we develop a novel, relational agglomerative clustering algorithm that exploits these dependencies. This algorithm iteratively merges nodes, enforcing relations and adjusting weights accordingly. For more details, we refer the reader to our technical report [2]
We experiment on two datasets of research paper cita-tions: Citeseer (1500 citations) and Cora 1 , (1800 cita-tions). Feature functions include string matches and cosine similarity of citation fields.

Table 1 shows the pairwise F1 performance of two systems: joint is the system we have advocated in this paper, and indep is the system which deduplicates records of different types independently , although records of the same type are deduplicated collectively as in McCallum and Wellner [5].
Venue performance improves considerably in the joint mo-del, which is reasonable considering the strong influence pa-per deduplication has on venue deduplication. The joint model obtains a 5% absolute recall boost in Citeseer , and a 9% boost in Cora data. This is because the hard constraint requiring the venues of duplicate papers to be merged often correctly merges venues with dissimilar surface forms. Table 1: Pairwise F1 deduplication performance.

More interestingly, a noticeable improvement in paper deduplication is attained by the collective model. Part of this is due to the precision enhancement provided by the clustering algorithm. Workshop and technical report ver-sions of journal or conference papers with the same title are correctly not merged when the venues are accurately iden-tified. Also, error analysis suggests that papers that would not have been otherwise merged were merged because their venues were determined to be coreferent.
We have introduced a collective model for deduplication of relational data and empirically demonstrated its advan-tage over competing methods. Future work includes mod-eling data where the relations R are unknown, for example discovering AdvisorOf relations between authors.
