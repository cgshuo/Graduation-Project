 The World Wide Web contains huge amount of information from around the world. Given a query, existing search engines such as Google, Lycos, Bing, Exalead, and Ask.com return different lists of web search results, ranked by their relevance to the given query. This difference corresponds to different rank algorithms employed by those search engines. Looking for a search engine that has high relevance score then become a need. To overcome this problem one possible solution is by clustering web document to many different groups of topics so a user can match user information need by directly traversing along the intended topic. By clustering web search results of a search engine, the user will quickly fi nd high precision documents. This is the idea the major advantage of a meta-search engines is their coverage of multiple search engines [ 6 ] hence we can use this to develop a high relevance data set using the Condorcet method [ 7 ] as a gold standard for a multi domain query. Furthermore, while FIHC outperforms best existing methods in terms of both clustering accuracy and scalability [ 8 ], KRIMP, an MDL-based algorithm, models the database very well [ 12 ]. As a result one can use KRIMP to generate only keywords that best represent web documents search results of a given query, and then pass it to FIHC clusters to obtain more relevant results of the query search results.
 In this paper, we present an MDL-based approach to web search result clustering (FIHC) that captures associations among keywords of sets of documents extracted from titles and snippets, given a query. Each document URL is then mapped to the most appropriate cluster and the cluster results of triples URL-title-snippet is returned. This paper provides one main contribution: it shows that an MDL-based FIHC can be used to signi fi cantly improve the relevance score of an individual search engine. The Minimum Description Length (MDL) principle is a method for inductive infer-ence, or better, for the model selection problem. The basic idea of MDL is to try to regularity in data.  X  Regularity  X  here may be identi fi ed with the viewing learning as data compression. Learning here means among web documents that usually go together. For a given set of hypotheses H and dataset D , one should try to fi nd the hypothesis in H that compresses D most. MDL (Minimum Description Length) is closely related to MML (Minimum Message Length) [ 9 ] and also to Kolmogorov Complexity [ 10 ]; in fact, one could see MML as fully Bayesian MDL. All three embrace the slogan Induction by Compression . Below is a brief description of MDL principle.
 Given a set of models H , the best model H  X  H is the one that minimises in which L(H) is the length, in bits, of the description of H; and L(D|H) is the length, in bits, of the description of the data when encoded with H. This is called two-part MDL or crude MDL, as opposed to re fi ned MDL, where model and data are encoded together [ 11 ]. Re fi ned MDL has a major weakness: it cannot be computed except for some special cases [ 11 ]. Hence, for modeling a database we use crude MDL, that is for compression purpose. MDL fi nds the set of frequent itemsets that yields the best compression. In this research a database is a model of a database of frequent itemsets-of-keywords that originated from retrieved web documents search results given a query to a search engine.
 table . A code table is a simple two-column translation table that has itemsets on the left-hand side and a code for each itemset on its right-hand side. Using such a table we can encode and decode databases. This is where MDL comes in: we search for the code table that compresses the data best. For further information about code tables in KRIMP see [ 12 ].
 clustering that is implemented on search results of an individual search engine. It differs from TDPM for evolutionary clustering that divide data into epochs where all data points inside the same epoch are exchangeable and the temporal order is maintained across epochs [ 13 ]. Even though the number of clusters produced by an MDL-based FIHC is also unbounded like in TPDM, but we do not divide data into epochs because we are focus in the of fl ine clustering, that is a clustering stage before go to the incremental clustering stage and then to the realtime clustering stage as suggested by Vadrevu et al. [ 14 ] to build a scalable clustering system of large collections of docu-ments. Our approach is more inspired by FIHC [ 8 ], a robust method in hierarchical clustering, that is more suitable for clustering the web search results rather than the parametric clustering techniques that use a fi xed upper bound on the number of clusters.
 representation of snippets by discover hidden topics from a very large external data collection such as proposed by Nguyen et al. [ 15 ]. Based on Nguyen et al. is a need to collect web pages from huge resources and the data must cover many useful topics, hence in our work the focus is more to fi nd associations among important keywords extracted from title and snippet. The MDL-based frequent itemset mining is a suitable technique for fi nding the association by mining only frequent itemsets that compresses the data best. Then the output of MDL-based technique is clustered using FIHC. We will use the produced clusters to create a list of high relevance retrieved documents. 3.1 Problem Formulation We de fi ne the general steps of web search clustering of a search engine in two steps: 1. Given a query q , a search engine is used to retrieve a list of results, database D = (r 2. A clustering C = (C 1 , ... ,C m ) of the results in D is obtained by means of a clustering algorithm.
 Consider the requirement of relevance; the problem formulation is: Given q , produce a set of relevant documents returned by a search engine: where r i C j is a triple URL-title-snippet of cluster C j number of documents retrieved from cluster C j ; and m is the maximum number of clusters used as search results. or we paraphrase it as: given a query q , assume C is the MDL-based FIHC clusters formed from a database D , and D is a list of a search engine search results, then let the system presents a high relevance documents to the user by merge a list of top- X  m clusters, ordered by fi rst found cluster fi rst out. 3.2 MDL-Based FIHC Steps for Clustering Individual Search Engine Our MDL-based FIHC algorithm is composed of four steps: 1. Search result fetching : Given a query q to a search engine (e.g. Google) we build a database D of web search results returned by the search engine. 2. Document parsing and keywords extractor : The obtained webpages snippets are analyzed by an HTML parser and then passed to the RAKE important keyword extractor [ 16 ]. Titles and snippets are informative enough to represent most relevant contents for a given query [ 14 ]. The RAKE extractor results in keywords of one or two words; it detects array of words separated by words such as and, the, and of, as its key candidates. Several metrics are used in RAKE: (1) word the degree ( deg ( w )/ freq ( w )), where w is a keyword. For further discussion on RAKE see [ 16 ]. 3. MDL -based FIHC clustering : The database of RAKE keywords named DB , ( DB D ), is then compressed by KRIMP to produce a code table CT ( DB , q ). Only keywords that exist in CT will be used for clustering, since they are frequent and compress the database DB very well, which means that the resulting CT is a good representation of the keywords of all retrieved documents in a search engine results of a given query q .Asa CT ( DB , q ) exists we then use it to form clusters using FIHC (see more detail of MDL-based FIHC clustering in Sect. 4 ). All the formed clusters is written to the fi nal table . For simplicity, there is no ranking within clusters. 4. Post -processing : Once a fi nal table is created, the post-processing involves a merging of top- X  search results documents from m clusters, ordered by cluster fi rst out. The merging results is the fi nal search results of the search engine. The KRIMP algorithm [ 12 ] uses a code table CT ( DB , q ) that best describes the data-base DB ,( DB D ). To do this, one should fi nd the minimal coding set [ 15 ]. candidate set. Cover is required to identify which elements of CT are used to encode keywords of a web document given a query q . The result of a cover function is a disjoint set of elements of CT that cover the keywords over q . Minimal coding set is found by seeking the smallest coding set CS F such that the corresponding code table CT has a minimal total encoded length L ( D | CT ). The search space for seeking the optimal code table is far too large, therefore the KRIMP algorithm [ 12 ] uses a heuristic, a simple greedy search strategy: 1. Start with a standard code table ST , containing the singletons only of itemsets X 2. Add the itemsets from F one by one. If the resulting codes lead to a better com- X  all interesting  X  itemsets. The algorithm starts with mining large 1-itemsets and using a very low minimum support obtained by trial i.e. minsup = 5, KRIMP results in best quality itemsets. Mining stops when  X  all interesting  X  itemsets are found. All interesting frequent itemsets are then saved in a code table. We propose the MDL-based frequent itemset mining method (KRIMP) to produce inputs to the FIHC hierarchical clustering because we want to show that KRIMP is also represents the web search results database best. KRIMP produces best quality frequent itemsets by rejecting frequent itemsets-of-keywords with minimum support lower than minsup .
 that the  X  cohesiveness  X  of a cluster is measured directly using frequent itemsets. Since the documents under a cluster contains the same topic, they are expected to share more common itemsets than those under different cluster.
 frequent itemsets of important keywords produced by RAKE. Once a code table CT processes: clustering, tree building, and tree pruning. There are two steps to construct An initial cluster is constructed for each global frequent itemset to contain all docu-ments contain the itemset. FIHC only allow any clusters that have cluster frequent items with cluster supports more than the minimum cluster support. In second step, FIHC identify the  X  best  X  initial cluster and keep a document only in the best initial cluster by measuring the goodness of a cluster for a document. In tree building process, cluster tree is built bottom-up by choose the  X  best  X  parent among such potential par-ents. The tree pruning criterion is based on the inter-cluster similarity between a parent and its child. A child will be pruned only if the child (the subtopic) is similar enough to its parent topic. Once fi nal clusters are formed after tree pruning process, the next step is creating a set of relevant documents as fi nal answer to the given q . This is done by merge top- X  search results documents from m clusters. We performed experiments in two steps: fi rst, we build a gold standard, viz., the relevant search results; and second, we build a list of documents with high score of relevance. To do the fi rst step, a meta-search engine environment is used to build 4 data fusions that then elected using Condorcet, an automatic relevance judgement suggested by Nuray and Can [ 7 ]. For the second step, extensive experiments of the MDL-based FIHC technique are performed on an individual search engine.
 Table 1 shows queries taken from Mohamed [ 18 ] that we use for all of our experiments including both of the two steps above. According to Jansen et al. [ 19 ], 97 % of internet queries consist of less than 6 terms . Even more, the average length of those queries is 2.5 terms . For our experiment, we use queries with length of 2 and 3 terms and expand them using operator AND/OR. Therefore using, for example, a three terms query  X  culturally responsive teaching  X  will result several combinations: turally AND responsive AND teaching  X  ,  X  culturally AND responsive OR teaching  X  culturally OR responsive AND teaching  X  , and  X  culturally OR responsive OR teaching  X  .
 5.1 Building the Gold Standards A meta-search engine allows us to search multiple search engines fast at once, returning more comprehensive and relevant results [ 20 ]. We use this ability to build different ideal relevant search results (the data fusions). The data fusions then are used to build gold standard for each query using an election technique namely Condorcet (Nuray and Can [ 7 ]). Our meta-search engine uses 3 out of 5 popular search engines: Google, Bing, AskJeeves, Lycos, and Exalead.
 The vote counting procedure then takes into account each preference of each voter for one candidate over another. The Condorcet voting method speci candidate, which beats each of the other candidates in a pair wise comparison. The Condorcet method is chosen as an automatic relevance judgement technique in our experiments since for data fusion the Condorcet method provides the best performance in terms of automatic ranking compares to the random selection method or RS, and the reference count method or RC. According to [ 7 ], the mean correlation values for Condorcet is 0.560, compares to 0.506 for RC and 0.493 for RS.
 data fusions. Each data fusion is built in a meta-search engine setting using all com-binations of 3 out of 5 component engines using a rank fusion algorithms. The four rank fusion algorithms used are: KE [ 17 ], two variants of Weight Borda-Fuse [ 22 , 23 ], and Count Function algorithm [ 24 ]. The choise of k -toplist search results ( k = 50, 100, 200) that is used by 3 component engines will produce different ranking of retrieved documents in the data fusion. The KE, the two variants of Weight Borda-Fuse (WBF), and the Count Function algorithms act as  X  the voters  X  , and their union of 10-toplist documents act as  X  the candidates  X  in Condorcet. As a gold standard dataset we only take the 10-toplist documents of the Condorcet results. 5.2 The Unclustered Web Search Result Experiments Google is a good sample of individual search engines. From Table 2 , the Google search engine in average above all queries shows best relevant search results. n (P@n) metric and the Mean Reciprocal Rank (MRR) as in [ 25 ]. Precision at rank n is de fi ned as the proportion of retrieved documents that is relevant with the gold standard, averaged over all documents. MRR measures where in the ranking the document (with the gold standard) is returned by the system, averaged over all the web documents. This measure provides insight in the ability of the system to return a relevant document at the top of the ranking.
 As an example the query is  X  adolescent AND alcoholism  X  built using Google, Bing, and Lycos. Table 3 shows an evaluation result using Condorcet ( 50 -toplist), for each original Google, Bing and Lycos web search results; these are before a clustering technique is applied to the individual results of the search engines. In this example Google outperforms other search engines. 5.3 The MDL-Based FIHC Algorithm Experiments In our experiments, we cluster Google  X  s search results using k -means or the MDL-based FIHC with different number of clusters. For MDL-based FIHC we set  X  = 1 and m = 30, while for k -means  X  = 3 and m = 10. Number of clusters in k -means is set to 10 clusters per query search due to we have enough documents from k -toplist a search engine  X  s search results { k = 50, 100, 200}. Number of clusters in FIHC is not de fi ned by the user but it is generated automatically by the hierarchical FIHC clustering by measuring  X  cohesiveness  X  of frequent itemsets to a cluster. As an example is for a search results of query  X  adolescent AND alcoholism  X  from a search engine with k -toplist ( k = 200), minimum cluster support = 0.1 and 1533 keywords, from 200 retrieved documents at initial stage of the FIHC we have 1535 clusters that then they drop to 200 clusters at fi nal stage. From hundreds of clusters we take only the 30 clusters of the hierarchical FIHC fi nal result since when we observed data the number of document in each cluster almost all is 1. For relevance evaluation we test the FIHC fi nal result using only the 10 fi rst clusters due to the Condorcet dataset is also 10 documents. Our search engine prototype displays only 10-toplist of search results. In fi nal result of hierarchical FIHC we have 30 documents. In k -means , since the number of clusters is constant equal to 10 clusters, then we must generate the same number of documents (30 documents) as the fi nal result by picking 3-toplist of doc-uments from each clusters.
 For all of our experiments, relevance is measured using only the 10-toplist of a result  X  s documents of either k -means or hierarchical FIHC against the Condorcet dataset as the gold standard.
 Here is how we set up parameters in the MDL-based FIHC. In the MDL-based FIHC clustering stage, a parameter for KRIMP we must set up is: the candidate type of itemset collection. For any candidate type of itemset collection we set all itemsets to minimum support equal to 5 (  X  all -5d -pop  X  ). The choice of frequent itemsets, is because we want to mine all interesting itemsets. We use a small value for the minimum support due to our web search results database is a sparse database, contains important keywords generated from RAKE. In the FIHC clustering stage, we must de fi ne two parameters: the minimum global support and the minimum cluster support . The minimum global support is 0 (ignored) because, in general, majority of the data are itemsets with global frequent itemsets 1 %. If for example we set the minimum global support to 1 we will lose so many global frequent itemsets data and it is not good for further clustering process. The minimum cluster support is set to 0.1. The cluster support of an item in C percentage of the documents in C j that contain the item. The minimum cluster support value must be set properly depend on the data. In our case, by setting minimum cluster support to 0.1 we still have enough cluster frequent items with cluster supports in initial clusters while at the same time remove many unimportant frequent items with cluster supports below 10 %.
 engine to the gold standard (Condorcet dataset). See Table 4 and Fig. 2 . The  X  Google  X  rows show the relevance of the original (and unclustered) search results obtained directly from the Google search engine, The results  X  relevance when the k -means clustering is applied, and the show what we get when the MDL-based FIHC clustering is used.
 From Fig. 2 and Table 4 , we have the following observations. 1. By P@x evaluation, the MDL-based FIHC clustering outperforms other standard clustering algorithms.
 Clustering web search results using k -means are not recommended since it performs worse than the original search results version of the search engine. 2. By P@x evaluation, the MDL-based FIHC clustering always performs better than the unclustered version, the original web search results of individual search engine. Even at P@10 the MDL-based FIHC clustering shows signi fi relevance or precision than the original one with best setting k -toplist at k = 50 of web search results (improvement up to 33.65 % of original Google results). Adding more documents ( k -toplist, k = {100, 200}) are not necessary in improving relevance. 3. By MRR evaluation, the fi rst found relevant document rank position was signi cantly improved using the MDL-based FIHC clustering.
 In this case the MDL-based FIHC clustering outperforms k -means in position is found. This also shows that there is no correlation between improvement in MRR and improvement in precisions of web documents (P@x).
 Example results of the gold standard ( Condorcet ), Google, the k -means clustering on Google, and the MDL-based FIHC clustering on Google, can be found in the Appendix at https://www.dropbox.com/s/vfc4xobyw83yaax/Appendix.docx?dl=0 . Several statistical Welch two sample t-tests are done on the lists of P@10 values of Google, k -means , and hierarchical (the MDL-based FIHC). The aim is to show that before and after it is clustered (Table 5 ). All tests uses con performed on a hierarchical clustered Google against its corresponding unclustered Google (hierarchical-vs-Google) and done also on a k -means clustered Google against its corresponding unclustered Google ( k -means -vs-Google); all tests are performed 3 times (3 k -toplist, k = {50, 100, 200}). The detail lists of P@10 of experiments in Table 4 are not shown here due to space limitations.
 See Table 5 , test results of an individual search engine (clustered vs unclustered search results). Let  X  1 be the mean of a clustered individual search engine results and  X  2 the mean of the original unclustered search engine hypotheses of interest are expressed as: Hypothesis 1:  X  1  X   X   X  of an individual search engine before and after it is clustered (Table 5 , part A and B). show hierarchical-vs-Google has evidence that  X  1  X   X  2 &gt; 0 while k -means -vs-Google has evidence of  X  1  X   X  2 &lt; 0. Since Table 5 part C and part D show the p -value is very low, then we reject the Hypothesis 1. From Table 5 part C, for hierarchical-vs-Google the results show that there is strong evidence of a mean increase in P@10 between search results of the hierarchical clustered Google and the original unclustered Google. It is contrary to the fact to the k -means -vs-Google that show a mean decrease (Table 5 , part D).
 of documents ( k -toplist, k = {50, 100, 200}). The whole process completes within 129 min tested on an Asus K45VD laptop machine with processor Intel Pentium dualcore 2.4 GHz, RAM 4 GB, and hardisk 500 GB. Implementation of the system was built in Python v2.6. The system uses very low minimum cluster support 0.1 since the system is not intended for speci fi c domain only but multi domain that in real situation leads to many very low cluster frequent items and cluster supports (CS). As a conse-quence it fi lters them at initial clusters and processes only cluster (label) with CS &gt;= 10 %. It demonstrates that the MDL-based FIHC clustering technique is a scalable method.
 Figure 3 also shows that tree pruning and clustering are the most time consuming stages in the MDL-based FIHC. This is different with runtime of tree building that completes in 6.1 min. In the clustering stage, most time is spent on constructing initial clusters while in tree pruning there is an indication of many subtopics are very similar to their parents  X  topics. The tree pruning scan the tree in the bottom-up oder and calculates Inter_Sim similarity between the node and each of its children. If Inter_Sim is above 1 then the system prunes the child cluster.
 Both in the tree pruning and in the clustering stages, their runtimes are linear with for each settings of individual search engines).
 Expensive clustering time as shown in Fig. 3 is due to we performed the MDL-based FIHC clustering viz. KRIMP in sequential. If we split a database of web documents into several smaller databases and feed them into KRIMP, the problem then is about how to merge different results of frequent keywords of each databases in KRIMP code tables into best representative of frequent keywords of the original web documents  X  database. MapReduce technique is suitable to be implemented in this MDL-based FIHC clustering algorithm. Tree pruning time in future also can be improved by parallelizing processes of computing many Inter_Sims . We leave these problems for further research and discussion. We introduced a new approach for frequent itemset-based hierarchical clustering to address the issue of improving the relevance of search results of an individual search engine. The novelty of this research is that it exploits frequent itemsets using an MDL-based algorithm, viz. KRIMP, for de fi ning clusters to generate more relevant retrieved documents from a search engine. Evaluated in a meta-search engine envi-ronment datasets, the experimental results show that our approach outperforms other clustering algorithms in terms of relevancy as well as shows signi improvement of web search results of an individual search engine.

