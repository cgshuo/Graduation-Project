 Since about one decade ago Information Extraction (IE) and Automated Text Summarization have been recognized as two tasks sharing the same goal [1]  X  extract accurate information from unstructured texts according to a user's specific desire, and studied separately and quite intensively over the past decade. Various corpora have been annotated for each task, a wide range of models and machine learning methods have been applied, and separate official evaluations have been organized. There has clearly been a great deal of progress on the performance of both tasks. 
Because a significant percentage of queries in the summarization task involve facts techniques in automatic summarization. Some earlier work (e.g. [2], [3]) used Message Understanding Conference (MUC) [4] IE to generate or improve summaries. The IE task has progressed from MUC-style single template extraction to the more comprehensive Automatic Content Extraction (ACE) that targets at more fine-grained types of facts. The IE methods have also been advanced from single-document IE to cross-document dynamic event chain extraction (e.g. [5]) and static attribute extraction confidence values (e.g. [5]). Therefore a summarization process can have more reasons we feel the time is now ripe to explore some novel methods to marry these two tasks again and raise summarization to a higher level of performance. 
From a collection of documents for a specific query, we extract facts in both queries and the documents. We use a high-performing multi-document extractive summarizer as our baseline, and tightly integrate IE results into its sentence ranking and compression. Experiment results show this integration method can achieve significant improvement on both standard summarization metrics and human judgement. 2.1 TAC Summarization Task The summarization task we are addressing is that of the NIST Text Analysis Conference (TAC) multi-document summarization evaluation [7]. This task involves generating fixed-length summaries from 10 newswire documents, each on a given query including a specific topic. For example, given a query  X  X udge Joan Lefkow's Family Murdered/Describe the murders of Judge Joan Lefkow's husband and mother, and the subsequent investigation. Include details about any evidence, witnesses, suspects and motives. X  and 10 documents, a summarization system is required to generate a summary about specific entities ( X  X udge Joan Lefkow X ), relations ( X  X amily X ) and events ( X  X urder X  and  X  X nvestigation X ). 2.2 Baseline Summarization System We apply a top-performing TAC summarization system [8] as our baseline. In this model, a summary is the set of sentences that best covers the relevant concepts in the document set, where concepts are simply word bigrams valued by their document frequency. The concepts with low-frequency or stop-words are filtered. The value of a modeled in a way to find the collection with maximum value, subject to a length constraint. This problem is solved efficiently with an integer linear programming (ILP) solver. A sentence compression component is used to post-process candidate sentences. The compression step consists of dependency tree trimming using high-confidence semantic role labeling decisions. Non-mandatory temporal and manner arguments are removed and indirect discou rse is reformulated in direct form. We apply a state-of-the-art English cross-document IE system ([6], [9]) to extract facts from the input documents. This system was developed for the NIST Automatic 
ACE2005 defined 7 types of entities, 18 types of relations and 33 distinct types of relatively  X  X ynamic X  events. KBP2010 defined 42 types of relatively  X  X tatic X  slots (e.g.  X  Ruth D. Masters is the wife of Hyman G. Rickover  X  indicates that the  X  per:spouse  X  slot for person  X  Hyman G. Rickover  X  is  X  Ruth D. Masters  X ). 
The IE pipeline includes name tagging, nominal mention tagging, coreference resolution, time expression extraction and normalization, relation extraction and event extraction. Names are identified and cla ssified using an HMM-based name tagger. Nominals are identified using a maximum entropy-based chunker and then semantically classified using statistics from the ACE training corpora. Relation extraction and event extraction are also based on maximum entropy models, incorporating diverse lexical, syntactic, semantic and ontological knowledge. At the end an event coreference resolution component is applied to link coreferential events, based on a pairwise maximum entropy model with linguistic attributes and a graph-cut clustering model. Then an event tracking component is applied to identify and order the events centered around each centroid entity on a time line. 
Our slot filling system includes a bottom-up pattern matching pipeline and a top-down question answering pipeline, with several novel enhancements including statistical answer re-ranking and Markov Logic Networks (MLN) based cross-slot reasoning. From both extraction systems confidence values are produced on various corresponding argument identification and classification. 
Based on the assumption that the documents for a given query are topically related, we apply the extraction methods to the each  X  X uper-document X  that includes the query and the related documents. As a result we can obtain a knowledge base including documents. 
This method can be considered as a combination of query expansion and fact retrieval. We not only obtain a  X  X rofile X  (potential fact categories) for the query so that we can design corresponding templates for abstractive summarization, but also assign weights to sentences including these specific categories of facts. Using the combination of fact types in ACE and KBP, we can cover rich information in news articles. For example, among the 92 TAC queries, 28 queries include explicit ACE events and their corresponding input documents include 2739 event instances. Some queries include specific events such as  X  Provide details of the attacks on Egypt's Sinai Penninsula resorts targetting Israeli tourists.  X , while others only inquire about a general series of events:  X  Describe the views and activities of John C. Yoo.  X  
Previous work has extensively focused on using entity extraction to improve summarization, so we only present some concrete examples of using relations and events to improve summarization quality as follows. 4.1 Relations/Events Can Push Up Relevant Sentences Traditional sentence ranking methods in summarization used key word matching, instance. 
In order to learn a more robust sentence ranker, the method of matching query and sentences should go beyond lexical and syntactic level in order to capture semantic structures. A lot of current extractive summarizers use semantic relations in WordNet [10]. This approach has two main limitations: (1) It cannot address broader semantic relatedness; (2). It cannot address the semantic relations between two words with different part-of-speech tags. Semantic relation and event classification can provide a more flexible matching framework. Our basic intuition is that a sentence should receive a high rank if it involves many relations and events such relations and events. For example, for the following query and sentences with high ranks: 
In sentences 1 and 2, the baseline summarizer is not able to detect  X  attacks  X  as the same events as  X  bombings  X  because they have different lexical forms. The event extraction component, however, predicts  X  conflict-attack  X  events and labels  X  London/Br itish  X  as  X  place  X  arguments in both sentences. This provides us much stronger confidence in increasing the ranks of sentence 1 and 2. 
Furthermore, even if the event triggers in sentence 3  X  bomb  X  can be matched with  X  bombings  X  in the query, the baseline summarizer assigns a low weight to sentence 3 because it cannot detect the  X  X  ocated-in X  relation between  X  King's Cross station  X  and  X  London  X . But the relation extraction component can successfully identify this  X  PHYS/Located  X  relation from another sentence in the same document set:  X  The subway tunnel between King's Cross and Russell Square is one of several "deep tubes" bored through London 's bedrock and clay more than a century ago  X . 4.2 Relations/Events Can Push Down Irrelevant Sentences semantic structure analysis. For example, 
The baseline summarizer mistakenly assigns a high rank to sentence 4 because it involves a name  X  Joan Humphrey Lefkow  X  specified in the query, and  X  wife  X  can be recognized to match  X  husband  X  by semantic clusters. However, event extraction can be used to successfully push down this sentence because it does not include any  X  X onflict-attack (murder) X  events. 4.3 Event Coreference Can Remove Redundancy What we have presented in section 4.1 and 4.2 is advancing summaries in terms of their Content quality. Another central track of summarization research is the issue of readability  X  especially how to remove redundancy existing in summaries from multiple documents. 
In this paper we propose an approach of using event coreference resolution to reach this goal. Compared to similarity computation methods based on lexical features, our method can detect similar pairs of sentences even if they use completely different expressions. For example, we can fuse the following sentences because they include coreferential  X  Conflict-attack  X  event instances = Both include indicative words  X  blasts/bombings  X  and involve  X  X ondon X  as their place arguments: It is challenging for the baseline summarizer to detect this sentence pair because most words don X  X  overlap. IE provides an effective way of modeling the central information described in the source documents. This model consists of entities, relations and events involving these entities. Even if this model described perfectly such information, it does not tell us what subset of this model should appear in a summary. 
The first question we have to tackle is  X  X hat is most relevant in IE output? X  A input and ensure that frequently described events appear in the summary. Another approach would be to build a graph of IE elements, and perform a random walk of this graph to weigh the most relevant nodes. However, both approaches do not account for three factors: relevance prior, co verage and confidence of the extraction. 
Another question is  X  X ow can we incorporate IE-derived model in a summarization system? X  Only considering extractive summarization, approaches vary from scoring sentences directly with supervised or unsupervised relevance assessments, to scoring sub-sentence units and finding best covering sentences. Under those models, IE can be units. For the purpose of this work, we focus on a simple linear model to blend sentence-level IE scores and a baseline summarizer. 5.1 Approach Overview Figure 1 depicts the general procedure of our approach to integrate IE results into our baseline summarizer. 5.2 IE-Based Re-ranking and Redundancy Removal Because the human summaries are not necessarily created from the original sentences of the input documents, we cannot adopt a supervised learning based re-ranking summarizer based on IE confidence values. 
Each IE component includes a statistical classifier and thus can generate reliable confidence values. For example, for each event mention in D , the baseline Maximum Entropy based classifiers produce three types of confidence values:  X  Conf(trigger,etype) : The probability of a string trigger indicating an event  X  Conf(arg, etype) : The probability that a mention arg is an argument of some For a given query Q and a collection of 10 documents D that includes N sentences, we generate a summary based on an integrated approach as follows. For any sentence s in D , we extract various confidence values in Table 1 and combine them to form the final IE confidence for s : cse confidence of s including an entity e j relevant to Q csr confidence of s including a relation r k relevant to Q (relation csev confidence of s including an event mention ev k relevant to Q c s evcoref confidence of s including a link evcoref m between two 
Assuming the ranking confidence from the baseline summarizer is () baseline cs , then we can get the combined weight for s : can provide a comprehensive representation of the concepts in the source collection of documents. Based on the combined weights, we select top sentences to form a summary redundancy), we conduct a greedy search through the high-ranked sentences for coreferential by our entity and event coreference resolvers, we remove the shorter one. In this section we present the results of applying IE to improve TAC summarization. 6.1 Data and Evaluation Metrics We randomly selected 30 topics from TAC 2008 and TAC 2009 summarization task as our development set to optimize parameters and another separate set of 31 topics as our blind test set. The summaries are evaluated automatically with ROUGE-2 and ROUGE-SU4 metrics [13]. In order to focus more on evaluating the ordering of sentences and coherence across sentences, we extend the length restriction in TAC setting from 100 words to 20 sentences. We also asked 16 human subjects to manually evaluate summaries based on the TAC Responsiveness metric [7] consisting of Content and Readability/Fluency measures. In order to compare different methods Poor, 2-Poor, 3-Barely Acceptable, 4-Good, 5-Very Good). 6.2 ROUGE Scores ROUGE-2/ROUGE-SU4 scores of varying the IE weight  X  , from 0 (baseline summarizer) to 1 (using IE only). 
We can see that our method achieved significant improvement on Recall. When we methods achieved 7.38% relative ROUGE-2 gain. In order to check how robust our approach is, we conducted the Wilcoxon Matched-Pairs Signed-Ranks Test on ROUGE scores for these 31 topics. The results show that we can reject the hypothesis that the improvements were random at a 95.7% confidence level. From these curves we can also conclude that using IE results only (  X  =1) for sentence ranking produced worse ROUGE scores than the baselines. 6.3 TAC Responsiveness Scores Table 2 presents the average scores across all topics based on manual evaluation using TAC Responsiveness metrics. 
Table 2 shows that our IE-integrated method received much better Content scores based on human assessment. For example, for the query  X  X rovide details of the kidnapping of journalist Jill Carroll in Baghdad and the efforts to secure her release X , the baseline summarizer received a score  X 2 X  because of mis-match between  X  X idnapping X  in the query and the  X  X rrest X  events involving other person and place arguments in the source documents. In contrast, our method received a score  X 4 X , sentences. Furthermore, according to the user feedback, our method produced fewer redundant sentences for most topics. 6.4 Discussion Error analysis shows that for 3 topics IE had negative impact because of incorrect event justice.  X , IE mistakenly recognized  X  X ustice X  as the main event type while missed a more facts types in the query. Nevertheless, as the above results indicate, the rewards of using the IE information outweigh the risks. Our work is a re-visit on the idea of exploiting IE results to improve multi-document such as entities and MUC events are combined with natural language generation techniques in summarization. White et al . [3] improved Radev et al. X  X  method by summarizing larger input documents based on relevant content selection and sentence extraction. They also formally evaluated the performance of this idea. More recently, Filatova and Hatzivassiloglou [14] considered the contexts involving any pair of names as general  X  X vents X  and used them to improve extractive summarization. Vanderwende et al. [15] explored an event-centric approach and generated summaries based on extracting and merging portions of logical forms. Biadsy et al. [16] exploited biographical summaries. Hachey [11] used generic relations to improve extractive summarization and remove redundancy. Compared to these previous methods, we extend the usage of IE from single template to much more complete relation/event types. To the best of our knowledge our approach is the first work to use the information extracted from KBP project in summarization and apply event coreference resolution to remove summary redundancy. 
In addition, our work is related to the summarization research that incorporates resolution and event resolution which are beneficial to summarization. Our approach of selecting informative facts is also similar to defining Summarization Content Units (SCUs) in the Pyramid Approach [18] because both methods aim to maximize the coverage of logical  X  X oncepts X  in summaries.. We investigated the once-popular IE-driven summarization approaches in a wider IE paradigm. We demonstrated that a simple re-ranking approach can achieve improvement over a high-performing extractive summarizer. We expect that as IE is summarization task can benefit more from extended semantic frames. This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149, Google, Inc., DARPA GALE Program, CUNY Research Enhancement Program, PSC-CUNY Research Program, Faculty Publication Program and GRTI Program. The views and conclusions contained in this document are those of the expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. 
