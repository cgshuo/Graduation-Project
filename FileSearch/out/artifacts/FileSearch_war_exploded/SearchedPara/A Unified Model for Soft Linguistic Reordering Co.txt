 Reordering models in statistical machine transla-tion (SMT) model the word order difference when translating from one language to another. The popular distortion or lexicalized reordering mod-els in phrase-based SMT make good local pre-dictions by focusing on reordering on word level, while the synchronous context free grammars in hierarchical phrase-based (HPB) translation mod-els are capable of handling non-local reordering on the translation phrase level. However, reorder-ing, especially without any help of external knowl-edge, remains a great challenge because an ac-curate reordering is usually beyond these word level or translation phrase level reordering mod-els X  ability. In addition, often these translation models fail to respect linguistically-motivated syn-tax and semantics. As a result, they tend to pro-duce translations containing both syntactic and se-mantic reordering confusions. In this paper our goal is to take advantage of syntactic and seman-tic parsing to improve translation quality. Rather than introducing reordering models on either the word level or the translation phrase level, we pro-pose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and of-ten crosses translation phrases. To show the ef-fectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-of-the-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger base-line, already including fine-grained soft syntac-tic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well.

Our syntactic constituent reordering model con-siders context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word align-ment information. Due to the fact that a con-stituent, especially a long one, usually maps into multiple discontinuous blocks in the target lan-guage, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is based on the leftmost aligned target word and the other based on the rightmost target word.

While recently there has also been some encour-aging work on incorporating semantic structure (or, more specifically, predicate-argument struc-ture: PAS) reordering in SMT, it is still an open question whether semantic structure reordering strongly overlaps with syntactic structure reorder-ing, since the semantic structure is closely tied to syntax. To this end, we employ the same reorder-ing framework as syntactic constituent reordering and focus on semantic roles in a PAS. We then an-alyze the differences between the syntactic and se-mantic features.

The contributions of this paper include the fol-lowing:  X  We introduce novel soft reordering con- X  We introduce a unified framework to incor- X  We provide a detailed analysis providing in-The rest of the paper is organized as follows. Section 2 provides an overview of HPB transla-tion model. Section 3 describes the details of our unified reordering models. Section 4 gives our ex-perimental results and Section 5 discusses the be-havior difference between syntactic constituent re-ordering and semantic role reordering. Section 6 reviews related work and, finally Section 7 con-cludes the paper. In HPB models (Chiang, 2007), synchronous rules take the form X  X  X   X , X ,  X  X  , where X is the non-terminal symbol,  X  and  X  are strings of lexical items and non-terminals in the source and target side, respectively, and  X  indicates the one-to-one correspondence between non-terminals in  X  and  X  . Each such rule is associated with a set of transla-tion model features {  X  i } , such as phrase transla-tion probability p (  X  |  X  ) and its inverse p (  X  |  X  ) , the lexical translation probability p lex (  X  |  X  ) and its inverse p lex (  X  |  X  ) , and a rule penalty that af-fects preference for longer or shorter derivations. Two other widely used features are a target lan-guage model feature and a target word penalty.
Given a derivation d , its translation log-probability is estimated as: Figure 1: Example of predicate-argument struc-ture. where  X  i is the corresponding weight of feature  X  i . See (Chiang, 2007) for more details. As mentioned earlier, the linguistic reordering unit is the syntactic constituent for syntactic reorder-ing, and the semantic role for semantic reordering. The syntactic reordering model takes a CFG rule (e.g., VP  X  VP PP PP) and models the reorder-ing of the constituents on the left hand side by ex-amining their translation or visit order according to the target language. For the semantic reorder-ing model, it takes a PAS and models its reorder-ing on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the an-notation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other.
Next, we use a CFG rule to describe our syn-tactic reordering model. Treating the two forms of reorderings in a unified way, the semantic re-ordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent.

Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering patterns. Therefore, we design two general constituent reordering sub-models. One is based on the leftmost aligned word (left-most reordering model) and the other is based on the rightmost aligned word (rightmost reordering model), as follows. Figure 2 shows the model-ing steps for the leftmost reordering model. Fig-ure 2(a) is an example of a CFG rule in the source ... ... ...
 parse tree and its word alignment links to the target language. Note that constituent XP 4 , which covers word f 8 , has no alignment. Then for each XP i , we find the leftmost target word which is aligned to a source word covered by XP i . Figure 2(b) shows that the leftmost target words for XP 1 , XP 2 , and XP 3 are e 2 , e 5 , and e 3 , respectively, while XP 4 has no aligned target word. Then we get visit order V = { v i } for { XP i } in the transformation from Figure 2(b) to Figure 2(c), with the follow-ing strategies for special cases:  X  if the first constituent XP 1 is unaligned, we  X  if a constituent XP i ( i &gt; 1 ) is unaligned, we  X  if k constituents XP m
Finally Figure 2(d) converts the visit order V = { v 1 ,...v n } into a sequence of leftmost reordering types LRT = { lrt 1 ,...,lrt n  X  1 } . For every two adjacent constituents XP i and XP i +1 with corre-sponding visit order v i and v i +1 , their reordering could be one of the following:
Up to this point, we have generated a se-quence of leftmost reordering types LRT = { lrt 1 ,...,lrt n  X  1 } for a given CFG rule cfg : XP  X  XP 1 ... XP n . The leftmost reordering model takes the following form: score lrt ( cfg ) = P l ( lrt 1 ,...,lrt n  X  1 |  X  ( cfg )) where  X  ( cfg ) indicates the surrounding context of the CFG. By assuming that any two reordering types in LRT = { lrt 1 ,...,lrt n  X  1 } are indepen-dent of each other, we reformulate Eq. 2 into: Similarly, the sequence of rightmost reordering types RRT can be decided for a CFG rule XP  X  XP 1 ... XP n .

Accordingly, for a PAS pas : PAS  X  R 1 ... R n , we can obtain its sequences of leftmost and right-most reordering types by using the same way de-scribed above. 3.1 Probability Estimation In order to predict either the leftmost or right-most reordering type for two adjacent constituents, we use a maximum entropy classifier to esti-mate the probability of the reordering type rt  X  { M,DM,S,DS } as follows: where f k are binary features,  X  k are the weights of these features. Most of our features f k are syntax-based. For XP i and XP i +1 in cfg , the features Table 1: Features adopted in the syntactic leftmost and rightmost reordering models. L ( XP ) returns the syntactic category of XP , e.g., NP, VP, PP etc.; H ( XP ) returns the head word of XP; P ( XP ) re-turns the POS tagger of the head word; S ( XP ) returns the translation status of XP on the target language: un. if it is untranslated; cont. if it is a continuous block; and discont. if it maps into multiple discontinuous blocks. are aimed to examine which of them should be translated first. Therefore, most features share two common components: the syntactic categories of XP i and XP i +1 . Table 1 shows the features used in syntactic leftmost and rightmost reordering mod-els. Note that we use the same features for both.
Although the semantic reordering model is structured in precisely the same way, we use dif-ferent feature sets to predict the reordering be-tween two semantic roles. Given the two adjacent roles R i and R i +1 in a PAS pas , Table 2 shows the features that are used in the semantic leftmost and rightmost reordering models. 3.2 Integrating into the HPB Model For models with syntactic reordering, we add two new features (i.e., one for the leftmost reorder-ing model and the other for the rightmost reorder-ing model) into the log-linear translation model in Eq. 1. Unlike the conventional phrase and lexi-cal translation features, whose values are phrase pair-determined and thus can be calculated offline, the value of the reordering features can only be obtained during decoding time, and requires word alignment information as well. Before we present the algorithm integrating the reordering models, we define the following functions by assuming XP i and XP i +1 are the constituent pair of interest in CFG rule cfg , H is the translation hypothesis and a is its word alignment: Table 2: Features adopted in the semantic leftmost and rightmost reordering models. P ( pas ) returns the predicate content of pas ; R ( R ) returns the role type of R , e.g., Pred, A0, TMP, etc. For features rf1, rf2, rf3, rf12 and rf13, we include another ver-sion which excludes the predicate content P ( pas ) for reasons of sparsity.
Algorithm 1 integrates the syntactic leftmost and rightmost reordering models into a CKY-style decoder whenever a new hypothesis is generated. Given a hypothesis H with its alignment a , it tra-verses all CFG rules in the parse tree and sees if two adjacent constituents are conditioned to trig-ger the reordering models (lines 2-4). For each pair of constituents, it first extracts its leftmost and rightmost reordering types (line 6) and then gets their respective probabilities returned by the max-imum entropy classifiers defined in Section 3.1 (lines 7-8). Then the algorithm returns two log-probabilities of the syntactic reordering models. Note that Function F 1 returns true if hypothesis H fully covers, or fully contains, constituent XP i , regardless of the reordering type of XP i . Do not confuse any parsing tag XP i with the nameless variables X i in Hiero or cdec rules.

For the semantic reordering models, we also add two new features into the log-linear transla-tion model. To get the two semantic reordering model feature values, we simply use Algorithm 1 and its associated functions from F 1 to F 5 replac-ing a CFG rule cfg with a PAS pas , and a con-stituent XP i with a semantic role R i . Algorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is ex-pressed in terms of syntactic reordering here for ease of presentation. We have presented our unified approach to in-corporating syntactic and semantic soft reorder-ing constraints in an HPB system. In this section, we test its effectiveness in Chinese-English trans-lation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stan-ford segmenter (Tseng et al., 2005). The En-glish data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidi-rectional alignments, which are symmetrized us-ing the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modi-fied Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k -best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system.
We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, re-ineni et al., 2002) for both tuning and evaluation.
To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to anno-tate semantic roles for all verbal predicates (part-of-speech tag VV , VE , or VC ).

Our basic baseline system employs 19 basic features: a language model feature, 7 transla-tion model features, word penalty, unknown word penalty, the glue rule, date, number and 6 pass-through features. Our stronger baseline employs, in addition, the fine-grained syntactic soft con-straint features of Marton and Resnik (2008), here-after MR08. The syntactic soft constraint features include both MR08 exact-matching and cross-boundary constraints (denoted XP = and XP +). Since the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP + fea-tures and 64 types of XP = features. 4.2 Model Training To train the syntactic and semantic reordering tains 7,870 sentences with 191,364 Chinese words and 261,399 English words. We first run syn-Table 3: Reordering type distribution over the re-ordering model X  X  training data. Hereafter, l-m and r-m are for leftmost and rightmost, respectively. tactic parsing and semantic role labeling on the Chinese sentences, then train the models by us-ing MaxEnt toolkit with L1 regularizer (Tsuruoka distribution over the training data. Interestingly, about 17% of the syntactic instances and 16% of the semantic instances differ in their leftmost and rightmost reordering types, indicating that the left-most/rightmost distinction is informative. We also see that the number of semantic instances is about 1/3 of that of syntactic instances, but the entropy of the semantic reordering classes is higher, indi-cating the reordering of semantic roles is harder than that of syntactic constituents.

A deeper examination of the reordering model X  X  training data reveals that some constituent pairs and semantic role pairs have a preference for a specific reordering type (monotone or swap). In order to understand how well the MR08 system respects their reordering preference, we use the gold alignment dataset LDC2006E86, in which the source sentences are from the Chinese Tree-bank, and thus both the gold parse trees and gold predicate-argument structures are available. Ta-ble 4 presents examples comparing the reordering distribution between gold alignment and the out-put of the MR08 system. For example, the first row shows that based on the gold alignment, for  X  PP,VP  X  , 16% are in monotone and 76% are in swap reordering. However, our MR08 system out-puts 46% of them in monotone and and 50% in swap reordering. Hence, the reordering accuracy for  X  PP,VP  X  is 54%. Table 4 also shows that the semantic reordering between core arguments and predicates (e.g.,  X  Pred , A1  X  ,  X  A0 , Pred  X  ) has a less ambiguous pattern than that between adjuncts and other roles (e.g.,  X  LOC,Pred  X  ,  X  A0,TMP  X  ), indicat-ing the higher reordering flexibility of adjuncts. Table 4: Examples of the reordering distribution (%) of gold alignment and the MR08 system out-put. For simplicity, we only focus on (M)onotone and (S)wap based on leftmost reordering. 4.3 Translation Experiment Results Our first group of experiments investigates whether the syntactic reordering models are able to improve translation quality in terms of BLEU. To this end, we respectively add our syntactic re-ordering models into both the baseline and MR08 systems. The effect is shown in the rows of  X + syn-reorder X  in Table 5. From the table, we have the following two observations.  X  Although the HPB model is capable of  X  There is no clear indication of whether the
Our second group of experiments is to vali-date the semantic reordering models. Results are Table 5: System performance in BLEU scores.  X  /  X  : significant over baseline or MR08 at 0.01 / 0.05, respectively, as tested by bootstrap re-sampling (Koehn, 2004) shown in the rows of  X + sem-reorder X  in Table 5. Here we observe:  X  The semantic reordering models also achieve  X  The syntactic reordering models outperform
Finally, we integrate both the syntactic and se-mantic reordering models into the final system. The two models collectively achieve a gain of up to 1.4 BLEU over the baseline and 1.0 BLEU over MR08 on average, which is shown in the rows of  X +syn+sem X  in Table 5. The trend of the results, summarized as perfor-mance gain over the baseline and MR08 systems averaged over all test sets, is presented in Table 6. The syntactic reordering models outperform the semantic reordering models, and the gain achieved by the semantic reordering models is limited in the presence of the MR08 syntactic features. In this section, we look at MR08 system and the systems improving it to explore the behavior differences between the two reordering models.

Coverage analysis : Our statistics show that syntactic reordering features (either leftmost or Table 6: Performance gain in BLEU over baseline and MR08 systems averaged over all test sets. rightmost) are called 24 times per sentence on av-erage. This is compared to only 9 times per sen-tence for semantic reordering features. This is not surprising since the semantic reordering features are exclusively attached to predicates, and the span set of the semantic roles is a strict subset of the span set of the syntactic constituents; only 22% of syntactic constituents are semantic roles. On aver-age, a sentences has 4 PASs and each PAS contains 3 semantic roles. Of all the semantic role pairs, 44% are in the same CFG rules, indicating that this part of semantic reordering has overlap with syn-tactic reordering. Therefore, the PAS model has fewer opportunities to influence reordering.
Reordering accuracy analysis : The reordering type distribution on the reordering model training data in Table 3 suggests that semantic reordering is more difficult than syntactic reordering. To val-idate this conjecture on our translation test data, we compare the reordering performance among the MR08 system, the improved systems and the maximum entropy classifiers. For the test set, we have four reference translations. We run GIZA++ on the data combination of our translation train-ing data and test data to get the alignment for the test data and each reference translation. Once we have the (semi-)gold alignment, we compute the gold reordering types between two adjacent syn-tactic constituents or semantic roles. Then we evaluate the automatic reordering outputs gener-ated from both our translation systems and max-imum entropy classifiers. Table 7 shows the ac-curacy averaged over the four gold reordering sets (the four reference translations). It shows that 1) as expected, our classifiers do worse on the harder semantic reordering prediction than syntactic re-ordering prediction; 2) thanks to the high accu-racy obtained by the maxent classifiers, integrat-ing either the syntactic or the semantic reorder-ing constraints results in better reordering perfor-mance from both syntactic and semantic perspec-tives; 3) in terms of the mutual impact, the syn-tactic reordering models help improving seman-tic reordering more than the semantic reordering Table 7: Reordering accuracy on four gold sets. models help improving syntactic reordering; and 4) the rightmost models have a learnability advan-tage over the leftmost models, achieving higher accuracy across the board.

Feature weight analysis : Table 8 shows the syntactic and semantic reordering feature weights. It shows that the semantic feature weights de-crease in the presence of the syntactic features, in-dicating that the decoder learns to trust semantic features less in the presence of the more accurate syntactic features. This is consistent with our ob-servation that semantic reordering is harder than syntactic reordering, as seen in Tables 3 and 7.
Potential improvement analysis: Table 7 also shows that our current maximum entropy classi-fiers have room for improvement, especially for semantic reordering. In order to explore the error propagation from the classifiers themselves and explore the upper bound for improvement from the reordering models, we perform an  X  X racle X  study, letting the classifiers be aware of the  X  X old X  re-ordering type between two syntactic constituents or two semantic roles, and returning a higher prob-ability for the gold reordering type and a smaller one for the others (i.e., we set 0.9 for the gold Table 9: Performance (BLEU score) comparison between non-oracle and oracle experiments. reordering type, and let the other non-gold three types share 0.1). Again, to get the gold reorder-ing type, we run GIZA++ to get the alignment for tuning/test source sentences and each of four ref-erence translations. We report the averaged per-formance by using the gold reordering type ex-tracted from the four reference translations. Ta-ble 9 compares the performance between the non-oracle and oracle settings. We clearly see that us-ing gold syntactic reordering types significantly improves the performance (e.g., 34.9 vs. 33.4 on average) and there is still some room for improve-ment by building a better maximum entropy clas-sifiers (e.g., 34.9 vs. 34.3). To our surprise, how-ever, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of se-mantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manu-ally designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or auto-matically learned (Xia and McCord, 2004; Gen-zel, 2010; Visweswariah et al., 2010; Khalilov and Sima X  X n, 2011; Lerner and Petrov, 2013), us-ing syntactic parses. Li et al. (2007) focused on finding the n -best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding.

Another approach in previous work added soft constraints as weighted features in the SMT de-coder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of captur-ing constituent reorderings (Chiang, 2010; Mylon-akis and Sima X  X n, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., X s) and soft constraints. Ge (2010) presented a syntax-driven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level.
Semantics-based reordering: Semantics-based reordering has also seen an increase in activity recently. In the pre-ordering ap-proach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft con-straint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios.

Non-syntax-based reorderings in HPB: Re-cently we have also seen work on lexicalized re-ordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntax-based reordering approach models the reorder-ing of translation words/phrases while the syntax-based approach models the reordering of syn-tactic constituents. Although there are overlaps between translation phrases and syntactic con-stituents, it is reasonable to think that the two re-ordering approaches can work together well and even complement each other, as the linguistic pat-terns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule bound-aries. Last, we also note that recent work on non-syntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. In this paper, we have presented a unified reorder-ing framework to incorporate soft linguistic con-straints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on Chinese-English translation show that the reordering ap-proach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also discussed the differences between the two linguistic reordering models.

There are many directions in which this work can be continued. First, the syntactic reordering model can be extended to model reordering among constituents that cross CFG rules. Second, al-though we do not see obvious gain from the se-mantic reordering model when the syntactic model is adopted, it might be beneficial to further jointly consider the two reordering models, focusing on where each one does well. Third, to better exam-ine the overlap or synergy between our approach and the non-syntax-based reordering approach, we will conduct direct comparisons and combinations with the latter.
 This research was supported in part by the BOLT program of the Defense Advanced Re-search Projects Agency, Contract No. HR0012-12-C-0015. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily re-flect the view of DARPA. The authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Ke Wu, Vladimir Eidelman, Hua He, Doug Oard, Yuening Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for useful discussions.
