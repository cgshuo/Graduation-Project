
The problem of record linkage focuses on determining whether two object descriptions refer to the same under-lying entity. Addressing this problem effectively has many practical applications, e.g., elimination of duplicate re cords in databases and citation matching for scholarly articles. In this paper, we consider a new domain where the record linkage problem is manifested: Internet comparison shop-ping. We address the resulting linkage setting that require s learning a similarity function between record pairs from streaming data. The learned similarity function is subse-quently used in clustering to determine which records are co-referent and should be linked. We present an online ma-chine learning method for addressing this problem, where a composite similarity function based on a linear combi-nation of basis functions is learned incrementally. We il-lustrate the efficacy of this approach on several real-world datasets from an Internet comparison shopping site, and show that our method is able to effectively learn various distance functions for product data with differing charac-teristics. We also provide experimental results that show t he importance of considering multiple performance measures in record linkage evaluation.
Record linkage is the problem of identifying when two (or more) references to an object are describing the same true entity. For example, an instance of record linkage would be identifying if two paper citations (which may be in different styles and formats) refer to the same actual paper . Addressing this problem is important in a number of do-mains where multiple users, organizations, or authors may describe the same item using varied textual descriptions.
Historically, one of the most examined instances of record linkage is determining if two database records for a person are referring to the same individual, which is an im-portant data cleaning step in applications from direct mar-keting to survey response (e.g., the US Census). More re-cently, record linkage has found a number of applications in the context of several web applications, e.g., the above-mentioned task of identifying paper citations that refer to the same publication is an important problem in on-line systems for scholarly paper searches, such as CiteSeer and Google Scholar . Additionally, record linkage has been a topic of interest in natural language processing research, where it is known as the co-reference resolution or named entity disambiguation problem [21, 24].

As we show in this paper, record linkage is a key compo-nent of on-line comparison shopping systems. When many different web sites sell the same product, they provide dif-ferent textual descriptions of the product (which we refer t o as  X  X ffers X ). Thus, a comparison shopping engine is faced with the task of determining which offers are referring to the same true underlying product. Solving this product nor-malization problem allows the shopping engine to display multiple offers for the same product to a user who is try-ing to determine from which vendor to purchase the prod-uct. Accurate product normalization is also critical for da ta mining tasks such as analysis of pricing trends.

In such a context, the number of vendors and sheer num-ber of products (with potentially very different character is-tics) make it difficult to manually craft a single similarity function that can correctly determine if two arbitrary offe rs refer to the same product. Moreover, for different categori es of products, different similarity functions may be needed t o capture the notion of equivalence for each category. Hence, an approach that allows learning similarity functions be-tween offers from training data becomes necessary.
Furthermore, in many record linkage tasks including product normalization, records to be linked contain mul-tiple fields (e.g., product name, manufacturer, price, etc. ). Such records may either come in pre-structured form (e.g., XML or relational database records), or the fields may have been extracted from an underlying textual description [12] . While it may be difficult for a domain expert to specify a complete similarity function between two records, they are often capable of defining similarity functions between in-dividual record fields . For example, it is relatively simple to define the similarity between two prices as a function re-lated to the inverse of the difference of the prices, or the difference between two textual product descriptions as the (well-known) cosine between their vector-space represent a-tions. Thus, an appropriate learnable similarity function for comparing records must be able to leverage multiple basis similarity functions that capture individual field similar ities.
An important property of the product normalization do-main is the fact that new data is becoming available con-tinuously as product offers arrive from merchants. At the same time, a small proportion of the incoming product of-fers includes the Universal Product Code (UPC) attribute which uniquely identifies products. This provides a contin-uous source of supervision, therefore a learning approach t o the linkage problem in such settings must be able to read-ily utilize new training data without having to retrain anew on previously seen data. Online learning algorithms are methods that process training examples incrementally, and employing such an algorithm provides the most flexibility in a record linkage setting where new data is arriving as a stream.

In this paper, we present an online learning approach for the record linkage problem in the product normalization setting, in which a similarity function is trained by induc-ing a linear combination of basis similarity functions be-tween the fields of different offers. The weights of basis functions in the linear combination are learned from labele d training data using a version of the voted perceptron algo-rithm [14], which is very efficient in an online learning set-ting for large volumes of streaming data. This algorithm can also be deployed in batch-mode learning using standard online-to-batch conversion techniques.

We show empirical results with our proposed method on several real-world product normalization tasks. Notably, we show that different similarity functions are learned for di f-ferent categories of products, and identify the strengths a nd weaknesses of the approach. We also compare three link-age rules for identifying co-referent records, one of which corresponds to a simple pairwise approach, and two oth-ers make linkage decisions between record pairs collec-tively. Finally, we provide some general observations re-garding evaluation methodology for record linkage tasks and present an argument for utilizing multiple accuracy measures when evaluating record linkage solutions.
The problem of identifying co-referent records in databases has been studied in the research community under various names, notably record linkage [13], the merge/purge problem [35], duplicate detection [25, 29, 4], reference/citation matching [23, 20], entity name matchin g and clustering [8], hardening soft databases [7], identity un-certainty [28], and robust reading [21].

The majority of solutions for record linkage treat it as a modular problem and consist of multiple stages. In the first stage, a function is selected for computing the simi-larity between all pairs of potentially co-referent record s. This similarity function can either be hand-tuned or its pa-rameters can be learned from training data. In the second stage, a blocking method is used to select a set of candidate record pairs to be investigated for co-reference, since it i s typically prohibitively expensive to compute pairwise sim -ilarities between all pairs of records in a large database. I n the final linkage stage, similarity is computed between can-didate pairs, and highly similar records are identified and linked as describing the same entity. This can be achieved either via pairwise or via collective inference over individ-ual record pairs.

Pairwise approaches classify all candidate record pairs into two categories:  X  X atches X  or  X  X on-matches X , where each candidate pair is classified independently of others. Some of the methods that employ the pairwise approach include the EM-based technique for finding optimal entity matching rules [34], the sorted neighborhood method for limiting the number of potential candidate pairs [16], and the domain-independent three-stage iterative merging alg o-rithm for duplicate detection [25].

In contrast, collective linkage methods take a more global view instead of independently processing each can-didate pair. These methods consider multiple linkage de-cisions in conjunction, and perform simultaneous inferenc e to find groups of co-referent entries that map to the same underlying entity. Recently proposed algorithms that be-long to this category include context-sensitive duplicate in-ference by iterative processing of the data [3], learning a declarative relational probability model that explicitl y specifies the dependencies between related linkage deci-sions [28], and multi-relational inference using conditio nal probabilistic models to simultaneously detect matches for all related candidate pairs [27, 24]. While some of these collective approaches have been shown to be more accurate than the pairwise approach on certain domains, the simul-taneous inference process makes these methods more com-putationally intensive.
An important aspect of any record linkage system is the choice of the function that is used to compute similarity be-tween records. Records in databases generally have mul-tiple attributes of different types, each of which has an as-sociated similarity measure. For instance, string similar -ity measures such as edit distance [15] or cosine similar-ity [1] can be used to compare textual attributes like prod-uct name or paper title, while numerical functions (e.g., relative difference) can be used for real-valued attribute s like price. Several recent papers have studied the prob-lem of combining such basis similarity functions to obtain a composite measure of similarity between two database en-tries [8, 29, 32, 4]. Learning a composite similarity mea-sure from several basis similarity functions also is relate d to recent research on learning composite kernels, where techniques such as boosting [10], optimizing kernel align-ment [11], and semi-definite programming [19] are em-ployed to learn an effective combination of simple base ker-nels from training data.

An adaptive framework for learning similarity functions is critical in the product normalization setting since the n o-tion of offer similarity is highly domain-dependent. For ex -ample, if linkage is performed between book offers, equiv-alence of product names (book titles and author names) is highly indicative of co-referent book records. For electro nic products, in contrast, product name similarity is insuffici ent to link records: offers named  X  X oshiba Satellite M35X-S309 notebook X  and  X  X oshiba Satellite M35X-S309 note-book battery X  have a high textual similarity but refer to dif-ferent products. At the same time, for electronic items pric e similarity is an important indicator of offer equivalence: the notebook and the battery records have very different prices , indicating that they are not co-referent. Thus, composite similarity between product offers must be adapted to a par-ticular domain using training examples.

An adaptive approach to product normalization can ex-ploit two unique domain characteristics: (1) availability of limited (and noisy) training data since a small percentage of offers contains the Universal Product Codes (UPCs) that uniquely identify products, and (2) the streaming nature of the data: new product offers are continuously added to the database, calling for a similarity learning method that can utilize incoming training data incrementally. In the remai n-der of the paper, we describe a linkage framework suitable for this setting. Overall, the main contributions of the pap er can be summarized as follows:  X  We introduce a novel application domain of product  X  We adapt an efficient online learning algorithm (aver- X  We compare different types of hierarchical clustering  X  We contrast different evaluation metrics for the record
Our proposed approach to product normalization is a modular framework that consists of several components: an initial set of basis functions to compute similarity be-tween fields of records to be linked, a learning algorithm for training the parameters of a composite similarity function , a method for generating linkage candidates to avoid comput-ing similarity between all pairs of records, and, finally, th e clustering-based linkage step. The following subsections describe the details of each of these components.
In formulating our approach, we begin with a set of K larity functions between fields of records R 1 and R 2 . We then learn a linear combination of these basis functions wit h K corresponding weights  X  i and an additional threshold pa-rameter  X  0 to create a composite similarity function, f  X 
Values provided by f  X  are not constrained to be positive: the learning formulation below assumes that the threshold  X  0 may take on a negative value so that for pairs of records that are not equivalent f  X  returns a negative value. Once trained, f  X  can be used to produce a similarity matrix S over all pairs of records. In turn, S can be used with any similarity-based clustering algorithm to identify cluste rs, each of which contains a set of records which presumably should be linked. Then, we can interpret each cluster as a set of records referring to the same true underlying item.
Identifying co-referent records requires classifying ev-ery candidate pair of records as belonging to the class of co-referent pairs M or non-equivalent pairs U . Given some domain  X  R from which each record is sampled, and K similarity functions f k :  X  R  X   X  R  X  R that oper-ate on pairs of records, we can produce a pair-space vec-tor x i  X  R K + 1 for every pair of records ( R i 1 , R i [ 1 , f 1 K values obtained from basis similarity functions concate-nated with a default attribute that always has value 1, which corresponds to the threshold parameter  X  0 .

Any binary classifier that produces confidence scores can be employed to estimate the overall similarity of a record pair ( R i 1 , R i 2 ) by classifying the corresponding fea-ture vector x i and treating classification confidence as sim-ilarity. The classifier is typically trained using a corpus o f labeled data in the form of pairs of records that are known to be either co-referent ( ( R i 1 , R i 2 )  X  M ) or non-equivalent ( ( R i 1 , R i 2 )  X  U ).

In previous work a number of classifiers have been successfully utilized for this purpose, including Naive Bayes [34], decision trees [32, 33], maximum entropy [8], and Support Vector Machines [29, 4]. These classifiers have been deployed in batch settings where all training data is available in advance. Since in a product normalization set-ting new data is arriving continuously, an online classifier that can be trained incrementally is required. For this rea-son, we employ averaged perceptron [9], a space-efficient variation of the voted perceptron algorithm proposed and analyzed by Freund and Schapire [14].

The averaged perceptron is a linear classifier: given an instance x i , it generates a prediction of the form  X  y  X  avg x i , where  X  avg is a vector of ( K + 1 ) real weights. Weights are averaged over all weight vectors observed dur-ing the training process (as opposed to just using the final weight vector in the regular perceptron algorithm). In our approach, the weights directly correspond to the weights of basis similarity functions in the composite function de-scribed in previous section. Each x i is a pair-space vec-tor defined above, and we assign label  X  1 to class U of non-equivalent record pairs, and label + 1 to class M of co-referent record pairs.

Voted perceptron has several properties which make it particularly appropriate for the large-scale streaming li nk-age task. First and foremost, is a highly efficient online learning algorithm: the hypothesis (similarity function p a-rameters) that it generates is updated as more labeled ex-amples become available without the need to re-train on all previously seen training data. Second, voted perceptron is a linear classifier that produces a hypothesis which is intuit ive and easily interpretable by humans as relative importance of basis similarity functions, a highly attractive propert y for a system to be deployed and maintained on a continuous real-world task. Finally, voted perceptron is a discrimina -tive classifier with strong theoretical performance guaran -tees [14]. While several previously used classifiers use lin-ear hypotheses, online variants of decision trees have been proposed, and SVMs are also discriminative classifiers with strong theoretical guarantees, none of the previously used classifiers combine all benefits of voted perceptron.
Figure 1 shows the training algorithm for learning the pa-rameters  X  avg . The algorithm can be viewed as minimiz-ing the cumulative hinge loss suffered on a stream of exam-ples [30]. As every training record pair ( R i 1 , R i 2 , corresponding feature vector x i is presented to the learner, it incurs a (hinge) loss l ( x i , y i ) = max { X  y i  X  x i the vector of weights  X  is updated in the direction of the gradient to reduce the loss:  X  =  X   X   X  l ( x i , y i )  X  this training procedure corresponds to iterative evaluati on of the prediction for every training pair, and if the predic-tion differs from the true label, the weights are adjusted to correct for the error. This view can lead to variations of the algorithm using other loss functions, e.g. log-loss l log ( x i , y i ) = ln ( 1 + exp (  X  y i  X  x i )) . In our experiments, varying the loss function did not lead to a qualitative diffe r-ence in final performance on the linkage task, so we only report results obtained using hinge loss.
Once a composite similarity function f  X  with weights  X  avg has been trained to correctly predict whether a pair of records is co-referent, it can be used to compute an m  X  m similarity matrix S = { s i j } between all pairs of records con-sidered for linkage, where each s i j = f  X  ( R i , R j ) m . Thus, the task of training a composite similarity func-tion over pairs of records can be viewed as the problem of learning a matrix of similarity values where only a few val-ues in the matrix are initially given (i.e., the labeled trai ning pairs (( R i 1 , R i 2 ) , y i ) . During training the sign of the known entries in the matrix is constrained, and they are used to es-timate the remaining matrix values, each of which is a linear combination of basis similarity functions for the underlyi ng record pair.

However, we note that for a large number of records m , computing the full similarity matrix (an O ( m 2 ) operation) may be impractical since the vast majority of record pairs are not co-referent. Therefore, it is necessary to select a subset of candidate pairs between which similarity is com-puted. Several approaches for limiting the number of can-didate pairs have been proposed in the literature, such as blocking, where candidate pairs are required to share some attribute values [18], sorted neighborhood methods, where only records within a sliding window over a database sorted on several keys are considered [16], and canopies, where a computationally cheap similarity function is employed in a preliminary pass to obtain clusters of records within which linkage is performed [23]. An experimental comparison of several methods as well as several new methods based on an inverted index can be found in recent work by Baxter et al. [2]. In current work, we employ a variant of the canopies method where we limit candidate pairs to those having a minimum token overlap in one of the attributes. Similarity is not computed for records that do not share a canopy, and they are not allowed to merge in the clustering process.
Armed with a similarity matrix, we face the subsequent problem of identifying groups of equivalent records. As de-scribed in Section 2, two primary approaches have been previously used for the actual task of linking co-referent records: (1) pairwise linkage followed by transitive closu re, and (2) collective approaches where linkage decisions be-tween multiple pairs are made in conjunction. In this work, we compare the linkage performance of three variants of the Hierarchical Agglomerative Clustering (HAC) algorithm, of which one, single-link HAC, is a pairwise approach, and the other two (group-average HAC and complete-link HAC) are bottom-up greedy collective approaches [17].

Given a combined similarity function trained as de-scribed above, hierarchical agglomerative clustering ini -tially places every record in its own singleton cluster. The n, at every step, two clusters which are most similar accord-ing to a chosen linkage rule are merged. With single-link linkage, similarity between clusters is defined as the highest similarity of any individual cluster members; with group-average linkage, similarity between clusters is defined as the average similarity of all pairs of objects from the two clusters; and with complete-link linkage, similarity betw een clusters is defined as the lowest similarity of any individ-ual cluster members. Thus, group-average and complete-link HAC variants are greedy but efficient collective link-age techniques since the merge decision for every record pair depends on similarities of other pairs from the cluster s to which the records under consideration belong.
We have evaluated our approach on three datasets sam-pled from the F ROOGLE comparison shopping site: Digi-talCameras (4823 product offers), Camcorders (4531 prod-uct offers), and Luggage (6912 product offers). Each of the datasets was created by selecting a corresponding cat-egory in the product hierarchy and sampling product of-fers that contain the UPC (Universal Product Code) field, which uniquely identifies any commercial product. While less than 10% of all data includes the UPC values, they pro-vide the  X  X olden standard X  labels for evaluating linkage ac -curacy. In our experiments, the UPC values were only used for evaluation purposes, while in an actual fielded product normalization system they can be used as a highly useful attribute for computing overall similarity (although our r e-sults below indicate that UPC values alone should not be used as a single linkage criterion due to the presence of noise and certain domain artifacts discussed below).
Every product offer contains several fields, which in-clude textual attributes such as ProductName and Brand , as well as numerical attributes such as Price , and categor-ical features such as ProductCategory . Table 1 presents an example of co-referent records from the DigitalCameras dataset.

We have used several base similarity functions f k . For all string attributes, cosine similarity was used [1], alon g with a variant of string edit distance [15] on extracted mode l strings for electronic products. For numeric attributes, r ela-ical attributes, the inverse of path length in the F ROOGLE category hierarchy was used.

In our experiments, every dataset was split into two folds for each trial. Every record was randomly assigned to one of the two folds, and cross-validation was performed with one of the folds used for training, in which the overall simi-larity function was trained as described in Section 3.2. The two folds were then merged, and clustering was done on the entire dataset. Performance statistics were collected sep -arately for the pairs on the training and testing folds, as well as the entire dataset. This methodology corresponds to the  X  X onsulting X  linkage evaluation methodology [5], wher e the test fold contains records corresponding both to entiti es seen in training data as well as entities for which records can only be found in the test set.

We note that in an operational on-line environment, it is not necessary to re-cluster all previous data whenever a new record becomes available. Rather, for efficiency, every new record can be assigned to the existing cluster closest to it as long as the record X  X  similarity to that cluster is above some predetermined threshold; otherwise the record is to be placed in a new singleton cluster. While we make note of this point, the operational set-up of the system is not the core focus of this paper, and thus our experimental results focus on batch clustering of all available data to accuratel y evaluate the efficacy of the learning approach and each clus-tering method. The problem of making merge decisions efficiently for incoming records has been recently studied in the context of data cleaning in data warehouses [6], and fielded systems should combine that method with our ap-proach for online learning of similarity functions.
In the clustering process, precision, recall and F-measure defined over pairs of duplicates were computed after each merge step in the clustering process. Precision is the frac-tion of identified co-referent pairs that are correct, recal l is the fraction of co-referent pairs that were identified, and F-measure is the harmonic mean of precision and recall. While traditional use of precision-recall curves involves i n-terpolating several precision points at standard recall le v-els to the highest value seen for any lower recall, we have found that such interpolation may grossly misrepresent the dynamics of the clustering process. Therefore, we report non-interpolated (observed) precision values, averaged o ver 10 trials of 2 folds each.
Figures 2-4 show precision-recall curves on the 3 datasets. Different points on the curves were obtained by successively merging the two most similar clusters dur-ing the clustering process, and measuring the resulting precision and recall on records from the test set. Go-ing along a precision-recall curve from left to right, more records are grouped into the same cluster as clusters are successively merged  X  this typically results in higher reca ll for co-referent records but decreases the precision if non-equivalent records with low similarity are put into the same cluster.

In the observed precision-recall plots, there is an initial drop followed by an immediate rise of precision at low re-call values. To take a closer look at the results, we zoom in on the early part of the precision-recall curve in Figure 5,
Figure 5. Early part of precision-recall curve for DigitalCamera which shows a sharp drop in precision soon after the begin-ning. Analysis of the experiment traces reveals that the ob-served decrease in precision is due to erroneous UPC iden-tifiers provided by the merchants on some of the product offers, leading to label noise. Because a number of the erro-neously labeled offers are highly similar to correctly labe led co-referent offers, they were merged at very early stages, r e-sulting in the observed precision decrease. Inspection of t he clustering traces revealed that these initial co-referenc e pre-dictions were actually correct but were marked as mistakes due to the erroneous UPCs. As more clusters were merged, subsequent correct linkages of offers with proper UPC iden-tifiers cancelled out this effect, bringing the precision ba ck up. This recovery of precision with increasing recall due to cluster merging continued till more heterogeneous cluster s started to get merged. This resulted in some non-coreferent offers being put into the same cluster, making the clusters impure and decreasing the overall precision, as can be seen in Figure 2.

Besides the noisy UPC values, we observed other do-main artifacts that lowered the precision numbers obtained in our experiments. For example, differently colored vari-ants of the same product often have different UPC labels. Their linkage therefore penalizes observed precision valu es, although these offers are co-referent for comparison shop-ping purposes. Nevertheless, compared to a human expert-constructed similarity function computed on the same at-tributes (which we considered as a baseline), the perfor-mance of the learned weights was significantly higher, im-proving the accuracy of actual fielded linkage systems.
In all the experiments, complete-link HAC outperformed group-average HAC, which in turn performed better than single-link HAC. These results conclusively show that for the product normalization task it is recommended to use complete-link linkage either in batch or online settings. This can be explained by the fact that complete-link link-age avoids elongated clusters created by the single-link ru le due to the chaining effect. Chaining occurs as offers are added to a cluster because of high similarity to a single clus -ter member, leading to long  X  X hains X  of linked offers, while offers at the extremes of the chain have low similarity, lead -ing to error accumulation. In contrast, complete-link link -age yields tight, well-separated clusters, while the natur e of clusters obtained by group-average linkage is typically in between single-link and complete-link. Overall, the clus-ters obtained by collective linkage methods are best suited for the product normalization task.

While our online approach to learning the similarity function is highly scalable, computational efficiency of th e clustering algorithm used for linkage is a big concern. Even though HAC is an O ( n 2 log n ) algorithm [22], it can be ef-ficiently implemented in practice. If batch clustering is de -sired, all the three variants of HAC can be parallelized effe c-tively, using different parallel computation models and da ta structures [26]. It is also easy to combine blocking tech-niques (e.g., canopy creation) for data-preprocessing wit h hierarchical clustering, further increasing its efficienc y.
Learning the similarity function from training data gave different weights for the various attributes of the 3 datase ts. For example, the weights corresponding to similarity be-tween Brand and the ModelStrings attributes in the Lug-gage dataset were higher than that for Camcorders or Dig-italCameras , which can be explained by the fact that these attributes tend to identify luggage products uniquely more often than electronic items. These results show the impor-tance of learning individual similarity functions for diff er-ent product categories in the product normalization settin g.
An important component of the hierarchical cluster merging stage is deciding when to stop the clusters from being merged further, so that the remaining clusters can be reported as groups of co-referent records. While one ap-proach is to choose the number of clusters which gives the peak F-Measure value in the precision-recall curve, it is advisable to look at other cluster evaluation measures be-
Figure 6. Purity and Precision values Vs Num-ber of clusters for Camcorders fore making this choice. Another popular cluster evaluatio n measure used by clustering practitioners is average clus-ter purity [31]. It measures the average proportion of the dominant class label in a cluster  X  a higher cluster purity measure typically implies purer clusters. Figure 6 shows how the pairwise precision and purity values change with the number of clusters for the Camcorders dataset, as clus-ters are successively merged in the complete-link HAC al-gorithm. The purity measure peaks around 4300 clusters, while precision has a peak value at 4200 clusters. Depend-ing on whether having homogeneous clusters or pairwise accuracy is a higher priority, one of these measures may be preferable. Thus, it is necessary to keep several metrics in mind when deciding at which level to stop merging to trade off between correct pairwise linkage and overall cluster pu -rity, one of which may be preferable in a real-world linkage system.
The record linkage problem is ubiquitous in modern large-scale databases, and in this paper we have introduced a scalable, adaptive approach for product normalization, a n instance of linkage critical in online comparison shopping systems. Our method relies on an online learning algorithm for training a combined similarity function, permitting it to be deployed in settings where data is arriving continuously . Our experimental results show the efficacy of the proposed approach. We have found that linkage methods which con-sider similarity of multiple pairs of records jointly lead t o significantly higher performance than a pairwise approach where each linkage decision is made in isolation. Addition-ally, we have observed that previously employed clustering and linkage evaluation metrics have non-overlapping max-ima, suggesting that for real-world tuning of record linkag e systems several evaluation measures should be considered.
We would like to thank Raymond J. Mooney for insight-ful comments and discussions. We would also like to thank the Froogle team for providing us access to their data and computing infrastructure. This work was done when the first two authors were at Google Inc.

