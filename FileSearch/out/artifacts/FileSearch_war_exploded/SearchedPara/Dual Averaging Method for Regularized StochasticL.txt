 effects, e.g., sparsity under  X  1.1 Regularized stochastic learning The regularized stochastic learning problems we consider a re of the following form: Examples of the regularization term  X ( w ) include: able. Suppose at time t , we have the most up-to-date weight w evaluate the loss f ( w ential of f with respect to w ). Then we compute the new weight w where and good generalization performance observed in practice ( e.g., [3, 4]). larization effect. An important example and motivation for this paper is  X  learning, where  X ( w ) =  X  w  X  1.2 Regularized online optimization In online optimization (e.g., [9]), we make a sequence of decision w time t , a previously unknown cost function f assume that the functions f that the total cost up to each time t , P t prediction of time series and sequential investment (e.g. [ 10]). tion  X ( w ) . For any fixed decision variable w , consider the regret Algorithm 1 Regularized dual averaging (RDA) method input: initialize: w for t = 1 , 2 , 3 , . . . do end for we write f ( w, z there exists a constant &gt; 0 such that convexity. In equation (4), Arg min In Algorithm 1, step 1 is to compute a subgradient of f based methods. Step 2 is the online version of computing aver age gradient  X  g w the case for many important learning problems in practice. we can choose a parameter &gt; 0 and use the sequence nondecreasing sequence { simplicity, in the following examples, we use We first give bounds on the regret R creasing. Notice that we just introduced an extra parameter we can set at the end of the step t = 1 , so  X  Theorem 1 Let the sequences { w } t is a constant L such that  X  g any t  X  1 and any w  X  X  X  consequences based on concrete choices of algorithmic para meters. defined in (8) together with The best that minimizes the above bound is  X  = L/D , which leads to for some D &gt; 0 , and there is an L &gt; 0 such that E  X  g  X  2 Then for any t  X  1 , we have lems, especially with  X  In an online setting, each iteration of the FOBOS method can b e written as where The difference is more clear in the special case of  X  specifically, each component of w t = O (1 / This is confirmed by our computational experiments in Sectio n 5. SGD TG RDA IPM SGD TG
RDA Figure 1: Sparsity patterns of the weight w We provide computational experiments for the  X  testing examples. No preprocessing of the data is employed. We use  X  its. In the experiments, we compare the  X  at optimal weights are all zeros). In the  X  regularization, or K = 10 for enhanced regularization effect, as suggested in [5]. Figure 1 shows the sparsity patterns of the solutions w Both the TG and RDA methods were run with parameters for enhan ced  X  the batch optimization results solved by IPM, especially fo r larger . In contrast, the RDA method demonstrate a much more smooth va riation in the NNZs. testing error rates of w demonstrates very robust performance (small standard devi ations) for w only give performance bound for the averaged weight  X  w tion, here we only plot results of the  X  see that, overall, the solutions obtained by the  X  Error rates (%) NNZs 0 1 2 3 4 5 6 7 8 9 show sparsity patterns of w NNZs of w [19] K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale  X 
