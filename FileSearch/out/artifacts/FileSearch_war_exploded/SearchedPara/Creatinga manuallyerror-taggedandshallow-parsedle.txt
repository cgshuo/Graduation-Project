 The availabili ty of learn er corp ora is still some what limited desp ite the obvio us usefu lness of such data in condu cting resear ch on nat ural langu age proc ess-ing of lear ner Englis h in recent yea rs. In partic ular , learn er corp ora tagged with grammatic al erro rs are rare bec ause of the dif cult ies inher ent in learne r corp us crea tion as will be descr ibed in Sect. 2. As sho wn in Table 1, error-tagg ed learne r corp ora are very few among exis ting lear ner corpo ra (see Lea-cock et al. (2010 ) for a more deta iled disc ussio n of lear ner cor pora). Ev en if data is error -tagge d, it is often not availa ble to the pub lic or its acces s is severely rest ricted . For exampl e, the Cambridg e Learner Corpus, which is one of the lar gest error-tagg ed learn er corpo ra, can only be used by auth ors and writers worki ng for Cambridg e Uni versity Press and by members of staf f at Cambrid ge ESOL .
Error -ta gged lear ner cor pora are crucia l for devel-opin g and evalu ating erro r det ection /corre ction al-gorit hms such as those descri bed in (Rozo vsk aya and Roth, 2010 b; Chodorow and Leacoc k, 2000 ; Chodor ow et al., 2007 ; Felice and Pulman, 2008 ; Han et al., 2004 ; Han et al., 2006 ; Izumi et al., 2003 b; Lee and Senef f, 2008 ; Nagata et al., 2004 ; Nagata et al., 2005 ; Nagata et al., 2006 ; Tetreau lt et al., 2010 b). This is one of the most acti ve rese arch areas in natura l lang uage proces sing of lear ner En-glish . Because of the restr iction s on the ir availa bil-ity, resear chers have used thei r own learn er corp ora to develo p and evalu ate error detec tion/c orrec tion method s, which are ofte n not common ly avail able to other rese arche rs. This means that the detec -tion/ correc tion perfo rmance of each existi ng metho d is not direct ly compara ble as Rozo vskaya and Roth (201 0a) and Tetreaul t et al. (2010 a) poin t out. In othe r words, we are not sure which methods achiev e the best perfor mance. Commonly availa ble error-tagg ed lear ner corpo ra are there fore essen tial to fur -ther resea rch in this area .

For similar reaso ns, to the best of our kno wledg e, there exists no such lear ner corpu s that is manuall y shall ow-parse d and which is also pub licly available, unlik e, say , nativ e-spea ker corp ora suc h as the Penn Treeban k. Such a compar ison brings up anoth er cru-cial ques tion:  X Do exist ing POS tagge rs and chun -kers work on lear ner Englis h as well as on edit ed text such as newspap er arti cles? X  Nobod y reall y kno ws the answer to the ques tion. The only except ion in the litera ture is the work by Tetreaul t et al. (201 0b) who evalua ted parsi ng perfo rmance in relat ion to prepo -sitio ns. Ne verthel ess, a grea t number of rese archer s have used exis ting POS tagg ers and chun kers to ana-lyze the writing of learne rs of Englis h. For inst ance, error detect ion methods normall y use a POS tagge r and/ or a chu nker in the error detec tion proc ess. It is there fore possi ble that a major cause of false pos -itives and negati ves in error detec tion may be at-trib uted to error s in POS-tagging and chu nking . In corp us ling uistic s, rese archer s (Aarts and Grange r, 1998 ; Grang er, 1998 ; Tono, 200 0) use such too ls to extrac t inte restin g patter ns from learn er corpo ra and to reveal lear ners' tend encie s. Ho we ver, poor per-forman ce of the tools may resu lt in mislea ding con -clusi ons.

Given this back grou nd, we des cribe in this pape r a manuall y error -tagge d and shal low-parse d learne r corp us that we crea ted. In Sect. 2, we disc uss the dif cult ies inh erent in learne r corpu s crea tion. Con-sider ing the dif cult ies, in Sect. 3, we desc ribe our method for lear ner corp us creat ion, incl uding its data collec tion metho d and anno tation scheme s. In Sect. 4, we descri be our learn er corp us in detail . The learn er corp us is called the Konan-JIEM lear ner cor-pus (KJ corp us) and is free ly available for rese arch and edu cation al purpo ses on the web 1 . Anothe r cont rib uti on of this pap er is that we tak e the rst step toward ans wering the quest ion about the per-forman ce of existin g POS -tag ging/ chunk ing tech -niqu es on learn er data. We report and discu ss the resul ts in Sect. 5. In addit ion to the common dif culti es in crea ting any corpu s, lear ner corpu s creati on has its own dif-cultie s. We clas sify them into the follo wing four cate go ries of the dif cul ty in: 1. colle cting texts written by learn ers; 2. trans forming coll ected texts into a cor pus; 3. cop yright trans fer; and 4. error and POS/parsing annot ation.

The rst dif cult y conc erns the proble m in col-lecti ng texts written by learn ers. As in the case of other cor pora, it is prefer able that the size of a learn er corp us be as lar ge as poss ible where the size can be measure d in several ways inclu ding the tota l number of texts, words , sente nces, writer s, topi cs, and texts per writer . Ho we ver, it is much more dif -cult to crea te a lar ge learne r corpu s than to creat e a lar ge nat ive-spea ker corp us. In the case of nativ e-spea ker corp ora, publ ished texts suc h as newspa -per arti cles or novels can be used as a corp us. By cont rast, in the case of learne r corp ora, we must nd learne rs and then let the m write sinc e ther e are no suc h publi shed texts written by learn ers of English (unl ess the y are part of a learn er corpu s). Here, it sho uld be emphas ized that learn ers ofte n do not spont aneou sly write but are typi cally obli ged to write, for example , in class, or duri ng an exam. Becaus e of this , lear ners may soon beco me tired of writing . This in itself can affec t lear ner corp us cre-ation much more than one would expec t esp eciall y when creat ing a long itudin al learne r corp us. Thus, it is cruc ial to keep learn ers moti vate d and focus ed on the writing assi gnments .

The secon d dif culty aris es when the colle cted texts are trans formed into a learn er corp us. This involv es severa l time-con suming and troubl esome tasks . The texts must be archiv ed in electro nic form, which requi res typ ing every singl e colle cted text sinc e learne rs normall y write on pape r. Be-sides , each text must be arch ived and maintai ned with acco mpan ying info rmation such as who wrote what text when and on what topic. Optional ly, a learn er corp us could includ e othe r piece s of infor-mation suc h as procien cy, rst langu age, and age. Once the texts have been electr onical ly archi ved, it is relati vely easy to mainta in and acc ess the m. Ho w-ever, this is not the case when the texts are rst col-lecte d. Thus, it is bett er to have an efcient metho d for managi ng such info rmation as well as the texts themse lves.

The third dif culty con cernin g cop yright is a daun ting prob lem. The cop yright for each text must be tran sferre d to the corp us creat or so that the learn er corp us can be made availa ble to the publ ic. Consid er the case when a number of learn ers par-ticip ate in a learne r corp us creati on proje ct and ev-eryo ne has to sign a cop yright transf er form. This is-sue beco mes even more compl icated when the writer does not actu ally have such a righ t to trans fer cop y-right . For instan ce, unde r the Japan ese law, thos e youn ger than 20 years of age do not have the right ; inste ad their pare nts do. Thus, corp us creato rs have to ask learn ers' paren ts to sign cop yrigh t tran sfer forms. This is often the case since the writers in learn er corpu s crea tion projec ts are normal ly junio r high sch ool, high schoo l, or colle ge stude nts.
The nal dif cult y is in erro r and POS/parsin g anno tation . For error ann otatio n, several ann ota-tion sch emes exis t (for example , the NICT JLE sche me (Izumi et al., 2005 )). While desig ning an an-nota tion sch eme is one issue , ann otatin g errors is yet anot her . No matter how well an annot ation sch eme is desig ned, there will always be excep tions . Ev ery time an exce ption appea rs, it become s neces sary to revise the annot ation scheme . Another issue we have to remember is that there is a trade-off betwee n the gran ularity of an annot ation sche me and the level of the dif cul ty in error ann otatio n. The more deta iled an anno tation scheme is, the more infor mation it can cont ain and the more dif cult ident ifying erro rs is, and vice versa .

For POS/parsing ann otatio n, there are also a num-ber of annot ation sch emes incl uding the Bro wn tag set, the Cla ws tag set, and the Penn Treeban k tag set. Ho we ver, none of them are design ed to be use d for learne r corpo ra. In other words, a vari ety of lin-guis tic phe nomena occ ur in learn er corp ora which the exis ting ann otatio n schemes do not cover. For insta nce, spel ling errors ofte n app ear in texts writ-ten by lear ners of English as in sar d year , which shou ld be thir d year . Grammatical errors pre vent us appl ying existin g annot ation sche mes, too. For in-stanc e, there are at leas t three possi bilitie s for POS-tagg ing the word sing in the sente nce every one sin g together . using the Penn Treeban k tag set: sing /VB, sing /VBP , or sing/ VBZ. The follo wing exampl e is more compl icated : I don' t succ ess coo king . Nor -mally , the word succe ss is not used as a verb but as a nou n. The inst ance, howe ver, appe ars in a po-sitio n where a verb appea rs. As a resul t, there are at leas t two pos sibilit ies for tagg ing: succe ss/NN and suc cess/VB. Errors in mechani cs are also prob -lematic as in Tonight ,we and beaut ifulho use (miss-ing spac es) 2 . One solut ion is to split them to obtai n the corr ect string s and then tag them with a nor mal sche me. Ho we ver, this would remo ve the infor ma-tion that spaces were orig inally missin g which we want to prese rve. To handl e thes e and oth er phe -nomena which are pec uliar to learne r corpo ra, we need to develo p a novel anno tation sch eme. 3.1 Ho w to Collect and Maintain Texts Writte n Our text-co llecti on method is based on writing exer-cises . In the writing exercis es, learn ers write essay s on a blog syst em. This very simple idea of using a blog system natu rally solv es the prob lem of archiv-ing texts in electro nic form. In addi tion, the use of a blog syste m ena bles us to easil y regis ter and main-tain acco mpan ying informa tion inclu ding who (use r ID) writes when (up loade d time) and on what topi c (title of blog item). Besides , once regis tered in the user prole, the optio nal piec es of infor mation suc h as procie ncy, rst langu age, and age are also eas y to maint ain and acce ss.

To des ign the writin g exercis es, we consu lted with severa l teache rs of Englis h and con ducte d pre-exper iments. Ten learn ers parti cipate d in the pre-exper iments and were assign ed ve essa y topic s on avera ge. Based on the expe rimental result s, we desig ned the proc edure of the writing exerci se as sho wn in Table 2. In the rst step , learne rs are as-sign ed an essay topic . In the secon d step, the y are given time to prep are durin g which the y thin k abou t what to write on the given top ic befor e the y star t writing . We found that this enable s the stude nts to write more. In the third step , the y actua lly write an essay on the blog syste m. After the y have nishe d writing , the y submit their essay to the blog sys tem to be regist ered.

The following steps were con sidere d optio nal. We implemen ted an article erro r det ection method (Na-gata et al., 2006 ) in the blog syst em as a trial at-tempt to keep the learn ers moti vated since lear ners are lik ely to beco me tired of doin g the same exercis e repea tedly . To redu ce this , the blog system high -light s where artic le error s exist after the essa y has been submit ted. The hop e is that this might pro mpt the learn ers to write more accu rately and to conti nue the exercise s. In the pre-e xperimen ts, the detec tion did ind eed seem to inter est the learn ers and to pro-vide them with addit ional moti vation. Conside ring these resul ts, we decide d to incl ude the fou rth and fth steps in the writin g exercis es when we create d our learn er corpu s. At the same time, we shou ld of cour se be aware tha t the use of error dete ction affect s learn ers' writing. For example, it may chang e the distr ibuti on of error s. Nagata and Nakatan i (2010 ) repo rted the effec ts in detail .

To solv e the prob lem of cop yrigh t transf er, we took legal prof ession al advic e but were informe d that, in Japan at least , the only way to be sure is to have a cop yrigh t tran sfer form signe d every time. We consid ered having it sign ed on the blo g syste m, but it soo n turn ed out that this did not work sinc e parti cipati ng learn ers may still be too you ng to have the legal righ t to sign the trans fer . It is left for our long -term future work to devise a bette r solu tion to this legal issue . 3.2 Ann otat ion Scheme This subs ection des cribes the error and POS/chunkin g ann otatio n sche mes. Note that error s and POS/chunkin g are anno tated separa tely , meanin g that ther e are two les for any given text. Due to space restri ctions we limit ours elv es to onl y summariz ing our anno tation schemes in this sectio n. The full desc riptio ns are available toget her with the anno tated corp us on the web . 3.2.1 Err or Annotation We based our error annot ation sch eme on that use d in the NICT JLE corp us (Izumi et al., 2003 a), whose detai led descr iption is read ily availa ble, for exam-ple, in Izu mi et al. (200 5). In that anno tatio n sche me and acco rding ly in ours , erro rs are tagge d usin g an XML synt ax; an error is annot ated by tag-ging a word or phra se that conta ins it. For in-stanc e, a tens e error is anno tated as foll ows: I v tns crr= X mad e X  mak e /v tns pies last year . where v tns deno tes a ten se error in a verb . It shou ld be emphas ized that the erro r tags con tain the infor mation on corre ction toget her with error anno -tatio n. For inst ance, crr= X mad e X  in the abo ve ex-ample deno tes the corr ect form of the verb is made . For missing word errors , error tags are placed where a word or phras e is missing (e.g., My friend s live prp crr= X in X  /prp thes e place s. ).

As a pilot stu dy, we appli ed the NICT JLE ann ota-tion sche me to a learne r cor pus to reveal what mod-icatio ns we neede d to mak e. The learn er corpu s cons isted of 455 essay s (39 ,716 words) written by junio r high and hig h schoo l stud ents 3 . The follo w-ing descr ibes the major modica tions deemed nec-essar y as a resu lt of the pilo t study .

The bigg est dif ference betwee n the NICT JLE corp us and our tar geted corp us is that the former is spok en data and the latt er is written dat a. This dif fer-ence ine vita bly requ ires severa l modicati ons to the anno tation scheme. In speec h data , ther e are no er-rors in spel ling and mecha nics such as punct uatio n and capit alizat ion. Ho we ver, since such erro rs are not usual ly regarded as grammatica l errors , we de-cided simply not to anno tate them in our anno tatio n sche mes.
 Anothe r major dif fere nce is frag ment erro rs. Fragments that do not form a compl ete sente nce of-ten appea r in the writing of lear ners (e.g., I hav e many boo ks. Because I like readin g. ). In writte n lang uage, fragmen ts can be regard ed as a grammat-ical error . To anno tate frag ment error s, we adde d a new tag f (e.g., I have many boo ks. f Becaus e I like readi ng. /f ).

As disc ussed in Sect. 2, ther e is a trade -of f be-tween the gran ularit y of an ann otatio n scheme and the level of the dif cult y in ann otatin g errors . In our anno tation scheme, we narro wed down the numbe r of tags to 22 from 46 in the origin al NICT JLE tag set to facil itate the annot ation; the 22 tags are sho wn in Appen dix A. The remo ved tags are mer ged into the tag for other . For inst ance, the re are only thre e tags for erro rs in nou ns (nu mber , lexis, and othe r) in our tag set wherea s there are six in the NICT JLE corp us (inect ion, numbe r, case, cou ntabil ity, com-plemen t, and lexis) ; the other tag ( n o ) covers the four remo ved tags . 3.2.2 POS/C hunkin g Ann ota tion We sele cted the Penn Treeban k tag set, which is one of the most widely used tag sets , for our POS/chunkin g anno tatio n scheme . Similar to the er-ror ann otatio n sch eme, we con ducte d a pilot stud y to determin e what modicati ons we need ed to mak e to the Penn Treebank sche me. In the pilo t study , we used the same learne r corp us as in the pilot study for the error anno tatio n scheme.
 As a resu lt of the pilo t stud y, we fou nd tha t the Penn Treeban k tag set suf ced in most cases excep t for erro rs which lear ners made. Conside ring this, we deter mined a bas ic rule as follows:  X Use the Penn Treeban k tag set and pre serv e the orig inal texts as much as poss ible.  X  To hand le such erro rs, we made severa l modicatio ns and added two new POS tags (CE and UK) and anoth er two for chun king (XP and PH), which are descri bed belo w.

A major modicat ion conce rns errors in mechan -ics such as Tonight,we and beau tifulh ouse as alre ady expla ined in Sect. 2. We use the symbol  X - X  to an-nota te such cases. For inst ance, the abo ve two ex-amples are anno tated as follows: Tonight,we/NN-,-PRP and bea utiful house /JJ-NN . Note tha t eac h POS tag is hyph enated . It can also be use d for ann otatin g chu nks in the same manner . For insta nce, Tonight,we is anno tated as [NP-PH-N P Tonight ,we/NN-,-PR P ] . Here, the tag PH stan ds for chun k label and den otes tok ens which are not normal ly chunk ed (cf., [NP Tonight /NN ] ,/, [NP we/PRP ]).

Anothe r major modicat ion was requir ed to han -dle grammat ical error s. Essentia lly, POS /chu nkin g tags are ass igned acco rding to the surf ace infor ma-tion of the word in questi on regard less of the ex-isten ce of any errors . For example, Ther e is ap-ples. is anno tated as [NP Ther e/EX ] [VP is/VBZ ] [NP appl es/NNS ] ./. Addition ally , we dene the CE 4 tag to anno tate erro rs in which learne rs use a word with a POS which is not allowed such as in I don' t succ ess cook ing . The CE tag enco des a POS which is obta ined from the surf ace info rmation to-geth er with the POS which would have been as-sign ed to the word if it were not for the error . For insta nce, the abo ve exampl e is tagg ed as I don' t succ ess/CE:NN:VB cook ing . In this format , the sec-ond and third POSs are separa ted by  X : X  which de-note s the POS which is obt ained from the sur face infor mation and the POS which would be assig ned to the word without an erro r. The user can selec t eithe r POS depe nding on his or her purpo ses. Note that the CE tag is compa tible with the basic anno -tatio n sch eme beca use we can retri eve the basic an-nota tion by extra cting onl y the seco nd elemen t (i.e., succ ess/NN). If the tag is unkn own becaus e of gram-matical error s or oth er pheno mena, UK and XP 5 are used for POS and chunk ing, resp ecti vely.

For spel ling errors , the corre spond ing POS and chun king tag are assign ed to mistak enly spe lled words if the corre ct forms can be gue ssed (e.g., [NP sird/ JJ year /NN ]); oth erwise UK and XP are used . We carr ied out a lear ner corp us creat ion proj ect us-ing the descr ibed metho d. Twenty six Jap anese col-lege stude nts partic ipated in the proje ct. At the be-ginn ing, we had the stude nts or the ir par ents sig n a con ventio nal pape r-bas ed cop yright tran sfer form. After that , the y did the writing exercis e des cribed in Sect. 3 once or twice a week over three months . Dur -ing that time, the y were assi gned ten top ics, which were dete rmined base d on a writin g textbo ok (Ok-ihara , 198 5). As desc ribed in Sect. 3, the y used a blog syst em to write, submit, and rewrite their es-says . Throug h out the exerci ses, the y did not have acces s to the others ' essays and thei r own pre vious essay s.

As a resu lt, 233 essa ys were coll ected; Table 3 sho ws the stati stics on the collec ted essay s. It turne d out that the lear ners had no dif cult ies in usin g the blog syste m and seemed to focu s on writin g. Out of the 26 par ticipa nts, 22 complet ed the 10 assign ments while one stude nt quit befo re the exercis es start ed.
We anno tated the grammatic al erro rs of all 233 essay s. Two pers ons were involv ed in the ann ota-tion. After the annot ation , ano ther per son chec ked the ann otatio n resu lts; dif fer ences in error ann ota-tion were resol ved by cons ulting the rst two. The error annot ation scheme was found to work well on them. The error-anno tated essay s can be used for evalua ting error dete ction /corre ction method s.
For POS/chunkin g anno tatio n, we chose 170 es-says out of 233 . We annot ated them using our POS/chunkin g sche me; hereaf ter, the 170 essay s will be referr ed to as the shall ow-parsed corpu s. 5.1 POS Tagging The 170 essa ys in the shallow-parsed corp us was used for evalua ting existin g POS-tagging tech nique s on texts written by learn ers. It con sisted of 2,41 1 sente nces and 22,45 2 tok ens.

HMM-based and CRF-based POS tagg ers were teste d on the shallow-parsed corpu s. The former was implemen ted usi ng tri-g rams by the autho r. It was train ed on a corp us con sistin g of Englis h learn ing materia ls (213 ,017 tok ens). The latte r was CRFT ag-ger 6 , which was trai ned on the WSJ corpu s. Both use the Penn Treeban k POS tag set.

The perf ormance was evalu ated usin g accu rac y dened by If the numbe r of tok ens in a sente nce was dif fer-ent in the human annot ation and the sys tem out-put, the sent ence was exclud ed from the calc ula-tion. This discr epanc y sometimes occu rred beca use the tok enizat ion of the syste m sometimes dif fere d from that of the human anno tators . As a resu lt, 19 and 126 sente nces (215 and 1,352 tok ens) were ex-clud ed from the evalu ation in the HMM-base d and CRF-based POS tagg ers, resp ecti vely .

Table 4 sho ws the resu lts. The seco nd colu mn corre spond s to accur acies on a nati ve-s peak er cor-pus (sect. 00 of the WSJ corp us). The third colu mn corre spond s to acc uracie s on the learn er corp us.
As sho wn in Table 4, the CRF -ba sed POS tagge r suf fer s a dec rease in accu rac y as expecte d. Inter est-ingly , the HMM-based POS tagg er perfor med bet-ter on the learn er corpu s. This is perha ps beca use it was train ed on a corpu s consi sting of English learn -ing materi als whose distr ibutio n of vocabula ry was expec ted to be relat ively similar to that of the learne r corp us. By contr ast, it did not perf orm well on the nati ve-speak er corpu s becau se the size of the train -ing corp us was rela tively small and the distributio n of vocabula ry was not similar , and thus unkn own words often appea red. This implie s that selec ting appr opriat e texts as a train ing corpu s may impro ve the perfo rmance.

Table 5 shows the top ve POSs mistak enly tagg ed as other POSs. An obvio us caus e of mis-tak es in both tagge rs is that the y ine vitably mak e error s in the POS s that are not dened in the Penn Treeban k tag set, that is, UK and CE. A close r look at the tagg ing resul ts reveale d that phen om-ena which were common to the writing of lear ners were major caus es of other mistak es. Errors in cap-italiz ation partl y expl ain why the tagg ers made so man y mistak es in NN (singu lar noun s). The y ofte n iden tied errone ously capita lized common noun s as prope r noun s as in This Summer/NNP Vaca-tion/ NNP . Spelling errors affect ed the tagg ers in the same way. Grammatica l error s also caus ed confu -sion bet ween POSs. For instan ce, omiss ion of a cer -tain word ofte n cause d confu sion between a verb and an adjec tive as in I frigh tened /VBD. which shoul d be I (was) frigh tened /JJ . Anothe r intere sting case is expre ssion s that lear ners overuse (e.g., and/ CC so/RB on/RB and so/JJ so/JJ ). Such phras es are not erron eous but are rela tively infreq uent in nativ e-spea ker corp ora. Therefo re, the tagge rs tende d to iden tify their POS s accor ding to the surf ace infor-mation on the tok ens themselv es when such phr ases appe ared in the lear ner corp us (e.g., and /CC so/RB on/IN and so/RB so/RB ). We shou ld be aware that tok enizati on is also pro blematic altho ugh fail ures in tok enizati on were exclud ed from the acc uracie s. The inuenc e of the decrea se in accu rac y on othe r NLP task s is expect ed to be task and/o r method de-pend ent. Methods that direc tly use or handl e se-quen ces of POSs are lik ely to suf fer from it. An example is the error detec tion metho d (Chodo row and Leacock , 200 0), which ident ies unna tural se-quen ces of POSs as grammatica l erro rs in the writ-ing of learn ers. As just discu ssed abo ve, exis ting techn iques ofte n fail in seque nces of POSs that have a grammatica l erro r. For insta nce, an exis ting POS tagg er lik ely tags the sente nce I frig htene d. as I/PRP frigh tened /VBD ./. as we have just seen, and in turn the erro r det ection method cann ot iden tify it as an error becau se the sequ ence PRP VBD is not unn atu-ral; it would corr ectly dete ct it if the sen tence were corre ctly tagg ed as I/PRP frigh tened /JJ ./. For the same reaso n, the decr ease in accu rac y may affect the method s (Aarts and Grang er, 1998 ; Grange r, 1998 ; Tono, 2000) for extra cting inte restin g sequ ences of POSs from learn er corpo ra; for exampl e, BOS 7 PRP JJ is an inte restin g seq uence but is never extra cted unle ss the phras e is correc tly POS-tagged. It re-quire s furthe r investigati on to reveal how much im-pact the decre ase has on thes e method s. By con trast, error detect ion/co rrecti on methods based on the bag -of-w ord feat ures (or featu re vector s) are expec ted to suf fer less from it since mistak enly POS-tagged to-kens are only one of the feat ures. At the same time, we shoul d notic e tha t if the tar get erro rs are in the tok ens tha t are mistak enly POS -tag ged, the detec -tion will lik ely fail (e.g., verbs shou ld be corre ctly iden tied in tense erro r detect ion).

In addit ion to the abo ve evaluat ion, we at-tempted to impro ve the POS tag gers usin g the trans formatio n-bas ed POS-taggin g techn ique (Brill, 1994 ). In the tech niqu e, transf ormation rules are obta ined by compar ing the outpu t of a POS tagge r and the human annot ation so that the dif fer ences be-tween the two are reduc ed. We used the shal low-parse d corpu s as a test corp us and the othe r man-ually POS-tagged corpu s crea ted in the pilo t stud y desc ribed in Subsec t. 3.2.1 as a train ing corpu s. We used POS-based and word-b ased trans formatio ns as Brill (19 94) descri bed.

Table 6 sho ws the impro vements toge ther with the origi nal accura cies. Table 6 reveal s that even the simple appl icatio n of Brill' s techn ique achiev es a sligh t impro vement in both tagg ers. Designi ng the templat es of the trans formatio n for lear ner corp ora may achi eve furth er impro vemen t. 5.2 Head Noun Identicati on In the evalu ation of chun king, we focu s on hea d noun iden ticatio n. Head noun iden tication ofte n play s an importa nt role in error dete ction/ correc tion. For example , it is crucia l to iden tify head noun s to detec t errors in artic le and number .

We again used the shallow-parsed corp us as a test corp us. The essa ys conta ined 3,58 9 head noun s. We implemen ted an HMM-based chun ker usin g 5-grams whose input is a seque nce of POS s, which was obtain ed by the HMM-based POS tagge r de-scrib ed in the pre vio us subse ction . The chu nker was train ed on the same cor pus as the HMM-based POS tagg er. The perf ormance was evalu ated by recall and preci sion dened by and respe ctively .

Table 7 sho ws the resu lts. To our surp rise, the chun ker perfor med bette r than we had expec ted. A poss ible reaso n for this is that sent ences written by learn ers of English tend to be sho rter and simple r in terms of thei r struct ure.

The resul ts in Table 7 also enabl e us to qua nti-tati vely estima te expe cted impro vemen t in erro r de-tecti on/co rrectio n which is achiev ed by impro vin g chun king. To see this, let us dene the follo win g symbol s: : Recall of hea d noun iden tication , : recal l of error detec tion without chunk ing erro r, recal l of error dete ction with chunk ing error . and are inter preted as the true recal l of error detec tion and its obs erv ed valu e when chu nking error exists, respe ctively . Here, note that can be expres sed as ing errors exist and reca ll of hea d noun iden ticatio n is icatio n to the error detec tion method . Precision can also be esti mated in a similar manner alth ough it re-quire s a more complica ted calcul ation . In this pape r, we disc ussed the dif culti es inher ent in learn er corpu s creati on and a method for efcientl y creat ing a learn er corp us. We descri bed the manu-ally error-anno tated and sha llo w-pars ed lear ner cor-pus which was create d usin g this metho d. We also sho wed its usef ulnes s in developing and evalua ting POS tagge rs and chunk ers. We belie ve that publ ish-ing this corp us will give rese archer s a common de-velop ment and test set for develop ing relat ed NLP techn iques inc luding error detec tion/c orrec tion and POS-tagging /chun king, which will faci litate furthe r resea rch in these areas.
 This is the list of our erro r tag set. It is base d on the NICT JLE tag set (Izumi et al., 2005 ).
