 Retrieving semi-structured entities to answer keyword queries is an increasingly important feature of many modern Web applications. The fast-growing Linked Open Data (LOD) movement makes it possible to crawl and index very large amounts of structured data describing hundreds of millions of entities. However, entity retrieval approaches have yet to find efficient and effective ways of ranking and navigating through those large data sets. In this paper, we address the problem of Ad-hoc Object Retrieval over large-scale LOD data by proposing a hybrid approach that combines IR and structured search techniques. Specifically, we propose an architecture that exploits an inverted index to answer keyword queries as well as a semi-structured database to improve the search effectiveness by automatically generating queries over the LOD graph. Experimental results show that our ranking algorithms exploiting both IR and graph indices outperform state-of-the-art entity retrieval techniques by up to 25% over the BM25 baseline.
 H.3.3 [ Information Storage And Retrieval ]: Infor-mation Search and Retrieval X  retrieval models ; H.3.4 [ Information Storage And Retrieval ]: Systems and Software X  performance evaluation (efficiency and effectiveness) Algorithms, Experimentation, Performance Ad-hoc Object Retrieval, Entity Search, LOD
Many modern websites, such as Web portals or news ag-gregators, are today including entity-centric functionalities. Common examples include the aggregation of all pieces of content related to a given person, or the extraction of the most important entities appearing in a given article. Some companies, like the New York Times, manually maintain a directory of entities and ask human experts to create links between their resources (e.g., news articles) and the corre-sponding entities (e.g., celebrities appearing in the articles). Increasingly, however, websites are turning to automated methods due to the sheer size of the resources they have to analyze, and the large number of entities they have to consider.

Recently, the Linked Open Data (LOD) movement started an effort to make entity data openly available on the Web. In this initiative, Uniform Resource Identifiers (URIs) are used to identify entities. Each entity can be looked up ( dereferenced ) online, where it is described and linked to further entities using the Resource Description Framework 3 (RDF). The fundamental difference between LOD and standard entity datasets like, for instance, Wikipedia, lies in the inherent structure of the data. On the LOD cloud, the data is provided by uncorrelated parties and is given as a giant graph of semi-structured data.
In the context of online entity search, the TREC Entity track [3] has studied two related search tasks:  X  X elated En-tity Finding X  (i.e., finding all entities related to a given en-tity query) and  X  X ntity List Completion X  (i.e., finding en-tities with common properties given some examples). The SemSearch challenge 4 focused on Ad-hoc Object Retrieval (AOR), that is, finding the entity identifier of a specific en-tity described by a user query [20].

This paper focuses on Ad-hoc Object Retrieval over semi-structured data. We propose a novel search architecture that exploits both IR and graph data management techniques to effectively and efficiently answer AOR queries. Specifi-cally, we propose a hybrid solution that starts by retriev-ing an initial list of results from an inverted index using a ranking function, and then re-ranks and extends the re-sult list by exploiting the underlying graph representation of the data. Our extended experimental evaluation per-formed over standard collections shows that our proposed solution significantly improves the effectiveness (up to 25% improvement over the chosen baseline) while maintaining very low query execution times. We consider our results as especially promising since the LOD test collections that are http://linkeddata.org/ http://tools.ietf.org/html/rfc3986 http://www.w3.org/RDF/ http://semsearch.yahoo.com today available are noisy and incomplete, and since we ex-pect both the quality and the coverage of LOD datasets to rapidly improve in the future.

In summary, the main contributions of this paper are:
The rest of the paper is structured as follows. We dis-cuss related work, focussing on Entity Search and AOR ap-proaches, in Section 2. We briefly introduce the Linked Open Data movement in Section 3. Section 4 is devoted to a high-level description of our hybrid system. We de-scribe several approaches for AOR based on inverted indices and NLP techniques in Section 5, and describe complemen-tary techniques based on structured queries and graph data management techniques in Section 6. In Section 7, we exper-imentally compare the performance of our hybrid approach to a series of state-of-the-art AOR approaches on two stan-dard test collections. Finally, we present our conclusions in Section 8.
Searching for entities instead of documents or Web pages is a recent trend in IR. Early approaches focused on single-type entity search, for example in the context of the expert finding task at the TREC Enterprise track [4], where stan-dard IR approaches such as language modeling [1] have been applied. Complex entity search tasks have been addressed more recently. For instance, the INEX Entity Ranking track [12] studied the problem of finding entities matching a key-word query (e.g.,  X  X ountries where I can pay in Euro X ) using Wikipedia. The Entity track at TREC [3] evaluated the Re-lated Entity Finding task, which aims at finding entities re-lated to a given entity (e.g.,  X  X irlines using the Boeing 747 X ) using both a collection of Web pages as well as a collection of RDF triples. Several effective approaches for this task focus on entity type and co-occurrence information [7, 17]. The task of ranking entities appearing in one document taking into account the time dimension has been studied in [13].
The main goal of the SemSearch Challenge is to create evaluation collections for the task of Ad-hoc Object Re-trieval [20] on the Web of data. This task consists in retriev-ing entity identifiers (i.e., URIs) given a keyword query de-scribing the single entity the user is looking for (e.g.,  X  X arry potter X ). Evaluation collections for this task have been cre-ated by crowdsourcing relevance judgements [5].

The first approaches for AOR exploited standard IR tech-niques that had previously been used for other entity search tasks such as expert finding. The basic idea is to construct an entity profile by collecting and aggregating all informa-tion available about an entity and to index the resulting collections of entity profiles with standard IR techniques in order to answer entity search queries. Looking back at pre-vious work from entity search, several techniques can be applied to improve the effectiveness of the AOR task. For example, different data types can be used to rank entities for AOR based on an inverted index and BM25F [21] as a ranking function [6].

Various methods already proposed for different entity search tasks can be exploited for AOR as well, such as approaches exploiting probabilistic models [2]. In [11], Demartini et al. adopt structured and Natural Language Processing approaches to improve the effectiveness of entity search. We suggest further AOR approaches relying on recent IR techniques in Section 5.

The contribution of this work is a novel hybrid approach that benefits both from known IR ranking functions as well as from an analysis of the graph structure of the entities. We compare below our novel approach against state-of-the-art entity search techniques.
A number of hybrid search systems have already been developed. In [14], Elbassuoni and Blanco propose rank-ing models for keyword queries over RDF data. Their task is different from AOR as they aim at selecting subgraphs matching the query and then ranking them by means of sta-tistical language models. CE 2 [23] is a hybrid IR-DBMS sys-tem that provides ranking schemes for hybrid queries (i.e., keyword queries that describe joins and predicates between entities) over RDF data. Beagle ++ [18] is a desktop search engine that exploits both an inverted index and a struc-tured repository for file metadata to provide more effective search functionalities to the desktop user. SIREn [10] is a hybrid Web search framework that supports keyword as well as structured queries over RDF data. SIREn focuses on scal-ability and efficiency through novel indexing schemes that can index data and answer user queries efficiently by using an inverted index. Instead, we propose methods to improve ranking effectiveness for AOR.
LOD is an online movement whose origins can be traced back to a note 5 published by Tim Berner-Lee a few years ago. LOD suggests to publish data on the Web following four principles: i) using URIs to identify things ii) using HTTP URIs such that things can be dereferenced online (using for example a Web browser) iii) providing useful, structured in-formation about the things when they are dereferenced, us-ing for example RDF/XML and iv) including links to other, related URIs in the exposed data to foster data aggregation and discovery.

Hence, LOD can be seen as a grassroots effort leverag-ing on the current architecture of the Web (HTTP, URIs, Content Negotiation, XML, RDF, webservers and browsers) to publish data. Figure 1 below includes on the right-hand side a macroscopic view on the LOD cloud as of September 2011, where each node depicts a separate data set and the edges symbolize links between the data sets 6 . As of Septem-http://www.w3.org/DesignIssues/LinkedData.html
Linking Open Data cloud diagram, by Richard Cyganiak and Anja Jentzsch. http://lod-cloud.net/ ber 2011, the LOD cloud contains approximatively 300 data sets and more than 30 billion RDF triples 7 . In our context, each RDF triple can be seen as a simple sentence, connect-ing a subject (an entity URI), a property (a predicate URI) and an object (which can be either a URI or a literal). The following triple, for instance (example.net/Alice, foaf:age,  X  X 17 X  X ) encodes the fact that Alice (as identified by the URI example.net/Alice ) is 17 years old.

Since the same URI can be used as a subject or object in different triples, RDF naturally forms labelled graphs that connect entities to values and further entities. In the fol-lowing, we make a distinction between object properties X  linking entities to other entities X  X nd datatype properties X  associating values to a given entity 8 .

Large collections of triples are typically stored in a database system (such as Oracle 11g 9 , Virtuoso RDF-3X [19]). LOD data sets are typically referenced online (e.g., on CKAN 11 ) and can be queried or crawled over HTTP using the SPARQL 12 declarative query language.
We describe the architecture of our hybrid search system in the following. Our approach is based on an inverted index that supports full text search on one hand, and on a struc-tured repository, which maintains a graph representation of the original data, on the other hand. The ability to query an inverted index and to obtain a ranked list of results allows us to retrieve an initial set of entities that match the user query. Starting from this set of retrieved entities, we take advantage of algorithms exploiting the graph structure of the data thanks to our structured repository: retrieved enti-ties are nodes in the data graph that can be used as starting points for graph traversals and neighborhood queries. Thus, the structured repository is used to refine the original IR-based results by navigating the data-graph, selecting poten-tially new entities or reinforcing the relevance of the results selected through the inverted index.

Figure 1 illustrates the main components of our system and their interactions. The search process proceeds as fol-lows. First, a user initiates an entity search through a key-word query. The query is parsed and extended (see below Sections 5.2 and 5.3). A first set of results is retrieved us-ing the expanded query and an inverted index on the LOD data. Several mechanisms can be used at this stage to im-prove ranking effectiveness, such as a multi-field index with BM25F scoring, NLP, or pseudo-relevance techniques (see below Section 5 for a detailed discussion on that point.)
This first set of results is then enriched using structured queries on (a subset of) the LOD graph. We take advan-tage of two types or graph queries in that context: scope queries following datatype properties (see Section 3), and http://www4.wiwiss.fu-berlin.de/lodcloud/state/ We hence follow the vocabulary introduced by the Web Ontology Language OWL, see http://www.w3.org/TR/ owl-ref/ http://www.oracle.com/technetwork/database/ options/semantic-tech/index.html http://virtuoso.openlinksw.com/ http://thedatahub.org/group/lodcloud http://www.w3.org/TR/rdf-sparql-query/ graph traversal queries following object properties. Both types of queries start from the LOD nodes corresponding to the first set of results, and retrieve additional information (sets of literals and sets of entities respectively) to refine the results. As the LOD graph is highly connected (i.e., there are many properties that connect entities among themselves or that connect entities to literals), we must at this stage identify the most effective properties to follow in order to limit the scope of the graph queries. The details of our graph approach are given in Section 6.1. At this point, new entities may be added to the result set and previously iden-tified entities may change position in the rankings due to the new information gathered. Finally, the top-k results are sent back to the querier.
The first approach to index graph-structured datasets such as those found in the LOD cloud is to aggregate all information attached to the entities. Thus, we create entity profiles by building an inverted index on the entity labels. We consider each entity profile as a document (similarly to candidate-centric expert finding approaches [1]) containing all versions of the entity name directly attached to it in the graph. This index structure considering entity profiles as bag-of-words combined with a BM25 ranking function will be our baseline approach. We describe below three further approaches we propose to improve this baseline. These are based on a structured inverted index, on NLP techniques (query expansion), and on pseudo-relevance feedback.
First, we consider a structured inverted index approach, which separately indexes different values attached to the en-tity. In order to create a structured inverted index, we index separately different types of information attached to the en-tity. This is similar to previous work [6]. Specifically, we create a structured inverted index for the three following pieces of information: URI First we tokenize the URI identifying the entity in the Labels We also consider a list of manually selected Attributes Finally, we consider all the other datatype Once the indices are created, we obtain a ranked list of re-sults by means of various ranking functions such as BM25F.
Natural Language Processing techniques have been ap-plied to other entity search tasks. In this paper, we exploit query expansion and relevance feedback techniques on top of the BM25 baseline for the AOR task. structured graph queries.

Specifically, given a disjunctive keyword query q we extend it by adding additional related terms such as synonyms, hy-pernyms, and hyponyms from Wordnet [15]. This makes the query (e.g.,  X  X ew York X ) retrieve additional results includ-ing different ways of referring to the specified entity (e.g.,  X  X ig Apple X ).

In addition, we exploit commercial search engines query autocompletion features to obtain additional keywords, given the original user query. Specifically, we send the original query q to a commercial search engine (Google) and we expand it by appending the first 5 terms the autocompletion algorithm suggests for q (e.g.,  X  X ew York Times X ).
Finally, we also implement pseudo-relevance feedback techniques on top of the baseline approach by first running the original query q and then considering the labels of the top-3 retrieved entities to expand the user query. For instance, the query  X  X =NY X  can be expanded to  X  X ew York X ,  X  X ew York City X  and  X  X orth Yorkshire X  (labels of the top-3 entities retrieved for the original query q ).
We report evaluation results comparing those various techniques on standard AOR collections in Section 7.
All the techniques described above neglect two key char-acteristics of LOD data: i) LOD entities are interlinked , in the sense that related entities are often connected, and ii) data values are clustered in the graph, such that many relevant values can be retrieved by following series of links iteratively from a given entity. In this section, we describe how we can leverage structured graph queries to improve the performance of our system taking advantage of those two characteristics.

As previously mentioned, our hybrid ranking solution con-sists of two steps. The first step (described in the preced-ing section) uses the IR infrastructure to obtain an initial ranking of entity identifiers. In the second step, the graph structure is exploited. We use the first set of entities from the original ranking as seeds in the graph. From those ini-tial points, we traverse the graph following promising edges, that is, we follow object property edges potentially leading to additional results, and datatype properties leading to ad-ditional values.

Let Retr = { e 1 ,e 2 ,..,e n } be the ranked list of entities resulting from the query to the inverted index. Our goal is to define a set of functions that use the top-N elements of Retr to create an improved list of results StructRetr = { e 1 ,e 0 2 ,..,e 0 m } . This new list of results may contain entities from Retr as well as new entities discovered when traversing the LOD graph. To obtain StructRetr , we exploit top LOD properties that are likely to lead to relevant results 13
Once StructRetr is obtained, we create a final result set and return it to the user. We adopt a simple model that linearly combines the two entity rankings: finalScore ( e 0 ) =  X   X  BM 25( q,e ) + (1  X   X  )( score ( q,e,e where q is the user query, score ( q,e,e 0 ) is our new ranking function (see below Section 6.2), and BM 25( q,e ) is the ini-tial score of the entity e  X  Retr (or of the entity in Retr that lead to e 0  X  StructRetr by following RDF properties).
We describe below how we take advantage of SPARQL queries to retrieve additional results by following object property links in the LOD graph.

Our simplest graph traversal approach looks for candidate entities directly attached to entities that have already been identified as relevant (i.e., entities in Retr ). First, we need to ensure that only meaningful edges are followed. The object property &lt;dbpedia:wikilink&gt; , for instance, represents links between entities in Wikipedia. As an example, the entity  X  X bpedia:Stevie Wonder X  has a &lt;dbpedia:wikilink&gt; pointing to  X  X bpedia:Jimi Hendrix X . Since the AOR task aims at finding all the identifiers of one specific entity, this kind of links is not very promising.
It is possible to obtain such a list of properties by means of a training test collection. In this paper, we perform cross validation across the two different AOR test collections in order to identify the most promising properties. On the other hand, the &lt;owl:sameAs&gt; object property is used to connect identifiers that refer to the same real world entity. For instance, the entity  X  &lt; dbpedia:Barack Obama &gt;  X  links via &lt;owl:sameAs&gt; to the corresponding Freebase entity  X  &lt; fbase:Barack Obama &gt;  X . This object property is hence most probably valuable for improving the effectiveness of AOR in a LOD context.

To obtain a list of object properties worth following, we rank all properties in the dataset by their observed likelihood of leading to a relevant results 14 . Table 1 gives the top object property scores for the SemSearch collections. Using such a ranked list of properties as a reference, we define a series of result extension mechanisms exploiting structured queries over the data graph to identify new results. At this stage, we focus on recall rather than precision in order to preserve all candidate entities. Then, we rank candidate entities by means of a scoring function.

The first and simplest approach (SAMEAS) we consider is to follow exclusively the &lt;owl:sameAs&gt; links. Specifically, for each entity &lt; e &gt; in the top-N retrieved results, we issue the following query: SELECT ?x WHERE { { &lt;e&gt; &lt;owl:sameAs&gt; ?x } which returns all entities that are directly linked to &lt; e &gt; via a &lt;owl:sameAs&gt; object property.

Our second approach is based on the property scores de-fined above and exploits information from DBPedia. As DB-Pedia originates from Wikipedia, it contains disambiguation and redirect links to other entities. For example,  X  X bpe-dia:Jaguar X  links via &lt;dbpedia:disambiguates&gt; to  X  X bpe-dia:Jaguar Cars X  as well as to  X  X bpedia:Jaguar (computer) X  and others. Our second approach (S1 1) also follows such links to reach additional relevant entities. Thus, we define the following structured query: SELECT ?x WHERE { { &lt;e&gt; &lt;owl:sameAs&gt; ?x } UNION which extends SAMEAS queries with two additional triple patterns.

Our third scope one approach (S1 2) includes properties that are more specific to the user queries. In addition to the generic properties of the previous two approaches, we add to the list of links to follow object properties like &lt;dbpedia:artist&gt; , &lt;skos:subject&gt; , &lt;dbpedia:title&gt; , and &lt;foaf:homepage&gt; , that appear in Table 1 and which lead to more general (albeit still related) entities. The final scope one approach we propose (S1 3) extends S1 2 by adding matching patterns using the &lt;skos:broader&gt; property that links to more general entities. In addition to those four approaches, we additionally evaluated more complex query patterns based on the property scores defined
We compute the property scores by counting the number of times a property represents a path from a retrieved to a relevant entity (or the other way around) and dividing the result by the total number of such paths. above but did not obtain significant improvements over our simpler approaches.

An obvious extension of the above approach is two look for related entities further in the graph by following object property links iteratively. In the following, we describe a few scope two approaches following pairs of links.

The number of potential paths to follow is increasing expo-nentially with the scope of the queries. To reduce the search space, we rank object property pairs by their likelihoods of leading to relevant entities 15 .

The first strategy (S2 1) to retrieve potentially interest-ing entities at scope two is based on structured join queries containing top-two property pairs. The query issued to the structured repository looks as follows: SELECT ?y WHERE { { &lt;e&gt; &lt;dbpedia:wikilink&gt; ?x .
The second approach (S2 2) uses a selection of the top-two property pairs considering the starting entity e both as a subject as well as an object of the join queries. Finally, the third scope two approach (S2 3) applies a join query with the most frequent property pairs that do not include a &lt;dbpedia:wikilink&gt; property. The assumption is that due to the high frequency of this type of links 16 too many entities are retrieved, which produces a noisy result set. On the other hand, by focusing on non-frequent properties we aim at reaching few high-quality entities.

We note that it would be straightforward to generalize our graph traversal approach by considering scopes greater than two, or by considering transitive closures of object proper-ties. Such approaches impose however a higher overhead and only marginally improve on scope one and scope two techniques from our experience.
Once a new set of entities ( StructRetr ) has been reached, there is the need to 1) rank them by means of a scoring function and 2) merge the original ranking Retr with StructRetr . Given an entity e 0  X  StructRetr and the entity e  X  Retr from which e 0 originated, we compute the e ranking score by defining a function score ( q,e,e 0 ) that is used to rank all entities in StructRetr .

Our scoring function exploits a text similarity metric ap-plied to the query and the literals directly or indirectly at-tached to the entity by means of datatype properties. As noted above, related literals are implicitly clustered around entities in the LOD cloud. While many literals are directly attached to their entities (e.g., age , label ), some are at-tached indirectly, either through RDF blank nodes (e.g., name-&gt; firstname , address-&gt;zip code ), or are attached to related entities.
Due to space limitations, we do not report here the list of such property pairs which was computed in the same way as for scope one queries.
As described at http://vmlion25.deri.ie/ this is the most frequent property in the test collection. retrieved and relevant entities.

We use neighborhood queries to retrieve all literals at-tached to a given entity through datatype properties, either at scope one or at scope two, e.g., SELECT ?S WHERE { { &lt;e&gt; &lt;datatypeproperty&gt; ?S } UNION To minimize the overhead of such queries, we only retrieve the most promising literals. Table 2 lists the datatype prop-erties whose values are most similar to the user queries. We adopt here the Jaro-Winkler (JW) similarity metric, which fits the problem of matching entity names well [9].
Additionally, we adopt a modified version of this scoring function by applying a threshold  X  on the value of JW ( e to filter entities that do not match well. Thus, only entities e for which JW ( e 0 ,q ) &gt;  X  are included in the result set. Also, we check the number of outgoing links of each entity in StructRetr and only keep those entities having at least one outgoing link. The assumption (which was also made by the creators of the AOR evaluation collections) is that entities with no literals attached and that are not the subject of any statement are not relevant.

The final results are then constructed by linearly combin-ing the Jaro-Winkler scores with the original entity score as noted above. The following section gives more information about this combination and compares the performance of several ranking methods.
We present below the results of a performance evaluation of our hybrid approaches for AOR. Our aim was to evaluate the overall effectiveness and efficiency of our approach as well as the impact of the various parameters involved. More specifically, we investigated the impact of the following pa-rameters:
In addition, we report results for different IR methods using BM25 scoring, structured IR indices using BM25F scoring, and graph traversals using scope one and two ap-proaches over the structured repository combined with the IR baseline. We conclude this section with a few efficiency remarks on the overhead generated by our two-phase hybrid approach.
In order to evaluate and compare the proposed approaches, we adopt standard collections for the AOR task. We use the testsets created in the context of the SemSearch challenge for the 2010 and 2011 editions 17 . This allows us to compare the different variants we proposed with previous work that has been evaluated on the same collections.
 The underlying dataset used in the testset is the Billion Triple Challenge 2009 dataset which consists of 1.3 billions RDF triples crawled from different domains of the LOD cloud. The two test collections contain 50 and 92 queries respectively, together with relevance judgements on a 3-level scale obtained by crowdsourcing the assessment task. We adopt the official evaluation metrics from the Sem-Search initiative and from previous work: Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and early Precision (P10). Statistical significance is measured against the BM25 baseline by means of a two-tailed paired t-test by considering a difference significant when p &lt; 0 . 05.
The AOR evaluation collections created in the context of the SemSearch initiative used crowdsourcing techniques to create relevance judgements for the entities by asking anony-http://km.aifb.kit.edu/ws/semsearch10/ for 2010 and http://km.aifb.kit.edu/ws/semsearch11/ for 2011. collection.
 mous Web users to judge entity relevance for a small eco-nomic reward. Such a novel approach to relevance judge-ment triggers obvious questions about the reliability of the results. In [5], Blanco et al. experimentally show how such an approach is reliable and, most importantly, repeatable.
Traditional effectiveness evaluation methods (e.g., based on the Cranfield paradigm) have been largely used in TREC initiatives. Document collections, queries, and relevance judgements form test collections, which are typically made available to foster repeatable and comparable experiments. Because of the increasing size of the corpora and the impossibility of manually judging all documents, a pooling methodology is typically used: top retrieved results from each run submitted to the evaluation initiative are judged; non-judged results are assumed to be not relevant. Such an approach has been shown to be fair for comparing participants but unfair with respect to any following systems evaluated on the same collection  X  X ecause the non-contributors will have highly ranked unjudged documents that are assumed to be not relevant X  [22]. Zobel [25] showed that TREC collections can still be used to provide an unbiased comparison. To overcome the limitations of pooling, novel measures taking judgement incompleteness into account (e.g., bpref [8]) or adopting sampling to delve deeper into the ranked list (e.g., infAP [24]) have been proposed. Such refinements are also needed because the coverage of the judgement pools gets smaller over time as the size of the collection grows (e.g., 300,000 documents with a pool depth of 100 in TREC-5 and 1 billion documents with a pool depth of 20 in TREC 2011).
The recent trend of crowdsourcing relevance judgments enables an alternative approach. As shown in [5], the re-sults obtained by crowdsourcing relevance judgements for the same collection at different points in time empirically demonstrates the repeatability of such experiments. Based on this important result and on the vision of  X  X valuation campaigns that run continuously  X  [5], we propose an ap-proach to fair comparison of different IR systems running on the same evaluation collection.

Assuming systems A, B, and C participated in an evalua-tion initiative and, therefore, contributed to the assessment pool with their top-10 results, then each result of A in the top-10 will be judged while results beyond rank 10 may be judged or not (they will only be judge if retrieved by B or C in their top-10). On the other hand, a new system D appear-ing after the evaluation initiative may retrieve in its top-10 results many unjudged results that are typically considered as not relevant. This strongly penalizes runs which retrieve results that are very different from the original results that were part of the evaluation initiative. Instead, new systems should exploit crowdsourcing to obtain the missing judge-ments by running the same micro-tasks that provided the original (crowdsourced) relevance judgements.

One key advantage of this approach is that subsequent runs created after the initial evaluation campaign can be judged on a fair basis as if they were part of the original results. One drawback however is that the number of rele-vant results changes (i.e., increases) in this way, thus making measures that take this into account (e.g., MAP) incompa-rable with the ones computed originally by the evaluation initiative. Anyhow, the availability of the original retrieved results (i.e., the submitted runs which are available for all tasks at TREC) allows the authors of later approaches to re-compute the measures and compare them against previous approaches.

As the approach proposed in this paper substantially dif-fers from the approaches run during the evaluation cam-paign, both in terms of system architecture and retrieved results (see Table 3), we decided to adopt this new ap-proach to obtain fair relevance judgements for the top-10 results of our various approaches. Thus, in the remainder of this paper, we report evaluation measures computed over relevance judgements complemented with additional crowd-sourced judgements.
In order to obtain additional relevance judgements for un-judged entities in the top-10 results of our runs, we pub-lished micro-tasks (HITs) on Amazon MTurk. We followed the same task design and setting as the ones used to cre-ate the test collections for the AOR task at SemSearch (as described in [5, 16]). We asked the crowd to judge a total of 1583 additional query-entity pairs for the 2010 collection and 1183 for the 2011 collection.

We split the tasks in batches of 10 entities per HIT, which were rewarded 0.20$ each and assigned to 3 different work-ers. The overall number of relevant entities increased by +8% (passing from 2028 to 2193) for the 2010 collection and by 17% (from 470 to 551) for the 2011 collection 18 .
As a consequence, Precision@10 for our approach is com-parable to the original SemSearch submissions as it is com-
The extended assessment files we created together with the script to generate them from MTurk output are available at http://diuf.unifr.ch/xi/HybridAOR Table 3: New retrieved results that were not re-trieved by the BM25 baseline for top-10 results of IR and graph-based approaches in both collections.
 puted on fully judged results. On the other hand, abso-lute values of MAP will be lower when computed with the extended relevance judgements as the number of relevant results has increased. In any case, the baseline we adopted closely matches the approaches used so far for the AOR task evaluated in SemSearch. Moreover, we compare below our proposed hybrid solution against a plethora of standard IR approaches like query extension and pseudo-relevance feed-back.
Table 4 gives the results obtained by using BM25 scor-ing 19 on the inverted index built on all the literals attached to the entity (entity profile). We compare this baseline ap-proach against standard IR techniques such as query exten-sion using related words (Extension), query autocompletion as provided by commercial Web search engines (Query Au-toc.), and Pseudo Relevance Feedback using top-3 retrieved entities (PRF3). We experimented with various approaches to handle the terms in the query and in the end opted for a disjunctive approach (i.e., we take each term in the query separately and  X  X R X  the results) since it performed best. The only exception is for the approaches based on a structured index and BM25F scoring for which we adopt a conjunctive approach. As we can see, the BM25 baseline performs best on both test collections. This indicates that methods work-ing well for other entity search tasks may not be directly ap-plicable to the AOR task due to the specific semantics of the user query, meant to uniquely describe one specific entity. Anyhow, we observe that the query autocompletion method obtains an Average Precision of 1 . 0 for a query (q16 of 2011), for which all other methods performed poorly (0 . 02). This originates from a misspelling in the original query, which was corrected by the external autocompletion functionality.
Table 5 gives results for the structured inverted index ap-
The parameter b in BM25 has been selected by cross-validation on the 2010 and 2011 testsets. Figure 2: Effectiveness values varying the number of top-N entities retrieved by IR approaches. proach (see Section 5.1). The table lists results for indices built on entity profiles constructed using different types of literals. When compared to the baseline index (which ag-gregates all literals directly attached to the entities in a sin-gle document ), we observe that structured indices perform better. Specifically, when a conjunctive query and BM25F ranking is used over a structured inverted index, effective-ness increases. The best results are obtained by the index encompassing all three fields, that is, URI, Label, and At-tributes (ULA).
Figure 2 shows how effectiveness (MAP) varies when vary-ing the parameter N indicating the number of entities con-sidered as input for the structured queries, both for the 2010 and 2011 collections. We observe that the best number of entities to consider lies between 3 and 4 in most cases, there-fore we fix this parameter to 3 for the following experiments.
Moreover, we observe that high values (i.e., 0 . 8  X  0 . 9) for the threshold  X  used to discard entities lead to higher MAP in all cases. The optimal value for  X  varies for scope one and two approaches. Interestingly, for the best performing method S1 1, the optimal value for  X  is 0 . 5 for both the 2010 and 2011 collections: for such an approach, our hybrid system reaches an optimal tradeoff between IR and graph data management techniques.

Table 6 gives the performance of different functions used to compute score ( q,e,e 0 ). As we can see, the most effective method is the one that exploits the original BM25 score of e to score e 0 . This is the scoring function we use to compute the final results of the hybrid system as reported in Table 8. Table 6: MAP values for S1 1 and S2 3 obtained by means of different instantiation of score ( q,e,e 0 ) using  X  = 0 . 5 on the 2010 collection. Table 7: Average query execution time (ms) on the 2011 dataset.

Table 8 gives results for the graph-based extension of the baseline ranking. We observe that most approaches based on scope one queries significantly improve over the baseline BM25 ranking. The simple S1 1 approach, which exploits &lt;owl:sameAs&gt; links plus Wikipedia redirect and disambigua-tion information, performs best obtaining a 25% improve-ment of MAP over the baseline on the 2010 dataset. Figure 3 shows Precision/Recall curves for the baseline BM25 rank-ing and for graph-based approaches. Again, we can see how S1 1 outperforms other approaches. The S1 2 approach per-forms best in terms of MAP on the 2011 dataset, although not significantly better than the baseline.
Table 7 shows execution times for the different components of our system and for various approaches Quite naturally, the IR baseline is faster than more complex approaches. Interestingly, we observe that structured approaches based on scope one queries perform very well, only adding a very limited overhead to the inverted index approach. The S1 1 approach, which is the best in terms of effectiveness, adds a cost of only 17% in terms of execution
We did not put any emphasis on improving the efficiency of our system. Experiments were run on a single machine with a cold cache and disk-resident indices for both the inverted indices and the structured repository. Figure 3: Precision/Recall curves for graph-based approaches on the 2010 collection. time to the BM25 baseline. The approaches based on scope two queries that use &lt;dbpedia:wikilink&gt; are more costly and would require a specific effort to make them scalable over large datasets. The scope two approach S2 6, which uses less frequent object properties, has still a reasonable overhead (37%).

In terms of storage consumption, the original collection containing 1.3 billions statements occupies 253GB. The baseline inverted index created with MG4J 21 is 8.9GB, the structured index used with BM25F scoring is 5GB, while the graph index created with RDF-3X 22 is 87GB. We note that the graph index could easily be optimized by discarding all the properties that are not used by the graph traversals and neighborhood queries (we could save considerable space this way, from 50% to 95% approximatively depending on the graph approaches used).
As more datasets become available on the Web, novel ap-plications can exploit semi-structured data to build entity-centric functionalities. In this paper, we present a hybrid system for effectively and efficiently solving Ad-hoc Object Retrieval tasks. Our approach is based on the combination of results from an inverted index storing entity profiles and from a structured database storing graph data. http://mg4j.dsi.unimi.it/ http://code.google.com/p/rdf3x/ statistically significant difference against the BM25 baseline.
Our extensive experimental evaluation over two different evaluation collections shows that the use of structured search on top of standard IR approaches can lead to significantly better results (up to 25% improvement over the BM25 base-line in terms of Mean Average Precision). This comes at the cost of incorporating additional components in the system architecture and of implementing additional merging and ranking functions in the processing pipeline. In any case, our measurements show that the overhead caused by the graph data management components is surprisingly limited and only represents 17% for the most effective approach.
We consider our initial hybrid results as very promising, especially given that the LOD sample data sets used in the test collections were extremely noisy and incomplete (auto-mated data cleaning and entity linking are two prominent research topics in the LOD community, which should hope-fully alleviates those issues on the medium term). As future work, we plan to focus on further user queries (e.g, Entity List Completion queries and queries specifying the category of the entity sought) and on improving the efficiency of our proposed hybrid system in distributed and cloud settings.
We would like to thank the anonymous reviewers for their helpful comments. This work was supported by the Swiss National Science Foundation under grant number PP00P2 128459, and by the Haslerstiftung in the context of the Smart World 11005 (Mem0r1es) project.
