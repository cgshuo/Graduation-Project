 Recently we presented a new approach [18] to the classifi-cation problem arising in data mining. It is based on the regularization network approach but, in contrast to other methods which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [49]. Thus, only involved. Here d denotes the dimension of the feature space and hn = 2 -'~ gives the mesh size. We use the sparse grid combination technique [28] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial dis-cretization. This allows to handle more dimensions and the algorithm needs less operations per data point. 
We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods. data mining, classification, approximation, sparse grids, com-bination technique, simplicial discretization requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 
Data mining is the process of finding patterns, relations and trends in large data sets. Examples range from scien-tific applications like the post-processing of data in medicine or the evaluation of satellite pictures to financial and com-mercial applications, e.g. the assessment of credit risks or the selection of customers for advertising campaign letters. For an overview on data mining and its various tasks and approaches see [5, 12]. 
In this paper we consider the classification problem aris-ing in data mining. Given is a set of data points in a d-dimensional feature space together with a class label. Prom this data, a classifier must be constructed which allows to predict the class of any newly given data point for future de-cision making. Widely used approaches are, besides others, decision tree induction, rule learning, adaptive multivari-ate regression splines, neural networks, and support vector machines. Interestingly, some of these techniques can be in-terpreted in the framework of regularization networks [21]. This approach allows a direct description of the most im-portant neural networks and it also allows for an equivalent description of support vector machines and n-term approx-imation schemes [20]. Here, the classification of data is in-terpreted as a scattered data approximation problem with certain additional regularization terms in high-dimensional spaces. 
In [18] we presented a new approach to the classification problem. It is also based on the regularization network ap-proach but, in contrast to the other methods which employ mostly global ansatz functions associated to data points, we use an independent grid with associated local ansatz func-tions in the minimization process. This is similar to the numerical treatment of partial differential equations. Here, a uniform grid would result in O(h~ d) grid points, where d denotes the dimension of the feature space and hn = 2 -n gives the mesh size. Therefore the complexity of the problem would grow exponentially with d and we encounter the curse of dimensionality. This is probably the reason why conven-tional grid-based techniques are not used in data mining up to now. 
However, there is the so-called sparse grids approach which allows to cope with the complexity of the problem to some extent. This method has been originally developed for the solution of partial differential equations [2, 8, 28, 49] and is now used successfully also for integral equations [14, 27], interpolation and approximation [3, 26, 39, 42], eigenvalue problems [16] and integration problems [19]. In the informa-tion based complexity community it is also known as 'hyper-bolic cross points' and the idea can even be traced back to [41]. For a d-dimensional problem, the sparse grid approach employs only O(h~l(log(h~l)) d-l) grid points in the dis-cretization. The accuracy of the approximation however is nearly as good as for the conventional full grid methods, pro-vided that certain additional smoothness requirements are fulfilled. Thus a sparse grid discretization method can be employed also for higher-dimensional problems. The curse of the dimensionality of conventional 'full' grid methods af-fects sparse grids much less. 
In this paper, we apply the sparse grid combination tech-nique [28] to the classification problem. For that the reg-ularization network problem is discretized and solved on a certain sequence of conventional grids with uniform mesh sizes in each coordinate direction. In contrast to [18], where d-linear functions stemming from a tensor-product approach were used, we now apply linear basis functions based on a simplicial discretization. In comparison, this approach al-lows the processing of more dimensions and needs less op-erations per data point. The sparse grid solution is then obtained from the solutions on the different grids by lin-ear combination. Thus the classifier is build on sparse grid points and not on data points. A discussion of the com-plexity of the method gives that the method scales linearly with the number of instances, i.e. the amount of data to be classified. Therefore, our method is well suited for realistic data mining applications where the dimension of the fea-ture space is moderately high (e.g. after some preprocessing steps) but the amount of data is very large. Furthermore the quality of the classifier build by our new method seems to be very good. Here we consider standard test problems from the UCI repository and problems with huge synthetical data sets in up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to those of the best existing methods. Note that the combi-nation method is simple to use and can be parallelized in a natural and straightforward way. The remainder of this paper is organized as follows: In Section 2 we describe the classification problem in the frame-work of regularization networks as minimization of a (qua-dratic) functional. We then discretize the feature space and derive the associated linear problem. Here we focus on grid-based discretization techniques. Then, we introduce the sparse grid combination technique for the classification prob-lem and discuss its properties. Furthermore, we present a new variant based on a discretization by simplices and dis-cuss complexity aspects. Section 3 presents the results of numerical experiments conducted with the sparse grid com-bination method, demonstrates the quality of the classifier build by our new method and compares the results with the ones from [18] and with the ones obtained with different forms of SVMs [33]. Some final remarks conclude the paper. 2. THE PROBLEM 
Classification of data can be interpreted as traditional scattered data approximation problem with certain addi-tional regularization terms. In contrast to conventional scat-tered data approximation applications, we now encounter quite high-dimensional spaces. To this end, the approach of regularization networks [21] gives a good framework. This approach allows a direct description of the most important neural networks and it also allows for an equivalent descrip-tion of support vector machines and n-term approximation schemes [20]. 
Consider the given set of already classified data (the train-ing set) Assume now that these data have been obtained by sampling of an unknown function f which belongs to some function space V defined over ~a. The sampling process was dis-turbed by noise. The aim is now to recover the function f from the given data as good as possible. This is clearly an ill-posed problem since there are infinitely many solutions possible. To get a well-posed, uniquely solvable problem we have to assume further knowledge on f. To this end, reg-ularization theory [43, 47] imposes an additional smooth-ness constraint on the solution of the approximation prob-lem and the regularization network approach considers the variational problem with Here, C(., .) denotes an error cost function which measures the interpolation error and if(f) is a smoothness functional which must be well defined for f E V. The first term en-forces closeness of f to the data, the second term enforces smoothness of f and the regularization parameter A balances these two terms. Typical examples are and with x7 denoting the gradient and A the Laplace operator. The value of A can be chosen according to cross-validation techniques [13, 22, 37, 44] or to some other principle, such as structural risk minimization [45]. Note that we find exactly approximation methods, see [1, 31], where the regularization term is usually physically motivated. 2.1 Diseretization 
We now restrict the problem to a finite dimensional sub-space VN E V. The function f is then replaced by Here the ansatz functions {~j }~Y= 1 should span VN and prefer-ably should form a basis for VN. The coefficients {~}~Y=I denote the degrees of freedom. Note that the restriction to a suitably chosen finite-dimensional subspace involves some additional regularization (regularization by discretization) which depends on the choice of VN. 
In the remainder of this paper, we restrict ourselves to the choice and for some given linear operator P. This way we obtain from the minimization problem a feasible linear system. We thus have to minimize with fN in the finite dimensional space VN. We plug (2) into (4) and obtain after differentiation with respect to c~, k=l,...,N O= Oak = -'M ~=~ \3=1 This is equivalent to (k = 1,..., N) In matrix notation we end up with the linear system Here C is a square N x N matrix with entries Cj,k = M  X  (P~/, P~)L=, j, k = 1,..., N, and B is a rectangular N x M matrix with entries Bj,i = %oj(xi),i ---1,...,M,j = l,..., N. The vector y contains the data labels yl and has length M. The unknown vector c~ contains the degrees of freedom c~ i and has length N. 
Depending on the regularization operator we obtain dif-ferent minimization problems in VN. For example if we use the gradient ~(fN) = [[VfN[[~2 in the regularization ex-pression in (1) we obtain a Poisson problem with an addi-tional term which resembles the interpolation problem. The natural boundary conditions for such a partial differential equation are Neumann conditions. The discretization (2) gives us then the linear system (7) where C corresponds to a discrete Laplacian. To obtain the classifier fg we now have to solve this system. 
Up to now we have not yet been specific what finite-dimensional subspace VN and what type of basis functions {~j}~=i we want to use. In contrast to conventional data mining approaches which work with ansatz functions associ-ated to data points we now use a certain grid in the attribute space to determine the classifier with the help of these grid points. This is similar to the numerical treatment of partial differential equations. 
For reasons of simplicity, here and in the the remainder of This situation can always be reached by a proper rescaling of the data space. A conventional finite element discretiza-tion would now employ an equidistant grid ~ with mesh size h= = 2 -= for each coordinate direction, where n is the refinement level. In the following we always use the gradient P = V in the regularization expression (3). Let j denote the multi-index (jl ..... jd) E N a. A finite element method with piecewise d-linear, i.e. linear in each dimension, test-and trial-functions  X ,,3(5) on grid ~n now would give and the variational procedure (4) -(6) would result in the discrete linear system of size (2" + l) d and matrix entries corresponding to (7). Note that f,~ lives in the space The discrete problem (8) might in principle be treated by an appropriate solver like the conjugate gradient method, a multigrid method or some other suitable efficient iterative method. However, this direct application of a finite element discretization and the solution of the resulting linear sys-tem by an appropriate solver is clearly not possible for a d-dimensional problem if d is larger than four. The num-ber of grid points is of the order O(h~ d) = 0(2 rid) and, in the best case, the number of operations is of the same order. Here we encounter the so-called curse of dimensionality: The complexity of the problem grows exponentially with d. At tem can not be stored and solved on even the largest parallel computers today. 
Therefore we proceed as follows: We discretize and solve the problem on a certain sequence of grids ~1 = ~h,...Jd with uniform mesh sizes ht = 2 -it in the t-th coordinate direction. These grids may possess different mesh sizes for different coordinate directions. To this end, we consider all grids ~l with For the two-dimensional case, the grids needed in the com-bination formula of level 4 are shown in Figure 1. The fi-nite element approach with piecewise d-linear test-and trial-functions on grid ~l now would give and the variational procedure (4) -(6) would result in the discrete system with the matrices (Ci)j,k = M. (V X ~,j, V X l,k) and (Bl)j,i = ~bi.j(xi), jt,kt = 0,...,2~t,t = 1,...,d,i = 1,...,M, and the unknown vector (al)j, jt = 0, ...,2~t,t = 1, ...,d. We then solve these ................. @ ......... $: : : : :@ Figure 1: Combination technique with level n = 4 in two dimensions problems by a feasible method. To this end we use here a diagonally preconditioned conjugate gradient algorithm. But also an appropriate multigrid method with partial semi-coarsening can be applied. The discrete solutions fl are contained in the spaces of piecewise d-linear functions on grid f~l. 
Note that all these problems are substantially reduced in size in comparison to (8). Instead of one problem with size dim(V,~) = O(h~ d) = O(2nd), we now have to deal with O(dn 'i-1) problems of size dim(Vl) = O(h~ 1) = O(T~). Moreover, all these problems can be solved independently, which allows for a straightforward parallelization on a coarse load balancing strategy available [25]. 
Finally we linearly combine the results fl(x) E P~, fl = ~j al,j X l,j(x), from the different grids ~l as follows: The resulting function f(~) lives in the sparse grid space This space has dim(V~ 8)) = O(h~l(log(h~l))d-1). It is spanned by a piecewise d-linear hierarchical tensor product basis, see [8]. 
Note that the summation of the discrete functions from different spaces P] in (13) involves d-linear interpolation which resembles just the transformation to a representation in the hierarchical basis. For details see [24, 28, 29]. How-ever we never explicitly assemble the function f(c) but keep instead the solutions fl on the different grids ~l which arise in the combination formula. Now, any linear operation F on f(~ X ) can easily be expressed by means of the combination Figure 2: Two-dimensional sparse grid (left) and three-dimensional sparse grid (right), n = 5 formula (13) acting directly on the functions fh i.e. 
F(f(~))=~--~(_l)q d 1 ~ F(f,). (14) 
Therefore, if we now want to evaluate a newly given set of data points {x~}~l (the test or evaluation set) by we just form the combination of the associated values for fl according to (13). The evaluation of the different fl in the test points can be done completely in parallel, their summa-tion needs basically an all-reduce/gather operation. 
For second order elliptic PDE model problems, it was proven that the combination solution f(c) is almost as accu-rate as the full grid solution fn, i.e. the discretization error satisfies provided that a slightly stronger smoothness requirement on f than for the full grid approach holds. We need the seminorm to be bounded. Furthermore, a series expansion of the error is necessary for the combination technique. Its existence was shown for PDE model problems in [10]. 
The combination technique is only one of the various meth-ods to solve problems on sparse grids. Note that there exist also finite difference [24, 38] and Galerkin finite element ap-proaches [2, 8, 9] which work directly in the hierarchical product basis on the sparse grid. But the combination tech-nique is conceptually much simpler and easier to implement. Moreover it allows to reuse standard solvers for its different subproblems and is straightforwardly parallelizable. 
So far we only mentioned d-linear basis functions based on a tensor-product approach, this case was presented in detail in [18]. But on the grids of the combination technique linear basis functions based on a simplicial discretization are also possible. For that we use the so-called Kuhn's triangulation [15, 32] for each rectangular block, see Figure 3. Now, the summation of the discrete functions for the different spaces Vt in (13) only involves linear interpolation. storage O(3 a.N) O(3 a.g) 0(2 4.M) ~ O((2.d+l).g) 0(2 4.g) level )~ training correctness 5 0.0003 94.87 % 6 0.0006 97.42 % 7 0.00075 100.00 % 8 0.0006 100.00 % 9 0.0006 100.00 % Table 2: Leave-one-out cross-validation results for the spiral data set the reconstruction gets more precise. 
This data set, taken from [36], consists of 250 training data and 1000 test points. The data set was generated syn-thetically and is known to exhibit 8 % error. Thus no better testing correctness than 92 % can be expected. 
Since we now have training and testing data, we proceed as follows: First we use the training set to determine the best regularization parameter A per ten-fold cross-validation. The best test correctness rate and the corresponding ~ are given for different levels n in the first two columns of Table 3. With this A we then compute the sparse grid classifier from the 250 training data. The column three of Table 3 gives the result of this classifier on the (previously unknown) test data set. We see that our method works well. Already level 4 is sufficient to obtain results of 91.4 %. The reason is surely the relative simplicity of the data, see Figure 5. Just a few hyperplanes should be enough to separate the classes quite properly. We also see that there is not much need to use any higher levels, on the contrary there is even an overfitting effect visible in Figure 5. 
In column 4 we show the results from [18], there we achieve almost the same results with d-linear functions, 
To see what kind of results could be possible with a more sophisticated strategy for determing A we give in the last two columns of Table 3 the testing correctness which is achiew~d for the best possible A. To this end we compute for all (discrete) values of A the sparse grid classifiers from the 250 data points and evaluate them on the test set. We then pick the best result. We clearly see that there is not much of a difference. This indicates that the approach to determine the value of A from the training set by cross-validation works well. Again we have almost the same results with linear and d-linear basis functions. Note that a testing correctness of Figure 5: Ripley data set, combination technique with linear basis functions. Left: level 4, A : 0.0035. Right: level 8, A = 0.0037 90.6 % and 91.1% was achieved in [36] and [35], respectively, for this data set. The BUPA Liver Disorders data set from Irvine Machine Learning Database Repository [6] consists of 345 data points with 6 features and a selector field used to split the data into 2 sets with 145 instances and 200 instances respectively. Here we have no test data and therefore can only report our ten-fold cross-validation results. 
We compare with our d-linear results from [18] and with the two best results from [33], the therein introduced smooth-ed support vector machine (SSVM) and the classical support vector machine (SVMibll~) [11, 46]. The results are given in Table 4. 
As expected, our sparse grid combination approach with linear basis functions performs slightly worse than the d-linear approach. The best test result was 69.60% on level 4. The new variant of the sparse grid combination tech-nique performs only slightly worse than the SSVM, whereas the d-linear variant performs slighly better than the support vector machines. Note that the results for other SVM ap-proaches like the support vector machine using the 1-norm approach (SVMII.Ha) were reported to be somewhat worse in [33]. 
To measure the performance on a massive data set we produced with DatGen [34] a 6-dimensional test case with 5 million training points and 20 000 points for testing. We used the call datgen -rl -X0/100,R,O:0/100,R,O:0/100,R,O: O/IO0,R,O:O/2OO,R,O:O/2OO,R,O -R2 -C2/4 -D2/5 -T10/60 -05020000 -p -e0.15. 
The results are given in Table 5. Note that already on level 1 a testing correctness of over 90 % was achieved with just A = 0.01. The main observation on this test case concerns the execution time, measured on a Pentium III 700 MHz machine. Besides the total run time, we also give the CPU time which is needed for the computation of the matrices 
We see that with linear basis functions really huge data sets of 5 million points can be processed in reasonable time. Note that more than 50 % of the computation time is spent for the data matrix assembly only and, more importantly, that the execution time scales linearly with the number of data points. The latter is also the case for the d-linear func-tions, but, as mentioned, this approach needs more opera-tions per data point and results in a much longer execution time, compare also Table 5. Especially the assembly of the data matrix needs more than 96 % of the total run time for this variant. For our present example the linear basis ap-proach is about 40 times faster than the d-linear approach on the same refinement level, e.g. for level 2 we need 17 minutes in the linear ease and 11 hours in the d-linear case. For higher dimensions the factor will be even larger. The forest cover type dataset comes from the UCI KDD Archive [4], it was also used in [30], where an approach simi-lar to ours was followed. It consists of cartographic variables for 30 x 30 meter cells and a forest cover type is to be pre-dicted. The 12 originally measured attributes resulted in 54 attributes in the data set, besides 10 quantitative variables there are 4 binary wilderness areas and 40 binary soil type variables. We only use the 10 quantitative variables. The class label has 7 values, Spruce/Fir, Lodgepole Pine, Pon-derosa Pine, Cottonwood/Willow, Aspen, Douglas-fir and Krummholz. Like [30] we only report results for the classifi-cation of Ponderosa Pine, which has 35754 instances out of the total 581012. 
Since far less than 10 % of the instances belong to Pon-derosa Pine we weigh this class with a factor of 5, i.e. Pon-derosa Pine has a class value of 5, all others of -1 and the treshold value for separating the classes is 0. The data set was randomly separated into a training set, a test set, and a evaluation set, all similar in size. In [30] only results up to 6 dimensions could be reported. In Table 6 we present our results for the 6 dimensions chosen there, i.e. the dimensions 1,4,5,6,7, and 10, and for all 10 dimensions as well. To give an overview of the behavior over several A's we present for each level n the overall correctness results, the correctness results for Ponderosa Pine and the correctness result for the other class for three values of A. We then give results on the evaluation set for a chosen A. 
We see in Table 6 that already with level 1 we have a testing correctness of 93.95 % for the Ponderosa Pine in the 6 dimensional version. Higher refinement levels do not give better results. The result of 93.52% on the evaluation set is almost the same as the corresponding testing correctness. Note that in [30] a correctness rate of 86.97 % was achieved on the evaluation set. 
The usage of all 10 dimensions improves the results slightly, we get 93.81% as our evaluation result on level 1. As before higher refinement levels do not improve the results for this data set. 
Note that the forest cover example is sound enough as an example of classification, but it might strike forest scientists as being amusingly superficial. It has been known for 30 years that the dynamics of forest growth can have a dom-inant effect on which species is present at a given location [7], yet there are no dynamic variables in the classifier. This one can see as a warning that it should never be assumed that the available data contains all the relevant information. 
To measure the performance on a still higher dimensional massive data set we produced with DatGen [34] a 10-dimen-sional test case with 5 million training points and 50 000 points for testing. We used the call datgen -rl -X0/200,R,O: 00,R,O:0/200,R,O:0/200,R,O:0/200,R,O -R2 -C2/6 -D2/7 -T10/60 -05050000 -p -e0.15. 
Like in the synthetical 6-dimensional example the main observations concern the run time, measured on a Pentium III 700 MHz machine. Besides the total run time, we also give the CPU time which is needed for the computation of the matrices Gl = Bl -B~". Note that the highest amount of memory needed (for level 2 in the case of 5 million data points) was 500 MBytes, about 250 MBytes for the matrix and about 250 MBytes for keeping the data points in mem-ory. 
More than 50 % of the run time is spent for the assembly 93 tions of the data matrix and the time needed for the data matrix scales linearly with the number of data points, see Table 7. The total run time seems to scale even better than linear. 
We presented the sparse grid combination technique with linear basis functions based on simplices for the classification of data in moderate-dimensional spaces. Our new method gave good results for a wide range of problems. It is capable to handle huge data sets with 5 million points and more. The run time scales only linearly with the number of data. This is an important property for many practical applications where often the dimension of the problem can substantially be reduced by certain preprocessing steps but the number of data can be extremely huge. We believe that our sparse grid combination method possesses a great potential in such practical application problems. 
We demonstrated for the Ripley data set how the best value of the regularization parameter A can be determined. This is also of practical relevance. 
A parallel version of the sparse grid combination tech-nique reduces the run time significantly, see [17]. Note that our method is easily parallelizable already on a coarse grain level. A second level of parallelization is possible on each grid of the combination technique with the standard tech-niques known from the numerical treatment of partial dif-ferential equations. 
Since not necessarily all dimensions need the maximum re-finement level, a modification of the combination technique with regard to different refinement levels in each dimension along the lines of [19] seems to be promising. 
Note furthermore that our approach delivers a continuous classifier function which approximates the data. It therefore can be used without modification for regression problems as well. This is in contrast to many other methods like e.g. decision trees. Also more than two classes can be handled by using isolines with just different values. Finally, for reasons of simplicity, we used the operator P = V. But other differential (e.g. P = A) or pseudo-differential operators can be employed here with their associated regular finite element ansatz functions. 
Part of the work was supported by the German Bun-desministerium fiir Bildung und Forschung (BMB+F) within the project 03GRM6BN. This work was carried out in cooperation with Prudential Systems Software GmbH, Chemnitz. The authors thank one of the referees for his remarks on the forest cover data set. [1] E. Arge, M. Dmhlen, and A. Tveito. Approximation of [2] R. Balder. Adaptive Verfahren fiat elliptische und [3] G. Baszenski. N-th order polynomial spline blending. [4] S. D. Bay. The UCI KDD archive. [5] M. J. A. Berry and G. S. Linoff. Mastering Data [6] C. L. Blake and C. J. Merz. UCI repository of [7] D. Botkin, J. Janak, and J. Wallis. Some ecological [8] H.-J. Bungartz. Diinne Gitter und deren Anwendun 9 [9] H.-J. Bungartz, T. Dornseifer, and C. Zenger. Tensor [10] H.-J. Bungartz, M. Griebel, D. RSschke, and testing correctness [11] V. Cherkassky and F. Mulier. Learning from Data -[12] K. Cios, W. Pedrycz, and R. Swiniarski. Data Mining [13] T. G. Dietterich. Approximate statistical tests for [14] K. Frank, S. Heinrich, and S. Pereverzev. Information [15] H. Freudenthal. Simplizialzerlegungen yon [16] J. Garcke and M. Griebel. On the computation of the [17] J. Garcke and M. Griebel. On the parallelization of [18] J. Garcke, M. Griebel, and M. Thess. Data mining [19] T. Gerstner and M. Griebel. Numerical Integration [20] F. Girosi. An equivalence between sparse [21] F. Girosi, M. Jones, and T. Poggio. Regularization [22] G. Golub, M. Heath, and G. Wahba. Generalized cross [23] M. Griebel. The combination technique for the sparse [24] M. Griebel. Adaptive sparse grid multilevel methods [25] M. Griebel, W. Huber, T. St5rtkuhl, and C. Zenger. [26] M. Griebel and S. Knapek. Optimized tensor-product [27] M. Griebel, P. Oswald, and T. Schiekofer. Sparse grids [28] M. Griebel, M. Schneider, and C. Zenger. A [29] M. Griebel and V. Thurner. The efficient solution of [30] M. Hegland, O. M. Nielsen, and Z. Shen. High [31] J. Hoschek and D. Lasser. Grundlagen der [32] H. W. Kuhn. Some combinatorial lemmas in topology. [33] Y. J. Lee and O. L. Mangasarian. [34] G. Melli. Datgen: A program that creates structured [35] W. D. Penny and S. J. Roberts. Bayesian neural [36] B. D. Ripley. Neural networks and related methods for [37] S. L. Salzberg. On comparing classifiers: Pitfalls to [38] T. Schiekofer. Die Methode der Finiten Differenzen [39] W. Sickel and F. Sprengel. Interpolation on sparse [40] S. Singh. 2d spiral pattern recognition with [41] S. A. Smolyak. Quadrature and interpolation formulas [42] V. N. Temlyakov. Approximation of functions with [43] A. N. Tikhonov and V. A. Arsenin. Solutios of [44] F. Utreras. Cross-validation techniques for smoothing [45] V. N. Vapnik. Estimation of dependences based on [46] V. N. Vapnik. The Nature of Statistical Learning [47] G. Wahba. Spline models for observational data, [48] A. Wieland. Spiral data set. [49] C. Zenger. Sparse grids. In W. Hackbusch, editor, 
