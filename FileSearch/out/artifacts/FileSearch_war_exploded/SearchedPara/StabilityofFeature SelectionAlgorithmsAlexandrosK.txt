
With the prolifer ation of extr emely high-dimensional data, featur e selection algorithms have become indispens-able components of the learning process. Str ang ely , despite extensive work on the stability of learning algorithms, the stability of featur e selection algorithms has been relatively neglected. This study is an attempt to ll that gap by quanti-fying the sensitivity of featur e selection algorithms to vari-ations in the training set. We assess the stability of fea-tur e selection algorithms based on the stability of the fea-tur e prefer ences that the y expr ess in the form of weights-scor es, ranks, or a selected featur e subset. We examine a number of measur es to quantify the stability of featur e pref-erences and propose an empirical way to estimate them. We perform a series of experiments with sever al featur e selec-tion algorithms on a set of proteomics datasets. The ex-periments allow us to explor e the merits of eac h stability measur e and create stability proles of the featur e selection algorithms. Finally we show how stability proles can sup-port the choice of a featur e selection algorithm.
High dimensional datasets are becoming more and more abundant in classication problems. A variety of feature se-lection methods have been developed to tackle the issue of high dimensionality . The major challenge in these applica-tions is to extract a set of features, as small as possible, that accurately classies the learning examples.

A relati vely neglected issue in the work on high dimen-sional problems, and in general in problems requiring fea-ture selection, is the stability of the feature selection meth-ods used. Stability , dened as the sensiti vity of a method to variations in the training set, has been extensi vely studied with respect to the learning algorithm itself. We propose to investig ate how dif ferent subsamples of a training set affect a method' s assessment of a feature' s importance and conse-quently the nal set of selected features.

The stability of classication algorithms was examined by Turne y [11] who proposed a measure based on the agree-ment of classication models produced by an algorithm when trained on dif ferent training sets. He dened the agreement of two classication models as the probability that the y will produce the same predictions over all possible instances dra wn from a probability distrib ution P ( X ) that instances are dra wn from P ( X ) and not from P ( X;C ) the joint probability distrib ution of class and training in-stances; the underlying reason is that the agreement of two concepts X classication models X should be examined in all possible input worlds. In order to estimate stability he suggested using m 2 -fold cross-v alidation. In each of the m repetitions of cross-v alidation a classication model is produced from each one of the two folds. The two mod-els are then tested on articial instances dra wn by sampling from P ( X ) and their agreement is computed. The nal esti-mation of stability is the average agreement over all m runs.
Related to the notion of stability is the bias-v ariance de-composition of the error of classication algorithms, [4, 1]. The variance term quanties instability of the classication algorithm in terms of classication predictions. Variance measures the percentage of times that the predictions of dif-ferent classication models, learned from dif ferent training sets, for a given instance are dif ferent from the typical (av-erage) prediction. Bias-v ariance decomposition is usually done via bootstrapping, where part of the data is kept as a hold-out test set and the remainder is used to create dif ferent training sets by using sampling with replacement. The nal estimation of variance is also the average over the dif ferent bootstrap samples.

In both approaches described abo ve, the predictions of the classication models are crucial in quantifying the sen-siti vity of classication algorithms to changes in the train-ing sets (note that both approaches can also be used for er-ror estimation which is then tightly coupled with the sta-bility analysis). Ho we ver when one wants to examine only feature selection algorithms without involving a classica-tion algorithm, the abo ve methods do not apply . Typical feature selection algorithms do not construct classication models and thus cannot pro vide classication predictions. The y usually output what we call a feature preference state-ment (for conciseness, featur e prefer ence ); this can tak e the form of a subset of selected features, or alternati vely of a weighting-scoring or a ranking of the features, based on which a small set of features can be selected (either by specifying a threshold or asking for a specic number of features). A classication algorithm should then be applied on the selected feature set to produce a classication model. If we used the stability estimation methods described abo ve to the combined feature selection and classication algo-rithms, we would be measuring their joint sensiti vity to training set variations and we would have no way to delimit the (in)stability the feature selection algorithm from that of the classication algorithm.

To address this dif culty we introduce the notion of pref-erential stability , i.e., the stability of the feature preferences produced by a feature selection algorithm, to quantify its sensiti vity to dif ferences in training sets dra wn from the same distrib ution. The same approach can in fact be used to measure the preferential stability of any classication algo-rithm that produces models from which weightings or rank-ings of the features can be extracted, e.g. linear discrimina-tion algorithms.

Stability , as introduced in [11 ], and the bias-v ariance de-composition frame works are not able to accurately quantify preferential stability . It is possible that dif ferent training samples lead to really dif ferent feature sets which howe ver yield the same prediction patterns. This can be especially true when the initial features have a high level of redun-danc y which is not handled in a principled way by the algo-rithms used.

The moti vation for investig ating the stability of feature selection algorithms came from the need to pro vide appli-cation domain experts with quantied evidence that the se-lected features are relati vely rob ust to variations in the train-ing data. This need is particularly crucial in proteomics ap-plications. In mass spectrometry based diagnosis, for in-stance, training data (protein mass spectra) are character -ized by high dimensionality and the goal is to output a small set of highly discriminatory features (protein biomark ers) on which biomedical experts will subsequently invest con-siderable time and research effort. Domain experts tend to have less condence in feature sets that change radically with slight variations in the training data. Data miners have to con vince them not only of the predicti ve potential but also of the relati ve stability of the proposed features or biomark-ers.

The rest of the paper is organized as follo ws: in Section 2 we introduce measures of stability that can be applied to any feature selection algorithm that outputs a feature preference as dened abo ve; we also sho w how we can empirically es-timate these measures. In Section 3 we describe the experi-mental setup, the datasets used, and the feature selection al-gorithms included in the study; in Section 4 we present the results of the experiments, investig ate the beha vior of the dif ferent stability measures and establish the stability pro-les of the chosen feature selection algorithms; in Section 5 we examine together classication performance and stabil-ity of feature preferences, and suggest how we can exploit the latter to support the choice of the appropriate feature selection algorithm; nally we conclude in Section 6.
The generic model of classication comprises: a gener -ator of random vectors x , dra wn according to an unkno wn but x ed probability distrib ution P ( X ) ; a supervisor that assigns class labels x , to the x random vectors, according to an unkno wn but x ed conditional probability distrib u-tion P ( C j X ) ; a learning space populated by pairs ( x;c ) dra wn from the joint probability distrib ution P ( X;C ) = P ( C j X ) P ( X ) .

We dene the stability of a feature selection algorithm as the sensiti vity of the feature preferences it produces to dif ferences in training sets dra wn from the same generat-ing distrib ution P ( X;C ) . Stability quanties how dif ferent training sets affect the feature preferences.

Measuring stability requires a similarity measure for fea-ture preferences. This obviously depends on the represen-tation language used by a given feature selection algorithm to describe its feature preferences; dif ferent representation languages call for dif ferent similarity measures. We can dis-tinguish three types of representation languages for feature preferences. In the rst type a weight or score is assigned to each feature indicating its importance. The second type of representation is a simplication of the rst where in-stead of weights ranks are assigned to features. The third type consists only of sets of features in which no weighting or ranking is considered. Ob viously any weighting schema can be cast as a ranking schema, which in turn can be cast as a set of features by setting a threshold on the ranks or asking for a given number of features.

More formally , let training examples be described by a vector of features f = ( f 1 ;f 2 ;:::;f m ) , then a feature selec-tion algorithm produces either: In order to measure stability we need a measure of similarity for each of the abo ve representations. To measure similarity between two weightings w;w 0 ; produced by a given feature selection algorithm we use Pearson' s correlation coef cient where S W tak es values in [-1,1]; a value of 1 means that the weightings are perfectly correlated, a value of 0 that there is no correlation while a value of -1 that the y are anticorre-lated.
 To measure similarity between two rankings r;r 0 ; we use Spearman' s rank correlation coef cient where r i and r 0 r 0 respecti vely . Here too the possible range of values is in [-1,1]. A value of 1 means that the two rankings are iden-tical, a value of 0 that there is no correlation between the two ranks, and a value of -1 that the y have exactly inverse orders.

Finally we measure similarity between two subsets of features using a straightforw ard adaptation of the Tanimoto distance between two sets, [2]: The Tanimoto distance metric measures the amount of over-lap between two sets of arbitrary cardinality . S S tak es val-ues in [0,1] with 0 meaning that there is no overlap between the two sets, and 1 that the two sets are identical.
To empirically estimate the stability of a feature selection algorithm for a given dataset, we can simulate the distrib u-tion P ( X;C ) from which the training sets are dra wn by using a resampling technique lik e bootstrapping or cross-validation. We opted for N-fold stratied cross-v alidation (N=10). In ten-fold cross-v alidation the overlap of training instances among the dif ferent training folds is roughly 78%. The feature selection algorithm outputs a feature preference for each of the training folds. The similarity of each pair of feature preferences, i.e. N ( N 1) = 2 pairs, is computed us-ing the appropriate similarity measure and the nal stability score is simply the average similarity over all pairs.
We want to couple stability estimates with classication error estimates in vie w of identifying feature selection algo-rithms which maximize both stability and classication per -formance. To this end we embed the procedure described abo ve within an error estimation procedure, itself conducted using stratied 10-fold cross-v alidation. In other words, at each iteration of the cross-v alidated error estimation loop, there is a full internal cross-v alidation loop aimed at mea-suring the stability of feature precedences returned by the feasture selection algorithm. The outer loop pro vides a clas-sication error estimate in the usual manner , while the inner loop pro vides an estimate of the stability of the feature se-lection algorithm.
As already mentioned, the stability of feature selection methods is of utmost importance in mass-spectra based di-agnosis. Briey , a biological sample is submitted to a mass spectrometer to produce a mass spectrum. This can be vie wed as a protein prole of the sample which should be analysed to extract potential disease mark ers, whether indi-vidual proteins or sets of interacting proteins. To disco ver biomark er patterns in mass spectra, the data miner must face a number of technical challenges, foremost among which is their extremely high dimensionality . A typical mass spec-trum has several thousands of features that exhibit a high degree of spatial redundanc y. In order to reduce dimension-ality the spectra are preprocessed, and peaks, which roughly correspond to indi vidual proteins, are extracted. Ho we ver this still lea ves us with a considerable number of features. Each feature corresponds to a specic mass value, M/Z, and pro vides the intensity of the signal at that mass value.
We work ed with three dif ferent mass spectrometry datasets: one for ovarian cancer [7], (version 8-07-02), an-other for prostate cancer [8] and an extended version of the early strok e diagnosis dataset used in [9]. The y all involv e two-class problems, diseased vs controls. Preprocessing for feature extraction consisted of baseline remo val, denoising, smoothing, peak detection and peak alingment (the exact details of preprocessing are given in [6]). A short descrip-tion of these datasets is given in table 1; all features corre-spond to intensities of M/Z values and are continuous.
For feature selection we selected Information Gain (IG), [2], ReliefF (RF), [10], and SVMRFE-[5 ]. Informa-tion gain is a uni variate feature scoring method for nomi-nal attrib utes or continuous attrib utes discretized using the method of [3]. ReliefF deli vers a weighting of the features while taking their interactions into account; it uses all fea-tures to compute distances among training instances and the K nearest neighbors of each of the M probe instances to up-date feature weights. We set K to 10 and M to the size of the training set, so that all instances were used as probes. SVMRFE also tak es account of feature interactions in pro-ducing a ranking, with the P % lowest rank ed features being eliminated at the earliest iterations of the algorithm. In our experiments, P was set to 10% and the comple xity param-eter C was set to 0.5.

We also include a simple linear support vector machine to sho w that the same type of stability analysis can be ap-plied to any linear classier; here too the comple xity param-eter was set to 0.5. Pro vided that all features are normalized
Tab le 1. Description of mass spectr ometr y datasets considered. dataset IG RF ovarian 95.53 94.67 2.93 96.97 95.37 72.95 prostate 82.47 78.19 0.91 95.72 93.99 55.29 strok e 83.87 79.39 2.68 88.06 82.30 34.10 avg 87.29 84.08 2.17 93.58 90.55 54.11 ovarian 93.79 84.76 45.62 NA 83.86 46.80 prostate 86.85 73.89 52.43 NA 73.23 44.84 strok e 81.74 70.33 27.21 NA 69.71 16.78 avg 87.46 76.33 41.75 NA 75.60 36.14
Tab le 2. Stability results for the diff erent sta X  bility measures. S ture sets of the best ten features proposed by eac h method. to a common scale, the absolute values or the squares of the coef cients of the linear hyperplane can be tak en to re-ect the importance of the corresponding features, in effect pro viding a feature weighting. This is actually the assump-tion under which SVMRFE works; alternati vely the support vector machine is equi valent to SVMRFE with a single iter -ation, where the ranking of the features is simply based on the absolute values or the squares of the coef cients of the support vector machine. We consider this version of support vector machines as yet another feature selection algorithm and identify it as SVMONE. The implementations of all the algorithms are those found in the WEKA machine learning environment [12].

As already mentioned the stability estimates are cal-culated within each training fold by a nested cross-validation loop and the nal results reported are the aver-ages, S W ; S R ; S S , over the ten external folds. In table 2 we give the stability results for S W , S R , and S , i.e., for weightings-scorings, rankings and selected fea-ture sets, for the four dif ferent methods considered. The values of S S depend on the imposed cardinality of the -nal feature set while S W and S R are independent of that. S
S was computed on the feature sets of the best ten fea-tures selected by each method. SVMRFE does not produce a weighting-scoring of features so the computation of S W does not mak e sense in that case. For SVMONE the stabil-ity results are computed on the square values of the coef -cients of the linear hyperplane found by the support vector machine.

S W and S R tak e into account the complete feature pref-erences produced by a method, while S S focuses on a given number of top rank ed or selected features. Thus the former two pro vide a global vie w of stability of feature preferences while the latter focuses to a more precise picture. The latter is usually of greater interest since the feature preferences are in general used to produce a restricted set of features. S
W pro vides a ner grain picture of stability in compari-son to S R since it is based on the actuall feature coef cients produced by a given method while the S R uses the rank-ing of these coef cients. Ho we ver this does not mean that the information pro vided by S W is of greater value than that pro vided by S R , but rather the other way around. This is because again in practise we are more interested in the actual ranks of the features since based on them we will se-lect the nal set of features, dif ferences in weights are not necessarily reected in rank dif ferences. A further disad-vantage of S W is that since it directly operates on the actual weights-scores produced by each method its results are not directly comparable among dif ferent methods due to possi-ble dif ferences in scales and interv als of the weights-scores, a problem that does not appear when we are working with the ranks. Ov erall the most important information is de-livered by S S , when we are examining the stability of the methods for sets of selected features of given cardinality , follo wed by S R .

This ordering of the three measures in terms of their in-formation content is someho w reected on the estimated stability performances, table 2. For any method S W gives always the highest stability estimate, follo wed in generally closely by S R . S S is always considerably lower and de-pends on the number of features that we ask in the nal feature set (remember that for the estimates of S S in table 2 this was set to ten, later we will examine in more detail the beha vior of S S with respect to the cardinality of the nal feature set). In some sense S W and S R pro vide overly op-timistical estimates of feature preference stability (although in no case it can be argued that the values of the dif ferent stability measures are comparable). The reason for that can be traced on the fact that the y treat all weights or ranks dif-ferences in a uniform manner . Ne vertheless dif ferences on the highest weighted or rank ed features should be penalized more than dif ferences on the lower weighted or rank ed fea-tures. A fact that points to the denition and use of more rened similarity measures that can tak e into account the level at which a dif ference appears. These similarity mea-sures would lie conceptually between S W , and S R , that give equal importance to everything, and S S , that only considers a given number of top rank ed features.

We will now examine the stability performance of the four dif ferent methods considered. The clear winner is ReliefF that achie ves the best performance under all sta-bility measures for the three datasets under consideration. The performance dif ference is quite astonishing for S S for which ReliefF scores 72.95%, 55.29% and 34.10% for the ovarian, prostate and strok e datasets respecti vely . These scores correspond to an average overlap of 8.43, 7.12 and 5.08 features out of the ten contained in the nal set of se-lected features, among the dif ferent subsets of the training Information Gain appears to have a better score than SV -MONE and SVMRFE for S W and S R (remember here that among the two it is S R that can pro vide a meaningful ba-sis for comparison of dif ferent methods). Ho we ver its re-sults are catastrophic when we consider S S , with scores of 2.93%, 0.91% and 2.68% for ovarian, prostate and strok e respecti vely (an average overlap of 0.56, 0.18, 0.52 features out of the ten, i.e. in average less than one common feature among the dif ferent subsets of the training folds!).
Finally the performances of SVMONE and SVMRFE are quite similar in terms of S R a fact that can be easily explained since the ranking of features pro vided by SV -MONE can be considered as a less rened version of the ranking pro vided by SVMRFE with the former being the result of a single execution of the support vector machine algorithm and the latter the result of an iterati ve execution where each time 10% of the lower rank ed features are re-mo ved. Ho we ver in terms of S S SVMONE appears to be slightly more stable (an average overlap of 6.26, 6.87, 4.27 features out of ten for SVMONE against 6.37, 6.19, 2.87 for SVMRFE). Ag ain the fact that SVMRFE is based on multiple iterations explains its slightly higher instability on the top ten rank ed features. When the dif ferences of the coef cients of two features are rather small and a choise is about to be made on which of the two to eliminate dif ferent training sets could result in opposite rankings for these two features thus eliminating a dif ferent feature each time.
The results of the estimation process of S S can be very eloquently visualized pro viding insight not only on the sta-bility of each indi vidual method, but also clearly indicating which features are considered important by each method. An example of such a visualization for the prostate dataset is given in gure 1 where the cardinality of the nal feature set is set to ten. In each of the graphs the x-axis corresponds
Figure 1. Stability results for the prostate dataset for selected feature sets of car dinality 10. to the indi vidual features. The y-axis is separated to 10 rows each one corresponding to one of the outer cross-v alidation folds. Within each row we nd 10 rows (not visibly sep-arated) corresponding to each of the inner cross-v alidation folds of the outer fold. A perfectly stable method, i.e. one that chooses always the same features, would have in its graph as man y vertical lines as the cardinality of the nal feature set. Each line would correspond to one selected fea-ture. The visualization results are in perfect aggreement with the S S estimates given in table 2. The less stable method is Information Gain with features sets selected even within the inner folds of a given outer fold being quite dif-ferent (inner folds of a given outer fold share more training instances than the inner folds of two dif ferent outer folds). The other three methods are quite stable selecting very often the same features among the dif ferent inner folds. The big dif ferences in the stability estimates of S R and S
S for Information Gain were puzzling. In order to see where the y could be coming from we took a closer look on the weighting-scorings produced by Information Gain. It turns out that the scorings are zero, i.e. the corresponding features have a zero information gain, for a lar ge number of features. More precisely for ovarian 35.07% of features have an information gain of zero, for prostate this goes up to 85.63%, and for strok e to 91.25% 2 . On the other hand for ReliefF the corresponding percentages are practically zero, and for SVMONE always less than 3%. When these weightings-scorings are turned into a ranking in order to compute S R there is a very big number of ties in the rank-ing of dif ferent features (in the case of information gain). The crusial element is how ties are dealt with. Originally we were assigning to all tied features their average rank. This meant that in the case of Information Gain 35.07%, 85.63% and 91.25% of the features, for ovarian, prostate and strok e respecti vely , had exactly the same rank; more-over these features were concentrated on the low end of the ranking. Due to the presence of a lar ge number of features with equal ranks the nal value of the S R estimate was op-timistically affected for Information Gain, moreo ver since this was happening on the low rank levels it was completely masking any information about the stability of the rank on the top positions.

To correct for this optimism we have chosen to break ties by assigning randomly the ranks among the tied fea-tures. For example, if belo w the tenth rank ed feature there was a group of 20 features with exactly the same weighting-scoring then each one of them would be assigned a dif ferent rank randomly from 11 to 30. This left unaf fected the S R estimates produced for ReliefF , SVMONE, and SVMRFE, since the rst two had a very low number of ties, and the latter was naturally producing a rank, but considerably low-ered the stability estimates for Information Gain, with the new estimates being 91.09%, 40.74% and 20.44% for ovar-ian, prostate and strok e respecti vely , being thus more con-sistent with the picture that S S is pro viding. Ho we ver these observ ations still call for a more rened version of S R that would reward similarities and penalize dif ferences more at the top level ranks.
It is clear that the more interesting stability estimation is pro vided by S S since it focuses only on a small subset of features, the ones selected by each method, which is ac-tually what interest us when we are performing feature se-lection. To get a more precise picture of the stability per -formance of the dif ferent methods with respect to S S we computed its values for dif ferent values of selected features ranging from 10 up to the cardinality of the full feature set with a step a ve, gure 2. Moreo ver we included as a sta-bility baseline a random feature selection that simply out-puts random sets of features of given cardinality .
First remark is that ReliefF clearly dominates all other algorithms for all interesting cardinalities of the nal fea-ture sets. Information Gain has a quite bad performance for prostate and strok e, explained by the great number of zero information gain features; actually it has almost the same stability beha vior as the random feature selection. In the ovarian dataset it exhibits a sharp increase of stability up to feature sets with around 300 features and then very slo wly increases towards one when all features have been included. The  X knot X  in this curv e actually corresponds to the inclusion of all features, with an information gain dif-ferent than zero, to the nal set of selected features. After this point features are actually added randomly . So in some sense it detects the cardinality of the most stable set of fea-tures. The same knot is also observ ed in the case of RelieF , quite strongly for the strok e and prostate and less for ovar-ian, and for SVMONE and SVMRFE in strok e. We belie ve that the presence of knots lik e these mark the inclusion of the most rob ust-stable features; features included later are added more or less randomly . The knots could be possibly used to determine the optimal cardinality of the most stable feature set, but this is something that needs further investi-gation.

SVMONE has a small adv antage over SVMRFE on se-lected feature sets of low cardinality but their performance is indistinguishable for high cardinalities. As we mo ve to higher cardinalities both methods add low rank ed features, which should more or less the same for both methods since for SVMRFE these are determined on the earliest iterations of the algorithm, being thus closely in beha vior to the single run of SVMONE. Mo ving to lower cardinalities the insta-bility of SVMRFE increases due to the already mentioned fact that small dif ferences in the coef cients can inverse the rank and thus remo ve dif ferent features. The dif ference in instability between SVMONE and SVMRFE increases as we mo ve to lower cardinalities beacause there the nal fea-ture sets of SVMRFE are determined in the last iterations of the support vector machine algorithm.
A feature selection algorithm, (FSA for bre vity), alone can pro vide an indication of which features are informati ve for classication but it cannot pro vide an estimate of the discriminatory information of these features, since it does not construct classication models whose error could be es-timated. In the same manner stability results cannot pro vide the sole basis on which to select an appropriate FSA; nev-ertheless the y can support the selection of an FSA when the latter is coupled with a classication algorithm, (CA for bre vity), and enhance the condence of the users on the analysis results (pro vided that the FSA is found to be sta-ble). The nal selection can be based on a combined evalu-ation of stability and classication performance.
 The simplest scenario goes as follo ws, couple a given CA with a number of FSAs and estimate the classication performance and the stability of the FSA using the process Figure 2. S the nal feature set. described earlier . Then calculate the statistical signicance of error dif ferences. Among the combinations of the CA and FSAs that were found to be better than all the others choose the combination that contains the most stable FSA.
To demonstrate the abo ve idea we selected as classica-tion algorithm the linear SVM with the comple xity param-eter set to 0.5. We performed a series of experiments in which each FSA was paired with the CA. In each experi-ment we x ed the number of selected features to N . We ranged N from ten to 50 with a step of ten. For a given N the four pairs of FSA-CA were compared with respect to their classication error and the stability of the FSA. Sta-tistical signicance of error dif ferences is computed by Mc-Nemar' s test of signicance (sig. level=0.05). The complete results are given in table 3. Each row of that table gives the classication errors of a FSA-CA pair follo wed by the sta-bility estimate, S S , of the FSA. The errors of the FSA-CA pairs that get the top positions, for a given N , without being signicantly dif ferent between them are typed in italics .
Applying the selection scenario mentioned abo ve we see that for strok e and ovarian and for dif ferent values of there are several FSA-CA pairs that are indistinguishable in terms of classication error . Consider the strok e dataset with N = 10 ; Information Gain, ReliefF and SVMRFE have similar classication performance. In this case we can also consider their stability . ReliefF is by far the most sta-ble with an S S value that is double of that of SVMRFE and more than an order of magnitude greater than that of Infor -mation Gain. Ob viously the adv antage of selecting the most stable FSA is that we have much more condence on the features. Moreo ver coupling the results with a visual repre-sentation of stability as the one given in gure 1 pro vides a clear picture of the important features and how rob ust the y are to perturbations of the training set.

One question that arises from the abo ve results is: how is it possible for a FSA to be very unstable and still when coupled with a CA to produce good results. This was ac-tually the case man y times with SVMRFE. For example in the strok e dataset and N = 20 SVMRFE coupled with the CA was signigicantly better than the other three FSA-SA paits. Ne vertheless its S S estimate was 0.16 (in feature sets of cardinality 20 this corresponds to an average of 5.5 com-mon features). One possible answer to that is redundanc y. Among the initial full feature set there are possibly man y dif ferent subsets of cardinality 20 on which classication models can be constructed that can accurately predict the tar get concept 3 . Cases lik e that, i.e., instability coupled with high classication performance, can be simply an indication of redundanc y within the full feature set. This also means that the feature selection algorithm under examination does not have a rob ust way to tackle redundanc y.
To the best of our kno wledge this is the rst time that a frame work that measures the stability of feature selection algorithms is proposed. We dened the stability of feature selection algorithms as the sensiti vity of the  X feature pref-erences X  that the y produce to training set perturbations. We examined three dif ferent stability measures and proposed a resampling technique to empirically estimate them. The most interesting one was based on S S a measure of the overlap of two feature sets. We exploited the frame work to investig ate the stability of some well kno wn feature selec-tion algorithms on three datasets coming from the domain of proteomics and gained some interesting insights. Sta-bility can be also used to support the selection of a feature selection algorithm.

We belie ve that the notion of stability is central in real world application where the goal is to determine the most important features. If these features are consistent among models created from dif ferent traning data the condence of the users on the analysis results highly increases. The results of the empirical estimation of stability can be ele-gantly visualized and pro vide a clear picture of the rele vant features, their rob ustnes to dif ferent training sets, and the stability of the feature selection algorithm.

Future work includes the examination of stability of more algorithms on a bigger and more diverse set of prob-lems; rening the S R stability measure in order to reect better lar ge dif ferences on the top rank ed features; aggre-gating the dif ferent feature sets produced from subsamples of a given training set in what can be vie wed as the analogue of ensemble learning and model combination for feature se-lection; nally we would lik e to examine the possibility of using the stability proles to select the appropriate number of features (the knots in the stability graphs).

N IG Relief SVM SVMRFE 10 32.22 -0.02 30.29 -0.34 37.02-0.27 26.45 -0.16 20 28.85-0.05 28.85-0.36 35.10-0.30 21.64 -0.16 30 27.89 -0.09 27.41 -0.37 28.37 -0.33 23.56 -0.17 40 29.81 -0.12 25.97 -0.38 25.00 -0.35 25.49 -0.18 50 27.89 -0.16 28.37 -0.40 26.45 -0.37 25.49 -0.19
N IG Relief SVM SVMRFE 10 10.28-0.01 10.28-0.72 07.12-0.59 01.19 -0.46 20 05.56-0.06 05.93-0.69 03.96 -0.58 01.19 -0.47 30 04.75-0.09 01.59 -0.69 01.19 -0.56 00.40 -0.45 40 03.17-0.12 01.59 -0.69 00.40 -0.56 00.40 -0.44 50 02.77 -0.16 01.59 -0.70 00.40 -0.58 00.40 -0.44
N IG Relief SVM SVMRFE 10 18.64-0.01 18.95-0.55 18.02-0.52 13.05 -0.44 20 17.71-0.01 17.09-0.60 16.46-0.51 11.50 -0.40 30 16.46-0.02 15.84-0.61 14.91-0.52 10.87 -0.38 40 16.15-0.03 14.91-0.62 13.36-0.52 09.01 -0.38 50 14.60-0.04 13.36-0.62 13.05-0.53 09.32 -0.38
Tab le 3. Classication err or estimations cou X  pled with S ture selection method. N is the number of selected features.
