 A growing number of clustering algorithms for categorical data have been pro-posed in recent years, along with interest ing applications, such as partitioning large software systems and protein interaction data [6,13,29]. In the past, poly-nomial time approximation algorithms have been designed for NP-hard parti-tioning algorithms [9]. Moreover, it has recently been shown that the  X  X urse of dimensionality X  involving efficient searches for approximate nearest neighbors in a metric space can be dealt with, if and only if, we assume a bounded di-mensionality [12,21]. Clearly, there are tradeoffs of efficiency and approximation involved in the design of categorical clustering algorithms. Ideally, a set of prob-abilistically justified goals for categorical clustering would serve as a framework for approximation algorithms [20,25]. This would allow designing and comparing categorical clustering algorithms on a more formal basis.
 Our work is motivated by density-based clustering algorithms, such as CLIQUE [1], CLICKS [28], CACTUS [10], COOLCAT [5], DBSCAN [8], OP-TICS [4], Chameleon [19], ROCK [14], DENCLUE [15], and others. Although most of these approaches are efficient and relatively accurate, we go beyond them and approach the problem from a different viewpoint. Many of these al-gorithms require the user to specify input parameters (with wrong parameter values resulting in a bad clustering), may return too many clusters or too many outliers, often have difficulty finding cluste rs within clusters or subspace clusters, or are sensitive to the order of object input [6,12,13,28]. We propose a categor-ical clustering algorithm that builds a hierarchy representing a dataset X  X  entire underlying cluster structure, minimizes user-specified parameters, and is insen-sitive to object ordering. This offers to a user a dataset X  X  cluster structure as a hierarchy, which is built independently of user-specified parameters or object ordering. A user can cut its branches and study the cluster structure at differ-ent levels of granularity, detect subclust ers within clusters, and know the central densest area of each cluster. Although such an algorithm is slow, it inspires faster simplifications that are useful for finding the rich cluster structure of a dataset.
A categorical dataset with m attributes is viewed as an m -dimensional  X  X ube X , offering a spatial density basis for clustering. A cell of the cube is mapped to the number of objects having values equal to its coordinates. Clusters in such a cube are regarded as subspaces of high object density and are separated by subspaces of low object density. Clustering the cube poses several challenges: (i) Since there is no ordering of attribute values, the cube cells have no or-dering either. The search for dense subspaces could have to consider several orderings of each dimension of the cube to identify the best clustering (unless all attributes have binary values). (ii) The density of a subspace is often defined relative to a user-specified value, such as a radius. However, different radii are preferable for different subspaces of the cube [4]. In dense subspaces where no information should be missed, the search is more accurately done  X  X ell by cell X  with a low radius of 1. In sparse subspaces a higher radius may be preferable to aggregate information. The cube search could start from a low radius and gradually move to higher radii. Al-though the term  X  X adius X  is borrowed from geometrical analogies that assume circular constructs, we use the term i n a looser way and it is not a Euclidean distance.

We present the HIERDENC algorithm for hierarchica l density-based cluster-ing of categorical data, that addresses the above challenges. HIERDENC clusters the m -dimensional cube representing the spatial density of a set of objects with m categorical attributes. To find its dense subspaces, HIERDENC considers an object X  X  neighbors to be all objects that are within a radius of maximum dis-similarity . Object neighborhoods are insensitive to attribute or value ordering. Clusters start from the densest subspaces of the cube. Clusters expand outwards from a dense subspace, by connecting nea rby dense subspaces. Figure 1 shows examples of creating and expanding clusters in a 3-d dataset. The radius is the maximum number of dimensions by which neighbors can differ.
 We present the MULIC algorithm , which is a faster simplification of HIER-DENC. MULIC is motivated by clustering of categorical datasets that have a multi-layered structure. For instance, in protei n interaction data a cluster often has a center of proteins with similar interaction sets surrounded by peripheries of proteins with less similar interaction sets [7]. On such data, MULIC outperforms other algorithms that create a flat clustering.

This paper is organized as follows. S ection 2 presents the HIERDENC algo-rithm. Section 3 describes the MULIC clustering algorithm and its relation to HIERDENC. Section 4 discusses the experiments. Section 5 concludes the paper. Basics. We are given a dataset of objects S (which might contain duplicates) with m categorical attributes, X 1 ,  X  X  X  ,X m . Each attribute X i has a domain D i with a finite number of d i possible values. The space S m includes the collection of possibilities defined by the cross-product (or cartesian product) of the domains, D cells (positions). A cell of the cube repres ents the unique logical intersection in a cube of one member from every dimension in the cube. The function  X  maps a cell x =( x 1 ,  X  X  X  ,x m )  X  S m to the nonnegative number of objects in S with all m attribute values equal to ( x 1 ,  X  X  X  ,x m ):
We define the HIERDENC hyper-cube C ( x 0 ,r )  X  S m , centered at cell x 0 with radius r , as follows: The dist (  X  ) is a distance function. The Hamming distance is defined as follows: HD is viewed as the most natural way to represent distance in a categorical space. People have looked for other distance measures but HD has been widely accepted for categorical data and i s commonly used in coding theory. Figure 2 illustrates two HIERDENC hyper-cubes in a 3-dimensional cube. Since r =1, the hyper-cubes are visualized as  X  X rosses X  in 3D and are not shown as actually having a cubic shape. A hyper-cube excludes cells for which  X  returns 0. Normally, a hyper-cube will equal a subspace of S m . A hyper-cube can not equal S m , unless r = m and  X  x  X  S m  X  ( x ) &gt; 0.
 The density of a subspace X  X  S m ,where X could equal a hyper-cube C ( x 0 ,r )  X  S m ,involvesthesumof  X  evaluated over all cells of X : This density can also be viewed as the likelihood that a hyper-cube contains a random object from S ,where | S | is the size of S . HIERDENC seeks the densest hyper-cube C ( x 0 ,r )  X  S m . This is the hyper-cube centered at x 0 that has the maximum likelihood of containing a random object from S . The cell x 0 is a member of the set { x  X  S m : Max ( P (  X   X  C ( x ,r ))) } ,where  X  is a discrete random variable that assumes a value from set S .

The distance between two clusters G i and G j is the distance between the nearest pair of their objects, defined as: Clusters G i and G j are directly connected relative to r if D ( G i ,G j )  X  r .Clusters A and B are connected relative to r if: A and B are directly connected relative to r , or if: there is a chain of clusters C 1 ,  X  X  X  ,C n , A = C 1 and B = C n , such that C i and C i +1 are directly conn ected relative to r for all i such that 1 HIERDENC Algorithm and Discussion. Figure 3 shows the HIERDENC clustering algorithm. The default initial value of radius r is 1. G k represents the k th cluster formed. The remainder set, R = { x : x  X  S m and x /  X  G i ,i = 1 ,  X  X  X  ,k } , is the collection of uncluster ed cells after the formation of k clusters.
Step 1 retrieves the densest hyper-cube C  X  S m of radius r . Step 1 checks that the densest hyper-cube represents more than one object ( density ( C ( x 0 ,r )) &gt; | ), since otherwise the cluster will not expand, ending up with one object. If the hyper-cube represents zero or one object, then r is incremented. Step 2 creates anew leaf cluster at level r  X  1. Starting from an existing leaf cluster, step 3 tries to move to the densest hyper-cube of radius r nearby. If a dense hyper-cube is found near the cluster, then in step 4 the cluster expands by collecting the hyper-cube X  X  cells. This is repeate d for a cluster until no such connection can be made. New objects are clustered until r = m ,or density ( R )  X  1% and the unclustered cells are identified as outliers ( step 5 ). For many datasets, most objects are likely to be clustered long before r = m .

Initially r = 1 by default, since most datasets contain subsets of similar ob-jects. Such subsets are used to initially identify dense hyper-cubes. When r is incremented, a special process merges clusters that are connected relative to r . Although the initial r = 1 value may result in many clusters, similar clusters are merged gradually. As Figure 4 sh ows, a merge is represented as a link between two or more links or leaf clusters, created at a level r  X  1. A link represents a group of merged clusters. This process g radually constructs one or more cluster tree structures, resembling hierarchica l clustering [18,24]. The user specifies a cut-off level (e.g. r = 3) to cut tree branches; links at the cut-off level are extracted as merged clusters. Step 5 checks if a newly formed cluster is connected to another cluster relative to r and if so links them at level r . Step 6 continues linking existing clusters into a tree, until r = m . By allowing r to reach m ,an entire tree is built. At the top of the tree, there is a single cluster containing all objects of the dataset.

In [3] we propose and evaluate several methods for setting the HIERDENC tree cut-off level. One method involves cutting the HIERDENC tree at level r such that the average connectivity of the resulting merged clusters is minimized. The connectivity r of a merged cluster (a set of conn ected leaf clusters) relative to r is the fraction of its objects that h ave another object within distance r in a different leaf cluster in the same co nnected set. Another method useful for finding clusters within clusters is to set the cut-off(s) for a branch of links from leafstorootatthe level(s) r  X  1 such that the resulting merged cluster has 0 . 0 &lt; connectivity r &lt; 1 . 0. Another method is to balance the number of clusters with the entropy of the partition [22]. This involves setting the cut-off at level r such that the Akaike X  X  Information Criterion (AIC) is minimized [2]. The AIC of a partition is entropy +2 k ,where k is the number of clusters.

Although HIERDENC has similarities to CLIQUE [1], the two have signifi-cant differences. HIERDENC is intende d for categorical data while CLIQUE for numerical data. HIERDENC minimizes input parameters, while CLIQUE takes as input parameters the grid size and a global density threshold for clusters. HIERDENC retrieves the densest hyper-cube relative to the radius. The radius relaxes gradually, implying that HIERDENC can find clusters of different densi-ties. HIERDENC can often distinguish the central hyper-cube of a cluster from the rest of the cluster, because of its hi gher density. HIERDENC creates a tree representing the entire dataset structur e, including subclusters within clusters. MULIC stands for multiple layer clustering of categorical data. MULIC is a faster simplification of HIERDENC. MULIC bala nces clustering accuracy with time efficiency. The MULIC algorithm is motiva ted by datasets the cluster structure of which can be visualized as shown in Figure 5. In such datasets a cluster often has a center of objects that are similar to one another, along with peripheral objects that are less similar to the central objects. Such datasets include protein interaction data, large software systems and others [7].

MULIC does not store the cube in memory and makes simplifications to de-crease the runtime. A MULIC cluster starts from a dense area and expands outwards via a radius represented by the  X  variable. When MULIC expands a cluster it does not search all member o bjects as HIERDENC does. Instead, it uses a mode that summarizes the content of a cluster. The mode of cluster c is a vector  X  c = {  X  c 1 ,  X  X  X  , X  cm } where  X  ci is the most frequent value for the i th attribute in the given cluster c [16]. The MULIC clustering algorithm ensures that when an object o is clustered it is inserted into the cluster c with the least dissimilar mode  X  c . The default dissimilarity metric between o and  X  c is the Hamming distance presented in Section 2.1, although any metric could be used. A MULIC cluster consists of layers formed gradually, by relaxing the maximum dissimilarity criterion  X  for inserting objects into exi sting clusters. MULIC does not require the user to specify the number of clusters and can identify outliers. Figure 6 shows the main part of the MULIC clustering algorithm. An optional final step merges similar clusters to reduce the number of clusters and find more interesting structures.
 Merging of Clusters. Sometimes the dissimilarity of the top layers of two clusters is less than the dissimilarity of the top and bottom layers of one of the two clusters. To avoid this, after the clustering process MULIC can merge pairs of clusters whose top layer modes X  dissimilarity is less than the maximum layer depth of the two clusters. For this purpose, MULIC preserves the modes of the top layers of all clusters. The default merging process, detailed in [3], merges clusters in a non-hierarchical manner such that clusters have a clear separation. However, a hierarchical cluster merging process is also proposed [3]. MULIC Discussion. MULIC is a simplification of HIERDENC. The tradeoffs between accuracy and time efficiency are as follows: (i) When creating a cluster, HIERDENC s earches the cube to retrieve the densest hyper-cube relative to r representing two or more objects, which is costly. MULIC creates a cluster if two or more objects are found within a dissimilarity distance of  X  from each other, likely indicating a dense subspace. Clusters of size one are filtered out. MULIC X  X   X  variable is motivated by HIERDENC X  X  radius r . The initial objects clustered with MULI C affect the modes and the clustering. For this issue we propose in [3] an optional preprocessing step that orders the objects by decreasing aggregated frequency of their attribute values, such that objects with more frequent values are cl ustered first and the modes will likely contain the most frequent values. This o bject ordering process has been evalu-ated in [3], which showed that it is better than a random ordering of objects; we do not include the same results here. (ii) When expanding a cluster HIERDENC searches the member cells to find dense hyper-cubes relative to r , which is costly. MULIC instead uses a  X  X ode X  as a summary of a cluster X  X  content and only clusters objects w ithin a distance of  X  from the mode. MULIC increases  X  by  X  X  when no new objects can be clus-tered, which is motivated by HIERDENC X  X  increasing r . MULIC can create new clusters at any value of  X  , just as HIERDENC can create new clusters at any value of r . Although MULIC can find clusters of arbitrary shapes by increasing  X  , it loses some of HIERDENC X  X  ability in this realm. (iii) MULIC X  X  cluster merging is motivated by HIERDENC X  X  merging. The MULIC cluster merging process can organize clusters into a tree structure as HIERDENC does. For MULIC applications, such as the one on protein interac-tion data discussed in [3], we do not construct a tree since we prefer the clusters to have a clear separation and not to specify a cut-off.

MULIC has several differences from traditional hierarchical clustering, which stores all distances in an upper square ma trix and updates the distances after each merge [18,24]. MULIC clusters have a clear separation. MULIC does not require a cut-off to extract the clusters, as in hierarchical clustering; this is of benefit for some MULIC applications, such as the one on protein interaction data discussed in [3]. One of the drawbacks of hierarchical clustering is that the sequence of cluster mergings will affect the result and  X  X ad X  mergings can not be undone later on in the process. Moreover, if several large clusters are merged then interesting local cluster structure is likely to be lost. MULIC, on the other hand, does not merge clusters during the ob ject clustering. Instead, any cluster mergings that may be desirable for the particular application are done after object clustering has finished. MULIC aims not to lose cluster structure caused by several large clusters being merged during the clustering process. Computational Complexity. The best-case complexity of MULIC has a lower bound of  X  ( mN k ) and its worst-case complexity has an upper bound of O ( mN 2 threshold  X  X  ). The cost is related to the number of clusters k and the number of objects N .Often k N , m N , and all objects are clustered in the initial iterations, thus N often dominates the cost. The worst-case runtime would occur for the rather uncommon dat aset where all objects were extremely dissimilar to one another, such that the algorithm had to go through all m iter-ations and all N objects were clustered in the last iteration when  X  = m .The MULIC complexity is comparable to that of k -Modes of O ( mN kt ), where t is the number of iterations [16]. To evaluate the applicability of HIERDENC and MULIC to the clustering prob-lem, we first use the zoo and soybean -data categorical datasets. These datasets were obtained from the UCI Repository [23]. Objects have class labels defined based on some domain knowledge. We ignore class labels during clustering. We compare the HIERDENC and MULIC results to those of several other density-based algorithms, ROCK [14], CLICKS [28], k -Modes [16], and AutoClass [26]. CLICKS was shown to outperform STIRR [11] and CACTUS [10]. To evaluate the clustering quality we use HA Indexes [17] and Akaike X  X  Information Cri-terion (AIC) [2]. HA Indexes is a class-label-based evaluation, which penalizes clustering results with more or fewer clusters than the defined number of classes. Since the class labels may or may not be consistent with the clustering structure and dissimilarity measure used, we also estimate the AIC of each clustering. AIC penalizes non-uniformity of attribute values in each cluster and too many clus-ters. In [3] we discuss MULIC with non-hierarchical and hierarchical merging of clusters applied to protein interaction data and large software systems.
For MULIC we set  X  X  =1, threshold = m , and we order the objects as de-scribed in [3]. We applied the other algorithms (except HIERDENC) on more than 10 random orderings of the objects. For k -Modes and ROCK we set the number of clusters k to the number of classes, as well as larger numbers. Auto-Class considers varying numbers of clusters from a minimum of 2. HIERDENC Results. Table 1 shows the HIERDENC results for these datasets before and after cutting the tree. After cutting the HIERDENC tree for zoo , its HA Indexes, Entropy, and AIC are slightly better than CLICKS. The HIERDENC results for soybean -data are significantly better than CLICKS. The Entropy is naturally lower (better) in results with many clusters; by comparing results of algorithms with similar numbers of clusters, the HIERDENC Entropy is often lower. The drawback we notice is that the HIERDENC runtime is sig-nificantly higher on soybean -data than on zoo .

Figure 4 illustrates the HIERDENC tree for zoo . There are 17 leaf clusters in total in the HIERDENC tree. Except for the last 3 created leaf clusters, all other leaf clusters are homogeneous with regards to the class labels of member objects. The last 3 leaf clusters were created for high r values of 7, 6, and 4. The rest of the leaf clusters were created for lower r values. For zoo we cut off the HIERDENC tree at level r =1; zoo is a rather dense cube with many nonzero cells and we do not want to aggregate information in the cube. The r =1cut-off minimizes the connectivity relative to r of the resulting clusters. By cutting the HIERDENC zoo tree at r = 1, there are 8 resulting clusters. There are a few cases of incorrectly clustered objects by cutting at r = 1. However, the lower number of clusters results in improved HA Indexes.

For the soybean -data set, there are 89 leaf clusters in total in the HIERDENC tree. The leaf clusters created for r  X  9 are homogeneous with regards to the class labels of member objects. Fo r leaf clusters created for r&gt; 9, the homogeneity of the class labels decreases. On ly 23 objects are clustered for r&gt; 9, so these could be labeled as outliers. For soybean -data we cut off the HIERDENC tree at r =4; soybean -data is a sparse cube of mostly  X 0 X  cells, since the dataset has 35 dimensions but only 307 objects. The r = 4 cut-off minimizes the connectivity relative to r of the resulting clusters. By cutting the HIERDENC soybean -data tree at r = 4, there are 20 resulting merged clusters.
 MULIC Results. Table 1 shows the MULIC results for these datasets with and without merging of clusters. MULIC has good Entropy measures and HA In-dexes, because the attribute values are quite uniform in clusters. It is interesting how MULIC finds subclusters of similar animals; for example, the animals  X  X orpoise X ,  X  X olphin X ,  X  X ealion X , and  X  X eal X  are clustered together in one MULIC cluster. MULIC with non-hierarchical merging of clusters has as a result that the number of clusters decreases, which o ften improves the quality of the result according to the HA Indexes. After merging the MULIC clusters, the number of clusters for zoo and soybean -data is close to the class-label-based number of classes. After merging the MULIC clusters for zoo , the HA Indexes, Entropy, and AIC are as good as CLICKS. The MULIC results for soybean -data are bet-ter than CLICKS. The Entropy is naturally lower (better) in results with many clusters; by comparing results of algorithms with similar numbers of clusters, the MULIC Entropy is often lower. MULIC runtimes are lower than HIERDENC. We have presented the HIERDENC algorithm for categorical clustering. In HI-ERDENC a central subspace often has a higher density and the radius relaxes gradually. HIERDENC produces good clustering quality on small-dimensional datasets. HIERDENC motivates developing faster clustering algorithms.
MULIC balances clustering accuracy wi th time efficiency. MULIC provides a good solution for domains where clustering primarily supports long-term strate-gic planning and decision making, such as analyzing protein-protein interaction networks or large software systems [3]. The tradeoffs involved in simplifying HIERDENC with MULIC point us to the challenge of designing categorical clustering algorithms that are accurate and efficient.

