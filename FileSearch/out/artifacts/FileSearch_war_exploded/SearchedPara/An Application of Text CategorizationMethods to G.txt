 This paper describes an application of IR and text categorization methods to a highly practical problem in biomedicine, specifically, Gene Ontology (GO) annotation. GO annotation is a major activity in most model organism database projects and annotates gene func-tions using a controlled vocabulary. As a first step toward automatic GO annotation, we aim to assign GO domain codes given a specific gene and an article in which the gene appears, which is one of the task challenges at the TREC 2004 Genomics Track. We approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various forms of gene synonyms, so as to exhaustively locate the occurrences of the tar-get gene. We extracted the words around the gene occurrences and used them to represent the gene for GO domain code anno-tation. As a classifier, we adopted a variant of k-Nearest Neighbor ( k NN) with supervised term weighting schemes to improve the per-formance, making our method among the top-performing systems in the TREC o ffi cial evaluation. Moreover, it is demonstrated that our proposed framework is successfully applied to another task of the Genomics Track, showing comparable results to the best per-forming system.
 H.2.4 [ Database management ]: Systems X  Textual databases ; H.3.1 [ Information storage and retrieval ]: Content Analysis and Index-ing X  Abstracting methods, Indexing methods, Linguistic process-ing ; J.3 [ Life and medical sciences ]: Biology and genetics Algorithm, Performance, Experimentation Text Categorization, Automatic Database Curation, Genomic IR
Given the intense interest and fast growing literature, biomedicine is an attractive domain for exploration of intelligent information processing techniques, such as information retrieval (IR), informa-tion extraction, and information visualization. As a result, it has been increasingly drawing much attention of researchers in IR and other related communities [6, 7, 9, 17]. As far as we know, how-ever, there have been few products of research e ff orts focusing on this particular domain in past SIGIR conferences. This paper in-troduces a successful application of general IR and text categoriza-tion methods to this evolving field of research targeting biomedical texts.

In the post-genomic era, one of the major activities in molecular biology is to determine the precise functions of individual genes or gene products, which has been producing a large number of publi-cations with the help of high throughput gene analysis. To structure the information related to gene functions scattered over the litera-ture, a great deal of e ff orts has been made to annotate articles by us-developed for describing functions of gene products in order to fa-cilitate uniform queries across di ff erent model organism databases, such as FlyBase, Saccharomyces Genome Database (SGD), and the Mouse Genome Informatics (MGI) Database. GO terms are ba-sically organized in hierarchical structures (but a child node may have multiple parent nodes) under three top level nodes: molecu-lar function (MF), biological process (BP), and cellular component (CC). Figure 1 illustrates the structure of GO.

Because of the large number of publications and specialized con-tent, GO annotation requires extensive human e ff orts and substan-tial domain knowledge, which is usually conducted by experts. Thus, there is a potential need to automate or semi-automate GO annotation, which could greatly alleviate the human curation. This was one of the primary objectives pursued at the Text Retrieval Conference (TREC) 2004 Genomics Track [8]. http://www.geneontology.org/
The 2004 Genomics Track consisted of two tasks: ad hoc re-trieval and categorization tasks. For the former, given 50 topics obtained through interviews with real research scientists, the par-ticipants were required to find relevant documents from 10 years X  worth of MEDLINE data. The latter task (which is our focus in this paper) was composed of two sub-tasks; one was called the triage task and the other the annotation task. Both tasks mimicked some parts of GO annotation process currently carried out by human ex-perts at Mouse Genome Informatics (MGI). Figure 2 depicts the conceptual flow of the two sub-tasks. Figure 2: A conceptual flow of the categorization sub-tasks. In short, the goal of the triage task was to correctly identify whether an input article contains experimental evidence that warrant GO an-notation regardless of specific GO codes. The annotation task was the next step to the triage decision, and the goal was to correctly assign GO domain codes, i.e., MF, BP, and CC (not the actual GO terms) or not to assign them, i.e., negative, for each of the given one gene associated with an article and there may be more than one domain code assigned to a gene.

The triage task can be seen as a standard text categorization prob-lem to classify an input article into predefined classes (positive and negative), while the annotation task required to classify not an arti-cle as a whole but each given gene appearing in the article. In other words, each  X  article, gene  X  pair was to be independently classified even when two (or more) genes appeared in a single article. We addressed the problem by extracting document fragments that were likely to contain the gene in question by gene name expansion and a flexible term matching scheme. The resulting set of document fragments were then used for representing the particular gene. For classification, we used a variant of the k-Nearest Neighbor ( k NN) classifier with supervised term weighting schemes [2] which con-sider word distributions in di ff erent classes.

This paper focuses on an application of general IR and text cate-gorization techniques to the domain-specific, highly practical prob-lems with careful consideration of the properties of the terminology in biomedicine. In the following, we first introduce our proposed framework for GO domain code annotation, and then describe the data and evaluation measures used in our experiments. We show the e ff ectiveness of our framework through a number of experi-ments with various di ff erent settings. In addition, it is demonstrated that our framework can be successfully applied to the triage task as well, making it among the top-performing systems for both triage and annotation tasks in the TREC o ffi cial evaluation. cases at the annotation stage. However, there were negative in-stances purposefully included in the TREC data (Section 3.1).
This section details our proposed framework for automatic GO domain code annotation. Hereafter, we will refer to GO domain code annotation as  X  X O annotation X  for short.
GO annotation needs to be made not for each input article but for each gene for which there is experimental evidence that war-rants GO annotation. Therefore, each  X  article, gene  X  pair needs to be treated as a  X  X ocument X  or  X  X ext X  in the sense of text categoriza-tion. For this purpose, we propose a simple but e ff ective approach to extract only the text fragments that are likely to contain the gene in question and treat a set of the extracted text fragments as a doc-ument associated with the  X  article, gene  X  pair. This process can be broken down into gene name expansion and gene name identifica-tion , each explained in the following.
 Gene name expansion. Gene name expansion refers to a pro-cess to associate synonyms with a given gene name. Gene names are known to have several types of synonyms including aliases, ab-breviations, and gene symbols. For instance,  X  X embrane associ-ated transporter protein X  can be referred to as underwhite , dominant brown , Matp , uw , Dbr , bls , Aim1 , etc. Therefore, all of these names should be searched to identify text fragments mentioning the gene. To obtain such synonyms, we used two sources of information: the article itself and a gene name dictionary. As described later in Sec-tion 3, the input article is annotated with SGML tags and there are two relevant fields, &lt;KEYWORD&gt; and &lt;GLOSSARY&gt; we also examined the use of body text because gene name abbre-viations often appear with parentheses immediately following the o ffi cial names [14]. However, it slightly degraded classification in our preliminary experiments and thus was not used in the following experiments.

As another source of gene name expansion, a gene name dic-tionary was automatically complied from existing databases. For this work, we experimentally used the SWISS-PROT [11] and Lo-cusLink [12] databases. The resulting name dictionary contained 493,473 records, where each record had a gene / protein name as a keyword and lists its synonyms. Hereafter, we use the word  X  X ene names X  to refer to all of o ffi cial names, aliases, abbreviations, and gene symbols.

It is often the case that gene name dictionaries automatically compiled from existing databases, such as ours, are not very ac-curate due to multi-sense gene names, inconsistent format in the databases, etc. It requires manual curation to obtain high-quality dictionaries [3, 5], which are important for general-purpose gene name recognition systems. Fortunately, the quality of dictionary would not be as important in our application, because even if the dictionary provides wrong gene names as synonyms of a given gene, the wrong names are unlikely to appear in the article as they are irrelevant to the target gene with which the article is associated. Gene name identification. The next step is to find text frag-ments mentioning the gene in question. Here, the problem is that, besides synonyms, gene names often have many variants due to ar-bitrary use of special symbols, white space, and capital and small letters [3]. To tolerate these minor di ff erences in identifying gene 3 The DTD is found at http://highwire.stanford.edu/ about/dtd/ names, both gene names and text were preprocessed as follows (the actual order is not important).
Then, each paragraph (identified by SGML tags) in the article was scanned if it contained any of the gene names associated with the gene in question. Note that section titles were appended to each paragraph since they were often found to be descriptive. In addition, if the paragraph referred to figures and / or tables for the first time in the article, their captions were also appended to the paragraph.

We have so far obtained gene name synonyms and normalized both gene names and text to facilitate gene name identification. However, there remains another problem. That is, gene names are frequently written in slightly di ff erent forms with extra words, dif-ferent word order, etc. For example,  X  peroxisome proliferator acti-vated receptor binding protein  X  may be referred to as  X  peroxisome proliferator activator receptor (PPAR) -binding protein  X  where un-derlines indicate the di ff erences. To deal with the problem, we used approximate word matching. To be precise, for each target gene name and each candidate which mentions any word composing the gene name, a word-overlap score defined below was computed. where M and U denote the numbers of matching and unmatching words, respectively;  X  is a penalty for unmatching words (set to 0.3); N is the number of words composing the gene name; and penalizes shorter gene names (set to 2). If any candidate associ-ated with a paragraph had a score exceeding a predefined threshold (set to 0.3), the paragraph was used to represent the  X  article, gene and stemming by Lovins stemmer [10]. For instance, the exam-ple of  X  peroxisome ...  X  above has five matching and two unmatch-ing words, resulting in an overlap score of 0.55. Because it is greater than the threshold (0.3), the paragraph containing the can-didate is extracted and used in part to represent the corresponding  X  article, gene  X  pair. Incidentally, the values of the parameters were determined based on our preliminary experiments on the training data.

Here, we treated a paragraph as a unit since it is thought to be organized in a single topic and seems to be an appropriate unit of extraction. In Section 4.2, we will examine other alternative units.
Along with the article itself, we took advantage of external re-signed to the article. MeSH terms are controlled vocabularies de-veloped at the national library of medicine (NLM) for indexing biomedical articles and are annotated by human experts at NLM. http://www.ncbi.nlm.nih.gov/entrez/query/static/ help/pmhelp.html http://www.nlm.nih.gov/mesh/
For each input article, all the associated MeSH terms were ob-cause these MeSH terms are annotated with articles (not with par-ticular genes), they were added to each document (a set of para-graphs) representing a pair of the article and any gene coupled with it. Note that a special symbol MESH+ was concatenated to each MeSH term so as to distinguish MeSH from other terms.
Feature selection identifies the features (terms) that are more in-formative in terms of classification according to some statistic mea-sure, which not only reduces the size of data but often improves classification [19]. For this work, we applied the chi-square statistic method to the terms contained in the documents obtained through the previous steps.

Chi-square statistic of term t in class c is defined as: where c is one of the GO domain codes (BP, MF, and CC) or nega-tive (NEG), A is the number of documents containing term t in class c , B is the number of documents containing t in classes other than c , C is the number of documents not containing t in c , D is the num-ber of documents not containing t in classes other than c , and N is the total number of documents. For each term t , chi-square statistic was computed for every class, and the maximum score was taken as the chi-square statistic for term t ; that is,  X  2 ( t ) Only the top n terms with higher chi-square statistics were used for the following processes. We empirically chose n = 3000 based on our preliminary experiments, where 234 terms were highest with class BP, 173 with MF, 277 with CC, and 2316 with NEG.
Each  X  article, gene  X  pair was associated with a set of selected terms in the preceding steps. To apply k NN for classification as de-scribed in the next section, we converted it to a term vector adopting the classic vector space model [13] with conventional TFIDF (term frequency-inverse document frequency) defined as: where TF( t , d ) is a term frequency of term t within document d , N is the total number of documents, and DF( t ) is the number of documents in which term t appears. In cases where TF( t , DF( t ) = 0, TFIDF( t , d ) is defined to be 0.

We also tested another term weighting scheme, so called super-vised term weighting , proposed by Debole and Sebastiani [2]. It takes into account pre-labeled class information in training data and re-uses statistics computed in the feature selection step (e.g., chi-square statistics, information gain, ... ) in place of IDF. We used TFCHI which is defined as a product of TF and chi-square statis-tics. Specifically, we tested two variants of the scheme, denoted as TFCHI 1 and TFCHI 2 .

We used a variant of k NN classifiers to assign GO domain codes to each pair of article and gene. k NN is an instance-based classifier http://www.ncbi.nlm.nih.gov/entrez/query.fcgi http://www.ncbi.nlm.nih.gov/entrez/query/static/ eutils help.html which is reported as one of the best classifiers for text categoriza-tion in both newswire and medical domains [18]. In brief, it clas-sifies input v to one or more predefined classes depending on what classes its neighbors belong to. The decision rule can be expressed as: where n c is the k nearest neighbors having class c  X  X  BP , MF , CC , NEG } t is a per-class threshold, and sim ( v , n c , i ) returns cosine similarity between the arguments. Threshold t c can be optimized to maximize an arbitrary metric (e.g. F 1 -score) using training data.
We slightly modified the scoring scheme to multiply the similar-ity scores by the number of k neighbors having class c , denoted as | n | . if Score ( c , v ) = It intended to boost the scores for more frequent classes within the k neighbors. This modification slightly but constantly improved classification (around 2% in F 1 score).

Although class c includes NEG, we did not apply the decision rule above for the negative class. In other words, if none of the GO domain codes was assigned to input, then it was considered to be negative. This ensures that an input does not have both positive (BP, MF, or CC) and negative classes together. It should be noted that, however, negative class does a ff ect classification because more negative instances included in k neighbors generally lead to lower scores for the positive classes.
We used the same data set as the Genomics Track annotation task. The data set is composed of 504 full-text articles for train-ing and 378 for test, both in SGML format. Each of the articles is associated with one or more genes and each gene is annotated with one or more classes (BP, MF, and CC) or negative by MGI cu-rators. The total numbers of triplets  X  article, gene, class (589 positives and 1072 negatives) and 1077 (495 positives and 582 negatives) for the training and test data, respectively. Because gene names often contain Greek alphabets, character entities used for representing Greek alphabets (e.g.,  X  &amp;agr;  X  for  X  ) were converted to the corresponding English spellings (e.g., alpha ) in advance to facilitate gene name identification.

The training data were used for tuning several parameters includ-ing the number of k neighbors and per-class thresholds t c tion (6) and were used as pre-labeled instances for k NN to classify the test data. Following the TREC Genomics Track, we used micro-averaged F score as an evaluation metric for GO annotation, so as to make our results comparable with the o ffi cial evaluation. F 1 the harmonic mean of precision and recall as in Equation (7).
Precision = where classes are biological process (BP), cellular component (CC), and molecular function (MF) and do not include negative (NEG).
Our proposed framework for GO annotation was applied to the test data, where the per-class thresholds t c in Equation (6) and other parameters including the number of k neighbors were optimized to maximize F 1 for each term weighting scheme using the training data. For instance, for TFCHI 2 , k was set to 140 and t to 1276, 936, and 1790 for BP, CC, and MF, respectively. Table 1 compares our results (TFIDF, TFCHI 1 , and TFCHI 2 ) on the test data and the representative results from the TREC o ffi cial evalua-tion.
 Table 1: The TREC o ffi cial results and our results for GO do-main code annotation (on the test data set).

Despite the simplicity of our method, it performed quite well, es-pecially, TFIDF and TFCHI 2 , as compared with the o ffi cial results. In the following sections, we take a closer look at major features or components of our framework and empirically investigate their contribution.
We have made a number of arbitrary decisions in developing our framework. To investigate the e ff ectiveness, we conducted several experiments with various di ff erent settings. Particularly, we were interested if the features or components below had made any im-pact.
Table 2 shows the best possible F 1 scores for each of the experi-mental settings above, where the training data were classified using leave-one-out cross-validation, while the test data were classified using the training data as before. Note that the bottom row  X  X e-fault X  used the same setting as TFCHI 2 in Table 1 but shows higher F 1 than TFCHI 2 , because threshold t c for k NN was optimized on the test data for the purpose of cross-setting comparison. Table 2: Results for alternative settings. Numbers in parenthe-ses under  X  X verage X  indicate percent increase / decrease relative to  X  X efault X .
 gene name identification severely deteriorated the performance on both of the training and test data sets. This supports our observa-tion that gene names are often written in slightly di ff erent forms from their canonical ones (database entries). Thus, flexible name matching schemes such as the one tested here are needed in order to exhaustively locate gene name occurrences. A possible drawback of approximate word matching is that it may recognize irrelevant word sequences as gene names (i.e., false positives), leading to an inclusion of irrelevant text fragments into the representation of the target gene. However, the influence can be minimized by tuning the threshold (and parameters) for the word-overlap score defined in Equation (1). According to our experiments, a threshold of 0.3 (which was used for our experiments) constantly yielded the best performance.
 Gene name dictionary. Not using the gene name dictionary also deteriorated classification both on the training and test data by 24.5% on average. It verifies that gene name expansion using the dictionary did help to identify text fragments relevant to the target gene, even though the dictionary was automatically compiled without manual curation.
 the use of glossary and keyword fields to search for gene synonyms was not found helpful for GO annotation. There was little or no di ff erence between the F 1 scores produced with and without the use of the fields. Close examination revealed that these fields hardly provided information regarding gene synonyms and thus had little e ff ect on the classification performance. To be exact, there were only 15 pairs of gene and synonyms found in these fields out of 882 articles in the training and test data sets.
 MeSH terms. Similarly to the case of glossary and keyword fields, the F 1 scores show little or no di ff erence between the set-tings where MeSH terms were used (Default) and unused (MeSH-unused). However, the di ff erence becomes more apparent when looking at precision and recall. On the test data, Default and MeSH-unused yielded nearly equal precision (0.503 and 0.509, respec-tively), whereas using MeSH terms (Default) achieved a recall ap-proximately 5% higher than MeSH-unused (0.847 and 0.808, re-spectively). It suggests that the inclusion of MeSH terms led to pre-dict more potential classes for a given gene (which raised recall), but also produced some amount of false positives (which slightly decreased precision). The lower precision may be due to the fact that MeSH terms are not gene-specific.
 Unit of extraction. Four di ff erent units were tested in extract-ing text fragments. In short, as going from G (only sentences con-taining target genes) to ART (entire articles) in Table 2, more text was extracted for document representation. As can be seen, there is a trend that F 1 slightly increases from G to P + G + S (sentences containing target genes plus immediately preceding and succeed-ing sentences) and then decreases when entire articles were used (i.e., ART ) compared to Default. However, we should not overlook that, surprisingly, ART performed comparably to Default on the test data, suggesting that our framework to use only text fragments containing target genes is not necessarily very e ff ective. Possible reasons are that (a) target genes were found everywhere in associ-ated articles so that almost all paragraphs were extracted, making little di ff erence whether entire articles or paragraphs were used; (b) not many articles were associated with multiple genes, and thus gene-specific document representation (such as ours) was not very important for the test data; and (c) multiple genes associated with single articles actually had almost the same classes. We investi-gated each possibility by comparing the training and test data but there was no noticeable di ff erence found between them. To closely examine the e ff ects of using paragraphs, we plotted recall-precision curves by varying the threshold for k NN (where the same thresh-olds were applied to all classes) as shown in Figure 3.
 The top two curves were obtained on the test data and the bottom two were based on the training. Although it is less apparent com-pared to the case of training data, it can be seen that, overall, using paragraphs (shown as solid lines) marginally improved the perfor-mance also on the test data.
Our current framework, to some extent, takes into account the logical structure of input in a sense that it makes use of paragraph boundaries in extracting text fragments containing target genes. However, it does not consider or distinguish the structure of an article, e.g., sections. Such information may be useful for GO an-notation because di ff erent parts of articles may have di Figure 3: The relation between recall and precision where ei-ther selected paragraphs or entire articles were used for docu-ment representation. portance with respect to GO annotation. For example, result and conclusion sections may be more relevant to GO annotation as they usually report findings from experiments. Therefore, we exam-ined how useful the individual sections were for GO annotation by using only one section at a time from which gene-bearing para-graphs were extracted. Specifically, we focused on the following sections: abstract, introduction, procedures, methods, and results. Both discussion and conclusion sections were regarded as result sections since they are sometimes not clearly separated from results (e.g.,  X  X esults and Discussion X  section). Incidentally, these sec-tions were identified based on section names annotated by SGML tags.

Figure 4 shows a histogram for F 1 scores produced using single sections on the training data, where we include results from the use of only titles and only MeSH terms for comparison. The rightmost bar  X  X ll X  used all the sections including MeSH, which corresponds to  X  X efault X  in Table 2.
 Figure 4: Results produced by individual sections.  X  X ll X  used all the sections. Percentages above bars indicate the respective proportions to  X  X ll X .

Surprisingly, the Result section alone yielded almost as good F as All, followed by Abs (abstract), MeSH, Proc (procedures), and so on. On the other hand, the Method sections showed the least performance for GO annotation. Although not presented here, ex-periments on the test data also showed similar results. The framework we described above aimed at GO annotation. However, it can be also applied to another task from the TREC Genomics Track, i.e., the triage task (see Section 1). In brief, the triage task is to determine if an input article contains experimen-tal evidence that warrants GO annotation, where no particular gene is specified. This task can be naturally regarded as a binary text categorization problem.

In terms of text categorization, a primary di ff erence between GO annotation tackled in the previous sections and the triage task is that the former takes a pair of article and gene as input, whereas the latter takes only an article. As input is not gene-specific, the triage task could simply rely on an entire article for document represen-tation without the necessity to locate the text fragments containing a particular gene. Yet, because the triage decision must be made in consideration of the genes mentioned in a given article, our frame-work to use only gene-bearing paragraphs may be more appropri-ate. Thus, we adapted our system to extract paragraphs that were likely to contain any gene names identified by the gene name rec-ognizer YAGI [15]. Note that MeSH terms associated with a given article were also included as features as in GO annotation.
We used the same methods described in Section 2 for document representation and classification except the followings:
We used the TREC data sets provided for the triage task, which was composed of 5,837 full text articles for training (375 positives and 5,462 negatives) and 6,043 for test (420 positives and 5,623 negatives). As is the case with GO annotation, the training data were used for tuning parameters and were used as pre-labeled in-stances for k NN to classify the test data. Specifically, the number of neighbors k and the value of threshold t POS were set to 160 and 93.4, respectively, which produced the best result in normalized utility measure (explained next) on the training data.

For the evaluation measure, normalized utility measure U norm fined below was used according to the TREC evaluation.
 where TP and FP denote the number of articles correctly identified as positive (true positive) and the number of articles falsely identified as positive (false positive), respectively.
Table 3 compares our results with the representative results from the TREC o ffi cial evaluation, where precision and recall results are also presented.
 Table 3: The TREC o ffi cial results and our results for the triage task (on the test data set).

Our framework with the term weighting scheme TFCHI 1 com-pared favorably with the best performing system reported at TREC, while TFIDF did not perform as well. This is mainly because the TFIDF scheme could not assign an appropriate (high) weight to the MeSH term  X  Mice  X  since it appeared in many documents (leading to a low IDF value). It was reported that a simple rule which clas-sifies articles annotated with the MeSH term Mice as positive and those without it as negative could achieve nearly as good perfor-mance as the best reported result [1].

Unlike TFIDF, TFCHI considered word distributions across dif-ferent classes and was able to assign higher weights even to the terms that appeared in many documents but almost only within a class, such as Mice in this particular data sets. To contrast the dif-for corresponding IDF and  X  2 as shown in Figure 5, where MeSH terms are indicated by capitalization. As can be seen, high- X  2 words, such as Mice , Animals , (the stem for embryonic), and knockout , were not necessarily as-signed high IDF values. Interestingly, the correlation coe IDF and  X  2 turned out to be  X  0.13, which is usually strongly and positively correlated as empirically known in the text categoriza-tion literature [19]. The result suggests that TFIDF, which is often used for text categorization, is not necessarily optimum depending on the characteristics of the target data. This supports the idea of the supervised term weighting schemes [2] that class-based term weights (e.g., chi-square statistics) is more appropriate for classifi-cation.

It may be also possible, however, that our framework with TFCHI performed well solely because of the notably high value of ciated with the MeSH term Mice (remember that simple heuristics using Mice could perform very well). To investigate, we applied the TFCHI 1 scheme to hypothetical test data where the MeSH term Mice was completely removed. The resulting normalized utility score was 0.548, which outperforms the TFIDF scheme in Table 3 and is still comparable to the second best system [4] (which pro-duced a U norm of 0.549) in the TREC evaluation.
This section discusses representative work by other researchers for the GO annotation and triage tasks.

For GO annotation, Settles et al. [16] developed a two-tier clas-sification framework using Na  X   X ve Bayes (NB) classifiers and Max-imum Entropy (ME) models with several external resources and specialized features. They exploited the structure of articles and distinguished six section types (such as introduction and discus-sion) as a unit of classification. They created an NB classifier for each section and the output probabilities of the NB classifiers were then combined using ME models which di ff erently weighted each of the section types and classes. The features used for the NB classifiers included not only words from body text but also syn-tactic patterns and what they call informative terms. The syntactic patterns are frequent patterns for subjects and direct objects (e.g.,  X  X ranslation of X  X ) automatically collected from training data us-ing a shallow parser. The informative words were word n -grams (1  X  n  X  3) having high chi-square statistics. To supplement the rela-tively small size of the training data provided by TREC, they used external resources including the BioCreAtIvE data set and MED-LINE abstracts with which specific genes and GO codes were asso-ciated in existing databases other than MGI. The reported F was 0.514, which is 14% lower than our best score reported here. The di ff erence is presumably due to the fact that their system did not employ gene name expansion and approximate word matching which we found highly important for GO annotation.

For the triage task, Dayanik et al. [1] applied Bayesian logistic regression (BLR) models, which estimate a probability that an in-put belongs to a specific class. For document representation, they used MeSH terms from the MEDLINE database in addition to in-put articles. Their best result was achieved by applying the follow-ing configuration. They used only title, abstract, and MeSH terms for features and applied the conventional TFIDF term weighting scheme, and proposed a two-stage classifier which assigned nega-tive to all articles not indexed with the MeSH term Mice and classi-fied those indexed with Mice by using BLR. The reported normal-ized utility score is 0.641. In spite of using TFIDF, which yielded suboptimal results in our experiments, their method outperformed other TREC participants.
This paper presented our work on automating GO domain code annotation. We approached this task by treating it as a text cate-gorization problem and adopted a variant of k NN classifiers. To apply k NN, we first represented each input,  X  article, gene by a term vector, where terms were collected from text fragments (paragraphs) containing the target gene. To exhaustively locate the gene name occurrences, we took advantage of existing databases to automatically compile a gene name synonym dictionary and pre-processed both gene names and text to tolerate minor di ff between them. In addition, we utilized approximate word match-ing to identify gene occurrences to deal with other irregular forms of the gene names. The collected words were then fed to feature selection using chi-square statistics, which were re-used for term weights adopting supervised term weighting schemes. We evalu-ated the proposed framework on the TREC Genomics Track data sets and showed that, overall, our method performed the best com-pared with the TREC o ffi cial evaluation. Further analyses revealed that the flexible gene name matching used in conjunction with the gene name dictionary was notably e ff ective. Another finding is that the result sections of articles contributed the most for GO annota-tion. It was also demonstrated that our framework was successfully applied to a related but di ff erent problem, the triage task, producing a normalized utility score of 0.651 which is comparable to the best reported performance at the recently held TREC. In addition, the TFIDF scheme was found suboptimal for this particular task and data sets.

For future research, we are planning to explore a better use of structure of articles (e.g., sections) and local context around the target genes. Such information may be incorporated into the current framework by way of term weights. Another direction is to extend our work to more advanced, realistic settings. For example, in the real-world GO annotation, genes are not given in advance. Taking only articles as input without specific genes would be an interesting challenge.
This project is partially supported by the NSF grant ENABLE #0333623. [1] A. Dayanik, D. Fradkin, A. Genkin, P. Kantor, D. D. Lewis, [2] Franca Debole and Fabrizio Sebastiani. Supervised term [3] Sergei Egorov, Anton Yuryev, and Nikolai Daraselia. A [4] Sumio Fujita. Revisiting again document length hypotheses [5] Daniel Hanisch, Juliane Fluck, Heinz-Theodor Mevissen, [6] William Hersh. Text retrieval conference (TREC) genomics [7] William Hersh. Report on TREC 2003 genomics track [8] W.R. Hersh, R.T. Bhuptiraju, L. Ross, A.M. Cohen, and D.F. [9] Lynette Hirschman, Jong C. Park, Jun-ichi Tsujii, Limsoon [10] Julie Beth Lovins. Development of a stemming algorithm. [11] Claire O X  X onovan, Maria Jesus Martin, Alexandre Gattiker, [12] Kim D. Pruitt and Donna R. Maglott. RefSeq and [13] Gerard Salton and Michael J. McGill. Introduction to [14] Ariel S. Schwartz and Marti A. Hearst. A simple algorithm [15] Burr Settles. Biomedical named entity recognition using [16] Burr Settles and Mark Craven. Exploiting zone information, [17] Hagit Shatkay and Ronen Feldman. Mining the biomedical [18] Yiming Yang and Xin Liu. A re-examination of text [19] Yiming Yang and Jan O. Pedersen. A comparative study on
