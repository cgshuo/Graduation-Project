 Imagine you are a social network user who wants to search, in a list of potential candidates, for the best candidate for a job on the basis of their PageRank-induced importance ranking. Is it possible to compute this ranking for a low cost, by visiting only small subnetworks around the nodes that represent each candidate? The fundamental problem underpinning this question, i.e. computing locally the Page-Rank ranking of k nodes in an n -node graph, was first raised by Chen et al. (CIKM 2004) and then restated by Bar-Yossef and Mashiach (CIKM 2008). In this paper we formalize and provide the first analysis of the problem, proving that any local algorithm that computes a correct ranking must take into consideration  X ( top k nodes of the graph, even if their PageRank scores are  X  X ell separated X , and even if the algorithm is randomized (and we prove a stronger  X ( n ) bound for deterministic algo-rithms). Experiments carried out on large, publicly available crawls of the web and of a social network show that also in practice the fraction of the graph to be visited to compute the ranking may be considerable, both for algorithms that are always correct and for algorithms that employ (efficient) local score approximations.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval; G.2.2 [Discrete Mathematics]: Graph Theory  X  Gra-ph algorithms General Terms: Theory, Algorithms, Experimentation Keywords: IR theory: ranking, link and graph mining, web and social media search, local computation, PageRank
This work tackles the problem of computing locally the ranking induced by PageRank on k target nodes of an n -node graph, studying if the correct ranking can be obtained for a low cost, i.e. by visiting only small subgraphs around those target nodes. Previous work shows that computing PageRank scores locally is infeasible (i.e. the cost is not bounded by polylog( n )) in the worst case; one could think, however, that the ranking provided by the algorithm (which is the only aspect that matters for many applications) might be obtained  X  X ore easily X  than the output scores: after all, the correct ranking can be reached with scores even vastly different from the correct ones. Instead, it turns out that the ranking problem is, in a sense, as non-local as the score com-putation problem: in a nutshell, we prove that no determin-istic algorithm exists which guarantees the correct ranking by visiting less than a number of nodes linear in the size of the input graph, and no randomized algorithm exists which guarantees the correct ranking by visiting in expectation less than a number of nodes proportional to the square root of the size of the input graph  X  even if only the top ranked nodes are examined and if their PageRank scores are well separated. Experimental results show that, in a large crawl of the .it web graph, the estimated cost of local ranking is substantially smaller than the graph size (but still very high), while in a smaller crawl of the LiveJournal friendship graph the estimated cost is an important fraction of the size of the graph and even higher than in the .it web graph.
This introductory section motivates our work and pro-vides related work (Subsection 1.1), and then outlines the structure of the paper (Subsection 1.2).
This paper ideally builds a bridge between the research on the ranking induced by PageRank and that on the local computation of PageRank.

The first research stream arises from the consideration that PageRank [10], originally proposed as a means to infer the importance of web pages from the topological structure of the web graph, is nowadays a reference algorithm in com-puter science, especially for the information retrieval, knowl-edge management and data mining communities. Paradoxi-cally, while its utility as a web search algorithm is decreasing in favour of new techniques such as clickthrough-based mea-sures, its importance is still growing, due to a large number of applications in diverse fields: for instance, in web crawl-ing [13], web spam detection [19], social network mining [20], ranking in databases [17], structural re-ranking [23], opinion mining [16], word sense disambiguation [29], credit and repu-tation systems [22], gene ranking [27] and bibliometrics [32]; indeed, PageRank has been rated among the top 10 algo-rithms in data mining [31]. This large and growing number of applications suggests to study PageRank in the abstract, considering it as a graph algorithm which accepts a graph in input and provides a score to each node of the graph as out-put. While in the original web application these scores were combined with traditional text-based information retrieval scores, in most of the other applications they are directly used to rank the nodes of the input graph. For instance, in web crawling , PageRank is used to decide  X  X n what order a crawler should visit the URLs it has seen, in order to obtain more  X  X mportant X  pages first X  [13]. In bioinformatics , modi-fied versions of PageRank are used to rank genes in order of their  X  X mportance X , offering  X  X n improved ranking of genes compared to pure expression change rankings X  [27]. This naturally moved research towards the ranking induced by PageRank, investigating its stability under graph perturba-tions [8, 25], the convergence of its iterative computation [28, 30], and its dependence on damping factor variations [26, 9].
On the other hand, a vast body of research has been car-ried out on the local computation of PageRank scores. This research is motivated by the fact that the input graph is of-ten accessible only for a high cost  X  or one may simply have access only to the local structure of the graph, without any knowledge of the full topology. The first case happens, for instance, when the input graph is too large to fit completely in one specific level of the memory hierarchy and must be stored in larger (and slower) levels, where (random) accesses have a significant cost [21]. The second case happens, for in-stance, when a web user can access the web graph only by querying a  X  X ink server X , e.g. using the  X  link:  X  option pro-vided by some search engines, or when a social network user has limited access to the friendship graph due to privacy settings (see [18]). In all these cases it would be useful to compute PageRank locally, i.e. exploiting only the structure of a small subgraph around the target nodes  X  indeed, this is the approach of [1]. To this end, [12] provides heuristics to locally approximate PageRank scores, [2] gives a well-founded local algorithm to approximate the contributions of the nodes of a graph to the PageRank score of a target node, while [3] proves the infeasibility of locally approxi-mating the PageRank scores in the worst case, and provides lower bounds for deterministic and randomized local score approximation algorithms.

Neither theoretical nor experimental work has ever been carried out on the local computation of PageRank-induced ranking  X  indeed, the problem itself has never been formally defined, even if previous work [12, 3] has repeatedly sug-gested the need for this kind of study.
This work formally defines the problem of locally comput-ing PageRank-induced rankings and provides the first theo-retical and experimental results on this subject. The paper is organized as follows.

Section 2 gives a short overview of the PageRank algo-rithm with an emphasis on the mathematical formulation which allows a local computation of the output scores.
Section 3 gives an informal introduction to the problem in hand, and proves that the reference algorithm used to locally approximate PageRank scores can not guarantee a correct ranking when visiting less than a number of nodes linear in the size of the input graph, or may even never stabilize.
Section 4 formally defines the problem of local ranking, and Section 5 provides the main theoretical results of the paper, showing that the results of Section 3 extend to every algorithm which attempts a local computation of PageRank-induced rankings. In short, it proves that the local computa-tion of PageRank-induced ranking is infeasible in the worst case both for deterministic and for randomized algorithms  X  even if only the top ranked nodes are examined and if their PageRank scores are well separated.

Section 6 describes two experiments suggesting that the cost of local ranking in real graphs is actually high and strongly dependent on the input graph.

Section 7 discusses our results and their consequences, and states a few open problems before concluding with the bib-liography.
Let G = ( V,A ) be an n -node graph with no dangling nodes (i.e. nodes with no outgoing arcs); if dangling nodes originally exist, the graph is preprocessed by adding to each of them outgoing arcs towards each node of the graph. An arc from u to v is interpreted as a conferral of importance from u to v . The entries of the transition probability matrix M = [ m i,j ] of G are Let the damping factor  X  be a real number in [0 , 1) and let where U is a matrix whose entries equal 1 /n ; T can be seen as the transition probability matrix of a Markov chain. The PageRank row vector P = [ P ( v 1 ) ,P ( v 2 ) ,...,P ( v fined as the limit probability vector of this Markov chain, and can be obtained either as the unique probability vector which is the solution of the linear system P = PT or as the limit of the iteration P ( n ) = P ( n  X  1) T , where the starting vector P (0) can be any probability vector. When consid-ering a local computation of PageRank, however, none of these roads can be followed, since the entire graph struc-ture is not available; in this case, straightforward algebraic manipulations are exploited which take to the formula: where 1 = [1 , 1 ,..., 1]. This leads to the following formula to compute the PageRank score of a node v : where the influence inf  X  ( z,v ) of z on v gives the probability of ending in v by following a chain of  X  arcs from z . There-fore, the score of node v can be seen as the weighted sum of the influences of its ancestors at each layer  X  , or as the sum of their contributions.
This section investigates the brute force algorithm, one of the simplest methods to locally approximate the PageRank score of a single node, in light of the local ranking prob-lem. Although unlikely to be among the best local score approximation algorithms to lever on in the local ranking problem (see Section 6), the brute force is still a reference algorithm for at least two reasons. First, it is the basis of vir-tually any known local score approximation algorithm (such as those proposed in [3] and [12]) which in turn are natural bases for local ranking algorithms. Second, it naturally de-rives from the expression of PageRank given by Equation (1) that turns out to be very useful in the analysis of those algo-rithms. Thus it is important to understand why the brute force may fail to be the basis for a good local ranking al-gorithm  X  one that, ideally, should incur a limited cost and return a correct result. Indeed, as we shall prove after a short review, a naive brute force-based ranking algorithm may incur extremely high costs to guarantee a correct result (Theorem 1) or oscillate indefinitely between two  X  X pposite X  results (Theorem 2).

Given an n -node graph G and a target node v  X  G , the brute force algorithm performs a breadth-first visit of the an-cestors of v , querying at iteration  X  all the ancestors of layer  X  and cumulating their influence on v weighted by 1  X   X  n After iteration ` , the resulting brute force score at layer ` is If graph G has no dangling nodes, P ( ` ) ( v ) converges from below to the PageRank score of v as given by Equation (1). A natural way to compute the relative ranking of two target nodes u and v is then to compare their brute force scores P ( ` ) ( u ) and P ( ` ) ( v ) once ` has reached a sufficiently large threshold ` 0 . Unfortunately, such a threshold is unknown a priori, and this may lead to a premature halt and an incor-rect ranking. Indeed we prove that, for any given ` 0 , there exist graphs where the  X  X ecisive X  layer is at depth ` while the ranking induced by the output of the brute force algorithm at every iteration up to ` 0 is the complete rever-sal of the correct ranking. Before stating the result formally in the next theorem, we provide a lemma that will greatly simplify our proofs.

Lemma 1. Let G be a graph, v  X  G a dangling node, and build G 0 adding a self-loop ( v,v ) to G . Let B ( v ) and B be the brute force scores of v computed respectively on G and G
Proof. Every path from a generic node z to v in G 0 is the unique concatenation of a path from z to v in G and t  X  0 self-loops ( v,v ). Conversely, every path p : z  X  v in G generates, for t = 0 ,...,  X  , the paths from z to v in G that are the concatenation of p : z  X  v and t self-loops ( v,v ) that damp the contribution of z by a factor  X  t . Thus the expression of B 0 ( v ) given by Equation (2) can be rewritten as:
Lemma 1 proves that one can compute the brute force score of a node whose only outgoing arc forms a self-loop by disregarding it at each iteration and multiplying the result by a constant factor. We can now state Theorem 1.

Theorem 1. For any k  X  2 , any  X   X  (0 , 1) and any ` 0 &gt; 0 there exists a graph where the ranking induced by P ( ` ) on the top k nodes v 1 ,...,v k is:
Proof. Due to space limitations, we give a sketch of the proof exhibiting a graph G that satisfies the statement. G consists of k subgraphs G 1 ,...,G k . Subgraph G i (Figure 1) contains node v i and its self-loop (making the graph free of dangling nodes), k  X  i + 1 orphan nodes that have v i as their sole child, and ` 0 + 1 layers of ancestors structured as a tree of depth ` 0 with root node v i and indegree d ( d will be computed below) pointed by an additional ` 0 + 1-th layer of im orphans ( m will be computed below). Figure 1: Subgraph G i (Theorem 1) containing node v and all its ancestors up to layer ` 0 + 1 .

Consider v i and v j for 1  X  i &lt; j  X  k . For `  X  ` 0 node v i receives the contribution of j  X  i parents more than v . But for `  X  ` 0 + 1, node v j receives from layer ` 0 + 1 the contribution of ( j  X  i ) m ancestors more than v i . Thus, intuitively, v j receives a lower contribution from the first ` layers but a higher contribution from layer ` 0 + 1. Indeed, ` and P ` ( v i ) &lt; P ` ( v j ) for `  X  ` 0 + 1.

To make v 1 ,...,v k the top k nodes in the graph, balance the im arcs from layer ` 0 + 1 so that the number of parents of any ancestor at layer ` 0 is at most d im d ` sufficiently large d , is no more than d . Note that v 1 have more than d parents and, in general, at any layer they have more ancestors (with outdegree 1, which translates to a higher contribution) than any other node in the graph  X  thus, they are the top k .

By Theorem 1, for any ` 0 &gt; 0 there exist graphs where, to compute the correct relative ranking of the top k nodes, the brute force algorithm not only must take into account at least the first ` 0 + 1 layers, but halting at any layer below ` +1 yields the same complete reversal of the correct ranking  X  a misleading  X  X tability X  condition.

Although to correctly rank the top k nodes of the graph of Theorem 1 the brute force algorithm incurs a very high cost (exponential in ` and linear in n ), it still converges in a finite number of iterations. Unfortunately, the situation can worsen in presence of cycles that produce periodical oscilla-tions in the relative ranking of nodes. The following theorem proves that in this case the number of iterations required to converge to a stable ranking may be unbounded:
Theorem 2. For any even integer k &gt; 1 and any rational  X   X  (0 , 1) there exists a graph where the ranking induced by P ( ` ) on the top k nodes v 1 ,...,v k satisfies, for any i  X  [1 , any j  X  [ k 2 + 1 ,k ] , and any `  X  0 :
Proof. Due to space limitations, we give a sketch of the proof exhibiting a graph G that satisfies the statement. G consists of k/ 2 identical subgraphs G 1 ,...,G k/ 2 . Subgraph G i (Figure 2) contains target nodes v i and v i + k/ 2 , two nodes that are the sole children of v i and the sole parents of v and a set W i of p nodes with q children each ( v i being one of them). Figure 2: Subgraph G i (Theorem 2) has a cycle that causes a perpetual oscillation in the relative ranking given by the brute force algorithm.

Intuitively, both target nodes receive the contribution of nodes in W i , but v i only at layers `  X  1 mod 3 and v i + k/ 2 only at layers `  X  0 mod 3 for ` &gt; 0; this makes the rel-ative ranking induced by their brute force scores at layer ` oscillate with ` . More formally, choose p and q such that the overall contribution of nodes in W i is p/q = 1 / (1 +  X  ); this is possible since  X  is rational by hypothesis. Plugging this value into the expressions of the brute force scores of v v i + k/ 2 at layer ` given by Equation (2), it is easy to see that P Furthermore, the target nodes are the top k ranked in the graph, since each child of v i has a brute force score smaller than that of both v i and v i + k/ 2 at any layer ` &gt; 1, while nodes of set W i are orphans and thus always have the lowest brute force scores in the graph.

By Theorem 2, there exist graphs where the relative rank-ing given by the brute force algorithm on the top k nodes never converges nor shows stability. Note that, depending on  X  , p and q , the size of the graph in Figure 2 can become arbitrarily large but also range in the tens  X  a situation where the brute force algorithm presents an excellent cost performance, yet its output oscillates indefinitely.
Although by Theorems 1 and 2 the brute force algorithm may in theory become impractical, other algorithms could perform better for at least three reasons. First, they could visit the ancestors of the target nodes using a strategy that includes highly-contributing ancestors earlier than the sim-ple breadth-first does. Second, they could compute the ap-proximate PageRank scores more accurately, for example taking into account the cycles in the graph to eliminate os-cillations and include  X  X n one shot X  the total contribution of an ancestor appearing in an infinite number of layers. Third, as the number of visited nodes increases, the approximation error on the PageRank scores drops (eventually going be-yond the resolution of a physical machine) and the correct ranking of the target nodes may become apparent after just a few steps, especially if their scores are not too close.
These observations raise the necessity to discuss and for-mally define the problem of local ranking. The next section is fully devoted to this task.
This section formally restates the local ranking problem informally introduced in Section 1, taking into account the observations raised at the end of Section 3.

Formally, a local algorithm is an algorithm that has no direct access to (the arcs of) a graph G , but must instead query a link server for G that accepts (the ID of) a node v in input and returns (the IDs of) the parents and children of v in output. The local ranking problem consists in rank-ing a subset of nodes of a graph using only local algorithms. Since the major bottleneck of local algorithms is the com-munication with the link server, their cost can be defined as the number of queries performed. The cost of locally rank-ing two target nodes u,v  X  G is then the minimum cost incurred by any correct algorithm A to rank u and v .
In this paper, the local ranking problem is considered in a PageRank context. Formally, given a graph G , a damp-ing factor  X  , and k target nodes v 1 ,...,v k  X  G , the local ranking problem requires to rank the target nodes in nonin-creasing order of their PageRank scores (with ties broken ar-bitrarily) using only local algorithms. In many applications, however, only the top PageRank scores really matter; fur-thermore, often only the ranking induced by well-separated scores matters  X  if two scores are very close to each other, they could be considered equivalent (after all, PageRank it-self gives an approximate model of the reality), and they are practically indistinguishable if their difference is smaller than the resolution of a modern machine. To deal with this last issue, we modify the problem and require to rank all and only the target nodes whose relative PageRank score difference is not less than a given . Formally, given a graph G , a damping factor  X  , and k target nodes v 1 ,...,v k  X  G , the local -ranking problem requires to rank each pair u,v of target nodes with P ( u ) /P ( v )  X  1 + in decreasing order of their PageRank scores using only local algorithms. The definitions of cost of an algorithm A and cost of local rank-ing given above can be ported in this context with obvious modifications.

It is thus natural to ask how high is the cost of local -ranking  X  even considering only the top nodes of a graph. In the next section we prove theoretical lower bounds on this cost, for both randomized and deterministic algorithms.
In general, locally computing the exact PageRank score of a single node may require a number of queries proportional to the size n of the whole graph. More surprisingly, a very high number of queries may be required even to compute an -approximation of the PageRank score of a single node, i.e. a value between 1  X  and 1 + times the score itself. In particular (see [3]), to -approximate the score of a node for a reasonable constant , any deterministic local algorithm incurs a cost of  X ( n ) queries in the worst case, while any randomized (with deterministic cost) Monte Carlo local al-gorithm with constant confidence incurs an expected cost of  X (  X 
It is not known, however, if the same bounds apply to the problem of locally computing the relative -ranking of two or more target nodes, as described in Section 4. We prove that this is indeed the case: the following Theorem 3 shows that, in the worst case, locally ranking the top k nodes of a graph requires  X (  X  p n/ ) queries for both Las Vegas algorithms and Monte Carlo algorithms with constant confidence, while Theorem 4 strengthens the bound to  X ( n ) for deterministic algorithms.

Theorem 3. Choose integers k &gt; 1 and n 0  X  6 k 3 , a damping factor  X   X  (0 , 1) , and an  X  h  X  2 k 2 4 n The proof is based on a reduction from the 1-OR problem. In this problem, the instance is a binary string x of length m  X  1 such that either all its bits are 0, in which case the solution is 0, or exactly one is 1, in which case the solution is 1  X  i.e., the solution is k x k . Local algorithms for 1-OR can retrieve the value of a bit only via a bit server . We use Yao X  X  principle [33] to show that the expected cost of any Las Vegas randomized local algorithm for 1-OR is at least in the worst case:
Lemma 2. The expected worst-case cost of any Las Vegas randomized local algorithm for 1 -OR is at least m 2 .
Proof of Lemma 2. Any (Las Vegas) algorithm issues a sequence of queries and stops either when it finds a 1 or every bit has been queried (every algorithm behaving dif-ferently would produce an incorrect result and/or incur un-necessary costs). Consider an input probability distribution where each of the m + 1 possible m -bit strings ( m of them contain exactly a 1, and one contains only zeros) has prob-ability 1 m +1 . Whatever the sequence of queries, the proba-bility that the j -th query returns 1 (and thus that the cost is j ) is 1 m +1 . Therefore the expected cost is at least By Yao X  X  principle, this is a bound on the expected number of queries performed by any randomized Las Vegas algo-rithm on its worst-case input.
 Proof of Theorem 3. Let A be a randomized Monte Carlo (Las Vegas) local algorithm that ranks the top k nodes of a graph performing (in expectation) S A queries to the link server; we use A to build a randomized Monte Carlo (Las Vegas) local algorithm B that solves k  X  1 independent instances of 1-OR performing (in expectation) S B  X  S queries to the bit server. Lemma 2 gives a lower bound on (the expected value of) S B , and thus on (the expected value of) S A .

Let x 1 ,..., x k  X  1 be k  X  1 m -bit instances of 1-OR and b a positive integer ( m and b will be computed below). We build a link server that lets A run on a graph G consisting of k disjoint subgraphs G 0 ,G 1 ,...,G k  X  1 . For i = 1 ,...,k  X  1, subgraph G i (Figure 3) contains the target node v i and its self-loop, m + 1 nodes u 0 i ,...,u m i that have v i as their sole child, ib nodes w 0 i ,...,w ib  X  1 i that have u 0 i as their sole child and, for j = 1 ,...,m , kb nodes w jkb i ,...,w ( j +1) kb  X  1 have u j i as their sole child if x i ( j ) = 1 or a self-loop if x 0. Subgraph G 0 contains the reference target node v 0 and its self-loop, m + 1 nodes u 0 0 ,...,u m 0 that have v 0 sole child and kb nodes w 0 0 ,...,w kb  X  1 0 whose sole child is u Figure 3: Subgraph G i (Theorem 3). Node u 0 i al-ways has exactly ib parents; node u j i has kb parents if x i ( j ) = 1 , else it has an associated group of kb  X  X is-connected parents X .

The link server for G is described by the following rules: 1. when queried for u j i with i,j  X  1, it queries the bit 2. when queried for w j i with i  X  1 and j  X  kb , it com-3. otherwise, it answers without querying the bit server. It is easy to see that this is a link server for the graph G . Given the output of algorithm A , algorithm B computes the solution of x i as follows. If A ranks v i lower than v 0 B returns 0; if A ranks v i higher than v 0 , then it returns 1. We prove that if A ranks correctly v 0 ,...,v k  X  1 then B solves correctly the k  X  1 instances of 1-OR.

Since G has no dangling nodes, by Lemma 1 the scores of the target nodes are 1 1  X   X  times the brute force score com-puted on the same nodes ignoring their self-loops. Thus, according to Equation (2) these scores become: By hypothesis, algorithm A ranks v 0 ,v 1 ,...,v k  X  1 ; in par-ticular, it computes the relative ranking of v 0 and v i = 1 ,...,k  X  1. But v 0 is ranked lower than v i if and only if k &lt; i + k k x i k , which is true if and only if k x versely, v i is ranked lower than v 0 if and only if i + k k x which is true if and only if k x i k = 0. Thus, if A ranks cor-rectly v 0 ,...,v k  X  1 , then B solves correctly the k  X  1 instances of 1-OR.

We now exhibit values of m and b such that G and the target nodes satisfy the thesis. In particular, choose It is immediate to verify that the size n of the graph is in  X ( mbk 2 ) which, substituting m and b , is in  X ( n 0 hypothesis n 0  X  6 k 3 guarantees also that b  X  1 and m  X  kb  X  2; therefore v 0 ,v 1 ,...,v k  X  1 are the top k nodes in the graph, since each of them has at least m + 1 &gt; kb parents and at least one grandparent  X  more than any other node in the graph. Furthermore, any two target nodes are -separated since one of them has the same number of parents but at least b grandparents more than the other, and thus their scores differ by  X  P  X   X  2 b n . This value, divided by the maximum possible score of a target node (obtained for i = k  X  1 and k x i k = 1), gives a lower bound on the score separation of any two target nodes:  X  P
P where the last inequality follows from kb  X  m and 1  X  m/ 2. Plugging in the above values for b and m we obtain: which proves that the target nodes are -separated.

We now compute the cost of locally ranking the target nodes v 1 ,...,v k . The link server performs at most one query to the bit server for each query performed by A , therefore S B  X  S A . If A (and thus B ) is a Las Vegas algorithm, Lemma 2 gives a worst-case expected cost of at least m/ 2 bit queries for each of the k  X  1 instances solved by B , and thus E [ S A ]  X  E [ S B ]  X  m 2 ( k  X  1)  X   X ( mk ). If A (and thus B ) is a Monte Carlo algorithm with constant confidence (note that B is correct if A is, thus the confidence of B is equal to at least the confidence of A ), a sensitivity argument for ran-domized algorithms with bounded error [11] gives a worst-case cost of  X ( m ) bit queries for each of the k  X  1 instances, and thus S A  X  S B  X   X ( mk ). In every case S A  X   X ( mk ) which, plugging in the above value for m and recalling that n 0  X   X ( n ), becomes: which concludes the proof.
 The bound for deterministic algorithms is even stronger. In-tuitively, an adversarial link server can adaptively build a worst-case graph by refusing to give enough information un-til  X ( n ) queries have been performed. More formally:
Theorem 4. Choose integers k &gt; 1 and n 0  X  k 2 , a damping factor  X   X  (0 , 1) , and an  X   X  2 20 k . For any de-terministic local algorithm A there exists a graph of size n  X   X ( n 0 ) where the top k nodes v 0 ,...,v k  X  1 are -separated and, to compute their relative ranking, A performs  X ( n ) queries.

Proof. Let A be a deterministic local algorithm that ranks the top k nodes of a graph performing S A queries to the link server; we exhibit an adversarial link server that lets A run on a worst-case graph G similar to that of Theorem 3 and consisting of k disjoint subgraphs G 0 ,G 1 ,...,G k  X  1 m and b be two positive integers (we compute them below). For i = 1 ,...,k  X  1, subgraph G i (Figure 4) contains target node v i and its self-loop, m + 1 nodes u 0 i ,...,u m i that have v as their sole child, ib nodes w 0 i ,...,w ib  X  1 i that have u have self-loops or, for some 1  X  j  X  m , have u j i as their sole child. Subgraph G 0 contains target node v 0 and its self-loop, m + 1 nodes u 0 0 ,...,u m 0 that have v 0 as their sole child, and kb nodes w 0 0 ,...,w kb  X  1 0 that have u 0 0 as their sole child. Figure 4: Subgraph G i (Theorem 4). Node u 0 i always may not have u j i as their sole child.
 The link server exploits the fact that the mapping of node IDs (i.e. the content of queries) to nodes in the graph is arbi-trary, and forces algorithm A to query almost all the nodes before giving enough information to compute a correct rank-ing. Indeed, the link server follows this simple rule: Note that this is a legitimate behaviour since each of the k subgraphs G 0 ,...,G k  X  1 has at least m nodes that are
We now exhibit values of m and b such that G and the target nodes satisfy the theorem. In particular, choose The size of the graph, n  X   X ( mk + k 2 b ), is thus in  X ( n The hypothesis n 0  X  k 2 guarantees that m  X  kb and b  X  1; thus v 0 ,...,v k  X  1 are the top k nodes in the graph, since at any layer they have more ancestors than any other node. It remains to prove that the target nodes are -separated. Since the number of parents and grandparents of v i is the same as in the graph of Theorem 3, and since kb  X  m , the difference between scores satisfies the same lower bound which proves that the target nodes are -separated.

Note that the link server can invalidate any ranking of the target nodes that A should output before querying at least mk nodes. Indeed, suppose that A has queried less than mk nodes and consider a target node v i such that one of its par-ents, u j i , has not been queried (one such node always exist). If A ranks v i higher than v 0 , the link server disconnects w i ,...,w u . The expression of P ( v i ) (see proof of Theorem 3) shows that in either case A gives an incorrect ranking. Therefore A must incur a cost of at least mk ; substituting m = n 0 and n 0  X   X ( n ), we conclude the proof: Note that, as the sizes of the graphs of Theorems 3 and 4 increase, the upper bound on the choice of falls within a small constant factor of  X  2 k , taking the lower bounds on the incurred costs within a small constant factor of, respectively,  X  kn (increasing to n/k for  X   X  2 k 2 n ) and n . Therefore, by Theorems 3 and 4 there exist graphs of size in the bil-lions (comparable to the estimated size of the web graph) where, assuming a  X  X tandard X  damping factor of 0 . 85, the PageRank scores of the top 10 nodes are  X  0 . 1-separated, which in absolute terms is orders of magnitude greater than the average PageRank score 1 n , yet any  X  X seful X  (i.e. with a reasonable confidence level) local -ranking algorithm incurs  X  100k queries (increasing to  X  100M for  X  10  X  7 ) in the randomized case and  X  1B queries in the deterministic case.
It is now clear that, at least in theory , a general efficient lo-cal ranking algorithm does not exist. In practice, real graphs may behave differently, and state-of-the-art algorithms may be able to exploit their properties to efficiently compute the correct ranking of the target nodes. The next section pro-vides experimental results to investigate this issue.
This section describes two experiments, both partially lev-ering on known score approximation algorithms ([3, 12]), aimed at estimating how far the theoretical lower bounds of Section 4 are in practice from the cost of local ranking on real graphs. Subsection 6.1 introduces the experimental setting, while Subsections 6.2 and 6.3 detail the two experi-ments and the results obtained.
We ran the experiments on two publicly available real graphs provided by the Laboratory for Web Algorithmics, University of Milan (see [5, 7]). The first is a large snapshot (over 40M nodes and 1150M arcs) of the 2004 .it web do-main; since the structure of the web graph is self-similar (i.e. fractal-like) [15], these experimental results naturally extend to the whole web graph. Furthermore, the .it graph is rel-atively isolated from the rest of the web because most pages are written in Italian, a language seldom used outside this domain; thus, this graph is well suited for link analysis ex-periments  X  carving it out of the web does not imply deleting many links. The second graph is a fairly large snapshot (over 5M nodes and 79M arcs) of the 2008 LiveJournal friendship graph, where nodes represent users and a directed arc from u to v means that u reputes v as a friend (the graph is not symmetric). Conferral of importance has a natural mean-ing in this graph, and its nature is completely different from that of the web, providing a wider experimental basis to test our theory. Furthermore, being a completely isolated graph, no arcs have been discarded by the crawling process.
Graph preprocessing deserves a special note. It is well known that PageRank treats dangling nodes as having vir-tual outgoing arcs towards all the nodes of the graph [24]. This makes it difficult to approximate PageRank scores lo-cally since the influence of these virtual arcs can only be guessed  X  the link server provides only the real arcs  X  unless their number is known a priori, in which case the correct scores can be computed applying a scaling factor that de-pends only on the fraction of dangling nodes in the graph [14, 6]. Surprisingly, the literature on local score approximation always neglects this issue. However, the estimation of such fraction is beyond the scope of this paper. This led us to eliminate all the dangling nodes by repeated pruning, as done in [12]. For both graphs, this removed less than 15% of the nodes and less than 6% of the arcs.

For the sake of clarity, in both experiments we focused on ranking pairs of target nodes, which we sampled among the top ranked nodes of the graphs. This latter choice has two reasons. First, in most practical applications only the top scores really matter. Second, nodes with a high score are likely to have many ancestors on many layers, and therefore are the best candidates to test the worst-case lower bounds provided by theorems of Section 4. We thus computed the exact PageRank scores of all the nodes of the graphs, using 100 iterations of the power method and a  X  X tandard X  value of 0 . 85 for  X  , and selected the top 10k nodes (i.e. less than the top 0 . 3%) from both graphs. Then we sampled 1000 pairs uniformly at random among all the pairs of nodes u,v that are (1 + , 1 + 2 )-separated, i.e. (1 + ) P ( v )  X  P ( u )  X  (1 + 2 ) P ( v ), for values of ranging from 0 . 01  X  2 0 differing by 1-2%) to 0 . 01  X  2 8 (scores differing by 256-512%) in doubling steps. On each sample we ran the experiments detailed in the following two subsections.
The goal of the first experiment is to estimate a lower bound on the cost of locally ranking -separated nodes as a function of . This bound can be seen as the size of the min-imal set of nodes that must be visited to compute correctly the relative ranking of two target nodes:
Definition 1. Let u,v be nodes of graph G and let A be a local ranking algorithm. The minimal set S A ( G,u,v ) for A contains the minimal number of nodes of G that A fetches to correctly rank u and v .
 In other words, S A ( G,u,v ) is the (not necessarily unique) smallest set of nodes such that A correctly ranks u and v if it fetches any set S  X  S A ( G,u,v ). Clearly, a general (i.e. valid for all algorithms) lower bound on the size of S A gives a lower bound on the cost of locally -ranking u and v in G . Note that, for any choice of G , u and v , there al-ways exists an algorithm A such that S A ( G,u,v ) =  X  . For example, a trivial algorithm returning a constant ranking could rank correctly some of the pairs of nodes in G ; un-fortunately, it would rank incorrectly all the remaining. It is therefore natural to restrict the definition to algorithms that provide a correct ranking for any pair of target nodes u and v of any possible graph G  X  in other words, algorithms that solve the -ranking problem. We further restrict to al-gorithms that compute approximate scores as the sum of the (known) contributions of the visited nodes on the target nodes, and thus do not overestimate the real score, similarly to existing algorithms (we conjecture that any algorithm that behaves differently would fail for some input G , u , v ). In this case, a(n ideal)  X  X erfect X  algorithm PER would build a set S PER ( G,u,v ) containing all and only the top c contrib-utors of the highest ranked node u for a sufficiently large c such that  X  P ( u ) = P z  X  S It is easy to see that, on any superset S  X  S PER algorithm PER estimates  X  P ( u )  X  P ( v ) and  X  P ( v )  X  P ( v ), and infers the correct ranking from the inequality  X  P ( u )  X   X  P ( v ), while any set smaller than S PER ( G,u,v ) would lead to  X  P ( u ) &lt; P ( v ) and to an incorrect result for some ( G,u,v ). Therefore the cardinality of S PER ( G,u,v ) is a lower bound to the cost of locally ranking u and v in G for any correct algorithm. Note that, since the contribution of an ancestor z is  X  times the average of the contributions of its children, at least one of them has a contribution not smaller than that of z . Therefore, if S PER ( G,u,v ) contains z , it also contains one of its children, and thus S PER ( G,u,v ) induces a connected graph which is in principle reachable by those (reasonable) algorithms that query only parents of already-queried nodes.
In practice, for each pair u , v we estimated their minimal set as follows. We collected the first 15 layers of the ances-tors of u , stopping earlier if their number reached a given threshold (0 . 02 n for the .it graph and 0 . 2 n for the Live-Journal graph). These ancestors induced a subgraph where we ran 40 iterations of the brute force algorithm with u as the target node. This yielded the contribution of each col-lected ancestor, which was used to sort them and build the estimated minimal set.

Figure 5 illustrates the average size of the estimated min-imal sets as a function of for both graphs. As expected, this size increases as decreases  X  ranking weakly-separated nodes costs more than ranking well-separated ones. In the .it web graph, the cost of local ranking is 100 to 1000 times smaller than the size of the graph, i.e. in the order of 10 4  X  10 5 queries, which may be intolerably high for ap-plications that use a remote link server. Surprisingly, in the LiveJournal graph minimal sets are much larger, in spite of the significantly lower graph size ( &lt; 4 . 8M vs. &gt; 37M nodes) and average degree ( &lt; 16 . 5 vs. &gt; 30). Indeed, ex-cept for  X  1, the number of collected ancestors almost always reached the threshold of 0 . 2 n and their contribution was not sufficient to give a correct ranking. Thus, -ranking in the LiveJournal graph is strongly non-local.
The second experiment evaluates the performance of two real (returning wrong results in some cases) algorithms, giv-ing an empirical upper bound on the cost of local ranking if one accepts an unavoidably positive rate of error.

We first tested the brute force (BF) algorithm, which serves more as a benchmark than as a realistic efficient method for local ranking. BF explores the ancestors of each average estimate minimal set size Figure 5: Average size of estimated minimal sets for ( , 2 ) -separated nodes as a function of for the .it web graph and the LiveJournal graph of the two target nodes layer by layer, computing their ap-proximate scores according to Equation (2) and inferring their relative ranking. Figures 6.1 -6.2 ( .it graph) and 6.3 -6.4 (LiveJournal graph) show respectively the average cost (number of visited nodes) and the average precision (fraction of correctly ranked node pairs), for different values of ( , 2 ), as a function of the number of layers visited from 0 to 25  X  a limit that seems sufficient to saturate the set of visited nodes. Unsurprisingly, BF incurs overwhelmingly high costs (several million queries) in both graphs, even to guarantee a precision  X  0 . 9 only for the most separated nodes.
We then tested ImPBF, an improved version of PBF, the pruned brute force algorithm [3]. PBF is an intuitively effi-cient variation of the BF algorithm that, at each layer  X   X  1, visits only the nodes whose estimated contribution (over paths known at iteration  X  ) to the score of the target node is above a given threshold. The conjecture underpinning this heuristics is that nodes with a small contribution over paths of length  X   X  probably give a small overall contribution, and their parents likely do the same. Counterexamples show that sometimes this is false [4], but [12] shows that this heuris-tics works on real web graphs; and although other algorithms for local score approximation exist (see [12]), they have less clear theoretical backgrounds, are unrealistic (e.g. assuming to know the total number of arcs), or sometimes overesti-mate the true PageRank score  X  all assumptions not fitting our general model. We terminated PBF either when no more candidates had a sufficient estimated contribution or when it visited more than 0 . 1 n nodes, which in practice is  X ( n ). On the subgraph induced by the visited nodes, ImPBF runs BF for 40 iterations (while PBF typically did not visit more than 20 layers) or until the relative per-iteration increment in the estimated score drops below 0 . 1%. This  X  X queezes X  most of the overall contribution out of the visited nodes, giving a more accurate score approximation.

We ran ImPBF for different contribution thresholds, rang-ing from 10  X  1 to 10  X  7 . Figures 6.5 -6.6 ( .it graph) and 6.7 -6.8 (LiveJournal graph) show respectively the average cost and the average precision of the ImPBF algorithm as a function of the contribution threshold, for different values of ( , 2 ). In the .it web graph, ImPBF reached a preci-sion  X  0 . 9 for &gt; 0 . 02 incurring a cost 2 . 5 to 5 times the estimated minimal set size and is thus reasonably close to the optimum. In the LiveJournal graph, ImPBF guaranteed Figure 6.1: .it web graph, BF algorithm: average cost vs. layers visited, for different values of ( , 2 ) Figure 6.2: .it web graph, BF algorithm: average precision vs. layers visited, for different values of ( , 2 ) a precision  X  0 . 9 only for  X  0 . 64, incurring a cost of a few thousand nodes, or  X  0 . 1 times the average estimated minimal set size. For &lt; 0 . 64, its precision dropped below 0 . 9, falling below 0 . 7 for &lt; 0 . 16, and the cost increased to a large fraction of the estimated minimal set size  X  confirm-ing that ranking in this graph is strongly non-local except for extremely-separated nodes. Counterintuitively, the pre-cision of ImPBF in the LiveJournal graph tends to decrease as the contribution threshold lowers and the cost increases. This is likely due to the saturation of the 0 . 1 n cost threshold and confirms that local ranking is definitely non-trivial.
Motivated by a large and growing number of applications (such as web crawling and social network mining) that use PageRank as a pure ranking algorithm, we define and inves-tigate the problem of computing locally the relative Page-Rank ranking of a set of k nodes in an n -node graph. We prove that a general, efficient local ranking algorithm does not exist: in the worst case, any correct algorithm must consider  X ( terministic, even when ranking the top k nodes in the graph and even if their scores are  X  X ell separated X . We show exper-imentally that, in practice, real (web and social) graphs be-have almost as poorly, requiring intolerably high costs both for exact algorithms and for algorithms based on state-of-the-art local score approximation algorithms.

Two questions naturally arise from our theoretical and ex-perimental results. First, what are the topological properties of a graph that make it a hard, or instead, easy instance for Figure 6.3: LiveJournal graph, BF algorithm: average cost vs. layers visited, for different values of ( , 2 ) Figure 6.4: LiveJournal graph, BF algorithm: average pre-cision vs. layers visited, for different values of ( , 2 ) local ranking? Second, how can these properties be exploited by local algorithms to compute the ranking efficiently? This work was supported by Univ. Padova Strategic Proj. AACSE, by MIUR Proj. AlgoDEEP, and by PAT, FBK, and INFN Proj. Aurora-Science. We thank Massimo Melucci, Enoch Peserico, Geppino Pucci, Michele Scquizzato, France-sco Silvestri, and the anonymous reviewers for their valuable comments; Paolo Bertasi and Michele Bonazza for setting up the computing infrastructure. Figure 6.5: .it graph, ImPBF algorithm: average cost vs. contribution threshold, for different values of ( , 2 ) Figure 6.6: .it graph, ImPBF algorithm: average precision vs. contribution threshold, for different values of ( , 2 ) Figure 6.7: LiveJournal graph, ImPBF algorithm: avg. cost vs. contrib. threshold, for different values of ( , 2 ) Figure 6.8: LiveJournal graph, ImPBF algorithm: avg. precision vs. contrib. threshold, for different values of ( , 2 )
