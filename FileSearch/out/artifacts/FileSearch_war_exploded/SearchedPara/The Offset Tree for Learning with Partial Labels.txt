 We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse any existing, fully supervised binary classification al-gorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classifica-tion. In particular, it has regret at most ( k  X  1) times the regret of the binary classifier it uses (where k is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just O (log 2 k ) work to train on an example or make a prediction.

Experiments with the Offset Tree show that it generally performs better than several alternative approaches. I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  Learning Algorithms,Theory Contextual bandits, associative reinforcement learning, in-teractive learning This paper is about learning to make decisions in partial feedback settings where the payoff of only one choice is ob-served rather than all choices.

As an example, consider an internet site recommending ads or other content based on such observable quantities as user history and search engine queries, which are unique or nearly unique for every decision. After the ad is displayed, a user either clicks on it or not. This type of feedback differs critically from the standard supervised learning setting since we don X  X  observe whether or not the user would have clicked had a different ad been displayed instead.

In an online version of the problem, a policy chooses which ads to display and uses the observed feedback to improve its future ad choices. A good solution to this problem must explore different choices and properly exploit the feedback.
The problem faced by an internet site, however, is more complex. They have observed many interactions historically, and would like to exploit them in forming an initial policy, which may then be improved by further online exploration. Since exploration decisions have already been made, online solutions are not applicable. To properly use the data, we need non-interactive methods for learning with partial feed-back.

This paper is about constructing a family of algorithms for non-interactive learning in such partial feedback settings. Since any non-interactive solution can be composed with an exploration policy to form an algorithm for the online learning setting, the algorithm proposed here can also be used online. Indeed, some of our experiments are done in an online setting.
 Here is a formal description of non-interactive data genera-tion: 1. Some unknown distribution D generates a feature vec-2. An existing policy chooses an action a  X  X  1 ,...,k } . 3. The reward r a is revealed.
 The goal is to learn a policy  X  : X  X  X  1 ,...,k } for choosing an action given x , with the goal of maximizing the expected reward with respect to D , given by We call this a partial label problem (defined by) D . Probably the simplest approach is to regress on the reward r a given x and a , and then choose according to the largest predicted reward given a new x . This approach reduces the partial label problem to a standard regression problem.
A key technique for analyzing such a reduction is regret analysis , which bounds the  X  X egret X  of the resulting policy in terms of the regressor X  X   X  X egret X  on the problem of predict-ing r a given x and a . Here regret is the difference between the largest reward that can be achieved on the problem and the reward achieved by the predictor; or X  X efined in terms of losses X  X he difference between the incurred loss and the smallest achievable loss. One analyzes excess loss (i.e., re-gret) instead of absolute loss so that the bounds apply to inherently noisy problems. It turns out that the simple ap-proach above has regret that scales with the square root of the regressor X  X  regret (see section 6 for a proof). Recalling that the latter is upper bounded by 1, this is undesirable. Another natural approach is to use the technique in [21]. Given a distribution p ( a ) over the actions given x , the idea is to transform each partial label example ( x,a,r a ,p ( a )) into an importance weighted multiclass example ( x,a,r a /p ( a )), where r a /p ( a ) is the cost of not predicting label a on input x . These examples are then fed into any importance weighted multiclass classification algorithm, with the output classifier used to make future predictions. Section 6 shows that when p ( a ) is uniform, the resulting regret on the original partial la-bel problem is bounded by k times the importance weighted multiclass regret, where k is the number of choices. The importance weighted multiclass classification problem can, in turn, be reduced to binary classification, but all known conversions yield worse bounds than the approach presented in this paper.
 We propose the Offset Tree algorithm for reducing the par-tial label problem to binary classification, allowing one to reuse any existing, fully supervised binary classification al-gorithm for the partial label problem.

The Offset Tree uses the following trick, which is easiest to understand in the case of k = 2 choices (covered in sec-tion 3). When the observed reward r a of choice a is low, we essentially pretend that the other choice a 0 was chosen and a different reward r 0 a 0 was observed. Precisely how this is done and why, is driven by the regret analysis. This basic trick is composable in a binary tree structure for k &gt; 2, as described in section 4.

The Offset Tree achieves computational efficiency in two ways: First, it improves the dependence on k from O ( k ) to O (log 2 k ). It is also an oracle algorithm, which implies that it can use the implicit optimization in existing learn-ing algorithms rather than a brute-force enumeration over policies, as in the Exp4 algorithm [3]. We prove that the Offset Tree policy regret is bounded by k  X  1 times the re-gret of the binary classifier in solving the induced binary problems. Section 5 shows that no reduction can provide a better guarantee, giving the first nontrivial lower bound for learning reductions.

Section 6 analyzes several alternative approaches. An em-pirical comparison of these approaches is given in section 7. The problem considered here is a non-interactive version of the contextual bandit problem (see [2, 3, 5, 15, 20] for back-ground on the bandit problem). The interactive version has been analyzed under various additional assumptions [4, 10, 13, 14, 16, 18], including payoffs as a linear function of the side information [1, 2]. The Exp4 algorithm [3] has a nice assumption-free analysis. However, it is intractable when the number of policies we want to compete with is large. It also relies on careful control of the action choosing distri-bution, and thus cannot be applied to historical data, i.e., non-interactively.

Sample complexity results for policy evaluation in rein-forcement learning [9] and contextual bandits [13] show that Empirical Risk Minimization type algorithms can find a good policy in a non-interactive setting. The results here are mostly orthogonal to these results, although we do show in section A that a constant factor improvement in sample complexity is possible using the offset trick.

The Banditron algorithm [4] deals with a similar setting but does not address several concerns that the Offset Tree addresses: (1) the Banditron requires an interactive setting; (2) it deals with a specialization of our setting where the reward for one choice is 1, and 0 for all other choices; (3) its analysis is further specialized to the case where linear separators with margin exist; (4) it requires exponentially in k more computation; (5) the Banditron is not an oracle algorithm, so it is unclear, for example, how to compose it with a decision tree bias.

Transformations from partial label problems to fully su-pervised problems can be thought of as learning methods for dealing with sample selection bias [8], which is heavily studied in Economics and Statistics. This section reviews several basic learning problems and the Costing method [22] used in the construction.

A k -class classification problem is defined by a distribu-tion Q over X  X  Y , where X is an arbitrary feature space and Y is a label space with | Y | = k . The goal is to learn a classifier c : X  X  Y minimizing the error rate on Q , given training examples of the form ( x,y )  X  X  X  Y . Here 1 (  X  ) is the indicator function which evaluates to 1 when its argument is true, and to 0 otherwise.

Importance weighted classification is a generalization where some errors are more costly than others. Formally, an im-portance weighted classification problem is defined by a dis-tribution P over X  X  Y  X  [ 0 ,  X  ). Given training examples of the form ( x,y,w )  X  X  X  Y  X  [ 0 ,  X  ), where w is the cost associated with mislabeling x , the goal is to learn a classifier c : X  X  Y minimizing the importance weighted loss on P , E
A folk theorem [22] says that for any importance weighted distribution P , there exists a constant w = E ( x,y,w )  X  P such that for any classifier c : X  X  Y , where Q is the distribution over X  X  Y defined by marginalized over w . In other words, choosing c to minimize the error rate under Q is equivalent to choosing c to minimize the importance weighted loss under P .

The Costing method [22] can be used to resample the training set drawn from P using rejection sampling on the importance weights (an example with weight w is accepted with probability proportional to w ), so that the resampled set is effectively drawn from Q . Then, any binary classifica-tion algorithm can be run on the resampled set to optimize the importance weighted loss on P .

Costing runs a base classification algorithm on multiple draws of the resampled set, and averages over the learned classifiers when making importance weighted predictions (see [22] for details). To simplify the analysis, we can augment the feature space with the index of the resampled set and then learn a single classifier on the union of all resampled data. The implication of this observation is that we can view Costing as a machine that maps importance weighted examples to unweighted examples. We use this method in Algorithms 1 and 2 below. This section deals with the special case of k = 2 actions. We state the algorithm, prove the regret bound (which is later used for the general k case), and state a sample complexity bound. For simplicity, we let the two action choices in this section be 1 and  X  1.
The Binary Offset algorithm is a reduction from the 2-class partial label problem to binary classification. The re-duction operates per example, implying that it can be used either online or offline. We state it here for the offline case. The algorithm reduces the original problem to binary im-portance weighted classification, which is then reduced to binary classification using the Costing method described in section 2 above. A base binary classification algorithm Learner is used as a subroutine.

The key trick appears inside the loop in Algorithm 1, where importance weighted binary examples are formed. The offset of 1 / 2 changes the range of importances, effec-tively reducing the variance of the induced problem. This trick is driven by the regret analysis in section 3.2.
Algorithm 1 : Binary Offset (binary classification algo-rithm Learner, 2-class partial label dataset S ) set S 0 =  X  for each ( x,a,r a ,p ( a ))  X  S do return Learner(Costing( S 0 )). This section proves a regret transform theorem for the Binary Offset reduction. Informally, regret measures how well a predictor performs compared to the best possible pre-dictor on the same problem. A regret transform shows how the regret of a base classifier on the induced (binary classifi-cation) problem controls the regret of the resulting policy on the original (partial label) problem. Thus a regret transform bounds only excess loss due to suboptimal prediction.
Binary Offset transforms partial label examples into bi-nary examples. This process implicitly transforms the dis-tribution D defining the partial label problem into a distri-bution Q D over binary examples, via a distribution over im-portance weighted binary examples. Note that even though the latter distribution depends on both D and the action-choosing distribution p , the induced binary distribution Q depends only on D . Indeed, the probability of label 1 given x and ~r , according to Q D , is independent of p .

The binary regret of a classifier c : X  X  X  X  1 , 1 } on Q D given by where the min is over all classifiers c 0 : X  X  { 1 ,  X  1 } . The importance weighted regret is defined similarly with respect to the importance weighted loss.

For the k = 2 partial label case, the policy that a classifier c induces is simply the classifier. The regret of policy c is defined as where is the value of the policy.

The theorem below states that the policy regret is bounded by the binary regret. We find it surprising because strictly less information is available than in binary classification. Note that the lower bound in section 5 implies that no re-duction can do better. Redoing the proof with the offset set to 0 rather than 1 / 2 also reveals that 2 reg e ( c,Q D ) bounds the policy regret, implying that the offset trick gives a factor of 2 improvement in the bound.

Finally, note that the theorem is quantified over all clas-sifiers, which includes the classifier returned by Learner in the last line of the algorithm.
 Theorem 3.1. (Binary Offset Regret) For all 2 -class par-tial label problems D and all binary classifiers c , Furthermore, there exists D such that for all values v  X  [0 , 1] there exists c such that v = reg  X  ( c,D ) = reg e ( c,Q the bound is tight).
 Proof. We first bound the partial label regret of c in terms of importance weighted regret, and then apply known results to relate the importance weighted regret to binary regret.
Conditioned on a particular value of x , we either make a mistake or we do not. If no mistake is made, then the regrets of both sides are 0, and the claim holds trivially. Assume that a mistake is made. Without loss of generality, r 1 &gt; r and label  X  1 is chosen. The expected importance weight of label  X  1 is given by where we use the operator ( Z ) + = Z  X  1 ( Z &gt; 0). The dif-ference in expected importance weights between label 1 and label  X  1 is This shows that the importance weighted regret of the bi-nary classifier is the policy regret. The folk theorem from section 2 (see [22]) says that the importance weighted re-gret is bounded by the binary regret, times the expected E ~r  X  D | x [ | r 1  X  1 / 2 | + | r  X  1  X  1 / 2 | ]  X  1 , since both r are bounded by 1. This proves the first part of the theorem.
For the second part, notice that the proof of the first part can be made an equality by having a reward vector (0 , 1) for each x always, and letting the classifier predict label 1 with probability (1  X  v ) over the draw of x . In this section we deal with the case of large k . The technique in the previous section can be applied repeat-edly using a tree structure to give an algorithm for general k . In this paper we are not concerned with construction of the tree, which is an important separate question. Conse-quently, we assume a binary tree structure is given. In our experiments, we simply construct the tree according to the bit representation of integer class labels.

Consider a maximally balanced binary tree on the set of k choices, conditioned on a given observation x . Every internal node in the tree is associated with a classification problem of predicting which of its two inputs has the larger expected reward. At each node, the same offsetting technique is used as in the binary case described in section 3.

For an internal node v , let  X ( T v ) denote the set of leaves in the subtree T v rooted at v . Every input to a node is either a leaf or a winning choice from another internal node closer to the leaves.
 The training algorithm, Offset Tree, is given in Algorithm 2. The testing algorithm defining the predictor is given in Al-gorithm 3.
The theorem below gives an extension of Theorem 3.1 for general k . For the analysis, we use a simple trick which allows us to consider only a single induced binary problem, and thus a single binary classifier c . The trick is to add the node index as an additional feature into each importance weighted binary example created algorithm 2, and then train based upon the union of all the training sets.

As in section 3, the reduction transforms a partial label distribution D into a distribution Q D over binary examples. To draw from Q D , we draw ( x,~r ) from D , an action a from the action-choosing distribution p , and apply algorithm 2 to
Algorithm 2 : Offset Tree (binary classification algo-rithm Learner, partial label dataset S )
Fix a binary tree T over the choices for each internal node v in order from leaves to root do return c = { c v }
Algorithm 3 : Offset Test (classifiers { c v } , unlabeled example x ) return unique action a for which every classifier c v from a to root prefers a . transform ( x,~r,a,p ( a )) into a set of binary examples (up to one for each level in the tree) from which we draw uniformly at random. Note that Q D is independent of p , as explained in the beginning of section 3.

Denote the policy induced by the Offset-Test algorithm using classifier c by  X  c . For the following theorem, the defi-nitions of regret are from section 3.
 Theorem 4.1. (Offset Tree Regret) For all k -class partial label problems D , for all binary classifiers c , where v ( a,a 0 ) ranges over the ( k  X  1) internal nodes in T , and a and a 0 are its inputs determined by c  X  X  predictions. Note: Section 5 shows that no reduction can give a better regret transform theorem. With a little bit of side informa-tion, however, we can do better: The offset minimizing the regret bound turns out to be the median value of the reward given x . Thus, it is generally best to pair choices which tend to have similar rewards. Note that the algorithm need not know how well c performs on Q D .

The proof below can be reworked with the offset set to 0, resulting in a regret bound which is a factor of 2 worse. Proof. We fix x , taking the expectation over the draw of x at the end. The first step is to show that the partial label regret is bounded by the sum of the importance weighted regrets over the binary prediction problems in the tree. We then apply the costing analysis [22] to bound this sum in terms of the binary regret.

The proof of the first step is by induction on the nodes in the tree. We want to show that the sum of the importance weighted regrets of the nodes in any subtree bounds the regret of the output choice for the subtree. The hypothesis trivially holds for one-node trees.

Consider a node u making an importance weighted deci-sion between choices a and a 0 . The expected importance of choice a is given by It is important to note that, by construction, only two ac-tions can generate examples for a given internal node. With-out loss of generality, assume that a 0 has the larger expected reward. The expected importance weighted binary regret wreg u of the classifier X  X  decision is either 0 if it predicts a or if the classifier predicts a .

Let T v be the subtree rooted at node v , and let a be the choice output by T v on x . If the best choice in  X ( T v from the subtree L producing a , the policy regret of T v given by If on the other hand the best choice comes from the other subtree R , we have proving the induction.
 The induction hypothesis applied to T tells us that Reg( T )  X  P v  X  T wreg v . According to the Costing theorem discussed in section 2, the importance weighted regret is bounded by the unweighted regret on the resampled distribution, times the expected importance. The expected importance of deciding between actions a and a 0 is since all rewards are between 0 and 1. Noting that Reg( T ) = reg  X  (  X  c ,D | x ), we thus have completing the proof for any x . Taking the expectation over x finishes the proof.
 The setting above is akin to Boosting [6]: At each round t , a booster creates an input distribution D t and calls an oracle learning algorithm to obtain a classifier with some error D . The distribution D t depends on the classifiers returned by the oracle in previous rounds. The accuracy of the final classifier is analyzed in terms of t  X  X . The binary problems induced at internal nodes of an offset tree depend, similarly, on the classifiers closer to the leaves. The performance of the resulting partial label policy is analyzed in terms of the oracle X  X  performance on these problems. (Notice that Theo-rem 4.1 makes no assumptions about the error rates on the binary problems; in particular, it doesn X  X  require them to be bounded away from 1 / 2.)
For the analysis, we use the simple trick from the begin-ning of this subsection to consider only a single binary clas-sifier. The theorem is quantified over all classifiers, and thus it holds for the classifier returned by the algorithm. In prac-tice, one can either call the oracle multiple times to learn a separate classifier for each node (as we do in our experi-ments), or use iterative techniques for dealing with the fact that the classifiers are dependent on other classifiers closer to the leaves. This section shows that no method for reducing the partial label setting to binary classification can do better. First we formalize a learning reduction that uses a binary classifica-tion oracle. The lower bound we prove below holds for all such learning reductions.
 Definition 5.1. (Binary Classification Oracle) A binary classification oracle O is a (stateful) program that supports two kinds of queries: 1. Advice . An advice query O ( x,y ) consists of a single 2. Predict . A predict query O ( x ) is made with a feature All learning reductions work on a per-example basis, and that is the representation we work with here.
 Definition 5.2. (Learning Reduction) A learning reduction is a pair of algorithms R and R  X  1 . 1. The algorithm R takes a partially labeled example 2. The algorithm R  X  1 takes an unlabeled example x and We are now ready to state the lower bound.
 Theorem 5.1. For all reductions ( R,R  X  1 ) , there exists a partial label problem D and an oracle O such that where R ( D ) is the binary distribution induced by R on D , and R  X  1 ( O ) is the policy resulting from R  X  1 using O . Proof. The proof is by construction. We choose D to be uniform over k examples, with example i having 1 in its i -th component of the reward vector, and zeros elsewhere. The corresponding feature vector consists of the binary represen-tation of the index with reward 1. Let the action-choosing distribution be uniform.

The reduction R produces some simulatable sequence of advice calls when the observed reward is 0. The oracle ignores all advice calls from R and chooses to answer all queries with zero error rate according to this sequence.
There are two cases: Either R observes 0 reward (with probability ( k  X  1) /k ) or it observes reward 1 (with prob-(and, hence 0 regret). In the second case, it has error rate (and regret) of at most 1. Thus the expected error rate of the oracle on R ( D ) is at most 1 /k .

The inverse reduction R  X  1 has access to only the unla-beled example x and the oracle O . Since the oracle X  X  answers are independent of the draw from D , the output action has reward 0 with probability ( k  X  1) /k and reward 1 with prob-ability 1 /k , implying a regret of ( k  X  1) /k with respect to the best policy. This is a factor of k  X  1 greater than the regret of the oracle, proving the lower bound. This section analyzes two simple approaches for reducing partial label problems to basic supervised learning problems. These approaches have been discussed previously, but the analysis is new.
The most obvious approach is to regress on the value of a choice as in Algorithm 4, and then use the argmax classifier as in Algorithm 5. Instead of learning a single regressor, we can learn a separate regressor for each choice.
 Algorithm 4 : Partial-Regression (regression algorithm Regress, partial label dataset S )
Let S 0 =  X  for each ( x,a,r a )  X  S do return f = Regress( S 0 ).

Algorithm 5 : Argmax (regressor f , unlabeled example x ) return arg max a f ( x,a ) The squared error of a regressor f : X  X  R on a distribution P over X  X  R is denoted by The corresponding regret is given by reg r ( f,P ) = ` r ( f,P )  X  min f 0 ` r ( f 0 ,P ).
 The following theorem relates the regret of the resulting pre-dictor to that of the learned regressor.
 Theorem 6.1. For all k -class partial label problems D and all squared-error regressors f , where P D is the regression distribution induced by Algo-rithm 4 on D , and  X  f is the argmax policy based on f . Furthermore, there exist D and h such that the bound is tight.
 The theorem has a square root, which is undesirable, because the theorem is vacuous when the right hand side is greater than 1.
 Proof. Let  X  f choose some action a with true value v a = E ( x,~r )  X  D [ r a ]. Some other action a  X  may have a larger ex-pected reward v a  X  &gt; v a . The squared error regret suffered f ( x,a )) 2 . Similarly for a  X  , we have regret ( v a  X  In order for a to be chosen over a  X  , we must have f ( x,a )  X  f ( x,a  X  ). Convexity of the two regrets implies that the min-ima is reached when f ( x,a ) = f ( x,a  X  ) = v a + v the regret for each of the two choices is  X  v a  X   X  v a regressor need not suffer any regret on the other k  X  2 arms. be induced, completing the proof of the first part. For the second part, note that an adversary can play the optimal strategy outlined above achieving the bound precisely.
Zadrozny [21] noted that the partial label problem could be reduced to importance weighted multiclass classification. After Algorithm 6 creates importance weighted multiclass examples, the weights are stripped using Costing (the rejec-tion sampling on the weights discussed in Section 2), and then the resulting multiclass distribution is converted into a binary distribution using, for example, the all-pairs reduc-tion [7]). The last step is done to get a comparable analysis.
Algorithm 6 : IWC-Train (binary classification algo-rithm Learn, partial label dataset S )
Let S 0 =  X  for each ( x,a,p ( a ) ,r a )  X  S do return All-Pairs-Train (Learn , Costing( S 0 )) All-Pairs-Train uses a given binary learning algorithm Learn to distinguish each pair of classes in the multiclass distribu-tion created by Costing. The learned classifier c predicts, given x and a distinct pair of classes ( i,j ), whether class i is more likely than j given x . At test time, we make a choice using All-Pairs-Test, which takes c and an unlabeled example x , and returns the class that wins the most pairwise comparisons on x , according to c .

Algorithm 7 : IWC-Test (binary classifier c , unlabeled example x ) return All-Pairs-Test( c,x ).
 A basic theorem applies to this approach. Theorem 6.2. For all k -class partial label problems D and all binary classifiers c , where  X  c is the IWC-Test policy based on c and Q D is the binary distribution induced by IWC-Train on D .
 Proof. The proof first bounds the policy regret in terms of the importance weighted multiclass regret. Then, we apply known results for the other reductions to relate the policy regret to binary classification regret.

Fix a particular x . The policy regret of choosing action importance weighted multiclass loss of action a is p ( a 0 ). This implies the importance weighted regret of which is the same as the policy regret.

The importance weighted regret is bounded by the un-weighted regret, times the expected importance (see [22]), which in turn is bounded by k . Multiclass regret on k classes is bounded by binary regret times k  X  1 using the all-pairs reduction [7], which completes the proof.
 Relative to the Offset Tree, this theorem has an undesirable extra factor of k in the regret bound. While this factor is due to the all-pairs reduction being a weak regret transform, we are aware of no alternative approach for reducing multiclass to binary classification that in composition can yield the same regret transform as the Offset Tree. We conduct two sets of experiments. The first set compares the Offset Tree with the two approaches from section 6. The second compares with the Banditron [4] on the dataset used in that paper.
Ideally, this comparison would be with a data source in the partial label setting. Unfortunately, data of this sort is rarely available publicly, so we used a number of publicly available multiclass datasets [17] and allowed queries for the reward (1 or 0 for correct or wrong) of only one value per example.

For all datasets, we report the average result over 10 ran-dom splits (fixed for all methods), with 2 / 3 of the dataset used for training and 1 / 3 for testing. Figure 1 shows the er-ror rates (in %) of the Offset Tree plotted against the error rates of the regression (left) and the importance weighting (right). Decision trees (J48 in Weka [19]) were used as a base binary learning algorithm for both the Offset Tree and the importance weighting. For the regression approach, we learned a separate regressor for each of the k choices. (A sin-gle regressor trained by adding the choice as an additional Figure 1: Error rates (in %) of Offset Tree versus the regression approach using two different base regres-sion algorithms (left) and Offset Tree versus Impor-tance Sampling (right) on several different datasets using decision trees as a base classifier learner. feature performed worse.) M5P and REPTree, both avail-able in Weka [19], were used as base regression algorithms.
The Offset Tree clearly outperforms regression, in some cases considerably. The advantage over importance weight-ing is moderate: Often the performance is similar and occa-sionally it is substantially better.

We did not perform any parameter tuning because we ex-pect that practitioners encountering partial label problems may not have the expertise or time for such optimization. All datasets tested are included. Note that although some error rates appear large, we are choosing among k alterna-tives and thus an error rate of less than 1  X  1 /k gives an advantage over random guessing. Dataset-specific test error rates are reported in Table 1.
The Banditron [4] is an algorithm for the special case of the problem where one of the rewards is 1 and the rest are 0. The sample complexity guarantees provided for it are particularly good when the correct choice is separated by a multiclass margin from the other classes.

We chose the Binary Perceptron as a base classification algorithm since it is the closest fully supervised learning al-gorithm to the Banditron. Exploration was done according to Epoch-Greedy [13] instead of Epsilon-Greedy (as in the Banditron), motivated by the observation that the optimal rate of exploration should decay over time. The Banditron was tested on one dataset, a 4-class specialization of the Reuters RCV1 dataset consisting of 673,768 examples. We number of examples use precisely the same dataset, made available by the au-thors of [4].

Since the Banditron analysis suggests the realizable case, and the dataset tested on is nearly perfectly separable, we also specialized the Offset Tree for the realizable case. In particular, in the realizable case we can freely learn from every observation implying it is unnecessary to importance weight by 1 /p ( a ). We also specialize Epoch-Greedy to this case by using a realizable bound, resulting in a probability of exploration that decays as 1 /t 1 / 2 rather than 1 /t The algorithms are compared according to their error rate. For the Banditron, the error rate after one pass on the dataset was 16 . 3%. For the realizable Offset Tree method above, the error rate was 10 . 72%. For the fully agnostic version of the Offset Tree, the error rate was 18 . 6%. These results suggest there is some tradeoff between being optimal when there is arbitrary noise, and performance when there is no or very little noise. In the no-noise situation, the re-alizable Offset Tree performs substantially superior to the Banditron. The code and dataset for this comparison are publicly available [12]. We have analyzed the tractability of learning when only one outcome from a set of k alternatives is known, in the re-ductions setting. The Offset Tree approach has a worst-case dependence on k  X  1 (Theorem 4.1), and no other reduction approach can provide a better guarantee (Section 5). Fur-thermore, with an O (log k ) computation, the Offset Tree is qualitatively more efficient than all other known algorithms, the best of which are O ( k ). Experimental results suggest that this approach is empirically promising.

The algorithms presented here show how to learn from one step of exploration. By aggregating information over multi-ple steps, we can learn good policies using binary classifica-tion methods. A straightforward extension of this method to deeper time horizons T is not compelling as k  X  1 is re-placed by k T in the regret bounds. Due to the lower bound proved here, it appears that further progress on the multi-step problem in this framework must come with additional assumptions. We would like to thank Tong Zhang, Alex Strehl, and Sham Kakade for helpful discussions. We would also like to thank Shai Shalev-Shwartz for providing data and helping setup a clean comparison with the Banditron. [1] N. Abe, A. Biermann, and P. Long. Reinforcement [2] P. Auer. Using confidence bounds for [3] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. [4] S. Kakade, S. Shalev-Schwartz, and A. Tewari.
 [5] E. Even-dar, S. Mannor, and Y. Mansour. Action [6] Y. Freund and R. Schapire. A decision-theoretic [7] T. Hastie and R. Tibshirani. Classification by pairwise [8] J. Heckman. Sample selection bias as a specification [9] M. Kearns, Y. Mansour, and A. Y. Ng. Approximate [10] S. Kulkarni. On bandit problems with side [11] J. Langford. Tutorial on practical prediction theory [12] J. Langford and A. Beygelzimer. [13] J. Langford and T. Zhang. The Epoch-greedy [14] S. Pandey, D. Agarwal, D. Chakrabati, V. Josifovski. [15] H. Robbins. Some aspects of the sequential design of [16] A. Strehl, C. Mesterham, M. Littman, and H. Hirsh. [17] C. Blake and C. Merz. UCI Repository of machine [18] C. C. Wang, S. Kulkarni, and H. Vincent Poor. Bandit [19] I. Witten and E. Frank. Data Mining: Practical [20] M. Woodruff. A one-armed bandit problem with [21] B. Zadrozny. Ph.D. Thesis, University of California, [22] B. Zadrozny, J. Langford, and N. Abe. Cost sensitive
This section proves a simple sample complexity bound on the performance of Binary Offset. For ease of comparison with existing results, we specialize the problem set to par-tial label binary classification problems where one label has reward 1 and the other label has reward 0. Note that this is not equivalent to assuming realizability: Conditioned on x , any distribution over reward vectors (0 , 1) and (1 , 0) is allowed.

Comparing the bound with standard results in binary clas-sification (see, for example, [11]), shows that the bounds are identical, while eliminating the offset trick weakens the per-formance by a factor of roughly 2.

When a sample set is used as a distribution, we mean the uniform distribution over the sample set (i.e., an empirical average).
 Theorem A.1. (Binary Offset Sample Complexity) Let the action choosing distribution be uniform. For all partial la-bel binary classification problems D and all sets of binary classifiers C , after observing a set S of m examples drawn independently from D , with probability at least 1  X   X  , holds simultaneously for all classifiers c  X  C . Furthermore, if the offset is set to 0 , then Proof. First note that for partial label binary classifica-tion problems, the Binary Offset reduction recovers the cor-rect label. Since all importance weights are 1, no exam-ples are lost in converting from importance weighted clas-sification to binary classification. Consequently, the Oc-cam X  X  Razor bound on the deviations of error rates im-plies that, with probability 1  X   X  , for all classifiers c  X  C , induced distribution Q D is D with the two reward vectors encoded as binary labels. Observing that e ( c,Q D ) =  X  ( c,D ) finishes the first half of the proof.

For the second half, notice that rejection sampling reduces the number of examples by a factor of two in expectation; and with probability at least 1  X   X / 3, this number is at least m/ 2  X  p m ln(3 / X  ). Applying the Occam X  X  Razor bound with probability of failure 2  X / 3, gives Taking the union bound over the two failure modes proves that the above inequality holds with probability 1  X   X  . Ob-serving the equivalence e ( c,Q D ) =  X  ( c,D ) gives us the final result.
 The sample complexity bound provides a stronger (abso-lute) guarantee, but it requires samples to be independent and identically distributed. The regret bound, on the other hand, provides a relative assumption-free guarantee, and thus applies always.
