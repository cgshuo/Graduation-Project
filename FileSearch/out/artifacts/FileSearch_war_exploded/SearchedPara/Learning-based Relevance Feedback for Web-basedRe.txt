 In a pilot application based on web search engine called Web-based Relation Completion (WebRC), we propose to join two columns of entities linked by a predefined relation by mining knowledge from the web through a web search engine. To achieve this, a novel re-trieval task Relation Query Expansion (RelQE) is modelled: given an entity (query), the task is to retrieve documents containing en-tities in predefined relation to the given one. Solving this problem entails expanding the query before submitting it to a web search engine to ensure that mostly documents containing the linked en-tity are returned in the top K search results. In this paper, we pro-pose a novel Learning-based Relevance Feedback (LRF) approach to solve this retrieval task. Expansion terms are learned from train-ing pairs of entities linked by the predefined relation and applied to new entity-queries to find entities linked by the same relation. After describing the approach, we present experimental results on real-world web data collections, which show that the LRF approach always improves the precision of top-ranked search results to up to 8.6 times the baseline. Using LRF, WebRC also shows perfor-mances way above the baseline.
 H.2.0 [ Database Management ]: General X  Security, integrity, and protection ; H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  relevance feedback, retrieval models Algorithms, Experimentation Web-based Relation Completion, WebRC As a pilot application based on web search engines, Web-based Relation Completion (WebRC) proposes to join two columns of en-tities in a given relation with the help of a web search engine. As shown in Figure 1, given two sets of entities in named relation and assuming we ha ve little knowle dge about the links between the two sets (merely examples of pairs linked by the relation), the task is to link every entity in one set to its corresponding entity in the other set. Previously the Relation Completion could only be processed manually by domain experts, which is quite expensive. Now with the rich knowledge available on the web, WebRC was proposed to mine web information to process the Relation Completion auto-matically. Several example scenarios that can use WebRC are given below: Scenario 1: A research institution needs to evaluate its researcher X  X  publication quality w. r. t . a given conference and journal ranking list, but most researchers do not provide the exact conference or journal names as per the ranking list within their publications lists. Scenario 2: An on-line book store, which used to sell books only on the American market, is now trying to sell its books to non-English speaking countries such as Japan. Therefore, it needs to build a new book database in Japanese, with all books and authors X  names translated into Japanese. However, they only have the En-glish names of these books and authors, and literal translation is not acceptable since some books and authors already have popular and quite different names in Japanese.
 Scenario 3: A head-hunting company needs to keep their database fresh, so they need to update, at least once a month, the employer, position, email address and office phone number of each person they are interested in. Web search engines with manual queries are the standard approach to maintain these databases, however, this is a very costly process.

In order to perform WebRC, a baseline search-based framework has been proposed recently [4]. By searching one given entity in the web search engine, it is assumed that its linked entity is men-tioned in the neighbourhood of the giv en entity within the retrieved documents. This approach works for some real-world situations such as finding the conference where a given paper has been pub-lished (Scenario 1). Using top-ranked snippets 1 of searched we-b pages is enough to reach a good join result on this latter task (precision=0.873, recall=0.831) [4]. However, for some loosely-connected entity pairs such as an academic staff member and its employer university (which can be seen as a specific example of Scenario 3 above), it is expected that things become complicat-ed in two respects. Firstly, the employer university is not always mentioned together with the staff member in the retrieved docu-ments returned by the web search engine when querying with the staff member X  X  name. Secondly, a university frequently mentioned with the staff member is not necessarily their employer. In this situation, we might need to process a large number of retrieved documents to find the right employer university of the staff mem-ber while noisy information will be brought in to weaken the join accuracy. The problem we have just illustrated is actually a gap between the search requirement and the search model of the web search engine. The search requirement for retrieving linked entities can be defined as: given an entity, the task is to retrieve documents containing the entity in named relation to the given one. However, if a query only contains the given entity, a web search engine also returns documents relevant to the given keywords, but not contain-ing the relation nor the linked entity.
To improve the applicability and performance of WebRC, we propose in this paper the Relation Query Expansion (RelQE). In short, we propose to automatically expand the query with relation terms before submitting it to the web search engine, thus expecting that more documents containing the target entity ( ie. , entity in the named relation to the query entity) will be returned among the top-K search results. We propose a novel Learning-based Relevance Feedback (LRF) approach for RelQE. The idea is to learn expan-sion terms for a given relation from a training set and then apply them to other queries for the same relation. Assuming an ideal training set 2 for entities in the given relation, the first step is to learn expansion terms for each query in the training set taken indi-vidually, where the LRF brings the positional and proximity infor-mation of terms into the adapted classic relevance model [3]. The second step is to select relation expansion terms from all queries X  expansion term sets, where a cluster-based model is proposed. By selecting relation expansion terms according to their popularity a-mongst clusters, instead of queries, we can prevent the generation of expansion terms too specific for the relation.
Our main contributions in this paper are summarized below:
Usually we only use snippets returned by web search engine in-stead of the original web pages w. r. t . time and space efficiency.
By ideal training set, we mean the training set should be big e-nough, and its distribution is close to that of the test set After covering related work in Section 2, we formalize RelQE in Section 3. The LRF approach is introduced in Section 4. Then the results of experimental studies are provided in Section 5. Finally we provide the conclusion in Section 6.
The Relation Completion problem emerged from the Record Link-age problem, which has been studied extensively due to its impor-tance in Data Cleaning. The difference between Record Linkage and Relation Completion is that there are common attributes be-tween two sets of records in record linkage, so that some similarity functions such as edit distance and its variations [6] can be used to measure the similarity between each pair of records. However, in Relation Completion, we are talking about linking two sets of entities under a given relation, and we assume there is no common attribute that can be leveraged between the two sets.

In the natural language processing community, another line of related and well-studied problems are the analogy, metaphor and semantic relations classification problems [2], which mainly fo-cus on recognizing or generating analogous string pairs under the same kind of relation such as (mason, stone) and (carpenter, wood). Most of the proposed approaches have used manually constructed lexicons and knowledge bases [7]. Differently, for Relation Com-pletion, we assume there are already two sets of entities, and the task is to find pairs of entities between the two sets, which satisfy a predefined relation.

There are new trends on retrieving  X  X bjects X ,  X  X ntities X  and their properties in both information retrieval communities and commer-cial systems [1]. There was even an entity track which defines re-lated entity finding task and entity list completion task in TREC 2010. As a specific type of entity retrieval, expert finding has been proposed and important progress have been made since 2005 [9]. However, entity retrieval and expert finding aims at retrieving enti-ties directly, while our work targets the retrieval of documents con-taining entities in a named relation to the given entity. Another line of work is the entity relation search, which focuses on searching re-lations between retrieved entities. Given the entity ranking results, entity relation search retrieves furth er details about the relationship between retrieved entities [9], which is also different from our task.
Let R be a relation, which defines the relationship between two entities. If an entity x is in the relation R with entity that ( x, y ) satisfies R , denoted as ( x, y )  X  X  . Assuming two sets of entities A and B , for each entity  X  in A there must be an entity  X  in B such that (  X ,  X  )  X  X  . The task of Relation Completion is to find all pairs of entities between A and B that satisfy
In order to perform Relation Completion we propose the Web-based Relation Completion (WebRC) framework.  X  and  X  still represent the two entity strings. By searching  X  in a web search engine, we retrieve web documents containing  X  , then recognize that  X  is the linked entity of  X  under R . We call this kind of query a relation query of R , which can be denoted as R -query. For an entity  X  , its objective R -query model (ignoring ranking) can be: where each document D contains  X  where (  X ,  X  ) satisfies the rela-tion R . Documents are retrieved from the collection of web docu-ments C indexed by the search engine under consideration.
On the other hand, the retrieval model of the mainstream keywords-based web search engines (ignoring ranking) can be expressed as: where each retrieved document D should contain the keywords from the query Q . This model expresses the constraint that al-l query terms have to appear in the documents retrieved by web search engine.

In order to use web search engines to solve our WebRC, we need to convert the R -query model into the web search engine retrieval model. In order to do that, we propose the Relation Query Expan-sion (RelQE), which aims to find a set of expansion terms the relation R . For each seed query  X  , by expanding it with each expansion term within the E ( R ) , the web search engine should returns less documents not containing  X  (noise), thus favouring the frequency of  X  in the retrieved documents. This conversion process can be represented as: where e i is an expansion term from E ( R ) ,and  X  WSE (  X  + e means to use e i to expand the query  X  before submitting it to the web search engine. In this paper, we only consider using one ex-pansion term at a time and the combination of these expansion terms will be studied in future work. It is unlikely that equal to  X  R (  X  ) since there is virtually no way to that ensure all re-trieved documents contain  X  without knowing  X  , but it should be a better approximation to  X  R (  X  ) than the  X  WSE (  X  ) .
We now present the Learning-basd Relevance Feedback (LRF) approach for solving RelQE. Assume that we have a good enough training set T = { (  X  1 , X  1 ) , (  X  2 , X  2 ) , ..., (  X 
To apply relevance feedback to an example pair &lt;  X  ,  X  uments retrieved through a web search engine are automatically assessed for relevance. For general queries, relevant documents are considered to be the top-k documents retrieved (pseudo-relevance feedback). But for the relation query, the relevant documents are those relevant to the relation, which consists of two entities. With a web search engine, the relevant documents containing both  X  can be retrieved by searching  X   X  +  X   X . For the remainder of this paper, consider Q R to be an R -query of searching  X  for  X  Q + refers to the query  X   X  +  X   X .
As a baseline model of learning expansion terms for single re-lation query, we adapt the classical relevance model [3], in which only relevance feedback is used to estimate the probability of an expansion term given the relation query Q R , ie.
 where P ( e | Q + ) is the probability of term e in the model of is the positive relevance feedback set (relevant documents retrieved with search engine), P ( e | D ) is the probability of the term document D , which can be estimated as: where tf ( e, D ) is the frequency of term e in document D is the maximum likelihood es timation of the probability of collection, | D | is the length of the document D ,and  X  is the Dirich-let prior parameter of Dirichlet smoothing [8]. In our experiments, we use the term frequencies from the Web1T corpus to estimate P
ML ( e |C ) , and we set Dirichlet prior  X  = 1500 .
The adapted relevance model above selects expansion terms from the whole documents, which is most likely non-optimal since there are usually multiple topics and irrelevant information within a rel-evant document. Therefore we use a positional relevance model, which exploits the position and proximity information of terms as cues for assessing if a term is  X  X lose X  enough to be used as an ex-pansion term for the R -query. Our positional relevance model is inspired by the one proposed by Lv et. al. [5], where pos (  X ,  X , D ) is the set of positions where  X  and together in D , with the distance between  X  and  X  is no larger than a given threshold  X  1 .The P ( e | D,i ) is the probability of term proximity of the position i in document D , which can be simplified as:
P ( e | D,i )= 1 . 0 if e occurs within  X  2 distance of i In our experiments, we set  X  1 =  X  2 =20 such that we can have in the snippet when we use e as the expansion term.
With the models above, we are likely to get the most distinctive terms between relevant documents and all other irrelevant docu-ments on the Web. However, there is a specific subset of irrelevant documents that we should pay attention to. These documents are those containing the query entity  X  itself but not the target entity We call this subset of irrelevant documents Semi-Irrelevant Doc-uments and consider that they express Semi-Negative Relevance Feedback (SNRF). Given the objective of RelQE, the improvemen-tof  X  R (  X  ) over the  X  WSE (  X  ) lies on decreasing the number of semi-irrelevant documents in top-ranked retrieved documents. Be-sides, semi-irrelevant documents and relevant documents are likely to be similar, so it is necessary to have expansion terms that are effective in distinguishing relevant documents from semi-irrelevant documents.

With a web search engine, SNRF can be retrieved by searching  X   X  - X   X . Here we let Q  X  refer to the query  X   X  - X   X . We have: where the Dirichlet prior  X  n is also set to 1500 in our experiments, P
ML ( e | Q  X  ) is the maximum likelihood estimation of term Q  X  , which can be estimated as: where F  X  is the set of SNRF retrieved with Q  X  .
The probability of a term e appearing in the retrieved documents of Q R is:
P ( e | Q R )  X  1 where the interpolation weight  X  is set at 0.4 which is an optimal value in our experiments. Besides, if we use P p ( e | D ) P ( e | D ) , we obtain the combined model of positional model and semi-negative model, which take both positional information and SNRF into account.
In order to guarantee the quality of expansion terms for each ex-ample, we verify their validity. For an example pair such as  X  is an unexpanded seed query. Each learned expansion term is used to expand the seed query in two ways: either the seed query plus the expansion term or the seed query minus the expansion ter-m. Then we measure the accuracy of the top-ranked (top-100 is enough to reflect the performance) retrieved documents, P@N, ie. the proportion of documents containing  X  .

Expansion terms are then categorized into three categories ac-cording to their effectiveness: 1) positive terms that improve P@N evidently when added, and hurt it evidently when subtracted; 2) negative terms are those having the opposite effect; 3) neutral terms are all the remaining ones. We use a threshold to define if the im-provement of P@N is evident. After this step, only positive terms will remain as expansion terms for each example pair.

E XAMPLE 1. We compare the expansion terms learned for an example pair of the relation (Academic Staff, University) with the three models (threshold is set to 0.2). Also, we label positive ex-pansion terms that can pass the verification process with boldfaced in Table1. In the verification process, a positive expansion term should improve the P@100 of a seed query by more than 30%.
As we can see, the baseline model generates 20 expansion terms, of which only 7 pass the verification; with the positional model, 16 expansion terms are generated 7 positive terms; while with the positional+semi-negative model, only 14 expansion terms are gen-erated with 6 passing the verification.
 Table 1: The learned expansion terms for example pair with the baseline relevance model, or the positional model, or the com-bined positional and semi-negative mixture model, with bold-faced terms are those passed the verification. 1. Baseline Model university cambridge dr college uk books professor history faculty london education library mr economics business edited publications communications email profile 2. Positional Model cambridge university college faculty education library london uk professor economics business studies master mr communications publications 3. Positional + Semi-Negative Model professor college communications education history faculty cambridge university economics laboratory master london mr studies
We consider how to generate E ( R ) from the verified expansion terms of each example pair in the training set. Intuitively, the ex-pansion term that belongs to more R -queries has a higher priority to be selected into E ( R ) . Hence, a query-based relation query ex-pansion model can be: where B ( e, Q R i )=1 if e is a positive expansion term of otherwise.

However, this query-based model may give some specific expan-sion terms a higher score than less specific expansion terms. For example, terms like  X  X ondon X  and  X  X ydney X  should have nothing to do with the (Academic Staff, University) relation. They come up as relation expansion terms because a number of examples take them as expansion terms (the reason being that some universities in the training examples are located in London or Sydney).
As an alternation, we propose a cluster-based query expansion model. In this model, we cluster all queries in the training set and then estimate the  X  X overage X  of each expansion term amongst clusters, instead of queries. The purpose of query clustering is to reduce the influence of a possibly imbalanced distribution of ex-amples. The cluster-based relation query expansion model is for-malised with: where B ( e, C i ) measures whether e is a positive expansion term of cluster C i , which can be simplified as: B ( e, C )=  X  2 where N ( e | C ) is the number of queries that have e as its expan-sion term in the cluster C . P ( e | C ) is the probability that expansion term for the cluster C i , which can be estimated as: Other parameters in Equation 13 are: | C | denotes the number of queries in the cluster, the interpolation weight  X  2 is set to 0.5 and the threshold t is set to 0.3, which are optimal in our experiments.
The clusters are created with the hierarchical agglomerative clus-tering, in which the number of clusters is not predefined. The hier-archical agglomerative algorithm iteratively selects the closest pair of clusters to merge them into a new cluster until the similarity be-tween the closest clusters drops below a given threshold.
Given two R -queries Q r (searching  X  r for  X  r ), Q s (searching  X  s for  X  s ) under relation R , the similarity between the two queries can be calculated as:
Sim ( Q r ,Q s )=0 . 5  X  Sim (  X  r , X  s )+0 . 5  X  Sim (  X  where Sim (  X  r , X  s ) and Sim (  X  r , X  s ) are the similarities between the two query entities and the two target entities respectively. The similarity between two entities is based on context, which can be measured by: Sim (  X  r , X  s )= 1 . 0 if  X  r =  X  s where V is the vocabulary of the whole collection, P ( w | CT the probability of w appearing in the context of  X  r . The context of  X  r can be approximated with words in the proximity of  X  r in the positive relevance feedback set F + which are retrieved by and similarly for P ( w | CT  X  s ) .
In our experiments, three real-world data sets are used to evaluate the effectiveness of learning-based relevance feedback (LRF).
In all experiments, we use Google API 6 to retrieve documents and snippets from the web. The number of feedback documents we use for each query is fixed to 100, which is the maximum that the search engine returns at a time.

For learning expansion terms for each query, when there are more than 100 expansion terms generated, only the top 100 terms with the highest P ( e | Q ) will be considered and verified. In the verification process for expansion terms, if a term can improve the P@100 of a query by more than 30%, it will be taken as a valid expansion term for the query. We adopt 30% since it has shown experimentally to be a good threshold to remove a great number of less effective expansion terms.

An arbitrary threshold would not be appropriate for selecting promising relation expansion terms since it is data-dependent. As a default setup, for each data set, the top-10 expansion terms with the highest score will be evaluated for each model. For all seed queries in a testing set, we use the same relation expansion terms to expand. Queries are expanded with only one expansion term at a time.
We perform experiments with several sizes of training set (|T|=2, 3, 5, 10, 30, 50, 80, 100, 120, 150). For each size, cross-validation is achieved by generating 5 different random sets for training, while all the remaining pairs in each data set are used for testing. The results are presented for learning expansion terms of single relation query with Combined Positional+Semi-Negative model.

As we can see in Figure 2, over the three data sets, the perfor-mance of Q-LRF improves dramatically when the size of the train-ing set is smaller than 10, then improves slightly from 10 to 50. http://www.arwu.org/ http://books.google.com/ http://en.wikipedia.org/wiki/List_of_inventors http://www.google.com The performance stays relatively stable for training sets larger than 50 examples. (a) The P@100 of Snippets Figure 2: The effect of the quantity of training set to the P@100 of snippets of Q-LRF and the join accuracy of the WebRC for each dataset
Based on the observation of Figure 2, we set the default size of training set to 100 in the following subsections, since it is the most stable value to reach almost the best performance across all datasets. The remaining example pairs in each data set are used for testing. For each data set, we present the average results over 5 different random training set.
In this section, we evaluate the four models for learning expan-sion terms for a single relation query. As shown in Table 2, the classic model has by far the largest N 1 ( the average number of ex-pansion terms gen-erated for verification), but the ratio of its valid expansion terms N 2 /N 1 ( N 2 is the average number of remaining expansion terms after the verification process) is the lowest, which means that it generates the most expansion terms for single queries, but most of them cannot pass the verification process. Both Posi-tional and Semi-Negative models generate less expansion terms, but the ratio of valid ones is higher than that of the classic mod-el. The combined model generates the fewest terms and has the highest ratio of valid terms, which is the most optimal in terms of time for verifying the expansion terms. The positional and semi-negative models usually reach a higher P@100 and join accuracy than the classic model, which proves the effectiveness of the two models. The combined model in some cases even outperforms the positional model and semi-negative model taken individually.
In the following subsection, the combined Positional+Semi-Negative model will be set as the default model for learning the expansion terms of all single queries. Now we compare the performance of Query-based model (Q-LRF) and Cluster-based model (C-LRF) in selecting relation ex-pansion terms. Here we set the similarity threshold ending the hi-erarchical agglomerative clust ering algorithm to 0.8, which is an optimal value on both the quality of expansion terms and the out-come of WebRC.

The average precision of top-ranked documents retrieved with expanded queries (or unexpanded queries for the baseline) are list-ed in Table 3. As we can see, the cluster-based model works bet-ter than the query-based model in all cases. We also measure the join accuracy of WebRC by using top-ranked documents retrieved with expanded queries (or unexpanded queries for the baseline). As shown in Table 4, the join results with both C-LRF and Q-LRF reach much higher accuracy than the baseline, which demonstrates the effectiveness of our LRF approach. Additionally, C-LRF X  X  per-Table 2: The Comparison of N 1 , N 2 , N 2 /N 1 , the P@100 of doc-uments retrieved with Q-LRF and the join accuracy of WebRC with the results of Q-LRF, based on the four different models for learning expansion terms of single relation query.
 Invention &amp; Inventor N 1 N 2 N 2 /N 1 P@100 Join formances are always above Q-LRF X  X , which shows the advantage of estimating the  X  X overage X  of expansion terms amongst clusters instead of queries.

The range of improvement of Q-LRF and C-LRF over the base-line mainly depends on the proportion of semi-negative documents in the top-ranked. This is illustrated by the massive improvement over unexpanded queries of Drug &amp; Disease that have a very low P@100 (0.005). Besides, it is also affected by the expansion terms shared amongst different queries of the same relation. The P@N and Join@100 of Book &amp; Author can also be greatly improved s-ince books are usually better in sharing common expansion terms than query entities in other data sets.
 Table 3: Average precision of top-ranked documents retrieved with unexpanded queries or expanded queries generated with Q-LRF or C-LRF models.
 In this paper, a novel retrieval problem named Relation Query Expansion (RelQE) is motivated and formalized on the client-side of web search engines, which can be a crucial step in the scenario of Web-based Relation Completion (WebRC). To solve the RelQE problem, the Learning-based Relevance Feedback (LRF) approach is proposed, which learns common relation expansion terms from existing pairs in relation. According to the experimental results Table 4: Join accuracy of WebRC using top-ranked documents retrieved with unexpanded queries or expanded queries gener-ated with Q-LRF or C-LRF models.
 based on three real-world data sets, both Q-LRF and C-LRF are successful in learning high-quality relation expansion terms, which can be used to effectively improve the retrieval precision of top-ranked results by at most 8.6 times comparing to that of the un-expanded query search. At the same time, they greatly improve the join accuracy of WebRC for at least 37%, and sometimes even as high as 118%, comparing to that based on unexpanded query results. Besides, C-LRF usually works even better than Q-LRF, which proves the advantage of estimating the  X  X overage X  of expan-sion terms among clusters instead of queries. [1] S. Adafre, M. de Rijke, and E. Sang. Entity retrieval. [2] R. French. The computational modeling of analogy-making. [3] V. Lavrenko and W. Croft. Relevance based language models. [4] Z. Li, L. Sitbon, L. Wang, X. Zhou, and X. Du. Approximate [5] Y. Lv and C. Zhai. Positional relevance model for [6] S. Needleman and C. Wunsch. A general method applicable to [7] P. Turney and M. Littman. Corpus-based learning of analogies [8] C. Zhai and J. Lafferty. A study of smoothing methods for [9] J. Zhu, A. de Vries, G. Demartini, and T. Iofciu. Evaluating
