 1. Introduction
Currently, two major systems exist that perform citation analysis: Science Citation Index (SCI) by Institute for Scientific Information (ISI) and CiteSeer by the NEC Research Institute. The idea
SCI for the evaluation of journals spanning many scientific fields, computer science included. The
ISI impact factor (Garfield, 1972, 1994) was the main metric used by SCI for journal evaluation and ranking, a necessary task to make decisions on tenure, funding, salary levels etc. This notion statistical purpose.) On the other hand, the CiteSeer system is a modern system and constructs the citation graph from publications acquired from the web (Lawrence, Giles, &amp; Bollacker, 1999a). CiteSeer is also based on the ISI Impact Factor for ranking conferences and journals (Goodrum, McCain, Lawrence, &amp; Giles, 2001).

Citation analysis is based on the notion of citation graphs , which are graphs representing papers as nodes, whereas an edge from node x to node y represents a citation from paper x to paper y .
Citation graphs can be used to derive useful statistical information related to evaluating and as conferences and journals as scientific collections.

In particular, citation graph analysis is similar to web-graph analysis. Notable is the PageRank algorithm by Brin and Page (1998), which is used by the Google search engine. This algorithm computes a score for a page as a summary of the fractions of the scores of the referrer pages. has been derived that the statistical distribution of the PageRank metric follows the familiar inverse polynomial law reported for webpage degrees (Dhyani, Bhowmick, &amp; Ng, 2002). Recently, the PageRank formula has been further analyzed (Pretto, 2002).

Except ranking, other operations can also be performed on citation graphs by using graph-set of scientific collections, related books, conferences, journals or/and technical reports X  X  X ith regard to the specific area X  X  X an be categorized by using clustering. In an analogous manner, authors can be grouped in clusters in order to find and establish their communities, i.e. authors clusters of authors that mostly cite and mostly get cited, respectively. Two important works on this area are the Kleinberg  X  s HITS (Hyperlink Induced Topic Search) algorithm (Kleinberg, 1999;
Kleinberg, Kumar, Raghavan, Rajagopalan, &amp; Tomkins, 1999), which computes a Weighted score for the above notions. A further study of the web as a graph and the hubs/authorities notions has appeared in Meghabghab (2002).

The structure of this paper is as follows. In the next section we review the SCI and CiteSeer systems and compare their advantages and disadvantages. Also, we review the literature on we investigate new alternative notions besides the ISI Impact Factor, in order to provide a novel approach aiming at evaluating and ranking scientific journals and conferences. Also, we present the basic functionalities of a web-based system that is called Scientific Collection
Evaluator by using Advanced Scoring (SCEAS). Our system has been built by extracting data from the Data Bases and Logic Programming (DBLP) website 1 system, by using the new citation metrics, emerges as a useful tool for ranking scientific col-lections, such as conferences or/and journals. In Section 4, some first remarks are presented in the respect, e.g. about ranking conferences related to databases. The last section concludes the paper. 2. Major systems for citation analysis As mentioned, currently there exist two major systems that perform citation analysis: SCI and
CiteSeer. Here, we will examine closer these systems in order to see their  X  X  X eak X  X  points and motivate the research of this paper.

Beforehand, it is important to notice that SCI has served the whole academic community for several decades by providing useful information, in the lack of anything better. However, now-adays the system disadvantages and limits are apparent. For example, the main disadvantages of the SCI system are: (1) Each scientific field is divided in certain areas, which remain static over the years and do (3) Although any such set is dynamic and updated periodically, this update is done in a subjective (4) In some cases, irrelevant journals (e.g. technical vs. popular) are grouped in a certain area (5) Scientific conferences, books and technical reports are not taken into consideration. (6) It is/was manually constructed and, therefore is an expensive system to built and maintain. (7) It is not for free neither for libraries nor for individuals.

On the other hand, CiteSeer is a modern system and constructs the citation graph from pub-lications acquired from the web (Lawrence et al., 1999a). More specifically, it is an autonomous system that collects computer science papers by crawling the web. Then, from the format each title, authors, etc.) as well as the included citations to construct the underlying citation graph (Lawrence, Bollacker, &amp; Giles, 1999b). The advantages of the CiteSeer system are: (1) It is automatic and transparent; thus, it is objective as human intervention is limited. (2) It takes into consideration all kinds of scientific publications, books and technical reports. (3) It is sensitive to the important fact that there are some highly competitive conferences with
However, the limitations of CiteSeer are: (2) It does not really focus on conference or journal evaluation/ranking. There exist only one The present work is motivated from the latter point. To comprehend the limits of the ISI times a year. On the other hand, a conference is held once per year or even less often. In such a case, what should be the k value of the ISI Impact Factor for a conference? By using a k value we actually evaluate the conference for the period of k previous years. But how could we rank all he the ISI Impact Factor for conference evaluation. In addition, one could argue against the  X  X  X lat X  X  nature of the ISI Impact Factor. For example, fessor  X  X  X nknown X  X ?  X  Is it fair to count a citation from Journal  X  X  X he-top X  X  as equivalent to a citation from Journal  X  X  X he-bottom X  X ? Paper  X  X  X he-worst X  X ?
From these simple questions, it is apparent that it is necessary to embed some kind of weighting to answer such questions. In this work we will investigate some new ideas for ranking scientific in a more generalized perspective. 3. Literature survey
Except SCI and CiteSeer several other digital libraries and indexing systems exist, which perform some kind of citation analysis. As already mentioned, the DBLP website maintained of computer science, and in particular in the areas of Data Bases and Logic Programming (Ley, 2002). More specifically, as of March 2003, the DBLP website contains bibliographic data about 240 000 authors, 1250 conferences, 300 journals and 360 000 papers, articles or books, with links to personal pages, research groups, publishing houses, etc. Navigation through the
DBLP content makes seeking information an easy task for the academics, researchers and professionals working in the areas of Data Bases and Logic Programming. The basic DBLP functionalities are author, conference, journal and term searching. In addition, for a great part of the indexed publications it provides full text retrieval and lists of citations. Although DBLP of the most cited papers in the above areas of computer science. However, this ranked list is based on the DBLP data only, i.e. on the citation lists per publication, which are not ex-haustive.

Recently, another indexing prototype has appeared in the literature (Bradshaw &amp; Hammond, science articles. Rosetta indexes research articles based on the way they have been described when cited in other documents. The concise description that occur in citations is similar to the short with an automatically generated directory of the information space surrounding the query. It is reported that the Rosetta corpus contains over 37 000 indexed documents using roughly 450 000 references.

Another interesting system is AuthorLink by Lin, White, and Buzydlowski (2003), which is a visualization prototype aiming at enhancing author searching. This is achieved by author co-active author maps in real time from a database of 1.26 million records related to Arts and
Humanities and supplied by ISI. These maps contain the 24 authors that are mostly co-cited to the query name, along with some data about counts. The user by clicking to any of these 24 names can proceed and have other maps constructed. In fact this facility helps in several occasions during author searching in a specific narrow area.

Finally, PubSearch is a system developed by He and Hui (2002) aiming at illustrating author co-citation analysis by using a data mining methodology. The authors use a similar technique to that of CiteSeer to collect bibliographic data by crawling the web. The collected entries are in-serted into a data warehouse and an agglomerative hierarchical clustering is performed to con-struct author maps, showing authors with similar interests to a given query name. The system is experimentally tested with data from the ISI Social Science Citation Index (SSCI). More spe-cifically, 1466 Information Retrieval related papers, which appeared during the period 1987 X 1997 in 367 journals, with 44 836 citations were used for system evaluation.

Finally, in Ding, Chowdhury, and Foo (2001) a co-word analysis system is presented. The authors of this paper selected 2012 Information Retrieval related papers from SCI and SSCI, which appeared during the period 1987 X 1997, and extracted 193 keywords, with 5.09 keywords per paper in order to perform co-word analysis and reveal patterns of the specific area evolution with time.

As mentioned, more or less the purpose of the above systems is indexing, citation analysis and visualization; however, their purpose is not ranking. With regards to ranking we meet several efforts in the literature spanning several disciplines except computer science.

For example, a citation analysis based ranking study (Baumgartner &amp; Pieters, 2000) is reported for Marketing journals. In essence, the authors performed a manual extraction of citations of the 1996 X 1997 issues of 49 marketing journals (26 of them not contained in the SSCI), where the collections of the 49 titles was largely based on a survey. A non citation analysis based ranking was reported in Hult, Neese, and Bashaw (1997), where the authors rank marketing related on a survey.

In Tahai and Rigsby (2002) the authors  X  concern is the area of Accounting. They obtained data from the SSCI (i.e. they extracted 351 articles from 8 journals published during the period 1992 X  1994 with 11 746 citations) and considered the notion of ISI Impact Factor and proposed vari-(1999) the author criticizes the theory and methodology of ranking journals having in mind Law related journals.

Mylonopoulos and Theoharakis (2001) focused in the area of Information Systems and per-formed an on-line survey for 87 journals with 1000 respondents approximately. Although, this ranking according to readers (and authors) perception as a function of the geographic location of the respondents.
 Having the same motivation with our paper, Keijnein and Groenendaal focus in the area of
Information Systems and try to rank journals, conferences and books, i.e. a larger set of publi-cations in comparison to the SCI practice (Kleinjnen &amp; Groenendaal, 2000). Using sampling, according to the number of citations received. However, the limitations of this work are:  X  it is restricted to a very specific domain of computer science,  X  it is a manual method that uses only a small set of publications (e.g. only 123 journal articles 1532 proceedings articles, 1577 books and 664 other publications),  X  like SCI mixes up diversifying publications (e.g. technical vs. managerial), of k  X  2 or 5 years as ISI proposes) in the course of Impact factor computation. This may give seriously misleading results by not capturing the dynamic nature of science evolution and au-thor preferences, since a journal may be ranked high in the past, but low currently. 4. The SCEAS system We built a system called Scientific Collection Evaluator by using Advanced Scoring (SCEAS).
Our system imports DBLP XML records into a MySQL database system. We have used the mainly reads rather than updates. The SCEAS system is available on the Internet user can easily access it, postqueries, get answers, and extract useful information. In our model, the main entities are: Publications which could be articles, in-proceedings (according to the latex terminology) etc. Collections such as conferences, books, journals etc.
 Persons which could be authors or editors.

Each publication belongs to a collection (or more, e.g. conference publications belong to a conference and to a proceedings collection). A collection may be a part of another one, e.g.
VLDB  X  97 is a collection and it is part of the VLDB collection. Persons can be related to publi-related to each other with the  X  X  X itation X  X  relation.

Based on the DBLP database, we built the citation graph of the collection, which includes journal as well as conference publications. Using this graph we derived two Collection Citation
Graphs, the Conference Citation Graph, and the Journal Citation Graph. In the same way, any other type of semantic grouping of the publications can be used to derive analogous citation graphs (e.g. Book Citation Graph). Fig. 1 is a small sample of a Conference Citation Graph. It consists of four nodes standing for the conferences SIGMOD, VLDB, PODS and ICDE of all years. The weight of the arc from node i to node j depicts the number of citations from all the publications of the first node to all the publications of the later node.
 all the citations should have the same weight. For example, the weight should depend on two two tasks that should be done. First, we must specify the scientific domains and then perform the ranking.

In particular, we have performed the following tasks: (1) Cluster the conferences based on the conference citation graph after a preprocessing phase (2) Cleansing, since the data of many conferences in the DBLP database are not complete. There-(3) Finally, ranking each conference cluster separately. We have performed ranking by taking  X  rankings using Plain Scoring,  X  rankings using Weighted Scoring, and  X  rankings using the Inverted Impact Scoring,  X  rankings using Weighted Inverted Impact Scoring.

All these new notions introduced above will be explained in the sequel. Please, notice that ranking is performed by using several algorithms, which will be presented in the sequel as well. 4.1. Clustering conferences according to topics
Based on the conference citation graph (as the example of Fig. 1), first we performed a clus-tering operation. As a utility for clustering the conferences we used the hMetis (Han, Karypis,
Kumar, &amp; Mobasher, 1997; Karypis, Aggarwal, Kumar, &amp; Shekhar, 1997), the leading hyper-graph partitioning tool for large hyper-graphs. hMetis has been successfully used in applications related to VLSI circuits, data mining and numerical analysis.

Fig. 2 shows an example of a conference citation graph, where nodes represent conferences, whereas edges represent citations. The edges are directed and weighted. An edge from node A to node B with weight w , means that there are w citations from publications of conference A to publications of conference B .

To perform clustering, we need to minimize the sum of the edge weights that cross from one cluster to another (this is computed by hMetis). For this purpose we do not need a directed graph.
Thus, we convert the later to an undirected graph (not avoiding some loss of information). In our example, Fig. 3 is produced from Fig. 2. In this graph, the weight on an edge that connects two nodes represents the total number of citations that these conferences make to each other. The derived graph cannot be used for ranking, which is our main purpose, but only for the clustering step. Actually, this is the graph type that we feed to hMetis for the clustering.
Clustering results were not perfect, since part of the citation graph is incomplete (i.e. no ci-tations are included in some DBLP records for a few conference publications). Therefore, we re-cluster the conferences, after performing some preprocessing based on keyword matches in the conference titles. In particular, we predefine 4 clusters:  X  Cluster 1: Databases . The keywords used to identify the conferences that belong in this domain where: data base, database, digital library, information retrieval, information system, mining, and geographic. knowledge, logic, and algorithm.  X  Cluster3: NetworksandDistributed Systems . distributed, network, parallel, web, www, and w3c.  X  Cluster 4: Operating Systems, Software Engineering, Compilers and Languages . for the respec-tive areas.

After defining the above 4 clusters with some conference members in them, we feed this pre-defined partitions to hMetis to continue with the unclustered conferences. 4.2. Cleansing the clusters
As mentioned above, the DBLP database is incomplete. For example, for some conferences or ranking algorithm to produce erroneous results, since the main metric is the average number of citations per publication.

In this step we exclude from the conference set, which will be used for ranking, the conferences that:  X  contain less than 3 publications,  X  are held only once, and  X  have average number of publications per year less than 0.5.

For these conferences we set a flag meaning that they will not be ranked, but we do not delete them from our database. Thus, any citations included in them do count. 4.3. Definition of the metrics
Here, we introduce the new metrics in order to establish a new perspective for conference and journal evaluation using the citation graph. These metrics are defined as follows: 4.3.1. Plain Score
If C is the set of all the conferences, then the Plain Score , S defined as: where N i ! c is the number of citations made from conference i to conference c , whereas the nor-malizing factor P c is the number of publications in conference c .

The rank is computed by ordering the conferences  X  scores. In case of a tie, the conference with the fewer publications precedes. Actually, this score is exactly the same as the in-degree of the corresponding node in the conference citation graph, divided by the number of publications in-it can only be used to compare and rank a set of conferences that have exactly the same life-time. 4.3.2. Weighted Score
Here we introduce the idea of the weighted ranking. This means that the citations do not count the same. Eq. (2) shows abstractly how the Weighted Score for conference c , defined as WS be computed.

How can we compute the weights? Which conferences are  X  X  X he-Top X  X  that should have larger weights and which are the  X  X  X he-Worst X  X  conferences? Here, arises the need for recursive com-puting. This computation is performed by using the following formula: next level, based on the weights computed in the previous one. The ranking we get at level 1, is equivalent with the Plain Score ranking (since we used weights of 1 for all entities). After com-algorithm. In Section 4.5 a detailed discussion on the computation of weights can be found.
After computing the weights for level 1, we continue computing the scores for the next levels by applying the same procedure until the ranking remains unchanged. This is our termination with that of the level L 1. Alternatively, if while at level L we get the same weights as in level weights. Then 8 l 2f L ... 1g the condition: WS i ; l  X  1
This type of ranking, similarly to the Plain Score, cannot be used for conference evaluation without risk. Despite the refinement of computing the average score per publication by means of more citations. This ranking can be used only for the conferences that have exactly the same life-time. 4.3.3. Plain Score per Year
Adapting the notion of the Plain Score in order to rank conferences for each distinct year, we introduce the Plain Score per Year metric as: where SY c ; y is the score for conference c in the year y , N conference i to conference c that was held in year y and P conference c during the year y . In particular, a more detailed expression that we used for our computations is: where N i ; z ! c ; y is the number of citations made from conference i in year z to conference c the current year). This ranking can be used to compare conferences that were held in the same year.
 4.3.4. Weighted Score per Year By combining WS (Weighted Score) and SY (Plain Score per Year), for l P 1 we produce the WSY, i.e. the Weighted Score per Year metric: The same way as above we set: The computation is made for each year by starting from the last year in reverse order.
Therefore, when computing scores for year Y , all the weights are known for years 4.3.5. Inverted Impact Score per Year Garfield (1994) defined the ISI Impact Factor by using the following example for the 1992 year: then form: that when we compute the ISI Impact Factor for a conference c for year y , we actually evaluate the events of c that were organized during the previous k years. For example, in order to compute the ISI Impact Factor of VLDB  X  95, we actually evaluate VLDB  X  94 and VLDB  X  93. This way, two distinct events (that have been organized in different continents) of a specific conference are grouped and evaluated together. Perhaps, we could evaluate VLDB  X  95 by computing the ISI Impact Factor for VLDB  X  96 and VLDB  X  97. In such a case, our results are not affected from the of conferences, the ISI Impact Factor cannot be used to evaluate a specific conference c held in year y .

For the above reasons, we  X  X  X evert X  X  the concept of ISI Impact Factor and instead of counting the citations made to the k previous years, we count the citations made during the next k years Let this factor be the  X  X  X nverted Impact Factor X  X  or  X  X  X -Impact Factor X  X . The I-Impact Score per Year is defined as follows: impact of VLDB during 1998 X  X  X ctually the impact of VLDB97 and 96). In (9) the impact is computed for a specific year of the conference (e.g. What is the impact of VLDB97?). This way we can rank individual conferences and for example we can get the information: which was the most  X  X  X uccessful X  X  conference in 1997. Actually, this is the reasoning why the VLDB Foundation es-tablished the 10-years best-paper award.

The Inverted Impact Score (Eq. (9)) metric is a sub-case of the Plain Score per Year algorithm if
Impact Factor is widely accepted, we use this metric in our tests as the basic metric to compare with. We cannot use for comparison the ISI Impact Factor as it is exactly defined by Garfield (1994) because it is semantically different from the metrics presented here. 4.3.6. Weighted I-Impact Score per Year
The same way, if in Weighted Score per Year (Eq. (6)) we set last year  X  y  X  k , where k  X  2or 5, then we get the I-Impact Score in a weighted manner, let it be WIISY vantages of the I-Impact Score metric, plus the advantages of a weighted metric. 4.4. The ranking algorithm The ranking algorithm is shown in Fig. 4. This algorithm is used for all 4 types of scores. The
Plain Scores per Year are the results of the algorithm of level 1. The Weighted Scores are the (instead of 1 for practical reasons). The I-Impact Scores can be computed by the same algorithm by setting the variable last year to y  X  2. 4.5. The weight set For every distinct ranking 4 we need to define a set of sets: where
We have to assign a specific weight value to each set G i the set:
At this point, it is necessary to introduce two important parameters. The number of clusters ( n ) and the range for the weights . For our tests, we have set the number of clusters equal to 5 (meaning: very strong, strong, average, weak, very weak). This leads us to have 5 distinct weights and 5 clusters in the conference ranking.

The selection of the weight range is also important as it affects the results in the sense that it tunes the importance of a citation from a  X  X  X ery strong X  X  conference in comparison to the im-portance of a citation from a  X  X  X ery weak X  X  conference. We decided to use the range 1 X 5 and specifically the weights W 1  X  1, W 2  X  2, W 3  X  3, W 4  X  not make any difference.

Actually, since the scores are normalized by dividing with the sum of weights, the important factor is the fraction of the weights divided with the minimum one, and not the absolute values.
Thus, it is safe to accept a minimum weight of 1. It is obvious that there is no sense in using a negative or zero weight. 5
Also, we defined that the conferences which belong in a different scientific domain than the one the ranking is computed for, to be members of the G 1 set. In addition to that, conferences that have zero score ( 0citations to them), are set by the classification algorithm in group G 4.6. Clustering conferences according to citations
The clustering algorithm is a hierarchical clustering algorithm applied on one-dimension points of conferences for ranking.

For each cluster we set G A x as the average value of G x find two sets G i and G j , for which the difference of their average values ( j G A of any other pair. We define a new set G k  X  G i [ G j and we delete the sets G until we get j G A i G A j j &gt; 0 8 G i ; G j 2 G . In Fig. 5 we show the clustering algorithm. 4.7. Weight refinement
The Weighted Score algorithm, as described above, is open to deadlocks. This is due to the fact that there is no guarantee that a conference will not move from one cluster to another at some point during the algorithm execution. We illustrate this situation with a simple example of two conferences A and B for which:
In this case: order to avoid this case, after the computation of the clusters G been raised in a previous level d . If there is a level d &lt;  X  l 1  X  for which W 8 c 2 C 7 , then we do not set W c ; l  X  W k (as we should do), but instead we set: x very fast.
 In the previous example the next steps should be: 5. Results
First, we note that the ranking is made for only one out of the four clusters that have been in the DBLP database. The database contains conferences from 1959 to 2003 (but  X  X  X omplete X  X  data for these conferences exist only for the year 198 0and afterwards). Thus, we find the ranking for each year separately by using:  X  the Plain Score per Year,  X  the Weighted Score per Year,  X  the Plain I-Impact Score per Year,  X  the Weighted I-Impact Score per Year.
 reach it if we set the variable last year equal to y  X  2.
Weighted Score and the Weighted I-Impact Score, a total of 88 individual rankings (the plain ranking is a sub-result of the weighted one).

An important concern is the computational cost, e.g. how many times we have to repeat the computation in order to get the termination condition  X  X  X o change X  X  in the course of ranking. As three instances (once for the Weighted Score and twice for the Weighted I-Impact Score) we had to reach level four. 5.1. Rank comparisons
In order to visualize the comparison of the various ranking results, we use q X  X  plots (quantile X  quantile plots), which illustrate the quantiles of one univariate distribution against the corre-sponding quantiles of another (in our case, we compare the rankings). Therefore, for comparing the type A ranking to type B ranking, for each conference c in our rank table, we put a dot in the using type B . Thus, the x -axes represent positions computed by A and y -axis positions computed by B . The two rankings would be equivalent iff y  X  x for every point in the graph. It is easy to  X  X  X ery strong X  X  or  X  X  X ery weak X  X  conferences but mainly for  X  X  X verage X  X  cluster.
In all q X  X  plots that compare I-Impact Score ranking (either Weighted or Plain) and Score ranking (either Weighted or Plain) (Figs. 7a and 8a, b), there are some outliers (marked as (blue squares in the web version)), for which x y , meaning that they have much better rank position by using Score than I-Impact Score. This is due to the nature of the Impact Score notion, where 2 years, but they have citations until  X  X  X ow X  X .
 (Combinatorial Pattern Matching 9 ). The specific conference does not get any citations during the to the academic community, we have to see the Score Ranking.

In an analogous way, the outliers for which y x , are conferences with big I-Impact Score but 7b and 8c and d, the outliers are very close to the line y  X  x and quantitatively few. This means, the fraction W 5 = W 1 we have used is high (  X  5). There are some re-orderings, which help in re-see that the HT Conference (  X  ACM Conference on Hypertext) and the SPIESR Conference (  X  Storage and Retrieval for Image and Video Databases) have swapped positions after com-puting the Weighted Score. In the Plain Score the scores for these two conferences are very close.
The Weighted Score for  X  X  X T X  X  is greater than the one of  X  X  X PIESR X  X , as it has more citations from  X  X  X ery strong X  X  conferences. 5.2. Rank results
Besides the introduction of the new metrics for citation analysis, in this paper we report some
SCEAS system are made by using two ways:  X  By a Rank table, assuming a specific type of ranking and a selected year. For example, in Table 2 we present the ranking by using the Weighted Score for year 1995.  X  By a Historical chart, where we can view the whole history of a conference for any specific type of ranking.

In Fig. 9, the history of ranking of VLDB conference is presented, according to all types rankings. Each bar consists of three parts. The bottom part (black; blue in the web version) gives gives the percentage of conferences that have a higher ranking, whereas the middle part (light grey; green in the web version) gives the percentage of conferences that have equal ranking. In addition, the ratio below the x -axis gives the relative rank for each year. A different position  X  X  X ery strong X  X  one during all years. 11
Note that the last 2 years ranking (i.e. 2001 and 2002) could not be considered as reliable, since there are no citations in our database to conferences organized during these years. The scores for all conferences in 2001 and 2002 are zero and, thus, the rank is up to the number of publications. three years of the rank history.
 6. Conclusion and future work
In this paper we first presented an overview of two major current systems for conference and they are based on the ISI Impact Factor, thus, considering citations in a flat way, e.g. without paying attention to the quality of the respective publication. Therefore, we introduced four new metrics in order to cure this deficiency, which are suitable for considering both journal and conference publications. These new metrics are used by a system that we have built, the SCEAS system (Scientific Collection Evaluator by using Advanced Scoring). The system is autonomous and has the following characteristics:  X  it imports the DBLP bibliography records into a local database (it could be extended to import any other scientific collection of publications),  X  it partitions the imported collection into clusters according to the topic of the conference and performs a cleansing step to provide reliable information,  X  it performs the ranking by all four metrics for the conferences that focus in databases.
The web user of SCEAS system has access to all the results produced at any stage of the rank process, can compare the various rank metrics and can study the rank results in order to derive useful information regarding the quality of the database conferences.

In the future we plan to extend the system by:  X  Computing more variations of the weighted metrics in which self-citations of the collection could be excluded or taken into account multiplied with a smaller weight.  X  Perform detailed citation analysis of each article and compute aggregated results for each col-lection.  X  Extending the ranking in more collections/scientific domains. This could give us the ability when ranking a cluster (e.g. DB conferences) to take into account  X  X  X eighted X  X  citations from other type of collections belonging in the same scientific field (e.g. DB Journals and Books).  X  Improving the clustering according to scientific domain by allowing one entity to be a member of more than one clusters.
 References
