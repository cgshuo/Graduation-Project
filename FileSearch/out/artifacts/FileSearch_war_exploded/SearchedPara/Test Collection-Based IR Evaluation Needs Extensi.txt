 very short queries but may try out several queries in a session [1-5]. Such queries may consist of only 1-2 search keys. Smith and Kantor [3], and Turpin and Hersh [5] both found that real users successfully compensa ted for the performance deficiencies of retrieval systems by issuing more queries and/or reading more documents. In real life a searcher typically issues an initial query and inspects some top-N result documents. If no or an insufficient number of relevant documents are recognized, the user may repeatedly launch further queries until the information need is satisfied or (s)he gives up. This setting is different from the Cranfield style IR experiments based on verbose result, measured in terms of available single query metrics, such as mean average cost, and short queries are not rewarded for their minor formulation costs. In the present paper, we take another look at user behavior and IR evaluation. In real reformulation. Searchers optimize the total cost-and-benefit of their entire sessions. This may render sessions of short queries reasonable. They allow minimal query for-mulation costs while taking chances with the quality of results. We call such queries as trivial queries: they employ very few search keys in various combinations. Typical real searchers interact with IR systems using such trivial queries. 
We show in this paper that trivial queries surprisingly quickly yield reasonable re-vance assessments are available. We will define idealized trivial query strategies and run systematically constructed sessions seeking to find one relevant document (using two distinct relevance thresholds) in Top-10 of each query, and reformulating the query in case of failure. To render our simulation empirically well-founded, we col-lected data for query candidates from test persons. Our findings theoretically and experimentally motivate the observed real-life user behavior, which real users must have learned through experience, when interacting with IR systems. As few very benefits of IR sessions before presenting the research problems. Section 3 discusses the construction of simulated sessions. Experimental results are given in Section 4 and discussed in Section 5. Section 6 presents our conclusions. 2.1 User Behavior Real searchers behave individually during search sessions. Their information needs may initially be muddled and change during the search process; they may learn as the session progresses, or switch focus. The initial query formulation may not be optimal and the searchers may need to try out different wordings [2]. In fact, it may be impos-sible for the searcher to predict how well the query will perform [7] because even if the query describes the topic well, it may be ambiguous [8] and retrieve documents that the users may have to revise their topical queries. 
Real-life searchers often prefer very short queries [1, 4]. They may also avoid ex-cessive browsing [1, 9]. Jansen and colleagues [1] analyzed transaction logs contain-ing thousands of queries posed by Internet search service users. They discovered that number of terms used in a query was even smaller, 1.45, in a study by Stenmark [4] focusing on intranet users. 
The stopping decisions regarding browsing the retrieved documents depend on the search task and the individual performing the task [2]. Jansen and colleagues [1] ob-results retrieved. Therefore real life sessions often consist of sequences of very short queries. The data in Table 1 reflect these findings. 
The data for Table 1 come from an empirical, interactive study comparing two search systems. Thirty domain experts each completed the same four realistic search tasks A  X  D simulating a need for specific information required to make a decision in a short time frame of several minutes. Each task formed a separate query session. The (a geographic, document type or other condition). Only 10 among the 60 sessions employed four or more unique search keys . These searchers were precision-oriented, i.e., they quit searching soon after finding one or a few relevant documents. The four bottom lines report the frequency of the query strategies (S1-S3) that we shall define sessions (60) because more than one strategy was employed in some sessions. Variable A B C D Tot the Cranfield type IR evaluation scenario. In the traditional test collection-based evaluation a single query per topic exists an d the queries used are longer (typically 7 to 15 search keys, see [1]). Because of these facts we will focus on trivial query ses-sions in the present study, including one-word queries for each topic. 2.2 Identifying Costs and Benefits What explains the great difference betwee n user behavior and effective laboratory ciently taken into account to explain user behavior. Early papers on IR evaluation had a comprehensive approach toward evaluation: Cleverdon, and colleagues [11] identified, among others, presentation issues and intel-lectual and physical user effort as important factors in IR evaluation. Salton [12] iden-tified user effort measures as important components of IR evaluation. More recently, Su [13] compared 20 evaluation measures for interactive IR, including actual cost of search, several utility measures, and worth of search results vs. time expended. 
Due to time pressure, documents retrieved in the top ranks may be of interest for real users [9, 14]. J X rvelin et al. [2] extended the Discounted Cumulated Gain metric [6] into a session-based evaluation metric which evaluates multiple query sessions and takes the searcher X  X  effort indirectly into account. Also the literature on usability has a comprehensive approach to costs and benefits, see, e.g., the ISO standard [15]. 
One may conclude that various costs and benefits of interactive IR systems have practices. In interactive settings both costs and benefits are present and affect searcher behavior (e.g., through expectations). Ther efore, interactive IR evaluation should incorporate the existing cost factors: search key generation cost, query execution cost, result scan cost, next result page access cost, and relevant document gain. Contempo-rary IR evaluation effectively assumes all costs as zero, thus focusing on benefits (the gain) at any cost. This hardly models real-life situations.

In the present paper we acknowledge that the query formulation costs may be a reasonable alternative for the user because their formulation costs are minimal and their effectiveness competitive if sessions are allowed. 2.3 Research Problem Our background assumption is that the observed user behavior [1, 4, 10] does satisfy real needs. Thus, the obvious question is whether it makes sense for the user to com-and reformulate in case of a failure. Our overall research question therefore is: What traditional approach of utilizing one verbose query?. 
In studying this problem, we make simplifying assumptions. First, we assume that the topical requests remain unchanged during a session: the simulated searcher neither learns nor switches focus during the session. Secondly, the simulated searcher is able to recognize the relevance of documents (see [16]). Third, the simulated searcher is assumed to scan the ranked list of documents from the top to bottom (see [17]). How-ever, reflecting the observed searcher behavior, we focus on the Top-10 results. We will focus on the search task of finding a single relevant document (see [18]). We made use of the TREC 7-8 test collection, and real test persons to generate candi-date queries and alternative search keys. The collection of these data is explained next session strategies, the retrieval protocol, and our evaluation method. 3.1 The Test Collection and Search Engine We used the reassessed TREC test collection including 41 topics from TREC 7 and 8 ad hoc tracks [19]. The document database contains 528 155 documents organized under the retrieval system Lemur . The database index is constructed by lemmatizing the document words. The relevance judgments were based on topicality using a four-point scale: (0) irrelevant document: the document does not contain any information about the topic; (1) marginally relevant document: the document only points to the fairly relevant document: the document contains more information than the topic the document discusses the themes of the topic exhaustively. In the recall base there documents for each topic [19]. 3.2 Collecting the Query Data As session-based collections do not currently exist we decided to construct one by ourselves on top of the TREC 7-8 test collection. 41 topics were analyzed intellectu-ally by test persons to form query candidate sets. During the analysis the test persons did not interact with a real system. They probably would have been able to make higher quality queries had they had a chance to utilize system feedback. 
A group of seven undergraduate information science students (Group A) and seven staff members (Group B) performed the analysis. Staff members having an extensive background regarding the specific test coll ection were excluded. Regarding each topic sons. Each of the 41 topics was analyzed twice -once by a student and once by a staff member. The users were asked to directly select and to think up good search words from topical descriptions and to create various query candidates. 
First a two-page protocol explaining the task was presented by one of the research-were presented for the users. Descriptions regarding non-relevance of the documents minutes per topic. The test persons were asked to mark up all potential search words directly from the topic description and to express the topic freely by their own words. Third, they were asked to form various query candidates (using freely any kinds of words) as unstructured word lists: (i) the query they would use first ( X 1 st query X ); (ii) versions of various lengths: (iii) one word (1w), (iv) two words (2w), and (v) three or more words (3w+). The very last task was to estimate how appropriate each query candidate was using a four-point scale. 3.3 Simulated Session Strategies Using the query data collected from the test persons we created four simulated session strategies (S1-S4) for the experiments. The strategies S1-S3 model five-query sessions (short queries), while Strategy S4 acts as a comparison baseline and utilizes only one , S1 to S3. In each session, the simulated searcher inspected at most 50 documents: in S1-S3 at most five distinct Top-10 results, and in S4 at most the single Top-50. Session Strategy S1: On e-word Queries Only In S1 strategy we experiment solely with one-word queries . Unique individual words are selected randomly from various query types in the following order:  X 1st query X ,  X 2nd query X ; 1w, 2w, and 3w+ queries until five distinct words are collected. Within a session, if any given one-word query does not retrieve a (highly) relevant document within its top-10 ranks, we immediately try out the next one-word query. S1 is justi-fied because (1) extremely short queries dominate in real life, and (2) the strategy was employed 21 times in the 60 real-live sessions of Table 1. Random selection obvi-ously creates some bad one-word queries. We purposefully experimented with such a often happen in real life. Session Strategy S2: Incremental Query Extension In S2 strategy we experiment with using incrementally longer queries in sessions. As stated above, our test persons were requested to form one-word (1w), two-word (2w), and longer (3w+) query versions, and the queries they would try first (1st query) and each query version (using query versions in the order explained above) until a se-quence of five, if possible, unique words w1,..., w5 is formed. These words are used to construct queries of varying lengths (i.e., w1; w1 w2; ...; w1 w2 w3 w4 w5) (from 1 to 5 words) for each topic. Within each topical session, the searcher starts with the one-word query. If a query does not retrieve the required (highly) relevant document, was employed in 13 times of the 60 real-live sessions of Table 1. It simulates a lazy searcher who tries to cope with minimal effort and adds one word at a time. Session Strategy S3: Variations on a Theme of Two Words modifications to successive queries are done in small increments by modifying, add-ing or deleting keys. We used first three words of the 3w+ query as the starting point, and varied randomly the third word by repl acing it with distinct words selected from the 3w+, or from 1st, 2nd, 1w or 2w queries (in that order) if 3w+ ran out of words. Session Strategy S4: Single Verbose Query Session strategy S4 consists of a single verbose query. It contains all the words of the tional laboratory testing and serves as a baseline. 3.4 Retrieval Protocol and Evaluation The run procedure went as follows: 1. Based on session strategies S1 to S4 query sequences were constructed. 2. Top-10 documents were retrieved for each query in S1-S3 and top-50 for S4. 3. Success of each session strategy S1 to S4 was determined. Stopping decisions often depend on the task, context, personality, and the retrieval results [10]. In this study, the stopping condition was defined as finding one relevant document. Failure was defined as inability to find a relevant document in a session. Above, general result for session strategies S1-S4 is presented for 41 individual que-ries (Table 2). Number 1 denotes that the first query in the session was successful in the second query being successful etc. The table shows the effectiveness of session strategies based on both liberal and stringent relevance threshold. The columns for the two searcher groups A and B indicate the variability under the same strategy. Color-coding is used in cells in addition to the ordinal numbers for visual evaluation: black indicates the first query being successful, white no success at all, and grey scale suc-cess by a non-first query. Based on liberal relevance criteria, S1 ( one-word queries only ) is 15-25 percent units weaker than strategies S2-S4 in its success ra te (Table 3). Strategies S2-S4 are equally good. The average number of queries in S1-S3 varies between 1.2 and 2.4 queries. Strategy S3 fairs almost as well as S4. On th e stringent level S1 is clearly weaker than the other strategies. Surprisingly, S2 and S3 are only 10  X  15 percentage units weaker that S4. The average number of queries in S1-S3 varies between 2.0 and 3.1; the aver-age number of pages browsed for S4 is 1.6. 
According to Friedman X  X  test the differences between the strategies are highly sig-Group A. We observe that S1 is significantly different from others; S3 is not signifi-cantly worse than S4, the queries require much less effort. 4.1 Liberal Relevance Threshold At the liberal relevance threshold the most successful strategy was the baseline ses-S4 strategy, number 1 in column S4 denotes the fact that the relevant document was found within the first results page (ranks 1 to 10); number 2 denotes second page, etc. queries -an average query length of 16.9 words per query. 
Among the trivial strategies S3 ( variations on a theme of two words ) was the most successful one. For 36 and 34 topics out of 41 (user group A and B, respectively) the For 27 and 29 topics (for A and B) the very first query (at this point a single key) was successful. Adding the second key to the query helped to find a relevant document for 9 and 7 additional topics (A and B), and the third key for 3 and 4 topics (A and B). the least successful. The fact that the single query keys were selected randomly obvi-ously hurt the performance. Yet, it failed only in 6 and 10 topics (A and B) out of 41. 4.2 Stringent Relevance Threshold If liberal relevance threshold is used, low quality documents are accepted as relevant. These documents may be only marginally relevant and escape the reader X  X  attention only had to pay minimum effort. Therefore, in the remainder of this paper we only accept highly relevant documents as relevant. They discuss the themes of the topic extensively [19] thus better justifying the user X  X  stopping decision in simulations. 
Table 5 summarizes the results based on stringent relevance threshold. The recall base of the test collection did not contain highly relevant documents for 3 topics (#378, #414, #437), leaving 38 topics in the stringent relevance threshold case. Cumu-lative percentages are shown in the table regarding the share of successful topics for each session strategy. Also when highly relevant documents are demanded, the baseline session strategy document is found within the first page. Table 5 presents these success figures as percentages (29/38 = 76.3 %). For three topics the second page needs to be inspected in strategy S4; for one topic the third page; for two topics the fourth, and for one topic the fifth page. This strategy fails only for two topics. 
Session strategy S3 ( variations on a theme of two words ) was the most successful one among strategies S1-S3 also when highly relevant documents are requested. For 21 and 26 topics out of 38 (for user groups A and B, respectively) the very first query was successful. For 7 and 8 topics (groups A and B) the strategy failed. Que ry # A B A B A B -1 st 23.7 39.5 44.7 47.4 55.3 68.4 76.3 2 nd 47.4 55.3 76.3 68.4 71.1 73.7 84.2 S1 8.6 3.5 3 rd 57.9 57.9 78.9 78.9 78.9 76.3 86.8 S2 4.3 2.3 4 th 60.5 60.5 84.2 81.6 78.9 78.9 92.1 S3 7.3 2.4 5 th 60.5 60.5 86.8 84.2 81.6 78.9 94.7 S4 16.9 1.0 The session strategy S2 ( incremental query extension ) was also effective. For 17 and 18 topics (A and B) the very first query (i.e., a singe key) attempted was successful. Adding the second key to the query helped to find a highly relevant document for 12 and 8 of the so far unsuccessful topics (A and B). Only in 5 cases for group A and in 6 cases for group B the incremental extension strategy failed. 
The session strategy S1 ( one-word queries only ) was the least successful, yet strategy was successful after only three single-word queries were attempted. 
We experimented also by measuring the effectiveness of S1 using traditional met-rics (P@10 and non-interpolated average precision (AP)). We evaluated the effective-ness of all query candidate sets, 1 st to 5 th queries for group A and B), averaged over 38 topics, based on the top-1000 documents, and stringent relevance threshold. The highest value observed for P@10 was 7.4 % and for AP 11.3 %. The corresponding values for S4 (verbose queries) were 25.2 % and 19.5 %. Thus, if one query per topic is assumed, S1 queries are inferior, but they make sense if multiple queries are used. Real users of IR systems typically search by very short queries but may try out several queries [1, 4]. Test collection-based evaluation, e.g., like the one performed in TREC [20], typically employs longer queries and one query per topic. We analyzed the ef-fectiveness of sessions of very short queries. We performed a simulation because it is types, avoid learning effects, and support repeatability of the experiment. The strengths of our approach include session strategies and intellectual word selections for queries based on an empirical ground truth. 
We therefore had two sets of test persons to create realistic content for trivial que-ries. They did not interact with the retrieval system or test collection and thus did not only makes our argument stronger. The idealized session strategies were constructed based on empirical data (Table 1). We set the limit of maximum of 5 queries per session because longer sessions are rare in real life. We set the limit of maximum of 10 documents per query results for strate-gies S1-S3 because scanning length in real life is limited [1]. The simplest strategy S1 ( one word queries ) was popular in our sample data (Table 1) and attempts to minimize three word queries ) seeks to fix a focus by two keys and vary by trying out different third keys. This was the most popular strategy in our sample data and to some degree corresponds to Bates X  Berry-picking strategy [21]. S4 represents a long TREC-type of single query strategy, and did not occur in our data. 
Table 6 shows the expected number of search keys and queries, when each strategy is successful. Regarding S1-S3, we assume that for unsuccessful topics the searcher would in desperation launch one more query (#6), a successful one represented by S4 containing on average 16.9 search keys, to guarantee comparable performance. Strategies S1-S3 yield a low query formulation cost in the number of search terms. If make sense to users. They mean low formulation costs while taking chances with the result. We focused on the limited task of finding one relevant document but our simu-lation method fits well to tasks where more than one relevant document is required. Traditional laboratory evaluation is not well-suited to study this phenomenon. 
We demonstrated session-based batch evaluations utilizing test collections and query data collected from test persons. We focused on studying the effectiveness of sessions of very short queries. We assumed searchers requiring one relevant document, browsing a limited length of results, and using a limited set of session strategies. Short query sessions turned out to be successful. 
Future evaluation should model processes where the searcher may try out several quality of the retrieved result. This may help toward resolving the current disparity of the observed searcher behavior and the assumptions of laboratory experiments. Lemur and ENGTWOL were used in the study. See http://www.lemurproject.org. ENGTWOL: Copyright  X  1989-1992 Atro Voutilainen and Juha Heikkil X . TWOL-R: Copyright  X  Kimmo Koskenniemi and Lingsoft plc.1983-1992. This work was funded by Academy of Finland under grants #120996 and #124131. 
