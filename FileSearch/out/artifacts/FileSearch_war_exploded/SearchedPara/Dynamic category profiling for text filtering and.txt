 1. Introduction
Information is often represented in text form and classified into multiple categories. In the information space spanned by the categories, upon receiving a document, automatic text filtering and text classification are essential. For each input document d , text filtering aims to filter out d if d falls out of the information mation space.

One of the popular ways to achieve the task is to delegate a classifier to each category. The classifier is asso-ciated with a threshold, and upon receiving a document, it may autonomously make a yes X  X o decision for the corresponding category. A document is  X  X  X ccepted X  X  by the classifier if its degree of acceptance (DOA) with respect to the category (e.g. similarity with the category or probability of belonging to the category) is higher than or equal to the corresponding threshold; otherwise it is  X  X  X ejected. X  X  With the help of the thresholds, text filtering is actually achieved in the course of text classification. Each document may be classified into zero, one, or several categories.

Unfortunately, since no classifiers may be perfectly tuned ( Arampatzis, Beney, Koster, &amp; van der Weide, 2000; Liu &amp; Lin, 2004; Zhang &amp; Callan, 2001 ), estimation of DOA values does not always be proper. A doc-ument that is similar to a category does not always get a higher DOA value with respect to the category. Sim-ilarly, a document that is not similar to a category does not always get a lower DOA value with respect to the category. Improper DOA estimations may heavily deteriorate the performance of both filtering and classification. 1.1. Problem definition and motivation
In this paper, we explore how various classifiers X  performances in text filtering and classification may be improved by selecting and integrating more suitable features (keywords) to distinguish relevant documents from irrelevant documents for each category. This goal differs from many previous attempts, which often aimed at improving the processes of classifier building ( Wu, Phang, Liu, &amp; Li, 2002 ), threshold tuning ( Liu hal, Mitra, &amp; Buckley, 1997 ). The research result of the paper may complement the previous techniques. Feature selection was often an experimental issue in previous studies ( McCallum, Rosenfeld, Mitchell, &amp; many feature selection techniques developed ( Mladenic  X  et al., 2004; Yang &amp; Pedersen, 1997 ). There were also studies that maintained an evolvable feature set covering all features currently seen (e.g. Cohen &amp; Singer, 1996 ), although inappropriate features may introduce inefficiency ( Yang &amp; Pedersen, 1997 ) and poorer perfor-mance in text classification. Therefore, there was no standard guideline to construct a perfect feature set. A feature set was often determined by an experimental tuning process.

However, even a feature set may be perfectly tuned to distinguish the categories, it is not necessarily suitable to filter out those documents not belonging to the categories. This problem is due to the common goal of pre-vious feature selection techniques: selecting those features that may be used to distinguish a category from oth-ers. Under such a goal, whether a feature may be selected mainly depends on the content relatedness among the categories, without paying much attention to how the contents of a category c and a document d overlap with each other. If d contains much information not in c or vice versa, d should not be classified into c , even though d mentions some content of c . Similar problems may also be found in similarity measurement between two documents: two documents should not be similar to each other if they have a lot of different contents, no matter how the feature set is tuned using training documents. This problem motivates the research in the paper.

To tackle the problem, features should be dynamically selected in response to each individual input docu-ment (rather than training documents in the categories). The features may help to measure the extent to which the content of the document overlaps with that of each category. They are helpful when a document of a cate-gory does not employ a terminology different from that employed by training documents of the category. 1.2. Organization of the paper
In the next section, we present an analysis that provides significant hints to conduct the research. Accord-ingly, in Section 3 , we present a novel technique DP4FC (Dynamic Profiling for Filtering Classification) that helps various classifiers to dynamically create category profiles so that the performance of text filtering and classification may be improved. DP4FC has been empirically evaluated under different circumstances, and hence in Section 4 , we present and analyze the results. The paper is finally concluded in Section 5 . 2. Misclassification of irrelevant documents: an analysis
Table 1 presents an analysis on the possible reasons of misclassifying an irrelevant document d into a cate-gory c . A feature tends to lead the classifier to make an error (i.e. classifying d into c ) when the feature is included in the feature set, and it also appears in both c and d (i.e. Case 1) or it does not appear in both c and d (Case 7). In both cases, d is similar to the category in the dimension (feature). The feature thus has the effect of suggesting c to accept d (and thus making an error). On the other hand, there are two cases where a feature may help the classifier to avoid making the error: the feature is included in the feature set, but it only appears in c (i.e. Case 3) or d (Case 5), but not both. In both cases, d is not similar to c in the dimension (fea-ture). The feature thus has the effect of suggesting c to reject d .

The analysis suggests a dynamic profiling strategy: (1) employing those terms that appear in c but do not appear in d (for increasing the probability of Case 3, while reducing the probability of Case 1), and conversely (2) employing those terms that appear in d but do not appear in c (for increasing the probability of Case 5, while reducing the probability of Case 1). Therefore, each category X  X  profile should be composed of a feature set, which is dynamic in the sense that it is reconstructed once a test document is entered.

Dynamic profiling may complement the functionality of those classifiers that distinguish c from other cat-constructed with respect to all categories (rather than individual input documents) and hence do not vary for each input document. Dynamic profiling complements the classifiers by considering how d contains those con-tents not in c and how c contains those contents not in d .If d contains much information not in c or c contains much information not in d , d should not be classified into c , even though it mentions some contents of c .
On the other hand, the implementation of dynamic profiling is challenging. Three tasks should be consid-ered: (1) estimating the correlation strength of each feature with respect to c and d , since a feature that happens to appear in a c or d is not necessarily a good feature, (2) tackling the side-effect of rejecting many relevant documents, and (3) integrating dynamic profiling with the underlying classifier in order to make filtering and classification decisions. These tasks should be achieved before dynamic profiling may become really help-ful for various classifiers. 3. Dynamic profiling for filtering and classification
Based on the above analysis, we develop a dynamic profiling technique DP4FC (Dynamic Profiling for Fil-tering Classification) to promote various classifiers X  performances in text filtering and classification. Fig. 1 illustrates the introduction of DP4FC to a classifier. In training, DP4FC joins the thresholding process, while in testing, DP4FC joins the process of making filtering and classification decisions. Both the underlying clas-sifier and DP4FC estimate each document X  X  DOA with respect to each category. The key point is that DOA values estimated by DP4FC are based on dynamic profiling, which aims to measure the extent to which a doc-ument X  X  content overlaps that of a category.

The algorithm is depicted in Table 3 . Given a category c and a document d , the dynamic profile of c is com-posed of two kinds of terms: those terms that are positively correlated with c but do not appear in d (ref. Step 2), and those terms that are negatively correlated with c but appear in d (ref. Step 3). Both kinds of terms lead to the reduction of the DOA values estimated by DP4FC (ref. Steps 2.2 and 3.2). Therefore, a smaller DOA value indicates that d contains more information not in c , and vice versa. It indicates that we have a lower confidence to classify d into c .

DP4FC employs the v 2 (chi-square) method to estimate the correlation strengths. For a term t and a cat-ber of documents, A is the number of documents that are in c and contain t , B is the number of documents that are not in c but contain t , C is the number of documents that are in c but do not contain t ,and D is the number of documents that are not in c and do not contain t . Therefore, v wise they are negatively correlated . Note that, t may appear in an input document d but do not appear in any training document. According to the dynamic profiling strategy, t should be considered. However, its v with respect to each category is incomputable (since both A and B are zero). DP4FC tackles the problem by treating d as a training document (i.e. both N and B will be incremented by 1).

With the DOA estimation, DP4FC may join the thresholding process to help the underlying classifier to derive proper thresholds for each individual category. The basic idea is that, each category has two thresholds: one for thresholding the DOA values produced by DP4FC, while the other is for thresholding the original
DOA values produced by the underlying classifier. The former threshold helps to reduce the number of doc-uments that should not be considered in tuning the latter threshold. The two thresholds work together in the hope to optimize the category X  X  performance in a predefined criterion.

Once a document is entered, its two DOA values (i.e. by DP4FC and the underlying classifier) are pro-duced, and the corresponding thresholds are consulted. The document may be classified into a category only if both DOA values are higher than or equal to their corresponding thresholds. DP4FC and the underlying classifier may thus work together to complement each other to make proper decisions. 4. Experiments
Experiments are conducted to investigate the contributions of DP4FC. For objective and thorough inves-tigation, DP4FC is evaluated under different circumstances, including (1) different sources of experimental tion methodologies, and (5) different parameter settings for the classifier. Table 4 summarizes the different cir-cumstances, which are explained in the following subsections. 4.1. Experimental data
Experimental data is from Reuter-21578, which is a public collection for related studies ( http:// www.daviddlewis.com/resources/testcollections/reuters21578 ). There are 135 categories (topics) in the collec-tion. We employ the ModLewis split, which skips unused documents and separates the documents into two parts based on their time of being written: (1) the test set, which consists of the documents after April 8, 1987 (inclusive), and (2) the training set, which consists of the documents before April 7, 1987 (inclusive).
The test set is further split into two subsets: (1) the in-space subset, which consists of 3022 test documents that belong to some of the categories (i.e. fall into the category space), and (2) the out-space subset, which consists of 3168 documents that belong to none of the categories. They help to investigate the systems X  performances in text classification and text filtering, respectively. An integrated text filtering and classification system should (1) properly classify in-space documents, and (2) properly filter out out-space documents.

As suggested by previous studies (e.g. Yang, 2001 ), the training set is randomly split into two subsets as well: the classifier building subset and the threshold tuning (or validation) subset. The former is used to build the classifier, while the latter is used to tune a threshold for each category. Therefore, to guarantee that each category has at least one document for classifier building and one document for threshold tuning, we remove those categories that had fewer than two training documents, and hence 95 categories remain. Among the 95 categories, 12 categories have no test documents. From both theoretical and practical standpoints, these cate-gories deserve investigation ( Lewis, 1997 ), although they were excluded by several previous studies (e.g. Chai,
Ng, &amp; Chieu, 2002; Yang, 2001 ). After removing those documents to which no categories are assigned (i.e. not belonging to any of the 95 categories), the training set contains 7780 documents. Moreover, since previous studies did not suggest the way of setting the documents for classifier building and threshold tuning, we try different settings to conduct thorough investigation: 50 X 50% and 80 X 20%, which conduct twofold and fivefold cross-validation, respectively. In the twofold cross-validation, 50% of the data is used for classifier building, and the remaining 50% of the data is used for threshold tuning, and the process repeats two times so that each training document is used for threshold tuning exactly one time. Similarly, in the fivefold cross-validation, 80% of the data is used for classifier building, and the remaining 20% of the data is used for threshold tuning, and the process repeats five times.

Moreover, to test those out-space documents that are less related to the categories, we randomly sample 370 documents from a text hierarchy that was extracted from Yahoo! ( http://www.yahoo.com ) and employed by previous studies ( Liu &amp; Lin, 2005; Liu &amp; Lin, 2003 ). The documents are randomly extracted from the catego-
Reuters categories. With the help of the Yahoo out-space documents, we may measure the system X  X  text fil-tering performance in processing those out-space documents with different degrees of relatedness with respect to the categories. 4.2. Evaluation criteria
The classification of in-space test documents and the filtering of out-space test documents require different evaluation criteria. For the former, we employ precision ( P ) and recall ( R ). Both P and R are common eval-uation criteria in previous studies ( Lewis, 1995; Yang, 2001 ). To integrate P and R into a single measure, we employ F 1 =2 PR /( P + R ), which places the same emphasis on P and R .

As in many previous studies, P , R , and F 1 have two versions: the micro-averaged version and the macro-averaged version. The micro-average version tends to view all categories as a system, and hence estimates P by [total number of correct classifications/total number of classifications made], and R by [total number of correct classifications/total number of correct classifications that should be made]. On the other hand, the macro-averaged version tends to view each individual category as a system, and hence estimates P , R , and
F by the average of the P , R , and F 1 values on individual categories, respectively. When computing the macro-average values, we exclude incomputable values (i.e. those whose denominators are zero).
On the other hand, to evaluate the filtering of out-space test documents, we employ misclassification ratio (MR), which is estimated by [number of misclassifications for the out-space documents/number of the out-space documents]. A system should avoid misclassifying out-space documents into many categories (i.e. lower
MR). 4.3. The underlying classifiers
Each category c is associated with a classifier. Upon receiving a document d , the classifier estimates the sim-ilarity between d and c (i.e. DOA of d with respect to c ) in order to make a binary decision for d : accepting d or rejecting d . To investigate the contributions of DP4FC to different kinds of classification methodologies,
DP4FC is applied to two kinds of classifiers: the Rocchio classifier (RO, which is based on vector-based meth-odology) and the Naive Bayes classifier (NB, which is based on probability-based methodology).
RO was originally designed for query optimization in relevance feedback ( Rocchio, 1971 ) and was com-monly employed in text classification (e.g. Wu et al., 2002 ), text filtering (e.g. Schapire et al., 1998; Singhal et al., 1997 ), and retrieval (e.g. Iwayama, 2000 ) as well. Some studies even showed that its performances were
RO constructs a vector for each category, and the similarity between a document d and a category c is esti-mated using the cosine similarity between the vector of d and the vector of c . More specially, the vector for a category c is constructed by considering both relevant documents and irrelevant documents of c : g documents in c ), while N is the set of vectors for irrelevant documents (i.e. the documents not in c ). Each doc-ument vector is built by the TF X  X DF (Term Frequency X  X nverse Document Frequency) technique, which gives ment d , and IDF( w ) is [total number of training documents/number of documents that contain w ]. Moreover, the parameters g 1 and g 2 govern the weights for relevant documents and irrelevant documents, respectively. In the experiment, g 1 = 16 and g 2 = 4, since previous studies show that such a setting was promising (e.g. Wu et al., 2002 ). For more complete evaluation, the effects of different settings for g as well (ref. Section 4.4.3 ).

On the other hand, NB was frequently employed and evaluated with respect to various techniques, includ-1996; Yang &amp; Lin, 1999 ) and hierarchical text classification (e.g. Koller &amp; Sahami, 1997; Dhillon, Mallela, &amp; Kumar, 2002; McCallum et al., 1998 ). It was shown to be competitive (and even better) when compared with various state-of-the-art text classification techniques, such as neural networks and support vector machines ( Dhillon et al., 2002; Yang &amp; Lin, 1999 ). In particular, it pre-estimates the conditional probability P ( w j c ) for every feature w and category c (with standard Laplace smoothing to avoid the probabilities of zero). The  X  X  X imilarity X  X  between a document d and a category c is estimated by P  X  c  X 
Q d , and notc is a dummy category covering all documents irrelevant to c .

All the classifiers require a fixed (predefined) feature set, which is built using the documents for classifier building. Each term that is not a stop word may be a candidate feature. No phrases extraction routine is invoked. Features are selected according to their weights, which are estimated by the v technique. The technique has been investigated and shown to be more promising than others ( Yang, 1999;
Yang &amp; Pedersen, 1997 ). As noted above, there is no perfect way to determine the size of the feature set. Set-ting a proper feature set size was often an experimental issue in previous studies (e.g. McCallum et al., 1998;
Yang &amp; Pedersen, 1997 ). Therefore, to conduct more thorough investigation, we try five feature set sizes, including 1000, 5000, 10,000, 15,000, and 20,000, since there are about 20,000 different features in the fivefold training data.

To make filtering and classification decisions, both RO and NB require a thresholding strategy to set a threshold for each category. As in many previous studies (e.g. Callan, 1998; Chai et al., 2002; Lewis, Schapire,
NB tune a relative threshold for each category by analyzing document-category similarities. The threshold tuning documents are used to tune each relative threshold. As suggested by many studies (e.g. Yang, 2001 ), the thresholds are tuned in the hope to optimize the system X  X  performance with respect to F we also design a version of NB that employs a fixed threshold of 0.5 for each category (i.e. no threshold tun-ing). This version of NB is named NBFixed05. It was tested in several previous studies as well (e.g. Chai et al., 2002 ). 4.4. Result and discussion
We separately discuss the results in text classification (i.e. classification of in-space test data) and text fil-tering (filtering of out-space test data). The results show that DP4FC significantly promotes all the classifiers X  performances in both filtering and classification under different environments. 4.4.1. Results on in-space test data
Figs. 2 and 3 illustrate the contribution of DP4FC to RO in classifying in-space documents. The data is averaged across all the seven folds (two 50 X 50% folds + five 80 X 20% folds). The figures show micro-average and macro-average results, respectively. In micro-average performances, DP4FC provides significant improve-ment to RO under all different feature set sizes. When comparing the average performances, it provides 14.8% improvement in F 1 (0.7032 vs. 0.6127). Moreover, in macro-average performances, DP4FC provides more sig-nificant improvement to RO under all different feature set sizes. When comparing the average performances, it provides 25.2% improvement in F 1 (0.6487 vs. 0.5182).

Fig. 4 shows the performances of RO and RO + DP4FC under different folds (i.e. two 50 X 50% folds and five 80 X 20% folds). The results are average performances under all features set sizes. The results show that
DP4FC stably provides significant improvement to RO in all folds.
We are also concerned with the contributions of DP4FC to NB and NBFix05. Figs. 5 and 6 illustrate the micro-average results and macro-average results, respectively. In micro-average performances, DP4FC pro-vides significant improvement to both NB and NBFix05 under all different feature set sizes. When comparing average performances, it provides 396.3% (0.7370 vs. 0.1485) and 31.3% (0.7855 vs. 0.5984) improvements in
F to NB and NBFix05, respectively. Moreover, in macro-average performances, DP4FC provides significant improvement to both NB and NBFix05 under all different feature set sizes as well. When comparing average performances, it provides 102.5% (0.6380 vs. 0.3150) and 62.8% (0.5802 vs. 0.3564) improvements in F and NBFix05, respectively.

It is interesting to note that, DP4FC provides significant improvements in macro-average precision and recall as well. DP4FC successfully helps RO (ref. Fig. 3 ) and NB (ref. Fig. 6 ) to achieve both better and more stable precision and recall under different features set sizes. Moreover, it also significantly promotes the pre-cision rates achieved by both NB and NBFix05, which are quite poor (about 0.2).

Fig. 7 shows the contributions of DP4FC to NB and NBFix05 under different folds. The results are average performances under all features set sizes. Again, the results show that DP4FC stably provided significant improvement to NB and NBFix05 in all folds.
 The results together show that DP4FC may significantly promote the performances of RO, NB, and
NBFix05 in processing in-space documents. Moreover, the performance improvements are stable in the sense that they occurred under various circumstances, including different feature set sizes and cross-validation folds.
The contributions justify the design of DP4FC. With the help of DP4FC, the classifiers may achieve both bet-ter and more stable performances under various circumstances.

It is also interesting to investigate the contributions of DP4FC under the environmental settings in which the underlying classifiers achieve their best performances in micro-average F settings and the improvements provided by DP4FC under the settings. DP4FC successfully promotes the performances of all the well-tuned classifiers. It also tends to provide more significant improvements in macro-average F 1 . This indicates that DP4FC may help the classifiers to uniformly have better performances on indi-vidual categories, rather than on larger categories only. Moreover, when comparing all the classifiers, no clas-sifiers may be the best one in both micro-average and macro-average performances. NBFix05 and RO may only perform better in micro-average F 1 (0.7496) and macro-average F the introduction of DP4FC, the best version became RO + DP4FC, which achieves 0.7952 micro-average F and 0.6903 macro-average F 1 . 4.4.2. Results on out-space test data
We are also concerned with the contributions of DP4FC in the filtering of out-space documents. Fig. 8 shows the contributions of DP4FC to RO. For Reuters out-space documents, when comparing their average performances, DP4FC provides 20.5% reduction in MR (0.9412 vs. 1.1842). On the other hand, for Yahoo out-space documents, DP4FC provides 10.0% reduction in MR (0.5105 vs. 0.5671).

Fig. 9 shows the contributions of DP4FC to NB and NBFix05. Without DP4FC, MR performances of both NB and NBFix05 dramatically oscillate. For Reuters out-space documents, when comparing their average performances, DP4FC provides 92.2% (1.2100 vs. 15.5616) and 57.2% (0.8517 vs. 1.9879) MR reductions to NB and NBFix05, respectively. On the other hand, for Yahoo out-space documents, DP4FC provides 91.4% (2.1376 vs. 24.8666) and 94.6% (0.9614 vs. 17.9442) MR reductions to NB and NBFix05, respectively.

Together with the results on in-space documents, the results on out-space documents further justify the con-tributions of DP4FC: it successfully promotes text classification performances, while at the same time, pre-vents the classifiers from over-fitting themselves to in-space documents. DP4FC achieves the task by basing its judgment on content overlapping, which is a general guideline to measure the relevance of each document.
The contributions are particularly meaningful, since in practice, there should be much more out-space docu-ments than in-space documents.
 4.4.3. Effects of different parameter settings
We are also interested in the effects of different parameter settings for the underling classifiers. Unlike NB and NBFix05, RO has two parameters: g 1 and g 2 , which govern the weights for relevant documents and irrel-evant documents, respectively. In the experiments reported above, we follow the suggestions from previous studies and set their ratio to 4:1 (i.e. g 1 = 16 and g 2 settings under the best environments for RO (i.e. feature set size = 5000 in the third fold).
 The result is shown in Fig. 10 . It indicates that when the ratio becomes smaller, RO tends to misclassify more Yahoo out-space documents. When the ratio ranges from 1:1 to 1:3, RO has much poorer performance for
Yahoo documents (ref. the dramatically increasing MR), although it achieves a little bit better performance for in-space documents (ref. the performance in F 1 ). Since Yahoo documents are less related to training doc-uments, this indicates that RO tends to over-fits itself to the training data when the ratio becomes too small.
The over-fitting is due to the fact that, when the ratio becomes too small, irrelevant documents for a category c will dominate the computation of the vector for c . In that case, those features that appear in irrelevant doc-uments tend to get negative weights in the vector for c . Unfortunately, when compared with the less related out-space documents, threshold tuning documents have a higher probability of having these features, making their DOA values smaller. This leads to a threshold that is too low to reject Yahoo out-space documents.
As noted above, in practice, there should be much more input documents that are less related to training data. Therefore, when considering the overall performances of RO for all kinds of data (i.e. Reuters in-space,
Reuters out-space, and Yahoo out-space data), the best setting for the ratio should range from 2:1 to 5:1, including the one suggested by previous studies and employed in the above experiments (i.e. 4:1). As shown in Fig. 10 , in these settings, DP4FC consistently promotes the performances of RO in classifying Reuters in-space documents and filtering out Reuters out-space documents.
 5. Conclusion
Given an information space spanned by a set of categories, misclassification of documents into the infor-mation space may deteriorate the management, dissemination, and retrieval of information. We thus present a technique DP4FC to complement and promote various classifiers X  performance in text filtering and classifica-tion. Instead of aiming at distinguishing a category from other categories, DP4FC aims at measuring whether a document d contains too much information not in a category c , or vice versa. If so, d should not be classified into c , even though d mentions some content of c . DP4FC helps the underlying classifier to create dynamic category profiles with respect to each individual document. It then works with the classifier to set proper thresholds, and accordingly make proper filtering and classification decisions. Empirical results show that
DP4FC may significantly promote different classifiers X  performances under different circumstances. The con-tributions are of both theoretical and practical significance to the automatic classification of suitable informa-tion into suitable categories.
 Acknowledgement This research was supported by the National Science Council of the Republic of China under the grant NSC 94-2213-E-320-001.
 References
