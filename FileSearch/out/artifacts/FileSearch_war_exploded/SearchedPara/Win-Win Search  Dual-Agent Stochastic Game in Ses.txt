 Session search is a complex search task that involves multi-ple search iterations triggered by query reformulations. We observe a Markov chain in session search: user X  X  judgment of retrieved documents in the previous search iteration af-fects user X  X  actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The frame-work, which we term  X  X in-win search X , is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Dynamic Information Retrieval Modeling; POMDP; Stochas-tic Game; Session Search
Users often need a multi-query session to accomplish a complex search task. A session usually starts with the user writing a query, sending it to the search engine, receiving a list of ranked documents ordered by decreasing relevance, then examining the snippets, clicking on the interesting ones, and spending more time reading them; we call one such se-quence a  X  X earch iteration. X  In the next iteration, the user modifies the query or issues a new query to start the search again. As a result, a series of search iterations form, which include a series of queries q 1 ,...,q n , a series of returned doc-uments D 1 ,...,D n , and a series of clicks C 1 ,...,C n which are SAT clicks ( satisfactory clicked documents [9]). The session stops when the user X  X  information need is sat-isfied or the user abandons the search [6]. The information retrieval (IR) task in this setting is called session search [7, Figure 1: A Markov chain of decision states in ses-sion search. (S: decision states; q: queries; A: user actions such as query changes; D: documents). 11, 17, 22, 24, 25, 30, 31]. Table 1 lists example information needs and queries in session search.

We are often puzzled about what drives a user X  X  search in a session and why they make certain moves. We observe that sometimes the same user behavior, such as a drift from one subtopic to another, can be explained by opposite reasons: either the user is satisfied with the search results and moves to another sub information need, or the user is not satisfied with the search results and leaves the previous search path. The complexity of users X  decision making patterns makes session search quite challenging [4, 29].

Researchers have attempted to find out the causes of topic drifting in session search. The causes under study include personalization [29], task types [20, 24], and previous docu-ments X  relevance [11]. A user study is usually needed to draw conclusions about user intent. However, the focus of this paper is not on identifying user intent. Instead, we simplify the complexity of users X  decision states into a cross prod-uct of only two dimensions: whether previously retrieved documents are relevant and whether the user would like to explore the next sub information need. Our work differs from existing work in that we consider that a session goes through a series of hidden decision states, with which we design a statistical retrieval model for session search. Our emphasis is an effective retrieval model, not a user study to identify the query intent.

The hidden decision states form a Markov chain in session search. A Markov chain is a memoryless random process where the next state depends only on the current state [18]. Figure 1 illustrates a Markov chain of hidden decision states for TREC 2013 Session 9. In a session, a user X  X  judgment of the retrieved documents in the previous iteration affects or even decides the user X  X  actions in the next iteration. A user X  X  actions could include clicks, query changes, reading the documents, etc. The user X  X  gain, which we call reward , is the amount of relevant information that he or she obtains in the retrieved documents. The reward motives the later user actions. If the decision states are known, we can use a Markov Decision Process (MDP) to model the process. However, in session search, users X  decision states are hidden. We therefore model session search as a Partially Observable Markov Decision Process (POMDP) [18].
In fact, not only the user, but also the search engine, makes decisions in a Markov process. A search engine takes in a user X  X  feedback and improves its retrieval algorithm it-eration after iteration to achieve a better reward too. The search engine actions could include term weighting, turning on or turning off one or more of its search techniques, or ad-justing parameters for the techniques. For instance, based on the reward, the search engine can select p in deciding the top p documents used in pseudo relevance feedback.

We propose to model session search as a dual-agent stochas-tic game . When there is more than one agent in a POMDP, the POMDP becomes a stochastic game (SG). The two agents in session search are the user agent and the search engine agent. In contrast to most two-player scenarios such as chess games in game theory, the two agents in session search are not opponents to each other; instead, they cooperate: they share the decision states and work together to jointly maxi-mize their goals. We term the framework  X  win-win search  X  for its efforts in ensuring that both agents arrive at a win-win situation. One may argue that in reality a commercial search engine and a user may have different goals and that is why some commercial search engines put their sponsors high in the returned results. However, this paper focuses on the win-win setting and assume a common interest  X  fulfilling the information needs  X  for both agents.

The challenges of modeling session search as a stochastic game lie in how to design and determine the decision states and actions of each agent, how to observe their behaviors, and how to measure the rewards and set the optimization goals. We present the details in Sections 4 and 5. As a retrieval framework, we pay more attention to the search engine agent. When the search engine makes decisions, it picks a decision that jointly optimizes the common interest.
We evaluate the win-win search framework on TREC 2012 &amp; 2013 Session data. TREC (Text REtrieval Conference) 2010 -2013 Session Tracks [20, 21] have spurred a great deal of research in session search [3, 10, 11, 14, 24]. The tracks provide interaction data within a session and aim to retrieve relevant documents for the last query q n in the ses-sion. The interaction data include queries, top returned doc-uments, user clicks, and other relevant information such as dwell time. Document relevance is judged based on informa-tion need for the entire session, not just the last query. In this paper, all examples are from TREC 2013. Our experi-ments show that the proposed framework achieves statisti-cally significant improvements over state-of-the-art interac-tive search and session search algorithms.

The remainder of this paper is organized as follows: Sec-tion 2 presents the related work, Section 3 provides prelim-inaries for POMDP. Section 4 details the win-win search framework, Section 5 elaborates the optimization, Section 6 evaluates the framework and Section 7 concludes the paper.
Session search has attracted a great amount of research from a variety of approaches [3, 11, 22, 25, 33]. They can be grouped into log-based methods and content-based methods.
There is a large body of work using query logs to study queries and sessions. Feild and Allan [8] proposed a task-aware model for query recommendation using random walk over a term-query graph formed from logs. Song and He X  X  work [27] on optimal rare query suggestion also used random walk, with implicit feedback in logs. Wang et al. [30] utilized the latent structural SVM to extract cross-session search tasks from logs. Recent log-based approaches also appear in the Web Search Click Data (WCSD) workshop series. 1
Content-based methods directly study the content of the query and the document. For instance, Raman et al. [25] studied a particular case in session search where the search topics are intrinsically diversified. Content-based session search also include most research generated from the recent TREC Session Tracks [20, 21]. Guan et al. [10] organized phrase structure in queries within a session to improve re-trieval effectiveness. Jiang et al. [14] proposed an adaptive browsing model that handles novelty in session search. Jiang and He [13] further analyzed the effects of past queries and click-through data on whole-session search effectiveness.
Others study even more complex search  X  search across multiple sessions [22, 24, 30]. Kotov et al. [22] proposed methods for modeling and analyzing users X  search behaviors in multiple sessions. Wang et al. [30] identified cross-session search by investigating inter-query dependencies learned from user behaviors.

Our approach is a content-based approach. However, it uniquely differs from other approaches by taking a Markov process point of view to study session search. http://research.microsoft.com/en-us/um/people/ nickcr/wscd2014
Session search is closely related to relevance feedback, a traditional IR research field. Classic relevance feedback methods include Rocchio [16], pseudo relevance feedback [2], and implicit relevance feedback [27] based on user behaviors such as clicks and dwell time. Recently, researchers have investigated new forms of relevance feedback. Jin et al. [15] employed a special type of click  X   X  X o to the next page X   X  as relevance feedback to maximize retrieval effectiveness over multi-page results. Zhang et al. [33] modeled query changes between adjacent queries as relevance feedback to improve retrieval accuracy in session search.

These relevance feedback approaches only considers one-way communication from the user to the search engine [15, 33]. On the contrary, this paper explicitly sets up a two-way feedback channel where both parties transmit information.
Markov Decision Process (MDP) is an important topic in Artificial Intelligence (AI). An MDP can be solved by a family of reinforcement learning algorithms. Kaelbling et al. [18] brought techniques from operational research to choose the optimal actions in partially observable problems, and designed algorithms for solving Partially Observable Markov Decision Processes (POMDPs). IR researchers have just begun showing interests in MDP and POMDP [11, 30, 32] in finding solutions for IR problems.

Early work on interactive search modeling by Shen et al. [26] used a Bayesian decision-theoretic framework, which is closely related to the MDP approaches. The QCM model proposed by Guan et al. [11] models session search as an MDP and effectively improves the retrieval accuracy. How-ever, [11] used queries as states while we use a set of well-designed hidden decision states. In addition, we explicitly model the stochastic game played between two agents, the user and the search engine, while [11] focused on just the search engine. Another difference is that we model a wide range of actions including query changes, clicks, and docu-ment content while [11] only used query changes.

Yuan and Wang [32] applied POMDP for sequential selec-tion of online advertisement recommendation. Their mathe-matical derivation shows that belief states of correlated ads can be updated using a formula similar to collaborative fil-tering. Jin et al. [15] modeled Web search as a sequential search for re-ranking documents in multi-page results. Their hidden states are document relevance and the belief states are given by a multivariate Gaussian distribution. They con-sider  X  X anking X  as actions and  X  X licking-on-the-next-page X  as observations. In win-win search, we present a different set of actions, observations, and messages between two agents. The fundamental difference between our approach and theirs is that we model the retrieval task as a dual-agent coopera-tive game while [15] uses a single agent.
Markov Decision Process provides the basics for the win-win search framework. An MDP is composed by agents, states, actions, reward, policy, and transitions [19]. An agent takes inputs from the environment and outputs actions. The actions in turn influences the states of the environment. An MDP can be represented by a tuple &lt; S,A,T,R &gt; :
States S is a discrete set of states. In session search, they can be queries [11] or hidden decision states (Section 4.2).
Actions A is a discrete set of actions that an agent can take. For instance, user actions include query changes, clicks, and reading the returned documents or snippets.

Transition T is the state transition function T ( s i ,a,s P ( s j | s i ,a ). It is the probability of starting in state s action a , and ending in state s j . The sum over all actions gives the total state transition probability between s i and s T ( s i ,s j ) = P ( s j | s i ); which is similar to the state transition probability in the Hidden Markov Model (HMM) [1].

Reward r = R ( s,a ) is the immediate reward, also known as reinforcement . It gives the expected immediate reward of taking action a at state s .

Policy  X  describes the behaviors of an agent. A non-stationary policy is a sequence of mapping from states to actions.  X  is usually optimized to decide how to move around in the state space to optimize the long term reward P  X  t =1
Value function and Q-function Given a policy  X  at time t , a value function V calculates the expected long term reward starting from state s inductively: V  X   X ,t ( s ) = R ( s, X  V  X ,t =1 ( s ) = R ( s,a =  X  t =1 ( s )), s is the current state, s is the next state, a =  X  t ( s ) is any valid action for s at t ,  X  is a future discount factor. Usually an auxiliary func-tion, called the Q-function , is used for a pair of ( s,a ): Q ( s t ,a ) = R ( s t ,a ) +  X  P a P ( s t | s t +1 ,a ) max where R ( s t ,a ) is the immediate reward at t , a is any valid action at t + 1. Note that V  X  ( s ) = max a Q  X  ( s,a ).
Q-Learning Reinforcement learning (RL) algorithms pro-vides solutions to MDPs [19]. The most influential RL al-gorithm is Q-learning. Given a Q-function and a starting state s , the solution can be a greedy policy that at each step, it takes the action that maximizes the Q-function:  X  ( s ) = arg max a h R ( s,a ) +  X  P a T ( s,a,s 0 ) Q ( s the base case maximizes R ( s 1 ,a ).

Partially Observable MDP (POMDP) When states are unknown and can only be guessed through a probabilistic distribution, an MDP becomes a POMDP [18]. POMDP is represented by a tuple &lt; S,A,T,  X  ,O,B,R &gt; , where S,A,R are the same as in MDP. Since the states are unknown, the transition function T models transitions between beliefs, not transitions between states any more: T : B  X  A  X  B  X  [0 , 1]. Belief B is the set of beliefs defined over S , which indicates the probability that an agent is at a state s . It is also known as belief state . Observations  X  is a discrete set of observations that an agent makes about the states. O is the observation function which represents a probabilistic distribution for making observation  X  given action a and landing in the next state s 0 . A major difference between an HMM and a POMDP is that POMDP considers actions and rewards while HMM does not.
A session can be viewed as a Markov chain of evolving states (Figure 1). Every time when a new query is issued, both the user and the search engine transition into a new state. In our setting, the two agents work together to achieve a win-win goal.
Figure 2 shows the proposed dual-agent SG, which is rep-resented as a tuple &lt; S,A u ,A se ,  X  u ,  X  se ,  X  u ,  X 
S is the decision states that we will present in Section 4.2. Name Symbol Meanings State S the four hidden decision states in Figure 3 User action A u add query terms, remove query terms, keep query terms Message from user to search engine  X  u clicked and SAT clicked documents Message from search engine to user  X  se top k returned documents User X  X  observation  X  u observations that the user makes from the world
A u ,A se ,  X  u , and  X  se are the actions. We divide the ac-tions into two types: domain-level actions A , what an agent acts on the world directly, and communication-level actions  X , also known as messages , which only go between the agents. User actions A u are mainly query changes [11] while search engine actions A se are term weighting schemes and adjust-ments to search techniques. Both  X  u and  X  se are sets of relevant documents, that an agent uses to inform the other agent about what they consider as relevant. (Section 4.3)
 X  is the observation that an agent can draw from the world or from the other agent. O is the observation function that maps states and actions to observations: O : S  X  A  X   X  or O : S  X   X   X   X . Note that the actions can be domain-level actions A or messages  X , or a combination of both. (S. 4.4) B is the set of belief states that shared by both agents. The beliefs are updated every time when an observation hap-pens. There are two types of belief: B  X   X  , beliefs before the messages and B  X   X  , beliefs after the messages. (Section 4.5)
The reward function R is defined over B  X  A  X  R . It is the amount of document relevance that an agent obtains from the world. R se is the nDCG score (normalized Discounted Cumulative Gain [20]) that the search engine gains for the documents it returns. R u is the relevance that the user gains from reading the documents. Our retrieval algorithm jointly optimize both R u and R se . (Section 5)
Table 2 lists the symbols and their meanings in the dual-agent SG. The two agents share the decision states and be-liefs but differ in actions, messages, and policies. Although they also make different observations, both contribute to the belief updater; the difference is thus absorbed. As a retrieval model, we only pay attention to the search engine policy  X  se : B  X  A . The following describes their interactions in the stochastic game: 1. At search iteration t = 0, both agents begin in the 2. t increases by one: t = t + 1. The user agent writes a 3. (*) The search engine agent makes observations  X  4. The search engine runs its optimization algorithm and 5. Search engine action a t se results in a set of documents 6. (*) The user agent receives message  X  t se and observes 7. Based on the current beliefs, the user agent sends its 8. (*) The search engine agent observes  X  se from the 9. The user agent picks a policy  X  u , which we don X  X  study
Steps 3, 6, and 8 happen after making an observation from the world or from the other agent. They then all involve a belief update. In the remainder of this section, we present the details of states (Section 4.2), actions (Section 4.3), ob-servation functions (Section 4.4), and belief updates (Section 4.5) for win-win search.
We often observe that the same user behavior in ses-sion search may be motivated by different reasons. For in-stance, in TREC 2013 Session 2 (Table 1), a user searches for  X  X cooter brands X  as q 1 and finds that the 6 th returned document with title  X  X cooter Brands -The Scooter Review -The Scooter Review X  is relevant. The user clicks this doc-ument and reads it for 48 seconds, which we identify as a SAT click since it lasts more than 30 seconds [12]. Next, the user adds a new term  X  X eliable X  into q 1 to get q 2 = X  X cooter brands reliable X . We notice that  X  X eliability X  does not ap-pear in any previously retrieved documents D 1 . It suggests that the user is inspired to add  X  X eliability X  from somewhere else, such as from personal background knowledge or the in-formation need. In this case, she finds relevant documents from the previously retrieved documents but still decides to explore other aspects about the search target.

On the contrary, in TREC 2013 Session 9 q 1 (Table 1), an-other user searches for  X  X ld US coins X  and also finds relevant documents, such as a document about  X ... We buy collectible U.S.A. coins for our existing coin collector clients... X  . He adds a new term  X  X ollecting X  to get the next query q 2  X  X ol-lecting old us coins X . After reducing the query terms and document terms into their stemmed forms, the added term  X  X ollecting X  does appear in this document as we can see. It suggests that the user selects a term from the retrieval re-sults and hones into the specifics. In this case, he finds relevant documents in the previously retrieved documents and decides to exploit the same sub information need and investigate it more.

We observe that even if both users show the same search behavior, e.g. adding terms, the reasons vary: one is adding the new search term because the original search results are not satisfactory, while the other is because the user wants to look into more specifics. This makes us realize that doc-ument relevance and users X  desire to explore are two inde-pendent dimensions in deciding how to form the next query.
Inspired by earlier research on user intent and task types [24, 28] and our own observations, we propose four hidden decision making states for session search. They are identified based on two dimensions: 1)  X  X elevant dimension X   X  whether the user thinks the returned documents are relevant, and 2)  X  X xploration dimension X   X  whether the user would like to explore another subtopic. The two dimensions greatly simplify the complexity of user modeling in session search. The relatively small number of discrete states enables us to proceed with POMDP and its optimization at low cost.
The cross-product of the two dimensions result in four states: i) user finds a relevant document from the returned documents and decides to explore the next sub information need (relevant and exploration, e.g. scooter price  X  scooter stores ), ii) user finds relevant information and decides to stay in the current sub information need to look into more relevant information (relevant and exploitation, e.g. hart-ford visitors  X  hartford connecticut tourism ), iii) user finds out that the returned documents are not relevant and de-cides to stay and try out different expressions for the same sub information need (non-relevant and exploitation, e.g. philadelphia nyc travel  X  philadelphia nyc train ), iv) user finds out that documents are not relevant and decides to give up and move on to another sub information need (non-relevant and exploration, e.g. distance new york boston  X  maps.bing.com ).

Figure 3 shows the decision state diagram for win-win search. The subscriptions stand for { RT = Relevantexploi Tation , RR = RelevantexploRation , NRT = NonRelevant exploiTation , NRR = NonRelevantexploRation } . We in-sert a dummy starting query q 0 before any real query and it always goes to S NRR . The series of search iterations in a ses-sion move in the decision states from one to the next. A se-quence of states can be time stamped and presented as s t S m , where t = 1 , 2 ,...,n and m = { RT,RR,NRT,NRR } .
There are two types of actions in our framework, domain-level actions and communications-level actions.

The domain-level actions A u and A se represent the actions directly performed on the world (document collection) by the user agent and by the search engine agent, respectively.
The common user actions include writing a query, clicking a document, SAT clicking a document, reading a snippet, reading a document , changing a query , and eye-tracking the documents . In this paper, we only study query changes and clicks as user actions. However, the framework can be easily adopted for other types of user actions.

Query changes  X  q [11] consist of added query terms + X  q t q \ q t  X  1 , removed query terms  X   X  q t = q t  X  1 \ q t terms q theme = LongestCommon Subsequence ( q t ,q t  X  1 ). For example, in Session 87 , given q 19 = X  X hiladelphia nyc travel X  and q 20 = X  X hiladelphia nyc train X , we obtain the following query changes: q theme = LCS ( q 19 ,q 20 ) =  X  X hiladelphia X ,  X   X  q 20 =  X  X ravel X , and + X  q 20 =  X  X rain X . All stopwords and function words are removed.

The search engine domain-level actions A se include in-creasing, decreasing, and maintaining the term weights, as well as adjusting parameters in one or more search tech-niques. We present the details in Sections 5 and 6.
The second type of actions are communication-level ac-tions (messages)  X  u and  X  se . They are actions that only performed between agents.

In our framework, the messages are essentially documents that an agent thinks are relevant.  X  u is the set of docu-ments that the user sends out; we define them as the clicked documents D clicked . In TREC 2013 Session, 31% search iterations contain SAT clicked documents. 23.9% sessions contain 1 to 4 SAT clicked documents, and a few sessions, for instance Sessions 45, 57 and 72, contain around 10 SAT clicked documents. 88.7% SAT clicked documents appear in the top 10 retrieved results.

Similarly,  X  se is the set of documents that the search en-gine sends out. They are the top k returned documents ( k ranges from 0 to 55 in the TREC setting). They demon-strate what documents the search engine thinks are the most relevant. In TREC 2013, 2.8% (10) search iterations return less than 10 documents, 90.7% (322) return exactly 10, 5.1% (18) return 10  X  20, and 1.4% (5) return 20  X  55 documents.
Section 4.1 illustrates the win-win search framework and the interactions between agents. This section shows how we calculate the observation functions.

The observation function O ( s j ,a t , X  t ), defined as P (  X  is the probability of observing  X  t  X   X  when agents take ac-tion a t and land on state s j . The first type of observation is related to relevance . In Section 4.1 Step 8, after the user sends the message  X  u (user clicks) out at Step 7, the search engine updates its after-message-belief-state b  X   X  se based on its observation of user clicks. The observation function for  X  X elevant X  states is:
O ( s t =Rel,  X  u ,  X  t =Rel) def ==== P (  X  t = Rel | s t Rel ,  X  u ) as a constant, we can approximate it by P (  X  Rel ,s t = Rel ,  X  u ) = P ( s t = Rel |  X  t = Rel ,  X  Rel ,  X  u ). Given that user clicks  X  u are highly correlated to  X  t , we can approximate P ( s t = Rel |  X  t = Rel ,  X  P ( s t = Rel |  X  t = Rel). Further, by taking P ( X ) as a con-stant, we have
O ( s t =Rel,  X  u ,  X  t =Rel )  X  P ( s t = Rel |  X  t = Rel ) P (  X   X  P ( s t = Rel |  X  t = Rel) P (  X  t = Rel |  X  u ) Similarly, we have
O ( s t =Non-Rel,  X  u ,  X  t =Non-Rel)  X  P ( s t = Non-Rel |  X  t = Non-Rel) P (  X  t = Non-Rel |  X  as well as O ( s t =Non-Rel,  X  u ,  X  t =Rel) and O ( s t =Rel,  X   X  =Non-Rel).

Based on whether a SATClick exists or not, we calculate the probability of the SG landing at the  X  X elevant X  states or the  X  X on-Relevant X  states (the first dimension of hidden decision states). At search iteration t , if the set of previously returned documents leads to one or more SAT clicks, the current state is likely to be relevant, otherwise non-relevant. That is to say, Based on this intuition, we calculate P (  X  t = Rel |  X  P (  X  t = Non-Rel |  X  u ) as:
The conditional probability of observations P ( s t = Rel |  X  Rel) and P ( s t = Non-Rel |  X  t = Non-Rel) can be calculated by maximum likelihood estimation (MLE). For instance, of observed true relevant X  is the number of times where the previously returned document set D t  X  1 contain at least one SAT clicks and those SAT clicked documents are indeed rel-evant documents in the ground truth.  X # of observed rele-vant X  is the number of times where D t  X  1 contains at least one SAT clicks. The ground truth of whether the SG lands on a  X  X elevant X  state is generated by documents whose rele-vance grades  X  3 (relevant to highly relevant). The relevance are judged by NIST assessors [21].

The second type of observation is related to exploitation vs. exploration . This corresponds to a combined observa-tion at Step 3 and the previous Step 6 (Section 4.1), where the SG update the before-message-belief-state b  X   X  se user action a u (query change) and a search engine message  X  se = D t  X  1 , the top returned documents at the previous it-eration. The search engine agent makes observations about exploitation vs. exploration (the second dimension of hidden decision states) by:
The search engine can guess the hidden states based on the following intuition: s The idea is that given that D t  X  1 is the message from search engine and a u =  X  q is the message from user, if added query terms + X  q appear in D t  X  1 , it is likely that the user stays at the same sub information need from iteration t  X  1 to t for  X  X xploitation X . On the other hand, if the added terms + X  q do not appear in D t  X  1 , it is likely that the user moves to the next sub information need from iteration t  X  1 to t for  X  X xploration X . In addition, if there is no added terms (+ X  q is empty) but there are deleted terms (  X   X  q t is not empty), it is likely that the user goes to a broader topic to explore. If + X  q t and  X   X  q t are both empty, it means there is no change to the query, it is likely to fall into exploitation. Hence, P (  X  t |  X  q t ,D t  X  1 ) can be calculated as:
P (  X  t = Exploration |  X  q t ,D t  X  1 ) = P (+ X  q t 6 =  X  X  X  + X  q
P (  X  t = Exploitation |  X  q t ,D t  X  1 ) = P (+ X  q t 6 =  X  X  X  + X  q where D t  X  1 include all clicked documents and all snippets that are ranked higher than the last clicked document at iteration t  X  1. User actions a u include the current query changes + X  q t and  X   X  q t . In fact, P (  X  t |  X  q t ,D be calculated for each specific case. For instance, P (  X  Exploration | a =  X  X elete term X  ,  X  q t ,D t  X  1 ) = calculate for the actions with  X  X eleted terms X .  X # of ob-served explorations X  is the number of observed explorations suggesting that the user is likely to explore another subtopic based on Eq. 8, while  X # of observed true explorations X  is the number of observed explorations judged positive by hu-man accessors in a ground truth. The annotations can be found online. 2
The conditional probability P ( s t = Exploitation |  X   X # of observed exploitations X  is the number of observed ex-ploitations suggesting that the user is likely to exploit the same subtopic (based on Eq. 9), and  X # of observed true ex-ploitations X  is the number of observed exploitations that are judged positive in the ground truth. P ( s t = Exploration |  X  Exploration) is calculated in a similar way.
At every search iteration the belief state b is updated twice; once at Step 3, another at Step 8. It reflects the interaction and cooperative game between the two agents.
The manual annotations for  X  X xploration X  transitions can be found at www.cs.georgetown.edu/~huiyang/win-win . states can be calculated as: b 0 ( s i = S z ) = P ( s i = S S ), where x  X  { R = Rel,NR = Non -Rel } , y  X  { R = exploRation,T = exploiTation } , z is the cross-product of x and y and z  X  { RR,RT,NRR,NRT } . In addition, 0  X  b ( s i )  X  1 and P s i b ( s i ) = 1.

The belief update function is b t +1 ( s j ) = P ( s by taking into account new observations  X  t . It is updated from iteration t to iteration t + 1: where s i and s j are two states, i,j  X  X  RR,RT,NRR,NRT } . t indices the search iterations, and O ( s j ,a t , X  t ) = P (  X  is calculated based on Section 4.4. P (  X  t | a t ,b t ) is the nor-malization factor to keep P s simplicity, we will only use a to represent actions from now on. However, it is worthy noting that actions can be both domain-level actions a and messages  X .

Transition probability T ( s i ,a t ,s j ) is defined as P ( s where Transition ( s i ,a t ,s j ) is the sum of all transitions that starts at state s i , takes action a t , and lands at state s Transition ( s i ,a t ,s  X  ) is the sum of all transitions that starts at state s i and lands at any state by action a t .
Finally, taking O ( s j ,a t , X  t ) = P (  X  t | s j ,a t equals to P (  X  t | s j ,a t ,b t ) when we consider beliefs, and T ( s ) = P ( s j | s i ,a t ,b t ), the updated belief can be written as: where b t ( s i ) is P ( s i | a t ,b t ), whose initial value is b
After every search iteration, we decide the actions for the search engine agent. We employ Q-learning [18] to find out the optimal action. For all a  X  A se , we write the search en-gine X  X  Q-function, which represents the search engine agent X  X  long term reward, as: where the reward for a belief state b is  X  ( b,a ) = P s  X  S P (  X  | b,a u ,  X  se ) corresponds to Eq. 8 and Eq. 9 and P (  X  | b,  X  corresponds to Eq. 4 and Eq. 5. b 0 is the belief state up-dated by Eq. 11.

In win-win search, we take into account both the search engine reward and the user reward. As in [11], we have Q calculated as the long term reward for the user agent: which recursively calculates the reward starting from q 1 continues with the policy until q t . P ( q t | d ) is the current reward that the user gains through reading the documents. The formula matches well with common search scenarios where the user makes decisions about their next actions based on the most relevant document(s) they examined in the previous run of retrieval. Such a document we call it maximum rewarding document(s) . We use document with the largest P ( q t  X  1 | d t  X  1 ) as the maximum rewarding docu-ment. P ( q t  X  1 | d t  X  1 ) is calculated as 1  X  Q t  X  q occurrences of term t in document d t  X  1 , and | d t  X  1 document length.

By optimizing both long term rewards for the user and for the search engine, we learn the best policy  X  and use it to predict the next action for the search engine. The joint optimization for the dual-agent SG can be represented as: where a se  X  A se at t = n and n is the number of search iterations in a session, i.e., the session length.

In win-win search, A se can include many search engine ac-tions. One type of actions is adjusting a query X  X  term weight. Assuming the query is reformulated from the previous query by adding + X  q or deleting  X   X  q . That is to say, A se = { in-creasing, decreasing, or keeping term weights } . The term weights are increased or decreased by multiplying a factor. We also use a range of search techniques/algorithms as ac-tion options for the search engine agent. They are reported in Section 6. Based on Eq. 14, the win-win search frame-work picks the optimal search engine action. We evaluate the proposed framework on TREC 2012 and TREC 2013 Session Tracks [20, 21]. The session logs are collected from users through a search system by the track organizers. The topics, i.e., information need (Table 1), are provided to users. The session logs record all URLs dis-played to the user, snippets, clicks, and dwell time. Table 3 shows the dataset statistics. The task is to retrieve a ranked list of 2,000 documents for the last query in a session. Doc-ument relevance is judged based on the whole-session rele-vance. We use the official TREC evaluation metrics in our experiments. They include nDCG@10, nERR@10, nDCG, and MAP [21]. The ground truth relevant documents are provided by TREC.

The corpora used in our experiments are ClueWeb09 CatB (50 million English web pages crawled in 2009, used in TREC 2012), and ClueWeb12 CatB (50 million English web pages crawled in 2012, used in TREC 2013). Documents with the Waterloo spam scores [5] less than 70 are filtered out. All duplicated documents are removed.

We compare our system with the following systems: Lemur [23] (language modeling + Dirichlet smoothing), PRF (Pseudo Relevance Feedback in Lemur assuming the top 20 docu-ments are relevant), Rocchio (relevance feedback that as-Table 4: Search accuracy on TREC 2012 Session (  X  indicates a statistical significant improvement over Rocchio at p &lt; 0.05 (t-test, one-sided));  X  indicates a statistical significant improvement over QCM+DUP at p &lt; 0.05 (t-test, one-sided)).
 Table 5: Search accuracy on TREC 2013 Session (  X  indicates a statistical significant improvement over Rocchio at p &lt; 0.01 (t-test, one-sided));  X  indicates a statistical significant improvement over QCM+DUP at p &lt; 0.01 (t-test, one-sided)).
 sumes the top 10 previous retrieved documents are relevant), Rocchio-CLK (implicit relevance feedback that assumes only previous clicked documents are relevant), Rocchio-SAT (im-plicit relevance feedback that assumes only previous SAT-clicked documents are relevant), QCM+DUP (the QCM ap-proach proposed by [11]), and QCM SAT (a variation of QCM by [33]). We choose Rocchio (a state-of-the-art inter-active search algorithm) and QCM+DUP (a state-of-the-art session search algorithm) as two baseline systems and all other systems are compared against them. TREC median and TREC best scores are also included for reference. Note that TREC best are an aggregation from the best scores of each individual submitted TREC runs; it is not a single search system. Our run, win-win, implements six retrieval technologies. They are: (1) increasing weights of the added terms (+ X  q ) by a factor of x = { 1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2 } ; (2) decreasing weights of the added terms by a factor of y = { 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95 } ; (3) the term weighting scheme proposed in [11] with parameters  X  ,  X  ,  X  ,  X  set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance Feedback) algorithm which assumes the top p retrieved doc-uments are relevant while p ranges from 1 to 20; (5) an ad-hoc variation of win-win, which directly uses the last query in a session to perform retrieval; and (6) a brute-force variation of win-win, which combines all queries in a session, extracts all unique query terms from them, and weights them equally. Win-win examine 21 search engine action options in total to find out the optimal action that maximizes the joint long term reward Q se ( b,a ) + Q u ( b,a u ) for both agents.
Table 4 shows the search accuracy of all systems under comparison for TREC 2012 Session Track. We can see that win-win search is better than most systems except QCM SAT. It statistically significantly outperforms Rocchio by 20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10 (p-value &lt; .05, one-side t-test). It also outperforms Rocchio-CLK, Rocchio-SAT and QCM+DUP, but the results are not statistically significant. The trends for other evaluation met-rics are similar to nDCG@10.

Table 5 shows the search accuracy of all systems for TREC 2013 Session Track. Since we only indexed ClueWeb12 CatB, after spam reduction, many relevant CatA documents are not included in the CatB collection. To evaluate the systems fairly, we created a filtered ground truth which only consists of relevant documents in CatB. The results are shown in Table 5. We can see that win-win is the best run among all systems. It shows statistically significant gain (p-value &lt; .01, one-sided t-test) over all other systems across all evalua-tion metrics. Particularly, the proposed approach achieves a significant 54% improvement of nDCG@10 comparing to QCM+DUP. The experimental results support that our ap-proach is highly effective.
TREC Session tasks request for retrieval results for the last query in a session. Theoretically, however, win-win search can optimize at every search iteration throughout a session. We hence compare our approach (the Win-Win run) with the top returned documents provided by TREC (the Original run) in terms of immediate search accuracy . We define immediate search accuracy at i as an evaluation score that measures search accuracy at search iteration i . The evaluation scores used are nDCG@10 and nERR@10.

We report the averaged immediate search accuracy for all sessions. It is worthy noting that session lengths vary. To average across sessions with different lengths, we make all sessions equals to the maximum session length in a dataset. TREC 2012 and 2013 Session have different maximum ses-sion lengths; they are 11 and 21, respectively. When a ses-sion is shorter than the maximum session length, we use the retrieval results from its own last iteration as the retrieval results for iterations beyond its own last iteration. In ad-dition, since TREC did not provide any retrieval results for the last query, the Original runs has no value at the last iteration.
 Figures 4 and 5 plot the immediate search accuracy for TREC 2012 &amp; 2013 Session Tracks averaged over all ses-sions. We observe that win-win search X  X  immediate search accuracy is statistically significantly better than the Origi-nal run at every iteration. In Figure 4, win-win outperforms Original since iteration 2 in nDCG@10 and outperforms it since iteration 3 in nERR@10. At the last iteration, win-win outperforms Original by a statistically significant 27.1% in nDCG@10 (p-value &lt; .05, one-sided t-test). We observe similar trends in Figure 5. Another interesting finding is that win-win search X  X  immediate search accuracy increases while the number of search iterations increases. In Figure 4, the nDCG@10 starts at 0.2145 at the first iteration and in-creases dramatically 37.1% to 0.2941 at the last iteration. It suggests that by involving more search iterations, i.e., learn-ing from more interactions between the user and the search Figure 4: TREC 2012 Im-mediate Search Accuracy.
 Figure 6: Long sessions (length &gt; = 4). Transition probabilities are listed with actions: Add (A), Re-move (R), and Keep (K). engine, win-win is able to monotonically improve its search accuracy.
This experiment investigates how legitimate the proposed states are in presenting the hidden mental states of users.
First, we use examples to demonstrate the state transi-tions in sessions. Session 87 (Table 1) is a long session with 21 queries. The chain of decision states identified for this session based on techniques presented in Sections 4.2 and 4.4 is: S NRR ( q 1 =best us destination)  X  S RT ( q 2 =distance new york boston)  X  S NRT  X  S NRR  X  S NRR  X  S RR  X  S  X  S NRR ( q 12 =nyc tourism)  X  S NRR ( q 13 =philadelphia nyc S
NRT ( q 19 =philadelphia nyc travel)  X  S NRT ( q 20 =philadelphia nyc train)  X  S NRT ( q 21 =philadelphia nyc bus). Our states correctly suggests that the user is in the exploration states (RR, NRR, NRR) from q 11 to q 13 , while he keeps chang-ing queries to explore from city to city (boston, new york city, and philadelphia). The user eventually finds the cities, philadelphia and nyc, that fulfill the information need  X   X  best US destinations within a 150-mile radius  X . During the last 3 queries, the user exploits the current subtopic (philadel-phia and nyc) to find out more specifics on transportations (travel, train, bus) about them. Our system correctly recog-nizes the last three states as exploitation states (NRT, NRT, NRT). This example suggests that the proposed states are able to reflect the real user decision states quite accurately.
Second, we examine state transition patterns in long ses-sions since they contain enough transitions for us to study. Figure 6 plots state probabilities, state transition probabil-ities, and that under different user actions for long sessions (sessions with 4 or more queries). The data are combined from both TREC 2012 &amp; 2013 Session Tracks. We notice that NRR (non-relevant and exploration) is the most com-mon state (42.4%). This reflects that a user spend may a long time to explore while receiving non-relevant documents. On the contrary, the RR state (relevant and exploration) is the least common state (11.3%).Moreover, we see that state transitions are not uniformly distributed. For instance, the transition from NRT to both relevant states (RT and RR) are very rare (in total 5.65%). In addition, we notice that actions are related to state transition probabilities. There are 90.8% transitions generated by adding terms and among all the transitions with removing terms, 84.8% of them lead to exploitation states (RT or NRT).

Third, we find that state probability distribution and state probability transitions differ among different session types. We plot the state probabilities and transition probabilities in Figures 7 to 10 for four different TREC session types, which were created along two aspects: search target ( factual or intellectual ) and goal quality ( specific or amorphous ). Sug-gested by [24], the difficulty levels of the session types usu-ally are FactualSpecific &lt; IntellectualSpecific &lt; FactualAm-orphous &lt; IntellectualAmorphous. An interesting finding is that as the session difficult level increases, the transition probability from state NRT (non-relevant and exploitation) to state RT (relevant and exploitation) becomes lower: Fac-tualSpecific (0.25), IntellectualSpecific (0.2), FactualAmor-phous (0.12), IntellectualAmorphous(0.1). It suggests that the harder the task is, the greater the necessity to explore rather than to exploit, when the user is not satisfied with the current retrieval results. In addition, we observe that Intellectual sessions (Figures 9 and 10) have a larger prob-ability, 0.1548, to be in the RR (relevant and exploration) state than the other session types (on average 0.1018).
This paper presents a novel session search framework, win-win search, that uses a dual-agent stochastic game to model the interactions between user and search engine. With a careful design of states, actions, and observations, the new framework is able to perform efficient optimization over a fi-nite discrete set of options. The experiments on TREC Ses-
Figure 10: Intellectual and Amorphous sessions. sion 2012 and 2013 datasets show that the proposed frame-work is highly effective for session search.

Session search is a complex IR task. The complexity comes from the involvement of many more factors other than just terms, queries and documents in most existing retrieval algorithms. The factors include query reformula-tions, clicks, time spent to examine the documents, person-alization, query intent, feedback, etc. Most existing work on sessions and task-based search focuses on diving into one aspect. Through significantly simplifying the factors, we re-alize the integration of all the factors in a unified framework. For example, we simplify users X  decision states into only four states, and discretize user actions and search engine actions into a finite number of options. Such simplification is nec-essary in creating practical search systems.

This paper views the search engine as an autonomous agent, that works together with user, another autonomous agent, to collaborate on a shared task  X  fulfilling the infor-mation needs. This view assumes that the search engine is more like a  X  X ecision engine X . Session search can be imag-ined as two agents exploring in a world full of information, searching for the goal in a trial-and-error manner. Here we assume a cooperative game between the two agents. How-ever, as we mentioned in the introduction, the search engine agent can of course choose a different goal. It will be very interesting to see how to still satisfy the user to achieve win-win. We hope our work calls for future adventures in the fields of POMDP in IR and game theory in IR. This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations ex-pressed in this paper are of the authors, and do not neces-sarily reflect those of the sponsor.
