 In recent years, latent annotation of PCFG has been shown to p erform as well as or better than stan-dard lexicalized methods for treebank parsing [1, 2]. In the latent annotation scenario, we imagine that the observed treebank is a coarse trace of a finer, unobse rved grammar. For example, the single treebank category NP (noun phrase) may be better modeled by s everal finer categories representing subject NPs, object NPs, and so on. At the same time, discrimi native methods have consistently provided advantages over their generative counterparts, i ncluding less restriction on features and PCFGs, hoping to gain the best from both lines of work.
 Discriminative methods for parsing are not new. However, mo st discriminative methods, at least generally impractical. Previous work on end-to-end discri minative parsing has therefore resorted to the benefits of discriminative methods, it has therefore bec ome common practice to extract n-best In such an approach, repeated parsing of the training set can be avoided because the discriminative often does not contain the correct parse tree. For example 41 % of the correct parses were not in the candidate pool of  X  30-best parses in [10].
 In this paper we present a hierarchical pruning procedure th at exploits the structure of the model and allows feature expectations to be efficiently approxima ted, making discriminative training of full-scale grammars practical. We present a gradient-base d procedure for training a discriminative grammar on the entire WSJ section of the Penn Treebank (rough ly 40,000 sentences containing 1 million words). We then compare L superior, requiring fewer iterations to converge and yield ing sparser solutions. Independent of the experiments. Context-free grammars (CFGs) underlie most high-performa nce parsers in one way or another [13, 12, 14]. However, a CFG which simply takes the empirical prod uctions and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its context-freedom assumptions are too strong in some places and too weak in othe rs. Therefore, a variety of techniques have been developed to both enrich and generalize the naive g rammar. Recently an automatic state-splitting approach was shown to produce state-of-the art pe rformance [2, 14]. We extend this line of work by investigating discriminative estimation techniqu es for automatically refined grammars. We consider grammars that are automatically derived from a r aw treebank. Our experiments are based on a completely unsplit X-bar grammar, obtained direc tly from the Penn Treebank by the binarization procedure shown in Figure 1. For each local tre e rooted at an evaluation category X , we introduce a cascade of new nodes labeled X so that each has two children in a right branching fashion. Each node is then refined with a latent variable, spl itting each observed category into k unobserved subcategories. We refer to trees over unsplit ca tegories as parse trees and trees over split categories as derivations .
 Our log-linear grammars are parametrized by a vector  X  which is indexed by productions X  X   X  . The conditional probability of a derivation tree t given a sentence w can be written as: duction occurs in the derivation t. The inside/outside algorithm [15] gives us an efficient way o f summing over an exponential number of derivations. Given a s entence w spanning the words w computed by summing over all possible children B and C spanning ( i, k ) and ( k, j ) respectively: 1 where we use  X  P they can nonetheless be normalized in the same way as probabi lities to produce the expected counts of productions needed at training time. The posterior proba bility of a production A  X  BC spanning ( i, j ) with split point k in a sentence is easily expressed as: To obtain a grammar from the training trees, we want to learn a set of grammar parameters  X  on generative grammars, where the parameters  X  are set to maximize the joint likelihood of the train-ing sentences and their parse trees, and discriminative gra mmars, where the parameters  X  are set to work on automatic grammar refinement has focused on differen t estimation techniques for learning generative grammars with latent labels (training with basi c EM [1], an EM-based split and merge approach [2], a non-parametric variational approach [16]) . In the following, we review how gener-ative grammars are learned and present an algorithm for esti mating discriminative grammars with latent variables. 2.1 Generative Grammars Generative grammars with latent variables can be seen as tre e structured hidden Markov models. A simple EM algorithm [1] allows us to learn parameters for gen erative grammars which maximize the log joint likelihood of the training sentences w and parse trees T : We then use Eqn. 3 to compute expectations which are normaliz ed in the M-Step to update the production probabilities  X  Here, E to P will write E where P no restriction on the model class, as probabilistic and weig hted CFGs are equivalent [18]. 2.2 Discriminative Grammars Discriminative grammars with latent variables can be seen a s conditional random fields [4] over trees. For discriminative grammars, we maximize the log con ditional likelihood: We directly optimize this non-convex objective function us ing a numerical gradient based method (LBFGS [19] in our implementation). 3 Fitting the log-linear model involves the following deriva -tives: parse tree and the second term is the expected count of the pro duction in all parses. The challenge in estimating discriminative grammars is tha t the computation of some quantities discuss ways to make their computation on large data sets pra ctical in the next section. Computing the partition function in Eqn. 6 requires parsing of the entire training corpus. Even with recent advances in parsing efficiency and fast CPUs, parsing the entire corpus repeatedly remains training sentences still requires more than 5 hours on a fast machine. Even in a parallel implemen-tation, parsing the training corpus several hundred times, as necessary for discriminative training, would and, in fact, did in the case of maximum margin training [6], require weeks. Generally speak-ing, there are two ways of speeding up the training process: r educing the total number of training iterations and reducing the time required per iteration. 3.1 Hierarchical Estimation The number of training iterations can be reduced by training models of increasing complexity in a hierarchical fashion. For example in mixture modeling [20] and machine translation [21], a sequence of increasingly more complex models is constructed and each model is initialized with its (simpler) grammar, splitting each annotation category in two and addi ng a small amount of randomness to break symmetry. In addition to reducing the number of traini ng iterations, hierarchical training has been shown to lead to better parameter estimates [2]. Howeve r, even with hierarchical training, large-scale discriminative training will remain impracti cal, unless we can reduce the time required to parse the training corpus. 3.2 Feature-Count Approximation High-performance parsers have employed coarse-to-fine pru ning schemes, where the sentence is rapidly pre-parsed with increasingly more complex grammar s [22, 14]. Any constituent with suf-While this method has no theoretical guarantees, it has been empirically shown to lead to a 100-fold speed-up without producing search errors [14].
 Instead of parsing each sentence exhaustively with the most complex grammar in each iteration, we can approximate the expected feature counts by parsing in a hierarchical coarse-to-fine scheme. We start by parsing exhaustively with the X-Bar grammar and t hen prune constituents with low posterior probability ( e  X  10 in our experiments). 4 We then continue to parse with the next more refined grammar, skipping over constituents whose less refin ed predecessor has been pruned. After parsing with the most refined grammar, we extract expected co unts from the final (sparse) chart. The expected counts will be approximations because many sma ll counts have been set to zero by the pruning procedure.
 Even though this procedure speeds-up each training iterati on tremendously, training remains pro-hibitively slow. We can make repeated parsing of the same sen tences significantly more efficient by caching the pruning history from one training iteration to the next. Instead of computing each stage in the coarse-to-fine scheme for every pass, we can comp ute it once when we start training a grammar and update only the final, most refined scores in every iteration. Cached pruning has the to worry about issues like subcategory drift and projection s [14].
 As only extremely unlikely items are removed from the chart, pruning has virtually no effect on the conditional likelihood. Pruning more aggressively lea ds to a training procedure reminiscent of contrastive estimation [23], where the denominator is restricted to a neighborhood of the correct sively did not hurt performance for grammars with few subcat egories, but limited the performance of grammars with many subcategories. Figure 2: Average number of constructed constituents per se ntence (a) and time to parse the training corpus for different pruning regimes and grammar sizes (b). We ran our experiments on the Wall Street Journal (WSJ) porti on of the English Penn Treebank using the standard setup: we trained on sections 2 to 21. Sect ion 22 was used as development set for intermediate results. All of section 23 was reserved for the final test. We used the EVALB parseval reference implementation for scoring. We will report F the final test, we selected the grammar that performed best on the development set.
 are replaced by one of 50 unknown word tokens based on a small n umber of word-form features. To parse new sentences with a grammar, we compute the posteri or distribution over productions at each span and extract the tree with the maximum expected numb er of correct productions [14]. 4.1 Efficiency The average number of constituents that are constructed whi le parsing a sentence is a good indicator for heavily refined grammars. In contrast, with cached pruni ng the number of constructed chart items stays roughly constant (or even decreases) when the nu mber of subcategories increases. The 2(b), and makes discriminative training on a large scale cor pus computationally feasible. We found that roughly 100-150 training iterations were need ed for LBFGS to converge after each split. Distributing the training over several machines is s traightforward as each sentence can be parsed independently of all other sentences. Starting from an unsplit X-Bar grammar we were able needed around 50 iterations of discriminative training unt il convergence, significantly speeding up the training, while maintaining the same final performance. 4.2 Regularization Regularization is often necessary to prevent discriminati ve models from overfitting on the training set. Surprisingly enough, we found that no regularization w as necessary when training on the en-tire training set, even in the presence of an abundance of fea tures. During development we trained on subsets of the training corpus and found that regularizat ion was crucial for preventing overfit-Table 1: Discriminative training is superior to generative training for exact match and for F 1 subcategory 67.3 7.8 23 K 44 67.4 7.9 35 K 67 2 subcategories 80.8 20.1 74 K 108 80.3 19.5 123 K 132 4 subcategories 85.6 31.3 147 K 99 85.7 31.5 547 K 148 8 subcategories 87.8 37.0 318 K 82 87.6 36.9 2,983 K 111 16 subcategories 89.3 39.4 698 K 75 89.1 38.7 11,489 K 102 Table 2: L regularization. ting. This result is in accordance with [16] where a variatio nal Bayesian approach was found to be beneficial for small training sets but performed on par with E M for large amounts of training data. L large parameter values. We investigated L
L where the regularization parameter  X  is tuned on a held out set. In the L a convex and differentiable function of the parameters and h ence can be easily intergrated into our training procedure. In the L rameter equals zero. To handle the discontinuinty of the gra dient, we used the orthant-wise limited-memory quasi-Newton algorithm of [24].
 Table 2 shows that while there is no significant performance d ifference in models trained with L L of the parameter vector. L parameters are zero in the 16 subcategory case), while no par ameter value becomes exactly zero with L ones when exponentiated in order to be used in the computatio n of inside and outside scores. 4.3 Final Test Set Results Table 1 shows a comparison of generative and discriminative grammars for different numbers of F -score for all numbers of subcategories. For our largest gra mmars, we see absolute improvements of 3.63% and 0.61% in exact match and F better parameter estimates, as the model classes defined by t he generative and discriminative model (probabilistic vs. weighted CFGs) are equivalent [18] and t he same feature sets were used in all experiments.
 Our final test set parsing F other systems, including basic generative latent variable grammars [1] (F even fully lexicalized systems [13] (F [12, 14], which achieve accuracies above 90%. However, many of the techniques used in [12, 14] are orthogonal to what was presented here (additional non-l ocal/overlapping features, merging of unnecessary splits) and could be incorporated into the disc riminative model. Figure 3: (a) Loss in F gories after merging 80% of the subcategories according to t he merging criterion in [2]. 4.4 Analysis Generatively trained grammars with latent variables have b een shown to exhibit many linguistically interpretable phenomena [2]. Space does not permit a thorou gh exposition, and post hoc analysis of the broad patterns that are learned. Not surprisingly, many comparable trends can be observed in ( some ) elements emerge under both training regimes. Another exam ple is the preposition category ( IN ) where subcategories for subordinating conjunctions like ( that ) and different types of proper prepositions are learned. Typically the divisions in the di scriminative grammars are much more pronounced, putting the majority of the weight on a few domin ant words.
 While many similarities can be found, it is especially inter esting to examine how generative and discriminative grammars differ. The nominal categories in generative grammars exhibit many clus-example, the following two subcategories of the proper noun ( NNP ) category { New, San, Wall } and {
York, Francisco, Street } (here represented by the three most likely words) are learne d by the gen-erative grammars. These subcategories are very useful for m odeling correlations when generating words and many clusters with such semantic patterns appear i n the generative grammars. How-ever, these clusters do not interact strongly with disambig uation and are therefore not learned by the discriminative grammars. Similar observations hold fo r plural proper nouns ( NNPS ), superlative the generative grammars but are split very little or not at al l in the discriminative grammars. grammars with two subcategories, which illustrates the mai n difference between generative and dis-criminative grammars. Simple declarative clauses ( S ) are the most common sentences in the Penn Treebank, and in the generative case the most likely expansi on of the ROOT category is ROOT  X  S being chosen 91% of the time. In the discriminative case this production is only the third likeliest with a weight of 13 . 2 . The highest weighted expansion of the ROOT in the discriminative grammar is ROOT  X  SBARQ ative grammar. While generative grammars model the empiric al distributions of productions in the training set, discriminative grammars maximize the discri minative power of the model. This can for example result in putting the majority of the weight on under represented productions. We applied the merging criterion suggested in [2] to two gram mars with two subcategories in order to quantitatively examine how many subcategories are learn ed. This criterion approximates the loss in joint likelihood incurred from merging two subcategorie s and we extended it to approximate the loss in conditional likelihood from merging two subcategor ies at a given node. Figure 3(a) shows the loss in F that the discriminative grammars learn far fewer clusters a re confirmed, as one can merge back 80% of the subcategories at almost no loss in F This suggest that one can learn discriminative grammars whi ch are significantly more compact and accurate than their generative counterparts. Figure 3(b) s hows which categories remain split when 80% of the splits are merged. While there is a substantial ove rlap between the learned splits, one likelihood is better maximized by refining the grammar. We have presented a hierarchical pruning procedure that all ows efficient discriminative training of log-linear grammars with latent variables. We avoid repeat ed computation of similar quantities by caching information between training iterations and appro ximating feature expectations. We pre-sented a direct gradient-based procedure for optimizing th e conditional likelihood function which in our experiments on full-scale treebank parsing lead to di scriminative latent models which outper-form both the comparable generative latent models, as well a s the discriminative non-latent base-lines. We furthemore investigated different regularizati on penalties and showed that L tion leads to extremely sparse solutions training of latent variable grammars and opens the door for m any future experiments: discrimina-tive grammars allow the seamless integration of non-local a nd overlapping features and it will be interesting to see how proven features from reranking syste ms [10, 11, 12] and other orthogonal improvements like merging and smoothing [2] will perform in an end-to-end discriminative system.
