 tion, we present a divisive algorithm that uses Kullback-Leibler divergence as the distance measure, and explicitly minimizes the global objective function. This is in contrast to [28] which considered the merging of just two word clus-ters at every step and derived a local criterion based on the Jensen-Shannon divergence of two probability distribu-tions. Their agglomerative algorithm, which is similar to Baker and McCallum's algorithm [2], greedily optimizes this merging criterion. Thus, their resulting algorithm can yield sub-optimal clusters and is computationally expensive (the algorithm in [28] is O(mSl) in complexity where m is the to-tal number of words and l is the number of classes). In con-trust our divisive algorithm is O(mkl) where k is the number hard clustering leads to a model size of O(k), whereas "soft" clustering in methods such as probabilistic LSI [16] leads to a model size of O(mk). Finally, we show that our enhanced word clustering leads to higher classification accuracy, es-pecially when the training set is small and in hierarchical classification of HTML data. 
In this section, we quickly review some concepts from in-formation theory which will be used heavily in this paper. For more details on some of this material see the authorita-tive treatment in the book by Cover &amp; Thomas [6]. 
Let X be a discrete random variable that takes on val-ues from the set X with probability distribution p(x). The (Shannon) entropy of X [27] is defined as The relative entropy or Kullback-Leibler(KL) divergence [19] between two distributions pl(x) and p2(x) is defined as KL-divergence is a measure of the "distance" between two probability distributions; however it is not a true metric since it is not symmetric and does not obey the triangle in-equality [6, p.18]. KL-divergence is always non-negative but can be unbounded; in particular when pl (x) ~ 0 and p2 (x) = 0, KL(pl,p2) = co. In contrast, the Jensen-Shannon(JS) di-vergence between pl and p2 defined by JS~(pl,p2) = ~i KL(pl, 7rlpx +Tr2p2) + Ir2KL(p2, trap1 + Ir~p2) symmetric in {=1,Pl} and {=2,p2}, and is bounded [20]. The JS-divergence can be generalized to measure the distance between any finite number of probability distributions as: 
JS~(~i : 1 &lt; i &lt; n}) = H Irlp, -~lriH~i), (1) symmetric in the {Tri,Pi}'s (~i 7ri = 1, 7ri &gt;_ 0). 
Let Y be another random variable with probability dis-tribution p(y). The mutual information between X and Y, I(X; Y), is defined as the KL-divergence between the joint distribution p(x, y) and the product distribution p(x)p(y): 
By (5), we can clearly see that Naive Bayes is a linear classifier. Despite its crude assumption about the class-conditional independence of words, Naive Bayes has been found to yield surprisingly good classification performance, especially on text data. Plausible reasons for the success of 
Naive Bayes have been explored in [9, 12]. 
Support Vector Machines(SVMs)[29] are inductive learn-ing schemes for solving the two class pattern recognition problem. Recently SVMs have been shown to give good re-sults for text categorization [17]. The method is defined over a vector space where the classification problem is to find the decision surface that "best" separates the data points of the two classes. In the case of linearly separable data, the de-cision surface is a hyperplane that maximizes the "margin" between the two classes. This hyperplane can be written as ~7.ff -b = 0, where ~ is a data point and the vector and constant b are learned from the training set. Let yi E {+1,-1}(+1 for positive class and -1 for negative class) be the classification label for input vector a~. Finding the hyperplane can be translated into the following opti-mization problem 
This minimization problem can be solved using quadratic programming techniques[29]. The algorithms for solving the linearly separable case can be extended to the case of data that is not linearly separable by either introducing soft mar-gin hyperplanes or by using a non-linear mapping of the orig-inal data vectors to a higher dimensional space where the data points are linearly separahle[29]. Even though SVM classifiers are described as binary classifiers they can be eas-ily combined to handle the multi class case. A simple, effec-tive combination is to train N one-versus-rest classifiers for the N class case and then classify the test point to the class corresponding to the largest positive distance to the separat-ing hyperplane. In all our experiments we used linear SVMs because they are fast to learn and classify new instances compared to non-linear SVMs. Further~ linear SVMs have been shown to do well on text classification[17]. 
Hierarchical classification utilizes a hierarchical topic struc-ture such as Yahoo! to decompose the classification task into a set of simpler problems, one at each node in the hierar-chy. We can simply extend any classifier to perform hier-archical classification by constructing a (distinct) classifier at each internal node of the tree using all the documents in its child nodes as the training data. Thus the tree is assumed to be "is-a" hierarchy, i.e, the training instances are inherited by the parents. Then classification is just a greedy descent down the tree until the leaf node is reached. 
This way of classification has been shown to be equivalent to the standard non-hierarchical classification over a flat set of leaf classes if maximum likelihood estimates of all features are used[23]. However, hierarchical classification along with feature selection has been shown to achieve better classi-fication results than a fiat classifier[18]. This is because each classifier can now utilize a different subset of features that are most relevant to the classification sub-task at hand. distribution of the constituent distributions p(Clwt ). Thus, since the extra log(p(ci)) terms cancel due to (7). The first term in (9), after rearranging the sum, may be written as 
Similarly, the second term in (9) may be written as where (11) is obtained by substituting the value ofp(ClWj) from (8). Substituting (10) and (11) in (9) and using the definition of Jensen-Shannon divergence from (1) gives us the desired result, fl 
Theorem 1 gives a global measure of the goodness of word clusters, which may be informally interpreted as follows: 1. The quality of word cluster ~'Vj is measured by the 
Jensen-Shannon divergence between the individual word distributions p(Clwt ) (weighted by the word priors, lrt = p(wt)). The smaller the Jensen-Shannon diver-gence the more "compact" is the word cluster, i.e., smaller is the increase in entropy due to clustering (see (1)). 2. The overall goodness of the word clustering is mea-sured by the sum of the qualities of individual word clusters (weighted by the cluster priors lr(Wj) = p(~'Vj )). 
Given the global criterion of Theorem 1, we would now like to find an algorithm that searches for the optimal word clustering that minimizes this criterion. We now rewrite this criterion in a way that will suggest a "natural" algorithm. 
LEMMA 1. The generalized Jensen-Shannon divergence of a finite set of probability distributions can be expressed as the (weighted) sum of Kullback-Leibler divergences to the (weighted) mean, i.e., ability distribution, m = ~ Ir,pi. 194 an iterative algorithm that repeatedly (i) re-partitions the distributions p(C[wt) by their closeness in KL-divergence to the cluster distributions p(CIWj) , and (ii) subsequently, given the new word clusters, re-computes these cluster distri-butions using (8). Figure 1 describes the algorithm in detail. 
Note that this divisive algorithm bears some resemblance to the k-means or Lloyd-Max algorithm, which usually uses squared Euclidean distances [11, 10, 15, 4]. 
Note that our initialization strategy is crucial to our algo-guarantees absolute continuity of each p(Clwt ) with at least one cluster distribution p(CIWj), i.e., guarantees that at least one KL-divergence is finite. This is because our initial-ization strategy ensures that every word wt is part of some cluster Wj. Thus by the formula for p(ClWj) in step 2, it cannot happen that p(cilw~) ~ O, and p(cilWj) = 0. Note that we can still get some infinite KL-divergence values but these do not lead to any difficulty (indeed in an implemen-tation we can handle such "infinity problems" without an extra "if" condition thanks to the handling of "infinity" in the IEEE floating point standard [14, 1]). 
We now discuss the computational complexity of our al-gorithm. Step 3 of each iteration requires the KL-divergence to be computed for every pair, p(C[wt) and p(C]Wj). This is the most computationally demanding task and costs a to-tal of 
O(mkl) operations. Generally, we have found that the algorithm converges in 10-15 iterations independent of the size of the data set. Thus the total complexity is the agglomerative algorithm of [28] costs O(mal) operations. 
The algorithm in Figure 1 has certain pleasing proper-ties. As we will prove in Theorem 3, our algorithm de-creases the objective function value at every step and thus is guaranteed to converge to a local minimum in a finite num-ber of steps (note that finding the global minimum is NP-complete[13]). Also, by Theorem 1 and (13) we see that our algorithm minimizes the "within-cluster" Jensen-Shannon divergence. It turns out that (see Theorem 4) that our algorithm simultaneously maximizes the "between-cluster" Jensen-Shannon divergence. Thus the different word clus-ters produced by our algorithm are "maximally" far apart. We now give formal statements of our results with proofs. Proof. Use the definition of KL-divergence to expand the left-hand side(LHS) of (14) to get Similarly the I:tHS of (14) equals 
Proof. By Lemma 1, the total JS-divergence may be writ-ten as where m = ~i ~rlpi. With mj as in (15), and rewriting (16) in order of the clusters Pj we get j=lptE~ j x = E Ir('ps) E ~j)KL(p,,r' mj) + Elr(Ps)KL(ms,m) = E~r(Pj)JS~,({pt :p, e "Pj}) + JS~,,({m,: 1 &lt; i &lt; k}), where lr~' = ~'('Pj), which proves the result. O 
This concludes our formal treatment. We now see how to use word clusters in our text classifiers. 
The Naive Bayes method can be simply translated into us-ing word clusters instead of words. This is done by estimat-ing the new parameters p(W, [ci) for word clusters similar to the word parameters p(w, lci ) in (4) as where n(142,,ds) = Ew, e~'V. n(w,, dj). 
Note that when estimates of p(wtlcl ) for individual words are relatively poor, the corresponding word cluster param-eters p(]/Vslci ) provide more robust estimates resulting in higher classification scores. 
The Naive Bayes rule (5) for classifying a test document d can be rewritten as c'(d) = argmax X i logp(ci) + ~p(W,I d) logp(w,[cl) , with word clusters as features. 
This section provides empirical evidence that our divisive clustering algorithm of Figure 1 outperforms agglomerative clustering and various feature selection methods. We com-pare our results with feature selection by Information Gain and Mutual Information[30], and feature clustering using the agglomerative algorithm in [2]. We call the latter Agglomer-ative Clustering in this section for the purpose of comparison with our algorithm which we call Divisive Clustering. We show that Divisive Clustering achieves higher classification accuracy than the best performing feature selection method when the training data is sparse and show improvements over similar results reported in [28]. 0.6 
Figure 3: Fraction of Mutual Information lost while clustering words with Divisive Clustering is signifi-cantly lower compared to Agglomerative Clustering at all number of features (on Dmoz data). numerator of the above equation is precisely the global ob-jective function that Divisive Clustering attempts to mini-mize (see Theorem 1). Figures 2 and 3 plot the fraction of mutual information lost against the number of clusters for both the divisive and agglomerative algorithms on the 20Ng and Dmoz data sets. Notice that less mutual information is lost with Divisive Clustering compared to Agglomerative 
Clustering at all number of clusters, though the difference is more pronounced at lower number of clusters. 
Next we provide anecdotal evidence that our word clusters are better at preserving class information as compared to the agglomerative approach. Figure 4 shows three word clusters, 
Cluster 9 and Cluster 10 from Divisive Clustering and Clus-ter 12 from Agglomerative Clustering. These clusters were obtained while forming 20 word clusters with a 1/3-2/3 test-train split. While the clusters obtained by our algorithm could successfully distinguish between tee.sport.hockey and rec.sport.baseball, Agglomerative Clustering combined words from both classes in a single cluster. This resulted in lower classification accuracy for both classes with Agglomerative Clustering compared to Divisive Clustering. While Divisive 
Clustering achieved 93.33% and 9~.07% on tee.sport.hockey and nee.sport, baseball respectively, Agglomerative Clustering could only achieve 76.97% and 52.42%. 
Figure 5 shows classification accuracies on the 20 News-groups data set for the algorithms considered. The hori-zontal axis indicates the number of features/clusters used in the classification model while the vertical axis indicates the percentage of test documents that were classified correctly. 
The results are averages of 5-10 trials of randomized 1/3-2/3 test-train splits of the total data. Note that we cluster only the words belonging to the documents in the training set. We used two classification techniques, SVMs and Naive Bayes (NB) for the purpose of comparison. Observe that Divisive 
Clustering (SVM as well as NB) achieves significantly bet-ter results at lower number of features than feature selection using Information Gain and Mutual Information. With only 50 clusters, Divisive Clustering (NB) achieves 78.05% accu-70 60 ~e 4o 30 20 10 
Figure 11: Classification results on Dmoz hierarchy using Naive Bayes. Observe that the Hierarchical Classifier achieves significant improvements over the 
Flat classifiers with very few number of features. worse than NB at low dimensionality but better at higher dimeusionality, which is consistent with known SVM behav-ior [29]. In future work we will use non-linear SVMs at lower dimensions to alleviate this problem. 
Figure 9 plots the classification accuracy on Dmoz data using Naive Bayes when the training set is just 2%. Note again that we achieve a 13% increase in classification ac-curacy with Divisive Clustering over the maximum possi-ble with Information Gain. This reiterates the observation that feature clustering is an attractive option when training data is limited. Figure 10 compares Divisive Clustering with 
Agglomerative Clustering on Dmoz data where we observe similar improvements as with 20 Newsgroups data. 
Figure 11 shows classification accuracies obtained by 3 different classifiers on Dmoz data (Naive Bayes was the un-derlying classifier). By Flat, we mean a classifier built over the leaf set of classes in the tree. In contrast, Hierarchical denotes a hierarchical scheme that builds a classifier at each internal node of the topic hierarchy (see Section 4.3). Fur-ther we apply Divisive Clustering at each internal node to reduce the number of features in the classification model at that node. The number of word clusters is the same at each internal node. 
Figure 11 compares the Hierarchical Classifier with two flat classifiers, one that employs Information Gain for fea-ture selection while the other uses Divisive Clustering. Note that Divisive Clustering performs remarkably well for Hi-erarchical Classification even at very low number of fea-tures. With just 10 features, Hierarchical Classifier achieves 64.54% accuracy, which is slightly better than the maximum obtained by the two fiat classifiers at any number of features. 
At 50 features, Hierarchical Classifier achieves 68.42%, a sig-nificant 6% higher than the maximum obtained by the flat [7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by Latent 
Semantic Analysis. Journal of the American Society for Information Science, 41(6):391-407, 1990. [8] I. S. Dhillon and D. S. Modha. Concept decompositions for large sparse text data using clustering. Machine Learnin9, 42(1):143-175, 2001. [9] P. Domingos and M. J. Pazzani. On the the optimality of the simple Bayesian classifier under zero-one loss. 
Machine Learning, 29(2-3):103-130, 1997. [10] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern 
Classification. John Wiley &amp; Sons, 2nd edition, 2000. [11] E. Forgy. Cluster analysis of multivariate data: Efficiency vs. interpretability of classifications. 
Biometrics, 21(3):768, 1965. [12] J. H. Friedman. On bias, variance, 0/l-loss, and the curse-of-dimensionality. Data Mining and Knowledge 
Discovery, 1:55-77, 1997. [13] M. R. Garey, D. S. Johnson, and H. S. Witsenhausen. The complexity of the generalized Lloyd-Max problem. [14] D. Goldberg. What every computer scientist should know about floating point arithmetic. ACM 
Computing Surveys, 23(1), 1991. [15] R. M. Gray and D. L. Neuhoff. Quantization. IEEE [16] T. Hofmann. Probabilistic latent semantic indexing. 
In Proc. ACM SIGIR. ACM Press, August 1999. [17] T. Joachims. Text categorization with support vector machines: learning with many relevant features. In 
Proceedings of ECML-98, pages 137-142, 1998. [18] D. Koller and M. Sahami. Hierarchically classifying documents using very few words. In ICML, 1997.. [19] S. Kullback and R. A. Leibler. On information and sufficiency. Ann. Math. Star., 22:79--86, 1951. [20] J. Lin. Divergence measures based on the Shannon entropy. IEEE Trans. Inform. Theory, 37(1), 1991. [21] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In AAAI-98 
Workshop on Learning for Text Categorization, 1998. [22] A. K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, www.cs.cmu.edu/mccallum/bow, 1996. [23] T. Mitchell. Conditions for the equivalence of hierarchical and non-hierarchical bayesian classifiers. 
Technical report, CALD, CMU, 1998. [24] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997. [25] F. Pereira, N. Tishby, and L. Lee. Distributional clustering of English words. In 31st Annual Meeting of the ACL, pages 183-190, 1993. [26] G. Salton and M. J. McGill. Introduction to Modern 
Retrieval. McGraw-Hill Book Company, 1983. [27] C. E. Shannon. A mathematical theory of communication. Bell System Technical J., 27, 1948. [28] N. Slonim and N. Tishby. The power of word clusters for text classification. In ZJrd European Colloquium on 
Information Retrieval Research (ECIR), 2001. [29] V. Vapnik. The Nature of Statistical Learning Theory. 
Springer-Verlag, New York, 1995. [30] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML, 1997. 
