 The classic dichotomy between generative and discriminative methods for classification in machine learning can be clearly seen in two distinct performance regimes as the number of training examples is varied [12, 18]. Generative models X  X hich employ models first to find the underlying distribu-tion p ( x | y ) for discrete class label y and input data x  X  R D  X  X ypically outperform discriminative methods when the number of training examples is small, due to smaller variance in the generative models which compensates for any possible bias in the models. On the other hand, more flexible discriminative methods X  X hich are interested in a direct measure of p ( y | x )  X  X an accurately cap-ture the true posterior structure p ( y | x ) when the number of training examples is large. Thus, given enough training examples, the best performing classification algorithms have typically employed purely discriminative methods.
 However, due to the curse of dimensionality when D is large, the number of data examples may not be sufficient for discriminative methods to approach their asymptotic performance limits. In this case, it may be possible to improve discriminative methods by exploiting knowledge of generative models. There has been recent work on hybrid models showing some improvement [14, 15, 20], but mainly the generative models have been improved through the discriminative formulation. In this work, we consider a very simple discriminative classifier, the nearest neighbor classifier, where the class label of an unknown datum is chosen according to the class label of the nearest known datum. The choice of a metric to define nearest is then crucial, and we show how this metric can be locally defined based upon knowledge of generative models.
 Previous work on metric learning for nearest neighbor classification has focused on a purely discrim-inative approach. The metric is parameterized by a global quadratic form which is then optimized on the training data to maximize pairwise separation between dissimilar points, and to minimize the pairwise separation of similar points [3, 9, 10, 21, 26]. Here, we show how the problem of learning a metric can be related to reducing the theoretical bias of the nearest neighbor classifier. Though the performance of the nearest neighbor classifier has good theoretical guarantees in the limit of infinite data, finite sampling effects can introduce a bias which can be minimized by the choice of an appropriate metric. By directly trying to reduce this bias at each point, we will see the classification error is significantly reduced compared to the global class-separating metric.
 We show how to choose such a metric by analyzing the probability distribution on nearest neighbors, provided we know the underlying generative models. Analyses of nearest neighbor distributions have been discussed before [11, 19, 24, 25], but we take a simpler approach and derive the metric-dependent term in the bias directly. We then show that minimizing this bias results in a semi-definite programming optimization that can be solved analytically, resulting in a locally optimal metric. In related work, Fukunaga et al. considered optimizing a metric function in a generative setting [7, 8], but the resulting derivation was inaccurate and does not improve nearest neighbor performance. Jaakkola et al. first showed how a generative model can be used to derive a special kernel, called the Fisher kernel [12], which can be related to a distance function. Unfortunately, the Fisher kernel is quite generic, and need not necessarily improve nearest neighbor performance.
 Our generative approach also provides a theoretical relationship between metric learning and the dimensionality reduction problem. In order to find better projections for classification, research on dimensionality reduction using labeled training data has utilized information-theoretic measures such as Bhattacharrya divergence [6] and mutual information [2, 17]. We argue how these prob-lems can be connected with metric learning for nearest neighbor classification within the general framework of F-divergences. We will also explain how dimensionality reduction is entirely different from metric learning in the generative approach, whereas in the discriminative setting, it is simply a special case of metric learning where particular directions are shrunk to zero.
 The remainder of the paper is organized as follows. In section 2, we motivate by comparing the met-ric dependency of the discriminative and generative approaches for nearest neighbor classification. After we derive the bias due to finite sampling in section 3, we show, in section 4, how minimizing this bias results in a local metric learning algorithm. In section 5, we explain how metric learning should be understood in a generative perspective, in particular, its relationship with dimensionality reduction. Experiments on various datasets are presented in section 6, comparing our experimental results with other well-known algorithms. Finally, in section 7, we conclude with a discussion of future work and possible extensions. In recent work, determining a good metric for nearest neighbor classification is believed to be cru-cial. However, traditional generative analysis of this problem has simply ignored the metric issue with good reason, as we will see in section 2.2. In this section, we explain the apparent contra-diction between two different approaches to this issue, and briefly describe how the resolution of this contradiction will lead to a metric learning method that is both theoretically and practically plausible. 2.1 Metric Learning for Nearest Neighbor Classification A nearest neighbor classifier determines the label of an unknown datum according to the label of its nearest neighbor. In general, the meaning of the term nearest is defined along with the notion of distance in data space. One common choice for this distance is the Mahalanobis distance with a positive definite square matrix A  X  R D  X  D where D is the dimensionality of data space. In this case, the distance between two points x 1 and x 2 is defined as and the nearest datum x NN is one having minimal distance to the test point among labeled training In this classification task, the results are highly dependent on the choice of matrix A , and prior work has attempted to improve the performance by a better choice of A . This recent work has assumed the following common heuristic: the training data in different classes should be separated in a new metric space. Given training data, a global A is optimized such that directions separating different class data are extended, and directions binding same class data together are shrunk [3, 9, 10, 21, 26]. However, in terms of the test results, these conventional methods do not improve the performance dramatically, which will be shown in our later experiments on large datasets, and we show why only small improvements arise in our theoretical analysis. 2.2 Theoretical Performance of Nearest Neighbor Classifier Contrary to recent metric learning approaches, a simple theoretical analysis using a generative model displays no sensitivity to the choice of the metric. We consider i.i.d. samples generated from two different distributions p 1 ( x ) and p 2 ( x ) over the vector space x  X  R D . With infinite samples, the probability of misclassification using a nearest neighbor classifier can be obtained: which is better known by its relationship to an upper bound, twice the optimal Bayes error [4, 7, 8]. By looking at the asymptotic error in a linearly transformed z -space, we can show that Eq. (2) is invariant to the change of metric. If we consider a linear transformation z = L T x using a full and accompanying measure change d z = | L | d x , we see E Asymp in z -space is unchanged. Since any positive definite A can be decomposed as A = LL T , we can say the asymptotic error remains constant even as the metric shrinks or expands any spatial directions in data space.
 This difference in behavior in terms of metric dependence can be understood as a special property that arises from infinite data. When we do not have infinite samples, the expectation of error is biased in that it deviates from the asymptotic error, and the bias is dependent on the metric. From a theoretical perspective, the asymptotic error is the theoretical limit of expected error, and the bias reduces as the number of samples increase. Since this difference is not considered in previous research, the aforementioned metric will not exhibit performance improvements when the sample number is large.
 In the next section, we look at the performance bias associated with finite sampling directly and find a metric that minimizes the bias from the asymptotic theoretical error. Here, we obtain the expectation of nearest neighbor classification error from the distribution of nearest neighbors in different classes. As we consider finite number of samples, the nearest neighbor considered and approximated to second order near a test point x 0 : with the gradient  X  p ( x ) and Hessian matrix  X  X  X  p ( x ) defined by taking derivatives with respect to x .
 Now, under the condition that the nearest neighbor appears at the distance d N from the test point, the expectation of the probability p ( x NN ) at a nearest neighbor point is derived by averaging the probability over the D -dimensional hypersphere of radius d N , as in Fig. 1. After averaging, the gradient term disappears, and the resulting expectation is the sum of the probability at x 0 and a residual term containing the Laplacian of p . We replace this expected probability by  X  p ( x 0 ) . Figure 1: The nearest neighbor x NN appears at a finite distance d N from x 0 due to finite sampling. Given the data distribution p ( x ) , the average probability density function over the surface of a D dimensional hypersphere is  X  p ( x 0 ) = p ( x 0 ) + d 2 N 4 D  X  2 p | x = x 0 for small d N . where the scalar Laplacian  X  2 p ( x ) is given by the sum of the eigenvalues of the Hessian  X  X  X  p ( x ) . If we look at the expected error, it is the expectation of the probability that the test point and its neighbor are labeled differently. In other words, the expectation error E NN is the expectation of distribution of nearest neighbor x NN for a given x : p ( x NN ) / ( p 1 ( x NN ) + p 2 ( x NN )) respectively, and approximate the expectation of the posterior E we expand E NN with respect to d N , and take the expectation using the decomposition, E x NN h f i = E When E d N [ d 2 N ]  X  0 with an infinite number of samples, this error converges to the asymptotic limit in Eq. (2) as expected. The residual term can be considered as the finite sampling bias of the error discussed earlier. Under the coordinate transformation z = L T x and the distributions p ( x ) on x and q ( z ) on z , we see that this bias term is dependent upon the choice of a metric A = LL T . which is derived using p ( x ) d x = q ( z ) d z and | L | X  2 q = tr [ A  X  1  X  X  X  p ] . Expectation of squared distance E d N [ d 2 N ] is related to the determinant | A | , which will be fixed to 1. Thus, finding the metric that minimizes the quantity given in Eq. (7) at each point is equivalent to minimizing the metric-dependent bias in Eq. (6). Finding the local metric that minimizes the bias can be formulated as a semi-definite programming (SDP) problem of minimizing squared residual with respect to a positive semi-definite metric A : where the matrix B at each point is This is a simple SDP having an analytical solution where the solution shares the eigenvectors with in  X   X  , we use the solution given by The solution A opt is a local metric since we assumed that the nearest neighbor was close to the test point satisfying Eq. (3). In principle, distances should then be defined as geodesic distances using this local metric on a Riemannian manifold. However, this is computationally difficult, so we use the surrogate distance A =  X I + A opt and treat  X  as a regularization parameter that is learned in addition to the local metric A opt .
 The multiway extension of this problem is straightforward. The asymptotic error with C -class dis-tributions can be extended to 1 C P C c =1 R p c P j 6 = i p j / P i p i d x using the posteriors of each class, and it replaces B in Eq. (9) by the extended matrix: Traditional metric learning methods can be understood as being purely discriminative. In contrast to our method that directly considers the expected error, those methods are focused on maximizing the separation of data belonging to different classes. In general, their motivations are compared to the supervised dimensionality reduction methods, which try to find a low dimensional space where the separation between classes is maximized. Their dimensionality reduction is not that different from metric learning, but often as a special case where metric in particular directions is forced to be zero. In the generative approach, however, the relationship between dimensionality reduction and metric learning is different. As in the discriminative case, dimensionality reduction in generative models tries to obtain class separation in a transformed space. It assumes particular parametric distributions (typically Gaussians), and uses a criterion to maximize the separation [2, 6, 16, 17]. One general form of these criteria is the F-divergence (also known as Csiszer X  X  general measure of divergence), that can be defined with respect to a convex function  X  ( t ) for t  X  R [13]: when  X  ( t ) = mutual information between data and labels can be understood as an extension of KL-divergence. The well known Linear Discriminant Analysis is a special example of Bhattacharyya criterion when we assume two-class Gaussians sharing the same covariance matrices.
 Unlike dimensionality reduction, we cannot use these criteria for metric learning because any F-divergence is metric-invariant. The asymptotic error Eq. (2) is related to one particular F-divergence Figure 2: Optimal local metrics are shown on the left for three example Gaussian distributions in a 5-dimensional space. The projected 2-dimensional distributions are represented as ellipses (one standard deviation from the mean), while the remaining 3 dimensions have an isotropic distribution. The local  X  p/p of the three classes are plotted on the right using a Euclidean metric I and for the similar as possible when the distance d N is varied. by E Asymp = 1  X  F ( p 1 ,p 2 ) with a convex function  X  ( t ) = 1 / (1 + t ) . Therefore, in generative models, the metric learning problem is qualitatively different from the dimensionality reduction problem in this aspect. One interpretation is that the F-measure can be understood as a measure of dimensionality reduction in an asymptotic situation. In this case, the role of metric learning can be defined to move the expected F-measure toward the asymptotic F-measure by appropriate metric adaptation.
 Finally, we provide an alternative understanding on the problem of reducing Eq. (7). By reformulat-the difference between  X  2 p 1 p probabilities p 1 /p 2 at the test point. This means that the expected nearest neighbor classification at a distance d N will be least biased due to finite sampling. Fig. 2 shows how the learned local metric A opt varies at a point x for a 3-class Gaussian example, and how the ratio of  X  p/p is kept as similar as possible. We apply our algorithm for learning a local metric to synthetic and various real datasets and see how well it improves nearest neighbor classification performance. Simple standard Gaussian dis-tributions are used to learn the generative model, with parameters including the mean vector  X  and covariance matrix  X  for each class. The Hessian of a Gaussian distribution is then given by the expression: This expression is then used to learn the optimal local metric. We compare the performance of our method (GLML X  X enerative Local Metric Learning) with recent metric learning discrimina-tive methods which report state-of-the-art performance on a number of datasets. These include Information-Theoretic Metric Learning (ITML) 1 [3], Boost Metric 2 (BM) [21], and Largest Margin Nearest Neighbor (LMNN) 3 [26]. We used the implementations downloaded from the correspond-ing authors X  websites. We also compare with a local metric given by the Fisher kernel [12] assuming a single Gaussian for the generative model and using the location parameter to derive the Fisher in-formation matrix. The metric from the Fisher kernel was not originally intended for nearest neighbor classification, but it is the only other reported algorithm that learns a local metric from generative models.
 For the synthetic dataset, we generated data from two-class random Gaussian distributions having two fixed means. The covariance matrices are generated from random orthogonal eigenvectors and random eigenvalues. Experiments were performed varying the input dimensionality, and the classi-fication accuracies are shown in Fig. 3.(a) along with the results of the other algorithms. We used 500 test points and an equal number of training examples. The experiments were performed with 20 different realizations and the results were averaged. As the dimensionality grows, the original nearest neighbor performance degrades because of the high dimensionality. However, we see that the proposed local metric highly outperforms the discriminative nearest neighbor performance in a high dimensional space appropriately. We note that this example is ideal for GLML, and it shows much improvement compared to the other methods.
 The other experiments consist of the following benchmark datasets: UCI machine learning reposi-tory 4 datasets (Ionosphere, Wine), and the IDA benchmark repository 5 (German, Image, Waveform, Twonorm). We also used the USPS handwritten digits and the TI46 speech dataset. For the USPS data, we resized the images to 8  X  8 pixels and trained on the 64-dimensional pixel vector data. For the TI46 dataset, the examples consist of spoken sounds pronounced by 8 different men and 8 dif-ferent women. We chose the pronunciation of ten digits ( X  X ero X  to  X  X ine X ), and performed a 10 class digit classification task. 10 different filters in the Fourier domain were used as features to preprocess the acoustic data. The experiments were done on 20 data sampling realizations for Twonorm and TI46, 10 for USPS, 200 for Wine, and 100 for the others.
 Except the synthetic data in Fig. 3.(a), the data consist of various number of training data per class. The regularization parameter  X  value is chosen by cross-regularization on a subset of the training regularized by setting  X  =  X   X  +  X I where  X   X  is the estimated covariance. The parameter  X  is set prior to each experiment.
 From the results shown in Fig. 3, our local metric algorithm generally outperforms most of the other metrics across most of the datasets. On quite a number of datasets, many of the other methods do not outperform the original Euclidean nearest neighbor classifier. This is because on some of these datasets, performance cannot be improved using a global metric. On the other hand, the local metric derived from simple Gaussian distributions always shows a performance gain over the naive nearest neighbor classifier. In contrast, using Bayes rule with these simple Gaussian generative models often results in very poor performance. The computational time using a local metric is also very competitive, since the underlying SDP optimization has a simple spectral solution. This is in contrast to other methods which numerically solve for a global metric using an SDP over the data points. In our study, we showed how a local metric for nearest neighbor classification can be learned using generative models. Our experiments show improvement over competitive methods on a number of experimental datasets. The learning algorithm is derived from an analysis of the asymptotic performance of the nearest neighbor classifier, such that the optimal metric minimizes the bias of the expected performance of the classifier. This connection to generative models is very powerful, and can easily be extended to include missing data X  X ne of the large advantages of generative models Figure 3: (a) Gaussian synthetic data with different dimensionality. As number of dimensions gets large, most methods degrade except GLML and LMNN. GLML continues to improve vastly over other methods. (b)  X  (h) are the experiments on benchmark datasets varying the number of training data per class. (i) TI46 is the speech dataset pronounced by 8 men and 8 women. The Fisher kernel and BM are omitted for (f)  X  (i) and (h)  X  (i) respectively, since their performances are much worse than the naive nearest neighbor classifier. in machine learning. Here we used simple Gaussians for the generative models, but this could be also easily extended to include other possibilities such as mixture models, hidden Markov models, or other dynamic generative models.
 The kernelization of this work is straightforward, and the extension to the k-nearest neighbor setting using the theoretical distribution of k-th nearest neighbors is an interesting future direction. Another possible future avenue of work is to combine dimensionality reduction and metric learning using this framework.
 Acknowledgments
