 Two flavors of the recommendation problem are the explicit and the implicit feedback settings. In the explicit feedback case, users rate items and the user X  X tem preference relation-ship can be modelled on the basis of the ratings. In the harder but more common implicit feedback case, the sys-tem has to infer user preferences from indirect information: presence or absence of events, such as a user viewed an item. One approach for handling implicit feedback is to minimize a ranking objective function instead of the conventional pre-diction mean squared error. The naive minimization of a ranking objective function is typically expensive. This diffi-culty is usually overcome by a trade-off: sacrificing the accu-racy to some extent for computational efficiency by sampling the objective function. In this paper, we present a compu-tationally effective approach for the direct minimization of a ranking objective function, without sampling. We demon-strate by experiments on the Y!Music and Netflix data sets that the proposed method outperforms other implicit feed-back recommenders in many cases in terms of the ErrorRate, ARP and Recall evaluation metrics.
 I.2.6 [ Artificial Intelligence ]: Learning X  parameter learn-ing alternating least squares, ranking, collaborative filtering
The goal of recommender systems is to provide personal-ized recommendations on items to users [1]. The basis of the recommendation is user event history related to items, and metadata about users and items. Two flavors of the  X  Also affiliated with Gravity Research and Development Ltd., Expo t  X er 5 X 7, Budapest, Hungary. recommendation problem are the explicit and the implicit feedback settings.

In the explicit feedback case, users rate items, therefore the preference relationship between a set of user X  X tem pairs is directly known. A famous example of the explicit feedback setting is the movie recommendation problem of the Netflix Prize competition [2]. A typical characteristic of explicit feedback problems is that user X  X tem pairs where no rating is present are not necessarily needed for modeling users X  taste.
In many application domains, however, ratings are not available. The recommender system has to infer user pref-erences from implicit feedback [4], such as the presence or absence of purchase, click, or search events. A significant part of the information is contained in the absence of events, therefore the system has to consider each user X  X tem pair. This makes the implicit feedback problem substantially harder than the explicit feedback one. This paper deals with the implicit feedback variant of the recommendation problem.
Many of the known methods for giving recommendation can be classified as prediction based or ranking based. Pre-diction based methods (e.g. [4]) try to predict the presence or absence of interaction between users and items. Ranking based methods (e.g. [5]) try to model users X  choice between item pairs. In both cases, model building (training) is typi-cally executed as the minimization of an objective function.
In the implicit feedback case, the naive minimization of the objective function is expensive. This difficulty is usually overcome by a trade-off: sacrificing the accuracy to some ex-tent for computational efficiency by sampling the objective function.

In this paper, we propose RankALS, a computationally effective approach for the direct minimization of a ranking objective function without sampling, that is able to cope with the implicit feedback case. We show an efficient imple-mentation of this method; for that we apply a similar but more complex derivation as used in ImplicitALS [4]. We also prove that the time complexity of RankALS is identical with that of ImplicitALS. We perform experiments on two  X  X mplicitized X  rating data sets, Y!Music and Netflix. In a comparison we show that RankALS outperforms Implicit-ALS in most cases in terms of three evaluation metrics, and it is usually better than ranking based methods employing the sampling strategy.

The rest of the paper is organized as follows. First, the notation is introduced. In Section 2, prediction and rank-ing based objective functions are reviewed. Next, we survey related work and highlight ImplicitALS, the algorithm that inspired RankALS. Section 4 introduces RankALS; first, its main idea and its advantageous computational properties are shown, then pseudocode with explanation is presented. The last part discusses experiments and performance com-parisons with other methods on two publicly available large-scale data sets.
U and I will denote the number of users and items. U = { 1 ,...,U } and I = { 1 ,...,I } will denote the set of items and users respectively. We use u  X  X  as index for users, and i,j  X  X  as indices for items.

The implicit rating of user u on item i is r ui , and its matrix R  X  R U  X  I . T denotes the set of ( u,i ) indexes of R where a positive feedback is provided from user u on item i . We assume that the implicit rating value is 0 for negative feedback (i.e. r ui = 0, if ( u,i ) /  X  X  ) and positive for positive feedback. We assume that users give positive feedback for items at most once.

T = |T| denotes the number of positive feedbacks. I u = { i : ( u,i )  X  X } denotes the set of items for which a positive feedback is provided by user u . U i = { u : ( u,i )  X  X } denotes the set of user who provided positive feedback for item i . z u = |I u | denotes the number of positive feedbacks of user u and z  X  R U denotes the vector of I u values.

We also employ the usual matrix algebraic notations diag( x ), for the diagonal matrix containing the elements of vector x in its main diagonal, and tr( X ) for the trace of square ma-trix X . In matrix algebraic expressions, 1 means a vector of all ones of appropriate size.

X [ S ] denotes the submatrix of matrix X selected by the row index set S , and x [ S ] denotes the subvector of vector x selected by the index set S . X [ S 1 , S 2 ] denotes the subma-trix of X selected by the row index set S 1 and the column index set S 2 . Submatrix selection has precedence over the transpose operation, i.e. X T [ ... ] is evaluated as ( X
Two major approaches for the recommendation problem are prediction based and ranking based recommendation. The debate on which approaches is better and when one is preferred to the other is still active. In the next subsections, we define objective functions for the two approaches. We will omit regularization terms in order to keep the discussion shorter.
The objective function associated with a simple prediction based approach can be defined as where  X  contains the parameters of the prediction model. 1 The c ui &gt; 0 values are parameters of the objective function that have to be fixed in advance. The goal of model building is to make the sum of weighted squared differences between the predicted and true ratings small. c ui means the extent to which we penalize the error on user u and item i .
The standard choice for c ui in the explicit feedback case is c ui = 1, if ( u,i )  X  T and 0 otherwise, which means that
Note that  X  r ui depends on  X , but for brevity, we will not write  X  r ui ( X ) explicitly. the objective function contains only T terms. In the implicit feedback case, this simplification is not reasonable, because a significant part of the information is contained in the absence of positive feedback.

A typical choice for c ui in the implicit feedback case is c ui = c + , if ( u,i )  X  X  and c  X  otherwise. The values c c  X  can be interpreted as confidence levels associated with positive and negative feedbacks respectively. We will refer to this specific variant of the prediction based objective func-tion as f I ( I stands for implicit). The function f I contains U  X  I terms, therefore its naive minimization is expensive.
Usually, c + c  X  , which means that the presence of pos-itive feedback has to be predicted more accurately than the absence of positive feedback. To see the rationale behind this asymmetry, consider a webshop scenario. If user u has bought item i that we can consider this as a positive feed-back with high confidence. If i is not present in the purchase history of u , then we can treat this as a negative feedback from user u on item i , but only with low confidence.
Note that the interpretation of implicit feedback data may not necessarily reflect user satisfaction. Even the presence of positive feedback may be misleading: (1) a purchased item could be disappointing for the user but the purchase information is generated before the user could evaluate the product; (2) the user may purchase the item for someone else, therefore her selection only reflects her knowledge and assumption on the preference of another person. The inter-pretation of the absence of positive feedback is even more speculative: we cannot directly interpret missing naviga-tional or purchase information as an intentional negative feedback; in the implicit setting direct negative feedback is not available.
The objective function associated with a simple ranking based approach can be defined as f R ( X ) = X where the c ui and s j values are parameters of the objective function. This function was introduced by [5] in the rec-ommender system literature (although other ranking based objectives appeared earlier). Here we only consider the im-plicit feedback case and assume that c ui = 0 if r ui = 0, and 1 otherwise. The role of c ui is to select user X  X tem pairs cor-responding to positive feedbacks from all possible pairs. The meaning of s j is the importance weight of the j -th item in the objective function. Note that the number of terms in f is T  X  I , therefore the naive minimization of f R is expensive.
Note that there is an asymmetric relation between the objective functions f I and f R . A good solution with respect to f I will also be a good solution with respect to f R , but the opposite is not necessarily true. One can also observe that user-only dependent terms in  X  r ui do not change the value of f , in other words, f R is invariant to user biases.
The literature of explicit feedback based recommendation algorithms is rich. We do not highlight any approach here, since this paper focuses on implicit feedback, just direct the interested reader to the survey work [10].

The area of implicit feedback based recommendation is much less explored. One of the earliest solutions for handling implicit feedback is Hu et al. X  X  ImplicitALS [4] that consists of a matrix factorization model, a prediction based objective function, and an alternating least squares (ALS) optimizer, tuned for the problem (see a more detailed discussion in Section 3.1).

A faster, approximative version of ImplicitALS was pro-posed by Pil  X aszy et al. [8]. The speedup is achieved by replacing the exact least squares solver by a coordinate de-scent method. The authors reported on a marginal loss of accuracy compared to the significant decrease in training time in their experiments.

Pan et al. [7] proposed two prediction based frameworks for handling implicit feedback. The first one is similar to ImplicitALS, but it contains a naive ALS optimizer instead of a tuned one. The second one is based on negative example sampling.

Recently, the results of Track 2 at KDD Cup 2011 [3] have shown an evidence that ranking based approaches are able to handle implicit feedback efficiently on large-scale prob-lems. Ranking based methods had significant weight in the winning solution [6], and they were the most important com-ponents of the second runner-up solution [5]. In particular, the key idea of the latter is to define a ranking based objec-tive function, and apply a stochastic gradient descent (SGD) optimizer on a sampled approximation of the objective func-tion.
 Another ranking based approach for implicit feedback is Rendle et al X  X  Bayesian Personalized Ranking (BPR) [9]. The objective function of BPR is derived from the Bayesian analysis of the problem. The optimization technique used for training is bootstrapping based SGD.

Our proposed RankALS method differs from all of the above approaches in various aspects: Here we overview the main points of Hu et al. X  X  Implicit-ALS [4] that can be considered as a straight predecessor of our proposed RankALS method. The prediction formula of ImplicitALS for user u and item i is  X  r ui = p T u q i . The param-eters of the model p 1 ,...,p U  X  R F and q 1 ,...,q I  X  R called user and item feature vectors; F denotes the number of features. 2
Let P and Q denote the matrices that contain user and item feature vectors as rows. The objective function associ-ated with the model is f I , thus the goal of ImplicitALS is to approximate the rating matrix as R  X  PQ T so that the weighted squared error of the approximation is low.
The key idea of ImplicitALS is that although the objective function consists of U  X  I terms, its alternating least squares
We omitted user and item bias parameters, because they are not necessary for illustrating the ideas.
 Algorithm 1 : Alternating least squares based training.
Initialize Q with small random numbers. for E times do end (ALS) based minimization can be done efficiently. The out-line of the training of ImplicitALS is given in Algorithm 1.
To see why this scheme can be implemented efficiently, let us rewrite the derivative of f I with respect to p u as
Minimizing f I in P means that we make the derivatives vanish by setting p u to  X  A  X  1 u  X  b u for all u . Note that simplified to P i  X  X  independent and a user-dependent part as  X  A u = P i  X  X  c P  X  A
Applying the same argument for items yields that item feature vectors can be updated in O ( TF 2 + IF 3 ) time. There-fore, the computational cost of one training step is O ( TF ( U + I ) F 3 ) which means much less operations than iterating over the terms of the objective function.

A nice property of ImplicitALS is that it does not replace objective function by an approximation, as other approaches often do. It achieves speedup by applying mathematical simplification.
This section presents RankALS, our proposed method for personalized ranking. The prediction formula of RankALS that the function to minimize is now the ranking objective function f R .

Recall that the ranking objective function contains T  X  I terms. We will show that even though f R contains more and more complex terms than f I , its ALS based minimiza-tion can be done at the same asymptotic computational cost. We will use a similar ideas as for ImplicitALS to break down the computational complexity of the naive implementation of RankALS, but the mathematical derivation and the re-sulting training algorithm will be more complex.

At first, let us rewrite the derivative of f R with respect to the user feature vector p u .
X | {z }  X 
X
We converted the expression of the derivative to 8 double sums by expanding the inner term. Then, we factorized each double sum in order to make the evaluation of the expression more efficient. The advantage of this rewritten form of the  X  b ,  X  q ,  X  r ,  X  1 for all u can be done in O ( TF 2 subscript them by u in order to make formulae cleaner.
Now let us rewrite the derivative of f R with respect to q
X  X   X   X   X   X   X 
X  X   X   X   X   X   X 
X
We applied the same idea as before. The expression of the derivative was converted into 8 double sums, and then the double sums were factorized. Again, computing the statis-tics  X  A ,  X  b ,  X  1,  X  q ,  X  1,  X   X  A ,  X   X  b , in O ( TF 2 ) time, therefore the evaluation of  X  X  R ( P,Q ) O ( TF 2 ) time.

Note that  X  A and  X  b are defined differently as in the user-depend on i or u , but we avoided subscripting them.
The previous results imply that the ALS based minimiza-tion of f R with a matrix factorization model can be imple-mented efficiently. The two main steps of ALS are evalu-ating the derivatives and setting them to zero. We have seen that the first step can be speeded up by rewriting the derivatives. The second step is relatively cheap, because it consists of solving linear systems.

Now we are ready to put blocks together and specify the model building procedure of RankALS. The pseudocode of RankALS training can be seen in Algorithm 2.

Each iteration consists of a P -step and a Q -step. Most of the pseudocode deals with the calculation of statistics needed for derivative evaluation. The statistic  X  1 is constant during the algorithm, therefore it is computed before the main loop.

The P -step starts with calculating the statistics  X  q and that are common for each user. Then, for each user u , the and p u is updated by setting the derivative  X  X  R ( P,Q )
The Q -step starts with calculating the item-independent Then, for each item i , the item-dependent statistics  X   X  b and  X   X  p 1 are computed, and q i is updated by setting the
The tunable parameters of the algorithm are as follows:
The computationally most expensive parts of the P -step are calculating  X  A and updating p u . Computing  X  A and p Algorithm 2 : Alternating least squares for ranking. Input : R  X  R U  X  I , E  X  N ,  X   X  R , s  X  R I
Q  X  uniform random numbers from [  X   X , X  ]  X  for e  X  1 ,...,E do end all users takes O ( TF 2 ) and O ( UF 3 ) time respectively. The most expensive parts of the Q -step are calculating updating q i Computing  X  A and q i for all items takes O ( TF and O ( IF 3 ) time respectively. The overall complexity of one iteration is O ( TF 2 + ( U + I ) F 3 ), which is the same as for ImplicitALS.

RankALS training is a relatively complex algorithm, and implementing it without error is not trivial. We validated our code by also implementing naive derivative evaluation and comparing the two variants. Apart from the very tiny difference due to round-off errors, the two versions produced the same result.
We compared our proposed method with other approaches on  X  X mplicitized X  versions of the Y!Music [3] and the Netflix [2] data sets. The characteristics of the data sets are as follows:
We compared the following methods:
We measured the following performance indicators:
The set I 0 of ARP and Recall measurement contained all 17,770 items in the case of the Netflix data set, and 17,770 uniformly drawn items in the case of the Y!Music data set. For Y!Music, the sets I  X  u were taken from the official test set of KDD Cup 2011, Track 2. For Netflix, the sets I were drawn randomly, since no official negative item sets are available. The parameter K of Recall measurement was 50.

The learning rate was  X  = 0 . 008 for RankSGD and  X  = 0 . 016 for RankSGD2 in all experiments. We did not ap-ply regularization for RankSGD, RankSGD, and RankALS. 4 For ImplicitALS and RankALS, training started with a Q -step and not with a P -step as in the pseudocodes. For SGD-based approaches we ran E = 30, for ALS-based approaches we ran E = 10 iterations. We stopped training, if the perfor-mance started to decrease on the test set. The confidence level parameters of the objective function f I were set to c + = 100 and c  X  = 1.
The choice of the item weight vector s can greatly influ-ence the behavior of the ranking based approaches (RankSGD, RankSGD2, RankALS). In the first experiment, we tried to find the appropriate variant of the methods for the different evaluation criteria. The results for the Y!Music and the Net-flix data set are shown in Table 1 and 2. The column s.w. indicates if support based weighting was applied ( s i = |U or not ( s i = 1).

The results suggest that support based weighting should be turned on if one optimizes for ErrorRate. This is not
This rule could be relaxed. We apply it, because it was used in Track2 of KDD Cup 2011.
According to [5], regularizing RankSGD and RankSGD2 models did not help too much in Track 2 of KDD Cup 2011. Table 1: Comparison of ranking based method vari-ants on the Y!Music data set ( F = 20 ) Table 2: Comparison of ranking based method vari-ants on the Netflix data set ( F = 20 ) surprising, since the negative items of the test set are drawn with probability proportional to |U i | in the case of Error-Rate. In further experiments with ErrorRate, we will always use the setting s.w. = yes.

If the evaluation criterion is ARP, then support based weighting should not be applied. This result was again ex-pectable, because negative items of the test set are drawn uniformly in the case of ARP. In further experiments with ARP, we will always use the setting s.w. = no.

The results for Recall are diversified: support based weight-ing helps significantly in the case of RankSGD, but has neg-ative effect for RankSGD2 and RankALS. In further exper-iments with Recall, we will always use the setting s.w. = no for RankSGD and RankALS, and s.w. = yes for RankSGD.
We also mention for comparison that the ImplicitALS ap-proach with F = 20 achieves ErrorRate = 0.1154, ARP = 0.0291, Recall = 0.3181 on the Y!Music, and ErrorRate = 0.2732, ARP = 0.0494, Recall = 0.1835 on the Netflix data set. These numbers suggest that ImplicitALS tends to be less accurate for a given evaluation metric than the best ranking based method variant, but is is less sensitive to the choice of the metric.
In this experiment, we ran all methods on all datasets with different number of features. The results are shown in Figure 1 X 6. It can be observed that RankALS clearly out-performs other methods on the Y!Music data set in terms of ErrorRate and ARP independently from the number of fea-tures, while it is on par with SGD-based method according to Recall. On the Netflix data set, RankALS shows simi-lar behavior, with the exception of F = 50 and ARP. In-terestingly, on this data set ImplicitALS outperforms other methods for Recall. In this experiment, we compared the convergence speed of RankSGD and RankALS. In particular, we investigated how the ErrorRate decreases with the number of iterations on the Y!Music data set. The number of features was set to F = 100 for both methods. The results are shown in Figure 7, where it can be observed that RankALS convergences faster.
We also compared the iteration times of the methods on the Y!Music data set. In the case of ImplicitALS and Rank-ALS an iteration contains both a P -and a Q -step. We used one core of a Intel Xeon E5405 CPU for the measurement. The results are shown in Table 3.

As expected, RankSGD and RankSGD2 methods that use Figure 7: ErrorRate per iteration for RankSGD and RankALS on the Y!Music data set. Table 3: Time per iteration in seconds on the Y!Music data set (column labels refer to the number of features, F ) SGD and sampling are faster then ALS-based methods that optimize the original objective function. Note that SGD-based methods have the learning rate as additional param-eter, which need to be adjusted appropriately; this intro-duces another optimization round into the parameter set-ting. We also mention that our RankSGD and RankSGD2 implementations were more optimized than our ImplicitALS and RankALS implementations.
Learning user X  X tem preference from implicit feedback is a key problem of recommender systems research. One ap-proach for handling implicit feedback is to minimize a rank-ing objective function instead of the conventional prediction mean squared error. Ranking objective functions are typ-ically expensive to optimize, and this difficulty is usually overcome by sampling the objective function.

In this paper, we proposed a computationally efficient ranking based method RankALS that optimizes the orig-inal objective function, without sampling. RankALS was inspired by the prediction based method ImplicitALS [4]. The key components of RankALS are a matrix factoriza-tion model, a ranking based objective function, and an al-ternating least squares optimizer. Speedup is achieved by the mathematical simplification of the objective function X  X  derivative.

As we have shown, RankALS outperforms the sampling based methods used in the comparison in terms of the Er-rorRate and ARP in most cases, and is also on par with SGD-based methods in terms of Recall. Another advantage order of training examples (unlike SGD-based models) and with the fewer parameters (no learning rate) its adaptation to a new data set can be done with less experiments. There are various ways to extend the RankALS method. One possible direction is to make RankALS faster by replac-ing the exact least squares solver by an approximate one [8]. Furthermore, its modeling capability can be improved by adding context-awareness and introducing time-dependent or user/item metadata-dependent terms into the prediction formula. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] J. Bennett and S. Lanning. The Netflix Prize. In Proc. [3] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [4] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [5] M. Jahrer and A. T  X  oscher. Collaborative filtering [6] T. G. McKenzie, C. S. Ferng, Y. N. Chen, C. L. Li, [7] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, [8] I. Pil  X aszy, D. Zibriczky, and D. Tikk. Fast ALS-based [9] S. Rendle, C. Freudenthaler, Z. Gantner, and [10] X. Su and T. M. Khoshgoftaar. A survey of
