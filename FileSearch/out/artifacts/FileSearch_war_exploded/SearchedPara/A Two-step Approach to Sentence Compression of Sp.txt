 Sentence compression aims to preserve the most im-portant information in the original sentence with fewer words. It can be used for abstractive summa-rization where extracted important sentences often need to be compressed and merged. For summariza-tion of spontaneous speech, sentence compression is especially important, since unlike fluent and well-structured written text, spontaneous speech contains a lot of disfluencies and much redundancy. The fol-lowing shows an example of a pair of source and compressed spoken sentences 1 from human annota-tion (removed words shown in bold):
In this study we investigate sentence compres-sion of spoken utterances in order to remove re-dundant or unnecessary words while trying to pre-serve the information in the original sentence. Sen-tence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this prob-lem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and La-pata, 2008) expands the operation set by including insertion, substitution and reordering, and incorpo-rates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spon-taneous speech domains. (Liu and Liu, 2010) mod-eled it as a sequence labeling problem using con-ditional random fields model. (Liu and Liu, 2009) compared the effect of different compression meth-ods on a meeting summarization task, but did not evaluate sentence compression itself.

We propose to use a two-step approach in this pa-per for sentence compression of spontaneous speech utterances. The contributions of our work are:  X  Our proposed two-step approach allows us to  X  We evaluate our methods using different met-We use the same corpus as (Liu and Liu, 2010) where they annotated 2,860 summary sentences in 26 meetings from the ICSI meeting corpus (Murray et al., 2005). In their annotation procedure, filled pauses such as  X  X h/um X  and incomplete words are removed before annotation. In the first step, 8 anno-tators were asked to select words to be removed to compress the sentences. In the second step, 6 an-notators (different from the first step) were asked to pick the best one from the 8 compressions from the previous step. Therefore for each sentence, we have 8 human compressions, as well a best one se-lected by the majority of the 6 annotators in the sec-ond step. The compression ratio of the best human reference is 63.64%.

In the first step of our sentence compression ap-proach (described below), for model training we need the reference labels for each word, which rep-resents whether it is preserved or deleted in the com-pressed sentence. In (Liu and Liu, 2010), they used the labels from the annotators directly. In this work, we use a different way. For each sentence, we still use the best compression as the gold standard, but we realign the pair of the source sentence and the compressed sentence, instead of using the labels provided by annotators. This is because when there are repeated words, annotators sometimes randomly pick removed ones. However, we want to keep the patterns consistent for model training  X  we always label the last appearance of the repeated words as  X  X reserved X , and the earlier ones as  X  X eleted X . An-other difference in our processing of the corpus from the previous work is that when aligning the original and the compressed sentence, we keep filled pauses and incomplete words since they tend to appear to-gether with disfluencies and thus provide useful in-formation for compression. Our compression approach has two steps: in the first step, we use Conditional Random Fields (CRFs) to model this problem as a sequence labeling task, where the label indicates whether the word should be removed or not. We select n-best candidates ( n = 25 in our work) from this step. In the second step we use discriminative training based on a maximum En-tropy model to rerank the candidate compressions, in order to select the best one based on the quality of the whole candidate sentence, which cannot be performed in the first step. 3.1 Generate N-best Candidates In the first step, we cast sentence compression as a sequence labeling problem. Considering that in many cases phrases instead of single words are deleted, we adopt the  X  X IO X  labeling scheme, simi-lar to the name entity recognition task:  X  X  X  indicates the first word of the removed fragment,  X  X  X  repre-sents inside the removed fragment (except the first word), and  X  X  X  means outside the removed frag-ment, i.e., words remaining in the compressed sen-tence. Each sentence with n words can be viewed as a word sequence X 1 ,X 2 ,...,X n , and our task is to find the best label sequence Y 1 ,Y 2 ,...,Y n where Y i is one of the three labels. Similar to (Liu and Liu, 2010), for sequence labeling we use linear-chain first-order CRFs. These models define the condi-tional probability of each labeling sequence given the word sequence as: where f j are transition feature functions (here first-order Markov independence assumption is used); g i are observation feature functions;  X  j and  X  i are their corresponding weights. To train the model for this step, we use the best reference compression to obtain the reference labels (as described in Section 2).
In the CRF compression model, each word is rep-resented by a feature vector. We incorporate most of the features used in (Liu and Liu, 2010), includ-ing unigram, position, length of utterance, part-of-speech tag as well as syntactic parse tree tags. We did not use the discourse parsing tree based features because we found they are not useful in our exper-iments. In this work, we further expand the feature set in order to represent the characteristics of disflu-encies in spontaneous speech as well as model the adjacent output labels. The additional features we introduced are:  X  the distance to the next same word and the next  X  a binary feature to indicate if there is a filled  X  the combination of word/POS tag and its posi- X  language model probabilities: the bigram prob- X  transition features: a combination of the current 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candi-dates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disflu-ency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: where f ( x,y ) are feature functions and  X  i are their weighting parameters; Z ( x ) is the normalization factor.

In this reranking model, every compression can-didate is represented by the following features:  X  All the bigrams and trigrams of words and POS  X  Bigrams and trigrams of words and POS tags in  X  The log likelihood of the candidate sentence  X  The absolute difference of the compression ra- X  The probability of the label sequence of the  X  The rank of the candidate sentence in 25 best
For discriminative training using the n-best can-didates, we need to identify the best candidate from the n-best list, which can be either the reference compression (if it exists on the list), or the most similar candidate to the reference. Since we have 8 human compressions and also want to evaluate system performance using all of them (see exper-iments later), we try to use multiple references in this reranking step. In order to use the same train-ing objective (maximize the score for the single best among all the instances), for the 25-best list, if m reference compressions exist, we split the list into m groups, each of which is a new sample containing one reference as positive and several negative can-didates. If no reference compression appears in 25-best list, we just keep the entire list and label the in-stance that is most similar to the best reference com-pression as positive. We perform a cross-validation evaluation where one meeting is used for testing and the rest of them are used as the training set. When evaluating the system performance, we do not consider filled pauses and incomplete words since they can be easily identi-fied and removed. We use two different performance metrics in this study.  X  Word-level accuracy and F1 score based on the  X  BLEU score. BLEU is a widely used metric
Table 1 shows the averaged scores of the cross validation evaluation using the above metrics for several methods. Also shown in the table is the com-pression ratio of the system output. For  X  X eference X , we randomly choose one compression from 8 ref-erences, and use the rest of them as references in calculating the BLEU score. This represents human performance. The row  X  X asic features X  shows the result of using all features in (Liu and Liu, 2010) except discourse parsing tree based features, and us-ing binary labels (removed or not). The next row uses this same basic feature set and  X  X IO X  labels. Row  X  X xpanded features X  shows the result of our ex-panded feature set using  X  X IO X  label set from the first step of compression. The last two rows show the results after reranking, trained using one best ref-erence or 8 reference compressions, respectively.
Our result using the basic feature set is similar to that in (Liu and Liu, 2010) (their accuracy is 76 . 27% when compression ratio is 0 . 7 ), though the experi-mental setups are different: they used 6 meetings as the test set while we performed cross validation. Us-ing the  X  X IO X  label set instead of binary labels has marginal improvement for the three scores. From the table, we can see that our expanded feature set is able to significantly improve the result, suggesting the effectiveness of the new introduced features.
Regarding the two training settings in reranking, we find that there is no gain from reranking when using only one best compression, however, train-ing with multiple references improves BLEU scores. This indicates the discriminative training used in maximum entropy reranking is consistent with the performance metrics. Another reason for the per-formance gain for this condition is that there is less data imbalance in model training (since we split the n-best list, each containing fewer negative exam-ples). We also notice that the compression ratio af-ter reranking is more similar to the reference. As suggested in (Napoles et al., 2011), it is not appro-priate to compare compression systems with differ-ent compression ratios, especially when considering grammars and meanings. Therefore for the com-pression system without reranking, we generated re-sults with the same compression ratio ( 77 . 15% ), and found that using reranking still outperforms this re-sult, 1 . 19% higher in BLEU score.

For an analysis, we check how often our sys-tem output contains reference compressions based on the 8 references. We found that 50 . 8% of sys-tem generated compressions appear in the 8 refer-ences when using CRF output with a compression ration of 77 . 15% ; and after reranking this number increases to 54 . 8% . This is still far from the oracle result  X  for 84 . 7% of sentences, the 25-best list con-tains one or more reference sentences, that is, there is still much room for improvement in the reranking process. The results above also show that the token level measures by comparing to one best reference do not always correlate well with BLEU scores ob-tained by comparing with multiple references, which shows the need of considering multiple metrics. This paper presents a 2-step approach for sentence compression: we first generate an n-best list for each source sentence using a sequence labeling method, then rerank the n-best candidates to select the best one based on the quality of the whole candidate sen-tence using discriminative training. We evaluate the system performance using different metrics. Our re-sults show that our expanded feature set improves the performance across multiple metrics, and rerank-ing is able to improve the BLEU score. In future work, we will incorporate more syntactic informa-tion in the model to better evaluate sentence quality. We also plan to perform a human evaluation for the compressed sentences, and use sentence compres-sion in summarization. This work is partly supported by DARPA un-der Contract No. HR0011-12-C-0016 and NSF No. 0845484. Any opinions expressed in this ma-terial are those of the authors and do not necessarily reflect the views of DARPA or NSF.

