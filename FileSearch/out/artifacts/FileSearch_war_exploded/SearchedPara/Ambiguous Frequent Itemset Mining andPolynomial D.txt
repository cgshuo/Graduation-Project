 The frequent pattern mining problem is to find patterns frequently appearing in a given database. It is one of the central tasks in data mining, and has been a focus of recent informatics studies. Par ticularly, when the database D is a collection of transactions 2 where a transaction is a subset of an itemset I = { 1 ,...n } ,and the patterns to be found are also subsets of itemsets, the problem is called the frequent itemset mining problem [1,4,11,12].

Precisely, a transaction of D including P is called an occurrence of P ,and the set of occurrences of P is denoted by Occ ( P ). we define the frequency of an itemset by | Occ ( P ) | ,andsayanitemsetisa frequent itemset if its frequency is no less than the given threshold value  X  , called minimum support . The frequency is often called support ,and  X  is called the minimum support .

Frequent pattern mining is often used for data analysis. For data so huge that humans can not get any intuition from an overview of it, the frequent pattern mining is a useful way to capture the features of the data X  X  features, both in a global sense and in a local sense. However, we often encounter difficulties in trying to use the frequent pattern mining on real-world data. One difficulty is that data is often incorrect or has missing parts. Such errors mean that some records that should include a pattern P do not include it, thus P may be overlooked because its frequency appears to be too low. A way to deal with this difficulty is to consider an ambiguous inclusion relation whereby we consider that a transaction T includes a pattern P if most items of P are included in T .

There are several studies on the frequent pattern mining with ambiguous inclusions. In some contexts, these patterns are called fault-tolerant frequent itemsets[5,7,8,9,16]. In some of these studies[16], ambiguous inclusion is defined such that an itemset P is included in a transaction T if | P  X  T | / | P | X   X  .Given this definition, the family of frequent itemsets is not always anti-monotone, thus the usual apriori based algorithms are not output sensitive in the sense of time complexity. On the other hand, the authors introduced an inclusion allowing a constant number of missing items, i.e., | P \ T | X   X  . This does not violate the monotonicity, thereby admits both apriori and backtrack with many related techniques developed for frequent itemset mining. However, it has a disadvantage that a transaction can miss only few item s of large itemsets whereas almost all small itemsets will be frequent.

In some studies[5,7,8,9], they considered that it is a fault if an item of the itemset is not included in a transaction, and treat mining pairs of an itemset and a transaction set such that there are fe w faults between their elements. Their algorithms find pairs with few faults, but they are not always minimum solutions.
In this paper, we address the problem from the algorithmic point of view, and model the problem in a different way. In the other words, the goal of this paper is to investigate the most simple and useful model of ambiguous frequency which allows fast computation. In the existing practice-based approach, the designed model often allows no fast algorithm. Heuristic approaches lose the complete-ness and exactness of the algorithm. For developing fast algorithms, we consider another model for ambiguous frequency.

For an itemset P and a transaction T ,the inclusion ratio is the ratio of items of P included in T , i.e., | T  X  P | / | P | . For an itemset P and a transaction set T ,the average inclusion ratio of T for P is defined by the average of the inclu-the inclusion between transactions and items by matrix, the average inclusion ratio corresponds to the density of the submatrix induced by the items and transactions. When the average inclusion ratio is high, the items co-occur in the transactions, or the transactions co-occur in the items. For a density threshold  X  , a transaction set of the largest size having average inclusion ratio no less than  X  is called the maximum co-occurrence set for P . Note that any maximum co-occurrence set can be obtained by choosing transactions in decreasing order of their inclusion ratio. The size of the maximum co-occurrence set is called the maximum co-occurrence size of P , and is denoted by cov ( P ). We denote the lexicographical minimum maximum co-occurrence set by AmbiOcc ( P ). Some examples are shown in Figure 1.

For the minimum support threshold  X  , an itemset is called an ambiguous fre-quent itemset if its maximum co-occurrence size is no less than  X  . The problem in this paper is formulated as follows.
 Ambiguous Frequent Itemset Enumeration Problem Input : transaction database D , minimum support  X  , density threshold  X  Output : all ambiguous frequent itemsets in D
We propose a polynomial delay polynomial space algorithm, and show the practical performance by co mputational experiments. Note that an algorithm is polynomial delay if the computation time between any two consecutive solutions is polynomial in the input size.

If we represent the inclusion relation by a bipartite graph, an ambiguous fre-quent itemset and its corresponding tran saction set corresp onds to a vertex set inducing a dense bipartite subgraph, which is a quasi bipartite clique. Enumer-ating dense subgraphs whose edge density is no less than a threshold value can be done in polynomial delay polynomial space[14]. However, since an ambiguous frequent itemset has a lower bound for transaction sets, and identifies the same itemsets with different transaction sets, a direct application of the algorithm to our problem is not polynomial delay.

The existence of a polynomial delay algorithm for the enumeration problem of ambiguous frequent itemset is not trivial, since as we will show, simple al-gorithms involve an NP-complete problem in each iteration. The framework of the algorithm in this paper is motivated from the enumeration algorithm for pseudo cliques[14]. We introduce an adjacency relation with respect to a re-moval of an item between ambiguous frequ ent itemsets, and implicitly construct a tree-shaped traversal route on the relation. Our algorithm searches traverses the tree in a depth-first search manner, so that the computation time is polyno-mial delay. To best of our knowledge, this is the first result of even an output polynomial time algorithm for this problem. The ambiguous frequency and our algorithm can be naturally extended to a weighted version, in a straightforward manner. The frequent itemset enumeration probl em is, from the viewpoint of complex-ity theory, an easy problem. The reason is that the frequency has a mono-tone property, thus obviously any frequent itemset can be obtained by itera-tively adding items to the emptyset by passing through only frequent itemsets. The repeated addition admits any ordering of items, hence we can efficiently avoid the duplications by adding items only in increasing order of their indices. Thus, we can construct a backtrack algorithm of a polynomial delay polynomial space. Precisely, the computation time f or each frequent itemset is linear in the size of the database, i.e., O ( ||D|| ), where ||D|| is the size of database D , i.e., ||D|| = |D| + T  X  X  | T | . The space complexity is optimal, that is, O ( ||D|| ).
However, the family of ambiguous frequent itemsets does not have this mono-tone property. For the database D in Figure 1,  X  = 65% and  X  =4,wecan { 1 , 2 , 3 } is an ambiguous frequent itemset. H owever, the maximum co-occurrence is not an ambiguous frequent itemset. Since the monotonicity is not held, a straightforward backtrack algorithm is not applicable to the enumeration.
We approach the problem in another way. For itemset P =  X  , we define e  X  ( P ) by the item e  X  P that minimizes | AmbiOcc ( P )  X  Occ ( { e } ) | .Tiesarebrokenby choosing the minimum index one. Using e  X  , we introduce an adjacency relation among ambiguous frequent itemsets, and construct an implicit traversal route. Lemma 1. For any itemset P =  X  , there exists an item e  X  P satisfying cov ( P \ { e } )  X  cov ( P ) .
 Proof. First we observe that the average inclusion ratio of AmbiOcc ( P )for P the observation, the average inclusion ratio of AmbiOcc ( P )for P \{ e  X  ( P ) } is it is no less than the average inclusion ratio of AmbiOcc ( P )for P .Itmeansthat the item e in the statement.
 For an itemset P =  X  , we define the parent Prt ( P )of P by P \{ e  X  ( P ) } . From Lemma 1, P \{ e  X  ( P ) } is an ambiguous frequent itemset. Particularly, thus the parent child relation induced by Prt is acyclic. Since every ambiguous frequent itemset other than the empty set has a parent, the relation induces a rooted tree spanning all ambiguous frequent itemsets. We call this tree the enu-meration tree of ambiguous frequent itemsets. An example of the enumeration tree is shown in Figure 2. By traversing the tree, we can find all ambiguous frequent itemsets without duplication.

To perform a depth-first search on the enumeration tree, we need to find all children of the current visiting ambiguous frequent itemset. By recursively find-ing the children, we can perform a depth-first search without using additional memory on visited vertices. This ensures the polynomiality of the memory com-plexity. The algorithm can be written as follows.
 ReverseSearch ( P ) 1. Output P 2. for each e  X  P 2-1. if P  X  X  e } is an ambiguous frequent itemset then 2-2. if Prt ( P  X  X  e } )= P then 2-3. call ReverseSearch ( P  X  X  e } )
The computation of the average inclusion ratio and the parent of P  X  X  e } in 2-1 and 2-2 can be done in O ( ||D|| ) time. They are executed at most n times in an iteration, thus the computation in an iteration except for those in the recursive calls is bounded by O ( ||D|| X  n ). This algorithm outputs an ambiguous frequent itemset in each iteration, thus the computation time per ambiguous frequent itemset is O ( ||D|| X  n ). The depth of the enumeration tree is bounded by n , thus we obtain the following theorem.
 Theorem 1. For given a transaction database D , minimum support threshold  X  , and density threshold  X  , ambiguous frequent itemsets in D can be enumerated in polynomial delay with polynomial space in terms of ||D|| . In particular, the computation time for one ambiguous frequent itemset is O ( n ||D|| ) where n is the number of items included in some transactions in D . For practical huge databases, the computation time O ( ||D|| X  n ) is quite long. It is not easy to reduce the time complexity, but possible to improve the practical efficiency by using at typic al structures of actual datasets. The heavy tasks in each iteration with respect to itemset P is the computation of cov ( P  X  X  e } )and ( P  X  X  e } )foreach e . Both need O ( n ||D| ) time, and we will describe techniques to reduce the computation time for each. Note that the computation of cov ( P ) is maybe heavier since we have to compute e  X  ( P  X  X  e } ) only when P  X  X  e } is an ambiguous frequent itemset.

We define Occ = h ( P ) by the set of transactions not including exactly h items in P , i.e., Occ = h ( P )= { T | T  X  X  , | P \ T | = h } . Similarly, Occ  X  h ( P )= { T | T  X  X  , | P \ T | X  h } . For the computation of cov ( P  X  X  e } ), we have to obtain Occ = h ( P  X  X  e } )foreach e and h , in increasing order of h .Wecanuse the following property and lemma for efficient computation.
 Property 1. [13] For a transaction T included in Occ = h ( P )forsome h, 0  X  h  X  k , T  X  Occ { i } )=( Occ From these, we can see that Occ = h ( P  X  X  e } ) for all h are obtained by moving time, which is expected to be small when the input database is sparse. To com-pute Occ = h ( P )  X  Occ ( { e } ), a method called delivery is efficient[11,12,13]. We briefly explain the framework of Delivery. An example is shown in Fig. 3. First, we prepare an empty bucket for each item e . Next, for each transaction T in Occ = h ( P ), we  X  X nsert T into the bucket of e for each item e  X  T  X . After performing this operation for all transactions in Occ = h ( P ), the content of the bucket of e is equal to Occ = h ( P  X  X  e } ). The pseudo code of occurrence deliver is described as follows. The code inputs a transaction set S ,thensets bucket [ e ]to S X  Occ ( { e } ) for all e . We suppose that the bucket of any item e is initialized, and thus is empty at the beginning.
 Delivery ( S ) 1. for each T  X  X  do 2. for each i  X  T ,insert T into bucket [ i ] Lemma 3. [12,11] Delivery computes S X  Occ ( { e } ) for all e in O ( ||S|| ) time. Let k  X  ( P ) be the smallest h satisfying AmbiOcc ( P )  X  Occ  X  h ( P ). Lemma 4. If P  X  X  e } is a child of P , k  X  ( P  X  X  e } )  X  k  X  ( P )+1 holds and (
P  X  X  e } ). This means that Occ  X  k  X  for P  X  X  e } .If P  X  X  e } is a child of P ,wehave cov ( P  X  X  e } )  X  cov ( P )  X  P  X  X  e } ,and k  X  ( P  X  X  e } )  X  k  X  ( P )+1.

This lemma implies that for the computation of cov ( P  X  X  e } ), we have to look sizes of transactions in Occ  X  k  X  ( P )+1 ( P ).

We next state a lemma to determine k  X  ( P  X  X  e } ) efficiently. Let Th ( P, k )=  X   X  ( | P | +1)  X | Occ  X  k ( P ) | X  T  X  Occ no less than  X  . For any transactions T  X  Occ = h ( P )and T  X  Occ = h +1 ( P ), the inclusion ratio of T for P  X  X  e } is always no less than that of T for P  X  X  e } . Thus, we have the following property.
 (
P Note that the statement holds for a unique k since Th ( P, k ) is monotone de-creasing for the increase of k . From the above lemma, we compute | Occ ( { e } )  X  the condition of Property 5 with k , and check whether P  X  X  e } is a child of P or not by computing AmbiOcc ( P  X  X  e } ). The algorithm based on this method is as follows.
 ALGORITHM FindAllChildren ( P ) 1. compute Th ( P, k )foreach k =0 , ..., k  X  ( P )+1 2. for k =0 to k  X  ( P ) 3. compute Occ = k ( P )  X  Occ ( { e } )foreach e 5. if | Occ ( { e } )  X  Occ  X  k ( P ) | &lt;Th ( P, k ) then 6. if  X   X | AmbiOcc ( P  X  X  e } ) | X | AmbiOcc ( P ) | then 7. if Prt ( P  X  X  e } )= P then P  X  X  e } is a child 8. end for 9. end for Step 6 computes AmbiOcc ( P  X  X  e } ), then obtain Prt ( P  X  X  e } ) by computing thus we have to compute only Occ = k  X  ( P  X  X  e } ) ( P )  X  Occ ( { e } ). Now the computation time in each iteration with respect to P is (a) O ( O ( || Occ itemset. In practical dat asets, it is expected that P  X  X  e } is an ambiguous fre-quent itemset only for few e  X  X . Otherwise the number of ambiguous frequent itemsets is huge so that we can not enumerate them in a practically short time, and we can not deal with huge output itemsets. Therefore, we can expect that (b) is not larger so much than (a), thus the computation time for an iteration is O ( In practical transaction databases, items of each transaction often has several different weights. For example, POS data includes the number or the price of each item purchased by a customer. In experiments in industry or natural science, each cell or item may have a kind of intensity. Such a database can be regarded as a matrix of item columns and transaction rows such that each cell has a value. One may be naturally motivated to find submatrices with a large average weight of cells. These locally heavy submatrices correspond to important objects such as clusters, and have applications in knowledge discovery and data engineering.
We define the problem as follows. We suppose that each item e of a transac-tion T has a weight w ( T,e ). For an itemset P and a transaction T , we define the average weight w ( T,P )of T with respect to P by ( e  X  P  X  T w ( T,e )) / | P | . For a set T of weighted transactions, we define the average weight w ( T ,P )by ( T  X  X  w ( T,P )) / |T| . When we are given a weight threshold  X  , we define the weighted maximum co-occurrence size of P by the maximum size of a transac-tion set having average weight no less than  X  . For a given support threshold  X  , an itemset is called a weighted ambiguous frequent itemset if its weighted maxi-mum co-occurrence size is no less than  X  . The weighted version of the ambiguous frequent itemset enumeration problem is to output all weighted ambiguous fre-quent itemsets. Given these definitions, we obtain a similar neighboring relation between weighted ambiguous frequent itemsets.
 Theorem 2. For given a weighted transaction database D , weight threshold  X  , and minimum support threshold  X  , all weighted ambiguous frequent itemsets can be enumerated in polynomial delay and polynomial space. In particular, the com-putation time is O ( ||D|| n ) for each, and the space complexity is linear in the input size, where n is the number of items in D .
 The method described in the above sections is not directly applicable to improve the practical efficiency of the weighted version of our algorithm. The reason is that Properties 1 and 2 are not valid for the weighted version. To compute the weighted maximum co-occurrence size of P  X  X  e } , we need to get the transactions T w (
T, { e } ) has to have a large value. Thus, by getting transactions having large average weights with respect to either P or { e } , we can efficiently compute the weighted maximum co-occurrence size. We show that a hardness result for simple approaches to answer the question that why we need a sophisticated enumer ation scheme. In a typical branch-and-bound algorithm, we may choose an item e and divide the enumeration problem into two subproblems; the enumeration problem of ambiguous frequent itemsets including e , and the problem for itemsets not including e . The division of the problem is done recursively until the problem includes a unique solution (ambiguous frequent itemset). In this approach we have to know the existence of solutions to the restricted problem, otherwise we will divide problems having no solution recursively, ther eby exponentially many times.

The following theorem states that this p roblem is NP-complete. Therefore, we observe that it is hard to get a polynomial delay algorithm by typical branch-and-bound since we have to solve an NP-complete problem in each iteration. Theorem 3. For given a transaction database D ,itemset S , density threshold  X  , and minimum support threshold  X  , the problem of answering whether an am-biguous frequent itemset including S exists or not, is NP-complete.
 Proof. Suppose that we are given a transaction database D , a minimum support threshold  X  , and a constant number k , and going to check for the existence of an itemset of size at least k that is included in at least  X  transactions. This is known to be NP-complete[17]. Let I be the set of items included in transactions in
D ,and I be a set of items of size |D| X | I | satisfying I  X  I =  X  .Wechoose an item e  X  from I .
 We now construct a transaction database D = { T  X  ( I \{ e  X  } ) | T  X  D } .Let X be a subset of I , T be a transaction set of D ,and T be the transaction set of
D corresponding to T . Then, X is a frequent itemset of D and T = Occ ( X ) if and only if the average inclusion ratio of T for X  X  I is strictly larger than ( Then, X is a frequent itemset of D of size at least k if and only if X  X  I is an ambiguous frequent itemset of D . Therefore we have the theorem. In general, the practical computation time of an algorithm often differs from the theoretical upper bound. The reason is that the computation time is dominated by the  X  X verage X , but the theoretical upper bound looks only at the worst case. To see the gap and to be a help for the practical use, we show the results of some computationa l experiments.

The C code is used for the implementati on. The computer used in the exper-iments was a notebook PC with a Pentium M 1.1GHz processor with 768MB memory. The experiments were done on cygwin which is an emulator of Linux environments on Windows. The implementation is a simpler version of our al-gorithm, which compute the parent in a straightforward way. The reason is to choose a simpler version is to see the performance of a simple implementation, which would help for coding. The implementation is available at the author X  X  homepage; http://research.nii.ac.jp/  X uno/index.html.

We examined two practical datasets ta ken from FIMI repository[6]. The first is BMS-WebView2 with about 3,300 items and 77,000 transactions. The aver-age size of transactions is 4.6, thus the dataset is quite sparse. The second is Mushroom with about 120 items and 8,000 transactions. The average size of transactions is 23, thus the dataset is not sparse.

We run the implementations with the thresholds  X  =0 . 8 , 0 . 9and1 . 0. Since we could not find any implementation for the ambiguous frequent itemset enu-meration, we have no comparison to other implementations. Instead of this, we compare the performance to that of an ord inary frequent itemset enumeration algorithm LCM[12,11]. Since the frequen t itemset enumeration is a special case of our problem, it can be considered as a kind of upper bound of the performance of the ambiguous frequent itemset enumeration.

The results are shown in Fig. 4. The left is BMS-WebView2, and the right is Mushroom. The horizontal axis is for minimum support threshold, and the vertical axis is for computation time, computation time for 1 million (ambiguous) frequent itemsets, and the number of o utput itemsets, written in log scales.
The computation time of our algorithm increases as the decrease of minimum support, but the computation time per one million itemsets does not change drastically. It seems to change as the change of average size of Occ ( { e } ). Com-paring to the ordinary frequent itemset mining algorithm, the performance of our algorithm is not so good. One of the reason is that the cost for computing parents of the candidate children. A simple duplication check by storing the discovered itemsets in memory will accelerate the computation when the output itemsets are few. The other reason is that in the ordinary frequent itemset mining, we can use the conditional database for the current operating itemset, which includes only items larger than the maximum item in the current operating itemset and are frequent in the database induced by the occurrence of the current operat-ing itemset. Usually the number of items in the conditional database is much smaller than the number of items in the original database, thus the computation is faster. To reduce the difference on the computation time, further techniques for the efficient computatio n are still needed. The numb er of ambiguous frequent itemsets increases drastically by the decr ease of density threshold. In practice, we should use a threshold slightly smaller than 1.0.

We also looked at several statistics on the experiments in Figure 5.  X  X ax X  means the ratio of ambiguous frequent itemsets and the number of maximal ambiguous frequent itemsets to which no item addition yields an ambiguous fre-quent itemset.  X  X rt X  shows the ratio of the number of accessed items between a straightforward algorithm and the sophisticated algorithm proposed in this paper, and  X  X cc X  indicates that between delivery for computing the frequencies of all additions of items, and delivery for computing the parent. As we can see, these ratio increase as the increase the number of solutions. Thus, we can expect the decrease of the number of solutions by outputting only maximal ones. The speedup is also expected by introducing our sophisticated parent computation, but the effect will be limited. The big ratio of  X  X cc X  implies that the big gap be-tween computation time of our algorithm and ordinary frequent itemset mining. It also implies that more pract ical improvements are needed. We formulated the enumeration problem of ambiguous frequent itemsets, and proposed a polynomial delay polynomial space algorithm. The algorithm is natu-rally extended to a weighted version. The e xperimental performance for practical datasets is accep table, but improvements on prac tical performance is a crucial future work. Another interesting research topic is extending the technique to other frequent pattern mining problems. This research was supported by Grant-in-Aid for Scientific Research of Japan,  X  X eveloping efficient and accurate algorithms for large-scale data processing in genome science X , and a joint-research fund of National Institute of Informatics.
