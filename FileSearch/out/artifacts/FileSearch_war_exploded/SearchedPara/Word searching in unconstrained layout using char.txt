 ORIGINAL PAPER Partha Pratim Roy  X  Umapada Pal  X  Josep Llad X s Abstract Word searching in non-structural layout such as graphical documents is a difficult task due to arbitrary ori-entations of text words and the presence of graphical sym-bols. This paper presents an efficient approach for word searching in documents of non-structural layout using an effi-cient indexing and retrieval approach. The proposed index-ing scheme stores spatial information of text characters of a document using a character spatial feature table (CSFT). The spatial feature of text component is derived from the neighbor component information. The character labeling of a multi-scaled and multi-oriented component is performed using support vector machines. For searching purpose, the positional information of characters is obtained from the query string by splitting it into possible combinations of char-acter pairs. Each of these character pairs searches the position of corresponding text in document with the help of CSFT. Next, the searched text components are joined and formed into sequence by spatial information matching. String match-ing algorithm is performed to match the query word with the character pair sequence in documents. The experimen-tal results are presented on two different datasets of graph-ical documents: maps dataset and seal/logo image dataset. The results show that the method is efficient to search query word from unconstrained document layouts of arbitrary ori-entation.
 Keywords Graphical document analysis  X  Graphics recognition  X  Information retrieval  X  Word spotting  X  Multi-Oriented text recognition 1 Introduction With the rapid growth of electronic media and internet the conversion of documents to the electronic domain is increas-ing. Not only the companies are using image processing tech-niques for optical character recognition (OCR), routing of documents, etc., but also digital libraries and archives are going for mass digitization and transcription to widen the access through web services. To manage the documents in electronic form, many document image applications are com-ing up to facilitate indexing, viewing, extracting the intended portions, etc. OCR systems [ 1 ] manage efficiently the tran-scription of documents with Manhattan layout. An OCR analyses each text line in scanned document images, seg-ments words and characters and provides a translation to ASCII/Unicode text, which can then be stored and manipu-lated electronically like any standard electronic document.
Similar to text search engines that retrieve text pages con-taining the query keywords, in this paper, we propose a sys-tem that retrieves graphical document text images of arbitrary orientation according to textual user queries. To the best of our knowledge, there is no reported work on multi-oriented string retrieval and this is the first work of its kind. The main novelty of this paper is to present a word searching archi-tecture in graphical documents (such as maps, engineering drawing, etc.) to facilitate searching of query word that can take care of multi-orientation as well as overlapping/touching strings. Also, we propose an efficient strategy for indexing and retrieval of large volume of graphical documents which is another contribution of the paper. The system is flexible enough to search curvilinear words in dataset of graphical documents. This work is motivated by our preliminary work reported in [ 2 ]. Moreover, the current work is an improved version with new matching technique, improved accuracy, additional dataset, etc., and it can take care of touching char-acters in word searching. Exhaustive experiments on differ-ent datasets have been performed to show the effectiveness of the proposed method.

Besides huge amount of documents with structured lay-out containing text information, many documents exist with graphics-rich information. Some of these documents are maps, engineering drawings, diagrams, musical scores, etc. [ 3 ]. In such graphics-rich documents, the text is annotated to characterize the graphical symbol/object and text appears alongwithgraphicalobjectssuchasborderlines,longlinesin diagrams, etc. Also administrative documents contain graph-ical entities such as logos, seals, etc. where text information is present in nonlinear orientation [ 4 ].

Word searching in such graphical documents is a challeng-ing problem. The commercial OCRs can only handle such graphics-rich documents partially due to the lack of struc-tural information of text characters. Because, the text lines often appear in curvilinear orientations other than the usual horizontal directions and the inter-character spacing differs according to annotation style in such non-structural docu-ments. The problem for detection and recognition of such text strings increases due to the overlapping of graphical objects (long lines) with text strings. Simple OCR methods or render-ing of a textual query to an image for a spotting purpose will notbeapplicableinsuchdocuments.Thisisbecause,agraph-ical document may contain characters of different fonts and the inter-character spacing within a word cannot be known a priori. Also, due to curvilinearity of the characters in a word, generation of such synthetic words may not be possible. To illustrate the problems, we show a portion of map in Fig. 1 a. It can be seen from Fig. 1 a, the string  X  X ahabharat Lekh X  is oriented in different directions in its two different instances on this image. In another string  X  X NDIA X , it can be seen that the inter-character spacing is much larger compared to that in other words. Figure 1 b demonstrates curved text linesinsealandlogoimages.Textinformationinsuchimages is generally not used for retrieval.

The fact of working with indexing words in non-structured documents raises several problems to tackle. One of the main reasons could be the character identification in such docu-ments for text word retrieval. Another important issue is the choice of the methodology allowing us to efficiently retrieve documents based on query word or symbol. Though there exist pieces of work that tried to solve different parts of this problem, there is no complete work which can index and search graphical documents from dataset. One of the pioneering work on text string extraction in graphical doc-uments was proposed by Fletcher and Kasturi [ 5 ]. Some of the existing work in the literature [ 6 , 7 ] consider the tradi-tional pipeline approach of character segmentation, grouping characters into words, and integrate the OCR later. Most of these approaches consider text lines to be straight in such documents and rarely search curvilinear words due to its complexity. Also, the indexing of large volume of graph-ical documents is not performed in the literature. To the best of our knowledge, there exist no other work which proposes indexing of graphical documents for query-based retrieval.

The proposed scheme of word searching is performed in two steps: (a) Indexing of character information and (b) Retrieval of words by character pair linking using spatial information.InFig. 2 ,theblockdiagramofthequeryretrieval process is shown. Indexing of character information in docu-ments is done offline. For this purpose, the connected compo-nents of a document are labeled as alpha-numeric text charac-ters. We use scale and orientation free features for text char-acter representation and a Support Vector Machine (SVM)-based classifier has been used for recognition [ 8 ]. For each component (text character), we find its pair component and the locations of all character pairs are stored in a Character Spatial Feature Table (CSFT).

Once the documents are indexed, the word searching or retrieval of query word location detection is performed online. Given a query word, the character code information is used to locate the hypothesis. The structural information of characters is obtained by splitting the query string into a chain of character pairs. Each of these character pairs searches the position of corresponding text in document with the help of the CSFT. Next, the text components are joined in a sequence using spatial information matching to find the corresponding result of the query word. Approximate string matching algo-rithm is used to match the query word with the character pair sequence in documents.

The rest of the paper is organized as follows. We discuss the related work of text information extraction in graphical documents in Sect. 2 . In Sect. 3 , the indexing scheme of spatial representation of text information is explained. The word retrieval process is detailed in Sect. 4 . In Sect. 5 ,we present the data details and experimental results. Finally, the paper is concluded in Sect. 6 . 2 Related work Word spotting is a content-based retrieval approach focused on ranking a list of word images from the database that are similar to a query word image. In this section, we review some of the existing works on word retrieval in structured as well as non-structured document images. 2.1 Word retrieval in structured documents There are mainly two categories of research work in this domain: character shape coding (analytical) and holistic word spotting. Many pieces of work have been published on character shape coding that annotates character images by a set of predefined codes. Nakayama [ 9 ] annotated character images by seven shape codes for word content detection. Lu et al. [ 10 ] proposed a technique to retrieve document images by a set of topological codes based on shape features includ-ing character ascenders/descenders, holes, water reservoirs information, etc. Often, word shape coding is used to encode the words in normal printed documents.

Holistic word spotting approaches [ 11 , 12 ] are used in degraded documents to treat the word image as a whole entity and thus avoids the difficulty of character segmentation. These approaches are widely employed in keyword spotting application. To find word similarity, Dynamic Time Warp-ing (DTW) [ 12 ] has been useful for computing a sequence alignment score which are different from the character-level processing strategy taken by OCR, and thus robust to degraded image quality. Konidaris et al. [ 13 ] proposed a combination of synthetic data and user feedback for searching word images. Gatos and Pratikakis [ 14 ] proposed a segmentation-free word spotting approach for historical printed documents using salient region detection by template matching at an initial stage. 2.2 Text detection and recognition in non-structured layout The word spotting approaches mentioned above are designed mainly for documents where words are horizontally aligned. Text character detection in unstructured layout is very diffi-cult as there is less rule in placement and annotation. In such documents, holistic word-spotting based approaches may not be applicable, because characters may be present in different fonts in a single document and their inter-character spacings are not known a priori. Due to curvilinearity of the characters in a word, generation of synthetic words is also not possible. Using symbolic encoding of text characters and then using approximate string matching of such character group can be a good strategy to view ranked queries in unstructured doc-uments [ 15 ].

In some unstructured layouts, e.g., graphics-rich docu-ments,thepresenceofgraphicalobjectsandtheiroverlapping with text strings make text information retrieval work more difficult. Text characters often touch/overlap with graphical lines and thus interpretation of such documents requires mul-tiple tasks such as handling of curvilinear character strings, taking care of text/graphics separation and recognition of multi-oriented characters. Due to the complexity, different techniques are generally proposed for different problems. These techniques concern segmentation of characters, recog-nition of these isolated characters, touching character sepa-ration, and grouping characters into words or lines [ 7 ]. We present below some existing works that tackle with such problems.
 Text/graphics separation In graphics-rich documents, Fletcher and Kasturi [ 5 ] used simple heuristics such as height and orientation of text characters for text separation. A num-ber of improvements were made later by Tombre et al. [ 16 ] to make it more stable for graphics-rich documents. One of the key improvements of Tombre et al. is that they also considered text touching graphics. Directional mathemati-cal morphology-based approach has been used by Luo et al. [ 17 ] for separation of character strings in maps. The idea was to separate large linear segments by directional mor-phology and histogram analysis of these segments. Large segments are considered as part of graphics; effectively leav-ing small text character segments. Tan and Ng [ 18 ] illustrates a system using Pyramid structure. Multi-resolution represen-tations using pyramid structure helps to identify and locate words or phrases in graphics-rich image efficiently. Cao and Tan [ 19 ] proposed a method of detecting and extracting text charactersthataretouchedtographics.Itisbasedontheinter-pretation of intersection of lines in the overlapped region on the vectorized image of text and graphics.
 Text line segmentation There are several methods to segment character strings in un-structured layouts. Goto and Aso [ 20 ] proposedalocallinearity-basedmethodcalledExtendedLin-ear Segment Linking ( ELSL ). Here, a text line is treated as a set of short linear segments that are tolerant to the local skew of the text line and can handle documents with a com-plex layout. Loo and Tan [ 21 ] used an irregular pyramid-based method for text line segmentation. This algorithm used the background information, density of a word region, the directional uniformity and continuity between words in a sentence. In another method, Deseilligny et al. [ 6 ]pro-posed an algorithm based on dynamic programming which enables construction of optimal strings using constraints on orientation homogeneity. A systematic connected-character search is employed at each extremity of detected strings. This searching is done to find a character along the same orienta-tion. Similarly, Water reservoir-based background informa-tion has also been used for curve text line segmentation in graphical documents [ 22 ]. Gatos et al. [ 23 ] proposed an algo-rithm based on text line and word detection for warped doc-uments. Horizontal smoothening is done to combine charac-ters into words. Next, based on word rotation and translation according to upper and lower word baselines, document is de-warped. Bai et al. [ 24 ] used a traditional perceptual grouping-based algorithm for extracting curved line from logos and slogans. Chains of connected components are extended to the left and the right according to the local orientations of the text lines.
 Multi-oriented character recognition Text characters appear in arbitrary orientations in graphical documents. For the purpose of such multi-oriented character recognition, Adam et al. [ 7 ] used a new set of features using Fourier Mer-lin transform for classification of multi-oriented and multi-scaled patterns. This method was applied to the documents of the French telephonic operator. Parametric eigen-space-based method is used by Hase et al. [ 25 ] for rotated and/or inclined character recognition. Yang and Wang [ 26 ] proposed a three-stage system for multi-oriented Chinese character recognition where features are computed from each pair of pixels. The approach is mainly based on geometric measures of the foreground pixels of the characters. Hayashi and Tak-agi [ 27 ] proposed a set of features such as curvature, angle information, length and arc-length, etc. to recognize Eng-lish numerals. Monwar et al. [ 28 ] projected character images of different angles onto a feature space that best encodes the variation among known character images. Iwamura et al. [ 29 ] proposed a simple algorithm of camera-based recognition of characters and pictograms. With help of new geometric hash-ing and voting techniques, the proposed method runs in real time for rotated character recognition. Pal et al. [ 8 ] proposed an approach based on zoning and angular information for multi-oriented character recognition.

Although there are several work in the pipeline for text information extraction in understanding unstructured layout, thereexistnoworkthatfocusesonwordspotting/searchingin unstructured document such as artistic/graphical documents. Most of the existing works proposed in the literature are in constrained domain [ 6 , 7 ] and considered a fixed set of doc-ument with a single font of characters for their experiment. Detection and retrieval of curvilinear words in such complex documents are not explored. Hence, in this paper we propose an efficient framework for searching query words that does not follow the traditional pipeline approach. 3 Indexing of character components The architecture of our word retrieval system is based on the sequence of connected components (classified as text char-acters) in graphical documents. Text characters are too local and very generic to be used for matching. Due to natural spatial arrangement between text characters, we characterize the connected component information in the document based on their relative spatial arrangement. These spatial informa-tion are indexed for efficient retrieval later and different steps toward this are mentioned below. 3.1 Component pairing To represent the spatial arrangement, the components are pairwise grouped in a list. A component forms a character pair with each of the components selected from its neigh-borhood. These component pairs are selected based on their proximity and size similarity. For each component, its neigh-bor zone is decided based on the radius ( R ) of the minimum enclosing circle (MEC). It is chosen to consider different neighbor area according to the size of character component. A multiplication factor ( q ) is chosen for deciding the area of neighbor zone that is computed by R  X  q . All the components in this zone are selected as neighbors of that component. As the inter-character spacing may be large in graphical docu-ments, we have fixed the value of q to 6 in our experiment. This value assures that neighbor characters are grouped. With this value, we have checked that 99.8% components of adja-cent characters are considered as neighbors in our experi-mental dataset.

When a component is found in the neighborhood, the size similarity is tested. For this purpose, the size S i of each com-ponent C i is computed. It is calculated by finding the maxi-mum of its height and width of the minimum enclosing rec-tangle of the component. The similarity between two neigh-boring components is tested based on their sizes. A compo-nent C 1 with size S 1 will be similar in size to its neighbor component C 2 with size S 2 if Eq.( 1 ) is satisfied. S / 1 . 5  X  S
The ratio between S 1 and S 2 is set to 1.5 because it is found that characters in a string are not always of similar sizes (e.g.,  X  X  X ,  X  X  X ). To consider these characters to be grouped in a string, we have considered such flexibility. This value is set according to experiment. After the conditions of neighbor-hood and size similarity are satisfied, we build a component proximity graph G p = ( V , E ) using the component pair configuration (here V = vertex and E = edges). If a docu-ment contains n character components, then, | V | = n .An edge, e ij  X  E is formed by a component pair ( C i , C j ) the components C i and C j are neighbor. See Fig. 3 a, where neighbor components of the character  X  X  X  (marked by dashed box) are shown by joining it with its four neighbors which are  X  X  X , X  X  X , X  X  X  and  X  X  X  (according to distance). 3.2 Indexing of spatial features The spatial relationship of each character pair is encoded into a feature vector. Next the encoded feature is stored in a Character Spatial Feature Table (CSFT). The encoded spa-tial feature is indexed using the component pair X  X  labels (ascii/unicode value) which are performed by a character classification process. Let L i be the ascii/unicode label of character component C i . Character labeling is described in brief in the following subsection. 3.2.1 Character labeling As mentioned earlier, non-structured documents contain charactersinmulti-orientedandmulti-scalefashion.Welabel these characters using an existing character feature detec-tion and classification system. Although there exist different methods to recognize individual characters, we have used our method [ 8 ] of character recognition because of its better per-formance. To recognize multi-scale and multi-oriented char-acters, a zone-wise feature is used in our system. Circular ring and convex hull ring-based concept have been used to divide a character into several zones to compute features. To make the system rotation invariant, the features are mainly based on angular information of the contour pixels of the charac-ters. A set of circular rings and convex hull rings are used to have local features. Additional local features are obtained from the slope of contour pixel with respect to the center of the minimum enclosing circle of the object. Finally, a 176 dimensional feature vector is obtained for each charac-ter shape [ 8 ]. Normalization of the feature is done to obtain scale invariance.

A Support Vector Machine (SVM) classifier has been used to build the character shape model from corresponding fea-ture of our training data. Both English upper-case and lower-case alpha-numeric characters were considered for our exper-iment, so we should have 62 classes (26 for upper-case, 26 for lower-case and 10 for digit). But because of shape simi-larity due to orientation, some characters such as  X  X  X  and  X  X  X ;  X  X  X  and  X  X  X ;  X  X  X  and  X  X  X ; etc. are grouped together. Hence, we have characters of 40 classes. Given an isolated component image of a document, we compute the recognition confidence to obtain the corresponding class label and use this label of that class to find out query word. 3.2.2 Touching character segmentation Due to noise, adjacent characters in a text word may touch together and they form touching characters. When touching occurs in multi-oriented character strings, it is very difficult to segment such multi-oriented touching characters. This is due to the fact that it is hard to know the angle of alignment of characters in the touching. Recently, we proposed a touch-ing character segmentation approach Roy et al. [ 30 ] that can handle touching strings in multiple orientations.

When two or more characters touch, they generate a big cavity region at the background portion. The background information in these cavity regions are used to find the seg-mentation points. To handle the background information, the convex hull is used. A set of initial segmentation points is predicted first based on the concave residues of the convex hull of a touching string. Next, based on the initial segmen-tation points, the touching string is segmented into primitive segments. A primitive segment consists of a single charac-ter or a sub-image of a single character. Next these primi-tive segments are merged to get optimum segmentation and dynamic programming is applied using total likelihood of characters as the objective function. Classification accuracy of each character is used to find the likelihood of a character.
In Fig. 4 , we show a portion of the text layer of a map and a logo containing isolated characters and their recog-nition label. Here, the text characters are separated using a text/graphics segmentation algorithm [ 16 ]. It is to be noticed that the character  X  X  X  is identified as  X  X  X  because of its shape similarity nature in rotation invariant environment. Some characters could not be labeled because they are touched with long lines and segmentation was not done from these lines. 3.2.3 Indexing The character labels of each component pair are used to obtain the index key ( ). A 2-dimensional lookup Table (LUT) is used to find ij from each pair of character compo-nents X  Ascii/unicode values ( L i , L j ) as shown by equation Eq. 2 .Here ij is the index key to find the spatial information of character pair L i , L j .
The size of the LUT depends on the total number of recog-nition labels used in the experiment. CSFT contains the spa-tial information of each character pair. The spatial informa-tion is encoded by their distance, angle and location of the character component pairs. We compute the CG (center of gravity) of each component for this purpose. These attributes are described as follows. 1. Size: The size S ij is estimated as the average of size of 2. Angle: The angle (  X  ij ) joining the CGs of component 3. Distance: The distance d ij (see,  X  d ij  X  X nFig. 5 ) of com-4. Position: The locations of component pair are also Webuildthefeaturevector F ij = ( S ij , X  ij , d ij , P i , considering size, angle, distance and location for each com-ponent pair C i and C j . D is the document id-number. The encoded spatial feature F ij is next stored in CSFT using the index key ij .InFig. 6 , we show the representa-tion of the CSFT and the indexing mechanism. Here, spa-tial feature of character pair  X  X  X , X  X  X  is stored in CSFT by
It is to be noted that in CSFT, for each index key ij there may be multiple entries of spatial feature for the presence of many similar character pairs (having similar codes L i , L components) in documents. The collision in CSFT is solved by a chaining method. Thus, the features of the same char-acter pair are linked one after another. 4 Query word processing and retrieval To retrieve a query word given by the user, we take into account the characters and their relative positions between them from the query word itself. The query characters are used in our system to search their possible location in the document. Finally, a string matching is used to locate differ-ent instances of the query word using their relative position information. This process is described below in two steps: (1) relative position of character pair from query word and (2) word matching and retrieval. 4.1 Relative position of character pair From the query word, the system extracts the knowledge about character information. As mentioned earlier, the text character in unstructured documents are multi-oriented. To search a query word in these documents, the ascii/unicode values of query characters are converted to the set of multi-oriented character labels used in our system with a mapping table. Thus, the ascii/unicode character space A is reduced to coded multi-oriented character space M = ( A ) . M repre-sents equivalence classes of rotationally similar characters. |
A | ( = 62 ) is the cardinality of alpha-numeric text charac-ters, and | M | ( = 40 ) is the total number of unique multi-oriented character labels (as mentioned in the 2nd Paragraph of Sect. 3.2.1 ). The conversion of A to M is done by a lookup table using : A  X  M . The reason to reduce the set of classes is that graphical documents may contain characters in a string of multi-oriented fashion. Some characters such as,  X  X  X ,  X  X  X  may appear as  X  X  X ,  X  X  X  in multi-oriented document. To take care of this problem, in our algorithm, the characters that look similar in multi-orientation are grouped together and the characters from a group are considered of same class in multi-oriented character space. Thus, a query word, e.g.,  X  X hone X  will be changed to  X  X houe X  using multi-oriented character set as the similar shaped characters  X  X  X  and  X  X  X  are grouped together and labeled as  X  X  X .

Next, we look for different character pairs in the query word. Let the user query Q u be of character sequence q q u 2 ... q u i ... q u l , where l is the length of the query string. By , each character q u i is converted to a multi-oriented char-acter code q m i and they form Q m = ( q m 1 q m 2 ... q m q ( QP ) is obtained using the relative positions of the charac-ters. The relative position of a character pair is defined by the positional difference of characters in the sequence. The list QP is built with character pairs ( q m i , q m i + r choices of relative position ( r ), where i + r  X  l . For the query  X  X houe X , the list QP will be formed with pairs (R, h), (h, o), (o, u), (u, e) for r = 1 and (R, o), (h, u), (o, e) for r
The reason of including bi-gram characters of positional difference r greater than 1 is that, sometimes due to touch-ing/overlapping characters, some text characters of a query word may be missing in a document. To cope up such missing character problems, the component sequence that would be obtained in the document is different. For example, in Fig. 4 a the character  X  X  X  in word  X  X hone X  is missing due to touching with graphical lines. Thus, the pair (R, h) and (h, o) are lost in this word portion. Considering r = 2, character pair (R, o) will help to search the word-part with the possibility of missing portion. 4.2 Word matching and retrieval Each character pair of list QP is used to generate the index key for CSFT and to retrieve the corresponding components X  spatial feature. The retrieved components will be joined sequentially to form a target string T m . This sequential join-ing is performed using the character pair stitching approach explained in the following sections. 4.2.1 Character pair stitching for retrieval Using the character sequence of a query ( q m 1 q m 2 ... objective is to match it with a similar sequence of character components from the document. This matching can be writ-ten by Eq.( 3 ), where { C q m a character label q m i . denotes the function to obtain the sequence of components from the string Q m . ( q m
Using the knowledge of a query character pair list QP and a component pair list CSFT , the above Eq. 3 can be modified according to successive sequence of pairs.
For each query character pair ( q m i  X  1 q m i ), the CSFT is searched for possible locations of the corresponding compo-nents in the document. The query pair fetches all the spatial features F i  X  1 , i from CSFT. F i  X  1 , i gives the feature infor-mation along with their location of all component pairs. We show some examples of retrieval of query character pair in Fig. 7 . Here, we mark the locations of query character pair using lines. 4.2.2 Component sequence formation Two consecutive component pairs will be selected for joining if both of them have a common component and their spatial features are compatible. The component pairs ( C i , C k ) and ( C between them are similar.

The similarity between F ik and F kj is checked by com-paring the size, linearity and distance among 3 components C , C measured for similarity by Eq. 1 . Linearity of component pairs is considered to be matched if the angles of component pairs are locally linear and it is formulated by,  X   X  The function lin computes the linearity between 2 compo-nent pairs. It is performed by subtracting the angles between character pairs.  X  ij is explained in Sect. 3.2.3 . Selection of T  X  is discussed in the Experiment Section. The similarity of distances d ik and d kj between two component pairs assures the inter-character spacing is satisfied. It is formulated by ( d ik / r )  X  ( d kj / r ) / min (( d ik / r ), ( d kj / r where,  X  is set up as 0.5 in our experiment. r and r are dis-tances between character pairs in query string (discussed in Sect. 4.1 ). When the above conditions of two pair of compo-nents are satisfied, the components pairs are joined to grow the sequence. After linking the components through this con-strained merging different component sequences are gener-ated. The components in each sequence follow the similarity rules of size, local linearity and distance.
 Algorithm 1 Word indexing from user query 4.2.3 Query word matching Thecomponentsequences T m thataregeneratedbytheabove process may not be similar to the query string Q m always. This is because during the formation of sequence, full char-acter sequence is not checked. Some sequences can be part of the query string. Thus, the objective now is to find the sequences which are similar to Q m and to minimize the error in obtaining the required string. The matching between a query word Q m and a target sequence T m is formulated as matching of two sequences of character labels. As the labels are led by multi-oriented character label, the problem is to match multi-oriented label strings of query and target sequence.

Approximate string matching (ASM) algorithm [ 31 ] has been used in our system for matching of sequence of labels. This method has frequently been used in the literature to refer to a class of pattern matching techniques, in which k errors are allowed between a pattern string S and a text string T . The approximation quality can be measured with different string distance function. The edit distance ED( S , T ) between S and T isdefinedastheminimumpossiblenumberofediting steps for conversion. The length of the strings S and T may be different. The algorithm finds all sub-strings of the text T that have at most d s errors (character that are not same) with the pattern S . When d s = 0 (no mismatches), it is a simple string matching algorithm. As, this algorithm is very popular, we do not give its details here.

Finally, words in the document are ranked on the basis of the similarity with query word by comparing the string through component pairs. The rejection of a character pair string is performed if the distance between sequences is greater than d s . Thus, we assume the number of errors obtained by improper segmentation of character or error in character label is upto d s . We discuss the choice of this value in the experiment section. The word indexing is described in Algorithm 1. 5 Data details and experimental results 5.1 Datasets There are documents comprising mostly graphical infor-mation such as maps, engineering drawings and diagrams. Administrative documents contain graphical symbols such as logos, stamps, seals, etc. These documents contain a huge amount of unstructured textual information. To the best of our knowledge, there exist no standard database to evaluate word segmentationandindexingmethodsindocumentsofunstruc-tured layout containing multi-orientated and multi-size text information. We have considered two different datasets of unstructured layout. These are obtained from (1) graphical entities containing logos and seals, (2) geographical maps showingthenamesoflocations,rivers,cities,etc.Someofthe datasets havegroundtruthandsomedonot havegroundtruth. Ifgroundtruthisnotavailable,thentheresultischeckedman-ually by viewing the results. The details of datasets and their results are described below. 5.1.1 Logo and seal dataset Graphical objects such as seals, logos, etc. are synthetic enti-ties consisting of a characteristic set of symbols according to their visual properties. Seals generally have closed, con-nective contour surrounding the text characters, and logos which indicate owner and usage of the seal [ 32 ]. Both seals and logos bear some constant character strings to convey the information about the owner-organization and its locality.
We have considered the collection of Trademark dataset for logos which is publicly available [ 33 ]. The logo images are in binary level. Fifty logos containing text information are considered for the experiment. Seal objects were collected from different sources, for example historical documents, postal letters, official documents from universities, etc., as a result this dataset was noisy. We created a database of 200 seals images that are segmented from these documents [ 4 ]. The documents were digitized in two-tone at 200 dpi. The seals have different shapes such as circular, rectangular, ellip-tical, etc. Text information is annotated in straight or curve according to the shapes of seals. Frequently, portion of seal information is missing due to noise or overlapping signa-tures. In total, we have 250 images of segmented logos and seals for this dataset. 5.1.2 Map dataset This database is formed by completely unstructured docu-ment images. It contains real as well as synthetic map images. The real data are collected from a set of maps from geography books and historical archive. The map images from geogra-phy books are digitized in gray tone with 300 dpi. In our historical maps, the text is printed in dark color compared to the background. The text layer (i.e., foreground information) is obtained using a color to gray-level conversion method.
We also use the datasets generated synthetically as illus-trated in [ 34 ]. The backgrounds (graphical lines, boundaries, etc.) of these maps are taken from a few real maps and the text words of country/river names are placed in the foreground using a set of rules. The texts are of average size 32  X  32 and they are randomly placed and arbitrarily oriented to generate different synthetic documents. A total of 40 images (15 real and 25 synthetic images) have been taken for the experiment. The average size of these images are 3,000  X  2,000. There are approximately 30 X 50 words in each of the documents. Some examples of such dataset are shown in Figs. 1 a and 4 a.
As mentioned earlier, the text lines in the datasets are in different scale and orientation. It is noticed that 62% words in maps and 74% words in logo and seal datasets are curve. We also checked the character sparsity (inter-character distance of the words) in the words. We divide the text words in 3 different categories mainly. (i) low (ii) medium and (iii) high according to sparsity. When the inter-character distance is less than the average size of the characters, we mark them as low sparse words. When the distance is more than double of the character size, we say them highly sparse. And the rest fall under medium. In Table 1 , we give a statistics of the sparsity of the words of our datasets. 5.2 Pre-processing In our proposed methodology of word searching, the assump-tion was taken that the documents are two-tone images. Thus, gray-level images were binarized by global binarization algo-rithm due to Otsu [ 35 ]. As discussed earlier, in graphical doc-uments, text and graphics appear simultaneously. We have used the connected component analysis for separation of text components [ 16 ]. Sometimes due to noise, the components are broken. We used a method to join the broken parts using the algorithm due to Roy et al. [ 36 ].

We have noticed that the character recognition leads the process of word searching in our system and hence we reduced the error by grouping the similar shaped charac-ters together [ 4 ]. During the training process of character recognition, a database of isolated characters of different font and rotation obtained from graphical documents are consid-ered to take care of multi-oriented and multi-font characters. Using Angle-based feature and SVM classifier, we obtained 98.89% accuracy with fivefold cross-validation [ 8 ]. 5.3 Word detection results In our experiment of word retrieval in graphical documents, wehavetestedourschemeon80querywords(40forlogoand seals, and 40 in maps). The individual components obtained from the documents are labeled and indexed. And the index file for each image is recorded. The query is provided in ASCII text and the detected words are marked according to the component sequence found in the document. In Table 2 , we list some of the query words used in our experiment. The query words used in logo and seal dataset are of upper-case letters as there are less frequent words in lower-case. In the maps, we have tested the query strings in upper-case as well as lower-case letters. We noticed, when the touching occurs with long graphical lines, one or two characters from the words are missing. Hence, for the relative positional infor-mation from the query words, we consider maximum value of r (discussed in Sect. 4.1 ) as 3 in this present work, i.e., we allow at most two characters missing due to touching or noise between every two isolated characters in a word. For better representation, both qualitative and quantitative exper-imental results are reported here. 5.3.1 Qualitative results To visualize, we draw a line through the centers of the com-ponent sequence of the word. We show in Fig. 8 some of the word detection results of our approach in logo images. In seal dataset, the text information is annotated in straight or curve way according to the shapes of seals. Frequently, there is missing of seal information due to noise or overlapping signatures. Figure 9 shows some word detection results in the seal images.

We show in Fig. 10 some word detection result of our approach in map documents. Given the query words  X  X RAG-ONIA X  and  X  X eguYoma X , the results are shown in Fig. 10 a, b.

The location of a query word ( X  X CEAN X ) detected in a document by our character pair indexing approach is shown in Fig. 11 . It is appreciable to see that our approach performs well even when there exists text overlapping with long graph-ical objects. In Fig. 11 , the word  X  X CEAN X  at the top position contains a touching character  X  X  X  with graphical lines. Our method has detected it correctly. At the bottom of the doc-ument, it can be seen that the query word is also detected although it is curved.

In Fig. 12 , we show variations of some other results using a few query words. The detected words are cropped from the document according to the matching. The result of these words show the advantages of our approach in different kinds of problem in graphical documents. Figure 12 ashowsthe retrieval of word when inter-character spacing is more than normal spacing. Figure 12 c shows a word where character  X  X  X  is overlapped with graphical line. Figure 12 d demonstrate how the method is tolerant to the graphical line touching. In Fig. 12 e, f, some results are given to show that our system can handle even when some characters are touching in a string. 5.3.2 Quantitative results In Table 3 , we show the overall word detection performance of the system. For the experiment, an adaptive threshold is selectedforworddetection.Therejectionofselectedwordsis performed if the distance between sequence and query string is greater than distance T L ( = l / 3 ) , where l is the length of the query string. True positive (TP) in map dataset is 122 and for logo and seal dataset it is 101. True negative (TN) of these datasets are 1,284 and 427, respectively. In Table 4 ,weshow the word detection accuracy obtained using ASM distances of d s = 0, 1 and 2. From the experiment we noted that with string edit distance of 2, we obtained 92.7% from maps and 95.19% in logo and seals, respectively.

To have an idea of the performance according to the query word length, we have measured the accuracy by dividing the query words in two parts mainly smaller and larger. In Table 5 , we show the accuracy of queries: (a) smaller size ( &lt; 7 letters), (b) larger size (  X  7 letters or multiple words). The value 7 was chosen based on the average length of the query strings. Total 55 words in maps and 31 words in logo and seal dataset were less than 7 letters. The above results (Tables 3 , 4 ) are obtained when the parameter  X  diff (discussed in Sect. 4.2.2 ) was set to 40, because the best result is obtained with this value from our experiment with different  X  diff Table 6 ). When the angle difference between successive char-acters is large, adjacent characters are joined correctly and hence improves the word detection accuracy.

Finally, we give an indication about the system X  X  compu-tation time. The proposed method need approximately 5.2s on an average to index the document of size 800  X  800 by a PC of 2.2-GHz Intel Core 2 Duo Processor with 2GB RAM. The searching of words having average length of 7 characters in that document took 0.6s. 5.4 Document retrieval result 5.4.1 Qualitative results Given a query text string, a ranked list of retrieved images/ documents from our database are shown in Fig. 13 . The rank-ing is done based on the string matching distance. It is inter-esting to notice how background noise effects the ranking of these images. In Fig. 13 , we show five results of the ranked documents, retrieved by query string  X  X TAR X  in logo dataset and  X  X IGUERAS X  in seal dataset. Figure 13 c shows portions of maps where the word  X  X angtze X  is found. 5.4.2 Quantitative results To evaluate the performance of the document searching sys-tem in graphical document dataset, we use common ratio of precision and recall. We have selected a subset of 50 query words (25 for logo and seal, and 25 for maps) for the evalua-tion.Thesewordshavemorethan4instancesintherespective dataset. In Fig. 14 , we show the performance of the proposed document retrieval system. In the dataset of logo and seals, the precision is obtained 88% when the recall is 80%. The accuracy in map database is less compared to that of logo and seal image database. It is due to many touching and broken characters in map database. The true positive (TP) of maps dataset is 83 and for logo and seal dataset it is 102. True negative of these datasets are 497 and 625, respectively. 5.5 Comparison with other methods As mentioned earlier, to our knowledge, there is no reported work on multi-oriented string retrieval and this is the first work of its kind. Since there is no reported word searching result in graphical documents, we could not compare the result with other method.

However, we have given a comparative analysis of this work with our earlier work [ 2 , 37 ]. The algorithm [ 37 ]isone of traditional methods of line segmentation and character extraction in multi-oriented environment. The procedures to detect words using [ 37 ] are designed as follows. We segment each text lines in graphical documents based on the fore-ground and background information of the text components. This method utilizes the background information using water reservoir concept. After segmenting text lines, connected components in each line are recognized using our multi-oriented character recognition method. Next, we search the query keyword in this lines using string matching algorithm. Theworkin[ 2 ] presented some preliminary results of word searching algorithm in graphics-rich documents.

InTable 7 ,weshowthecomparativeworddetectionresults of these three systems obtained from the considered data-bases. This experiment of word detection is performed as explained in Sect. 5.3.2 . It is noticed that the word detec-tion approach based on character coding pair shows better accuracy than the traditional method [ 37 ] in all the datasets. The reason of better performance in this approach is that the method is flexible and can consider curvilinearity of the char-acter in complex graphical layout in efficient way. Another reason is that simple line segmentation method sometimes fails to connect all text characters in a complex curve line. Thus, some lines are broken into different line-segments or wrongly join with different lines. 5.6 Discussion and error analysis For the experiment, we noted that the detection accuracy is affected mainly due to broken text characters in the document andthepresenceoflonggraphicallinesovertextregions.Due to noise when characters are broken and the broken compo-nents of a character could not be joined through preprocess-ing, the character labeling process might be wrong, and hence the word retrieval process is sometimes erroneous. In Fig. 15 , we show some images which were not detected by our system because of improper character segmentation (see Fig. 15 a). In Fig. 15 b, both the characters  X  X  X  were not segmented prop-erly due to its fragmentation due to background noise. In the word  X  X holka X , there were many characters touching with background graphical lines. In  X  X urma X , the touching char-acters  X  X m X  were wrongly segmented as  X  X n X . Here due to touching, only two characters  X  X  X  and  X  X  X  were segmented correctly. In Fig. 16 , we show some erroneous results using this system. In Fig. 16 a, due to similar character sequence we have obtained a part of word  X  X OUTHERN X  using the query string  X  X ORTHERN X . Figure 16 b shows the retrieval result with query word  X  X armada X  because of the rotation invariance nature of the character recognition labels. Here, the component  X  X  X  is mis-labeled as  X  X  X  and  X  X  X  is recognized as  X  X  X  due to rotation invariance nature. Thus, using rotation invariant classes, the component sequence is recognized as  X  X rmad X  which is similar to the sequence of 5 characters in the middle of  X  X armada X .

Some good quality characters are mis-labeled sometimes due to unfamiliar style/font, the presence of noise, complex touching, etc. of the characters. The size of a touching com-ponent is usually bigger than an isolated component. As our text line segmentation approach is performed by clustering the character components based on their size and positional information, touching components may not be clustered in the text line because of the size difference of touching com-ponents and isolated characters. 6 Conclusion We have presented a word searching algorithm based on spatial arrangement of character components in graphical documents. A multi-oriented and multi-scaled text charac-ter recognition method has been used to generate high level local features to take care of complex multi-oriented and multi-sized character strings in graphical documents. The text label of these components and their pair information are used to index local spatial information. Relative positional information of text characters in a string are used for this purpose and hypotheses were generated based on that.
We tested our method in two datasets of graphical doc-uments containing characters in touching, overlapping con-text. Because of such touching and overlapping some errors occurred. As we discussed, the orientation information of the character has not been used in our approach for recog-nition. Hence, the orientation angle can be integrated for word retrieval in future work. The performance could also be improved by removing the rotation ambiguities of some characters by detecting the context information of the char-acter from its neighboring characters. One advantage of our approach is that the system can detect the query word even if we do not extract all the text characters of a query string.
Most of the state-of-the-art approaches in graphical doc-uments rarely use text information for indexing and retrieval due to their complexity. For example, in the approaches of document indexing based on logo spotting, the  X  X ogo X  along with its textual content are considered as a whole symbol and document indexing is performed using the whole sym-bol. Our approach of document retrieval is based on the text information. It provides an additional indexing approach of graphical documents.

One of the novelties of this approach is the detection of words on-the-fly in graphical documents by segmenting and validating them at the same time. There are not many pub-lished works with a complete system which can deal with word searching in graphical documents. It is due to several limitations when trying to apply in large collections of graph-ical documents. Most of these existing pieces of work [ 6 ]pro-posed in the literature considered a fixed set of documents with a single font of characters for their experiment. Though our present approach has a few limitations due to some touch-ing and broken components, we believe it is a step forward in the analysis of graphical document applications. References
