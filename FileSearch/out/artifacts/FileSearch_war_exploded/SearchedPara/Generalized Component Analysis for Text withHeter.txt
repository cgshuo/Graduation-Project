 We present a class of richly structured, undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities. Our model generalizes techniques such as principal compo-nent analysis to heterogeneous data types. In contrast to other approaches, this framework allows modalities such as words, authors and timestamps to be captured in their nat-ural, probabilistic encodings. A latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method. We demon-strate the effectiveness of our framework on the task of au-thor prediction from 13 years of the NIPS conference pro-ceedings and for a recipient prediction task using a 10-month academic email archive of a researcher. Our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework.
 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Experimentation, Measurement, Performance Undirected Graphical Models, Topic Modeling, Text Min-ing, Author Prediction, Recipient Prediction, Multimodal Heterogeneous Data
Many tasks in data mining involve the processing of high dimensional data with heterogeneous attributes. In practice, Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. we often ignore the heterogeneity of attributes and assume that they come from the same source or distribution, and deal with the problem of data dimensionality by projecting the data into a lower dimensional representation. Princi-pal component analysis (PCA) [14] is widely used in data mining and knowledge discovery to achieve dimensionality reduction from real valued input data. The singular value decomposition (SVD) of a centered data matrix can be used to obtain the eigen decomposition of the covariance matrix of data. The eigenvectors of the SVD are called the principal components [11] of the data.

Latent semantic analysis (LSA), proposed by Deerwester et al. [6], is a way to index documents through decom-posing a term-document matrix using PCA. In this frame-work, a matrix consisting of integer word counts for each document is decomposed and the eigenvectors found tend to place higher weight on groups of words which correspond to the notion of semantic topics. Documents projected into the lower dimensional latent space can then be indexed more efficiently. While ad hoc methods can be used to augment a term document matrix with other information, the under-lying assumptions of traditional PCA for modeling text are inappropriate.

In particular, recently Roweis and Ghahramani [21] out-lined the connection between linear Gaussian latent variable models and a wide variety of methods including PCA. Prob-abilistic PCA is defined as a linear Gaussian latent variable model. The generative process defined by the model is that data in the reduced dimensional latent space are drawn from an isotropic Gaussian distribution. The observed data are then drawn from a Gaussian distribution with a linearly projected mean. Standard PCA can be easily derived as a special case of probabilistic PCA, where observed data are drawn deterministically from the linear projection of the lower dimensional data [26]. Other recent work has used this insight to obtain supervised forms of the probabilistic PCA techniques [32].

While vector space representations for text documents are widely used in a variety of fields and applications [23], from the probabilistic interpretation of PCA, it is clear that the model assumptions implicit within the original LSA ap-proach for documents are inappropriate. For example, nega-tive values in the term document matrix should not be pos-sible within the framework of an underlying model. Further-more, documents can consist of a rich variety of attributes such as information concerning authors, time stamps and various other relationships. We are interested in capturing this richness of information using an appropriate probabilis-tic model.

A number of probabilistically motivated methods have been proposed to obtain more realistic principal components for documents. One variation of LSA, the probabilistic LSA (PLSA) model was proposed by Hofmann [13] in which doc-uments are represented with a more natural  X  X ag of words X  encoding in which each word arises from a hidden, discrete topic variable.

The general approach of PLSA has been extended to a method known as latent Dirichlet allocation (LDA) [3], which is now a state-of-the-art method for document topic mod-eling. LDA is a three-level hierarchical Bayesian model in which each item in a collection is modeled as a finite mixture over an underlying set of topics and each topic is modeled as an infinite mixture over an underlying set of topic probabil-ities. A survey of a number of similar techniques, also called discrete PCA, is given in [4]. These methods are all based on directed probabilistic graphical models where interactions between variables are encoded as conditional probabilities. However, for richly structured documents such modeling re-strictions are limiting, as explained in Section 2.1.
To address existing modeling limitations we develop and present an approach here based on undirected probabilis-tic graphical models. Our model couples words encoded as draws of discrete random variables with a multidimensional continuous latent variable in a probabilistically principled framework. Extending this general approach we show how to model other attributes of documents such as authors and timestamps in a natural way. We then show how the well-known benefits of supervised dimensionality reductions are also easily obtained through a minor modification of our op-timization procedure. Finally, we present qualitative results recovering topics within 13-year academic conference pro-ceedings and a 10-month academic email archive. We then present quantitative results for authorship identification for research papers and recipient prediction for email messages.
It is common to describe and categorize probabilistic mod-els as either directed or undirected graphical models (also known as, using Bayesian networks and Markov random fields). Models of words alone such as LDA [3, 9] are an ex-ample of a directed model. Other directed models have been proposed for heterogeneous information also associated with documents. For example, stochastic block structure models [20, 16] have been developed for relations between entities, the mixed membership technique [7] models words and re-search paper citations, words and authors are modeled by Steyvers et al. [24], senders and recipients in an email social network are modeled by McCallum et al. [17], words and relations such as voting patterns are modeled by Wang et al. [28], while words and their timestamps are modeled by Blei and Lafferty [2] and Wang and McCallum [27].
Directed graphical models can be described as generative processes and thus enjoy modeling and computational bene-fits conferred from conditional independencies, such as sim-ple sampling procedures. However, in many applications, the dependency between two random variables in directed models can be difficult to describe and specify as a gen-erative process and the direction of directed edges in the underlying graph can arguably be set either way. For exam-ple, when considering the authors and topics of documents, one can give reasonable arguments about either authors  X  topics or topics  X  authors. Particularly, when dealing with multiple modalities, the huge number of possible configu-rations of these directions between a large number of ran-dom variables have complicated the application of directed models to more complex multimodal, heterogeneous textual data.

Furthermore, in state-of-the-art hierarchical Bayesian mod-els such as LDA, exact posterior inference over hidden topic variables and parameters is typically intractable and approx-imate inference techniques such as variational methods [15], Gibbs sampling [1] and expectation propagation [19] are em-ployed to address these issues. As a result, the inference for obtaining a topic decomposition for a previously unseen doc-ument can be slow and troublesome.
Recently, a class of structured undirected latent variable models have gained attention for topic modeling  X  largely due to the fact that once model parameters have been op-timized, inference of hidden topics for a new document has the complexity of a matrix multiplication, which is fast com-pared to hierarchical Bayesian models.

The exponential family harmonium (EFH) is one of the earlier pieces of work in this direction [29]. In Welling et al. [29], a specific model for latent semantic indexing of documents is also outlined in which a consistent conditional Gaussian distribution for hidden (topic) variables is coupled with a corresponding Bernoulli or Discrete distribution for bucketed counts of every word across the vocabulary of a text document collection.

The two-layer structures in EFHs have an important prop-erty: the random variables at the two layers are condition-ally independent given each other, which provides the prop-erty that the mapping from one layer to the other layer can be done by a simple matrix multiplication (and possi-bly some trivial follow-up transformations). However, there is no free lunch. The faster inference leads to more difficult learning due to the intractable normalizing constant in these types of undirected models. Fortunately, the contrastive di-vergence [12] approach has been shown to be efficient for inference and effective for learning in these models. Further and more importantly, in many situations involving docu-ment processing, training can be done off-line, which gives us more freedom in learning.

Based on the two-layer factorization structure of an EFH, there are several other undirected topic models that have been recently proposed for various tasks. For example, a dual-wing harmonium (DWH) model [30] has been applied to captioned images. In this model hidden topics are condi-tional Gaussian given words and word counts are distributed according to a Poisson distribution and Gaussians for color histograms. This model, with some extensions, has been ap-plied to video classification on a benchmark data set with good performance [31]. The rate adapting Poisson (RAP) model [8] is similar, but with Poisson distributions for words counts and Binomial (Bernoulli) distributions for hidden topics. The RAP model has been applied to document re-trieval and object recognition to demonstrate its properties. authors and timestamps of documents.
 Most recently, a two-layer structured model has been shown to be very effective when it is applied to the Netflix movie ratings, a large but sparse tabular data set [22].
Undirected models of this structure have another impor-tant property that directed models lack: a more accurate characterization of rare words. As discussed in [30], in di-rected models such as latent Dirichlet allocation, a word is always generated from a single topic. When its count is low, this behavior becomes a very strong assumption or limita-tion. In the harmonium-structured models, a word arises from a distribution influenced by all the topics. This differ-ent mechanism might play a crucial role in certain applica-tions.
Textual documents such as research papers are very rich media that contains not only the text body considered by most of the topic models, but authors, citations, venues, and timestamps as well. Like the DWH model [30] for cap-tioned images, we want to take advantage of the multimodal information from text documents.

In this paper, we propose a novel model, called general-ized component analysis (GCA), based on the traditional two layer factorization structure but with dramatically dif-ferent semantics. At the hidden layer, previous models as-sume either Gaussian distributions or Binomial (Bernoulli) distributions. In our model, conditioned on observations, a random variable at the hidden layer follows a Log-normal distribution and takes advantage of both continuity and pos-itivity. We believe that in this setting more interpretable results arise.

To capture the rich structure of a document including attributes such as authors and timestamps we associate dif-ferent coupling matrices for each of the different attributes. In general each attribute type is encoded as a different  X  X ag of discrete attributes X . Importantly, when conditioned on topics, draws from the bag are independent. However, when topics are unobserved, all draws are dependent. In our spe-cific experiments here, we model words, authors and times-tamps using this construction.

We associate a Discrete distribution for the identity of each observed word, thus each word token is drawn in a replicated fashion akin to traditional  X  X ag-of-words X  mod-els. Note here that all the word tokens share a common SYMBOL DESCRIPTION T number of topics D number of documents V number of unique words A number of authors C number of discretized timestamps N d number of word tokens in document d S d number of authors in document d M w T  X  ( V  X  1) connection matrix for word M a T  X  ( A  X  1) connection matrix for author
M c T  X  ( C  X  1) connection matrix for time t di the i th topic of document d w dj the j th word of document d a dk the k th author of document d c d the (discretized) timestamp of document d connection matrix between word layer and topic layer. By contrast, in [29] a different connection matrix is needed for each word and word count level. As discussed in [29], vari-ous continuous exponential family distributions can be used to augment models for real valued attributes. The Pois-son distributions adopted in [30] and [8] make it possible to use only one connection matrix, but when reconstructing the document counter vectors during contrastive divergence training (see Section 3), there is no guarantee that the re-constructed document has the same length of the original document. In such a case, at early stages of learning, the learning rate of the gradient update has to be carefully set to a small value as reported in [8] and this makes the model difficult to learn. Our model implicitly takes the document length as an input, and we find empirically that the learning process converges faster. Authors are also associated with a Discrete distribution in our setting. We now present the details of our model for generalized component analysis.
In contrast to previous undirected topic models, in our new model, generalized component analysis (GCA), words are encoded as individual observations instead of word counts. Because of the conditional independencies between two lay-ers, we can describe the model in plate notation, shown in Figure 1(a). The notation used in this paper is shown in Ta-ble 1. For clarity, we expand the model for document d as shown in Figure 1(b) into a restricted Boltzmann machine or exponential family harmonium structure.

Following a common approach for describing a general exponential family two layer architecture, we specify our model as follows: 1. Consider first, at hidden (topic) layer, a Log-normal 2. at the observation layer, where we use the notation Log-normal(  X  ,  X  2 ) for a Log-normal distribution with parameters  X  and  X  2  X  the mean and variance of the variable X  X  logarithm, and Discrete(  X  )isa Discrete distribution (for example, with words) with natural parameter  X  k ( k =1 ,  X  X  X  ,V  X  1) that can be transformed into the probability vector  X  k = e  X  k / we set  X  V = 0).

For simplicity, as shown in the above description, we do not use local potentials, but it is straightforward to define and learn these potentials as well, as demonstrated in pre-vious harmonium-structured models [29, 30, 8]. Also, as in the DWH model, it is possible to mix together discrete and continuous distributions for different modalities, for exam-ple, we could utilize some continuous distribution such as Gaussian and Beta to model time without discretization as in [27].

Once we defined the form we wish the observed and hidden layers to take, we couple the random variables within the two layers by the connection matrix M w , M a and M c to obtain a joint probability distribution in exponential family form as follows: P ( t d , w d , a d ,c d )  X  exp( where, for notational convenience, we set M w iV = 0, for i = 1 ,  X  X  X  ,T , M a i =1 ,  X  X  X  ,T .

Consequently, it is easy to verify the conditional distribu-tions still remain in the same exponential family but with shifted parameters, = Log-normal( = Log-normal( where m dj is the count of word j in document d .
From the joint probability of all random variables (Eqn. 1), we can marginalize out the latent topic variables, and obtain the marginal likelihood of the observed document d . Note that there is no marginal independence between the observed variables although they are conditionally indepen-dent give the hidden topics.

Our objective function, the marginal likelihood of the whole corpus, thus can be calculated (up to a normalizing constant) as  X  exp( 1
Parameters of our model could be learned by gradient as-cent on the marginal (log) likelihood in Eqn. 6. However, due to the intractability of the normalizing constant, it is dif-ficult to calculate the gradient of the log-likelihood. We use contrastive divergence [12] which has been shown to greatly improve learning efficiency in harmonium architectures [29, 30, 8].

The main idea of contrastive divergence is that we can truncate a Gibbs sampler with only one (or a few) iterations, and use the distribution of the samples (say,  X  w d or equiv-alently  X  m dk , d =1 ,...,D ,and k =1 ,...,V  X  1) from the truncated chain to approximate the model distribution. 1 In this way, the learning rule, by taking derivatives of the (un-nomalized) log-likelihood objective function in Eqn. 6, can be written as the difference between the empirical average  X  p where  X  p denotes the empirical distribution determined by our observations, and the approximated (by contrastive divergence) model average  X  p CD where p CD denotes the model distribution approximated by the samples from the truncated Gibbs chain in our contrastive divergence learn-ing.
More details on contrastive divergence learning can be found in [12].
 Algorithm 1 Learning via Contrastive Divergence 1: Input: document w d , a d ,c d ( d =1 ,  X  X  X  ,D ), topic# T 2: Initialize connection matrix M w ,M a ,M c randomly 3: repeat 4: for d =1to D do 5: for i =1to T do 6: Draw t di , according to p ( t di | w d , a d ,c d )inEqn. 2 7: end for 8: for j =1to N d do 9: Draw  X  w dj , according to P ( w dj | t d )inEqn. 3 10: end for 11: for k =1to S d do 12: Draw  X  a dk , according to P ( a dk | t d )inEqn. 4 13: end for 14: Draw  X  c d , according to P ( c d | t d )inEqn. 5 15: end for 16: for i =1to T do 17: for j =1to V  X  1 do 18: Update M w ij , according to Eqn. 7 19: end for 20: for k =1to A  X  1 do 21: Update M a ik , according to Eqn. 8 22: end for 23: for b =1to C  X  1 do 24: Update M c ib , according to Eqn. 9 25: end for 26: end for 27: until M w ,M a ,M c converge
Similarly, we can obtain the learning rules for the other two connection matrices,  X  I ( k  X   X a d )( and,  X  I ( b = X  c d )( where I ( q  X  Q )and I ( a = b ) are indicator functions, and the last terms in all formulae come from a Gaussian prior over parameters (with variance  X  2 ) which provides smoothing to help cope with sparsity in the training data [5]. This prior favors parameters that are closer to zero, and penalize (both positive and negative) large values of parameters. We sum-marize the above contrastive divergence learning procedures in Algorithm 1.

The introduction of this prior also helps alleviate the iden-tifiability problem as reported in [29] and [8], that is, it makes the model more identifiable. Without further spe-cial handling of identifiability issues, we still get surpris-ingly accurate and interpretable results as shown in Section 5. Priors over weights can also influence the effectiveness of dimensionality reduction. A corpus usually has an intrinsic number of topics that is unknown, and in general, we either try many settings and select the best, or use nonparametric methods to estimate this number [25]. When given inappro-priate number of topics, a model without a prior will try to duplicate some topic or create some random (but usually not trivial) topics. With priors, the spurious topics will gradu-ally become trivial (near zero everywhere) since the priors push the weights toward zero where there is not enough data evidence supporting them.
To explicitly emphasize that we want to infer one modal-ity from other modalities, we can perform discriminative training by optimizing our model for a conditional likelihood (CL). Other alternatives include multi-conditional learning (MCL), a training criterion based on weighted combinations of multiple log conditional likelihoods [18].

The update rules can be simplified during discriminative learning since we do not need to reconstruct the modalities that we are not aiming to infer. In this section, we use an author prediction task as an example to illustrate how to do discriminative training in GCA, namely, we are interested in inferring authors from the given text and timestamp of a document. This task is fundamentally difficult considering there could often be hundreds of possible authors.
Using the discriminative learning criterion, we can obtain an alternative, simpler objective function Similar to Eqn. 7, 8, and 9 in regular training, we can ar-rive at the learning rules under discriminative learning as follows,
In Section 5, we show the difference between the two train-ing criteria, and empirically demonstrate that discriminative learning is significantly better for tasks such as author pre-diction for research papers and recipient prediction for email messages.
We apply our models to two large text corpora, academic research papers and email messages of a researcher, and show the results in Section 5.
The NIPS proceeding data set consists of the full text of the 13 years of proceedings from the Neural Information Processing Systems (NIPS) Conferences 1987 to 1999. 2 In addition to downcasing and removing stopwords and num-bers, we also removed the words appearing less than five times in the corpus X  X any of them produced by OCR er-rors. Two-letter words (primarily coming from equations), http://www.cs.toronto.edu/  X  roweis/data.html well known to exist within the NIPS community. were removed, except for  X  X L X ,  X  X I X ,  X  X L X ,  X  X P X ,  X  X M X  and  X  X R. X  We also remove the authors who published fewer than 6 NIPS papers during 1987-1999, and only keep the papers co-authored by at least one of the remaining authors. Our data set contains 873 research papers, 125 authors, 13,576 unique words, and 1,173,343 word tokens in total. The timestamps we use are the publication years of the papers.
This data set consists of the last author X  X  email archive of the ten months from January to October 2004 and here we only consider all the emails sent by McCallum to facilitate the recipient prediction task. In order to model only the new text entered by the author of each message, it is necessary to remove  X  X uoted original messages X  in replies. We elimi-nate this extraneous text by a simple heuristic: all text in a message below a  X  X orwarded message X  line or timestamp is removed. This heuristic does incorrectly delete text that are interspersed with quoted email text. Words are formed from sequences of alphabetic characters; stopwords are removed, and all text is downcased.

Similarly to the preprocessing steps used for the NIPS data set, we remove the recipients who got fewer than 6 emails from McCallum during that period and only keep the emails received by at least one of the remaining recipients. The data set contains 4,643 email messages, 190 recipients, 8,693 unique words, and 97,418 word tokens in total. Each document X  X  timestamp is determined by the month the mes-sage was sent.
In this section, we first show several lists over words, au-thors and time for several learned topics, as anecdotal evi-dence, and then we compare our model with previous mod-els in author prediction on the NIPS data set and recipient prediction on the Email data set.
We present the word list for a subset of topics learned within our weight matrices from the NIPS data set, first only using the text modality as shown in Table 2. Immedi-ately, we can see that all the positive words provide a broad, vivid summary of topics well known to exist within the NIPS community: Biological Neuroscience, Reinforcement Learn-ing and Probabilistic Methods. Other topics not shown ex-hibit words characteristic of topics such as Computational Neuroscience. Interestingly, the negatively weighted words are also common words in other topics, and serve to separate this topic from others possibly confused with it.
In contrast, we believe that the topics which emerge when our model possesses author and time components tend to be more subtle and in some sense of higher fidelity. For example, Table 3 illustrates a VLSI topic and a Vision Sci-ence topic extracted from the NIPS data under the richer model. Interestingly, authors exhibit different co-occurrence patterns. For example, in our selection here, C. Koch is present in both the VLSI and Vision Science topic while T. Sejnowski is highly prominent only in the Vision Science topic and J. Platt is highly prominent only in the VLSI topic. A selection of (early) NIPS publications from these authors is given in Table 4 to further illustrate the effectiveness of our model and the relevance of these topics.
 Table 5 depicts a selection of topics extracted from Mc-Callum X  X  email archive. The first topic concerns the writ-ing of a paper with collaborators with usernames: fuchun , wellner and mhay . User jensen was also involved in ear-lier stages of the research and other people lower on the list were not involved with the paper but are collaborators and assistants. The second topic concerns the construction of a system for finding email contact information from the web. Users culotta and ronb were heavily involved in the system construction. User pereira is involved with the associated project called  X  X ALO. X  Also interestingly, from the tempo-ral modality, we can find this piece of work was primarily done in January and February, 2004, the annual spring pa-per submission season.
Author prediction for a document is fundamentally diffi-cult: (1) in practice, the pool of potential authors for a given document could be very large; (2) the number of authors on a test document is often unknown. Obviously, accuracy across so many authors would not be informative. Here, we use the mean reciprocal rank (MRR) measure to evaluate the performance of models.

In traditional information retrieval, given a query, we rank the documents in a corpus by some score, such as vector-space cosine similarity between document and query [23], and query likelihood [33] and take the top ones as the re-trieved documents. Obviously, not all the retrieved docu-ments are relevant to the given query. In our setting, we project a given test document and all of the training docu-ments into latent space, and rank all the training documents according vector based cosine similarity with the test docu-ment. When the intersection of the author sets of the test document and a retrieved document is not empty, we out-put that the retrieved document as relevant. The reciprocal rank of a test document is the reciprocal of the rank at which the first relevant response was returned, or 0 if none of the responses contained a relevant answer. The score for a sequence of queries is the mean of the individual query X  X  reciprocal ranks.

We randomly split the NIPS data set into training set (9/10, 786 documents) and test set (1/10, 87 documents). We compare our models (discriminatively trained and reg-ularly trained) with the author-topic (AT) model [24] and singular value decomposition (SVD), all using 20 hidden top-ics.

We compare our results with GCA to those of the author-topic (AT) model  X  a Bayesian network, in which each au-thor X  X  interests are modeled with a mixture of topics [24]. In its generative process for each document d ,asetofau-thors, a d , is observed. To generate each word, an author x is chosen uniformly from this set, then a topic t is selected from a multinomial topic distribution  X  x that is specific to the author, and then a word w is generated from a topic-specific multinomial distribution  X  t over words.  X  and  X  are drawn from conjugate Dirichlet priors. The posterior estimates  X   X  and  X   X  of these two mixtures can be obtained models are trained with 20 hidden topics. conveniently during the training stage by Gibbs sampling, variational methods, or expectation propagation.

To predict an author a of a given new document d ,we can calculate the posterior probability of author a given w plicity, we treat the timestamps of documents as additional words. The author(s) with highest posterior probabilities are our predictions. The prior of author a , P ( a ), can be es-timated by counting how many times he/she (co-)authored a paper in the training set, and the data likelihood of the words can be obtained by summing over all possible topic assignments of each token, as shown below,
We also compare our model versus SVD: we ignore the heterogeneity of the data, and aggregate the word counts, the authors, and the timestamps of the documents into a big matrix, conduct SVD analysis, and then find the lower dimensional representations of the documents.

The MRR scores are shown in Table 6. To demonstrate the advantage of incorporating information from multiple modalities, we also run the model on words only. As ob-served in Table 6, even with regular training, our model outperforms SVD and AT with uniform prior on authors (not shown in Table 6). With discriminative training, our model is significantly better than SVD and the author-topic model, achieving a MRR score more than twice as large as from SVD.

Note that, (1) author prediction on test documents for the author-topic model is relatively slow because we need sum over all possible topic assignments of each word token. On the other hand, both for GCA and SVD, this can be done by simple matrix multiplication; (2) training can be done offline, however we want to point out that discriminative learning is much more efficient in our setting than regular learning because we do not need to reconstruct the words and time during contrastive divergence learning.

We also show how the MRR scores change on the NIPS data set as the number of learning iterations increase in Fig-Figure 2: The mean reciprocal rank (MRR) scores vs. number of iterations for different models on the NIPS data set. All models are trained with 20 hid-den topics. ure 2. These curves also serve as additional evidence regard-ing whether the number of training iterations is sufficient.
Recipient prediction (also called CC prediction) has re-cently attracted significant interest. As an important office application, recipient prediction seems very similar to au-thor prediction discussed in the previous section. We can easily adapt the same setting used for author prediction to do recipient prediction. We randomly split the Email data set into training set (9/10, 4,179 documents) and test set (1/10, 464 documents).

The MRR scores are reported in Table 6. Again, we can quickly see that the discriminatively trained GCA greatly outperforms other models. The MRR scores change on the Email data set as the number of learning iterations increase are shown in Figure 3 for different models.

Also, we can find that MRR scores are consistently worse than the ones on NIPS data set. Our conjecture is that the body message of an email is in general much shorter than a research paper; an email X  X  body message can be as simple as one word. Additionally, the text in email body is composed by the sender, although it should reflect recipients X  interests or expertise.
We have proposed a new harmonium-structured undirected model for large text collections that simultaneously take into account information from multiple modalities. For the discrete attributes of documents such as words, unlike the previous models, the new model still allows the words to come from a discrete distribution in a  X  X ag-of-words X  fash-ion. Thus, our model implicitly takes document length as input, which greatly increases the efficiency during the con-trastive divergence learning.

We have shown interpretable topics over various document attributes (words, authors, time) on two large text collec-tions, and demonstrate better mean reciprocal rank (MRR) performance, over other models, on author prediction task Figure 3: The mean reciprocal rank (MRR) scores vs. number of iterations for different models on the Email data set. All models are trained with 20 hid-den topics. on the NIPS data set and recipient prediction task on the Email data set. Our models can be applied to tasks with similar objectives such as targeted advertising.

Our models with these hidden layer structures allow a great deal of flexibility to incorporate information from mul-tiple modalities as demonstrated. In directed models, typ-ically when a new source of information is introduced, de-pendencies with other variables are carefully hand specified, and in many cases, dependencies are too complicated to be explicitly expressed. Furthermore, likelihoods from different modalities are often not comparable and weighting param-eters are often needed as in [27]. We see great potential to combine a wide variety of information from other attributes and robustly create extremely rich models that could have been particularly hard to devise in a directed model. We believe the model presented in this paper and other simi-lar ones will play an important role in modeling data with heterogeneous attributes.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by The Central In-telligence Agency, the National Security Agency and Na-tional Science Foundation under NSF grant #IIS-0326249, and in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract number NBCHD030010. The second author appreciates support by Microsoft Research under the Memex and eScience funding programs and support from Kodak Research. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors. [1] C. Andrieu, N. de Freitas, A. Doucet, and M. Jordan. [2] D. Blei and J. Lafferty. Dynamic topic models. In [3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [4] W. Buntine and A. Jakulin. Applying discrete PCA in [5] S. F. Chen and R. Rosenfeld. A Gaussian prior for [6] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [7] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed [8] P. Gehler, A. Holub, and M. Welling. The rate [9] T. Griffiths and M. Steyvers. Finding scientific topics. [10] T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. [11] T. Hastie, R. Tibshirani, and J. H. Friedman. The [12] G. Hinton. Training products of experts by [13] T. Hofmann. Probabilistic latent semantic analysis. In [14] I. T. Jolliffe. Principal Component Analysis . Springer [15] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [16] C. Kemp, T. L. Griffiths, and J. Tenenbaum.
 [17] A. McCallum, A. Corrada-Emanuel, and X. Wang. [18] A.McCallum,C.Pal,G.Druck,andX.Wang.
 [19] T. Minka and J. Lafferty. Expectation-propagation for [20] K. Nowicki and T. A. Snijders. Estimation and [21] S. Roweis and Z. Ghahramani. A unifying review of [22] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [23] G. Salton and M. McGill. Introduction to Modern [24] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [25] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [26] M. E. Tipping and C. M. Bishop. Probabilistic [27] X. Wang and A. McCallum. Topics over time: A [28] X. Wang, N. Mohanty, and A. McCallum. Group and [29] M. Welling, M. Rosen-Zvi, and G. Hinton.
 [30] E. Xing, R. Yan, and A. G. Hauptmann. Mining [31] J. Yang, Y. Liu, E. P. Xing, and A. Hauptmann. [32] S. Yu, K. Yu, V. Tresp, H.-P. Kriegel, and M. Wu. [33] C. Zhai and J. Lafferty. A study of smoothing
