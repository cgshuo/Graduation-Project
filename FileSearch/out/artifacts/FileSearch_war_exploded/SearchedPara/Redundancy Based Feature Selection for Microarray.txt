 In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sam-ple size. Traditional gene selection methods often select the top-ranked genes according to their individual discrimina-tive power without handling the high degree of redundancy among the genes. Latest research shows that removing re-dundant genes among selected ones can achieve a better representation of the characteristics of the targeted pheno-types and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature rele-vance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our method in comparison with representa-tive methods has been demonstrated through an empirical study using public microarray data sets.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications  X  data mining ; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recogni-tion]: Design Methodology  X  feature evaluation and selec-tion General Terms: Algorithms, Experimentation Keywords: feature redundancy, gene selection, microarray data
The rapid advances in gene expression microarray tech-nology enable simultaneously measuring the expression lev-els for thousands or tens of thousands of genes in a single experiment [19]. Analysis of microarray data presents un-precedented opportunities and challenges for data mining in areas such as gene clustering [2, 10], sample clustering and class discovery [2, 7, 20], sample classification [1, 7], and gene selection [20, 22]. In sample classification, a mi-Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. croarray data set is provided as a training set of labeled samples. The task is to build a classifier that accurately predicts the classes (diseases or phenotypes) of novel unla-beled samples. A typical data set may contain thousands of genes but only a small number of samples (often less than a hundred). The number of samples is likely to remain small at least for the near future due to the expense of collecting microarray samples [6]. The nature of relatively high dimen-sionality but small sample size in microarray data can cause the known problem of  X  X urse of dimensionality X  and over-fitting of the training data [6]. Therefore, selecting a small number of discriminative genes from thousands of genes is essential for successful sample classification [5, 7, 22]. Re-search on feature selection is receiving increasing attention in gene selection for sample classification.

Feature selection, a process of choosing a subset of fea-tures from the original ones, is frequently used as a prepro-cessing technique in data mining [4, 15]. It has proven effec-tive in reducing dimensionality, improving mining efficiency, increasing mining accuracy, and enhancing result compre-hensibility [3, 12]. Feature selection methods can broadly fall into the wrapper model and the filter model [12]. The wrapper model uses the predictive accuracy of a predeter-mined mining algorithm to determine the goodness of a se-lected subset. It is computationally expensive for data with a large number of features [3, 12]. The filter model sepa-rates feature selection from classifier learning and relies on general characteristics of the training data to select feature subsets that are independent of any mining algorithms. In gene selection, the filter model is often adopted due to its computational efficiency [7, 22].

Among existing gene selection methods, earlier methods often evaluate genes in isolation without considering gene-to-gene correlation. They rank genes according to their in-dividual relevance or discriminative power to the targeted classes and select top-ranked genes. Some methods based on statistical tests or information gain have been shown in [7, 16]. These methods are computationally efficient due to linear time complexity O ( N ) in terms of dimensionality N . However, they share two shortcomings: (1) certain do-main knowledge or trial-and-error is required to determine the threshold for the number of genes selected (e.g., a to-tal of 50 genes were selected in Golub et al X  work [7]); and (2) they cannot remove redundant genes. The issue of  X  X e-dundancy X  among genes is recently raised in gene selection literature. It is pointed out in a number of studies [5, 23] that simply combining a highly ranked gene with another highly ranked gene often does not form a better gene set be-cause these two genes could be highly correlated. The effects of redundancy among selected genes are two-fold. On one hand, the selected gene set can have a less comprehensive representation of the targeted classes than one of the same size but without redundant genes; on the other hand, in or-der to include all representative genes in the selected gene set, redundant genes will unnecessarily increase the dimen-sionality of the selected gene set, which will in turn affect the mining performance on the small sample [6]. Some latest methods [5, 22] take into account the gene-to-gene correla-tion and remove redundant genes through pair-wise correla-tion analysis among genes. They can handle redundancy to certain extent but require time complexity of O ( N 2 ). In ad-dition, they still need to decide the threshold for the number of selected genes.

In this paper, we tackle gene selection by developing a novel redundancy based approach that overcomes the above two shortcomings in an efficient way. The remainder of this paper is organized as follows. In Section 2, we introduce gene and feature selection, review notions of feature rele-vance, and identify the need for feature redundancy analy-sis. In Section 3, we provide formal definitions on feature redundancy, and reveal the relationship between feature rel-evance and feature redundancy. In Section 4, we describe correlation measures and present our redundancy based fil-ter method. Section 5 contains experimental evaluation and discussions. Section 6 concludes this work.
In sample classification and gene selection, the microar-ray data for analysis is in the form of a gene expression ma-trix (shown in Figure 1), in which each column represents a gene and each row represents a sample with label c i . For each sample the expression levels of all the genes in study are measured, so f ij is the measurement of the expression level of the j th gene for the i th sample where j = 1 , ..., N and i = 1 , ..., M . The format of a microarray data set con-forms to the normal data format of machine learning and data mining, where a gene can be regarded as a feature or attribute and a sample as an instance or a data point. Figure 1: An example of gene expression matrix.

Let F be a full set of features and G  X  F . In general, the goal of feature selection can be formalized as selecting a minimum subset G such that P ( C | G ) is equal or as close as possible to P ( C | F ), where P ( C | G ) is the probability distribution of different classes given the feature values in G and P ( C | F ) is the original distribution given the feature values in F [13]. We call such a minimum subset an optimal subset, illustrated by the example below.

Example 1. (Optimal subset) Let features F 1 , ..., F 5 be Boolean. The target concept is C = g ( F 1 , F 2 ) where g is a Boolean function. With F 2 = F 3 and F 4 = F 5 , there are only eight possible instances. In order to determine the tar-get concept, F 1 is indispensable; one of F 2 and F 3 can be dis-posed of (note that C can also be determined by g ( F 1 , F but we must have one of them; both F 4 and F 5 can be dis-carded. Either { F 1 , F 2 } or { F 1 , F 3 } is an optimal subset. The goal of feature selection is to find either of them.
In the presence of hundreds or thousands of features, re-searchers notice that it is common that a large number of features are not informative because they are either irrel-evant or redundant with respect to the class concept [22]. Traditionally, feature selection research has focused on search-ing for relevant features. Although empirical evidence shows that along with irrelevant features, redundant features also affect the speed and accuracy of mining algorithms [8, 12, 13], there is little work on explicit treatment of feature re-dundancy. We next present a classic notion of feature rele-vance and employ the same example above to illustrate why it alone cannot handle feature redundancy.

Based on a review of previous definitions of feature rel-evance, John, Kohavi, and Pfleger classified features into three disjoint categories, namely, strongly relevant, weakly relevant, and irrelevant features [11]. Let F i  X  F and S F  X  X  F i } . These categories of relevance can be formalized as follows.

Definition 1. (Strong relevance) A feature F i is strongly relevant iff
Definition 2. (Weak relevance) A feature F i is weakly relevant iff
Corollary 1. (Irrelevance) A feature F i is irrelevant iff
Strong relevance of a feature indicates that the feature is always necessary for an optimal subset; it cannot be removed without affecting the original conditional class distribution. Weak relevance suggests that the feature is not always neces-sary but may become necessary for an optimal subset at cer-tain conditions. Irrelevance (following Definitions 1 and 2) indicates that the feature is not necessary at all. Accord-ing to these definitions, it is clear that in previous Exam-ple 1, feature F 1 is strongly relevant, F 2 , F 3 weakly relevant, and F 4 , F 5 irrelevant. An optimal subset should include all strongly relevant features, none of irrelevant features, and a subset of weakly relevant features. However, it is not given in the definitions which of weakly relevant features should be selected and which of them removed. Therefore, we can conclude that feature relevance alone cannot effectively help remove redundant features and there is also a need for fea-ture redundancy analysis.
Notions of feature redundancy are normally in terms of feature correlation. It is widely accepted that two features are redundant to each other if their values are completely correlated (for example, features F 2 and F 3 in Example 1). In reality, it may not be so straightforward to determine feature redundancy when a feature is correlated (perhaps partially) with a set of features. We now formally define feature redundancy in order to devise an approach to ex-plicitly identify and eliminate redundant features. Before we proceed, we first introduce the definition of a feature X  X  Markov blanket given by Koller and Sahami (1996).
Definition 3. (Markov blanket) Given a feature F i , let M i  X  F ( F i /  X  M i ) , M i is said to be a Markov blanket for F i iff P ( F  X  M i  X  X  F i } , C | F i , M i ) = P ( F  X  M i  X  X  F
The Markov blanket condition requires that M i subsume not only the information that F i has about C , but also about all of the other features. It is pointed out in Koller and Sahami (1996) that an optimal subset can be obtained by a backward elimination procedure, known as Markov blanket filtering : let G be the current set of features ( G = F in the beginning), at any phase, if there exists a Markov blanket for F within the current G , F i is removed from G . It is proved that this process guarantees a feature removed in an earlier phase will still find a Markov blanket in any later phase, that is, removing a feature in a later phase will not render the previously removed features necessary to be included in the optimal subset. According to previous definitions of feature relevance, we can also prove that strongly relevant features cannot find any Markov blanket. Since irrelevant features should be removed anyway, we exclude them from our definition of redundant features. Hence, our definition of redundant feature is given as follows.
 Definition 4. (Redundant cover) A Markov blanket M i of a weakly relevant feature F i is called a redundant cover of F i .

Definition 5. (Redundant feature) Let G be the cur-rent set of features, a feature is redundant and hence should be removed from G iff it has a redundant cover within G .
From the property of Markov blanket, it is easy to see that a redundant feature removed earlier remains redundant when more features are removed. Figure 1 depicts the re-lationships between definitions of feature relevance and re-dundancy introduced so far. It shows that an entire feature set can be conceptually divided into four basic disjoint parts: irrelevant features (I), redundant features (II, part of weakly relevant features), weakly relevant but non-redundant fea-tures (III), and strongly relevant features (IV). An optimal subset essentially contains all the features in parts III and IV. It is worthy to point out that although parts II and III are disjoint, different partitions of them can result from the process of Markov blanket filtering. In previous Example 1, either of F 2 or F 3 , but not both, should be removed as a redundant feature.

In determining relevant features and redundant features, it is advisable to use efficient approximation methods for Figure 2: A view of feature relevance and redun-dancy. two reasons. First, searching for an optimal subset based on the definitions of feature relevance and redundancy is combinatorial in nature. It is obvious that exhaustive or complete search is prohibitive with a large number of fea-tures. Second, an optimal subset is defined based on the full population where the true data distribution is known. It is generally assumed that a training data set is only a small portion of the full population, especially in a high-dimensional space. Therefore, it is not proper to search for an optimal subset from the training data as over-searching the training data can cause over-fitting [9]. We next present our method.
In this section, we first introduce our choice of correlation measure for gene selection, and then describe our approxi-mation method based on the previous definitions.
In gene selection, there exist broadly two types of mea-sures for correlation between genes or between a gene and the target class: linear and non-linear. Since linear correla-tion measures may not be able to capture correlations that are not linear in nature, in our approach we adopt non-linear correlation measures based on the information-theoretical concept of entropy , a measure of the uncertainty of a ran-dom variable. The entropy of a variable X is defined as and the entropy of X after observing values of another vari-able Y is defined as H ( X | Y ) =  X  where P ( x i ) is the prior probabilities for all values of X , and P ( x i | y i ) is the posterior probabilities of X given the values of Y . The amount by which the entropy of X decreases reflects additional information about X provided by Y and is called information gain , given by Information gain tends to favor variables with more values and can be normalized by their corresponding entropy. In this work, we use symmetrical uncertainty ( SU ), defined as which compensates for information gain X  X  bias toward fea-tures with more values and restricts its values to the range [0 , 1]. A value of 1 indicates that knowing the values of either feature completely predicts the values of the other; a value of 0 indicates that X and Y are independent. Entropy-based measures handle nominal or discrete values, and therefore continuous values in gene expression data need to be dis-cretized [14] in order to use entropy-based measures.
In developing an approximation method for both rele-vance and redundancy analysis, our goal is to efficiently find a feature subset that approximates the optimal subset (parts III and IV in Figure 2). We first differentiate two types of correlation between features and the class.

Definition 6. (Individual C -correlation) The corre-lation between any feature F i and the class C is called indi-vidual C -correlation, denoted by ISU i .

Definition 7. (Combined C -correlation) The corre-lation between any pair of features F i and F j ( i 6 = j ) and the class C is called combined C -correlation, denoted by For combined C -correlation, we treat features F i and F j as one single feature F i,j and the cartesian product of the domains of F i and F j as the domain of F i,j .

In relevance analysis, the individual C -correlation for each feature is measured and ranked. Without choosing any threshold, we heuristically treat all features as relevant fea-tures which are subject to redundancy analysis. In Sec-tion 2, we apply the concept of redundant cover to exactly determine feature redundancy. When it comes to approx-imately determine feature redundancy, the key is to de-fine approximate redundant cover. In our method, we ap-proximately determine the redundancy between two features based on both their individual C -correlations and combined C -correlation. We assume that a feature with a larger indi-vidual C -correlation value contains by itself more informa-tion about the class than a feature with a smaller individ-ual C -correlation value. For two features F i and F j with ISU i  X  ISU j , we choose to evaluate whether feature F i form an approximate redundant cover for feature F j (instead of F j for F i ) in order to maintain more information about the class. In addition, if combining F j with F i does not provide more predictive power in determining the class than F i alone, we heuristically decide that F i forms an approx-imate redundant cover for F j . Therefore, an approximate redundant cover is defined as follows.

Definition 8. (Approximate redundant cover) For two features F i and F j , F i forms an approximate redundant cover for F j iff ISU i  X  ISU j and ISU i  X  CSU i,j .
Recall that Markov blanket filtering, a backward elimina-tion procedure based on a feature X  X  Markov blanket in the current set, guarantees that a redundant feature removed in an earlier phase will still find a Markov blanket (redundant cover) in any later phase when another redundant feature is removed. It is easy to verify that this is not the case for backward elimination based on a feature X  X  approximate re-dundant cover in the current set. For instance, if F j is the only feature that forms an approximate redundant cover for F , and F i forms an approximate redundant cover for F j , after removing F k based on F j , further removing F j based on F i will result in no approximate redundant cover for F in the current set. However, we can avoid this situation by removing a feature only when it can find an approximate redundant cover formed by a predominant feature, defined as follows.

Definition 9. (Predominant feature) A feature is pre-dominant iff it does not have any approximate redundant cover in the current set.
 Predominant features will not be removed at any stage. If a feature F j is removed based on a predominant feature F in an earlier phase, it is guaranteed that it will still find an approximate redundant cover (the same F i ) in any later phase when another feature is removed. Since the feature with the highest ISU value does not have any approximate redundant cover, it must be one of the predominant fea-tures and can be used as the starting point to determine the redundancy between the rest features. In summary, our ap-proximation method of relevance and redundancy analysis is to find all predominant features and eliminate the rest. It can be summarized by an algorithm shown in Table 1. Algorithm RBF:
Relevance analysis
Redundancy analysis Table 1: A two-step Redundancy Based Filter (RBF) algorithm.

We now analyze the efficiency of RBF before conducting an empirical study. Major computation of the algorithm involves ISU and CSU values, which has linear time com-plexity in terms of the number of instances in a data set. Given dimensionality N , the algorithm has linear time com-plexity O ( N ) in relevance analysis. To determine predomi-nant features in redundancy analysis, it has a best-case time complexity O ( N ) when only one feature is selected and all of the rest features are removed in the first round, and a worse-case time complexity O ( N 2 ) when all features are se-lected. In general cases when k (1 &lt; k &lt; N ) features are selected, based on the predominant feature identified in the previous round, RBF typically removes a large number of features in the current round. This makes RBF substan-tially faster than algorithms of subset evaluation based on traditional greedy sequential search, as will be demonstrated by the running time comparisons reported in Section 5. As to space complexity, the algorithm only requires space lin-ear to dimensionality N to compute and store ISU values in relevance analysis, as CSU values can be dynamically computed in redundancy analysis.
Our method is suboptimal due to the way individual and combined C -correlations are used for relevance and redun-dancy analysis. It is fairly straightforward to improve the optimality of the results by considering more complex combi-nations of features in evaluating feature relevance and redun-dancy, which in turn increases time complexity. To improve result optimality without increasing time complexity, fur-ther effort is needed to design and compare different heuris-tics in determining a feature X  X  approximate redundant cover. In our previous work [24], the redundancy between a pair of features is approximately determined based on their individ-ual C -correlations to the class and the correlation between themselves which is measured by their symmetrical uncer-tainty value. The method has similar complexity to RBF, but it does not directly consider the overall predictive power of two features when determining feature redundancy, while RBF does based on their combined C -correlation.
In this section, we empirically evaluate the efficiency and effectiveness of our method on gene expression microarray data. The efficiency of a feature selection algorithm can be directly measured by its running time over various data sets. As to effectiveness, since we often do not have prior knowledge about which genes are irrelevant or redundant in microarray data, we adopt two indirect criteria: (a) num-ber of selected genes and (b) predictive accuracy on selected genes. For gene selection in sample classification, it is desir-able to select the smallest number of genes which can achieve the highest predictive accuracy on new samples.
In our experiments, we select four microarray data sets which are frequently used in previous studies: colon cancer, leukemia, breast cancer, and lung cancer 1 . Note that except for colon data, each original data set comes with training and test samples that were drawn from different conditions. Here we combine them together for the purpose of cross validation. The details of these data sets are summarized in Table 2.

Two representative filter algorithms are chosen in com-parison with RBF in terms of the evaluation criteria iden-tified before. One algorithm representing feature ranking methods is ReliefF, which searches for nearest neighbors of instances of different classes and ranks features according to their importance in differentiating instances of different classes. A subset of features is selected from top of the ranking list [18]. The other algorithm is a variation of the http://sdmc.lit.org.sg/GEDatasets/Datasets.html CFS algorithm, denoted by CFS-SF (Sequential Forward). CFS uses symmetrical uncertainty to measure the correla-tion between each feature and the class and between two features, and exploits best-first search in searching for a fea-ture subset of maximum overall correlation to the class and minimum correlation among selected features [8]. Sequential forward selection is used in CFS-SF as previous experiments show CFS-SF runs much faster to produce similar results than CFS. A widely used classification algorithm C4.5 [17] is adopted to evaluate the predictive accuracy of the selected features. We use programs for ReliefF, CFS-SF, and C4.5 from Weka X  X  collection [21]. RBF is also implemented in the Weka environment.

For each data set, we first apply all the feature selection algorithms in comparison, and obtain the running time and the selected genes for each algorithm. Note that in apply-ing ReliefF, the number of nearest neighbors is set to 5 and all instances are used in weighting genes. We then apply C4.5 on the original data set and each of the three newly obtained data sets (with only the selected genes), and ob-tain overall classification accuracy by leave-one-out cross-validation, a performance validation procedure adopted by many researchers due to the small sample size of microarray data [5, 22]. The experiments were conducted on a Pentium III PC with 256 KB RAM.
Table 3 reports the running time, number of selected genes, and the leave-one-out accuracy for each feature selection al-gorithm. As shown in the table, RBF produced selected genes in seconds for each data set. ReliefF took from 4 sec-onds (on colon data) to 6 minutes (on lunge cancer data) to produce the results. CFS-SF produced its result on colon data in 5 seconds, but it failed on the other three data sets as the program ran out of memory after a period of time ( &gt; 10 minutes) due to its O ( N 2 ) space complexity in terms of the number of genes N . These observations clearly show the superior efficiency of RBF for gene selection in high-dimensional microarray data.

We now examine the effectiveness of these three algo-rithms based on the number of genes selected and the leave-one-out accuracy reported in Table 3. We pick the colon data to explain the difference in gene selection results. As we can see from Table 3, based on the original colon data (2000 genes), 12 out of 62 samples were incorrectly classified, resulting an overall accuracy of 80.65%. RBF selected only 4 genes and helped to reduce the number of misclassified samples to 4 (increasing the overall accuracy to 93.55%). ReliefF also selected 4 genes but only reduced the number of errors to 9 (85.48% on accuracy) since it cannot handle feature redundancy. CFS-SF resulted in 7 errors (88.71% on accuracy), but it selected more genes than RBF. Across the four data sets, we can observe that RBF selected less num-ber of genes than ReliefF and CFS-SF. At the same time, the genes selected by RBF led to the highest accuracy by leave-one-out.

In summary, the above results suggest that RBF is an effi-cient and effective method for gene selection and is practical for use in sample classification. It is worthy to emphasize that genes selected by RBF are independent of classification algorithms, in other words, RBF does not directly aim to increase the accuracy of C4.5.
In this work, we have introduced the importance of re-moving redundant genes in sample classification and pointed out the necessity of studying feature redundancy. We have provided formal definitions on feature redundancy and pro-posed a redundancy based filter method with two desirable properties: (1) it does not require the selection of any thresh-old in determining feature relevance or redundancy; and (2) it combines sequential forward selection with elimination, which substantially reduces the number of feature pairs to be evaluated in redundancy analysis. Experiments on microar-ray data have demonstrated the efficiency and effectiveness of our method in selecting discriminative genes that improve classification accuracy.
 We gratefully thank anonymous reviewers and Area Chair for their constructive comments. This work is in part sup-ported by grants from NSF (No. 0127815, 0231448), Prop 301 (No. ECR A601), and CEINT 2004 at ASU. [1] A. Alizadeh and etal. Distinct types of diffuse large [2] U. Alon and et al. Broad patterns of gene expression [3] A. Blum and P. Langley. Selection of relevant features [4] M. Dash and H. Liu. Feature selection for [5] C. Ding and H. Peng. Minimum redundancy feature [6] E. R. Dougherty. Small sample issue for [7] T. R. Golub and et al. Molecular classification of [8] M. Hall. Correlation-based feature selection for [9] D. D. Jensen and P. R. Cohen. Multiple comparisions [10] D. Jiang, J. Pei, and A. Zhang. Interactive exploration [11] G. John, R. Kohavi, and K. Pfleger. Irrelevant feature [12] R. Kohavi and G. John. Wrappers for feature subset [13] D. Koller and M. Sahami. Toward optimal feature [14] H. Liu, F. Hussain, C. Tan, and M. Dash.
 [15] H. Liu and H. Motoda. Feature Selection for [16] F. Model, P. Adorjan, A. Olek, and C. Piepenbrock. [17] J. Quinlan. C4.5: Programs for Machine Learning . [18] M. Robnik-Sikonja and I. Kononenko. Theoretical and [19] M. Schena, D. Shalon, R. W. Davis, and P. O. Brown. [20] C. Tang, A. Zhang, and J. Pei. Mining phenotypes [21] I. Witten and E. Frank. Data Mining -Pracitcal [22] E. Xing, M. Jordan, and R. Karp. Feature selection [23] M. Xiong, Z. Fang, and J. Zhao. Biomarker [24] L. Yu and H. Liu. Feature selection for
