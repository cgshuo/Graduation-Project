 Model selection in kernel methods is the problem of choosing an appropriate hypothesis space for kernel-based learning algorithms to avoid either underfitting or overfitting of the resulting hypothesis. One of main problems faced by model selection is how to control the sample complexity when de-signing the model selection criterion. In this paper, we take balls of reproducing kernel Hilbert spaces (RKHSs) as candi-date hypothesis spaces and propose a novel model selection criterion via minimizing the empirical optimal error in the ball of RKHS and the covering number of the ball. By in-troducing the covering number to measure the capacity of the ball of RKHS, our criterion could directly control the sample complexity. Specifically, we first prove the relation between expected optimal error and empirical optimal error in the ball of RKHS. Using the relation as the theoretical foundation, we give the definition of our criterion. Then, by estimating the expectation of optimal empirical error and proving an upper bound of the covering number, we repre-sent our criterion as a functional of the kernel matrix. An efficient algorithm is further developed for approximately calculating the functional so that the fast Fourier transform (FFT) can be applied to achieve a quasi-linear computa-tional complexity. We also prove the consistency between the approximate criterion and the accurate one for large enough samples. Finally, we empirically evaluate the per-formance of our criterion and verify the consistency between the approximate and accurate criterion.
 I.2.6 [ Artificial Intelligence ]: Learning X  Parameter Learn-ing ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier Design and Evaluation ; H.2.8 [ Database Man-agement ]: Database Applications X  Data Mining  X  Co ntact Author.
 Theory, Algorithms, Experimentation model selection; kernel methods; covering number; multi-level circulant matrix
Model selection is the problem of choosing an appropriate hypothesis space, in which the learning algorithm searches the optimal hypothesis with the available training data, so as to avoid either underfitting or overfitting of the resulting hypothesis [16]. For kernel-based learning algorithms, the candidate hypothesis spaces are the constrained reproduc-ing kernel Hilbert spaces (RKHSs) which are determined by the choice of the kernel function and the regularization pa-rameter. In this situation, model selection is the problem of selecting the kernel function and the regularization param-eter. For the common kernel-based methods such as kernel ridge regression (KRR) [26], support vector machine (SVM) [32] and least squares support vector machine (LSSVM) [30], when the kernel function and the regularization parameter are given, the learning problem becomes a convex optimiza-tion problem or matrix inverse. Therefore, model selection is critical to learning algorithms and it is also a challenging and central problem in kernel-based learning [23, 1].
Model selection is strictly related to the estimation of the generalization error. The optimal hypothesis space is char-acterized by the smallest generalization error. Since the true probability distribution generating the data is unknown, the generalization error cannot be directly computed and it is necessary to resort to the estimate of its value. The esti-mate can be empirical or theoretical. The cross validation (CV) is a commonly used empirical estimate of the gen-eralization error and the extreme form of cross validation, leave-one-out (LOO), gives an almost unbiased estimate of the generalization error [21]. However, CV and LOO require training the learning algorithm for every candidate model for several times, unavoidably bringing high computational burdens. For the sake of efficiency, some approximate CV approaches are proposed, such as, generalized cross valida-tion (GCV)[15], influence function [11], and Bouligand in-fluence function cross validation (BIFCV) [19]. Minimizing theoretical estimate bounds of the generalization error is an alternative to model selection. The commonly used the-oretical estimates usually introduce some measures of the complexity of the hypothesis space [3], such as VC dimen-si on [32], Rademacher complexity [2], maximal discrepancy [3], radius-margin bound [6], compression coefficient [22] and eigenvalues perturbation [18]. In addition, maximizing the kernel target alignment (KTA) can minimize the bound of generalization error for Parzen window estimator [9]. Hence, KTA is also used for model selection. Several related criteria were also proposed, such as centered kernel target alignment (CKTA) [8] and feature space-based kernel matrix evalua-tion (FSM) [24].

Despite the large amount of work on this important topic, some problems are still faced by existing model selection ap-proaches, especially for large scale applications [16]. From the theoretical point of view, when designing the model se-lection criterion, the sample complexity, the number of sam-ples guaranteeing the leaning to succeed, should be taken into consideration. From the applicable point of view, when calculating the model selection criterion, efficient computa-tional strategies should be proposed [16]. Such considera-tions drive the study of this paper.

In this paper, we propose a novel model selection crite-rion by minimizing the empirical optimal error in the ball of RKHS and the covering number of the ball. The main contributions of this paper can be summarized as follows. First, we establish the link between expected optimal error and empirical optimal error in the ball of RKHS, which is the theoretical foundation of our criterion. Second, with the covering number introduced into the model selection prob-lem for the first time, our criterion could explicitly control the sample complexity of the learning in the ball of RKHS. Third, by estimating the expectation of optimal empirical error and proving an upper bound of the covering number, we represent our criterion as a functional of the kernel ma-trix. Fourth, we develop an efficient algorithm to calculate the functional by approximating the kernel matrix with a multilevel circulant matrix so that the fast Fourier trans-form (FFT) can be applied to achieve a quasi-linear compu-tational complexity. We also prove the consistency between the approximate criterion and the accurate one for large enough samples. Finally, empirical studies are conducted to evaluate the performance of our criterion and verify the consistency between the approximate and accurate criterion.
The rest of this paper is organized as follows. In Section 2, we establish the theoretical foundation of our model se-lection criterion. Section 3 gives the definition of the model selection criterion. In Section 4, we represent our criterion as a functional of the kernel matrix. In Section 5, we de-velop an efficient algorithm for approximately calculating the functional and prove the consistency between the ap-proximate and accurate criterion. In Section 6, we conduct the empirical evaluation. Finally, we conclude in Section 7.
In this section, we demonstrate the theoretical foundation of our model selection criterion.

We consider the supervised learning where the learning algorithm receives a sample of l labeled points where X denotes the input space and Y denotes the output domain, Y = R in the regression case and Y = { X  1 , 1 } in classification case. We assume S is drawn identically and independently from a fixed, but unknown probability distri-bution  X  on X  X Y .

Let  X  : X X X  X  R be continuous, symmetric and positive definite, that is, for any finite set { x 1 , . . . , x l (SPD). We call  X  a Mercer kernel . The reproducing kernel Hilbert space (RKHS) H associated with the kernel  X  can be defined as where span denotes the closure of the linear span of the set of functions {  X  ( x,  X  ) : x  X  X } . The inner product  X  X  , H satisfies For r &gt; 0, let B r be the ball of H with radius r , where  X  X  X  X  H is the norm in H . Since B r is not neces-sarily compact in C ( X ), the space of continuous functions on X , we cannot directly take it as the hypothesis space We introduce an inclusion operator I : H  X  C ( X ) as a compact embedding [10]. Then I ( B r ) is a subset of C ( and its closure I ( B r ) in C ( X ) is a compact subset of In the following, I ( B r ) is adopted as the hypothesis space. Model selection is to select a kernel function  X  and a regu-larization parameter (radius) r to determine an appropriate hypothesis space I ( B r ).
 F or learning, one wants to find a function f : X  X  Y in I ( B r ) that provides the smallest expected (square) error Due to the compactness of I ( B r ), there is a function f minimizing the expected error E ( f ) over I ( B r ) [27], We call f  X  the expected optimal hypothesis in I ( B r ) and E ( f  X  ) the expected optimal error in I ( B r ).

S ince the probability distribution  X  generating the data is unknown, we cannot directly evaluate the error E , nor find f . The only available option is to employ the empirical error of f based on the available data S We denote f  X  S as the minimizer of E S ( f ) in I ( B r ), i.e., We call f  X  S the empirical optimal hypothesis in I ( B r E ( f  X  S ) the empirical optimal error in I ( B r ).
W e further introduce the notion of covering number. For a compact set D in a metric space and  X  &gt; 0, the covering number N ( D ,  X  ) is defined to be the minimal integer m such that there exist m disks with radius  X  covering D [34].
Using the above notions, we prove the following theorem.
Th e compactness of the hypothesis space is required for the existence of the optimizer in the hypothesis space [10].
The orem 1. Assume that, for all f  X  I ( B r ) , | f ( x ) y |  X  M almost everywhere. Then, for all  X  &gt; 0 , Th eorem 1 establishes the theoretical foundation of our model selection criterion, whose proof is given in Appendix A.
Setting the failure probability  X  = N in Theorem 1, we can prove that if l satisfies it is ensured that
From the above theoretical results, we know that the fail-ure probability  X  and the sample complexity are closely re-lated to the covering number of I ( B r ). If the covering number of I ( B r ) is small, the failure probability  X  and the sample complexity will be both small. On the other hand, we can minimize the expected optimal error E ( f  X  ) by min-imizing the empirical optimal error E S ( f  X  S ), since E bounded by E S ( f  X  S ) as shown in Theorem 1. However, the requirement that covering number of I ( B r ) is small is con-flict with the requirement of the small empirical optimal error in I ( B r ). This is because, for different kernels  X  and radiuses r , if the covering number of I ( B r ) is small, the ca-pacity of I ( B r ) will be small, and hence the empirical error of f  X  S in I ( B r ) may be large, that is, E S ( f  X  S ) in I ( B large. For model selection problem, one wishes to select an appropriate hypothesis space I ( B r ), which best trades off these two requirements.
In this section, we give the definition of our model selec-tion criterion.

Following the discussions in Section 2, one wishes to select a hypothesis space I ( B r ) by considering both the empirical optimal error in I ( B r ) and the covering number of I ( B Therefore, the model selection criterion is defined as where  X  is a regularization parameter, which tunes the trade-off between the empirical optimal error in I ( B r ) and the covering number of I ( B r ). Here we denote the model selec-tion criterion M (  X  ) as a functional of the kernel  X  and take the radius r as a parameter in M (  X  ). In most cases, the random noises. For example,  X  y i = y i +  X  i for 1  X  i  X  y = ( y 1 , . . . , y l ) T are the true outputs and  X  i for 1 are independent random variables with mean 0 and variance  X  . Due to the randomness of  X  i for 1  X  i  X  l ,  X  y and E are random variables. Therefore, we take the expectation of E ( f  X  S ) with respect to  X  y in M (  X  ). The detailed represen-tation of E ( E S ( f  X  S )) will be given in next section.
Four characteristics of the proposed criterion listed below make it different from the existing criteria.
In this section, we will represent M (  X  ) as a functional of the kernel matrix K .
Optimization problem (5) conforms to Ivanov regulariza-tion. According to the equivalence between Ivanov and Tikhonov regularization [33] and the representer theorem [17], the solution f  X  S to the problem (5) can be written as posed linear system where  X  &gt; 0 is the regularization parameter, which is related to the radius parameter r [13], and I is the identity matrix.
We let f  X  S = ( f  X  S ( x 1 ) , . . . , f  X  S ( x l )) T obtain f  X  S = K c . As shown in (10), c = ( K +  X  I )  X  1 denote K = K +  X  I , we have which implies Substituting (11) into (4), we obtain Writing K  X  2 = [ a ij ] l i;j =1 , we have For each 1  X  i  X  l , we have and for each i  X  = j  X  X  1 , 2 , . . . , l } , S ubstituting (14) and (15) into (13) yields where  X  X  X  X   X  is the trace norm. Equation (16) can be used both in classification and regression problems. Since we em-ploy the square loss, in the learning stage we will adopt LSSVM for classification and KRR for regression.
In this section, we prove an upper bound of the covering number of I ( B r ). For model selection purpose, we can estimate the covering number of I ( B r ) using the terms that are related to  X  and r and omitting the constants in the bound.

The following theorem shows the upper bound of the cov-ering number, whose proof is given in Appendix B.
Theorem 2. Assume that, for all f  X  I ( B r ) , | f ( x ) y |  X  M almost everywhere. For any 0 &lt;  X   X  12 M r and S = { ( x i , y i ) } l i =1  X  ( X  X Y ) l , there holds where  X  max denotes the maximal element in K .
 For given sample S and fixed  X  , the covering number of I ( B r ) depends only on the choice of kernel function  X  and radius parameter r . We can minimize N timating the covering number of I ( B r ), we will omit the number of samples l in (17), since the covering number of I ( B r ) is intrinsically independent of the sample data [10].
Now we can give the representation of M (  X  ) in kernel matrix form.

Substituting E [ E S ( f  X  S )] given in (16) and the estimation r X  we can obtain M ( K ) =  X  2 wh ere  X  =  X r ,  X  i is the i -th eigenvalue of K and A ( K ), B ( K ), C ( K ) respectively denote the three parts of the crite-rion facilitating the following analysis.

For given  X  and  X  , if a prescribed kernel set K is given, the optimal kernel can be selected as There are three cases for the choice of the kernel set K : (i) K includes a fixed type of kernel that has finite candidate parameters; (ii) K includes a fixed type of kernel that has continuous candidate parameters; (iii) K is defined as a set of non-negative combinations of base kernels [7, 20]. Our criterion can be applied to these three cases. In this paper, we concentrate on the definition, representation and compu-tation of the criterion, so we only consider the first case and leave the latter two as future work.

For the criterion M ( K ), computing A ( K ) demands com-puting the inverse of K and computing B ( K ) and C ( K ) requires calculating all the eigenvalues of K . Therefore, the computational complexity of M ( K ) is O ( l 3 ).
As pointed out in [6, 5], a model selection criterion is not required to be an unbiased estimate of the generaliza-tion error, instead the primary requirement is merely for the minimum of the model selection criterion to provide a reli-able indication of the minimum of the generalization error. Therefore, we argue that it is sufficient to calculate an ap-proximate criterion that can discriminate the optimal model from the candidates.

Since the main computational cost of M ( K ) is from cal-culating the inverse and eigenvalues of the kernel matrix, we consider to approximate the kernel matrix by a  X  X ice X  ma-trix with a significantly lower computational cost when cal-culating its inverse and eigenvalues. The multilevel circulant matrix (MCM) is a good choice since its built-in periodicity allows the multi-dimensional fast Fourier transform (mFFT) to be utilized in calculating its inverse and eigenvalues with complexity of O ( l log( l )) [29, 28].
In this section, we first construct a MCM to approximate the kernel matrix K , and then develop an efficient algorithm to approximately compute the proposed criterion. We first review the notion of multilevel matrices [31]. Let N denote the set of positive integers. For m  X  N , let [ m ] = { 0 , 1 , . . . , m  X  1 } . For a fixed positive integer p , let m = ( m 0 , m 1 , . . . , m p  X  1 )  X  N p . We set
 X  m = m 0 m 1 . . . m p  X  1 (continued product) , [ m ] = [ m 0 ]  X  [ m 1 ]  X  X  X  X  X  [ m p  X  1 ] (Cartesian product) .
A multilevel matrix is defined recursively. According to [31], a matrix A m is called a p -level matrix of level order m if it consists of m 0  X  m 0 blocks and each block is a ( p entries of the multilevel matrix A m , we use multi-indices. For any i = ( i 0 , . . . , i p  X  1 ) , j = ( j 0 , . . . , j where ( i s , j s ) , s  X  [ p ] is the location at level s .
In the following, we mainly consider the kernels  X  defined by radial basis functions (RBFs), such as Gaussian RBF. Hence,  X  max = 1. It is assumed that there exists a real-valued function k  X  L 1 ( R ) on X such that  X  ( s, t ) = k ( t  X  2 ) for all s, t  X  X  , where  X  X  X  X  2 is the Euclidean norm. k is always an even function, since  X  ( t, s ) =  X  ( s, t ) for s, t
To facilitate the analysis, we rewrite the kernel matrix K in multilevel notation. For a given m  X  N p , we assume the number of elements in S is  X  m , that is, |S| = l =  X  m . We relabel the elements in S using multi-index, I n this notation, we rewrite K as
We now describe the definition of multilevel circulant ma-trices. We begin with the definition of circulant matrices [14]. A circulant matrix C is a matrix having the form where each column is a cyclic shift of its left column. The ( i, j ) entry of C , c i;j , satisfies c i;j = c i  X  j (mod m ) determined by its first column, so we write
A block circulant matrix of type ( m, t ) is an mt  X  mt matrix of the form  X  where each block A i , i  X  [ m ] is a t  X  t matrix.
A multilevel circulant matrix (MCM) [31] is defined re-cursively. A 1-level circulant matrix is an ordinary circulant matrix. For any positive integer s , an ( s + 1)-level circulant matrix is a block circulant matrix whose blocks are s -level circulant matrices. According to [31], for m  X  N p , is a p -level circulant matrix if, for any i , j  X  [ m ], A . We write where a i = a i ; 0 , for i  X  [ m ].

We now construct a multilevel circulant matrix U m which approximates the given kernel matrix K m [29, 28]. For an m  X  N p , we choose a sequence of positive numbers h m = ( h m ; 0 , . . . , h m ;p  X  1 )  X  R p . For i  X  [ m ], define
For i  X  [ m ] and s  X  [ p ], we introduce the sets D i ;s if i s = 0, and D i ;s = { i s , m s  X  i s } if 1  X  i s  X  m s  X  and The complexity of calculating [ u i : i  X  [ m ]] is O ( p 2 [12], where p is the number of levels of U m and usually set to be 2 or 3.

It has been analyzed that the computational complexity of
M ( K m ) is O ( X  3 tational cost, we will use U m instead of K m to calculate the criterion M ( U m ) in the following section.
We first introduce two lemmas about the eigenvectors and eigenvalues of a multilevel circulant matrix [31].
Lemma 1. Suppose that A m is a multilevel matrix of or-der m and a = [ a i : i  X  [ m ]] is the first column of A Then A m is a p -level circulant matrix of order m if and only if where the superscript H denotes conjugate transpose and  X  = F m 0  X  F m 1  X  X  X  X  X  X  F m p 1 where  X  denotes the Kro-necker product of matrices and F m = m  X  N with i the imaginary unit.
 Lemma 2. The eigenvalues of a p -level circulant matrix A m = circ m [ a i : i  X  [ m ]] are given by
F rom Lemma 1, we find that  X  is the Kronecker product of the Fourier matrices, which means that for any vector x = [ x i : i  X  [ m ]],  X  x comprises the multi-dimensional discrete Fourier transform (mDFT) of x . That is, we can calculate  X  x by a multi-dimensional fast Fourier transform (mFFT). According to Lemma 2, the eigenvalues of a mul-tilevel circulant matrix can be calculated using the mFFT of its first column.

From Lemma 1, the multilevel circulant matrix U m can be represented as where It follows that
Now we present an algorithm for calculating M ( U m ), shown in Algorithm 1. The computational complexity of Algorithm 1 is given in Theorem 3.

Theorem 3. The computational complexity of Algorithm 1 is O ( X  m log( X  m )) .
 Proof. The computational complexity of the mFFT is O ( l log( l )), where l is the total number of data points. It follows that the computational complexity of step 1 and 2 in Algorithm 1 is O ( X  m log( X  m )), since each of them is the mFFT of  X  m data points. The complexity of step 3 is O ( X  m ), since the products of vectors with a diagonal ma-trix are needed. The complexity of step 4 and 5 is O ( X  m Therefore, the total complexity is O ( X  m log( X  m )).
W e further study how the MCM approximation of kernel matrix impacts the model selection criterion and whether this impact can be ignored for large enough samples. To this end, we prove the following theorem. The proof sketch is given in Appendix C. Al gorithm 1: Computation of M ( U m ) Re quire: y = [ y i : i  X  [ m ]], [ u i : i  X  [ m ]],  X  ,  X  ,  X  ;
Ensure: M ( U m );
The orem 4. If the following assumptions: (H1) there exist positive constants c 0 and  X  such that for (H2) there exists a positive constant h such that h m ;j  X  (H3) there exist positive constants  X  1 and c 1 such that (H4) there exist positive constants  X  2 and c 2 such that for hold, then where m  X  X  X  means all of its components go to infinity. Theorem 4 shows that, for large enough samples, the ap-proximate model selection criterion M ( U m ) is consistent with the accurate criterion M ( K m ), that is, by minimizing the approximate criterion M ( U m ), we could also obtain the optimal model as the accurate criterion M ( K m ) does.
In this section, we empirically evaluate the performance of the proposed criterion and verify the consistency between the approximate and accurate criterion.

The benchmark data sets adopted in our experiments are introduced in [25] and widely used in model selection [6, 4, 5], as shown in Table 1. For each data set, there are 100 random training and test pre-defined partitions 2 (except 20 for image and splice). The use of multiple benchmarks means that the evaluation is more robust as the selection of data sets that provide a good match to the inductive bias of a particular classifier becomes less likely. Likewise, the use of multiple partitions provides robustness against sensitivity to the sampling of data to form training and test sets. h ttp://theoval.cmp.uea.ac.uk/matlab/
I n R  X  atsch X  X  experiment [25], model selection is performed on the first five training sets of each data set. The median values of the parameters over these five sets are then de-termined and subsequently used to evaluate the error rates throughout all 100 partitions. However, for this experimen-tal scheme, some of the test data is no longer statistically  X  X ure X  since it has been used during model selection. Fur-thermore, the use of median of the parameters would intro-duce an optimistic bias [5]. In our experiments, we perform model selection on the training set of each partition, then train the classifier with the obtained optimal parameters on the same training set, and finally evaluate the classifier on the corresponding test set. Therefore, we can obtain 100 test errors for each data set (except 20 for image and splice). The mean test error will be used to evaluate the performance of the criterion. This experimental scheme is rigorous and can avoid the major flaws of the previous one [5].

In the first experiment, we compare our criterion M ( K ) with six popular model selection criteria including: 1. kernel target alignment (KTA) [9]; 2. centered KTA (CKTA) [8]; 3. feature space-based kernel matrix evaluation (FSM) [24] 4. 5-fold cross validation (5-CV); 5. leave-one-out (LOO); 6. LOO with Bayesian regularisation (LOOBR) [4]. We use Gaussian kernels K ( x , x  X  ) = exp candidate kernels for  X   X  { 2 i , i =  X  6 ,  X  5 , . . . , 4 , 5 parameter  X   X  { 10 i , i =  X  4 , . . . , 2 } and the parameter  X  { 10 i , i =  X  6 , . . . , 2 } . The noises  X  i for 1  X  i  X  distributed with mean 0 and standard deviation  X  = 0 . 01.
For each model selection criterion and each training set, we chose the optimal kernel parameter  X  for each fixed  X  , and then we evaluated the test errors for the chosen parameters. The mean test errors over different data sets are plotted in Figure 1. From Figure 1, we can find that our criterion can obtain the optimal or near optimal mean test errors for all data sets, as compared with all popular model selection criteria. These results verify the feasibility of our criterion.
In the second experiment, we explore the effect of the regularization parameters  X  and  X  on M ( K ). The mean test errors of M ( K ) with different  X  and  X  are given in Figure 2. We find that the optimal  X  belong to [10  X  3 , 10 and  X  = 10  X  2 is a good choice. The optimal  X  belong to [10  X  6 , 10  X  3 ] and  X  = 10  X  4 is a good choice. In addition,  X  has more influence on M ( K ) than  X  for some data sets.
In the third experiment, we verify the consistency between the approximate criterion M ( U m ) and accurate criterion M ( K m ), which has been proved in Theorem 4.

To satisfy the assumption (H4) in Theorem 4, we con-struct synthetic data sets. The target function is f ( x ) = sin( x ) + 1 2 sin(3 x ). The sampled inputs are { x j , j Fi gure 1: Comparison of mean test errors between our criterion M ( K ) and other ones including kernel target alignment (KTA), centered KTA (CKTA), feature space-based kernel matrix evaluation (FSM), 5-fold cross validation (5-CV), leave-one-out (LOO) and LOO with Bayesian regularisation (LOOBR). Fi gure 2: The effect of the regularization parame-ters  X  and  X  on M ( K ) . The mean test errors of M ( K ) with different  X  and  X  are plotted. For each training set, each pair of  X  and  X  , we chose the kernel param-eter  X  by M ( K ) , and then we evaluate the test error for the chosen parameter. { x { x j , j  X  [ m ] } is centered at 0 with fixed difference 0.1 of any two successive numbers, and y j = f ( x j )+  X  , where the noise  X  is normally distributed with mean 0 and standard devia-tion 0.01.

For different sample sizes  X  m , we plot the values of crite-ria M ( K m ) and M ( U m ) with respect to kernel parameters  X   X  [2  X  14 , 2 15 ] in Figure 3. We find that as the sample size increases, two curves of the approximate and accurate crite-ria become close and the optimal kernel parameters of differ-ent criteria tend towards the same value. These results mean that by minimizing the approximate criterion M ( U m ), we could also obtain the near optimal parameter as the accu-rate criterion M ( K m ) does. In addition, we also compare the computation time of the accurate and approximate cri-teria for different sample sizes. The results are shown in Figure 4. We can observe that the larger the sample size is, the more obvious the efficiency gain of M ( U m ) is, which supports the result in Theorem 3.
In this paper, we proposed a brand new criterion for model selection of kernel methods via introducing the covering num-ber of the ball of RKHS. By estimating the covering number, we represented our criterion as a functional of the kernel ma-trix. We developed an approximation algorithm to calculate the functional, which achieves a quasi-linear computational complexity. We also proved the consistency between the approximate criterion and the accurate one. It is worth not-ing that the approximation strategy by using the multilevel circulant matrix could also be applied to other model selec-tion criteria in kernel matrix form. By empirical studies, we evaluated the performance of our criterion and verified the consistency.

In near future, we will apply our theoretical results to dif-ferent kernel sets K including kernels with continuous can-didate parameters and non-negative combinations of base kernels. Besides, the application of our approximate model selection strategy to real large problems will also be one of major concerns.
 The work is supported in part by the National Natural Sci-ence Foundation of China under grant No. 61170019. Since I ( B r ) is a compact and convex subset of C ( X ), we give the following corollary by Theorem C* of [10].
Corollary 1. Assume that, for all f  X  I ( B r ) , | f ( x ) y |  X  M almost everywhere. Then, for all  X  &gt; 0 ,
Acc ording to the definition of expected error E and em-pirical error E S , we have Let  X  = N probability at least 1  X   X  , Then with probability at least 1  X   X  , which completes the proof.
 The proof idea is similar to that of Theorem 1 in [34]. For x  X  X , let  X  x =  X  ( x,  X  ). Denote X l = { x 1 , . . . , x X l satisfying u i  X  span {  X  x j } l j =1 and In [35], it has been proved that the nodal functions are uniquely given by
Let f  X  B r . Then  X  f  X  H  X  r . According to the repro-ducing property (1), for x  X  X  , where  X  max denotes the maximal element in K . Further by the reproducing property, we have The Cauchy-Schwarz inequality gives where  X  ( x ) equals to (  X  ( x, x )  X  2 Deno ting  X  = sup x  X  X   X  ( x ), we obtain
In the following, we estimate the covering number. If  X  2, then for any f  X  B r , we can find d = ( f ( x i )) l i =1 According to (28),  X  d  X  2  X  space E with dim E = m , the covering number of B R satisfies N ( B R ,  X  )  X  (4 R/ X  ) m [10] . Consider the space R l . Take Fi gure 4: Comparison of computation time between M ( K m ) and M ( U m ) for different sample sizes. Then there are sequences { c ( n ) , n = 1 , . . . ,  X  (4 R/ X  ) such that for any d  X  R l with  X  d  X  2  X  R , we can we can find some n satisfying  X  d  X  c ( n )  X  2  X   X  . According to (27), for any given x  X  X  , Then  X  Co mbining this inequality with the inequality (29), we have Th en, I ( B r ) is covered by balls with centers and radius " 24 M . Therefore N ( I ( B r ) , " 24 M )  X  (4 R / X  ) replacing  X  K  X  1  X  2 with  X  K  X  1  X   X  , we can obtain To prove Theorem 4, we first define the following sets of matrices for m  X  N p If the assumptions (H1), (H2), (H3) and (H4) in Theorem 4 hold, we say the matrix sets K and U are asymptotically equivalent [28]. Then we have the following theorem [28]. We denote K m ; = K m +  X  I m and U m ; = U m +  X  I m .
Theorem 5. If K and U are asymptotically equivalent, there exists a positive constant c such that for any m  X  N p where m min = min { m s : s  X  [ p ] } . If in addition, there exist positive constants c 3 and r 1 such that for any m  X  N p and i  X  [ m ] , | y i |  X  c 3 e  X  r 1 m ( i ) , where  X  m then there exist positive constants c and r such that for any m  X  N p , Under the assumptions (H1) X (H4), we could also prove the asymptotical equivalence between K and U as well as K 2 and . Similar theorems as Theorem 5 could also be established for theses matrix sets. Therefore, we can obtain
Further according to the triangle inequality, we could di-rectly derive Theorem 4 for m  X  X  X  . [1] D. Anguita, A. Ghio, L. Oneto, and S. Ridella. [2] P. Bartlett and S. Mendelson. Rademacher and [3 ] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model [4] G. Cawley and N. Talbot. Preventing over-fitting [5] G. Cawley and N. Talbot. On over-fitting in model [6] O. Chapelle, V. Vapnik, O. Bousquet, and [7] C. Cortes, M. Mohri, and A. Rostamizadeh. L 2 [8] C. Cortes, M. Mohri, and A. Rostamizadeh.
 [9] N. Cristianini and J. Shawe-Taylor. On kernel-target [10] F. Cucker and S. Smale. On the mathematical [11] M. Debruyne, M. Hubert, and J. A. Suykens. Model [12] L. Ding and S. Liao. Approximate model selection for [13] G. Gnecco and M. Sanguineti. Regularization [14] G. Golub and C. Van Loan. Matrix Computations . [15] G. H. Golub, M. Heath, and G. Wahba. Generalized [16] I. Guyon, A. Saffari, G. Dror, and G. Cawley. Model [17] G. S. Kimeldorf and G. Wahba. A correspondence [18] Y. Liu, S. Jiang, and S. Liao. Eigenvalues [19] Y. Liu, S. Jiang, and S. Liao. Efficient approximation [20] Y. Liu, S. Liao, and Y. Hou. Learning kernels with [21] A. Luntz and V. Brailovsky. On estimation of [22] U. v. Luxburg, O. Bousquet, and B. Sch  X  olkopf. A [23] C. Micchelli and M. Pontil. Learning the kernel [24] C. H. Nguyen and T. B. Ho. Kernel matrix evaluation. [25] G. R  X  atsch, T. Onoda, and K. M  X  uller. Soft margins for [26] C. Saunders, A. Gammerman, and V. Vovk. Ridge [27] S. Smale and D. Zhou. Estimating the approximation [28] G. Song. Approximation of kernel matrices in machine [29] G. Song and Y. Xu. Approximation of [30] J. Suykens and J. Vandewalle. Least squares support [31] E. E. Tyrtyshnikov. A unifying approach to some old [32] V. N. Vapnik. Statistical Learning Theory . John Wiley [33] V. V. Vasin. Relationship of several variational [34] D.-X. Zhou. The covering number in learning theory. [35] D.-X. Zhou. Capacity of reproducing kernel spaces in
