 Case structure (predicate-argument structure or log-ical form) represents what arguments are related to a predicate, and forms a basic unit for conveying the meaning of natural language text. Identifying such case structure plays an important role in natural lan-guage understanding.

In English, syntactic case structure can be mostly derived from word order. For example, the left ar-gument of the predicate is the subject, and the right argument of the predicate is the object in most cases. Blaheta and Charniak proposed a statistical method for analyzing function tags in Penn Treebank, and achieved a really high accuracy of 95.7% for syn-tactic roles, such as SBJ (subject) and DTV (da-tive) (Blaheta and Charniak, 2000). In recent years, there have been many studies on semantic structure analysis (semantic role labeling) based on PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). These studies classify syntactic roles into se-mantic ones such as agent, experiencer and instru-ment.

Case structure analysis of Japanese is very differ-ent from that of English. In Japanese, postpositions are used to mark cases. Frequently used postposi-tions are  X  ga  X ,  X  wo  X  and  X  ni  X , which usually mean nominative, accusative and dative. However, when an argument is followed by the topic-marking post-position  X  wa  X , its case marker is hidden. In addi-tion, case-marking postpositions are often omitted in Japanese. These troublesome characteristics make Japanese case structure analysis very difficult.
To address these problems and realize Japanese case structure analysis, wide-coverage case frames are required. For example, let us describe how to apply case structure analysis to the following sen-tence: In this sentence, taberu (eat) is a verb, and bentou-wa (lunchbox-TM ) is a case component (i.e. argu-ment) of taberu . The case marker of  X  bentou-wa  X  is hidden by the topic marker ( TM )  X  wa  X . The an-alyzer matches  X  bentou  X  (lunchbox) with the most suitable case slot (CS) in the following case frame of  X  taberu  X  (eat).
 Since  X  bentou  X  (lunchbox) is included in  X  wo  X  ex-amples, its case is analyzed as  X  wo  X . As a result, we obtain the case structure  X   X  : ga bentou : wo taberu  X , which means that  X  ga  X  (nominative) argument is omitted, and  X  wo  X  (accusative) argument is  X  bentou  X  (lunchbox). In this paper, we run such case structure analysis based on example-based case frames that are constructed from a huge raw corpus in an unsu-pervised manner.

Let us consider syntactic analysis, into which our method of case structure analysis is integrated. Re-cently, many accurate statistical parsers have been proposed (e.g., (Collins, 1999; Charniak, 2000) for English, (Uchimoto et al., 2000; Kudo and Mat-sumoto, 2002) for Japanese). Since they somehow use lexical information in the tagged corpus, they are called  X  X exicalized parsers X . On the other hand, un-lexicalized parsers achieved an almost equivalent ac-curacy to such lexicalized parsers (Klein and Man-ning, 2003; Kurohashi and Nagao, 1994). Accord-ingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem. Bikel also indicated that Collins X  parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on just phrasal and part-of-speech categories (Bikel, 2004).
This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized proba-bilistic model for Japanese syntactic and case struc-ture analysis. Lexical information is extracted not from a small tagged corpus, but from a huge raw cor-pus as case frames. This model performs case struc-ture analysis by a generative probabilistic model based on the case frames, and selects the syntactic structure that has the highest case structure proba-bility. We employ automatically constructed case frames (Kawahara and Kurohashi, 2002) for our model of Table 1: Case frame examples (examples are ex-pressed only in English for space limitation.). case structure analysis. This section outlines the method for constructing the case frames.

A large corpus is automatically parsed, and case frames are constructed from modifier-head exam-ples in the resulting parses. The problems of auto-matic case frame construction are syntactic and se-mantic ambiguities. That is to say, the parsing re-sults inevitably contain errors, and verb senses are intrinsically ambiguous. To cope with these prob-lems, case frames are gradually constructed from re-liable modifier-head examples.

First, modifier-head examples that have no syn-tactic ambiguity are extracted, and they are dis-ambiguated by a couple of a verb and its closest case component. Such couples are explicitly ex-pressed on the surface of text, and can be consid-ered to play an important role in sentence mean-ings. For instance, examples are distinguished not by verbs (e.g.,  X  tsumu  X  (load/accumulate)), but by couples (e.g.,  X  nimotsu-wo tsumu  X  (load baggage) and  X  keiken-wo tsumu  X  (accumulate experience)). Modifier-head examples are aggregated in this way, and yield basic case frames.

Thereafter, the basic case frames are clustered to merge similar case frames. For example, since  X  nimotsu-wo tsumu  X  (load baggage) and  X  busshi-wo tsumu  X  (load supply) are similar, they are clustered. The similarity is measured using a thesaurus (Ike-hara et al., 1997).

Using this gradual procedure, we constructed case frames from the web corpus (Kawahara and Kuro-hashi, 2006). The case frames were obtained from approximately 470M sentences extracted from the web. They consisted of 90,000 verbs, and the aver-age number of case frames for a verb was 34.3.
In Figure 1, some examples of the resulting case frames are shown. In this table,  X  X S X  means a case slot. &lt; agent &gt; in the table is a generalized example, which is given to the case slot where half of the ex-amples belong to &lt; agent &gt; in a thesaurus (Ikehara et al., 1997). &lt; agent &gt; is also given to  X  ga  X  case slot that has no examples, because  X  ga  X  case com-ponents are usually agentive and often omitted. The proposed method gives a probability to each possible syntactic structure T and case structure L of the input sentence S , and outputs the syntactic and case structure that have the highest probability. That is to say, the system selects the syntactic struc-ture T best and the case structure L best that maximize the probability P ( T, L | S ) : The last equation is derived because P ( S ) is con-stant. 3.1 Generative Model for Syntactic and Case We propose a generative probabilistic model based on the dependency formalism. This model considers a clause as a unit of generation, and generates the input sentence from the end of the sentence in turn. P ( T, L, S ) is defined as the product of a probability for generating a clause C i as follows:
P ( T, L, S ) = Y where n is the number of clauses in S , and b h modifying bunsetsu 1 . The main clause C n at the end
Figure 1: An Example of Probability Calculation. of a sentence does not have a modifying head, but we handle it by assuming b h tence).
 For example, consider the sentence in Figure 1. There are two possible dependency structures, and for each structure the product of probabilities indi-cated below of the tree is calculated. Finally, the model chooses the highest-probability structure (in this case the left one).

C i is decomposed into its predicate type f i (in-cluding the predicate X  X  inflection) and the rest case structure CS i . This means that the predicate in-cluded in CS i is lemmatized. Bunsetsu b h decomposed into the content part w h f
P ( C i | b h The last equation is derived because the content part in CS i is independent of the type of its modifying head ( f h dent of the content part of its modifying head ( w h
For example, P ( bentou-wa tabete | syuppatsu-shita ) is calculated as follows:
We call P ( CS i | f i , w h structure and P ( f i | f h cate type . The following two sections describe these models. 3.2 Generative Model for Case Structure We propose a generative probabilistic model of case structure. This model selects a case frame that
Figure 2: An example of case assignment CA k . matches the input case components, and makes cor-respondences between input case components and case slots.

A case structure CS i consists of a predicate v i , a case frame CF l and a case assignment CA k . Case assignment CA k represents correspondences between input case components and case slots as shown in Figure 2. Note that there are various pos-sibilities of case assignment in addition to that of Figure 2, such as corresponding  X  bentou  X  (lunch-box) with  X  ga  X  case. Accordingly, the index k of CA k ranges up to the number of possible case as-signments. By splitting CS i into v i , CF l and CA k , P ( CS i | f i , w h
P ( CS i | f i , w h
The above approximation is given because it is natural to consider that the predicate v i depends on its modifying head w h depends on the predicate v i , and that the case assign-ment CA k depends on the case frame CF l and the predicate type f i .

The probabilities P ( v i | w h estimated from case structure analysis results of a large raw corpus. The remainder of this section il-lustrates P ( CA k | CF l , f i ) in detail. 3.2.1 Generative Probability of Case
Let us consider case assignment CA k for each case slot s j in case frame CF l . P ( CA k | CF l , f i ) can be decomposed into the following product de-pending on whether a case slot s j is filled with an input case component (content part n j and type f j ) or vacant: P ( CA k | CF l , f i ) = where the function A ( s j ) returns 1 if a case slot s j is filled with an input case component; otherwise 0.
P ( A ( s j ) = 1 | CF l , f i , s j ) and P ( A ( s j ) = 0 | CF l , f i , s j ) in equation (5) can be rewritten as P ( A ( s j ) = 1 | CF l , s j ) and P ( A ( s j ) = 0 | CF because the evaluation of case slot assignment de-pends only on the case frame. We call these proba-bilities generative probability of a case slot , and they are estimated from case structure analysis results of a large corpus.

Let us calculate P ( CS i | f i , w h ample in Figure 1. In the sentence,  X  wa  X  is a topic marking ( TM ) postposition, and hides the case marker. The generative probability of case structure varies depending on the case slot to which the topic marked phrase is assigned. For example, when a case frame of  X  taberu  X  (eat) CF taberu1 with  X  ga  X  and  X  wo  X  case slots is used, P ( CS ( bentou-wa taberu ) | te , syuppatsu-suru ) is calculated as follows: P 1 ( CS ( bentou-wa taberu ) | te , syuppatsu-suru ) = P 2 ( CS ( bentou-wa taberu ) | te , syupatsu-suru ) = Such probabilities are computed for each case frame of  X  taberu  X  (eat), and the case frame and its cor-responding case assignment that have the highest probability are selected.

We describe the generative probability of a case component P ( n j , f j | CF l , f i , A ( s j ) = 1 , s j 3.2.2 Generative Probability of Case
We approximate the generative probability of a case component, assuming that:  X  a generative probability of content part n j is in- X  and the interpretation of the surface case in-Taking into account these assumptions, the genera-tive probability of a case component is approximated as follows: P ( n j , f j | CF l , f i , A ( s j ) = 1 , s j )  X 
P ( n j | CF l , A ( s j ) = 1 , s j ) is the probability of generating a content part n j from a case slot s j in a case frame CF l . This probability is estimated from case frames.

Let us consider P ( f j | s j , f i ) in equation (8). This is the probability of generating the type f j of a case component that has a correspondence with the case slot s j . Since the type f j consists of a surface case c 2 , a punctuation mark (comma) p j and a topic (using the chain rule):
P ( f j | s j , f i ) = P ( c j , t j , p j | s j , f i ) This approximation is given by assuming that c j only depends on s j , p j only depends on f j , and t j depends on f j and p j . P ( c j | s j ) is estimated from the Kyoto Text Corpus (Kawahara et al., 2002), in which the relationship between a surface case marker and a case slot is annotated by hand.

In Japanese, a punctuation mark and a topic marker are likely to be used when their belong-ing bunsetsu has a long distance dependency. By considering such tendency, f i can be regarded as ( o i , u i ) , where o i means whether a dependent bun-setsu gets over another head candidate before its modifying head v i , and u i means a clause type of v . The value of o i is binary, and u i is one of the clause types described in (Kawahara and Kurohashi, 1999).

P ( t j | f i , p j ) = P ( t j | o i , u i , p j ) (11) 3.3 Generative Model for Predicate Type Now, consider P ( f i | f h the probability of generating the predicate type of a clause C i that modifies b h depending on the type of b h
When b h dinate clause embedded in the clause of b h the types f i and f h tuation marks ( p i , p h To capture a long distance dependency indicated by punctuation marks, o h head candidate before b h
When b h clause in b h tuation mark of the modifiee do not affect the prob-ability.
 Table 3: Experimental results for syntactic analysis. We evaluated the syntactic structure and case struc-ture outputted by our model. Each parameter is es-timated using maximum likelihood from the data described in Table 2. All of these data are not existing or obtainable by a single process, but ac-quired by applying syntactic analysis, case frame construction and case structure analysis in turn. The process of case structure analysis in this table is a similarity-based method (Kawahara and Kurohashi, 2002). The case frames were automatically con-structed from the web corpus comprising 470M sen-tences, and the case structure analysis results were obtained from 6M sentences in the web corpus.
The rest of this section first describes the exper-iments for syntactic structure, and then reports the experiments for case structure. 4.1 Experiments for Syntactic Structure We evaluated syntactic structures analyzed by the proposed model. Our experiments were run on hand-annotated 675 web sentences 3 . The web sen-tences were manually annotated using the same cri-teria as the Kyoto Text Corpus. The system input was tagged automatically using the JUMAN mor-phological analyzer (Kurohashi et al., 1994). The syntactic structures obtained were evaluated with re-gard to dependency accuracy  X  the proportion of correct dependencies out of all dependencies except for the last dependency in the sentence end 4 .
Table 3 shows the dependency accuracy. In the table,  X  X aseline X  means the rule-based syn-tactic parser, KNP (Kurohashi and Nagao, 1994), and  X  X roposed X  represents the proposed method. The proposed method significantly outperformed the baseline method (McNemar X  X  test; p &lt; 0 . 05 ). The dependency accuracies are classified into four types according to the bunsetsu classes (VB: verb bun-setsu , NB: noun bunsetsu ) of a dependent and its head. The  X  X B  X  VB X  type is further divided into two types:  X  X M X  and  X  X thers X . The type that is most related to case structure is  X  X thers X  in  X  X B  X  VB X . Its accuracy was improved by 1.6%, and the error rate was reduced by 10.9%. This result indicated that the proposed method is effective in analyzing dependencies related to case structure.

Figure 3 shows some analysis results, where the dotted lines represent the analysis by the baseline method, and the solid lines represent the analysis by the proposed method. Sentence (1) and (2) are in-correctly analyzed by the baseline but correctly ana-lyzed by the proposed method.

There are two major causes that led to analysis errors.
 Mismatch between analysis results and annota-tion criteria
In sentence (3) in Figure 3, the baseline method correctly recognized the head of  X  iin-wa  X  (commissioner-TM ) as  X  hirakimasu  X  (open). How-ever, the proposed method incorrectly judged it as  X  oujite-imasuga  X  (offer). Both analysis results can be considered to be correct semantically, but from Table 4: Experimental results for case structure anal-ysis. the viewpoint of our annotation criteria, the latter is not a syntactic relation, but an ellipsis relation. To address this problem, it is necessary to simultane-ously evaluate not only syntactic relations but also indirect relations, such as ellipses and anaphora. Linear weighting on each probability
We proposed a generative probabilistic model, and thus cannot optimize the weight of each proba-bility. Such optimization could be a way to improve the system performance. In the future, we plan to employ a machine learning technique for the opti-mization. 4.2 Experiments for Case Structure We applied case structure analysis to 215 web sen-tences which are manually annotated with case structure, and evaluated case markers of TM phrases and clausal modifiees by comparing them with the gold standard in the corpus. The experimental re-sults are shown in table 4, in which the baseline refers to a similarity-based method (Kawahara and Kurohashi, 2002). The experimental results were re-ally good compared to the baseline. It is difficult to compare the results with the previous work stated in the next section, because of different experimental settings (e.g., our evaluation includes parse errors in incorrect cases). There have been several approaches for syntactic analysis handling lexical preference on a large scale. Shirai et al. proposed a PGLR-based syntactic analysis method using large-scale lexical preference (Shirai et al., 1998). Their system learned lexical preference from a large newspaper corpus (articles of five years), such as P ( pie | wo , taberu ) , but did not deal with verb sense ambiguity. They reported 84.34% accuracy on 500 relatively short sentences from the Kyoto Text Corpus.

Fujio and Matsumoto presented a syntactic anal-ysis method based on lexical statistics (Fujio and Matsumoto, 1998). They made use of a probabilistic model defined by the product of a probability of hav-ing a dependency between two cooccurring words and a distance probability. The model was trained on the EDR corpus, and performed with 86.89% ac-curacy on 10,000 sentences from the EDR corpus 5 .
On the other hand, there have been a number of machine learning-based approaches using lexical preference as their features. Among these, Kudo and Matsumoto yielded the best performance (Kudo and Matsumoto, 2002). They proposed a chunking-based dependency analysis method using Support Vector Machines. They used two-fold cross valida-tion on the Kyoto Text Corpus, and achieved 90.46% accuracy 5 . However, it is very hard to learn suffi-cient lexical preference from several tens of thou-sands sentences of a hand-tagged corpus.

There has been some related work analyzing clausal modifiees and TM phrases. For exam-ple, Torisawa analyzed TM phrases using predicate-argument cooccurences and word classifications in-duced by the EM algorithm (Torisawa, 2001). Its accuracy was approximately 88% for  X  wa  X  and 84% for  X  mo  X . It is difficult to compare the accuracy of their system to ours, because the range of tar-get expressions is different. Unlike related work, it is promising to utilize the resultant case frames for subsequent analyzes such as ellipsis or discourse analysis. We have described an integrated probabilistic model for syntactic and case structure analysis. This model takes advantage of lexical selectional preference of large-scale case frames, and performs syntactic and case analysis simultaneously. The experiments indi-cated the effectiveness of our model. In the future, by incorporating ellipsis resolution, we will develop an integrated model of syntactic, case and ellipsis analysis.

