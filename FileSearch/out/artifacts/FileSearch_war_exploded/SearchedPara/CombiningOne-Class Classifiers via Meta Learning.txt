 Selecting the best classifier among the available ones is a dif-ficult task, especially when only instances of one class exist. In this work we examine the notion of combining one-class classifiers as an alternative for selecting the best classifier. In particular, we propose two one-class classification perfor-mance measures to weigh classifiers and show that a simple ensemble that implements these measures can outperform the most popular one-class ensembles. Furthermore, we pro-pose a new one-class ensemble scheme, TUPSO, which uses meta-learning to combine one-class classifiers. Our experi-ments demonstrate the superiority of TUPSO over all other tested ensembles and show that the TUPSO performance is statistically indistinguishable from that of the hypothetical best classifier.
 H.3.3 [ Information Storage and Retrieval ]: Selection process; I.2.6 [ Artificial Intelligence ]: Learning X  Concept learning Ensemble of Classifiers, One-Class Ensemble, Meta Learning
In regular classification tasks we aim to classify an un-known instance into one class from a predefined set of classes. One-class classification aims to differentiate between instances of class of interest and all other instances. The one-class classification task is of particular importance to information retrieval tasks [12]. Consider, for example, trying to identify documents of  X a  X  AIJinterest X a  X  A  X  I to a user, where the only in-formation available is the previous documents that this user has read (i.e. positive examples), yet another example is ci-tation recommendation, in which the system helps authors in selecting the most relevant papers to cite, from a poten-tially overwhelming number of references [1]. Again, one can obtain representative positive examples by simply going over the references, however it would be hard to identify typical negative examples (the fact that a certain paper is not cited by another paper does not necessarily indicate it is irrele-vant). Many one-class classification algorithms have been investigated [2, 16, 9]. While there are plenty of learning al-gorithms to choose from, identifying the one that performs best in relation to the problem at hand is difficult. This is because evaluating a one-class classifier X  X  performance is problematic. By definition, the data collections only con-tain one-class examples and thus, performance metrics, such as false-positive ( F P ), and true negative ( T N ), cannot be computed. In the absence of F P and T N , derived per-formance metrics, such as classification accuracy, precision, among others, cannot be computed. Moreover, prior knowl-edge concerning the classification performance on some pre-vious tasks may not be very useful for a new classification task because classifiers can excel in one dataset and fail in another, i.e., there is no consistent winning algorithm.
This difficulty can be addressed in two ways. The first option is to select the classifier assumed to perform best ac-cording to some heuristic estimate based on the available positive examples (i.e., T P and F N ). The second option is to train an ensemble from the available classifiers. To the best of our knowledge, no previous work on selecting the best classifier in the one-class domain has been published and the only available one-class ensemble technique for di-verse learning algorithms is the fixed-rule ensemble, which in many cases, as we later show, makes more classification errors when compared to a random-selected classifier.
In this paper we search for a new method for combining one-class classifiers. We begin by presenting two heuris-tic methods to evaluate the classification performance of one-class classifiers. We then introduce a simple heuris-tic ensemble that uses these heuristic methods to select a single base-classifier. Later, we present TUPSO, a general meta-learning based ensemble, roughly based on the Stack-ing technique [19] and incorporates the two classification performance evaluators. We then experiment with the dis-cussed ensemble techniques on forty different datasets. The experiments show that TUPSO is by far the best option to use when multiple one-class classifiers exist. Furthermore, we show that TUPSO X  X  classification performance is strongly correlated with that of the actual best ensemble-member.
The main motivation behind the ensemble methodology is to weigh several individual classifiers and combine them to obtain a classifier that outperforms them all. Indeed, previous work in supervised ensemble learning shows that combining classification models produce a better classifier in terms of prediction accuracy [13].

Compared to supervised ensemble learning, progress in the one-class ensemble research field is limited [7]. Specifi-cally, the Fix-rule technique was the only method which was considered for combining one-class classifiers [18, 8, 17]. In this method, the combiner regards each participating clas-sifier X  X  output as a single vote upon which it applies an ag-gregation function (a combining rule), to produce a final classification. In the following few years, further research was carried out and presently there are several applications reaching domains, such as information security (intrusion detection), remote sensing, image retrieval, image segmen-tation, on-line signature verification, and fingerprint match-ing.

Fixed-rule ensemble techniques, however, are not optimal as they use combining rules that are assigned statically and independently of the training data. As a consequence, as we will show later, the fixed rule ensembles produce inferior classification performance in comparison to the best classi-fier in the ensemble.

In the following lines we use the notation P k ( x | ! T c the estimated probability of instance x given the target class !
T c , f r ( T;k ) as the fraction of the target class, which should be accepted for classifier k = 1 : : : R , N as number of fea-tures, and k notates the classification threshold for classifier k . A list of fixed combining rules is presented in Table 1.
In stead of using the fix-rule (e.g., weighting methods), technique to combine one-class classifiers, the meta-learning approach can be used.
Meta-learning is the process of learning from basic clas-sifiers (ensemble members); the inputs of the meta-learner are the outputs of the ensemble-member classifiers. The goal of meta-learning ensembles is to train a meta-model (meta-classifier), which will combine the ensemble members X  predictions into a single prediction. To create such an en-semble, both the ensemble members and the meta-classifier need to be trained. Since the meta-classifier training re-quires already trained ensemble members, these must be trained first. The ensemble members are then used to pro-duce outputs (classifications), from which the meta-level dataset (meta-dataset) is created. The basic building blocks of meta-learning are the meta-features, which are measured properties of the ensemble members output, e.g., the ensem-ble members X  predictions. A vector of meta-features and a classification k comprise a meta-instance, i.e., meta-instance  X  of the meta-instance that is identical to the class of the in-stance used to produce the ensemble members X  predictions. A collection of meta-instances comprises the meta-dataset upon which the meta-classifier is trained.
Traditional classifier evaluation metrics, such as true neg-ative and false positive, cannot be computed in the one-class setup, since only positive examples exist. Consequently, measures, such as a classifier X  X  accuracy, precision, AUC, F-score, and Matthew X a  X  A  X  Zs correlation coefficient (MCC), cannot be computed, since accuracy = ( T P + TN ) = ( T P + TN + FP + F N ), P recision = T P= ( T P + FP ) and F -score = 2  X  P  X  R= ( P + R ), where P is precision and R is recall. Instead of computing the aforementioned metrics, [11] and [10], proposed heuristic methods for estimating, rather than actually measuring, the classifier X  X  accuracy and F-score, respectively. Next, we describe the two performance estimators.
Liu et al. [11] demonstrated that by rewriting the error probability, one can estimate the classification error-rate, in the one-class paradigm, given a prior on the target-class : 0 |
Y = 1] P r [ Y = 1] where f ( x ) is the classifier X  X  classification result for the ex-amined example x , P r [ f ( x ) = 1] is the probability that the examined classifier will classify Positive , P r [ f ( x ) = 0 is the probability that the classifier will classify Negative when given a Positive example, and lastly, P r [ Y = 1] is the prior on the target-class probability.

Naturally, we define the one-class accuracy (OCA), esti-mator as follows: OCA = 1  X  P r [ f ( x )  X  = y ]. Note that the probabilities P r [ f ( x ) = 1] and [ f ( x ) = 0 | Y = 1] can be es-timated for any one-class problem at hand using a standard cross-validating procedure.

An additional performance criteria, r 2 P r [ f ( x )=1] , denoted as One-Class F-score (OCF), is given in [10]. Using this cri-teria, one can estimate the classifier X  X  F-score in the semi-supervise paradigm. However, when only positive-labeled instances exist, the recall, r = P r [ f ( x ) = 1 | y = 1], equals to P r [ f ( x ) = 1] (because P r [ y = 1] = 1), which only mea-sures the fraction of correct classifications on positive test examples, i.e., true-positive rate (TPR). Using the TPR to measure the classification performance makes sense, because the TPR is strongly correlated with the classification accu-racy when negative examples are very rare, such as in the case of most one-class problems.
Using the discussed classification performance estimators, we define a new and very simple ensemble: Estimated Best-Classifier Ensemble (ESBE). This ensemble is comprised of an arbitrary number of one-class ensemble-members (clas-sifiers). During the prediction phase, the ensemble X  X  out-put is determined by a single ensemble-member, denoted as the dominant classi er . The ensemble X  X  dominant member is selected during the training phase. This is achieved by evaluating the performance of the participating ensemble-members using a 5x2 cross-validation procedure, as described in [6]. During this procedure only the training-set X  X  in-stances are used, and the metric used to measure the ensemble-members X  performance is either OCA or OCF. In this section, we introduce the TUPSO ensemble scheme. The main principle of TUPSO is combining multiple and possibly diverse one-class classifiers using the meta-learning technique. TUPSO is roughly based on the Stacking tech-nique, and as so, it uses a single meta-classifier to combine the ensembles X  members. As opposed to Stacking, however, where the meta-classifier trains directly from the ensemble-members X  outputs, TUPSO X  X  meta-classifier trains on a se-ries of aggregations from the ensemble-members X  outputs. To elevate the effectiveness of some of the aggregations used by TUPSO, and with that improve the ensemble X  X  over-all performance, during the training phase, the ensemble-members are evaluated using the aforementioned one-class performance evaluators. The performance estimates are then translated into static weights, which the meta-learning algo-rithm uses during the training of the meta-classifier, and during the prediction phases.

The TUPSO ensemble, as shown in Figure 1, is made up of four major components: (1) Ensemble-members, (2) Performance evaluator, (3) Meta-features extractor, and (4) Meta-classifier. Next, we describe each component. In TUPSO, the ensemble members are one-class, machine-learning-based, classifiers. TUPSO regards its ensemble mem-bers as black boxes, in order to avoid any assumption regard-ing their inducing algorithm, data structures or methods for handling missing values and categorical features. During the ensemble X  X  training phase, the ensemble-members are trained several times, as part of a cross-validation process, which is required for generating the meta-classifier X  X  dataset. This process is described later in Section 4.
 Me ta-Feature Name A ggregation Function T able 2: Aggregate functions which generate TUPSO X  X  meta features.
 The Performance Evaluator estimates the ensemble mem-bers X  classification performance during the ensemble X  X  train-ing phase. To fulfill its task, the Performance Evaluator uses one of the available classification performance estima-tors, i.e., OCA or OCF.
 The meta-features are measured properties of one or more ensemble-members X  output. A collection of meta features for a single instance makes a meta-instance. A collection of meta-instances is called a meta-dataset. The meta-dataset is used to train the meta-classifier. The Meta Features Ex-tractor computes the meta-features by using multiple ag-gregations of the ensemble-members X  output. Let P m = &lt; p ( m 1 ) ; : : : ; p ( m k ) &gt; be the vector containing the ensemble-members X  outputs p ( m 1 ) ; : : : ; p ( m k ) , where k is the number of members in the ensemble. A set of aggregate features is computed for each instance in the training set. A single set makes a single meta-instance, which will later be used either as a training instance for the meta-learner or as a test meta-instance.
 Table 2 defines eight experimental aggregate meta-features. The aggregate functions f 2 : : : f 5 and f 6 : : : f 8 are based on the first and second moments, respectively. The first mo-ment computes the X  X verage X  X nsemble-members X  prediction, whereas the second moment computes the variability among the ensemble-members X  predictions. The first moment based aggregation, a subtle version of the mean voting rule, is mo-tivated by Condorcet's Jury Theorem , and is used in several supervised-learning ensembles, e.g., Distribution-Summation [4]. Furthermore, the second moment based aggregation is motivated by the knowledge it elicits over the first mo-ment, i.e., the level of consent among the ensemble-members. From this information, unique high-level patterns of ensem-ble members X  predictions can be learned by the meta-learner, and thereafter be at the disposal of the meta-classifier. Table 3 shows the resulted structure of TUPSO X  X  meta-dataset.
I nstance f 1 ( P m ) f 2 ( P m ) f 3 ( P m ) .. . f 7 ( P T able 3: The training-set of the meta-classifier. Each column represents one aggregate feature over the ensemble members X  predictions and ma i;j de-notes the value of meta-feature j for meta-instance i Th e meta-classifier is the ensemble X  X  combiner, thus, it is responsible for producing the ensemble X  X  prediction. Simi-lar to the ensemble-members, the meta-classifier is a one-class classifier; it learns a classification model from meta-instances, which consist of meta-features. Practically, the meta-features used in training the meta-classifier can be either aggregate features, raw ensemble-members X  predic-tions or their combination. However, preliminary experi-mentsshowed that training the meta-classifier using the raw ensemble-members X  predictions alone or alongside the aggre-gate meta-features yielded less accurate ensembles.
The training process of TUPSO begins with training the ensemble-members followed by training the meta-classifier. The ensemble-members and the meta-classifier are trained using an inner k -fold cross-validation training process. First, the training-set is partitioned into k splits. Then, in each fold, the ensemble-members are trained on k -1 splits. After-wards, the trained ensemble-members classify the remain-ing split to produce the instances for training the meta-classifier. The meta-instances in each fold are added to a meta-dataset. After k iterations, the meta-dataset will con-tain the same amount of instances as the original dataset. Lastly, the ensemble-members are re-trained using the en-tire training-set and the meta-classifier is trained using the meta-dataset.
In order to calculate certain meta-features, e.g., f 3 , the ensemble-members X  predictions have to be weighed. To do so, a set of weights, one per ensemble-member, are learned as part of the ensemble training process. During the meta-classifier training, the ensemble-members predict the class of the evaluated instances. The predictions are fed to the Per-formance Evaluator, which calculates either OCA or OCF estimations for each of the ensemble-members, P erf vect = &lt; P erf 1 ; : : : ; P erf m &gt; , where P erf i is the estimated per-formance of ensemble-member i . Finally, a set of weights, ; 2 ;  X  X  X  ; m , is computed as follows:
We now specify the methods and conditions in which we investigated the presented ensemble schemes. First, we in-dicate the ensemble-members that participate in the ensem-bles. Next, we discuss the evaluated ensemble schemes. For evaluation purposes, we made use of four, one-class al-gorithms: OC-SVM [15], OC-GDE, OC-PGA, and ADIFA [14]. We selected these ensemble-members because they rep-resent the prominent families of one-class classifiers, i.e., nearest-neighbor (OC-GDE, OC-PGA), density (ADIFA), and boundary (OC-SVM). The first two algorithms are our adaptations of two well-known supervised algorithms to one class learning.

We used a static pool of six ensemble-members for all the evaluated ensembles: (i) ADIFA HM , (ii) ADIFA GM , (iii) OC-GDE, (iv) OC-PGA, OC-SVM 1 , and (vi) OC-SVM 2 . The ensemble-members properties, illustrated in Table 4, were left unchanged during the entire evaluation. ADI FA HM ADIF A  X  = HarmonicM ean; s = 2% ADIFA GM ADIF A  X  = GeometricM ean; s = 1% OC-GDE OC -GDE n=a OC-PGA OC -P GA k = 3, p = 0 : 01 OC-SVM 1 OC -SV M k = linear , = 0 : 05 OC-SVM 2 OC -SV M k = polynomial , = 0 : 05 T able 4: ensemble-members setup parameters. The non-default parameters are illustrated.
 The following evaluation includes several ensemble combin-ing methods from three groups of algorithms: Heuristic-Ensemble: estimated best-classifier ensemble (ESBE); Fixed-rules: majority voting, mean-voting, max-rule and product-rule ; and Meta-learning-based: TUPSO . The learning algo-rithm used for inducing the meta-classifier in TUPSO was ADIFA, as it outperformed the other three mentioned learn-ing algorithms on the evaluation set.
 During the evaluation we used a total of 40 distinct datasets from two different collections, UCI and KDD-CUP99. All datasets are fully labeled and binary-class.

We selected 34 datasets from the widely used UCI dataset repository [3]. The datasets vary across dimensions, number of target classes, instances, input features, and feature type (nominal or numeric). So as to have only two classes in the UCI datasets, a pre-process was completed where only the instances of the two most prominent classes were selected. The other instances were filtered out.

The KDD CUP 1999 dataset contains a set of instances that represent connections to a military computer network. The dataset contains 41 attributes, 34 of which are numer-ical and 7 of which are categorical. The original dataset contained 4,898,431 multi-class data instances. In order to divide the dataset into multiple binary-class sets, we fol-lowed the method performed in [20]. Compared with the UCI datasets, the KDD99-CUP are much more natural one-class datasets, as they are highly imbalanced (instances of the network X  X  normal state make the lion X  X  share of the de-rived binary datasets).
 During the training phase, only the examples of one-class were available to the learning algorithms and to the classi-fication performance estimators. During the testing phase, however, both positive and negative examples were avail-able, to evaluate the classifiers in real-life conditions. The generalized classification accuracy was measured by perform-ing a 5x2 cross-validation procedure [6]. We used the area under the ROC curve (AUC) metric to measure the classifi-cation performance of the individual classifiers and ensemble methods.

In order to conclude which ensemble performs best over multiple datasets, we followed the procedure proposed in [5]. In the case of multiple ensembles of classifiers or features, we first used the adjusted Friedman test so as to reject the null h ypothesis, followed by the Bonferroni-Dunn test to examine whether a specific ensemble or feature produces significantly better AUC results than the reference method.
In this section we examine the performance of the dis-cussed ensembles. Our goal is to learn which ensemble, if any, performs at least as good as the best ensemble mem-ber. We first investigate the performance of the discussed ensembles schemes using statistic tools. Next, we determine whether any of the ensemble of classifiers performs as good as the actual best ensemble-member classifier. Note the dif-ference between the actual , best ensemble member and the estimated , best ensemble member. The first is determined during the evaluation phase, where both positive and nega-tive instances exist, whereas the second is computed during the training phase, where only positive instances exist, and therefore, we should expect it to be inferior to the actual , best ensemble-member.
In the following experiment we examine the classifica-tion performance of the one-class ensembles using the same dataset used in the previous experiment. Next, the ensem-bles X  classification performances are compared with the hy-pothetical best-classifier, to tell which of the ensembles, if any, can match its performance.
 T able 5: Ensembles classification AUC results.

The results in Table 5, show that the ensemble with the highest average rank is the meta-learning based, TUPSO. The next best ensemble, by a large margin, is the product-rule, which was the only fixed-rule that was ranked higher than the random-classifier scheme. This is an indication for the high independence among the ensemble participants, in-duced by the heterogeneous learning algorithms. Indeed, T able 6: The statistical significance of the difference in the AUC measure of the examined ensembles.
 The p -value of the test statistic is inside the brack-ets. The  X + X  ( X - X ) symbol indicates that the average AUC value of the ensemble, indicated at the row X  X  beginning, is significantly higher (lower), compared to the ensemble indicated in the table X a  X  A  X  Zs header with a confidence level of 95%. [18] showed that the product-rule is motivated by indepen-dence of the combined models. In that work, however, the authors applied feature-set partitioning to decrease the de-pendency among the combined models.

The heuristic ensemble, ESBE, was ranked, on average, higher than the random classifier, showing, along with TUPSO, the benefits of using classification performance estimation. This simple ensemble was also ranked higher than the majority-voting, max-rule, and the mean-voting.

The statistical significance of the ranking difference be-tween the examined ensembles is presented in Table 6
The statistical tests reveal four clusters of classifiers, each comprised of statistically comparable ensembles or classi-fiers. TUPSO and the best-base classifier populate the clus-ter that represents the top-tier classification performance. ESBE and product-rule ensembles comprise the cluster that represents the above-average classification performance. The majority-voting, mean voting and randomly selected classi-fier make the below-average performance cluster, and finally, the cluster that represents the lowest classification perfor-mance is comprised of the max-rule ensemble.
We continue further with our experiment to find out which of the ensembles can be used as an alternative for the best base-classifier. Assuming that the users require a one-class classifier for a single classification task (i.e., a single dataset), they will be more concerned by how well their classifier will perform on their classification task, rather than how it will perform on average (i.e., on many different datasets). In our case, the best performer, .i.e, TUPSO, is on average as good as the best base-classifier and therefore is the natural choice of ensemble. However, it might be the case that on several datasets TUPSO significantly outperforms the best base-classifier, while on other datasets, it significantly falls behind the best base-classifier. If this is the case, one might want to use another ensemble that has a greater chance of classifying like the best base-classifier.

Figure 2 graphically shows the link between the AUC per-formance of the best base-classifier and that of the ensem-bles. Each dataset is represented by a single data point. To find out which of the ensembles X  AUC performance is most Fi gure 2: Classification performance: ensembles vs. actual best classifier. tightly linked with the best base-classifier, we computed a correlation matrix using the Pearson Correlation routine. The results in Table 7 show that the TUPSO ensemble has the most correlated AUC performance with the best base-classifier.
Thus far, the only combining scheme used for diverse learning algorithms, in the context of one-class learning, is the Fix-rule. Judging by publication quantity, schemes, such as majority voting, mean-voting, and max-rule are the most popular. However, in this study we hypothesized the ex-istence of an innate limitation with such combining meth-ods, as they do not take into account the properties of the ensemble-members they combine. Further along, we empir-ically demonstrated this limitation. The fixed-rule schemes indeed produced a lower classification accuracy when com-pared to the best base classifier.

In this paper we searched for an improved method for com-bining one-class classifiers. We implemented two one-class classification performance evaluators, OCA and OCF, which evaluated ensemble-members using only the available posi-tive labeled instances. We then introduced ESBE, a simple ensemble that uses OCA or OCF to select a single classifier from the classifiers pool. Our experiment showed that while this method is inferior to the best ensemble-member, it still outperforms the Majority voting.
 Lastly, we introduced a meta-learning based ensemble, TUPSO, which learns a combining function upon aggregates of the ensemble-members X  predictions. Thus, in contrast to the fix-rule scheme, TUPSO depends on the classifica-tion properties of the ensemble-members. Our experiments demonstrated the superiority of TUPSO over all other tested ensembles, both in terms of classification performance and in correlation to the best ensemble-member. Furthermore, TUPSO was found to be statistically indistinguishable from the best ensemble-member, thus, it completely removes the necessity of choosing the best ensemble-member. [1] S. Bethard and D. Jurafsky. Who should i cite: [2] C. M. Bishop. Novelty detection and neural network [3] C. Blake and C. Merz. UCI repository of machine [4] P. Clark and R. Boswell. Rule induction with cn2: [5] J. Dem X sar. Statistical comparisons of classifiers over [6] T. G. Dietterich. Approximate statistical test for [7] G. Giacinto, R. Perdisci, M. D. Rio, and F. Roli. [8] P. Juszczak and R. P. W. Duin. Combining one-class [9] A. Kontorovich, D. Hendler, and E. Menahem. Metric [10] W. S. Lee and B. Liu. Learning with positive and [11] B. Liu, W. S. Lee, P. S. Yu, and X. Li. Partially [12] L. M. Manevitz and M. Yousef. One-class svms for [13] E. Menahem, L. Rokach, and Y. Elovici. Troika -an [14] E. Menahem, A. Schclar, L. Rokach, and Y. Elovici. [15] B. Schlkopf, J. C. Platt, J. Shawe-taylor, A. J. Smola, [16] B. Sch  X  olkopf, J. C. Platt, J. Shawe-Taylor, A. J. [17] S. Segu  X  X , L. Igual, and J. Vitri`a. Weighted bagging for [18] D. M. Tax and R. P. Duin. Combining one-class [19] D. H. Wolpert. Stacked generalization. Neural [20] K. Yamanishi, J. ichi Takeuchi, G. J. Williams, and
