 In recent years, researchers have studied politi-cal texts detecting ideological positions (Sim et al., 2013; Hasan and Ng, 2013), predicting vot-ing patterns (Thomas et al., 2006; Gerrish and Blei, 2011) and characterising power based on lin-guistic features (Prabhakaran et al., 2013). While there is a vast amount of theoretical research on the rhetoric of politicians, only recently there has been a growing interest in understanding the argumenta-tion processes involved in political communication by means of computational linguistics (Hasan and Ng, 2013; Boltu  X  zi  X  c and  X  Snajder, 2014).
During a debate, a speaker tries to convince the audience of a particular point of view. This nor-mally involves an argumentation process, where the structuring of ideas is built upon logical connections between claims and premises, and a persuasive com-munication style. In this paper, we study the impact of persuasive argumentation in political debates on candidates X  power/influence ranking. As opposed to previous approaches, we propose to characterise political debates based on persuasive argumentation modelled through semantic frames.

Previous work (Rosenberg and Hirschberg, 2009) has analysed political speech transcripts identify-ing prosodic and lexical-syntactic cues which cor-relate with political personalities. Prabhakaran et al. (2013) proposed interactions within political de-bates as predictors of a candidate X  X  relative power or influence rank in polls. More recently they also found topic-shifting to be a good indicator of candi-date X  X  relative rankings in polls (Prabhakaran et al., 2014). Argumentation in debates has been studied from the perspective of automatic argument extrac-tion (Cabrio and Villata, 2012) and stance classifi-cation (Hasan and Ng, 2013). However, to the best of our knowledge, argumentation has not been ex-plored as a influence rank indicator. Moreover the study of persuasion in the NLP community has been so far limited.

The novelty of our work is the proposal of a method to automatically extract persuasive argu-mentation features from political debates by means of the use of semantic frames as pivoting features. We have trained a rank Support Vector Machine (SVM) model based on content and persuasive ar-gumentation features in order to rank debate speak-ers. Our experimental results on the 20 debates for the Republican primary election show that certain types of persuasive argumentation features such as Premise and Support Relation appear to be better predictors of a speaker X  X  influence rank compared to basic content features such as unigrams. When combining with content-related features, most per-suasive argumentation features give superior perfor-mance compared to the baselines. Argumentation has been defined as a verbal and so-cial activity of reason which aims to increase the ac-ceptability of a controversial standpoint by putting forward a set of connected propositions intending to justify or refute a standpoint before a rational judge (van Eemeren et al., 1996). Different argumenta-tion theories propose various schemes for describ-ing the underlying structure of an argument (Toul-min., 1958; Walton et al., 2008; Freemen, 2011; Peldszus and Stede, 2013). All these theories gen-erally agree in that an argument can be structured by means of two argument components and two ar-gumentative relations. The argument components include claims and premises . A claim is a central component of an argument and is characterised as being a controversial statement to be judged as true or false. Moreover a claim cannot be accepted by an audience without additional support. Such support is provided in the form of premises underpinning the validity of the claim. The following sentence illus-claim and premises:  X   X  X eople aren X  X  investing in America because this president has made America a less attractive place for investing and hiring than other places in the world.  X  (Former Governor Mitt Romney)
While argumentation focuses on the rational sup-port structured to justify or refute a standpoint, per-suasion focuses on language cues aiming at shaping, reinforcing and changing a response. In persuasive communication such response ranges from percep-tions, beliefs, attitudes and behaviours.
Persuasive language is characterised by the use of emotive lexicons (e.g., atrocious, dreadful, sen-sational, highly effective) where the speaker tries to engage with the audience X  X  emotions (Macagno and Walton, 2014). Often words with emotive meanings can present values and assumptions as uncontrover-sial, acting therefore as potentially manipulative in-struments of argumentation (Macagno, 2010). Other characteristics of persuasive language include the use of alliteration, which is a stylistic device charac-terised by the repetition of first consonants in series of words. This artistic constraint enables the speaker to sway the audience by feeling an urgency towards a rhetorical situation by intensifying any attitude be-ing signified (Bitzer, 1968; Lanham, 1991). The use of a repeating sounds engages auditory senses lead-ing to the evoking of emotions that engage the audi-ence.

The following is an example of persuasive  X  X  X  X  convinced that part of the divide that we X  X e experiencing in the United States , which is un precedented, it X  X  u nnatural, and it X  X  un -
American, is because we X  X e divided economically, too few jobs, too few opportunities  X  (Former Gov-ernor Huntsman).

To the best of our knowledge however, the study of the relation of persuasion and argumentation in political debates is limited. One of the main challenges is the lack of annotated corpora which include both argument annotations and persuasive messages annotations. While there has recently been released a corpus of persuasive essays (Stab and Gurevych, 2014) containing annotations for both class-level argument components and argument rela-tions, there is yet none annotated corpora for persua-sive arguments in political debates. In order to study whether persuasive cues and persuasive argumenta-tion can be used as predictors of speakers X  influence ranking on a debate, we propose to bridge between existing persuasive and political corpora through se-mantic frame features. The following section intro-duces the proposed strategy to port annotation be-tween two corpora. In order to study whether persuasive argumenta-tion can be used as predictors of speakers X  influence ranking on a debate, we propose to use the persua-sive essays corpus compiled by Stab and Gurevych (2014) to study persuasive argumentation in political debates through the use of semantic frames. 3.1 Persuasive Essays (PE) Corpus A persuasive essay is an essay written with the aim of convincing a reader on adopting a way of thinking regarding a stance taken on a topic. Unlike speech where an audience can be persuaded by means of social features or speech style, essays only rely on the written word depending therefore solely on the writer X  X  persuasive style.

The Persuasive Essays (PE) corpus consists of 90 essays comprising 1,673 sentences. It contains an-notations for both class-level argument components and argument relations. The class-level annotations include: 1) major claims; 2) claims; 3) premises and 4) the argumentative relations being either  X  X upport X  or  X  X ttack X . Argumentative relations are directed re-lations between source and target components (e.g., between premises, claims and major claims). The PE argument annotations follows the scheme de-scribed in Table 1.
 3.2 Presidential Political Debates (PD) Corpus Presidential political debates enable candidates to expose and discuss their stances on policy issues contrasting them with other candidates X  stances. During a debate, speakers unveil their discourse style as well as the premises supporting their claims. For our experiments, we collected the manual tran-scripts of debates for the Republican party presiden-tial primary election from The American Presidency of 20 debates which took place between May 2011 and February 2012. A total of 10 candidates partici-pated in these debates with an average participation of 6.7 candidates per debate. This corpus comprises 30-40 hours of interaction time and an average of 20,466.6 words per debate.

These debates follow a common structure in which a moderator directly addresses questions to the candidates where disruptions to answers are common due to interruptions from other candidates. In this corpus, each debate transcript lists the speak-ers including moderator and candidates and ques-tions asked during the debate. Each transcript also clearly delimits turns between speakers and moder-ators as well as mark-up occurrences of the audi-ence X  X  reactions such as booing and laughter. 3.3 Semantic Frames We propose to make use of the persuasion essays corpus annotations to understand persuasive argu-mentation in political debates by means of the use of semantic frames. A semantic frame is a descrip-tion of context in which a word sense is used. We make use of FrameNet (Baker et al., 1998), which consist of over 1000 patterns used in English (e.g., Leadership, Causality, Awareness, and Hostile en-counter). In this work we extract such patterns using SEMAFOR (Das et al., 2010).

Consider the sentence in Table 2 in which two semantic frames are detected. Each parsed seman-tic frame consists of { Frame, SemanticRole, label } providing a higher level characterisation of a text, highlighting the semantics of the discourse used in this text. If such semantic frames appear to be some of the most prominent features for a certain persuasive argumentation annotation scheme (e.g.,  X  X laim X ), then we can extract persuasive argumen-tation features from the unlabelled Political Debates corpus using semantic frames as pivoting features.
In this work we propose to port annotations be-tween the Persuasive Essays ( PE ) and Political De-bates ( PD ) corpora by means of the use of semantic frames as pivoting features.

To represent the PE corpus, let A = { a 1 ,..,a n } be the set of annotation schemes described in Ta-ble 1 and let T a = { t 1 ,..,t n } be the collection of sentences annotated with argument scheme a . To represent the PD corpus, let X  X  U D = { u 1 ,..,u n } be the set of speakers taking part on a debate D . Let S uD = { s 1 ,..,s n } be the set of sentences generated by speaker u on debate D .

Taking the PE corpus as a reference corpus, we propose to generate a vector representation of each annotation scheme in A for each speaker in each debate of corpus PD by following the steps below: i) Based on tf-idf we extract the most representative semantic frames for each annotation scheme a in PE as the vector SF a ; ii) We compute the weighted rep-resentation of each annotation scheme a in the PD debate d as follows: a) First we compute the bag based on the speaker X  X  content on the debate; then b) For each annotation scheme a we weight vector 3.4 Semantic Frames and Argument Types The statistics of the extracted semantic frames from PE for each argument type are presented in Table 3.
Such semantic frames provide a vector represen-tation characterising each persuasive argumentation scheme described in Table 1. Table 4 presents a sam-ple of the top semantic frames representing each ar-gumentation type.

Using the vector representation of each annota-tion scheme generated from PE , we computed the persuasive argumentation features for the PD corpus. Table 5 presents a sentence sample for each argu-ment type identified in the PD corpus along with the semantic frames characterising the sentence. We study a speaker X  X  influence on an audience based on his/her persuasiveness language and argumenta-tion styles during a political debate. To measure how influential a speaker is on an audience, we make use of the influence index (Prabhakaran et al., 2013), which is calculated based on a speakers rela-tive standing on poll released prior to the debate.
Poll scores describe the influence a speaker has to favourably change the electorate position towards his/her campaign. Given a debate D and the set of speakers U D we retrieve the poll results released prior to the debate and use the percentage of elec-torate supporting each candidate. If for a given de-bate there are multiple polls then the index is com-puted taking the mean of poll scores. Therefore the influence index P of speaker u  X  U D is: where p i is the poll percentage assign to speaker u in poll i in the reference polls. 4.1 Features We characterise each speaker in each debate based on the content and emotion cues he/she generated. Specifically we analyse each candidate in three di-mensions: i) what they said (content features); ii) the persuasiveness of the language they used in-cluding persuasive argumentation features and emo-tive language; iii) and external emotions evoked dur-ing the debates. We described each set of features below. 4.1.1 Content Features
We use a set of features which characterise con-tent of a candidate X  X  participation on a debate (Prab-hakaran et al., 2013). These include: 1) Unigrams (UG), which represents lexical patterns by counting frequencies of word occurrences; 2) Question De-viation (QD), difference between observed percent-age of questions asked to a candidate and the fair share percentage of questions in the debate; 3) Word Deviation (WD), difference between observed per-centage of words spoken by a candidate and the fair share percentage of words in the debate; 4) Men-tion Percentage (MP), a candidate mention counts normalised based on all candidates X  mentions in a debate. 4.1.2 Persuasiveness Features
We represent three types of persuativeness fea-tures as follows: 1) Persuasive Argumentation Features . Follow-ing the method described in the previous section, we extract the semantic frame feature vector represent-on each debate. These vectors provide information of different argumentation dimensions. We have ex-tracted a total of 710 semantic frames in PD . 2) Alliteration . After removing stopwords, we computed alliteration as repetitions of part of a word or a full word within a sentence. 3) Emotive Language . To characterise the use of emotive language, we generated a list of emotion-related semantic frames (e.g., emotion directed, emotions by stimulus, emotions by possibility) 4 , then for each speaker u in each debate d , we generated an emotion-frame vector weighted by tf-idf.

Once the features for each speaker have been gen-erated, we followed a supervised learning approach for ranking speakers of a debate based on their influ-ence Index, which can be used to denote how well a speakers participation on a debate has impacted the audience endorsement of his/her campaign. 4.1.3 External Emotion Cues
Previous work (Strapparava et al., 2010) has shown that an audiences X  social signal reactions to an idea, such as booing or cheering, are good pre-dictors of hot-spots where persuasion attempts suc-ceeded or at least such attempts were recognised by the audience. In this work, rather than recog-nising such persuasion hot-spots, we explore these audiences X  reaction cues (e.g applause) as poten-tial predictors of a candidate success on a politi-cal debate, we refer to such cues as external emo-tion cues. For each speaker in a debate, we com-puted the number of i) applauses (APL); ii) booings (BOO); iii) laughs (LAU); and iv) crosstalks (CRO) he/she received during his/her participation on a de-bate. These counts were normalised based on the total number of each emotion appeared on the de-bate.

With these features, we train a supervised learning classifier for ranking speakers of the debates based on their influence indices described in the following section. 4.2 Influence Ranking Approach In ranking, a training set consists of an ordered data set. Let  X  X  is preferred to B X  be denoted as  X  A B X . Let D denote a debate with a set of speakers U
D = { u 1 ,u 2 ,..u n } and influence indexes P ( u i ) for 1 &lt; i &lt; n . We specify a training set for ranking as R = { ( u i , X  i ) ,.., ( u n , X  n ) } where  X  i is the rank-ing of u i based on its P ( u i ) so  X  i &lt;  X  j if u i u
We want to find a ranking function F which out-puts a score for each instance from which a global ordering of data is constructed. So the target func-tion F ( u i ) outputs a score such that F ( u i ) &gt; F ( u for any u i u j . In this work we use the Ranking SVM (Joachims, 2006) to estimate the ranking func-tion F . For our experiments we used the Persuasive Essays ( PE ) and Political Debates ( PD ) corpora introduced in previous sections. While the PE was used as a ref-erence corpus, all our experiments were performed on the PD corpus.

All features are computed for the aggregation of a candidate X  X  content in a debate. For content and alliteration features, we first removed stopwords. In particular, for computing unigram features we also stemmed words using a Porter stemmer (Porter, 1997).

To compute persuasive argumentation features we used the collection of semantic frame features for the reference corpus PE . 5.1 Evaluation In this work, the ranking task evaluation for each debate consists on comparing the generated ranked list of candidates, using the influence ranking ap-proach introduced above, against a reference ranked list. Such a reference ranked list corresponds to our gold standard of ranked list of candidates generated based on the polled scores for that debate.

Following a 5-fold cross validation, we report re-sults applying four commonly used evaluation met-rics for ranking tasks, nDCG , nDCG-3 , Kendall X  X  Tau and Spearman correlations. The discounted cu-mulative gain metric ( nDCG ) penalises inversions happening at the top n elements 5 of a ranked list more than those inversions happening at the bot-tom. While the nDCG metric penalises certain ele-ments in the list, Kendall X  X  tau and Spearman X  X  rank correlations penalises inversions equally across the ranked list. 6.1 Correlation Analysis We performed a correlation analysis for the con-computed the Pearson X  X  product correlation between each feature with the candidate X  X  influence index P ( u ) derived from the polls. The computed corre-lations for these features are presented in Figure 1. Darker bars indicate statistical significance correla-tion at p &lt; 0 . 001 ; lighter dark bars at p &lt; 0 . 05 ; and light bars not statistically significant.

These results show that for the content features, both question deviation (QD) and word deviation (WD) correlate moderately with the influence index; while the mention percentage (MP) feature corre-lates highly with the influence index ( p &lt; 0 . 05 ). For the emotion cues, we obtained statistically signifi-cant ( p &lt; 0 . 05 ) moderate correlations between the applause (APL), laugh (LAU), crosstalk (CRO) and the influence index; while the correlation between the booing (BOO) cue and the influence index was not statistically significant. These results indicate that speakers with higher influence index spoke for longer periods of time, in line with existing empiri-cal findings in sociological studies (Ng and Bradac, 1993; Reid and Ng., 2000; Prabhakaran et al., 2013), and were asked a higher number of questions. This analysis also indicates that speakers with higher in-fluence index generated more crosstalk, in line with previous empirical sociological findings (Ng et al., 1995); received more applauses and made the audi-ence laugh more often. 6.2 Influence Ranking Results Following the results of the correlation analysis, we conducted experiments using those content and emotion cue features presenting statistically signif-icant correlations with the influence index. Apart from these features, we also consider the persuasive argumentation features and a combination of fea-tures from both content and persuasion categories. Results for the prediction of influence ranking using these features are presented in Table 6.

For the content features , using the simple uni-grams gives the best results. The mention percent-age (MP) feature also attains competitive perfor-mance. A combination of word deviation, question deviation and mention percentage (WD+RD+MP) however degrades the performance. This is in con-trast to the results reported in (Prabhakaran et al., 2013) (denoted as [PR13] in Table 6), where the uni-gram feature gives much worse results and their best results were obtained using WD+RD+MP. One pos-sible reason is that for the unigram feature used in our experiments, we have performed pre-processing by removing stop words and stemming.

For external emotion cues , although some emo-tion cues appeared to be significantly correlated with the influence index in our analysis, they did not out-perform the unigram baseline. We suspect that this might be due to the fact that depending on the loca-tion of a debate, certain candidates may bring bigger crowds into the debate X  X  venue, therefore emotion cues can be a deliberate biased way of support. Con-sequently emotion cues happening within the debate venue may not reflect the emotions induced to the audiences that followed the broadcast of the debate.
When analysing the persuasion features , alliter-ation and emotive language features give better re-sults compared to external emotion cues. But they did not outperform the unigram baseline either.
We find that persuasive argumentation features alone provide improvement upon the unigram base-line. In particular, in terms of nDCG and nDCG-3 7 , the premise and support relation types provide the best results for this feature category. In terms of Tau and Spearman 8 correlations, the attack relation type provides the best results. When focusing only on persuasive argumentation features, these results sug-gest that speakers with higher influence index tend to use well supported arguments (i.e. present more premisses supporting their claims) and/or tend to at-tack more other candidates by presenting premises refuting a claim.

When combining features , we found that the top 100 persuasive argumentation features ranked by tfidf together with word deviations and mention per-centages significantly improve upon the baselines for particular argumentation cases including Claim, Premise, ForStance, and SupportRel.

The overall best performing features for predict-ing influence ranking in terms of nDCG , Tau and Spearman was consistently obtained with the com-bined feature for the Premise type.

Our results improve upon those recently obtained in (Prabhakaran et al., 2014) in both nDCG and Tau where topic shift patterns have been added for influ-ence ranking (denoted as [PR14] in Table 6).
These results suggest the relevance of  X  X hat they said X , the  X  persuasiveness style of their arguments X  and the relative importance given by others by means of mentions are good predictors of influence ranking in political debates. In particular when com-bining the lexical content of candidates X  discourse with their persuasive argumentation style, our re-sults indicate that candidates with higher influence ranking tend to present more premises while clearly stating their stance (i.e. supporting a claim) on a particular topic. In this paper, we have studied the impact of argu-mentation in speaker X  X  discourse and their effect in influencing an audience on supporting their candi-dature. In particular, we have conducted the study in the domain of political debates. In order to ex-tract persuasive argumentation features from polit-ical debates, we have proposed a novel method to port annotations from a persuasive essay corpus us-ing semantic frames as pivot features.
 Our experimental results on the 20 debates for the Republican primary election show that when com-bined with word deviations and mention percent-ages,most persuasive argumentation features give superior performance compared to the baselines. Particularly with the Premise and SupportRel types appear to be better predictors of a speaker X  X  influ-ence rank. In future work, we will aim to improve the accuracy of the extracted persuasive argumenta-tion features by exploring other methods for identi-fying persuasive argumentations from text.
 This work was supported by the EU-FP7 project SENSE4US (grant no. 611242)
