 . Results for fault diagnosis of the Tennessee Eastman process using the 1. Introduction
Due to recent advances in instrumentation and control systems technology, vast amount of process data is available.
However, for most of the plants, at the best, only lumped-parameter models are available. Thus, modern plants are data rich and information poor. Due to increasing complexity of modern chemical plants, quick fault detection and diagnosis has been recognized as an important activity for their optimal and safe operation. Venkatasubramanian et al. (2003a X  X ) have presented a review of various methods for fault diagnosis. Among various methods, data-driven approaches have been described to be most useful for industrial applications. The process data can be used to assess the process states by utilizing the important features present in the data. Qualitative trends (e.g., increasing, constant, decreasing, etc.) are the most natural representation of features that have been widely used for fault diagnosis (FD). In this work, an algorithm for the on-line extraction of qualitative trends is developed based on an existing technique for off-line trend extraction, namely, interval-halving ( Dash et al., 2004 ). A frame-work for on-line fault diagnosis using the qualitative trends is also developed.

Every diagnostic system that uses process trends to achieve fault classification has three components: (i) a language for trend representation such as triangular episodes ( Cheung and Stephanopoulos, 1990 ), primitive-based language ( Janusz and
Venkatasubramanian, 1991 ) and piecewise-linear elements (Mah et al., 1995 ), (ii) a methodology to extract the trends such as wavelet-based method ( Bakshi and Stephanopoulos, 1994a ), use of wavelet, neural networks and B-Splines-based method (Vedam, 1999) and (iii) a classification methodology and a knowledge-base to map the sensor-trends into faults such as decision trees ( Bakshi and Stephanopoulos, 1994b ), weighted symptom trees (WST) ( Oh et al., 1997 ) and fault or sensor centric trees ( Vedam, 1999). Recently, Dash et al. (2004) proposed an interval-halving-based algorithm for automatic trend extraction.
The primitive-based language ( Janusz and Venkatasubramanian, 1991) is used for the representation of the qualitative trends. The seven primitives, viz .A(0,0),B(+,+),C(+,0),D(+, ), E( ,+),F( , 0), G( , ) where the signs are of the first and second derivatives, respectively, are shown in Fig. 1. The primitives B, D, E and G are nonlinear primitives. One may note that the interval-halving algorithm uses a second-order polynomial to directly extract nonlinear primitives (Dash et al., 2004 ). It is possible to propose more specific primitives such as sharp increase or decrease, etc. (Olszewski, 2001 ), or even use a lesser number of basic shapes (Keogh et al., 2001; Mah et al., 1995 ). Dash et al. (2003) also developed a fuzzy-logic-based framework for trend-matching (estimation of similarity between two trends) and fault diagnosis.
Other approaches such as decision-t ree-based classification schemes (Bakshi and Stephanopoulos, 1994b ), principal component analysis-based similarity (Johannesmeyer et al., 2002 ), wavelet-coefficients based similarity ( Lu et al., 2003; Sun et al., 2003 ) and subsequence-matching also have been discussed in the literature ( Agrawal et al., 1995; Hetland, 2004; Wong et al., 2001 ). The overall activity of trend-extraction and tr end-matching is called qualitative trend analysis (QTA). A detailed review of QTA is not intended here. For a detailed discussion on various contributions to the research on trend-analysis and/or similarity-b ased fault diagnosis, an interested reader is referred to the literature ( Vingron and Argos, 1989 ;
Konstantinov and Yoshida, 1992 ; Bakshi and Stephanopoulos, 1994a, 1994b; Faloutsos et al., 1994; Agrawal et al., 1995 ; Wong et al., 1998 ; Perng et al., 2000 ; Okabe and Masuyama, 2001 ; Wong et al., 2001 ; Colomer et al., 2002 ; Johannesmeyer et al., 2002 ;
Sun et al., 2003 ; Sundarraman and Srinivasan, 2003 ; Dash et al., 2004, 2003 ; Venkatasubramanian et al., 2003b ; Lu et al., 2003 ; Hetland, 2004 ; Charbonnier et al., 2005 ; Maurya et al., 2007a ).
Keogh and coworkers ( Fu et al., 2008; Mueen et al., 2009 )havemade seminal contributions in the area of similarity-based pattern-query in time-series databases and its extensions to shape analysis and motif discovery. Recent applicatio ns of qualitative trends include prediction of aerodynamic instability in a blast furnace ( Gamero et al., 2006 ), integration of QTA with other methods such as signed directed graphs or principal component analysis ( Maurya et al., 2005, 2007b ), the work of Villez et al. (2008) on the control of pH in a bioreactor and abnormality diagnosis in oil export pumps in an offshore oil and gas production facility ( Raza and Liyanage, 2009 ). The concept of QTA has also been utilized in diagnosis of valve-stiction in oscillating control loops ( Rengaswamy et al., 2001 ; Srinivasan et al., 2005 ; Yamashita, 2006 ; He et al., 2007 ; Scali and G helardoni, 2008 ). Below, the motivation and novelty of the present work is succinctly described.

For fault diagnosis, QTA needs to be implemented on-line. While the interval-halving algorithm ( Dash et al., 2004 ), the trend-matching methodology and their application for off-line diagnosis have been discussed in detail by Dash et al. (2003) , on-line implementation has not been carried out. Though the research work by Vedam (1999) dealt with some of the issues involved in on-line implementation of trend-similarity-based FD, little has been discussed in the published literature. Keogh et al. (2001) have presented an algorithm known as SWAB (Sliding Window And Bottom-up) for on-line extraction of trends, but their methodology deals only with linear primitives and hence, the algorithm as such cannot be directly extended for the interval-halving technique to extract nonlinear primitives in on-line fashion. Same is true about the recent work by Charbonnier et al. (2005) which is conceptually similar to the contribution by Mah et al. (1995). The advantage of using nonlinear primitives is that primitives of longer duration are generated, smoothness is also captured due to the use of the second derivatives and that more data-compression is achieved ( Dash et al., 2004 ). Also, as compared to the interval-halving technique, the linear-trending scheme of Charbonnier et al. (2005) requires many parameters, possibly independent of the noise-level, which need to be tuned. In their work, Charbonnier et al. (2005) do not use historical data and similarity search to perform fault diagnosis although they have shown the use o f linear qualitative trends for alarm management and operator support. Villez et al. (2008) use the concept of nonlinear primitives ( Cheung and Stephanopoulos, 1990 ), but they do not elaborate on the issues in on-line application.
While the existing interval-halving algorithm can be applied on theentiredatainoff-linemanner,duringon-lineapplication,ithas to be applied on a window consisting of the newly arrived data and only a part of previous (recent) data. Selection of this window is a crucial issue. Another issue is concerned with the possibility of averaging the primitives in the region of overlap between two consecutive windows. A third issue relates to accurate estimation of fault-occurrence time for robust trend-matching d uring on-line implementation. As it would be discussed later, some of the concepts used in the SWAB algorithm, albeit discovered independently, are used in the methodology discussed in the present work. In this article, we discuss the above important issues and present solution approaches for them ( Maurya et al., 2004b ). In particular, an algorithm for on-line trend extraction using the i nterval-halving technique is presented. A framework for on-line FD using QTA is also presented. The organization of this article is as follows.

In the next section, an overview of QTA is presented. In Section 3, a detailed discussion on the need for an on-line variant of the Nomenclature C.I. confidence index F flow rate S similarity measure between two trends VLp liquid volume in the stripper, m 3 VLr liquid volume in the reactor, m 3
VLs liquid volume in the separator, m 3 maxCFlen maximum length of a linear primitive minNLP _ len minimum length of a nonlinear primitive n the number of sensors t time Subscripts VP valve position m measurement 1, 4 streams in TE flowsheet bias bias in the sensor or the valve position Superscripts set set point for a controlled variable interval-halving algorithm and the challenges involved in the same is presented. In Section 4, the design of the algorithm for on-line trend-extraction is discussed. The framework for on-line FD is discussed in Sections 5 and 6. Section 5 deals with development of a knowledge-base (KB) for QTA. Section 6 deals with the use of the
QTA-KB for on-line FD. In Section 7, a succinct discussion on the development of a prototype diagnostic system in Matlab (The Mathworks Inc., www.mathworks.com ) is presented. Sample results of FD of the Tennessee Eastman (TE) process are presented in Section 8. 2. Overview of QTA
There are two subtasks in QTA: (i) trend extraction ( Dash et al., 2004) and (ii) trend matching ( Dash et al., 2003 ). A brief discussion follows.

Trend extraction by using the interval-halving algorithm : The algorithm works by fitting either a constant, a first-order or a second-order polynomial to the data and halving the interval if the fit-error is significant as compared to the noise present in the signal, as dictated by the F -test, even for the quadratic function.
Once the polynomial is fitted over a certain interval, a primitive is assigned based upon the signs of the first and second derivatives. Student X  X  t -test is used to check the significance of the derivatives.
Then the interval-halving procedure is applied to the remaining data till the entire signal is transformed into a sequence of primitives.

Fuzzy trend-matching : Trend-matching involves calculation of the following: (a) fuzzy similarity match between two primitives, (b) similarity measure between two trends (the time-weighted average of the similarity match between the primitives involved in different time intervals) for the same sensor and (c) multi-variate (overall) similarity measure or confidence index ( C.I .) between two scenarios given by C.I .=min( S 1 , S 2 , y , S the number of sensors and S k is the similarity measure between the trends of the k th sensors in the two scenarios. To perform fault diagnosis by using QTA, the faults stored in the QTA-KB are rank ordered in decreasing order of their C.I . The fault with the highest C.I . is the fault that has most likely occurred. A low value of C.I ., say, below 0.50, for all the faults indicates the potential occurrence of a novel fault. 3. On-line fault diagnosis using QTA: motivation and issues
The ultimate aim is to implement the QTA-based fault diagnosis in real plants. In off-line trend extraction, the interval halving algorithm is applied on the entire data. During on-line implementation, more and more data are available from sensor measurements. The primitives obtained at current time may no longer correctly represent the trend at a future time. Thus, a key feature of an algorithm for on-line trend extraction is that the primitives should be updated as more data become available. On one hand, trend extraction cannot be performed on the entire data available so far at every sample time. On the other hand, if one were to find trends corresponding to only the new data, in general, one would end up assigning only  X  X  X  primitives since no useful trend is contained in a few samples. Thus, the data set for trend extraction should comprise of some of the past data and the newly available data. This poses the question  X  how should one choose the data segment for trend extraction? Some related questions are: (i) should the data segments chosen for two consecutive trend extractions overlap?, (ii) can one calculate an average primitive in the overlapping region?, etc. Answers to these questions are sought in this work. Further, for real-time fault diagnosis, one needs to consider the following: (i) develop-ment of a database of fault-symptom signatures, (ii) detection of the occurrence of an abnormal event (fault detection), (iii) extraction of the relevant portion of trend from an arbitrarily long sequence of primitives (does fault occurrence time play a vital role?), (iv) time-efficient computation of similarity measure, and (v) updating the QTA knowledge-base if a novel fault is manually diagnosed by the operator. Discussion on on-line trend extraction follows. 4. On-line trend extraction
Given the measurements available from a sensor, the basic interval-halving algorithm can be applied on the entire data in off-line fashion to extract trends. From the above discussion it is clear that the trend extraction has to be performed over a window of data, that the window should move as more data become available and that the trend extracted in the current window has to be intelligently combined with the already extracted trends.
Before presenting the algorithm for on-line trend extraction, let us briefly analyze the off-line and on-line implementation of some other data processing activities such as wavelet-based denoising (Bakshi et al., 1997 ) and B-Splines based trend extraction ( Vedam, 1999). 4.1. On-line implementation of wavelet-based denoising
Wavelet-based denoising is implemented on-line as follows (Bakshi et al., 1997 ). In sliding window approach, data from a window of length n =2 k , where k is a positive integer, are chosen for denoising by using the off-line algorithm ( Fig. 2 ). As more (say, p o n ) data points become available, the window is slid by p data points and the off-line denoising algorithm is used on the new window. In the overlapping region, one can either replace the older denoised data by the new denoised data or can take the average of both (average value at the starting point of the new window in either case). A similar algorithm is implemented in the hopping window approach where the last data point of the current window overlaps with the first data point of the next window. The drawback of the hopping window approach is that one has to wait for n 1 future samples before denoising can be performed again. Data averaging in the overlapping region is possible because wavelet analysis is linear in nature, and that the averaging is done for individual sample instances and not for a trend. 4.2. On-line implementation of B-Splines based trend-extraction
To implement the B-Splines based algorithm, which can extract only linear primitives, for on-line trend extraction by using a sliding-window approach ( Vedam, 1999), off-line algo-rithm is applied on a window containing 2 k +1 samples. After measuring p more samples, the window is slid by p data points.
The window size does not change. To get a consolidated list of linear primitives till the current time, the primitives in the current window are combined with the old list of consolidated linear primitives. Since the primitives to be combined are linear in nature, linear combination can be used at the starting point of the current window. After this time instant, the old primitives are replaced by the primitives in the current window. Another important feature of the B-Splines based algorithm is that every time trends are extracted, after updating the list of linear primitives, linear primitives are concatenated to get higher order primitives. Notice that it is the list of linear primitives that is updated and that the higher order primitives are not updated directly. On-line implementation of the interval-halving algo-rithm for trend extraction is discussed next. 4.3. On-line implementation of the interval-halving algorithm
The basic interval-halving algorithm is capable of extracting nonlinear primitives (the primitives  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X ) directly and no concatenation is needed. This means that if the interval-halving algorithm identifies a nonlinear primitive then there is no need to allow this nonlinear primitive to evolve further except when the primitive length is very small. This is a good feature since the number of rules for concatenation of linear primitives to generate nonlinear primitives could be large and there is no systematic procedure to find till what depth these rules should be applied. Also, for concatenation, a number of parameters, e.g., magnitude threshold on the slopes, are needed for every sensor which makes the concatenation process less transparent. Hence, this feature (i.e., no need to concatenate) of the interval-halving algorithm should be preserved during on-line implementation.
Thus, concatenation is avoided during the on-line implementa-tion. Also, unlike on-line implementation of B-Splines based methodology for trend extraction, in the interval-halving approach, linear combination (averaging) at the starting point of the current window is not an option because averaging would render the polynomial coefficients useless; the polynomial coefficients can be used for data compression. While accounting for these issues, the primitives should evolve as more data come in. Given a current window of trend-extraction, this evolution of trends is achieved by extending the window to include new data points and ejecting some of the primitives from the back of the window to keep the window-size under control. A detailed description is presented below. 4.3.1. Unrestricted evolution of the last primitive of the current window
By the nature of the interval-halving algorithm, it can be taken for granted that as more samples are available, it is only the last primitive that can evolve. All the previous primitives have already evolved completely and that they cannot evolve any further. This procedure is somewhat similar to the SWAB algorithm presented by Keogh et al. (2001) . In the SWAB algorithm, only the first primitive from the current window is appended to the existing list of primitives. Rest of the data are again processed with the future data.

The above assumption may be violated for very small time intervals where one of the primitives before the last primitive could be an  X  X  X  primitive. To elucidate this issue a little more, recall the F -test for checking if the fit-error is significant as compared to the noise. After all, no matter how qualitative or fuzzy a methodology may be, comparison of two numbers is a quantitative operation. Hence, minor offsets from the desired effect can be expected. Such local violations do not have much effect on the calculation of similarity measure.

If the last primitive is a linear primitive ( X  X  X ,  X  X  X  or  X  X  X ), then allow it to evolve until it becomes a nonlinear primitive or it becomes very long so that further evolution would require excessive computation. If the last primitive is a nonlinear primitive then it should not be allowed to evolve except when its length is too small. For example, if the characteristic time for the evolution of a fault is few hours then one may not want to store any primitives of length less than 5 sample points (minutes). 4.3.2. Parameters and selection of the window
As it can be inferred from the above discussion, three important parameters are the default window length, the length of the shortest primitive (minNLP_len) and the length of the longest linear primitive (maxCFlen). For choosing the window, the endpoint of the current window should coincide with the last sample available. The starting point of the window can be chosen to coincide with the starting point of the last primitive. With this rule in effect, the window size changes adaptively. One exception to this rule is that if the last primitive is nonlinear and is very long then one can choose the starting point of the window so as to keep the window size equal to the default window size. Also, the maximum allowed window size (which is related to maxCFlen) and the window shift parameter (the number of new samples needed to move the window) can be identified based upon computations required for one pass of trend extraction and trend matching. Depending upon how the window is selected, a record of the primitives that would not evolve anymore should be kept so that the most updated list of primitives can be obtained simply by appending the primitives in the current window to the existing list of non-evolving primitives. The above procedure of window selection is schematically shown in Fig. 3 . A flowchart for the overall algorithm is shown in Fig. 4 . The pseudo-code for the algorithm is given below.

Pseudo-code for on-line implementation of the interval-halving algorithm on_line_interval_halving(init_win, cur_list_prim){ of the next window */ /* Last Primitive */ maxCFlen))){ primitive to evolve */ } point of the next window */ size of the next window */ primitive to evolve */ allowed to evolve */ last_prim); allowed to evolved */ window */ }
Fig. 5 shows a snapshot of trend-extraction in two consecutive windows for a sample signal. If one were to use the interval-halving algorithm on a sliding window of fixed length (65), the last primitive at the current time would still have been  X  X  X . Also, unnecessary computation would have been performed over a certain portion of the first primitive.

As mention earlier, at a conceptual level, the above methodology has good similarity with the SWAB algorithm proposed by Keogh et al. (2001) .Asanexample,inSWABalgorithmasnewdataarrive, the window is extended to the righ t to include the new data points and the last primitive is ejected from the back of the window. This results in adaptive window size. Keogh et al. (2001) also use bounds on the window size to avoid missing global features (minimum window size similar to minNLP_len) and to manage complexity for real-time application (maximum window size similar to maxCFlen).
However, there are two crucial differences between the SWAB algorithm and the present algorithm. The first is that the present approach is tailored towards extracting both linear and nonlinear primitives. The second is that in the present approach all primitives except the last primitive are ejected in the process of moving the window. The reason lies in the assumption stated earlier that only the last primitive can evolve when the window is moved. If necessary, this restriction can be removed, e.g., the last 2 or 3 primitives in the window can be allowed to evolve. 5. Building QTA knowledge-base
Development of the knowledge-base for QTA primarily involves data-collection and trend-extraction for all known fault scenarios. Some of the parameters related to QTA-database, which are universal in nature, are  X  description of the faults stored (including whether the fault truly corresponds to an abnormal scenario), description of the sensors which are used for trend extraction (including sample rate, normal value, noise, etc.), the start and end points of the data used for trend extraction for each sensor in every fault scenario, the global parameters related to on-line trend extraction such as default window length, etc. For small-scale plants, where the number of sensors is small (say, less than 100), one may be able to compute similarity measure for all the sensors with all the fault scenarios in real-time. However, for large plants, it may be infeasible. For large plants, one must consider the issue of computational complexity. 5.1. Importance of selection of fault-specific sensors
For large chemical plants, the nu mber of sensors could run into few hundreds or even thousands. Evaluation of similarity measure for such a large number of sensors may not be computationally feasible. Apart from computational infeasibility, it is expected that the sensors that are physically lo cated near the fault origin would show departure from their normal value range (normal operating region (NOR)) before the sensors that are located far away. Hence, such sensors are useful in detecting the fault occurrence and they should be chosen for estimating the similarity measure. Other sensors may not play as vital role in quick fault detection and diagnosis as these sensors. Thus, optimal selection of sensors that should be used for evaluating similarity measure corresponding to each fault is very important. Other issues in optimal selection of sensors are  X  (i) consistency among similar faults: the sensors should show similar trends for multiple manifestations of the same fault, and (ii) discrimination from other faults: the sensors should be chosen so that they provide maximum discrimination from other (different) faults. These ideas have been earlier discussed and implemented by Vedam and others ( Vedam, 1999 ). Further, when lesser but still sufficient number o f sensors is chosen, most likely the chosen sensor would show fast evolution. This would result in robustness in the calculation of similarity measure, particularly during the incipient stage of fault evolution. To understand why, one needs to understand how on-line trend-extraction may affect the time-averaged similarity measure for slow sensors. During the short time after fault occurrence, when the fault evolution is still at the incipient stage, the last primitive is yet to evolve and in many cases it is a linear primitive, whereas the corresponding primitive in the database could be a nonlinear primitive. Quite often, this is the duration in which one is interested in diagnosing the fault. This may result in a low similarity measure for the slow sensors. Since the confidence index assigned to a fault is the minimum of the similarity measure for the sensors dedicated to diagnose/monitor the fault, if all the sensors are chosen blindly for assessing every fault, then correct diagnosis would be delayed. To summarize these ideas, the sensors dedicated for diagnosing various faults should be chosen with respect to three criteria: (i) c onsistency among similar faults, (ii) fast dynamics for the fault, and (iii) discrimination from other faults. The procedure for optimal screening of sensors is discussed below. 5.2. An algorithm for optimal selection of fault-specific sensors
The algorithm consists of the following seven steps. (1) Extract trends for all the sensors for all the fault scenarios (2) Compute a global similarity matrix containing the similarity (3) Identify the set of faults that are similar to this fault. Identify (4) Rank the sensors in the decreasing order of similarity with (5) Rank the sensors in the increasing order of similarity with (6) Rank the sensors in decreasing order of speed of evolution (7) Prepare a new list by selecting sensors from lists 1 and 3 after In the above procedure, a fail-safe approach should be adopted so that a sufficient number of sensors are chosen to ensure discrimination from other faults. In some rare cases, where the data used for developing the knowledge-base are not collected properly, consistency with similar faults and discrimination from different faults could be in conflict. In such cases, a robust and reliable knowledge-base cannot be developed. 6. Use of the database for on-line fault diagnosis
During on-line implementation, the primitives are continu-ously extracted. When a fault occurs, the sensors start deviating from their normal values. Thus, the first step is fault detection. In QTA-based fault detection, the presence of a non-A primitive and departure from the NOR indicates presence of a fault. A suitable multivariate-analysis-based methodology also can be used for fault detection ( Gertler and Cao, 2004; MacGregor et al., 1994; Misra et al., 2002; Wold et al., 1987 ). More discussion on the same is beyond the scope of work presented in this article. After detection, it is very important to estimate the time at which the fault occurred so that an appropriate portion from a long sequence of primitives can be extracted. If the fault occurrence time is not estimated properly then poor similarity measure may be obtained even for very similar trends ( Fig. 6 ). Fig. 6 shows a trend/signal from real-time measurements when a fault occurs and a trend/signal from the database for the same fault. If the fault occurrence time is estimated accurately, and this time is matched with the starting point of the trend in the database, then high similarity measure is achieved. On the contrary, if the starting point of the trend in the database is matched with the time point of fault detection, which is usually always few samples after the fault actually occurred, then poor similarity measure is obtained even though the signals are very similar. It must be mentioned that though this problem can be alleviated to some extent by incorporating dynamic time-warping (DTW) ( Kassidas et al., 1998) and similar approaches ( Sundarraman and Srinivasan, 2003), correct estimation of the fault occurrence time is still important. Due to excessive computational complexity, similarity measures with multiple shifted trends cannot be evaluated in real-time and hence, a good estimate of fault occurrence time is required. The methodology used for the estimation of the fault occurrence time is called backtracking. As shown in Fig. 7 , once a fault is detected, one tries to fit an  X  X  X  primitive in the last interval.
If an  X  X  X  primitive can be fitted, then there is little variation, which means that the fault occurred sometime before this interval. So, the estimation window is stretched backwards and this procedure is repeated over the stretched window until an  X  X  X  primitive cannot be fitted, i.e., there is enough variation in the data in the estimation window. This procedure always terminates since the last primitive is a non-A primitive. Trends are re-calculated for the data after the fault occurrence time. It can be seen that this method does not take into account the time-delay but that is not a problem because, to ensure robust estimation of similarity measure, the same methodology can be used during the development of the QTA knowledge-base (i.e., off-line development of the QTA knowledge-base but in on-line fashion). As the fault evolves, more and more sensors deviate from their NOR. These sensors are used for the estimation of similarity measure and confidence index ( C.I .) for various faults.
This ensures that similarity measure for a slowly evolving sensor or a sensor that is not showing enough deviation would not result in incorrect C.I . for the actual fault. To summarize, the main activities involved in on-line FD are:
Fault detection based upon departure from the normal operating region.
 Estimation of the fault occurrence time.
 Computationally efficient trend matching and fault diagnosis.
Updating the QTA-database based upon operator X  X  feedback particularly in the case of novel event and manual diagnosis. 7. A prototype diagnostic system
A prototype diagnostic system to implement the framework discussed above has been designed in Matlab s . To give a brief idea about the important features of the framework, some of the components are succinctly discussed below. The names of various
Matlab functions that are used in these sub-modules of the QTA-based diagnosis framework are also given below. Since the function names are self-explanatory, they are not described in great detail.

Development of QTA-KB: Trends for various fault scenarios are extracted and sensors to be used for estimating the C . I . for various faults are identified. batch_trend_extraction build_qta_knowledgebase
On-line trend extraction: The adaptive trend-extraction algo-rithm is used to identify the trend extraction window (adaptive size) and to extract the trends. get_next_window get_next_trends, interval_half
Fault detection: Presence of a fault is indicated if a sensor deviates from its NOR and shows a non-A primitive. detect_abnormal_sensor
Fault occurrence time: The backtracking methodology is used to estimate the fault occurrence time. The relevant sensor-trends are re-calculated. get_fault_initiation_time backtracking update_trend_after_detection
Fault diagnosis: C.I . for various faults are calculated. The operator is informed about the abnormal sensors and the known or an unknown event. diagnose_the_abnormality inform_the_operator_known_unknown_fault
Learning and updating the QTA-KB: If a novel event occurs and the operator diagnoses the fault, it is added to the QTA-KB. The operator can also add an already known fault to the QTA-KB that is automatically diagnosed by the QTA module but its fault magnitude is considerably different from the already stored (same) fault(s). manual_diagnosis add_QTA_diagnosed_fault_2KB add_operator_diagnosed_fault_2KB 8. Evaluation of the diagnostic system on Tennessee Eastman process The diagnostic system has been tested on the Tennessee
Eastman (TE) benchmark problem. The TE problem was first presented by Downs and Vogel (1993), and since then it has been considered as a benchmark problem for a number of applications such as state estimation ( Ricker and Lee, 1995b ), model predictive control (MPC) ( Ricker and Lee, 1995a), fault diagnosis using statistical and speech recognition techniques ( Kassidas et al., 1998; Raich and Cinar, 1997 ), sensor-location ( Bhushan and
Rengaswamy, 2002 ), etc. The relevant description of the faults, measurements, etc., in the TE case study can be found elsewhere (Maurya et al., 2004a ). There are 33 possible fault scenarios (excluding the normal process operation) and 14 variables are measured which were identified using a signed-directed graph-based sensor location ( Maurya et al., 2004a, 2007b). Sample time is 0.001h for all the 14 sensors. No optimal sensor assignment has been carried out for this case study. Out of the 33 possible faults, 26 faults are used to build the QTA-KB. The parameters such as normal window size, the length of the shortest primitive, etc., can be decided based upon the sampling rate and the process characteristic time. For example, in this case study, characteristic number of samples per hour). maxCFlen and minNLP_len are 1000 and 50, respectively. The window shift length is 15 (to allow the necessary computations every minute). Results for three test scenarios are as follows. In each scenario, the fault is introduced at sample number 400. 8.1. Fault 1 (F 1 high)
The fault is detected at sample 420 ( Fig. 8 ). The estimated fault occurrence time is sample 405. Initially, the abnormal sensors are {1, 2, 3, 6}. The sensor evolution (along with the fitted trends), the abnormal sensors and the top two faults and their confidence indices are displayed in respective windows. After some time, the set of abnormal sensors becomes {1, 2, 3, 4, 5, 6}, and then it becomes {1, 2, 3, 4, 5, 6, 9, 10}, etc. After 0.27h from the estimated time of fault occurrence, QTA shows that the fault showing the the fault with ID (identification number in the database) 5 ( X  X ault 4 X  ( F 4 high), C.I .= 0.616). The operator is requested to verify if the actual fault is one of the above two faults. In a real plant, the operator would have to actually go to the concerned units, valves, pipelines, etc., and check whether or not they are functioning normally. Thus, it takes some time. In this case, since the actual fault is  X  X ault 1 X  (it is assumed that the operator is able to confirm this), the operator verifies that correct diagnosis has been performed. Now the operator is asked if he/she would like to add this new fault to the database. One reason for allowing this option is that the current fault magnitude may be quite different from the fault magnitude for the stored fault  X  X ault 1 X . The operator says  X  X o X . The operator is requested to reset this diagnosis session once this fault has evolved completely, i.e., all the variables return to NOR, so that the diagnostic system would be ready to diagnose faults in the future. The diagnosis session can be reset before evolution of the current fault is complete, but then, to diagnose a new fault that occurs before the current fault has completely evolved, QTA should be applied on the residuals (the actual sensor reading  X  the predicted signal values for the fault just diagnosed) instead of the actual signals. Clearly, for doing so, a good estimate of the fault occurrence time and the fault magnitude is required. It is suggested that a more quantitative approach should be employed to handle such multiple fault scenarios (even if the two faults occur at different times). A screen-shot of the prototype diagnosis station, after the operator has confirmed correct diagnosis, is shown in Fig. 8 . 8.2. Fault 25 (VLr m,bias low)
The fault is detected at sample 405. The estimated fault occurrence time is sample 391 ( Fig. 9 ). Initially, the abnormal sensors are {1, 2, 3, 6, 7, 9, 10, 11, 12}. After 0.4h from the estimated time of the fault occurrence, QTA shows that the fault showing the highest C.I . is  X  X ault 17 X  ( VLr m set high). Two other faults with high C.I . are the faults with ID 26 ( X  X ault 25 X ) and 19 ( X  X ault 18 X  ( VLr VP,bias high)). It can be noted that the Fault 17, Fault 25 and
Fault 18 are located in the same control loop. The operator is requested to verify if the actual fault is one of the above three faults. Since the actual fault is  X  X ault 25 X , the operator verifies that correct diagnosis has been performed. Now the operator is asked if he/she would like to add this new fault to the database. The operator says  X  X o X . At this point, the operator is requested to reset this diagnosis session once this fault has evolved completely. A screen-shot of the prototype diagnosis station, after the operator has confirmed correct diagnosis, is shown in Fig. 9 . 8.3. Fault 28 (VLs m,bias low, a novel fault with respect to the current database)
The fault is detected at sample 405. The estimated fault occurrence time is sample 391 ( Fig. 10 ). Initially, the abnormal sensors are {7, 11, 12}. After 0.9h from the estimated time of the fault occurrence, QTA shows that the fault showing the highest C.I . is  X  X ault 20 X  ( VLs m set high). In this case the diagnosis time has been longer since initially C.I . was low for a long time. No other fault showed a C : I : 4 0 : 5. The operator is requested to verify if the actual fault is the above fault. Since the actual fault is  X  X ault 28 X  (not contained in the list displayed), the operator indicates that an incorrect diagnosis has been performed, though the C.I . for  X  X ault 20 X  is quite high. Again, it is assumed that the operator is able to make this decision by actually checking that the set point for the separator level is normal. It can be noted that the fault  X  X ault 20 X  and the fault  X  X ault 28 X  are located in the same control loop. Since this is a novel fault, a request is automatically placed to add this fault to the database. The operator is requested to perform the diagnosis manually. The abnormal sensors are displayed and listed to assist the operator in manual diagnosis. A screen-shot of the prototype diagnosis station at this stage is shown in Fig. 10 . Trend extraction is continued.

After sometime, the operator indicates that manual diagnosis has been completed and that the actual fault is  X  X ault 28 X .
The operator is also requested to verify if the fault is actually an abnormal event or it is some operational event such as routine change, start-up of a pump, etc. In this case, it is a true abnormal event. A screen-shot of the prototype diagnosis station at this stage is shown in Fig. 11 after the operator has confirmed correct diagnosis. When operator indicates that the fault has sufficiently evolved, its signature is added to the database.
The diagnosis session is reset after complete evolution of the fault.

Thus, the prototype diagnostic system is quite robust with respect to correct fault diagnosis of the known faults (events) and detection of novel events. Interaction with the operator and maintenance are also facilitated. Another important characteristic of the overall approach is that most of the parameters used can be easily tuned and that reasonable variation in them does not degrade the overall performance. 9. Conclusions
In our previous research, we developed a novel algorithm for off-line extraction of qualitative trends from temporal data. In this article, various issues in on-line trend extraction and fault diagnosis have been discussed, and an algorithm for the same has been developed. The key feature of the algorithm is the use of an adaptive window size. A framework for on-line fault diagnosis has been discussed and a prototype has been developed in Matlab s programming environment. The utility of the framework has been illustrated through fault diagnosis of the Tennessee
Eastman benchmark process. The diagnostic system is able to correctly diagnose previously observed faults and is able to assist the operator in manual diagnosis of novel faults. The prototype diagnostic system also facilitates the update of the fault-signature database upon manual diagnosis of novel faults.
 References
