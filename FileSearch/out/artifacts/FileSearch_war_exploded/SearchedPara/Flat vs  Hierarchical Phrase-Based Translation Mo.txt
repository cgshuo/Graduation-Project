 Although context-independent word-based approaches re-main popular for cross-language information retrieval, many recent studies have shown that integrating insights from modern statistical machine translation systems can lead to substantial improvements in effectiveness. In this paper, we compare flat and hierarchical phrase-based translation mod-els for query translation. Both approaches yield significantly better results than either a token-based or a one-best trans-lation baseline on standard test collections. The choice of model manifests interesting tradeoffs in terms of effective-ness, efficiency, and model compactness.
 Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: SCFG, query translation
Despite the prevalence of context-independent word-based approaches for cross-language information retrieval (CLIR) derived from the IBM translation models [4], recent stud-ies have shown that exploiting ideas from machine transla-tion (MT) for context-sensitive query translation produces higher-quality results [17, 19, 24]. State-of-the-art MT sys-tems take advantage of sophisticated models with  X  X eeper X  representations of translation units, e.g., phrase-based [13], syntax-based [25, 27], and even semantics-based [11] models. In particular, hierarchical phrase-based machine translation (PBMT) systems [5] provide a middle ground between ef-ficient  X  X lat X  phrase-based models and expressive but slow syntax-based models. In terms of translation quality, ef-ficiency, and practicality, flat and hierarchical PBMT sys-tems have become very popular, partly due to successful open-source implementations.

This paper explores flat and hierarchical PBMT systems for query translation in CLIR. Previously, we have shown that integrating techniques from hierarchical models lead to significant gains in effectiveness X  X owever, it is unclear if such gains could have been achieved from  X  X lat X  represen-tations. This question is interesting because it opens up a different region in the design space: flat representations are faster, more scalable, and exhibit less complexity X  X ncoding a different tradeoff between efficiency and effectiveness.
There are two main contributions to this work: First, we test the robustness of query translation techniques intro-duced in earlier work [24] by comparing flat and hierarchical phrase-based translation models. In addition, we examine the effects of three different heuristics for handling one-to-many word alignments. We show that a combination-of-evidence approach consistently outperforms a strong token-based baseline as well as a one-best translation baseline for three different languages, Arabic (Ar), Chinese (Zh) and French (Fr), using either flat or hierarchical translation gram-mars. Second, we discuss differences between the two MT models and provide insights on the tradeoffs each represent. Experiments show that a hierarchical translation model yields higher effectiveness, which suggests that there is value in more sophisticated modeling of linguistic phenomena.
Although word-by-word translation provides the starting point for query translation approaches to CLIR, there has been much work on using term co-occurrence statistics to select the most appropriate translations [10, 15, 1, 21]. Ex-plicitly expressing term dependency relations has produced good results in monolingual retrieval [9, 18], but extend-ingthatideatoCLIRhasnotproventobestraightforward. Another thread of research has focused on translating multi-word expressions in order to deal with ambiguity [2, 28]. Borrowing ideas from MT for IR dates back to at least Ponte and Croft X  X  work on retrieval using language model-ing [20]. That work was later extended to translation models for retrieval [3], followed by a series of successful adaptations to the cross-language case [26, 14, 8].

As MT systems have evolved away from the token-based translation approach, researchers have started exploring ways to integrate various components of modern MT systems for better CLIR effectiveness. Magdy et al. [17] showed that preprocessing text consistently for MT and IR systems is beneficial. Nikoulina et al. [19] built MT models tailored to query translation by tuning model weights with queries and reranking the top n translations to maximize effective-ness on a held-out query set. While improvements were more substantial using the latter method, another interest-ing finding was the low correlation between translation and retrieval quality. This indicates that better translation may not necessarily help retrieval. As a baseline, we consider the technique presented by Darwish and Oard [6]. Given a source-language query s , we represent each token s j by its translations in the tar-get language, weighted by the bilingual translation proba-bility. These token-to-token translation probabilities, called Pr token , are learned independently from a parallel bilin-gual corpus using automatic word alignment techniques [4]. In this approach, the score of document d ,givensource-language query s , is computed by the following equations: In order to reduce noise from incorrect alignments, we im-pose a lower bound on the token translation probability, and also a cumulative probability threshold, so that translation alternatives of s j are added (in decreasing order of probabil-ity) until the cumulative probability has reached the thresh-old. Any weighting function can be used in conjunction with the tf and df values, and we chose the Okapi BM25 term weighting function (with parameters k 1 =1 . 2, b =0 . 75).
Machine translation can be divided into three steps: train-ing the translation model, tuning parameters, and decoding. We will mostly focus on the first step, since that is where flat and hierarchical MT approaches differ the most. The output of the first step is the translation model (called TM hereafter). For both flat and hierarchical variants, the TM consists of a set of rules (i.e., the translation grammar) in the following format:  X  =  X  0  X  1 ... ||  X  =  X  0  X  1 ... || A || (  X   X   X  ) We call the sequence of  X  i  X  X  the source side of the rule, and sequence of  X  j  X  X  the target side of the rule. The above indicates that the source side translates into the target side with a likelihood of (  X   X   X  ). 1 A contains token alignments in the format i -j , indicating that source token  X  i is aligned to target token  X  j .

A hierarchical model [5] differs from a flat model [13] in terms of rule expressivity: rules are allowed to contain one or more nonterminals, each acting as a variable that can be expanded into other expressions using the grammar, car-ried out in a recursive fashion. These grammars are called synchronous context-free grammars (SCFG), as each rule describes a context-free expansion on both sides. Consider the following two rules from an SCFG: R . [X] leave in europe || cong  X e de [X] en europe R . maternal || maternit  X e || 0-0 || 0.69
In R 1 , the non-terminal variable [X] allows an arbitrarily long part of the sentence to be moved from the left of the sentence in English to the middle of the sentence in French, even though it generates a single token (i.e., maternal )using R 2 in this particular example. As a result, an SCFG can capture distant dependencies in language that may not be realized in flat models.

Each sequence of rules that covers the entire input is called a derivation , D , and produces a translation candidate, t , which is scored by a linear combination of features. One can use many features to score a candidate, but two fea-tures are the most important: the product of rule likelihood values indicates how well the candidate preserves the origi-nal meaning, TM( t, D | s ), whereas the language model score, LM( t ), indicates how well-formed the translation is. Com-bining the two, the decoder searches for the best translation: There is a tradeoff between using either flat or hierarchical grammars. The latter provides more expressivity in rep-resenting linguistic phenomena, but at the cost of slower decoding [16]. On the other hand, flat models are faster but less expressive. Also, due to the lack of variables, flat grammars contain more rules, resulting in a more verbose translation grammar.
In our previous work [24], we described two ways to con-struct a context-sensitive term translation probability distri-bution using internal representations from an MT system. These distributions can then be used to retrieve ranked doc-uments using equations (1) X (3).
With appropriate data structures, it is possible to effi-ciently extract all rules in a TM (either flat or hierarchical) that apply to a given source query, s , called TM s .Foreach such applicable rule r , we identify each source token s j ignoring any non-terminal symbols. From the token align-ment information included in the rule structure, we can find all target tokens aligned to s j . For each such target token t the likelihood value of s j being translated as t i is increased by the likelihood score of r . At the end of the process, we have a list of possible translations and associated likelihood values for each source token that has appeared in any of the rules. We can then convert each list into a probability distri-bution, called Pr PBMT for flat and Pr SCFG for hierarchical grammars by normalizing the sum of likelihood scores: where s j  X  t i represents an alignment between tokens s j and t i and  X  is the normalization factor.

When a source token s j is aligned to multiple target tokens in a rule, it is not obvious how to distribute the probability mass. In our previous implementation [24], each alignment was treated as an independent event with the same proba-bility. We call this the one-to-one heuristic, and introduce two alternatives due to the following drawback: the target tokens aligned to s j are usually not independent. For exam-ple, the token brand is aligned to three tokens marque , de , fabrique (En. brand, of, factory ),whichisanappropriate translation when put together. Even if de is discarded as a stopword, the one-to-one heuristic will learn the token pair ( brand , fabrique ) incorrectly. An alternative heuristic is to ignore these rules altogether, assuming that good transla-tion pairs will appear in other rules, thus discarding these cases would not cause any harm (we call this the one-to-none technique). A third approach is to combine the target tokens into a multi-token expression. Thus, in the above example, we would learn the translation of brand as marque de fabrique , which is a useful mapping that we might not learn otherwise. We call the third technique one-to-many , and compare these three heuristics in our evaluation.
Given t (1) , the most probable translation of query s com-puted by equation (4), we can score a document d as follows:
Since MT systems generate a set of candidate translations in the process of computing equation (4), we can consider the n most likely candidates. For each candidate translation t ( k ) , and for each source token s j , we use token alignments to determine which tokens in t ( k ) are associated with s there are multiple target tokens, we apply one of the three methods introduced previously: one-to-none , one-to-one ,or one-to-many . By the end of the process, we obtain a prob-ability distribution of translations for each s j based on the n best query translations. If source token s j is aligned to (i.e., translated as) t i in the k th best translation, the value ( t ( k ) | s ) is added to its probability mass, producing the fol-lowing for Pr nbest (where  X  is the normalization factor):
For Pr token , translation probabilities are learned from all sentence pairs in a parallel corpus, whereas Pr SCFG/PBMT only uses portions that apply to the source query, which reduces ambiguity in the probability distribution based on this context. Pr nbest uses the same set of rules in addition to a language model to search for most probable transla-tions. This process filters out some irrelevant translations at the cost of less diversity, even among the top 10 or 100 translations. Since the three approaches have complemen-tary strengths, we can perform a linear interpolation of the three probability distributions: Pr Replacing any of these probability distributions introduced above for Pr token in equations (1) X (3) yields the respective scoring formula. We performed experiments on three CLIR test collections: TREC 2002 En-Ar CLIR, NTCIR-8 En-Zh Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 En-Fr CLIR, with sizes 383,872, 388,589 and 177,452 documents, respectively. We used the title text of the 50 topics for the Arabic and French collections, and we treated the 73 well-formed questions in NTCIR-8 as queries.
 For the flat and hierarchical translation models, we used Moses [12] and cdec [7], respectively. The training data consisted of Ar-En GALE 2010 evaluation (3.4m sentence pairs), Zh-En FBIS corpus (0.3m pairs), and Fr-En Europarl corpus v7 (2.2m pairs). A 3-gram language model was built for Arabic and Chinese using the target side of the parallel corpora. For French, we trained a 5-gram LM from the monolingual dataset provided for WMT-12. More details of the experimental setup can be found in [23].

Source code for replicating all the results presented in this paper is available in the open-source Ivory toolkit. 2 The baseline token-based model yields a Mean Average Precision (MAP) of 0.271 for Arabic, 0.150 for Chinese, and 0.262 for French. These numbers are competitive when compared to similar techniques applied to these collections. For each collection, we evaluated the three CLIR techniques ploring the effect of the different alignment heuristics as well as flat vs. hierarchical phrase-based translation models. Pa-rameters of the interpolated model were learned by a grid search. Experimental results are summarized in Table 1. 3
Based on a randomized significance test [22], the interpo-lated model outperforms (with 95% confidence, marked *) the token-based model for all runs except for Arabic with Moses , consistently with the one-to-many heuristic and in some cases with the two other heuristics. Furthermore, in five out of the six conditions, the interpolated model with the one-to-many heuristic is significantly better than the one-best MT approach (marked  X  ). This confirms that com-bining different query translation approaches is beneficial, and is also robust with respect to the test collection, lan-guage, and underlying MT model. The one-to-many term mapping heuristic seems to be the most effective overall.
However, the two MT models display significant differ-ences in the  X  X rammar X  column, as the hierarchical model significantly outperforms the flat model. This supports the argument that the former is better at representing trans-lation alternatives since it is more expressive. Also as a result of this difference, the flat grammar is much larger than the hierarchical one, which leads to an order of magni-tude increase in processing time for Pr PBMT . 4 These differ-ences become especially important for the Arabic collection, using either MT system. An additional benefit of using Pr
SCFG/PBMT is that we do not need to tune model param-eters for translation, which is computationally intensive. It is also interesting that the differences between the two MT models are insignificant for the 10-best approach, where the decoder finds similar translations in both cases. There-fore, it might be better to use flat representations for the 10-best approach for efficiency, since the end-to-end trans-lation process is faster than hierarchical models. http://ivory.cc/ In this paper, we extended an MT-based context-sensitive CLIR approach [24], comparing flat and hierarchical phrase-based translation models on three collections in three differ-ent languages. We make a number of interesting observa-tions about the tradeoffs in incorporating machine transla-tion techniques for query translation.

A combination-of-evidence approach was found to be ro-bust and effective, but we have not examined how the in-terpolation model parameters can be learned using held-out data X  X his is the subject of ongoing work. Also, we are exploring ways of leveraging the translation of multi-token source-side expressions. Although we demonstrated the ben-efits of hierarchical grammars, we still do not explicitly take advantage of non-terminal information in the rules. It might be beneficial to perform a detailed error analysis to see what types of topics are improved with the use of SCFGs over flat grammars. Finally, we briefly discussed interesting trade-offs between efficiency and effectiveness, but more detailed experiments are required to better understand different op-erating points and the overall design space.
This research was supported in part by the BOLT pro-gram of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under awards IIS-0916043 and IIS-1144034. An y opinions, findi ngs, conclu-sions, or recommendations expressed are those of the au-thors and do not necessarily reflect views of the sponsors. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.
