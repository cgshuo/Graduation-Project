 1. Introduction
Using an intelligent modeling approach, a model can be set up for a target system, with which the relationship of input X  X utput behavior can be approximated. This is known as system modeling. For function approximation, optimization, and forecasting, system modeling has been widely investigated for years. Time series forecasting is one of the momentous applications in system modeling. In a time series, time is usually a very important factor to make decision or prediction. Data in past history recorded in time sequence is called a time series. Managers usually use historical data to forecast various types of variables such as changes in stock, sales of products, population growth, and many others. The accurate and valuable prediction of these variables can assist managers to make a useful decision. For a more refined definition, time series is a group of statistics, according to the order which events are occurred in time sequence. For instances, daily average temperature or monthly rainfall at a place, daily stock market closing price, company X  X  turnover, unemployment rate, economic growth rate, and total amount of national income and export. Modern business and economic activities, in essence, are dynamic, and they change frequently. How to make a reliable forecast is one of the most important issues for modern enter-prises and organizations. The usage of time series to forecast the future tendency has to make use of detailed data, which were generated for some time past in the cause of understanding the trend of changes.

In the past four decades, various approaches have been presented for time series forecasting ( Kim et al., 1997 ; Xiong, 2001 ; Box and Jenkins, 1976 ; Chen et al., 1991 ; Jang, 1993 ; Cho and Wang, 1996 ; Kasabov et al., 1997 ; Kim and Kasabov, 1999 ; Nauck and Kruse, 1999 ; Paul and Kumar, 2002 ; Sfetsos and
Siriopoulos, 2004 ; Chen et al., 2005 ; Gao and Er, 2005 ; Chen 2008 ; Sugiarto and Natarajan, 2007 ; Zounemat-Kermani and
Teshnehlab, 2008 ; Deng and Wang, 2009 ; Graves and Pedrycz, 2009 ; Zhao and Yang, 2009 ), including powerful auto-regressive moving average (ARMA), fuzzy-based paradigm, neural-net com-puting model, neural fuzzy hybrid system, and others. In the recent twenty years, fuzzy system is one of the most frequently used methods ( Kim et al., 1997 ; Xiong, 2001 ; Jang, 1993 ; Cho and
Wang, 1996 ; Kim and Kasabov, 1999 ; Nauck and Kruse, 1999 ; Paul and Natarajan, 2007 ; Zounemat-Kermani and Teshnehlab, 2008 ).
Many kinds of optimization algorithms have been used for fuzzy systems. The design of fuzzy systems to forecasting has been also proposed in related researches. For instance, Takagi and Sugeno (T X  X ) fuzzy model with fuzzy C -regression clustering model was used to determine number of fuzzy rules and gradient descent algorithm was applied for tuning the free parameters of T X  X  fuzzy model ( Kim et al., 1997 ). A fuzzy model with genetic-based premise learning was proposed by Xiong (2001) , for the well-known gas furnace time series dataset of Box and Jenkins. Hybrid neural fuzzy inference system (HyFIS) was developed by Kim and Kasabov (1999) , for building and optimizing the fuzzy model.
Adaptive-network-based fuzzy inference system (ANFIS) was proposed, where the backpropagation (BP) method and the recur-sive least-squares estimator (RLSE) were combined to adapt free parameters ( Jang, 1993 ). Almost all of these studies used general
T X  X  fuzzy model ( Takagi and Sugeno, 1985 ) for system modeling and time series forecasting. Besides, neural networks are another major approach to time series forecasting in artificial intelligence 2006 ; Deng and Wang, 2009 ). Most of these studies focused on designing new structure of neural network and applying different learning methods to improve the accuracy of prediction. For example, Chen et al. (2006) presented local linear wavelet neural network (LLWNN). A hybrid training algorithm of particle swarm optimization (PSO) with both diversity learning and gradient descent method was introduced for the training of the LLWNN.
Deng and Wang (2009) proposed a novel incremental learning approach (ILA) based on a hybrid fuzzy neural net framework.
However, most of these researches used stationary time series to verify the forecasting performance of their approaches or models. Basically, a time series can be viewed as a set of observations from a stochastic process. A time series is considered stationary if the stochastic process is with the property that probability distribution does not change with time shift, so that the statistics such as mean, variance, and covariance of the time series are of finite constant. In contrast, a non-stationary time series whose probability distribution is changing with time. Thus, the mean, variance and covariance vary with time. This class of time series is usually hard to find the regularity for a prediction model. Traditional mathematical approaches to non-stationary time series are usually feeble and hard to attack such problems with satisfactory performance. Practically, time series in real world usually are non-stationary, especially in economic and finance field. The fluctuations and changes of these non-station-ary time series are quite enormous, such as daily closing stock price. Accordingly, the prediction by forecasting models may deviate from the future value easily. For non-stationary time series, the auto-regressive integrated moving average (ARIMA) model, first proposed by Box and Jenkins, has been used in order to make forecasting more accurate. For ARIMA process, models that describe homogeneous non-stationary behavior can be obtained by supposing some suitable difference of the process so that the differenced version of the time series may become stationary ( Box and Jenkins, 1976 ). An ARIMA model combines three different processes including an auto-regressive (AR) pro-cess, integration part, and a moving average (MA) process. In previous researches, the concept of ARMA model was used in the fuzzy inference system or neural network. Gao and Er (2005) proposed a ARMA-like model using fuzzy neural network method , called the nonlinear autoregressive moving average with exo-genous inputs (NARMAX). They used TSK-type weight vector to mimic ARMA model with exogenous inputs, and cast the NAR-MAX model into a fuzzy neural network (FNN). Basically, the
NARMAX approach is based on the so-called G-FNN framework, whose functionality is equivalent to a TSK-type fuzzy inference system. A hybrid methodology combining an artificial neural network (ANN) and ARMA models was proposed ( Rojas et al., 2008 ) to investigate time series forecasting. However, the fore-casting performance and accuracy were not good enough in these previous works. In our study, we propose an innovative hybrid computing paradigm, integrating both neuro-fuzzy system (NFS) and ARIMAs together to achieve ARIMA-based neuro-fuzzy non-linear-mapping ability for accurate forecasting. Our approach expands ARIMA from its linear modeling ability to the horizon of nonlinear modeling by neuro-fuzzy method, so that excellent performance for accurate forecasting can be achieved. With the neuro-fuzzy system (NFS) theory, ARIMA models are embedded into fuzzy If-Then rules to construct a new NFS X  X RIMA predictor to the problem of time series forecasting, equipped with a novel hybrid learning ability. NFS has been a useful modeling tool to represent and process linguistic information and to deal with uncertainty and imprecision ( Jang et al., 1997 ). In fuzzy theory, fuzzy sets can be used to reflect human concepts and thoughts, which tend to be incomplete and vague ( Zadeh, 1965 , 1975 ; learning and adaption, there are perspectives for the use of neuro-fuzzy framework. Basically, neural networks (NNs) and fuzzy systems (FSs) have been proven universal approximators, which can theoretically approximate any function to any degree of accuracy on a compact set. Neuro-fuzzy systems have been successfully applied in various applications. The integration of them tends to adopt their merits to become a unified framework of computing model for information processing, where NN can provide capability for flexible adaptive low-level structure for learning from examples and pattern recognition, and FS can provide high-level comprehensive rule inference for imprecise information processing and decision-making. In this paper, the basic idea of combining fuzzy system and neural network is to design a neuro-fuzzy framework t hat uses a fuzzy inference system to represent knowledge in an interpretable manner, embedded in
PSO method does not need the support of NN, we think this neuro-fuzzy integration can augment the value of fuzzy system in the perspective of neural network. They complement to one another.
Through the connectionist structure by NN, FS can be transformed into a neuro-fuzzy system, providing an insight to the formation of knowledge-base structure and possible design types for FS in the view point of neural network. Therefore, we adopt the participation of NN that can positively provide adaptive flexibility and unified model framework to th e proposed approach.

In this paper, a novel intelligent approach is presented for time series forecasting, using Takagi X  X ugeno (T X  X ) neuro-fuzzy model ( Jang et al., 1997 ; Sugeno and Kang, 1988 ) and ARIMAs. The implementation of a T X  X  neuro-fuzzy model is very similar to that of a fuzzy logic inference system, except the consequents are described by functions of crisp inputs. An appropriate neuro-fuzzy model for prediction is very important to forecast time series accurately. How to design and adjust fuzzy sets and fuzzy neuro-fuzzy approach is denoted as NFS X  X RIMA, where ARIMA models are embedded in the fuzzy rules. For different time series, different order of NFS X  X RIMA can be conducted to make fore-casting as good as possible. Furthermore, because there are usually many unknown parameters in the NFS X  X RIMA, the selec-tion of learning algorithm plays an extremely pivotal position. In order to adapt the free parameters, a hybrid learning algorithm is proposed in this paper, using both the particle swarm optimiza-tion (PSO) ( Kennedy and Eberhart, 1995 ) and the recursive least-used for the update of the premise parameters of the NFS X  X RIMA; the RLSE for the consequence parameters; they work together in a hybrid way. The focus is to find the optimal or near-optimal solution for the NFS X  X RIMA, so that the forecasting performance can be as accurate as possible. The PSO is a swarm based heuristic method, which is useful for global optimization problems, and the RLSE can estimate a linear model optimal in a very efficient way. With the hybrid learning method, the NFS X  X RIMA is expected to have accurate forecasting.

In Section 2 , we present the theory of NFS X  X RIMA. In Section 3 , we describe the hybrid learning method, combining the PSO and the RLSE for the proposed NFS X  X RIMA. In Section 4 , three examples are used to test the proposed approach, including the Mackey-Glass chaos time series, the star brightness time series and the daily IBM stock closing price time series. Finally, the paper is concluded in Section 5 for the proposed approach and experimental results. 2. ARIMA based neuro-fuzzy system
To specify the proposed multiple-input X  X ingle-output ARIMA based neuro-fuzzy system, we begin with the first-order T X  X  fuzzy model. To generate fuzzy rules from a given input X  X utput data set, the first T X  X  fuzzy model was proposed by Takagi and Sugeno (1985) to develop a systematic approach, where the consequents of one-output fuzzy system with K fuzzy rules in the rule base. The form of first order T X  X  fuzzy rules can be given as follows: Rule i : IF x 1 is i s 1  X  h 1  X  and x 2 is i s 2  X  h 2  X  ... and x s  X  h M  X  Then i y  X  i a 0  X  i a 1 h 1  X  X  i a M h M  X  1  X  numerical variables), which can be obtained from observations of a time series; i y is the output variable of the i th fuzzy rule; f 1 , i s 2 , ... i s M g are the fuzzy sets of the i th rule; f the consequent parameters of the i th rule. The fuzzy model can be cast into neural structure to be a n euro-fuzzy system. The proposed approach is based on the NFS methodology. In the study, input data are from the observations of a time series.

In time series forecasting, the approach of autoregressive moving average (ARMA) model, sometimes called Box X  X enkins model, is the most used approach to predict future value. For an ARMA model, the observation of a variable in interest for a time series can be expressed as a linear combination of past observa-tions and shocks. Basically, an ARMA process consists of two parts, an auto-regressive (AR) process and a moving average (MA) for a time series can be expressed as follows: where t is the time index, a 0 is a constant term, f a 1 , a the process parameters, and e ( t ) is a random shock at time t , which is assumed to be independently and identically distributed order, the observation X ( t ) is given below X  X  t  X  X  u  X  e  X  t  X  y 1 e  X  t 1  X  y 2 e  X  t 2  X  y q e  X  t q  X  X  3  X  where u is the expectation of X ( t ), f y 1 , y 2 , ... , y the iid property. For an ARMA model, the observation X ( t ) can be expressed as follows: X  X  t  X  X  a 0  X  a 1 X  X  t 1  X  X  a 2 X  X  t 2  X  X  X  a p X  X  t p  X  X  e  X  t  X 
For nonstationary time series, auto-regressive integrated mov-ing average (ARIMA) models are the most common class to tranquilize time series, using the differencing transformation. The first ARIMA was presented by Box and Jenkins (1976) . The
ARIMA and its variants have been used prosperously in many areas of time series forecasting. Let us define the operations below
B
X  X  t  X  X  X  t n  X  X  5  X  r  X  1 B  X  X  6  X  where B is called the backward shift operator and r is called the difference operator. Based on (5) and (6), we further make the following definition: c  X  t  X  r d X  X  t  X  X  X  1 B  X  d X  X  t  X  X  7  X  where d is called the difference order, which is a non-negative integer. Based on (4), the general form of an ARIMA process is expressed below c  X  t  X  X  a 0  X 
Based on (8), an ARIMA predictor can be setup. Assume the errors in the forecasting process. The general form of the ARIMA prediction model can be expressed below ^ c  X  t  X  X  a 0  X  j  X  1,2, y , q } are the model parameters. Note that if d  X  0, ARI-degenerates to AR( p ), and ARIMA(0,0, q ) to MA( q ). The forecast e  X  t  X  X  X  t  X  ^ X  X  t  X  X  10  X 
Based on (7), the relationship between ^ X  X  t  X  and ^ c  X  t  X  is expressed as ^ c  X  t  X  X  r d ^ X  X  t  X  X  X  1 B  X  d ^ X  X  t  X  X  11a  X  we have ^ c  X  t  X  X  r ^ X  X  t  X  X  X  1 B  X  ^ X  X  t  X  X  ^ X  X  t  X  observation X ( t 1) is already known at time t , it can be used to replace ^ X  X  t 1  X  . Thus, the expression for the forecast given as follows: ^ X  X  t  X  X  ^ c  X  t  X  X  X  X  t 1  X  X  11b  X 
For d  X  2, the forecast ^ X  X  t  X  can be expressed as follows: ^ X  X  t  X  X  ^ c  X  t  X  X  X  X  t 1  X  X  r X  X  t 1  X  X  11c  X 
For d  X  3, the forecast ^ X  X  t  X  can be given below ^ X  X  t  X  X  ^ c  X  t  X  X  X  X  t 1  X  X  2 r X  X  t 1  X  r X  X  t 2  X  X  11d  X 
For higher difference order ( d Z 4), based on (11a), similar operation can be applied to obtain the forecast ^ X  X  t  X  .
The consequent in (1) can be viewed as AR( p ) model, where p  X  M . With the consideration of time, the form of consequent part can be extended with ARMA( p , q ) model, given as follows: y  X  t  X  X  i a 0  X  i a 1 h 1  X  t  X  X  X  i a M h M  X  t  X  i y 1 e  X  t 1  X 
Furthermore, the ARIMA( p , d , q ) in (9) can be embedded to T X  X  fuzzy rules whose consequents are expressed in the following form: y  X  t  X  X  i ARIMA  X  p , d , q  X  X  i a 0  X  for i  X  1,2, y , K , where { i a j , j  X  0,1, y p } and { consequent parameters of the i th fuzz rule; { e ( t k ), k  X  1,2, are prediction errors; { c ( t j ), j  X  1,2, y , p } are the ( p , d , q ) state.

For example, if an ARIMA(2,1,2) model is used, the form of i y  X  t  X  for the i th fuzzy rule can be expressed as follows: y  X  t  X  X  i a 0  X  i a 1  X  X  X  t 1  X  X  X  t 2  X   X  i ARIMA  X  2 , 1 , 2  X  X  14  X  For convenience, the proposed predictor is denoted by NFS X  written as follows:
Rule i : IF x 1 is i s 1  X  h 1  X  t  X  X  and x 2 is i s 2  X  h s  X  h M  X  t  X  X  Then i y  X  t  X  X  i ARIMA  X  p , d , q  X  X  15  X  be an ARIMA based neuro-fuzzy system, shown in Fig. 1 , which is a six-layer neuro-fuzzy system. The explanation for the six layers is specified as follows:
Layer 0: this layer is called the input layer. Each node in this layer corresponds to a crisp variable of the input vector H ( t )  X  [ h ( t ), h 2 ( t ), y , h M ( t )].

Layer 1: the layer is called the fuzzy-set layer. Each node of the layer represents a linguistic value, characterized by a fuzzy set.
Each node output indicates a membership degree. For the design of fuzzy sets, we select Gaussian membership functions, which have the following advantages. The class of Gaussian membership functions (MFs) only needs less number of parameters than other class of MFs, such as triangular MF (using three parameters) and trapezoidal MF (using four parameters). A Gaussian MF may only need the parameters of mean and spread to define its function.
Gaussian MFs have smooth and continuously differentiable prop-erty, providing good transition behavior from full membership to zero membership for the definition of fuzzy set. And, Gaussian
MFs are with very long tails theoretically, which are good for the design of proposed models, providing excellent completeness for input-space partition and good overlap of fuzzy sets. In general, the properties of completeness and fuzzy-set overlap are very important to NFS-based applications. The general form of Gaus-sian membership function is given as follows: gaussmf  X  h ; m , s  X  X  exp  X  0 : 5  X  X  h m  X  = s  X  2  X  X  16  X  where h is a input base variable; m and s are the parameters of mean and spread, respectively. The parameters of mean and spread for all the fuzzy sets to define the premises of the proposed NFS X  X RIMA are called the premise parameters.
 Layer 2: the layer is for the firing strengths of the K fuzzy rules.
The nodes perform the fuzzy-and operations for the premise parts of the fuzzy rules. Each node output indicates a firing strength for its corresponding fuzzy rule. The firing strength for the i th rule, denoted as i b , is given below b  X  t  X  X  set of the i th rule.

Layer 3: the normalization of the firing strengths of the fuzzy rules is performed in this layer. The normalized firing strength of the i th fuzzy rule is given as follows: i g  X  t  X  X 
Layer 4: the layer is called the consequent layer. The nodes in the layer perform the normalized consequents of all the fuzzy follows: parameters of the i th fuzzy rule; B is the backward shift operator.
Layer 5: the layer is called the output layer. There is only one node in the layer for single output. The node in this layer combines all the outputs from Layer 4 to produce the inferred ^ c  X  t  X  X 
Once the consequent parameters are determined, the inferred result by the NFS X  X RIMA can be expressed as follows: ^ c  X  t  X  X  obtained by (11b), (11c), and (11d), respectively.

Assume that there are observations for a time series, which can be collected to be used as training data for the proposed neuro-fuzzy predictor. The training data (TD) is given as follows: TD  X f X  X  i  X  , i  X  1 , 2 , ... g X  22  X 
To overview the prediction approach to the problem of time series forecasting, an illustrative diagram is given in Fig. 2 .
The input vector H ( t ) is from the TD in (22). The NFS X  X RIMA predictor generates an inferred result, which is then transformed produced. Assume there are N prediction errors { e ( t ), t  X  1,2, to involve in the training process, where these errors are used to design a cost function in terms of root mean squared error (RMSE) for training purpose. The cost function is to be minimized so that the proposed NFS X  X RIMA predictor can be optimized. The defini-tion of RMSE is given as follows:
RMSE  X 
The RMSE is used as a performance measure. Note that mean squared error (MSE), defined as MSE  X  RMSE 2 , can also be used for performance measure. Based on RMSE in (23a), the non-dimen-sional error index (NDEI) can be defined as follows: NDEI  X  RMSE s where s X is the standard deviation of the target time series. The premise parameters in Layer 1 and the consequent parameters in Layer 4 can be viewed as the two subsets of the free parameters. These parameters are called the system parameters of the NFS X 
ARIMA. They can be adapted by machine learning algorithms such as PSO and RLSE. In the following section, we specify the PSO X 
RLSE hybrid learning method for the proposed NFS X  X RIMA. 3. Hybrid PSO X  X LSE learning method 3.1. Particle swarm optimization(PSO)
The original PSO was first proposed by Kennedy and Eberhart (1995) . The method of PSO is an excellent approach, having collective wisdom concept to the evolution of optimization search. The PSO and its variants have been applied successfully to a variety of application problems ( Shi and Eberhart, 1999 ;
Parsopoulos and Vrahatis, 2002a , 2002b ; Juang, 2004 ; Coello et al., 2004 ; Shi and Eberhart, 1998 ). For the PSO algorithm, a and all the particles comprise a population, called a  X  X  X warm X  X  of the birds. Each particle moves toward its own best position and the swarm-best position. All particles compete with each other to become the swarm best. By this way, PSO combines the behaviors of both individual search and swarm search. There exists a subtle relationship for competition and cooperation in the search pro-cess. All particles shall remember their own best positions during the searching process. Besides, all particles have their own velocities in order to determine their search movements. Itera-tively, particles search for the optimal solution, using the concept of fitness function which is designed with RMSE. A fitness function sometimes is called a cost function. Particles change their search directions by means of two search memories, which are Pbest (the best location of individual particle) and Gbest (the is shown in Fig. 3 (a), and an illustration of position evolution in the search process by a PSO swarm is shown conceptually in
Fig. 3 (b), where the target position denotes the location of the swarm, labeled with A and B, respectively. Both particles are
B is the Gbest of the swarm at t  X  2. Assume the problem space is with Q dimensions. The particles X  velocities are updated as follows:
V  X  t  X  1  X  X  w V i  X  t  X  X  c 1 x 1  X  Pbest i  X  t  X  L i  X  t  X  X  where V i ( t )  X  [ v i ,1 ( t ), v i ,2 ( t ), y , v i , Q particle at time t ,{ c 1 , c 2 } are the parameters for PSO , { x random numbers in [0,1], and w is the inertia weight ( Shi and Eberhart, 1998 ). The particles locations are updated as follows:
L  X  t  X  1  X  X  L i  X  t  X  X  V i  X  t  X  1  X  X  25  X  where L i  X  t  X  X  X  l i , 1  X  t  X  , l i , 2  X  t  X  , ... , l at time t .

The implementation procedure of PSO is given in Fig. 4 , where f ( L ) indicates the cost in RMSE for the current location of the i th the cost for Gbest . 3.2. Recursive least-squares estimator (RLSE)
In general, the least squares estimation (LSE) problem ( Jang y  X  f  X  u  X  y 1  X  f 2  X  u  X  y 2  X  X  f m  X  u  X  y m  X  e  X  26  X  where y is the target; u is the model X  X  input; { f i (.), i  X  1,2, to be estimated; e indicates model error. Substituting training equations, given as follows: y  X  f 1  X  u 1  X  y 1  X  f 2  X  u 1  X  y 2  X  X  f m  X  u 1  X  y m  X  e 1 y  X  f 1  X  u 2  X  y 1  X  f 2  X  u 2  X  y 2  X  X  f m  X  u 2  X  y m  X  e 2 ^^^^ y  X  f 1  X  u N  X  y 1  X  f 2  X  u N  X  y 2  X  X  f m  X  u N  X  y m  X  e
The LSE problem can be written in a concise matrix form below
Y  X  A h  X  e  X  28a  X  where
A  X  2 6 6 6 6 4 h  X  X  y 1 , y 2 , ... , y m T  X  28c  X 
Y  X  X  y 1 , y 2 , ... , y N T  X  28d  X   X  hi T
Note that A is the input matrix; h is the parameter vector to be estimated; Y is the target vector; e is the error vector.
By the recursive least-squares estimator (RLSE) method ( Jang et al., 1997 ), the optimal solution for h can be obtained sequen-tially and recursively. At iteration k , the transient estimator is denoted as h k , which can be calculated recursively below
P  X  P k h (28a). And then, the h N is the estimate of h for the consequent parameters of the proposed NFS X  X RIMA. Before the RLSE in (29a) and (29b) is started, h 0 can be initially set to zero and P set below
P  X  a I  X  30  X  where a is a large positive value and I is the identity matrix. 3.3. Hybrid PSO X  X LSE learning algorithm A hybrid PSO X  X LSE method is devised to train the proposed NFS X  X RIMA for the purpose of fast learning, where the PSO is used to update the premise parameters and the RLSE is used to update the consequent parameters. The implementation flow-chart of the PSO X  X LSE is shown in Fig. 5 .
 The procedure in steps for traini ng the NFS X  X RIMA is given below. Step 1. Collect sample data. Some portion of the data is used for Step 2. Update the premise parameters by the PSO.
 Step 3. Update the consequent parameters by the RLSE. The Step 4. After the update of all parameters for the NFS X  X RIMA, Step 5. Calculate fitness in RMSE for each PSO particle. Step 6. Update Pbest for each particle and Gbest for the Step 7. If any stopping criterion is satisfied, stop the training 4. Experimentation Example 1. Mackey-Glass chaos time series
The well-known Mackey-Glass chaos time series is used to test the proposed approach for performance comparison to other approaches in the literature. The time series is defined as follows: _ x  X  t  X  X  where t  X  17. The time step is given as 0.1 s. The initial condition the proposed NFS X  X RIMA predictor for the chaos time series is given below: where t is the time index, P  X  D  X  6, and D  X  4. From the Mackey-training of NFS X  X RIMA is given as follows: H  X  t  X  X  X  x  X  t 18  X  , x  X  t 12  X  , x  X  t 6  X  , x  X  t  X  X  35  X 
X  X  t  X  X  x  X  t  X  6  X  X  36  X  and X ( t ) is the corresponding target. There are 1000 data pairs generated. The first 500 data pairs are used for training and the remaining data pairs are used for testing. Two proposed predic-tors, NFS X  X RIMA(4,0,0) and NFS X  X RIMA(4,1,0), are designed and tested for the example. Each predictor has four inputs. Each input variable has two fuzzy sets. There are 16 fuzzy rules. For each predictor, there are 16 premise parameters and 80 consequent parameters. All of the fuzzy sets are with the form of Gaussian NFS-ARIMA RLSE PSO membership function in (16). For the training of the two NFS X 
ARIMA predictors, the settings of the PSO X  X LSE hybrid method are given in Table 1 . After training, both of the NFS X  X RIMA predictors performed very well, and the NFS X  X RIMA(4,1,0) per-formed better than the NFS X  X RIMA(4,0,0). The prediction perfor-mance by the NFS X  X RIMA(4,1,0) is 8.7 10 4 in RMSE for training phase and 8.6 10 4 for testing phase, while the NFS X 
ARIMA(4,0,0) has the prediction performance 1.4 10 3 and 1.3 10 3 for training and testing phases, respectively.
The learning curve of NFS X  X RIMA(4,1,0) is shown in Fig. 6 . The prediction result is shown in Fig. 7 (a), where the Mackey-Glass line. Practically, the proposed approach has very accurate perfor-mance, whose prediction response is very close to the real time series. The prediction and real time series curves are almost coincided with each other. For clear display, we enlarge a portion one another, even in this enlargement scale. After training, the fuzzy sets of the four inputs are shown in Fig. 8 .

The performance comparison to other approaches in the literature is given in Table 2 , where the proposed approach shows very excellence performance superior to the compared approaches, especially the NFS X  X RIMA(4,1,0).

After training, the parameters of NFS X  X RIMA(4,1,0) are given in Table 3 .
 Example 2. Star brightness time series
For 600 successive days, the brightness observations at mid-night of a variable star were recorded to be the star brightness time series. The dataset is obtained from Hyndman , denoted as
The first 300 samples are used for training and the remaining 300 samples are used for testing. The data range is normalized into the predicting model are selected as follows: H  X  t  X  X  X  y  X  t 1  X  , y  X  t 2  X  , y  X  t 3  X  X  37  X  X  X  t  X  X  y  X  t  X  X  38  X 
Two predictors NFS X  X RIMA(3,0,0) and NFS X  X RIMA(3,2,2) are used in the example. Each NFS X  X RIMA predictor is with three inputs. Each input variable has two fuzzy sets. For each predictor, there are 8 fuzzy rules. The fuzzy sets are with the form of Gaussian membership function. The cost function is designed by MSE. To train the predictors, the settings for the PSO X  X LSE hybrid learning method are given in Table 4 .
 Both of the proposed predictors did well, especially for NFS X 
ARIMA(3,2,2), whose prediction performance is 1.3977 10 4 MSE for training, 1.9932 10 4 in MSE for testing and 0.0536 in
NDEI for testing. The learning curve of NFS X  X RIMA(3,2,2) is shown in Fig. 9 . 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 membership degree 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 membership degree
Obviously, the proposed hybrid learning method shows very fast convergence speed. In few iterations, it already reaches near to the converged MSE. The prediction response by the NFS X  X RIMA(3,2,2) is shown in Fig. 10 .

The performance comparison to other approaches in the literature is given in Table 5 , where the NFS X  X RIMA(3,2,2) shows outstanding performance to the compared approaches. The parameters of NFS X  X RIMA(3,2,2) after training are given in Table 6 .
 Example 3. Time series of daily IBM stock closing price
The 1000 data of daily IBM stock closing price from March 3, 2006 to February 25, 2010 are obtained from Yahoo Website , ing fluctuation dramatically and the range variety of stock closing prices is severe, from 70 to 130 largely. The data is normalized to the rest data are for testing. The time series is highly non-stationary. Three predictors NFS X  X RIMA(2,0,0), NFS X  X RIMA (2,1,0) and NFS X  X RIMA(2,1,1) are designed in the example.
Because the stock closing price is usually correlated to previous predictors is given as follows: H  X  t  X  X  X  y  X  t 1  X  , y  X  t 2  X  X  39  X  X  X  t  X  X  y  X  t  X  X  40  X 
Each of the proposed predictors has two inputs, each of which has two Gaussian fuzzy sets. There are 4 fuzzy rules for each predictor. The cost function is designed by RMSE. The settings of the PSO X  X LSE hybrid learning method are given in Table 7 .
After learning, the three predictors are compared for their performances. The performance comparison for the three pre-dictors is shown in Table 8 , where the NFS X  X RIMA(2,1,0) outper-forms the NFS X  X RIMA(2,0,0) and shows slightly better than NFS X  X RIMA(2,1,1).
 The prediction responses by the three predictors are shown in Figs. 11 X 13 , respectively. The learning curves for the NFS X  ARIMA(2,1,0) and the NFS X  X RIMA(2,1,1) are shown in Figs. 14 and 15 , respectively.

The parameters of NFS X  X RIMA(2,1,0) and NFS X  X RIMA(2,1,1) after learning are shown in Tables 9 and 10 , respectively. Remarks . The difference between NFS X  X RIMA(2,1,0) and NFS X 
ARIMA(2,1,1) is only by the 1st-order moving average (MA), which is the information of prediction error at last time step.
The information by MA may become either error-correction information to improve prediction or the inquisitive signal to degrade performance, depending on the nature of time series. Through the experiment, NFS X  X RIMA(2,1,0) outperforms NFS X 
ARIMA(2,1,1) slightly, showing the time series of the daily IBM stock could tend to the latter nature. But, the performances by the
NFS X  X RIMA(2,1,0) and NFS X  X RIMA(2,1,1) are very close. 5. Discussion and conclusion
The proposed NFS X  X RIMA approach with the novel PSO X  X LSE hybrid learning method has been presented for the problem of time series forecasting. Three examples have been demonstrated for the proposed approach in the paper. The rationales of neuro-fuzzy system and ARIMAs are combined to form the proposed
NFS X  X RIMA prediction computing model. An NFS is a hybrid of fuzzy logic and neural network, where fuzzy logic can be used to handle ambiguous information and perform inference to produce useful result for decision-making while neural network is famous for its learning ability and fault tolerance. Both fuzzy inference system (FIS) and neural network (NN) have been proved universal approximators that the fusion of FIS and NN become natural to have their advantages in applications. Moreover, ARIMA is well-known for its ability to deal with non-stationary time series and and MA. Therefore, the proposed NFS X  X RIMA model becomes an excellent adaptive predictor with great flexibility in structure selection, in terms of AR, ARI, MA, ARMA, or ARIMA. And, it also has great potential to be an accurate predictor, if appropriate learning method is applied to adapt the predictor to optimal state (or near optimal state).

For the parameter learning pe rspective, the set of NFS X  X RIMA parameters is divided into two subs ets, the subset of If-part para-meters and the subset of Then-part parameters. The division of the parameter set of NFS X  X RIMA makes it much easier to evolve the model to the optimal (or near optimal) state. The novel PSO X  X LSE hybrid learning method has been devised for this purpose. The well-known PSO is excellent in swarm-based search for optimization. With appropriate settings, PSO has much less chance of being trapped by a local minimum in the problem space. On the other hand, for the linear least squares estimation problem, the RLSE is famous for its capability of speedy optimization, using much less computational overhead than its traditional LSE version, in terms of computational time and resource. As a result, the hybrid PSO X  X LSE method shows very fast learning convergence f or the proposed NFS X  X RIMA, for example, the learning curves in Figs. 6, 9, 14, and 15 for the three its converged value. And, the prediction performance by the proposed approach is superior or comparable to those by the compared researches, for example, as shown in Tables 2 and 5 .In Example 3 , with 4 fuzzy rules only, the propo sed approach has been tested, using the IBM stock time series to show its capability for real-world application. As a result, the NFS X  X RIMA(2,1,0) and the NFS X  X RIMA(2,1,1) performed better than the NFS X  X RIMA(2,0,0), as shown in Table 8 . In this case the NFS X  X RIMA(2,1,0) and the NFS X  X RIMA(2,1,1) have shown over 12% more accurate than the
NFS X  X RIMA(2,0,0) in testing. This confirms our thought that with difference processing (integrati on part for the model) the proposed approach may perform better prediction. Moreover, in Example 1 , with NFS X  X RIMA(4,1,0) comparing to NFS X  X RIMA(4,0,0), even much better (more than 33%) result has been shown in Table 2 for Mackey-found as well.
 The main advantages of the proposed approach are specified.
First, the new NFS X  X RIMA hybrid model has been proposed to time series forecasting, with which more accurate forecasting can be made. Second, the novel PSO X  X LSE hybrid learning method has been presented to make accurate prediction possible. The para-meters of NSF-ARIMA are divided into two subsets, the If-part subset and the Then-part subset. With this division strategy, the
PSO X  X LSE method has evolved the proposed NFS X  X RIMA with excellent performance. Third, the prediction performances by the proposed approach are great, as shown in the three examples. In
Tables 2 and 5 , the performances by the proposed approach are generally much better than the compared approaches.
 Acknowledgments This research work is supported by the National Science
Council, Taiwan (R.O.C.), under the Grant contract no. NSC98-2221-E-008-086. The authors thank the anonymous reviewers for their constructive comments.
 References
