 We describe the architecture of an automatic domain-specific Web portal construction system. The syst em has three major components: i) a focused crawler that collects the domain-specific pages on the Web, ii) an information extraction engine that extracts useful fields from these Web pages, and iii) a query engine that allows both typical keyword based queries on the pages and advanced queries on the extracted data fields. We present a prototype system that works for the course homepages domain on the Web. A user study with the prototype system shows that our approach produces high quality results and achieves better precision figures than the typical keyword based search. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process, query formulation Experimentation, Design. Focused Crawling, Information Extraction, Querying. With the very fast growth of WWW, the quest for locating the most relevant answers to users X  information needs becomes more challenging. In addition to gene ral purpose Web directories and search engines, several domain specific Web portals or search engines also exist, which essentially aim to cover a specific domain/topic (e.g., education) , product/material (e.g., product search for shopping), geographic region (e.g., transportation, hotels etc. at a particular country [2]) or media/file type (e.g., mp3 files or personal homepages [9]). Such specialized search tools may be constructed manually (by also benefiting from possible assist ance of the domain experts) or automatically. Some examples of the automatic approaches simply rely on intelligent combination and ranking of results obtained from traditional search tools (just like meta search engines), whereas some others fi rst attempt to gather the domain specific portion of the Web using focused crawling techniques information extraction, integration, etc.) on this collection (e.g., [7]). In [10], focused crawling is used for obtaining high quality pages on a mental health topic (depression) and constructing a Web portal. In [7], a prototype sy stem is constructed that achieves focused crawling and multilingual information extraction on the laptop and job offers domains. In this paper, we describe the architecture of a domain-specific Web portal construction system. This system will enlarge and refresh its collection by periodic focused crawls, and allow simple keyword based search on the raw Web pages as well as the advanced queries on the structured data obtained by using information extraction techniques. As a prototype, we present Cour se Homepage Finder which is a Web portal for course homepages . To our knowledge, there is no automatically built portal that aims to maintain and query course homepages for various depart ments/majors, although several manually maintained pointers to course related resources may exist. academicians, instructors and teaching assistants who want to survey similar course contents while preparing the course materials as well as the students who want to reach additional material for studying. We conduct a user study with the prototype system and show that the advanced queries supported by our approach produce higher quality results than the typical keyword based queries. The overall architecture of the system is depicted in Figure 1. In what follows, we briefly describe each stage of the proposed system. Focused Crawling. In this study, we first implemented the baseline focused crawler described in [3]. In particular, the crawler involves a document classi fier which is trained with example Web pages for each topic in the system. The target topic is given by the user as one or more of these training topics. During the crawling, the crawler maintains a priority queue of URLs to be visited. A URL score is computed according to the classifier score indicating the relevancy of the target topic to the page from which the URL is extracted. In addition to the above approach, two recent methods that exploit link context information are also explored [6], as well as a third one that we propose. In the first appr oach, so called text-window, only a number of words around each hyperlink are used for determining the priority of that link. The s econd one, tag-tree heuristic, uses the words that are in the DOM (Document Object Model) tree different technique, so-called pa ge segmentation method, which fragments a Web page according to the use of HTML tags. More specifically, for each tag, we define rules to decide whenever it pair including more than, say 50 words, is a group. The list of some rules to determine the page segmen ts in a Web page is given in Table 1. The segments are obtained in a recursive manner starting from the inner-most segment. Our intuition is that segmentation of a page would yield a set of coarse structural groups that are probably classifier adequate evidence to decide a segment X  X  class, in contrast to the fine grain approaches, which provide only some words around each link. Figure 2 shows an example faculty web page that is segmented by our approach. Each page segment is shown by rectangular blocks. Table 1. Use of HTML tags fo r page segmentation method. Explanation &amp; usage Semantic group 
Unordered list: &lt;UL&gt; &lt;LI&gt;* &lt;/UL&gt;. 
Ordered list: &lt;OL&gt; &lt;LI&gt;* &lt;/OL&gt;. 
Paragraph: &lt;P&gt;&lt;/P&gt; 
Table: &lt;TABLE&gt;&lt;TBODY&gt; (&lt;TR&gt;&lt;TD&gt;&lt;/TD&gt; &lt;/TR&gt;)* &lt;/TBODY&gt;&lt;/TABLE&gt; &lt;FRAME&gt; Information Extraction. In our IE engine, we employ two different approaches and merge their results. The SRV (Sequence Rules with Validation) algorithm [4] based on a top-down machine learning technique, and Sa nner X  X  IE software [8] based on the Hidden Markov Model are adapted to our framework. SRV is a machine learning algorithm that learns first order logic extraction pattern rules from training examples. Training set consists of annotated HTML page s. Each learned extraction rule consists of predicates. Rule learning phase starts with an empty rule and it greedily adds predicates to the rule which achieves the most information gain by covering as many positive examples as possible and at the same time as few negative examples as possible. Each learned rule cove rs different positive examples in the training set. Sanner X  X  IE software is based on the algorithm given in [5]. The software learns three different HMMs using the same training data. Each of these HMMs has diffe rent state transition structures. These HMMs are combined by a weighted sum of their probability values. Searching and Querying. The system provides two basic methods to retrieve relevant information to a user query.  X  Inverted index based keyword searching: An inverted index  X  SQL-like advanced querying: In this case, the users can Dataset. For the prototype system, we only concentrate on the Computer Science (CS) course homepages and use a collection of 25,614 pages obtained from 4 Tu rkish university Web sites (namely Bilkent, METU, Bogazici and Koc). For this dataset, freely available crawlers are seeded with the university homepages and the crawls are restricted to .edu domain. Note that, we prefer to evaluate crawler strategies by using offline datasets, to avoid fluctuations of the online crawling with several parallel threads. Crawling Stage. For the classifier component of the focused crawler, we use an SVM package, namely libSVM (http://www.csie.ntu.edu.tw/~cjlin/libsv m/), as discussed in [6]. The classifier is trained with the WebKB dataset (www.cs.cmu.edu/afs/cs.cmu.e du/project/theo-20/www/data/). This dataset has been manually constructed from the CS departments of various univers ities and includes seven classes (topics), namely, course, student, staff, project, faculty, department and other, which constitu te to 8,282 pages in total. As mentioned before,  X  X ourse X  topic is set to be the target topic. In Figure 3, we evaluate several crawlers on the pages collected from Bilkent University Web site. In particular, we compare the performance of breadth-first crawler (BFS), and four different focused crawler (FC) strategi es, which assign URL scores according to either the full page content, or the context defined by the text-window, tag-tree or pa ge segments. For the context-sensitive approaches, the final scor e of a URL is computed as a linear combination of the full page score and context score as computed by the corresponding strategy. Following the practice in [6], we use 25% of the full page score and 75% of the context score as the final score of a URL. The plot shows the harvest rate, i.e., the ratio of the number of relevant pages to the number of crawled pages at each point. Note that, as all strategies will eventually converge to the same harvest rate for the entire dataset, the performance at the beginning of the crawl (shown with dots) would be more informativ e for comparison purposes. The results reveal that, BFS, the ba seline strategy, is inferior to all focused approaches. Furthermore, the findings observed in earlier works are confirmed in that combining context information usually improves focused crawler performance and especially the text-window strategy may improve the harvest rate [6]. Finally, the page segmentation based appro ach proposed in this paper is also shown to be a worthwhile st rategy as it is better than or comparable to the text window approach. During the focused crawling stage, the pages with the classifier score 1.0 are decided to be the course homepages and stored to be fed to the IE component. We also manually determined CS related course pages and used them to evaluate the precision and recall for the data passed to the IE e ngine. For our dataset, 1084 pages are assigned the score 1.0 by the classifier, which constitutes the 4.2% of the entire collection. Ou r findings show that, for this dataset, the average precision and recall are 0.37 and 0.54, respectively. Information Extraction Stage. The IE-engine is trained with a subset of WebKB dataset (i ncluding 900 annotated course pages) to extract the following fields: course id X  X , course names, semester, instructors X  names and emails. The extracted data is stored in a relational DBMS, as shown in Figure 1. We manually evaluated the success of the extraction process for CS related course pages. Since user study experiments are performed by searching for c ourse homepages using course name field, the performance of IE-engine is evaluated only for course name extraction. Wh ile SRV system achieves 0.28 precision and 0.43 recall on course name field, Sanner X  X  IE software has 0.48 precision and 0.71 recall figures for the same field. Querying stage. The prototype system (available at http://139.179.21.106/~ismaila/sen iorTR/index.php) supports both keyword-based searches and SQL-like advanced queries on the extracted fields. In particular, the keyword-based search uses an inverted index over a ll crawled dataset including 25K pages, whereas the SQL-like system uses the fields extracted from the 1084 pages that are decide d to be course pages, as described above. The user in terface for Course Homepage Finder is shown in Figure 4. User study. A user study including a group of senior and graduate CS students is c onducted to measure the user satisfaction for the advanced and keyword-based querying approaches. In particular, a random set of 60 course names are determined from a disjoint dataset and each user is assigned 5 of them. Each user is required to enter the given query and evaluate the resulting records as relevant or irrelevant. Figure 5 shows the user interface that is used for the relevance judgment part of the user study. The results from the keyword-based and advanced querying approaches are shuffled, to prevent any bias. Next, for each query, we compute the precision score for the number of documents that are returned by the advanced approach. This is necessary, since the SQL-like advanced approach may return a restrict ed number of results per query whereas keyword-based query ma y return much more pages (due to keyword similarity based ranking). For this case, the precision of the advanced system is 66%, whereas the keyword-based system achieves 57%. Note that, comparing recall is not possible since it is impossible to know the all relevant pages in the entire dataset. (The Web site for the user study is available at http://139.179.21.106/~ismaila /SENIOR-1-TR/login.html) In this study, we present an automatic approach to create domain-specific Web-portals and show its usability by conducting a user study on the prototype system c onstructed for course homepages. Future work involves extensive e xperimentation with the proposed focused crawling methods, as well as adapting some other Web page segmentation methods in the literature (e.g., the VIPS algorithm, which segments Web page s based on the visual clues [1], or the method proposed in [11]). The effectiveness of IE component will also be further investigated, giving special emphasis on adaptive methods where only the most similar instances to a test instance in the train set are used for IE purposes. Finally, we plan to combine results from keyword-based and advanced-querying components, to further in crease user satisfaction. This work is supported by The Sc ientific and Technical Research Council of Turkey (T X B  X  TAK) under the grant no 105E024. We also would like to thank P. A ngin, L. Ak, B. Atikoglu, A. Boynuegri,  X . R. Atay, O.  X . Dolu,  X  . Durmaz, E. Karaca, T. Y  X  ld  X  z, E. Kucukoguz, A. T X rk and E. Karaca for their help during the implementation. [1] Cai, D., Yu, S., Wen, J.-R., and Ma, W.-Y. Extracting [2] Cambazoglu, B. B., Karaca, E., Kucukyilmaz, T., Turk, A. 
Figure 5. Relevance judgment interface for the user study. [3] Chakrabarti, S. Mining the Web Discovering Knowledge [4] Freitag, D. Machine Learning for Information Extraction in [5] Freitag, D. and McCallum, A. Information Extraction with [6] Pant, G. and Srinivasan, P. Link Contexts in Classifier-[7] Pazienza, M. T., Stellato A., and Vindigni, M. Purchasing [8] Sanner X  X  HMM-based Text Mi ning and Extraction Tool. [9] Shakes, J., Langheinrich, M. and Etzioni, O. Dynamic [10] Tang, T., Hawking, D., Craswell, N., Griffiths, K. Focused [11] Vadrevu, S., Gelgi F., Davulcu, H. Semantic Partitioning [12] Witten, I. H., Moffat, A. and Bell, T. C. Managing Gigabytes 
