 There has been an explosion in the amount of digital text information available in recent years, leading to challenges of scale for traditional inference algorithms for topic mod-els. Recent advances in stochastic variational inference al-gorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on very large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian in-ference for LDA, which is simpler and more efficient than the state of the art method. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than previous methods. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitat-ing the use of topic models in interactive document analysis software.
 I.5.1 [ Models ]: Statistical; I.2.7 [ Natural Language Pro-cessing ]: Text analysis Algorithms, Experimentation, Performance Topic models, variational inference, stochastic learning
Topic models such as latent Dirichlet allocation (LDA) [7] are now widely used in modern machine learning. Infer-ence algorithms for topic models provide a low-dimensional representation of text corpora that is typically semantically meaningful, despite being completely unsupervised. Their use has spread beyond machine learning to become a stan-dard analysis tool for researchers in many fields [15, 4, 8]. In the internet era, there is a need for tools to learn topic mod-els at the  X  X eb scale X , especially in an industrial setting. For example, news aggregators such as Yahoo! News publish a continually updated stream of online articles. These applica-tions need to analyze candidate articles for topical diversity and relevance to current trends, which can be facilitated by topic models [1].

In this context it would be useful to have the tools to build topic models that scale to such large corpora, taking advantage of the large amounts of available data to create models that are both more complex (e.g. have more topics) and more accurate. Traditional inference techniques such as Gibbs sampling and variational inference do not readily scale to corpora containing millions of documents or more. In such cases it is very time-consuming to run even a single iteration of the standard collapsed Gibbs sampling [12] or variational Bayesian inference algorithms [7], let alone run them until convergence. For these algorithms, the first few passes through the data are inhibited by randomly initialized values of the parameters and latent variables which misin-form the updates, so multiple such expensive iterations are required to learn the topics.

A significant recent advance was made by Hoffman et al. [13], who proposed a stochastic variational inference algo-rithm for LDA topic models. Because the algorithm does not need to see all of the documents before updating the topics, this method can often learn good topics before a sin-gle iteration of the traditional batch inference algorithms would be completed. The algorithm processes documents in an online fashion, so it can be applied to corpora of any size, or even to never-ending streams of documents. A more scal-able variant of this algorithm was proposed by Mimno et al. [16], which approximates the gradient updates in a sparse way in order to improve performance for larger vocabularies and greater numbers of topics. A complementary direction that has been useful for im-proving inference in LDA is to take advantage of its  X  X ol-lapsed X  representation, where parameters are marginalized out, leaving only latent variables. It is possible to perform inference in the collapsed space and recover estimates of the parameters afterwards. For inference techniques that oper-ate in a batch setting, the algorithms that operate in the collapsed space are more efficient at improving held-out log probability than their uncollapsed counterparts, both per iteration and in wall-clock time per iteration [12, 24, 3]. Reasons for this advantage include the per-token updates which propagate updated information sooner, simpler up-date equations, fewer parameters to update, no expensive calls to the digamma function, and the avoidance of tightly coupled pairs of parameters which inhibit mixing for Gibbs sampling [10, 3, 24]. For variational inference, perhaps the most important advantage of the collapsed representation is that the variational bound is strictly better than that for the uncollapsed representation, leading to the potential for collapsed variational algorithms to learn more accurate topic models than uncollapsed variational algorithms [24]. Exist-ing online inference algorithms for LDA do not fully take advantage of the collapsed representation. An exception is the sparse online LDA algorithm of Mimno et al. [16] which collapses out per-document parameters  X  , however the topics themselves are not collapsed out.

In this work, we develop a stochastic algorithm for LDA that operates fully in the collapsed space, thus transferring the aforementioned advantages of collapsed inference to the online setting. This facilitates learning topic models both more accurately and more quickly on large datasets. The proposed algorithm is also very simple to implement, re-quiring only basic arithmetic operations. In addition to ex-periments on large datasets, we also explore the benefit of our method on small problems, showing that it is feasible to learn human-interpretable topics in seconds.
Probabilistic topic models such as LDA [7] use latent vari-ables to encode co-occurrence patterns between words in text corpora and other bag-of-words data. In the LDA model, there are K topics  X  k ,k  X  X  1 ,...,K } ,eachofwhich are discrete distributions over words (see Table 1 for relevant notation). For example, a topic on baseball might give high probabilities to words such as  X  X itcher, X   X  X at X  and  X  X ase X . The assumed generative process for the LDA model is Generate each topic  X  k  X  Dirichlet(  X  ) ,k  X  X  1 ,...,K } For each document j
To scale LDA inference to very large datasets, a stochastic variational inference algorithm was proposed by Hoffman et al. [13]. We will discuss its more general form [14], which ap-plies to graphical models whose parameters can be split into  X  X lobal X  parameters G and  X  X ocal X  parameters L j pertain-ingtoeachdatapoint x j , and whose complete conditional distributions for each variable are exponential family distri-butions. The algorithm examines one data point at a time to learn that data point X  X  local variational parameters, such as  X  j in LDA. It then updates global variational parameters, K Number of topics D Number of documents C Number of words in corpus
C j Number of words in document j z ij Topic for ( i, j ), the i th word of the j th document w ij Dictionary index for word ( i, j )  X  j Distribution over topics for document j , K  X  1  X  k Distribution over words for topic k , W  X  1  X  Dirichlet prior parameters for  X  , K  X  1  X  Dirichlet prior parameters for  X , W  X  1  X  ij Variational distribution for word ( i, j ), 1  X  K N  X  Expected topic counts per document, D  X  K N  X  Expected topic counts per word, W  X  K N Z Expected topic counts overall, 1  X  K Y ( ij ) Estimate of N  X  based only on word ( i, j ), W  X  K
M Minibatch, a set of documents  X 
N  X  Estimate of N  X  from current minibatch, W  X  K  X 
N Z Estimate of N Z from current minibatch, 1  X  K t Step size for N  X  at timestep t t Step size for N  X  and N Z at timestep t w aj Dictionary index for a th distinct word of j m aj Count of a th distinct word of j  X  aj Variational dist. for a th distinct word of j ,1  X  K Algorithm 1 Stochastic Variational Inference (Hoffman et al.) such as topics  X  k , via a stochastic natural gradient update. Their general scheme is given in Algorithm 1.

For an appropriate local update and sequence of step sizes  X  , this algorithm is guaranteed to converge to the optimal variational solution [14]. In the case of LDA, let  X  k be the parameter vector for a variational Dirichlet distribution on topic  X  k .Foreachdocument j , the method computes vari-ational distributions for both the topic assignments and the document X  X  distribution over topics using regular VB up-dates. These values are then used to update the topics. Specifically, for each topic k the algorithm computes  X   X  estimate of what  X  k would be if all D documents were iden-tical to document j . The algorithm then updates the  X  k  X  X  via a natural gradient update, which takes the form
In a somewhat broader context, the online EM algorithm of Cappe and Moulines [9] is another general-purpose method for learning latent variable models in an online setting. This EM algorithm alternates between a standard M-step which maximizes the EM lower bound with respect to parameters  X  , and a stochastic expectation step, which updates expo-nential family sufficient statistics s with an online average with Y n +1 beinganewdatapoint,  X  being the current pa-rameters, and  X  s ( Y n +1 ;  X  ) being an estimate of the sufficient statistics based on these values.

In this article, we show how to perform stochastic varia-tional inference in the collapsed representation of LDA, us-ing an algorithm inspired by both the online algorithms of Hoffman et al. and Cappe and Moulines. This new algo-rithm takes advantage of a fast collapsed inference method called X  X VB0 X  X 3] to further improve the efficiency of stochas-tic LDA inference.
In the collapsed representation of LDA, we marginalize out topics  X  and distributions over topics  X  ,andperform inference only on the topic assignments Z . The collapsed variational Bayesian inference (CVB) approach of Teh et al. [24] maintains variational discrete distributions  X  ij over the K topic assignment probabilities for each word i in each document j . The coordinate ascent updates to optimize the evidence lower bound with respect to  X  are intractable. Nonetheless, Teh et al. showed that an algorithm using ap-proximate updates works well in practice, outperforming the classical VB algorithm in terms of prediction performance. Asuncion et al. [3] later showed that a simpler version of this method called CVB0, based on additional approximations, is much faster while still maintaining the accuracy of CVB. The CVB0 algorithm iter atively updates each  X  ij via for each topic k ,with w ij corresponding to the word index for the j th document X  X  i th word, and where a :  X  b denotes that a is assigned to be proportional to b .The N Z , N  X  N
 X  variables, henceforth referred to as the CVB0 statistics, are variational expected counts corresponding to their in-dices, and the  X  ij superscript indicates the exclusion of the current value of  X  ij . Specifically, N Z is the vector of ex-pected number of words assigned to each topic, N  X  j is the equivalent vector for document j only, and each entry w, k of matrix N  X  is the expected number of times word w is assigned to topic k across the corpus, N
Note that N  X  j +  X  is an unnormalized variational estimate of the posterior mean of document j  X  X  distribution over top-ics  X  j ,andcolumn k of N  X  +  X  is an unnormalized variational estimate of the posterior mean of topic  X  k .

CVB0 is currently the fastest known technique for LDA inference for single-core batch inference in terms of conver-gence rate [3]. It is also as simple to implement as collapsed Gibbs sampling, and has a very similar update procedure except that the update is deterministic. Sato and Naka-gawa [22] showed that the terms in the CVB0 update can be understood as optimizing the  X  -divergence, with different values of  X  for each term. The  X  -divergence is a generaliza-tion of the KL-divergence that variational Bayes minimizes, and optimizing it is known as power expectation propagation [17]. A disadvantage of CVB0 is that the memory require-ments are large as it needs to store a variational distribution  X  for every token in the corpus (although this can be im-proved slightly by  X  X lumping X  every occurrence of a specific word in each document together and storing a single  X  for them).
Given the discussion above, a desirable stochastic algo-rithm would be one that exploits both (a) the efficiency and simplicity of CVB0, and (b) the improved variational bound of the collapsed representation. Such an algorithm should not need to maintain the  X  variables, thus circumventing the memory requirements of CVB0. It should also be able to provide an estimate for the topics when only a subset of the data have been visited. Recall that the CVB0 statistics N Z , N  X  and N  X  are all that are needed to both perform a CVB0 update and to recover estimates of the topics. Given this, we wish to estimate the CVB0 statistics based only on the subset of tokens we have observed.

Suppose we have seen a token w ij , and its associated  X  ij The information this gives us about the statistics depends on how the token was drawn. If the token was drawn uniformly at random from all of the tokens in the corpus, the expected value of N Z with respect to the sampling distribution is C X  ij ,where C is the number of words in the corpus. For the same sampling procedure, the expectation of the word-topic expected counts matrix N  X  is C Y ( ij ) ,where Y ( ij ) W  X  K matrix with the w ij th row being  X  ij andwithzeros in the other entries. Now if the token was drawn uniformly from the tokens in document j ,theexpectedvalueof N  X  j is C  X  ij ,where C j is the length of document j . 1
Since we may not maintain the  X   X  X , we cannot perform these sampling procedures directly. However, with a current guess at the CVB0 statistics we can update a token X  X  varia-tional distribution, and observe its new value. We can then use this  X  ij to improve our estimate of the CVB0 statistics. This suggests an iterative procedure, alternating between a  X  X aximization X  step, approximately optimizing the evidence lower bound with respect to a particular  X  ij via CVB0, and an  X  X xpectation X  step, where we update the expected count statistics to take into account the new  X  ij . As the algorithm continues, the  X  ij  X  X  we observe will change, so we cannot simply average them. Instead, we can follow Cappe and Moulines [9] and perform an online average of these statis-tics via Equation 2.

In the proposed algorithm, we process the corpus one to-ken at a time, examining the tokens from each document in turn. For each token, we first compute a new  X  ij .Wedo not store the  X   X  X , but compute (updated versions of) them as needed via CVB0. This means we must make a small ad-ditional approximation in that we cannot subtract current values of  X  ij in Equation 3. With large corpora and large documents this difference is negligible. The update becomes
Other sampling schemes are possible, which would lead to different algorithms. For example, one could sample from the set of tokens with word index w to estimate N  X  w .Our choice leads to an algorithm that is practical in the online setting.
We then use this to re-estimate our CVB0 statistics. We use one sequence of step-sizes  X   X  for N  X  and N Z ,andan-other sequence  X   X  for N  X  . While we are processing ran-domly ordered tokens i of document j , we are effectively drawing random tokens from it, so the expectation of N  X  j C  X  ij .Weupdate N  X  j with an online average of the current value and its expected value,
Although we process one document at a time, we even-tually process all of the words in the corpus. So for the purposes of updating N  X  and N Z , in the long-run the algo-rithm is effectively drawing tokens from the entire corpus. The expected N  X  after observing one  X  ij is C Y ( ij ) ,andthe expected N Z is C X  ij . In practice, it is too expensive to update the entire N  X  after every token, suggesting the use of minibatch updates. The expected N  X  after observing a minibatch M is the average of the per-token estimates, and similarly for N Z , leading to the updates: Depending on the lengths of the documents and the num-ber of topics, it may also be beneficial to perform a small number of extra passes to learn the document statistics be-fore updating the topic statistics. We found empirically that one such burn-in pass was sufficient in all of the datasets we tried in our experiments. Pseudo-code for the algorithm, which we refer to as  X  X tochastic CVB0 X  (SCVB0) is given in Algorithm 2.
 Algorithm 2 Stochastic CVB0
An optional additional optimization of the above algo-rithm is to only perform one update for each distinct token in each document, and scale the update by the number of copies in the document. This process, often called  X  X lump-ing, X  X s standard practice for fast implementations of all LDA inference algorithms (e.g. see Teh et al. [24] and Jonathan Chang X  X  R package for LDA 2 ), though it is only exact for uncollapsed algorithms, where the z ij  X  X  are D-separated by  X  . Suppose we have observed w aj ,whichoccurs m aj times in document j . Plugging Equation 6 into itself m aj times and noticing that all but one of the resulting terms form a geometric series, we can see that performing m aj updates for N  X  j while holding  X  aj fixed is equivalent to
This section describes an experimental analysis of the proposed SCVB0 algorithm, with direct comparison to the stochastic variational Bayes algorithm of Hoffman et al., hereafter referred to as SVB. As well as performing an anal-ysis on several large-scale problems, we also investigate the effectiveness of the stochastic LDA inference algorithms at learning topics in near real-time on small corpora.
We studied the performance of the algorithms on three large corpora. The corpora are:
We explored predictive performance versus wall-clock time for both SCVB0 and SVB. To compare the algorithms fairly, we implemented both of them in the fast high-level language Julia [6]. Our implementation of SVB closely follows the python implementation provided by Hoffman, and has sev-eral optimizations not mentioned in the original paper in-cluding handling the latent topic assignments z implicitly,  X  X lumping X  of like tokens, and sparse updates of the topic matrix. The SCVB0 algorithm was implemented as it is written in Algorithm 2, using the clumping optimization but with no additional algorithmic optimizations. Specifically, neither implementation used the complicated optimizations taking advantage of sparsity that are exploited by the Vow-pal Wabbit implementation of SVB 4 andinthevariantof http://cran.r-project.org/web/packages/lda/ http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/ https://github.com/JohnLangford/vowpal_wabbit/wiki SVB proposed by Mimno [16]. Instead, our implementa-tions represent a  X  X est-effort X  attempt to implement each algorithm efficiently yet following the spirit of the original pseudo-code.

In all experiments, each algorithm was trained using mini-batches of size 100. We used a step-size schedule of s (  X  + t )  X  for document iteration t ,with s = 10,  X  = 1000 and  X  =0 . For SCVB0, the document parameters were updated using thesameschedulewith s =1,  X  =10and  X  =0 . 9, with t referring to the word iteration of the current document. We used LDA hyper-parameters  X  =0 . 1and  X  =0 . 01 for SCVB0. ForSVB,wetriedboththesesamehyperparame-tervaluesaswellasshiftingby0 . 5 as recommended by [3] to compensate for the implicit bias in how uncollapsed VB treats hyper-parameters. We used a single pass to learn doc-ument parameters for SCVB0, and tried both a single pass and five passes for SVB.

For each experiment we held out 10,000 documents and trained on the remaining documents. We split each test document in half, estimated document parameters on one half and computed the log-probability of the remaining half of the document. Figures 1(a) through 1(c) show held-out log-likelihood versus wall-clock time for each algorithm. In the figures, SVB-B x -O y corresponds to running SVB with  X  X urn-in X  passes per document and with hyper-parameters offset from  X  =0 . 1and  X  =0 . 01 by y .

For the PubMed Central data, we found that all algo-rithms perform similarly after about an hour, but prior to that SCVB0 is better, indicating that SCVB0 makes bet-ter use of its time. All algorithms perform similarly per-iteration (see Figure 2), but SCVB0 is able to benefit by processing more documents in the same amount of time. The per-iteration plots for the other datasets were similar.
Our experiments show that SCVB0 shows a more sub-stantial benefit when employed on larger datasets. For both the New York Times and Wikipedia datasets (which are each significantly larger than the PubMed Central dataset in terms of the number of documents), SCVB0 converged to a better solution than SVB for any of its parameter set-tings. Furthermore, SCVB0 outperforms SVB throughout the run. The superior performance of SCVB0 over the un-collapsed SVB method is consistent with the fact that the variational bound for the collapsed representation is strictly better than the bound for the uncollapsed representation of LDA [24].
 For completeness, we also compared SCVB0 to the batch VB algorithm on the Wikipedia dataset (Figure 3); other standard batch algorithms such as Gibbs sampling tend to perform similarly to VB at convergence, particularly if the hyper-parameters are learned for each algorithm [3]. Note that it was not possible to perform even a single iteration of batch VB on the full dataset in the allotted time of twelve hours. Following Hoffman et al., we show instead the per-formance of the algorithms on subsets of the data. This facilitates faster convergence, but reduces the quality of the final solution as the algorithms are consequently unable to exploit all of the data. In contrast, the stochastic algorithms are able to make use of large datasets while still converging quickly. Avg. Log Likelihood Avg. Log Likelihood Avg. Log Likelihood Avg. Log Likelihood Figure 2: Log-likelihood vs Iteration for the PubMed Central experiments. Avg. Log Likelihood Figure 3: Log-likelihood vs Iteration compared to batch VB for the Wikipedia experiments, where N is the number of documents used for training.
Stochastic algorithms for LDA have previously only been used on large corpora, however they have the potential to be useful for finding topics very quickly on small corpora as well. The ability to learn interpretable topics in a mat-ter of seconds is very beneficial for exploratory data anal-ysis (EDA) applications, with a human in the loop. Near real-time topic modeling opens the way for the use of topic models in interactive software tools for document analysis.
We investigated the performance of the stochastic algo-rithms in this small-scale scenario using a corpus of 1740 scientific articles from years 1987  X  1999 of the machine learning conference NIPS. We ran the two stochastic infer-ence algorithms for five seconds each, using the parameter settings from the previous experiments but with 20 topics. Each algorithm was run ten times. In the five seconds of training, SCVB0 was typically able to examine 3300 docu-ments, while SVB was typically able to examine around 600 documents.

With the EDA application in mind, we performed a human-subject experiment in the vein of the experiments proposed by Chang and Blei [11]. The sets of topics returned by each run were randomly assigned across seven human subjects. The participants were all machine learning researchers with technical expertise in the subjects of interest to the NIPS community. The subjects did not know which algorithms generated which runs. The top ten words of the topics in each run were shown to the subjects, who were given the following instructions:
The results provide an estimate of the number of  X  X rrors X  that a topic model inference algorithm makes, relative to human judgement. It was found that the SCVB0 algorithm had 0.76 errors per topic on average, with a standard de-viation of 1.1, while SVB had 1.6 errors per topic on aver-age, with standard deviation 1.2. A one-sided two sample t-test rejected the hypothesis that the means of the errors per topic were equal, with significance level  X  =0 . 05. Ran-domly selected example topics are shown in Table 2. As can be seen from the table, both algorithms successfully learned coherent topics in this relatively short time frame.
We also performed a similar experiment on Amazon Turk using the New York Times corpus. We ran the two stochas-tic inference algorithms for 60 seconds each using the same parameter settings as above but with 50 topics. Each user was presented with 20 random topics from each algorithm. Again, the subjects did not know which algorithms gener-ated each set of topics. We included two easy questions with obvious answers and removed results from users who did not answer them correctly. This step eliminated 4 users, and the analysis was performed with the data from the remaining 52 participants. Comparing the number of  X  X rrors X  for SCVB0 to SVB for each user, we find that SCVB0 had 2.1 errors per topic on average, with standard deviation 1.0, and SVB had 4.4 errors on average with standard deviation 2.4. A paired t-test finds these differences significant for the sam-pled population at the  X  = . 05 level, with p-value &lt;. Example topics selected uniformly at random from a ran-domly selected run of each algorithm are shown in Table 3, illustrating the relative difference in the coherence of the topics recovered by the two methods in this time period.
In the SCVB0 algorithm, because the  X   X  X  are not main-tained we must approximate Equation 3 with Equation 5, neglecting the subtraction of the previous value of  X  ij from the CVB0 statistics when updating  X  ij . Inanextendedver-sion of this paper, available on the arXiv, 5 , we show that this approximation results in an algorithm which is equivalent to an EM algorithm for MAP estimation, due to Asuncion et http://arxiv.org/abs/1305.2452 . al. [3], which operates on an unnormalized parameteriza-tion of LDA. Using this interpretation of the algorithm, we can alternatively derive SCVB0 as an adapted version of Cappe and Moulines X  online EM algorithm [9], where the algorithm is extended to perform MAP estimation and to handle document-specific parameters. Therefore, the ap-proximate collapsed variational updates of SCVB0 can also be understood as MAP estimation updates. The MAP in-terpretation of the algorithm implicitly uses adjusted values of the hyperparameters, so this does not contradict the orig-inal CVB interpretation, but suggests that there is a close relationship between the optimal solutions of the CVB and MAP estimation problems.

It is difficult to establish the convergence properties of the original CVB0 algorithm, as its updates are approxi-mate. However, the alternative view of the algorithm is more amenable to convergence analysis as the MAP updates are exact. Under the MAP estimation interpretation of SCVB0, it can be shown that the algorithm converges to a station-ary point of the MAP objective function, computed as if the prior were modified by increasing the hyper-parameters by one. The proof strategy broadly follows that of Cappe and Moulines. First, the algorithm is written as a Robbins and Monro [20] stochastic approximation (SA) algorithm. Then, it is shown that there exists a Lyapunov function satisfying the conditions of Andreiu et al. [2], which are sufficient to establish convergence for an SA algorithm. In the context of an SA algorithm, a Lyapunov function can be understood as an  X  X bjective function X  which, in the absence of stochastic noise, the SA would improve monotonically if small enough steps were taken in the direction of the updates. We refer the reader to the extended version of this paper for details.
The MAP estimation interpretation of SCVB0 may also help to explain the improvement in predictive performance relative to SVB. The MAP estimate approximates the pos-terior distribution by a delta function at its mode, while mean field variational Bayes approximates the posterior by a factorized distribution. As the amount of training data increases, the posterior distribution should become more peaked around the mode, i.e. more similar to the delta func-tion at the MAP. The factorized distribution of mean field, on the other hand, may not be able to accurately represent the posterior distribution in the large data regime. So we conjecture that in many cases, given enough data it may be preferable to perform MAP estimation instead of variational inference. This observation seems particularly relevant in the case where stochastic algorithms are necessary due to the large amount of data available.
Connections can be drawn between SCVB0 and other methods in the literature. The SCVB0 scheme is reminis-cent of the online EM algorithm of Cappe and Moulines [9], which also alternates between per data-point parameter up-dates and online estimates of the expected values of sufficient statistics. Online EM optimizes the EM lower bound on the log-likelihood in the M-step and computes online averages of exponential family sufficient statistics, while SCVB0 (ap-proximately) updates the mean-field evidence lower bound in the M-step and computes online averages of sufficient statistics required for a CVB0 update in the E-step. As discussed in the previous section, when viewed as a MAP estimation algorithm SCVB0 can also be derived as an ex-tension of online EM, applied to LDA.
The SCVB0 algorithm also has a very similar structure to SVB, alternating between passes through a document (the optional  X  X urn-in X  passes) to learn document param-eters, and updating variables associated with topics. How-ever, SCVB0 is stochastic at the word-level while SVB is stochastic at the document level. In the general framework of Hoffman et al., inference is performed on  X  X ocal X  param-eters specific to a data point, which are used to perform a stochastic update on the  X  X lobal X  parameters. For SVB, the document parameters  X  j are local parameters for document j , and topics are global parameters. For SCVB0, the  X  ij  X  X  are local parameters for a word, and both document param-eters N  X  and topic parameters N  X  are global parameters. This means that updates to document parameters can be made before processing all of the words in the document. The incremental algorithm of Banerjee and Basu [5], for MAP inference in LDA, is also closely related to the pro-posed algorithm. They estimate topic probabilities for each word sequentially, and update MAP estimates of  X  and  X  incrementally, using the expected assignments of words to topics in the current document. SCVB0 can be understood as the collapsed, stochastic variational version of Banerjee and Basu X  X  incremental uncollapsed MAP estimation algo-rithm. Interpreting SCVB0 as a MAP estimation algorithm, SCVB0 is the online EM algorithm for MAP estimation op-erating on the unnormalized representation of LDA, while Banerjee and Basu X  X  algorithm is the incremental EM al-gorithm operating on the usual normalized representation of LDA. A related algorithm is the sequential Monte Carlo (SMC) approach used by Ahmed et al. [1], which sequen-tially Gibbs samples the topic assignments of each document for each of F importance-weighted particles. This method updates count statistics for each particle incrementally via sampling, while SCVB0 updates count statistics with online-averaged updates via optimization.

Another stochastic algorithm for LDA, due to Mimno et al. [16], operates in a partially collapsed space, placing it in-between SVB and SCVB0 in terms of representation. Their algorithm collapses out  X  but does not collapse out  X  .Esti-mates of online natural gradient update directions are com-puted by performing Gibbs sampling on the topic assign-ments of the words in each document, and averaging over the samples. The gradient estimate is non-zero only for word-topic pairs which occurred in the samples. When carefully implemented to take advantage of the sparsity, the updates scale sub-linearly in the number of topics, causing large im-provements in high-dimensional regimes. For SCVB0, the minibatch updates are sparse in the rows (words), so some performance enhancements along the lines of those used by Mimno et al. are likely to be possible.

There has been a substantial amount of other work on speeding up LDA inference in the literature. Porteous et al. [19] improved the efficiency of the sampling step for the col-lapsed Gibbs sampler, and Yao, Mimno and McCallum [26] explore a number of alternatives for improving the efficiency of LDA. The Vowpal Wabbit system for fast machine learn-ing, 4 due to John Langford and collaborators, has a version of SVB that has been engineered to be extremely efficient. Parallelization is another approach for improving the effi-ciency of topic models. Newman et al. [18] introduced an approximate parallel algorithm for LDA where data is dis-tributed across multiple machines, and an exact algorithm for an extension of LDA which takes into account the dis-tributed storage. Smola and Narayanamurthy developed an efficient architecture for parallel LDA inference [23], using a distributed (key, value) storage for synchronizing the state of the sampler between machines. All of these computational improvements are somewhat orthogonal to those proposed in this paper, and it is likely that some of these ideas could be adapted to apply to SCVB0 as well.
This paper introduces SCVB0, an algorithm for perform-ing fast stochastic collapsed variational inference in LDA, and shows that it outperforms stochastic VB on several large document corpora, converging faster and often to a better solution. The algorithm is relatively simple to implement, with intuitive update rules consisting only of basic arith-metic operations. We also found that the algorithm was ef-fective at learning good topics from small corpora in seconds, finding topics that were superior than those of stochastic VB according to human judgement.

There are many directions for future work. The method could potentially be adapted to Teh et al. [25] X  X  hierarchi-cal Dirichlet process version of LDA, leveraging the work of Sato et al. [21]. The speed of the method could likely be im-proved by exploiting sparsity, using techniques such as those employed by Mimno et al. [16]. Furthermore, the collapsed representation facilitates the use of the parallelization tech-niques explored by Newman et al. in [18]. Finally, SCVB0 could be incorporated into an interactive software tool for exploring the topics of document corpora in real-time. JF, CD, and PS were partially supported by the Office of Naval Research under MURI grant N 00014-08-1-1015 and by a Google Faculty Research Award. LB was supported by the National Science Fo undation under G rant No. 0914783, 0928427, 1018433, 1216045. The authors would also like to thank Arthur Asuncion for many helpful discussions. [1] A.Ahmed,Q.Ho,C.H.Teo,J.Eisenstein,A.Smola, [2] C. Andrieu,  X  E. Moulines, and P. Priouret. Stability of [3] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. [4] D. C. Atkins, T. N. Rubin, M. Steyvers, M. A. [5] A. Banerjee and S. Basu. Topic models over text [6] J. Bezanson, S. Karpinski, V. B. Shah, and [7] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [8] S. Block and D. Newman. What, where, when and [9] O. Capp  X  e and E. Moulines. On-line [10] B. Carpenter. Integrating out multinomial parameters [11] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and [12] T. Griffiths and M. Steyvers. Finding scientific topics. [13] M. Hoffman, D. Blei, and F. Bach. Online learning for [14] M. Hoffman, D. Blei, C. Wang, and J. Paisley. [15] D. Mimno. Computational historiography: Data [16] D. Mimno, M. Hoffman, and D. Blei. Sparse stochastic [17] T. Minka. Power EP. Technical Report [18] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [19] I. Porteous, D. Newman, A. Ihler, A. Asuncion, [20] H. Robbins and S. Monro. A stochastic approximation [21] I. Sato, K. Kurihara, and H. Nakagawa. Practical [22] I. Sato and H. Nakagawa. Rethinking collapsed [23] A. Smola and S. Narayanamurthy. An architecture for [24] Y. Teh, D. Newman, and M. Welling. A collapsed [25] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [26] L. Yao, D. Mimno, and A. McCallum. Efficient
