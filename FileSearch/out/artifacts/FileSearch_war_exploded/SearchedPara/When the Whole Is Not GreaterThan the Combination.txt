 University of Rome  X  X or Vergata X  University of Rome  X  X or Vergata X  University of Trento models and convolution kernels. 1. Introduction
Distributional semantics approximates word meanings with vectors tracking co-proach to phrases and sentences through vector composition (Clark 2015). Resulting compositional distributional semantic models (CDSMs) estimate degrees of seman-paraphrasing.

CDSMs affect similarity measurements invo lving the vectors they produce for phrases be decomposed into operations performed on the subparts of the input phrases,
This establishes a strong link between CD SMs and convolution kernels (Haussler Conjecture. X 
Convolution Conjecture holds. We will first illustrate the conjecture for linear methods, and then briefly consider two nonlinear approaches: the dual space model of Turney (2012), for which it does, and a representative of the recent strand of work on neural-network models of composition, for which it does not. 2. Mathematical Preliminaries matrices as capital letters in bold A and their elements are A fourth-order tensors as capital letters in the form A A A
The symbol represents the element-wise product and  X  is the tensor product. The dot product is a , b and the Frobenius product X  X hat is, the generalization of the dot product to matrices and high-order tensors X  X s represented as A , B
Frobenius product acts on vectors, matrices, and third-order tensors as follows:
Frobenius product between two general tensors is the following: where I is the identity matrix. The dot product of A x and B y can be rewritten as: 166 tensor product between two third-order tensors. In this particular tensor contraction, the elements C iknm of the resulting fourth-order tensor elements D iknm of the tensor D D D = x  X  y  X  a  X  c are D 3. Formalizing the Convolution Conjecture
Structured Objects. In line with Haussler (1999), a structured object x that can be decomposed into n subparts. We indicate with x
X is the set of the structured objects and T X  X  X is the set of the terminal objects. A structured object x can be anything according to the representational needs. Here, x is a representation of a text fragment, and so it can be a sequence of words, a sequence of words along with their part of speech, a tree structure, and so on. The set the set of decompositions of x relevant to define a specific CDSM. Note that a given decomposition of a structured object x does not need to contain all the subparts of the original object. For example, let us consider the phrase x
R the phrase: ( tall
Recursive formulation of CDSM. A CDSM can be viewed as a function f that acts recur-sively on a structured object x .If x is a non-terminal object operate differently on different kinds of structured objects, with tensor degree varying accordingly. The set R ( x ) and the functions f ,  X  ,and depend on the specific CDSM, and the same CDSM might be susceptible to alternative analyses satisfying the form in
Equation (5). As an example, under Additive, x is a sequence of words and f is where R (( w 1 , ... , w n )) = { ( w 1 ), ... ,( w n ) } summing and  X  is identity. For Multiplicative we have
With a single decomposition, the repeated o peration reduces to a single term; and here is the product (it will be clear subsequently, when we apply the Convolution Conjecture to these models, why we are assuming different decomposition sets for Additive and Multiplicative).
 Definition 1 (Convolution Conjecture)
For every CDSM f along with its R ( x ) set, there exist functions K , K such that:
K (Haussler 1999). K is usually the dot product, but this is not necessary: We will show that for the dual-space model of Turney (2012) K turns out to be the fourth root of the
Frobenius tensor. 4. Comparing Composed Phrases
CDSMs, exemplifying with adjective X  X oun and subject X  X erb X  X bject phrases. Without loss of generality we use tall boy and red cat for adjective X  X oun phrases and goats eat grass and cows drink water for subject X  X erb X  X bject phrases.

Equation (6). The structure of the input is a word sequence (i.e., x relevant decompositions consist of these single words, R ( x ) phrases of different length. 168
Multiplicative Model. K , g are dot products, K i the component-wise product, and f is as in Equation (7). The structure of the input is x single term): product is obtained in two steps: first separately operating on the adjectives and on the nouns; then taking the dot product of the resul ting vectors. The comparison operations are thus reflecting the input syntactic structure. The results can be easily extended to longer phrases and to phrases of different lengths.
 that is, R ( x ) = { ( L 1 w 1 ), ... ,( L n w n ) } .TheCDSM f is defined as
In the CC form, K is the dot product, g the Frobenius product, K using the property in Equation (3)): K ( f ((A tall) (N boy)), f ((A red) (N cat))) = A tall + = A tall , A red + A tall , N cat + N boy , A red + N boy , N cat = A T A , tall red T product of two matrices representing syntactic labels and the tensor product between two vectors representing the corresponding words. For subject X  X erb X  X bject phrases (( Sw 1 )( Vw 2 )( Ow 3 )) we have Again, we observe the factoring into products of syntactic and lexical representations. matrix pairs, it degenerates to Additive. Int erestingly, Full Additive can also approx-imate a semantic convolution kernel (Mehdad, Moschitti, and Zanzotto 2010), which obtain this approximation by choosing two nearly orthonormal matrices A and N such that AA T = NN T  X  I and AN T  X  0 and applying Equation (2): A tall N cat  X  tall , red + boy , cat .
 and O are such that XX T  X  I with X one of the three matrices and YX comparing two sentences by summing the dot products of the words in the same role, that is,
Lexical Function Model. We distinguish composition with one-vs. two argument pred-icates. We illustrate the first through adject ive X  X oun composition, where the adjective acts as the predicate, and the second with transitive verb constructions. Although we use the relevant syntactic labels, the formulas generalize to any construction with the same argument count. For adjective X  X oun phrases, the input is a sequence of (label, only the single trivial decomposition into all the subparts:
The method itself is recursively defined as 170
Here, K and g are, respectively, the dot and Frobenius product, K and K 2 ( f ( x ), f ( y )) = f ( x ) f ( y ) T . Using Equation (3), we have then (label, word) tuples, and the relevant decomposition set only includes the single trivial decomposition into all subparts. The CDSM f is defined as that is, the tensor contraction 2 along the second index of the tensor product between f ( x )and f ( y ) X  X nd K 2 ( f ( x ), f ( y )) = K 3 ( f ( x ), f ( y )) dot product of goats EAT EAT EAT grass and cows DRINK DRINK DRINK sors. The first combines the two third-order tensors of the verbs and the second combines the vectors repres enting the arguments of the verb, that is: goats  X  grass  X  cows  X  water . In this case as well we can separate the role of predicate and argument types in the comparison computation.
 by using the identity element for missing parts. As an example, we show here the comparison between tall boy and cat where the identity element is the identity matrix I : the effective Dual Space model of Turney (2012), which assumes that each word has two distributional representations, w d in  X  X omain X  space and w separate similarities between the first and secondwordsinbothspaces.Eventhough there is no explicit composition step, it is still possible to put the model in CC form. resentations w d and w f : f ( w ) = w d w f T .Definealso K
K ( f ( x 2 ), f ( y 2 )) = f ( x 2 ), f ( y 2 ) F and g ( a , b )tobe
A Neural-network-like Model. Consider the phrase ( w 1 , w we have a single trivial decomposition that includes all the subparts, and two two-word phrases ( ab )and( cd )
We need to rewrite this as as Equation (22). This example can be regarded as a simplified version of the neural-network model of Socher et al. (2011). The fact that the CC does not apply to it suggests that it will not apply to other models in this family. 5. Conclusion
The Convolution Conjecture offers a gener al way to rewrite the phrase similarity com-also suggests a strong connection between CDSMs and semantic convolution kernels.
This link suggests that insights from the CDSM literature could be directly integrated in the development of convolution kernels , with all the benefits offered by this well-understood general machine-learning framework.
 Acknowledgments 172
