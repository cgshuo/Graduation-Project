 Planning systems require action models as input. A typical way to describe action mod-els is to use action languages such as the planning domain description language (PDDL) [6]. A traditional way of building action models is to ask domain experts to analyze a planning domain and write a complete action model representation. However, it is very difficult and time-consuming to build action models in complex real world scenarios in such a way, even for experts. Thus, researchers have explored ways to reduce the human efforts of building action models by learni ng from observed examples or plan traces.
However, previous algorithms and experiments show that action model learning is a difficult task and the performances of the state-of-the-art algorithms are not very sat-isfying. A useful observation is that in many different planning domains, there exists some useful information that may be  X  X orrowed X  from one domain to another, provided that these different domains are similar in some aspects. In particular, we say that two domains A and B are similar if there is a mapping between some predicates of the two domains, in that the underlying principle of these actions, although their corresponding predicates are similar, resemble inherent similarities, then such a mapping can enable us to learn the action model in domain B by the mapping from the learned action model in domain A [9].

In this paper, we present a novel action model learning algorithm called t -LAMP ( t ransfer L earning A ction M odels other domains ). We use the shared common informa-tion from source domains to help to learn action models from a target domain (we call the domains whose information is transferred source domains, while the domain from which the action models need to be learned a target domain). We propose a method of building a metric to measure the  X  X imilarity X  between two domains, which is a difficult and not being answered question in planning domains. t -LAMP functions in the follow-ing three steps. Firstly, we encode the input plan traces into propositional formulas that are recorded as a DB . Secondly, we encode action models as a set of formulas. Finally, we learn weights of all formulas by transferring knowledge from source domains, and generate action models according to the weights of formulas.

The rest of the paper is organized as follows. We first give the definition of our problem and then describe the detailed steps of our algorithm. Then we will discuss some related works. In the experiment section, we will evaluate our algorithm in five planning domains of transfer learning action models and evaluate our transfer learning framework. Finally, we conclude the paper and discuss future work. Recently, some researchers have proposed various methods to learn action models from plan traces automatically. Jim, Jihie, Surya, Yolanda [3] and Benson [1] try to learn action models from plan traces with intermed iate observations. What they try to learn are STRIPS-models [5,6]. One limitation of their algorithm is all the intermediate states need to be known. Yang, Wu and Jiang designed an algorithm called ARMS [2], which can learn action models from plan traces with only partial intermedi ate observations, even without observations.

Another related work is Markov Logic Networks (MLNs)[4]. MLN is a powerful framework that combines probability and first-order logic. A MLN is a set of weighted formulae to soften constraints in first-order logic. The main motivation behind MLN to  X  X often X  constraints is that when a world violates a formula in a knowledge base, it is less probable, but not impossible.

In the transfer learning literature, Lilyana, Tuyen and Raymond[7] address the prob-lem of how to leverage knowledge acquired i n a source domain to improve the accuracy and speed of learning in a related target domain. [9] proposes to learn action models by transferring knowledge from another domain, which is the first try to transfer knowl-edge across domains. We represent a planning problem as P =(  X , s 0 ,g ) ,where  X  =( S, A,  X  ) is the plan-ning domain, s 0 is the initial state, and g is the goal state. In  X  , S is the set of states, A is the set of actions,  X  is the deterministic transition function, which is S  X  A  X  S .Aso-lution to a planning problem is called a plan, an action sequence ( a 0 ,a 1 ,...,a n ) which makes a projection from s 0 to g . Each a i is an action schema composed of a name and zero or more parameters. A plan trace is defined as T =( s 0 ,a 0 ,s 1 ,a 1 ,...,s n ,a n ,g ) , where s 1 , ..., s n are partial intermediate state observations that are allowed to be empty.
We state our learning problem as: g iven as input (1) a set of plan traces T in a target domain (that is, the domain from which we wish to learn the action models), (2) the description of predicates and actio n schemas in the target domain, and (3) the completely available action models in source domains , Our algorithm t -LAMP outputs preconditions and effects of each action mode l. An example of the input and output are shown in Fig.1. Before giving our algorithm t -LAMP , we present an overview of the algorithm as shown in Fig.2. In the following subsections, we give the detail description of the main steps which are highlighted. 4.1 Encoding Each Plan Trace as a Proposition Database As is defined in the problem definition, each pl an trace can be briefly stated as an action sequence with observed states, including initial state and goal state. We need to encode states and actions which are also called state transitions. We represent facts that hold in states using propositional formulae, e.g. consider the briefcase domain in Fig.1. We have an object o1 and a location l1 . We represent the state where the object o1 is in the briefcase and the briefcase is at location l1 with the propositional formula: in(o1)  X  is-at(l1) ,where in(o1) and is-at(l1) can be viewed as propositional variables. A model of the propositional formula is the one that assigns true value to the propositional variables in(o1) and is-at(l1) . Every object in a state should be represented by the propositional formula, e.g. if we have one more location l2 , the above propositional formula should be modified as: in(o1)  X  is-at(l1)  X  X  is-at(l2) . The behavior of deterministic actions is described by a transition function  X  . For instance, the action move(l1,l2) in Fig.1 is de-is at l2 . The states s 1 and s 2 can be represented by: is-at(l1)  X  X  is-at(l2) and  X  is-at(l1)  X  is-at(l2) . We need different propositional variables that hold in different states to spec-ify that a fact holds in one state but does not hold in another state. We introduce a new parameter in predicates, and represent the transition from the state s 1 to the state s 2 by that the action move(l1, l2) causes the transition can be represented by a propositional variable move(l1, l2, s 1 ) . Thus, the function  X  ( s 1 ,move ( l 1 ,l 2)) can be represented as a plan trace can be encoded correspondingly.

Thus, plan traces can be encoded as a set of propositional formulae, each of which is a conjunction of propositional variables. As a result, each plan trace can be represented by a set of propositional variables, whose eleme nts are conjunctive. This set is recorded in a database called DB , i.e. each plan trace is corresponded to its own DB . 4.2 Encoding Action Models as Formulae We consider an action model is a strips model plus conditional effects, i.e. a precon-dition of an action model is a positive atom, and an effect is either a positive/negative atom or a conditional effect. According to the semantic of an action model, we equally encode an action model with a list of formulae, as addressed in the following. T1: If an atom p is a positive effect of an action a ,then p must hold after a is executed. The idea can be formulated by:  X  i.a ( i )  X  X  p ( i )  X  p ( i + 1) ,where i corresponds to s i . T2: Similar to T1, the negation of an atom p is an effect of some action a ,which means p will never hold (be deleted) after a is executed, which can be formulated by:  X  i.a ( i )  X  X  p ( i + 1)  X  p ( i ) .
 T3: If an atom p is a precondition of a ,then p should hold before a is executed. That is to say, the following formula should hold:  X  i.a ( i )  X  p ( i ) .
 a conditional effect of some action a , which means for any  X  x ,if f (  X  x ) is satisfied, then q (  X  x ) will hold after a is executed. Here, f (  X  x ) is a formula in the conjunctive form of  X  q (  X  x ) )  X , can be encoded by:  X  i.  X  x.a (  X  x, i )  X  f (  X  x, i )  X  X  q (  X  x, i + 1) . By T1-T5, we can encode an action model by requiring its corresponding formulas to be always true. Furthermore, for each source domain D i , we can encode the action models in D i with a list of formulae F ( D i ) . 4.3 Building the Best Mapping In step 4, we find the best mapping between the source domain and the target domain, to bridge these two domains. To map two domains, firstly, we need to map the predicates between the source domain D i and the target domain D t ; secondly, map the action schemas between D i and D t . The mapping process of these two steps is the same, which is: for each predicate p i in D i and a predicate p t in D t , we build a unifier by mapping their corresponding names and arguments (we require that the number of arguments are the same in p i and p t , otherwise, we find next p t to be mapped with p i ); and then substitute all the predicates in D t by this unifier; for each p i and p t , we repeat the process of unifier-building and substitution until the unifier-building process stops.
By applying a mapping to the list of formulae F ( D i ) , we can generate a new list of formulae F  X  ( D t ) , which encodes action models of D t . We manage to calculate a score function on F  X  ( D t ) to measure the similarity between D i and D t . We exploit the idea of [4,8] to calculate the score WPLL (which will be defined soon) when learning weights of formulae. The calculate process is given in Fig.3 In the highlighted step (step 4) of Fig.3, WPLL , the Weighted Pseudo-Log-Likelihood [4], is defined as WPLL ( w )= l =1 log P w ( X l = x l | MB x ( X l )) where, P w ( X l = x l | MB x ( X l )) = database DB ). n is the number of all the possible groundings of atoms appearing in all the formulae F  X  ( D t ) ,and X l is the l th groundings of the all. MB x ( X l ) is the state of the Markov blanket of X l in x . The more detail description is presented by [4].
Using the algorithm, we will attain one score WPLL for each mapping. We keep the mapping (which is mentioned as the best mapping) with the highest score WPLL ,the resulting F  X  ( D t ) and their weights. 4.4 Generating Candidate Formulae and Action models In steps 6 and 7, using the predi cates and action schemas from D t , we will generate all the possible action models by doing a combination between them. We initially asso-ciate each candidate formulae with a weight of zero to indicate that no contribution is provided initially.

From the definition of WPLL , we can see that the larger the WPLL is, the more proba-ble the formulae F  X  ( D t ) are satisfied by DB s, i.e. the more similar the source domain and the target domain (from which DB s are attained) are. Thus, we use WPLL to mea-sure the similarity between source/target domains, and the weights of the resulting for-mulae F  X  ( D t ) to transfer information of the  X  X imilarity X . We exploit the idea that the  X  X imilarity X  information is strengthened (weakened) when other domains strengthen (weaken) it, by simply adding up the weights  X  w j = w j + w k  X  in step 10. With the weights attained by steps 7-12, in step 13 we learn weights of the candidate formulas by the algorithm of Fig.3.

From the learning process of WPLL , we can see that the optimization of WPLL indi-cates that when the number of true grounding of f i is larger, the corresponding weight of f i will be higher. In other words, the larger the weight of a candidate formula is, the more likely to be true that formula will be. When generating the final action models from these formulae in step 14, we need to determine a threshold, based on the valida-tion set of plan traces and our evaluation cr iteria (definition of error rate), to choose a set of formulae converted to action models in step 15. 5.1 Data Set and Evaluation Criteria We collect plan traces from the fo llowing planning domains: briefcase 1 , elevator 2 ,de-pots 3 , driverlog 3 , the plan traces numbers of which are 150, 150, 200 and 200 respec-tively. These plan traces are generated by generating plans from the given initial and goal states in these planning domains using the human encoded action models and a planning algorithm, FF planner 4 . Each of the domains will be used as the target do-main in our experiment. The source domains are: briefcase, elevator, depots, driverlog, zenotravel 3 .

We define error rates of our learning algorithm as the difference between our learned action models and the hand-written action m odels that are considered as the  X  X round truth X . If a precondition appears in the pr econditions of our learned action models but not in the ones of hand-written action models, the error count of preconditions, denoted by E ( pre ) , increases by one. If a precondition appears in hand-written ac-tion models but not in our learned action models, E ( pre ) increases by one. Like-wise, error count of effects are denoted by E ( ef f ) . Furthermore, we denote the total number of all the possible preconditions and effects of action models as T ( pre ) and T ( ef f ) , respectively. In our experiments, the error rate of an action model is defined preconditions and effects are equally important, and the range of error rate R ( a ) should be within [0,1]. Furthermore, the error rate of all the action models A is defined as R ( A )= 1 | A | a  X  A R ( a ) ,where | A | is the number of A  X  X  elements. 5.2 Experimental Results The evaluation results of t -LAMP in two domains are shown in Fig.4. The red curve (I) is the learning result without transferring any information from other domains; the blue curve (II) is the learning result with transferring information from the most similar domain based on WPLL ; the green curve (III) is the result with transferring informa-tion from the least similar domain based on WPLL ; the black curve (IV) is the result with transferring information from all the other source domains (when learning action models of briefcase , the source domains are elevator, depots, driverlog, zenotravel ). From these two figures, we can see that, the result by transferring information from all the other source domains is the best. Furthermore, by comparing the results of (II) and (III), we can see that, when we choose the most similar domain for transferring, the result is generally better than choosing the least similar domain, i.e. the score function WPLL works well in measuring the similarity of two domains.

The first row of Fig.4 shows the result of learning the action models of briefcase with transferring the information from depots, driverlog, zenotravel, elevator , while the sec-ond row shows the result of learning the action models of depots with transferring the information from briefcase, driverlog, zenotravel, elevator . We have chosen different thresholds with weights 1.0, 0.5, 0.1 and 0.01 to test the effect of the threshold on the performance of learning. The results show that generally the threshold can be neither too large nor too small, but the performance is not very sensitive to the choice of the value. An intuitive explanation is that, a threshold that is too large may lose useful can-didate formulae, and a threshold that is too small may contain too many noisy candidate formulae that will affect the overall accuracy of the algorithm. This intuition has been verified by our experiment. In our experiment, it can be shown that when we set the threshold as 0.5, the mean av erage accuracy is the best.

Our experiment shows that in most cases, the more states that are observable, the lower the error rate will be, which is consistent with our intuition. However, there are some other cases, e.g. when threshold is set to 0.01, when there are only 1 / 4 of states that are observable, the error rate is lower than the case when 1 / 3 of states are observable.
From our experiment results, we can see t hat transferring useful knowledge from another domain will help improve our action model learning result. On the other hand, determining the similarity of two domains is important. In this paper, we have presented a novel approach to learn action models through trans-fer learning and a set of observed plan traces. we propose a method to measure the sim-ilarity between domains and make use of the idea of Markov Logic Networks to learn action models by transferring informatio n from other domains according to  X  X imilar-ity X . Our empirical tests show that our met hod is both accurate and effective in learning the action models via information transfer. In the future, we wish to extend the learn-ing algorithm to more elaborate action representation languages including resources and functions. We also wish to explore how to make use of other inductive learning algorithms to help us learn better.

