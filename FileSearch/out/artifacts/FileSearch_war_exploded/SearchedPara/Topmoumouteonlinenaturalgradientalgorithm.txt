 An efcient optimization algorithm is one that quickly nds a good minimum for a given cost func-tion. An efcient learning algorithm must do the same, with the additional constraint that the func-tion is only kno wn through a proxy . This work aims to impro ve the ability to generalize through more efcient learning algorithms.
 Consider the optimization of a cost on a training set with access to a validation set. As the end the training error while monitoring the validation error to ght overtting. This approach mak es the underlying assumption that overtting happens at the later stages. A better perspecti ve is that with the true gradient, and the dot product between the two actually eventually becomes negati ve. Early stopping is designed to determine when that happens. One can thus wonder: can one limit overtting before that point? Would this actually postpone that point? From this standpoint, we disco ver new justications behind the natural gradient [1]. Depending on certain assumptions, it corresponds either to the direction minimizing the probability of increasing generalization error , or to the direction in which the generalization error is expected to decrease respect to computation time and memory , when the number of parameters becomes lar ge. To address this issue, we propose a generally applicable online approximation of natural gradient that scales linearly with the number of parameters (and requires computation time comparable to stochastic gradient descent). Experiments sho w that it can bring signicant faster con vergence and impro ved generalization. Let e L be a cost dened as e L ( ) = encountered and can be quite dif cult. There exist various techniques to tackle it, their efcienc y depending on L and p . In the case of non-con vex optimization, gradient descent is a successful technique. The approach consists in progressi vely updating using the gradient e g = d e L [1] sho wed that the parameter space is a Riemannian space of metric e C (the covariance of the gradients), and introduced the natural gradient as the direction of steepest descent in this space. The natural gradient direction is therefore given by e C 1 e g . The Riemannian space is kno wn to correspond to the space of functions represented by the parameters (instead of the space of the parameters themselv es).
 The natural gradient some what resembles the Ne wton method. [6] sho wed that, in the case of a mean squared cost function, the Hessian is equal to the sum of the covariance matrix of the gradients and of an additional term that vanishes to 0 as the training error goes down. Indeed, when the data are generated from the model, the Hessian and the covariance matrix are equal. There are two important dif ferences: the covariance matrix e C is positi ve-denite, which mak es the technique more stable, but contains no explicit second order information. The Hessian allo ws to account for variations in even if the mean suggests otherwise. In that sense, it is a conserv ative gradient. Until now, we supposed we had access to the true distrib ution p . Ho we ver, this is usually not the samples dene a cost L (resp. a gradient g ) that, although close to the true cost (resp. gradient), is danger is then to overt the parameters to the training set, yielding parameters that are not optimal with respect to the generalization error .
 A simple way to ght overtting consists in determining the point when the continuation of the optimization on L will be detrimental to e L . This can be done by setting aside some samples to on the validation set, the optimization should be stopped. We propose a dif ferent perspecti ve on overtting. Instead of only monitoring the validation error , we consider using as descent direction an estimate of the direction that maximizes the probability of reducing the generalization error . The goal is to limit overtting at every stage, with the hope that the optimal point with respect to the validation should have lower generalization error .
 the training error drops. Since the learning objecti ve is to minimize generalization error , we would lik e v T e g as small as possible, or at least always negati ve.
 By denition, the gradient on the training set is g = 1 number of training samples. With a rough approximation, one can consider the g true gradient distrib ution and assume all the gradients are independent and identically distrib uted. The central limit theorem then gives We will now sho w that, both in the Bayesian setting (with a Gaussian prior) and in the frequentist setting (with some restrictions over the type of gradient considered), the natural gradient is optimal in some sense. 2.1 Bay esian setting the samples g the g Denoting e C = I + e C Using this result, one can choose between several strate gies, among which two are of particular interest: 2.2 Fr equentist setting consider (as all second-order methods do) the directions v of the form v = M T g (i.e. we are only allo wed to go in a direction which is a linear function of g ).
 Since g N e g; e C The matrix M which minimizes the probability of v T e g to be positi ve satises 0 for all e g , one must have M / e C 1 and we obtain the same result as in the Bayesian case: the natural gradient represents the direction minimizing the probability of increasing the generalization error . The pre vious sections pro vided a number of justications for using the natural gradient. Ho we ver, Indeed, considering p as the number of parameters and n as the number of examples, a direct batch specti vely with the gradients' covariance storage, computation and inversion. This section revie ws existing low comple xity implementations of the natural gradient, before proposing TONGA, a new low comple xity , online and generally applicable implementation suited to lar ge scale problems. In we of course use an empirical estimate, and here this estimate is furthermore based on a low-rank approximation denoted C (actually a sequence of estimates C 3.1 Lo w complexity natural gradient implementations [9] proposes a method specic to the case of multilayer perceptrons. By operating on blocks of nique is quite involv ed, specic to multilayer perceptrons and requires two assumptions: Gaussian general approach based on the Sherman-Morrison formula used in Kalman lters: the technique the memory requirement remains O ( p 2 ) . It is howe ver not necessary to compute the inverse of the gradients' covariance, since one only needs its product with the gradient. [10 ] offers two approaches to exploit this. The rst uses conjugate gradient descent to solv e Cv = g . The second revisits [9] thereby achie ving a lower comple xity . [8] also proposes an iterati ve technique based on the minimization of a dif ferent cost. This technique is used in the minibatch setting, where Cv can be computed cheaply through two matrix vector products. Ho we ver, estimating the gradient covariance only from a small number of examples in one minibatch yields unstable estimation. 3.2 TONGA Existing techniques fail to pro vide an implementation of the natural gradient adequate for the lar ge scale setting. Their main failings are with respect to computational comple xity or stability . TONGA was designed to address these issues, which it does this by maintaining a low rank approximation of the covariance and by casting both problems of nding the low rank approximation and of computing the natural gradient in a lower dimensional space, thereby attaining a much lower comple xity . What we exploit here is that although a covariance matrix needs man y gradients to be estimated, we can tak e adv antage of an observ ed property that it generally varies smoothly as training proceeds and mo ves in parameter space. 3.2.1 Computing the natural gradient dir ection between tw o eigendecompositions Ev en though our moti vation for the use of natural gradient implied the covariance matrix of the em-pirical gradients, we will use the second moment (i.e. the uncentered covariance matrix) throughout the paper (and so did Amari in his work). The main reason is numerical stability . Indeed, in the batch setting, we have (assuming C is the centered covariance matrix and g the mean) v = C 1 g , thus Cv = g . But then, ( C + gg T ) v = g + gg T v = g (1 + g T v ) and Ev en though the direction is the same, the scale changes and the norm of the direction is bounded Since TONGA operates using a low rank estimate of the gradients' non-centered covariance, we must be able to update cheaply . When presented with a new gradient, we inte grate its information using the follo wing update formula 2 : where C greater rank, and the problem resides in computing its low rank approximation ^ C X With such covariance matrices, computing the (re gularized) natural direction v Using the Woodb ury identity with positi ve denite matrices [7], we have If
X t is of size p r (with r &lt; p , thus yielding a covariance matrix of rank r ), the cost of this computation is O ( pr 2 + r 3 ) . Ho we ver, since the Gram matrix G the cost of computing G 3.2.2 Updating the low-rank estimate of C To keep a low-rank estimate of C the rst k eigen vectors. This can be made at low cost using its relation to that of G The cost of such an eigendecomposition is O ( kr 2 + pkr ) (for the computation of the eigendecom-X using with U 3.2.3 Computational complexity The computational comple xity of TONGA depends on the comple xity of updating the low rank approximation and on the comple xity of computing the natural gradient. The cost of updating the the natural gradient v k + b is then O ( pb ) .
 Furthermore, by operating on minibatch gradients of size b 0 , we end up with a cost per example of cal comparison of cpu time also sho ws comparable CPU time per example, but faster con vergence. In our experiments, p was in the tens of thousands, k was less than 5 and b was less than 50 . The result is an approximate natural gradient with low comple xity , general applicability and exi-bility over the tradof f between computations and the quality of the estimate. One might wonder if there are better approximations of the covariance matrix C than computing its rst k eigen vectors. One possibility is a block-diagonal approximation from which to retain only the rst k eigen vectors of every block (the value of k can be dif ferent for each block). Indeed, [4] sho wed that the Hessian of a neural netw ork with one hidden layer trained with the cross-entrop y cost con verges to a block diagonal matrix during optimization. These blocks are composed of the weights linking all the hidden units to one output unit and all the input units to one hidden unit. Given the close relationship between the Hessian and the covariance matrices, we can assume the y have a similar shape during the optimization.
 Figure 1 sho ws the correlation between the standard stochastic gradients of the parameters of a 16 50 26 neural netw ork. The rst blocks represent the weights going from the input units to each hidden unit (thus 50 blocks of size 17, bias included) and the follo wing represent the weights going from the hidden units to each output unit (26 blocks of size 51). One can see that the block-diagonal approximation is reasonable. Thus, instead of selecting only k eigen vectors to represent Ho we ver, the rank of the approximation goes from k to k number of blocks . In the matrices sho wn in gure 1, which are of size 2176 , a value of k = 5 yields an approximation of rank 380 . Figure 1: Absolute correlation between the standard stochastic gradients after one epoch in a neural netw ork with 16 input units, 50 hidden units and 26 output units when follo wing stochastic gradient directions (left) and natural gradient directions (center and right).
 or block-diagonal). We can rst notice that approximating only the blocks yields a ratio of : 35 (in comparison, taking only the diagonal of C yields a ratio of : 80 ), even though we considered only 82076 out of the 4734976 elements of the matrix ( 1 : 73% of the total). This ratio is almost obtained with k = 6 . We can also notice that, for k &lt; 30 , the block-diagonal approximation is much better (in terms of the Frobenius norm) than the full approximation. The block diagonal approximation is therefore very cost effecti ve. Figure 2: Quality of the approximation C of the covariance C depending on the number of eigen vec-C (full matrix or block diagonal) This sho ws the block diagonal approximation constitutes a powerful and cheap approximation of the covariance matrix in the case of neural netw orks. Yet this approximation also readily applies to any mixture algorithm where we can assume independence between the components. We performed a small number of experiments with TONGA approximating the full covariance ma-trix, keeping the overhead of the natural gradient small (ie, limiting the rank of the approximation). Re grettably , TONGA performed only as well as stochastic gradient descent, while being rather sen-siti ve to the hyperparameter values. The follo wing experiments, on the other hand, use TONGA with the block diagonal approximation and yield impressi ve results. We belie ve this is a reection of the phenomenon illustrated in gure 2: the block diagonal approximation mak es for a very cost effecti ve approximation of the covariance matrix. All the experiments have been made optimizing hyperparameters on a validation set (not sho wn here) and selecting the best set of hyperparameters for testing, trying to keep small the overhead due to natural gradient calculations. One could worry about the number of hyperparameters of TONGA. Ho we ver, def ault values of k = 5 , b = 50 and = : 995 yielded good results in every experiment. When goes to innity , TONGA becomes the standard stochastic gradient algorithm. Therefore, a simple heuristic for is to progressi vely tune it down. In our experiments, we only tried powers of ten. 5.1 MNIST dataset The MNIST digits dataset consists of 50000 training samples, 10000 validation samples and 10000 test samples, each one composed of 784 pix els. There are 10 dif ferent classes (one for every digit). Figure 3: Comparison between stochastic gradient and TONGA on the MNIST dataset ( 50000 train-ing examples), in terms of training and test classication error and Ne gati ve Log-Lik elihood (NLL). The mean and standard error have been computed using 9 dif ferent initializations.
 Figure 3 sho ws that in terms of training CPU time (which includes the overhead due to TONGA), TONGA allo ws much faster con vergence in training NLL, as well as in testing classication error and testing NLL than ordinary stochastic and minibatch gradient descent on this task. One can also note that minibatch stochastic gradient is able to prot from matrix-matrix multiplications, but this adv antage is mainly seen in training classication error . 5.2 Rectangles problem The Rectangles-ima ges task has been proposed in [5] to compare deep belief netw orks and support vector machines. It is a two-class problem and the inputs are 28 28 gre y-le vel images of rectangles located in varying locations and of dif ferent dimensions. The inside of the rectangle and the back-ground are extracted from dif ferent real images. We used 900,000 training examples and 10,000 val-idation examples (no early stopping was performed, we sho w the whole training/v alidation curv es). All the experiments are performed with a multi-layer netw ork with a 784-200-200-100 -2 architec-ture (pre viously found to work well on this dataset). Figure 4 sho ws that in terms of training CPU time, TONGA allo ws much faster con vergence than ordinary stochastic gradient descent on this task, as well as lower classication error . Figure 4: Comparison between stochastic gradient descent and TONGA w.r.t. NLL and classica-tion error , on training and validation sets for the rectangles problem (900,000 training examples). [3] revie ws the dif ferent gradient descent techniques in the online setting and discusses their re-direction of is v = M g with g the gradient and M a positi ve semidenite matrix) is optimal (in terms of con vergence speed) when M con verges to H 1 . Furthermore, the speed of con vergence depends (amongst other things) on the rank of the matrix M . Given the aforementioned relationship between the covariance and the Hessian matrices, the natural gradient is close to optimal in the sense dened abo ve, pro vided the model has enough capacity . On mixture models where the block-diagonal ap-proximation is appropriate, it allo ws us to maintain an approximation of much higher rank than a standard low-rank approximation of the full covariance matrix.
 the greatest probability of not increasing generalization error or the direction with the lar gest ex-pected increase in generalization error , we obtain new justications for the natural gradient descent direction. Second, we present an online low-rank approximation of natural gradient descent with computational comple xity and CPU time similar to stochastic gradientr descent. In a number of experimental comparisons we nd this optimization technique to beat stochastic gradient in terms of speed and generalization (or in generalization for a given amount of training time). Ev en though de-fault values for the hyperparameters yield good results, it would be interesting to have an automatic procedure to select the best set of hyperparameters.

