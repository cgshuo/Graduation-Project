 Arabic is written without certain orthographic sym-bols, called diacritics , which represent among other things short vowels. 1 The restoration of diacritics to written Arabic is an important processing step for several natural language processing applications, including training language models for automatic speech recognition, text-to-speech generation, and so on. For a discussion of the role of diacritiza-tion, see (Maamouri et al., 2006). In this paper , we present a new diacritization module that outperforms the best pre viously published results, using a new combination of techniques. A more detailed presen-tation can be found in (Habash and Rambo w 2007). Arabic script consists of two classes of symbols: letters and diacritics. Letters are always written whereas diacritics are optional: written Arabic can be fully diacritized, it can have some diacritics (to disambiguate certain words), or it can be entirely undiacritized. There are three types of diacritics: vowel, nunation, and shadda. Vowel diacritics rep-resent Arabic X  s three short vowels and the absence of any vowel. The follo wing are the four vowel-diacritics exemplified in conjunction with the letter bu , bi and bo (no vowel) . Nunation diacrit-ics can only occur in word final positions in nomi-nals (nouns, adjecti ves and adv erbs). The y represent a short vowel follo wed by an n sound: 2 bF , bN and bK . Nunation is an indicator of nominal indefiniteness. Shadda is a consonant doubling dia-critic: b . The shadda can combine with vowel or nunation diacritics: b u or b N . Addi-tional diacritical marks in Arabic include the hamza, which appears in conjunction with a small number of letters (e.g., , , , , ). Since most Arabic en-codings do not consider the hamza a diacritic, but rather a part of the letter (lik e the dot on the lower -case Roman i or under the Arabic b : ), we do not count it here as part of the diacritic set.
Functionally , diacritics can be split into two dif-ferent kinds: lexemic diacritics and inflectional di-acritics . Le xemic diacritics distinguish between two lexemes. 3 For example, the diacritization dif fer -ence between the lexemes kAtib  X  X riter X  and meanings of the word rather than their inflections. Thus, there are lexemes that look alik e when undia-critized but are spelled dif ferently when diacritized. Note that there are also distinct lexemes that are al-ways spelled the same way, even when diacritized  X  their dif ference is only a dif ference in word sense.
Inflectional diacritics distinguish dif ferent in-flected forms of the same lexeme. For instance, the final diacritics in katabtu  X  X  wrote X  and the subject of the verb . We further distinguish be-tween two types of inflectional diacritics: variant inflectional diacritics and invariant inflectional dia-critics. The distinction is made with respect to two morphosyntactic features: nominal case and verbal mood. The variant inflectional diacritics need not al-ways appear at the end of the word. For instance, the variant inflectional diacritics at the penultimate po-sitions of the follo wing two words distinguish their case: kAtib uhu  X  X is writer [nominati ve] X  and In a pre vious publication, we described the Mor -phological Analysis and Disambiguation of Ara-bic (M ADA ) system (Habash and Rambo w, 2005). The basic approach used in M ADA is inspired by the work of Haji  X  c (2000) for tagging morphologi-cally rich languages, which was extended to Ara-bic independently by Haji  X  c et al. (2005). In this approach, a set of taggers are trained for indi vid-ual linguistic features which are components of the full morphological tag (such as core part-of-speech, tense, number , and so on). In Arabic, we have ca. 2,000 to 20,000 morphological tags, depending on how we count. The Buckw alter Arabic Morpholog-ical Analyzer (BAMA) (Buckw alter , 2004) is con-sulted to produce a list of possible analyses for a word. BAMA returns, given an undiacritized in-flected word form, all possible morphological anal-yses, including full diacritization for each analy-sis. The results of the indi vidual taggers are used to choose among these possible analyses. The algo-rithm we proposed in (Habash and Rambo w, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguis-tic features in each candidate analysis. Haji  X  c et al. (2005), howe ver, weigh the predicted values by their probability or confidence measure. To our kno wl-edge, no results on diacritization have been pre vi-ously reported using this particular approach to tag-
In this paper , we extend our basic M ADA sys-tem in the follo wing ways: First, we follo w Haji  X  c et al. (2005) in including case, mood, and nunation as features, because of its importance to diacritiza-tion. Second, we replace the YAMCHA (K udo and Matsumoto, 2003) implementation of Support Vec-tor Machines (SVMs) with SVMT ool (Gim  X  enez and M` arquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accurac y. Lik e Haji  X  c et al. (2005), we do not use Viterbi decoding. Finally , we introduce a specialized module for resolving residual ambiguity after the ba-sic tagging is done. We explain this module in detail next. We train our classifiers on the exact training set defined by Zitouni et al. (2006), a subpart of the third segment of the Penn Arabic Treebank (Maamouri et al., 2004) ( X  X TB3-T rain X , 288,000 words). We also (reluctantly) follo w them in having a single set for development and testing ( X  X TB3-De vtest X , 52,000 words), rather than separate development and test sets (as is common), in order to be able to compare our results to theirs.

Up until this point, M ADA -D has narro wed the list of possible analyses of a word (supplied by BAMA) down to a small number . This number can sometimes be greater than one for two reasons: first, the way in which we use the output of the taggers to choose among the analyses may yield a tie among several analyses; second, there may be lexeme-based diacritic ambiguity , and the morphological taggers cannot disambiguate lexemic diacritization. To ad-dress the residual ambiguity , we implemented a sec-ond component. Ideally , this would be (or include) a full word sense disambiguation (WSD) system, but WSD is a hard problem. Instead, we approximate WSD using standard n-gram language models. We use two types of data for training: fully diacritized word forms, and data in which we have replaced the inflected word by the diacritized citation form of the lexeme. Note that this procedure conflates lexemes that dif fer only in meaning, not in diacritization, as we are not actually interested in WSD for its own sak e in this paper . The training corpus is the same corpus we use for the classifiers, ATB3-T rain. This means that the diacritization and the choice of lex-eme are done by hand, but it also means that the training set is quite small by the standards of lan-guage models. We build an open-v ocab ulary lan-guage model with Kneser -Ne y smoothing using the SRILM toolkit (Stolck e, 2002). We will call the re-sulting language models X LM-n , where X is  X  X  X  for the fully diacritized word forms, or  X  X  X  for the lexeme citation forms, and n is the order of the n-grams ( n = 1 , 2 , 3 ). When all candidate tok ens (di-acritized word or lexeme citation form) are unkno wn (out-of-v ocab ular y), the language model does not actually mak e a choice among them. We then use a diacritization unigram model, and then finally ran-dom choice. In the case of a preceding DLM-n model, this simply amounts to random choice, but in the case of a preceding LLM-n model, the dia-critization model may actually mak e a non-random choice. We revie w three approaches that are directly rele-vant to us; we refer to the excellent literature revie w in (Zitouni et al., 2006) for a general revie w. Vergyri and Kirchhof f (2004) follo w an approach similar to ours in that the y choose from the diacritizations pro-posed by BAMA. Ho we ver, the y train a single tag-ger using unannotated data and EM, which necessar -ily leads to a lower performance. The most salient dif ference, howe ver, is that the y are moti vated by the goal of impro ving automatic speech recognition, and have an acoustic signal parallel to the undiacritized text. All their experiments use acoustic models. The y sho w that WER for diacritization decreases by nearly 50% (from 50%) when BAMA is added to the acoustic information, but the tagger does not help. It would be interesting to investigate ways of incorpo-rating acoustic model information in our approach.
Ananthakrishnan et al. (2005) also work on dia-critization with the goal of impro ving ASR. The y use a word-based language model (using both di-acritized and undiacritized words in the conte xt) but back off to a character -based model for unseen words. The y consult BAMA to narro w possible di-acritizations for unseen words, but BAMA does not pro vide much impro vement used in this manner .
Zitouni et al. (2006) use a maximum entrop y clas-sifier to assign a set of diacritics to the letters of each word. The y use the output of a tok enizer (se g-menter) and a part-of-speech tagger (which presum-ably tags the output of the tok enizer). The y then use segment n-grams, segment position of the character being diacritized, the POS of the current segment, along with lexical features, including letter and word n-grams. Thus, while man y of the same elements are used in their and our work (w ord n-grams, fea-tures related to morphological analysis), the basic approach is quite dif ferent: while we have one pro-cedure that chooses a correct analysis (including to-Model WER DER WER DER Only-DLM-1 39.4 14.5 13.8 6.6 Tagger -DLM-1 15.9 5.3 6.2 2.5 Tagger -DLM-2 15.2 5.1 5.8 2.4 Tagger -DLM-3 15.1 5.0 5.7 2.4 Tagger -LLM-1 16.0 5.3 6.3 2.6 Tagger -LLM-2 15.0 4.9 5.6 2.2 Tagger -LLM-3 14.9 4.8 5.5 2.2 Only-LLM-3 35.5 10.8 8.8 3.6 Tagger -noLM 16.0 5.3 6.3 2.6 Zitouni 18.0 5.5 7.9 2.5 Figure 1: Diacritization Results (all follo wed by single-choice-diac model); our best results are sho wn in boldf ace; Only-DLM-1 is the baseline;  X  X itouni X  is (Zitouni et al., 2006) kenization, morphological tag, and diacritization), the y have a pipeline of processors. Furthermore, Zi-touni et al. (2006) do not use a morphological lexi-con. To our kno wledge, their system is the best per -forming currently , and we have set up our experi-ments to allo w us to compare our results directly to their results. There are several ways of defining metrics for dia-critization. In order to assure maximal comparabil-ity with the work of Zitouni et al. (2006), we adopt their metric. 5 We count all words, including num-bers and punctuation. Each letter (or digit) in a word is a potential host for a set of diacritics; we count all diacritics on a single letter as a single binary choice. So, for example, if we correctly predict a shadda but get the vowel wrong, it counts as a wrong choice. We approximate non-v ariant diacritization by remo ving all diacritics from the final letter ( Ig-nor e Last ), while counting that letter in the evalua-tion. We give diacritic error rate ( DER ) which tells us for how man y letters we incorrectly restored all diacritics, and word error rate ( WER ), which tells us how man y words had at least one DER.

The results are sho wn in Figure 1. Going top to bottom, we first see the baseline, Only-DLM-1 , which is simply a diacritization unigram model with random choice for unseen words. We then sho w the results using the morphological tagger along with a language model. We first sho w results for the dia-critization model, with 1-, 2-, and 3-grams. As we can see, the bigram language model helps slightly . The next three lines are the three lexeme n-gram models. Here we see that the unigram model per -forms worse than the unigram diacritization model, while the bigram and trigram models perform better (the trigram lexeme model is our best result). We interpret this as meaning that the lexeme model is useful only when conte xt is tak en into account, be-cause it is actually performing a rudimentary form of WSD. We tease apart the contrib ution of the tagger and of the language model with two further experi-ments, in the next two lines: using just the lexeme language model (trigrams), and running just the tag-ger , follo wed by random choice. We can see that the tagger alone does as well as the tagger with the unigram lexeme model, while the lexeme model on its own does much worse. Ho we ver, as expected, the lexeme model on its own for the Ignore Last measure is much closer to the performance of the tagger on its own. We conclude from this that the quite simple lexeme model is in fact contrib uting to the correct choice of the lexemic diacritics. Finally , we give the results of Zitouni et al. (2006) on the last line, which we understand to be the best published results cur -rently . We see that we impro ve on their results in all cate gories. We can see the effect of our dif ferent ap-proaches to diacritization in the numbers: while for WER we reduce the Zitouni et al error by 17.2%, the DER error reduction is only 10.9%. This is because we are choosing among complete diacritization op-tions for white space-tok enized words, while Zitouni et al. (2006) mak e choices for each diacritic. This means that when we mak e a mistak e, it may well affect several diacritics at once, so that the diacritic errors are concentrated in fewer words. This effect is even stronger when we disre gard the final letter (30.4% reduction in WER versus 12.0% reduction in DER), suggesting that singleton errors in words tend to be in the final position (case, mood), as it is often hard for the tagger to determine these features. We have sho wn that a diacritizer that uses a lexical resource can outperform a highly optimized ad-hoc diacritization system that dra ws on a lar ge number of features. We speculate that further work on WSD could further impro ve our results. We also note the issue of unkno wn words, which will affect our sys-tem much more than that of (Zitouni et al., 2006). It is possible to construct a combined system which uses a lexicon, but backs off to a Zitouni-style sys-tem for unkno wn words. Ho we ver, a lar ge portion of the unkno wn words are in fact foreign words and names, and it is not clear whether the models learned handle such words well.

