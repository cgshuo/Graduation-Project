 The World Wide Web evolves dynamically through constantly adding, deleting, mov-ing, or otherwise changing web pages and hyperlinks, which leads to the problem of broken hyperlink (unresolvable hyperlink). A typical example is that many URL cita-tions in research papers become invalid as ear ly as a year or two after publication [2]. How to find the desired web page when its URL is broken is a challenging problem. Among the proposed approaches to address this problem, robust hyperlinks [7] offer high probability of being successfully resolved even after the target page had made an unannounced move and left no forwarding address, even if the page had been edited as well. A robust hyperlink is a URL augmented with a small lexical signature con-sisting of carefully chosen words taken from the referenced web page. For example, the lexical signature for the web page referred by  X  X ttp://www.britishhorrorfilms. co.uk/rillington.shtml X  may be  X  X illington+geeson +Christie+constable+hurt X . If the address-based portion of the URL (i.e.  X  X ttp://www.britishhorrorfilms. co.uk/ rilling-ton.shtml X ) fails, this content-based signature (i.e.  X  X illington+geeson + Christie+ constable+hurt X ) can be submitted as a query to web search engines to locate the web page. Robust hyperlinks can be computed cheaply, exploited automatically and con-veniently, and understood easily. The performance of robust hyperlinks relies heavily on the computation of lexical signatures. As stated in [4, 5], lexical signatures should easily extract the desired web page and be useful enough to find relevant information when the precise web pages being searched are lost. Moreover, lexical signatures should have minimal search engine dependency and new lexical signatures should have minimal overlap with existing lexical signatures. 
A number of basic and hybrid methods are empirically examined in [4, 5], where basic lexical signatures are computed using a single metric, and hybrid signatures combine words generated from two different basic methods. The detailed explanation [4, 5] are as follows: 
Basic Lexical Signature Methods 1. TF: Select words in decreasing term freque ncy (TF) order. If there is a tie, then 2. DF: Select words in increasing DF order. If there is a tie, then pick words based 3. TFIDF: Select words in decreasing term-frequency inverse-document-frequency 4. PW: Select words based on Phelps and Wilensky X  X  method [7], or decreasing 
Hybrid Lexical Signature Methods 1. TF3DF2: Select two words in increasing DF order. Then filter out all words 2. TF4DF1: Select one word based on increasing DF order first. Then filter out all 3. TFIDF3DF2: Select two words based on increasing DF order first. Then filter 4. TFIDF4DF1: Select one word based on increasing DF order first. Then filter and the second part for uniquely identifying the desired web page. Note that we only examine five-word lexical signatures in this study. 
In the above lexical signature computation methods, different words in a web page are usually assumed to be independent. In fact, different words could express the same or similar meanings due to the synonym phenomenon. An example of synonym is the words  X  X at X  and  X  X eline X . In this study, we propose a WordRank-based method to compute lexical signatures, which can take into account the semantic relatedness between words and choose the most representative and salient words as lexical signa-ture. Experiments on the web show that WordRank-based lexical signatures are best for retrieving highly relevant web pages when the desired web page cannot be ex-tracted. The WordRank method is inspired by the TextRank method [3] and it can take into account the semantic relatedness between words and choose the most representative and salient words as lexical signature. The semantic relatedness between words is computed with the vector measure [6] based on the electronic lexical database-WordNet [1]. 
The basic idea underlying the WordRank method is the mutual reinforcement prin-ciple widely employed in many graph-based algorithms, e.g. PageRank, HITS and Positional Function. In the graph, when one vertex links to another one, it is basically casting a weighted vote for that other vertex. The higher the total weight of votes that this information is also taken into account by the ranking model. The WordRank method goes as follows: 
First we have to build a graph that represents the text and interconnects words with meaningful relations to enable the application of graph-based ranking algorithms. We denote the graph as G=(V,E) with the set of vertices V and set of edges E . V contains weighted. Two vertices are connected if their corresponding words are semantically edge. normalized tf*idf value of the corresponding word. The ranking algorithm formulated as follows is run on the graph for several iterations until it converges. initial score of vertext V i and w ji represents the weight of the edge between vertex V j tor and usually set to 0.85. 
Note that the convergence is achieved when the error rate for any vertex in the proximated with the difference between the scores computed at two successive itera-tions: WS k+1 (V i ) -WS k (V i ). The convergence threshold is 0.0001 in our experiments. decreasing order of their score. The WordRank-based method is then defined as follows: 
WordRank: Select words in decreasing WordRank score calculated above. If there 
In order to better identify the desired web page uniquely, two new hybrid lexical signature methods combining DF are defined as follows: 
WordRank3DF2: Select two words based on increasing DF order first. Then filter 
WordRank4DF1: Select one word based on increasing DF order first. Then filter The experiments are based on two popular search engines: Google and MSN Search . We randomly extracted 2000 URLs from the open directory of DMOZ.org and downloaded the corresponding web pages. The URLs that could not be downloaded pdf, ps, doc, etc) were excluded. After removing HTML tags, some web pages do not have any words or have only a few words in their content. After removing stop words from word tokens, we excluded all documents that contained less than five unique words. Lastly, 1337 documents were reserved. 
For each web document, eleven lexical signatures consisting of five words were computed based on different methods. Then we used the lexical signatures as queries for two search engines: Google and MSN . If a search engine did not return any docu-ments, we removed a word from the given lexical signatures based on its lowest DF order and re-queried the search engine. Th is procedure was con tinued until the search engine returned documents or all of words in the given lexical signature were re-moved. After the search engine returned the list of documents, only the top ten docu-ments were downloaded and the similarity between returned documents and the target document was calculated using the cosine measure. the returned list, the better the lexical signature is. As in [15, 16], four disjoint classes are defined as follows: 
Unique represents the percentage of lexical signatures that successfully extract and return the single desired document. 
Top represents the percentage of lexical sign atures that extract a list of documents with the desired document first ranked. with the desired document but not first ranked, but one of top ten. 
Other represents the percentage of lexical signatures that failed to extract the desired document. 
Note that the above added together represent 100% of all classes. Figures 1 and 2 show the desired document retrieval performance and we can see that DF-based lexi-cal signatures are most efficient for uniquely extracting the desired documents. How-ever, in practice, if the desired documents ar e highly ranked in the returned list, such are combined, and then hybrid methods with two words chosen by DF (i.e. TF3DF2, TFIDF3DF2 and WordRank3DF2) are most ef ficient. WordRank3DF2 is a little more efficient than TF3DF2 and TFIDF3DF2. 
Our second concern is whether or not the lexical signature can find a related docu-documents could not be retrieved from Google using all lexical signature methods and 24% from MSN . The possible reasons are the following: The desired documents are not yet indexed by the search engines; they are moved, deleted, modified or updated; highly relevant web documents. We analyze the average cosine similarity values of top ten retrieved documents to the desired document. Figure 3 shows the average cosine values of top ten documents in the Not Retrieved Document class and Figure 4 shows the average cosine values of top ten documents for all 1337 documents. 
Seen from Figures 3 and 4, DF and PW yield smaller average similarity values, while WordRank and its hybrid methods yield larger average similarity values for both Not Retrieved Documents and all 1337 documents. It demonstrates that the WordRank-based lexical signatures can retrieve more relevant documents to the de-sired document. 
To summarize, DF is the best method for uniquely identifying the desired docu-ments; TF is easy to compute and does not need to be updated unless the documents are modified; TFIDF and the hybrid methods combining TFIDF and DF are good candidates for extracting the desired documents; WordRank-based methods are best for retrieving highly relevant documents. 
