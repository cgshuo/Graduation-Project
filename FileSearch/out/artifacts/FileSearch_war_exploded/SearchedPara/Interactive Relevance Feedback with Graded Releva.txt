 Research on relevance feedback (RFB) in information retrieval (IR) has given mixed results. Su ccess in RFB seems to depend on the searcher X  X  willingness to provide feedback and ability to iden-tify relevant documents or query keys. The paper is based on simulating many user scenarios regarding the amount and quality of RFB. In addition, we experi ment with query-biased sentence extraction for query reformulation. The baselines are initial no-feedback queries and queries based on pseudo-relevance feed-back. The core question is: unde r which conditions would RFB based on sentence extraction be successful? The answer depends on user X  X  behavior, implementati on of feedback query formula-tion, and the evaluation methods. A small amount of feedback from a short browsing window seem s to improve the final ranking the most. Longer browsing allows more feedback and better que-ries but also consumes the av ailable relevant documents. H.3.1 [ Content analysis and indexing ]: Linguistic processing H.3.3 [ Information Search and Retrieval ]: Relevance feedback Measurement, Performance, Theory. User simulation, relevance feedback, summarization. It is common knowledge in informati on retrieval that real users of information retrieval (IR) systems often use simplistic initial que-ries, which are prone to fail due to vocabulary mismatch, ambigu-ity and/or lack of discrimination power. Real searchers X  first query formulation often acts as an entry to the search system and is followed by browsing and query reformulations [5]. Relevance feedback (RFB) based on initial query results, and query expan-sion (QE) have been the main appr oaches to query reformulation. There are several reviews of the techniques, e.g., [1] [7]. In the present paper we focus on interactive RFB. In this method, users either point out relevant documents and the retrieval system infers the expansion keys for the feedback query, or the retrieval system presents a list of candidate expansion keys for the user to choose from. Knowledgeab le experienced searchers may benefit more of RFB because they recognize relevant vocabulary and are better able to articulate their needs initially [8]. Users also seem more likely to identify highly relevant documents than mar-ginal ones [12]. There are however two difficulties in providing feedback: capability and willingness [7]. Pseudo-relevance feedback (PRF) [7] avoids the challenges of RFB by assuming that the first documents of an initial search result are relevant without user X  X  interaction. Evaluation results have been somewhat mixed while there is a dominating belief in the IR community in the potentia l of PRF. Long documents and non-relevant documents however introduce much noise in the PRF process causing query drift. To counteract this, one may use query-biased summaries [4] [10]. Lam-Adesina &amp; Jones [4] em-ployed query-biased document summarization on the initial result and extracted the expansion keys from the summaries. Their re-sults show improvement in re trieval performance (MAP) using document summaries for term sel ection of up to 15% (to 0.275) compared to the baseline search w ithout feedback (0.24). Further, the use of document summary expansion performed up to 11% better than using standard whol e document term selection (from MAP 0.244 to 0.274). Best results using query-biased summaries were better than those for sta ndard summaries, but overall there was little difference between them. Retrieval improvement was also discovered not to be depende nt on the relevance of feedback documents. The study was based on using the Top-5 documents (summaries) as feedback and binary relevance. In the present paper, we take anot her look at user behavior and IR evaluation. The novel features are based on graded relevance assessments in feedback and ev aluation [3], and simulation of user behavior [3]. Binary relevance cannot reflect the possibility that documents may be relevant to different degrees [9]. Highly relevant documents may be more effective in RFB due to the richer relevant vocabulary they provide. While [3] employed both simulation and graded assessments , their work was based on full-document feedback. We shall employ query-biased document summaries. While [4] employed both query-biased and query-independent summaries, their expe riment was base d on PRF alone and binary relevance in evalua tion. Highly relevant documents are effective in RFB [3] and user s can readily recognize them in search results [12]. Apparently relevant query-biased summaries are also good indicators of document relevance [11]. Thus users could provide effective feedback and summaries would be effec-tive sources of search keys. We base our experiment s on user simulation (like [3]) rather than tests with real users. This has several advantages, including cost-effectiveness and rapid testing w ithout learning effects. The in-formativeness and realism of user simulation can be enhanced by explicitly modeling those aspects of users and RFB that pertain to RFB effectiveness. We shall employ several RFB scenarios (as in [3]) to evaluate the effectiveness of a range of behaviors. We will study, whether some amount of user effort in providing RFB would be justified based on improved results over initial query results or PRF results, whic h do not require additional user effort. The user X  X  evaluation dile mma in RFB is that, in addition to the feedback effort, the documen ts seen in the feedback process need to be frozen to their ra nks and only the unseen documents may be re-ranked for presentation. PRF allows re-ranking of the entire collection before presentation of any results to the user. We will show that PRF provides a hard challenge to RFB and that RFB is most promising when the us er searches for highly relevant documents only and provides mixed quality RFB early, without excessive browsing. We utilize the TREC 7-8 corpus w ith 41 topics for which graded relevance assessments are available [9]. The search engine is Le-mur. We will have initial query re sults and PRF results as base-lines to our simulated RFB expe riments. To render our simula-tions empirically relevant, we focus on user X  X  browsing depth shorter than or equal to 20 docum ents and the number of feedback documents less than or equal to 10. More is unlikely to happen. Our overall research question concer ns the interaction of RFB and query-biased sentence extraction in query reformulation effec-tiveness. More specifically:  X  What is the effect of the quality of feedback on effective- X  What is the effect of the quan tity of user feedback on effec- X  What are the effects of extraction and QE parameters? Here We used the reassessed TREC 7-8 test collection including 41 topics [9]. The document data base contains 528155 documents indexed under the retrieval system Lemur . The index was con-structed by lemmatizing documen t words. The relevance assess-ments were done on a four-point s cale: (0) irrelevant, (1) margin-ally relevant, (2) fairly releva nt, and (3) highly relevant docu-ment. In the recall base there are on average 29 marginally rele-vant, 20 fairly relevant, and 10 highly relevant documents for each topic. Expansion sentences and expansi on keys were extracted from the feedback documents using the RATF weighting scheme [6]. The scheme computes relative average term frequency values for key words as follows: RATF(k) = (cf k / df k ) * 10 3 / (ln(df k + SP)) p The scheme gives high values fo r the keys whose average term frequency (i.e., cf/df ) is high and df low. The scaling parameter SP is used to down weight rare words. For SP and p we used the values of SP = 3000 and p = 3. These values are based on previ-ous work using different topic sets but a similar database. When scoring sentences, if a non-stop query word did not match any sentence word, an n-gram type of approximate string match-ing with a threshold was attempted [2]. Initial queries were constructe d by applying stopping and lemma-tization on long (T+D) topic texts to construct bag-of-word que-ries. Feedback queries were constructed by appending the feed-back keys to the initial query as a second bag-of-words. The study design consists of the following major variables:  X  RFB document relevance threshold (R): 0, 1, 2, 3  X  Max number of documents browsed (B): 5, 10, 20  X  Max number of RFB documen ts (F): 1, 3, 5, 10  X  Max number of sentences extracted per doc (SD): 2, 4, 6  X  Max number of total extracted sentences (ST): 10, 15  X  Max number of QE keys (E): 5, 10, 15, 20, 30, 40. These variables yield some 1700 combinations or cells in the search space, each containing a run of 41 topics and associated evaluation results. Clearly, an e xhaustive search would be very laborious. We have aimed to iden tify in preliminary experiments the best or empirically reasonable ranges for each variable. Figure 1 illustrates the overall experimental protocol. TREC top-ics are first turned to initial Lemur queries and executed, followed by feedback document selection. This is based on the simulated user X  X  relevance requirement (R ), amount of feedback (F) and max browsing depth (B). Document relevance data come from the recall base in our simulation. Th e basis of relevance assessment (seen summary vs. full document) was left open  X  see [11] for potential effects of this decision. In feedback query construction, the feedback documents for each query are split into sentences, and the sentences are scored on the basis of the query and the word RATF scores. Word to word matches are facilitated by lemmatization and, in the case of Out-of-Vocabulary words (OOVs), by n-gram string matching. The sentences are ranked and the SD best ones are extracted for each document. After processing a ll feedback documents, the ST over-all best sentences are identified for expansion key extraction. For each query X  X  set of feedback sentences, their non-query, non-stop words are ranked by their RATF scores and the E overall best keys are identified as expansion keys for the query and added to the initial query. The new query is executed and both the original and feedback query results go to evaluation. We use standard evaluation metrics available in the TREC-eval package and report evaluation results for P@10 documents, and mean average precision MAP. We employ three RFB and evalua-tion levels, where liberal accepts all at least marginal documents as relevant, fair accepts all at least fairly relevant as relevant, and on Friedman X  X  test between select ed RFB runs and their baselines. Table 1 reports the baseline query performances for the T+D que-ries at the three evaluation levels. For the PRF queries, perform-ance is shown for three feedback document counts (B = 5, 10, 20) and three QE levels (10, 20 and 30 keys). Bold face (for MAP) and underlining (for P@10) are used to indicate the best perform-ance in each browsing lot. Gray background indicates the best overall performance for each query type. Table 1. Baseline performance fo r evaluation levels: initial queries, and PRF queries by number of feedback documents and number of expansion keys extracted from summaries The greatest PRF improvements in MAP for T+D queries are from 3.2 % units (liberal) to 0.4% units (strict). The greatest PRF improvements in P@10 are from 3.2% units (fair) to 1.8% units (strict). Tighter evaluation tends to weaken PRF effectiveness, and shorter browsing with larger QE to improve it. Tables 2-3 present the results for RFB expanded T+D queries under liberal and strict feedback. Bolding, underlining and back-ground shading are used as above. Note that we report P@10 even for browsing lengths B  X  10 despite freezing, because the effec-tive browsing lengths (and thus effective freezing) can be less than 10, when F is less than 10. When liberal RFB is used on T+D queries, the best performance is scattered (Table 2). Just gi ving the first satisfactory document from Top-5 as feedback puts one however within 0.4% units (MAP), or 0.8% units (P@10) from the best performance across all evaluation levels  X  marginally less considering user effort in browsing beyond Top-5. Comparing to best PRF, user effort im-proves effectiveness 0.4 -2.5% units in MAP, and -1.0 to +4.4 % units in P@10, depending on evaluation level and user effort. Table 2. Effectiveness of simulated RFB runs for evaluation levels by browsing length B, number of feedback documents F and number of expansion keys E extracted from max 10 over-all best sentences. Liberal RFB -selected results When strict RFB is used, the best performance is nearly always obtained by pointing just one hi ghly relevant document in Top-20 (Table 3). However, by identif ying the one highly relevant docu-ment in Top-5, if it exists, one is within 0.3-0.8 % units in MAP, or 0.7-2.2 % units in P@10, from the best performance. Compar-ing to PRF, the best user effort improves effectiveness 0.5 -3.5 % units in MAP, and -1.0 to +3.2 % units in P@10, depending on the evaluation level, hardly worth the minor effort. Table 4 compares simulated RFB T+D queries to the two T+D query baselines. As representativ es of simulated RFB T+D que-ries we have chosen those perfo rming best with the least user effort, i.e. little browsing and little feedback. We note that, over-all, simulated RFB improves ove r both baselines. In comparison to initial T+D queries, the RFB improvements have no clear ten-dency in MAP or P@10 when evaluation becomes tighter. In comparison to PRF queries, RFB queries improve MAP -0.2 to 3.0 % units, and P@10 0.2 to 3.2 % units. When evaluation be-comes tighter, the difference in MAP tends to grow while the difference inP@10 tends to diminish. Table 3. Effectiveness of simulated RFB runs for evaluation levels by browsing length B, number of feedback documents F and number of expansion keys E extracted from max 10 over-all best sentences. Strict RFB -selected results Table 4. T+D queries -simulated RFB difference to initial queries and PRF queries in MAP and P@10 at three evalua-tion levels. For each e valuation level, the 1 st data column is the difference to initial queries and the 2 nd to PRF queries Interactive RFB performance is uninteresting unless clearly better than PRF performance. Taking at least 2 % units X  difference as the criterion, Table 4 shows in grey background the four interest-ing cases. When MAP is the metric, the greatest benefits of RFB turn out when the evaluation criterion is strict. By Friedman X  X  (p&lt;0.5%), and liberal RFB is almost so (p=6.2%). Regarding P@10, RFB appears most beneficial when evaluation is liberal but not significantly so (Friedman X  X  test, p&gt;10%). In conclusion, RFB systemati cally improved performance over both baselines. RFB is most effective when just one high-quality feedback document, or a few of mixed quality are indicated in the very top ranks of the initial result, and evaluation is by strict rele-vance. MAP shows favorable results for RFB if evaluation is by strict relevance. P@10 is impr oved when RFB and evaluation are liberal (i.e. more marginal documents are allowed in the top ranks). However, RFB requires user feedback effort, and long queries for best performance, whereas PRF is fully automatic and not far behind in average performance. RFB may remain little used in practice unless it becomes clearly easier to employ and more effective compared to its alternatives. This research was supported by the Academy of Finland grants #120996 and #124131. E. Airio, H. Kes kustalo and T. Talvensaari contributed to test system realization. [1] Efthimiadis, E. N. 1996. Quer y expansion. In Annual Re-[2] J X rvelin, A. &amp; J X rvelin, A. &amp; J X rvelin, K. 2007. s-grams: [3] Keskustalo, H., J X rvelin, K., and Pirkola, A. 2008. Evaluat-[4] Lam-Adesina, A. M. and Jone s, G. J. F. 2001. Applying [5] Marchionini, G., Dwiggens, S., Katz, A., and Lin, X. 1993. [6] Pirkola, A., Lepp X nen, E., and J X rvelin, K. 2002. The RATF [7] Ruthven, I. and Lalmas, M. 2003. A survey on the use of [8] Sihvonen, A. and Vakkari, P. 2004. Subject knowledge im-[9] Sormunen, E. 2002. Liberal Relevance Criteria of TREC -[10] Tombros, A., and Sanderson, M. 1998. Advantages of query [11] Turpin, A. &amp; al. 2009. Including Summaries in System [12] Vakkari, P. and Sormunen, E. 2004. The influence of rele-
