 Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct trans-lations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level eval-uation (Blatz et al., 2003). Later approaches to im-prove sentence-level evaluation performance can be summarized as falling into four types:  X  Metrics based on common loose sequences of  X  Metrics based on syntactic similarities such as  X  Metrics based on word alignment between MT  X  Combination of metrics based on machine Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metrics. We explore both approaches. Much work has been done on computing MT scores based on the pair of MT output/reference, and we aim to investigate whether some other information could be used in the MT evaluation, such as source sen-tences. We propose two types of source-sentence related features as well as a feature based on part of speech. The three new types of feature can be sum-marized as follows:  X  Source-sentence constrained n-gram precision.  X  Source-sentence reordering agreement. With  X  Discriminative unigram precision. We divide Along the direction of feature combination, since indirect weight training using SVMs, based on re-ducing classification error, cannot always yield good performance, we train the weights by directly opti-mizing the evaluation performance, i.e., maximizing the correlation with the human judgment. This type of direct optimization is known as Minimum Error Rate Training (Och, 2003) in the MT community, and is an essential component in building the state-of-art MT systems. It would seem logical to apply similar methods to MT evaluation. What is more, Maximum Correlation Training (MCT) enables us to train the weights based on human fluency judg-ments and adequacy judgments respectively, and thus makes it possible to make a fluency-oriented or adequacy-oriented metric. It surpasses previous MT metrics X  approach, where a a single metric evaluates both fluency and adequacy. The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based metrics and introduces our three extensions to them; Section 3 introduces MCT for MT evaluation; Section 4 describes the experi-mental results, and Section 5 gives our conclusion. Since our source-sentence constrained n-gram preci-sion and discriminative unigram precision are both derived from the normal n-gram precision, it is worth describing the original n-gram precision met-ric, BLEU (Papineni et al., 2002). For every MT hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. The formula for computing BLEU is shown below: where C denotes the set of MT hypotheses. Count clip ( ngram ) denotes the clipped number of n-grams in the candidates which also appear in the references. BP in the above formula denotes the brevity penalty, which is set to 1 if the accumulated length of the MT outputs is longer than the arith-metic mean of the accumulated length of the refer-ences, and otherwise is set to the ratio of the two. For sentence-level evaluation with BLEU, we com-pute the score based on each pair of MT hypothe-sis/reference. Later approaches, as described in Sec-tion 1, use different ways to manipulate the morpho-logical similarity between the MT hypothesis and its references. Most of them, except NIST, consider the words in MT hypothesis as the same, i.e., as long as the words in MT hypothesis appear in the references, putes the n-grams weights as the logarithm of the ra-tio of the n-gram frequency and its one word lower n-gram frequency. From our experiments, NIST is not generally better than BLEU, and the reason, we conjecture, is that it differentiates the n-grams too much and the frequency estimated upon the evalua-tion corpus is not always reliable. In this section we will describe two other strategies for differentiating the n-grams, one of which uses the alignments with the source sentence as a further constraint, while the other differentiates the n-gram precisions according to POS. 2.1 Source-sentence Constrained N-gram The quality of an MT sentence should be indepen-dent of the source sentence given the reference trans-lation, but considering that current metrics are all based on shallow morphological similarity of the MT outputs and the reference, without really under-standing the meaning in both sides, the source sen-tences could have some useful information in dif-ferentiating the MT outputs. Consider the Chinese-English translation example below: Source: wo bu neng zhe me zuo Hypothesis: I must hardly not do this Reference: I must not do this It is clear that the word not in the MT output can-not co-exist with the word hardly while maintain-ing the meaning of the source sentence. None of the metrics mentioned above can prevent not from being counted in the evaluation, due to the simple reason that they only compute shallow morphologi-cal similarity. Then how could the source sentence help in the example? If we reveal the alignment of the source sentence with both the reference and the MT output, the Chinese word bu neng would be aligned to must not in the reference and must hardly in the MT output respectively, leaving the word not in the MT output not aligned to any word in the source sentence. Therefore, if we can somehow find the alignments between the source sentence and the reference/MT output, we could be smarter in se-lecting the overlapping words to be counted in the for all n-grams w do Figure 1: Algorithm for Computing Source-sentence Constrained n-gram Precision metric: only select the words which are aligned to the same source words. Now the question comes to how to find the alignment of source sentence and MT hypothesis/references, since the evaluation data set usually does not contain alignment information. to-one alignments between source sentences and the could generate many-to-one alignments either from source sentence to the MT hypothesis, in which case every word in MT hypothesis is aligned to a set of (or none) words in the source sentence, or from the reverse direction, in which case every word in MT hypothesis is aligned to exactly one word (or none) word in the source sentence. In either case, using M T align sitions of the words in the source sentences which are aligned to a word in the MT hypothesis and a word in the reference respectively, the algorithm for computing source-sentence constrained n-gram pre-cision of length n is described in Figure 1.
Since source-sentence constrained n-gram preci-sion (SSCN) is a precision-based metric, the vari-able length penalty is used to avoid assigning a short MT hypothesis a high score, and is computed in the same way as BLEU. Note that in the algo-rithm for computing the precision of n-grams longer than one word, not all words in the n-grams should satisfy the source-sentence constraint. The reason is that the high order n-grams are already very sparse in the sentence-level evaluation. To differentiate the SSCNs based on the source-to-MT/Ref (many-to-one) alignments and the MT/Ref-to-source (many-to-one) alignments, we use SSCN1 and SSCN2 to denote them respectively. Naturally, we could com-bine the constraint in SSCN1 and SSCN2 by either taking their union (the combined constrained is sat-isfied if either one is satisfied) or intersecting them (the combined constrained is satisfied if both con-straints are satisfied). We use SSCN u and SSCN i to denote the SSCN based on unioned constraints and intersected constraints respectively. We could also apply the stochastic word mapping proposed in SIA (Liu and Gildea, 2006) to replace the hard word matching in Figure 1, and the corresponding met-rics are denoted as pSSCN1, pSSCN2, pSSCN u, pSSCN i, with the suffixed number denoting differ-ent constraints. 2.2 Metrics Based on Source Word Reordering Most previous MT metrics concentrate on the co-occurrence of the MT hypothesis words in the ref-erences. Our metrics based on source sentence re-orderings, on the contrary, do not take words identi-ties into account, but rather compute how similarly the source words are reordered in the MT output and the references. For simplicity, we only consider the pairwise reordering similarity. That is, for the source word pair w MT hypothesis and a reference are in the same order, we call it a consistent word pair. Our pairwise re-ordering similarity (PRS) metric computes the frac-tion of the consistent word pairs in the source sen-tence. Figure 2 gives the formal description of PRS. SrcM T i and SrcRef k,i denote the aligned position of source word w reference respectively, and N denotes the length of the source sentence.

Another criterion for evaluating the reordering of the source sentence in the MT hypothesis is how well it maintains the original word order in the for all word pair w such that i &lt; j do
Figure 2: Compute Pairwise Reordering Similarity for all word pair w such that i &lt; j do Figure 3: Compute Source Sentence Monotonic Re-ordering Ratio source sentence. We know that most of the time, the alignment of the source sentence and the MT hy-pothesis is monotonic. This idea leads to the metric of monotonic pairwise ratio (MPR), which computes the fraction of the source word pairs whose aligned positions in the MT hypothesis are of the same order. It is described in Figure 3. 2.3 Discriminative Unigram Precision Based The Discriminative Unigram Precision Based on POS (DUPP) decomposes the normal unigram pre-cision into many sub-precisions according to their POS. The algorithm is described in Figure 4.
These sub-precisions by themselves carry the same information as standard unigram precision, but they provide us the opportunity to make a better combined metric than the normal unigram precision with MCT, which will be introduced in next section. for all unigram s in the MT hypothesis do Figure 4: Compute DUPP for N-gram with length n Such division could in theory be generalized to work with higher order n-grams, but doing so would make the n-grams in each POS set much more sparse. The preprocessing step for the metric is tagging both the MT hypothesis and the references with POS. It might elicit some worries about the robustness of the POS tagger on the noise-containing MT hypothesis. This should not be a problem for two reasons. First, compared with other preprocessing steps like pars-ing, POS tagging is easier and has higher accuracy. Second, because the counts for each POS are accu-mulated, the correctness of a single word X  X  POS will not affect the result very much. Maximum Correlation Training (MCT) is an in-stance of the general approach of directly optimiz-ing the objective function by which a model will ultimately be evaluated. In our case, the model is the linear combination of the component metrics, the parameters are the weights for each component met-ric, and the objective function is the Pearson X  X  corre-lation of the combined metric and the human judg-ments. The reason to use the linear combination of the metrics is that the component metrics are usu-ally of the same or similar order of magnitude, and it makes the optimization problem easy to solve. Us-ing w to denote the weights, and m to denote the component metrics, the combined metric x is com-puted as:
Using h and combined metric for a sentence respectively, and N denote the number of sentences in the evaluation set, the objective function is then computed as: Now our task is to find the weights for each compo-nent metric so that the correlation of the combined metric with the human judgment is maximized. It can be formulated as: The function P earson ( X ( w ) , H ) is differentiable with respect to the vector w , and we compute this derivative analytically and perform gradient ascent. Our objective function not always convex (one can easily create a non-convex function by setting the human judgments and individual metrics to some particular value). Thus there is no guarantee that, starting from a random w , we will get the glob-ally optimal w using optimization techniques such as gradient ascent. The easiest way to avoid ending up with a bad local optimum to run gradient ascent by starting from different random points. In our ex-periments, the difference in each run is very small, i.e., by starting from different random initial values of w , we end up with, not the same, but very similar values for Pearson X  X  correlation. Experiments were conducted to evaluate the perfor-mance of the new metrics proposed in this paper, as well as the MCT combination framework. The data for the experiments are from the MT evalua-tion workshop at ACL05. There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), each of which contains 919 English sentences translated from the same set of Chinese sentences. There are four references (E01, E02, E03, E04) and two sets of human scores for each MT hypothesis. Each hu-man score set contains a fluency and an adequacy score, both of which range from 1 to 5. We create a set of overall human scores by averaging the human fluency and adequacy scores. For evaluating the au-tomatic metrics, we compute the Pearson X  X  correla-tion of the automatic scores and the averaged human scores (over the two sets of available human scores), for overall score, fluency, and adequacy. The align-ment between the source sentences and the MT hy-pothesis/references is computed by GIZA++, which is trained on the combined corpus of the evalua-tion data and a parallel corpus of Chinese-English newswire text. The parallel newswire corpus con-tains around 75,000 sentence pairs, 2,600,000 En-glish words and 2,200,000 Chinese words. The stochastic word mapping is trained on a French-English parallel corpus containing 700,000 sentence pairs, and, following Liu and Gildea (2005), we only keep the top 100 most similar words for each En-glish word. 4.1 Performance of the Individual Metrics To evaluate our source-sentence based metrics, they are used to evaluate the 7 MT outputs, with the 4 sets of human references. The sentence-level Pearson X  X  correlation with human judgment is computed for each MT output, and the averaged results are shown in Table 1. As a comparison, we also show the re-sults of BLEU, NIST, METEOR, ROUGE, WER, and HWCM. For METEOR and ROUGE, WORD-NET and PORTER-STEMMER are enabled, and for SIA, the decay factor is set to 0.6. The number in brackets, for BLEU, shows the n-gram length it counts up to, and for SSCN, shows the length of the n-gram it uses. In the table, the top 3 results in each column are marked bold and the best result is also underlined. The results show that the SSCN2 met-rics are better than the SSCN1 metrics in adequacy and overall score. This is understandable since what SSCN metrics need is which words in the source sentence are aligned to an n-gram in the MT hy-pothesis/references. This is directly modeled in the alignment used in SSCN2. Though we could also get such information from the reverse alignment, as in SSCN1, it is rather an indirect way and could con-tain more noise. It is interesting that SSCN1 gets better fluency evaluation results than SSCN2. The SSCN metrics with the unioned constraint, SSCN u, by combining the strength of SSCN1 and SSCN2, get even better results in all three aspects. We can see that SSCN metrics, even without stochastic word mapping, get significantly better results than their relatives, BLEU, which indicates the source sen-tence constraints do make a difference. SSCN2 and SSCN u are also competitive to the state-of-art MT metrics such as METEOR and SIA. The best SSCN metric, pSSCN u(2), achieves the best performance among all the testing metrics in overall and ade-quacy, and the second best performance in fluency, which is just a little bit worse than the best fluency metric SIA.

The two reordering based metrics, PRS and MPR, are not as good as the other testing metrics, in terms of the individual performance. It should not be sur-prising since they are totally different kind of met-rics, which do not count the overlapping n-grams, but the consistent/monotonic word pair reorderings. As long as they capture some property of the MT hypothesis, they might be able to boost the per-formance of the combined metric under the MCT framework. 4.2 Performance of the Combined Metrics To test how well MCT works, the following scheme is used: each set of MT outputs is evaluated by MCT, which is trained on the other 6 sets of MT outputs and their corresponding human judgment; the aver-aged correlation of the 7 sets of MT outputs with the human judgment is taken as the final result. 4.2.1 Discriminative Unigram Precision based
We first use MCT to combine the discriminative unigram precisions. To reduce the sparseness of the unigrams of each POS, we do not use the original POS set, but use a generalized one by combining all POS tags with the same first letter (e.g., the dif-ferent verb forms such as VBN , VBD , and VBZ are transformed to V ). The unified POS set contains 23 POS tags. To give a fair comparison of DUPP with BLEU, the length penalty is also added into it as a component. Results are shown in Table 2. DUPP f, DUPP a and DUPP o denote DUPP trained on hu-man fluency, adequacy and overall judgment respec-tively. This shows that DUPP achieves obvious im-provement over BLEU, with only the unigrams and length penalty, and DUPP f/ a/ o gets the best re-sult in fluency/adequacy/overall evaluation, showing that MCT is able to make a fluency-or adequacy-oriented metric. 4.2.2 Putting It All Together
The most interesting question in this paper is, with all these metrics, how well we can do in the MT evaluation. To answer the question, we put all the metrics described into the MCT framework and use the combined metric to evaluate the 7 MT outputs. Note that to speed up the training process, we do not directly use 24 DUPP components, instead, we use the 3 combined DUPP metrics. With the met-rics shown in Table 1, we then have in total 31 met-rics. Table 2 shows the results of the final combined metric. We can see that MCT trained on fluency, adequacy and overall human judgment get the best results among all the testing metrics in fluency, ade-quacy and overall evaluation respectively. We did a t-test with Fisher X  X  z transform for the combined re-sults and the individual results to see how significant the difference is. The combined results in adequacy and overall are significantly better at 99.5% confi-dence than the best results of the individual metrics (pSSCN u(2)), and the combined result in fluency is significantly better at 96.9% confidence than the best individual metric (SIA). We also give the upper bound for each evaluation aspect by training MCT on the testing MT outputs, e.g., we train MCT on E09 and then use it to evaluate E09. The upper-bound is the best we can do with the MCT based on linear combination. Another linear framework, bine the testing metrics except DUPP. Since DUPP is based on MCT, to make a neat comparison, we rule out DUPP in the experiments with CSVM. The testing scheme is the same as MCT, except that we only use 3 references for each MT hypothesis, and the positive samples for training CSVM are com-puted as the scores of one of the 4 references based on the other 3 references. The slack parameter of CSVM is chosen so as to maximize the classifica-tion accuracy of a heldout set of 800 negative and 800 positive samples, which are randomly selected from the training set. The results are shown in Ta-ble 2. We can see that MCT, with the same number of reference sentences, is better than CSVM. Note that the resources required by MCT and CSVM are different. MCT uses human judgments to adjust the weights, while CSVM needs extra human references to produce positive training samples.

To have a rough idea of how the component met-rics contribute to the final performance of MCT, we incrementally add metrics into the MCT in descend-ing order of their overall evaluation performance, with the results shown in Figure 5. We can see that the performance improves as the number of metrics increases, in a rough sense. The major improvement happens in the 3rd, 4th, 9th, 14th, and 30th metrics, which are METEOR, SIA, DUPP a, pSSCN1(1), and PRS. It is interesting to note that these are not the metrics with the highest individual performance. Another interesting observation is that there are no two metrics belonging to the same series in the most beneficial metrics, indicating that to get better com-bined metrics, individual metrics showing different sentence properties are preferred. This paper first describes two types of new ap-proaches to MT evaluation, which includes making Figure 5: Performance as a Function of the Number of Interpolated Metrics use of source sentences, and discriminating unigram precisions based on POS. Among all the testing met-rics including BLEU, NIST, METEOR, ROUGE, and SIA, our new metric, pSSCN u(2), based on source-sentence constrained bigrams, achieves the best adequacy and overall evaluation results, and the second best result in fluency evaluation. We fur-ther improve the performance by combining the in-dividual metrics under the MCT framework, which is shown to be better than a classification based framework such as SVM. By examining the contri-bution of each component metric, we find that met-rics showing different properties of a sentence are more likely to make a good combined metric.
 Acknowledgments This work was supported by NSF grants IIS-0546554, IIS-0428020, and IIS-0325646.

