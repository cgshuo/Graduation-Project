 The focus of my work is the expression of emotion in human speech. As normally-functioning people, we are each capable of vocally expressing and aurally recogniz-ing the emotions of others. Ho w often have you been put off by the  X tone in someone' s voice X  or tickled others with the humorous telling of a good story? Though we as everyday people are intimately familiar with emotion, we as scientists do not actually kno w precisely how it is that emotion is con veyed in human speech. This is of spe-cial concern to us as engineers of natural language tech-nology; in particular , spok en dialogue systems. Spok en dialogue systems enable users to interact with computer systems via natural dialogue, as the y would with human agents. In my vie w, a current decienc y of state-of-the-art spok en dialogue systems is that the emotional state of the user is not modeled. This results in non-human-lik e and even inappropriate beha vior on the part of the spok en dialogue system.

There are two central questions I would lik e to at lest partially answer with my dissertation research: (1) Ho w is emotion communicated in speech? and (2) Does emo-tion modeling impro ve spok en dialogue applications? In an attempt to answer the rst question, I have adopted the research paradigm of extracting features that charac-terize emotional speech and applying machine learning algorithms to determine the prediction accurac y of each feature. With regard to the second research question, I plan to implement an emotion modeler  X  one that detects and responds to uncertainty and frustration  X  into an In-telligent Tutoring System. This section describes my current research on emotion classication in three domains and forms the foundation of my dissertation. For each domain, I have adopted an experimental design wherein each utterance in a corpus is annotated with one or more emotion labels, features are extracted from these utterances, and machine learn-ing experiments are run to determine emotion prediction accurac y. 2.1 EPSaT The publicly-a vailable Emotional Prosody Speech and Transcription corpus 1 (EPSaT) comprises recordings of professional actors reading short (four syllables each) dates and numbers ( e.g. , `tw o-thousand-four') with dif-ferent emotional states. I chose a subset of 44 utterances from 4 speak ers (2 male, 2 female) from this corpus and conducted a web-based surv ey to subjecti vely label each utterance for each of 10 emotions, divided evenly for va-lence. These emotions included the positi ve emotion cat-egories: condent , encour aging , friendly , happy , inter -ested ; and the negati ve emotion cate gories: angry , anx-ious , bor ed , frustr ated , sad .

Several features were extracted from each utterance in this corpus, each one designed to capture emotional con-tent. Global acoustic-prosodic information  X  e.g. , speak-ing rate and minimum, maximum, and mean pitch and in-tensity  X  has been well kno wn since the 1960s and 1970s to con vey emotion to some extent ( e.g. , (Da vitz, 1964; Scherer et al., 1972)). In addition to these features, I also included linguistically meaningful prosodic information in the form of ToBI labels (Beckman et al., 2005), as well as the spectral tilt of the vowel in each utterance bearing the nuclear pitch accent.

In order to evaluate the predicti ve power of each fea-ture extracted from the EPSaT utterances, I ran machine learning experiments using RIPPER , a rule-learning al-gorithm. The EPSaT corpus was divided into training (90%) and testing (10%) sets. A binary classication scheme was adopted based on the observ ed ranking dis-trib utions from the perception surv ey:  X  not at all  X  was considered to be the absence of emotion x ; all other ranks was recorded as the presence of emotion x . Performance accurac y varied with respect to emotion, but on average I observ ed 75% prediction accurac y for any given emotion, representing an average 22% impro vement over chance performance. The most predicti ve included the global acoustic-prosodic features, but interesting novel ndings emer ged as well; most notably , signicant correlation was observ ed between negati ve emotions and pitch con-tours ending in a plateau boundary tone, whereas positi ve emotions correlated with the standard declarati ve phrasal ending (in ToBI, these would be labeled as /H-L%/ and /L-L%/, respecti vely). Further discussion of such nd-ings can be found in (Liscombe et al., 2003). 2.2 HMIHY  X Ho w May I Help You SM  X  (HMIHY) is a natural lan-guage human-computer spok en dialogue system devel-oped at AT&amp;T Research Labs. The system enables AT&amp;T customers to interact verbally with an automated agent over the phone. Callers can ask for their account bal-ance, help with AT&amp;T rates and calling plans, explana-tions of certain bill char ges, or identication of num-bers. Speech data collected from the deplo yed system has been assembled into a corpus of human-computer dialogues. The HMIHY corpus contains 5,690 com-plete human-computer dialogues that collecti vely con-tain 20,013 caller turns. Each caller turn in the corpus was annotated with one of seven emotional labels: posi-tive/neutr al, some what frustr ated, very frustr ated, some-what angry , very angry , some what other negative 2 , very other negative . Ho we ver, the distrib ution of the labels was so skewed (73.1% were labeled as positive/neutr al ) that the emotions were collapsed to negative and non-negative .

In addition to the set of automatic acoustic-prosodic features found to be useful for emotional classication of the EPSaT corpus, the features I examined in the HMIHY corpus were designed to exploit the discourse information available in the domain of spontaneous human-machine con versation. Transcripti ve features  X  lexical items, lled pauses, and non-speech human noises  X  we recorded as features, as too were the dialogue acts of each caller turn. In addition, I included conte xtual features that were de-signed to track the history of the pre viously mentioned features over the course of the dialogue. Specically , conte xtual information included the rate of change of the acoustic-prosodic features of the pre vious two turns plus the transcripti ve and pragmatic features of the pre vious two turns as well.

The corpus was divided into training (75%) and testing (25%) sets. The machine learning algorithm emplo yed was B OOS T EXTER , an algorithm that forms a hypothesis by combining the results of several iterations of weak-learner decisions. Classication accurac y using the auto-matic acoustic-prosodic features was recorded to be ap-proximately 75%. The majority class baseline (al ways guessing non-ne gative ) was 73%. By adding the other feature-sets one by one, prediction accurac y was itera-tively impro ved, as described more fully in (Liscombe et al., 2005b). Using all the features combined  X  acoustic-prosodic, lexical, pragmatic, and conte xtual  X  the result-ing classication accurac y was 79%, a healthy 8% im-pro vement over baseline performance and a 5% impro ve-ment over the automatic acoustic-prosodic features alone. 2.3 ITSpok e This section describes more recent research I have been conducting with the Uni versity of Pittsb urgh' s Intelli-gent Tutoring Spok en Dialogue System (ITSpok e) (Lit-man and Silliman, 2004). The goal of this research is to wed spok en language technology with instructional tech-nology in order to promote learning gains by enhanc-ing communication richness. ITSpok e is built upon the Why2-Atlas tutoring back-end (VanLehn et al., 2002), a text-based Intelligent Tutoring System designed to tutor students in the domain of qualitati ve physics using natural language interaction. Several corpora have been recorded for development of ITSpok e, though most of the work presented here involv es tutorial data between a student and human tutor . To date, we have labeled the human-human corpus for anger , frustration, and uncertainty .
As this work is an extension of pre vious work, I chose to extract most of the same features I had extracted from the EPSaT and HMIHY corpora. Specically , I extracted the same set of automatic acoustic-prosodic features, as well as conte xtual features measuring the rate of change of acoustic-prosodic features of past student turns. A new feature set was introduced as well, which I refer to as the breath-group feature set, and which is an auto-matic method for segmenting utterances into intonation-ally meaningful units by identifying pauses using back-ground noise estimation. The breath group feature set comprises the number of breath-groups in each turn, the pause time, and global acoustic-prosodic features calcu-lated for the rst, last, and longest breath-group in each student turn.

I used the W EKA machine learning softw are package to classify whether a student answer was percei ved to be uncertain , certain , or neutr al 3 in the ITSpok e human-human corpus. As a predictor , C4.5, a decision-tree learner , was boosted with AdaBoost, a learning strate gy similar to the one presented in Section 2.2. The data were randomly split into a training set (90%) and a test-ing set (10%). The automatic acoustic-prosodic features performed at 75% accurac y, a relati ve impro vement of 13% over the baseline performance of always guessing neutr al . By adding additional feature-sets  X  conte xtual and breath-group information  X  I observ ed an impro ved prediction accurac y of 77%. Thus indicating that breath-group features are useful. I refer the reader to (Liscombe et al., 2005a) for in-depth implications and further analy-sis of these results. In the immediate future, I will extract features pre viously mentioned in Section 2.2 as well as the exploratory features I will discuss in the follo wing section. In this section I describe research I have begun to con-duct and plan to complete in the coming year , as agreed-upon in February , 2006 by my dissertation committee. I will explore features that are not well studied in emotion classication research, primarily pitch contour and voice quality approximation. Furthermore, I will outline how I plan to implement and evaluate an emotion detection and response module into ITSpok e. 3.1 Pitch Contour Clustering The global acoustic-prosodic features used in most emo-tion prediction studies capture meaningful prosodic vari-ation, but are not capable of describing the linguisti-cally meaningful intonational beha vior of an utterance. Though phonological labeling methods exist, such as ToBI, annotation of this sort is time-consuming and must be done manually . Instead, I propose an automatic al-gorithm that directly compares pitch contours and then groups them into classes based on abstract form. Specif-ically , I intend to use partition clustering to dene a disjoint set of similar prosodic contour types over our data. I hypothesize that the resultant clusters will be the-oretically meaningful and useful for emotion modeling. The similarity metric used to compare two contours will be edit distance, calculated using dynamic time warping techniques. Essentially , the algorithm nds the best t between two contours by stretching and shrinking each contour as necessary . The score of a comparison is calcu-lated as the sum of the normalized real-v alued distances between mapped points in the contours. 3.2 Voice Quality Voice quality is a term used to describe a perceptual col-oring of the acoustic speech signal and is generally be-lieved to play an important role in the vocal communica-tion of emotion. Ho we ver, it has rarely been used in au-tomatic classication experiments because the exact pa-rameters dening each quality of voice ( e.g. , creak y and breathy) are still lar gely unkno wn. Yet, some researchers belie ve much of what constitutes voice quality can be described using information about glottis excitation pro-duced by the vocal folds, most commonly referred to as the glottal pulse waveform. While there are ways of directly measuring the glottal pulse waveform, such as with an electroglottograph, these techniques are too inva-sive for practical purposes. Therefore, the glottal pulse waveform is usually approximated by inverse ltering of the speech signal. I will deri ve glottal pulse waveforms from the data using an algorithm that automatically iden-ties voiced regions of speech, obtains an estimate of the glottal o w deri vative, and then represents this using the Liljencrants-F ant parametric model. The nal result is a glottal pulse waveform, from which features can be ex-tracted that describe the shape of this waveform, such as the Open and Sk ewing Quotients. 3.3 Implementation The moti vating force behind much of the research I have presented herein is the common assumption in the re-search community that emotion modeling will impro ve spok en dialogue systems. Ho we ver, there is little to no empirical proof testing this claim (See (Pon-Barry et al., In publication) for a notable exception.). For this rea-son, I will implement functionality for detecting and re-sponding to student emotion in ITSpok e (the Intelligent Tutoring System described in Section 2.3) and analyze the effect it has on student beha vior , hopefully sho wing (quantitati vely) that doing so impro ves the system' s ef-fecti veness.

Research has sho wn that frustrated students learn less than non-frustrated students (Le wis and Williams, 1989) and that human tutors respond dif ferently in the face of student uncertainty than the y do when presented with cer -tainty (Forbes-Rile y and Litman, 2005). These ndings indicate that emotion plays an important role in Intelli-gent Tutoring Systems. Though I do not have the ability to alter the discourse-o w of ITSpok e, I will insert acti ve listening prompts on the part of ITSpok e when the sys-tem has detected either frustration or uncertainty . Acti ve listening is a technique that has been sho wn to dif fuse negati ve emotion in general (Klein et al., 2002). I hy-pothesize that dif fusing user frustration and uncertainty will impro ve ITSpok e.
 After collecting data from an emotion-enabled IT-Spok e I will compare evaluation metrics with those of a control study conducted with the original ITSpok e sys-tem. One such metric will be learning gain, the dif fer -ence between student pre-and post-test scores and the standard metric for quantifying the effecti veness of edu-cational devices. Since learning gain is a crude measure of academic achie vement and may overlook beha vioral and cogniti ve impro vements, I will explore other metrics as well, such as: the amount of time tak en for the stu-dent to produce a correct answer , the amount of negati ve emotional states expressed, the quality and correctness of answers, the willingness to continue, and subjecti ve post-tutoring assessments. I see the contrib utions of my dissertation to be the extent to which I have helped to answer the questions I posed at the outset of this paper . 4.1 Ho w is emotion communicated in speech? The experimental design of extracting features from spo-ken utterances and conducting machine learning experi-ments to predict emotion classes identies features im-portant for the vocal communication of emotion. Most of the features I have described here are well established in the research community; statistic measurements of fun-damental frequenc y and ener gy, for example. Ho we ver, I have also described more experimental features as a way of impro ving upon the state-of-the-art in emotion mod-eling. These exploratory features include breath-group segmentation, conte xtual information, pitch contour clus-tering, and voice quality estimation. In addition, explor -ing three domains will allo w me to comparati vely ana-lyze the results, with the ultimate goal of identifying uni-versal qualities of spok en emotions as well as those that may particular to specic domains. The ndings of such a comparati ve analysis will be of practical benet to fu-ture system builders and to those attempting to dene a uni versal model of human emotion alik e. 4.2 Does emotion modeling help? By collecting data of students interacting with an emotion-enabled ITSpok e, I will be able to report quan-titati vely the results of emotion modeling in a spok en di-alogue system. Though this is the central moti vation for most researchers in this eld, there is currently no deni-tive evidence either supporting or refuting this claim. M. E. Beckman, J. Hirschber g, and S. Shattuck-Hufnagel, 2005. Prosodic Typolo gy  X  The Phonolo gy of Intona-tion and Phr asing , chapter 2 The original ToBI sys-tem and the evolution of the ToBI frame work. Oxford, OUP .
 J. R. Da vitz, 1964. The Communication of Emotional
Meaning , chapter 8 Auditory Correlates of Vocal Ex-pression of Emotional Feeling, pages 101 X 112. Ne w York: McGra w-Hill.
 Kate Forbes-Rile y and Diane J. Litman. 2005. Using bigrams to identify relationships between student cer -tainness states and tutor responses in a spok en dialogue corpus. In Proceedings of 6th SIGdial Workshop on Discour se and Dialo gue , , Lisbon, Portugal. J. Klein, Y. Moon, and R. W. Picard. 2002. This com-puter responds to user frustration: Theory , design, and results. Inter acting with Computer s , 14(2):119 X 140, February .
 V. E. Le wis and R. N. Williams. 1989. Mood-congruent vs. mood-state-dependent learning: Implications for a vie w of emotion. D. Kuik en (Ed.), Mood and Mem-ory: Theory , Resear ch, and Applications, Special Is-sue of the Journal of Social Behavior and Personality , 4(2):157 X 171.
 Jackson Liscombe, Jennifer Venditti, and Julia
Hirschber g. 2003. Classifying subject ratings of emotional speech using acoustic features. In Proceedings of Eur ospeec h , Gene va, Switzerland. Jackson Liscombe, Julia Hirschber g, and Jennifer Ven-ditti. 2005a. Detecting certainness in spok en tutorial dialogues. In Proceedings of Inter speec h , Lisbon, Por -tugal.
 Jackson Liscombe, Guiseppe Riccardi, and Dilek
Hakkani-T  X  ur. 2005b . Using conte xt to impro ve emo-tion detection in spok en dialogue systems. In Proceed-ings of Inter speec h , Lisbon, Portugal.
 Diane Litman and Scott Silliman. 2004. Itspok e: An in-telligent tutoring spok en dialogue system. In Proceed-ings of the 4th Meeting of HL T/N AA CL (Companion Proceedings), Boston, MA, May .
 Heather Pon-Barry , Karl Schultz, Elizabeth Owen Bratt,
Brady Clark, and Stanle y Peters. In publication. Re-sponding to student uncertainty in spok en tutorial dia-logue systems. International Journal of Articial In-tellig ence in Education (IJ AIED) .
 K. R. Scherer , J. Koivumaki, and R. Rosenthal. 1972. Minimal cues in the vocal communication of affect:
Judging emotions from content-mask ed speech. Jour -nal of Psyc holinguistic Resear ch , 1:269 X 285. K. VanLehn, P. Jordan, and C. P. Rose. 2002. The archi-tecture of why2-atlas: A coach for qualitati ve physics essay writing. In Proceedings of the Intellig ent Tutor -ing Systems Confer ence , Biarritz, France.
