
A bottleneck to detecting distance and density based out-liers is that a nearest-neighbor search is required for each of the data points, resulting in a quadratic number of pair-wise distance evaluations. In this paper, we propose a new method that uses the relative degree of density with respect to a fixed set of reference points to approximate the degree of density defined in terms of nearest neighbors of a data point. The running time of our algorithm based on this ap-proximation is O ( Rn log n ) where n is the size of dataset and R is the number of reference points. Candidate outliers are ranked based on the outlier score assigned to each data point. Theoretical analysis and empirical studies show that our method is effective, efficient, and highly scalable to very large datasets.
Detecting distance-based outliers in very large datasets has attracted much attention over the past several years in data mining. Compared to traditional outliers studied in statistics [1, 6], the definition of distance-based outliers is distribution-free, more flexible, and more computationally feasible. A bottleneck to the detection of distance-based outliers is that a nearest-neighbor search is required for each of the n data points. Consequently, straightforward imple-mentations such as the Nested-Loop method need to com-pute the distance between each pair of data points, resulting in an O ( n 2 ) running time.

Since the seminal work of Knorr and Ng [8], much effort has been devoted to improving the efficiency of algorithms for detecting distance-based outliers. By using spatial index data structures such as the k-d tree and its variants, the aver-age running time can be reduced to O ( n log n ) with a hid-den constant depending exponentially on the dimensionality of the data. Several heuristics have also been proposed to re-duce the number of required nearest neighborhood search. In [8], a cell-based approach for detecting distance-based outliers was investigated, which is still exponential in the dimensions, but linear in the size of the dataset under the assumption that both of the two negatively-correlated pa-rameters (percentage p and radius D ) of the algorithm are set to their ideal values. In [12], some clusters of data points are eliminated from consideration based on the result of pre-clustering the dataset, a task that is dual to outlier detection. In [2], it is observed that by keeping track of the closest neighbors found so far, the nearest-neighbor search for a specific data point can be stopped if it becomes clear that the data point cannot be one of the pre-specified number of outliers. While the algorithm can indeed prune many distance calculations, the worst-case running time is still O ( n 2 ) . Empirical evidence and theoretical arguments un-der some assumptions in [2] show that the algorithm based on this observation may have a sub-quadratic execution time in practice.

In this paper, we propose a new approach to reduce the number of distance evaluations. The idea is to rank the data points based on their relative degree of density with respect to a fixed set of reference points. For each reference point, we calculate its distance to each of the data points and trans-form the original data space into a one dimensional dataset. Based on the obtained one-dimensional dataset that con-tains the distances from a reference point to each data point, the relative degree of density (w.r.t the reference point) of each data point is calculated. The overall relative degree of density of a data point is defined as the minimum relative degree of density over all the reference points. The execu-tion time of the algorithm is in O ( Rn log n ) where R is the number of reference points and n is the size of the dataset. Theoretical analyses and extensive empirical studies indi-cate that the proposed approach is effective, efficient, and highly scalable in detecting global and local outliers in large datasets.

The paper is organized as follows. In Section 2, we review the basic definitions of distance-based outliers and their extensions in the literature. In Section 3, we describe the proposed reference-based outlier detection method, in-cluding the definition of the reference-based outlier score and its relationship with the traditional distance-based out-lier detection approaches. Also discussed in Section 3 are (1) an algorithm that compute the reference-based outlier score in O ( Rn log n ) time and several implementation de-tails; and (2) a comparison study on the benefit of using the reference-based approach in detecting local outliers. In Section 4, we report empirical performance evaluation of our proposal on synthetic and real-world data. Section 5 is the conclusion.
In the following discussion, let X = { x 1 ,  X  X  X  ,x n } be the dataset and let d (  X  ,  X  ) be a distance metric.
Distance-based outliers are defined by using some de-gree of density relative to the nearest neighbors, or the so called neighborhood density of a data point, under a dis-tance metric [8, 12, 4]. Let x be a data point in X , k be an integer and t be a real number, the neighborhood density of a data point x  X  X can be defined in three ways: Definition 2.1. 0-1 Density D ( x, k, t ) : D ( x, k, t )=0 if there are fewer than k other data points within distance t and D ( x, k, t )=1 otherwise;
The definition considers being an outlier as a binary property such that the obtained density for each data point divides the whole dataset into exactly two groups: inliers and outliers. There is no measure of how much a data point is outlying and the identified outliers are not ranked. Definition 2.2. Max Density D m ( x, k ) : D m ( x, k ) is the reciprocal of the distance to the k -th nearest neighbor. Definition 2.3. Average Density D a ( x, k ) : D a ( x, k ) is the reciprocal of the average distance to the k nearest neigh-bors.

Definitions 2.2 and 2.3 introduce the ranking mechanism based on the distance to the k nearest neighbors of a point. The identified outliers are more meaningful since the infor-mation of the degree of being an outlier has been integrated into the analysis process.

Some local outlier detection methods [3, 7, 14] gener-alize the above concepts further. For example, the well-known local outlier factor (LOF) introduced in [3] measures the degree of being an outlier by taking into consideration the data point X  X  relative density as compared to those of its nearest neighbors. The advantage of LOF is that the local densities of the non-outlier datapoints will have less impact on the ranking of the outliers. The major parameter in LOF is MinPts , the minimum number of the nearest neighbors to consider. This parameter is highly application-dependent and some insight into the structure of the dataset is required in order to set it correctly. What makes the selection of MinPts even harder is the fact that the LOF of a given data point is not monotone in MinPts , as has been observed by the authors [3]. Another related issue with LOF is the exis-tence of duplicated data in a dataset. Roughly speaking, the LOF of duplicated data points is infinity unless the MinPts is larger than the number of duplicated data points. As has been mentioned in [3], this difficulty can be overcome by slightly changing the original definition of LOF to ignore the neighboring data points that are duplicated.

Our approach follows Definition 2.3. Based on the aver-age distance to the k nearest neighbors, each data point is assigned an outlier score indicating the degree of its devia-tion from its close neighbors. Outliers are those with a low neighborhood density but high outlier scores.
We use the relative degree of neighborhood density with respect to a fixed set of reference points to approximate the degree of density defined in the distance-based method. Let X = { x 1 , ..., x n } be a dataset and p be a point (not nec-essarily in X ). Consider the vector that consists of the dis-tances between p and each of the data points in X : which can be viewed as a one-dimensional representation (w.r.t p ) of the original data.
 Definition 3.1. Given a data point x  X  X . A data point y  X  X, ( y = x ) is the reference-based nearest neighbor of x with respect to the vector X p if where the minimum is taken over all the x i  X  X and x i = x .
Figure 1. Reference-based nearest neighbors in the one-dimensional dataset X p
The above idea is illustrated in Figure 1 with a small dataset X = { x 1 ,x 2 ,x 3 ,x 4 ,x 5 } . For the given reference point p , we have a one-dimensional dataset, whose values are the distances to p , i.e., X p = { d ( x 1 ,p ) ,d ( x 2 ,p ) ,d ( x 3 ,p ) ,d ( x 4 ,p ) ,d ( x given data point, the reference-based nearest neighbor is the closest point to it in the one dimensional data space X p example, the reference-based nearest neighbor of x 1 is x and the reference-based nearest neighbor of x 2 is x 1 . Intu-itively, reference-based nearest neighbors with respect to p are not necessarily the closest in the original data space. For 2D data, points located on the same circle ( p is the center) have a reference-based distance of 0. This property indi-cates that we usually need more than one reference points to improve the distance approximation in order to find global outliers. On the other hand the use of reference points is a major factor to contribute to the successful identification of local outliers in a complex dataset. We will discuss this in detail in the next section.
 Definition 3.2. Let x be a data point in X and { x 1 , ...x be the set of k reference-based nearest neighbors to x . The relative degree of density for x in the one-dimensional data space X p , denoted as D ( x, k, p ) , is defined as
Given a reference point, the neighborhood density of x is the reciprocal of the average distance to its k reference-based nearest neighbors in the one-dimensional space X p = { d ( x i ,p ) , 1  X  i  X  n } .
 Definition 3.3. Let P = { p 1 ,  X  X  X  ,p R } be a set of R refer-ence points. We define the neighborhood density of a data point x w.r.t. P as where k is a fixed parameter, indicating the number of reference-based nearest neighbors considered for each ref-erence point.

Essentially, data points deviated from their surrounding data patterns will get lower neighborhood density values. Based on the neighborhood density, each data point is as-signed a reference-based outlier score, or ROS, which is defined as
Data points in a given dataset are ranked according to their relative degree of density computed on a set of refer-ence points. Outliers are those with higher values of outlier scores.
By using a set of reference points, we intend to best ap-proximate the k nearest neighbor search in the original data space so that global outliers can be identified in a more ef-ficient way. If the distance metric satisfies the triangle in-equality, we have the following observations which show evidence on the effectiveness of using the relative degree of density to approximate the traditional definition of density.
Consider a reference point p and two data points x i ,x j X . Based on the triangle inequality, we have Note that the two sides in the above equation are equal when p, x i and x j are on the same line.

Figure 2. Reference-based nearest neighbors satisfying the triangle inequality
Using the dataset in Figure 1 as a simplified example, we can see from Figure 2 that the reference-based nearest neighbor of x 3 is x 2 , and Formally, we have the following Lemma 3.1. For any set of reference points P and any data point x i  X  X = { x 1 ,x 2 ,  X  X  X  ,x n } , where D a ( x i ,k ) is the average density as defined in Defini-tion 2.3.
 Proof. Let { x i 1 ,  X  X  X  ,x i k } be the k nearest neighbors of x with respect to the distance in the original data space X . Then, based on the triangle inequality, Since { x i 1 ,  X  X  X  ,x i k } are not necessarily the k referenced-based nearest neighbors of x i in the data space X p ,wehave by Definition 3.2 From equations (1) and (2), we get Since above holds for any reference point in P , it follows that
The above shows that for a given data point x , the reference-based density is lower bounded by the neigh-borhood density computed using the traditional k nearest neighbor search method. If a data point has a small enough density to be identified as outliers using the reference-based approach, it will have a even smaller density value using the distance-based method. If, instead of ranking, we are to use a density threshold  X  to determine the outliers, then the above analysis indicates that outliers detected under our reference-based density definition are also outliers under the original density definition.

The following special cases are also interesting and pro-vide further justification to our proposal. When the data set is one-dimensional, a single reference point (say 0) is sufficient and our approach is equivalent to the traditional distance-based approach. On the other hand, if the dataset itself is used as the set of reference points, then our ap-proach is identical to the traditional KNN approach. For-mally, we have the lemmas which are easy to prove. Lemma 3.2. Assume that the dataset X = { x 1 ,  X  X  X  ,x n } one dimensional. Then, D P ( x i ,k )= D a ( x i ,k ) ,  X  for any set P of reference points.
 Lemma 3.3. Assume that the set P of reference points con-tains a data point x i in the dataset X = { x 1 ,  X  X  X  ,x n Then, D P ( x i ,k )= D a ( x i ,k ) .
Let P = { p r , 1  X  r  X  R } be a given set of refer-ence points. The algorithm finds the potential outliers in the dataset X in three major steps: 1. For each reference point p  X  P , sort the origi-2. For each data point x  X  X , find the k reference-based 3. Set D P ( x, k ) of each point x to be the minimum of Outliers tend to have a higher value of ROS and they are ranked according to their ROS value. See Algorithm 1 for the details.
 Algorithm 1 The Algorithm for computing D P ( x, k )
It takes O ( n ) time to compute the distance vector X p for each reference point p  X  P . The calculation of the average neighborhood density D ( x, k, p ) involves find-ing the k reference-based nearest neighbors. Since the reference-based nearest neighbors are calculated on the one-dimensional space X p , it suffices to find them by sorting the original dataset X using the values in X p (i.e., the dis-tances to the given reference point p ) as the key. Sorting the distance vector X p can be done in O ( n log n ) time. Once the distance vector X p has been sorted, the calculation of D ( x, k, p ) for all the data points x  X  X can be done in O ( n ) time. Thus, the overall time complexity of our algo-rithm is in O ( Rn log n ) where R is the number of reference points. 3.2.1 Further Speedup To further improve the efficiency in computing D P ( x, k ) , we make the following observation. Assume that p 1 and p 2 are two reference points and that d ( p 1 ,p 2 ) is small. Then, data points in X sorted according to their distances to p is usually  X  X lmost X  sorted according to their distance to p . Thus, if we have processed p 1 and recorded the cor-responding sorted order X p 1 = { x i 1 ,  X  X  X  ,x i n } , we can calculate D ( x, k, p 2 ) by sorting the ordered list X p { x i 1 ,  X  X  X  ,x i n } with the various adaptive sorting algorithms that can take advantage of the  X  X ear sortedness X  of the vector X p 1 . One example of such adaptive sorting algo-rithms is the simple insertion sort whose running time is in O ( n + REV ) where REV is the number of pairs of ele-ments whose relative order is wrong [5]. Therefore, while the worst case execution time of computing D P ( x, k ) is O ( Rn log n ) , the practical execution time of our algorithm can be much lower. 3.2.2 Determination of Reference Points The determination of suitable reference points plays an im-portant role in terms of both the effectiveness and the effi-ciency of our algorithm. In our implementation, each refer-ence point is not necessarily a data point in X . We use as reference points the vertices on a grid obtained by partition-ing the axes in the data space to facilitate the selection of the closest next reference point in the second for loop of the above algorithm. The advantages of using vertices on the grid over randomly selecting reference points is two-fold: (1) Reference points are evenly distributed in the whole data space, and (2) the outlier detection result is deterministic, i.e., the obtained results would be the same with each run. Recall that the overall running time of our algorithm is in O ( Rn log n ) where R is the number of the reference points that is determined by the characteristics of the dataset rather than the size n of the dataset. For simple datasets that con-tain only one cluster, a few reference points are enough to correctly detect the outliers in even very large datasets. For more complicated datasets, domain knowledge can usually help in determining the number of reference points. By us-ing the grid vertices, we are able to partition the space in-crementally from coarse resolution to fine resolution to de-termine the appropriate number of reference points. Also, notice that all calculations in the current partition is not lost in the next partition, and only calculation for additional ref-erence points is computed. Our experiments show that with datasets containing a single data distribution, a few refer-ence points are enough to correctly detect all the outliers even with very large dataset.
The distance-based method is static in that it uses param-eters with fixed values for all the data points in a dataset. It ignores the cases where data patterns have different densi-ties, thus considers all data points in a dataset in the same setting. Consequently, it always assigns low density values to data points located in sparse regions even though some of them are deep in a sparsely distributed clusters. The authors in [3] also argue that the distance-based outlier detection method can only take a global view of the dataset, resulting in its failure to identify outliers local to certain clusters in a complex dataset.

By using a set of reference points, our reference-based approach is dynamic and able to see the whole dataset from various viewpoints. It is possible that the reference-based nearest neighbors of a given data point are different with re-spect to different reference points. Therefore, at one refer-ence point, the local outliers may have a high neighborhood density, while at another reference point, it may be shown lying in a very sparse neighborhood. Since the reference-based neighborhood density of a data point x is determined by the minimum among all its densities computed based on the set of reference points, it is guaranteed that with a set of reference points evenly covering the data space, data de-viated from the surrounding data patterns will be assigned lower density values. In the following, we will use exam-ples to show that reference-based method can successfully identify local outliers as well as global outliers in complex datasets that contain clusters of different densities.
To facilitate the evaluation of data clustering and outlier analysis, we developed a synthetic data generation system that can produce datasets with various cluster and outlier patterns. Interested readers are referred to [10, 11] for the details. Using our synthetic data generator, we generated a small 2D dataset X that contains 850 data points. There are three clusters C 1 ,C 2 and C 3 , where data in clusters C 1 and C 2 are uniformly distributed and data in C 3 are in normal distribution. Cluster C 1 has a smaller density com-pared to C 2 and C 3 . In addition to the three clusters which form the main body of the dataset, there are a few local out-liers as well as some global outliers in this dataset. In our experiment, we set the number of reference-based nearest neighbors k =4 . To ensure that the reference points are evenly positioned in the 2D data space, we set the number of reference points to be the 16 points on the 4 by 4 grid that divides the data space evenly. In the first run to mine the top 3 outliers, the three global outliers o 1 ,o 2 and o found. See Figure 3 for the details of the dataset. Since we want to check if the local outliers specific to the clusters can be found, the program is set to mine the top eight outliers in the second run. The result is displayed in Figure 3 (a), where the outliers are marked with a cross.
Figure 3. Finding top eight outliers (a) using reference-based method and (b) using the traditional KNN method
Figure 4. Finding top 76 outliers (a) using reference-based method and (b) using the traditional KNN method
We tested the distance-based outlier detection method with the same dataset. The implementation is based on Definition 2.2, where the neighborhood density D m ( x, k ) is the reciprocal of the distance to the k th nearest neigh-bor. As above, we set k =4 . The top 3 outliers iden-tified are the same as those identified using our method. However, distance-based method fails to find the two lo-cal outliers specific to cluster C 2 when mining the top 8 outliers. The result is shown in Figure 3 (b), where the un-detected local outliers are marked with arrows. It is ob-served that D m ( x, k ) -based method tends to identify data points in sparse regions as outliers such as those located in the perimeter of cluster C 1 . The ranking result shows that using D m ( x, k ) method, o 5 is ranked in the 9 th place but o is ranked in the 76 th place. In order to find the local out-lier o 4 , the distance-based method has to falsely mark many other data points as outliers.

This actually poses an interesting question: what will happen if the number of real outliers in a dataset is less than the number of top outliers the program is set to mine? Such issue can be important with large real world datasets whose domain knowledge is unknown. We test both reference-based approach and the distance-based approach to mine the top 76 outliers, when o 4 can only be identified using the distance-based method. The results are shown in Figure 4. We can see that in Figure 4 (b) many data points deep in the relatively sparse cluster C 1 are falsely marked as outliers before distance-based method is able to find the real local outlier o 4 . Although there are a few points inside cluster C 1 that are marked as outliers by our approach, they ranked af-ter most of the real outliers. Careful observation shows that they are the next best outliers within C 1 compared to those points on the border of C 1 . The overall outliers identified indicate that the reference-based approach is more likely to consider those data points deviating from or lying on the edge of the data patterns as outliers.

The above examples demonstrate that compared to the distance-based approach, reference-based method is not only superior in differentiating data deep inside a sparsely pact cluster from local outliers deviated from a dense pat-tern in a dataset, but also capable of eliminating false identi-fication of outliers inside the sparse data patterns of a com-plex dataset.

One of the motivations of the reference-based approach is to best approximate the neighborhood density obtained using the traditional k nearest neighbor approach so that global outliers can be effectively identified in a rather ef-ficient way. In this sense, the more the number of reference points, the better the distance approximation and the more accurate the identification of global outliers in a dataset. However as shown earlier, when the whole dataset is used as the set of reference points, reference-based approach is reduced to the traditional distance-based approach, which can lead to false identification of local outliers in a complex dataset. While this is an intrinsic problem with the exist-ing distance-based approaches, it can be easily solved in our reference-based method by starting with a few number of reference points and incrementally increasing the num-ber of reference points. Since the reference-based near-est neighbors of the data points computed with respect to a given set of reference points can be reused for the next round when more reference points are added, such adjust-ment will have little impact on the performance of the out-lier detection method. The detection process stops when certain outliers have been found and such inspection often involves human intervention.

To further explain why the reference-based density def-inition can facilitate the identification of local outliers, we use the previous dataset as an example. Suppose we have a set of 16 reference points P = { p 1 , ..., p 16 } , which are the vertices on a grid obtained by partitioning the axes in the data space as shown in Figure 5.

Figure 5. Finding local outliers using reference-based approach
Take the reference point p 1 as an example. If we draw circles with p 1 as the center and the distance to each data point as the radius, the k reference-based nearest neighbors of a certain data point x with regard to p 1 would be those on or closest to the circle where x is located. As is shown in the plot, there are no other data points that fall on the circle that passes through the local outlier o 4 . In contrast, data deep in clusters C 1 ,C 2 , and C 3 generally have nearest neigh-bors with little or no distance difference. Consequently, o will have a relatively smaller neighborhood density in the Though with another reference point, say, p 2 , it is possi-ble that o 4 may be assigned a high density if the circle ( p as the center and d ( p 2 ,o 4 ) as the radius) pass through one or more dense data patterns, the overall neighborhood den-sity is determined by the minimum among all the calculated points are sparsely distributed such that each data point in the data patterns are not isolated by the circle around a ref-erence point, i.e., the number of reference points accounts only a small portion of the number of the whole data, then data deep in clusters are ensured to have very close neigh-bors which contribute to high neighborhood densities with regard to each reference point. Therefore, o 4 will have a lower neighborhood density and a higher outlier score than data in the clusters.

It is noticed that there is indeed a tradeoff between the number of reference points and the ability of the reference-based method to detect global and/or local outliers. On the one hand, if all the data points in the original dataset are used as the reference points, then our approach is equivalent to the traditional KNN approach. On the other hand, by using a small set of reference points, local outliers will have a better chance of being detected at the potential cost of some inaccuracy in detecting global outliers. We leave it as a future research topic to investigate how to achieve such a tradeoff and how to integrate our approach with the various approaches to local and/or global outlier detection in the literature such as LOF [3].
In this section, we show experimently that the proposed method can efficiently identify local and global outliers in various datasets. We compare the performance of our ap-proach with the existing distance and density based meth-ods.
To compare the performance of the proposed reference-based approach with the existing k nearest neighbor ap-proach, our first test is to see how fast each method can find outliers in large datasets. We generated a set of syn-thetic datasets using the synthetic data generating system [10, 11]. It is well accepted that the evaluation of outlier detection involves human intervention. We limit our exper-iments to two-dimensional synthetic data so that the evalu-ation can be performed by simple visual inspection. The sizes of these datasets range from 1,000 to 500,000 data points. To be consistent, each dataset has a major data pat-tern that is normally distributed. Based on the fact that out-liers accounts for only a very small portion in a dataset, the number of outliers to be mined is set to be 1% of the data size in all the following experiments. For the reference-based approach, the number of reference points can be set to a constant for all these datasets. This is because all the test datasets have similar probability distribution and each of them contains only one normally distributed cluster. In such cases, a few reference points that cover different areas in the data space are sufficient to ensure the successful de-tection of outliers. As discussed before, the reference points are evenly located in the grid vertices of the data space. In our experiment, the number of reference points is set to 9 for all these datasets. We implemented the reference-based method in Java to facilitate the visualization of the outlier detection results. For the distance-based method, we down-loaded the executable version of Orca, the C implementa-tion of the distance-based method discussed in [2]. Since Orca is based on the distance-based algorithm with a near-linear running time, it is believed to be one of the most efficient KNN-based outlier detection methods. To com-pare the two programs, we run our method in command line mode so that the execution time includes time for input and output as Orca does. Orca also requires a preprocessing step to randomize the order of the original data and convert it to binary format. In our experiments, the cost of data pre-processing for Orca is not counted when recording Orca X  X  execution time. The number of nearest neighbors is set to be k =6 for both programs.

Figure 6. Log-scale execution time vs data size for reference-based approach and Orca
Since each dataset contains a single cluster that is nor-mally distributed, both programs can effectively mark the data lying farther away from the mean as outliers in a dataset. There is hardly any difference in the identified out-liers using the two methods. However, the difference of execution time between our reference-based approach and Orca gets bigger and bigger with the increase of the data size, despite the fact that our implementation of ROS is in Java while Orca is in C. Figure 6 is the log-scale plot of ex-ecution time vs data size for the two methods. Although the plot for Orca does not include the data preprocessing time, Figure 6 shows that with large datasets, reference-based ap-proach has orders of magnitude improvements in execution speed compared to Orca, one of the optimized implementa-tion of the distance-based approach.

Outliers are defined as data points that deviate from the main patterns of a dataset. They are most likely to be con-sidered in the context of clusters with different types of data distributions. That is, an object is marked as an outlier if it is isolated from the clusters in a given dataset. To test if the reference-based approach can effectively find meaning-ful outliers in a complex dataset, we generated a dataset of 10,000 datapoints. There are six data patterns. Three are uniformly distributed and the other three are normally dis-tributed. The densities of the six data patterns are different.
Unlike the datasets containing only normally distributed data where outliers lie on the outer fringe of each cluster, this dataset has two types of outliers: outliers uniformly dis-tributed around the main data patterns and outliers lying on the outer skirt of the three normally distributed clusters. To demonstrate the effectiveness of reference-based approach in finding both global and local outliers, we implemented LOF according to [3] that is well known for its performance in detecting local outliers. We compare the effectiveness of the distance-based method, LOF, and our ROS on this dataset. For the distance-based method, and ROS, we set k =4 . For LOF, we set MinPts =30 as recommended in [3]. The number of reference points for ROS is set to be 2% of the data size.

Figure 7. Outlier detection result from distance-based approach
Figure 7, 8 and 9 are the screen shots showing the re-sults for mining the top 100 (1% of data size) outliers. In all the three figures, detected outliers are marked with crosses. Visual inspection shows that the KNN-based method, LOF, and our referenced-based method are all able to discover
Figure 9. Outlier detection result from reference-based approach the global outliers. Furthermore, two local outliers (the two crosses surrounded by small circles) are discovered by our reference-based method, while LOF can only find one of them and the KNN-based method fails to find either of them. Therefore, our approach is not only effective in find-ing outliers that lie far away from the major patterns in the dataset, it also achieves similar or even better results than LOF in detecting local outliers.
The National Hockey League (NHL) data have been used as a benchmark in several outlier analysis works [8, 3, 13]. We use the statistics of NHL 2003-2004 season obtained from NHL website [9]. The dataset contains 916 entries. We compare our approach with both Orca and LOF on this dataset. The goal is to see if our approach can effi-ciently find the outliers identified by Orca and/or LOF in real-world datasets. We set the number of reference points for reference-based approach to be two to the power of the data dimensionality. For example, given a 3D set of data, the reference points are the 8 vertices of a cube in the 3D space with all data inside the cube. We set k =4 for both Orca and our approach and MinPts =30 for LOF to en-sure LOF finding meaningful outliers.

We conduct the experiment in a similar way as other outlier analysis works. It consists of two tests. The first test mines the outliers based on the three attributes: games played, goals scored and shooting percentage. LOF, Orca and our reference-based approach achieve identical results and the top three outliers are listed in Table 1(a). The out-lier status of the three identified players are obvious. They only played a few games and scored once, but their shooting percentage is unusually high.

The second test is to mine outliers based on the three attributes: points scored, plus-minus statistic and penalty minutes. The top 3 outliers found by our reference-based approach are listed in Table 1(b). Sean Avery is on top be-cause his points and plus-minus figures are moderate but the number of penalty minutes is the highest among all the play-ers. LOF gets the similar result as ours while Orca X  X  result is slightly different. The top rank from Orca, Zdeno Chara, is ranked as the 9 th outlier by our approach and 26 th outlier by LOF. Careful investigation shows that outliers identified by Orca tend to be in a sparse region regardless of the data distribution. Reference-based method instead favors out-liers that deviate from the main data patterns and the results are close to LOF in this aspect. For example, Jody, who is ranked third by our approach lies far away from the trend of the data body due to his low points and plus-minus statistic but extremely high penalty minutes. With 3D datasets of size about 1,000, the execution time for both Orca and our reference based approach is within 0.1 seconds and can be neglected.
In this paper, we have proposed an efficient reference-based outlier detection method that uses the relative degree of density with respect to a fixed set of reference points to calculate the neighborhood density of a data point. In addi-tion to being compatible with the traditional distance-based outlier detection methods, our approach performs better in identifying local outliers that deviate from the main patterns in a given dataset. The execution time of our algorithm is O ( Rn log n ) where n is the size of dataset and R is the number of reference points. Candidate outliers are ranked based on the reference-based outlier score that is assigned to each data point. Theoretical analysis and empirical studies show that the reference-based approach is not only highly efficient and scalable to very large datasets, but also able to achieve better results than distance-based approaches. When all the data points are used as references points, our method becomes a distance-based approach finding global outliers. When only one reference point is used, local out-liers relative to the reference point are discovered. We advo-cate the use of a small number of reference points uniformly distributed over the data (using a grid) to find global and lo-cal oultiers effectively and efficiently.
 The authors thank the support from NSERC and the anony-mous referees for their comments. Part of the work was done while Yaling Pei was with the University of British Columbia Okanagan.

