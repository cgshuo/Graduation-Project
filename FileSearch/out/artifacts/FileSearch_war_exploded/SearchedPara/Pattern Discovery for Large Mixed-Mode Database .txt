 In business and industry today, la rge databases with mixed data types (continuous and categorical) are very common. There are great needs to discover patte rns from them for knowledge interpretation and understanding. In the past, for classification, this problem is solved as a discrete data problem by first discretizing the continuous data based on the class-attribute interdependence relationship. Howe ver, so far no proper solution exists when class information is unavailable. Hence, important pattern post-processing tasks su ch as pattern clustering and summarization cannot be applied to mixed-mode data. This paper presents a new method for solving the problem. It is based on two essential concepts. (1) Though cla ss information is absent, yet for interdependence with others in the group can be used to drive the discretization of the continuous data. (2) For a large database, correlated attribute groups must fi rst be obtained by attribute clustering before (1) can be applied. Based on (1) and (2), pattern discovery methods are developed for mixed-mode data. Extensive experiments using synthetic and real world data were conducted to validate the usefulness and effectiveness of the proposed method. H.2.8 [ Database Management ]: Database Applications  X  data mining . I.5.2 [ Pattern Recognition ]: Design Methodology  X  pattern analysis . I.5.3 [ Pattern Recognition ]: Clustering  X  algorithms .
 Algorithms, Design, Experimentation. Data mining, pattern discovery , unsupervised discretization, attribute clustering, mixed mode data, mutual information. In the recent years, with th e progress of microelectronics, information technologies together w ith the ever broadening use of computers in a vast spectrum of business and industry, the types of data in databases become more diverse. They contain numeric, symbolic or categorical data an d are referred to as mixed-mode data. Often these sensed or doc umented data are acquired from different aspects or components of a complex system. Their use is not necessarily confined to clas sification and often they contain no specific class labels. Nevertheless, there are great needs to discover patterns from these types of data for the comprehensive analysis, interpretation and understanding of the knowledge inherent in them. This pape r presents new methods for discovering patterns from larg e mixed-mode databases where class information is non-existing or unavailable. To solve the classification problems with mixed-mode data, the general practice in the past is to quantize the data of a continuous attribute optimizing its interdependence with the class labels so that the entire data set can be treated as a categorical data set. When class labels are unavailable , descretization of a continuous attribute maximizing its entropy ha s been used. However, such an approach does not utilize the co rrelated information inherent in the data which is the most valuable information in a dataset. To gain insight in solving this probl em, two questions are raised. 1) In the absence of class labels, is it reasonable to find the most representative or governing attribute in the dataset and use it just like a class label to drive the discretization of the continuous attributes? 2) For a large database, if we conceive that it might consist of more than one correlate d attribute group, would it make better sense if we first partition the dataset into optimal correlated attribute groups before the representative attribute of each group is used to discretize the cont inuous attributes of that group? Following this line of thought, we have developed a methodology to solve this problem. In this paper, we first present algorithms to compute the normalized interdependence measures R (referred to as Redundancy in information theory) be tween: a) discre te attributes; b) discrete and continuous attribut es and c) continuous attributes respectively. We next define the most representative attribute (referred to as the mode) that maximizes the sum of its R measures with others in that group (referred to as MR, Multiple Inter-Dependence Redundancy). We then present an attribute clustering algorithm that partitions the large mixed-mode database into attribute clusters optimizing the intra-group interdependence. Besides the mode, another way of de fining the most representative is treated as the  X  X lass label X . Both of these representative attributes will be experimentally evaluated. We then use it within each group to drive the discretization of the continuous attributes in that group. After turning all the continuous data into categorical data, pattern discovery [10], patter n clustering [25, 26] and pattern summarization [25, 26] can then be applied to each of the attribute groups or to the dataset obtained after their re-merging. To validate and evaluate the proposed methods, various sets of synthetic, expository and real-world datase ts are used with comparison with other existing methods. A mixed-mode database contai ns data which could assume continuous and categorical values. In a broader coverage, the data items in the database could be of a) ordered nature such as a real or an integer valued number, or ranking which could be represented as integers or b) u nordered discrete nature such as categorical item made up of symbols, terms, intervals. In most practice, continuous data is converted into interval data so that all the data items could be treated as a discrete event to render a uniform framework for pattern recognition, machine learning, event association and rule discovery. For years, most effective classification algorithms in machin e learning can only be applied to nominal (categoric al) database or database with continuous values separately [22, 25] but ar e unable to deal with mixed-mode database directly. However, in th e real world, a large portion of data is mixed mode. To have in ductive learning systems applied to these kinds of databases, continuous values need to be discretized. Recently, researchers also found that even if some systems are explicitly designed fo r continuous attributes, they can attain a higher accuracy if continuous data are appropriately discretized. Through discretizing th e continuous data, most of the inductive learning algorithms can accommodate continuous and mixed-mode data more e ffectively [7, 9, 18, 25]. Discretization is a process of partitioning the value space of a continuous attribute into a finite number of intervals and assigning a nominal value to each of them . After discretization, continuous values are converted to discrete events and hence the mixed mode database is transformed into categorical database suitable for machine learning, data mining and pattern discovery. In this paper we describe a new technique fo r discretizing the values of continuous attributes for mixed-mode data without requiring class information. It is based on an information measure that reflects interdependence between the continuous attribute and the representative attribute in attribute groups. Discretization algorithms can be classified into five groups [13]: supervised versus unsupervised, static versus dynamic, global versus local, top-down (splittin g) versus bottom-up (merging), and direct versus incremental. Several supervised or class-dependent discretization methods ha ve been proposed [8, 10, 14, 16, 20, 24]. Nevertheless, in th e more general settings, some serious problems still prevail. In supervised learning, we often assume that the class label is th e ground truth. For some problems, the class information could be questionable. Even if it is available, some of the attributes are not necessarily closely related to the class attribute as observed by subject matter experts and demonstrated by our experimental runs (e.g. in Mushroom data from UCI Machine Learning Archive [3], some attributes are more correlated with the rest of attributes than the class attribute). Although unsupervised discretizati on such as Equal Width [20] and Equal Frequency [20] does not require the class information, the selection of the number of intervals is not adequately addressed and their criteria of discretization fail to consider the relationship between the interv al boundaries and the correlated attributes if they exist. In cases when data are of mixed mode nature or the continuous attributes contain multi-modal distributions, this approach fails to capture attribute inter-dependence relation which is very important in a database. When machine learning was first introduced, researchers targeted a relatively small set of attributes. As the size of databases and the diversity of attributes increased, data clustering began to break down though the classification problems were not seriously affected yet their effectiveness was diminishing. In supervised learning, the problems were pa rtly solved through feature selection. Later, as data mining and pattern discovery came into play, the dimensionality problems were a little relaxed yet the ultimate problem of it still prevailed. Even up to today, most of conventional clustering algorithms will often face the challenges related to the nature of large s cale mixed-mode database with a large number of attributes. In unsupervised learning, attribute clustering was proposed to provide a partial solution as a remedy to the problems but in general, class-dependent discretization had to be used to convert the continuous data into interval data [5]. To cluster or select attributes, the t-value method is wide ly used [1]. It is important to note that the t-value can only be used when the samples are pre-classified. If no class informati on is provided, it cannot be used for attribute selection. So the A ttribute C lustering A lgorithm, ACA [5] was proposed to cluster attributes. In ACA, continuous data have to be converted into interval data before attribute clustering could be applied. To close this gap also, this paper extends ACA so that it is able to deal with mixed-mode data by introducing attribute interdep endence redundancy measures between attributes of various attribute types and a multiple interdependence measure [2, 23] for selecting attributes with the highest correlation with the rest of attributes within an attribute cluster. After turning all the cont inuous data into categorical data, pattern discovery [10], pattern clustering [25, 26] and pattern summarization [25, 26] can then be applied to each of the attribute groups or to the dataset obtai ned after their re-merging. Today, pattern discovery for intelligent decision support, knowledge-based reasoning, and data analysis applies more and more to large scale complicated systems and problem domains [9]. In most of the existing systems, data preprocessing, such as data cleansing, filtering, attribute reduction, are included so as to remove noises, to bring out more relevant information from the data and to reduce the search space. However, they often depend on prior knowledge, such as pa rameters and preconceived classificatory framework. Thus, they could be very biased and usually involve long iterative sear ch and examination process. To respond to these needs, a data-dri ven pattern discovery approach has been advanced [16]. It is able to discover, in an unbiased manner, statistically significant events automatically, and to generate decision rules for cate gorization and prediction. In general, pattern discovery [10] extracts previously unknown regularities in the data and is a useful tool for categorical data analysis. In most real world problems, pa ttern discovery typically produces an overwhelming number of pattern s, resulting in very difficult and time-consuming effort fo r problem comprehension and interpretation. To combine fragments of information from individual patterns to produce more generalized forms of information and to use them to further explore or analyze the data, pattern clustering [25, 26] is de veloped to simultaneously cluster the discovered patterns and their associated data. Pattern summarization [25, 26] can be a pplied as pattern post-processing method to select from the di scovered patterns a most representative subset which could be considered as the summary of the pattern cluster. Once th ese steps are completed we know how patterns relate locally and ho w pattern groups are realized in data subspaces and related in the entire data space. Consider a data set D containing a set of N-tuples of mixed-mode data. Every tuple is described by N attributes, some assuming discrete values and others continuous values. Let X = { X represent this attribute set. Fo r convenience, let us permute the attributes (without affecting the analysis) that the first M attributes { Xi | 1  X  i  X  M } are discrete valued and the remaining { Xi X  | M+1  X  i X   X  N } are continuous valued. Then, each X i , 1  X  i  X  M can be seen as a discrete random variable ta king on values from its alphabet i attribute. Each X i , M +1  X  i  X  N can then be seen as a continuous random variable. Thus , a realization of X can be denoted by x { x assume any value in value in { M i X  X   X   X   X  N i X  X  } where  X  is the real number. Thus, each tuple in the data set is a realization of X . The basic elements required fo r mode finding and attribute clustering are the interdependence measures between various data types of mixed-mode attributes. In order to use them under a unified framework, we use interdependence redundancy measure R built upon mutual information [16, 24] to account for the interdependence between various attribute types. Definition 3-1 The interdependence redundancy measure [4] between two discrete X  X alued attributes, A i and A {1,..., M }, i  X  j , is defined as: where I ( A i : A j ) is the mutual information between A H ( A i , A j ) is the joint entropy of A i and A j I ( A i : A j ) measures the interdependence between A i independence between A i and A j [9, 10, 16]. R is a normalized A ) = 0, they are statistically independent. If 0&lt; R ( A and A j are partially dependent. To assess the strength of interdependence of an attribute with other attributes in a dataset, th e sum of its redundancy measures with other attributes is used. We refer it as the multiple interdependence redundancy measure, denoted as MR.
 Definition 3-2 The multiple interdependence redundancy measure [6, 14] of A i within an attribute cluster, C = { A j where R ( A i : A j ) is the interdependence redundancy measure between A i and A j . We next introduce the concept of the  X  mode  X  in an attribute group. It is defined as the attribute with the highest multiple interdependence redundancy in that group. Definition 3-3 The mode of an attribute group C = { A p }, denoted by  X  ( C ), is an attribute, say A i , in that group such that There are two phases of using R . Phase I is for directing attribute clustering. Phase II is for di scretizing continuous attribute outcomes. In both phases, we a dopt the discretization approach when computing R between continuous attributes and between a discrete and a continuous attribut e. In Phase I, for accurate approximation, we use as many bins as we could as long as each cell in the two dimensional bins contain a number of data points designated by a rule of thumbs. In Phase II, since the goal of discretization is for discovering high order patterns of discrete events, there is a desirable guideline to confine the number of discretized values for each attribute in line with the discrete attributes to maintain the cons istence of the  X  X ntrinsic group interdependence X . For attribute clustering of mixed-mode data, to gain higher estimation accuracy, we should keep the number of bins as large average, there are at least a few points in each bin. Let S be the sample size and m be the number of bins. In practice, the number of data points per cell denoted as  X  is usually chosen as the parameter in the rule of thumb manner (say 2 or 3). Then and thus In another word, if m is chosen as the numbe r of bins, then each contingence table cell will have at least  X  data points. Once m determined for all i X , we can treat each of them as a discrete attribute taking on values from its alphabet  X   X  i m we compute I, H and R between continuous attr ibutes for attribute clustering. In Phase II, since the goal of discretization is to describe the association patterns among the ca tegorical events, to ensure consistence and uniform ity of the transforme d database, we would like to see the number of the categorical events in the continuous attribute after the discretization is in line with those in the discrete valued attributes. Hence we could first select a m in line with other discrete attributes or select a reasonable one subjectively. If an objective is to be achieved, several trials of m could be made to optimize the intended goal. Once an m is decided, it is used to partition the outcome values of the representative continuous attribute, which is the mode is a continuous attribute, via entropy maximization [20] and then use it to drive the discretization of other continuous attribut es in that group. As for R between a discrete attribut e and a continuous attribute, we will use an Optimal Class Dependence Discretization Algorithm (OCDD) [14] to first discretize the outcome values of the continuous attribute assuming th at the discrete attribute as the class label. Once the continuous ra ndom variable is discretized we could treat the pair of attributes as discrete ones in deriving their R measure. As for R between two discrete attributes A i and A compute I and H and so R from [10, 16] respectively taking m and m j as their alphabet size respectively. For a large database, clustering a ttributes into smaller groups not only significantly reduces the search dimension but also brings out various aspects of attribut e correlation among those clusters. Definition 3-4 Attribute clustering is a process which finds c we define attribute clustering as a process that  X  A N }, A i is assigned to a C r , r  X  {1, ..., c }, where C all s  X  {1, ..., c }  X  { r }. The basis of attribute clustering is to cluster the attribute set such that attributes within a cluster should have high interdependence whereas attributes in different clusters are less correlated. We adapt the k-Mode Attribute Clustering Algorithm (ACA) [5] for categorical data to mixed-m ode data by introducing new R measures between various types of attributes. The k -Mode Attributed Clustering Algorithm is evolved from the k-mean sample clustering algorithm [5] where  X  the  X  X amples X  are replaced by the  X  X ttributes X ;  X  the  X  X ample means X  is replaced by the  X  X ode X  of an attribute  X  the  X  X istance between samples X  is replaced by R between By assigning an integer k , ACA will obtain k clusters optimizing the intra-group attribute interdependence. The best choice for k can be determined by the highest sum of the multiple significant R denoted as SR obtained from clusters for each cluster configuration obtained. To find the best choice for k , we choose the cluster configuration with highest sum of the multiple significant R, for the configuration with k clusters. That is, for all k  X  {2, ..., p }, we select k such that To investigate the complexity of ACA, we consider a relational table composed of n samples such that each sample is characterized by p attribute values. The k -modes algorithm requires O ( np ) operations to assign each a ttribute to a cluster. It then performs O ( np 2 ) operations to comput e the mode for each cluster. Let t be the number of itera tions, the computational complexity of the k -modes algorithm is given by After the correlated attribute groups are obtained, we try to find in each group the most representative attribute which, in a certain way resembles the class label, to drive the discretization of the continuous attributes in the group. In classificati on, the rationale to use the class label to drive discretization is that if the outcomes of continuous attributes are transformed into a set of interval events, that event set should maximize the interdependence between attributes and the class label. For selecting the most representative attribute of an at tribute group to have a similar function of the class label, there are two ways: i) selection of the attribute with highest MR in that group; ii) sel ection of an attribute which if used as a class label in that group will give its events the highest classification ra te based on other attributes in that group. The first is based on the average result of interdependence between the two attri butes. The second is more targeted on the classification result. If the objectiv e is classificatory; we should adopt the second. However, since the highest classification rate might be biased towards a certain subspace but not necessarily represent the average interdependence among the attributes, to bring out the overall correlated in formation between the attributes, the mode seems to fit into the picture better. Nevertheless, both approaches will be explored and evaluated experimentally. Once the representative attribute is chosen for a group, we will adopt an optimum iterative dynamic programming algorithm known as OCDD (Optimal Class-Dependent Discretization Algorithm) [14] for discretization. Such process could be viewed as partitioning of the outcome values of a continuous-valued attribute into a number of discrete intervals that maximize its R with the representative attribute. OCDD is an iterative dynamic programming algorithm which solves an optimization problem by catching sub-problem solutions rather than re-computing them. It is a branch of nonlinear optimization involving ratio functions. OCDD is an iterative dynamic programming algorithm that has two important components. One attempts to at tain the maximum value of a parametric objective function by dynamic programming. The other is an iterative process that uses the first component to drive towards the final globally optimum solution of the class-dependent discretization objectiv e. It adopts a process called memorization which writes down the results of the recursive sub-problems and looks them up late r if needed. It results in significantly saving of processing time. In practice, a problem is particularly efficient when the feasible solutions are subsets or subsequences of the data. The basic steps of OCDD [14] are: 1) Meticulously generate all feasible solutions; 2) Eliminate partial feasible solu tions that are either redundant, or cannot be extended, using a pruning rule to reach an optimal solution; 3) Transform the algorithm to an iterative table algorithm. When OCDD is applied to discretization, the objective function chosen is R ( C : A ) where C is the representative attribute and A is the continuous attribute. Theoretically, the proposed method would obtain an optimal partitio ning. However there are a number of practical issues we have to c onsider in real wo rld applications. Hence, methods to reduce the number of intervals and methods for smoothing the raw data be fore partitioning have been integrated into OCDD. See [14] for details. Once a mixed-mode database is tr ansformed into one containing only categorical events, patter n discovery [19] and pattern clustering [25] methodologies coul d be readily applied to the transformed database. Hence in our final implementation, all the definitions for events, event associations and patterns, will be based on discrete variables within a unified framework which will not be exemplified here. The efficacy of our proposed method will be demonstrated via the experimental results. Since this paper proposes a novel a pproach to tackle the discovery of patterns for mixed-mode da ta, we must design appropriate experiments to verify the premises and reveal how realistic the proposed approach is when applied to various types of mixed-mode data. In this section we fi rst present a set of experiments with selected data of various type s to test our premises. Next we will apply our proposed methods to two large sets of real world databases which are complex, not containing class labels but backed by adequate domain know ledge for affirmation of the analytical results. This experiment is designed to verify the applicability of attribute clustering and our discretization method to mixed-mode datasets. It tries to demonstrate the role of the representative attribute in inducing discretization of the con tinuous data just like the class attribute would even when the class label is absent. The synthetic data set is composed of 20 attributes in which 5 of them are discrete and 15 of them are continuous. Each tuple is pre-classified into one of the five classes: C 1 , C 2 , C imposing the values of A 1 and A 13 among the tuples as shown in figure 1. Let us denote the attributes as A 1 , ..., A 20 discrete attributes which can take on a value from alphabets { X  T  X , value from alphabets { X  X  X ,  X  Y  X ,  X  Z  X  X . A 6 , ..., A attributes which can take on values in {0  X   X   X  1} where a real number. As in our design ed experiment, attribute values A and A 13 of each tuple are able to determine the class membership. For values of other attributes including A 2 , ..., A A , they are generated randomly in the following manner:  X  A 2 :  X  T  X  if A 13 &lt; 0.5;  X  F  X , otherwise.  X  A 4 :  X  X  X  if A 6 &lt; 0.3;  X  Y  X  if 0.3  X  A 6 &lt; 0.6;  X  Z  X , otherwise.  X  A 5 :  X  Y  X  if A 6 &lt; 0.3;  X  Z  X  if 0.3  X  A 6 &lt; 0.6;  X  X  X , otherwise.  X  A 6 -A 7 : uniformly distributed within [0, 0.5] if A  X  A 8 -A 12 : uniformly distributed within [0, 0.5] if A  X  A 14 -A 17 : uniformly distributed within [0, 0.3) if A  X  A 18 -A 20 : uniformly distributed within [0.3, 0.6) if A Using this scheme to generate the synthetic data set, it is clear that A and A 13 are two representative attributes correlating with the attribute groups of { A 4 -A 12 } and { A 2 , A 3 , A Regardless of the class membersh ip of each tuple, if such correlation can be revealed, one should seek the most representative attribute of each attribute group to drive the discretization of the continuous a ttributes. In our experiment, we generated 250 tuples where each cl ass contains 50 tuples in the synthetic data set. Noises are then added by replacing 25 percent of the tuples with randomly generated values within the range of the corresponding attributes. Firstly, the interdepe ndence redundancy measure R as defined in definition 3-1 between each pair of discrete attributes, each pair of continuous attributes and each pair of discrete and continuous attributes is calculated. As shown in figure 2, the optimal attribute cluster configuration (no. of attribute clusters) obtained by ACA is two ( k =2). ACA identifies two attribute clusters: { A 1 , A 4 , ..., A ..., A 20 }. It shows that the proposed discretization algorithm is able to correctly compute the mutual information between a pair of continuous attributes, and betw een a discrete attribute and a continuous attribute for ACA to reveal the correlation between the mixed-mode attributes embedded in the synthetic dataset. It was found that A 1 is the mode of the first cluster whereas A mode of the second cluster, indicat ing that the attributes with the most intrinsic governing or cla ssificatory characteristics as reflected via their statistical inter-dependence with other attributes in their group are found as the modes. To evaluate the effectiveness of the generated discretization schemes on the performance of th e classification algorithm, we used the discretized synthetic data set with 25% noise to train a C5.0 decision tree algorithm. 30% of samples are randomly selected from the data set as the training data to build a decision 
Figure 1. Imposition of intr insic classes by adjusting the tree and the rest of samples are treated as the testing data. In comparison, the synthetic data set was also discretized by OCDD making use of the class la bel information. OCDD is experimentally proven to be a ve ry effective discretizer when comparing with unsupervised disc retization like Equal Width, Equal Frequency and Maximum En tropy [20] and supervised discretization like CADD [8]. The classification accruracy of C5.0 on data discretized by OCDD, which is an existing class dependent algorithm, and on ou r proposed method, which does not require any a priori knowledge, is 74% and 83.67% respectively. The comparison re sults show that the proposed method surprisingly reac hed higher classificati on accuracy. It is worth noting that the discretization scheme generated by the proposed method can improve cla ssification accuracy even when the class label is excluded. As regards to the number of generated rules/nodes, the proposed method also achieves better performance (13 leaf nodes and 10 non leaf nodes) while C5.0 produced significantly more nodes (17 leaf nodes and 10 non leaf nodes) when using the discreti zation scheme of OCDD which makes use of class label. The objective of this experiment is to show how the proposed method is able to be applied to continuous data where the class labels are missing and how the expe rimental results are related to the ground truth provided by the re moved class labels. Because of the transparency characteristics of pattern discovery, new light could be shed to reveal how th e representative attributes are related to the correlated aspects of the attributes and also with the class labels. The Iris data set [3] with 150 samples and 4 numeric attributes contains 3 classes (Setos a, Versicolour and Virginica) of 50 instances each, where each class refers to a type of iris plant. The 4 numeric attributes are sepal length, sepal width, petal length and petal width. We first use the class attribute to discretize the rest of the attributes and obtain the classi fication rate by discover*e, a commercial tool for pattern discove ry [17]. The classification rate for the class labels from the data set with labels retained is 96%. We then remove the class labels from the data set and assume that each of the remaining four as the class attributes (representative attributes) in turn to drive the discretization of all the continuous data and conduct the classificati on afterward. The classification rate obtained by considering sepal length, sepal width, petal length and petal width as the go verning ones is 76.67%, 64.67%, 92% and 92% respectively.
 From the classificatory results obtained, it is clear that the last two attributes, petal length and petal wi dth, could be considered as the representative attributes as they both yield the highest classification rate even without the class labels. To reveal how the representative attribute relates to the correlated aspects of the other attributes, we disc retize the four attribute a) driven by the class label and b) driven by the representative attribute, which is the last attribute, when the class label is taken from the data set. To our surprise the discretization results driven by the last attribute is identical to those driven by the class labels. After converting all the data into discrete valued events, pattern discovery methods can then be applied. Some examples of patterns discovered after the Iris da ta is discretized include a) if sepal width is within [1, 3] and petal length is within [0.1, 3], then it is classified as Setosa, b) if sepal width is within [3, 4.9] and petal length is within [0.1, 1], then it is classified as Versicolour and c) if petal width is within [6.3, 7.9] and sepal width is within [4.9, 6.9], then it is classified as Virginica. This data set obtained by UCI Machine Learning Archive [3] was extracted from US Census Bureau database. It contains 48,842 instances of a mix of continuous and discrete data with 14 attributes. It has been used for prediction task whether a person makes over 50K a year or not. This experiment is used: 1) to demonstrate the existence of at tribute subgroups in the mixed-mode data set; 2) to illustrate the attainment of attribute cluster configuration and the grouping of cl uster items in situations with or without class label; 3) to show the classification characteristics of various attributes in different attribute groups found by ACA; and 4) to show that the attribute with highest normalized MR , or simply the mode, in the attribute group is usually with high classification rate if it is assumed to take the role of a class label. The experiment results do show that the mode in each attribute group/cluster can be considered as the most representative attribute to drive the discretization of continuous attributes in the attribute group/cluster. Attribute Group Dropped Class Label 
Key: *-The mode of the attrib ute group. A mode is with the In order to demonstrate the effectiveness of the proposed method in extracting the same intrinsic information inherent in the classes, we experimented on the data se t with class label excluded and those with class label included. Based on S R values, ACA found the optimal cluster configurations that 3 attribute clusters and 5 attribute clusters are local optimal for the data with the class label excluded and those with the class label included respectively. In our proposed method, no class information is required; nevertheless, the results reported in table 1 show that even without class information, our proposed method and ACA are able to group interdependent attributes together. To further investigate the attributes resided in each attribute group, we study the classificatory aspe ct of them to show that, in a normal setting, the mode is also the attribute that renders good enough classification rate if it is regarded as a class label. The attribute clusters, the MR values and the classification performance of their attributes are tabulated in table 2.
 Once the mixed-mode Adult data set is transformed into one containing only categorical even ts using the proposed method, pattern discovery methodology coul d be readily applied to the transformed data set. Some exam ples of patterns discovered after the Adult data is discretized include a) if education is  X  X S-grad X  and education-num is within [9, 10] , then income is  X &lt;=50K X  b) if marital-status is  X  X arried-c iv-spouse X  and relationship is  X  X usband X , then income is  X &gt;50K X  and c) if marital-status is  X  X arried-civ-spouse X  and hours-per-week is within [40, 99], then income is  X &gt;50K X . After examining the performance of the proposed method, it is applied to 2 sets of large real world data of mixed-mode nature. The data was collected by the candidates with the help of domain experts. In the meantime, additional domain knowledge was acquired to see whether or not the subtle operational patterns could be discovered by the pro posed system without relying on prior knowledge before the analysis. The meteorological (MET) database is a large database consisting of 8,784 samples and 43 attributes of which 18 are categorical and 25 are continuous. The MET data was taken from 5 different surface stations over a one-year-long period (8760 recorders) in the great urban region of Guangzhou City, Guangdong province, China. The types of the meteor ological parameters (attributes) collected from each surface station include 6 discrete attributes and 5 continuous attributes. A ll those parameters have their internal relationship based on the geographic location of the surface stations and might be governed by local terrain and land use. The five surface stations de noted by the alphabets S = {A, B, C, D, E} are stations as listed in figure 3. Station A, B, C, D and E is Guangzhou metropolis, Foshan city, Shenzhen city, Dongguan city and Zhongshan city respectively. The description of data collected by each station is listed in table 3. We first applied ACA on a set of meteorological dataset. The SR of the clustering process fo r various attribute cluster configurations is plotted on figure 4. It is obvious that a local optimal cluster configuration woul d consist of 5 clusters of MET parameters. data is sparse. #-The attribute marked with  X # X  holds the highest Table 4 displays that the mixe d-mode meteorol ogical database with 43 attributes has been cluste red into 5 sub-groups. The first 4 of 5 clusters are grouped based on the interdependence among the similar characteristics (types) of the attributes within each cluster formed. This implies that those a ttributes within cluster are highly dependent upon each other or they are very  X  X lose X  to each other or one  X  X ollowed X  by the others. We then study the mode and the characteristics of each of the clustered parameter groups From the patterns discovered by our method, significant features within the data collected from the surface stations have been found which comply with the domain knowledge. Attributes in each of the first 4 clusters reflect the regional (global) characteristics of the correlated meteorological parameters. The mode found in each group has been treated as the reference parameters for those of the same type taken from the five stations. Regarding the last cluster group, all of the attributes therein reflect local characteristics which are significantly influenced by the local geographical feature such as land use and land coverage. The discovered modes in these clus ters cover only 3 stations, A, B and C, indicating that the remain ing two, D and E, are in very weak position for the weather condition analysis. The data is taken from the delay coking unit (DCU) of the Sinopec SJZ Petro-Chemical refi nery for about 5-month-long period. It consists of 22,096 sa mples and 47 attributes out of which 11 of them are discrete valued data and 36 are continuous valued data. It was acquired di rectly from the ABB DCS sensors by which the temperatures, the levels, the flow rates and the pressures as well as the control actions of PLCs were collected. It is a semi-continuous thermal cracking process in which a heavy hydrocarbon feedstock is converted to lighter and more valuable products and coke. Its mechanism of coking can be broken down to three distinct stages as show n in figure 6. The feed undergoes partial vaporization and mild cracking as it passes through a specially designed coking furnace. Since this is a set of very complex data taken directly from th e delay cooking plant, there is no specific class information. It is relatively a large database. Since we have a certain degree of partial domain knowledge concerning DCU, this set of data will be ideal to challenge the usefulness and effectivene ss of the proposed system. We first apply ACA to cluster th e database into sub-database containing subgroups of attributes. Figure 5 shows the sum of the configurations. It is found that k = 5 would render a local optimal configuration. We next proceed to discretize the continuous data for each cluster based on the mode discovered. The result of each attribute group revealing subtle oper ations is included in figure 6. Attribute Name Types Notes 
Table 4. Attributes in the attribute clusters of the optimal 
AG Attribute Cluster Items 1 C *B5, A5, C5, D5, E5 --RH (Relative Humidity) 2 C *C7, A7, B7, D7, E7 --WD (Wind Direction) 3 D *C1, A1, B1, D1, E1 --TC (Total Cloudiness) 4 C *A6, B6, D6, E6, MM --AP (Site Pressure) 5 M attribute group. AG-Attribute Group. C-Continuous Attribute Group. D-Discrete Attribute Gro up. M-Mixed-Mode Attribute Figure 5. The plot of sum of significant MR values against k, Based on the five clusters fro m our developed method for the patterns, the most important rela tionships with the sensors and controllers of the coking facilities have been found: including the temperature-oriented groups, pressure-oriented groups and flow-oriented groups. Figure 6 displays the 5 clusters associated with the 3 stages of DCU. The attribute number and distribution of the largest group, i.e. attribute group 1, indicates that its mode acts as a control factor for the entire processing system and has globally influenced almost all of the proc ess parameters fo r the facility. From the parameter grouping, the discovered results indicate that attribute group 2 and 4 control the output distributions of the two internal units like fractionator and coke drum. They are very important groups for the local performances of the processing usually referred to as performance factor. The last group, attribute group 5, discovered is exactly associated with the critical safety mechanism designed for this pressure-temperature-mixed processing facil ity. Its mode actually controls the temperature condition as a trigging factor to activate the emergency release response. All of the five cluster groups with the patterns and mode attributes discovered provided us the strong pattern discovery and analysis evidence in revealing the underlying control principle in industrial systems. The research presented in this paper is meeting the pressing demand of extracting subtle info rmation from large mixed-mode databases in scientific, business and industrial communities. It provides an integrated, flexible and generic framew ork for pattern discovery and analysis of larg e mixed-mode da tabases. Its contribution in theoretical and methodological perspectives as well as in real world applications has been conveyed. The validity and the effectiveness of the proposed methods have been backed by a number of successful experi mental results. Their usefulness in real world applications is de monstrated by the intriguing and revealing results from two mixed-mode databases. One is taken from a geographic area in Southern China and another is a set of massive multi-senor data taken from a delay coking plant. Its operating platform will not only help data management but also bring out the subtle knowledge trapped in the collected data in science, business and industry. [1] Agrawal, R., Ghost, S., Imielinski, T., Iyer, B., and Swami, [2] Alon, U., Barkai, N., Notterman, D.A., Gish, K., Ybarra, S., [3] Asuncion, A., and Newman , D.J. 2007. UCI machine [4] Au, W.H., Chan, K.C.C., and Yao, X. 2003. A novel [5] Au, W.H., Chan, K.C.C., Wong, A.K.C., and Wang, Y. 2005. [6] Ben-Dor, A., Bruhn, L., Fr iedman, N., Nachman, I., [7] Chau, T., and Wong, A.K.C. 1999. Pattern discovery by [8] Ching, J.Y., Wong, A.K.C., an d Chan, K.C.C. 1995. Class-[9] Chiu, D., Wong, A.C.K., and Cheung, B. 1990. Information [10] Ho, K. M., and Scott, P.D. 1997. Zeta: A global method for [11] Kohavi, R., John, G., Long, R., Manley, D., and Pfleger, K. [12] Kurgan, L., and Cios, K.J. 2001. Discretization algorithm [13] Liu, H., Hussain, F., Tan, C.L., and Dash, M. 2002. [14] Liu, L., Wong, A.K. C., and Wang, Y. 2004. A global optimal [15] Quinlan, J.R. 1993. C4.5: Programs for Machine Learning . [16] Wang, C. C., and Wong, A. K. C. 1979. Classification of [17] Wang, Y., and Wong, A.K. C. 2010. Discover*e. Pattern [18] Wang, Y., and Wong, A.K.C. 2003. From association to [19] Wong, A.K.C., and Wang, Y. 2003. Pattern discovery: a data [20] Wong, A.K.C., and Chiu, D. K. Y. 1987. Synthesizing [21] Wong, A.K.C., and Liu, T.S. 197 5. Typicality, diversity and [22] Wong, A.K.C., and Wang, Y. 1997. High order pattern [23] Wong, A.K.C., Chiu D.K.Y. , and Huang, W. 2001. A [24] Wong, A.K.C., Liu, T.S., and Wang, C.C. 1976. Statistical [25] Wong, A.K.C., and Li, G.C.L. 2008. Simultaneous pattern [26] Wong, A.K.C., and Li, G.C.L. 2010. Association pattern 
