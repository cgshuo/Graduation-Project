 This paper presents a different perspective on diversity in search results: diversity by proportionality. We consider a result list most diverse, with respect to some set of top-ics related to the query, when the number of documents it provides on each topic is proportional to the topic X  X  popu-larity. Consequently, we propose a framework for optimiz-ing proportionality for search result diversification, which is motivated by the problem of assigning seats to members of competing political parties. Our technique iteratively deter-mines, for each position in the result ranked list, the topic that best maintains the overall proportionality. It then se-lects the best document on this topic for this position. We demonstrate empirically that our method significantly out-performs the top performing approach in the literature not only on our proposed metric for proportionality, but also on several standard diversity measures. This result indi-cates that promoting proportionality naturally leads to min-imal redundancy, which is a goal of the current diversity approaches.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models Algorithms, Measurement, Performance, Experimentation. Search result diversification, proportional representation, pro-portionality, redundancy, novelty, Sainte-Lagu  X  e.
Search result diversification techniques have been studied as a method of tackling queries with unclear information needs. Standard retrieval models and evaluations are based on the assumption that there is a single specific topic asso-ciated with the relevant documents for a query. Diversifica-tion models [4, 1, 26], on the other hand, identify the prob-able  X  X spects X  of the query and return documents for each of these aspects, making the result list more diverse. As-pects denote the multiple possible intents, interpretations, or subtopics associated with a given query. By explicitly representing and providing diversity in the result list, these models can increase the likelihood that users will find docu-ments relevant to their specific intent and thereby improve effectiveness.

This problem of finding a diverse ranked list of documents, with respect to the aspects of the query, has been studied primarily from the perspective of minimizing redundancy . In other words, existing work focuses on penalizing result lists with too many documents on the same aspect, which increases the redundancy of coverage, and promoting lists that contain documents covering multiple aspects. Most of the effectiveness measures for diversity [7, 8] are also based on this notion of redundancy. They penalize the redundancy in a ranked list of documents by judging each of the docu-ments given the context of those retrieved at earlier ranks [9].

In this paper, we approach the same task from a different perspective. We view the problem of finding a good result list of any given size as the task of finding a representative sample of the larger set of ranked documents. Hence, the quality of the subset (a result list) should be measured by how well it represents the whole set (a much larger sample of the ranking). Using a simple (and well-worn) example, in a ranked list for a query  X  X ava X , 90% of the documents may be about the programming language and 10% about the island. From our perspective, a result list containing ten documents where only one of them was about the island would be more representative than a result list containing five documents on each subtopic. Consequently, we treat the problem of finding a diverse result of documents as finding a proportional representation for the document ranking.
Finding a proportional representation is a critical part of most electoral processes. The problem is to assign a set of seats in the parliament to members of competing political parties in a way that the number of seats each party pos-sesses is proportional to the number of votes it has received. In other words, the members in the elected parliament must be a proportional representation of these parties. If we view each position in our ranked list as a X  X eat X , each aspect of the query as a  X  X arty X  and the aspect popularity as the  X  X otes X  f or this  X  X arty X , the problem of diversification becomes very similar to this seat allocation problem.

Based on the above analogy, we propose a novel tech-nique for search result diversification. It is an adaptation of the Sainte-Lagu  X  e method, a standard technique for find-ing proportional representations that is used in the official election in New Zealand 1 . Generally, our technique starts with an empty ranked list of a certain size. It sequentially visits each  X  X eat X  in the list and determines for each of them to which aspect it should be allocated in order to main-tain proportionality. Then it selects the best document for the selected aspect to occupy this  X  X eat X . In addition, we also present a new effectiveness measure that captures pro-portionality in search results. We demonstrate empirically that our method is more effective than the top performing approach in the diversity literature not only according to the proportionality measure but also using several standard metrics including  X  -NDCG [7] and NRBP [8] that existing work has been designed to optimize. This indicates that op-timizing search results for proportionality naturally leads to minimal redundancy and a diverse, effective result list. In the next section, we briefly mention some related work. Section 3 presents our approach to proportionality and the effectiveness measure based on it. Section 4 describes in de-tail our proportionality-driven framework for search result diversification. Section 5 and 6 contains the experimental setup and results, as well as analyses and discussions. Fi-nally, Section 7 concludes.
The literature of diversification has been concentrating on the notion of novelty and redundancy . These two notions are considered under the context of user behavior with the assumption that users examine the result lists top down and eventually stop reading. Therefore, a document at any rank providing the same information as those at earlier ranks is considered redundant . Likewise, a novel document is one that provides information that has not been covered by any of the previous documents. As a result, a ranked list is considered more diverse if it contains less redundancy, or equivalently, more novelty.

This is clearly demonstrated through several standard ef-fectiveness metrics such as  X  -NDCG [7] and NRBP [8]. They measure the diversity of a ranked list by explicitly rewarding novelty and penalizing redundancy observed at every rank. Similarly, diversification techniques [4, 29, 1, 5, 26] attempt to form a diverse ranked list by repeatedly selecting doc-uments that are different to those previously selected. In other words, they try to accommodate novelty at every po-sition in the list.

In this paper, we present a different perspective on di-versity. This view of diversity emphasizes proportionality, which is the property that the number of documents re-turned for each of the aspects of the query is proportional to the overall popularity of that aspect. Consequently, the framework we derive is driven by this notion of proportion-ality, thus is different from the existing work.

Several metrics have been proposed to measure the pro-portionality in the outcome of an electoral process, an excel-lent summary of which is provided by Gallagher and Lijphart [17, 19]. They can be classified into two broad categories: the first concentrates on the absolute difference between the percentage of seats and the percentage of votes, the sec-ond focuses on the the ratio between them. These measures appear mathematically simple but attempt to address com-plex specific issues of elections that are not always relevant to our context. As a result, we apply Gallagher X  X  Index [17], or the least square index, which is reasonably suited to our problem.

For completeness, we will provide a brief survey of tech-niques in the current literature of diversification. They can be classified as either implicit or explicit approaches. The former [4, 29] assumes that similar documents cover similar aspects without modelling the actual aspects. They itera-tively select documents that are similar to the query but different to the previously selected documents in terms of vocabulary [4] or divergence from one language model to another [29]. More recent work [25, 22] applies the portfo-lio theory to document ranking, which views diversification as a mean of risk minimization. Explicit approaches [23, 1, 5, 26], on the other hand, explicitly models aspects of a query with a taxonomy [1], top retrieved documents [5] or query reformulations [23, 26] and thus can directly select documents that cover different aspects. Experimentally, ex-plicit approaches have been demonstrated to be superior to implicit approaches [26].
In this paper, we view the task of diversification as finding a proportional representation for a document ranking. In this section, we will explain the notion of proportionality as well as describing an effectiveness measure for it.
Let T = { t 1 , t 2 , ..., t n } indicate a set of aspects for a query q and D denote a large set of documents related to q . Let p indicate the popularity of the aspect t i  X  T , which is the percentage of documents in D covering this aspect. Addi-tionally, let S be any subset of D . We define S to be pro-portional to D , or a proportional representation of D , with respect to T if and only if the number of documents in S that is relevant to each of the aspects t i  X  T is proportional to its popularity p i .

Let us revisit the example in Section 1, in which there are 90% of documents in D about the  X  X ava X  programming language and the rest 10% is about an island named  X  X ava X . Let { x, y } denote any subset of D with x documents about programming and y documents about the island. In this case, { 9 , 1 } is proportional and thus is a proportional repre-sentation of D . While { 8 , 2 } is not proportional, it is more proportional than { 7 , 3 } .

Let R indicate a ranking of documents in D and S now represent a sub-ranking of R . We define S to be proportional to R if the subset of documents S provides at every rank is a proportional representation of D .
This notion of proportionality is, in fact, frequently used in evaluating the outcome of elections in which seats are assigned to members of competing political parties. This problem can be stated as follows. We have a limited num-ber of seats in the parliament and a number of competing parties. Each party has its own members. Through election campaigns, each party obtains a number of votes from peo-p le around the country. The goal is to assign members of different parties to the seats such that the number of seats each party gets is proportional to the votes it receives.
Several metrics have been proposed to measure such pro-portionality. Most of them are based on the difference be-tween the percentage of votes each party receives and the percentage of seats it gets. Among those, the least square index (LSq) [17] is one of the standard metrics for measuring dis-proportionality: w here v i and s i are the percentage of votes and the per-centage of seats the i -th party received. Let us illustrate this with an example in which we have ten seats and three competing parties, namely A , B and C . Let us assume both A and B receive 50% of the votes and C gets 0%. Clearly, the proportional assignment which provides A and B each with five seats and C with none will result in LSq = 0. The value for LSq will increase when the seat assignment becomes more disproportional.

We will now turn our attention to the proportionality of a retrieved list of ten documents for the query  X  X atellite X , which we assume to have two aspects:  X  X atellite internet X  and  X  X atelilte phone X  with equal popularity of 50%. Due to the possible presence of non-relevant documents, we have to create a third  X  X spect X  to account for non-relevant doc-uments. As a result, proportionality requires this list to contain five relevant documents for each of the two aspects and zero documents for the  X  X on-relevant X  aspect. This sit-uation seems to be very similar to the election described above. Unfortunately, we cannot apply LSq to measure the dis-proportionality of this result list due to two differences.
First, each member typically belongs to exactly one po-litical party. As a result, one party gets more seats than it should always indicates that some other party is getting less than they deserve. A document, however, might be related to multiple aspects of a query. It then is possible that an aspect can be  X  X ewarded X  with additional documents while others still have as many relevant documents as they deserve.
Second, it is equally bad for any party to get any more seats than it should. In our case, however, selecting for the result list a document that is relevant to an aspect that already has enough relevant documents in the list is not as bad as selecting a non-relevant document.
 Taking both differences into consideration, we argue that LSq, since is designed for the seat allocation problem, puts too much penalty on overly representing query aspects. LSq fails to recognize that some of these situations do not create any undesirable consequences in our setting, and thus should not be penalized. Therefore, we propose a new metric, dis-proportionality at rank K , calculated as follows: where v i is the number of relevant documents that the aspect t should have, s i is the number of relevant documents the system actually found for this aspect, n NR is the number of non-relevant documents, and Formula (1) has two important properties. The first is that it penalizes a result set for under-representing any aspect of the query ( s i &lt; v i ) but not for over-representing them ( s i &gt; v i ), which addresses the first issue associated with LSq. The second is that while the over-representation of a query aspect is not penalized, the over-representation of the  X  X on-relevant X  aspect ( n NR &gt; 0) is, which overcomes the second issue associated with LSq.

A perfectly disproportional set of documents in the con-text of information retrieval would be a set with all non-relevant documents. Thus, the Ideal-DP is given by: T he last step is to derive our proportionality measure by normalizing the DP score with Ideal-DP in order to make it comparable across queries:
F inally, the Cumulative Proportionality (CPR) measure for rankings is calculated as follows:
In this section, we first introduce the Sainte-Lagu  X  e method, a standard technique for finding proportional representa-tions that is used to solve the seat allocation problem de-scribed in Section 3.2. We then demonstrate the analogy between this problem and our problem of propotionality-based diversification, which helps us derive our technique from the Sainte-Lagu  X  e method. This method considers all of the available seats iteratively. For each of them, it computes a quotient for all of the par-ties based on the votes they receive and the number of seats they have taken. This seat is then assigned to the party with the largest quotient, which helps maintain the overall proportionality. We assume the selected party will then as-sign one of its members to this seat. Finally, it increases the number of seats assigned to the chosen party by one. The process repeats until all seats are assigned. Pseudo code for this procedure is provided as Algorithm 1. In this procedure, P = { P 1 , P 2 , ..., P n } is the set of parties and M P . v i and s i indicate the number of votes P i receives and the number of seats that have been assigned to P i so far. Algorithm 1 T he Sainte-Lagu  X  e method for seat allocation 2 : for all available seats in the parliament do 3: for all parties P i do 5 : end for 11: end for
Let q indicate the user query, T = { t 1 , t 2 , ..., t n the aspects for q whose popularity is { p 1 , p 2 , ..., p tion, let R = { d 1 , d 2 , ..., d m } be the ranked list of documents returned by an initial retrieval and P ( d i | t j ) indicate some estimate of the probability that the document d i is relevant to the aspect t j . The task is to select a subset of R to form a diverse ranked list S of size k .

As mentioned earlier, existing techniques [1, 26] generally favor an S with smaller redundancy. Our idea, on the other hand, is to favor an S with higher proportionality. The opti-mal S , consequently, is a ranked list in which the number of relevant documents for each of the aspects t i is proportional to its popularity p i . This objective is, in fact, very similar to that of the seat allocation problem above. As a result, we derive a general proportionality framework for diversifi-cation directly from the procedure presented above, which is described as Algorithm 2.

This framework can be explained as follows. We start with a ranked list S with k empty seats. For each of these seats, we compute the quotient for each aspect t i following the Sainte-Lagu  X  e formula. We then assign this seat to the aspect t i  X  with the largest quotient, which marks this seat as a place holder for a document about the aspect t i  X  . After that, we need to employ some mechanism to select the ac-tual document with respect to t i  X  to fill this seat. Depending on that mechanism, we then need to update the number of seats occupied by each of the aspects t i accordingly. This process repeats until we get k documents for S or we are out of candidate documents. The order in which each doc-ument is put into S determines its ranking. Assuming each document selected for t i is truly relevant to t i , the Sainte-Lagu  X  e method guarantees proportionality in the final set of documents.

Different choices of document selection mechanisms, which subsequently determine the choices of seat occupation up-date procedures, will result in different instantiations of our framework. We now present two such instantiations. Algorithm 2 A Proportionality Framework 2 : for all available seats in the ranked list S do 3: for all aspects t i  X  T do 5 : end for 10: end for
W e first present a straightforward adaptation from the seat allocation problem above. The Sainte-Lagu  X  e method assumes that each member belongs to exactly one party. When a member is assigned to a certain seat, the party naturally takes up the entire seat. Directly applying this technique to our context means assuming each document is associated with a single aspect. As such, we have to deter-mine the aspect for each of the documents d j  X  R , which we assume to be the aspect t i  X  T to which d j is most relevant: As a result, we construct for each aspect t i a list of doc-uments associated with it in decreasing order of relevance, documents in M i . It follows naturally that the best docu-ment for an aspect t i is the first in the list M i . We refer to this native adaptation as PM-1 and codify it as Algorithm 3. Algorithm 3 P M-1 2 : for all seats in the ranked list S do 3: for all aspects t i  X  T do 5 : end for 10: end for W e now provide a probabilistic interpretation of the Sainte-Lagu  X  e method, which removes the naive assumption that a document can only be associated with a single aspect. In-stead, we assume all documents d j  X  D are relevant to all aspects t i  X  T , each with a probability P ( d j | t i ). This prob-abilistic interpretation, which we call PM-2, is described by Algorithm 4.

A first point to note is that PM-2 has a different mech-anism for document selection. Once a seat is given to the aspect t i  X  with the largest quotient, we need to assign to this seat a document that is relevant to t i  X  . In the context of multi-aspect documents, however, among several documents all of which are relevant to t i  X  , it is sensible to promote doc-uments that may be slightly less relevant to t i  X  but are at the same time relevant to other aspects, compared to those that are slightly more relevant to t i  X  but are non-relevant to all others. This is, after all, what motivates diversifica-tion: we want more users to be able to find what they want. Therefore, PM-2 introduces the parameter  X  : d  X  arg max that trades relevance to t i  X  with relevance to more aspects. We abbreviate quotient [ i ] to qt [ i ] due to the space limitation.
A second point is that when a document d  X  is selected for the current seat, since it is assumed to be relevant to all aspects t i  X  T , each aspect occupies a certain  X  X ortion X  of this seat as opposed to a single aspect taking up the entire seat as previously. Intuitively, the degree of occupation of the seat is proportional to the normalized relevance to d w here s i is the  X  X umber X , which is now better regarded as  X  X ortion X , of seats occupied by t i .

PM-2 can be summarized as a two-step procedure as fol-lows. For each of the k seats in S , it first employs the Sainte-Lagu  X  e formula to determine which aspect this seat should go to in order to best maintain the proportionality. Then, Algorithm 4 P M-2 2 : for all seats in the ranked list S do 3: for all aspects t i  X  T do 5 : end for 10: for all aspects t i  X  T do 13: end for 14: end for it selects the document that, in addition to being relevant t o this aspect, is relevant to other aspects as well. Finally, it updates the  X  X ortion X  of seats in S occupied by each of the aspects t i according to how relevant it is to the selected document. Query and Retrieval Collection . Our query set consists of 98 queries, 50 of which are from the diversity task of the TREC 2009 Web Track (WT-2009) [10] and the other 48 are from TREC 2010 Web Track (WT-2010) [11]. Our evaluation is done on the ClueWeb09 Category B retrieval collection 2 , which is also used in both WT-2009 and WT-2010. This collection contains approximately 50 million web pages in English. During query and indexing time, both the query and the collection are stemmed using the Porter stemmer. In addition, we perform stopword removal using the standard INQUERY stopword list.
 Baseline Retrieval Model . We use the standard query-likelihood model within the language modeling framework [14] to conduct the initial retrieval run. This run serves both as a mean to provide a set of documents for the diversity models to diversify and a baseline to verify their usefulness. We also use this model as the estimate of relevance P ( d between the document d j and the aspect t i .

Spam filtering is known to be an important component of web retrieval [3]. Following Bendersky et al. [3], we use a spam filtering technique as described by Cormack et al. [12] with the publicly available Waterloo Spam Ranking for the ClueWeb09 dataset, which assigns a X  X pamminess X  X ercentile S ( d ) to each document d in the collection. In particular, let p ( d i | q ) indicate the score the retrieval model assigns to the document d i , the final score of d i is given by: Diversity Models . We evaluate PM-2, the proportionality-aware model we propose for search result diversification. In addition, we will also present results obtained by PM-1 for comparison. Our first diversity baseline model for compari-son is MMR [4], which is considered standard in the diversity literature. Since the explicit approach for diversification is h ttp://boston.lti.cs.cmu.edu/Data/clueweb09/ generally superior to the implicit approach, we also compare our models to xQuAD, which has been demonstrated to out-perform many others in this class [26]. In fact, xQuAD is among the top performers in both diversity tasks of TREC 2009 and TREC 2010 [10, 11]. In addition to these two baselines, we also compare our results to those published by TREC whenever appropriate.
 Experiment Design . We use Lemur/Indri 3 to conduct the baseline query-likelihood retrieval run with the toolkit X  X  default parameter configuration. All of the diversification approaches under evaluation are applied on the top-K re-trieved documents. All of these models except for PM-1 has a single parameter  X  to tune. Readers should refer to the original papers [4, 26] for the interpretation of this parame-ter in the respective models. We consider for  X  values in the enforces complete separation between tuning and testing. In particular, each system is tuned on WT-2009 and tested on WT-2010 and vice versa. We present the result averaged across two folds unless stated otherwise. PM-1, since it is parameter-free, has no tuning involved.

As for the parameter K , we tested K  X  X  50 , 100 , 500 , 1000 } and found that all four models achieved their best at K = 50. Therefore, all results presented here are achieved with K = 50.
 Evaluation Metric . We first report our results using CPR, the proportionality metric we propose in Section 3. Since this metric certainly favors our models as they are designed to capture proportionality in the search results, we also re-port the results of several standard metrics that existing work was designed to optimize. This includes those used in the official evaluation of the diversity task WT-2010 [11]:  X  -NDCG [7], ERR-IA (a variant of ERR [6]) and NRBP [8]. These measures penalize redundancy at each position in the ranked list based on how much of that information the user has seen and how likely it is that the user is willing to scan down to that position. In addition, we also report our results using Precision-IA [1] and subtopic recall, which indicate respectively the precision across all aspects of the query and how many of those aspects are covered in the re-sults. Last but not least, all of these measures are computed using the top 20 documents each model retrieves also to be consistent with the official TREC evaluation [10, 11]. h ttp://www.lemurproject.org Query Aspects . Explicit approaches such as xQuAD and PM-2 assume the availability of the query aspects and their popularity. We first consider the official sub-topics iden-tified by TREC X  X  assessors for each of the queries as its aspects. This simulates the situation where we know ex-actly what aspects the query has and provides a controlled environment to study the effectiveness of different diversi-fication approaches. As for the aspect popularity, since it is not available in TREC X  X  judgment data, we assume uni-form probability for all aspects, which is also consistent with existing work [26, 27, 28].

In order to simulate more practical settings in which we do not know but have to guess the aspects of the query, we follow Santos et al. [26] by adopting suggestions provided by a commercial search engine as aspect representations. How-ever, the search engine is unable to provide suggestions for four of the queries in our set. As a result, these experiments are conducted on the subset of 94 queries for which we can obtain aspect representation. We also assume uniform as-pect distribution since it was demonstrated to be the most helpful [26].

It is worth noting that the aspects obtained from the search engine certainly do not completely align with the judged aspects provided by TREC assessors. In other words, there will be overlap between the two sets but there will also be generated aspects that are not in the judged set. We will refer to this problem as the misalignment between different sets of aspects and we do not attempt to evaluate the rele-vance of these misaligned aspects (those that are not in the judged set) in this paper.
In this section, we evaluate how well different methods maintain proportionality in the search results using both TREC sub-topics and suggestions from a commercial search engine as aspect descriptions. Table 1 shows the Cumu-lative Proportionality score for each system as well as the Win/Loss ratio  X  the number queries each system improves and hurts respectively. The letters Q, M, X and P indi-cate statistically significant differences (p-value &lt; 0.05) to Query-likelihood, MMR, xQuAD and PM-1 respectively.
From Table 1, we first notice that although all diversity models are able to provide improvement over the initial re-trieval, the magnitude of improvement is very different. The improvement from MMR, for example, is insignificant while the improvement from the other three is more substantial.
Among the four diversity models, PM-2 outperforms all others on both sets of aspects with statistical significance, which demonstrates the effectiveness of our method at cap-turing proportionality. MMR is the least effective since it is completely unaware of the query aspects, and thus is unable to capture the proportionality among them. xQuAD, on the other hand, does take into account the query aspects to penalize redundancy. For each document selected, xQuAD downweights each of the aspects based on the degree of its relevance to the selected document so that the aspects that have less relevant documents will have higher priority in the next round. Further details about xQuAD can be found in [26]. Hence, xQuAD indeed has the effect of implicitly pro-moting proportionality, which explains why it significantly outperforms query-likelihood, and also MMR on one of the Table 1: Performance of all techniques in CPR. The letters Q , M , X and P indicate statistically signifi-cant differences to Query-likelihood, MMR, xQuAD and PM-1 respectively (p-value &lt; 0.05).
 aspect sets. PM-2, despite the conceptual difference, can b e explained using xQuAD X  X  framework of reweighting as-pects as well. From this perspective, the biggest difference between the two is that PM-2 uses a more proportional-ity aware aspect weighting function which is based on the Sainte-Lagu  X  e algorithm. This result indeed confirms the ef-fectiveness of this proportionality-driven aspect weighting function.

It is worth noting that PM-1, despite being a parameter-free naive version of PM-2, is comparable to xQuAD. There is no statistically significant difference between these two. Comparing PM-1 to PM-2, however, reveals the weakness of its naive assumption. PM-1 associates each of the doc-uments with exactly one aspect, thus it has the risk of as-sociating documents with the wrong aspects. In addition, PM-1 fails to promote documents relevant to multiple as-pects. Both of these account for its inferiority to PM-2 with both sets of aspects.
We now compare our proposed techniques to MMR and xQuAD using standard metrics from the diversity literature as mentioned earlier. Instead of showing the results aver-aged across two folds, we show the results obtained in each fold separately so that we can compare our results with the official results from TREC. It should be noted that the com-parison between our results and those from TREC should be taken as indicative only, since our systems and theirs use dif-ferent initial retrieval run. Table 2 shows the results for all of the techniques we studied as well as the best performing system on ClueWeb09 Category B reported by TREC. In addition to the scores in each metric, Table 2 also presents the Win/Loss ratio each system achieves over the query like-lihood baseline in terms of  X  -NDCG.

The first observation from Table 2 is that all systems per-form worse in all metrics on WT-2009 than they do on WT-2010. The effectiveness of all systems certainly depends on the quality of documents retrieved by the initial retrieval run. Since all of our systems rerank the top 50 returned documents for each query, we examine these documents in both precision-IA and sub-topic recall. The former indi-cates how many relevant documents for each aspect we have for reranking and the latter indicates how many of the as-pects for which we have relevant documents. The results are shown in Table 3, which suggests that the top 50 documents for queries in WT-2009 cover less topics (i.e. many sub-Query-likelihood, MMR, xQuAD and PM-1 respectively (p-value Table 3: Quality of the baseline run for the WT-2 009 and WT-2010 query sets in sub-topic recall and precision-IA.
 topics do not get any relevant documents) and also contain c onsiderably less relevant documents for each of the topics than WT-2010. Therefore, there is far less room for improve-ment on WT-2009 than there is on WT-2010, which leads to the fact that all systems perform better on WT-2010.
Regarding the comparison among diversification techniques, we see a very similar trend as in the previous case with pro-portionality. In particular, MMR is least effective method due to its lack of awareness of the query aspects. PM-2, on the other hand, outperforms all other methods in almost all metrics with statistically significant improvement in many cases. PM-2 with automatically generated aspects even out-performs the best performing system in TREC evaluation. It should be noted that the best performing system in TREC 2010 of which results we report also use suggestions gener-ated by a search engine as aspect descriptions. This further confirms the effectiveness of PM-2: it provides results with not only a higher degree of proportionality but also a lower degree of redundancy.
The analyses in this section are conducted on the entire query set as there is no need to consider WT-2009 and WT-2010 separately. In addition, we only present our analyses with the manually generated set of aspects because we have similar findings with the other set, only to a slightly lesser extent due to the aspect misalignment problem.

We are interested in two aspects of the improvement each technique provides over the initial retrieval: (1) the robust-ness [21] of the improvement, and (2) the reasons that ac-count for this improvement. Robustness refers to the num-ber of queries each technique improves and hurts together with the magnitude of the performance change. To under-stand the robustness of each model, we provide a more de-tailed view of the Win/Loss ratio that was provided earlier in Table 1 and Table 2. In particular, instead of showing how many queries each system improves and hurts over the entire query set, we now look at these numbers with respect to the percentage of the improvement. The histogram in Fig. 1 shows, for various ranges of relative increases (pos-itive ranges) and decreases (negative ranges) in CPR and  X  -NDCG, the number of queries improved and hurt with respect to the query likelihood baseline.

It can be seen from Fig. 1 that most of the performance changes resulting from using MMR is in the two low ranges  X  [0%,25%] and +[0%, 25%], which indicates that MMR rarely improves or hurts a query drastically. Combined with the fact that it helps and hurts about the same number of queries (35/33), MMR can only provide slight improvement over the baseline.

In contrast, PM-2 and xQuAD provide substantial im-provement ( &gt; +100%) for several queries. In addition, com-pared to MMR, these two models hurt about the same num-ber of queries but they improve many more. As a result, PM-2 and xQuAD significantly outperform MMR in most cases. Comparing PM-2 and xQuAD, although they help and hurt about the same number of queries, PM-2 has a much larger magnitude of improvement. This is demon-Figure 1: Robustness of all techniques with respect t o the baseline query-likelihood. strated through Fig. 1 with the fact that PM-2 has more queries in the highest range ( &gt; +100%).

The effectiveness of each model depends on two factors: the quality of the initial retrieved set of documents and the model X  X  power to select a diverse subset from that pool of documents. Since all models operate on the same pool, the former factor becomes irrelevant. As for the model power, the key component of both our method and xQuAD is the query likelihood estimate of relevance P ( d j | t i ) between an aspect t i and a document d j . While xQuAD uses P ( d j | t to penalize redundancy at every rank, PM-2 uses it to ac-commodate proportionality. Intuitively, the more accurate P ( d j | t i ) is at telling which document is relevant to which of the aspects of the query, the more diverse the final ranked list will be. In this experiment, we study how well these techniques perform at different level of accuracy P ( d j provides.

In order to quantify the accuracy of P ( d j | t i ), we do as follows. Let D be the set of top 50 documents returned for the query q , which has a set of aspects { t 1 , t 2 , ..., t rank all documents d j  X  D for each of the aspects t i with P ( d j | t i ) and record the NDCG score. We then use the aver-age of NDCG across all aspects as the measure of accuracy of P ( d j | t i ). Table 4 presents the absolute improvement each model has over the baseline (in both CPR and  X  -NDCG separately) on different ranges of accuracy of P ( d j | t Table 4: CPR and  X  -NDCG breakdown by ranges of accuracy of P ( d j | t i ) .
 also show the number queries and how the baseline query l ikelihood performs in each of these ranges.

Since MMR does not use P ( d j | t i ), its performance obvi-ously does not correlate with the accuracy of P ( d j | t 2 consistently provides larger improvement than xQuAD across all ranges of accuracy and metrics. In addition, the gap between the improvement in both CPR and  X  -NDCG of PM-2 and xQuAD is generally larger as the accuracy of P ( d j | t i ) increases. This clearly indicates using P ( d optimize proportionality is much more effective than to min-imize redundancy, which explains the all-round superiority of PM-2.

In summary, we have demonstrated that MMR is the least effective because it helps and hurts about the same num-ber of queries. PM-2 and xQuAD both help more queries than they hurt, but PM-2 is able to provide substantially larger improvement over the baseline than xQuAD, helping PM-2 to be statistically significantly better than xQuAD even though they both outperform MMR and the baseline. The reason for PM-2 X  X  superiority over xQuAD is that PM-2 uses P ( d j | t i ) to accommodate proportionality at every rank, which is more effective than using it to penalize redundancy. The results obtained with PM-2 contain not only a higher degree of proportionality but also a lower degree of redun-dancy.
Our techniques sequentially go over all  X  X eats X  in the re-sult ranked list and decide for each of them which aspect it should go to. After the aspect is determined, PM-1 sim-ply chooses the best document for this aspect according to P ( d j | t i ) while PM-2 might promote documents that are slightly less relevant to this aspect but relevant to other as-pects as well.

The problem arises when the initial retrieval fails to find relevant documents for some of the aspects. When a X  X eat X  X s assigned to an aspect without relevant documents, P ( d j will mistakenly provide some false positive non-relevant doc-uments to fill in that seat, leading to undesirable results. In this section, we will investigate this effect.

Sub-topic recall of the baseline run is certainly the best metric for studying the effect of coverage. Table 5 shows how different systems behave on different ranges of sub-topic recall of the top 50 documents retrieved by the baseline run. For each of the sub-topic recall ranges, Table 5 provides the percentage of queries that each system helps and hurts (marked as  X %Q+ X  and  X %Q- X  respectively) together with the its relative improvement ( X %Imp. X ) in both CPR and  X  -N DCG with respect to the baseline. We also show for each range the number of queries as well as the performance of the baseline for references.

We first examine the percentage of queries helped and hurt by each system. At the low recall range ([0,0.5)), both PM-1 and PM-2 hurt more queries than they improve. As the recall goes up, these numbers improve. This trend is especially clear with  X  -NDCG. This clearly demonstrates the effect sub-topic recall has on our techniques. xQuAD by its nature does not have the same problem. As a result, the negative effect of low subtopic recall on xQuAD is smaller than it is on our methods: xQuAD has better Win/Loss ratios than both PM-1 and PM-2 on low ([0,0.5)) and medium ([0.5,0.75)) recall range. This helps further explain what we saw earlier in Table 2: although the same techniques perform worse on WT-2009 than they do on WT-2010, PM-1 and PM-2 are the ones with the largest per-formance difference in terms of Win/Loss ratio. The reason is the subtopic recall of the baseline for WT-2009 is con-siderably lower than that for WT-2010 (as demonstrated previously in Table 3), which affects our systems the most.
MMR despite not having this problem, it hurts about the same number of queries as our techniques in the low and medium recall ranges due to its overall ineffectiveness. In addition, it helps significantly less queries compared to ours.
With respect to the relative improvement over the base-line, even though xQuAD has better Win/Loss ratios than PM-2 on the low and medium recall ranges, PM-2 still man-ages to provide larger improvement than xQuAD. Addition-ally, the gap between the two models becomes substantially larger in the high recall range. This provides additional ev-idences to support the effectiveness of PM-2.
 In summary, even though our proportionality-aware method PM-2 is very effective overall, it depends critically on the coverage of the baseline run.
Given that automatically generated aspects can be help-ful for diversification, it is important to know how to gener-ate them. Using query suggestions from commercial search engines in effect is using a  X  X lack box X  for this important component. Hence, this section aims to provide a prelim-inary discussion on whether we can use aspects generated by existing work in query suggestion and reformulation for diversification.

While most of those reformulation techniques focus on making user queries more effective [2, 18, 24, 20, 15], some aim to generate reformulations that cover different aspects of the original query [16]. We have adapted these techniques [16] to generate a set of clusters for each of our queries, where each cluster is assumed to represent an aspect of the original query. Details can be found in [16]. We concatenate all queries in each cluster to form a  X  X ocument X , from which we then construct a language model. Finally, we use Indri X  X  weighted query representation of this model as the aspect description and the frequency of the cluster as the popularity of the aspect. The resulting query set consists of 77 queries for which the reformulation technique can provide clusters.
We now re-evaluate all of our techniques using this set of aspects. The results are presented in Table 6. Interestingly, we observe that the performance of xQuAD, PM-1 and PM-2 is substantially lower than with the previous two aspect sets. We observe that this set of aspects, in comparison with the set obtained from the search engine, contains (1) considerably less of the TREC sub-topics and more of other aspects that are not identified by TREC X  X  assesors, and also (2) some unclear aspect descriptions. The low performance of all systems, in fact, clearly demonstrates the aspect mis-alignment issue and the noisiness of this set.

With these noisy aspects, PM-2 is still better than xQuAD with most of the metrics. The performance of xQuAD is, in fact, even lower than that of the query likelihood baseline in both CPR and  X  -NDCG. PM-2 still manages to provide improvement, but it is more comparable to MMR. Interest-ingly, PM-1 is now the best performing approach.

To conclude, aspects generated by the current query re-formulation technique are generally not very  X  X ffective X  for diversification. This notion of  X  X ffectiveness X , however, has to be taken with care. We have been penalizing all sys-tems for finding documents for aspects that are different to TREC sub-topics. In practice, these unjudged aspects might be relevant to the query as well. This raises the question of how reliable the current evaluation paradigm is that relies on pre-defining a fixed set of aspects for each queries. We will investigate this issue in future work.
In this paper, we present a different perspective on search result diversification: diversity by proportionality. We con-sider a result list to be more diverse if the number of doc-uments relevant to each of the aspects is proportional to the overall popularity of that aspect. We then propose Cumulative Proportionality (CPR), an effectiveness mea-sure for proportionality which is based on metrics commonly used for evaluating outcomes of elections. Motivated by the Sainte-Lagu  X  e method for assigning seats in a parliament to members of competing political parties, we also present a proportionality-driven framework for diversification. It se-quentially determines for each of the  X  X eats X  in the result list the aspect that best maintains the overall proportion-ality with respect to the previously selected topics. It then determines for this seat the best document with respect to that topic. Using this framework, we derive PM-1  X  a naive adaptation of the seat allocation mechanism, from which we then develop the probabilistic interpretation, which we called PM-2.

Our results have demonstrated that, with both manu-ally and automatically generated aspect descriptions, PM-2 is statistically significantly better than the top performing redundancy-based technique not only in CPR, but also on several other standard redundancy-based measures. This in-dicates that promoting proportionality will result in minimal redundancy, as desired by the current standard in diversity.
For future work, we will compare the aspects generated by existing reformulation techniques to the TREC sub-topics in order to quantify the aspect misalignment problem. If many of these misaligned aspects are indeed sensible, we might have to re-examine if predefining a set of topics for each query is a valid strategy for evaluating diversification tech-niques. In addition, Santos et al. has pointed out that learn-ing to dynamically provide different diversification strategies for different queries based on how ambiguous they are [27] and what intent they have [28] significantly improves the performance of xQuAD. We plan to investigate these ap-proaches since they are potentially beneficial for our model. improvement of each technique over the baseline query likelihood (QL). difference to Query-Likelihood, MMR and xQuAD respectively.
T his work was supported in part by the Center for Intelli-gent Information Retrieval, and in part by Vietnam Educa-tion Foundation. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
