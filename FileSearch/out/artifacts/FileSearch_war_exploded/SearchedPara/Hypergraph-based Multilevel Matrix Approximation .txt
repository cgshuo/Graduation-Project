 In Latent Semantic Indexing (LSI), a collection of docu-ments is often pre-processed to form a sparse term-document matrix, followed by a computation of a low-rank approxi-mation to the data matrix. A multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term-document matrix representing the data. The main goal is to reduce the cost of the matrix approximation without sacrificing accuracy. Because coarsening by multilevel hy-pergraph techniques is a form of clustering, the proposed approach can be regarded as a hybrid of factorization-based LSI and clustering-based LSI. Experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods; G.2.2 [ Graph Theory ]: Hypergraphs; F.2.1 [ Numerical Algorithms and Problems ]: Computations on matrices Algorithms, Experimentation Multilevel Hypergraph Partitioning, Low-rank Matrix Ap-proximation, Latent Semantic Indexing, Text Information Retrieval
Matrix approximation techniques appear in many fields, including data mining, machine learning, and computer vi-sion, where a set of data is converted into numerical form as This work was supported by NSF grants DMS 0510131 and DMS 0528492 and by the Minnesota Supercomputing Insti-tute.
 a matrix. The goal is to produce a low-rank matrix approx-imation, in order to extract the most important features of the data. On the other hand, there are instances such as in text mining when the original data itself is sparse. For such cases, there is a hypergraph ca nonically associated with the data. The hypergraph model combined with a multilevel approach, using coarsening among other tools, has had a re-markable success for scientific computing with sparse data, such as parallel sparse-matrix techniques [2, 16] and VLSI design [9, 10]. Motivated by these works, we explore multi-level matrix approximation methods for LSI, by exploiting hypergraph coarsening schemes.

The technique proposed in this paper recursively com-putes a coarsened version of the original hypergraph (i.e., one with fewer vertices) of the data set. This is performed with a method called maximal-weight matching which re-peatedly merges pairs of vertices, e.g., [5]. The coarsened hypergraph is then processed with a term weighting scheme and the resulting matrix is approximated by a matrix factor-ization algorithm before the query matching is performed. We refer to the resulting method as multilevel-LSI .
The factorization performed at the last level can be any of the common factorizations used in the literature. Three such factorizations will be explored in this paper: the singu-lar value decomposition (SVD), the semi-discrete decompo-sition (SDD) [11, 12], and the non-negative matrix factor-ization (NMF) [13, 14].
In the vector space model (VSM), a collection of n docu-ments indexed by m terms is usually pre-processed to form an m -by-n term-document matrix [1, 7]. In a simple form, the ( i, j )-th entry of the matrix is the number of occurrences of term i in document j , called term frequency. In informa-tion retrieval, a query is a column vector of size m reflecting the relevance of the query to the m terms. The objective is to retrieve the relevant documents from the collection.
Term weighting schemes are normally applied in order to enhance the retrieval effectiveness [3, 15]. We use f ij to de-note the term frequency, the number of occurrences of term i in document j ,and s ( f ij ) is a boolean set to one if term i occurs in document j ( f ij &gt; 0), and to zero otherwise. The  X  X eighted X  term-document matrix A =[ a ij ]  X  R m  X  n has a in the form a ij = t ij g i d j , representing some weight of term i in document j . The three components g i , t ij ,and d j for collection frequency (global term weight), term frequency (local term weight), and normalization, respectively. A term weighting scheme is represented by three letters. For exam-ple, the classical TF-IDF (term frequency, inverse document frequency) weight with normalization is coded as tfn , corre-sponding to the formula a ij = f ij log( n P is chosen to make weighting scheme a ij = 1 2 ( s ( f ij )+ f ij max
A weighting scheme is also applied to a query, but the collection frequency component (global weight) is from the collection of documents. The overall weighting is speci-fied by a six-letter string, three for document weights and three for query weights. In the experiments reported in Sec-tion 4, we used the tfn.cfx term weighting, which is an effective method for vector space model [15]. Some other term weighting formulas can be found in [15].

Query matching is the process of finding the documents in the collection relevant to a given query. The similarity of two vectors, a query q  X  R m and a document a  X  R m ,is measured by the cosine distance q T a/ ( q 2 a 2 ), the cosine of the acute angle between them.

Another approach for information retrieval is by hierar-chical agglomerative clustering, which forms a binary tree with leaves associated with documents and internal nodes representing clusters. To be precise, a node represents the cluster containing all the leaves. The retrieval is performed by a bottom-up search, until a preset number of documents are retrieved, e.g., [8].

In text mining, common word usage may affect the re-trieval performance of the vector space model. Two well-known issues are synonymy (two or more words with one meaning) and polysemy (one word with more than one mean-ing). The main assumption of Latent Semantic Indexing (LSI) is that there is an underlying latent semantic struc-ture that represents the contextual meaning of words rather than their literal usage. LSI uses the singular value decom-position (SVD) 1 of the term-document matrix to extract this intrinsic, or latent, structure [4]. Attempts have been made to replace the SVD by other factorizations, such as semi-discrete decomposition (SDD) [11]. In either case, the ap-proximation of the term-document matrix can be expressed in the form A  X  A d = U d  X  d V T d  X  R m  X  n ,where X  d  X  R d  X  d is non-negative and diagonal, U  X  R m  X  d , V  X  R n  X  d ,with d a certain desired number of vectors to approximate A .For SVD, U and V consist of orthogonal columns. For SDD, the entries of U and V arediscretevaluesfrom { X  1 , 0 , 1 } .
When the document vectors (columns in A  X  R m  X  n )and the query vector q  X  R m are normalized, or when the in-ner product measure instead of cosine distance measure is utilized, the similarity scores are the entries of q T A in the vector space model. Replacing A by A d = U d  X  d V T d , where  X  is a scalar which controls the splitting of the matrix approximation [11]. The reduced representations of query b and document vectors (columns in b A ) are defined by
Assuming U d consists of linearly independent columns, b A  X  R d  X  n is a linear projection of A d  X  R m  X  n : Strictly, we mean  X  X runcated X  singular value decomposition. The word  X  X runcated X  is omitted as there is no ambiguity. where U + d  X  R d  X  m is the pseudo-inverse of U d  X  R m  X  d Another way to obtain the reduced representation of a query q  X  R m is by applying the projector P = X   X   X  q . The resulting formula is
We call (4) the Type-I LSI and (2) the Type-II LSI. In both cases, one can optionally renormalize b q and the columns of b
A . Renormalization usually slightly improves the retrieval performance [11]. As in the case of the vector space model, a document is deemed relevant to a query if their similarity score is larger than some pre-defined threshold.
Consider the concept decomposition [6] in the form C k b Z from clustering the columns of A  X  R m  X  n ,where k is the number of clusters, each column of C k  X  R m  X  k is the cen-troid of a cluster, and b Z k  X  R k  X  n is chosen to minimize
A  X  C k b Z k 2 F . One way to further reduce the dimensional-ity is to compute a low-rank approximation of C k . Following the notation in Section 2, we denote this decomposition by C whereweset Z d = b  X  d b V T d b Z k .
 When b U d is of full column rank, the minimizer of (5) is Z The term-document matrix approximation is then
The similarity scores of a query q to the documents of A are computed by q T ( b U d b U + d A )=( q T b U d b  X   X  which we obtain the reduced representations of q and A , Alternatively, we can apply the same linear transformation b  X  In both (6) and (7), one can optionally renormalize the pro-jected document vectors for similarity score computations.
Two considerations of the hybrid approach presented here are as follows. First, the clustering result depends on the term weighting scheme applied to the term-document ma-trix. The centroid of documents scaled by term weighting loses some intrinsic information, specifically the count of oc-currences of each term. Second, some clustering algorithms, such as the spherical k -means clustering, need the number of clusters k in advance, which may not be easy to deter-mine in practice. Also, changing the value of k may require recomputing the approximation. We present a multilevel technique using hypergraphs for matrix approximation to address these two issues.

A hypergraph H =( V, E ) consists of a set of vertices V and a set of hyperedges E . Each hyperedge, also called a net , is a non-empty subset of V ; the size of this subset is called the degree of this hyperedge. Likewise, the degree of a vertex is the number of hyperedges which include it. Two vertices are called neighbors if there is a hyperedge connecting them.
A hypergraph H =( V, E ) can be canonically represented by a boolean matrix A , where the vertices in V and hy-peredges (nets) in E are represented by the columns and rows of A , respectively. This is called the row-net model . Each hyperedge, a row of A , connects the vertices whose corresponding entries in that row are non-zero. For exam-ple, V = { 1 ,..., 9 } and E = { a,...,e } with a = { 1 , 2 , 3 , 4 b = { 3 , 5 , 6 , 7 } , c = { 4 , 7 , 8 , 9 } , d = { 6 , 7 , 8 The boolean matrix representation of this hypergraph is
Coarsening a hypergraph H =( V, E ) means finding a  X  X oarse X  approximation  X  H =(  X  V,  X  E )to H with |  X  which is a reduced representation of the original hypergraph H , in that it retains as much of the structure of the original hypergraph as possible. By recu rsively coarsening we obtain a succession of smaller hypergraphs which approximate the original graph. Several methods exist for coarsening hyper-graphs [2, 10]. The method used in this paper is based on merging pairs of vertices.

In order to select which pairs of vertices to merge in a hy-pergraph, we consider the maximum-weight matching prob-lem, e.g., [2, 5]. Pairing two vertices is termed matching . The edge weight between two vertices is the number of hy-peredges connecting them. In the hyperedge-vertex matrix representation (a boolean matrix), the weight of the pair i, j (vertices) is the inner product of the two columns i and j .
This inner-product weight is adopted as a similarity met-ric in two software packages for hypergraph partitioning, hMETIS [9] and Mondriaan [16]. The maximum-weight matching problem consists of finding a matching that max-imizes the sum of edge weights of the vertex pairs. In prac-tice, it is not necessary to find the optimal solution, as sub-optimal greedy approaches yield satisfactory results [5].
A greedy algorithm for maximum-weight matching is used in our experiments. For each unmatched vertex v ,allthe unmatched neighbor vertices u are considered, and the inner product between v and each u is computed. The vertex u with the highest non-zero inner product u, v is matched with v and the procedure is repeated until all vertices have been matched. The computed matching is a coarse repre-sentation of the hypergraph, with the coarse hyperedges in-herited from the fine hypergraph. More precisely, the coarse vertex set consists of matched pairs of fine vertices. A pair of fine vertices is in a coarse hyperedge if any of the two ver-tices is in the corresponding fine hyperedge. It is convenient to present the hypergraph coarsening procedure in matrix form as in Algorithm 1.

Given a sparse term-document matrix A , we can use Algo-rithm 1 to recursively coarsen t he corresponding hypergraph in the row-net model level by level, and obtain a sequence of sparse matrices A 1 ,A 2 ,...,A r with A 1 = A ,where A corresponds to the coarse graph H i of level i .

For the quality of the clusters, a matrix of term frequen-cies is usually preprocessed by a term weighting scheme be-{ Coarsen a hypergraph A  X  R m  X  n . } p := 0 # vertices already in the coarsened graph repeat until S =  X  { The coarse hypergraph is represented by  X  A  X  R m  X  p . fore applying a clustering algorithm, such as the spherical k -means algorithm. However, the clustering result by multi-level hypergraph coarsening is independent of term weight-ing, since only the sparsity patterns of matrices are taken into account during hypergraph coarsening.

We take a matrix of term frequencies as input A .The coarsening result A r of the lowest level r has entries still being term frequencies, since each column of A r is a sum of multiple columns in A . We then apply a term weighting scheme to A r to form e A r , followed by a low-rank approxi-mation e A r  X  e U d e  X  d e V T d =( e U d e  X   X  d )( e  X  or NMF, where d is the desired number of basis vectors, and  X  is the splitting scalar. Following the discussion leading to (6) and (7), we obtain the Type-I multilevel-LSI formula and the Type-II multilevel-LSI formula where a term weighting scheme has been applied to the query e q  X  R m and the term-document matrix e A  X  R m  X  n .For consistency, the global weights for e q and e A , if any, follow those applied to e A r .

Finally, the query matching is made by similarity scores between the query b q and documents as column vectors of b in the reduced form. A key property of our multilevel-LSI is that the term weighting is applied posterior to multilevel hypergraph coarsening. We used a Linux machine equipped with an Intel Core 2 Duo 2.4GHz CPU for our experiments. The codes are all in Matlab, including the public libraries SDD-PACK [12] and the NMF implementation [14]. We used the Lanczos al-gorithm with full reorthogonalization for SVD computation. The standard evaluation metric average precision is adopted to measure the retrieval performance. Three public data sets were used in our experiments 2 : Medline , Cranfield ,and NPL . The characteristics of these sets are listed in Table 1.
 # documents 1033 1398 11429 # terms 3681 2331 4322 # of terms/document 48.36 52.11 19.68
Sparsity (%) 1.314% 2.235% 0.455% # queries 30 225 93 # of terms/query 9.90 9.12 7.14 # of relevances/query 23.20 8.17 22.40
To see the effect of our multilevel scheme on CPU time and retrieval performance, we report the hypergraph coars-ening time, factorization time, and mean average precision using the Medline , Cranfield ,and NPL data sets, with d = 100 , 200 , 500 in Tables 2 X 4, respectively. We adopted the term weighting scheme tfn.cfx , and used the Type-II meth-ods with the splitting parameter  X  =0.

Table 2: Medline results (Type-II,  X  =0 , d = 100 ).
Table 3: Cranfield results (Type-II,  X  =0 , d = 200 ).
As shown in Tables 2 X 4, we obtained improved or com-parable retrieval performance at a reduced cost. Our multi-level technique significantly reduced the CPU time for NMF-based LSI, and dramatically improved the retrieval result of the SDD-based LSI. Note however that we used the term weighting scheme tfn.cfx that involves global term weights, which are discouraged for SDD-based LSI [11].
The hypergraph-based multilevel framework presented in this paper is applicable to any situation where the data ma-trix is sparse, since the pattern of non-zero entries of the sparse matrix yields a hypergraph. The graph is coarsened, with a known scheme such as maximal-weight matching, which merges pairs of vertices. Then a matrix approxima-tion technique, e.g., SVD, is then performed on the coars-ened data matrix at the lowest (coarsest) level. The corre-sponding projection is then applied to the original matrix. ftp://ftp.cs.cornell.edu/pub/smart In text mining, the term-document matrices are sparse. Therefore, this hypergraph-based multilevel technique is ide-ally suited for the latent semantic indexing (LSI) approach. We call the resulting method multilevel-LSI. The experi-ments showed good retrieval improvement at reduced cost. [1] M.W.BerryandM.Browne. Understanding Search [2] U.V.CatalyurekandC.Aykanat.
 [3] E. Chisholm and T. G. Kolda. New term weighting [4] S.Deerwester,S.T.Dumais,G.W.Furnas,T.K.
 [5] K. Devine, E. G. Boman, R. Heaphy, R. Bisseling, and [6] I. S. Dhillon and D. S. Modha. Concept [7] L. Eld  X  en. Matrix Methods in Data Mining and Pattern [8] N. Jardine and C. J. van Rijsbergen. The use of [9] G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. [10] G. Karypis and V. Kumar. Multilevel k -way [11] T. G. Kolda and D. P. O X  X eary. A semi-discrete [12] T. G. Kolda and D. P. O X  X eary. Algorithm 805: [13] D. D. Lee and H. S. Seung. Learning the parts of [14] C.-J. Lin. Projected gradient methods for [15] G. Salton and C. Buckley. Term-weighting approaches [16] B. Vastenhouw and R. H. Bisseling. A
