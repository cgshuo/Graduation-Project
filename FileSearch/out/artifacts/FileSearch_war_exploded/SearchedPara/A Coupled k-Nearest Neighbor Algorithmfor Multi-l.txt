 Although traditional single-label classification approaches have been proved to be successful in handling some real world problems, for the problems which the objects not fit the single-label rule, they may not work well, for example, in image classification, an image may contain several concepts simultaneously, such as beach, sunset and kangaroo. Such tasks are usually denoted as multi-label classification problems. In fact, a conventional single-label classification problem can simply be taken as a special case of the multi-label classification problem where there has only one label in the class label space. Multi-label classification problems exist in many domains, for example, in automatic text categorization, a document can associate with several topics, such as arts, history and Archeology; and in gene functional analysis of bio-informatics, a gene can belong to both metabolism and transcription classes; and in music categorization, a song may labeled as Mozart and sad.
 In the last decades, there have been a variety of methods developed for multi-label classifications. These methods are generally grouped into two categories: One is the problem transformation methods and another is the algorithm adap-tation methods. Problem transformation methods first transform the multi-label learning tasks into multiple single-label learning tasks, which are then handled by the standard single-label learning algorithms. Another approach is called algorithm adaptation method, which modifies existing single-label learning algo-rithms in order to extend its ability to handle multi-label data, such as ML-k NN [ 17 ], IBLR [ 7 ], BSVM [ 2 ], and BP-MLL [ 16 ].
 classification problem, such as ML-k NN. ML-k NN applies maximum a posteriori principle for classification and ranking, and the likelihood is estimated by using the k nearest neighbors of an instance. Although simple and powerful, there are some shortcomings in its processing strategy. ML-k NN uses the popular binary relevance (BR) strategy [ 13 ], which may transfer the problem into many class-imbalance tasks, and then tend to degrade the performance of the classifiers. Another problem of it is the estimation of the posteriori may be affected by the facts that the instances with and without a particular label are typically highly imbalanced. Furthermore, its ignorance of the inter relationship between labels is another issue which limits its usage. Such relationship is described as analysis the coupling relationship on categorical data. These works all proved the effectiveness of considering the dependency between different attributes. (CML-k NN for short) based on non-iidness [ 5 ]. The major contribution of this paper is summarized as follows: -We propose a novel multi-label learning algorithm that based on lazy learning -We introduce a new coupled label similarity for multi-label k NN algorithm. -We extended the concept of the nearest neighbor in multi-label classification the ML-k NN algorithm. Preliminary definitions are specified in Section 3.1 .And section 3 gives a detailed description of the new algorithm we proposed. The experimental results are discussed in Section 4 . Finally, the conclusion is dis-cussed in Section 5 . A number of multi-label learning methods are adapted from k NN [ 3 , 11 , 15 , 17 ]. ML-k NN, the first multi-label lazy learning approach, is based on the traditional k NN algorithm and the maximum a posteriori (MAP) principle [ 17 ]. on the number of neighbors that possess identical labels. Given an instance x with an unknown label set L ( x )  X  L ,ML-k NN first identifies the k nearest neighbors in the training data and counts the number of neighbors belonging to each class (i.e. a variable z from0to k ). Then the maximum a posteriori principle is used to determine the label set for the test instance. The posterior probability of l i  X  L is given by where z is the number of neighbors belonging to each class (0 for each label l i  X  L , the algorithm builds a classifier h means it does not. The prior and likelihood probabilities in Eq. 1 are estimated from the training data set in advance.
 ML-k NN has two inheriting merits from both lazy learning and MAP princi-ple: One is the decision boundary can be adaptively adjusted due to the varying neighbors identified for each new instance, and another one is the class-imbalance issue can be largely mitigated due to the prior probabilities estimated for each class label. However, ML-k NN is actually a binary relevance classifier, because it learns a single classifier h i for each label independently. In other words, it does not consider the correlations between different labels. The algorithm is often criticized because of this drawback. 3.1 Problem Statement We formally define the multi-label classification problem as this: Let X denotes the space of instances and Y = { l 1 ,...,l n } denotes the whole label set where |
Y | = n . T = { ( x 1 ,L ( x 1 )) ,..., ( x m ,L ( x m )) } ( ing data set, whose instances are drawn identically and independently from an unknown distribution D . Each instance x  X  X is associated with a label set L ( x )  X  Y . The goal of our multi-label classification is to get a classifier h : X  X  Y that maps a feature vector to a set of labels, while optimizing some specific evaluation metrics. 3.2 Coupled Label Similarity It is much easier for numerical data to calculate the distance or similarity, since the existing metrics such as Manhattan distance and Euclidean distance are mainly built for numeric variables, but the labels are categorical data. How to denote the similarity between them is a big issue. As we all know, matching and frequency [ 1 ] are the most common ways to measure the similarity of cate-gorical data. Accordingly, two popular similarity measures are defined: For two categorical value v i and v j , the Overlap Similarity is defined as and the Frequency Based Cosine Similarity between two vectors V defined as The overlap similarity between two categorical values is to assign 1 if they are identical otherwise 0 if different. Further, for two multivariate categorical data points, the similarity between them will be proportional to the number of features in which they match. While for frequency based measures, they assume the different categorical values but with the same occurrence times as the same. by just giving the equal importance to matches and mismatches. The co-occurrence information in categorical data reflects the interaction between features and can be used to define what makes two categorical values more or less similar. However, such co-occurrence information hasn X  X  been incorporated into the existing similar-ity metrics.
 Intra-Coupling Label Similarity (IaCLS) and an Inter-Coupling Label Similarity (IeCLS) below to capture the interaction of two label values from two different labels.
 Definition 1 . Given a training multi-label data set D and two different labels l and l j ( i = j ) , the label value is v x i ,v y j respectively. The Intra-Coupling Label Similarity (IaCLS) between label values v x i and v formalized as: where RF ( v x i ) and RF ( v y j ) are the occurrence frequency of label value v in label l i and l j , respectively.
 label values in the label space. The higher these similarities are, the closer such two values are. Thus, Equation ( 5 ) is designed to capture the label value similar-ity in terms of occurrence times by taking into account the frequencies of cate-gories. Besides, since 1  X  RF ( v x i ) ,RF ( v y j )  X  m , then  X  Similarity below to capture the interaction of two different label values according to the co-occurrence of some value (or discretized value group) from feature spaces. Definition 2 . Given a training multi-label data set D and two different labels l and l j ( i = j ) , the label value is v x i ,v y j respectively. v to be Inter-Coupling related if there exists at least one pair value ( v that occurs in feature a z and labels of instance U p .The Inter-Coupling Label Similarity (IeCLS) between label values v x i and v y i according to feature value v of feature a z is formalized as: value pair v zx p or v zy p ,and RF ( v x i ) and RF ( v y related class label. v z p is the value in categorical feature a group in numerical feature a z .
 Accordingly, we have  X  Ie  X  [0 , 1]. The Inter-Coupling Label Similarity reflects the interaction or relationship of two label values from label space but based on the connection to some other features.
 Definition 3 . By taking into account both the Intra-Coupling and the Inter-Coupling, the Coupled Label Similarity (CLS) between two label values v and v j is formalized as: where v x i and v y j are the label values of label l i and l  X 
Inter are the intra-coupling label similarity (Eq. 5 ) and inter-coupling label sim-ilarity (Eq. 6 ), respectively. The n is the number of attributes and v the values in the k th feature a k .
 The Coupled Label Similarity defined in Eq. 7 reflects the interaction or similarity of two different labels. The higher the CLS , the more similar two labels be. In Table 1 , for example, CLS ( l 1 ,l 4 )=0 . 33, CLS ( l the data set, an instance with label l 4 is more similar or close to instances with label l 1 than those instances with label l 3 do. That is to say, label pair ( l label similarity array which showed in Table 2 .
 3.3 Extended Nearest Neighbors Based on the Coupled Label Similarity, we introduce our extended nearest neigh-bors. Based on the similarity between labels, we can transfer a label set into a set with only a certain label, it also means a multi-label instance can be extended to formed into a set with only one label l b . For example, in Table 1 , instance u be transformed into { 1  X  l 2 , 0 . 5  X  l 2 , 0 . 33  X  l 2 with a label of { 1 . 83  X  l 2 | l 2 } .
 label l 2 , and vice versa, instance u 5 also presents there are (1 (1  X  0 . 33) = 1 . 17 instances which not contain the label l nearest neighbors. 3.4 Coupled ML-k NN identified in data set D . For the j -th class label, CML-k NN chooses to calculate the following statistics: Where L i is the label set of the i -th neighbor and L i  X  the sum of the CLS values of the i -th neighbor X  X  label set to the j -th label l and Round () is the rounding function. Namely, C j is a rounding number which records all the CLS value of all x  X  X  neighbors to label l j .
 Let H j be the event that x has label l j ,and P ( H j | C probability that H j holds under the condition that x has exactly C that H j doesn X  X  hold under the same condition. According to the MAP rule, the predicted label set is determined by deciding whether P ( H P (  X  H j | C j ) or not: According to the Bayes Theory, we have: Here, P ( H j )and P (  X  H j ) represents the prior probability that H and doesn X  X  hold. Furthermore, P ( C j | H j ) represents the likelihood that x has exactly C j neighbors with label l j when H j holds, and ( P ( Cj the likelihood that x has exactly C j neighbors with label l hold.
 When we count the prior probabilities, we integrated our coupled label sim-ilarity into the process: where (1  X  j  X  n )and m is the records number in training set, and s is a smoothing parameter controlling the effect of uniform prior on the estimation which generally takes the value of 1 (resulting in Laplace smoothing). Same as ML-k NN, for the j -th class label l j ,ourCML-k NN maintains two frequency arrays  X  j and  X  j . As our method considers the other labels which have a similarity to a specific label, the frequency arrays will contain k elements: Where (0  X  r  X  k  X  n ). We take an instance with  X  L  X  which does have label j and we take an instance with  X  L  X  which doesn X  X  have label j . Therefore,  X  j [ r ] counts the sum of CLS values to label j of training examples which have label l j and have exactly r neighbors don X  X  have label l j and have exactly r neighbors with label l likelihood can be estimated based on elements in  X  j and  X  into Eq.( 10 ), we will get the predicted label set in Eq.( 9 ).
 Algorithm 1. Coupled ML-k NN Algorithm 3.5 Algorithm Given an unknown test instance x t , the algorithm determines the final label set of the instance. Algorithm 1 illustrates the main idea of our process. Our proposed CML-k NN contains of six main parts. a)Maintain the label similarity array; b)Finding the nearest neighbors for every instance in training set; c)Getting the prior probabilities and frequency arrays; d)Finding the nearest neighbors for the target instance; e)Calculate the statistics value; f)Calculate the result. and maintain the Coupled Label Similarity Array A ( L ) from the training data set. Secondly, for every training instance, we identify its traditional k nearest neighbors. After that, for every different label, we calculate its prior probability which combined with CLS . Simultaneously, we expand the neighbors set for every instance to a new label-coupled neighbors set using the CLS , and calculate the frequency array for every label. After these works done, we identify the k neighbors of the test instance x t . After applying CLS on this neighbor set and calculate the label statistics, we can finally get the predicted label set. It is worth noting that our key idea is the label similarity, which tries to learn the label distance and then transfer any label into a specific label. 4.1 Experiment Data A total of eight commonly used multi-label data sets are tested for experiments in this study, and the statistics of the data sets are shown in Table 4 . Given a multi-label data set M = { ( x i ,L i ) | 1  X  i  X  q } ,weuse to represent the number of instances, number of features, number of total labels, and feature type respectively. In addition, several multi-label statistics [ 9 ]are also shown in the Table. The Label cardinality ( LC ( M )) measures the average number of labels per example; the Label density ( LD ( M )) normalizes LC ( M ) by the number of possible labels; the Distinct label sets ( DL ( M )) counts the number of distinct label combinations appeared in the data set; the Proportion of distinct label sets ( PDL ( M )) which normalizes DL ( M ) by the number of instances. As shown in Table 4 , eight data sets are included and are ordered by Label density LD ( M ). 4.2 Experiment Setup In our experiments, we compare the performance of our proposed CML-k NN with that some state-of-the-art multi-label classification algorithms: ML-k NN, IBLR and BSVM. All nearest neighbor based algorithms are parameterized by the size of the neighborhood k . We repeat the experiments with k =5 , 7 , 9 respectively (odd number for voting), and use the Euclidean metric as the distance function when computing the nearest neighbors. For BSVM, models are learned via the cross-training strategy[ 2 ]. We also choose the BR-k NN as the basic algorithm to compare with. We perform 10-fold cross-validation three times on all the above data sets.
 4.3 Evaluation Criteria Multi-label classification requires different metrics than those used in traditional single-label classification. A lot of criteria have been proposed for evaluating the performance of multi-label classification algorithms [ 12 ]. In this paper, we use three popular evaluation criteria for multi-label classification: the Hamming Loss ,the One Error and the Average Precision . The definitions of them can be found in [ 10 ]. 4.4 Experiment Results The experiment results are shown in Table 5 -Table 7 . For each evaluation criterion,  X   X   X  indicates  X  X he smaller the better X , while  X  the better X . And the numbers in parentheses denote the rank of the algorithms among the five compared algorithms.
 algorithms significantly, which implies that exploiting the frequency of neigh-bors X  label is effective, and especially for our CML-k NN, the improvement is sig-nificant compared to BR-k NN, that means incorporating the label relationship will greatly improve the BR strategy. Meanwhile, ML-k NN, IBLR and BR-k NN do not perform as well compared to the other algorithms. This implies that only exploiting the exact neighbor information is not sufficient, and the similar neighbor (correlations between labels) should also be considered.
 Overall, our proposed CML-k NN outperforms all the compared methods on all three measures. The average ranking of our method on these data sets using the second best algorithm, BSVM, only achieves (2.50, 2.38, 2.25). The BR-k NN performs the worst, which only achieves (4.13,4.25,4.75).
 It is worth noting that although our proposed method runs the best on aver-age, it does not mean that it is suitable for all kinds of data. For example, when used on data set  X  X nron X  and  X  X enbase X , the result is not as good as on other data sets. Sometimes it even got a worse result than BR-k NN. For exam-ple, when used on  X  X nron X  and evaluated by the Hamming Loss, our supposed CML-k NN only achieved a 4th rank(0.061), while BR-k NN can get a second well result(0.052). The reason is because of the weak or loose connection between dif-ferent labels in those data sets, and our extended neighbors may introduce more noisy information than useful information. But in terms of average performance, our method performs the best (the first rank). ML-k NN learns a single classifier h i for each label l i tions between different labels. The algorithm is often criticized for this drawback. In this paper, we introduced a coupled label similarity, which explores the inner-relationship between different labels in multi-label classification according to their natural co-occupance. This similarity reflects the distance of the different labels. Furthermore, by integrating this similarity into the multi-label k NN algo-rithm, we overcome the ML-k NN X  X  shortcoming and improved the performance. Evaluated over three commonly-used multi-label data sets and in terms of Ham-ming Loss, One Error and Average Precision, the proposed method outperforms ML-K NN, BR-k NN, IBLR and even BSVM. This result shows that our sup-posed coupled label similarity is appropriate for multi-label learning problems and can work more effectively than other methods.
 ical multi-label data, and even mixed type multi-label data for which current numerical distance metrics is not suitable.

