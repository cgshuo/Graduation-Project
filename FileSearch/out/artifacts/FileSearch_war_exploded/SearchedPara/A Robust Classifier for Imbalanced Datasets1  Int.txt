 In machine learning, the classification problem aims to predict class labels of new unseen examples on the basis of previously observed training datasets. Many methods have been proposed to gene rate high accuracy classifiers, but most classification methods show good performance on balanced class problems -the number of instances of classes is balanced -while yielding relatively poor performance on imbalanced class problems.

Imbalanced class distribution -one class (majority class, denoted as  X - X ) vastly outnumbers the other class (minority class, denoted as  X + X ) in training datasets -hinders the accuracy of classification of minority class, since typical classification algorithms, such as decision trees, intend to maximize the overall prediction accuracy and tend to have bias toward the majority class [1]. The class imbalance problem, however, is important, since imbalanced datasets are prevalent in real world (e.g. fraud/intrusion detection, medical diagnosis/monitoring) and the cost of misclassification for a minority class is usually much higher in many cases. For instance, since the examples of patient with a rare cancer are relatively very small, the classifier usually has a poor ability to predict the rare cancer. Therefore, the classifier can simply classify the patient with a rare cancer into the patient with a common cancer while th e classifier keeps high accuracy. When such a misclassification happens, however, the misclassified patient may suffer from misdiagnosis.

There have been various approaches to tackle the class imbalance problems; kernel modification methods, sampling methods, and cost-sensitive methods. In this paper, we focus on methods that can apply to Decision Tree Induction Clas-sification, as decision tree is one of the most effective methods for classification [2]. Based on the decision tree, there have been many studies that showed per-formance improvement on imbalanced da tasets. However, there is still a room for improvement.

In this paper, we propose Hellinger Distance Extra Decision Tree (HeDEx) that employs Hellinger Distance as a split criterion and builds extremely ran-domized ensemble trees. Hellinger Distance Extra Decision Tree is named after Hellinger Distance Decision Tree [3] and Extra-Trees [4]. We also propose a novel oversampling method that helps not only our proposed decision tree but also other existing classifiers to get better performance for minority class. Our experiments show that HeDEx has gener ally better performance than other ex-isting decision tree methods in terms of AUC and F-Measure for minority class. The proposed oversampling method improves the performance of F-Measure for minority class. Information gain and Gini index are used as the splitting criteria for the popu-lar decision trees such as C4.5 and CART, respectively. However, several stud-ies [5][6][7] have shown that these measures are skew-sensitive so that they have bias toward majority class. Equation 1 and 2 denote Entropy and Information Gain for binary class and binary split, respectively. In order to maximize the in-formation we need to minimize the second term of Equation 2, since the first term Entropy ( S ) is fixed for the dataset S. The second term of Equation 2 is denoted Therefore, the minority class could not influence equally on Entropy like majority class, since | S 1+ | and | S 2+ | is relatively small in class imbalanced datasets. The classifier that uses Gini Index (Equation 4) also has the same problem. The stud-ies also shows the skew-sensitivity of Inf ormation Gain using isometric form [8][3].
Cieslak et al. [3] proposed Hellinger Distance as a splitting criterion that is skew-insenstive. Equation 5 shows Hellinger Distance, and it shows that the class priors do not influence the distance calculation. Thus the minority class would not be ignored on distance calcuation. The experiments of Cieslak et al. showed that Hellinger Distance Decision Tree (HDDT) outperforms C4.4 -unpruned, uncollapsed C4.5 with Laplace smoothing -[3] in imbalanced class problems. Sampling methods are typically used to tackle imbalanced class problems. The experimental studies[9] have shown that using sampling methods gener-ally improve the classifier performance. The sampling methods alter the class distribution to make the class distribution balanced.

Synthetic Minority Oversampling TEchnique (SMOTE) [10] takes the same approach with One-Sided Selection; mitigating bias toward majority class. The difference between two methods is that while One-Sided Selection removes the majority class data, SMOTE proposes creating synthesized minority class ex-amples. Synthetic examples are gener ated by selecting a random point along the line between two minority class examples. Chawla et al. shows that this synthesized examples facilitate larger decision regions in feature space for mi-nority class avoiding overfitting. Despite improved performance of SMOTE, the problem of SMOTE is overgeneralization, which means the region enlarged for minority class could be blindly generali zed so that synthetic instances can lead to overlapping between classes. Many other synthetic sampling methods based on SMOTE have been proposed to address the overgeneralization problem of SMOTE; Border-line SMOTE [11], Safe-Level SMOTE [12], and LN-SMOTE [13].

Cluster-Based Oversampling [14] is the other method to solve the small dis-junction problem by using clustering approach. It clusters the training data of each class separately and oversamples the minority instances per each cluster. In this idea, the new generated minority instances cannot be located in some ma-jority class cluster. Thus, by clustering approach, it can handle both inter-class imbalance and between-class imbalance simultaneously.

The other approach for oversampling is pattern-based synthetic method. Al-hammady et al. [15] proposed Emerging Patterns Decision Tree (EPDT) that is decision tree induction classifier using Emerging Patterns of minority class for generating new minority class instances. Emerging Pattern of minority class is a pattern whose support changes significantly from majority class to minority class in the training dataset.

Ensemble method is known to be very efficient to reduce variance by building multiple classifiers and averaging the classifiers output so that it allows us not to choose the classifier with a poor perfor mance. There have been many ensemble methods based on Bagging [16] and Boosting [17]. In this review, however, we focus on Randomized ensemble trees that use a subset of attributes, since our method is based on Random Subspace [18].
 Instead of using all attributes to find the optimal split-point at a tree node, Random Subspace and Random Forests randomly select a subset of attributes at a tree node and find the optimal split-point among only these selected attributes. The level of randomization is determined by the number K of attributes to be chosen at each node. The difference between two methods is the formation of the training examples of each tree. Random Subspace uses the entire training dataset for each tree so that only the randomly chosen attributes impact the variability of base classifiers. On the other hand, Random Forests is based on Bagging [16] -random sampling of training dataset with replacement -so that both attribute selection and training data impact the variability.

Extra-Trees [4] was proposed as extr emely randomized tr ees. It randomly selects not only the number K of attributes but also a candidate split-point for each chosen attribute. The split-point at a tree node is determined among the number K of candidate split-points so that the split-point is sub-optimal for a corresponding node. Therefore, among three methods (Random Subspace, Random Forests, and Extra-Trees) Extra-Trees has the highest randomness. The advantage of Extra-Trees is computational efficiency while keeping comparative accuracy, reducing variance and slightly increasing bias. In this paper, we propose Hellinger Distance Extra Decision Tree (HeDEx) that employs Hellinger Distance as a splitting criterion and build extremely random-ized ensemble trees using entire trainin g dataset for imbalanced dataset prob-lems. HeDEx is basically based on Extra -Trees [4]. The main differences with other ensemble methods are that Extra-Trees and HeDEx randomly choose not only the attributes but also split-points for splitting the tree nodes and use the entire training examples (rather than a bootstrap replica) to build each base classifier. Due to the randomization on both attribute selection and split-point selection, Extra-Trees and HeDEx can ach ieve high level of varierity of trees even without sampling of training examples (bootstrap replicas).

The two main differences with Extra-Trees and HeDEx are splitting-criterion and the number of candidate split-points. Extra-Trees selects one candidate split-point for each attribute, while HeDEx selects more than one split-points, since we found from experiments that considering multiple candidate split-points to find a sub-optimal split-point showed better a ccuracy. The other difference is splitting criterion. HeDEx uses Hellinger Distance for skew-insensitiveness, while Extra-Trees uses a score measure based on information gain. We provide the comparison table of existing random ensemble methods in Table 1. Notice that although the number of split-points c of HeDEx becomes equal to the number of values of the corresponding attribute in learning samples, HeDEx is different from Random Subspace. This is because HeDEx draws the split-points independently from the values in the learning samples.

We use weighted method as the aggregation rule for our HeDEx. Equation 6 shows how to predict the class label of a test example. C in Equation 6 is a set such that c  X  C p t i ,c = 1 for each tree. 3.1 Variant of Hellinger Distance Extra Decision Tree We also propose a variant of HeDEx that employs different decision boundary from the original HeDEx. Hellinger Distance Extra Hyperplane Decision Tree (HeDExh) uses an arbitrary hyperplane a s a decision surface, thus selects the optimal hyperplane at a node among K candidate hyperplane decision bound-aries. To be brief, we could say that HeDEx uses single variable inequations when it tries to split the node according to the split-point, while HeDExh uses two variable inequations when it tries to split the node. The arbitrary hyperlane is determined by choosing two different p oints from feature space of the dataset; The number of candidate hyperplane for a pair of chosen attributes is also an option parameter, C. The number of attributes to be used for hyperplane could be more than two. In this paper, we present 2-dimensional hyperplane only but the dimensionality can be extended. In this section, we propose the new method for generating minority instances based on patterns that are detected from HeDEx. This process consists of three parts; pattern detection, instance generation, and instance validation. 4.1 Pattern Detection The main idea of pattern detection is tha t in decision tree the splitting criteria at each node of tree represent the patterns of the examples at the node. The patterns are detected from HeDEx that are built using the training samples. As HeDEx builds sub-optimal trees, it has a variety of patterns that are not highly coupled with training examples. After building HeDEx, we look at the leaf node of each tree, and if the number of minori ty instances is greater than that of majority, we build a pattern according to the split rules from root to the leaf node. For example, from the trees in Figure 1, we can build patterns; { a 1 &gt; 10 and a 3  X  4 } and { a 2 /  X  X  0 , 1 } and a 4 /  X  X  y }} , respectively.
The patterns detected from multiple He DEx trees get scores with respect to the strength of the pattern, and a pattern with higher score has higher probabil-ity of being selected for generating n ew instances. The equations 7 and 8 show how to get strength score of a pattern. For example, for the pattern detected minority class.
 4.2 Instance Generation New instances are generated based on t he patterns. If the selected pattern does not cover all attributes, the values of missing attributes are generated at random but based on the distribution of the values of the training examples. Figure 2 shows an example of generating a new instance. As the pattern 1 has the rule { a 1 &gt; 10 and a 3 rule; 10 &lt;v 1  X  max ( a 1 )and min ( a 3 )  X  v 3  X  4. For the remaining attributes, we randomly draw the values such that v i  X  histogram ( a i )for i th attribute.
For the nominal attributes, we select the attribute values at random according to the rule. For example, the second pattern of Figure 1 is { a 2 /  X  X  0 , 1 } and a of a 4 , is drawn uniformly from Set a 4 \{ y } where Set a i is the set of attribute a  X  X  distinct values. For remaining attributes, we randomly draw each value such that v i  X  histogram ( a i )for i th attribute. 4.3 Instance Validation We propose the instance validation phase to ensure that the newly generated instances are not noisy for minority class. The main idea of this validation phase is that if a classifier cannot distinguish instances of two different classes well, which can mean that the instances of two different classes are similar.
After generating new instances, we mak e a synthetic training dataset using the original minority instances and newly generated instances to validate new instances. We set the original minority i nstances as positive class and the new instances as negative class. We build a Hellinger Distance Decision Tree and visit each leaf node to check the instance distribution. Figure 3 shows the example. The right-most leaf node illustrates that the negative instances are not distin-guishable from the positive instances, so we can add these negative instances (newly generated instances) as the instances for minority class to the training examples for building a classifier. On the other hand, the left most node con-tains only the negative instances, which implies that the three negative instances are well distinguishable from the positive class instances so that the three in-stances could become noisy if we add thes e instances to training examples. In that case, we do not include such instances for our training examples. Notice that we employ Hellinger Distance Decision Tree for validating instances, since HDDT builds a tree with optimal, discriminative splitting criteria.
 The 23 datasets were chosen from the datasets that were used in HDDT exper-iments 1 . These datasets originate from UCI 2 , LibSVM 3 and two other studies [8][10]. The 23 datasets have a variety of characteristics in terms of the level of imbalance and the number of attributes including both binary class and multi-class. Dataset  X  X age X  and  X  X atimage X  are originally the same dataset with  X  X age-5 X  and  X  X at X , respectively, but have different number of classes. In order to measure the level of imbalance in class distribution, we employ the coefficient of variance ( CV =  X /  X  ) of class distribution like the study of HDDT [3]. Higher CV means higher skewness in class distribution. Table 3 provides details of the dataset used in this experiment.

We chose Bagging HDDT and Extra-Trees as the methods to be compared with our proposed methods. As Cieslak et al. showed in the study of HDDT [3] that Bagging HDDT outperforms other methods including C4.4, C4.4 combined with Bagging and Sampling methods, and HDDT combined with Sampling, we excluded other ensemble and sampling methods in this paper. We also use Bag-ging HDDT for the performance comparison of pattern-based oversampling while excluding other oversampling methods, since Bagging HDDT is proved that it has better performance than other oversampling methods[3]. We included Bag-ging C4.5 as a baseline and Logistic Regression for comparison purpose as it is insensitive to class distribution. Table 4 shows terminologies for algorithms that we use in this experiment. 5x2 cross-validation is used to evaluate each algorithm.

We used the default parameters that are mentioned on the corresponding papers for both Bagging HDDT and Extra-Trees; 100 unpruned trees (M = 100) with laplace smoothing and n min = 2. For Extra-Trees and our methods the K number of candidate attributes is K = and HeDExh have another parameter for the number of candidate split-points, C. We set C = 10 as default, as we found that for most datasets above 10 candidate split-points has no significant effects on the performance improvement. For the pattern-based oversampling method, the amount of newly generated minority instances makes the class distribution of final training dataset balanced, since we noticed that balanced distribution brought the best performance on HeDEx, although we cannot present the experiments results due to the space limitation.
The popular evaluation measure for imbalanced dataset problems is Area Un-der the Receiver Operating Characteristic curve (AUC), thus we employ AUC in our paper to compare different classifiers  X  performance. In addition, we present the comparison of F 1 -Measure for the minority class to show the classifiers X  per-formance toward minority class. In this paper, F-Measure + denotes F 1 -Measure for the minority. We used corrected paired t-test to determine the statistical significance of performances of the compared algorithms.
 6.1 Comparison of Methods Table 5 shows win/draw/loss counts for F-Measure + of 23 datasets comparing the algorithm in the column versus the algorithm in the row. As can be seen, HeDEx has almost the same or better performance in terms of F-Measure + than HDDT and has bigger improvements from C4.5 than HDDT does. The only loss of HeDEx against HDDT and C4.5 is the dataset PhoSs. This is due to that Extra-Trees X  performance becomes p oorer as the dimensionality of dataset becomes higher. However, we noticed th at HeDExh overcomes this disadvantage of extremely randomized tr ees. We also noticed that Logistic Regression has better performance on imbalanced dataset with higher dimensionality than other classifiers we compared. In terms of both AUC presented in Table 6 and F-Measure + , HeDEx and HeDExh are the best option among others on average. Table 7 and 8 show the figures of F-Measure + and AUC for each method. 6.2 Effects of Pattern-Based Oversampling Table 7 and Table 8 show the effects of p attern-based oversampling in terms of F-Measure for minority class and AUC. Pattern-based oversampling helps the performance improvement especially when the dataset has higher dimensionality. The F-Measure + is improved from 150% to more than 3000% compared to not using pattern-based over sampling. For F-Measure + , we noticed that the degree of performance improvement due to oversampling varies among the classifiers. Decision Trees based on Hellinger Distance splitting criterion (HDDT, HeDEx, HeDExh) generally have lower improveme nts on performance than other classi-fiers such as LR, C4.5 and Extra-Trees.

In terms of AUC, however, pattern-based oversampling shows statistically sig-nificant losses on performance for 4 datasets among 23 datasets. That is why over-sampling favors to minority class and AUC weights more on majority due to the class distribution. This result is similar to other studies [3][7] in which authors said that sampling is not helpful if the classifier employs a skew-insensitive split criterion. In this paper, we have proposed a new decision tree induction classifier (HeDEx) that combines extremely ra ndomized tree (Extra -Trees [4]) with multiple candi-date split-points and Hellinger Distance as a splitting criterion for imbalanced dataset problems. We also have proposed a variant of HeDEx that employs a hyperplane decision surface (HeDExh). The main contribution of our proposed methods is that they build robust decision trees against imbalanced datasets with high computational efficiency. Moreover, because of choosing sub-optimal split-points at each node, the ensemble trees produced are all independent from each other. Due to the diversity of the shape of trees we can gather the variety of patterns from training examples, and the patterns are used to generate new minority class instances.

Overall, we ensure that Hellinger Distance is skew-insensitive as a splitting criterion, since applying Hellinger Distance to Extra-Trees shows improvement in the performance for imbalanced datasets. Moreover, we also verify that random-ization at both attribute and split-point selection improves the performance of decision tree methods especially when combined with Hellinger Distance. There-fore, HeDEx and HeDExh for imbalanced dataset gives better prediction ability at lower or similar computational cost, respectively. In addition, as shown in study of HDDT[8], HeDEx can also be employed in the case of balanced datasets. Thus, we recommend HeDEx should be considered as one of classification meth-ods to be chosen.

