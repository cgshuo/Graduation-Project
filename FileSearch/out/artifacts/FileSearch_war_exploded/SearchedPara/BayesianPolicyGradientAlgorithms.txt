 Polic y Gradient (PG) methods are Reinforcement Learning (RL) algorithms that maintain a param-of an estimate of the gradient of a performance measure. Early examples of PG algorithms are the class of REINFORCE algorithms of Williams [1] which are suitable for solving problems in which to the cases of innite-horizon Mark ov decision processes (MDPs) and partially observ able MDPs (POMDPs), and pro vided much needed theoretical analysis. Ho we ver, both the theoretical results and empirical evaluations have highlighted a major shortcoming of these algorithms, namely , the of the true average reward, resulting in the sample-inef cienc y of these algorithms. estimates. Another solution, which does not involv e biasing the gradient estimate, is to subtract a reinforcement baseline from the average reward estimate in the updates of PG algorithms (e.g., [4, 1]). Another approach for speeding-up polic y gradient algorithms was recently proposed in [5] the inverse Fisher information matrix of the polic y.
 Ho we ver, both con ventional and natural polic y gradient methods rely on Monte-Carlo (MC) tech-sizes (see [9] for a discussion). In [10] a Bayesian alternati ve to MC estimation is proposed. The idea is to model inte grals of the inte grand as a random function , the randomness of which reects our subjecti ve uncertainty distrib ution. Observing (possibly noisy) samples of f at a set of points ( x and the gradient covariance are pro vided at little extra cost. Reinforcement Learning (RL) [11 , 12] is a class of learning problems in which an agent inter -some measure of its long-term performance. This interaction is con ventionally modeled as a MDP . assume that P and q are stationary); and P according to which the agent selects actions at each possible state. We assume that this rule does actions, conditioned on the current state. The MDP controlled by the polic y induces a Mark ov chain over state-action pairs. We generically denote by = ( x path generated by this Mark ov chain. The probability (or density) of such a path is given by We denote by R ( ) = P T path . R ( ) is a random variable both because the path is a random variable, and because even for given is denoted by R ( ) . Finally , let us dene the expected return , ods, we dene a class of smoothly parameterized stochastic policies f ( j x ; ) ; x 2X ; 2 g , es-gradient [1, 2, 3]. The gradient of the expected return ( ) = ( ( j ; )) is given by 2 function or likelihood ratio . Since the initial state distrib ution P are independent of the polic y parameters , we can write the score of a path using Eq. 1 as Pre vious work on polic y gradient methods used classical Monte-Carlo to estimate the gradient in Eq. 3. These methods generate i.i.d. sample paths the gradient r ( ) using the MC estimator inte grand. We consider the problem of evaluating the inte gral
If of and the inte gral is estimated as ^ variance that diminishes to zero as M ! 1 . Ho we ver, as O'Hag an points out, MC estimation is of the data at hand [9] .
 is random simply because it is numerically unkno wn. We are therefore uncertain about the value of remo ved, since measured samples of f ( x ) may be corrupted by noise. Modeling f as a Gaussian process (GP) means that our uncertainty is completely accounted for by specifying a Normal prior denoted by f ( ) N f f into the estimation procedure. When we are pro vided with a set of samples D where y distrib ution of f jD
Here and in the sequel, we mak e use of the denitions: and [ is assumed that the measurement noise is i.i.d., in which case variance and I is the identity matrix.
 Gaussian, and the posterior moments are given by Substituting Eq. 7 into Eq. 8, we get where we made use of the denitions:
Note that In order to pre vent the problem from  X de generating into innite regress X , as phrased by O'Hag an [10 ], we should choose the functions p , k , and f in Eq. 10 are products of multi variate Gaussians and polynomials, referred to as Bayes-Hermite quadrature. One of the contrib utions of the present paper is in pro viding analogous analysis for kernel functions that are based on the Fisher kernel [13, 14]. It is important to note that in MC sample points, allo wing us, for instance to acti vely design the samples ( x In this section, we use Bayesian quadrature to estimate the gradient of the expected return with respect to the polic y parameters, and propose Bayesian policy gradient (BPG) algorithms. In the frequentist approach to polic y gradient our performance measure was ( ) from Eq. 2, which is the mulated in each path. In the Bayesian approach we have an additional source of randomness, which us denote
B ( ) the quadratic loss, our Bayesian performance measure is E ( gradient of
Consequently , in BPG we cast the problem of estimating the gradient of the expected return in the form of Eq. 6. As described in Sec. 3, we partition the inte grand into two parts, f ( ; ) and p ( ; ) . We will place the GP prior over f and assume that p is kno wn. We will then proceed by calculating the posterior moments of the gradient r Bayesian models. Table 1 summarizes the two models we use in this work. Our choice of Fisher -type kernels were guided by the requirement that the posterior moments of the gradient be analytically tractable. In Table 1 we made use of the follo wing denitions: F number of polic y parameters, and G = E u ( ) u ( ) &gt; is the Fisher information matrix. We can now use Models 1 and 2 to dene algorithms for evaluating the gradient of the expected return with respect to the polic y parameters. The pseudo-code for these algorithms is sho wn in Algorithm 1 : A Bayesian Polic y Gradient Ev aluation Algorithm Consequently , every time we update the polic y parameters we need to recompute G . In Alg. 1 we outline two possible approaches for estimating the Fisher information matrix.
 MC Estimation: At each step j , our BPG algorithm generates M sample paths using the current polic y parameters paths to estimate the Fisher information matrix G ( pirical averaging as ^ G Model-Based Policy Gradient: The Fisher information matrix depends on the probability distri-polic y, and the other corresponding to the MDP dynamics P dynamics are kno wn, the Fisher information matrix can be evaluated off-line. We can model the MDP dynamics using some parameterized model, and estimate the model parameters using maxi-mum lik elihood or Bayesian methods. This would be a model-based approach to polic y gradient, which would allo w us to transfer information between dif ferent policies.
 Alg. 1 can be made signicantly more efcient, both in time and memory , by sparsifying the so-lution. Such sparsication may be performed incrementally , and helps to numerically stabilize the algorithm when the kernel matrix is singular , or nearly so. Here we use an on-line sparsication method from [15 ] to selecti vely add a new observ ed path to a set of dictionary paths D method in further detail (see Chapter 2 in [15 ] for a thorough discussion).
 initial vector of polic y parameters mean of the gradient of the expected return, computed by Alg. 1. This is repeated N times, or alternati vely , until the gradient estimate is suf ciently close to zero.
 Algorithm 2 : A Bayesian Polic y Gradient Algorithm In this section, we compare the BQ and MC gradient estimators in a continuous-action bandit prob-lem and a continuous state and action linear quadratic regulation (LQR) problem. We also evaluate the performance of the BPG algorithm (Alg. 2) on the LQR problem, and compare it with a standard MC-based polic y gradient (MCPG) algorithm. 5.1 A Bandit Pr oblem In this simple example, we compare the BQ and MC estimates of the gradient (for a x ed set of Thus, each path paths is given by a N ( information matrix are given by u ( ) = [ a; a 2 1] &gt; and G = diag (1 ; 2) , respecti vely . Table 2 sho ws the exact gradient of the expected return and its MC and BQ estimates (using 10 and 100 samples) for two versions of the simple bandit problem corresponding to two dif ferent tractable and is reported as  X Exact X  in Table 2 for reference.
 As sho wn in Table 2, the BQ estimate has much lower variance than the MC estimate for both small and lar ge sample sizes. The BQ estimate also has a lower bias than the MC estimate for the lar ge sample size ( M = 100 ), and almost the same bias for the small sample size ( M = 10 ). 5.2 A Linear Quadratic Regulator return over 20 steps. Thus, it is an episodic problem with paths of length 20 . System Policy
Initial State: x
Re wards: r
Transitions: x We rst compare the BQ and MC estimates of the gradient of the expected return for the polic y induced by the parameters = 0 : 2 and = 1 . We use several dif ferent sample sizes (number of sample size, we compute both the MC and BQ estimates 10 4 times, using the same samples. The true gradient is estimated using MC with 10 7 sample paths for comparison purposes. Figure 1 sho ws the mean squared error (MSE) (rst column), and the mean absolute angular error (second column) of the MC and BQ estimates of the gradient for several dif ferent sample sizes. The absolute angular error is the absolute value of the angle between the true gradient and the estimated gradient. In this gure, the BQ gradient estimate was calculated using Model 1 without sparsication. With a good choice of sparsication threshold, we can attain almost identical results much faster and more efciently with sparsication. These results are not sho wn here due to space limitations. To give an intuition concerning the speed and the efcienc y attained by sparsication, we should mention that the dimension of the feature space for the kernel used in Model 1 is 6 a kernel matrix of size M = 5 j ; j = 1 ; : : : ; 20 without sparsication.
 We ran another set of experiments, in which we add i.i.d. Gaussian noise to the rewards: r covariance matrix = T 2 random variable with variance 2 variable with variance T 2 These experiments indicate that the BQ gradient estimate has lower variance than its MC counter -part. In fact, whereas the performance of the MC estimate impro ves as 1 BQ estimate impro ves at a higher rate. Ne xt, we use BPG to optimize the polic y parameters in the LQR problem. Figure 2 sho ws the performance of the BPG algorithm with the regular (BPG) and the natural (BPNG) gradient es-timates, versus a MC-based polic y gradient (MCPG) algorithm, for the sample sizes (number of with the number of updates set to N = 100 , and Model 1 for the BPG and BPNG methods. Since ods are averaged over 10 4 runs for sample sizes 5 and 10 , and over 10 3 runs for sample sizes the learned parameters do not exceed an acceptable range, the polic y parameters are dened as = 1 : 999 + 1 : 998 = (1 + e 1 ) and = 0 : 001 + 1 = (1 + e 2 ) . The optimal solution is 0 : 92 and = 0 : 001 ( Figure 2 sho ws that MCPG performs better than the BPG algorithm for the smallest sam-ple size ( M = 5 ), whereas for lar ger samples BPG dominates MCPG. This phenomenon is also reported in [16 ]. We use two dif ferent learning rates for the two components of the gradient. For a x ed sample size, each method starts with an initial learning rate, and de-creases it according to the schedule initial learning rates for each algorithm. The selected learning rates for BPNG are signif-icantly lar ger than those for BPG and MCPG, which explains wh y BPNG initially learns faster than BPG and MCPG, but contrary to our expectations, eventually performs worse. So far we have assumed that the Fisher information matrix is kno wn. In the next experiment, we estimate it us-ing both MC and maximum lik elihood (ML) methods as described in Sec. 4.
 In ML estimation, we assume that the transition probability function is P ( x parameters by observing state transitions. Figure 4 sho ws that when the Fisher information matrix is estimated using MC (BPG-MC), the BPG algorithm still performs better than MCPG, and outper -forms the BPG algorithm in which the Fisher information matrix is estimated using ML (BPG-ML). algorithm in which the Fisher information matrix is kno wn (BPG). mation procedures, which is based on the Bayesian vie w. Our algorithms use GPs to dene a prior observ ed data. The experimental results are encouraging, but we conjecture that even higher gains may be attained using this approach. This calls for additional theoretical and empirical work. use of the covariance information pro vided by the gradient estimation algorithm (Alg. 1). Two ob-gating other possible partitions of the inte grand in the expression for r a kno wn term p , 2) using other types of kernel functions, such as sequence kernels, 3) combining proach to Actor -Critic type of algorithms, possibly by combining BPG with the Gaussian process temporal dif ference (GPTD) algorithms of [15].
 Ackno wledgments We thank Rich Sutton and Dale Schuurmans for helpful discussions. M.G. would lik e to thank Shie Mannor for his useful comments at the early stages of this work. M.G. is supported by iCORE and Y.E. is partially supported by an Alberta Ingenuity fello wship.
