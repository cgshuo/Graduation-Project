 There has been increasing agreement amongst the data mining community that certain types of applications that produce large volumes of data on an open-ended basis cannot be mined by inducing models on a randomly dr awn sample. This is primarily because data streams are non-stationary in nature and models built on a small random sample will not suffice as concept drift causes a lo ss of accuracy over time. Examples of such data streams include network event logs, telephone call records, credit card transactional flows, sensor networks and surveillance video streams.

Research in data stream mining has focused on incremental learning approaches that do not re-build models on every batch of new data, but update models continuously as new data arrives in the stream [1,2,3]. In high-speed data streams that produce tens of thousands of instances per second, the c hallenge lies in accurately capturing con-cept drift and building accura te models, while keeping pace with the data that streams in continuously. A variety of approaches have been proposed [2,4,5,6,7,8] that are all based on some form of incremental learning.

Most incremental approaches use a sliding time window that is used to update statis-tics maintained in tree nodes that determine when structural changes need to be made. In certain scenarios time windows are ineff ective, as data encapsulating a certain pattern of behavior slides out of the window to be r eplaced by a new pattern. The re-emergence of the old pattern then presents the learner with a severe disadvantage as its usage statis-tics now do not hold information on the old pattern, thus causing a loss in accuracy until the old pattern has been captured once again. We believe that a more effective way of capturing patterns in a data stream is to use actual information content to decide which information should be retained in the model. In this paper, we present a concept-based window approach, which is integrated w ith a high-speed decision tree learner. In the next section, we present previous approaches to mining high speed data streams. Thereafter, in section 3, we present our methodology for implementing a concept-based window. Our experimental results and a comparison with CVFDT are presented in section 4. The paper then concludes in section 5 and e xplores some avenues for future research. The complete version of this paper is available from: http://www.hoeglinger.co.nz/cbdt One of the earliest high speed data stream mining algorithms was presented by Domin-gos and Hulten [1]. The authors describe and evaluate VFDT (Very Fast Decision Tree learner), a decision tree learning system based on the Hoeffding bound. The Hoeffding bound provides a confidence interval in which the mean of a set of n independent ob-servations lies with a given probability. Thi s bound was exploited by determining the number of instances required at each internal node of the decision tree in order that the mean information gain resulting from splitting on a given attribute is within a certain guaranteed interval. One important feature of the Hoeffding bound is that it is indepen-dent of the probability distribution of the original dataset. Although VFDT X  X  speed of processing is impressive, its main limitation is its inability to deal with concept drift. In response to the above problem, VFDT was extended into a new algorithm called CVFDT [2] which was capable of making changes at decision nodes. CVFDT basically uses the same engine as VFDT for growing a tree, but also considers alternate trees at each decision node. This gives it the cap ability of reacting to changes in concept by replacing the main tree with an alternate as it becomes more accurate as a result of concept drift. While CVFDT is a definite improvement over VFDT, it is still not equipped to deal with certain types of concept drift as mentioned in the introduction to this paper.

In [4], Wang, Fan and Yu take a radically different approach. According to them the most fundamental problem in learning drifting concepts is how to identify data in the training set that are no longer consistent with the current concept. They propose that the expiration of such outdated data must rely on the data X  X  distribution rather than the arrival time of the data.

Hoeglinger and Pears [9] take a similar stand to Wang et al [4] in dispensing with the time-based window. They illustrate the advantages of the concept-based approach and propose a decision tree classifier that uses the idea of a least-used decision path to decide which nodes represent outdated concepts. The Concept Based Decision Tree (CBDT) l earner maintains a forest of trees with each tree rooted on a different attribute and grows independently. The fact that every at-tribute is represented at the root level allows CBDT to be more responsive to changes in concept drift, by simply switching to the winner tree having the highest classification accuracy.

The information needed for model induction is based on usage statistics that is main-tained at each tree node and consists of a 3 dimensional array S i,j,k which records the count of class i for attribute j with value k , where the identity of the class is deter-mined by the model induced so far. Whenever a new instance arrives, it is routed to all trees in the forest and is incorpor ated into each of the models by updating the node statistics. When a sufficient number of instances have arrived at a node, a statistically significant decision whether or not the node should be split can be made on the basis of the information gain metric and the Hoeffding bound.

The usage statistics S above are incremented each time an instance is presented and are never decremented, as in CVFDT. In addition to these basic statistics, we also maintain a counter C for each node, which records the accumulated total of the number of instances that has passed through that node at any given point in time.

The critical component is the purge control mechanism that decides which infor-mation is outdated and therefore needs to be removed from the model. The counter C maintained at each tree node is used to identif y the sub-tree containing the least amount of information content, which thus becomes a candidate for removal. The removal of such sub-trees enables us to reverse a split that was made earlier that is no longer viable due to its low information content. Such removal enables new sub-trees to be grown which can capture the evolving concept more faithfully than the previous structure. We now formally define the concept of the Information Content of a sub-tree.
 Definition 1. Decision sub-tree A decision sub-tree is a sub-tree that consists of a set of leaf nodes and the parent decision node.
 Definition 2. Information Content The information content I ST of a decision sub-tree ( ST ) is a weighted sum of the information gain contained across all leaf nodes within that decision sub-tree. I sociated with a decision path comprising the decision node and leaf L ; C L , C root and C DecisionNode are the counter values at the leaf, root and decision nodes respectively.
When a leaf is split, the new decision node X  X  counter value is replaced by the current counter value in the root node of the tree, thereby allowing us to determine the rela-tive age of the split later on. Thus, immediately after a split, the weights of all leaves in the sub-tree are at their highest possible value. As new data streams in, each leaf weight will gradually change to reflect its importance in terms of the relative number of instances that reach the leaf through its g overning decision node. To obtain the in-formation content of a sub-tree, we scale the weight by the information gain G ,where G is the highest information gain (taken over all attributes qualifying for a split at the leaf node) contained in the leaf, if it were to split after C L number of instances has been presented to it. Thus it can be seen that IC ST records the potential information content contained in a decision sub-tree ST . We now need to determine the sub-tree to be removed when CBDT X  X  memory allocator requires memory to be freed. We define the concept of a Least Attractive Sub-Tree (LAST).
 Definition 3. Least Attractive Sub-Tree (LAST) The least-attractive sub tree for a given tree T is that decision sub-tree with the least information content taken over all decision sub-trees maintained by tree T .
More formally, we have LAST T =argmin T IC ST where T is a tree in the collection of trees maintained by CBDT. The identification of the LAST T allows CBDT to keep individual trees within certain si ze boundaries, but still adjust to conceptual changes in the data stream by learning new information and discarding information with the least impact on the tree X  X  error rate.

The purge-controller module that impleme nts the LAST detection functionality is shown below.

We now turn our attention to CBDT X  X  memory allocation mechanism. As CBDT al-lows each tree in its forest to grow independen tly, each tree competes with the others for memory when the total memory available to CBDT is exhausted. The memory is divided amongst the trees in proportion to classification accuracy. However, in order to reward trees that are performing at a much higher accu racy level than its rivals, a logarithmic bias function is used whose value falls sharply as classification accuracy decreases.
We next present the pseudo code for the rest of the CBDT system. The main module runs for each tree in the forest and accumulate s split requests from trees into a queue. As the Hoeffding bound requires a number of instances to be collected at a node before a statistically reliable decision can be made on a split, access to the queue only occurs on a periodic basis. When a node is split the memory allocation for the tree involved needs to be re-assessed as memory allo cation is dependent on tree accuracy. We conducted a series of experiments to assess CBDT X  X  ability to cope with differ-ent types and levels of concept drift. Our e xperimentation was conducted on different datasets generated with the synthetic gene rator used by Hulten and Domingos [1] and Wang et al. [4]. 4.1 Experiments The first dataset, D1 that we used represent s a data stream that starts off with a concept that changes slowly over time, followed by a period that exhibits progressively higher levels of drift. We tracked the variation of c lassification accuracy with the changes in the data stream. We captured the rate of concept d rift as the ratio of instances in a particular class (A) to that of the other class (B) and p lotted this on a minor axis so that its effect on accuracy can be assessed.

Fig 1 shows that both algorithms exhibit the same behavior when the level of drift is relatively low, with both algorithms mainta ining virtua lly the same level of accuracy. However, as the rate of concept drift gath ers momentum the difference between the algorithms becomes apparent. As the rate of concept drift increases, CBDT starts to outperform CVFDT with the rate of improvement increasing as the drift level grows. This is to be expected as CVFDT is heavily influenced by the data in its current time window and changes in the data stream from previous time windows are forgotten as data slides out of scope. Due to this, the changes in CVDFTs alternate trees are not substantial enough to capture the long-term effects of a progressive shift in concept.
In contrast to CVFDT which has a relatively large tree, CBDT performs better due to two very different reasons. First of all, it remembers past trends in data that are significant. Secondly, its structure is more flexible than that of CVFDTs as it maintains a forest of small trees, rather than one large tree. Thus changes in the stream are easily accommodated, either by making changes at t he upper level nodes of trees or the winner switching from one tree to another.

We now examine CBDTs performance on dataset D2 which exhibits very different characteristics to that of D1. Dataset D2 represents a data stream that is highly volatile with a very high volume of high intensity changes in drift. Due to space constraints, we only show the results for the first half of the stream, which is representative of the results for the entire stream. Fig 2 shows that CBDT consistently outperforms CVFDT throughout the range tested. The effect of short-term memory for CVFDT is amplified for dataset D2 and as a result it is insensitive in the face of vast changes in the data stream. CBDT on the other hand reacts to the changes and produces patterns of accuracy that are well synchronized with the changes in the data stream. The only time when the two algorithms perform similarly are in periods of stability in the stream that takes place between rises and falls in the drift level.

We now examine how the algorithms scale with respect to the number of attributes in the dataset. In Fig 3 we track the accu racy of both algorithms as the number of attributes is varied in the range from 5 to 100. We ran both algorithms across 10 datasets with similar drift characteristics to that of D1 and for each value of dimensionality we conducted ten runs and recorded the average accuracy value across the 10 runs. Fig 3 show that both algorithms scale well with the dimensionality of the data with CBDT showing better accuracy.

We also collected data on the memory consumption of both systems in the same runs when running the previous experiment. Fig 4 shows that CVFDTs memory consump-tion is consistently greater across the range of dimensionality as CBDT does not grow deep trees in periods of high drift, but switches to the winner tree that contains such attributes in its upper levels. In this research we have demonstrated the superiority of CBDT over window based approaches such as CVFDT in mining concep t drifting data streams. CBDT maintains a higher level of accuracy than CVFDT while us ing lesser memory. Our experimentation showed that CBDT is able to retain a high level of accuracy in the face of massive changes in concept drift, as demonstrated in our experimentation with dataset D2. In future research we will address two issues: one is to parallelize the operation of CBDT as each tree is grown independently. Another option would be to devise a metric that would quantify the importance of recurring patterns.

