 To optimize the performance of web crawlers, various mea-sures of page importance have been studied to select and order URLs in crawling. Most sophisticated measures ( e.g . breadth-first and PageRank ) are based on link structure. In this paper, we treat the problem from another perspective and propose to directly measure page importance through mining user interest and behaviors from web browse logs. Unlike most existing approaches which work on single URL, in this paper, both the log mining and the crawl ordering are performed at the granularity of URL pattern. The pro-posed URL pattern-based crawl orderings are capable to properly predict the importance of newly created (unseen) URLs. Promising experimental results proved the feasibility of our approach.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information filtering Algorithms, Performance, Experimentation.
 web crawling, URL pattern discovery, web log mining
A web crawler cannot download everything but should be selective. The strategy of a crawler to choose URLs from its crawling queue is called crawl ordering policy [14]. The essence of designing crawl ordering policy is to quantitatively measure the  X  X mportance X  of a page.

In the past decade, a series of ordering policies have been proposed, based on various importance measures. Most pop-ular measures are based on link structure. For example, the breadth-first policy [19] believes pages located within a few links to a website X  X  portal are important. More sophisticated  X  T his work was performed at Microsoft Research Asia. Figure 1: The relationships between PageRank and u ser traffic distribution on several example websites. measures like in-degree [7], PageRank [15], and their deriva-tives [2, 3, 8, 9] take more complicated link structures into account to estimate page importance. Besides link struc-ture, some new measures were proposed recently to leverage other information. For example, topical crawler [13,18] (also called focused crawler [6]) was introduced to prioritize pages according to their content relevance to some pre-defined se-mantic topics. And in [16, 17], URLs with higher search impact (in terms of click-through rates) are prompted in crawling. Another noticeable trend is to design site-specific importance measures. For example, the structure-driven ap-proach [21] is interested in selecting some certain kinds of target pages in a website; and for forum crawling, only pages containing user-generated content are considered to be im-portant [5,22]. In the literature, promising results have been reported using all these approaches. Some of them ( e.g . PageRank) have been proven to work well in industry [4].
In nature, these importance measures adopt different hy-potheses to predict how a page could be interesting to users [19]. All these hypotheses are reasonable but just characterize user interest indirectly and incompletely. Fig. 1 shows that even the most successful PageRank still lose considerable user traffic. This is because user interest is influenced by multifarious factors when the Web becomes more dynamic and complex. Therefore, we propose in this paper to directly measure page importance through mining user interest and behaviors from web browse logs.

Simply prioritizing a URL according to its frequency be-ing recorded is impractical as the log data is very sparse in comparison with the Web. In addition, user behaviors on a single URL are usually noisy and unstable. We propose to summarize log data with URL patterns, and design crawl ordering policies at pattern-level. The motivation is that in a website, URLs generally follow some syntax schemes (patterns) pre-defined by the designers, and URLs belong-ing to the same pattern usually act similar roles [12]. More-over, pattern-level statistics are more reliable and robust. In practice, our approach has two additional advantages:
Figure 2: The overview of the proposed approach.  X  F irst, our approach is capable of predicting the importance of unseen URLs . URL patterns of a website are steady in a relatively long period, and the user behaviors related to a certain pattern won X  X  change too much.  X  Second, our approach integrates the merits of both general and site-level crawl ordering policies . General policies like
PageRank deal with a majority of websites but cannot optimize the efficiency of a particular one; while site-level policies [21, 22] are designed for specific websites but are hard to be scaled up to the whole Web. By contrast, our approach can go deep to optimize site-level crawl ef-ficiency, as well as go wide to provide a general solution. Two technical obstacles are solved to make the proposed approach work:  X  How to determine the granularities of URL patterns ? Gen-eral patterns are apt to mix up URLs with different char-acteristics and cannot help crawling; while subtle patterns have the risk of over-fitting which leads to poor general-ization ability on unseen data. We propose to organize
URL patterns in a tree structure, and select appropriate patterns from the tree through investigating the corre-sponding user behaviors.  X  How to properly leverage URL patterns to design crawl or-dering policies ? URLs from various patterns act different roles in a website. We propose a behavior graph-based solution to rank URL patterns for two common crawling scenarios X  comprehensive fetch a website and timely dis-cover new pages .

Elaborate experiments have been carried out and the re-sults are quite promising. First, the discovered URL pat-terns can describe user behaviors very well. Furthermore, the patterns are temporally reliable and with good gener-alization ability to unseen data. Second, the crawling effi-ciencies are noticeably improved. Under the same download throughput, the proposed approach discovers and fetches more informative pages than several traditional methods.
The framework overview of the proposed approach is shown in Fig. 2, which mainly consists of three components: (1) log processing , (2) URL pattern discovery , and (3) pattern rank-ing for crawl ordering .

We work on browse log collected from anonymous users who have agreed to contribute their surfing history. Records in the raw log data are in the form of a triple where URL t is the target being browsed and URL r is the re-ferrer the user comes from. GUID is a hexadecimal string to identify anonymous users. Duplicate records ( i.e . the same user transit from URL r to URL t for multiple times) are re-moved to avoid bias brought by individual users. Several Figure 3: Illustrations of (a) URL decomposition a nd (b) the pattern tree (only shows a partial part for a clear view) for URLs from www.playlist.com . measurements are statistically defined to characterize user browsing behaviors:  X  F in ( u ). The frequency of a URL u being a visit target. It is defined as the number of records whose URL t is u .  X  F out ( u ). The frequency of a URL u being a referrer. It is defined as the number of records whose URL r is u .  X  F trans ( u, v ). The frequency of transiting from a URL u to v . It is defined as the number of records whose URL t is v and URL r is u .

To discover appropriate URL patterns, as shown in Fig. 2, we first construct a pattern tree to organize URLs based on their syntax structures. The  X  X arent-child X  relationship between tree nodes characterizes the syntax similarity very well at various granularities. Then we tailor the tree through merging child nodes to their parent if they have consistent user behaviors.

Next, we analyze the statistical characteristics of user be-haviors on each URL pattern. For example, how many URLs in a pattern act as referrer in browsing? And what is the frequency that users transiting from URLs in one pattern to URLs in another? From such statistics we can estimate how users browse a website, and analogically design crawl ordering policies. Currently two crawling scenarios  X  com-prehensive fetch and timely discovery  X  are considered and their ordering policies are discussed in the modules ranking-for-fetch and ranking-for-discovery in Fig. 2.

At last, we would like to emphasize that the proposed approach can be run in parallel in multiple sandboxes, each of which handles one website, as shown in Fig. 2. The only input of a sandbox is the site-level logs. It is easy to deploy this approach on a cluster to deal with millions of websites. In this section, we introduce how to discover appropriate URL patterns from web browse logs.
A URL is not an ordinary string but complies with the syntax scheme strictly defined in [1]. Based on the syntax scheme, a URL can be decomposed into a series of  X  X ey X  value X  pairs, as shown in Fig. 3 (a). In addition, URLs in the same website usually follow some designing principles. Specifically, different keys usually have different functions and play different roles.

Following the recursive split process in [12], we construct a pattern tree in a top-down manner. That is, we start from the root (which contains all the input URLs), and iteratively divide URLs into subgroups according to their values under a particular key. The selected key in each iteration is the one Table 1: Pattern-level statistics of user behaviors. who has the most concentrated distribution of values. For m ore details please refer to [12]. Fig. 3 (b) shows an example pattern tree constructed for the website www.playlist.com
Table 1 defines several pattern-level statistics, which are statistically significant than URL-level ones. Two browsing behaviors, visit and transit , are then considered in sequence to cut down the syntax-based tree, as shown in Fig. 4.
Visit-based tree-cut is to remove sibling nodes having sim-ilar visit traffic ( e.g . the nodes in the red dashed ellipse in Fig. 4 (b)). The process can be considered as using a model (a tree-cut plan) to characterize a dataset (URL-based user traffic). A model with lower complexity is preferred if it can describe the data well. In this paper, the well-known mini-mum description length (MDL) principle [20] is adopted to balance the model complexity and the fit to the data. Let U be the set of input URLs. A tree-cut plan is denoted by a set  X  = { P 1 ,  X   X   X  , P K } in which each P k (1  X  k  X  K ) is a leaf node survived after the cut. The best plan  X  best is the one that minimizes the sum of model description length L ( X ) and data description length L ( U |  X ), as According to [20], L ( X ) can be estimated by where F in ( P root ) = P u  X  U F in ( u ) is the total traffic on all the URLs in U , and K is the size of  X . L ( U |  X ) can be described by the negative log-likelihood of the data given the model [10], where p ( u |  X ) is defined as In other words, the probability of a single URL being visited is smoothed by the corresponding pattern-level average.
As user behaviors are easy to be disturbed by temporal interruptions, child nodes belonging to the same functional role are still possible to have different visit frequencies. In such a case, transit behavior can help remove redundant nodes. For example, it is reasonable to remove the two nodes in the blue rectangle in Fig. 4 (b) if they have identical in-links and out-links, as shown in Fig. 4 (c).

Given a pattern P k , it is straightforward to characterize its in-link distribution p in k and out-link distribution p Figure 4: User behavior-based pattern selection.
 Figure 5: Illustrations of (a) a complex case to judge t he transit similarity of node 6 and 7; and (b) prop-agate transit possibilities on the tree to smooth the in-link distributions of 6 and 7.
 Such a definition can handle the simple case in Fig. 4 (c), while real cases are more complicated. As shown in Fig. 5 (a), the nodes 6 and 7 have different in-link and out-link nodes. However, it is noticed that their in-link nodes (10 and 11) and out-link nodes (3 and 4) are siblings of each other. The nodes 6 and 7 still have similar transit relationships if node 9 and 2 are respectively treated as their in-link and out-link nodes. To smooth the in-link and out-link distribu-tions, the transits are propagated on the tree, as shown in Fig. 5 (b). The propagation is defined by a K  X  N matrix whose element P k,n is defined as where H k and H n are the heights of k and n on the tree, H tree is the total tree height, and N is the total number of tree nodes. Eq. (6) is defined following two criteria:  X  The transit possibility of a leaf node can only be propa-gated to its direct ancestors.  X  The transit possibility keeps weakening when the propa-gation goes farther. The decay factor is experimentally father node.
 With the P matrix, the K -dimensional sparse vectors in Eq. (5) are converted to N -dimensional dense ones, as And the transit similarity sim transit ( i, j ) between P is defined as the average of their in-link and out-link cosine similarities on the smoothed distributions, as Based on Eq. (8), the cutdown process is carried out in a bottom-up way, as shown in Algorithm 1.
In this section, we introduce how to design crawl ordering policies based on the discovered URL patterns. Algorithm 1 T ransit-based Tree Cut. The input is a pat-1: L et Q be a priority queue in which tree nodes are sorted in 2: Initializing Q . For each node, add it to Q if the node itself is 3: while Q is not empty do 7: Turn q into a leaf by cutting all the nodes in C q . 9: if all the siblings of q are leaves then 10: Add q  X  X  parent to Q . 11: end if 12: end if 13: end while 14: return URL patterns of all the remaining leaf nodes.
T wo common crawling scenarios X  batch and incremental modes [14] X  X re exported to site-level here, as:  X  Comprehensive fetching a website is similar to the batch mode, to periodically snapshot a website. The goal is to fetch as much as possible valuable information from the target website. Importance measurement in this situation should describe the value of a downloaded page itself.  X  Timely discovering new pages partially realizes the func-tion of the incremental mode, to discover newly created
URLs via frequently checking a few  X  X ub X  pages. This is useful to monitor websites of social network or news.
Importance measurement here should consider the total value of URLs linked from the downloaded page.

It is straightforward to design ordering strategies directly based on the pattern-level statistics of browsing behaviors. Ranking-for-fetch strategies:  X  Rank-by-Volume . URL patterns are sorted in descending order based on its volume || P || . Large volume means that pattern is a dominant resource in a website, and should provide more information to users.  X  Rank-by-Overall-Visit . Patterns attracting more overall user visit traffic, i.e . F in ( P ), are with higher priorities.  X  Rank-by-Average-Visit . Patterns are ordered according to the averaged visit frequency F in ( P ) / || P || . Ranking-for-discovery strategies:  X  Rank-by-Overall-Referral . The importance of a pattern is evaluated based on the total amount of times that its
URLs act as referrers in browse logs, i.e ., F out ( P ).  X  Rank-by-Average-Referral . Patterns are ranked based on the average frequency that their URLs play a referrer role in browsing, i.e ., F out ( P ) / || P || .

All these strategies are partially reasonable from certain perspectives. However, they just locally consider the char-acteristics of an individual pattern, but neglect the relation-ships between patterns. To better combine both the behav-ior statistics and the relationships between URL patterns, we propose to use graph-based strategy to rank patterns.
The graph constructed in this paper is called behavior graph as it is different from the traditional link graph in several aspects. First, link graph in existing work is based on URLs, while nodes in the behavior graph are URL pat-terns. Therefore, the behavior graph is small and the related Table 2: The dataset scales (#URLs) in evaluation. computation is cheap. Second, the behavior graph is initial-i zed based on statistics at pattern-level, while link graphs are initialized almost randomly. Third, the transition prob-abilities ( i.e . weights of graph edges) of the behavior graph are voted by users, while the edge weights of a link graph are based on hyper-links ( e.g . in-degree and out-degree).
Of course, PageRank-like link analysis strategy is still fea-sible for the behavior graph. However, it measures the over-all importance of a graph node and thus is only suitable for comprehensive fetch . In comparison, another popular link analysis method, the HITS algorithm [11], measures page importance from two aspects X  authority and hub . For each node, the authority score depicts the value of its content and the hub score describes the value of its links. This per-fectly matches the requirements of both comprehensive fetch and timely discovery . Following the instruction of the HITS algorithm, the two scores of a node P i are initialized as The two scores are continually updated in a series of itera-tions. The updating formulas of the k th iteration are Both auth k ( P i ) and hub k ( P i ) are normalized to unit mag-nitude after the update. The converged authority and hub of each node can properly estimate the possibilities of URLs in that pattern to be targets or referrers. Consequently, for comprehensive fetch , URL patterns are ranked in descend-ing order based on 0 . 5  X  [ auth ( P i )+ hub ( P i )]; and for timely discovery , patterns are simply ordered by hub ( P i ).
Elaborate experiments have been carried out to demon-strate the feasibility of the proposed approach, for both URL pattern discovery and efficient site-level crawling.
The experiments were carried out on 9 websites, as shown in Table 2. These websites include forums, video websites, and several popular Web 2.0 portals. For each website there are three data resources: (1) the mirror cached at May 2010, using a brute-force crawler; (2) 6-month browse logs from Nov. 2009 to Apr. 2010, collected by web browsers; and (3) search results returned by Google, Bing, and Yahoo!, in-cluding URLs from that website and were clicked in web search. Browse logs and search results can be considered to describe user interest from two different perspectives. The scales of these data resources are listed in Table 2. In the experiments, the URL patterns were mined based on one-month log data (Nov. 2009), and the other five months X  logs Table 4: Several evaluations of pattern qualities
Figure 6: Temporal reliability of URL patterns. were used for evaluation. All the crawling was simulated on the mirrors, which provide a consistent experimental envi-ronment. In the following evaluations, the average perfor-mances of all the 9 websites are reported.
According to the steps in Section 3, the generated pattern trees are denoted as T SY N , T V ISIT and T T RAN . Some basic statistics of these pattern trees are listed in Table 3. Sev-eral evaluations were designed to measure the URL pattern qualities from four aspects.
 Mirror coverage is to measure the generalization ability of URL patterns. Specifically, we measure how many URLs in the mirrors can be covered by the discovered patterns. From Table 4, it is noticed that around 10% URLs in the mirrors cannot be covered by the patterns from T SY N . In other words, these patterns somewhat overfit the training log data. In comparison, the patterns from T V ISIT and T
T RAN significantly improve the generalization ability and can cover almost all the URLs in the mirrors.
 Page layout similarity is to measure the distinguishability of URL patterns. That is to say, a pattern should contain only one certain resource in a website. Here, the distin-guishability is evaluated via checking the layout similarity between pages under the same pattern. This makes sense as most modern websites are built based on templates [5, 21]. We computed the pair-wise layout similarities between pages under the same patterns, as well as some random pages from different patterns for comparison. From Table 4, it is clear that pages grouped by URL patterns have better layout con-sistencies than randomly selected pages.
 Visit traffic approximation is to measure the summa-rization ability of URL patterns. The pattern-level sum-marizations smooth the user browsing behaviors on individ-ual URLs. However, such smoothing shouldn X  X  distort the browsing behaviors too much. We calculated the Jensen-Shannon divergences between the visit traffic distribution on original URLs and the smoothed distributions on pattern-level. From Table 4, it is clear that the smoothing process ( e.g . T SY N vs . T T RAN ) just slightly hurt the distribution of browsing behaviors.
 Temporal reliability is to measure the period of validity of URL patterns. Both the URL coverage and the traffic Figure 7: Comparisons of different pattern-based or-d ering strategies, for comprehensive fetch .
 Figure 8: Comparisons of the breadth-first, PageR-a nk, and the proposed behavior-driven approach, for comprehensive fetch . coverage were evaluated along time. For comparison, we also measured the URL-level reliability, that is, the inter-section percentage of URLs in logs of different periods. As shown in Fig. 6, the URL-level reliability drops very quickly, which perfectly echoes the statement in [14] that URLs re-tire rapidly. By contrast, URL patterns (especially those from the final tree-cut T T RAN ) are quite reliable. This is a convincing justification to do pattern-level crawl ordering.
The evaluations of crawling performance were set up re-spectively for comprehensive fetch and timely discovery .
Two factors were monitored for the evaluation of the com-prehensive fetch scenario. One is the throughput which indi-cates how many web pages have been downloaded; the other is the amount of downloaded information which is described by the sum of traffic 1 of the fetched URLs.

In Fig. 7, we compare the behavior graph-based ranking strategy with the three na  X   X ve ranking-for-fetch strategies in Section 4.1. Several different throughput conditions, from 1% to 80%, were investigated. Among the four strategies, rank-by-volume is always the worst. This indicates that the visit traffic is not in proportion to the resource scale (a URL pattern can be considered as a kind of resource in a web-site). Rank-by-overall-traffic and rank-by-average-traffic are more efficient but still not good enough. In comparison, the behavior graph-based strategy remarkably outperforms all those na  X   X ve strategies, especially when the throughput is small. This is a nice advantage as throughput is always the biggest bottleneck in crawling.

Fig. 8 compares the proposed approach with two popular ordering policies, breadth-first and PageRank , deployed in industry. The link-depth and PageRank scores were calcu-lated based on the mirrored websites. Besides the browse logs, the information amount was also estimated based on the click numbers in the search results. It is noticeable that
F or a URL in browse log, the traffic is F in ( u ); for a URL in search results, the traffic is its click number; otherwise the traffic is simply set as zero. Figure 9: Comparisons of different pattern-based or-d ering strategies, for timely discovery .
 Figure 10: Comparisons of the breadth-first, PageR-a nk, and the proposed behavior-driven approach, for timely discovery . the proposed approach is much better than the breadth-first and PageRank methods. For example, to download 80% visit traffic, our approach needs to fetch less than 40% pages form a website; while the other two methods need to download more than 60% pages to achieve this goal. Under the measurement of click number in search results, the su-periority is even more obvious, as shown in the right part of Fig. 8. This is a good news to search engines, as URLs frequently clicked in search results are quite important to search relevance.
For evaluation of the timely discovery scenario, we replace the amount of downloaded information to the amount of dis-covered information. The discovered information is defined as the sum of traffic of all the seen URLs. A URL is called a seen URL once it was parsed from a fetched page and has been added to the crawling queue. The discovered informa-tion provides an expectation of the amount of information can be downloaded in the future.

In Fig. 9, we compare the behavior graph-based ranking strategy with the other two ranking-for-discovery strategies in Section 4.1. Among the three strategies, rank-by-average-referral is better than rank-by-overall-referral. This makes sense as dominant hub patterns in a website usually have only a few URLs which frequently act as referrers in brows-ing. Although the count of overall referral may be small, the average number of referrals on each page is large. Again, the behavior graph-based strategy is the best under all the throughputs. This suggests the quality of referrals is also important in ranking a referrer.

Finally, the efficiencies of the breadth-first, PageRank, and our approach are compared in Fig. 10. Our approach is still better than the breadth-first and PageRank methods. For instance, to discover 80% traffic, our approach needs to download around 10% pages; while PageRank needs to download about 35% pages and the breadth-first needs to fetch 55%. In other words, a crawler equipped with our ap-proach can monitor 80% information of a website via only downloading and checking 10% URLs. This is especially useful for the crawling of Web 2.0 websites. In addition, our approach also held the superiority under the measurement of click number, as shown in the right part of Fig. 10.
In this paper we proposed a user behavior-driven method for efficient crawling. The contributions are four-fold:  X  Propose to leverage web log data to help web crawling.
This is more direct than existing technologies to predict user interest and optimize crawl ordering.  X  Propose to summarize web logs with URL patterns. Come up with a pattern tree-based method to identify URL pat-terns in a proper granularity from log data.  X  Propose several strategies to rank patterns for two crawl-ing scenarios, comprehensive fetch and timely discovery.  X  The proposed approach goes deep to optimize site-level crawl ordering policies, and is capable of going wide to deal with millions of websites in parallel mode.
