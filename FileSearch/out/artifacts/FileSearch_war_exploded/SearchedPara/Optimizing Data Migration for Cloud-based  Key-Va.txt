 As one database offloading strate gy, elastic key-value stores are often introduced to speed up the application performance with dynamic scalability . Since the workload is varied, efficient data migration with minimal impact in service is critical for the issue of elasticity and scalability. However, due to the new virtualization technolog y, real-time and low-latency requirements, data migration within cloud-based key-value stores has to face new challenges: effects of VM interference, and the need to trade off between the two ingredients of migration cost, namely migration time and performance impact. To fulfill these challenges, in this paper we ex plore a new approach to optimize the data migration. Explicitly, we build two interference-aware models to predict the migration time and performance impact for each migration action using statistical machine learning, and then create a cost model to strike a balance between the two ingredients. Using the load rebala ncing scenario as a case study, we have designed one cost-aware migration al gorithm that utilizes the cost model to guide the choice of possible migration actions. Finally, we demonstrate the effectiveness of the approach using Yahoo! Cloud Serving Benchmark (YCSB). H.3.4 [ Information Storage and Retrieval ]: Systems and Software, Distributed systems; C.4 [ Performance of systems ]: Design studies, Modeling techni ques, Performance attributes; Management, Performance, Design, Experimentation, Algorithms. Data migration, Key-value store, VM interference, Migration cost. With the growing popularity of cl oud computing, applications are more prone to provide highly-res ponsive, low latency service to a vast number of users. Too often, the database is the primary bottleneck. Additionally, the traditional OLTP applications are evolving into Extreme Transactio n Processing (XTP) applications [1]. Supporting this confluence of trends requires new techniques and mechanisms. As a result, elastic Key-Value stores have emerged as a preferred choice. Virtualization techniques are commonly used in today X  X  cloud plat forms. Key-value stores such as Gigaspaces XAP, Hazelcast and Couchbase support running on Amazon EC2, which uses Xen virtualization. 
As the load changes during diff erent usage periods, live data migration with minimal cost and no disruption to the service availability is critical for the elasticity. However, due to the new virtualization technolog y, real-time and low-latency requirements, data migration within cloud-based key-value stores has to face new challenges. First, the VM interference (i.e., cache interference, I/O interference) poses adverse impacts on running applications and is especially seve re for I/O intensive ones due to the resource aware CPU scheduling[2,3]. Second, it is necessary to trade off between the two ingredients of migration cost, namely migration time and performance impact 1 . Explicitly, if too few network resources allocated to th e data migration and migration time is long, it may cause an extended period of suboptimal performance problems. However, if too many network resources are allocated, the data migration completes quickly while the performance may suffer from a big hurt (see Figure 1). 
In this paper, we first study the impacts of VM interference on data migration for cloud-based key-value stores, which has received comparatively little attention. To fulfill these challenges, we explore a new approach to op timize the data migration within cloud-based key-value stores. Explicitly, we construct two interference-aware prediction mode ls using statistical machine learning (SML), including migrat ion time and performance impact models. The new factor of VM interference is incorporated. A rectangular area-based cost model is then presented to make trade-offs between the two ingredients. In order to evaluate the new approach, we use the load rebalancing scenario as a case study and present a cost-aware migration algorithm that utilizes the cost model to guide the choice of possible migration actions. The experiments show that the migration approach could respond well to load imbalance and run data migration with low costs. Performance impact is defined as the difference between the average response time during and after data migration. In Xen architecture, domain 0 has access to the hardware via native drivers and performs I/O operations on behalf of the unprivileged guest domains (In this paper, we also call them VMs), acting as a privileged domain. The frontend drivers in the guest domains and a backend dr iver in domain 0 communicate with each other using shared I/O rings and event channels [4]. In order to enable low overhead I/O, Xen implements a page-flipping mechanism to exchange data be tween the frontend and backend drivers. Figure 2 shows a typical Xen architecture where CPU scheduling, resource control and processing virtual interrupts, etc are all performed in VMM level. The key-value store used in this paper is called ElastiCamel developed based on memcached. The whole system consists of multiple independent servers mainly responsible for data service and executing migration actions, and one configuration server responsible for membership management, routing table maintenance, migration planning and control. We build the request routing mechanism on clie nt side. With respect to the partitioning algorithm, we adopt a variant version of consistent hashing proposed by Amazon [5]. In the implementation, each node is allocated 16-18 partitions. To study the impacts of VM inte rference on data migration, we use the Yahoo! Cloud Serving Benchmark (YCSB) which is applied to help evaluate the pe rformance of cloud data serving systems [6].YCSB allows users to vary different parameters to emulate a variety of workloads. The parameters include read/write ratio, distribution of requests, thread count and data size.
 In our experiments, each no de corresponds to a separate VM instance. We run each test under a fixed workload pattern (the read/write ratio, request distribution and are set to 95/5 and Zipfian respectively), and vary the number of VMs on the host to indicate different degrees of interference. We focus on understanding its impact on migration time and system performance. Figure 3 shows the migration time measured under the condition that two data migration nodes are co-located on the same host. We vary the number of VMs and data size to be migrated (50M, 100M, 200M and 300M) in x-axis and measure the different migration time which is normalized against that when there are only two running VMs on the host. Figure 4 shows the migration time measured when two data migration nodes are co-located on multiple hosts. We use the tuple (x,y) to denote the number of VMs running on host A and host B respectively. The results are normalized against those when each host has only one running VM. Figure 5 illustrates the performance impact of data migration. Based on these figures, we have the following interesting observations: Figure 3: The migration time (normalized against vm=2) Figure 4: The migration time (normalized against vm=(1,1))  X  The VM interference has a si gnificant impact on data  X  Some previous work (i.e.,[7]) has suggested that performance 
Due to the facts mentioned above, the VM interference has non-ignorable effects on data migration, not only migration time but also performance impact during data migration. This new factor needs to be incorporated into the migration design. Here the data migration problem is divide d into three subproblems: how to model the migration time and performance impact per action (migration action) for cloud-based key-value stores? How to strike a right balance between the two ingredients of the cost? How to determine the choice of migration actions when creating a migration plan using the cost model? TO characterize the underlying VM interference, we choose a set of resource consumption metrics listed in Table 1. Metrics for guest domains include CPU usage, waiting time, execution count and blocked time. Considering all I/O requests of guest domains (VMs) are forwarded to domain 0, which is often regarded as the causes of communication interferences, its resource consumption metrics are also included. All these metrics can be easily collected using the XenMon tool. 
With respect to modeling, we ap ply several regression analysis methods that are commonly used for modeling the relationship between controlled input vari ables and observed response. Bandwidth throttle refers to the maximum amount of bandwidth allocated to data migrations for each node.  X  Model 1 : the migration time ( c t ) is as a function of VMs'  X  Model 2 : the performance impact ( c i ) is as a function of VMs' 
With respect to the details on how to build the prediction models, we introduce three appr oaches, including a stepwise linear regression ( SLR ) approach, a PCA-based linear regression ( PLR ) approach and a non linear regression ( NLR ) approach. The SLR approach aims to fit a multiple linear model using a stepwise technique [8]. It builds a mode l by successively adding or removing variables one at each time. The PLR approach uses Principal component analysis (PCA ) [9] to reduce dimension and then applies the least squares curve fitting to deduce a linear model. Sometimes linear models may not provide a good fit of observed data, i.e., the comple x VM communication mechanism, time-varying or skewed access patterns of applications tend to make this assumption no longer hold true. Thus we also explore the NLR approach, explicitly a quadr atic polynomial regression formally presented as equation (1). where  X  i ,  X  ij ,  X  i are coefficients, p is principal component and c is a constant. To build the models, we use YCSB clients to simulate a variety of workload mixes(workload characteristics include read-write ratios 95/5 80/20 50/50, distribution of requests zipf/uniform/hotspot/ latest, thread count and request size ranging from 512B, 1k, 4k to 10k), as well as varying the nu mber of VMs. The bandwidth throttle is controlled in six different intensities ranging from 8M, 20M, 40M, 60M, 80M to 100Mbps. Size of data to be migrated is also adjusted over a range fro m 1M to 400M. We measure the migration time and impact value on client side with the help of log information and test agent. Reca ll that two migration nodes may reside on the same (condition 1) or multiple hosts (condition 2), we differentiate these two condit ions when constructing models. 
In order to ensure the models' accuracy, we adopt an iterative training approach [10]. During each iteration, a set of test data is used to measure the prediction accuracy of the model when the training is finished. If the accuracy threshold (It is set to 70% in this work) is not reached, we further subdivide the range of responses and examine whether each sub-range is represented by existing training data. If not, additional training data needs to be collected that aims at creating a more uniform spread in the output. This procedure iterates until the prediction accuracy achieves the desired level. 
The accuracy of offline prediction models might degrade once new unmodeled features appear. This degradation has a negative impact on decision-making. In this work, we keep track of the prediction errors online a nd periodically rebuild the models. To trace the impact value, storage cl ient library coll ects response time data for "get" and "put" request s (group by node) periodically, and send data to the configuration server at a fixed interval (We use 2 seconds). We collect the impact va lue of each action on overall system performance. Time taken for each migration action is traced by configuration server. To make trade-offs between th e two seemingly contradictory ingredients, we represent the migr ation cost in a two-dimensional space where x-axis denotes migration time c t while y-axis denotes performance impact c i . The cost value (cost) can be represented by the area of a given recta ngle and computed as follows: cost = c c i . The value varies as different choices of rectangles (see Figure 6).Consequently, the trade-off problem is converted into a rectangle selection problem. In fact, this rectangular area-based cost model is built based on the idea of integration in mathematics that given a function of response time and an interval of time, the definite integral could be computed. In order to evaluate the new data migration approach, we use the load rebalancing scenario as a case study and build one rebalancing framework based on the Monitor-Analyze-Plan-Execute (MAPE) loop model. The imbalance de tection module calculates the balance rate, a metric that denotes how load is distributed across the cluster n odes. A rebalancing is invoked when it falls below the predefined threshold. The migration planning module determines a set of migration actions. Let l i be the load value on node i . We firstly normalize each load value using formula (2). Let P={ p 1 , p 2 , p normalized load values for cluster nodes (n is the number of storage nodes). According to Shannon's theory, the information entropy may serve as a measure of "mix-up" of a distribution. Thus we build the balance functi on F based on this concept. The entropy of P is denoted as formula (3). High entropy means P follows a uniform distribution while low entropy means the distribution tends to be varied [11]. 
Obviously, the maximum of entropy is H(P) max obtained for p i =1/n that corresponds to the most uniform load interpretation, the balance function that we build ranges in value from zero to one. To meet this requirement, we use the normalized entropy to denote how load distribution tends to be (see formula (4)) .
 (4) In order to identify hot data efficiently and deliver a more fine-grained data migration while reduce the amount of data to be migrated, we manage data at each node in a small unit called partition (the basic unit of migration) and monitor the workload on these partitions. Before calling the algorithm, the cluster nodes are divided into two sets based on their norm alized load values, MI_set and MO_set , such that MI_set contains all nodes whose load values are less than 1/n while MO_set contains all nodes th at meet the opposite condition. 
As mentioned above, the data migration problem can be generalized to a multi-objective optimization problem, which is NP hard [12]. We seek to solve this problem using a greedy-based approach. Algorithm 1 gives the pseudo-code of the algorithm. It firstly greedily selects one migration node from MO_set and determines which of its partition to be migrated (lines 4-7) and then selects several candidate nodes from MI_set being able to accommodate the selected partition (lines 9-11). To determine the target migration node is a non-trivial task. For each candidate node, it first calculates the bandwidth throttle b allocated to data migration that leads to a minimum cost gi ven the observed values (i.e., v , s and l ). The algorithm then computes its minimum cost (denoted as cost min &lt;u, v, p&gt; ). Finally, it compares each computed minimum cost and chooses a minimum one. Thus on e migration action and its target node are determined (denoted as w , lines 12-19). Each selected action is organized in a four-tuple style. For each node that participates in data migration "out", multiple migration actions initiated by this node are executed in sequence. However, actions initiated by different nodes are scheduled in parallel. During data migration, the location of data is changing while the system has to determine whether user request needs be routed to the new or original server in accordance with the migration state. The routing table also needs to be synchronized across the cluster nodes have designed a lightweight n-phase data access protocol based on piggyback. Due to the space limitation, we do not present more details. The experimental testbed consists of four powerful servers each with 16G of memory and two Quad-Core Intel Xeon E5620 processors (2.4GHz per core) running Xenserver 6.0. The store system consists of 30 nodes, each of which corresponds to a separate VM instance and is allocated 500M of memory. We create 15, 10 and 5 VMs running on three host servers respectively. Each VM is alloca ted one virtual core and 800M of memory. We compare the performance between our cost-aware (CA) approach and cost oblivious (CO) approach which only considers the imbalance rate when creating a migration plan and then uses a fixed bandwidth throttle (We use 20M, 40M and 60Mbps respectively) for all migratio n actions. Figure 8 shows the measured response time for the two approaches. We can see that the CA approach usually perform s better than the CO approach. This could be explained as follows: Firstly, the CO approach uses a fixed bandwidth throttle while the optimal bandwidth throttle for each migration action can be varied due to the different levels of workload at each node and interference between VMs. Secondly, different possible migration ac tions have different costs. However, it is not considered in the selection of target nodes in the CO approach. We also compare the throughput for the two approaches shown in Figure 9. As can be seen, the CA approach performs better than its counterparts by a percent of 9-18%. Figure 8: Measured response time for different approaches 
Figure 9: Measured throughput for different approaches The approach works well in such scenarios that workloads tend to experience cyclic fluctuations. It also works efficiently when meeting new and unexpected worklo ads and can be used for other cases, e.g., scaling a key-value st ore system. However, the work still yields to some limitations.  X  In order to reduce the complexity of problem formulation  X  The work uses the load rebalancing scenario as a case study In this paper we explore a new approach to optimize the data migration. We build two interference-aware prediction models to predict the migration time and performance impact using statistical machin e learning (SML) and then create a rectangular area-based cost model to make trade-off between the two ingredients. A cost-aware data migration algorithm based on the cost model is proposed. The al gorithm delivers a fine-grained control over bandwidth throttle for each migration action. The work was supported by the National Natural Science Foundation of China under Grant No. 61173003, 61100068, the National Grand Fundamental Research 973 Program of China under Grant No. 2009CB320704 and the Nationa l High-Tech Research and Development Plan of China under Grant No. 2012AA011204. [1] A. K. Nori. Distributed Caching Platforms. In VLDB , pp. [2] X. Pu, L. Liu, Y. Mei, S. Sivathanu, Y. Koh, C. Pu. [3] Y. Mei, L. Liu, X. Pu, S. Si vathanu, X. Dong. Performance [4] A. Menon, A. L. Cox, W. Zw aenepoel. Optimizing Network [5] D. Hastorun, M. Jampani, G. Kakulapati, A. Pilchin, S. [6] Cooper, B.F., Silberstein, A. , Tam, E., Ramakrishnan, R., [7] H. C. Lim, S. Babu, J. S. Chase. Automated control for [8] N. R. Draper and H. Smith. Applied Regression Analysis . 3rd [9] R. A. Johnson, D. W. Wichern. Applied multivariate [10] Kundu, S., Rangaswami, R., Dutta, K., Zhao, M. [11] A. W. Moore. Information Gain. [12] B. Trushkowsky, P. Bodik, A. Fox. The SCADS Director: 
