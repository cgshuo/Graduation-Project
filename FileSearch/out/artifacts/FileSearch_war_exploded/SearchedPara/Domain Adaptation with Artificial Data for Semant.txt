 As the quality of natural language parsing improves and the sophistication of natural language under-standing applications increases, there are several do-mains where parsing, and especially semantic pars-ing, could be useful. This is particularly true in adaptive systems for spoken language understand-ing, where complex utterances need to be translated into shallow semantic representation, such as dia-logue acts.

The domain on which we are working is goal-directed system-driven dialogues, where a system helps the user to fulfil a certain goal, e.g. booking a hotel room. Typically, users respond with short an-swers to questions posed by the system. For exam-ple In the South is an answer to the question Where would you like the hotel to be? Parsing helps iden-tifying the components ( In the South is a PP) and semantic roles identify the PP as a locative, yield-ing the following slot-value pair for the dialogue act: area=South . A PP such as in time is not identified as a locative, whereas keyword-spotting techniques as those currently used in dialogue systems may pro-duce area=South and area=time indifferently.
Statistical syntactic and semantic parsers need treebanks. Current available data is lacking in one or more respects: Syntactic/semantic treebanks are de-veloped on text, while treebanks of speech corpora are not semantically annotated (e.g. Switchboard). Moreover, the available human-human speech tree-banks do not exhibit the same properties as the system-driven speech on which we are focusing, in particular in their proportion of non-sentential utter-ances (NSUs), utterances that are not full sentences. In a corpus study of a subset of the human-human dialogues in the BNC, Fern  X  andez (2006) found that only 9% of the total utterances are NSUs, whereas we find 44% in our system-driven data.

We illustrate a technique to adapt an exist-ing semantic parser trained on merged Penn Tree-bank/PropBank data to goal-directed system-driven dialogue by artificial data generation. Our main con-tribution lies in the framework used to generate ar-tificial data for domain adaptation. We mimic the distributions over parse structures in the target do-main by combining the text treebank data and the artificially created NSUs, using a three-component model. The first component is a hand-crafted model of NSUs. The second component describes the dis-tribution over full sentences and types of NSUs as found in a minimally annotated subset of the target domain. The third component describes the distribu-tion over the internal parse structure of the generated data and is taken from the source domain.

Our approach differs from most approaches to do-main adaptation, which require some training on fully annotated target data (Nivre et al., 2007), whereas we use minimally annotated target data only to help determine the distributions in the ar-tificially created data. It also differs from previ-ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial cor-pus of user utterances generated by a hand-coded domain-specific grammar and one on available cor-pora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dia-logues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006).

The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern  X  andez (2006). More specifically, we find short answers, plain and repeated affirmative an-swers, plain and helpful rejections, but also greet-ings.

Current linguistic theory provides several ap-proaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern  X  andez, 2006). Follow-ing the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. There-fore, we reason, we can create an artificial data set of NSUs by extracting phrasal projections from an annotated treebank.

In the example given in the introduction, we saw a PP fragment, but fragments can be NPs, APs, etc. We define different types of NSUs based on the root label of the phrasal projection and define rules that allow us to extract NSUs (partial parse trees) from the source corpus. 1 Because the target corpus also contains full sentences, we allow full sentences to be taken without modification from the source tree-bank. The distributional model consists of two compo-nents. By applying the extraction rules to the source corpus we build a large collection of both full sen-tences and NSUs. The distributions in this collec-tion follow the distributions of trees in the source do-main (first distributional component). We then sam-ple from this collection to generate our artificial cor-pus following distributions from the target domain (second distributional component).

The probability of an artificial tree P ( f i ( c j )) gen-erated with an extraction rule f i applied to a con-stituent from the source corpus c j is defined as P ( f i ( c j )) = P ( f i ) P ( c j | f i )  X  P t ( f i )
The first distributional component originates from the source domain. It is responsible for the internal structure of the NSUs and full sentences extracted. P ( c from the source treebank ( c j ), given that the rule f i is applicable to that constituent.
 Sampling is done according to distributions of NSUs and full sentences found in the target corpus (
P types of NSUs found in the target domain. This sec-ond component describes the distributions of types of NSUs (or full sentences) found in the target do-main. It determines, for example, the proportion of NP NSUs that will be added to the artificial corpus.
To determine the target distribution we classified 171 (approximately 5%) randomly selected utter-ances from the TownInfo data, that were used as a development set. 2 In Table 1 we can see that 15.2 % of the trees in the artificial corpus will be NP NSUs. 3 We constructed our artificial corpus from sections 2 to 21 of the Wall Street Journal (WSJ) section of the Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005). We included all the sentences from this dataset in our artificial corpus, giving us 39,832 full sentences. In accordance with the target distribution we added 50,699 NSUs extracted from the same dataset. We sampled NSUs according to the distribution given in Table 1. After the extraction we added a root FRAG node to the extracted NSUs 4 and we capitalised the first letter of each NSU to form an utterance. There are two additional pre-processing steps. First, for some types of NSUs maximal projections are added. For example, in the subset from the tar-get source we saw many occurrences of nouns with-out determiners, such as Hotel or Bar . These types of NSUs would be missed if we just extracted NPs from the source data, since we assume that NSUs are maximal projections. Therefore, we extracted single nouns as well and we added the NP phrasal projec-tions to these nouns in the constructed trees. Sec-ond, not all extracted NSUs can keep their semantic roles. Extracting part of the sentence often severs the semantic role from the predicate of which it was originally an argument. An exception to this are VP NSUs and prepositional phrases that are modifiers, such as locative PPs, which are not dependent on the verb. Hence, we removed the semantic roles from the generated NSUs except for VPs and modifiers. We trained three parsing models on both the original non-augmented merged Penn Treebank/Propbank corpus and the artificially generated augmented tree-bank including NSUs. We ran a contrastive ex-periment to examine the usefulness of the three-component model by training two versions of the augmented model: One with and one without the target component. 5
These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors 6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Hender-son (2007) with annotations which identify seman-tic role labels, and has competitive performance. The parser uses a generative history-based proba-bility model for a binarised left-corner derivation. The probabilities of derivation decisions are mod-elled using the neural network approximation (Hen-derson, 2003) to a type of dynamic Bayesian Net-work called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007).

The ISBN models the derivation history with a vector of binary latent variables. These latent vari-ables learn to represent features of the parse history which are useful for making the current and subse-quent derivation decisions. Induction of these fea-tures is biased towards features which are local in the parse tree, but can find features which are passed arbitrarily far through the tree. This flexible mecha-nism for feature induction allows the model to adapt to the parsing of NSUs without requiring any design changes or feature engineering. 5.2 Results In Table 2, we report labelled constituent recall, pre-cision, and F-measure for the three trained parsers (rows) on the two test sets (columns). 7 These mea-sures include both syntactic labels and semantic role labels.

The results in the first two lines of the columns headed TownInfo indicate the performance on the real data to which we are trying to adapt our parser: spoken data from human-machine dialogues. The parser does much better when trained on the aug-mented data. The differences between training on newspaper text and newspaper texts augmented with artificially created data are statistically significant ( p &lt; 0.001) and particularly large for recall: almost 12%.

The columns headed PTB nonaug show that the performance on parsing WSJ texts is not hurt by training on data augmented with artificially cre-ated NSUs (first vs. second line). The difference in performance compared to training on the non-augmented data is not statistically significant.
The last two rows of the TownInfo data show the results of our contrastive experiment. It is clear that the three-component model and in particular our careful characterisation of the target distribution is indispensable. The F-measure drops from 79.5% to 63.4% when we disregard the target distribution. We have shown how a three-component model that consists of a model of the phenomenon being stud-ied and two distributional components, one from the source data and one from the target data, allows one to create data artificially for training a seman-tic parser. Specifically, analysis and minimal anno-tation of only a small subset of utterances from the target domain of spoken dialogue systems suffices to determine a model of NSUs as well as the nec-essary target distribution. Following this framework we were able to improve the performance of a statis-tical parser on goal-directed spoken data extracted from human-machine dialogues without degrading the performance on full sentences.
 The research leading to these results has received funding from the EU FP7 programme (FP7/2007-2013) under grant agreement nr 216594 (CLASSIC project: www.classic-project.org).

