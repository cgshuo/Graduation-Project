 Department of Computer Science, University of Pisa, Pisa, Italy 1. Introduction
With the advent of Web 2.0 and the Semantic Web era, ontologies have been gaining momentum, and have replaced traditional stori ng systems in many applications [ 31]. Currently th ey represent the new frontier in terms of knowledge representation, data storage and information. As with databases, ontologies are also equipped with query languages that enable information to be retrieved. Ontology query systems, which correspond to Data Base Management Systems, are frameworks that provide several  X  X ools X  such as reasoning engines, languages for querying, and languages for de fi ning rules. A reasoner makes implicit knowledge explicit by implementing a decision procedure which starts from a set of logical axioms and fi nds the relations between them and whether (or not) they can be satis fi ed. From this point of view, the ontology is a collection of axioms (relations, de fi nitions and constraints).
Given these features, interest is moving towards fi nding new methods for handling these structures and for ef fi ciently obtaining information in addition to the information obtained by the traditional reasoning systems.

The aim of this paper is to tackle the problem of extracting interesting and implicit knowledge from ontologies, in a novel way compared to existing methods. Taking inspiration from the semantic web and data mining environments, we provide a Bayesian interpretation to the relationships that already exist in an ontology in order to return a set of weighted If-Then rules which we call In fl uence Rules (IRs). In this context an ontology can be perceived as an oriented connected graph, the nodes of which are concepts and the connections are the relations among these concepts. Formally speaking, rule mining is a four-step process which combines a deductive and an inductive technique (Fig. 1). The deductive step operates on the ontology schema (T-box) by exploring the concepts and relationships between them, while the inductive step focuses on the ontology instances (the A-box).

The four steps can be outlined as follows:
Step 1 Identi fi cation of the relevant concepts. This consists in analysing the ontology schema and
Step 2 In fl uence rule schema building. This step constructs implications between pairs of concepts
Step 3 Characterization of the in fl uence rules schemas. This step assigns values to the IRs items (in
Step 4 Validation. The Validation is needed to guarantee that the IRs are consistent and do not con fl ict
For example, let us suppose we have a fragment of ontology such as the one depicted in Fig. 2 which describes companies and the business environment. Company , Manager and Project are concepts, continuous arrows represent properties of the ontologies while the dotted arrows are used to connect instances to the classes they belong to. Starting from this ontology and the corresponding instances we are able, at the end of the process, to produce IRs such as the following: The premise ( Manager.hasAge &lt; 45 ) and the consequence ( Project.hasInnovationDegree = good )are made up of ontology elements, i.e. a class with an associated datatype property and value, while w ,the weight, measures the strength of the in fl uence. This rule must be read as:
What we want to prove, besides the correctness and feasibility of the project, is that the approach allows us to extract  X  X igher level X  rules w.r.t. classical knowledge discovery techniques. In fact, ontology metadata give a general view of the domain of interest and supply information regarding all the elements except for the actual presence and number of instances. The technique is completely general and knowledge or to support any other DM processes.

In a recent application, related to the European MUSING project, 1 we used these rules to enrich prior knowledge provided by experts in economics and coded by means of a Bayesian Causal Map. According to the strategies presented both in [4,6], the map is the data source, used by modi fi ed data mining algorithms (DrC4.5 and Yadt-DRb), for driving and improving the construction of classi fi cation trees.
 The paper is organised as follows.
 2. Related context
In terms of ontologies and DM, we enter into a domain in which DM techniques and ontologies are combined to improve existing knowledge discovery tools and processes or to support decision systems. Ontologies and DM are related in different ways depending on the perspective from which the two fi elds are seen: is it the ontology that improves DM or is it DM that operates on ontologies?
In actual fact, both perspectives are right and three signi fi cant research lines can be identi fi ed: 1) using ontologies to drive DM; 2) using DM to build ontologies; 3) using ontologies to describe DM processes.

One of the fi rst proposals for the use of  X  X omain knowledge X  to drive learning methods was by Clark and Matwin [14]. There was no mention yet of ontologies, but rather of qualitative models. Nevertheless, these models are similar to ontologies in a broader sense. A qualitative model in fact comprises nodes, representing parameters of application domains,and arcs,representing their relationships. The qualitative model is used to induce rules which are not only accurate but also explainable with respect to the model itself.
 Great efforts are currently payed to the development of DM models driven by domain knowledge. For example Geller and colleagues [19] describe the use of taxonomies for improving the results of association rule mining. Their goal is to produce association rules with greater support from a large set of tuples regarding demographic and personal interest information. Since the collection of people X  X  interests tends to be too abstract for actual applications, Geller uses a hierarchy of concepts to increase data instances to higher levels during the pre-processing step, before running the DM algorithm.
A similar approach is described in [8] where an ontology in the domain of super market products is used for extracting constraint-based multi-level association rules. In this case the use of a full featured ontology (instead of a simple taxonomy) means that constraints can be de fi ned and concepts can be used requests and needs, and also to identify possible target items for seasonal promotions.
Since the construction of an ontology is a complex (manual) task for the domain experts,DM techniques are often of great help. A very simple approach is described in [15], where Quinlan X  X  C4.5 algorithm is used for building an ontology starting from the generated decision tree. The ontology is constructed by means of a mapping function from the tree elements: root node, internal nodes and decision branches are mapped onto OWL classes, while the leaves (which can be used to identify the association rules) are coded as individuals.

A complex task in ontology learning from texts, is to give semantics to the relations discovered among concepts. Maedche et al. [22] describe a learning method that provides lexical entries as potential labels for these  X  X nonymous X  relations. The idea is based on the assumption that the  X  X redicate X  of a non-taxonomic relation can be characterised by verbs frequently occurring in the neighborhood of pairs of lexical entries corresponding to associated concepts.

The ASIUM system [17,25] is an example of ontology learning from texts by using consolidated DM techniques. The method is based on conceptual and hierarchical clustering. Basic clusters are made up of words that occur with the same verb after the same preposition in the analyzed texts. The hierarchy of clusters is used to construct the ontology.

A more structured work is presented in [28], where the authors describe how to enrich an existing seed ontology using text mining techniques, especially by mining the domain speci fi c texts and glos-saries/dictionaries in order to fi nd groups of concepts/terms which are related to each other. Although the extraction of new concepts or instances from texts is automatic, the enrichment of the seed ontology is done manually by the experts. The advantage is automatically discovering many important concepts and interesting relationships directly from the data.
 Other contributions in this fi eld are described in [13,34].

In [13], the authors describe the implementation of an unsupervised system that combines syntactic parsing, a collocation extraction and selectional restriction learning. The system is applied to a set of data (in this case to a molecular biology corpus of data) and generates a list of labelled binary relations between pairs of ontology concepts. The authors demonstrate that the system can be easily applied in text mining and ontology building applications.

In [34] a method is outlined to extend existing domain ontologies (or for the semi-automatic generation documents. The rules are extracted by using statistical methods and are used to derive ontology classes from linguistic annotations. The new classes can be added to already existing ontologies or used as a starting point for a new ontology.

Ontologies are frequently also employed in context-aware systems. In [33], for example, they are used to describe both contexts and the DM process in a dynamic way. The authors separate the context-aware DM into two parts: the representation of the contexts through the ontology, and a framework that queries the ontology, invokes the mining processes and coordinates these processes according to the ontology design.
 Our work can only partially be seen as a contribution in the fi rst research line (use of ontology to drive DM). This is because what we do is to move from Knowledge Discovery in Databases to Knowledge Discovery in Ontologies using a combination of DM and Link analysis methods. In fact, the analysis of the T-Box of an ontology is used to prepare the process of actual mining out of the A-Box (the instances). 3. Background knowledge: ontologies, link analysis and data mining
This section gives some background information regarding ontologies, plus a short description of the link analysis and DM techniques.

The fi rst formalization of ontologies goes back to Aristotle X  X  work  X  X ategories X  [3], in which he provided a list of categories seen as an inventory of most types of entities. From a scienti fi c point of view, an ontology is essentially a table of categories in which every type of entity is captured by some node within a hierarchical tree.

With reference to the semantic web architecture proposed by Tim Berners-Lee [7], OWL [26], the recommended W3C standard language for web ontologies, is syntactically layered on RDF [30], but adds more vocabulary to describe relationships and objects. In agreement with the standard, the basic elements of the OWL ontology are classes , properties (datatype and object), instances of classes, and relationships between these instances.

The main reasons for developing an ontology are: 1) To share a common understanding of the structure of information among people or software agents. 2) To standardize the terminology. 3) To enable domain knowledge to be reused. 4) To make domain assumptions explicit. 5) To separate domain knowledge from operational knowledge. 6) To analyse domain knowledge.

The Link Analysis  X  X as conceived X  almost simultaneously at the IBM Research Centre and at Stan-ford University in 1998 thanks to Kleinberg, and Brin and Page. While Brin and Page X  X  PageRank algorithm [11] became the core component of Google X  X  search engine, , Kleinberg X  X  work (the HITS algorithm [21]) was not immediately a commercial success.

The Link Analysis is a set of methods for determining, in the web world, the relative authority of a web page on the basis of the hyperlink structure . A hyperlink is a technological capability that enables, hyperlinks among web sites allow documents and pictures to be referred to through the web. A hyperlink between two websites brings them closer together on a functional level. These methods also aim to produce improved algorithms for the ranking of web search results.

DM is a particular step in the Knowledge Discovery (KDD) process proposed by Piatetsky-Shapiro in 1996 [27], which searches for patterns of interest in the data. Classi fi cation, clustering, regression and association rule learning are the most common classes of methods for extracting knowledge from raw data.

In this work, we need to solve a frequent itemset mining problem. Frequent itemsets, which play an essential role in many DM tasks, are made up of items that occur most frequently together in a given dataset. Historically, the need to search for frequent itemsets came from the need to examine customer behaviour in terms of products purchased [1] (the so-called market basket analysis problem). We opted for PATTERNIST, a pattern discovery algorithm developed at the Italian National Research Council in Pisa. This algorithm is the result of previous research that has now led to the implementation of a more sophisticated (and documented) system: ConQueSt [9,10,12]. PATTERNIST is used to extract frequent itemsets of size 2 from ontology instances, fo llowing the strategy ou tlined in the introduction and detailed in the following sections.
 4. Ontology mining process
We now detail the strategy for extracting a set of IRs from an ontology, which was outlined in Section 1. 4.1. Identi fi cation of the ontology concepts
The fi rst step consists analysing the ontology schema, i.e. identifying the most important concepts using the information available through the structure.

This methodology was inspired by link analysis and in particular by the algorithms for ranking web pages (e.g. PageRank, HITS, SALSA). We chose HITS because it evaluates authoritative and hub pages separately. This idea is well suited for identifying the implicant (premise) and implicated (conclusion) concepts of the rules (step 2).

We customized Kleinberg X  X  algorithm by implementing HITSxONTO. By analysing the structure of the ontology (see Section 4.1.1) and using the rules described below (see Section 4.1.2), the algorithm returns two lists of concepts ranked w.r.t. the authority and hub. 4.1.1. HITSxONTO
Like HITS, HITSxONTO is based on the concept of authority and hub, and its purpose is to measure the importance of the ontology concepts, based only on the ontology topology (the T-Box). In other words, it tries to deduce which concepts can be considered particularly  X  X mportant X  (authorities) and which ones give a particular importance to other concepts (hubs).

The general ideas of the original version, as well as the main strategy, have been preserved due also to the natural comparisons that can be made between web and ontology  X  X lements X  (web pages vs ontology concepts and hyperlinks vs object properties). In this case, the elements that enabled the ontology to be analysed are essentially the relationships among the concepts (object properties and the is-a relations). Instead datatype properties, are not relevant at this stage, however they are indispensable for completing the whole ontology mining process.

HITSxONTO is iterative as well, and follows the same core steps as HITS. Algorithm 1 shows the pseudo code.

Algorithm 1 description. The algorithm takes an ontology O as input and returns a set of ranked concepts. The ontology O is loaded in memory and read in order to derive the corresponding graph representation G (steps 1 and 2). Each G node corresponds to an ontology concept, and each edge to an object property or to a hierarchical relation is-a .
 At step 3 the adjacency matrix M is built using the translation rules discussed in the next Section 4.1.2. Each entry represents the number of edges connecting two nodes of G . The transposed matrix M T is computed (step 4) following the Linear Algebra de fi nition ([ M T ] ij = [ M ] ji ). M and M T are n x n matrices, where n is the number of concepts of the ontology/nodes of the graph. hScore and aScore are two n -dimensional vectors, where the ranking values (hub and authority re-spectively) associated with the concepts, are stored at each iteration (the cycle is performed by steps 7, ... , 13). In order to support the iterative computation, two temporary vectors are used: h prev and a prev store the ranking values, computed at the iteration i  X  1 and needed in iteration i . In steps 5 and 6 the distribution, such as 1 /n , 1 or 1 /
The cycle stops when the fi xed point is reached, i.e. when hScore and aScore reach a stable value. At step 14 the resulting vectors are returned. 4.1.2. From the ontology to the weighted adjacency matrix
Besides the fact that HITSxONTO works on ontologies instead of web pages, the other main changes w.r.t. HITS, concern the adjacency matrix ( M in the Algorithm 1) used in input. The ontology cannot be represented as a simple di rected graph b ecause  X  X ultiple rela tionships X  can occu r between two nodes, i.e. two nodes can be connected by more than one object property with different labels (semantics). The solution is to use a weighted adjacency matrix where each entry represents the number of edges connecting two nodes. If we call W the n x n matrix associated with the ontology O , the corresponding de fi nition is:
All the possible cases arising from the ontology relationships (object properties and is-a relations) need to be considered as well as the way in which they are solved in the matrix.

Critical aspects relate to the existence of more than one object property between the same two concepts, and to the relationships in the presence of hierarchies of concepts. In the latter case, the inheritance property needs to be handled. The general rule that drives the inheritance is the fact that only the super-concept determines the hierarchy, i.e. it speci fi es whether each of its sub-concepts is the domain or the range. This is because in an ontology the sub-concepts inherit the object properties of the parents and not vice versa.

In OWL restrictions on properties can be speci fi ed in such a way that the general notion of inheritance among classes (subconcepts andsuperconcepts)does notalwayshold. By means ofrestriction, exceptions the structure of the ontology (e.g. its graph representation) and the information regarding the restrictions is lost during the translation from ontology to a graph. Nevertheless, this non-trivial aspect is very interesting and could be part of future extensions in which we re fi ne the structural analysis by using additional information coded in the ontology.
 The following are the most critical cases.

Case 1: Simple inheritance (Fig. 6). The sub-concepts inherit the object properties of the super-
Case 2: Complex inheritance (Fig. 7). With respect to Case 1, we also have one object property ( r 3 )
Case 3: No inheritance (1/3) (Fig. 8). The inheritance is not applicable from the sub-concepts to
Case 4: No inheritance (2/3) (Fig. 9). This is the case in which neither the parent concept, nor the
Case 5: No inheritance (3/3) (Fig. 10). As already stated, sibling concepts do not inherit  X  X utual X 
Case 6: Circular properties (Fig. 11). A  X  X ircular property X  exists when its domain and its range
Case 7: Class intersection (Fig. 12). The class intersection contains instances that are common to
Case 8: Class union (Fig. 13). The class union corresponds to the disjunction in its  X  X nclusive sense X ,
Case 9: Multiple properties (Fig. 14). In an ontology two concepts can be related by more than one 4.2. In fl uence rule schema building
An IRs Schema is the basic skeleton for a fi nal IR. It is composed of two concepts tied by an if-then relation. The relation can be induced either by a direct object property, or by a chain of more than one object properties that connect the two concepts in the ontology.
 In the construction of the schemas, only the most important concepts chosen from the ones ranked by HITSxONTO are used. A concept is  X  X mportant X  if its score is greater than a fi xed threshold. The choice of both the authority and hub thresholds (referred to later as ThresholdA and ThresholdH) is not fi xed for every computation and every domain; in fact, they strongly depend on how big the ontology is (in terms of the number of concepts) and its degree of connectivity. For example, if the ontology is weakly connected, it has been experimentally shown that the majority of concepts obtain a very low score (or even 0). For this reason the thresholds have to be set properly in order to avoid empty authority and hub lists. Usually setting their values entails a try and check process.
 For the construction of the sche ma we exploited HITSXONTO and ge nerated two lists of concepts. The concepts in the authority list are important because of their incoming links, while the concepts in the hub list are important because of their outgoing links. For this reason we construct the schemas following these simple format and criterion: where, Premise is a concept belonging to the hub list and Conclusion contains a concept belonging to the authority list.

At this point we speak about a  X  X chema X  of rules (or implications) rather than IRs, because two basic elements are still missing: values and weights. The next phase will provide attributes (de fi ned in the ontology by the datatype properties) and values that best characterize the concepts, and a weight that measures the strength of in fl uence. 4.3. Characterization of the in fl uence rules schemas
So far, the only information we have used to construct the IRs has been extracted at the  X  X chema level X , i.e. exploiting only what the ontology structure provides (concepts and relationships). What we have obtained is a set of rule schemas that identify only the important items and how they are related, but they do not supply information regarding the values and strengths of the relations. At this stage of the process, the ontology instances play a fundamental role: in fact they contain all the information necessary for supplementing (characterizing) the rule schemas.

The idea is to process the ontology instances using a pattern discovery strategy in order to extract the frequent itemsets. As stated in Section 3, we used the PATTERNIST algorithm. In this context, the frequent itemsets are sets of instance datatype properties of ontology concepts that appear(together) more frequently and whose support is greater than the user-speci fi ed minimum support. A crucial point here is to be able to organize the set of datatype properties of concept instances as transactions as requested by PATTERNIST (and by all the pattern discovery algorithms). In general, this type of organization is suggested by the kinds of rules we want to extract and, more generally, by the kind of analysis we want to perform. This action is possible by appropriately querying the ontology and by handling the results as if they were tuples of a relational database. What we expect is a set of records (the transaction) composed of items. Each item must contain the information regarding the concept, one of its datatype properties and the associated value.

For example, referring to the ontology fragment in Fig. 2, if we are interested in the age of the manager, an appropriate encoding for the corresponding item could be Manager.hasAge .
 Example 1. Let us suppose that there is a populated ontology describing the business environment in Fig. 2 is a small fragment. Additional concepts such as Market , Bank , FinancialInstitution , Employee , Share , Investment ... could be in the ontology. It is reasonable to suppose that all these (added) concepts are related with Company , Manager and Project (and each other) with suitable properties. By applying HITSxONTO to this ontology, the most important concepts are those in the fragment in Fig. 2 (because they satisfy the requirements of the hub and authority sets).
 Then, from the second step of the analysis, we get the following set of IR Schemas: In order to construct the dataset on which PATTERNIST has to be run, it is suf fi cient to retrieve from the ontology the frequent sets of instance datatype properties of the concepts Company , Manager and Project . What we expect is a set of tuples like the ones in Table 1.

PATTERNIST is now able to discover the frequent item sets and the corresponding supports. At the end we can imagine the following two IRs: IR 1 : Manager.hasAge = young w =0 . 80  X  X  X  IR 2 : Company.hasManagementSystem = monthly w =0 . 55  X  X  X  5. A case study
The system we presented in the previous sections has been tested in several domains. As described in [18], we used it in the European MUSING project as part of a tool for the assessment of businesses -the Online Self Assessment (thus in an economic domain). MUSING was aimed at developing a new generation of Business Intelligence tools and modul es based on semantic knowledge and contentsystems. It is composed of a large ontology covering several aspects (operational risk, fi nancial risk management, internationalization). Focusing on the fi nancial risk management ontology, and on instances collected using a quali-quantitative questionnaire describing a company pro fi le, we applied the ontology mining process to extract interesting IRs. The following Table 2 summarizes the data, and the intermediate (the results of HITSxONTO and PATTERNIST) and fi nal results.
 From the analysis of this large ontology and the questionnaire instances we were able to extract 14 IRs. Here is an example rule: which can be read as:
In 73% of the cases, if the management team has more than 10 years of experience in the industrial sector, then the company is not expected to increase its capital.
 These rules have been used to extend an expert knowledge base, thus supporting an improved classi fi er according to the strategies reported in [5,6].

The successful results obtained in the MUSING project and in the economic domain encouraged us to do further work on the system extending it with new features, and to carry out new experiments. The extension of the system covers two aspects:
To implement the fi rst feature we group each simple rule with the same consequence, and we construct  X  X uper-sets X  composed of all the combinations of 2, 3, ... , n implicants. Then, we maintain only the sets that, together with the consequence, have a corresponding itemset in the fi le produced by PATTERNIST. This requirement is necessary to get the right weight to associate with the new complex rule. Then we build the IRs in the traditional way.

As is usual for association rules, con fi dence denotes the conditional probability of the head of the rule, given the body . This parameter allows us to measure the reliability of a rule and in particular of an outlier, i.e. an IR with a low probability of occurring.

A case study in intrusion detection follows. 5.1. The idea and the data
Intrusion Detection is the process of monitoring the events occurring in a computer system or network and analysing them for signs of possible incidents. These are violations or imminent threats of violation of computer security policies, acceptable use policies, or standard security practices [32].
As reported in [20], ontologies can be used as a way of grasping the knowledge of a domain, by expressing the intrusion detection system much more in terms of the end user X  X  domain, thus generating intrusion detection more easily and performing intelligent reasoning.

Pinkston and colleagues in [29] created an ontology and used it to detect different attacks. In constructing their ontology, they relied on an empirical analysis of the features and attributes, and their interrelationships, of over 4000 classes of computer attacks and intrusions, and implemented it in DAML + OIL. Figure 15 shows the main classes they identi fi ed and the corresponding relationships. A Host has a current state and can be the victim of an Attack . The attack is effected by an Input and causes a Consequence .

For our case study we used this ontology as a starting point, and from a set of data regarding intrusion detection taken from the UCI KDD Archive. 2 The data are part of the set used for The Third International Knowledge Discovery and Data Mining Tools Compe tition (KDD CUP 1999) and are curren tly available at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html.

The fi rst step was to reconcile the ontology and data.
The ontology was translated from DAML + OIL to OWL 1.0 format, and was extendedwith information extracted from the data and their description. 3 To reconcile the data, we considered the dataset variables as datatype properties of (already de fi ned or new) ontology concepts. The alignment of the ontology with the data was done manually because the T-Box and the A-Box come from different sources. In general if the collected data re fl ect the ontology exactly, no alignment is necessary. To accomplish the above task we needed to acquire some basic knowledge of the domain, especially to understand the meaning of each variable and ontology concept, and identify any possible matches.
 Table 3 shows the correspondence.
 From the translation and extension of the ontology in Fig. 15, we obtained the one depicted in Fig. 16. The general structure was maintained, but its expressivity was substantially improved by new datatype properties and several new object properties.

Before starting the ontology miner, we performed a discretization step of the dataset, since the majority of variables (e.g. the number of bytes, the duration of the connection) were continuous. As is well known, algorithms for the discovery of frequent itemsets cannot be correctly performed when the dataset contains continuous variables or nominal variables with a high number of classes. The discretization was done by looking at the frequencies of the values taken by each variable.

For example, the values of the variable dst bytes, which indicate the number of data bytes from the destination to the source, were grouped into three categories: 0 (exactly 0 bytes), 0 100 (from 1 to 100 bytes) and &gt; 100 (more than 100 bytes).

This solution is not restrictive, since we are guided by the frequency of the values, which in any case is necessary in order to make the dataset manageable.

At the end of this long and non-trivial preprocessing step, we obtained:  X  An ontology with 57 classes, 21 object properties and 47 datatype properties.  X  An instance set with 27 variables and 311029 records. 5.2. Running the ontology miner The Ontology Miner was run using the following settings:
Under these constraints HITSxONTO returned a set of 49 IR schemas containing exactly two concepts (one implicant and one consequence).

PATTERNIST returned a set of 442 frequent itemsets composed of exactly two items and a support value.
 The fi nal result, after the merging phase was a set of 70 IRs.

We obtained both a large number of frequent itemsets and rules because we kept the minimum support low and because of the size of the dataset. Besides the number of records, by computing the frequent itemsets, both the number of variables and the corresponding number of associated classes have an impact. In this case we had 27 variables, and for each variable we had an average of 4 values (and for some variables more than 10). These three dimensions affect the computation and generate intermediate results (temporary fi les for example) that can become intractable. In our case too, PATTERNIST generated a fi le of all possible frequent itemsets of more than 3.7Gb in size, thus reducing the system performance in terms of speed.
 The following rules look quite interesting: R : Attack.hasClassi fi cation =  X  X murf X  52%  X  X  X  Consequence.hot =  X 0 X  ( c = 100% ) R : Connection. fl ag =  X  X F X , Connection.srcBytes =  X 0 1032 X , Connection.dstBytes =  X  &gt; 100 X , Connection.count =  X 0 100 X  20%  X  X  X  Login.numFailedLogins =  X 0 X  ( c = 99% ) R : Attack.hasClassi fi cation =  X  X murf X  52%  X  X  X  Consequence.numCompromised =  X 0 X  ( c = 100% ) R : Attack.hasClassi fi cation =  X  X murf X  52%  X  X  X  Network.protocolType =  X  X cmp X  R : Connection. fl ag =  X  X F X , Connection.srcBytes =  X 0 1032 X , Connection.dstBytes =  X  &gt; 100 X , Connection.count =  X 0 100 X  20%  X  X  X  Files.numFileCreations =  X 0 X  ( c = 100% ) R : Connection.duration =  X 0 X  25%  X  X  X  Destination.service =  X  X rivate X  ( c = 26% ) R : Connection.duration =  X 0 X , Connection. fl ag =  X  X F X , Connection.srcBytes =  X 0 1032 X , Connection.dstBytes =  X 0 X , Connection.count =  X  &gt; 500 X  42%  X  X  X  Destination.service =  X  X cr i X  ( c = 99% ) where the values have the following meanings:  X  smurf attack, is a denial-of-service attack which uses spoofed broadcast ping messages to fl ood a  X  SF refers to the normal status of the connection.  X  icmp is the type of protocol used in the connection.  X  private is the type of network service at the destination.  X  ecr i corresponds to the ECHO REPLY service.

R 1 means that in 52% of the cases, when the attack is classi fi ed as a  X  X murf X , the number of the hot indicator is  X 0 X  (i.e. the attack is not dangerous).
 To con fi rm this, R 3 says that under the same conditions the number of compromised conditions is  X 0 X .
R 5 means that in 20% of the cases, when the connection status is normal, the number of bytes from the source to the destination is between 1 and 1032, the number of bytes from the destination to the source is greater than 100 and there were no connections to the same host as the current connection in the past two seconds, thus there were no fi les created.

With appropriate settings (i.e. a lower support) we focused on events that rarely happen but that can be very signi fi cant because they could identify infrequent happenings or unusual intrusion attacks, like: R : Connection.duration =  X 0 X , Connection. fl ag =  X  X 0 X , Connection.srcBytes =  X 0 X , Connection.dstBytes =  X 0 X  5%  X  X  X  Traf fi c.wrongFragment =  X 0 X  ( c = 100% ) R : Connection.duration =  X 0 X , Connection. fl ag =  X  X F X , Connection.srcBytes =  X 0 1032 X , Connection.dstBytes =  X  &gt; 100 X , Connection.count =  X 0 100 X  12%  X  X  X  Login.loggedIn =  X 1 X  ( c = 67% ) R 10 : Connection.duration =  X 0 X , Connection. fl ag =  X  X 0 X , Connection.srcBytes =  X 0 X , Connection.dstBytes =  X 0 X  5%  X  X  X  SameHost.SameSrvRate =  X 0 02 X  ( c = 94% ) R 11 : Attack.hasClassi fi cation =  X  X ormal X  where the values have the following meaning:  X   X 0 1032 X : from 1 to 1032 bytes;  X   X 0 100 X : from 1 to 100 connections to the same host as the current connection within the past two  X   X  X 0 X : Flag S0 signals only the SYN packet seen in a particular connection.  X   X 0 02 X : from 1% to 20% of connections to the same service.

On the other hand, we could increase the minimum support threshold to retrieve the most important events, thus discarding the infrequent ones.

These rules are wellsuited forsupporting intrusion detection systems basedonclassi fi cation techniques, because they can be integrated with the classi fi ers themselves, thus improving their performance and enriching the knowledge of the data.
 To conclude this part, Table 4 shows statistics regarding six runs of the system using the Intrusion Detection dataset.

For each test, we used different setti ngs for hub and authority thresholds ( HITSxONTO parameters), and for the minimum support (a PATTERNIST parameter). We then measured the intermediate results of each analysis step: the number of IR Schemas, the number of candidate frequent itemsets, and the number of fi nal IRs.  X  X ax length X  is the maximum length of a rule in terms of the number of items. As expected, by relaxing the thresholds, the number of IR Schemas increases (compare Test2 with Test6, for example). By decreasing the minimum support, for equal values of both ThresholdH and ThresholdA, the number of candidate frequent itemsets increases (see Test1 vs. Test4). The number of IRs increases accordingly.

A general consideration, for which it would be too long to provide data in this paper, is that the number of IRs strongly depends on the choice of the most important concepts, and on the number of different values that the data can assume (i.e. on the way in which the dataset is discretized). The choice of the most important concepts is made on the basis of the T-Box, by evaluating the connectivity of each concept. The T-Box of an ontology is usually described by experts without considering a speci fi c set of data as instances according to the intended semantics. When the ontology is instantiated for a speci fi c application, the actual data may not comply with such an intended general semantics. In fact the schemas are made up of selected concepts (termed  X  X mportant X  because they are selected through the mining process), but the instances may not support the schemas when the inductive step is performed. There are several possible reasons for this: missing data, not enough data, and no matches for domain concepts and collected data.

When the support of the instances is missing, the corresponding rule schemas are deleted from the set of potential rules and, as a consequence, no IRs are generated. The number of generated rules is also in fl uenced by the way the values are discretized; discretization acts directly on the number of frequent itemsets and on the support.

Comparing Test1 and Test2 provides some evidence of this. For equal values of Min Supp, when some important concepts (with many instances in the dataset) are missing (because we increased ThresholdH and ThresholdA), the number of IRs decreases a lot (moving from 11 to 2).

Note that the mining process on the T-box does not miss any of the concepts. Each concept in the ontology obtains a score in relation to its relationships (connectivity). The process does not modify the connectivity but it discovers hidden relationships. The higher the acceptance thresholds, the smaller the set of important concepts the algorithm returns. Interest in concepts with a lower score requires a relaxation of the threshold. Consequently, with more concepts than before, more IR schemas and thus rules can arise. As previously stated, the importance of a concept is a  X  X elative notion X ; a weakly connected concept can also be part of the set of important concepts.

The above experimentation illustrates how dangerous an attack can be, however the target attribute of the dataset (i.e. the kind of attack  X  X ttack.hasClassi fi cation X ) never appeared as a consequent in any IR. Instead it would be very interesting, to have IRs that suggest possible causes of facts related to the kinds of attacks.

To achieve this new objective, we modi fi ed the ontology a little, alt hough maintaining the content and the semantics of the ontology itself. We replaced two object properties associated with the Attack concept with the corresponding inverse object properties as shown in Fig. 17.

Technically, speaking in terms of the ontology evaluation, this modi fi cation strengthens the hubness of the Attack concept, thus making it appear among the implicated candidates.

As we had hoped, the new set of IRs, which were mined by running the system on the new ontology maintaining the same dataset and settings, con fi rmed our hypothesis: in fact it contained the item Attack.hasClassi fi cation as the consequent of some rules.
 Below is the list of the new set of IRs: NR 1 : Network.protocolType =  X  X cp X , Consequence.hot =  X 0 X , Consequence.numCompromised =  X 0 X  14%  X  X  X  Attack.hasClassi fi cation =  X  X ormal X  ( c = 37% ) NR 2 : Network.protocolType =  X  X cmp X , Consequence.hot =  X 0 X , Consequence.numCompromised =  X 0 X  52%  X  X  X  Attack.hasClassi fi cation =  X  X murf X  ( c = 99% ) NR 3 : Network.protocolType =  X  X cp X , Consequence.hot =  X 0 X , Consequence.numCompromised =  X 0 X  18%  X  X  X  Attack.hasClassi fi cation =  X  X eptune X  ( c = 49% )
NR 2 associates the type of protocol (icmp) to the type of attack, by saying that:  X  If the network protocol used in the communication is of type icmp and no malicious consequences are detected ( in terms of hot conditions and number of compromised systems ) , then the type of attack is a smurf with 52% probability of and 99% con fi dence  X .
NR 3 also associatesthe same items, but it says that:  X  If the network protocolused in the communication is tcp and no malicious consequencesare detected ( in terms of hot conditions and number of compromised systems ) , then the type of attack is a neptune with 18% probab ility of and almo st 50% con fi dence  X . 6. Conclusions and future works
In this article we have tackled the problem of extracting interesting and implicit knowledge out of an ontology, thus presenting the results in the form of in fl uence rules. Our idea was to drive the extraction process using the ontology structure, while only exploiting the instances in a second step. The main problem was understanding if and how to use traditi onal methods for DM in the context of the ontology. Clearly, traditional systems can only be used as models, however they are not directly applicable to ontologies. By decomposing the problem into sub-problems, we succeeded in fi nding a methodology, taking inspiration from consolidated theories (pattern discovery methods) and recent developments (new strategies based on link analysis).

Our approach is based on the idea that using general knowledge, via the knowledge of experts incorporated in the T-Box, and usefully combined with the inductive knowledge extracted from the A-Box, can provide better results than just directly using the A-Box. This approach is important when there is not enough data or the data available is not suf fi ciently representative of the domain.
Our experiments in the fi eld of economics support this hypothesis and we are planning experiments in other fi elds as further evidence.

Besides the theoretical results, we tested our system in an actual case by exploiting our involvement in a European industrial research project: MUSING. Thus we had at our disposal an integrated framework and a real set of data. In this domain, our analysis tool mainly solves the problem of the availability of expert knowledge. In fact, in the fi eld of economics, obtaining a cognitive net of relationships from rules are based on the beliefs of experts or their own experience).

The promising results gained in MUSING, encouraged us to extend the system and to carry out further experiments in a different domain (intrusion detection). Beside the opportunity of contributing to the creation of an ontology in this fi eld, the research yielded satisfactory results so that we are planning to improve the system further.

We believe that one of the many advantages of our approach is the interpretability of the rules, thanks to the semantics provided by the ontology. From the ontology we get not only a list of concepts but also their description and the meaning of their relationships. The process of interpretation, however, is still not fully automatic and, as happened in MUSING, the support of the domain expert is valuable. Thus the challenge is to exploit the information (explicit and hidden) in the ontology and from other sources as much as possible, in order to automate the interpretation process, especially because the support of experts is not always available.

Two of the changes will involve the use of the quickest and most ef fi cient algorithm for pattern discovery, and the creation o f a user-fri endly interface.
 Acknowledgments The authors would like to thank the MUSING project for their support of this research. References
