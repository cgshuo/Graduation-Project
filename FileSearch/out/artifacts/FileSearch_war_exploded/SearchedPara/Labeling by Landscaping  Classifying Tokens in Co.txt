 State-of-the-art approaches to token labeling within text documents typically cast the problem either as a classifica-tion task, without using complex structural characteristics of the input, or as a sequential labeling task, carried out by a Conditional Random Field (CRF) classifier. Here we ex-plore principled ways for structure to be brought to bear on the task. In line with recent trends in statistical learning of structured natural language input, we use a Support Vector Machine (SVM) classification framework deploying tree ker-nels. We then propose tree transformations and decorations, as a methodology for modeling complex linguistic phenom-ena in highly multi-dimensional feature spaces. We develop a general purpose tree engineering framework, which enables us to transcend the typically complex and laborious process of feature engineering. We build kernel-based classifiers for two token labeling tasks: fine-grained event recognition, and lexical answer type detection in questions. For both, we show that in comparison with a corresponding linear kernel SVM, our method of using tree kernels improves recognition, thanks to appropriately engineering tree structures for use by the tree kernel. We also observe significant improvements when comparing with a CRF-based realization of structured prediction, itself performing at levels comparable to state-of-the-art.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  text analysis, language parsing and understanding  X  W ork done at IBM Research during a summer internship. Algorithms, Design support vector machines, tree kernels, token classification
Among classification tasks in text analysis is a subclass characterized as assigning a binary label to each token in a given text span. Typically, the label assigned to any given token depends on the larger context in which the token is situated. Broadly speaking, current systems for such tasks are implemented as binary classification, either by suitably deriving features from the environment surrounding the to-ken, or by using structured prediction inherent to sequence labeling, such as Conditional Random Field (CRF) [12].
One example of such a task is event recognition, where the system must identify words within a document that denote events X  X efined in their broad linguistic sense, as situations that happen or occur, denoted largely by verbs, event nom-inals, or predicative adjectives. Different approaches (dis-cussed in more detail in the next section) appeal to some no-tions of structure (shallow parse, SVO trigrams, local domi-nance, and so forth), but these are not necessarily informed by principled analysis of feature informativeness.
For us, this offers an opportunity to explore advances in machine learning and classification, which actively uses the complex and intertwined characteristics of (linguistic) struc-ture as complex features X  X n contrast to the approaches we discuss in the next section, approaches that do not use this structure in a principled way, and rely on manually engi-neered features.

Recent research [16] on statistical learning of natural lan-guage structured input seeks to develop methods for more di-rect encoding of inter-dependencies among elements of com-plex representations, both bypassing the need for non-trivial feature (and feature inter-dependence) engineering, and pro-viding globally optimizing learning algorithms with more di-rect access to such data elements. Tree kernels, in particular, used within kernel methods (such as SVMs [26, 10, 27]), of-fer an effective way to capture possible correlations between f eatures and categories of features.

Consequently, many classification tasks where the data can be transformed into tree structures have been shown to benefit from tree kernels (see Section 2). Tree kernels are directly applied to such structures within the classification models, reducing, or even eliminating, the need for inten-sive feature definition based on introspection over domain knowledge. Feature engineering is thus replaced with the engineering of tree structures appropriate for the encapsu-lation of the essential information required for the effective application of tree kernels.

In general, the process of engineering trees is faster than engineering features, and therefore has the potential to be-come an enabling tool in the machine learning toolkit. How-ever, existing approaches to engineering tree structures have been ad-hoc and largely task-specific. The literature pro-vides little guidance on how one may go about generating tree structures for a class of tasks employing tree kernels.
In this paper, we begin to articulate a general method-ology for generating tree representations appropriate to the general class of tasks outlined earlier X  X amely, those that re-quire classifying tokens within their context. Our method-ology provides a convenient way to rapidly engineer tree representations and to train quality classification models.
To develop the specific methodological steps, we defer to existing work using tree kernels, where a small number of  X  X perations X  are consistently used in engineering trees. These include, in particular, operations for tree transforma-tions and enrichment (we offer an outline in Section 3 below), as primary components of a flexible mechanism for modeling complex linguistic phenomena in highly multi-dimensional feature space. It has been shown that such tree  X  X runing X  and X  X ecoration X  X perations improve system performance, as they can reduce structural variability and inject contextual and semantic information into the feature space. Conse-quently, we have developed a general-purpose tree engineer-ing framework, which supports tree transformations, includ-ing pruning and decorating to facilitate rapid experimenta-tion. We motivate our design with the need for exposing data elements of different nature to the classification algo-rithms.

The framework we describe builds on the basic notion of representing classifier instances as trees, with the operations above providing for enrichment and focusing of these trees. We also present the particular strategies for tree adaptations aimed at using structured input, given general characteris-tics of a category of classification tasks.

In this work, we also use our general methodology to in-vestigate the novel application of tree kernels to two token labeling tasks. First, we demonstrate the effectiveness of our approach by showing results for event recognition, where our best performing system is a combination of linear and tree kernels, utilizing tree shapes derived from complete parses. In this setting, the system also stacks well against the ac-cepted baseline for the task. We note that while SVMs have been previously used for event detection, these have not used tree kernels. In our second application, we then follow the same approach, but for a very different token classification task: lexical answer type detection. The task is to find within a given question, the word that expresses the type of the answer (a more detailed description of the task is presented in Section 4). This is a particularly critical com-ponent of open-domain question answering. Again, an SVM classifier capturing context with our tree kernel methodol-ogy, in addition to a manually engineered feature set, shows improvement over systems using only manually engineered features.
In this section, we first summarize existing work that employs tree kernels, and proffer that: (a) the techniques for generating tree representations are typically ad-hoc and task-specific, (b) have not been previously used to label to-kens within their context. Second, we examine work in the literature addressing the two tasks to which we apply our methodology in this paper.
Applying kernels, especially those for structured input, re-quires kernel engineering. This includes, in particular, the design of structures appropriate for the task at hand. Early work in this direction either used portions of syntactic con-stituency trees [19], or defined new dependency or shallow syntactic structures [28, 8]. More recent work promotes the notion of marking or enriching target nodes of constituency trees [29, 21], similarly to what we do (Section 3 below) for marking the target in our dependency structure.

The flexibility and effectiveness of tree kernels is apparent from the various types of tasks that they have been applied to X  X anging from relation extraction [8] to semantic role la-beling [21] to question classification [22]. In each of these, we find that the tree representations used by the tree kernels are derived from a syntactic parse or predicate argument struc-ture (PAS), and typically transformed in some way for the task at hand. Based on observations of some common tree transformation operations in these previous approaches, we aim to establish a general purpose methodology in this pa-per. For instance, in the question classification work by Mos-chitti et al., we see that they use  X  X lot X  parent nodes in the PAS tree to indicate PAS argument labels. Our tree  X  X ec-oration X  operations described later in the paper are loosely based in this idea. Similarly, in the relation extraction work, Culotta and Sorensen keep only the smallest common sub-tree that includes the entities of the relation. This is akin to the tree pruning operation we describe later in the paper. Our main contribution in this paper is to provide a general purpose methodology to effectively apply tree kernels to a specific class of classification tasks and quickly enable the classifier to explore a rich feature space with little effort.
A related approach for exploring a richer feature space without explicit feature engineering is that of kernel engi-neering (e.g., [7]). However, this requires expertise in devel-oping novel kernels for specific tasks. Our goal in this paper is to rely on existing tree kernel technology and focus our efforts, instead, on tuning the inputs to the kernel so as to achieve an appropriate feature space representation for our tasks. Our methodology provides the flexibility to add var-ious types of information (such as semantic classes, named entities, etc.) within the same tree representation frame-work, enabling the feature space to capture complex inter-actions between syntactic, lexical and semantic attributes of each node in the tree. Such a feature space provides accurate and high generalization, which allows for learning classifiers with much less training data. Indeed, experiments in Section 5 confirm this for our tasks.
Wi th small exceptions [25], all approaches to TimeBank-token tagging. In most cases, the task is that of sequential labeling of tokens by encoding chunk information into to-ken tags (IOB2 encoding scheme [24]). Classification, and interpretation of token sequences, differs across approaches: Boguraev and Ando [3] use a robust risk minimization classi-fier followed by Viterbi-style decoding; step [2] combines an SVM enhanced with a suite [11] for general-purpose chunk-ing; Llorens et al. [15] use a CRF model.

Most approaches share intuitions about morpho-lexical properties as event indicators; they also share the view that extraneous information can be beneficial: Boguraev and Ando [3] use a word-profiling technique [1] to exploit large unannotated corpora for tagging/chunking, and Llorens et al. [15] attribute the performance boost they report to the use of semantic roles. Structural characteristics of the in-put, however, are used in these event recognition classifiers minimally, and are cumbersome in their implementation as features. For instance, some syntactic information is en-coded in Boguraev and Ando X  X  features as word uni-and bi-grams based on subject-verb-object and preposition-noun constructions. Similarly, some phrasal information is ren-dered in feature form by Llorens et al., abstracted from a parse tree. Information (such as SRL and word profiling) beyond the  X  X ase X  feature set is clearly useful: for the two approaches cited, the  X  X ase X  classifier performance goes from 78.6%/78.67% (F-score) respectively, to 80.3%/81.40%.
There is no systematic way for such classifiers to cap-ture extraneous information. As we will see (in Section 3) however, configurational and semantic information can be encoded X  X n a principled way X  X n a tree kernel by pruning and decorations. This is the primary line of investigation in our work.
Question answering systems interchangeably use notions like  X  X ocus X ,  X  X nswer type X  and  X  X exical answer type X  (LAT). Without going into finer details, these terms refer to the words that indicate the type of entity being asked for by the question. For example, in a question like  X  X e was a bank clerk in the Yukon before he published  X  X ongs of a Sourdough X  in 1907. X  , LATs are X  X e X  X nd X  X lerk X . LATs are different from semantic types, and detecting them is another example of context-dependent token classification task. A particularly thorough study of LAT detection is presented by Lally et al. [13]; salient to their work is the statistical LAT detection procedure presented, which sets the bar for state-of-the-art performance. Relevant intuitions leading to feature design can be found in previous work by Li and Roth [14]. In sum-mary, a logistic regression classifier is trained for the task, with a feature set encapsulating numerous lexical and syn-tactic features, as well as the results of applying an extensive set of patterns developed specifically to match contexts in-dicative of LAT-ness. This classifier performs at close to 83.6% F-score.
As a classification task, event or LAT detection reduces to assigning a binary label to each token in a given document. Existing approaches include standard discriminative classi-fiers such as SVM, and sequence tagging approaches like CRF. All approaches follow a similar methodology. They analyze each token within a document in sequence, and iden-tify a set of features likely to indicate the  X  X ventness X , or  X  X AT-ness X  of that token. Machine Learning models are then trained using training data to classify each token as a positive or negative instance, based on the feature values associated with the token.

Many features capture only the context independent prop-erties of the tokens  X  such as their part-of-speech, tense, lemma, etc. While these features are quite informative, they are not sufficient to achieve state-of-the-art performance. For instance, most events are represented as either verbs or nouns (typically nominalizations of verbs) and, as a re-sult, part-of-speech is an important feature in the model. similar observations hold for the part-of-speech feature in a LAT model. However, beyond such inherent characteristics of tokens, their context is essential in resolving many of the more subtle or ambiguous cases. It is no surprise, then, that the best performing systems additionally model the context surrounding each target token.

The context surrounding the target token is typically cap-tured by either including properties of neighboring tokens as features or by identifying certain structural clues surround-ing the token, within the syntactic parse of the sentence. Take, for example, the event detection system implemented by Boguraev and Ando [3]. Their research employed features of the target token, such as: token/capitalization/part-of-speech (POS) in 3-token window , bi-grams of adjacent words in 5-token window , words in the same syntactic chunk , and word unigrams/bigrams based on subject-verb-object among others. Many such features were manually engineered to capture the context of a token through adjacency in a token window, through adjacency in syntactic chunks, and through proximity in the syntactic parse.

This type of feature engineering tends to rely heavily on human analysis of the data, and risks resulting in ad-hoc rules/patterns, that may not be able to model all the subtle pieces of contextual information required for the task. In this work, we take a more principled approach to modeling the context of tokens through the use of trees structures representing their context. SVM classifiers with tree kernels effectively exploit these tree structure to improve labeling performance over manually engineered features.
Tree kernels are an instance of a general class of convolu-tion kernels. First introduced by Haussler [9], they can be used to compare abstract objects, like strings, instead of fea-ture vectors. Convolution kernels involve a recursive calcula-tion over the  X  X arts X  of abstract objects. This calculation is made computationally efficient by using dynamic program-ming techniques. By considering all possible combinations of fragments, tree kernels capture any possible correlation between features and categories of features.

Tree kernels in the past have been successfully applied for numerous Natural Language Processing tasks such as parser and tagger re-ranking [6], and question answer classification [22]. These are classification tasks where each example to be classified is a sentence (and, as such, has an associated tree). Cases where examples can be naturally represented Figure 1: Dependency parse tree for the sentence: t he real referendum campaign has clearly begun . as trees also fall easily into the tree-kernel space (for e.g., semantic role labeling [21] and relation extraction [28, 8]).
More recently, the notion emerged that the tree repre-sentation in the kernel itself may be transformed X  X or in-stance, by pruning, to remove extraneous material [23], or by decorating, to add additional information [7]. Still not completely formalized, this notion is gaining ground; we will build upon it below.

In general, however, tree kernels have not been used for token labeling tasks like event or LAT detection. To the best of our knowledge, ours is the first work that explores the use of tree kernels, and presents a novel and effective data rep-resentation explicitly exploiting notions of tree transforma-tions, for these tasks. We describe below the representation of the tree structures and motivation behind this design.
While standard tree kernel classifiers have previously been used in numerous language processing tasks, the tree struc-tures representing the instances to be classified in these tasks have typically been developed on a task-by-task basis. For many tasks, instances for classification can be naturally transformed into tree representations that are provided as input to tree kernels. For instance, in sentence classification, the parse trees of the sentences are tree structures that can be input for tree kernel classifiers. A token within a sen-tence, on the other hand, does not naturally lend itself to a tree representation. The key contribution of this paper is an approach for effectively capturing the context of a token with a tree representation so as to enhance the feature space of the classification task with standard tree kernel technol-ogy.

To generate the tree representation of a token within its context, we begin with the dependency parse of the sen-tence in which the target token appears. We map the token to a node within the parse tree, and then apply some tree transformations to generate the final contextual tree given as input to the classifier. Our tree transformations include node  X  X ecorating X  operations and tree  X  X runing X  operations around the token.

Before we begin decorating and transforming operations, we first obtain a tree that represents the basic structure of the sentence containing the target token. We take the dependency parse of a sentence and label each node with its syntactic role in the tree. Each node in the tree corresponds to a token from the sentence. Since the tree kernel ignores edge labels, these are dropped from the structure. This tree, with node labels representing the relationship of each node to its parent is the starting point for our tree operations to follow. Figure 1 is an example of this tree for the sentence: the real referendum campaign has clearly begun . Note that the labels in red, outside the nodes, are the tokens from the sentence corresponding to the respective nodes. They are for illustrative purposes only, and are not part of the data structure used in our tree operations.

Our tree operations then enable the classifier to essentially focus on the token to be classified, and to view its relation-ships with the immediate context of the token. The opera-tions of focusing on a target token, and transforming the tree around it to both reduce noise and inject extra contextual information, are described in the following subsections.
Given a parse tree containing the target token to be clas-sified, we would first like to be able to  X  X dentify X  this token to the classifier in some way. Standard tree kernels operate on pairs of trees and compute a similarity score for each pair by counting the number of substructures that are common between the given input pair. The similarity score is essen-tially a function of these overlap statistics. Since we would like our trees to represent the relationships of the target token with its context, identifying the target token to the kernel essentially boils down to identifying any relationships between the target token and its neighboring nodes that are common between any two given trees. We try to achieve this by  X  X ecorating X  or marking the target node in the tree. There are several possibilities that can be explored: (a) re-place the label of the target node with an identifier (such as -target-) that is common across any pair of trees, (b) insert a parent node above the target, with an identifier that is common across trees, or (c) insert a child node below the parent, with an identifier that is common across trees.
In our work, we mark the target node by inserting a par-ent node with the label -target-above the target node. We tried several experiments, and found this to be the most ef-fective for our tasks (the differences between the three meth-ods, however, were very small). This decoration enables us to match relationships across trees between the target node and nodes above it in tree structures being compared by the kernel. Relationships between the target and nodes below it are matched by the kernel only if the target plays the same syntactic role in the trees being compared. Figure 2(a) il-lustrates the  X  X arget X  node decoration for the example tree, with the node corresponding to the word X  campaign  X  X arked as the target.
Many of the trees we typically deal with can come from long sentences, and are complex trees with many nodes and edges. For our classification tasks, many of the nodes in the tree can be in a parts of the tree completely separate and unrelated to the target node. Such nodes should not be considered as a part of the local context of the target node, and should have little or no impact on the classification de-cision. Leaving such nodes in the tree can allow these to match within the kernel, and could artificially increase the kernel similarity score of a pair of trees that do not actually represent a similar context for the target tokens. In other words, large trees can introduce noise into the contextual representation of target tokens. We, therefore, prune the trees around the target token to eliminate such noise. As an we add to the tree nodes. added benefit, we find that pruning also substantially speeds up the classifier (as it now needs to match fewer subtrees).
There are many possible ways in which a tree may be pruned to capture the essential context required by the clas-sifier. For instance, one may choose to keep only the direct ancestors of the target as its required context. Alternatively, one may choose to base pruning decisions using a more prin-cipled linguistic approach using the types of parse nodes and edges for pruning. Depending on the task, this could mean keeping only the pre-modifiers and post-modifiers of the target or restricting the pruning to the clause in which the target appears. We found that for our tasks, an effective pruning strategy was to keep only the direct ancestors and child nodes of the target, and discard all other nodes and edges in the tree. The dotted line in Figure 2(a) illustrates this pruning approach for the example sentence.
Having pruned the tree to restrict the context around the target, and marked the target node within the tree, we now perform one additional (and important) step to better repre-sent the context of the target  X  node decoration. Observe that after pruning, the tree contains only the information about the syntactic relationships of each node with its neigh-bors. Hidden in each node is a wealth of additional infor-mation about the context that could be effectively exposed in the tree. For instance, each node in the tree represents a token, which has a lexical surface form, part of speech, se-mantic class information, etc. that could be effectively used by the classifier in its decisions. We incorporate these  X  X ode features X  as  X  X ecorations X  on the tree.

We decorate each node corresponding to a token with to-ken features by inserting them as new child nodes with labels encoding the token features. For instance, we add new child nodes  X  X emma=campaign X ,  X  X art-of-speech=noun X , etc. to the node corresponding to the token  X  X ampaign X . Figure 2(b) illustrates these new token feature nodes (with blue la-bels) in our example tree. Due to space constraints we do not show the feature names ( X  X emma= X ,  X  X art-of-speech= X , etc.) in the figure. The set of node features can be obtained from existing NLP tools, such as the parser, a named entity tagger, semantic class detector, and the like.

This tree representation combines many categories of fea-tures (lexical, syntactic and semantic) in one succinct rep-Figure 3: Example partial trees in tree kernel fea-t ure space. resentation. This gives the learner an opportunity to learn combinations of features which would otherwise be tedious, if not impossible, for humans to encode in an explicit fea-ture space. For this reason, we use a Partial Tree (PT) kernel (first proposed by Moschitti [20]) for defining the im-plicit feature space of the learner. A PT kernel calculates the similarity between two trees by comparing all possible subtrees. For example, some of the subtrees in the implicit feature space of a partial tree kernel on the tree in Figure 2(b) are shown in Figure 3. This means that the tree kernel attempts to combine all of the information available to it in different ways, resulting in various levels of abstraction depending on which pieces of information were selected for a particular subtree. One subtree may drop all lexical items of the selected nodes, keeping only syntactic and semantic information for the included nodes. Another may keep only the lexical information. And yet another have a combination of these different pieces of information for different nodes in the subtree. As one would imagine, this results in a massive (implicit) feature space of all possible subtrees. However, learning and classification is made tractable by the kernel, which uses dynamic programming for an efficient enumera-tion of this space. Thus, with the use of this approach, it is not necessary to manually engineer features at all levels of abstraction.
T he event detection task is a token-in-context labeling problem, in the sense defined earlier, in Section 1. Con-ventionally, however, many of the approaches to date (as discussed in Section 2) treat it as a sequential tagging prob-lem. For us, it is an instance of a more general category of classification problems, interesting because of its intrin-sic characteristics which make it a suitable example for the application of our tree kernel-based methods. In order to demonstrate broader applicability of these methods, we also focus on another token labeling task, namely that of LAT detection. The results we report below (Section 5) underpin our claim for the effectiveness of our tree transformations-based methods of leveraging tree kernels for token labeling. In future work, we plan to apply these methods to other to-ken labeling tasks, such as, for instance, markable detection for coreference resolution.
Events in general text may be expressed in a variety of ways; morpho-syntactically, they may be realized by means of untensed or tensed verbs, nominalizations, predicative ad-jectives or clauses, or certain prepositional phrases. For ex-ample, only some of the nouns and verbs in the following sentence are to be labeled as event mentions.

Several pro-Iraq demonstrations have taken place in the la st week.
 In line with the event recognition work discussed in Sec-tion 2, we use the TimeBank event-specific data set to train our models, as well as to evaluate their performance on the event recognition task.

The distribution of event mentions in the corpus follows the natural distribution of event-denoting linguistic tokens in (news) discourse. There are 56,632 (non-punctuation) tokens in TimeBank, of which 7,921 are captured by event annotations. TimeBank is relatively small, and Boguraev and Ando [4] discuss its make-up in some detail.

Practically, only a tiny proportion of event instances in the data are realized as multi-token mentions. In fact, the guide-lines for TimeBank-compliant event annotation are heavily leaning to a convention which favors marking single tokens as  X  X roxies X  for event-denoting phrases. It turns out that in the entire TimeBank corpus (  X  57K word tokens), there are only 30 or so multi-token event mentions. This imbal-ance is such that the rationale for casting the task as that of multi-class sequential tagging X  X s most of the approaches in Section 2 do X  X s questionable: it is unreasonable to expect that concurrent models can be built for  X  X egin X  and  X  X nside X  event tags from such impoverished data.

From our point of view, this is sufficient justification to model the event recognition task as a binary token classifi-cation.
The Lexical Answer Type (LAT) detection task (a subtask of question answering) is also a binary token classification problem of identifying an answer type string in a question. For example, in a question like:
Which US president was first to be re-elected for a s econd term? the text span  X  X resident X  is the LAT (it identifies what type of entity is being asked for in the question). Multi-token Table 1: The set of features we add as children to t he nodes of the dependency parse tree.
 LATs may apparently (and occasionally) be encountered in the data (e.g.  X  X ice president X  ), but our parser (see Sec-tion 5.1 below) handles these as single tokens.
 The dataset we use is the one described in Lally, et al. [13]. It is a manually annotated set of 9,128 Jeopardy! questions. In terms of class distribution, this question data set contains of the order of 111,597 tokens, 11,906 of which are positive (LAT) examples. The dataset is split into 8,000 training and 1,128 test questions (97,741 training tokens and 13,856 testing). Given the observation in Section 2, concerning the relatively small volumes of training data sufficient for learn-ing good classification functions when the tree kernel can capture dependencies between lexical, syntactic and seman-tic features, this size of dataset turns out to be adequate. We will return to this point in the next section (5).
Within the framework and methodology described in the previous section, we have developed classifiers for event de-tection and LAT detection. In both cases, we demonstrate the effectiveness of tree kernels, by enhancing an existing manually feature engineered model with our tree kernel. This is done by implementing the manually feature engi-neered model using a linear kernel, and combining that with our tree kernel. Note that by design, in the tree kernels set-tings there are additional features in the feature space: this reflects our approach of letting the kernel identify salient structural characteristics present in the data, and use them as features in a highly multi-dimensional space. The combi-nation of linear and tree kernels is done using a composite kernel that sums the two. Our SVM classifier uses the com-posite kernel for the task at hand.
In this work, we use dependency parses derived from the output of the English Slot Grammar parser [17]. Each to-ken in that sentence then maps to a specific node in the dependency parse. A node is only labeled with the syntac-tic role of the token within this parse. We then apply the tree transformations (described in Section 3) to obtain the  X  X ontextual trees X  for the tokens. This includes marking the target with a parent node, pruning the tree around the tar-get, and inserting feature  X  X ecorations X  as child nodes. The various types of information that we attach as child nodes are obtained from the syntactic parser are listed in Table 1.
Our experiments are designed to investigate our claims: namely, that combining linear and tree kernels outperforms individual kernel settings, and that a tree kernel utilizing pruning and decorations in a particular way can effectively drive a classifier (SVM) for token labeling tasks. For both t asks, a model based on the linear kernel uses a manually en-gineered set of features. For event detection, this is loosely based on the feature set from Boguraev and Ando [3]. For LAT detection, we refer to the feature set from Lally et al. [13]. The features broadly fall into lexical, morphological, and syntactic categories, and capture sequentiality of tokens by observing occurrences of lexical elements (tokens, lem-mas), and/or their properties (orthography, part-of-speech, inflectional information) in 3-or 5-token windows.
The features include: token, capitalization, part-of-speech (POS) in 3-token window; bigrams of adjacent words in 5-token window; and trigrams of POS, capitalization and word suffix. To the extent that it is used, structural information is derived largely by observing the output of a parser. In con-trast to the work by Boguraev and Ando [3], who use a shal-low parser, we extract features from a deep syntactic tree; this allows us to have reliable word uni-and bi-grams based on subject-verb-object and preposition-noun constructions. Additional grammatical information is injected into the fea-ture space by mapping the labels of immediately dominating phrases, and by windowing over adjacent base phrasal units: we derive features for words in the same (base) syntactic phrase, and for head-words in 3-phrase window.

The two sets of features (for event, and for LAT, detec-tion) largely overlap; the differences, where they exist, stem from the different nature of the two tasks. For instance, for lexical answer types, the shape of the encompassing phrasal unit (typically a noun phrase) is very important; conse-quently, there are a large number of structural pattern rules, which detect LAT-indicative contexts. Even so, as the re-sults show, the tree kernels are able to identify additional salient features.

Further, and in contrast to Boguraev and Ando [3], who do not use semantic features, we bring semantics into the feature space by deferring to the semantic properties of parse nodes, as they are constructed by the parser and integrated into larger, semantically coherent, constituents. Our parser implements its own set of ontological types, and some of these bear directly upon both the task of determining the eventness of a verb or a noun and the task of determining that a word in a question acts as its lexical answer type.
All the feature sets from Boguraev and Ando [3], Lally et al. [13], and our implementations have been shown to achieve good accuracy at their respective tasks. Overall, the features have been manually engineered to represent the attributes of the token and its surrounding context within the linear kernel. An SVM trained only on the linear kernel forms a baseline system for comparison.

Given that the tasks are somewhat akin to sequence la-beling, our experiments also include results from applying a linear chain CRF [12], a popular and effective discrimina-tive technique for supervised sequence labeling. CRFs are undirected graphical models, a special case of conditionally-trained finite state machines. This makes them appropriate (especially in the event detection case), since a CRF by its nature does a form of structured prediction, and thus incor-porates the structure (albeit only  X  X equential X  structure) of the data in its decision making process.

Intended as an additional baseline, our CRF implementa-see http://sourceforge.net/apps/trac/minorthird/ SVM-prune-tree+lin 94.36% 0.754 0.874 0.810 SVM-prune-tree+lin 96.46% 0.816 0.861 0.838 i ng linear kernels (see below), for both tasks. In the case of event detection, our CRF baseline runs at a level compara-there are no CRF results reported to compare to, but our CRF-based LAT classifier offers a data point which further confirms the superiority of the framework and method we propose here.
Table 2 presents the results of our experiments. All our event detection models are trained on a 150 document train-ing set of randomly chosen documents from the 183 Time-bank documents. We evaluate these on the remaining 33 documents. Similarly, out LAT detection models are trained on 8,000 Jeopardy! questions and evaluated on 1,128 ques-tions, all annotated with LATs. We report overall accuracy and also precision, recall and F-score on the positive class. The row labeled SVM-lin contains the performance of the linear kernel SVM using the manually engineered feature set; the row labeled CRF contains the performance of our CRF baseline; the row labeled SVM-prune-tree contains the performance of our tree kernel SVM model; the row labeled SVM-full-tree+lin contains the performance of our model which is a combination of the linear and full parse tree ker-nel; the row labeled SVM-prune-tree+lin contains the per-formance of our model which is a combination of the linear and pruned parse tree kernel. We were unable to generate performance scores for LAT detection with the SVM-full-tree+lin configuration primarily due to the sheer size of the trees and the size of the data set, that required for too many computational resources for the experiment to complete suc-cessfully.

Our results show that our SVM-prune-tree+lin configu-ration outperforms the CRF baseline X  X or both tasks X  X y 2.3% F-score absolute on event detection, and by 1.5% F-score absolute on LAT detection. As we pointed out earlier (Section 5.2), our CRF classifier for event detection is run-ning at state-of-the-art level, as its feature set compares with an experimental setup (orthographic, morpho-syntactic, and lexical semantic, without semantic roles) reported by Llorens et al. [15]: F=78.67%. The additional claim of that work, that CRFs outperform SVMs for event detection, refers to a very different SVM configuration to ours, and should only be considered in the context of that work. the  X  X ase X  classifiers used by Boguraev and Ando [3] and Llorens et al. [15], which are very close, at 78.6% and 78.67% respectively.
SVM-prune-tree+lin 94.36% 0.754 0.874 0.810
SVM-prune-tree+lin 96.46% 0.816 0.861 0.838 Table 3: Ablating mark target and tree decorations.
Ou r best system also outperforms the SVM-lin baseline by 1.0% F-score absolute (statistically significant with p &lt; using McNemar X  X  test [18]) for event detection and by 0.9% F-score absolute (statistically significant with p &lt; 0 LAT detection. While tree kernels alone perform worse than the two baselines, when combined with the linear feature space, we achieve our best performing system. Clearly, there is structural information offered by the kernel design that is absent in both the CRF and linear kernel baseline.
Our results show that pruning is crucial. In comparison to the configuration using the full parse tree (SVM-full-tree+lin), the pruned version (SVM-prune-tree+lin) per-forms substantially better. We also measured the impact of marking the target nodes in the trees and that of deco-rating the nodes with features as child nodes by ablating these from the SVM-prune-tree+lin configuration. Table 3 presents these results. The rows labeled  X  X blate mark-target X  are the results for the SVM-prune-tree+lin config-uration without the target node marked with parent node. The rows labeled X  X blate mark-target X  X re the results for the SVM-prune-tree+lin configuration without the child node decorations. In both tasks, we see that the feature decora-tions clearly have the most impact, while marking the target node has a lower (but positive) impact.

Previous work has also shown that the use of tree kernels can reduce the amount of training data required to train SVM classifiers. We demonstrate that this is also true for our tasks by generating learning curves. We evaluate models trained on randomly sampled subsets of the training data. Figures 4 and 5 present the learning curves for the event de-tection and LAT detection, respectively. The x-axis in each graph is the percentage of training data used for training model, and the y-axis represents the F-score of the model on the test data. Each figure has one graph (Linear) gen-erated for the basline (SVM-lin) configuration of the SVM and one graph (Linear+TK) for the tree kernel (SVM-prune-tree+lin) configuration. It is clear from both figures that the tree kernels can learn a good model with far fewer training examples as compared to the linear kernel baseline.
In our experiments, we do not offer direct comparison with related work. This reflects, partly, lack of a uniform base-line. While performance is reported in experimental settings with cross-validation over the entire corpus, the nature and relative weight of the individual folds are not identical. As there is no develop/train/test split of the reference corpus (stratified or otherwise), the measures reported in the lit-erature can not be directly comparable. In any case, our primary focus in this work is not to come up with an event, or LAT detection device, which will best, say, CRF methods for the same task; rather, we are interested in the novel use of structured input (tree-kernel-based) for a sequence labeling task, which allows us to implement an alternative classifica-tion framework, performing at, or close to, state-of-the-art levels. Even though we cannot directly compare our systems with the state-of-the-art, our performance numbers suggest that both our systems are within state-of-the-art levels.
The work presented in this paper was originally framed as a classification-based framework for recognizing events in text. Fundamentally, however, we were motivated to study the applicability, and effectiveness, of recent work on tree kernel-based methods for structured-input based learning. More specifically, the event detection task highlighted a class of labeling applications where the goal is to assign binary labels to token targets, and where correct labeling crucially depends on the larger context.

Common to the classification frameworks developed for token labeling tasks are the intrinsic characteristics of fea-ture sets for the respective classifiers: manual feature engi-neering underpins all efforts, and invariably is largely con-cerned with exposing lexical, morpho-syntactic, and in some cases semantic features. While it is recognized that contex-tual dependencies are of importance, corresponding features capture only approximations of such dependencies, within a tiny window, at that.

Structural characteristics of the inputs, which may well carry significant signal for the task at hand, are hardly en-coded in the feature set. We propose a set of extensions to an SVM-based classifier deploying tree kernels, such that the information intrinsic in a parse tree representation of the input can be directly folded into multi-dimensional feature space.  X  X aw X  trees, however, may be somewhat distracting, as well as incomplete with respect to the broad set of data categories which may be of relevance to the classification task. Our framework builds on notions of tree pruning and decorating, to provide for refocusing the learning algorithms to appropriate signal-bearing tree fragments, additionally adorning them with extrinsic information. In effect, we re-introduce feature engineering, but instead of introspecting about individual features, we allow for such multiple fea-tures, and inter-dependencies therebetween, to be directly accessible by globally optimizing learning algorithms.
In the methodology we have developed, we recast the token labeling tasks appropriately, re-purposing traditional features as decorations on suitably pruned tree fragments; these now become richly adorned representations of the to-ken targets, and the focus of new classifiers deploying tree kernels, which capture structural inter-dependencies between contextually related data elements. We have instantiated this methodology for two diverse labeling tasks, event recog-nition and lexical answer type detection. Our results show that the information coming from the tree kernel comple-ments the information captured in a manually engineered set of features. This is manifested in better resuts: the new (composite kernel) classifiers show statistically signifi-cant improvement over configurations using manually engi-neered features. This research is supported in part by Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading Pro-gram. [1] R. K. Ando. Exploiting Unannotated Corpora for [2] S. Bethard and J. H. Martin. Identification of Event [3] B. Boguraev and R. K. Ando. TimeML-compliant [4] B. Boguraev and R. K. Ando. Analysis of TimeBank [5] W. Cohen. MinorThird: Methods for Identifying [6] M. Collins and N. Duffy. Convolution kernels for [7] D. Croce, A. Moschitti, and R. Basili. Structured [8] A. Culotta and J. Sorensen. Dependency Tree Kernels [9] D. Haussler. Convolution Kernels on Discrete [10] V. Kecman. Learning and Soft Computing . The MIT [11] T. Kudo and Y. Matsumoto. Chunking with Support [12] J. Lafferty, A. McCallum, and F. Pereira. Conditional [13] A. Lally, J. Prager, M. McCord, B. Boguraev, [14] X. Li and D. Roth. Learning Question Classifiers: The [15] H. Llorens, E. Saquete, and B. Navarro-Colorado. [16] L. M`arquez and A. Moschitti. Special Issue on [17] M. McCord. Slot Grammar: A System for Simpler [18] Q. McNemar. Note on the Sampling Error of the [19] A. Moschitti. A Study on Convolution Kernels for [20] A. Moschitti. Efficient Convolution Kernels for [21] A. Moschitti, D. Pighin, and R. Basili. Tree Kernels [22] A. Moschitti, S. Quarteroni, R. Basili, and [23] T.-V. T. Nguyen, A. Moschitti, and G. Riccardi. [24] L. Ramshaw and M. Marcus. Text Chunking using [25] R. Saur  X  X , R. Knippen, M. Verhagen, and [26] V. Vapnik. The Nature of Statistical Learning Theory . [27] L. Wang, editor. Support Vector Machines: Theory [28] D. Zelenko, C. Aone, and A. Richardella. Kernel [29] M. Zhang, J. Zhang, and J. Su. Exploring Syntactic
