 Magnetic resonance imaging (MRI) [10, 6] is a very flexible imaging modality. Inflicting no harm on patients, it is used for an ever-growing number of diagnoses in health-care. Its most serious limitation is acquisition speed, being based on a serial idea (gradient encoding) with limited scope for parallelization. Fourier (aka. k -space) coefficients are sampled along smooth trajectories (phase encodes), many of which are needed for reconstructions of sufficient quality [17, 1]. Long scan times lead to patient annoyance, grave errors due to movement, and high running costs. The Nyquist sampling theorem [2] fundamentally limits traditional linear image reconstruction, but with modern 3D MRI scenarios, dense sampling is not practical anymore. Acquisition is accelerated to some extent in parallel MRI 1 , by using receive coil arrays [19, 9]: the sensitivity profiles of different coils provide part of the localization normally done by more phase steps. A different idea is to use (nonlinear) sparse image reconstruction, with which the Nyquist limit can be undercut robustly for images, emphasized recently as compressed sensing [5, 3]. While sparse reconstruction has been used for MRI [28, 12], we address the more fundamental question of how to optimize the sampling design for sparse reconstruction over a specific real-world signal class (MR images) in an adaptive manner, avoiding strong assumptions such as exact, randomly distributed sparsity that do not hold for real images [23]. Our approach is in line with recent endeavours to extend MRI capabilities and reduce its cost, by complementing expensive, serial hardware with easily parallelizable digital computations.
 We extend the framework of [24], the first approximate Bayesian method for MRI sampling opti-mization applicable at resolutions of clinical interest. Their approach falls short of real MRI practice on a number of points. They considered single image slices only, while stacks 2 of neighbouring slices are typically acquired. Reconstruction can be improved significantly by taking the strong statistical dependence between pixels of nearby slices into account [14, 26, 18]. Design optimiza-tion is a joint problem as well: using the same acquisition pattern for neighbouring slices is clearly redundant. Second, the latent image was modelled as real-valued in [24], while in reality it is a complex-valued signal. To our knowledge, the few directly comparable approaches rely on  X  X rial-and-error X  exploration [12, 16, 27], requiring substantially more human expert interventions and real MRI measurements, whose high costs our goal-directed method aims to minimize.
 Our extension to stacks of slices requires new technology. Global Gaussian covariances have to be approximated, a straightforward extension of which to many slices is out of the question. We show how to use approximate Kalman smoothing, implementing message passing by the Lanczos algorithm, which has not been done in machine learning before (see [20, 25] for similar proposals to oceanography problems). Our technique is complementary to mean field variational inference approximations ( X  X ariational Bayes X ), where most correlations are ruled out a priori . We track the dominating posterior covariance directions inside our method, allowing them to change during optimization. While our double loop approach may be technically more demanding to implement, relaxation as well as algorithm are characterized much better (convex problem; algorithm reducing to standard computational primitives), running orders of magnitude faster. Beyond MRI, applications could be to Bayesian inference over video streams, or to computational photography [11]. Our approach is parallelizable on several levels. This property is essential to even start projecting such applications: on the scale demanded by modern MRI applications, with practitioners being used to view images directly after acquisition, little else but highly parallelizable approaches are viable. Large scale variational inference is reviewed and extended to complex-valued data in Section 2, lifted to non-Gaussian linear dynamical systems in Section 3, and the experimental design extension is given in Section 4. Results of a preliminary study on data from a Siemens 3T scanner are provided in Section 5, using a serial implementation. Our motivation is to improve MR image reconstruction, not by finding a better estimation technique, but by sampling data more economically. A latent MR image slice u  X  C n ( n pixels) is measured by a design matrix X  X  C m  X  n : y = Xu +  X  (  X   X  N ( 0 , X  2 I ) models noise). For Cartesian MRI, X = I S,  X  F n , F n the 2D fast Fourier transform, S  X  { 1 ,...,n } the sampling pattern (which partitions into complete columns or rows: phase encodes , the atomic units of the design). Sparse reconstruction works by encoding super-Gaussian image statistics in a non-Gaussian prior, then finding the posterior mode (MAP estimation): a convex quadratic program for the model employed here. To improve the measurement design X itself, posterior information beyond (and independent of) its mode is required, chiefly posterior covariances .
 We briefly review [24], extending it to complex-valued u . The super-Gaussian image prior P ( u ) is adapted by placing potentials on absolute values | s j | , the posterior has the form Here, B is a sparsity transform [24]. We use the C  X  R 2 embedding, s = ( s j ) , s j  X  R 2 , optimization problem by lower-bounding the log partition function [7] (intuitively, each Laplace to P ( u | y ) , with A = X H X + B T  X   X  1 B and u  X  = u  X  (  X  ) . The complex extension is formally B [24]. Used within an automatic decision architecture, convexity and robustness of inference become assets that are more important than smaller bias after a lot of human expert attention. Second,  X  (  X  ) can be minimized very efficiently by a double loop algorithm [24]. The compu-tationally intensive log | A | term is concave in  X   X  1 . Upper-bounding it tangentially by the affine z
T (  X   X  1 )  X  g  X  ( z ) at outer loop (OL) update points, the resulting  X  z  X   X  decouples and is mini-mized much more efficiently in inner loops (ILs). min  X  0  X  z leaves us with min tem to be solved of the structure of A . Refitting z (OL updates) is much harder: z  X  ( I  X  1 fields, the inner optimization needs posterior mean computations only, while OL updates require bulk Gaussian variances [21, 15]. The reason why the double loop algorithm is much faster than previous approaches is that only few variance computations are required. The extension to complex-valued u is non-trivial only when it comes to IRLS search direction computations (see Appendix). Given multi-slice data ( X t , y t ) , t = 1 ,...,T , we can use an undirected hidden Markov model over image slices u = ( u t )  X  C nT . By the stack-of-slices methodology, the likelihood poten-tials P ( y t | u t ) are independent, and P ( u t ) from above serves as single-node potential, based on s t = Bu t . If s t  X  := u t  X  u t +1 , the dependence between neighbouring slices is captured by node are complemented by coupling parameters  X  t  X   X  R n + . The Gaussian Q ( u | y ) , y = ( y t ) , has the same form as above with a huge A  X  C nT  X  nT . Inheriting the Markov structure, it is a Gaussian linear dynamical system (LDS) with very high-dimensional states. How will an efficient extension of the double loop algorithm look like? The IL criterion  X  z should be coupled between neighbouring slices, by way of potentials on s t  X  . OL updates are more difficult to lift: we have to approximate marginal variances in a Gaussian LDS. We will do this by Kalman smoothing, approx-imating inversion in message computations (conversion from natural to moment parameters) by the Lanczos algorithm.
 The central role of Gaussian covariance for approximating non-Gaussian posteriors has not been emphasized much in machine learning, where if Bayesian computations are intractable, simpler  X  X ariational Bayesian X  concepts are routinely used, imposing factorization constraints on the poste-rior up front. While such constraints can be adjusted in light of the data, this is difficult and typically not done. Factorization assumptions are a double-edged sword: they radically simplify implemen-tations, but result in non-convex algorithms, and half of the problem is left undone. Our approach offers an alternative: by using Lanczos on Q ( u | y ) , we retain precisely the maximum-covariance directions of intermediate fits to the posterior, without running into combinatorial or non-convex problems. Finally, we place more varied sparsity penalties on the in-plane dimensions [24] than on the third one. This is justified by voxels typically being larger and spaced with a gap in the third dimension, with partial volume effects reducing sparsity. Moreover, a non-local sparsity transform along the third dimension would destroy the Markovian structure essential for efficient computation. We aim to extend the single slice method of [24] to the hidden Markov extension, thereby reusing code whenever possible. The variational criterion is (1) with h (  X  ) = X The coupling term log | A | is upper-bounded (  X   X   X  z ), so that the IL criterion  X  z ( u ) is the sum between neighbours, are routinely addressed in parallel convex optimization. In order to update u t ,  X   X  u = u t . These updates can be run asynchronously in parallel, sending u t to neighbours after every few IRLS steps. For OL updates, we have to compute z t =  X   X  2 ( I  X  1 T )Var Q [ s t | y ] and z t  X  =  X   X  2 ( I  X  1 value, an estimate of log | A | is required as well. We use the two-filter Kalman information smoother, which entails passing Gaussian-form messages along the chain in both directions. Once e M M practice, we average over  X  t . The algorithm is sketched in Algorithm 1.
 Algorithm 1 Double loop variational inference algorithm repeat until outer loop converged For reconstruction, we run parallel MAP estimation. Following [12], we smooth out the nondiffer-gradients with Armijo line search. Nodes return with  X  u t  X  z at the line minimum u t , the next search direction is centrally determined and distributed (just a scalar has to be transferred). This is not the same as centralized CG: line searches are distributed and not done on the global criterion. We briefly comment on how to approximate Kalman message passing by way of the Lanczos algo-rithm [8], full details are given in [22]. Gaussian (Markov) random field practitioners will appre-ciate the difficulties: there is no locally connected MRF structure, and the Q ( u | y ) are highly non-stationary, being fitted to a posterior with non-Gaussian statistics (edges in the image, etc ). Message passing requires the inversion of a precision matrix A . The idea behind Lanczos approximations is PCA: if A  X  U  X  U T ,  X  the l n smallest eigenvalues, U  X   X  1 U T is the PCA approximation of A  X  1 . With matrices A of certain spectral decay, this representation can be approximated by Lanc-zos (see [24, 22] for details). For a low rank PCA approximation of  X  A t  X  , M t  X  has the same rank (see Appendix), which allows to run Gaussian message passing tractably. In a parallel implementa-tion, the forward and backward filter passes run in parallel, passing low rank messages (the rank k m of these should be smaller than the rank k c for subsequent marginal covariance computations). On a lower level, both matrix-vector multiplications with X t (FFT) and reorthogonalizations required during the Lanczos algorithm can easily be parallelized on commodity graphics hardware. With our multi-slice variational inference algorithm in place, we address sampling optimization by Bayesian sequential experimental design, following [24]. At slice t , the information gain score X  X   X  C d  X  n not yet in X t , the score maximizer is appended, and a novel measurement is acquired (for the maximizer only).  X ( X  X  ) depends primarily on the marginal posterior covariance matrix Cov Q [ u t | y ] , computed by Gaussian message passing just as variances in OL updates above (while a single value  X ( X  X  ) can be estimated more efficiently, the dominating eigendirections of the global covariance matrix seem necessary to approximate many score values for different candidates X  X  ). Once messages have been passed, scores can be computed in parallel at different nodes. A purely sequential approach, extending one design X t by one encode in each round, is not tractable. In practice, we extend several node designs X t in each round (a fixed subset C it  X  X  1 ,...,T } ;  X  X t X  the round number). Typically, C it repeats cyclically. This is approximate, since candidates are scored independently at each node. Certainly, C it should not contain neighbouring nodes. In the interleaved stack-of-slices methodology, scan time is determined by the largest factor X t (number of rows), so we strive for balanced designs here.
 To sum up, our adaptive design optimization algorithm starts with an initial variational inference rounds. Each round starts with Gaussian message passing, based on which scores are computed at nodes t  X  C it , new measurements are acquired, and designs X t are extended. Finally, variational inference is run for the extended model, using a small number of OL iterations (only one in our experiments). Time can be saved by basing the first OL update on the same messages and node marginal covariances than the design score computations (neglecting their change through new phase encodes). We present experimental results, comparing designs found by our Bayesian joint design optimization prior previously used in [24] (potentials of strength  X  a on wavelet coefficients, of strength  X  r on Cartesian finite differences). While the MRI signal u is complex-valued, phase contributions are mostly erroneous, and reconstruction as well as design optimization are improved by multiplying generic setup by appending I  X   X  T 2 to B . We focus on Cartesian MRI (phase encodes are complete columns 3 in k -space): a more clinically relevant setting than spiral sampling treated in [24]. We use data of resolution 64  X  64 (in-plane) to test our approach with a serial implementation. While this is not a resolution of clinical relevance, a truly parallel implementation is required in order to run our method at resolutions 256  X  256 or beyond: an important point for future work. 5.1 Quality of Lanczos Variance Approximations We begin with experiments to analyze the errors in Lanczos variance approximations. Recall from [24] that variances are underestimated. We work with a single slice of resolution 64  X  64 , using a design X of 30 phase encodes, running a single common OL iteration (default-initialized z ), comparing different ways of continuing from there: exact z computations (Cholesky decomposition of A ) versus Lanczos approximations with different numbers of steps k . Results are in Figure 1. While the relative approximation errors are rather large uniformly, there is a clear structure to them: the largest (and also the very smallest) true values z j are approximated significantly more accurately than smaller true values. This structure can be used to motivate why, in the presence of large errors over all coefficients, our inference still works well for sparse linear models, indeed in some cases bet-ter than if exact computations are used (Figure 1, upper right). The spectrum of A shows a roughly linear decay, so that the largest and smallest eigenvalues (and eigenvectors) are well-approximated by Lanczos, while the middle part of the spectrum is not penetrated. Contributions to the largest values z j come dominatingly from small eigenvalues (large eigenvalues of A  X  1 ), explaining their smaller relative error. On the other hand, smaller values z j are strongly underestimated ( z k,j z j ), which means that the selective shrinkage effect underlying sparse linear models (shrink most co-efficients strongly, but some not at all) is strengthened by these systematic errors. Finally, the IL zos approximation errors lead to strengthened sparsity in subsequent ILs, but least so for sites with largest true z j . As an educated guess, this effect might even compensate for the fact that Laplace potentials may not be sparse enough for natural images. 5.2 Joint Design Optimization We use sagittal head scan data of resolution 64  X  64 in-plane, 32 slices, acquired on a Siemens 3T scanner (phase direction anterior-posterior), see [22] for further details. We consider joint and independent MAP reconstruction (for the latter, we run nonlinear CG separately for each slice), for a number of different design choices: { X t } optimized jointly by our method here [ op-jt ]; each X t optimized separately, by running the complex variant of [24] on slice u t [ op-sp ]; X t = X for all t , with X optimized on the most detailed slice (number 16, Figure 2, row 2 middle) [ op-eq ]; and encodes of each X t drawn at random, from the density proposed in [12] [ rd ], respecting the typical spectral decay of images [4] (all designs contain the 8 lowest-frequency encodes). Results Hyperparameters are adjusted based on MAP reconstruction results for a fixed design picked ad hoc (  X  and MAP reconstruction runs. We run the op-jt optimization with an odd-even schedule { C it } (all odd (even) t  X  0 ,...,T  X  1 for odd (even)  X  X t X ); results for two other schedules of period four come out very similar, but require more running time. For variational inference, we run 6 OL iterations in the initial, 1 OL iteration in each design round, with up to 30 IL steps (ILs in design rounds typically converged in 2 X 3 steps). The rank parameters (number of Lanczos steps) 4 were k m = 100 , k c = 250 (here, u t has  X  n = 8192 real coefficients). Results are given in Figure 2.
 First, across all designs, joint MAP reconstruction improves significantly upon independent MAP reconstruction. This improvement is strongest by far for op-jt (see Figure 2, rows 3,4), which for joint reconstruction improves on all other variants significantly, especially with 16 X 30 phase encodes, where scan time is reduced by a factor 2 X 4 (Nyquist sampling requires 64 phase encodes). op-eq does worst in this domain: with a model of dependencies between slices in place, it pays off to choose different X t for each slice. rnd does best from about 35 phase encodes on. While this suboptimal behaviour of our optimization will be analyzed more closely in future work, it is our experience so far that the gain in using greedy sequential Bayesian design optimization over simpler choices is generally largest below 1/2 Nyquist. We showed how to implement MRI sampling optimization by Bayesian sequential experimental Restricting ourselves to undersampling of Cartesian encodes, our method can be applied in prac-tice whenever dense Cartesian sampling is well under control (sequence modification is limited to skipping encodes). We exploit the hidden Markov structure of the model by way of a Lanczos approximation of Kalman smoothing. While the latter has been proposed for spatial statistics ap-plications [20, 25], it has not been used for non-Gaussian approximate inference before, nor in the context of sparsity-favouring image models or non-linear experimental design. Our method is a gen-eral alternative to structured variational mean field approximations typically used for non-Gaussian dynamical systems, in that dominating covariances are tracked a posteriori , rather than eliminating most of them a priori through factorization assumptions. In a first study, we obtain encouraging results in the range below 1/2 Nyquist. In future work, we will develop a truly parallel implementa-tion, with which higher resolutions can be processed. We are considering extensions of our design optimization technology to 3D MRI 5 and to parallel MRI with receiver coil arrays [19, 9], whose combination with k -space undersampling can be substantially more powerful than each acceleration technique on its own [13].
 Appendix do not have the form of A anymore. In order to understand this, note that we do not use complex calculus here: s 7 X  | s | is not complex differentiable at any s  X  C . Rather, we use the C  X  R 2 ( h s the Hessian is X H X + BH ( s ) B T . If  X  s := ((diag  X  )  X   X  ) s , then for any v  X  R 2 q : H ( s ) v = Hessian matrix-vector multiplications, thus to implement IRLS steps in the complex-valued case. Recall that messages are passed, alternating between  X  A t  X  and M t  X  matrices. For a PCA approxi-k m Lanczos steps for computed in O ( nk 2 m ) by way of a Cholesky decomposition. Now,  X  A ( t +1)  X  = A t +1 + V t  X  V T t  X  becomes the precision matrix for the next Lanczos run: MVMs have additional complexity of O ( nk m ) . Given all messages, node covariances are PCA-approximated by running Lanczos on A timated by running Lanczos on vectors of size 2  X  n (say for k c / 2 iterations; the precision matrix is given in Section 3). More details are given in [22].
 Acknowledgments References
