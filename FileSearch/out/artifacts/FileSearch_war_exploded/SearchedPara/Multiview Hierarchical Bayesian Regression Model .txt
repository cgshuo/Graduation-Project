 With the development of Web applications, large scale data are popular; and they are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about multi-view data with im-plicit structure. In this paper, we propose a novel hierar-chical Bayesian mixture regression model, which discovers and then exploits the relationships among multiple views of the data to perform various machine learning tasks. A stochastic EM inference and learning algorithm is derived; and a parallell implementation in Hadoop MapReduce [9] paradigm is developed to scale up the learning. We apply the developed model and algorithm on click-through-rate (CTR) prediction and campaign targeting recommendation in online advertising to measure its effectiveness. The ex-periments on both synthetic data and large scale ads serving data from a real world online advertising exchange demon-strate the superior CTR prediction accuracy of our method compared to existing state-of-the-art methods. The results also show that our model can recommend high performance targeting features for online advertising campaigns. 1.2.6 [ Artificial Intelligence ]: Learning; G.3 [ Probability and Statistics ]: Algorithms Hierarchical Bayesian Regression, Online Advertising
Many kinds of data, such as web pages, academic papers, and online videos, have the characteristic of multi-views. For example, papers can be described by their textual con-tent in one view. On the other hand, each paper can also be described by its author(s) and institution(s), which is another view. Similarly, online videos can be described by their content-based features, such as visual features in one view (denoted as content view ), as well as their meta-data(e.g. categories, publishers, tags etc) in another view ( meta view ). In most existing work, the attributes from the additional views are either ignored or are appended as addi-tional features for data, which fails to discover and exploit the implicit structures among objects across multiple views. The implicit structure, when discovered with fidelity, can be very useful for different objectives in data mining tasks, such as classification or regression. Thus, it is important to develop models which can holistically leverage the different views in the data, such as content view and meta view, to boost classification or regression performance.

In this paper, we propose a novel hierarchical Bayesian mixture regression (HBMR) model to discover and exploit the interactions of multi-views of the data. The motivat-ing application for our work is user response prediction and targeting recommendation in online advertising. In display advertising, display ad instances are shown to users on web pages in different formats such as image, flash, and video. Each display ad instance is called a creative and it belongs to one advertising campaign. Each campaign is associated with a list of targeting attributes from different aspects such as users, publishers etc., which specifies the targeted au-dience/webpage this campaign is intended to be shown to. For example, one campaign can target users of (gender:male, age: 20-26, location: Chicago, IL, behavior: consumer elec-tronics fan) and target the creative venues to (publisher: tech.yahoo.com OR publisher: techcrunch.com). The num-ber of possible targeting attribute combinations for each campaign is huge, and it is a challenging task to determine the optimal targeting in terms of advertiser desired perfor-mance metrics. Each creative impression is a high dimen-sional sample which contains the information from the user, the publisher, and the creative itself, as well as whether there is a user action (e.g., click) on the impression. In our context setting of creative impression data, these content features belong to content view, while the campaign related meta information is regarded as meta view.

By showing the creatives to users, advertisers aim to either promote brand awareness among users (brand advertising) or receive desirable responses from users (performance ad-vertising), such as clicks on the advertisements (ads). In performance advertising, advertisers strive to optimize their campaign X  X  performance metrics such as the effective cost per click (eCPC), which in turn relates to maximizing the user response rate on the creatives as measured by click through rate (CTR). In this application, we have two ob-jectives by applying the developed HBMR model on the ads serve data: 1) CTR prediction for a given campaign. 2) targeting attributes recommendation for campaigns to max-imize their CTR.

For all the creative impressions, there are shared clus-tering patterns for the co-occurrence of targeting attribute features (e.g. geological regions, publishers, user segments) across all the advertising campaigns. We propose to learn those patterns by modeling both content view and meta view within the Bayesian framework. The intuition is that differ-ent creative impressions are generated from clustering pat-terns according to content view. The ads click is a rare event and CTR is regarded as a logistic regression signal from interactions of feature clusters (in content view) and campaign structures (meta view). The regressors for CTR are shared with different feature clusters and campaigns. Based on the clustering pattern discovered in content view, we obtain a cluster-specific regressors CTR prediction. In the mean time, from meta view of the data, campaign-specific regressors under each cluster are modeled explicitly to capture the relationship between campaign and the clus-ter patterns since impressions belonging to campaigns share different cluster patterns. Specifically, the contribution of this paper is four-fold: 1. The proposed multiview HBMR model is novel in the 2. We develop a stochastic (Monte Carlo) EM based in-3. The experiments of applying the model and the al-
The rest of the paper is organized as follows. After dis-cussion of the related work in Section 2, we introduce the HBMR model in Section 3. The derived stochastic EM based inference and learning algorithm is presented in Section 4. In Section 5 the developed Map-Reduce implementation of the learning algorithm is described. The experimental results for CTR prediction and campaign targeting recommendation in online advertising, as well as the discussion on convergence and efficiency of the Map-Reduce implementation, are dis-cussed in Section 6. Finally, we conclude the paper with future work in Section 7. Multi-view learning is originally introduced in [3], where Blum et al. propose a co-training method in bootstrap style by incrementally labeling unlabeled samples with high confi-dence by weak classifiers trained from different views of data. Conditional independence or weak dependence among multi-views given labels would be essential for better performance. Yu et al. [22] propose a Bayesian undirected graphical model for semi-supervised multi-view learning. Motivated by the insights from graphical model, a novel co-training kernel is developed for Gaussian process classifiers by using side in-formation. However, the different views considered in their work are assumed to be flat structured and consensus poten-tial is able to be logically decoupled, which are not valid in most real world data set. The content and meta views in our problem are coupled in more complicate way and are corre-lated with implicit hierarchical structure within each view. As it is challenging to ensemble content and meta views, our motivation is to capture the implicit structures among and within multi-views for modeling to achieve better classifica-tion or regression performance. Another difference is that our proposed model is not motivated for semi-supervised learning, instead it tries to exploit the prediction and recom-mendation of our tasks with labeled training samples more naturally and effectively.

For CTR prediction in online advertising system, there are a number of existing work. Chakrabarti et al. [4] pro-poses regularized logistic regression model with words from pages and ads augmenting ad-page relevance for contextual advertising. Richardson et al. [17] presents a logistic regres-sion using features of ads, terms, and advertisers to pre-dict CTR for new ads without history. Graepel et al. [12] describes a Bayesian CTR prediction algorithm based on probit regression model that maps features to probabilities. An approximated parallel inference is derived for large scale CTR prediction. However, clustering patterns and struc-tures of certain features in the data are not explicitly mod-eled in their previous work. We will show that discovering and exploiting those patterns will improve CTR prediction. In addition, in our CTR prediction problem the advertiser campaign hierarchy is explicitly introduced in our model, which is not addressed in previous work.
 The HBMR model is very general. It can be reduced into Dirichlet Process mixtures of generalized linear models [14] if there is only one object in meta view with one level of regressors and use generalized linear regression instead of logistic regression in our model. And it would reduce into Dirichlet Process multinomial logistic models [19] if we have only one object in meta view and binomial logistic response specified in the model. We note that HBMR model can find its applications in many problems with multi view data, CTR prediction and campaign targe ting recommendation presented in this paper are application instances. For dif-ferent problems, specific distributions may be assumed but same HBMR model structure can be applied. One example is to predict whether a user likes particular articles in blogs or news. Here user meta data (demographic, interest etc) are the meta view, and each of his/her reply post and blogs or news (bag of words) are the content view. Users share Figure 1: Graphical Representation for Hierarchical Bayesian Mixture Regression Model different topics (clusters) of bag of words features in corpus and a specific user X  X  preference for a specific blogs or news is treated as regression response based on the user and topic specific regressors. In this setting, HBMR model fits very well and can be applied to the problem successfully.
We use the online advertising problem as an instance to il-lustrate the formulation of multi-view hierarchical Bayesian mixture regression model. The content view in this prob-lem contains feature vector x c i and response (click) y c each impression under campaign c ; the meta view contains campaign identity. Here, meta view and content view are coupled together. Impression sample x c i of content view is generated from mixture model under possible clustering pat-terns; meta view also has influence on the clustering pat-terns (e.g., different campaigns have different prior CTR). Therefore, a random regressor  X  c k is introduced to connect the response y c i in content view and campaign identity in meta view. For general content and meta view problems, the similar model can be applied that random variables are created to model the relationships between features in dif-ferent views with appropriate distribution assumption made under specific problem formulation.

Figure 1 shows the graphical representation of the HBMR model. In the figure, each creative impression from a spe-cific campaign is a high dimensional binary feature vector x . Each component of x c i denotes whether a user or pub-lisher attribute (e.g., user age falls to bin 15-20, geological location=Washington DC, web site=tech.yahoo.com) is true or false. The feature vector is very sparse. This sample is generated from an underlying distribution F with cluster-specific parameter  X  k . A prior is put on this parameter from a base distribution G . Wedenote X  0 as the hyper-parameter for G . Distributions F and G depend on the spe-cific problem and we can choose conjugate distributions for the purpose of easy computation. For data with numerical features such as the synthetic data used in our experiments, F is modeled as Gaussian distribution, thus parameters  X  k include mean and precision matrix. Accordingly G is mod-eled as Gaussian-Wishart distribution, and  X  0 = {  X  0 ,  X  are its hyper-parameters. For multiview data with discrete features, such as the creative impression data in our problem or bag of words data in blog/news recommendation, F will be multinomial distribution, and G is Dirichlet distribution.  X  0 = {  X  0 } is the hyper-parameter for Dirichlet distribution as  X  = {  X  k } and  X  k  X  G (  X  0 ).

For infinite (arbitrary) number of clusters, we may put the Dirichlet Process prior and construct an infinite mixture model for creative impression samples. The mixture propor-tions  X   X  GEM (  X  ) is constructed from the stick breaking process [18],
Then we sample the cluster indicator z c i for impression i of campaign c , x c i from a multinomial z c i  X  Mult ( Once we know which cluster impression x c i is from, we have x ple x c i . We adopt logistic regression to model their inter-actions in HBMR, but other regression model can also be used. One important observation is that click behavior also depends on latent clusters of features (e.g., user regions, pub-lisher websites) which capture the intrinsic factors, as well as advertising campaign which describe the creative infor-mation. Based on this motivation, we develop a hierarchi-cal structure for the regressors under different cluster and in different campaigns to explicitly model the interactions among those variables. Specifically, random variable  X  k is introduced to be the cluster-specific regressor for cluster k , and  X  k is the corresponding precision matrix. Again, the Gaussian-Wishart prior is placed on them. Then the cam-paign specific regressor  X  c k is a Gaussian sample with mean from the corresponding cluster specific regressor  X  k and pre-cision matrix  X  k and conjugate prior is Here,  X  k is the Wishart distribution with  X  k degrees of free-dom and d  X  d matrix W k ,
 X  0 = {  X  k 0 : X  k 0 = {  X  k , X  k , X  k ,W k }} is the hyper-parameters for the prior Gaussian-Wishart distribution of  X  K ; X  K = {
 X  k : X  k = {  X  k ,  X  k }} is the means and precision matrices for all the cluster regressors;  X  C = {  X  c : X  c = {  X  c k set of campaign regressors under all clusters.
As logistic regression of y c i implies, CTR of an impression i in campaign c is the sigmoid function of impression fea-tures and campaign specific regressor learned from feature clusters: where  X  ( x )=1 / (1 + exp (  X  x )) and &lt;&gt; is inner product.
Finally, the click (or not) signal y c i is a Bernoulli sample from its CTR ctr c i .
For each campaign c , the joint distribution of impressions { x i } , clicks { y c i } and hidden cluster indicator { z c parameters is derived by integrating out  X  , For all the campaigns, the joint distribution of impressions X , clicks Y , and hidden cluster indicators Z under hyper-parameters can be calculated as,
In generative components of HBMR model, we need to sample the cluster state z c i for each impression i belonging to campaign c , and cluster-specific regression coefficients  X  In addition, regression coefficients  X  c k for each cluster k in each campaign c need to be decided as well. We derive a stochastic Expectation-Maximization based inference and learning algorithm for our model. Following subsections de-scribe the algorithm in detail.
To get cluster states for all impressions in all campaigns, we utilize Gibbs Sampling method. Gibbs Sampling is used to get the posterior states by cycling through the latent vari-ables, sampling each from posterior distribution conditioned on other variables. here, X  X  ( i,c ) is the data set with all impressions in all cam-paigns excluding the current impression x c i ; Y  X  ( i,c ) similarly for response variable y c i .

The conditional distribution for latent cluster variable z is from Chinese Restaurant Process [1]: where n c k = j  X  c 1( z c j = k ) is the count of impressions in campaign c assigned to cluster k ,and n c = j  X  c 1( x c j the count of impressions in campaign c .

As distribution F and G are conjugate, we are able to integrate out cluster component parameters  X  to get the predictive likelihood for x c i under cluster k .
The integration has analytical solutions by conjugacy of prior and posterior. For Multinomial Dirichlet conjugacy, we have Dirichlet distribution,  X   X  k = z c parameter for posterior Dirichlet distribution. For unrepre-sented cluster k = K +1,  X  k =  X  0 as we have no observations for it. In experiments, we set  X  0 =0 . 1  X  1 ( d, 1) and d is the dimension of impression feature vectors.

The click y c i is simply Bernoulli distributed from logistic signal: p ( y c i | x c i , X  c k ,z c i = k )=  X  ( &lt;x c i , X  c
The posterior distribution for regressor  X  c k of cluster k in campaign c is,
The posterior is the product of Gaussian and logistic func-tion. We can Laplace approximate [2] it as a Gaussian with mean the MAP (maximum posterior) estimate of  X  c k and precision matrix the second derivatives of the negative log likelihood As in our CTR prediction problem, we have millions of sam-ples and the variance would be reasonably small, thus the approximated Gaussian would be peaked. Thus in our ex-periment implementation we calculate  X  c k from MAP esti-mation instead of sampling  X  c k from N (  X  c k ( MAP ) , ( A
Maximizing Eq. 5 w.r.t  X  c k to obtain MAP estimate is equivalent to L 2 regularized logistic regression, we use iter-ative reweighed least squares (IRLS) method [13] to solve it. Newton-Raphson [8] method is adopted in the algorithm where H c k is the Hessian matrix and E c k is the gradient for all samples of cluster k in campaign c . where  X  c i =  X  ( &lt;x c i , X  c k &gt; )and  X  ( x )=1 / (1 + exp(
In the model cluster-specific regressors {  X  k } and precision matrices {  X  k } are parameters for campaign regressors  X  The prior for parameters  X  K = {  X  k : X  k = {  X  k ,  X  k }} from Gaussian-Wishart distribution,
Once we have Gaussian  X  X bservations X   X  C , we can obtain posterior Gaussian-Wishart as
The updated hyper-parameters  X  0 can be derived using the method from [10] as follows, In our experiments, we set all  X  k = 0 ,  X  k = d ,  X  k =1,and W
Based on posterior Gaussian-Wishart distribution, we sam-ple  X  k and  X  k which are used in the MAP estimation of  X 
The inference and learning for Z , X  C and  X  K in HBMR model is developed in a stochastic EM [16] (Monte Carlo EM [20])framework. Specifically, in E-Step we sample clus-ter states Z by Gibbs Sampling. In M-Step, we estimate the MAP of campaign regressors  X  C based on the current states Z and cluster-specific regressors and precision matrices  X  After that, cluster-specific regressors {  X  k } and precision ma-trices {  X  k } are sampled from updated posterior distribu-tion of Gaussian-Wishart. An important hyper-parameter needed to be learned is the concentration parameter  X  for Dirichlet Process [7]. We adopt the method from Escobar and West [6]; they proposed a gamma prior and derived an auxiliary variable update for  X  . This estimation is used in synthetic data; in real online advertising data, we just fix  X  = 0 . 1 K as we truncate Dirichlet Process.
 Algorithm 1 Map-Reduce implementation of inference and learning for HBMR for each stochastic EM iteration loop end loop
With millions or even billions of creative impressions and clicks generated every day in online advertising exchange, it X  X  important to make the inference and learning algorithm forHBRMbeabletoscaleuptorealworlddatasetsize. To achieve this objective, we want to parallelize the algorithm so it can leverage distributed computing platform such as Hadoop Map-Reduce [9] for large scale data modeling.
Map-Reduce is a software development framework for de-veloping applications which need processing of vast amounts of data in-parallel on large clusters of commodity hardware in a reliable, fault-tolerant manner. In the framework each Hadoop job has two interfaces, Mapper and Reducer, which are needed to be implemented for the desired logical func-tions. In the Hadoop job, the input dataset is split into independent chunks. Each chunk is processed by a Mapper task in parallel. The Mapper filters input key and value into output key and value. Then values are shuffled, combined and sorted to feed into the specific Reducer task according to the output key. In each Reducer, it processes a list of values with the same key and emits new key and value pairs line by line. Multiple Reducers on different key indexed data can be run simultaneously on different nodes.
 In our Map-Reduce implementation, We construct two Hadoop jobs for each EM iteration in the algorithm, JobE for E-Step and JobM for M-Step. The algorithm architec-ture is shown in Algorithm 1.

JobE is mainly responsible for Gibbs Sampling of the clus-ter states. In the mapper of JobE, all the data including im-pressions and clicks (each impression represented as a line with a key lineID ) are loaded and randomly split into rea-sonable number of data partitions with randkey as index. Then, all the lines with the same randkey are aggregated into a list during shuffle (or combiner) stage. In the Reducer, Gibbs Sampling is carried on for each of the individual parti-tion fed together with model parameters. The reducer emits each line with the new output key as combination of cluster ID k and campaign ID c . This idea is similar to ADLDA [15]. The dependency be-tween different impressions and clicks in different partitions are weak. As each partition has sufficient number of samples this approximated Gibbs Sampling would be quite accurate.
In JobM, MAP of campaign regressors  X  c k is estimated by iterations of Newton-Raphson [8]. In more detail, impres-sions and clicks along with their computed cluster states from the output of JobE are loaded and filtered by identity mapper All samples (lines) with the same key (cluster and campaign IDs) would be aggregated together into a list of lines in shuf-fle (or combiner) stage. In each Reducer, Newton-Raphson is adopted to calculate MAP estimation based on all the corresponding samples (impressions and clicks) indexed by the same key. After JobM, we update the hyper-parameters  X  0 based on  X 
C from all the Reducers of JobM. Then regressor param-eters  X  K are sampled from Gaussian-Wishart posterior dis-tribution with updated hyper-parameters. A schematic view of the logical functions of Mapper and Reducer of JobE and JobM is shown by Algorithm 2.
As we have large scale samples of impressions and clicks in the data, IO operations (data loading, replication, and sav-ing) would be very expensive. By far, Hadoop Map-Reduce framework is not efficient to work with iterative machine learning algorithms such as our stochastic EM because each iteration of computations in the algorithm needs to access all the data samples. If in JobE and JobM, we only take one iteartion to do computations (Gibbs Sampling or New-ton method), the states of samples would mix poorly; the parameters would be poorly estimated and the outer loop of JobE and JobM would be very long. Consequently, the IO overhead across Hadoop jobs takes a large portion of overall running time in comparison with the computational expense needed by Gibbs Sampling and Newton-Raphson. To alleviate the expensive IO overhead, we run many it-erations of Gibbs Sampling and Newton-Raphson in each iteration of JobE and JobM. With more iterations of com-putations within each pass of Stochastic EM Algorithm 2, two benefits are obtained: 1) the IO expense becomes rela-tive smaller compared to the computational expense, and 2) we have more mixed cluster states and accurate parameters estimated.
 One technical detail worth pointing out is that we use Distributed-Cache mechanism in Hadoop to store and pass model parameters between multiple jobs. Distributed-Cache is a facility in Hadoop to cache files (text, archives, jars and so on) needed by applications. In JobE, we load old regressors  X  C from Distributed-Cache passed from previous JobM, and in JobM we save  X  C into Distributed-Cache for further use. Other parameters such as  X  0 and statistics n , n c are similarly stored and passed through Distributed-Cache.

A challenging problem in the implementation on Hadoop is how to parallel Gibbs Sampling with arbitrary varying number of clusters. There are related works for paralleliz-ing Gibbs Sampling for unstructured Graphical Models ([21, 11]); however, it is still difficult to handle the suitations when the number of variables are changing over time. For exam-ple, samples are split into different computing nodes and the Gibbs Sampling are running on each node. If old clusters are churning and new clusters are emerging on different nodes, we will no longer be able to track the exact correspondence of clusters between different computing nodes. Thus, for sim-plicity, we fix the number of clusters; although fully parallel Gibbs Sampling for varying number of variables in Graphical Models would be more accurate and need further investiga-tion. This approximated truncation of Dirichlet Process in the implementation is simple yet effective.
 Algorithm 2 Mapper and Reducer in JobE and JobM
JobE Mapper for each sample, generate a rand key.

JobE Reducer for all samples indexed by the same rand key, do Gibbs
Sampling(Eq. 4) to get the cluster ID; and generate the output key combined with campaign ID for each sample. JobM Mapper Identity Mapper, pass outputs of JobE to inputs of JobM.
JobM Reducer for samples indexed by cluster and campaign ID, do MAP (Eq. 6) estimation of parameters  X  c k and get statistics n
HBMR is a general probabilistic model to incorporate the multi-view asepcts of data. We investigate two important applications: CTR prediction and targeting attributes rec-ommendation in the online advertising.
Click-through rate (CTR) is the click probability when ads are shown to users, and it measures the ads perfor-mance from users experience. Since HBMR directly models the click behavior, CTR can be computed from the learned model. The whole dataset are randomly splitted into train-the model learned from the training data, CTR prediction for testing data item x c i is, where  X  c k (in  X  C ) is the regression coefficients for campaign c in cluster k , n c k and n c are the number of samples in different campaigns and clusters, and  X   X  k is the posterior estimation of hyper-parameter for cluster k .

Note that one posterior sample of parameters ( X  C , {  X   X  and statistics is used in Eq. (8) . To reduce the prediction variance, we might get several posterior samples of parame-ters and statistics after burn in of stochastic EM to calculate CTR prediction for test data.
Based on the count statistics inferred and parameters learned from training data, we are able to find suitable feature bits for each campaign x c by optimizing objective function such as CTR in Eq. (8).
The solution to Eq. (10) has specific feature bits turned on and we then rank these bits according to their weighted average of coefficient weights  X   X  c = k n c k n
Based on the weight vector  X   X  c , the important targeting at-tributes with high weight can be recommended to advertisers such that advertisers are able to adjust the targeting pro-file accordingly. The improvement of ads performance (e.g., CTR) is expected with the adjusted targeting profile. On the other hand, it is necessary to evaluate the quality of the recommendation beforehand such that advertisers have better understanding of benefits that the adjusted targeting profile will bring.

If we exclude the impressions with one specific targeting attribute (e.g. gender female), we calculate CTR based on the corresponding data subset (e.g. without considering gen-der female). This CTR measures the importance of the spe-cific targeting attribute (e.g. gender male) and is denoted as tCTR. If tCTR for specific targeting attribute is higher than the overall CTR from the overall data set, intuitively Figure 2: CTR predictions comparison on Synthetic Data Set this targeting attribute is important and it should be rec-ommended. Based on the learned model, the targeting at-tributes are ranked in descending order of weight, and the percentage of targeting attributes with higher tCTR than the overall CTR in the top positions (e.g., top 200) are cal-culated as ptCTR.

The metric ptCTR measures the consistency between the weight vector  X   X  c and tCTR. However, ptCTR cannot mea-sure how good the ranking of the targeting attributes with high tCTR. Inspired by normalized discounted cumulative gain (nDCG) in information retrieval, we propose the nDCG metric defined as follows.
 where i denotes the position of targeting attributes in the descending order of learned weights and IDCG is the DCG score for perfect ranking where the order of tCTR is exactly the same as the order of learned weight in the HBMR model. nDCG ctr provides more reliable and accurate measurement for the consistency between the weight vector  X   X  c and tCTR than ptCTR.
We generated 1000 data items (feature samples and re-sponses), where each feature sample is from the mixture of two-dimensional gaussian distributions and each response is generated from logistic regression based on the correspond-ing feature sample and cluster-specific coefficient weights. As this is a relative small data set, our learning and infer-ence algorithms are able to handle complex Dirichlet Process Prior and run with arbitrary number of clusterings. In the experiment, the number of campaign is set to be 1 since there is no concept of campaign in this simple data set. Thus, our HBMR model reduces to Dirichlet Process Mixture of Gen-eralized linear model with L2 regularized logistic regression.
Figure 2 shows the CTR prediction performance compar-ison between our HBMR model and logistic regression using Eq. (8). HBMR achieves noticeable better prediction with AUC 0.97283 because the synthetic data has obvious cluster-ing patterns with different underlying gaussian distributions and cluster-specific coefficient weights.
We apply our method to the world X  X  largest performance ads exchange system, RightMedia, and conduct evaluation with real online event log data and advertising campaigns. In RightMedia X  X  daily ads serving, the information of both publisher and user are logged. Users are anonymized to hide personal identifiable information. Those information are our features for model fitting. We sampled ads impressions from RightMedia X  X  ads serving logs for the experiment. The data set has about 25 million impressions and clicks from 13 cam-paigns with more than 2GB storage. Each impression is a high dimensional sparse sample.

We pick region ID and publisher ID features as our target-ing attributes; the original sample dimensionality is about 4000; each dimension is a binary feature bit. In order to speed up learning and inference while still preserving suf-ficient important feature bits; we plot the frequency his-togram of these two attributes and pick about 300 region IDs and 500 publisher IDs to reduce the dimensionality. As there is the power law distribution of region IDs and pub-lisher IDs; for top 300 regions and 500 publishers, we pre-serve about more than 96% and 98% samples.

Figure 3 illustrates the CTR prediction performance com-parison between HBMR with number of clusters 10 and 50 and logistic regression on real media data set. Since there is no  X  X ampaign X  conception in logistic regression, we run each logistic regression for all the impressions and clicks for each campaign. HBMR has better prediction with AUC 0.92699 for K = 10 and 0.9107 for K = 50 while AUC is 0.89609 for logistic regression. Here, the number of clusters is sensitive to performance of our HBMR. In our impression features, there are clustering patterns such as clustering of regional areas and publisher sites, therefore, the regression is able to take advantage of related impressions from different campaigns to aid the CTR prediction. Another contribu-tion for the better prediction comes from the additional co-occurrence patterns of region and publisher features, which could be considered as extra nonlinear features. However, the performance gain in figure 3 is much lower than the simulation data in figure 2. The possible reason might be our parallel Gibbs Sampling is simply fixed the number of clusters and the number is not accurate enough. Another reason may be related to the semantic meaningful features we picked. Currently, we only consider the regions and pub-lisheres; it is possible there are more significant semantic features such as user segments, creative IDs, channel IDs and so on. However, with introcutions of more higher dimen-sional features, feature selection and regularization would be needed in our models. Therefore, more further work need to carry on to verify our models and produce better prediction and recommedation performance.

We compare the performance on targeting recommenda-tion between HBMR and logistic regression reported the re-sults in Table 1. The larger values of nDCG ctr and ptCTR indicates that our model HBMR is more consistent with tCTR.

Table 2 lists the top targeting attributes in the publisher targeting for one campaign from the advertiser Netflix and corresponding regressor weights learned by HBMR. By run-Figure 3: CTR predictions comparison on real Data set Table 1: Performance comparisons on targeting rec-ommendation ning this campaign on various publisher sites, the advertiser Netflix promotes its online video service to interested users. The properties of specific publisher site reflects interests of users to some extent and thus significantly affects the cam-paigns X  performance on the publisher site. As shown in the table, most of the top publishers are in the area of enter-tainment, and are closely related to this online video service campaign from Netflix. Therefore, the users on these pub-lishers should have explicit or implicit interest on the online video service and the good performance of this campaign is expected. In fact, we submit these targeting recommenda-tions to the advertisers who own those campaigns and they found those topics of publisher sites are related to their ad-vertising products. After they take the recommended tar-geting, the CTR on their creatives are increased in a large margin.
 Convergence Analysis
For each EM iteration, we have two MapReduce Jobs; one for E-Step Gibbs Sampling and another for M-Step param-eter estimation. We use 30 iterations for Gibbs Sampling in E-Step; and 20 iterations for Newton-Raphson iterations in M-Step.
 To study the convergence of Gibbs Sampling and Newton-Raphson method, the experiment results are plotted in Fig-ures 4 and 5. In each node, 30 iterations Gibbs Sampling is enough for mixing; and as Newton-Raphson is quadratic rate of convergence, 20 iteration is enough. Within each EM pass, we have enough iterations to mix the cluster states and estimate the accurate parameters; only several dozens EM passes are needed for the inference and learning over the entire data at all nodes in our experiments. There is a trade-off between number of iterations within Gibbs Sam-pling and Newton-Raphson of each Job and the number of Table 2: Top attributes for campaign from Netflix EM Job passes. If we have few iterations of Gibbs Sam-pling and Newton-Raphson; the states would mix poorly, while we need longer passes of EM Jobs in order to con-verge to the stationary for Gibbs Sampling and Newton-Raphson. As each Job is quite expensive to load and save so many samples; while the running time for computations (e.g. Gibbs Sampling and Newton-Raphson) would become relative light. Therefore, it would take longer times than our proposed algorithm.
 Scalability Analysis
Table 3 reports computation time on two different data sets with about 25 millions and 1 million impressions, re-spectively. There are 518 targeting attributes and the num-ber of clusters K is fixed at 10. We fixed 30 iterations within eachE-Stepand20iterationswithineachM-Stepwithearly termination if the norm of difference of two weight vectors in consecutive iterations is less than threshold 0.0001. The fourth and fifth columns are number of mappers and reduc-ers in JobE (E-Step) and JobM (M-Step). The computa-tion cost of E-Step X  X  Gibbs Sampling mainly depends on the number of clusters and number of impressions. MapReduce is able to put the same scale of samples to each node and consequently the running time would be at the same scale with more mappers and reducers for larger data set. The computational complexity for M-Step mainly depends on in-verse of 518  X  518 Hessian matrix and number of samples used to calculate gradient and Hessian matrix. The resulting running times for M-Step of two data sets are at the same scale. Thus, for the billions of impressions or even larger data sets, we are able to scale our inference and learning old Table 3: Scalability in MapReduce implementation
Data Set E-Step M-Step JE M/R JM M/R 25M impressions 6 X 11 X  4 X 14 X  5/100 100/20 1M impressions 5 X 2 X  1 X 8 X  2/5 5/5 as long as we have enough number of mappers and reduc-ers. Another important issue needed to be carefully han-dled is the IO expense (such as loading, passing and saving huge data), which would become relative large compared to computational expense such as Gibbs Sampling and Newton method. The running time would increase with the increas-ing scale of data set. In this paper, we have proposed Hierarchical Bayesian Mixture Regression model to do CTR prediction and target-ing attribute recommendation for online advertising. This multi-view hierarchical Bayesian model is the combination of generative and discriminative model with  X  X ampaign-impression X  structure; and the graphical model structure is general for a class of similar problems with content and meta views. Com-pared to logistic regression for each campaign, our model is able to take advantage of side information such as ex-tra co-occurrence features and related samples from differ-ent campaigns to do better prediction. We derive stochas-tic EM to do inference and learning for our model. Our stochastic EM is scalable to tens of millions of impressions in Hadoop MapReduce framework and achieves promising results against logistic regression.

For the further work, we need to scale our model into billions or even larger number of samples. Our model is feature extensible from content view; further investigation need to carry on to test on more different attributes such as user segments, creative IDs, channel IDs, user gender, age and so on. One limitation is the parallel approximation of the Gibbs Sampling with fixed number of clusters. This is still an open question in parallel inference. [1] D. Aldous. Exchangeability and related topics. Ecole [2] C.M.Bishop. Pattern Recognition and Machine [3] A. Blum and T. Mitchell. Combining labeled and [4] D. Chakrabarti, D. Agarwal, and V. Josifovski. [5] J. Dean and S. Ghemawat. Mapreduce: Simplified [6] M. D. Escobar and M. West. Bayesian density [7] T. S. Ferguson. A bayesian analysis of some [8] R. Fletcher. Practical methods of optimization .John [9] A. Foundation. Hadoop mapreduce. [10] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. [11] J. Gonzalez, Y. Low, A. Gretton, and C. Guestrin. [12] T. Graepel, J. Q. Candela, T. Borchert, and [13] P. J. Green. Iteratively reweighted least squares for [14] L. A. Hannah, D. M. Blei, and W. B. Powell. Dirichlet [15] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [16] S. F. Nielsen. The stochastic em algorithm: [17] M. Richardson, E. Dominowska, and R. Ragno.
 [18] J. Sethuraman. A constructive definition of Dirichlet [19] B. Shahbaba and R. M. Neal. Nonlinear models using [20] G. C. G. Wei and M. A. Tanner. A monte carlo
