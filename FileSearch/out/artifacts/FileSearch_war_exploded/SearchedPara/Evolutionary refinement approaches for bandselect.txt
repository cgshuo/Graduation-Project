
Department of Computer Science, Texas Tech University, Lubbock, TX, USA
Department of Industrial Design, Xi X  X n Jiaotong Liverpool University, Suzhou, Jiangsu, China Department of Industrial and Systems Engineering and RUTCOR (Rutgers Center for Operations Research), Rutgers, the State University of New Jersey, Piscataway, NJ, USA
Office of the Texas State Chemist, Texas A&amp;M, TX, USA School of Animal Biology, The UWA Institute of Agriculture, The University of Western Australia, Crawley, Perth, Western Australia, Australia 1. Introduction
Monitoring animal feed quality is particularly important because contaminants in feed can, in ad-dition to hindering the development and health of livestock animals, be transferred to milk and meat products and so present health risks to consumers. Specifically, ruminant-derived feed products may be carriers of the Bovine Spongiform Encephalopathy (BSE) prion [19,27]. Potential points of prohibited animal protein introduction include cross-contamination by transporters, protein blenders working with multiple sources of animal protein, and feed mills t hat manufacture protein supplement for cattle and non-ruminant species, with the latter containing prohibited animal protein. The latest case of BSE in the US was confirmed in 2006 from a cow in Alabama [7]. In Europe, there is a ban on use of all rendered animal protein in feedstuffs for food production animals [1,3]. In the US, the Food and Drug Administra-tion (FDA) and state feed control officials evaluate ruminant feed samples for the presence of prohibited animal protein as part of a nationwide BSE detection program.

The current procedure for monitoring of animal feed quality is as follows. Feed samples are col-lected during inspections of feedlots and feed mills using approved analytical sampling methods and subsequently inspected under laboratory conditions based on standardized microscopy procedures of FDA [2]. The physical preparation of feed samples for inspection takes about 2 hours per sample, and careful microscopy based inspection by specially training technicians takes additional 2 hours per feed sample. If bonemeal fragments or other potentially prohibited ruminant-derived contaminants are found during microscopy, the feed sample is subjected to further analyses using polymerase chain reaction (PCR) technology. The current monitoring system is time consuming and highly subjective to human error, so there is a great need for development of automated quality control procedures for inspection of feed materials.

In a recent study of feed materials and ruminant derived bonemeal, a sequence of linear discriminant analyses (LDA) was deployed to gradually improve the classification accuracy of hyperspectral profiles. Based on independent testing, it was found that the minimum detection level for an automated machine vision based system was about 1% bonemeal contamination (by weight) [36].

In order to overcome the limitations of a manual quality control system, this paper proposes the auto-matic monitoring approach to animal feed quality based on reflectance data acquired with a hyperspec-tral imaging system. Using HSI technology has the fo llowing advantages over traditional microscopy: it is non-destructive, generally does not require much physical preparation of the target objects, and can provide real-time results. Hyperspectral imaging technology has been evaluated as part of machine vision based quality control of a wide range of food products, including meat [28,37,40,42], fruits and vegetables [8,11,20,21,30,31,35,38,48], grain and flour [6,13,14,16,39,47], and animal feed [5,17,34, 36].

Hyperspectral images contain reflectance information for a large number of narrow spectral bands for each pixel (hyperspectral profile), providing a unique reflectance signature of a given object. This allows for classifying material types based on spectral characteristics at specific wavelengths. However, the large number of spectral bands present in hyperspectral images makes the real-time processing for sification of independent test data becomes associated with low accuracy. Thus, it is crucial to develop an efficient procedure to select a few important spectral bands in which it is possible to detect unique contamination signals in order to reliably distinguish contaminants and feed materials.
Spectral band selection is the process of identifying a spectral band subset that contains a significant amount of information to distinguish contaminants and feed materials. Many different methods have been used to assign importance scores to each spectral band, after which a set of spectral bands is chosen based on those scores. Methods in which spectral bands are considered in sets tend to perform better than those which rank them individually, as nearby spectral bands are frequently highly correlated [12,15,49].
We considered two factors that limit many existing approaches. Deterministic recursive methods add one spectral band to the set at a time, and bands are not removed once added. Bands selected later in the process are thus influenced by bands selected earlier in the process, and so we wanted to develop a method where the bands selected for sets with n +1 bands would be independent of those selected for sets with n bands. Additionally, wrapper methods for band selection (methods that use the classifier itself in the band selection process) are computationally expensive, so we focused on filter methods that do not involve the classifier.

In this paper, we developed two methods: a divergence recursive feature elimination (DRFE) method (essentially a filter method based on the recursive feature elimination method) and an evolutionary re-finement (ER) method. A hyperspectral image camera was used to acquire reflectance data in narrow spectral bands across the visible range from 29 feed material samples, including fish and chicken meal, as well as from commercial samples of bonemeal from ground cattle cadavers. Support Vector Machine (SVM) binary classification was used to distinguish between hyperspectral profiles from feed samples and bonemeal samples on the basis of the hyperspectral reflectance values. Classification accuracy was determined when analyses were based upon different feature selection methods. The organization of the paper is as follows. In Section 2, a hyperspectral imaging system for monitoring animal feed quality is described. Section 3 introduces our proposed spectral band selection methods. Experimental results are presented in Section 4, and the conclusion of the paper is presented in Section 5. 2. A hyperspectral imaging system for monitoring animal feed quality
Our hyperspectral imager utilizes a push-broom (line-scan) design, with dispersion provided by a diffraction grating. A frame grabber is not required as scanning is performed with a linear stage con-trolled by a stepper motor. The sensor is then mounted on an aluminum tower-structure at 35 cm above a 5-cm diameter Petri dish holding the feed materials to collect hyperspectral images at a magnification representing a spatial resolution of about 166 hyperspectral profiles (pixels) per mm 2 . Sensor specifics are presented in Table 1.

All hyperspectral images were collected in a dark room, and artificial lighting consisted of 2  X  315W and 12 V light bulbs mounted in two angled rows  X  one on either side of the lens. As power source for the lighting, we used a voltage stabilizer (Tripp-Lite, PR-7b, www.radioreference.com). A bright pink piece of paper was placed in the bottom of the Petri dish used to hold feed materials, so that hyperspectral profiles from background were easily separated from feed and bonemeal materials. A piece of white Teflon was used for white calibration and, for each spectral band, reflectance profiles were converted into proportion of the reflection from Teflon (denoted relative reflectance). All hyperspectral images were collected at ambient temperature conditions of 21 X 23  X  C and 40 X 50% relative humidity. Prior to imaging, all feed materials were placed in a single layer. Figure 1 shows some of the challenges of the classification tasks in that bonemeal constitutes a he terogeneous mixture of particles and feed samples are very diverse in their composition.

Our first step after capturing hyperspectral images was to filter out pink hyperspectral profiles that represented the background material on which the feed and bonemeal samples were placed. Next, we separated the data into training and testing sets. Regarding the training data, we selected spectral band sets using both the recursive divergence method and the ER method. The test data was used to evaluate the performance of SVM classifiers using those spectral band sets.

In order to automate the filtering of background hyperspectral profiles from our data, we examined the normalized reflectance curves of background hyperspectral profiles and non-background hyperspectral we based our filtering method on the values for spectral bands (using 0-based indexing) 19 (465 nm), 44 (544 nm), and 88 (683 nm), which roughly correspond to blue, green, and red light, respectively. We calculated the means and standard deviations of the reflectance values for those three spectral bands in an all-background sample image, after first normalizing the reflectance values for each hyperspectral profile.

To filter background hyperspectral profiles out of our testing and training data, we calculated for each hyperspectral profile the distance (in standard deviations) between the hyperspectral profile X  X  normal-ized reflectance values for those three spectral bands and the normalized reflectance values from the background sample image. If the sum of those distances was less than a threshold, we identified the of the three distances is greater than 10, this means that each band is about more than 3.3 (on average) standard deviations away from the background. 3. Evolutionary refinement approaches for spectral band selection 3.1. Related works
Band selection is similar to the feature selection where we select a few important input variables (features) that are most predictive of a given output. Feature selection can identify only a few relevant features and give a better generalization error [10,22,49,50]. Also, based on the success of SVM, several feature selection algorithms in the SVM domain have been proposed including Guyon X  X  SVM-RFE [22], the SVM gradient method [9,24], the M-fold SVM [18] and FGSVM-RFE [44].

Initially, we looked at three individual spectral ban d ranking methods: separ ation measure, envelope of x + and x  X  , and assume that x + x  X  (otherwise, treat it in a symmetric manner). Let the number of positive observations with x x  X  be n + x , and let the number of negative observations with x&lt;x  X  be n be the minimum and maximum of the values of feature x for the positive data points; likewise let l  X  x and u and smaller absolute values for this ratio indicate more relevant features. Finally, the signal-to-noise the values of x for the positive and negative samples, respectively.

Recursive feature elimination takes a different approach. Unlike the above methods, features are not on how important they are when considered alongside the other features [22]. The result is a list of features ranked from most important to least important. We performed the recursive feature elimination (RFE) process using a linear kernel SVM. A brief explanation of SVM-RFE follows: A linear SVM is trained with all features and a weight is identified for each feature. The feature with the weight with the smallest magnitude is removed; this feature ( k ) is the least important. The linear SVM is retrained without the feature k . The feature with the weight with the smallest magnitude is eliminated and this are exhausted. In this way, the last eliminated feature is the most important.

Another measure that has been used to evaluate feature sets is the between-class divergence [45]. The divergence of a set of features is related to the correlation between those features and can be used to compare the discriminating power of sets of features. In the following, we briefly introduce the general formula for the between-class divergence and then introduce the divergence when data is normally dis-i ( j ) . We select class i if p i ( x ) &gt;p j ( x ) . So the ratio p i ( x ) p for the discriminatory capabilities regarding x . The mean value over class i for different values of x is
Each spectral band of the collected data was normally distributed, so the following equation, which mean vector and covariance matrix, respectively, for class i ) [45]:
Except for when both the number of total spectral bands and desired spectral band set sizes are quite number of such subsets grows exponentially (2 n with n being the number of features). The recursive divergence (RD) feature selection method instead creates a spectral band set incrementally, so that at any step the number of sets to be evaluated is less than or equal to the total number of spectral bands. The first step in the recursive divergence method is to find the single spectral band with the highest divergence. That spectral band is the only element in the initial feature set. At each additional stage, a spectral band is added to the current feature set such that the divergence of the current feature set plus the new spectral band is maximized [15]. 3.2. Proposed methods
We propose two new approaches in spectral band selection. One method is a greedy approach and it uses the divergence as an elimination criterion as explained in Section 3.2.1. The other approach is based on evolutionary search as explained in Section 3.2.2. Both approaches attempt to relieve the heavy burden of an exhaustive search in feature subset selection. 3.2.1. Divergence recursive feature elimination spectral band selection We developed a divergence recursive feature elimination (DRFE) method using a similar principle as SVM-RFE. Unlike SVM-RFE, DRFE does not require a particular machine learning method to evaluate relevance of features. DRFE considers subsets of features during the evaluation process, which is an advantage over those approaches implicitly assuming feature independence. Below is the description of the algorithm in detail. Our data set contains 160 features to be considered and therefore there are 2 160 subsets to be searched. An exhaustive search is not practical. Divergence has been shown to be highly correlated with the success rate [15]. In our DRFE implementation we used a greedy approach (in backward) using divergence values as an elimination criteria. Our DRFE needs O( n 2 ) divergence computation compared to O(2 n ) of the exhaustive search.
 Algorithm 1 A divergence recursive feature elimination method (DRFE)
Let S = { 1 , 2 ,...,n } where n is the total number of features and L be an empty list. 1. While | S | &gt; 1: 2. Let m = | S | . 3. For each feature x  X  S, compute d x , the between-class divergence for S  X  X  x } . 4. Let i = argmax j  X  S d j . That is, the subset of S without feature i has the maximum divergence 5. S = S  X  X  i } . 6. Append feature i to the end of L . 7. After the loop terminates, S contains only one feature. Append that feature to the end of L . L now 3.2.2. Evolutionary spectral band selelection method
We also developed a nondeterministic evolutionary search algorithm for spectral band selection, us-ing between-class divergence as a fitness function. We begin with randomly selected sets of spectral bands and, through a number of successive generations, mutate and recombine the sets with the highest divergence values at a given generation in order to find a set with a very high divergence. The proce-dure followed for a given generation is explained in Algorithms 2 X 6. Algorithms 2 and 3 are helper algorithms used in Algorithms 4 X 6. Algorithm 4 describes elites set selection. Descriptions for roulette wheel selection and tournament selection are given in Algorithms 5 and 6, respectively.
Algorithm 2 The BREED helper method  X  a roulette-wheel method for selecting k unique features from a list of features L where | L | &gt;k Let the set S contain the distinct features from L .
 Let F be a frequency list with each entry F i being the count of the feature S i in L divided by | L | .
Let N be an empty list. 1. While | N | &lt;k 2. Choose a feature f from S using the frequencies in F as selection probabilities. 3. If f/  X  N , append f to N 4. Return N .

Algorithm 3 The MUTATE_FEATURES helper method  X  randomly modify the features in a feature set S Let i be the number of available spectral bands
Let N be an empty set 1. For each feature x  X  S 2. Do 3. Let y = x + r where r  X  N (0 , 1) .Then y is rounded to the nearest integer and rolling the 4. While y  X  N 5. Append y to N
Algorithm 4 An evolutionary spectral band selection method (ER) using elites set selection, given a cutoff threshold c and n initial feature sets of size k . 1. Compute the between-class divergence for each of the n sets using the training data. 2. Let m be n  X  c , the number of elite sets to carry into the next generation. 3. Let S selected be the m sets with the highest divergence scores, and let S next , the number of sets to 4. Let L be an array containing the pooled distinct spectral band numbers from the sets in S selected . 6. Randomly append a feature number not currently in L to L 8. Create n mutate new sets by applying MUTATE_FEATURES to randomly selected sets from S selected 9. Create n recombine new sets using roulette-wheel selection by using the BREED method to select sets
The while loop in Algorithm 4 is included for cases with small set sizes and large numbers of sets, where otherwise as the feature sets converge the algorithm may end up with too few distinct features to produce enough distinct feature sets for the next generation.

Because this algorithm is nondeterministic, we performed multiple trials of the evolutionary spectral band set search and looked at the mean and standard deviations of the between-class divergence and the balanced success rate from the SVM classifier of the sets selected for each spectral band set size n .
With a crossover operation for two parents, a point in the string is chosen as the crossover point, and all the elements before that point from one parent are concatenated with the elements after that point from the other parent. We are looking for sets of distinct features, so even when keeping them in sorted order, there is a chance that feature k could be found in both parents. For example, feature k could be
Roulette-wheel and tournament methods for set selection were tested as well as the elite selection. We tested these methods both with the same recombination and mutation steps from Algorithm 4, where the only difference was that the m sets S selected were selected in a randomized manner using one of the two following methods, instead of with elites selection.
 Algorithm 5 A roulette-wheel feature set selection method to select m sets from a set S of n sets
Let F be a frequency array where each entry F i is the divergence score of set S i divided by the sum of all the sets X  divergence scores.

Let S be an empty list. 1. While | S | &lt;m 2. Let S 1 be a feature set selected from S using F as the selection probabilities for the sets in S . 3. With probability p _ direct ,let S 0 be S 1 ,otherwise: 4. Select a feature set S 2 from S using F . 5. Let S 0 be the result of BREED ( S 1  X  S 2) . 6. With probability p _ mutate , apply MUTATE_FEATURES to S 0 . 7. Append S 0 to S 8. Return S Algorithm 6 A tournament feature set selection method to select m sets from a set S of n sets
Let S be an empty list. 1. While | S | &lt;m 2. Let S 1 be a feature set selected from S by selecting a random subset of sets from S and choosing 3. With probability p _ direct ,let S 0 be S 1 ,otherwise: 4. Select a feature set S 2 from S using the same tournament selection as for S 1 . 5. Let S 0 be the result of BREED ( S 1  X  S 2) . 6. With probability p _ mutate , apply MUTATE_FEATURES to S 0 . 7. Append S 0 to S 8. Return S 3.2.3. Parameter selection
Both evolutionary search and SVM training requires careful selection of parameters. Parameter se-lection for evolutionary search is described in Section 3.3.1 and that for SVM training is described in Section 3.3.2. 3.2.4. Evolutionary search algorithm parameter selection
The parameters to the evolutionary search algorithm are the number of sets to consider at each gener-ation, which is determined by the number of start sets given; the number of generations for which to run; the sigma value to use when mutating candidate sets; and the cutoff value, which controls what fraction of the sets to carry forward at each generation. We tested these for various values in order to determine what effects the various parameters had on the search performance and to select the parameters we would use when comparing this method to other methods (results in Tables 2 X 4 and 6).

The number of generations most directly affected the magnitude of the divergence values of the larger sets found by the search. The utility of the additional generations appears to be related to the size of the search space (for example, for sets with 32 spectral bands the number of possible sets is ( 160 32 )= 4 . 646  X  10 33 , while for sets with six spectral bands it is ( 160
Conversely, the number of sets considered was a primary factor in the consistency of the divergence values of the sets found by the search. This was most true for small sets; the effect was less pronounced for larger sets, again presumably because of how rapidly the search space grows as the set size increases. Increasing the number of sets improves the odds that any particular feature will be considered in at least one of the randomly-selected start sets. When independently drawing, without replacement, c sets of size s from n total features, the probability of any given feature occurring in at least 1 set is:
For instance, when drawing 20 sets of size 10 from 160 total features, the probability of any given feature appearing at least once in those sets is 0.725 (so we would expect roughly 116 distinct features to appear in the 20 sets). When we increase the number of sets to 40, the probability rises to 0.924 (and roughly 148 expected distinct features in the 40 sets).
 Changing the cutoff parameter produced results similar to changing the number of generations. Smaller cutoff values produced sets with, on average, higher between-class divergence values. Table 2 shows that cutoff value = 0.125 yielded about 6% higher between-class divergence values than that by cutoff value = 0.5. The effect of this parameter is closely related to the effects of both of the above parameters: a smaller cutoff means fewer sets are carried forward from the previous generation, mean-ing more new sets are generated in each generation, similar to the additional sets that are generated and considered when the number of generations is increased; on the other hand, the new sets are generated based on a smaller number of sets, so the search of the scope narrows more quickly, possibly discarding found by focusing on the smaller range of selected sets.

After comparing the results of several different combinations of parameters, we chose the following parameters: generations = 160, number of sets = 80, cutoff = 0.125. These parameters were selected to provide high yet consistent divergence values even for larger spectral band sets. With these parameters, the number of divergence calculations that are performed to find a spectral band set of a given size is 80 + (160  X  1)(80  X  (1  X  0.125)) = 11,210. By comparison, the number of divergence calculations required to create the feature ranking of all 160 features using the divergence-based elimination method is 160 i =2 i = 12,879. For small sets, the 11,210 divergence calculations performed on a small number of spectral bands can be computed much more quickly than the calculations involved in analyzing 160 sets of size 159,159 sets of size 158 ,..., 2 sets of size 1, but as the set size increases the advantage decreases, so these search parameters can also be used to establish an upper limit on the set size for which it is advantageous to use this method. For smaller sets, the number of sets and generations could probably be comfortably decreased to 40 sets and 80 generations to speed things up without having much effect on the performance of the search; the results for different parameters shown in Tables 2 and the divergence values of the resulting sets. 3.2.5. SVM parameter selection
SVM [32,41,43,46,52] has been extensively used on various data domains including hyperspectral data [15,20,51], due to its high generalization performance in various application domains. We have to optimize two parameters of the Gaussian kernel SVM: C and sigma (  X  ). One of popular approaches is to separate the data set into two parts: training data and testing data and then apply k -fold cross-validation for the training data to select the parameters of SVM. That is, we first divide the training remaining k  X  1 subsets to prevent the overfitting problem [23,25,26,29,33]. Thus, each instance of the whole training set is predicted once so the cross-validation accuracy is the percentage of data which are correctly classified. In this paper, we used 5-fold cross-validation for the parameter selection. 4. Experimental results
We evaluated the performance of our proposed methods in two ways: by comparing the divergences of the sets it found to the divergences of the sets selected by the recursive divergence methods, and by comparing the balanced success rate for the SVM classifier when using those sets with the rates from the sets produced by other methods. Because of the nondeterministic nature of the evolutionary divergence graphs being the mean values for 20 trials (see Tables 2 X 4).

Our training data contained 5760 feed hyperspectral profiles and 5681 bonemeal hyperspectral pro-files. 59 of the feed hyperspectral profiles were identified as background, as were 160 of the bonemeal hyperspectral profiles, leaving 5701 feed training hyperspectral profiles and 5521 bonemeal training hyperspectral profiles. Our testing data contained 499,200 feed hyperspectral profiles and 500,000 bone-meal hyperspectral profiles. 24,887 and 5,017 feed and bonemeal hyperspectral profiles, respectively, were identified as background, leaving 474,313 feed training hyperspectral profiles and 494,983 bone-meal training hyperspectral profiles. Table 5 summarizes this breakdown of the data.

Figure 3 shows a rough RGB conversion of a sample hyperspectral image and a bitmap mask where the white hyperspectral profiles are the hyperspectral profiles identified as background. The RGB conversion was done by simply selecting the spectral bands corresponding to red, green, and blue light, as discussed in Section 2, and scaling the reflectance values to the 8-bit 0 X 255 range.

Our tests confirmed that between-class divergence was a fairly good indicator of SVM classifier per-formance for our data. From Table 7, an average correlation coefficient between SVM success rates and divergence is 0.75 when the set size is greater than 12. The recursive divergence selection method produced sets that performed much better than those created from the individual feature rankings; sets with 8 or more features found using this method even had higher success rates than those found with SVM-RFE. This makes divergence well suited for use as the fitness function for our feature set search. 4.1. Comparison of the divergences
Figure 4 focuses only on sets with 12 or fewer spectral bands, as that is where both ER and DRFE methods had the biggest advantage. For small sets, both of those methods found sets with significantly higher between-class divergence values than the normal recursive divergence method excepting sets of size 2, in which case the RD method found the same set as the evolutionary search method  X  for the given search parameters, the ER method found the same set in all 20 trials  X  and the DRFE method ER tended to have extremely similar divergence values as those found by DRFE. Tables 6 and 7 contain the results of the four tested methods for a range of set sizes between 2 and 64.

The RD method did not start finding sets with higher divergence values than the average set found by the ER method until set size became 40 features. The DRFE method had a slight advantage over both methods starting with sets with 14 features and up, though at set size 40 the RD method slightly outdid it.

The advantage in divergence values for ER compared to the RD method was consistent across the 20 trials. For sets with 4 to 24 spectral bands, the difference between the mean divergence value for the sets found by ER and the divergence value of the set found by the RD method was at least 3 times the standard deviation of the divergence value for the sets found by ER  X  for sets with 6 and 7 spectral bands was 1.5 times the standard deviation.
 Also, Table 8 contains results from using roulette-wheel and tournament methods for set selection. Neither of them outperformed the elites selection method. 4.2. Comparison of the balanced success rate
Figure 5 shows that for small sets, the divergence advantages of the ER and DRFE methods also translated into an SVM success rate advantage, although the success rates of the sets found by those two methods were not nearly as similar as the divergence values for sets with just 2 and 3 features. For sets with more than 2 features and fewer than 8 features, the sets selected by the SVM-RFE method also had noticeably higher SVM balanced success rates than those selected by the RD method. For 8 X 12 features, the sets selected by the RD method outperformed those selected by our methods; however, as the divergence values for those sets were still lower than those for the sets selected by our methods, we believe this to be mainly the result of chance and not representative of an inherent advantage of the RD method.

As set size increased, the success rates converged  X  particularly for the divergence-based methods, as shown in Fig. 6. The sets from the recursive divergence (RD), divergence-based elimination (DRFE), and evolutionary divergence (ER) search methods resulting in very close success rates for sets with 20 or more features, with declining gains from adding more features. For instance, for the evolutionary search method going from 2 to 5 features caused a 0.133 increase in balanced success rate, while going from 5 to 64 features only caused a 0.062 increase, and from 12 to 64 only caused a 0.036 increase. The sets found by the RD and DRFE methods followed almost the same curve; the sets found by the SVM-RFE method also followed a similar curve, but with success rates that were consistently about 0.01 lower for sets with 20 or more spectral bands. Additionally, the standard deviation of the balanced success rates for the sets found by the ER method declined consistently as larger spectral band sets were considered, while the standard deviation of the divergence values for those sets did not  X  there is a correlation between divergence and SVM success rate, and set size itself appears to have more influence on the consistency of the success rates than the consistency of the divergence values of the sets, as seen in Table 7.

Also note that for 48 features, the subsets selected by the RD and DRFE methods outperformed the set by the ER method did so as well.
Figure 7 shows the success rates of the ER and DRFE methods compared to the separation measure, envelope eccentricity, and signal-to-noise correlation rankings. As mentioned before, nearby spectral are often highly correlated, so methods that look at combinations of spectral bands instead of individual individual spectral band feature rankings. 5. Conclusion
There is an increasing demand for reliable and sensitive machine vision systems to be used in quality control of feed and food products due to growing public concerns about food safety. Because feed mate-rials are highly diverse in composition, they can be considered a challenging model system for detection of contaminants. This paper presents methods for spectral band selection in hyperspectral image (HSI) cubes, containing spectral reflectance in both spatial dimensions and spectral bands. A hyperspectral image camera was used to acquire reflectance data in narrow spectral bands across the visible range from 29 feed material samples. Support Vector Machine binary classification was used to distinguish between hyperspectral profiles from feed samples and bonemeal samples on the basis of the hyperspec-tral reflectance values. Classification accuracy was determined when analyses were based upon different feature selection methods. Two new methods were developed utilizing divergence values for selecting spectral band sets, 1) divergence-based recursive feature elimination (DRFE) approach, and 2) evolu-tionary search (ER) method. The ER algorithm and DRFE methods provide a significant advantage for selecting small spectral band sets over the recursive divergence (RD) method other than for two spec-tral bands, in which case ER never found a set with higher divergence than the one selected by the RD method, and the DRFE method found one with slightly lower divergence. The advantage of the evolu-tionary spectral band set search decreases as the set size increases, with the recursive divergence method reclaiming the advantage for most cases when 40 spectral bands are considered, likely due to the in-creasing size of the search space. Future work will explore ways to determine how many features would be best based on various feature rankings.
 References
