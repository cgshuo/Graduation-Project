 1. Introduction
The increasing availability of information in different languages and the growing number of people speak-ing different mother tongues who want to find information have been motivating research on cross-language information retrieval (CLIR). CLIR is responsible for receiving search requests expressed in one language and retrieving documents written in another language. The scope of CLIR typically involves mapping a concept from one language into another.

Some traditional information retrieval (IR) techniques, such as relevance feedback (RF) acquire a new dimension in this cross-linguistic environment: the user X  X  ability to recognise relevant documents written in a foreign language or translated, by some means, into his language. The RF operation is an automatic process for the modification of search requests based on relevance assessments provided by the user population for important terms attached to previously retrieved documents that have been identified as relevant by the user.
Thus, RF usually involves query expansion and term reweighting. The method consists of asking the user to analyse an initial sample of documents retrieved in response to a query and judge them for relevance. The ori-ginal query is then modified (by the IR system) and re-submitted. A new list of retrieved documents is gener-be repeated several times, until the user is satisfied with the results.

RF can also be performed without user interference, in a technique known as pseudo-relevance feedback (PRF). In PRF, n top ranked documents are assumed relevant and used for the feedback runs. This technique typically achieves less improvement than original RF, however it has the advantage of being done automat-ically without any burden to the user. Xu and Croft (1996) proposed a related strategy called local context analysis (LCA) that combines PRF and global analysis. Global analysis extract concepts from text, based on word co-occurrences using statistical techniques.

There is yet another type of feedback, discussed by Efthimiadis and Robertson (1989) , in which query refor-offer related search terms for the user to choose from and expand the query. These related terms could be obtained from a thesaurus or from previous search results.

This paper focuses on the RF process, and the scenario considered is that of a Portuguese X  X nglish cross-language information retrieval (CLIR) system. The aims of this paper are twofold. The first aim is to find out how well native Portuguese searchers can recognise relevant documents written in English compared to doc-uments that are hand translated and automatically translated to Portuguese. The second aim is to analyse the effect that misjudged documents have on the change in performance achieved through the RF process. The remainder of this paper is organised as follows: Section 2 presents related work; Section 3 describes the
CLIR system used; Section 4 details the experimental design; Section 5 evaluates the users ability in making relevance judgements; Section 6 assesses the impact that errors in judgement have on the performance of RF;
Section 7 finalises the paper presenting the summary and conclusions. 2. Related work
The concept of RF was introduced in the mid-1960s. The first RF methods were designed to be used with vector queries. Some early experiments were performed by Rocchio (1971) on the SMART Retrieval System.
Since then, the method has been applied to other IR approaches. In 1976, Robertson and Sparck Jones (1976) experimented with RF on a probabilistic model; and Dillon and Desper (1980) proposed an RF method for a
Boolean Retrieval System. In addition, several different feedback procedures have been proposed. Some of the best known were developed by Ide (1971) . More recently, new approaches include Neural Networks ( Crestani, Several experiments report on the performance improvement achieved by relevance feedback; Salton and
Buckley (1997) experimented with several RF methods. The improvement achieved with the technique (mea-sured using the residual collection approach) ranged between 47% (for the CISI collection) and 160% (for the
Cranfield collection). Harman (1992b) reports improvements of 112% for the Cranfield collection when expanding the query with 20 terms chosen from the relevant documents.

Salton and Buckley (1997) also concluded that some types of collections may benefit more from the RF process. These are collections with short queries; collections with queries that perform relatively poorly in an initial search; and technical collections. Collections with short queries can benefit from the RF process as it will add context, making the query more complete. Collections with queries that have a weak perfor-mance on the initial run have more potential for improvement. In technical collections, it is possible that the set of relevant documents for a given query is concentrated in a small area of the document space.
McNamee and Mayfield (2003) report that RF does not always improve performance, and in some cases findings were based on PRF which normally performs worse than user RF.

Despite the great research interest on RF, very few experiments have been carried out using human search-ers. A user-centered investigation has been made by Efthimiadis (2000) ; he observed 25 searchers querying the
INSPEC database. The main findings confirm the effectiveness of RF. The initial search produced on average three highly relevant documents, and the feedback run produced on average nine further highly relevant doc-uments. Another study involving users was done by Spink (1994) ; she performed an experiment with forty users to assess how humans perform query expansion in an interactive environment. She concluded that users not possible to calculate evaluation measures and thus quantify the gain obtained from reformulation.
RF has been widely applied to CLIR with good results. However, the vast majority of the experiments have
Users X  assessments of relevance are especially important for CLIR since the feedback process involves the sub-the user X  X  language.

This lack of user experiments in the CLIR environment has been addressed in part by the Interactive Track at the cross-language evaluation forum (iCLEF) ( Oard &amp; Gonzalo, 2001, 2003a, 2003b ), which provides a common framework for participant groups to evaluate several aspects related to the formulation of queries, translation of queries, and assessment of relevance.

A related study developed by Karlgren and Hansen (2003) for iCLEF compared the performance of users assessing documents in their native language (Swedish) with their performance in assessing documents in a language they know well (English). As expected, they found that users take longer and make more mistakes when judging documents in a foreign language even assuming a good knowledge of that language.
The use of Machine Translation (MT) in aiding relevance assessments was analysed by iCLEF in three studies:
Wang and Oard (2001) compared the performance of full MT and term-for-term gloss translations obtained from bilingual term lists found on the web. Subjects had little or no knowledge of the language of the documents. The results show that searchers were able to make relevance judgements with either approach. However, MT achieved slightly better results.

Bathie and Sanderson (2002) compared users X  ability in judging native language documents and documents originally written in a foreign language and automatically translated into the user X  X  language. The docu-ments were articles from the LA Times in their original language and from Le Monde automatically trans-lated into English. The study concluded that users were able to make judgements with the same accuracy for both types of documents.

Lo  X  pez-Ostenero, Gonzalo, Pen  X  as, and Verdejo (2001) compared the performance of MT and a phrase translation based algorithm developed with the use of comparable corpora. Searchers had low or no pro-ficiency on the language of the documents. The results show that precision was similar for both systems, but recall was better when using phrasal translations.

Though these experiments demonstrate that MT can facilitate relevance assessment in a CLIR environ-ment, no study has yet examined the extent to which judging relevance may be better when using human trans-lations of foreign language documents rather than MT. Further, the reviewed literature does not present any research on how the errors in judgement affect the change in performance achieved by the RF process. Those aspects are addressed by the experiment described in this paper. 3. The CLIR system
The CLIR system used in this paper 1 was implemented using Latent Semantic Indexing (LSI), a method proposed by Deerwester, Dumais, Furnas, Landauer, and Harshman (1990) , and extensively tested by Dumais (1991, 1995) and Dumais and Nielsen, 1992 . The main goal of using LSI for CLIR is to provide means for matching text segments in one language with text segments of similar meaning in another language without needing to translate either, by creating a language-independent representation of the words.
LSI was first applied to CLIR by Landauer and Littman (1990) . The method used here is essentially the same as theirs. However, since there is no parallel corpus containing Portuguese and English, SYSTRAN 3.0 Professional was used to translate a sample of documents (approximately 20%) from the collection described in Section 4.2 to simulate a parallel corpus. The reason for choosing SYSTRAN is that it is a widely used translator in CLIR literature, especially for CLEF experiments ( Braschler, 2003 ).

LSI is applied to a matrix of terms by documents. Therefore, the first step is to build such a matrix based on a set of dual-language documents. 2 The matrix contains the number of occurrences (or weights) of each term in each document. In an ideal situation the pattern of occurrence of a term in language A should be identical to the pattern of occurrence of its match in language B. The resulting matrix tends to be very sparse, since most terms do not occur in every document.

This matrix is then factorised by singular value decomposition sions, throwing away the small sources of variability in term usage. The k most important dimensions are kept.
Roughly speaking, these dimensions (or factors) may be thought of as artificial concepts; they represent extracted common meaning components of many different words and documents. Each term or document concepts. Since the number of factors or dimensions is much smaller than the number of unique terms, words will not be independent. For example, if two terms are used in similar documents, they will have similar vec-tors in the reduced-dimension representation.

LSI implements the vector-space model, in which terms, documents and queries are represented as vectors in a k -dimensional semantic space. After deriving the semantic space with an initial sample of dual-language documents, new documents can be added. Those new documents will be placed at a location calculated by as pseudo-documents and placed at the weighted sum of its component term vectors. The similarity between query and documents is measured using the cosine between their vectors.

SVD causes synonyms to be represented by similar vectors (since they would have many co-occurrences), which allows relevant documents to be retrieved even if they do not share any terms with the query. This is what makes LSI suitable for CLIR, given that a term in one language will be treated as a synonym to its match in the other language. The main advantages of using LSI for CLIR are:
There is no traditional-style translation. All terms and documents are transformed to a language-indepen-dent representation.
 New languages can be added easily, provided you have training data.
 There is no need for expensive resources such as dictionaries, thesauri or machine translation systems.
Furthermore, Yang et al. (1997) performed tests with several CLIR approaches, including query translation and statistical methods, in all tests, LSI X  X  performance was among the best. In addition, the loss in perfor-mance between monolingual and bilingual executions was small, about 15%. The bilingual version of our sys-tem achieved 81% of the monolingual performance (with the LA Times test collection, described in Section 4.2 ). This also shows MT is a feasible alternative for simulating a parallel corpus.

An important aspect of LSI which made us choose this method for the experiments described in the next section is that the same RF strategy used in a monolingual LSI system can be directly applied to a CLIR sys-tem. That represents an important advantage, since most RF methods cannot be directly applied to CLIR as the words from the documents will not match the words from the queries. The literature contains some cases in which RF methods have been adapted for CLIR. Yang et al. (1997) proposed a method for PRF on a bilingual finding the translation mates for the top ranked documents; and then using those documents to create a query in the target language. Ballesteros and Croft (1998) proposed a method for using PRF with dictionary meth-a combination of both. Qu et al. (2000) , suggested a similar method for PRF in MT-based systems. All meth-ods reported above mention performance improvements. 4. Experimental design
The design of the experiment aims at answering two main questions: (i) How well can native Portuguese searchers recognise relevant documents written in English, compared to (ii) What is the impact of misjudged documents on the performance improvement that can be achieved by
It is worth pointing out that in order to answer the second question, we could have employed simulated user judgements. However, real users provide a better insight on the type and frequency of judgement errors that are made in an operational setting, since their choices are not random. Considering that user judgements were vital for answering the first question, we opted for using them for the second aspect as well.
The next subsections describe the design of the experiment. Characteristics of the searcher, document col-lection, query topics and procedure are detailed. 4.1. Searcher
The aim was to obtain subjects that would be likely users of a CLIR system. In this case: Portuguese speak-ers who have basic or no knowledge of English, that are not able to express their queries in English and that are familiar with computer searching. The searchers were recruited among students and lecturers from UCPel pants were obtained. The average age was 29.

Language skills are hard to measure accurately; what may be considered  X  X  X ntermediate X  X  to one person, might be considered  X  X  X dvanced X  X  by another. Ideally, the searchers would have taken a standard English lan-guage test such as TOEFL, enabling a more exact categorisation of their knowledge. However, that was not sample question. Most of the answers (13) fell into category 4. The remainder fell into categories 3 (8) and 5 (6). 4.2. Document collection
The collection used in the experiments consists of over 113000 news articles from the Los Angeles Times amounting to 450 Mb. The documents, which were published in 1994, deal with a broad variety of subjects such as politics, business, sports, culture and entertainment. This collection has been provided by the cross-language evaluation forum (CLEF) . 4 4.3. Query topics
Six query topics were extracted from CLEF 2002, which had a total of 50 queries. The Portuguese version of the topics was used. Below we present the criteria for topic selection, which were defined based on initial search results:
Select topics that had more than 10 relevant documents. This criteria prevents the situation in which all relevant documents are presented to the user for feedback.

Select topics that have relevant documents among top ten retrieved. Since the RF method used only posi-tive feedback, this criteria was used to prevent the situation in which the user does not judge any document as being relevant.
 Seventeen of the fifty topics satisfied the above conditions. Six of them were then randomly selected. The English version of the selected topics is presented below: Topic 1  X  num  X  C092  X  /num  X   X  EN-title  X  UN sanctions against Iraq  X  /EN-title  X 
 X  EN-desc  X  What measures has Iraq taken to effect the lifting of the UN economic embargo and political sanctions imposed after its invasion of Kuwait in 1990?  X  /EN-desc  X 
 X  EN-narr  X  Documents must include ways in which Iraq has attempted to get the sanctions lifted. Mere descriptions of the sanctions or rhetoric against the sanctions are not relevant. Expressions of regret for invading Kuwait by Iraqi officials are relevant.  X  /EN-narr  X  Topic 2  X  num  X  C094  X  /num  X   X  EN-title  X  Return of Solzhenitsyn  X  /EN-title  X 
 X  EN-desc  X  Find documents which report about the return of the Nobel prize winner for literature Solzhe-nitsyn to Russia.  X  /EN-desc  X   X  EN-narr  X  Relevant documents report the reasons and the time of the return of Solzhenitsyn to Russia. They may also talk about the reasons for his emigration to the US.  X  /EN-narr  X  Topic 3  X  num  X  C107  X  /num  X   X  EN-title  X  Genetic Engineering  X  /EN-title  X   X  EN-desc  X  How does genetic engineering affect the human food chain?  X  /EN-desc  X 
 X  EN-narr  X  Articles must directly address the introduction of genetic engineering, and its effects on the human food chain. They will discuss both pros and cons. Reports on tobacco bioengineering and human gene engineering are not relevant.  X  /EN-narr  X  Topic 4  X  num  X  C123  X  /num  X   X  EN-title  X  Marriage Jackson X  X resley  X  /EN-title  X 
 X  EN-desc  X  Find documents that report on the presumed marriage of Michael Jackson with Lisa Marie Pres-ley or on their separation.  X  /EN-desc  X   X  EN-narr  X  In May 1994, the famous pop star, Michael Jackson, was reported to have married Lisa Marie
Presley, the daughter of the king of rock and roll. Relevant documents must either contain some details regarding the wedding, such as where or when it was held, or must discuss the later separation of the cou-ple.  X  /EN-narr  X  Topic 5  X  num  X  C130  X  /num  X   X  EN-title  X  Death of Nirvana leader  X  /EN-title  X   X  EN-desc  X  How did the lead singer of the American rock and grunge group, Nirvana, die?  X  /EN-desc  X 
 X  EN-narr  X  Kurt Cobain, lead singer of Nirvana, the famous popular music group, died in April 1994. Doc-uments that report the death of Cobain without mentioning the cause are not relevant.  X  /EN-narr  X  Topic 6  X  num  X  C140  X  /num  X   X  EN-title  X  Mobile phones  X  /EN-title  X   X  EN-desc  X  Prospects for the use of cellular phones.  X  /EN-desc  X 
 X  EN-narr  X  Relevant documents report on the prospects for the use of cellular phones and the development of the mobile phone industry.  X  /EN-narr  X  4.4. Procedure
Fig. 2 shows how the experiment was performed. Searchers, represented by the man in the left of the figure, were presented with query topics (written in Portuguese) and a ranked list of 10 documents returned in response to an initial query. This ranked list was produced by presenting all terms from the  X  X  X itle X  X  and had the highest cosine with the query vector were ranked as best matches. The participants were asked to clas-
Similar to what is done by iCLEF ( Oard &amp; Gonzalo, 2003b ), the participants were given a definition of should consider relevant any document that contains information on the topic. Documents with only a part (or portion) related to the topic should also be considered relevant. Additionally, each document should be judged independently of other documents, even if they contain the same information.

Each participant read 6 queries and 10 documents for each query, amounting to 60 relevance judgements per participant and 1620 in total. The users saw the full text of the documents, which was presented in one of the three formats presented below: the original English text, as returned from the CLIR-LSI system (System 1), a machine translation produced using SYSTRAN 3.0 Professional (System 2), a human translation, produced by the first author (System 3).

The number of relevant documents per query varies. Similarly, the number of relevant documents ranked in a Latin square design, which controlled for learning effect and tiredness of the searchers. The order in which the different systems were presented has also been varied. Table 2 shows a 9-subject matrix. As there were 27 participants, the same matrix was used three times. Participant 1 saw the documents for topics 1 and 2 in the original language (English), then the documents for topics 3 and 4 automatically translated into Portuguese and finally, documents for topics 5 and 6 manually translated to Portuguese. Participants 1, 2 and 3 had the same topic-system combination, however the order in which the query topics were presented was different for each subject. The average time taken to judge all sixty documents was one hour.

Besides providing relevance judgements, the users were asked some questions related to their language skills, experience in computer searching, confidence in the judgements made, prior knowledge of the query topics, difficulty of the judgements, and if they preferred to view the documents in their original language or translated into Portuguese.

After gathering the judgements for all 27 searchers, the queries were re-formulated, resubmitted and re-evaluated for recall and precision. RF was performed by replacing the original query with the vector average of the documents the user selected as relevant, as described in Dumais (1991) .
 5. Users ability in making relevance judgements As reported in the previous section, a three-point relevance scale was used. However, to be compatible with
CLEF assessments and the evaluation software, all  X  X  X ot sure X  X  were forced to  X  X  X rrelevant X  X . Analysis of the data concentrated mainly on the following aspects: 1. The number of mistakes made by the searcher. 2. The level of agreement between the CLEF judgements and the judgements of each user. 3. Confidence of the judgements, the difficulty of the task, and prior knowledge of the topics.
The data collected for most variables is not perfectly normally distributed. The statistical test chosen to compare the results for different groups was an ANOVA as it is robust in dealing with data that depart from the normality assumption. For all tests reported, a was set to 0.05. 5.1. Number of mistakes
The relevance judgements provided by CLEF were considered as  X  X  X orrect answers X  X . Each judgement col-lected from the participants was compared against them. Two types of mistakes were analysed: (i) false alarm, document as irrelevant.

A total of 1620 judgements were made, 540 for each system (see Section 4.4 ). Each query topic had a var-properly weighted to allow for fair comparisons between topics.

Table 3 shows the numbers for missed relevant, false alarms, and correct judgements. It also displays how the judgements spread across the 3 possible categories: relevant, not relevant and not sure.
The number of relevant missed was virtually the same for the machine translated texts and the hand trans-lated texts. The number of relevant missed for the original texts was much higher (43%). That happened because most judgements (63%) for this system fell into the  X  X  X nsure category X  X . The number of false alarm was very small in the original texts for the same reason, and was the largest for the hand translated texts because people made more positive judgements in that system. An ANOVA test on missed relevant and false alarm has shown no significant difference between judgements made using hand and machine translated texts ( p -values 0.95 and 0.12, respectively). 5.2. Overlap
Overlap has been defined by Lesk and Salton (1968) as the intersection of the relevant documents divided by the union of the relevant document sets. In the context of this experiment, overlap measures how accurate the participant X  X  judgements were, as it tells how similar each participant X  X  judgements were compared to the relevance judgements provided by CLEF. Only the documents presented to the users were considered when calculating the overlap.

The average overlap between the CLEF assessors and the participants of the experiment is shown at the lated and machine translated texts are very similar, and much better than the scores achieved when assessing judgements made on hand and machine translation texts. 5.3. Difficulty, confidence and knowledge
The participants were asked to rate each topic in terms of difficulty of the task and the confidence they had in the judgements made. They were also asked how familiar they were with the subject of the query topics. For all three questions there were five levels. An English version of the questions asked is shown in Fig. 3 .
Participants found judging English documents considerably more difficult than judging MT or hand trans-lated documents. They also had less confidence in the judgements made. Confidence and difficulty were similar for MT and hand translated documents.

Those three measures have also been evaluated separately for each topic. Difficulty and confidence had very of the topics. Likewise, no significant difference was found among the degree of confidence in the judgements level of confidence in the judgements). 5.4. Testing other user groups
The experiment was repeated with a smaller sample of 6 bilingual participants, all native Portuguese speak-ers with very good English skills. Four participants rated their ability in reading and writing in English on category 1 (proficient) and the remaining two rated their ability on category number two.

The results are summarised in Table 4 . Bilingual participants were able to make judgements with the same accuracy using hand translated, machine translated and original texts. An ANOVA test using the data from false alarm, missed relevant and the overlap showed no significant difference among the three means.
The experiment has also been repeated with native English speakers. The aim was to establish the expected degree of agreement between a participant and CLEF judges, when the participant fully understood the lan-guage of the documents. The 6 participants recruited saw only the English documents, and each judged 2 query topics (20 documents). In total, data for 12 queries were analysed. Table 5 shows the results for this group.

The results shown here are comparable to the ones obtained by Voorhees (1998) in an experiment using three groups of relevance assessors, all native English speakers from a similar background judging English documents. She found that the overlap between pairs ranged from 0.42 to 0.49.

The performance of the Portuguese participants from both groups (poor English skills and bilingual) judg-ing hand and automatically translated documents is equivalent to the performance of native English speakers judging English documents. That confirms the conclusions of the main experiment and indicates that MT is as effective as hand translation in aiding users to assess relevance. 5.5. Discussion
The superior performance of machine translated texts in comparison to the judgements made on the origi-nal texts happened despite the many translation errors and awkward grammar, which led to several com-plaints from the participants. Those results imply that there is no advantage in judging relevance on hand translated documents despite the extra time and cost incurred in generating them.

It seems logical that the presence of proper names from the queries such as Nirvana or Solzhenitsyn in the documents would provide the user with great help for the judgements. This was expected to aid the assessment of the English documents in particular. However, the results do not confirm that assumption, as even with the presence of such keywords, most users were not able to accurately judge documents. A possible reason is that the participants did not rely on the presence of such clues alone, when they could not understand the context in which such proper names were used. 6. Errors in judgement and change in performance
The precision of the initial query (baseline) was compared with the precision attained after the RF process for each user. Recall that the RF process involved replacing the initial query with the vector average of the documents the user judged relevant. The performance was evaluated using the  X  X  X esidual collection X  X  method, whereby the documents that have been judged by the searcher are excluded from the collection and from the relevance assessments. The second ranked list will only contain documents that have not been judged. This method provides an unbiased evaluation of RF and is a de facto standard used by most RF research ( Harman, 1992a ).

Table 6 shows average precision for the baseline run and different feedback runs using different sets of rel-evance judgements. The feedback runs compared are: CLEF X  X sing the official judgements provided by CLEF.
 Best X  X sing the judgements provided by the participant who achieved the biggest overall improvement. Worst X  X sing the judgements provided by the participant who got the largest overall decline. Optimal X  X electing the set of judgements that yielded the best result for each topic.
 Average X  X ombining the results from all participants for each topic.

The change in performance varied greatly from one user to another, ranging from a deterioration of 22% to an improvement of 54%. Averaging the results for all users resulted in an improvement of 12.53% on average precision (change from 0.2553 to 0.2873). For the user, this means that for each query there were on average two new relevant documents identified amongst the top ten. The relevance assessments provided by 6 partic-ipants yielded a decrease in performance. An important observation is that the official CLEF judgements, which are the gold standard, did not produce the biggest overall improvement. In fact, the official run was outperformed by the runs of 10 participants of the main experiment, which implies that judgement errors may sometimes help the RF process. This fact has also been observed by Shen, Tan, and Zhai (2005) in an implicit feedback experiment. Their system achieved performance improvement even when users clicked on non-relevant documents.

In order to establish what affects the performance of RF, it is necessary to analyse the effect that each var-iable had in the change in performance achieved. This can be done by calculating correlation coefficients between the change in performance and the other variables analysed. The correlation coefficients obtained when analysing the results for all 162 queries are shown in Table 7 .

The overall results show a moderate negative correlation between the change in performance and the missed relevant. A similar correlation is found between the change in performance and false alarm. That is is a moderate positive correlation between the change in performance and the overlap. In summary, it can be said that 7% of the change in performance can be explained by the false alarms, 6% by the missed relevant and 11% by the overlap or accuracy in judgement. 6.1. Evaluation by system
With the purpose of understanding if the different systems had an effect on the performance change, the next step is to compare the results for different systems. Table 8 shows correlation coefficients between the improvement in precision with the misjudged documents (relevant missed and false alarm), and with the over-lap for each system.
 that system suffered more from that type of mistake. However, the differences among systems are not large enough to justify assigning the effect of the changes in performance to system variations. 6.2. Evaluation by topic
The next step then is to evaluate the correlation coefficients topic by topic. Table 9 presents correlation coefficients between change in precision and the mistakes for each of the six query topics.
The figures presented in Table 9 show big differences from one topic to another. Some topics show a strange behaviour, for example, missed relevant has a positive correlation with change in performance for topic 2; false alarm presents a positive correlation with the change in performance for topics 1 and 6, and no corre-lation for topic 5.

The different behaviour of topics in response to the judgement mistakes indicates that the factors that affect the performance of RF vary from one topic to another. This fact seems analogous to the fact that different queries are better solved by different IR systems. In other words, if a system achieved a high precision in one query, that does not determine it will achieve a good result with another topic. Mandl and Womser-
Hacker (2003) have also shown this when evaluating several CLEF runs. They observed a high standard devi-ation for the performance of the topics and a high standard deviation for the performance of each run. They concluded that no run performed well in all topics. Presently, there are no means for determining which type of topics will do better with which type of IR system.

In conclusion, this study was able to establish that the effects of misjudged documents are different for each topic. We have shown that the main source of impact on the change in performance produced by RF is the ship between change in performance and the misjudged documents remain unclear. An analysis of the char-acteristics of the topics would require a larger number of queries than used in this experiment. 7. Summary and conclusion
This paper reported experiments to evaluate RF in a CLIR system. Portuguese speakers were asked to judge the relevance of some documents returned in response to an initial query. The 27 participants recruited have assessed English documents, documents hand-translated to Portuguese, and documents automatically translated to Portuguese. The accuracy of such judgements was evaluated by comparing them to the official relevance assessments provided by (CLEF). In addition the relationship between accuracy in judgement and the performance of RF was studied. The main findings are summarised below: Less than half (44%) of the participants were able to assess English documents.

Machine Translation can indeed aid searchers in making relevance assessments, despite producing docu-ments that are awkward to read. Participants judged machine translated documents with the same accuracy they judged hand-translated documents.

There is a moderate negative correlation between the number of misjudged documents and the improve-ment that RF can provide.

The factors that impact the change in performance vary greatly from one topic to another. Each topic responded differently to judgement errors. However, the characteristics of the topics that determine the relationship between change in performance and errors in judgement remain unclear.

No relationship was found between the change in performance and the difficulty of the topics or the con-fidence in the assessments or the knowledge of the subject.

Most participants consider a CLIR system very useful and would like the results translated into their native language.

Possibilities for future work include repeating the experiment using a different CLIR system, language-pair, and topics to assess whether the same results are achieved in different conditions.
 Acknowledgements We thank Pa  X  raic Sheridan for the helpful comments on this paper.
 This work was partially supported by a CAPES-PRODOC Grant (Brazilian Government).
 References
