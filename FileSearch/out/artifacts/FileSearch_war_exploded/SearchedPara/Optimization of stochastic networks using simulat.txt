 1. Introduction
Pattern Storage is one of the prerequisite for the pattern recogni-tion task to be realized using ANN. Feedback neural networks are the most commonly used networks for implementing the pattern storage task ( Kumar and Singh, 2012 ).Thefastreliableandcomputerized recognition of fi ngerprint images is a remarkable problem in pattern recognition that has not to this date received a complete solution.
Numerous approaches to Fingerprin t recognition have been adopted so far. The two most popular approa ches are Minutiae Matching and
Global Recognition. The Minutiae based matching is more micro-scopic requiring extensive preprocessing and is extremely sensitive
Recognition approach, on the other hand, matches features charac-operation. The ef fi ciency of the two methods depends to a great extent on the quality of the feature extraction methods adopted. A number of techniques have been developed for fi ngerprint veri tion such as a novel fi lter-based representation technique by Prab-hakar ( Prabhakar and Jain, 2001 ). The technique exploits both the local and global characteristics in a fi ngerprint image to make veri fi cation. The matching stage computes the Euclidean distance between the template fi nger code and the input fi nger code. Wang and Lee proposed Fingerprint Veri fi cation Using Directional Micro-pattern Histograms and LVQ Networks ( Chikkerur et al., 2005 ).
Venkatammani and Kumar proposed the design of the correlation
Huang and Aviyente (2004) have proposed the fi ngerprint veri tion based on wavelet subbands . Jain et al. ( 1997 )usedridgepatterns in fi ngerprint matching. The metho d suffered from high computa-tional cost. Minor modi fi cations to reduce the computational cost and establish better minutiae correspondence have been introduced proposed a new feature called polyline to extract ridge information.
In this method for each ridge sampling point, three transformation invariant features were calculated and matching was based on these features. However, it did not perform very well when the distortions were present. Isener proposed Graph based fi ngerprint matching algorithm ( Isenor and Zaky, 1986 ). This method was complicated and time consuming and therefore improvement was presented by
Hrechak ( Hrechak and McHugh, 1990 ). Sujan classi fi ed binary fi ngerprints using HAVNET ( Sujan and Mulqueen, 2002 ). The HAV-
NETarchitectureconsistsofasmanyoutputnodesasthenumberof stored fi ngerprints and therefore additional output nodes must be added as soon as a new fi ngerprint is to be stored. Recently some genetic algorithm based approaches have been suggested for print recognition. Tan et al. proposed a straightforward Genetic Algorithm (GA) for fi ngerprint matching ( Tan and Bhanu, 2006 ). These methods employ either the s imple evolutionary algorithm or its variants and thus convergence may take a lot of time. Sheng et al. (2007) proposed a memetic fi ngerprint matching algorithm and an improved integrated method ( Sheng et al., 2009 )whichworksby suggesting a consensus matching function in GA.

Most of the work in this fi eld focuses on adopting ef fi feature extraction and matching mechanisms to help reduce the the accuracy of the results. The performance of all the fi veri fi cation systems is limited by the quality of the fi ngerprint images, which depends on the skin conditions, age of the indivi-dual and sensitivity of the sensor which leads to different impressions of the same fi nger at different instances. Hence there is requirement of a system that can identify the similarities in all the varying images of the same fi nger and associate them to the same stored pattern. The Hop fi eld neural network is known to function in this manner. It is a fully connected feedback neural network which stores patterns in a manner rather similar to the brain i.e. the full pattern can be recovered even if the network is presented with only partial or incomplete information. This paper therefore explores the capabilities of the Hop fi eld neural network for recognition of fi ngerprint images.

The standard Hop fi eld model uses a very simple weight matrix formed with one-shot Hebbian learning that produces a network with relatively poor capacity and performance ( Davey et al., 2004 ).
A number of learning rules have been suggested to improve its performance ( Davey et al., 2004 ; Wiles and Elman,1995 ). One such rule is the Pseudoinverse rule which is known to enhance the capacity, recall ef fi ciency and pattern correction ( Chartier and
Lepage, 2002 ; Streib, 2005 ). The network capacity increases in the order of L / N and the retrieval ef fi ciency falls sharply as the capacity approaches 0.5 N ( Amari, 1972 ; Wang et al., 2011 ). Hillar et al. present an algorithm to store binary memories in a Little
Hop fi eld neural network using minimum probability fl ow, a recent technique to fi t parameters in energy based probabilistic models ( Hillar et al., 2012 ). The results have been compared with those obtained with OPR (i.e. Hebb rule) and PER (Pseudoinverse rule) and have been found to be impressive. The capacity is found to be increasing with noiseless patterns but with noisy patterns the algorithm fails to obtain results. This research work is an effort towards enhancing the capacity and recall ef fi ciency for both the noiseless and noisy patterns by introducing FFT, DWT and SOM as the feature extraction mechanisms and testing the HNN with Hebbian and modi fi ed Pseudoinverse rules.
 The stable states of the network represent the stored patterns.
The key idea in pattern storage by feedback networks is the formation of basins of attraction in the energy landscape by associating an energy function with each state of the network in the activation or output state space ( Sulehria and Zhang, 2008 ).
The number of basins of attraction depends not only on the network state but also the number of processing units and their interconnection strengths. A major drawback of this type of neural networks is that the memory attractors are constantly accompa-nied with a huge number of spurious memory attractors so that the network dynamics is very likely to be trapped in these attractors ( Sulehria and Zhang, 2008 ; Smith, 1999 ), and thereby preventing the retrieval of the memorized patterns. Rodriguez et al. also mention about the problem of spurious minima by introducing multiple reference points in the analysis of pattern storage and their recalling ( Rodr X guez et al., 2005 ). A number of approaches to enhance the solution quality (i.e. the storage and retrieval capabilities) of the basic HNN have been suggested ( Jeffrey and Rosner, 1986 ). These approaches can be broadly classi fi ed as deterministic and stochastic approaches. The deter-ministic approaches include problem speci fi c enhancements to enhance the feasibility of the solutions. On the other hand stochastic approaches work by attempting to escape the local minima thereby improving the solution quality. The prime meth-ods to incorporate stochasticity in the Hop fi eld Network are (a) Modifying the nature of the activation dynamics from deter-(b) Adding noise to the weights of the network. (c) Adding noise to the external inputs of the network. (d) Any combination of the above three methods.

The current paper is exploring the fi rst approach, whereby the deterministic state update is app lied in probabilistic manner. The stochastic neural network employed simulated annealing technique which helps in escaping the local minima, suffers from extremely large computation times for the recalling purpose. Simulated annealing is a compact and robust technique for providing excellent solutions to 1986 ; Suman and Kumar, 2006 ). It is based on the concept of annealing which is a metallurgical process, where if a material is can reach a global minimum thus avoiding the various shallow minima of the energy function. This concept of simulated annealing controlled by a temperature parameter will be applied in the simula-tions discussed in the paper. The Hop fi eld network, when presented initial temperature till the time it reaches a thermal equilibrium. At this point the temperature will be r educed and the procedure repeated till the temperature reaches a very small value close to zero.
In the proposed simulation we are studying the optimization of the Hop fi eld neural network and the hybrid SOM  X  HNN for storage and recall of fi ngerprint images with simulated annealing. The pre-processed patterns will be fi ltered using FFT and DWT and then stored in the Hop fi eld network using Hebbian and modi fi doinverse learning methods. The next set of experiments will test the mapping of self organizing maps with Hop fi eld Network. Here the image patterns are input into the self organizing map of fi dimensions. The codewords generated from the SOM are input into the Hop fi eld network as pattern information for storage using Hebbian and Pseudoinverse rules. The recall ef fi ciency of the Hop-fi eld and the SOM  X  Hop fi eld network will be compared and the effect of adopting stochasticity in the recall mechanism will be analyzed. It is observed from the simulations that the capabilities of the Hop fi eld network can be suf fi ciently enhanced by making modi tions in the feature extraction of the input patterns. DWT and SOM together can be used to signi fi cantly enhance the recall ef The probability of error in recall in the form of spurious minima can be eliminated by adopting stochasticity in the recall mechanism.
The paper is organized in following sections. Section 2 describes FFT, DWT and SOM as the methods for the feature extraction to create feature vectors for the input stimuli. In Section 3 the Hop fi eld neural network and its learning mechanisms have been discussed. Section 4 elaborates the stochastic neural net-works and the concepts of simulated annealing. The next section i. e. Section 5 depicts the simulation design and Section 6 describes the results obtained. Conclusions and the future scope are speci-fi ed in the Section 7 followed by references in the last. 2. Preprocessing for feature extraction
The pattern set used for the current study are the scanned fi ngerprint images of multiple individuals collected as dabbed fi ngerprints on a white paper. The scanned RGB images scaled down to dimension 30 30 are not of perfect quality and thus require enhancement methods to reveal the fi ne details of the images which may remain uncovered due to insuf fi cient ink or imperfect impressions. Hence images are preprocessed through histogram equalization and binarization before converting them to suitable patterns for further processing. Fig. 1 depicts the respec-tive enhancements in one of the fi ngerprint images.

The feature extraction algorithms extract unique information from the images ( Sandirasegaram and English, 2005 ). The ef -ciency of the adopted feature extraction method decides to a great extent the quality of the image for further processing. Therefore the following feature extraction algorithms have been considered for extracting the useful features from the images for their storage. 2.1. Fast Fourier transform
The binarized image is subjected to FFT fi ltering. The FFT transform obtained is fi ltered through unsharp fi lter and there-after the inverse transform is obtained The mathematical nota-tions for FFT and inverse FFT can be referred from ( Sandirasegaram
FFT fi ltering of the binarized image shown in Fig. 2 a. 2.2. Discrete wavelet transform
The digitized binary images are subjected to DWT fi ltering and the corresponding component wavelets are obtained from a single prototype wavelet called mother wavelet by dilations and shifting as  X 
 X  where, a is the scaling parameter and b is the shifting parameter. obtained by Inverse DWT.
 Fig. 2 c.
 bipolar pattern with each pixel having value + 1or 1. The bipolar pattern is represented as bipolar pattern vector of dimension 900 1. Thus, the pattern vector x l can be represented in general form as where N  X  1to900.
 comprehensive matrix of order N L as
P  X  where l  X  1to L is the number of patterns to be stored in the network during learning. 2.3. Self organizing maps works by detecting regularities and patterns in the input data and creates an organized description of data. They convert input patterns of arbitrary dimensions into one or two dimensional array of neurons. Each input is connected to all output neurons. Attached to every neuron there is a weight vector with the same dimensionality as the input vectors. Learning occurs through adaptation of the weights connecting the input layer to the array of neurons ( Smith, 1999 ). SOMs differ from competitive l ayers in the way that neighbor-ing neurons in the self organizing map learn to recognize neighbor-ing sections of the input space. Thus, self organizing maps learn both the distribution and the topology of the input vectors. The neurons are connected to adjacent neurons by a neighborhood relation, which dictates the topology and structure of the map.

The learning process comprises a competitive stage where a winning neuron closest to the input vector (in accordance with the de fi ned similarity measure) is identi fi ed and declared the winner.
Then follows the adaptive stage where the winner neuron and all the neurons in its activated neighborhood (chosen through a neighborhood function) adjust their weights in relation to the current input pattern. The rule for weight update can be de w  X  t  X  1  X  X  w ij  X  t  X  X   X   X  t  X  X  x i w ij  X  t  X  X   X  i  X  h ji  X  t where x i is the input pattern,  X   X  t  X  is the learning rate, h neighborhood function de fi ning the region around the winner neuron
The importance of the neighborhood lies in the fact that weight adjustment is done only for the neurons that lie in the neighbor-hood of the winning neuron. Further the size of the neighborhood shrinks as training progresses, thus localizing the area of max-imum activity.

This continues until the weights connecting the input data to the learning, the network is tuned to create localized responses to input
This ordering re fl ects the feature space for the pattern set P. The feature vectors extracted from SOM of 10 10 dimensions can be represented as a comprehensive matrix of order 100 900.
The features extracted from the SOM can be used as pattern vectors for storing or encoding in the feedback neural network of
Hop fi eld type. The associative memory feature of Hop fi eld neural network for pattern storage and their recalling can be accom-plished to incorporate symmetric feedback synaptic interconnec-tion between the processing units of the output layer in self organizing map. The processing units of the grid in SOM i.e. the feedback neural network architecture for the pattern storage are considered as bipolar units. 3. Hop fi eld neural network
The proposed Hop fi eld Model to store the L number of patterns each of which is of order N 1 consists of N processing units and
N n
N connection strengths. The state of the processing unit is considered bipolar with symmetric connection strength between the processing units. Each neuron can be in one of the two stable states i.e. 7 1. Storage as patterns is accomplished with Hebbian rule and the Pseudoinverse rule.

The Hebbian rule to store L patterns is given by the summation of correlation matrices for each pattern ( Kumar and Singh, 2012 ) as
W  X  1  X  0 for i  X  j ; 1  X  i  X  N  X  3 : 1 : 1  X  where N is the number of units/neurons in the network, x l to L are the patterns/images to be stored, where each component of x is bipolar.

The equations for storing L fi ngerprint images in an N -unit bipolar Hop fi eld neural network can be referred from ( Kumar and be represented as W where S i represents the state of i th unit at stability. This square matrix is known as parent or suboptimal weight matrix for storing the given input patterns because it represents the storing of input patterns.

Hop fi eld suggested that the maximum storage capacity with allowed. This was later revised to 0.138 N (Amit, Gutfreund, Sompo-linsky) using the replica method ( Atithan, 1995 ). While with bipolar in the network ( Streib, 2005 ). The pattern correction and recall ef fi ciency of the network trained with Hebbian rule is also poor and further deteriorates as the number of patterns stored in the network increases ( Sharma et al., 2007 ; Huang and Kuh, 1992 ). The storage capabilities can be improved by adopting variants of the Hebbian rule such as the weighted Hebbian rule ( Amari, 1972 ) and Hebbian rule with decay ( Kohonen, 1989 ). But these do not provide solution to the problem of crosstalk that appears with Hebbian learning. Crosstalk ampli fi es with the increase in the number of patterns and as it becomes too large creates spurious states other than the negative memorized patterns ( Rojas, 1996 ).
 Since Hebbian rule has the above mentioned limitations, Pseudoinverse rule was considered to improve upon the learning behavior of the Hop fi eld network.

The pseudoinverse rule also called the projection learning rule has been proposed by Kohonen and Ruohonen for linear networks without feedback ( Kohonen and Ruohonen, 1973 ). Personnaz et al. incorporated these rules with feedback ( Personnaz et al., 1986 ). The standard Pseudoinverse rule is known to be better than the Hebbian larly at minimizing the crosstalk between the stored patterns.
Pseudoinverse rule uses the pseudoinverse of the pattern matrix in contrast to the classical Hebbian rule which uses the correlation matrix of the patterns ( Wang et al., 2011 ). Considering the pattern set P containing L patterns each of size N , the pseudoinverse weight matrix is given by W  X  PP  X   X  3 : 1 : 7  X  where P is the matrix whose rows are x n and P  X  is its pseudoin-verse ( Streib, 2005 ; Labiouse et al., 2002 ).

The Pseudoinverse rule is neither local nor incremental as com-pared to the Hebbian rule. It involves inverting and N N matrix, thus the training is very slow and impractical. Hence the rule is modi and incrementality. Therefore fi rst the weight matrix is calculated using Hebbian rule stated in Eq. (3.1.1) and then the pseudoinverse of the weight matrix can be obtained as W where  X  W L  X   X  is the transpose of the weight matrix W L  X  W
The Pseudoinverse rule performs better than the Hebbian rule when the patterns are correlated. Theoretically, for L o N and uncor-related patterns the Pseudoinverse rule has zero error and storage capacity as N 1( Rojas, 1996 ). For correlated patterns the network capacity is of the order of L / N and the retrieval ef fi as the capacity approaches 0.5 N ( Amari, 1972 ; Wang et al., 2011 ).
Once the pattern set P has been stored in the Hop fi eld neural network using either the Hebbian or the Pseudoinverse rule, it is required that the performance of the network be tested for the memorized patterns, their noisy variants and also for incomplete pattern information. For this, the process of recalling is considered, whereby a test pattern, which can be the memorized pattern or its noisy form, is input into the network and the network is allowed to evolve through its activation dynamics. The output state of the network is then tested for resemblance with one of the expected stable states. But there is a possibility that the network may not settle into the same stable state that corresponds to the memor-ized pattern but into some false minima. This problem of false minima can be taken care of by modifying the activation dynamics of the network from deterministic to stochastic. 4. Stochastic neural networks
In associative memory architect ures, the memorized patterns act as attractors in the state space of the network i.e. they exercise a region of in fl uence around them so that states which are suf similar are mapped to them after repeated iterations of the system dynamics ( Sulehria and Zhang, 2008 ).Itcanbeobservedinthe
Hop fi eld Neural network that if any prototype input pattern is presented to the network, the activation dynamics of the network closely resembles the presented pattern. However, there exist addi-tional attractors that do not correspond to any memorized pattern and thus are unhelpful. These additional attractors may be either the reversed states or spurious states (comprising mixture states and spin may lead to probability of error in pattern recalling.

The addition of noise to the deterministic Hop fi eld Network can be bene fi cial in the elimination of the spurious minima because as the traversal of energy landscape is changed, the spurious local minima of the energy function may no longer be stable and therefore the probability that the network converges to a memorized pattern is increased ( Kohonen, 1988 ). This approach of attempting to escape the local minima for improving the solution quality is called stochastic approach. In the deterministic model the activation dynamics select any unit randomly for update. The new state of the network is a state where the energy is either decreased or remains the same. Thus if the energy landscape moves the system around some spurious minima then it is sure to be trapped into the shallow false minima. But on the other hand by using stochastic update in each unit it is possible to realize a transition to a higher energy state from a lower energy state ( Molter et al., 2005 ). With stochastic update, the activation value of a unit does not decide the next output state of the unit by directly using the output function, instead the update is expressed in probabilistic terms. The probability of fi ring for an activation value of x can be expressed as
P  X  s  X  1 = x  X  X  X  1 = 1  X  e  X  x  X   X  = T  X  X  4 : 1
 X  where T is the temperature of the network.

At T  X  0, the probability function is sharp with a discontinuity at x  X   X  . In this case the stochastic update reduces to a determi-nistic update. As the temperature is increased, the uncertainty in making the update according to f ( x ) increases, giving thus a chance for the network to go to a higher energy state. Therefore the result nonzero temperatures. Finally when T  X 1 , then the update of the unit does not depend on the activation value ( x ) any more. The state of the network changes randomly from 1 to 0 or vice versa. It is obvious that the stochastic update for different T does not change the energy landscape itself (as its shape depends on the network, its weights and the output function, which are fi only the traversal in the landscape will change.
 update of the units is described by the following set of equations: x i  X   X  f  X  x i  X  X  0for x i  X  0
Aunit i is selected at random for updating. The output is updated according to the stochastic update law, speci fi ed by the probability that the output s i  X  1fortheactivation x i is expressed as P  X  s i  X  1 = x i  X  X  X  1 = 1  X  exp  X  x i = T  X   X  X  4 : 4  X  A network with the above dynamics is called the Stochastic Network. never be a static stable state as the state of the network is always changing due to the stochastic update for each unit. However dynamic equilibrium for stochastic networks can exist, if the ensemble average state of the network does not change with time.
 at which the averages over all possible realizations of the states are independent of time, since the probability distribution of the states does not change with time. Pattern storage networks with symmetric weights reach thermal equilibrium at a given temperature, and since the average values do not change here, stable states can be achieved at thermal equilibrium in a stochastic network. Further, stable equilibrium states can be achieved only at temperatures below a critical temperature. The number of such states is very small and the actual number depends on the temperature. 4.1. Simulated annealing process of annealing controlled by the temperature parameter.
Annealing is based on a metallurgical process in which a material is heated and then slowly brought to a lower temperature. The crystalline structure of the material can reach a global minimum in this way. The high temperature excites all atoms or molecules, but later, during the cooling phase, they have enough time to assume fractures and fewer irregularities in the crystal. Annealing can avoid local minima of the lattice energy because the dynamics of the particles contains a temperature-dependent component. The parti-cles not only lose energy during the cooling phase, but sometimes borrow some energy from the background and assume a higher-energy state. Shallow minima of t he energy function can be avoided in this way. Simulated annealing has been widely applied to solve various combinatorial optimization problems ( Jeffrey and Rosner, 1986 ; Sanz, 2004 ; Matsuda, 1998 ). The recall of the memorized patterns corresponding to the presented noisy prototype patterns from the Hop fi eld network can also be formulated as an optimization problem ( Wang et al., 2011 ; Atithan, 1995 ). In this problem the criteria is to minimize the netwo rk energy in such a way that the network settles into the global minima corresponding to the mem-orized pattern and thus avoids getting trapped into any false minima.
Thus it provides a procedure to accomplish the task of pattern storage and its recalling in an optimal way ( Wang et al., 2011 ). schedule, becomes critical in realizing the desired probability of states near T  X  0. Thus, for recall, when a prototype input pattern is presented the network is allowed to reach an equilibrium state near T  X  0, following an annealing schedule starting from a high temperature. This will reduce the effects of local minima and thus reduces the probability of error in the recall.

Thus, for the process of recall a test pattern, which is a memorized pattern or its noisy form, is input into the network and the network is allowed to evolve through its stochastic dynamics. The temperature is initialized to a high value. Then a unit, say k, is randomly selected from the network for updating.
The energy of the network in the current con fi guration is com-puted via the Hop fi eld energy function as E  X  1
The state of the chosen unit is fl ipped to generate a new con fi guration. The energy of the new con fi guration is computed as E  X  1 Change in energy is computed as  X 
E  X  E B E A  X  4 : 1 : 3  X 
If the change in energy is negative, i.e.  X  E  X  0, the system is moving to a lower energy con fi guration, then the new con tion obtained as a result of fl ipping of the chosen neuron is considered as a better state and the new state is accepted. But if the change in energy is positive then the new state is accepted with a probability calculated as per Eq. (4.1.4) .

P  X  s
 X  X  1
The above procedure of randomly selecting the neurons, testing them and setting their states continues several times until a thermal equilibrium is reached. At this point the temperature is lowered and the procedure is repeated. This continues until the temperature reaches a very small value.

A critical aspect of the algorithm is the choice of initial temperature and the annealing schedule. The annealing schedule can be speci fi ed as T  X  cT K  X  4 : 1 : 5  X  where T K is the current temperature, T K  X  1 is the new temperature and c is a constant with value ranging between 0 and 1 ( Wang temperature is gradually reduced to skip false minima.
 The process terminates as the temperature approaches zero i.e.
T  X  0. The higher non-zero probability state of the network at this 5. Simulation design
Experiment 1 : In this simulation design, we implement the pattern storage feedback neural network architecture of Hop type with 900 processing units to accomplish the storage and recalling of fi ngerprint input pattern vectors. This feedback neural network is considered for pattern storage with two learning rules i.e.
Hebbian rule and improved Pseudoinverse rule. The patterns being considered for storage have been preprocessed and fi ltered through
FFT and DWT separately. In these experiments, we are analyzing the storage capacity and recall ef fi ciency for the memorized patterns from this pattern storage network with the two learning rules and the process of recalling using Simulated annealing. The pattern storage can be described through Algorithm 1 and Algorithm 2
Algorithm 1. Pattern storage with Hebbian rule () { initialize weight matrix of size N N to zero; do } Algorithm 2. Pattern storage with modi fi ed Pseudoinverse rule () { }
The pattern information of the training set is encoded in the terms of weights between interconnection of the units. The weight matrix is thus created which contains the encoded pattern information of the memorized patterns. Now, the process of recalling is initiated by presenting prototype input pattern of any one of the already memorized patterns to the network. The recall is evaluated with the standard Hop fi eld recall algorithm and with Simulated annealing. The recall ef fi ciency provides an indication of the storage capacity of any pattern storage network. The network is also analyzed for presented noisy input or incomplete patterns of any one of the stored pattern. Network starts from any arbitrary state after the test prototype pattern is presented and then its activation dynamics starts iteration. The network fi nally settles into one of the stable states. This stable state corresponds to one of the memorized pattern that the input prototype pattern best matches. This process has the probability of error due to occur-rence of false minima. The pattern recall can be described through Algorithm 3 and Algorithm 4 : Algorithm 3. Pattern recall Hop fi eld () { }
Algorithm 4. Pattern recall simulated annealing () { } until (the temperature reaches a very small value close to zero)
Experiment 2 : The next experiment is conducted for feature extraction with SOM and mapped into the Hop fi eld neural net-work. In this simulation the input images are preprocessed with either FFT or DWT and compressed using SOM before storing them into the Pattern storage network. The training set of L patterns is fed through a sequential algorithm into the self organizing map of dimension 10 10 and with hexagonal topology. The storage of input patterns into SOM can be described with Algorithm 5 .
Algorithm 5. Self organizing map algorithm () { }
The Fig. 3 depicts the fi nal grid obtained after training the SOM for scanned example image of Fig. 2 .

The 100 feature vectors of dimension 900 1 each of the input vectors of images are generated by SOM and these are converted into bipolar vectors as shown in Fig. 4 .

These pattern vectors are passed as input to the Pattern storage network. The pattern storage is done with Hebbian and modi
Pseudoinverse rules as per the Algorithms 1 and 2 . Thereafter the recalling of patterns involves presenting a prototype or noisy input pattern for the already memorized patterns to the trained SOM.
SOM in response will map it to the BMU (Best Matching Unit) in the grid map. The codeword corresponding to the BMU is then presented to the Hop fi eld network and the pattern recalled is the one that best matches the prototype input. The Hop fi eld network with memorized patterns encoded with Hebbian and modi fi ed
Pseudoinverse will behave differently for the prototype input patterns and their noisy variants. The network is also analyzed for noisy variants of the memorized patterns. Such patterns when presented to SOM, are mapped to the unit with closest weight values termed the BMU. The codebook vector of the BMU is converted to feature vector and mapped to Hop fi eld Network.
The Hop fi eld network starts iterating and settles into one of the stable states. This stable state corresponds to one of the memor-ized patterns or false minima. The recall process for this SOM
Hop fi eld mapping is tested with Standard Hop fi eld method and through the Simulated annealing method depicted in Algorithm 6 and Algorithm 7
Algorithm 6. Pattern recall self organizing map  X  Hop fi eld algo-rithm () create feature vector for the codebook vector; read the parent weight matrix of Hop fi eld; input the feature vector; initialize feature vector to the network; do { to 1; }
Algorithm 7. Pattern recall SOM  X  HNN simulated annealing algorithm () { read the trained SOM; input the prototype input pattern; apply the input to each neuron in the SOM grid; select the BMU that best resembles the input vector; select the codebook vector corresponding to the BMU; create feature vector for the codebook vector; read the parent weight matrix of Hop fi eld; input the feature vector; initialize feature vector to the network; do { generated between 0 and 1 the accept the change lower the temperature as per Eq. (4.1.4) } until (the temperature reaches a very small value close to zero) } 6. Results and discussions
Experiment 1 : The simulated results of experiment 1 depicting prototype noisy input pattern fi ltered with FFT and DWT, with the standard Hop fi eld recall algorithm and with simulated annealing are depicted in Tables 1 and 2 respectively. The recall ef fi analyzed at various packing densities of the network i.e. 40, 70, 100, 130, 150, 180, 200 and 230 patterns. At all packing densities the prototype patterns were distorted by explicitly introducing noise upto percentages of 30%, 40% and 50% by modifying 270, 360 and 450 bits respectively.
 The following points can be observed from the results in Table 1 : (a) The HNN trained with Hebbian rule for FFT fi ltered patterns (b) The HNN trained with modi fi ed Pseudoinverse rule for FFT (c) At 50% distortion the network settles into some false minima The following points can be observed from the results in Table 2 : (a) The HNN trained with Hebbian rule for FFT fi ltered patterns (b) The HNN trained with modi fi ed Pseudoinverse rule for FFT (c) The results clearly show the complete absence of false minima
Fig. 5 shows the comparative analysis of the two tables. It can be seen from the graph that Simulated annealing produces the best recall results as compared to the Standard Hop fi eld Recall procedure. Simulated annealing with DWT fi ltered patterns and pseudoinverse followed by DWT fi ltered patterns memorized with Hebbian Learning and recalled with Simulated annealing. But due to the limited capacity associated with Hebbian learning the graph for it starts falling after packing density of 180 patterns. FFT fi ltered patterns memorized respectively with Pseudoinverse learning and Hebbian learning and recalled with Simulated annealing follow next. The patterns recalled with standard Hop fi eld procedures occupy the lowest segments of the graphs.

Experiment 2 : The results obtained from experiment 2 have been summarized in Tables 3 and 4 . The recall ef fi ciency has been analyzed at various packing densities of the network i.e. 40, 70, 100, 130, 150, 180, 200, 230, 250, 280, 310, 350 and 400 patterns.
At all packing densities the prototype patterns were distorted by explicitly introducing noise upto percentages of 20%, 30%, 40% and 50% by modifying 180, 270, 360 and 450 bits respectively. Fig. 6 presents a comparative analysis of the Pattern recall ef fi the SOM  X  HNN using the Standard Hop fi eld Recall Mechanisms and with Simulated annealing.
 The following points can be observed from the results in
Tables 3 and 4 : (a) The SOM  X  HNN trained with FFT and DWT produce extremely divergent results. The SOM  X  HNN trained with FFT patterns using Hebbian Rule almost crashes. Even at 40 packing density and 20% distorted prototype patterns, the network recognizes hardly 50% patterns. At 30% and 40% distortion there is hardly any recall ef fi ciency. Adopting the modi fi ed Pseudoinverse
Rule improves the recall ef fi ciency only slightly. Further the appearance of false minima has increased to such an extent that it can be concluded that all unassociated patterns are converging to false wells. (b) In contrast the SOM  X  HNN trained with DWT fi ltered patterns shows remarkable performance. With Hebbian rule, the SOM  X 
HNN outperforms the results in Table 1 and it can be seen that even at 250 packing density the network is able to perfectly associate all patterns at 20% distortion, almost 40% patterns at 30% distortion and close to 18% patterns at 40% distortion.
Thereafter the performance of the network starts falling slowly. With the adoption of the modi fi ed Pseudoinverse rule the network is seen performing well at packing density of 400 also, where all patterns are associated at 20% distortion, 55% (c) The recall ef fi ciency depicted at 20% distortion is a pointer to (d) The results with DWT fi ltered patterns are quite encouraging 7. Conclusions
Pattern Storage networks have been used to ef fi ciently store and works depends on a number of factors like preprocessing techniques, learning methods adopted and the activation dynamics followed by the network. The traditional learning method i.e. Hebbian learning of by using the modi fi ed Pseudoinverse rule. The feature extraction techniques adopted in this paper i.e. FFT, DWT and SOM have been implemented with Hebbian and pse udoinverse rule and the results are analyzed and compared. It has been analyzed through the simulation design that the ef fi ciency of the Hop fi eld network can be enhanced by (a) training it with modi fi ed Pseudoinverse rule instead of Hebbian rule, (b) presenting the prototype input patterns fi lteredwithDWTincomparisontoFFT,and(c)Thecapacityand enhanced if the feature vectors produced by FFT and DWT are compressed with SOM and then stored in the Hop fi eld network. Here, the network receiving DWT fi ltered patterns performs well and improves both the storage capacity and recall ef fi ciency but network receiving FFT fi ltered patterns performs poorly and this mapping also ampli fi es the occurrence of false minima. The increase in the occurrence of false minima in the SOM  X  HNN model can be attributed to overlapping of the feature spaces. Hence the distorted feature vectors when presented to the network tend to get associated to the pre-stored patterns that they most closely resemble leading to error model in the form of increased temperature creates a state of unrest. Hence, in contrast to the deterministic case where the system always moves to a lower energy con fi guration or stays at the same energy, helps in avoiding the false minima, which are relatively unstable. As the temperature is gradually decreased the system approaches the global minima thereby retrieving the memorized patterns only.
However since the feature spaces are overlapped, increase in the packing density beyond 100 patterns reduces the size of the attractor and above) memorized patterns starts falling.

The results of the simulations discussed in this paper show an enhancement over the traditional limits on capacity and recall ef fi ciency of the Hop fi eld Network ( Davey et al., 2004 ; Amari, 1972 ; Wang et al., 2011 ). The SOM  X  HNN Model shows an enhanced capacity and recall ef fi ciency with both the Hebbian rule and the Pseudoinverse rule. The results when compared to that in ( Hillar et al., 2012 ) show that the network is able to perform for both the noiseless and the noisy patterns. Further, with the adoption of simulated annealing in the recall mechanism the occurrence of false minima has been completely eliminated.
The optimization of Pattern storage network has been explored in this paper. The aim is to enhance the pattern correction and recall ef fi ciency of the network in such a way that false minima are eliminated. The results from the experiment have been quite encouraging. But still there are various scopes and dimensions to undertake the research in this area like the use of different evolutionary algorithms for SOM  X  Hop fi eld neural network to further improve the recall ef fi ciencies.
 References
