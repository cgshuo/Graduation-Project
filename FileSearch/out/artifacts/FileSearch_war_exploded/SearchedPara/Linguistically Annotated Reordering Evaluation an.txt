 Institute for Infocomm Research Institute for Infocomm Research Institute for Infocomm Research Institute for Infocomm Research itive lexicalized reordering approach. When combined with BWR, LAR provides complementary captured by reordering models. With the proposed analysis method, we conduct a comparative ment but also reveals new challenges in phrase reordering. 1. Introduction
The phrase-based approach is a widely accepted formalism in statistical machine trans-lation (SMT). It segments the source sentence into a sequence of phrases (not necessarily syntactic phrases), then translates and reorders these phrases in the target. The reason for the popularity of phrasal SMT is its capability of non-compositional translations and local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data.
To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering boundary words) is not adequate to move phrases to appropriate positions. In this example, boundary words and are able to decide that the translation of the PP phrase ... should be postponed until some phrase that succeeds it is translated. context VP  X  PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal counterpart in English.
 corporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In the second step, we inject soft linguistic information into nodes of the skeleton. to generate hierarchical structures. 2 This not only enhances phrasal SMT X  X  capability for hierarchical and long-distance reordering but also establishes a platform for phrasal
SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders ( straight or inverted ) of nodes in any binary branching structure, it is broadly verified that the ITG constraint has good coverage of word reorderings on various language pairs (Wu,
Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task.
 soft linguistic information into the nodes of the skeleton. We annotate each BTG node 536 with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree. The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this nodes during decoding with very little overhead, regardless of whether the BTG nodes linguistic elements are then used to guide phrase reordering under the ITG constraint. ing (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reorder-ing strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word X  X ased reordering (BWR) . LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Further-more, they can be combined.

The BLEU scores show that LAR is comparable to BWR and significantly improves phrase reordering when combined with BWR.
 particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT.
 translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic constituent movement. To that end, we introduce a syntax-based analysis method. We parse source sentences, and align the parse trees with reference translations as well as system translations. We then summarize syntactic reordering patterns using context-free grammar (CFG) rules from the obtained tree-to-string alignments. The extracted reordering patterns clearly show the trace of syntactic constituent movement in both reference translations and system translations.
 vs. BWR alone. There are essentially three issues that are addressed in this syntax-based comparative analysis. 1. The first issue concerns syntactic constituent movement in human/ 2. The second issue concerns the change of phrase movement after rich 3. The last issue concerns which constituents remain difficult to reorder even information about BTG-based phrasal SMT and phrase reordering under the ITG constraint. Section 3 describes algorithms which extract training instances for reorder-reordering model. Section 5 describes LAR and the combination of LAR with BWR.
Section 6 elaborates the syntax-based analysis method. Section 7 reports our evaluation results on large-scale data. Section 8 demonstrates our analysis results and addresses the various issues discussed above. Section 9 discusses related work. And finally, Section 10 summarizes our main conclusions. 2. Background 2.1 BTG-Based Phrasal SMT are two kinds of rules in BTG, lexical rules (denoted as r as r
A lexical rule translates a source phrase x into a target phrase y and generates a leaf node A in the BTG tree. Merging rules combine left and right neighboring phrases A and A r into a larger phrase A p in an order o  X  X  straight , inverted  X  X ] X  to denote a straight order and  X   X  an inverted order.
 and merging rules ( D = r l 1 .. n
BTG-based SMT is to find a best derivation, which yields the best translation. and corresponding weights  X  , then multiply them to obtain P ( D ). To keep in line with the common understanding of standard phrasal SMT (Koehn, Och, and Marcu 2003), here we re-organize these features into a translation model ( P ( P ), and a target language model ( P L ) as follows: where exp ( | e | ) is the word penalty.
 where p (  X  ) represents the phrase translation probabilities in both directions, p penalty. 538
One of the most important and challenging tasks in building a BTG-based phrasal SMT system is to define P ( r m ). 2.2 Reordering Under the ITG Constraint
Under the ITG constraint, three nodes { A l , A r , A p } to define the ITG reordering P ( r m ) as a function as follows: where o  X  X  straight , inverted } .
 highly related to the properties of language pairs. It is formulated as value of p s can be set as high as 0 . 8 to prefer monotone orientations because the two languages have similar word orders in most cases.
 contexts. To be context-dependent, the ITG reordering might directly model the condi-likelihood estimate (MLE) by taking counts from training data, in the manner of the lexicalized reordering model (Tillman 2004; Koehn et al. 2005):
Unfortunately this lexicalized reordering method usually leads to a serious data sparse-ness problem under the ITG constraint because A l and A r to the merging rules, and are finally unseen in the training data.
 of A l and A r , instead of nodes themselves, are used as reordering evidence in a new perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats the ITG reordering as a binary-classification problem where the possible order straight or inverted between two children nodes is the target class which the reordering model predicts given A l , A r ,and A p . 3. Reordering Example Extraction
Because we consider the ITG reordering as a classification problem, we need to obtain ing example , which is formally defined as a triple of ( o , b neighboring blocks and o  X  X  straight , inverted } is the order between them. where b must be consistent with the word alignment M
By this, we require that no words inside the source phrase c outside the target phrase e j 2 j words inside the target phrase. This definition is similar to that of the bilingual phrase except that there is no length limitation over blocks. Figure 1 shows a word alignment matrix between a Chinese sentence and English sentence. In the matrix, each block can be represented as a rectangle, for example, blocks ( c 4 4 and ( c 3 2 , e 3 3 ), ( c 3 1 , e 3 1 ) in blue rectangles.
 word-aligned bilingual data. The first algorithm AExtractor (described in Section 3.1) gual phrase extraction algorithm. The second algorithm TExtractor (described in Sec-tion 3.2) extracts reordering examples from BTG-style trees which are built from word alignments.
 540 3.1 AExtractor: Extracting Reordering Examples from Word Alignments
Before we describe this algorithm, we introduce the concept of junction in the word alignment matrix. We define a junction as a vertex shared by two neighboring blocks.
There are two types of junctions: a straight junction , which connects two neighboring Figure 1).
 as follows. 1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction 2. Detect junctions and store blocks in the arrays of detected junctions (lines 7 3. Select block pairs from each detected junction as reordering examples 1. strINV : We select the smallest blocks (in terms of the target length) for 2. STRinv : We select the largest blocks (in terms of the target length) for 3. RANDOM : For any junction, we randomly select one block pair from its 4. COMBO : For each junction, we first select two block pairs using selection 3.2 TExtractor: Extracting Reordering Examples from BTG-Style Trees keeping some block pairs as reordering examples while abandoning other block pairs.
The kept block pairs are not necessarily the best training instances for tuning an ITG
BTG trees of sentence pairs. Reordering examples extracted in this way are naturally suitable for BTG order prediction.
 produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu (1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models
BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse trees and a better reordering model is therefore induced from the extracted reordering examples. Using the better reordering model, the bilingual sentences are parsed again. translation or parsing accuracy. Formally, we can use expectation-maximization (EM)
BTG trees of sentence pairs with the current BTG model. Then we extract reordering examples and collect counts for them, weighted with the probability of the BTG tree where they occur. In the maximization step, we can train a more accurate reordering model with updated reordering examples. Unfortunately, this method is at high com-putational cost.
 over sentence pairs. Supposing we have word alignments produced by GIZA++, we then use the shift-reduce algorithm (SRA) introduced by Zhang, Gildea, and Chiang (2008) to decompose word alignments into hierarchical trees. The SRA can guarantee that each node is a bilingual phrase in the decomposition tree. If the fan-out of a node is larger than two, we binarize it from left to right: for two neighboring child nodes, if 542 they are also neighboring on both the source and target sides, we combine them and create a new node to dominate them. In this way, we can transform the decomposition tree into a BTG-style tree. Note that not all multi-branching nodes can be binarized. We extract reordering examples only from binary nodes.
 according to the method mentioned here. From this tree, we can easily extract four re-ordering examples in a straight order and one reordering example in an inverted order. 4. Boundary Word-Based Reordering Following the binary-classification perspective of the ITG reordering, Xiong, Liu, and
Lin (2006) propose a reordering model which exploits the maximum entropy (MaxEnt) classifier for BTG order prediction where the functions h i  X  X  0, 1 } are reordering features and the  X  features.
 example ( inverted, 7 15 | on July 15, | held its presidential and parliament elections ), leftmost/rightmost source words { words { on, 15, held, elections } will be extracted as boundary words. Each boundary word will form a reordering feature as follows where fn denotes the feature name, and bval is the corresponding boundary word. reordering: 1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase 2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further from word-aligned bilingual data as described in the last section, then generate reorder-ing features using boundary words from the reordering examples, and finally estimate feature weights. 5. Linguistically Annotated Reordering In order to employ more linguistic knowledge in the ITG reordering, we annotate each
BTG node involved in reordering using linguistic elements from the source-side parse tag ht of the head word, and (3) its syntactic category sc .Inthissection,wedescribethe annotation algorithm and the LAR model, as well as the combination of LAR and BWR. 5.1 Annotation Algorithm
There are two steps to annotating a BTG node using source-side parse tree information: (1) determining the sequence on the source side which is exactly covered by the node, then (2) annotating the sequence according to the source-side parse tree. If the sequence sequence , otherwise it is a non-syntactic sequence . One of the challenges in this an-notation is that phrases (BTG nodes) do not always cover syntactic sequences; in other words, they are not always aligned to constituent nodes in the source-side tree. To solve this problem, we generate a pseudo head word and composite category which consists phrases and therefore providing linguistic information for any phrase reordering. notation is trivial. Annotation elements directly come from the subtree that covers the sequence exactly. For a non-syntactic sequence, the process is more complicated. Firstly, we need to locate the smallest subtree c  X  covering the sequence (line 6). Secondly, we try to identify the head word/tag of the sequence (lines 7 X 12) by using its head word directly if it is within the sequence. Otherwise, the word within the sequence which is nearest to hw will be assigned as the head word of the sequence. Finally, we determine
L/R refers to the syntactic category of the left/right boundary node of s , which is the highest leftmost/rightmost sub-node of c  X  not overlapping the sequence. If there is no such boundary node (the sequence s is exactly aligned to the left/right boundary of c
L/R will be set to NULL. C is the syntactic category of c the external syntactic context of s . The composite category we define for non-syntactic phrases is similar to the CCG-style category in Zollmann, Venugopal, and Vogel (2008). annotated for each internal node. Some sample annotations are given in Table 1. 544 5.2 Reordering Model
The linguistically annotated reordering model P R model, which can be formulated as where the feature functions h i  X  X  0, 1 } are defined using annotated linguistic elements of each BTG node. Here we use the superscripts a l , a r ,and a are linguistically annotated. the left node A a l l as an example, the model could use its head word w as a feature as follows: ing examples from source-side parsed, word-aligned bilingual data using the reordering example extraction algorithm and the annotation algorithm. We then generate features using the linguistic elements of these examples. Finally we tune feature weights to build the MaxEnt model. 5.3 Combining LAR and BWR
LAR and BWR can be combined at two different levels: 1. Feature level. Because both LAR and BWR are trained under the 2. Model level. We can also train two reordering models separately and
We will empirically compare these two combination methods in Section 7.4. 6. A New Syntax-Based Reordering Analysis Method
In order to understand the influence of linguistic knowledge on phrase reordering, we propose a syntax-based method to analyze phrase reordering. In this analysis method, 546 we leverage the alignments between source-side parse trees and reference/system translations to summarize syntactic reordering patterns and calculate syntax-based measures of precision and recall for each syntactic constituent. 6.1 Overview
The alignment between a source parse tree and a target string is a collection of rela-tionships between parse tree nodes and their corresponding target spans. reordering pattern (SRP) is defined as [ i ] ... [ i n ] indicates the order of target spans  X  T 1 ... X  side. 6 cision and recall can be obtained. On the target side, the order of PP might be [1][2] or [2][1]. Therefore we have two syntactic reordering patterns for this structure:
Suppose that the two reordering patterns occur a times in the alignments between source parse trees and reference translations, b times in the alignments between source parse trees and system translations, and c times in both alignments. Then the reordering reordering model can reorder this structure. By summarizing all reordering patterns of all constituents, we can obtain an overall precision, recall, and F reordering model.

Collins, and Koehn 2007). Here we focus on the order transformation of syntactic con-stituents performed by reordering models during translation. In addition to aligning parse trees with reference translations, we also align parse trees with system transla-tions so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by com-paring both alignments.
 tracted from the alignments between source parse trees and reference translations as
REF-SRP and those from the alignments between source parse trees and system trans-lations as SYS-SRP . We refer to those present in both alignments under some conditions that will be described in Section 6.4 as MATCH-SRP . To conduct a thorough analysis on the reorderings, we carry out the following steps on the test corpus (source sentences + reference translations): 1. Parse source sentences. 2. Generate word alignments between source sentences and reference 3. According to the word alignments of Step 2, for each multi-branching 4. Generate REF-SRPs, SYS-SRPs, and MATCH-SRPs according to the target 5. Summarize all SRPs and calculate the precision and recall as described
We further elaborate Steps 2 X 4 in the Sections 6.2 X 6.4. 6.2 Generating Word Alignments
To obtain word alignments between source sentences and multiple reference transla-tions, we pair the source sentences with each of the reference translations and include the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the new corpus in both directions, and apply the  X  X row-diag-final X  refinement rule (Koehn et al. 2005) to produce the final word alignments.
 store the word alignments within each phrase pair in our phrase table. When we output the system translation for a source sentence, we trace back the original source phrase for each target phrase in the system translation. This will generate a phrase alignment between the source sentence and system translation. Given the phrase alignment and word alignments within the phrase stored in the phrase table, we can easily obtain word alignments between the whole source sentence and system translation. 6.3 Generating Target Spans and Orders
Given the source parse tree and the word alignment between a source sentence and a reference/system translation, for each multi-branching node  X  determine the target span  X  T i for each child node  X  i following Fox (2002). If one child target span will remain the same as the child node occurring in  X  neighboring nodes  X  i and  X  i + 1 , we combine these two nodes together and redefine a neighboring nodes can be combined. For example, the target span of nodes a and b in 548
Figure 6 overlap ((1, 3) vs. (2, 2)). Therefore these two nodes are to be combined into a new node, whose target span is (1, 3).
 call the multi-branching node reorderable , otherwise non-reorderable .Togetaclearer picture of the reorderable nodes, we divided them into two categories:
In Figure 6, both nodes a and c are fully reorderable nodes. reorderable node. Node g is a non-reorderable node because (1) the target spans of its child nodes d and f overlap, and (2) child nodes d and f cannot be combined because they are not neighbors.
 reorderable attribute of a node in the system translation, we prefix  X  X YS- X  to reorderable, reorderable, fully-reorderable, partially-reorderable 6.4 Generating SRPs
After we obtain the orders of the child nodes for each multi-branching node, we gener-ate REF-SRPs and SYS-SRPs from the fully/partially reorderable nodes. We obtain the
MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with the REF-SRPs for this node under the following conditions: 1. Because we have multiple reference translations, we may have different 2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit the structure VP  X  PP 1 ADVP 2 VP 3 . We obtain four REF-SRPs from four different ref-erence translations and one SYS-SRP from the system output. Here we only show the orders:
References c and d have the same order. Therefore we have three different REF-SRPs for this structure. In the SYS-SRP, PP 1 and ADVP 2 are combined and moved to the right distance to that of Reference b , we use the order of Reference b to compare the system order. In the Reference b order, both PP 1 and ADVP 2 are also moved to the right side of
VP 3 . Therefore the two orders of Reference b and SYS match. We have one matched SRP for this structure. 7. Evaluation
Our system is a BTG-based phrasal SMT system, developed following Section 2. We integrate the boundary word X  X ased reordering model and the linguistically annotated reordering model into our system according to our reordering configuration. We car-ried out various experiments to evaluate the reordering example extraction algorithms of Section 3, the linguistically annotated reordering model vs. boundary word X  X ased reordering model, and the effects of linguistically annotated features on the Chinese-to-
English translation task of the NIST MT-05 using large scale training data. 7.1 Experimental Setup then applied the  X  X row-diag-final X  refinement rule (Koehn, Och, and Marcu 2003) to 550 obtain many-to-many word alignments. From the word-aligned corpora, we extracted bilingual phrases.
 our reordering models, which consist of 33.3M Chinese words and 35.79M English words. We ran the reordering example extractor AExtractor and TExtractor of Section 3 on the chosen word-aligned corpora. We then extracted boundary word features from 2005) which was trained on the Penn Chinese Treebank with an F ran the off-the-shelf MaxEnt toolkit 8 to tune the reordering feature weights with the iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.
NIST MT-02 evaluation test data as our development set (18 words/31 characters per words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set contains four translations per source sentence. Both the development and test sets were also parsed using the parser mentioned above.
 shortest reference sentence length for the brevity penalty. The model feature weights are tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn 2004). 7.2 Bias in AExtractor selective extraction raises three questions: 1. Is it necessary to extract all reordering examples? 2. If it is not necessary, do the heuristic selection rules impose any bias on 3. Does the bias have a strong impact on the performance in terms of BLEU all reordering examples because even a very small training set will produce millions of reordering examples if we enumerate all block pair combinations. Secondly, extracting all reordering examples introduces a great amount of noise into training and therefore ing examples extracted using different extraction algorithms and selection rules. The
AExtractor with the COMBO selection rule extracts the largest number of reordering examples. However, it does not obtain the highest BLEU score compared with other selection rules which extract a smaller number of reordering examples. This empirically suggests that there is no need to extract all reordering examples.
 by our system. The BWR reordering model is trained on reordering examples which are extracted using different selection rules. Then we calculate the average number of words on the target side which are covered by binary nodes in a straight order. We refer to this number as straight average length . Similarly, inverted average length is calculated on all binary nodes in an inverted order. The third and fourth columns of Table 3 show the two average variables. Comparing these average numbers, we clearly observe that two selection rules indeed impose noticeable bias on the reordering model.
 552
Note that the selection rules RANDOM and COMBO do not impose bias on the length of extracted reordering examples compared with strINV and STRinv. The latter two se-lection rules have special preferences on the length of reordering examples and transfer these preferences to the reordering models as shown in Table 3.
 reordering example extraction algorithm with special selection rules, one might wonder whether we can allow the decoder to decide its own reordering preference. We add two ordered. And at the same time we add the number of words which are covered by these two neighboring nodes to rl . Their weights are tuned using MERT to maximize BLEU the word/phrase penalty features, we can allow the decoder to favor shorter or longer phrases. Similarly, with the two new features rc and rl , we can allow the decoder to favor shorter or longer reorderings.

RANDOM and COMBO selection rules because these two rules do not impose bias on means that the decoder prefers shorter reorderings to longer reorderings. However, the
BLEU scores on the test set are 31.89 and 32.0 for RANDOM and COMBO, respectively, which are worse than the BLEU scores of RANDOM and COMBO without using rc and rl in Table 3, and also worse than the performance of strINV and STRinv which impose preferences on reordering examples. This seems to suggest that the preference for shorter/longer reorderings imposed by the reordering example extraction algorithm is better than that decided by the decoder itself.
 much different although we have quite the opposite bias imposed by different selection rules. The changes in BLEU score, which happen when we shift from one selection rule
STRinv rule achieves the highest BLEU score. The reason might be that the bias towards distance swappings (Xiong et al. 2008b). 7.3 AExtractor vs. TExtractor
We further compared the two algorithms for reordering example extraction. In Table 3, we find that TExtractor significantly underperforms in comparison to AExtractor. This is because the transformation from decomposition trees to BTG trees is not complete.
Many crossing links due to errors and noise in word alignments generated by GIZA++ make it impossible to build BTG nodes over the corresponding words. It would be better to use alignments induced by the ITG and EM procedure described in Section 3.2 but this has a very high cost.
 below. 7.4 LAR vs. BWR Table 4 shows the results of the different integration of BWR and LAR into our systems. Only using LAR achieves a BLEU score of 32.17, which is comparable to that of BWR. This suggests that LAR is promising given that: word features are not adequate to move phrases to appropriate positions because they cannot recognize syntactic contexts which are very relevant to phrase reordering. can use syntactic information on the one hand and not worry too much about syntactic divergences on the other hand.
 level and the model level. When we combine them at the model level, we achieve an absolute improvement of 0.83 and 1.13 BLEU points over BWR and LAR, respectively, which are both statistically significant (p &lt; 0 . 01). This shows that LAR and BWR are significantly improve a very competitive lexicalized reordering model (BWR). icant improvements over BWR and LAR but marginally underperforms compared to
BWR+LAR. In our later experiments we use the combination method BWR+LAR. 7.5 Varying Training Data Size
To investigate how LAR improves BWR when we vary our training data size, we carried out experiments on three different training data sets: FBIS (7.09M Chinese words, 9.28M Nations corpus (33.3M Chinese words, 35.79M English words); and Large2 , which 554 consists of Large1 and the United Nations corpus (101.93M Chinese words, 112.78
English words). The language model remains the same for these three data sets because it is trained on a much larger data set (181.1M words).
 improvement of 1.55 BLEU points over BWR on smaller training data. When we enlarge the training data set from FBIS to Large1, both BWR and BWR+LAR improve quite a
When we continue to use more training data (Large2), the improvement obtained by integrating LAR becomes stable at the 0.8 level. 7.6 Effects of Linguistically Annotated Features
We conducted further experiments to evaluate the effects of individual linguistically an-notated features. Using the reordering configuration of BWR+LAR, we augment LAR X  X  feature pool incrementally: firstly using only syntactic categories
The experimental results are presented in Table 6, from which we have the following observations: 1. Syntactic category alone improves the performance statistically 2. Other linguistic information, provided by the categories of boundary 8. Analysis between source sentences and system/reference translations as described in Section 6.2. movement in the reference translations and system translations which are generated using two different reordering configurations: BWR+LAR vs. BWR. In LAR, we use the best reordering feature set ( sc + cc + hw + ht ). 8.1 Syntactic Constituent Movement: Overview be movable as a unit. To denote the proportion of syntactic constituents to be moved as a unit, we introduce two variables REF-R-rate and SYS-R-rate , which are defined as this table, we have the following observations: 1. A large number of nodes are REF-reorderable, accounting for 79.82% of all 2. The R-rates of BWR and BWR+LAR are 77.46% and 81.79%, respectively.
 556 8.2 Syntactic Constituent Movement among Multiple Reference Translations 8.2.1 Differences in Movement Orientation. Because each source sentence is translated by four different human experts, we would like to analyze the differences among reference the overall distribution over the number of different orders for each multi-branching constituent among the reference translations.
 for syntactic constituents. This makes it easier for our analysis to compare the system order with the reference order. However, there are 22% cases where two different orders are provided, which shows the flexibility of translation. According to our study, noun phrases taking DNP or CP modifiers, as well as DNPs and CPs themselves, are more likely to be translated in two different orders. Table 9 shows the percentages in which two different orders for these constituents are observed in the reference corpus. DNP constructs a phrasal modifier whereas CP constructs a relative-clause modifier. There is no fixed reordering pattern for DNP and CP and therefore for NP which takes DNP/CP as a pre-modifier. In the DNP  X  NP DEG structure, the DEG ( ) can be order for NP  X  DNP NP will both be straight: [1][2]. Otherwise, the two orders will be inverted: [2][1]. Similarly, there are also different translation patterns for CP and NP  X  CP NP. CP can be translated into  X  that + clause X  or adjective-like phrases in English. Figure 7 shows an example where the CP constituent is translated into an phrase which it modifies, the order for adjective-like phrases is flexible (see Figure 7). the system translation to that of the reference translation which has the shortest edit distance to the system translation as described herein so that we can take into account the potential influence of different translations on the order of syntactic constituents. 8.2.2 REF-Non-Reorderable Constituents. We also study REF-R-rates for the 13 most fre-quent constituents listed in Table 10. We find that two constituents, VP
NP reasons why they are non-reorderable in reference translations, we further investigate REF-non-reorderable cases for the constituent type VP 1  X  the reasons into three categories as follows. 1. Outside interruption. The reordering of PP and VP 2 is interrupted by 2. Inside interruption. The reordering of PP and VP 2 is interrupted by the 558 3. Parse error. This accounts for 29.90% of REF-non-reorderable cases.

VP  X  PP VP, they can be used to explain other REF-non-reorderable constituents, such as NP  X  CP NP. 8.3 Syntactic Constituent Movement in System Translations syntactic reordering patterns (REF-SRP, SYS-SRP, and Match-SRP) for all constituents, we can calculate the overall reordering precision and recall of syntactic constituents.
Table 11 shows the results for both BWR+LAR and BWR, where BWR+LAR clearly outperforms BWR. 8.3.2 The Effect of Linguistic Knowledge on Phrase Movement. To understand the change in phrase movement caused by linguistic knowledge, we further investigate how well
BWR and BWR+LAR reorder certain constituents, especially those with high distribu-tion probability. Table 10 lists the 13 most frequent constituents, which jointly account for 51.46% of all multi-branching constituents. Except for NP ing F 1 score of all these constituents in BWR+LAR is better than that in BWR. linguistic knowledge makes phrase movement in BWR+LAR pay more respect to syn-tactic constituent boundaries. The overall R-rates of BWR+LAR vs. BWR described in
Section 8.1 indicate that BWR+LAR tends towards moving more syntactic constituents constituent type. The fourth and fifth columns in Table 10 present the R-rate for each
BWR+LAR is much higher than that of BWR for almost all constituents. This indicates that higher R-rate is one of the reasons for the higher performance of BWR+LAR. the reordering of VP  X  PP VP in Figure 8. In both examples, BWR fails to move the PP constituent to the right of the VP constituent, whereas BWR+LAR does it successfully.
By tracing the binary BTG trees generated by the decoder, we find that BWR generated a very different BTG tree from the source parse tree whereas the BTG tree in BWR+LAR almost matches the source parse tree. In the first example, BWR combines the VP phrase 560 the NP phrase NHK , which makes the translation of NHK interrupt the reordering of
VP  X  PP VP in this example. The BWR tree in the second example is even worse. The non-syntactic phrase in the VP phrase is first combined with , which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures.
 for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant improvements on different language pairs, demonstrates that linguistically annotated reordering provides an alternative way to incorporate syntactic cohesion into phrasal SMT. 8.4 Challenges in Phrase Reordering and Suggestions more difficult to reorder, as indicated by their relatively lower F scores indicate that BWR+LAR is not fully sufficient for reordering these constituents although it performs much better than BWR. We find two main reasons for the lower F scores and provide suggestions accordingly as follows. 1. Constrained decoding. We observe that in reorderable constituents which 2. Integrating special reordering rules. Some constituents are indeed 8.5 Discussion
In the definition of syntactic reordering patterns, we only consider the relative order of individual constituents on the target side. We do not consider whether or not they remain contiguous on the target side. It is possible that other words are inserted be-
Table 12 shows the revised overall precision and recall of syntactic reordering patterns when we also compare gaps. The revised results show that BWR+LAR still significantly outperforms BWR. This also applies to the 13 constituents identified in Table 10. The analysis results obtained before are still valid when we consider gaps. 9. Related Work 9.1 Linguistically Motivated Phrase Reordering
There are various approaches which are devoted to incorporating linguistic knowledge into phrase reordering. Generally, these approaches can be roughly divided into three categories: (1) reordering the source language in a preprocessing step before decoding begins; (2) estimating phrase movement with reordering models; and (3) capturing reorderings by synchronous grammars. The preprocessing approach applies manual or automatically extracted reordering knowledge from linguistic structures to transform
The second reordering approach moves phrases under certain reordering constraints and estimates the probabilities of movement with linguistic information. In the third 562 approach, reordering knowledge is included in synchronous rules. The last two cate-gories reorder the source sentence during decoding, which distinguishes them from the first approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular.
Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-grained human-written rules, mostly concentrating on VP and NP structures. source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nie X n and Ney (2001), Popovi  X  c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological,
POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM con-straint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don X  X  respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn
Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang knowledge from source parse trees. Our reordering approach is most similar to those in
Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automat-ically learned from word-aligned corpus. In linguistically syntax-based models, string-to-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and
Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering
Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose with regard to reordering. In their method, common word n -grams occurring in both reference translations and system translations are extracted and generalized to part-of-speech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors. The larger the difference, the more reordering errors there are.
 lar to our method in Steps (1) X (3). They also parse the source sentence and automatically align the parse tree with the reference/system translations. The difference is that they highlight constituents from the parse tree to enable human evaluation of the translations of these constituents, rather than automatically analyzing constituent movement. They use this method for human evaluation in the shared translation task of the 2007 and 2008 ACL Workshop on Statistical Machine Translation.
 using human translations and alignments. Compared with her work, our analysis here includes, but is not limited to, an investigation of syntactic cohesion in an actual MT system . 10. Conclusion We have presented a novel linguistically motivated phrase reordering approach:
Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic
BTG in phrasal SMT. To automatically learn reordering features, we have introduced algorithms for reordering example extraction and linguistic annotation. We have also proposed a new syntax-based analysis method to detect syntactic constituent movement in human/machine translations.

BWR as well as the reordering example extraction algorithms. Our evaluation results show that: 1. Extracting reordering examples directly from word alignments is much 2. Selection rules which bias the reordering model towards smaller 3. BWR+LAR significantly outperforms BWR, which suggests that the syntax-based analysis method. Our analysis results show that: 1. BWR+LAR achieves a significantly higher reordering precision and recall 564 2. For most reorderable constituents, integrating source-side linguistic 3. For non-reorderable constituents or constituents involving long-distance interrupted by phrases outside the zones. Beginning and ending positions of the zones are automatically learned using lexical and syntactic knowledge. To capture complex re-orderings which cross constituent boundaries, phrasal SMT should integrate reordering rules with richer contextual information.
 Acknowledgments References 566
