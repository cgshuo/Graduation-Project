 Recent developments in com puting and information technology have enabled us to jointly analyze data from multiple sources such as multiple news feeds, social media streams and so on. Discovering structures and patterns from multiple data sources helps us to unravel certain commonalities and differences which otherwise is not possible when analyzing each data source separately . This information pro vides valuable inputs for various data mining and representation tasks. Whilst the data mining community has developed techniques to analyze a single data source, there is a need to develop formal frameworks for analyzing multiple data sources exploiting their common strengths.
However, modeling the data across multiple heterogeneous and disparate sources is a challenging task. For example, how do we model text , image and video together? At the semantic level, they provide much richer information together, and the question is how to exploit these strengths at lower levels? One solution is to exploit textual metadata for each data source in the form of tags . These tags are rich metadata sources, freely available across disparate data sources (images, videos, blogs etc.) and at topical or conceptual levels, they are often more mean ingful than what current content processing methods extract [2]. However, tags can be ambiguous, incomplete and subjective [7] due to a lack of constraints during their creation. Due to this problem, performance of any data mining task using tags suffers si gnificantly. Therefore, it is imperative to model the uncertainties of tag data to improve the performance of several data mining tasks. Work on tag denoising has been, broadly speaking, aimed at determining tag relevance through modification or the reco mmendation of additional tags [11,7]. But these approaches typically focus solely within the internal structure of a given tagging source, and are thus bounded by the information content and noise characteristics of the tagging source. Moreover, these methods, w orking individually for each data source, lack in exploiting the collective strengths of all data sources.

Addressing the problem of constructing a unified framework for disparate sources, we develop a Bayesian framework which can model the uncertainties of multiple data sources jointly using the textual tags from each source as a unified piece of metadata. Our method allows multiple data sources to exploit collective strength by learning prob-abilistic shared subspaces and, at the same time, crucially retains the differences of each data source by learning probabilistic individual subspaces. Retaining the differ-ences between data sources is very important; ignoring this aspect often leads to  X  X eg-ative knowledge transfer X . Another strength of the proposed framework is that both the shared and individual subspaces are probabilistic in nature, which helps in modeling the uncertainties involved in real world applications. Similar work by Gupta et al [5] also models the shared and individual subspaces but has certain limitations. First, their framework is restrictive in part as it can model only nonnegative data sources. Second, their model supports only two data sources, which renders the model unusable when working with more than two data sources. Third, the subspaces learnt are not proba-bilistic and do not cater for uncertainties such as missing tags and tag ambiguity.
Previous works on shared subspace learning are mainly focused on supervised or semi-supervised learning. Ji et al [6] and Yan et al [12] provide frameworks for ex-tracting shared structures in multi-label classification by learning a common subspace which is assumed to be shared among multiple labels. Si et al [10] propose a family of transfer subspace learning algorithms by minimizing Bregman divergence between the distributions of the training and test samples. This approach, while being fairly generic for transfer learning, is not appropriate for multi-task learning and can not exploit the knowledge strengths from the multiple data sources. In another work, Gu and Zhou [4] propose multi-task clustering by learning a s hared subspace for all tasks. This approach provides a way to learn shared subspaces, but has no way of controlling the sharing level, a crucial aspect when dealing with heterogeneous data sources. Moreover, the sharing is imposed among all tasks wh ich is unrealistic in many scenarios.
Our framework is based on the state-of-the-art Bayesian probabilistic matrix factor-ization (BPMF) model, recently proposed in [9]. We extend BPMF to enable joint mod-eling of multiple data sources deriving common and individual subspaces, and derive inference algorithms using Rao-Blackwellized Gibbs Sampling (RBGS). To demon-strate the usefulness of our approach, we examine two applications  X  improving social media retrieval using auxiliary sources, and cross-social media retrieval. We use three disparate data sources  X  Flickr, YouTube and Blogspot  X  to show the effectiveness of shared subspace learning frameworks.

Our main contributions are :  X  The construction of a novel Bayesian shared subspace learning framework for ex- X  Efficient inference based on RBGS (Gibbs sampling) for joint factorization.  X  Algorithms for retrieval within one social medium or across disparate social media  X  Two real-world applications using three popular social media sources: Blogspot, The novelty of our approach lies in the framework and algorithms for learning across diverse data sources. Our probabilistic shared and individual subspaces not only exploit the mutual strengths of multiple data sources but also handle the uncertainties. The significance of our work lies in the fact that theoretical extensions made to BPMF [9] for multiple data sources allow for the flexible transfer of knowledge across disparate media. In addition, RBGS samplin g derived for our model achieves better inference (i.e. Markov chain mixes bette r) compared to [9]. Our work brings a broad scope of open opportunitie s and applications X  X t is appropriate wherever one needs to exploit multiple and heterogeneous datasets for knowledge transfer among them, such as collaborative filtering or sentiment analysis.

The rest of the paper is organized as follows . Section 2 presents the Bayesian shared subspace learning formulation and describes Gibbs sampling based inference proce-dure. Section 3 and 4 demonstrate the appli cability of the proposed framework to social media retrieval applications. Conclusions are drawn in Section 5. We introduce a framework for learning indivi dual and shared subspaces across an arbi-trary number of data sources. Let a set of n data sources be represented by data matrices X 1 ,..., X n , which, for example, can be term-document matrices (each row a word and each column a document with tf -idf features [1]) for retriev al applications or user rating matrices (each row a user and each column an item with ratings as features) for collaborative filtering applications. We assume that matrices X i have the same number of rows. Whenever it is not the case, one can always merge the vocabularies of each data source to form a common vocabular y. Our goal is to factorize each matrix X i as X vectors among data sources whilst preserving their individual bases. For example, when n =2 , we create three subspaces: a shared subspace matrix W 12 and two individual subspaces W 1 ,W 2 . We thus write X 1 and X 2 as To define subspaces at data source level, we define W 1 =[ W 12 | W 1 ] and W 2 = [ W 12 | W 2 ] for the two data sources. Note however that the encoding coefficients corre-sponding to the shared subspace W 12 are different, and thus, an extra subscript is used to make it explicit in H 1 , 12 and H 2 , 12 . Notation-wise, we use bold symbols W , H to denote the entire decomposition at a data source level and normal capital letters W , H at the shared level . E 1 and E 2 denote the residual factorization error.
To see how we can generalize these expressions for n datasets, we continue with this Our intention is to create an index set over the subscripts { 1 , 2 , 12 } used in matrices presented in Eq (1) so that a summation can be conveniently written. To do so, we further use S (2 ,i ) to denote the subset of S (2) in which only elements involving i relaxation in the set notation, we rewrite them as S (2 , 1) = { 1 , 12 } and S (2 , 2) = { 2 , 12 } . Thus, Eq (1) can be re-written as each i =1 ,...,n , denote by S ( n, i )= { v  X  S ( n ) | i  X  v } the index set associated with the i -th data source. Our proposed shared subspace learning framework seeks a set of expression in the following forms for i =1 ,...,n It is also clear from Eq ( 2 ) that the total subspace W i and its corresponding encoding matrix H i for the i -th data matrix are horizontally augmented matrices over all W v and vertically augmented over all H i,v for v  X  S ( n, i ) respectively. That is, if we explicitly list the elements of S ( n, i ) as S ( n, i )= { v 1 ,v 2 ,...,v Z } then W i , H i are 2.1 Bayesian Representation and normally distributed with mean zero and precisions  X  X Bayesian shared subspace learning for arbitrary number of data sources, for simplicity, we show the graphical model for the case of two data sources in Figure 1. For each i  X  X  1 , 2 ,...n } and v  X  S ( n, i ) , our probabilistic model is then given as Bayesian, we further use a normal-Wishart prior on the parameters {  X  W v , X  W v } .The normal-Wishart prior is given by where W ( . |  X  0 , X  0 ) is Wishart distribution with K v  X  K v scale matrix  X  0 and  X  0 degree of freedom. Similar priors are placed on the parameters {  X  H i , X  H i } . For future reference, we define all the hyperparameters as  X  0 {  X  0 ,s 0 , X  0 , X  0 , X  X 2.2 Gibbs Inference Given data matrices { X i } n i =1 , the goal of BSSL is to learn the factor matrices W v and H i for all i performing posterior inference on the distribution of random (row or column) vectors from W v and H i . Since we are treating these vectors as Gaussian with proper conjugate prior normal-Wishart distribution, posterior inference can be conveniently carried out using Gibbs sampling, which is guaranteed to converge asymptotically.

In a typical Gibbs sampling setting, our state space for sampling is { W v , X  W v , X  W v } and { H i , X  H i , X  H i } conditioned on the hyperparameters  X  0 and data { X i } n i =1 .How-ever, {  X  W v , X  W v } and {  X  H i , X  H i } are nuisance parameters which can be integrated out to reduce the variance of the Gibbs samples (for better mixing of Markov chain)  X  a scheme which is known as Rao-Bl ackwellized Gibbs Sampling (RBGS).

After integrating out these nuisance parameters, our state space reduces to only the then iteratively samples each row of W v and column of H i conditioned on the observed data and the remaining set of variables in the state space from the previous Gibbs it-eration. Algorithm 1 outlines these sampling steps while the rest of this section shall briefly explain how to obtain the Gibbs conditional distributions as in Eqs (6-9).
Recalling W i and H i from Eq. (3), the conditional distribution over W ( m ) v , condi-tioned on matrices W u for all u  X  S ( n, i ) except v , all the rows of matrix W v except for all i and the hyperparameters  X  0 ,isgivenby Note that the above posterior is proportional to the data-likelihood as a function of W v and H i for each i and the predictive distribution of W over the parameters of the normal X  X nverse X  X ishart posterior distribution and is multi-variate Student X  X  [3]. Assuming  X  l &gt;K v +1 , this predictive density has finite covari-ance and is known to be approximated well by a normal distribution through matching the first two moments [3]. Thus, the predictive distribution is given as  X   X  Using Eqs (4) and (5), the posterior distribution can be written as over the l -th column of matrix H i conditioned on its remaining columns is normally distributed with mean vector and precision matrix as 2.3 Subspace Dimensionality and Complexity Analysis Let the number of rows in X 1 ,..., X n be M and the number of columns be N i ,giving M  X  N i dimension for X i . Since each W i consists of an augmentation of individual and shared subspaces W v ,weuse K v to denote the number of basis vectors in W v . Assuming R i to be the total number of basis vectors in W i ,wehave v  X  S ( n,i ) K v = R . Determining the value of K v is a model selection problem and depends upon the common features among various data sources. According to a heuristic proposed in [8], a rule of thumb is to use K v  X  M v / 2 where M v is the number of common features in sharing configuration v . Following the above notation for the dimensionalities of matrices, for each i  X  X  1 , 2 ,...n } , assuming R i &lt;M (generally the case for real world data), the complexity of sampling H i matrices from its posterior distributions is
O ( M  X  N its posterior distributions is O M  X  N i  X  R 2 i . Thus the computation complexity of BSSL remains similar to BPMF model [9] and does not grow any further. We demonstrate the usefulness of BSSL in two real world applications. (1) Improving social media retrieval in one medium by transferring knowledge from auxiliary me-dia sources. (2) Performing retrieval across multiple media. The first application can be seen as an multi-task learning application, whereas the second application is a direct manifestation of mining from multiple data sources. 3.1 BSSL Based Social Media Retrieval Let the tag-item matrix of the target medium (from which retrieval is to be performed) be denoted as X k . Further, let us assume that we have many other auxiliary media sources which share some features with th e target medium. Let the tag-item matrices of these auxiliary media be denoted by X j , j = k . In a multi-task learning setting, we leverage these auxiliary sources to improve the retrieval precision for the target medium, and given a set of query keywords S Q , a vector q of length M (vocabulary size) is constructed by putting tf-idf values at each index where the vocabulary contains a word from the keywords set or else setting it to zero. Next, we follow Algorithm 2 for BSSL based retrieval. 3.2 BSSL Based Cross-Social Media Retrieval To retrieve items across media, we use the common subspace among them along with the corresponding coefficient matri ces for each medium. As an example, for n =3 (three media sources), we use the common subspace matrix W 123 and coefficient ma-trices H 1 , 123 , H 2 , 123 and H 3 , 123 for first, second and third medium respectively.
Similar to subsection 3.1, we construct a vector q of length M using a set of query keywords S Q . We proceed similar to Algorithm 2 with the following differences. Given q , we wish to retrieve relevant items from each domain, which is performed by project-ing q onto the augmented common subspace matrix ( W 123 for the case when n =3 media sources) to get its representation h in the common subspace. Next, we compute similarity between h and the columns of matrices H 1 , 123 , H 2 , 123 and H 3 , 123 (the rep-resentation of media items in the common subspace spanned by columns of W 123 )to find similar items from medium 1, 2 and 3 resp ectively. The results are ranked based on these similarity scores either individually or jointly.

For both retrieval applications, we use cosine-similarity as it seems to be more robust than Euclidean distance base d similarity measures in high-dimensional spaces. As we are dealing with distributions, we also tried out KL-divergence based similarity mea-sures, but cosine-similarity gives better results.
 4.1 Dataset To conduct the experiments, we created a soc ial media dataset using three different sources : YouTube 1 , Flickr 2 and Blogspot 3 . To obtain the data, we first chose common concepts  X   X  X cademy Awards X ,  X  X ustralian O pen X ,  X  X lympic Games X ,  X  X S Election X   X  and queried all three websites using their service APIs. To have some pairwise sharing, we additionally used the concept  X  X hristmas X  to query Blogspot and Flickr,  X  X error At-tacks X  to query YouTube and Flickr, and  X  X arthquake X  to query Blogspot and YouTube. Lastly, to retain some differences between each medium, we used the concepts  X  X ricket World Cup X ,  X  X oli X  and  X  X lobal Warming X  to query Blogspot.com, Flickr and YouTube respectively. Table 1 provides further details of the dataset size and average tag counts for each data source. The total number of t ags from all three s ources combined is 3740 . 4.2 Subspace Learning and Parameter Setting For clarity, let us denote YouTube, Flickr and Blogspot tag-item matrices as X 1 , X 2 and X 3 respectively. To learn BSSL based factor ization, we use Eqs (6) X (9) to sample W and H matrices. Recalling the notation K v (dimensionality of subspace spanned by W v ), for learning factorization, we set the individual subspace dimensions as K 1 = K 2 = K 3 =10 , pairwise shared subspace dimensions as K 12 = K 23 = K 13 =15 , and the common to all subspace dimension as K 123 =25 . To obtain these param-eters, we first initialize them using the heuristic described in [8] and then do cross- X  X 1 =  X  X 2 =  X  X 3 =2 , hyperparameters  X  0 =[0 ,..., 0]  X  0 = K v for corresponding W v , H i,v . The values of  X  X i depend upon the quality of the tags and a small value implies high tag uncertainty. For the dataset described above, Gibbs sampling usually takes around 50 iterations to converge (convergence plots are omitted due to space restrictions), however, we collect 100 samples to ensure convergence. The first 20 samples are rejected for  X  X urn-in X  and the remaining 80 Gibbs samples are averaged to get an estimate of W v , H i,v matrices. 4.3 Experiment 1: Social Media Retrieval Using Auxiliary Sources To carry out our experiments, we choose YouTube as the target dataset and Blogspot and Flickr as auxiliary datasets. To perform BSSL based retrieval from YouTube, we first generate samples of basis matrix W 1 [ W 1 | W 12 | W 13 | W 123 ] and representation estimate of W 1 and H 1 following Algorithm 2.

To compare the performance with other methods, we choose three baselines. The first baseline performs retrieval by matching the tag-lists of videos without any subspace results are ranked based on the similarity scores. The second baseline is retrieval work based on subspace learning using Principle Component Analysis (PCA). For the third baseline, we use a recent state-of-the-art BPMF model proposed in [9] for Bayesian matrix factorization. For both second and third baselines , we do not use any auxiliary data (e.g. tags of Flickr or Blogspot) and use the tags of YouTube only, but increase the YouTube datasize so as to keep the total datasize equal to the combined data (target + auxiliary) used for the first baseline to make the comparison fair.

To evaluate our retrieval algorithm, we use a query set of 20 concepts defined as Q = { X  X each X ,  X  X merica X ,  X  X omb X ,  X  X nimal X ,  X  X ank X  ,  X  X ovie X ,  X  X iver X ,  X  X able X ,  X  X limate X ,  X  X ed- X  X sunami X  X . Since there is no public groundtruth available, we manually go through the set of retrieved items and evaluate the results.

Figure 2 compares the retrieval perform ance of BSSL with all the baselines in terms of precision-scope (P@N) curve 5 , mean average precision (MAP) and 11-point precision-recall curve [1]. Figure 2 clearly shows that BSSL outperforms the baselines in terms of all three evaluation criteria. Although, BPMF performs better than PCA due to its ability to handle uncertainties well, it can not surpass BSSL as it is confined to the tag data of YouTube only. Intuitively, BSSL is able to utilize the related data from auxiliary sources and resolve the tag ambiguities by reducing the subjectivity and incompleteness of YouTube tags. In essence, the use of multiple sources in subspace learning helps discover improved tag co -occurrences and gives better results. 4.4 Experiment 2: Cross Media Retrieval The effectiveness of BSSL for cross-media retrieval is demonstrated using the YouTube-Flickr-Blogspot dataset with the subspace learning parameters as in subsection 4.2. For evaluation, we again use precision-scope (P@N), mean average precision (MAP) and 11-point interpolated precision-recall curves. Let q be a query term, G i be the ground truth set for the i -th medium and A i be the answer set for query q using a retrieval method for the i -th medium. Then, the precision and recall measures for cross-media retrieval are As far as baselines are concerned, we note that both BPMF and PCA are not applica-ble for cross-media retrieval as they do not support analysis of multiple data sources in their standard form. Therefore, we compare the performance of BSSL against tag-based matching (based on Jaccard coeffici ent without any subspace learning) only. Other works on cross-media retrieval [13,14] use the concept of a Multimedia Doc-ument (MMD), which requires co-occurring multimedia objects on the same webpage which is not available in our case. Therefore, these methods can not be applied directly. Figure 3 depicts the cross-media retrieval results across all three media -Blogspot, Flickr and YouTube. To generate the graphs, we again average the retrieval results over the query set Q defined in subsection 4.3. It can be seen from Figure 3 that BSSL significantly outperforms tag based matching i n terms of all three evaluation criteria. Improvement in terms of MAP criteria is around 13% . This improvement in perfor-mance is due to the learning of shared subs paces which not only handle the problem of  X  X ynonymy X  and  X  X olysemy X  in tag-space, but a lso the uncertainties probabilistically. We have presented a Bayesian framework to learn individual and shared subspaces from multiple data sources (BSSL) and demonstrated its application to social media re-trieval across single and multiple media. Our framework, being based on the principle of Bayesian probabilistic matrix factorization (BPMF) [9], provides an efficient algo-rithm to learn the subspace. Our Gibbs samp ler (RBGS) provides better Markov chain mixing than BPMF without increasing the complexity of the model. Our experiments have demonstrated that BSSL significantly outperforms the baseline methods for both retrieval tasks on Blogspot, Flickr and YouTube datasets. More importantly, our solution provides a generic framework to exploit collective strengths from heterogeneous data sources and we foresee its wider adoption in cross-domain data mining and beyond.
