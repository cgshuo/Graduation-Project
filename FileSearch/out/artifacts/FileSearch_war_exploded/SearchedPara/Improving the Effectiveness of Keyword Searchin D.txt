 The success of keyword queries as a common way of Web search and exploration has spurred much interest in the research community in supporting effective and efficient keyword search in relational databases. It allows information retrieval (IR) from the databases by simply giving a set of keywords, without requiring users to know either query languages (such as SQL) or the database schema. A large body of literature has appeared in this area, which can be broadly classified into two categories: the schema-graph-based approach (e.g., DISCOVER [ 1 ], and SPARK [ 2 ]) and the data-graph-based approach (e.g., BANKS [ 3 ], and Blinks [ 4 ]). based approaches. For a given query consisting of one or more keywords, the schema-graph-based approach first locates the relations that contain the key-words, and for each such relation, generates a tuple set that consist of only the tuples matching a given keyword. It then generates candidate networks ( CN s) where each CN corresponds to a join expression of the tuple sets. The final step is to evaluate all the generated CN s to produce join networks of tuples, which are presented as answers to the users. Recent work along this direction has attempted to improve the effectiveness of the search through better ways of ranking the final results. For example, some authors (e.g., [ 2 , 5 , 6 ]) address the effectiveness issue by leveraging the relevance-ranking strategies which have been proved effective over text data.
 Despite the recent advances in keyword search over databases, very few work explicitly incorporate query feedback into the ranking of query results. For exam-ple, in DISCOVER, the CN generation step simply outputs all the generated CN s in ascending order by the number of joins. User preferences are not explic-itly considered during the whole process. This is also true for other existing keyword search methods. In contrast, a user is more likely to find the answer he/she is interested in if his/her preference (captured through search history, etc.) is taken into account when ranking the results.
 As an example, consider the following scenario. Company XY Z is a whole-sale supplier with geographically distributed warehouses, each of which serves several sales districts. A database D is used to manage the information of the company X  X  products and customers. D consists of seven tables with the fol-lowing schema: warehouse ( warehouseID, ...), district ( districtID, warehouseID, ...), customer ( customerID, districtID, warehouseID, ...), item ( itemID, ...), stock ( itemID, warehouseID, ...), order ( orderID, districtID, warehouseID, cus-tomerID, ...), orderline ( orderID, number, itemID, ...) . We assume that the warehouses are named W 1 ,W 2 ,...,W m and items I 1 ,I 2 of D from different departments of the company would want different information from the database even when they issue the same query. For example, when an employee from the sales department issues a query  X  W i ,I fer retrieving information regarding the sales of I j in warehouse W search results corresponding to the join item orderline order warehouse should be promoted towards the top of the ranked list of results. This preference can be naturally reflected in the query log through past queries issued by her or her colleagues in the sales department. In contrast, an employee from the dis-tribution department, who often checks stock and distributes goods from ware-houses to stores, may prefer the stock information of item I W ( item stock warehouse ) for the same search. Again, this preference can be reflected in the log of past queries.
 In this paper, we introduce a new ranking strategy to adapt the ranking of query results to user preferences. Query logs, which record the queries along with the results chosen by users for each query, are the source of user feedback. We first mine the frequent patterns in the query log. For a given query, we then score all CN s obtained by a standard CN -generating algorithm, such as that from DISCOVER, by a new scoring function that combines the score based on the user query log and the score on the CN size through normalization and weighting. The re-ranked CN s can better reflect user preferences. The above scoring process involves a NP-hard problem, which is proved. To solve the problem, we present a dynamic programming algorithm. The experiments we conducted on the DBLP dataset demonstrate the effectiveness of our strategy.
  X  We propose a novel ranking strategy to re-rank the CN s utilizing frequent  X  We prove the hardness result on the scoring problem, and provide an optimal  X  Extensive experiments and user studies are conducted to evaluate the pro-of related work. Section 3 defines some related basic concepts. Section 4 presents our ranking strategy based on query log and the NP-hard problem. Section 5 reports the experimental results. We conclude this paper in Section 6 . 7  X  10 ]. Existing approaches can be categorized into data graph based and schema graph based. The schema graph based approach [ 1 , 5 , 7 ] executes the querying process by two steps: CN generation and CN evaluation. We take DISCOVER as an example. The schema of the relational database is modeled as a directed graph for which the node represents the corresponding relation and the edge indicates the key-foreign key constraint between two relations. To generate all CN s, a set of join expressions are constructed by breadth-first traversing the tuple set graph expanded from the schema graph. In the evaluation step, a execution plan is generated to evaluate all the generated CN s.
 as pruning condition to avoid generating unnecessary tuple trees and designed a greedy algorithm to produce a near-optimal execution plan. More studies [ 1 , 5 , 11 ] have been done to further improve the query efficiency, and much work has also been done to improve effectiveness such as [ 2 , 6 ]. Liu [ 6 ] identifies the formal-ization of four new factors (tuple tree size formalization, etc.) to improve the ranking formula in [ 5 ]. However, existing work on the effectiveness issue primar-ily focuses on returning results with basic semantics, while few have considered adapting user preferences.
 IR. However, the methods of relevance feedback in IR cannot be directly applied to the context of keyword search in databases becasue in the IR and Web search context, the entities are existing documents, Web pages, etc., where as in the database context, the entities are join networks of tuples, etc., which are dynam-ically assembled based on the query. Moreover, in the database context, user preferences are often reflected in the structure of the result (e.g., which CN this result is based on), which is a non-existent issue in IR and Web search. to help learn how to correctly integrate data. In the literature of keyword search in databases, some previous works have also considered user feedback. Gao and Yu [ 12 ] employ query logs for keyword search, but the purpose is different from ours: the query logs are used to help improve the effectiveness of keyword query cleaning. Peng et al. [ 13 ] aim at better reformulating a user X  X  initial query to retrieve more relevant query results in relational databases by applying user feedback. However, this work is still based on the vector space model and applies the IR-Style ranking for query reformulation without considering the information (i.e., frequent patterns) in the query logs. We first introduce some terms and notations used throughout this paper, and formulate the problem that this paper focuses on.
 We consider a relational database with n relations R 1 ,..., R R has m i attributes a i 1 ,..., a i m i .
 Definition 1. (Labeled Directed Graph). Given a relational database D ,we define the schema graph of D as a Labeled Directed Graph (LDG) G =( V, E ) . Each node v  X  V represents the corresponding relation in D , and each edge e  X  v j ( v i , v j  X  V , e  X  E ) corresponds to a primary-key-foreign-key relationship between the relations represented by v i and v j . We assign unique ids (i.e., label) to all nodes and edges respectively.
 Fig. 1 depicts a sample of five tables from the DBLP biography database [ 14 ]. The tables Paper and Author contain information on papers and researchers respectively; table Conference contains conference information. Table PaperCi-tation stores the citation relationships between papers; and table Write records the m : n relationships between authors and papers. The LDG of the sample DBLP database from Figure 1 is shown in Figure 2 .
 Given a query Q = { k 1 ,..., k m } , where k i is a keyword, we can obtain a set of basic tuple sets R consists of all tuples of relation R i that contain the keyword k tuple sets are processed to produce tuple sets R K j for the non-empty subset K of Q . R K i ,a non-empty tuple set that contains the tuples of R keywords of K and no other keywords, is defined as R K i = K, t contains k  X  X  X  k  X  Q  X  K, t does not contain k } . For example, Paper is the set { P1,P2 } ,and Paper LDA is { P4 } . The database relations that appear in the schema graph is free tuple set denoted as R {} which means that the relation R does not have tuples that contain a keyword. The non-empty tuple sets combine with the schema graph of the database by adding corresponding edges to form the tuple set graph G TS .
 Definition 2. (Candidate Network). A candidate network (CN) is a join network of tuple sets formed by traversing G TS in a breadth-first mode. belongs to the family of free trees -the connected, acyclic and undirected graphs. Figure 3 illustrates the labeled free tree of one CN for the query  X  Markov,LDA  X , which represents the join network Paper Markov P aperCitation P aper Definition 3. (Query Log). Aquerylog L is a set of entries. Each user has his own query log. For a specific user, each entry in his/her user log records the candidate network (in the form of labeled free tree) that the user chose to visit when all the candidate networks were presented to him/her in answering a given query. In short, L contains the chosen results of a user for one or more queries. Definition 4. (Mining Frequent Patterns from Query Logs).
 cific user (or a group of similar users) u , his/her entries in the query log form a set L u where each recorded CN c  X  L u is a labeled free tree. For a given pattern p (a free tree), we say that p occurs in a logged CN c or c supports p if p is isomorphic to a subtree of c . The support of a pattern p is the number of CNs in L u that supports p . A pattern p is said to be frequent if its support, sup ( p ) , is no less than a predefined minimum support minsup . The problem of mining frequent patterns from query logs is to compute, for a given user u andalog L , the set P ( L )= { p | sup ( p )  X  minsup } .
 FreeTreeMiner described in [ 15 ]. FreeTreeMiner , which applies the bottom up Apriori method [ 16 ], first computes all frequent subtrees with 2, 3 and 4 vertices using brute-force method, based on which larger candidate frequent subtrees can be generated by Apriori . Each candidate would be checked if it is really frequent. Iteratively executing the steps above, all frequent subtrees would be generated. Given a keyword query Q over relational database D , the first stage of the schema graph based approach produces the set of CN s. These generated CN s are simply ranked according to their sizes. For example, DISCOVER adopts the following formula for scoring a CN c : By Equation ( 1 ), smaller CN s are ranked before larger ones, and ties are broken arbitrarily. In the second stage, the final results are obtained by evaluat-ing the CN s. Users get query results ranked by the sizes of their corresponding CN s, which could be very different from the users X  real needs. As an example, consider the following case. Suppose that a user issues a query  X  Markov, LDA  X . The CN s (i) Paper Markov Conf erence P aper LDA and (ii) Paper W rite Author W rite P aper LDA are included in the results of the CN generation step. According to Equation (1), CN (i) is ranked higher than (ii) as it has less joins and hence a smaller size. But if the query log contains entries related to this user X  X  past queries, we can take them into consideration when ranking the CN s. For example, we consider an extreme case where there is no pattern Paper Conference and the support of the pattern Author Write Paper is very large. Intuitively, CN (ii) should be ranked higher than (i) as the preference of the user can be clearly inferred from his/her search history. The problem we study in this paper, is how to adapt to user preferences through query logs. This can be reduced to the problem of ranking the candidate networks using information from query logs. In this section, we discuss how to take user feedback information into consider-ation when ranking the CN s. Before presenting the proposed ranking strategy, let us first take a look at a straightforward way of using feedback information for ranking. 4.1 A First Attempt A simple method to incorporate the query log information is to assign, for each user, a degree of preference (e.g., p  X  [0 , 1]) for each table in the database based on frequency of that table appearing in the log. The score of a generated CN for a given query can be computed by a linear combination of the preference degree of each table involved and the size of the CN . However, this does not work in some cases as the score may be dominated by a minority (sometimes even one) of the tables in the CN . For example, for the query  X  Markov, LDA  X , the candidate network Paper Markov W rite Author W rite P aper may be ranked higher than Paper Markov P aperCitation P aper degree of preference for Author is very high for that user. However, although the user has a strong preference for Author , it is very likely that for this particular query the user would prefer a pattern in which one paper cites another. In this example, the high preference degree of a single table Author has dominated the scoring of the CN .
 quent tables may not be frequent. For example, it is possible that two tables, say Paper and Author , are both of high frequency, but the join Paper Write Author may be rare in this log. In this case, any CN with this join as a compo-nent should not be ranked high despite the high frequency of Paper and Author . Intuitively, instead of considering the frequency of single tables, we should focus more on the frequency of those  X  X oin structures X . This leads us to develop the methods described in the sequel. 4.2 Ranking Functions We seek to augment the scoring function in Equation ( 1 ) using the frequent patterns mined from query logs. Let P ( L u ) (or P ( L ) when there is no ambi-guity) denote the set of frequent patterns mined from log L for a given user (or group of users) u .Fora CN c , we define FS ( c ) to be the set of frequent { p | p is a subtree of c  X  p  X  X  ( L ) } . The set of edges in c not covered by FS ( c ), together with their corresponding vertices, constitute another set denoted by NFS ( c ). Naturally, NFC ( c )and FS ( c ) do not have any overlapping edges. cover of c by a combination of elements from FS ( c )and NSF ( c ) such that -There is no overlap between any pair of elements; and -The union of the edges in all of the elements in the combination is equal to Obviously, each edge of c is contained in exactly one element of the combination. We can assign a score to proper partition P c as follows. where CFS denotes the set of frequent subtrees used in the partition and sup ( FS i ) is the support of FS i  X  CFS . The configurable parameter t ( t&gt; 0) is used to control the degree of preference for larger frequent structures, and N (  X  ) is a normalization function to be described next. Notice that elements in NFS ( c ) do not contribute the scoring of the partition.
 frequent pattern mined from the query log ranges from minsup to an unknown large number. If the value of sup ( FS i ) is too big, the score of the CN that con-tains the subtree FS i will become unreasonably large. Therefore, normalization must be done to limit the influence of the support value. In particular, when the support is extremely large, its effect should be dampened even more. Based on the above consideration, we use a sigmoid function as a starting point for nor-malizing the support values, which takes the form of sigmoid ( x )=1 / (1 + e where the weight parameter  X  controls the linearity of the curve. In our case, the range of support is [ minsup ,+  X  ). As minsup is greater than zero, the range of the sigmoid function is (0.5,1). However, the range of Equation ( 1 ) is [0,1]. So, we have to scale the range of the sigmoid function to the range of (0,1) by the transformation N ( x )=2  X  ( sigmoid ( x )  X  0 . 5).
 Note that there may exist many proper partitions for a candidate network. Let
P c denote the set of all proper partitions of c , each of which has a corre-sponding score computed by Equation ( 2 ). The largest such score is used as the query log score , as indicated in Equation ( 4 ). The rule comes from the intuition that we always want to get a partition in which the frequent subtrees have both larger support and larger size. But in fact, smaller patterns in supports. Thus, a trade-off is needed. So, a candidate network is assigned the largest combination score as the log score. N is the set of all combination scores.
Finally, we combine the original size-based score and the query log score by weighting as in Equation ( 5 ), where  X  is the weight of the original score and controls the relative importance between the two parts. If  X  = 1, then the new scoring function is the same as the one used in DISCOVER.

To illustrate the advantage of this new scoring function, consider the example we give in Section 3 . For the query {  X  Markov , LDA  X  } Paper Markov Conf erence P aper LDA and (ii) Paper Markov Author W rite P aper LDA are assigned with an score respectively by our ranking functions. As P(L) has no the pattern Paper Conference while the pattern Author Write Paper has a large support, S ( i ) &lt;S ( ii ). Apparently, by incorporating the query log, the generated candidate networks can be ordered emerging user preferences.
 Algorithm 1 summarizes the procedure to score and rank all generated can-didate networks of a given query using the new scoring function. Since there can exist a large number of proper partitions for a given CN , the most complex and time consuming part of this procedure is to compute the largest score of a c over all such proper partitions. 4.3 Complexity Result We show that to find the largest score corresponding to the best proper partition of a candidate network c basedonthesets FS ( c )and NFS ( c ) is a NP-hard problem.
 We first introduce the notations that will be used. Let  X  be the set of all edges in the CN c ; S = FS ( c )  X  NFS ( c ). We can represent a subtree with the corresponding subset of  X  . Each element S  X  S has a score Score ( S ). Algorithm 1. Scoring generated candidate networks Definition 5. (Best Proper Partition Problem). With  X  and the best proper partition problem is to find a set S  X   X  S ment of  X  appears in only one element of S  X  and S  X  maximizes Score ( lows. -Instance: Given a set of elements  X  , and a set of subsets of  X  , -Question: Find a set S  X   X  S , which ensures that each element of  X  appears the best proper partition decision problem , can be formulated as follows. -Instance: Given a set of elements  X  , and a set of subsets of  X  , -Question: Is there a set S  X   X  S such that each element of  X  just in one and Theorem 1. The best proper partition problem is NP-hard.
 Complete. We can apply the restriction technique which shows the NP-Completeness of an NP problem by stating that a special case of the problem is NP-Complete. By limiting B = min { w i | S i  X  X   X  } , the decision problem can be restricted to the exact cover problem , a problem known to be NP-Complete. Then, the decision problem is proved to be NP-Complete. Hence, the best proper partition problem is NP-hard. 4.4 A Dynamic Programming Solution We show that the best proper partition problem can be solved by dynamic pro-gramming. Assuming that the elements in S are numbered, we define a set of indicator variables x i for a given set S X  S such that x i S appears in S ,and x i = 0 otherwise. Then an indicator vector ( x can be formed.
 respect to S subject to the constraints: (i) S i S j =  X  , if x i =1 ,x (ii) ( 3 ). The constraint (i) ensures that for a partition no pairs of subtrees share the same edges, and (ii) makes sure that a partition can cover all the edges in  X  . The exists a recursive structure that allows us to use dynamic program to solve the optimization problem.
 Theorem 2. Define F  X  ( S ) as the maximum score for the CN c given the set S X  S . Then the optimal solution is given by F  X  ( S ) = max Score ( S i ) } .
 Based on Theorem 2 , we propose a dynamic programming algorithm to com-pute the optimal partition and its corresponding score. Algorithm 2 performs the dynamic programming and outputs the optimal combination score. The time complexity of the dynamic programming algorithm is O (2 | S | Algorithm 2. Dynamic Programming Algorithm We conduct experiments to evaluate the proposed dynamic programming algo-rithm ( Dynamic ) and compare it with existing approaches that do not consider user feedback. 5.1 Dataset and Settings Due to the lack of publicly available databases with query logs, we use the DBLP database 1 in our experiments and build our own query log through a controlled user study. The DBMS used is MySQL with default configurations. We build indexes for all primary keys and foreign keys. Full-text indexes are built for all textual attributes. The experiments are conducted on a workstation with two 2.33GHz Intel Core2 Duo processors and 2GB of main memory.
 research areas participate in our experiment as query initiators. They formu-lated 60  X  X eaningful X  queries consisting of varying number of keywords related to their research areas. For each participant, we applied 3-fold cross-validation on his queries. The queries were randomly divided into three sets. In each trial, two folds were used as the training set to generate the query log and the other was testing set. For each query in the training set, with the predefined parameters (such as T max ), all of the generated CN s, the number of which may range from tens to hundreds , were presented to the corresponding participant in sequence, in ascending order of size; the participant was asked to choose  X  X es X  or  X  X o X  for each CN according to whether that CN meets his/her requirement. The  X  X es X  CN s were recorded in the query log. Up to here, each participant had his own query log in each trail. We set minsup =10.By the settings above, each partic-ipant has more than 200 frequent subtrees on average and the corresponding supports range from 10 to about 300. For each query in testing set, all the gen-erated CN s were ranked by our methods and presented to the participant. The participant assessed the result quality using a six-point scale ranging from 0 to 5 (5= X  X erfect X  and 0= X  X ad X ).
 mance of them by DBLP database. The parameters that involve in the exper-iments are illustrates in Table 1 with explanation. And Table 2 shows some sample queries from a user. A user is required to issue queries that are reason-able. Generally, a query contains at least two keywords and no more than four. 5.2 Effectiveness To measure the effectiveness, we adopt four metrics, namely, Normalized Dis-counted Cumulative Gain (NDCG), Precision at K (P@K), 11-point Preci-sion/Recall and F-measure . Each is described in detail as follows.  X  NDCG at K: For a given query q in the testing set, the ranked candidate  X  Precision at K: P@K shows the fraction of the candidate networks ranked in top K results that are preferred by the user. In our settings, we define that a candidate network assessed with 3 or larger is preferred. The position of preferred candidate networks within top K is unconcerned. As the most intuitive metric, P@K measures the overall user satisfaction with the top K results.  X  11-point Precision/Recall: For a query result, this metric reports the precision that is measured at the 11 recall levels of 0.0, 0.1, 0.2, ... , 1.0.
In our experiments, 11-pt Precision/Recall is the average result for all the testing queries.  X  F-measure: Given K , we can compute the precision and the recall at K .
Here, the candidate networks with 3 or larger points are preferred. F-measure is the harmonic mean of precision and recall.
 Effect of Parameter t,  X  and  X  of Dynamic . Set T max =7and K =10.
 Table 3 shows the experiment results. Each line in Table 3 indicates that when fix the value of t and then vary  X  and  X  , the best setting of  X  and  X  and the corresponding result are presented (we do not show other settings here which only generate worse results). And we can conclude that when t (preference to larger frequent subtrees) is 4, NDCG@10 reaches the best value 0.890 with  X  = 0 . 01 and  X  =0 . 1.
 Effectiveness Comparison for DISCOVER, Dynamic . We use DIS-COVER as a baseline here as its scoring function directly corresponds to the  X  X riginal score X  part of the proposed new strategy. Fig. 4 shows that incorporat-ing query log results in significant improvements over DISCOVER. In Fig. 4(a) ( T max = 7), when K increases, Dynamic always outperforms DISCOVER by a considerable margin. This can also be seen in Fig. 4(b) which shows the effect of K on Precision. Now, we vary T max as Fig. 4(c) ( K =10). The gap between our proposed algorithms and DISCOVER remains similar; meanwhile, each of them progresses with a downward trend when T max increases.
 Overall Effectiveness Comparison for DISCOVER, Dynamic . Figure 5(a) ( T max = 7) shows the 11-points precision/recall graph for DISCOVER, Dynamic , in which the precision goes down with recall growing. In the global perspective, Dynamic behaves well with points (0.1,0.988), ... , (0.9,0.230). DIS-COVER takes the worst performance. Meanwhile, as an auxiliary, Figure 5(b) ( T max = 7) presents the F-measure value by varying K . The three preserve some differences as in the previous case.
 Existing work on keyword search in databases has considered the problem of improving the search effectiveness extensively. However, few work has explicitly taken user preferences into consideration when ranking the query results. In this paper, by introducing user feedback to the problem of ranking candidate net-works, we have proposed a new ranking strategy to adapt to user preferences. As this new ranking strategy involves a NP-hard problem, we provide the Dynamic Programming algorithm. We have evaluated the proposed strategy by the DBLP dataset through a user study, which verifies the effectiveness of our strategy.
