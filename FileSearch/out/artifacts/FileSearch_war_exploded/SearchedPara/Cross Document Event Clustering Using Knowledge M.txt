 News, wh i ch i s an i mportant i nformat i on source, i s reported anyt i me and anywhere, and i s d i ssem i nated across geograph i c barr i ers through Internet . Detect i ng the occur-rences of new events and track i ng the processes of the events (Allan, Carbonell, and Yamron, 2002 ; Ch i eu and Lee, 2004) are useful for dec i s i on-mak i ng i n th i s fast-chang i ng network era . The research i ssues beh i nd event cluster i ng i nclude: how many features are used to determ i ne event clusters, wh i ch cue patterns are employed to relate news stor i es i n the same event, how the cluster i ng strateg i es affect the cluster-i ng performance us i ng retrospect i ve data or on-l i ne data, how the t i me factor affects cluster i ng performance, and how cross-document co-references are resolved .

Several stud i es, for example, text class i f i cat i on (Kolcz et al . , 2001) and web-page class i f i cat i on (Shen et al . , 2004), suggest that even s i mple summar i es are qu i te effec-t i ve i n carry i ng over the relevant i nformat i on about a document . They showed that i f ma i n top i c and i mportant content . Moreover, for deeper document understand i ng, the co-reference cha i ns (Card i e and Wagstaff, 1999) of documents capture i nformat i on on prov i des i mportant clues to f i nd text fragments conta i n i ng sal i ent i nformat i on, var i ous pract i cal tasks i nclud i ng, for example, text summar i zat i on (Azzam, Humphreys and Ga i zauskas, 1999 ; Chen, et al . , 2003), quest i on answer i ng (Morton, 1999 ; L i n, et al . , 2001) and event cluster i ng (Kuo and Chen, 2004), can be done more rel i ably . On the other hand, dur i ng produc i ng summar i es from mult i ple documents, cross-document co-reference analyses (Bagga and Baldw i n, 1998 ; Goo i and Allan, 2004) cons i der further i f ment i ons of a name i n d i fferent documents are the same .

In th i s paper, we w i ll show that us i ng summar i zat i on as pre-process i ng i n event cluster i ng i s a v i able and effect i ve techn i que . Furthermore, we w i ll i ntegrate co-reference cha i ns from more than one document by un i fy i ng cross-document co-references of nom i nal elements . Instead of us i ng the trad i t i onal cluster i ng approaches, we w i ll propose a novel threshold model that i ncorporates t i me decay funct i on and spann i ng w i ndow to deal w i th on-l i ne stream i ng news . The rest of the paper i s organ-i zed as follows . Sect i on 2 rev i ews the prev i ous work and shows our arch i tecture . Sect i on 3 descr i bes a document summar i zat i on algor i thm us i ng co-reference cha i ns . Sect i on 4 tackles the i ssues of m i n i ng controlled vocabulary . A normal i zed cha i n ed i t d i stance and two algor i thms are proposed to m i ne controlled vocabulary i ncremen-tally from cross-document co-reference cha i ns . Sect i on 5 proposes an algor i thm for set and the exper i mental results, us i ng the metr i c adopted by Top i c Detect i on and Track i ng (F i scus and Dodd i ngton, 2002) . Sect i on 7 concludes the remarks . Kuo and Chen (2004) employed co-reference cha i ns to cluster stream i ng news i nto event clusters . As the co-reference cha i ns and event words are complementary i n some sense, they also i ntroduced the event words (Fukumoto and Suzuk i , 2000) . The i r exper i mental results showed that both the two factors are useful . Furthermore, they presented two approaches to comb i ne the two factors for event cluster i ng, wh i ch are called summation model and two-level model , respect i vely . However, the best performance i n terms of detect i on cost was i mproved 2% only compared to the base-l i ne system . One of the reasons i s that the nom i nal elements used i n cross-document co-reference cha i ns may be d i fferent . The goal of th i s paper i s to m i ne controlled vocabulary from co-reference cha i ns of d i fferent documents i ncrementally for event cluster i ng on stream i ng news .

F i gure 1 shows the arch i tecture of event cluster i ng . We rece i ve documents from mult i ple Internet sources, such as newspaper s i tes, and then send them for document pre-process i ng . Pre-process i ng module deals w i th the sentence extract i on, the lan-guage i d i osyncracy, e . g . , Ch i nese segmentat i on, and co-reference resolut i on . Docu-ment summar i zat i on module analyzes each document and employs the co-reference cha i ns and the related feature words, such as event words, to produce the respect i ve summar i es . M i n i ng controlled vocabulary module i ntegrates the co-reference cha i ns to generate controlled vocabulary automat i cally, wh i ch w i ll be used for we i ght com-putat i on i n event cluster i ng module . F i nally, event cluster i ng module ut i l i zes we i ghts of word features, and s i m i lar i ty funct i on to cluster the documents .
 Kuo and Chen (2004) cons i dered the event words only to be features for cluster i ng . The bas i c hypothes i s i s that an event word assoc i ated w i th a news art i cle appears across paragraphs, but a top i c word does not . Take Ch i na A i rl i nes a i r acc i dent as an example . Each related news art i cle has d i fferent event words, e . g . ,  X  body recovery  X  , negot i at i on  X  , and so on . Extract i ng such keywords i s useful to understand the events, and d i st i ngu i sh one document from another . Nevertheless, due to the str i ct dec i s i on thresholds there are only a few event words extracted and may lose some i mportant feature words . Thus, th i s paper further i ntroduces the h i gher tf-idf words to be our document features . Document summar i zat i on module extracts the event words and the 20 h i ghest tf-i df words from each document . Then, the score of each sentence i n a document i s computed by add i ng three scores, i. e . , the number of event words, the number of the h i ghest tf-i df words, and the co-reference scores shown i n the follow-i ng paragraphs . Rather than us i ng f i xed number of sentences to generate summary, the sentence select i on procedure i s repeated unt i l a dynam i c number of sentences i s document . For example, let the compress i on rate and total sentences be 0 . 35 and 15, respect i vely . In th i s case, the length of summary i s 5, i. e . , 0 . 35 *15 .

Co-reference score of a sentence i s computed as follows . Headl i nes of a news story can be regarded as i ts short summary . That i s, the words i n the headl i ne repre-sent the content of a document i n some sense . The co-reference cha i ns that are i n i t i -ated by the words i n the headl i nes are assumed to have h i gher we i ghts . A sentence wh i ch conta i ns any words i n a g i ven co-reference cha i n i s sa i d to  X  cover  X  that cha i n . Those sentences wh i ch cover more co-reference cha i ns conta i n more i nformat i on, and should be selected to represent a document . F i ve scores shown below are computed sequent i ally to break the t i e dur i ng sentence select i on . Score 1 only cons i ders nom i nal features . Comparat i vely, Score 2 cons i ders both nom i nal and verbal features together . Both scores are i n i t i ated by headl i nes . Scores 3 and 4 cons i der all the co-reference cha i ns no matter whether these cha i ns are i n i t i ated by headl i nes or not . These two scores rank those sentences of the same scores 1 and 2 . Bes i des, they can ass i gn scores to news stor i es w i thout headl i nes . Scores 1 and 3 are recomputed i n the i terat i on . F i nally, s i nce news stor i es tend to conta i n more i nforma-t i on i n the lead i ng paragraphs, Score 5 determ i nes wh i ch sentence w i ll be selected accord i ng to pos i t i on of sentences, when sentences are of the same scores (1)-(4) . The smaller the pos i t i on number of a sentence i s, the more i t w i ll be preferred . Stream i ng news are d i ssem i nated from d i fferent sources and wr i tten by d i fferent con-vent i ons and styles . The express i on of an ent i ty i n a document may be d i fferent from the express i on of the same ent i ty i n another document . F i gure 2 shows an example of four short co-reference cha i ns i n four d i fferent documents DOC1-DOC4 .
 Cons i der i ng the co-reference cha i n i n DOC1,  X   X  (Pres i dent George W . Bush) and  X   X  (Pres i dent Bush) denote the same person . There are two i dent i cal words  X   X  (Pres i dent) and  X   X  (Pres i dent Bush) between the cha i ns i n DOC1 and DOC2, so that word match i ng tells us these two cha i ns have the same denotat i on . However, d i rect word match i ng between two co-reference cha i ns may suffer from the follow i ng two problems .
 (1) Because stream i ng news i s d i ssem i nated from d i fferent sources anyt i me, the arr i val sequence of documents w i ll affect the qual i ty of controlled vocabulary . For example, when DOC3 arr i ves before DOC2, the two cha i ns i n DOC1 and DOC3 w i ll denote two d i fferent named ent i t i es due to no word match i ng between the two co-reference cha i ns . In th i s case, a resolut i on algor i thm may m i ss some correct cross-document co-references . (2) Because there are two match i ng words  X   X  (Pres i dent) and  X   X  (he) be-tween the co-reference cha i ns DOC2 and DOC4, they may be m i s-regarded as the same person i n sp i te of d i fferent person ent i t i es, i. e . ,  X   X  (Pres i dent Bush) and  X   X  (Pres i dent Cl i nton) . In th i s case, a resolut i on algor i thm may pro-duce i ncorrect cross-document controlled vocabulary . 4.1 Normal i zed Cha i n Ed i t D i stance Instead of us i ng word match i ng, the concept of normal i zed cha i n ed i t d i stance i s pro-posed . The ed i t d i stance of two str i ngs, s1 and s2, i s def i ned as the m i n i mum number str i ngs s1 to s2 . Cons i der an example . Let str i ngs s1 and s2 be def i ned as AAABB and BBAAA, respect i vely . The ed i t d i stance between s1 and s2, called str i ngs are . Here, the ed i t d i stance i s extended to determ i ne whether two g i ven co-reference cha i ns are s i m i lar or not . Assume there are two co-reference cha i ns  X  say, Given and Incoming . Algor i thm 1 computes the cha i n ed i t d i stance of Incoming and Given co-reference cha i ns . If the score i s smaller than a predef i ned threshold, the Incoming co-reference cha i n denotes the same ent i ty as the Given co-reference cha i n, and w i ll be merged i nto Given cha i n i n Algor i thm 1 . Otherw i se, they are regarded as d i fferent ent i t i es .
 Cons i der the sample co-reference cha i ns shown i n F i gure 2 . Assume DOC1 and DOC2 are Given and Incoming co-reference cha i ns, respect i vely . The normal i zed cha i n ed i t d i stance between these two co-reference cha i ns i s (0+0+1+1+1/3+0+0+ Incoming cha i n i n DOC4 i s (3/5+0+1+3/5)/4=0 . 55 . Let the threshold value be 0 . 45 . The two co-reference cha i ns i n DOC1 and DOC2 are deemed the same ent i ty . Mean-wh i le, the co-reference cha i n i n DOC4 denotes a d i fferent ent i ty from that i n DOC1 . On the other hand, although there i s no match i ng word between the cha i ns i n DOC1 and DOC3, the i r normal i zed cha i n ed i t d i stance i s low enough, i. e . , (3/7+1/3)/2=0 . 38 (&lt;0 . 45) . Thus, these two cha i ns can also be deemed to denote the same ent i ty . In summary, the above two i ssues can be tackled i n Algor i thm 1 .
 Algor i thm 1. Compute the normalized chain edit distance of Incoming and Given co-reference chains 1 . Let len1 and len2 be the length ( i. e . , number of words) of Incoming and Given co-reference cha i ns, respect i vely . 2 . Let word1[i] and word2[j] be the ith and the jth elements i n Incoming and Given co-reference cha i ns, respect i vely . 3 . In i t i al i ze score to be 0 . 4 . for i = 1 to len1 { m i n =  X  ; length(word2[ j ])) socre += m i n ; } 5 . Compute score = score / len1 and output the score .

Pronouns (e . g . ,  X   X  (he)) and t i tle words (e . g . ,  X   X  (Pres i dent)) are less spec i f i c i n a co-reference cha i n, so that they contr i bute less i nformat i on and are prone to i ncur errors i n creat i ng cross-document controlled vocabullary . DOC2 and DOC4 show an example . ((3/5+0+0+3/5)/4=0 . 30 &lt; 0 . 45) In such a case, an alternat i ve solut i on may be: pronouns and t i tle words are excluded from cross document co-reference cha i ns dur i ng m i n i ng controlled vocabulary . 4.2 Creat i ng Controlled Vocabulary cross document temporal references i nto controlled vocabulary . Thus, we i gnored the temporal references i n our approach . Algor i thm 2 spec i f i es how to m i ne controlled vocabulary i ncrementally . F i gure 3 shows some examples i n controlled vocabulary . The term i n bold font i s a header (canon i c form) of a un i f i ed co-reference cha i n . (a) (Ch i na A i rl i nes) , , , , Ch i na A i rl i nes, (b) (Peng-Hu) , , , (c) (Sea around Peng-Hu) , , , , (d) (The Execut i ve Yan) , , (e) (The M i n i stry of Transportat i on and Commun i cat i ons) , (f) (C i v i l Aeronaut i cs Adm i n i strat i on M i n i stry of Trans-Algor i thm 2 . M i n i ng Controlled Vocabulary 1 . Set the threshold value to be  X  . 2 . Get the f i rst news document and the accompany i ng co-reference cha i ns . 3 . In i t i al i ze the controlled vocabulary to be the co-reference cha i ns . 4 . Get the next news document and i ts co-reference cha i ns unt i l all are processed . 4.3 Evaluat i on We adopted the B-CUBED metr i c (Bagga and Baldw i n, 1998) shown below to meas-ure the prec i s i on and recall of the created controlled vocabulary . ent i t i es . Bes i des the d i rect evaluat i on, we also employed the created controlled vo-cabulary to the event cluster i ng system proposed i n Sect i on 5 to evaluate the perform-ance i nd i rectly . 4.3.1 Data Set In our exper i ment, we used the knowledge base prov i ded by the Un i ted Da i ly News (http://udndata . com/), wh i ch has collected 6,270,000 Ch i nese news art i cles from 6 Ta i wan local newspaper compan i es s i nce 1975/1/1 . To prepare a test corpus, we f i rst set the top i c to be  X   X  (A i r Acc i dent of Ch i na A i rl i nes), and the range of search i ng date from 2002/5/26 to 2002/9/4 (stopp i ng all rescue act i v i t i es) . Total 964 related news art i cles, wh i ch have publ i shed date, news source, headl i ne and content, respect i vely, are returned from search eng i ne . All are i n SGML format . After read i ng those news art i cles, we deleted 5 news art i cles wh i ch have headl i nes but w i thout any content . The average length of a news art i cle i s 15 . 6 sentences . Bes i des, all the above art i cles have been manually tagged w i th co-reference cha i ns . Furthermore, we asked three research ass i stants to merge the related co-reference cha i ns i nto controlled vo-cabulary separately, and then we used ma j or i ty rule to create the gold answer . 4.3.2 Exper i mental Results Pronouns and t i tle words occur frequently i n co-reference cha i ns . To ver i fy i f they have s i gn i f i cant d i scr i m i nat i on among cha i ns, two alternat i ves are exper i mented . M1 used the or i g i nal co-reference cha i ns to create controlled vacabulary . In contrast, M2 excludes pronouns and t i tle words i n co-reference cha i ns . The related F-scores are shown i n F i gure 4 . The threshold i s the  X  value i n Algor i thm 2 . The basel i ne system uses the word match i ng only . Normal i zed cha i n ed i t d i stance i s super i or to word match i ng no matter wh i ch k i nds of co-reference cha i ns are adopted . The exper i mental results also ver i fy that pronouns and t i tle words i n a co-reference cha i n contr i bute l i ttle i nformat i on no matther word match i ng or ed i t d i stance approaches are employed . When the approach of ed i t d i stance us i ng M2 w i th threshold 0 . 33 i s adopted, the best performance, i. e . , prec i s i on 96 . 49%, recall 96 . 67%, and F-score 96 . 58%, i s ach i eved .

Analyz i ng the created controlled vocabullary us i ng M2, we found that there are three ma j or types of errors shown below . (1) Amb i guous abbrev i at i on problem, e . g . ,  X   X  (Macau) and  X   X  (Austral i a) (2) Lack of semant i c i nformat i on, e . g . ,  X   X  (southern area) and  X   X  (3) Word order problems, e . g . ,  X   X  (Remason Typhoon) can not be A s i ngle-pass complete l i nk cluster i ng algor i thm i ncrementally d i v i des the documents i nto several event clusters . In i t i ally, the f i rst document d 1 i s ass i gned to cluster t 1 , and the co-reference cha i ns of d 1 form i n i t i al controlled vocabulary (refer to Steps 2-3 of Algor i thm 2) . Assume there already are k clusters when a new art i cle d i i s cons i dered . one of k clusters, or i t may form a new cluster t k +1 . That i s determ i ned by the s i m i lar-i ty measure def i ned below .

At f i rst, we m i ne new controlled vocabulary from current controlled vocabulary and the i ncom i ng news story . The procedure refers to Step 4 of Algor i thm 2 . Then we compute the s i m i lar i t i es of the summary of the i ncom i ng news story w i th each sum-mary i n a cluster . The newly-m i ned controlled vocabulary i s global to each s i m i lar i ty computat i on . Let V 1 and V 2 be the vectors for the two summar i es extracted from documents D 1 and D 2 . Event cluster i ng module used the headers of the m i ned con-trolled vocabulary to replace the related terms i n the process i ng summary . Each term i s represented as a vector of normal i zed TF-IDF we i ghts shown as follows . where tf ij i s frequency of term t j i n summary i , The s i m i lar i ty between V 1 and V 2 i s computed as follows . 130 J . -J . Kuo and H . -H . Chen If all the s i m i lar i t i es are larger than a f i xed threshold, the news story i s ass i gned to the cluster . Otherw i se, i t forms a new cluster i tself .
 appear i ng further apart . Thus, i nstead of us i ng a f i xed detect i on threshold for com-par i son strategy, a dynam i c detect i on threshold us i ng t i me decay funct i on and span-n i ng w i ndows i s proposed as follows . A dynam i c detect i on threshold ( d_th ) i s i ntro-duced, where th i s an i n i t i al threshold . In other words, the earl i er the documents are put i n a cluster, the smaller the i r thresholds are . Assume the publ i cat i on day of docu-ment D2 i s later than that of document D1 . where dist (denoted as day d i stance) denotes the number of days away from the day at wh i ch the event happens, and w_size (denoted as w i ndow s i ze) keeps the threshold unchanged w i th i n the same w i ndow . 6.1 Data Set t i fy th i rteen focus events, e . g . , rescue status . Meanwh i le, two annotators are asked to read all the 959 news art i cles and class i fy these art i cles i nto 13 events . If a news art i -reports more than one event may be class i f i ed i nto more than one event cluster . We compare the class i f i cat i on results of annotators and cons i der those cons i stent results between Ta i wan and Hong Kong (20), Cause of a i r acc i dent (57), Conf i rmat i on of a i r acc i dent (6), Influence on stock market (27), Influence on i nsurance fee (11), Influ-ence on Ch i na A i rl i nes (8), Influence on Peng-Hu arch i pelagoes (26), Pun i shment for persons i n charge (10), News report i ng (18), Wreckage found (28), Rema i ns found (57), Rescue status (65), Solat i um (34) and others (664), respect i vely . The number i n the parentheses i s the document number . 6.2 Evaluat i on Metr i c We also adopt the metr i c used i n Top i c Detect i on and Track i ng (F i scus and Dodd i ng-ton, 2002) . The evaluat i on i s based on m i ss and false alarm rates . Both m i ss and false alarm are penalt i es . They can measure more accurately the behav i or of users who try w i th the cluster i ng results . The performance i s character i zed by a detect i on cost, C det , i n terms of the probab i l i ty of m i ss and false alarm: Cross Document Event Cluster i ng Us i ng Knowledge M i n i ng from Co-reference Cha i ns 131 where C Miss and C FA are costs of a m i ss and a false alarm, respect i vely, P Miss and P FA (=1-P target ) are the pr i or target probab i l i t i es . Manmatha, Feng and Allan (2002) i nd i -cated that the standard TDT cost funct i on used for all evaluat i ons i n TDT i s 6.3 Exper i mental Results For compar i son, the centro i d-based approach and s i ngle pass cluster i ng i s regarded as a basel i ne model . Convent i onal tf -idf scheme selects 20 features for each i ncom i ng news art i cles and each cluster uses 30 features to be i ts centro i d . Whenever an art i cle i s ass i gned to a cluster, the 30 words of the h i gher tf -idf s are regarded as the new centro i d of that cluster . On the other hand, we used the algor i thm descr i bed i n Sec-t i on 3 to study the effects of document summar i zat i on us i ng co-reference cha i ns, wh i ch selected four sentences to represent the correspond i ng document . The i r ex-per i mental results w i th var i ous thresholds are shown i n Table 1 . The best results are 0.012990 and 0.013137 , respect i vely when the threshold i s set to 0 . 05 .

Although the performance us i ng document summar i zat i on i s lower than the cen-tro i d approach, we further study the effects of dynam i c thresholds descr i bed i n Sec-t i on 5 . Table 2 shows the results us i ng var i ous w i ndow s i zes . The best detect i on cost, best exper i mental results us i ng the summat i on model (Kuo and Chen, 2004) are also shown i n Table 2 .
 Coref + Event words 0 . 112233 0.011603 0 . 013109 0 . 013109 press i on rates are adopted . The exper i mental results are shown i n Table 3 . The detec-t i on cost us i ng compress i on rate 0 . 35 i s 0.011496 , wh i ch i s better than the above basel i ne system, i. e . , a f i xed length summary (&lt; 0 . 011603) . We conclude that the flex i ble length summary conveys more i nformat i on than the f i xed length . 132 J . -J . Kuo and H . -H . Chen Compress i on Rate 0 . 25 0 . 3 0.35 0 . 4 0 . 45 
Bes i des summary length i ssue, we used the summar i zat i on module i n Sect i on 3 to select sentences under d i fferent w i ndow s i zes . Here, compress i on rate i s 0 . 35 and threshold value i s 0 . 04 . The exper i mental results are shown i n Table 4 . The case of the approach of us i ng the 20 h i ghest tf-idf words can select more i nformat i ve sen-tences i n document summar i zat i on .

F i nally, we i ntroduce controlled vocabulary m i ned from co-reference cha i ns i n-crementally . Table 5 and Table 6 show the exper i mental results w i thout and w i th remov i ng dupl i cat i ons i n the co-reference cha i ns, respect i vely . It i s out of our expec-tat i on that the detect i on cost w i thout dupl i cat i on removal i s better than that w i th du-pl i cat i on removal . It seems that occurrences play i mportant roles to some degree . That i s, the more occurrences i n a co-reference cha i ns a word has, the more i mportant i t i s . In the last exper i ments, we kept the occurrences of top i c elements except pronouns and t i tle words, and m i ned controlled vocabulary from the result i ng cha i ns . As the qual i ty of the controlled vocabulary i s i mproved, the exper i ments show that the per-formance of the model i s further i mproved to 0.010915 . Compar i ng w i th the best detect i on costs of the basel i ne system ( 0.012990 ) and the summat i on model ( 0.011604 ), the best results have 16% and 6% performance ga i n, respect i vely .
 Controlled vocabulary s i ze 2143 2030 2021 1785 
Controlled vocabulary s i ze 2023 1959 1896 1719 Th i s paper proposes a normal i zed cha i n ed i t d i stance to m i ne controlled vocabulary from cross-document co-reference cha i ns i ncrementally, and ut i l i zes the results to un i form the features used i n event cluster i ng on stream i ng news . T i me decay funct i on and spann i ng w i ndow capture the spec i f i c character i st i cs of on-l i ne news . The ex-Cross Document Event Cluster i ng Us i ng Knowledge M i n i ng from Co-reference Cha i ns 133 per i ments show that occurrences of d i scr i m i nat i ve elements i n a cha i n are useful, and pronouns as well as t i tle words may i ntroduce errors . The best model demonstrates 16% and 6% i mprovement compared to the basel i ne and the summat i on model, re-or suff i x, or word senses, to i mprove the performance of controlled vocabulary con-struct i on . Bes i des, we w i ll also extend the results to construct mult i l i ngual controlled vocabulary for mult i l i ngual event cluster i ng .
 Research of th i s paper was part i ally supported by Nat i onal Sc i ence Counc i l, Ta i wan, under the contracts NSC93-2752-E-001-001-PAE and NSC94-2752-E001-001-PAE .
 1 . Allan, James ; Carbonell, Ja i me and Yamron, Jonathan (Eds) (2002) Top i c Detect i on and 2 . Azzam, Sal i ha ; Humphreys, Kev i n ; and Ga i zauskas, Robert (1999)  X  Us i ng Coreference 3 . Bagga, Am i t and Baldw i n, Breck (1998)  X  Ent i ty-Based Cross-Document Coreferenc i ng 4 . Card i e, Cla i re and Wagstaff, K i r i (1999)  X  Noun Phrase Co-reference as Cluster i ng,  X  Pro-6 . Ch i eu, Ha i Leong and Lee, Yoong Keok (2004)  X  Query Based Event Extract i on along a 8 . Fukumoto, F . and Suzuk i , Y . (2000)  X  Event Track i ng based on Doma i n Dependency,  X  9 . Goo i , Chung Heong and Allan, James (2004)  X  Cross-Document Coreference on a Large 134 J . -J . Kuo and H . -H . Chen 13 . Manmatha, R .; Feng, A . and Allan, James (2002)  X  A Cr i t i cal Exam i nat i on of TDT X  X  Cost 14 . MUC (1998) Proceed i ngs of 7th Message Understand i ng Conference, Fa i rfax, VA, 29 
