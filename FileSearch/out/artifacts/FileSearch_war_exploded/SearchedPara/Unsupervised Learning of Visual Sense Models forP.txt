 We address the problem of unsupervised learning of object cl assifiers for visually polysemous words. Visual polysemy means that a word has several dictionary sen ses that are visually distinct. Web images are a rich and free resource compared to traditional h uman-labeled object datasets. Potential training data for arbitrary objects can be easily obtained f rom image search engines like Yahoo or Google. The drawback is that multiple word meanings often le ad to mixed results, especially for polysemous words. For example, the query  X  X ouse X  returns mu ltiple senses on the first page of results:  X  X omputer X  mouse,  X  X nimal X  mouse, and  X  X ickey Mou se X  (see Figure 1.) The dataset thus obtained suffers from low precision of any particular visua l sense.
 Some existing approaches attempt to filter out unrelated ima ges, but do not directly address poly-semy. One approach involves bootstrapping object classifie rs from labeled image data [9], others cluster the unlabeled images into coherent components [6], [2]. However, most rely on a labeled seed set of inlier-sense images to initialize bootstrapping or t o select the right cluster. The unsupervised approach of [12] bootstraps an SVM from the top-ranked image s returned by a search engine, with the assumption that they have higher precision for the categ ory. However, for polysemous words, the top-ranked results are likely to include several senses .
 We propose a fully unsupervised method that specifically tak es word sense into account. The only input to our algorithm is a list of words (such as all English n ouns, for example) and their dictionary entries. Our method is multimodal, using both web search ima ges and the text surrounding them in the document in which they are embedded. The key idea is to l earn a text model of the word sense, using an electronic dictionary such as Wordnet toget her with a large amount of unlabeled text. The model is then used to retrieve images of a specific se nse from the mixed-sense search results. One application is an image search filter that autom atically groups results by word sense for easier navigation for the user. However, our main focus in th is paper is on using the re-ranked images Figure 1: Which sense of  X  X ouse X ? Mixed-sense images returned from an image keyword search. as training data for an object classifier. The resulting clas sifier can predict not only the English word that best describes an input image, but also the correct sens e of that word.
 A human operator can often refine the search by using more sens e-specific queries, for example,  X  X omputer mouse X  instead of  X  X ouse X . We explore a simple met hod that does this automatically by generating sense-specific search terms from entries in Wo rdnet (see Section 2.3). However, this method must rely on one-to three-word combinations and is therefore brittle. Many of the generated search terms are too unnatural to retrieve any res ults, e.g.,  X  X ercoid bass X . Some retrieve many unrelated images, such as the term  X  X icker X  used as an al ternative to  X  X atch X . We regard this method as a baseline to our main approach, which overcomes th ese issues by learning a model of each sense from a large amount of text obtained by searching t he web. Web text is more natural and is a closer match to the text surronding web images than di ctionary entries, which allows us to learn more robust models. Each dictionary sense is represen ted in the latent space of hidden  X  X opics X  learned empirically for the polysemous word.
 To evaluate our algorithm, we collect a dataset by searching the Yahoo Search engine for five poly-semous words:  X  X ass X ,  X  X ace X ,  X  X ouse X ,  X  X peaker X  and  X  X atc h X . Each of these words has anywhere from three to thirteen noun senses. Experimental evaluatio n on this dataset includes both retrieval and classification of unseen images into specific visual sens es. The inspiration for our method comes from the fact that text s urrounding web images indexed by a polysemous keyword can be a rich source of information about the sense of that word. The main idea is to learn a probabilistic model of each sense, as define d by entries in a dictionary (in our case, Wordnet), from a large amount of unlabeled text. The use of a d ictionary is key because it frees us from needing a labeled set of images to learn the visual sense model.
 for nouns. Like standard word sense disambiguation (WSD) met hods, we make a one-sense-per-document assumption [14], and rely on words co-occurring wi th the image in the HTML document to indicate that sense. Our method consists of three steps: 1 ) discovering latent dimensions in text associated with a keyword, 2) learning probabilistic model s of dictionary senses in that latent space, and 3) using the text-based sense models to construct sense-specific image classifiers. We will now describe each step in detail. 2.1 Latent Text Space Unlike words in text commonly used in WSD, image links are not g uaranteed to be surrounded by grammatical prose. This makes it difficult to extract struct ured features such as part-of-speech tags. We therefore take a bag-of-words approach, using all availa ble words near the image link to evaluate the probability of the sense. The first idea is to use a large co llection of such bags-of-words to learn coherent dimensions which align with different senses or us es of the word. We could use one of several existing techniques to discover l atent dimensions in documents consist-ing of bags-of-words. We choose to use Latent Dirichlet Allo cation, or LDA, as introduced by Blei et. al.[4]. LDA discovers hidden topics, i.e. distribution s over discrete observations (such as words), in the data. Each document is modeled as a mixture of topics z  X  X  1 , ..., K } . A given collection of
M documents, each containing a bag of N d words, is assumed to be generated by the follow-ing process: First, we sample the parameters  X  j of a multinomial distribution over words from a Dirichlet prior with parameter  X  for each topic j = 1 , ..., K . Then, for each document d , we sample the parameters  X   X  . Finally, for each word token i , we choose a topic z word w Our initial approach was to learn hidden topics using LDA dir ectly on the words surrounding the images. However, while the resulting topics were often alig ned along sense boundaries, the approach suffered from over-fitting, due to the irregular quality and low quantity of the data. Often, the only clue to the image X  X  sense is a short text fragment, such as  X  X is hing with friends X  for an image returned for the query  X  X ass X . To allieviate the overfitting problem, we instead create an additional dataset of text-only web pages returned from regular web search. We the n learn an LDA model on this dataset and use the resulting distributions to train a model of the di ctionary senses, described next. 2.2 Dictionary Sense Model We use the limited text available in the Wordnet entries to re late dictionary sense to topics formed above. For example, sense 1 of  X  X ass X  contains the definition  X  X he lowest part of the musical range. X  first-level hypernyms (e.g.,  X  X ound property X ). We denote t he bag-of-words extracted from such a dictionary entry for sense s as e The model is trained as follows: Given a query word with sense s  X  X  1 , 2 , ...S } we define the likelihood of a particular sense given the topic j as or the average likelihood of words in the definition. For a web image with an associated text docu-ment d = w The above requires the distribution of LDA topics in the text context, P ( z | d ) , which we compute by marginalizing across words and using Bayes X  rule: particular dictionary sense given the image to be equal to P ( s | d ) . Thus, our model is able to assign sense probabilities to images returned from the search engi ne, which in turn allows us to group the images according to sense. 2.3 Visual Sense Model The last step of our algorithm uses the sense model learned in the first two steps to generate training data for an image-based classifier. The choice of classifier i s not a crucial part of the algorithm. We choose to use a discriminative classifier, in particular, a s upport vector machine (SVM), because of its ability to generalize well in high-dimentional spaces w ithout requiring a lot of training data. Table 1: Dataset Description: sizes of the three datasets, and distribution of ground trut h sense labels in the keyword dataset. For each particular sense s , the model re-ranks the images according to the probability of that sense, and selects the N highest-ranked examples as positive training data for the S VM. The negative train-ing data is drawn from a  X  X ackground X  class, which in our case is the union of all other objects that we are asked to classify. We represent images as histograms o f visual words, which are obtained by detecting local interest points and vector-quantizing the ir descriptors using a fixed visual vocabulary. We compare our model with a simple baseline method that attem pts to refine the search by automat-ically generating search terms from the dictionary entry. E xperimentally, it was found that queries consisting of more than about three terms returned very few i mages. Consequently, the terms are generated by appending the polysemous word to its synonyms a nd first-level hypernyms. For exam-ple, sense 4 of  X  X ouse X  has synonym  X  X omputer mouse X  and hype rnym  X  X lectronic device X , which produces the terms  X  X omputer mouse X  and  X  X ouse electronic d evice X . An SVM classifier is then trained on the returned images. To train and evaluate the outlined algorithms, we use three d atasets: image search results using the given keyword, image search results using sense-specific se arch terms, and text search results using the given keyword.
 and downloading the returned images and HTML web pages. The k eywords used were:  X  X ass X , term, as in  X  X ass guitar X ;  X  X ace X  has a multitude of meanings , as in  X  X uman face X ,  X  X nimal face X ,  X  X ountain face X , etc.;  X  X peaker X  can refer to audio speaker s or human speakers;  X  X atch X  can mean a timepiece, the act of watching, as in  X  X urricane watch X , or the action, as in  X  X atch out! X  Samples that had dead page links and/or corrupted images were remove d from the dataset.
 The images were labeled by a human annotator with one sense pe r keyword. The annotator labeled in computer mouse,  X  X peaker X  as in an audio output device, an d  X  X atch X  as in a timepiece. The annotator saw only the images, and not the text or the diction ary definitions. The labels used were 0 : unrelated , 1 : partial , or 2 : good . Images where the object was too small or occluded were labeled partial . For evaluation, we used only good labels as positive, and grouped partial and unrelated images into the negative class. The labels were only used in t esting, and not in training. The second image search dataset was collected in a similar ma nner but using the generated sense-original keywords. Neither of these two datasets were label ed. Table 1 shows the size of the datasets and distribution of labels. When extracting words from web pages, all HTML tags are remove d, and the remaining text is tokenized. A standard stop-word list of common English word s, plus a few domain-specific words like  X  X pg X , is applied, followed by a Porter stemmer [11]. Wo rds that appear only once and the actual word used as the query are pruned. To extract text context wor ds for an image, the image link is located automatically in the corresponding HTML page. All w ord tokens in a 100-token window surrounding the location of the image link are extracted. Th e text vocabulary size used for the sense model ranges between 12K-20K words for different keywords.
 To extract image features, all images are resized to 300 pixe ls in width and converted to grayscale. Two types of local feature points are detected in the image: e dge features [6] and scale-invariant salient points. In our experiments, we found that using both types of points boosts classficiation performance relative to using just one type. To detect edge p oints, we first perform Canny edge detection, and then sample a fixed number of points along the e dges from a distribution proportional to edge strength. The scales of the local regions around poin ts are sampled uniformly from the range of 10-50 pixels. To detect scale-invariant salient points, we use the Harris-Laplace [10] detector with the lowest strength threshold set to 10. Altogether, 40 0 edge points and approximately the same number of Harris-Laplace points are detected per image . A 128-dimensional SIFT descriptor is used to describe the patch surrounding each interest poin t. After extracting a bag of interest point descriptors for each image, vector quantization is perform ed. A codebook of size 800 is constructed by k-means clustering a randomly chosen subset of the databa se (300 images per keyword), and all images are converted to histograms over the resulting vi sual words. To be precise, the  X  X isual words X  are the cluster centers (codewords) of the codebook. No spatial information is included in the image representation, but rather it is treated as a bag-o f-words. 5.1 Re-ranking Image Search Results In the first set of experiments, we evaluate how well our text-based sense model can distinguish between images depicting the correct visual sense and all th e other senses. We train a separate LDA model for each keyword on the text-only dataset, setting the number of topics K to 8 in each case. Although this number is roughly equal to the average number o f senses for the given keywords, we do not expect nor require each topic to align with one particu lar sense. In fact, multiple topics can represent the same sense. Rather, we treat K as the dimensionality of the latent space that the model uses to represent senses. While our intuition is that it shoul d be on the order of the number of senses, it can also be set automatically by cross-validation. In our initial experiments, different values of K did not significantly alter the results.
 To perform inference in LDA, a number of approximate inferen ce algorithms can be applied. We use a Gibbs sampling approach of [7], implemented in the Matl ab Topic Modeling Toolbox [13]. We used symmetric Dirichlet priors with scalar hyperparame ters  X  = 50 /K and  X  = 0 . 01 , which have the effect of smoothing the empirical topic distributi on, and 1000 iterations of Gibbs sampling. The LDA model provides us with topic distributions P ( w | z ) and P ( z ) . We complete training the model by computing P ( s | z ) for each sense s in Wordnet, as in Equation 2. We train a separate model for each keyword. We then compute P ( s | d ) for all text contexts d associated with images in the keyword dataset, using Equation 3, and rank the correspondi ng images according to the probability of each sense. Since we only have ground truth labels for a sin gle sense per keyword (see Section 3), we evaluate the retrieval performance for that particul ar ground truth sense. Figure 2 shows the resulting ROCs for each keyword, computed by thresholdi ng P ( s | d ) . For example, the first plot shows ROCs obtained by the eight models corresponding t o each of the senses of the keyword  X  X ass X . The thick blue curve is the ROC obtained by the origin al Yahoo retrieval order. The other thick curves show the dictionary sense models that correspo nd to the ground truth sense (a fish). The results demonstrate that we are able to learn a useful sense m odel that retrieves far more positive-class images than the original search engine order. This is i mportant in order for the first step of our method to be able to improve the precision of training dat a used in the second step. Note that, for some keywords, there are multiple dictionary definition s that are difficult to distinguish visually, for example,  X  X uman face X  and  X  X acial expression X . In our ev aluation, we did not make such fine-grained distinctions, but simply chose the sense that appli ed most generally.
 In interactive applications, the human user can specify the intended sense of the word by providing an extra keyword, such as by saying or typing  X  X ass fish X . The c orrect dictionary sense can then be selected by evaluating the probability of the extra keyword under each sense model, and choosing the highest-scoring one. Figure 2: Retrieval of the ground truth sense from keyword search results. Thick blue lines are the ROCs for the original Yahoo search ranks. Other thick lines a re the ROCs obtained by our dictionary model for the true senses, and thin lines are the ROCs obtaine d for the other senses. 5.2 Classifying Unseen Images The goal of the second set of experiments is to evaluate the di ctionary-based object classifier. We train a classifier for the object corresponding to the ground -truth sense of each polysemous keyword in our data. The clasifiers are binary, assigning a positive l abel to the correct sense and a negative label to incorrect senses and all other objects. The top N unl abeled images ranked by the sense model are selected as positive training images. The unlabeled poo l used in our model consists of both the keyword and the sense-term datasets. N negative images are c hosen at random from positive data for all other keywords. A binary SVM with an RBF kernel is trai ned on the image features, with the C and  X  parameters chosen by four-fold cross-validation. The base line search-terms algorithm that we compare against is trained on a random sample of N images fr om the sense-term dataset. Recall Figure 3: Classification accuracy for the search-terms baseline (terms) and our dictionary mo del (dict). that this dataset was collected by simply searching with wor d combinations extracted from the target sense definition. Training on the first N images returned by Ya hoo did not qualitatively change the results.
 We evaluate the method on two test cases. In the first case, the negative class consists of only the ground-truth senses of the other objects. We refer to this as the 1-SENSE test set. In the second detection of  X  X omputer mouse X  among other keyword objects a s well as  X  X nimal mouse X ,  X  X ickey Mouse X  and other senses returned by the search, including un related images. We refer to this as the MIX-SENSE test set. Figure 3 compares the classification acc uracy of our classifier to the baseline search-terms classifier. Average accuracy across ten trial s with different random splits into train and test sets is shown for each object. Figure 3(a) shows results on 1-SENSE and 3(b) on MIX-SENSE, with N equal to 250. Figure 3(c) shows 1-SENSE results averag ed over the categories, at different numbers of training images N . In both test cases, our dictionary method significantly imp roves on the baseline algorithm. As the per-object results show, w e do much better for three of the five objects, and comparably for the other two. One explanation w hy we do not see a large improvement in the latter cases is that the automatically generated sens e-specific search terms happened to return relatively high-precision images. However, in the other th ree cases, the term generation fails while our model is still able to capture the dictionary sense. A complete review of WSD work is beyond the scope of the present paper. Yarowsky [14] proposed an unsupervised WSD method, and suggested the use of dictiona ry definitions as an initial seed. Several approaches to building object models using image se arch results have been proposed, al-though none have specifically addressed polysemous words. F ei-Fei et. al. [9] bootstrap object classifiers from existing labeled image data. Fergus et. al. [6] cluster in the image domain and use a small validation set to select a single positive compon ent. Schroff et. al. [12] incorporate text features (such as whether the keyword appears in the URL ) and use them re-rank the images before training the image model. However, the text ranker is category-independent and does not learn which words are predictive of a specific sense. Berg et. al. [2] discover topics using LDA in the text domain, and then use them to cluster the images. However, the ir method requires manual intervention by the user to sort the topics into positive and negative for each category. The combina-tion of image and text features is used in some web retrieval m ethods (e.g. [5]), however, our work is focused not on instance-based image retrieval, but on category-level modeling.
 A related problem is modeling images annotated with words, s uch as the caption  X  X ky, airplane X , biguate word senses in such loosely labeled data. Models of a nnotated images assume that there is a correspondence between each image region and a word in th e caption (e.g. Corr-LDA, [3]). Such models predict words, which serve as category labels, b ased on image content. In contrast, our model predicts a category label based on all of the words i n the web image X  X  text context. In general, a text context word does not necessarily have a corr esponding visual region, and vice versa. In work closely related to Corr-LDA, a People-LDA [8] model i s used to guide topic formation in news photos and captions, using a specialized face recogniz er. The caption data is less constrained than annotations, including non-category words, but still far more constrained than text contexts. We introduced a model that uses a dictionary and text context s of web images to disambiguate image senses. To the best of our knowledge, it is the first use of a dic tionary in either web-based image retrieval or classifier learning. Our approach harnesses th e large amount of unlabeled text available through keyword search on the web in conjunction with the dic tionary entries to learn a generative model of sense. Our sense model is purely unsupervised, and i s appropriate for web images. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions. The definition text is used to learn a distribution over the empirical text topics that best represents the sense. As a final step, a discriminat ive classifier is trained on the re-ranked mixed-sense images that can predict the correct sense for no vel images.
 We evaluated our model on a large dataset of over 10,000 image s consisting of search results for five polysemous words. Experiments included retrieval of th e ground truth sense and classifica-tion of unseen images. On the retrieval task, our dictionary model improved on the baseline search engine precision by re-ranking the images according to sens e probability. On the classification task, our method outperformed a baseline method that attemp ts to refine the search by generating sense-specific search terms from Wordnet entries. Classific ation also improved when the test objects included the other senses of the keyword, making distinctio ns such as  X  X oudspeaker X  vs.  X  X nvited speaker X . Of course, we would not expect the dictionary sens es to always produce accurate vi-sual models, as many senses do not refer to objects (e.g.  X  X as s voice X ). Future work will include annotating the data with more senses to further explore the  X  visualness X  of some of them.
