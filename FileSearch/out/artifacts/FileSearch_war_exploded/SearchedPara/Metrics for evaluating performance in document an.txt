 ORIGINAL PAPER AnaCostaeSilva Abstract Is an algorithm with high precision and recall at identifying table-parts also good at locating tables? Several document analysis tasks require merging or splitting certain document elements to form others. The suitability of the com-monly used precision and recall for such division/aggregation tasks is arguable, since their underlying assumption is that the granularity of the items at input is the same as at output. We propose a new pair of evaluation metrics that better suit document analysis X  needs and show their application to sev-eral table tasks. In the process, we present a number of robust table location algorithms with which we draw a road-map for creating Hidden Markov Models for the task.
 Keywords Performance evaluation  X  Document analysis  X  Table processing  X  Metrics 1 Introduction 1.1 Diversity as a comparative evaluation inhibiter Some of the difficulties that prevent us from adequately com-paring experimental results by different authors in document analysis derive from a set of difficulties which we group under the heading Diversity :  X  Diverse goals. Document analysis methods suggested  X  Diverse input types. Take for instance the location task,  X  Diverse input domains. Domains exist where tables fol- X  Diverse concepts. In particular, in the table case, there  X  Diverse output granularity. In the particular case of  X  Diverse evaluation moments. There is no consensual  X  Diverse evaluation metrics. The non-existence of gener-1.2 A survey of performance metrics used in table tasks Recall, precision and their geometric mean (F-measure) have been common metrics for evaluating document and table processes:  X  In table location, they have been taken at different levels  X  In table segmentation, Hurst [ 11 ] proposes two metrics.  X  In table functional analysis, Hurst [ 10 ] measures preci-Apart from precision and recall, to evaluate location, Hu et al. [ 8 ] and Cesarini et al. X  X  [ 3 ] measure the similarity of the recognised document and its ground-truth in terms of their table structure. Both metrics are positively correlated with the total area that is correctly detected as table and negatively cor-related with the total area of non-tables incorrectly detected. The main difference between Hu et al. X  X  [ 8 ] t X  X val and Cesarini et al. X  X  [ 3 ] table evaluation index is that the first asso-ciates different costs to different possible errors -splitting, merging, missing and false detection, while in the latter, all error types weigh the same. These metrics are interesting but may be improper outside a location context.

To evaluate location and segmentation results, Kboubi et al. [ 12 ] take the proportion of cases where eleven error types occurred, these error types referring to split-ting or fusion of cells, lines, columns, tables and table non-detection. Apart from that, they measure the proportion of tables correctly processed. In segmentation, Long [ 14 ]also has very verbose metrics, defining seven error types at six different levels of granularity.

On the other hand, as we saw above, one of the dif-ficulties in evaluating table-related tasks is that there are table-like portions of documents that different users might ground-truth differently. Hu et al. [ 9 ] propose an evaluation model that is resistant to this difficulty. The output of their table analysis method is a graph model of the table. In par-allel, a ground-truth graph model is manually created. Three classes of questions are then asked to both graphs and the percentage of agreement is measured: 1. the first class of questions evaluates the quality of the 2. the second class evaluates the functional model with 3. the third set aims at the structural model and mimics Wang [ 25 ] combines three measures similar to Hu et al. X  X  [ 9 ] by weighing them by the costs of the errors made in each class of questions. Hurst [ 11 ] calls this sort of approach functional , in contrast to the more common absolute approaches. One potential problem with functional approaches is that different performances may be reached with different questions. 1.3 On the inadequacy of precision and recall As we saw in the previous section, P&amp;R and their geometric mean (F-measure) have been extremely common metrics in table tasks. The suitability of precision and recall as absolute performance measurements in aggregation and division is questionable. Originally developed for information retrieval and later adapted for classification purposes, Precision (P) is the proportion of elements correctly identified w.r.t. the number of identified elements and Recall (R) uses the same numerator w.r.t. the number of target elements. The under-lying assumption of both metrics is that the granularity of items at output is the same as at input.

However, for many tasks, this assumption is not valid and document analysis holds many examples X  X lipping images from documents, locating objects in images, joining lines into paragraphs, dividing strings into cells or aggregating cells into columns. In fact, several document analysis methods take as input a given document and subdivide it into its con-stituent parts, which are then tagged in a computer searchable way [ 3 , 25 ]. This can be viewed as a division task in a top-down view, or as an aggregation task in a bottom-up view. Table location and segmentation can be viewed as aggre-gation or division tasks also X  X e join characters into cells, columns, rows and tables in a bottom-up approach or divide pages into their constituent parts in a top-down approach.
In this type of aggregation (division) tasks, elements are merged (split) to form others, so inherently the granularity is different at input and output. Because the underlying assump-tion of precision and recall is that the granularity of the items returned is the same as the granularity of the items originally available, authors must choose between measuring them in relation to input or output units. This forces a uniformity between input and output granularities that is unnatural and that becomes uninformative of the type of choices the algo-rithm represents, eventually to the point of being misleading of its performance as we shall see. We need a measure of our ability to, from an input granularity, generate the granularity required at output, i.e. our ability to change input into output. 2 Completeness and purity We believe that, in any aggregation (division) tasks, two main types of error can occur: either different elements are merged together, so the detected element is impure , or one element gets split in two or more parts, so the detected elements are incomplete . Based on these concepts, we propose the follow-ing absolute metrics [ 20 ]:  X  Completeness , proportion of completely identified ele- X  Purity , proportion of purely detected elements with These are absolute measurements that can be applied to any task that requires a transformation of the input, such as clip-ping images from documents, clipping objects from images, joining lines into tables, dividing strings into cells, or aggre-gating cells into columns. More importantly, because they directly reflect the two basic types of mistake that can occur in division and aggregation tasks, our metrics are informative of the operational choices underlying an algorithm and even suggestive of how it can be improved and user-tuned.
We note that our metrics are correlated with [ 17 ]sixmet-rics, which were created having in mind the aggregation of pixels into different document structures. However, ours are simpler to compute and more self-explanatory, in that two metrics with direct trade-off between each other can be used instead of six.

Finally, there is trade-off between these metrics, since high completeness can be obtained at the cost of purity. For exam-ple, in aggregating table cells to form columns or rows, if we want total purity, we can place each cell into a differ-ent column or row; inversely, for total completeness, we can attribute all cells to the same column or row, thus reaching 0% purity. This trade-off is in fact one of the characteristics that makes precision and recall so appreciated as it facili-tates comparison between different algorithms: as we shall see below, a production possibility curve can be defined with the combinations of maximum purity or completeness that different algorithms attain X  X t can then by set against a user X  X  utility functions to determine his/her favoured algorithm.
To illustrate our metrics, we will use the example in Fig. 1 , first presented in [ 11 ]. The goal was to join cells into columns, which is an aggregation task. Of the 16 rows and 6 columns of the real table, Hurst X  X  algorithm identified all rows but 9 columns. It was able to completely and purely identify all 16 rows and the first and fifth columns. The remaining 7 detected columns did not have all the cells that belong to the corre-spondent five real columns. At proto-link level, in each of columns 2 to 5, two extra proto-links were added while two were lost. As such, of the 115 real proto-links, the algorithm detected 118, 113 correctly.

In Table 1 , we show the resulting evaluation. We note that completeness and recall have the same value because purity is 100% (completeness X  X  numerator is the number of com-plete columns, while recall X  X  is the number of complete and pure columns). As is apparent, with P&amp;R taken at either col-umn/row or proto-link level, we do not know which sort of problems occurred. Furthermore, with proto-link evaluation being taken at too low granularity, the errors made in some crucial connections get diluted among the many bi-direc-tional inter-cell connections that each table holds, which can cause a rather incorrectly recognised table to still present high performance. On the other hand, with C&amp;P, not only can we see the results of the table analysis were less than perfect, but we also know this occurred because splitting errors were common. This knowledge not only clearly informs the user what to expect from the algorithm, but it also points the devel-oper in the right direction as to how to improve it.
In Sect. 3 , we shall see practical applications of C&amp;P to different tasks of the table problem. After presenting five alternative table location algorithms in Sect. 3.1 , we see, in Sect. 3.1.6 , whether relative C&amp;P results are statistically significant. In Sect. 3.1.7 , we present some approaches for choosing the favourite combinations of C&amp;P for users with different preferences. In Sect. 3.2 , we use C&amp;P for table segmentation. 3 Application to real cases In this section, we will see the practical application of our per-formance metrics in different table-related tasks. To anchor our experiments, we gathered initially 22 PDF financial state-ments. This context is particularly demanding, as Tupaj et al. [ 23 ] agree: financial tables tend to be very varied, having unequally filled-in columns or rows and complicated multi-line headers, an algorithm doing well in this context is likely to do better in more  X  X ell behaved X  tables, e.g. in scientific journals and governmental statistics tables. Our documents have lengths varying between 13 and 235 pages with very diverse page layouts; they contain between 3 and 162 tables. In Fig. 2 , we show some of our PDF pages.
 Notice that, although the original form of our data was PDF, we ultimately aim at creating a method for extracting information from tables from any unstructured data-source. Therefore, we converted the PDF to ASCII, thus generating noisier data (which any good table analysis method should be able to cope with), and manually ground-truth X  X d all the documents, delimiting their table lines, cells and columns. We then randomly chose 19 documents for training and 3 for testing (we call this test set 1). Later, we collected another 19 documents, which we call test set 2; we ground-truth X  X d it only for the location task. Table 2 shows the datasets X  dimensions along an array of levels and how they compare against datasets used by other authors. Worthy of note, though not included in Table 2 , is Cafarella et al. X  X  [ 2 ], who cre-ated the first large repository of HTML tables, with 154 mil-lion tables. These however consist of non-marked up HTML tables detected by Wang and Hu [ 26 ] algorithm, which is naturally subject to errors. 3.1 Completeness and purity in location Seen top-down, location can be characterised as cropping tables from documents, i.e. a division problem; alternatively, in a bottom-up view, it can be seen as aggregating different lines (cells, characters or pixels) into tables. A pure table will be one with only lines of one original table; a com-plete table will be one that includes all the lines of one orig-inal table. Four sorts of problems can occur in this task, as Hu et al. [ 8 ] and Cesarini et al. [ 3 ] pointed out: a table can be entirely missed , split into more than one structure (i.e. par-tially missed), merged with other structures or false tables can be detected. Completeness reflects issues of missed and split tables, while purity reflects merging and false tables. For this task, we have applied seven different algorithms. Some lines in our dataset have different horizontally adjacent tables. Table 3 shows their detailed performance evaluation. 3.1.1 Algorithm A We applied Silva et al. X  X  [ 19 ] algorithm, which uses a two-step approach for locating tables, common in other interdepen-dent classification problems [ 13 ]. Silva X  X  algorithm begins by using descriptive statistics of the distribution of white space in each document to calculate a document-specific threshold for pre-classifying each line as being in a table or not. After-wards, in pages with a high enough percentage such candidate table lines, if n are close, lines in their neighbourhood will be classified as table-parts simply if they are narrow or have any interior white space. Each individual table area is assigned an ID.

As can be seen in Table 3 , the algorithm detected 963 table areas. At line level, which is the input granularity, it reached a recall of 94% and a precision of 69% in the test set (non-table lines were detected even better); but, at table level, recall and precision were both only 9%. So, taken two by two, both pairs of metrics are highly uninformative of the way in lines got grouped into tables and the issues therein and, in fact, presenting just a line-based evaluation would shade impor-tant flaws of the algorithm. On the other hand, completeness was 81% and purity 17%. As is apparent, with only two met-rics, we encompass a clear view of the algorithm X  X  pros and cons. 3.1.2 Algorithm B We constructed a decision tree using SAS Enterprise Miner, which aims at minimising entropy and holds three pruning parameters [ 7 ]. Our vision for how to create a decision tree is to characterise each observation using as many features as possible so as to allow the algorithm to pick eventually sur-prising features as the most relevant (rather than limiting it by using a small choice of features, result of subjective ulti-mately biased personal preferences). In fact, we believe this is a potentially important pre-step to other models as well. Our features correspond to combining 35 descriptive statis-tics, quantiles and hypothesis testing metrics (all that are cal-culated by SAS X  X  proc univariate command) on 5 variables (inner spaces, content length, indentation, whether the line is a table candidate, and how far the line X  X  number of spaces is from the threshold defined in Model A) over 20 dimen-sions (all document*, all page*, neighbourhoods of between 1 and 6 surrounding lines, plus individual characteristics of 12 lines around* (6 above and 6 below)); the dimensions sig-nalled with asterisk are analysed according only to the first three variables. As such, taking the training dataset, we char-acterised each line on 1,296 features. We present the pruned decision tree in Table 4 ; we used Test Set 1 to choose the optimal tree depth.

It is interesting to see how, to decide on each line, the tree prefers features that slowly zoom into it . In fact, the first level of the tree is based on the behaviour of X in a neighbourhood of 4 lines; the next two levels use the sum and mean of X over a neighbourhood of 2 lines; finally, X in the line itself and the line X  X  position on the page matter. This suggests that a uniformly sized neighbourhood (used by Ng et al. [ 15 ]in table location and Hurst [ 10 ] in functional analysis) is not the best strategy for this problem. Another important aspect is that  X  X ummaries X  of the behaviour of variables within the neighbourhoods were chosen, rather than the specificities of any one neighbouring lines.

Applying this classifier led to detecting 1,134 table areas, i.e. there are many issues with split and missed tables. In fact, although purity increases relative to algorithm A, we can see that completeness drops. Split tables were now more com-mon, because even if the tree classifier takes into account the characteristics each line X  X  neighbourhood, it is blind to the final classification its neighbours received. This is why Pinto et al. [ 16 ] found that Conditional Random Fields (CRF) and other algorithms that take explicitly into account line inter-dependence perform well. 3.1.3 Algorithm C In C, by using the decision tree described for Algorithm B to derive table candidates and then applying the same table consolidation heuristic as in Algorithm A, we were able to raise completeness to its highest levels. In terms of total cost, this is the favoured algorithm, both in test as in training. 3.1.4 Algorithms D In D, we use a Hidden Markov Model (HMM) to group of table candidates into tables; we investigate into how HMMs can best be built in document analysis. Three options were tested, each being different from the previous in one aspect only, so as to evaluate the contribution of that aspect. Model D0 is a simple HMM. HMMs typically use P ( X i | Z i ) in their formulation; however, by noting that P ( X i | Z i ) is proportional to P ( Z i | X i )/ P ( Z i ) when P ( X i ) is observed, we simply used the probabilities of the decision tree in Table 4 straight into the HMM [ 1 ]. Model D1 perfects P ( Z i | X i ) by incorporat-ing the accuracy rates of parsers for titles, text columns and other known document structures. Model D2 uses an adapt-able transition table, to accommodate the fact that sequen-tial document elements that are physically more distant are more likely to belong to different document structures. The results of each model show how using HMMs, incorpo-rating in it information from different document structure recognisers, and using adaptable transition tables are good strategies when creating a table finder: in fact, both C&amp;P simultaneously improve from model to model. 3.1.5 Algorithm E To compensate for some discrepancies between HMMs and table-specific needs, some heuristics had to be applied. To the results of these, we applied a technique we term flag-ging, which consists of creating a classifier to distinguish the cases where an algorithm is likely to make mistakes [ 22 ]. This allowed us to identify and exclude perceived false detec-tions (which in our dataset are those where over 96% of cells share the same content type, numeric or textual). Model E includes these improvements. This algorithm brought purity to unprecedented levels, 60%, with a completeness of 68% in the test set. Table-based F-measure was 37% and line-based 87%, which again shows how P&amp;R can be misleading. 3.1.6 Statistical significance of results Without conducting statistical tests, it is not possible to know whether the observed differences in results can be extended beyond the test set or are, instead, merely incidental. Dem X ar [ 6 ] justifies why commonly used statistical tests are inappro-priate for result comparison. He instead suggests comparing a group of algorithms on more than three independent data-sets and applying Friedman X  X  test to verify whether there are statistically significant differences between the C&amp;P of all algorithms put together. Then, we can use the Nemenyi test to verify which pairs of algorithms are responsible for these differences. Dem X ar [ 6 ] explains these tests in detail.
We apply them to the results of our table location algo-rithms. We here considered each of the 19 documents in test set 2 to be an independent dataset itself. Applying Friedman X  X  test, we prove that C&amp;P are significantly different among the seven algorithms. Nemenyi X  X  test further identifies that method E has better purity than all models until D1 and is equivalent to D2 (with 10% significance level) and that mod-els A and C are worse than all the rest. On the other hand, model E had more completeness than model B, D0 and D1, being similar to the remainder; models A and C also have better completeness than all others except E. As such, model E is better than all others in either completeness or purity, not being significantly worse than any other: it is the rational choice for any user. However, for the purposes of presenting the methodology in the following subsection, let us pretend this is not the case. 3.1.7 Choosing between competing algorithms Algorithms that present either higher completeness or higher purity than all others are considered efficient. A choice between efficient algorithms is always individual, i.e. it depends on users X  personal preferences. In this subsection, we shall present different ways of doing this with C&amp;P. Different methodologies can lead to different choices.
To begin with, we can however summarise C&amp;P further while accommodating users X  preferences. For example, we can calculate the total cost that each algorithm represents, where each type of mistake is weighed according to user preferences: Total cost = where i is each of the models under comparison (in Table 3 , i  X  {A,B,C,D,E}), j is each of the possible error types (e.g. j {incompletely detected, missed, impure, false detec-tion}), Error i , j is the total number of errors achieved by model i on error type j and UnitCost i is the relative cost a user attributes to each error type j . Under the heading  X  X ost X  in Table 3 , we present our personal view of the relative costs for our problem: we deem completeness to be preferred because it is typically more difficult to recover from in later table processing steps. Total cost is at its smallest for algorithm C.
We can also calculate the harmonic mean of completeness and purity (CPF) for a user X, that likes C&amp;P equally, and a user Y, that prefers completeness n times more than purity: From the unit costs presented in Table 3 , we estimate n to be, on average, equal to 6. CPF is highest for both users in algorithm E.
 In the remainder of this subsection, we shall apply a meth-odology that is common in Economics (the science of opti-mising resource allocation to competing needs) for finding the optimal algorithm for a user with a given set of prefer-ences. If we represent all algorithms available for a given task in a scatter plot that has C&amp;P as axes and then connect the efficient algorithms to each other, we obtain a line that rep-resents the maximum combinations of C&amp;P attainable with the resources available. This is called a Production Possibility Curve (PPC).

The degree by which one user prefers completeness to purity can be represented in the same chart using Utility Curves (UCs) [ 24 ]. A utility curve joins together all the com-binations of C&amp;P that satisfy a user equally; UCs closer to the top right corner of the plot, i.e. farther from the origin, represent a higher level of utility for the user, leaving him more satisfied; the UCs of a user who is consistent in his preferences never intersect.

In Fig. 3 , we estimate the PPC from the test set 2. Only algorithms C and E are efficient, since the remainder repre-sent simultaneously less C&amp;P; therefore, only C and E appear on the PPC. We also draw two sets of UCs representing the preferences of two different users, X and Y. 2 User X likes purity and completeness equally, thus the slope of his utility function is  X  1, while user Y considers purity to be 6 times less serious than completeness, and thus the slope of his UCs X  is 1/6. To determine each user X  X  favourite algorithm, we draw parallels to their UCs until they intersect the PPC at the far-thest point from the origin: this is the point that maximises user utility. We can see that user X prefers method E while user Y prefers C.

In Fig. 3 , we represented each algorithm by a point that corresponds to a particular choice of parameters. Let us now imagine that, by varying the under-lying parameters, we can achieve different combinations of C&amp;P such that each algo-rithm X  X  performance can be represented by a continuous line. In that case, we can draw the PPC as the union of the right-most portion of either line. By placing this continuous PPC against the UCs, we can determine the choice of algorithms and respective parameters that users with different prefer-ences consider ideal. 3.2 Completeness and purity in segmentation Joining the characters in a line into cells is a division task from the perspective of the line and an aggregation task from the perspective of the characters. Pure cells will have only the characters of a single true cell; and complete cells will hold all the elements of a given true cell. Since all characters that are in the input will also be in the output, only merging and splitting errors can occur, which will reflect on purity and completeness, respectively.

Our algorithm is a very simple adaptation of tokenising techniques. At input, we give it pure and complete table lines. Therein, it searches for space characters. In the case of numbers, the presence of a single space between two dig-its is analysed to decide whether it is a cell delimiter or a thousands/decimal separator (e.g. in  X 19 456 34 567 X , a human knows precisely which space is the cell delimiter, and so should a computer); in non-numeric contents, three or more contiguous spaces are considered cell delimiters. At output, each cell is horizontally segmented. We applied this method to the datasets described above. In Table 5 ,we can see that performance is above 95% in most metrics and datasets, tending to be better in numeric tables (i.e. tables where at least 40% of the cells have numeric contents). Com-pleteness and purity reach a similar level, which means that splitting errors are just as likely as merging errors. Thus, when algorithms perform well, all four metrics are similar. 4 Conclusion We propose two new evaluation metrics, completeness and purity (C&amp;P), and demonstrate through examples how they are more relevant for division/aggregation tasks, so com-mon in document analysis, than the traditionally used pre-cision and recall. In fact, we demonstrate our metrics can reveal important performance issues that remain obscure using other evaluation metrics. We illustrate C&amp;P in table location and segmentation. In table location, we present the results of different algorithms, each of which adds one aspect only to the previous, which allows us to understand the ori-gin of observed improvements in performance. From it, as a side product of this research, we suggest a road-map of how to plug a Hidden Markov Model to the output of dif-ferent document structure locators and have it balance out eventually conflicting results into a coherent table location. We then statistically test the relevance of our results, using for that matter the correct statistical tests, Friedman X  X  and Nemenyi X  X . Finally, we present, among others, a method-ology commonly used in Microeconomics for graphically choosing the optimal algorithm for users with different pref-erences for C&amp;P.
 References
