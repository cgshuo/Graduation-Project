 Redwood Center for Theoretical Neuroscience coefficients are nonzero while all the others are exactly zer o. improvements in denoising performance [5, 7].
 this model are also  X  X ard-sparse X , which is a desirable feat ure [2]. Let x  X  R n be an image patch, where the x generative model: where  X  = [  X  are its basis functions.  X   X  X  (0 ,  X  2 I has a Boltzmann-Gibbs distribution p ( s ) = 1 If the spin s ( s i = 1 Gaussian distribution with u prior as it is a mixture of a point mass at zero and a Gaussian. then x according to p ( x | a )  X  X  ( X  a,  X  2 I The parameters of the model to be learned from data are  X  = ( X  , (  X  2 these dependencies. W connection, whereas W b 3.1 Coefficient estimation a is easy once s is known.
 Given s =  X  s , let  X  = { i :  X  s Hence, we have x =  X  to linear-Gaussian, where a Least-Square (BLS) and maximum a posteriori (MAP) estimato rs of a . 3.2 Multiplier estimation The MAP estimate of s given x is given by  X  s = arg max distribution N (0 ,  X ) , where  X  =  X  2 I p ( s | x )  X  p ( x | s ) p ( s )  X  e  X  E x ( s ) , where Gibbs sampling procedure, the probability that node i changes its value from s other nodes s where  X  E proposed state (  X  s i.e.  X   X  =  X  +  X  X  of the Sherman-Morrison formula and of a similar formula for the log det term Using (1) and (2)  X  E  X  3.3 Model estimation on the log-likelihood derived using Jensen X  X  inequality log-likelihood of D can be written as We perform coordinate ascent in the objective function L (  X , q ) . 3.3.1 Maximization with respect to q We want to solve max the algorithm.
 is factorial, i.e. p ( a ) = Q With S ( a pattern  X  s , where  X  s algorithm [18]. Another possible choice is S ( a (OMP) [19].
 Section 3.1 where  X  s = arg max p ( s | x ) and  X  a = E [ a |  X  s, x ] . 3.3.2 Maximization with respect to  X  We want to solve max function as the sum of the three terms L (  X  Maximization of L We add the constraint that k  X  functions grows and the coefficients tend to 0. We solve this  X  using the Lagrange dual as in [20].
 Maximization of L a 0-mean Gaussian random variable, where we only consider th e samples  X  a equal to 1, i.e. Maximization of L We use Gibbs sampling to obtain estimates of E the same as in the usual sparse coding algorithm. We then maxi mize L enter the second phase of the algorithm. to parameters  X  algorithm is able to recover  X  coefficients provided that p &lt; 1 the noiseless case for simplicity. Let k s k to be such that P r k s k us to recover ( W ( e 1 , . . . , e 7 ) b 2 , and the weights W  X   X  Figure 2: Recovery of the model. The histogram of k s k parameters ( W, b ) learned from synthetic data are close to the parameters ( W data was generated.  X  right are the learned variances (  X  2 within the 16  X  16 patch. function  X  tions ( W of the basis functions  X  rigue/nips07.html for the whole set.
 and its reconstruction  X  X  a , and the  X  performance. between a Laplacian prior over the coefficients and our propo sed prior. (  X  we sparsify using (4), we obtain a number of spins ( X  s pirical distribution p order correlations, and the factorial distribution p of the empirical distribution, and results in better coding efficiency since KL ( p whereas KL ( p derlying contour integration in the visual cortex.
