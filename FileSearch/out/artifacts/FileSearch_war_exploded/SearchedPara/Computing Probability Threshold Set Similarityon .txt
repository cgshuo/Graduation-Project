 Lei Wang, Ming Gao, Rong Zhang ( Jaccard Coefficient (Jac.) is a common metric to evaluate similarity between two sets. Let A and B be two sets, Jaccard Coefficient between sets A and B , Jac ( A, B ), is defined as shown in Equation 1 .
 In these applications, sets often are uncertain due to many factors, including evaluate the similarity between two probabilistic sets. Lian and Chen define similarity ( PTSS ) between two probabilistic sets based on the possible world semantics. Compared with Lian and Chen X  X  work, we exploit an exact solution to compute PTSS via the dynamic programming. In addition, people may be interested in whether the value of PTSS is larger than a pre-defined threshold. We therefore define a probability threshold set query ( PTSQ ) and drive a more efficient and effective pruning rule to speed up the processing of probability threshold set query ( PTSQ ). In summary, our model achieves a good balance between the model complexity and efficiency of similarity retrieval algorithms. The contributions of this paper are summarized as follows.  X  We propose an exact solution to compute PTSS . The similarity value can  X  We derive an efficient and effective pruning rule to speed up PTSQ .The  X  We conduct extensive experiments on both synthetic and real probabilistic preliminary knowledge such as data models, definitions of PTSS and PTSQ . quick-test approach to process PTSQ . Furthermore, we report the experimental we review some related work and conclude the paper. We model a probabilistic set A in a discrete domain D as where a i  X  D and p a i  X  (0 , 1] for 1  X  i  X  n , each element a w  X  X  Pr [ w ] = 1. In fact, many applications indicate that the model is useful of the probabilistic set [ 8 ].
 each possible world w ( w  X  X  ) only contains a subset of elements from The probability of a possible world is computed as Pr [ w ]=  X  p out of w . 2.1 Formulation Consider two independent probabilistic sets A and B as follows. The joint possible worlds of two probability sets, W ( A , ( w ,w b )  X  X  ( A , B ) w a  X  X  ( A )and w b  X  X  ( B ) is Pr [ w Example 1. Given two probabilistic sets, A = { 1:1 , 2:0 . 7 0 . 5 } , Table 1 enumerates all of joint possible worlds over W ( the 3rd possible world consists of { 1 ( A ) , 2 ( A ) } and former and latter are 0 . 7(= 1  X  0 . 7) and 0 . 05(= (1  X  For each joint possible world ( w a ,w b ), Jaccard Coefficient between w w can be computed as shown in Equation 1 . All of the similarities over ing to the mean and minconf  X  quantile of the distribution, respectively. Both the P robability T hreshold S et S imilarity ( PTSS ) as follows. Definition 1 (PTSS). Let  X  be a similarity threshold. The T hreshold S et S imilarity ( PTSS ) of two probabilistic sets, puted as follows PTSS is the probability of Jaccard Coefficient of all of joint possible worlds ( w ,w b )  X  X  ( A , B ) being greater than a pre-defined threshold  X  . The larger value of PTSS indicates that it is more confidential to believe that Jaccard Coefficient of a joint possible world of W ( A , B ) is larger than  X  . Example 2. In Table 2 , we also show the different similarity values of given in Example 1 .The ES value is the average of Jac. in Table 1 weighted by their respective probabilities. The CS value is the 50%  X  + 0.135 + 0.135) because Jac. of the 2nd, 5th and 6th joint possible worlds in Table 1 are not less than 0.5.
 In Table 2 , we can also observe that PTSS can give us a more detailed picture of the distribution between two probabilistic sets by varying the values of  X  . In some applications, we may also be interested in whether the value of PTSS is greater than a pre-defined  X  or not. Formally, we define a P robability T hreshold S et Q uery ( PTSQ ) in terms of PTSS as follows.
 Definition 2 (PTSQ). Let  X  and  X  be two pre-defined parameters. The P robability T hreshold S et Q uery ( PTSQ ) of two probabilistic sets, defined as follows where I PTSS ( A , B , X  )  X   X  is an indicator function and defined by That is, PTSQ is a boolean query which return 1 ( true ) when PTSS is greater than  X  , otherwise 0 ( false ). For example, when  X  =0 . 3 , X  =0 . 8, in Example 1 , PTSQ ( A , B , X , X  ) returns 1 because PTSS ( 0.8.
 2.2 Important Symbols Let k denote the number of identical elements between A and of the generality, we assume the first k elements in A and a i = b i  X  c i for 1  X  i  X  k . Consequently, probabilistic sets redescribed below.
 (or B l f ,1  X  l  X  k ) be the first l elements of A [1 ,k ] (or l common elements, and A l t (or B l t ) be the first l elements of B [ k +1 ,m ]), i.e., the first l distinct elements. For convenience, are shorted in A t and B t , respectively. Similarly, A k and B f , respectively. Some important symbols are listed in Table 3 . worlds based on its definition. However, the number of joint possible worlds is exponential to the sizes of two probabilistic sets. The key observation for an efficient solution is that PTSS only involves a few key statistics which can be computed by the dynamic programming. 3.1 Overview As shown in Equation 6 , we observe that only the joint possible worlds, whose Jaccard Coefficients are larger than given threshold  X  , can contribute to the value of PTSS .
 where I Jac ( w a ,w b )  X   X  is an indicator function that returns 1 if Jac ( w In terms of Equation 6 , the union size of a joint possible world ( the joint possible worlds of A and B into the equivalent classes based on their intersection and union sizes, the value of PTSS can be computed as where H [ i, j ]= be easily computed and shown in Table 4 . For computing PTSS ( 0 . 5), we only need to consider the second row of Table 4 . When we fix i =1 (i.e., intersection size), j (i.e., union size) cannot be larger than 2 ( PTSS ( A , B , 0 . 5) = 0 . 45 + 0 . 135 = 0 . 585 .
 3.2 The Computation of H [ i, j ] Now, we illustrate how to compute H [ i, j ] in polynomial time via the dynamic programming. Since the intersection size only relies on the common elements of A and B , we consider the common elements and the distinct elements separately. i  X  j  X  l  X  k , which corresponds to probability Pr [ | w ( | For  X  l  X  [1 ,k ], we have In Equation 8 , four possible cases, which regard the occurrence of the common that the l  X  th common element does not appear in either A set F 0 [0 , 0] = 1, otherwise F 0 [ i, j ] = 0. We also return F 0  X  i  X  j  X  l  X  k is violated.
 dimensional table G l X [ i ]( X can be A or B ), which is probability Pr [ where p l = p a l if X is A , otherwise p l = p b l .
 where j c is the union size of A f and B f , j a is the size of the time complexity of computing H [ i, j ]is O ( k 3 + n is O ( kn ). Given parameters  X  and  X  , we can address a PTSQ by directly computing PTSS ( A , B , X  ), and comparing the value with  X  . However, the drawback is to compute the similarity value between every probabilistic set in a database and comparing the upper bound of PTSS with  X  . 4.1 Analysis The upper bound of PTSS is given in Theorem 1 .
 Theorem 1. Let w denote a randomly selected possible world upon two prob-abilistic sets A and B .Let M 1 and M 2 be two random variables, such that M = | w ( A )  X  w ( B ) | and M 2 = | w ( A )  X  w ( B ) | .For Proof. Let events H 1 and H 2 be We decompose the event H 1 into two disjoint events H 1  X   X  c  X  [0 , 1], we bound their probabilities as follows.
 Pr [ H Pr [ H Thus, we can obtain Inequation 10 . 4.2 Pruning Rule Theorem 1 indicates an upper bound of PTSS due to the following equation. Given a query probabilistic set q , we can filter out a probabilistic set not need to compute the value of PTSS if the upper bound of Pr [  X  | w ( q )  X  w ( A ) | ] is smaller than  X  .
 It is cheap to compute the upper bound since the complexities of time and space for computing E ( M 1 )and E ( M 2 )are O ( m + n )and O (1), respectively. of the proposed algorithm upon both synthetic and real datasets. All programs were implemented in Java, and were conducted on a dual core 64-Bit processor with 3.06 and 3.06 GHz CPUs respectively, and 128GB of RAM.  X  SYN -N : this is a synthetic probabilistic dataset, which is a collection of (iii) the memory usage and; (iv) the pruning ratio. We measure the elapsed time and the pruning time in milliseconds, the memory usage presenting the total amount of memory used by an algorithm, and the pruning ratio is defined as
T , where N , C and T are the sizes of database, candidate probabilistic sets and probabilistic sets violated the query condition.
 PTSQ , denoted as PUBP .Wetreat PUBP as a baseline, and compare it with our proposed pruning rule, denoted as PTSQ . Note that PUBP-r ( PUBP-s ) presents the experimental results of PUBP on real dataset pDBLP -N (synthetic dataset SYN -N ). For PTSS and PTSQ , the notations are similar. Unless spec-5.1 Efficiency of the Exact Solution posed in Section 3 .
 PTSS on both SYN -N and pDBLP -N , where N is from 100 to 5,000. The memory usage goes up as the size of a probabilistic set increases. Note that the memory usage exceeds 1G bytes when there are merely five thousands of elements in two probabilistic sets.
 Figure 2 illustrates the elapsed time of the exact solution for computing PTSS on both SYN -N and pDBLP -N . The elapsed time increases as the size of a probabilistic set increases. 5.2 Comparison with PUBP [ 17 ] Here, we mainly compare PTSQ with PUBP as shown in Figures 3 and 4 . Figure 3 shows the scalability of PUBP and our proposed pruning rule for PTSQ on both SYN -N and pDBLP -N . We observe that our proposed pruning rule outperforms PUBP significantly. That is due to the factor that PUBP requires to construct a 2D table in memory, so that it has complexity O ( n for both time and space. Contrarily, our method only needs one scan of the probabilistic sets. Thus, the time and space complexities are O ( m + n )and O (1), respectively. It means that our proposed pruning rule can support the larger probabilistic sets.
 ing rule on both SYN -N and pDBLP -N . In Figures 4 (a) and (c), the pruning value of  X  results in a smaller value of PTSS . Comparing to  X  , a smaller PTSS further leads to a higher pruning ratio. In Figures 4 (b) and (d), the pruning ratios go down for PUBP when the set sizes increase. This is due to the factor that two probabilistic sets with larger sizes have larger upper bound given by PUBP . On the contrary, the upper bound given by PTSQ goes down as the proposed approach outperforms PUBP significantly. Similarity Search returns objects in a set of data collection which are similar or close to the query object, such as top-k, K-NN, skyline, and range query, between two uncertain objects. For two images or PDFs presented by histogram, we can measure their distance by Earth Mover X  X  Distance (EMD)[ 20 ]. For the string set data, the paper [ 14 ] proposed expected edited distance to measure the distance between two probabilistic strings. And the paper [ 17 ]proposed probability threshold set similarity for Jaccard index.
 sively studied in many applications with different similarity metrics, such as ilarity and Hamming distance for two sets [ 1 ], etc. The most common technique is the index-based join solution which constructs index structure built on both cation, in the stage of pruning, some indexes, inverted indexes and approximate computing the distance between two objects. However, the problem becomes more complex in the uncertain data. There are some works, which studied the level uncertain model. And the work of [ 17 ] responded the set similarity join paper[ 17 ] was the only work related to it, which studied the PUBP over proba-bilistic sets. Furthermore, only the second pruning rule proposed in that paper could work for PUBP between two probabilistic sets. What X  X  more important, The importance of probabilistic similarity queries has been recognized in many speeding up the probabilistic threshold set query. The experimental results on both synthetic and real probabilistic datasets show the effectiveness and effi-ciency of our proposed solutions. Future work includes handling streaming data, supporting other uncertain data models.

