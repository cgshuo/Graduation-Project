 Distributions over permutations play an important role in applications such as multi-object tracking, visual feature matching, and ranking. In tracking, for example, permutations represent joint assignments of individual identities to track positions, and in ranking, permutations represent notoriously difficult problem since there are n ! permutations, and standard representations, such as graphical models, are ineffective due to the mutual exclusivity constraints typically associated with permutations. The quest for exploitable problem structure has led researchers to consider a number of possibilities including distribution sparsity [17, 9], exponential family parameteriza-tions [15, 5, 14, 16], algebraic/Fourier structure [13, 12, 6, 7], and probabilistic independence [8]. While sparse distributions have been successfully applied in certain tracking domains, we argue that they are less suitable in ranking problems where it might be necessary to model indifference over a number of objects. In contrast, Fourier methods handle smooth distributions well but are not easily scalable without making aggressive independence assumptions [8]. In this paper, we argue that while probabilistic independence might be useful in tracking, it is a poor assumption in ranking. We propose a novel generalization of independence, called riffled independence , which we argue to be far more suitable for modeling distributions over rankings, and develop algorithms for working with riffled independence in the Fourier domain. Our major contributions are as follows.  X  We introduce an intuitive generalization of independence on permutations, which we call riffled independence , and show it to be a more appropriate notion of independence for ranked data, offering possibilities for efficient inference and reduced sample complexity.  X  We introduce a novel family of distributions, called biased riffle shuffles , that are useful for riffled independence and propose an algorithm for computing its Fourier transform.  X  We provide algorithms that can be used in the Fourier-theoretic framework of [13, 8, 7] for joining riffle independent factors ( RiffleJoin ), and for teasing apart the riffle independent factors from a joint ( RiffleSplit ), and provide theoretical and empirical evidence that our algorithms perform well. In the context of ranking, a permutation  X  = [  X  1 ,..., X  n ] represents a one-to-one mapping from n objects to n ranks, where, by  X  j = i (or  X  ( j ) = i ), we mean that the j th object is assigned rank i under  X  . If we are ranking a list of fruits/vegetables enumerated as (1) Artichoke, (2) Broccoli, (3) Cherry, and (4) Dates, then the permutation  X  = [  X  A  X  B  X  C  X  D ] = [2 3 1 4] ranks Cherry first, Artichoke second, Broccoli third, Dates last. The set of permutations of { 1 ,...,n } forms a group with respect to function composition called the symmetric group (written S n ). We write  X  X  to denote the permutation resulting from  X  composed with  X  (thus [  X  X  ]( j ) =  X  (  X  ( j )) ). A distribution h (  X  ) , defined over S n , can be viewed as a joint distribution over the n variables  X  = (  X  1 ,..., X  n ) (where  X  j  X  { 1 ,...,n } ), subject to mutual exclusivity constraints ensuring that objects i and j never map to the same rank ( h (  X  i =  X  j ) = 0 whenever i 6 = j ). Since there are n ! permutations, it is intractable to represent entire distributions and one can hope only to maintain compact summaries. There have been a variety of methods proposed for summarizing distributions over permutations ranging from older ad-hoc methods such as maintaining k -best hypotheses [17] to the more recent Fourier-based methods which maintain a set of low-order summary statistics [18, 2, 11, 7]. The first-order summary , for example, stores a marginal probability of the form h (  X  :  X  ( j ) = i ) for every pair ( i,j ) and thus requires storing a matrix of only O ( n 2 ) numbers. For example, we might store the probability that apples are ranked first. More generally, one might store the s th -order marginals , which are marginal probabilities of s -tuples. The second-order marginals, for example, take the form h (  X  :  X  ( k,` ) = ( i,j )) , and require O ( n 4 ) storage. Low-order marginals correspond, in a certain sense, to the low-frequency Fourier coefficients of a distribution over permutations. For example, Fourier coefficients. In general, one requires O ( n 2 s ) coefficients to exactly reconstruct s th -order marginals, which quickly becomes intractable for moderately large n . To scale to larger problems, Huang et al. [8] demonstrated that, by exploiting probabilistic independence , one could dramatically improve the scalability of Fourier-based methods, e.g., for tracking problems, since confusion in data association only occurs over small independent subgroups of objects in many problems. Probabilistic independence on permutations. Probabilistic independence assumptions on the symmetric group can simply be stated as follows. Consider a distribution h defined over S n . Let X q = n  X  p . We say that  X  X = (  X  1 , X  2 ,..., X  p ) and  X   X  X = (  X  p +1 ,..., X  n ) are independent if Storing the parameters for the above distribution requires keeping O ( p ! + q !) probabilities instead of the much larger O ( n !) size required for general distributions. Of course, O ( p ! + q !) can still be quite large. Typically, one decomposes the distribution recursively and stores factors exactly for small enough factors, or compresses factors using Fourier coefficients (but using higher frequency terms than what would be possible without the independence assumption). In order to exploit probabilistic independence in the Fourier domain, Huang et al. [8] proposed algorithms for joining factors and splitting distributions into independent components in the Fourier domain.
 Restrictive first-order conditions. Despite its utility for many tracking problems, however, we argue that the independence assumption on permutations implies a rather restrictive constraint on distributions, rendering independence highly unrealistic in ranking applications. In particular, using the mutual exclusivity property, it can be shown [8] that, if  X  X and  X   X  X are independent, then for some fixed p -subset Y  X  X  1 ,...,n } ,  X  X is a permutation of elements in Y and  X   X  X is a permutation of its complement,  X  Y , with probability 1. Continuing with our vegetable/fruit example with n = 4 , if the vegetables and fruits rankings,  X  veg = [  X  A  X  B ] and  X  fruit = [  X  C  X  D ] , are known to be inde-pendent, then for Y = { 1 , 2 } , the vegetables are ranked first and second with probability one, and the fruits are ranked third and last with probability one. Huang et al. [8] refer to this as the first-order condition because of the block structure imposed upon first-order marginals (see Fig. 1). In sports tracking, the first-order condition might say, quite reasonably, that there is potential identity confu-sion within tracks for the red team and within tracks for the blue team but no confusion between the two teams. In our ranking example however, the first-order condition forces the probability of any vegetable being in third place to be zero, even though both vegetables will, in general, have nonzero marginal probability of being in second place, which seems quite unrealistic. In the next section, we overcome the restrictive first-order condition with the more flexible notion of riffled independence . The riffle (or dovetail) shuffle [1] is perhaps the most popular method of card shuffling, in which one cuts a deck of n cards into two piles, X = { 1 ,...,p } and  X  X = { p + 1 ,...,n } , of sizes p and q = n  X  p , respectively, and successively drops the cards, one by one, shuffles, we present a novel relaxation of full independence, which we call riffled inde-pendence . Rankings that are riffle independent are formed by independently selecting rankings for two disjoint subsets of objects, then interleaving the rank-ings using a riffle shuffle to form a ranking over all objects. For example, we might first  X  X ut the deck X  into two piles, vegetables ( X ) and fruits (  X 
X ), independently decide that Broccoli is preferred over Artichoke (  X 
B &lt;  X  A ) and that Dates is preferred over Cherry (  X  D &lt;  X  C ), then in-terleave the fruit and vegetable rankings to form  X  B &lt;  X  D &lt;  X  A &lt;  X  C (i.e.  X  = [3 1 4 2] ). Intuitively, riffled independence models complex relationships within each of set X and  X  X while allowing correlations between sets to be modeled only by a constrained form of shuffling.
 Riffle shuffling distributions. Mathematically, shuffles are modeled as random walks on S n . The ranking  X  0 after a shuffle is generated from the ranking prior to that shuffle,  X  , by drawing a permutation,  X  from a shuffling distribution m (  X  ) , and setting  X  0 =  X  X  . h convolution operation.
 The question is, what are the shuffling distributions m that correspond to riffle shuffles? To answer this question, we use the distinguishing property of the riffle shuffle, that, after cutting the deck into two piles of size p and q = n  X  p , the relative ranking relations within each pile are preserved. Thus, if the i th card lies above the j th in one of the piles, then after shuffling, the i th card remains above the j th . In our example, relative rank preservation says that if Artichoke is preferred over Broccoli prior to shuffling, it is preferred over Broccoli after shuffling. Any allowable riffle shuffling distri-bution must therefore assign zero probability to permutations which do not preserve relative ranking relations. The set of permutations which do preserve these relations have a simple description. Definition 1 (Riffle shuffling distribution) . Define the set of ( p,q ) -interleavings as: where Y (1) represents the smallest element of Y , Y (2) the second smallest, etc. A distribution m p,q The ( p,q ) -interleavings can be shown to preserve the relative ranking relations within each of the subsets X = { 1 ,...,p } and  X  X = { p + 1 ,...,n } upon multiplication. In our veg-etable/fruits example, we have n = 4 , p = 2 , and so the collection of subsets of size |  X  p,q | = n p = n q = 4! / (2!2!) = 6 . One possible riffle shuffling distribution on S 4 and zero probability to everything else, reflecting indifference between vegetables and fruits. We now formally define our generalization of independence where a distribution which fully factors independently is allowed to undergo a single riffle shuffle. Definition 2 (Riffled independence) . The subsets X = { 1 ,...,p } and  X  X = { p + 1 ,...,n } shuffling distribution m p,q and distributions f,g , respectively. We denote riffled independence by: h = f  X  m p,q g , and refer to f,g as riffled factors .
 To draw from h , one independently draws a permutation  X  p , of cards { 1 ,...,p } , a permutation  X  q , example, the rankings  X  p = [2 1] (Broccoli preferred to Artichoke) and  X  q = [4 3] (Cherry preferred to Dates) are selected, then shuffled (multiplied by  X  { 1 , 3 } = [1 3 2 4] ) to obtain  X  = [3 1 4 2] . We remark that setting m p,q to be the delta distribution on any of the ( p,q ) -interleavings in  X  p,q recovers the definition of ordinary probabilistic independence, and thus riffled independence is a strict generalization thereof. Just as in the full independence regime, where the distributions f and g are marginal distributions of rankings of X and  X  X , in the riffled independence regime, they can be thought of as marginal distributions of the relative rankings of X and  X  X .
 Biased riffle shuffles. There is, in the general case, a significant increase in storage required for distributions f and g , we now require O ( n p ) storage for the nonzero terms of the riffle shuffling now introduce a family of useful riffle shuffling distributions which can be described using only m p,q , which assigns uniform probability to all ( p,q ) -interleavings and zero probability to all other elements in S n . Used in the context of riffled independence, m unif p,q models potentially complex relations within X and  X  X , but only captures the simplest possible correlations across subsets. We might, for example, have complex preference relations amongst vegetables and amongst fruits, but be completely indifferent with respect to the subsets, vegetables and fruits, as a whole. There is a simple recursive method for uni-formly drawing ( p,q ) -interleavings. Starting with a deck of n cards cut into a left pile ( { 1 ,...,p } ) and a right pile ( { p + 1 ,...,n } ), pick one of the piles with probability propor-tional to its size ( p/n for the left pile, q/n for the right) and drop the bottommost card, thus mapping either card p or card n to rank n . Then recurse on the n  X  1 remaining undropped cards, drawing a ( p  X  1 ,q ) -interleaving if the right pile was picked, or a ( p,q  X  1) -interleaving if the left pile was picked. See Alg. 1.
 It is natural to consider generalizations where one is preferentially biased towards dropping cards from the left hand over the right hand (or vice-versa). We model this bias using a simple one-parameter family of distributions in which cards from the left and right piles drop with probability proportional to  X p and (1  X   X  ) q , respectively, instead of p and q . We will refer to  X  as the bias parameter , and the family of distributions parameterized by  X  as the biased riffle shuffles . 1 In the context of rankings, biased riffle shuffles provide a simple model for expressing groupwise preferences (or indifference) for an entire subset X over  X  X or vice-versa. The bias parameter  X  can be thought of as a knob controlling the preference for one subset over the other, and might reflect, for example, a preference for fruits over vegetables, or perhaps indifference between the two subsets. Setting  X  = 0 or 1 recovers the full independence assumption, preferring objects in X (vegetables) over objects in  X  X (fruits) with probability one (or vice-versa), and setting  X  = . 5 , recovers the uniform riffle shuffle (see Fig. 3). Finally, there are a number of straightforward generalizations of the biased riffle shuffle that one can use to realize richer distributions. For example,  X  might depend on the number of cards that have been dropped from each pile (allowing perhaps, for distributions to prefer crunchy fruits over crunchy vegetables , but soft vegetables over soft fruits ). We have presented riffle independent distributions as fully independent distributions which have been convolved by a certain class of shuffling distributions. In this section, we provide an alternative view of riffled independence based on conditional independence, showing that the notion of riffled independence lies somewhere between full and conditional independence.
 In Section 3, we formed a ranking by first independently drawing permutations  X  p and  X  q , of object sets { 1 ,...,p } (vegetables) and { p + 1 ,...,n } (fruits), respectively, drawing a ( p,q ) -interleaving is ranked in position  X  Y (  X  q ( j )) ). An equivalent way to form the same  X  , however, is to first draw an interleaving  X  Y  X   X  p,q , then, conditioned on the choice of Y , draw independent permutations of the sets Y and  X  Y . In our example, we might first draw the (2,2)-interleaving [1 3 2 4] (so that after shuffling, we would obtain  X  V eg &lt;  X  Fruit &lt;  X  V eg &lt;  X  Fruit ). Then we would draw a permutation [4 2] , to obtain a final ranking over all items:  X  = [3 1 4 2] , or  X  B &lt;  X  D &lt;  X  A &lt;  X  C . It is tempting to think that riffled independence is exactly the conditional independence assumption, case of conditional independence, however, has O ( n p ( p ! + q ! + 1)) parameters, while riffled independence requires only O ( n p + p ! + q !) parameters.
 We now provide a simple correspondence between the conditional independence view of riffled independence presented in this section to the shuffle theoretic definition from Section 3 (Def. 2). Define the map  X  , which, given a permutation of Y (or  X  Y ), returns the permutation in  X  p  X  S p (or S ) such that [  X  p ] i is the rank of [  X  X ] i relative to the set Y . For example, if the permutation of the vegetable ranks is  X  X = [3 1] (with Artichoke ranked third, Broccoli first), then  X  (  X  X ) = [2 1] since, relative to the set of vegetables, Artichoke is ranked second, and Broccoli first. Proposition 3. Consider a riffle independent h = f  X  m p,q g . For each  X   X  S n , h factors as h (  X  ) = Proposition 3 is useful because it shows that the probability of a single ranking can be computed without summing over the entire symmetric group (a convolution) X  a fact that might not be riffled independence behaves essentially like full independence (without the first-order condition), where, in addition to the independent variables  X  X and  X   X  X , we also independently randomize over the subset Y . An immediate consequence is thatjust as in the full independence regime, conditioning operations on certain observations and MAP (maximum a posteriori) assignment problems decompose according to riffled independence structure.
 Proposition 4 (Probabilistic inference decompositions) . Consider riffle independent prior and like-and h like = f like  X  m like g like , respectively. The posterior distribution under Bayes rule can be written as the riffle independent distribution: h post  X  ( f prior f like )  X  m prior m like ( g prior g like ) ,where the symbol denotes the pointwise product operation.
 A similar result allows us to also perform MAP assignments by maximizing each of the distributions m p,q , f and g , independently and combining the results. As a corollary, it follows that conditioning on simple pairwise ranking likelihood functions (that depend only on whether object i is preferred to object j ) decomposes along riffled independence structures. In this section, we present two algorithms for working with riffled independence in the Fourier theo-retic framework of [13, 8, 7]  X  one algorithm for merging riffled factors to form a joint distribution ( RiffleJoin ), and one for extracting riffled factors from a joint ( RiffleSplit ). We begin with a brief introduction to Fourier theoretic inference on permutations (see [11, 7] for a detailed exposition). Unlike its analog on the real line, the Fourier transform of a function on S n takes the form of a collection of Fourier coefficient matrices ordered with respect to frequency. Discussing the analog of frequency for functions on S n , is beyond the scope of our paper, and, given a distribution h , we simply index the Fourier coefficient matrices of h as b h 0 , b h 1 , ... , b h K ordered with respect to some measure of increasing complexity. We use b h to denote the complete collection of Fourier coefficient matrices. One rough way to understand this complexity, as mentioned in Section 2, is by the fact that the low-frequency Fourier coefficient matrices of a distribution can be used to reconstruct low-order marginals. For example, the first-order matrix of marginals of h can always be reconstructed from the matrices  X  h 0 and  X  h 1 . As on the real line, many of the familiar properties of the Fourier transform continue to hold. The following are several basic properties used in this paper: Proposition 5 (Properties of the Fourier transform, see [2]) . Consider any f,g : S n  X  R . A number of papers in recent years ([13, 6, 8, 7]) have considered approximating distributions over permutations using a truncated (bandlimited) set of Fourier coefficients and have proposed inference algorithms that operate on these Fourier coefficient matrices. For example, one can perform generic marginalization, Markov chain prediction, and conditioning operations using only Fourier coefficients without ever having to perform an inverse Fourier transform. Huang et al. [8] introduced Fourier domain algorithms, Join and Split , for combining independent factors to form joints and for extracting the factors from a joint distribution, respectively.
 In this section, we provide generalizations of the algorithms in [8] that we call RiffleJoin and Riffle-Split . We will assume that X = { 1 ,...,p } ,  X  X = { p + 1 ,...,n } and that we are given a riffle inde-pendent distribution h : S n  X  R ( h = f  X  m p,q g ). We also, for the purposes of this section, assume that the parameters for the distribution m p,q are known, though it will not matter for the RiffleSplit algorithm. Although we begin each of the following discussions as if all of the Fourier coefficients are provided, we will be especially interested in algorithms that work well in cases where only a trun-cated set of Fourier coefficients are present, and where h is only approximately riffle independent. RiffleJoin . Given the Fourier coefficients of f , g , and m , we can compute the Fourier coefficients of h using Definition 2 by applying the Join algorithm from [8] and the Convolution Theorem (Prop. 5), which tells us that the Fourier transform of a convolution can be written as a pointwise product of Fourier transforms. To compute the  X  h  X  , our RiffleJoin algorithm simply calls the Join algorithm on b f and to Fourier transform the riffle shuffling distribution m p,q . However, for the class of biased riffle shuffles from Section 3, one can efficiently compute the low-frequency terms of d m  X  p,q by employing the recurrence relation in Alg. 1. In particular, Alg. 1 expresses a biased riffle shuffle on S n as a linear combination of biased riffle shuffles on S n  X  1 . By invoking linearity of the Fourier transform (Prop. 5), one can efficiently compute d m  X  p,q via a dynamic programming approach. To the best of our knowledge, we are the first to compute the Fourier transform of riffle shuffling distributions. RiffleSplit . Given the Fourier coefficients of the riffle independent distribution h , we would like to tease apart the riffle factors f and g . From the RiffleJoin algorithm, we saw that for each frequency level i ,  X  h i = [ a deconvolution by multiplying each b h i term by the inverse of the matrix [ b h ) and call the Split algorithm from [8] on the result. Unfortunately, the matrix [ non-invertible. Instead, our RiffleSplit algorithm left-multiplies each b h i term by can be shown to be equivalent to convolving the distribution h by the  X  dual shuffle  X , m  X  , defined as m pendently, the Split algorithm from [8] can still be shown to recover the Fourier transforms  X  f and  X  g : Theorem 6. If h = f  X  m p,q g , then RiffleSplit (Alg. 3) (with  X  h as input), returns  X  f and  X  g exactly . As with RiffleJoin, it is necessary Fourier transform m unif p,q , which we can again accomplish via the recurrence in Alg. 1. One must also normalize the output of Split to sum to one via Prop. 5. Theoretical guarantees. We now briefly summarize several results which show how, (1) our algorithms perform when called with a truncated set of Fourier coefficients, and (2) when RiffleSplit is called on a distribution which is only approximately riffle independent.
 Theorem 7. Given enough Fourier terms to reconstruct the k th -order marginals of f and g , Rif-fleJoin returns enough Fourier terms to exactly reconstruct the k th -order marginals of h . Likewise, given enough Fourier terms to reconstruct the k th -order marginals of h , RiffleSplit returns enough Fourier terms to exactly reconstruct the k th -order marginals of both f and g .
 Theorem 8. Let h be any distribution on S n and m p,q any riffle shuffling distribution on S n . If [ b f 0 , b g 0 ] = R IFFLE S PLIT ( b h ) , then ( f 0 ,g 0 ) is the minimizer of the problem: where D KL is the Kullback-Leibler divergence. In this section, we validate our algorithms and show that riffled independence exists in real data. APA dataset. The APA dataset [3] is a collection of 5738 ballots from a 1980 presidential election of the American Psychological Association where members ordered five candidates from favorite to least favorite. We first perform an exhaustive search for subsets X and  X  X that are closest to riffle independent (with respect to D KL ), and find that candidate 2 is nearly riffle independent of the remaining candidates. In Fig. 4(a) we plot the true vote distribution and the best approximation by a distribution in which candidate 2 is riffle independent of the rest. For comparison, we plot the result of splitting off candidate 3 instead of candidate 2, which one can see to be an inferior approximation. The APA, as described by Diaconis [3], is divided into  X  academicians and clinicians who are on uneasy terms  X . In 1980, candidates { 1 , 3 } and { 4 , 5 } fell on opposite ends of this political spectrum with candidate 2 being somewhat independent. Diaconis conjectured that voters choose one group independence sense. After removing candidate 2 from the distribution, we perform a search within candidates { 1 , 3 , 4 , 5 } to again find nearly riffle independent subsets. We find that X = { 1 , 3 } and  X  X = { 4 , 5 } are very nearly riffle independent and thus are able to verify that candidate sets { 2 } , { 1 , 3 } , { 4 , 5 } are indeed grouped in a riffle independent sense in the APA data. Finally since there not well approximated by a biased riffle shuffle. Instead, we fit a mixture of two biased riffle shuffles to the data and found the bias parameters of the mixture components to be  X  1  X  . 67 and  X  2  X  . 17 , indicating that the two components oppose each other (since  X  1 and  X  2 lie on either side of . 5 ). Sushi dataset. The sushi dataset [10] consists of 5000 full rankings of ten types of sushi. Com-pared to the APA data, it has more objects, but fewer examples. We divided the data into training and test sets and estimated the true distribution in three ways: (1) directly from samples,(2) using a riffle independent distribution (split evenly into two groups of five) with the optimal shuffling log-likelihood as a function of training set size  X  we see that riffle independence assumptions can help significantly to lower the sample complexity of learning. Biased riffle shuffles, as can be seen, are a useful learning bias with very small samples. As an illustration, see Fig. 4(c) which shows the first-order marginals of Uni (Sea Urchin) rankings, and the biased riffle approximation. Approximation accuracy. To understand the behavior of RiffleSplit in approximately riffle in-dependent situations, we draw sample sets of varying sizes from a riffle independent distribution on S 8 (with bias parameter  X  = . 25 ) and use RiffleSplit to estimate the riffle factors from the empirical distribution. In Fig. 4(d), we plot the KL-divergence between the true distribution and that obtained by applying RiffleJoin to the estimated riffle factors. With small sample sizes (far less than 8! ), we are able to recover accurate approximations despite the fact that the empirical distributions are not exactly riffle independent. For comparison, we ran the experiment using the Split algorithm [8] to recover the riffle factors. Somewhat surprisingly, one can show (see Appendix) that Split also recovers the riffle factors, albeit without the optimality guarantee that we have shown for Rifflesplit (Theorem 8) and therefore requires far more samples to reliably approximate h .
 Running times. In general, the complexity of Split is cubic ( O ( d 3 ) ) in the dimension of each when p  X  O ( n ) . If we precompute the Fourier coefficients of m p,q , (which requires O ( n 2 d 3 ) ) for times of RiffleJoin (no precomputation) as a function of n (setting p = d n/ 2 e ) scaling up to n = 40 . There are many open questions. For example, several papers note that graphical models cannot compactly represent distributions over permutations due to mutual exclusivity. An interesting models by substituting conditional generalizations of riffled independence for ordinary conditional independence. Other possibilities include going beyond the algebraic approach and studying riffled independence in non-Fourier frameworks and developing statistical (riffled) independence tests. In summary, we have introduced riffled independence and discussed how to exploit such structure in a Fourier-theoretic framework. Riffled independence is a new tool for analyzing ranked data and has the potential to offer novel insights into datasets both new and old. We believe that it will lead to the development of fast inference and low sample complexity learning algorithms.
 Acknowledgements This work is supported in part by the ONR under MURI N000140710747, and the Young Investiga-tor Program grant N00014-08-1-0752. We thank K. El-Arini for feedback on an initial draft.
