 1. Introduction The peer-to-peer (P2P) approach, which has become popular in the context of file-sharing systems such as
Gnutella or KaZaA, allows handling huge amounts of data in a distributed and self-organized way. In such a of failure and the load is evenly balanced across a large number of peers. These characteristics offer enormous and dynamics. Additionally, such a search engine can potentially benefit from the intellectual input, e.g., bookmarks ( Bender, Michel, Weikum, &amp; Zimmer, 2004 ) and query logs ( Luxenburger &amp; Weikum, 2004 ), of a large user community. One of the key difficulties, however, is to efficiently select promising peers for a particular information need, given the total number of relevant peers in a network is not known a priori and peer relevance also varies from peer to peer.
 Semantic Overlay Networks (SONs) ( Aberer, Cudre-Mauroux, Hauswirth, &amp; Pelt, 2004; Bawa, Manku, &amp;
Raghavan, 2003; Crespo &amp; Garcia-Molina, 2002; Triantafillou, Xiruhaki, Koubarakis, &amp; Ntarmos, 2003 ) appear as a network organization that improves query performance while maintaining a high degree of node autonomy. In a SON nodes with semantically similar content are  X  X  X lustered X  X  together. Queries are, then, rou-ted only to the appropriated SONs, increasing the chances that matching information (e.g. files, documents) will be found quickly, and thus, the load on nodes having unrelated content is reduced. Such overlay networks are actually not only useful in search engines. It will be shown in this document that there are other applica-tions that can also benefit from SONs.

We show how peers acting autonomously can form context-rich SONs, and how the proposed SONs can be utilized during query routing in P2P Web search engines, and in a decentralized algorithm for computing authority scores of documents.
 There are many challenges when building SONs, regarding how nodes are assigned to SONs and to which
SONs a query should be sent to. According to the initial idea proposed, peers should be evenly distributed among SONs, so we can answer queries fast, as we have to ask fewer peers, and each peer should belong to a small number of SONs, so that each peer has to handle only a few number of connections. However, in the real world, the distribution of peers over semantic classes is expected to be very skewed and dynamic as many peers will belong to some very popular topics that are constantly changing, whereas some uncommon classes will be less populated.

Moreover, in most of the early approaches, nodes decide which SONs to join based on the classification of their documents. This leads to a fixed configuration of the Semantic Overlay Networks, so that the perfor-mance is highly dependable on a good choice of the classification algorithm, and requires a classifier that is the same for all peers. To overcome the restriction that the peers are squeezed into these strict topic schemes, we propose a new method for building Semantic Overlay Networks that gives more autonomy for the peers, when deciding which SONs they should join. The method works by rearranging the connections between peers, according to the peers X  criteria of a  X  X  X ood X  X  neighbor (i.e. a  X  X  X riend X  X ), and using caching to remember the peers that were defined as friends. Possible measures for deciding if a peer should be considered a friend or not could be, for instance, level of overlap between documents from the peer and documents from the candi-date for being a friend, the similarity between their documents, history of the peer, level of trust, etc. A peer teria or found more interesting peers.

The rest of the document is organized as follows: Section 2 briefly discusses related work. The p2pDating, our approach for creating dynamically evolving semantic overlay networks is then presented in more detail in
Two applications that can directly benefit from p2pDating, namely the JXP algorithm and the Minerva sys-tem, are presented in Sections 5 and 6 , respectively. Moreover, we performed experiments that show how p2pDating can improve the performance of the JXP algorithm, and that it can be used to efficiently and effec-tively find promising peers during query routing. The results are also presented in Sections 5 and 6 . Section 7 concludes this paper with a discussion about future work. 2. Related work Recent research on P2P systems, such as Chord ( Stoica, Morris, Karger, Kaashoek, &amp; Balakrishnan, 2001 ),
CAN ( Ratnasamy, Francis, Handley, Karp, &amp; Schenker, 2001 ), Pastry ( Rowstron &amp; Druschel, 2001 ), P2P-Net forms of distributed hash tables (DHTs) and supports mappings from keys, e.g., titles or authors, to locations in a decentralized manner such that routing scales well with the number of peers in the system. However, these systems serve mainly as a network infrastructure and do not provide sufficiently enough IR functionality to support ranked retrieval for multi-keyword queries.
 In the following we briefly discuss some existing approaches towards P2P Web search. Galanx ( Wang, Galanis, &amp; DeWitt, 2003 ) is a peer-to-peer search engine implemented using the Apache HTTP server and BerkeleyDB. It directs user queries to relevant nodes by consulting local peer indexes.

PlanetP ( Cuenca-Acuna, Peery, Martin, &amp; Nguyen, 2002 ) is a publish-subscribe service for P2P communi-ties and the first system supporting content ranking search. PlanetP distinguishes local indexes and a global index to describe all peers and their shared information. The global index is replicated using a gossiping algorithm.

Odissea ( Suel et al., 2003 ) assumes a two-layered search engine architecture with a global index structure distributed over the nodes in the system. A single node holds the entire index for a particular text term (i.e., keyword or word stem). Query execution uses a distributed version of Fagin X  X  threshold algorithm ( Fagin, 1999 ). The system appears to cause high network traffic when posting document metadata into the network, and the query execution method presented currently seems limited to queries with one or two key-words only.

The system outlined in Reynolds et al. (2003) uses a fully distributed inverted text index, in which every participant is responsible for a specific subset of terms and manages the respective index structures. Particular emphasis is put on three techniques to minimize the bandwidth used during multi-keyword searches.
Lu and Callan (2003) consider content-based retrieval in hybrid P2P networks where a peer can either be a simple node or a directory node. Directory nodes serve as super-peers, which may possibly limit the scalability and self-organization of the overall system. The peer selection for forwarding queries is based on the Kull-back X  X eibler divergence between peer-specific statistical models of term distributions.

Most relevant to our work is the idea proposed by Crespo and Garcia-Molina (2002) . However, our method goes beyond the fixed classification method for assigning peers into the SONs. Each peer defines its own criteria for joining a SON. We can think of the original idea as being a special case of our method, where all peers define their good neighbors as the ones whose documents belong to the same concept as their own documents.

Tang, Xu, and Dwarkadas (2003) present another notion of semantic overlay, where documents are distrib-uted among peers in a content-addressable network (CAN) according to their semantics, such that the distance (routing hops) between two documents in the network is proportional to their dissimilarity in semantics, and the document semantics is produced using latent semantic indexing (LSI). Li and Lee (2004) choose a similar approach but propose a new dimension reduction technique that they incorporate directly in the construction of their overlay networks, so called Semantic Small Worlds. Bawa et al. (2003) propose a technique that arranges peers into so called topic segments that consist of semantically related peers. Topic segments are con-structed via clustering.
 Triantafillou et al. (2003) propose an architecture that is focused on harnessing all available resources in a
P2P system where users contribute with their own content and own resources to the community. The presented architecture imposes a logical structure, based on document categories, node clustering, and their associations, and considers fair load distribution inside and across these node-clusters, so called inter-cluster and intra-cluster load balancing .

The work presented in Tempich, Staab, and Wranik (2004) considers query answering on RDF(S) data. It defines a method for query routing in which peers observe which queries are successfully answered by other peers and remember these peers in future query routing decisions. It is a lazy learning approach based on learning peers X  query results.

In Aberer et al. (2004) the authors address the problem of building scalable semantic overlay networks and identify strategies for their traversal using P-Grid ( Aberer et al., 2002 ).

In addition to this work on using semantics in distributed search, prior research on distributed IR and metasearch engines is relevant, too. Callan (2000) gives an overview of algorithms for distributed IR style result merging and database content discovery. Fuhr (1999) presents a formal decision model for database selection in networked IR. Nottelmann and Fuhr (2003) investigate different quality measures for database selection. Notwithstanding the relevance of this prior work, collaborative P2P search is substantially more challenging than metasearch or distributed IR over a small federation of sources such as digital libraries, as these approaches mediate only a small and rather static set of underlying engines, as opposed to the high dynamics of a P2P system. 3. p2pDating: an overview
The idea of p2pDating is to create semantic overlay networks in a P2P environment, where a peer has autonomy when deciding which SONs it wants to join. The approach works by having peers meeting other has information that is interesting for the peer, it might want to remember this peer, and insert it into the it might not want to remember this peer, so no link is created or if there is already a link between them, it might be dropped. We believe that caching (i.e. remembering) of high quality peers is the natural way to create semantic overlay networks. It can either be seen as the criteria to create network links between peers, or also be they call regularly. This is a kind of intuitive caching. If people have a few favorite services for each kind of food (Italian, Mexican, German) this can be seen as topic-specific  X  X  X aching X  X .

The process starts with a randomly connected network and runs infinitely since peers join and leave the network at a high rate. Semantic overlay networks will dynamically evolve from this process, as semantic links are more and more refined. (cf. Fig. 1 ).

In a dynamic P2P network, we expect that the SONs are continuously changing to adapt to the changes in the network, the peers X  behavior, the peers X  content, etc. The semantic links are represented by entries in the friend list. It is important to emphasize that no physical real links are created. Semantic links can be seen as abstract links. When a peer joins the network its friend list is empty and will be filled over time.
Each peer has two types of links: random links and semantic links. The random links are physical links needed to keep the whole network together and are dictated by the underlying P2P network protocol. 3.1. The semantic routing table
The friends are annotated with statistics to form a semantic routing table (SRT) in which the peers are ordered according to their usefulness. SRTs are maintained in a soft-cache. Fig. 2 shows an example of a semantic routing table.

There are many ways to define a friend, as will be discussed in Section 4 . The values displayed in the exam-ple are just an illustration.

When a new friend is found, an entry containing information about this peer is added to the table. Friend lists have a fixed length, which means that current friends might need to be dropped (according to some cri-teria) from the table, so that new friends can be added. Dropping a friend corresponds to remove an abstract link in the network. Each peer creates its friend list totally independent from other peers. In particular,  X  X  X riendship X  X  is not in general symmetric. If peer A adds peer B into its friend list, A is not automatically inserted into B  X  X  list. It is up to B to decide whether to add A or not. This means that the links created by p2pDating form a directed graph. 3.2. Finding new friends
Friend lists, besides defining the links in the SONs, can also be used to find new friends in a very intuitive manner, by routing over semantic routing tables of different peers: if a peer A finds an interesting peer B , a friend of one of its friends, instead of picking a peer at random since being a friend of a friend is a strong recommendation. Alternatively, a criteria other than the one used to define a friend can be used to decide whether to add new candidates to the list or not. In this case, candidates are not necessarily friends of one of the peer X  X  friends. It will be shown later in this paper that this can be very useful for certain applications.

Candidate lists, like friend lists, also have fixed length, which means that if the maximum number is reached, candidates have to be dropped. Instead of using these candidate lists, a peer might consider search-ing for new friends only when needed, e.g., if the peers in its semantic neighborhood join and leave the sys-tem at a extremely high rate. This will reduce the cost for maintaining the candidate list, e.g., removing dead links. 3.3. Putting everything together
Algorithm 1 shows the procedure of picking a peer for the next meeting. According to some predefined probabilities, it can be a peer from the candidate list or a peer from the friend list, or a random peer in the network.
 Algorithm 1 ( ThechoosePeerToMeet()procedure ) P a peer from the candidate list, with probability a P a peer from the friend list, with probability b
P a random peer in the network, with probability (1 a b ) return P
We can think of scores for peers in the candidate list, based on the scores of the peers where they were defined as friends. Thus, we can select the peer with the highest score when choosing a peer from the candidate list. It is important that peers have an updated view of the network, as peers can change their contents or even-tually leave the network. Therefore, peers have to visit their friends from time to time. For the search engine applications, friends will be visited during query execution, so these updates can be integrated into the stan-dard querying process. Another possibility is to assign a time to live (TTL) to every friend so that peers can automatically be dropped, or visited to re-assess their usefulness. In addition, the probability of picking a peer at random should not be equal to zero, to assure that every peer in the network can be reached.
Algorithm 2 shows the pseudo-code for the p2pDating algorithm. A peer chooses another peer for the next meeting and contacts it. Then it decides whether the peer is a friend or not, based on the peer X  X  content. If so, the candidates list in case of a positive answer. As said before, we can simply choose to follow the chain of friends, by making hasGoodFriends ( P ) return true whenever P is a friend, or we can use any other criteria to implement this function.

The process of adding peers to the friends or candidate list checks if the maximum number of the peers on the list has been reached, and removes peers, if necessary.
 Algorithm 2 ( p2pDating Algorithm repeat )
P choosePeerToMeet () contact P if sFriend ( P ) add ( P , friend list ) endif if hasGoodFriends ( P )
C friends of P add ( C , candidate list ) endif
Fig. 3 illustrates a meeting between two peers, showing their friend and candidate lists before and after the meeting. We can see two possible cases, namely, when a peer decides that the other peer is a friend and has good friends, where the friend and candidate lists are updated, and when it decides that the other peer is not a friend and also does not have good friends, where the lists remain the same. The other two possible cases are when peer decides to update only its friend or its candidate list. 3.4. Replacement strategies
To avoid that the friend/candidate lists grow forever we need a replacement strategy that keeps track about peers that are no longer interesting and thus replaced by other peers or just dropped from the cache, e.g., if it turns out that these peers have left the network.

As we limited the size of a friend list, we use a ranking of friends so that if the size of the list reaches its maximum, the lowest-rank friend is dropped. The friend list X  X  order is defined by a combination of measures that will be presented in the following section. 4. Defining good friends
As explained in the previous section, when a peer A meets peer B in the network, it accesses B  X  X  content and decides whether to establish a link to B or not, based a measure of the quality/usefulness of peer B . It also decides, based on the same or, alternatively, another measure, if B  X  X  friends should be also visited. There are many different measures that can be used, depending on the purpose of the semantic overlay network that is being formed, like good behavior in the past, collection similarity, overlap between the collection, authority scores, etc. Some of these measures are explained in more detail at the end of this Section.

Fig. 2 also shows some measures that can be used to find the most promising peer for a particular need (for instance, query routing). We can see that sometimes a single measure might give enough information to decide which peer to choose since, for instance, peer A is the best choice if we consider the number of credit points, whereas peer B seems to be most promising if we take a look at the overlap. So, obviously, there is great need for an aggregation function that combines the single measures in a meaningful way, since selecting a peer based only on a particular measure can be misleading. For instance, it might be the case where a high quality peer has many documents that we already know, and a peer that offers a lot of new information has a lower quality measure.

Aggregation functions can be of any kind, but usually they are expected to be very simple, for instance, a linear combination of two or more quality measures, since the definition of a good friend and/or candidate is most of the times very intuitive.

As a peer X  X  content can be very broad and diverse, applying quality measures to its complete collection can be inaccurate. In such cases, a peer might consider to split its document set into topic-specific subsets. Each peer would then maintain more than one semantic routing table, more precisely, one for each topic it is inter-ested in, and usefulness assessment would be made for each particular topic. Although this creates additional cost, it will increase the accuracy of the quality assessments, since comparing the semantic similarity of two collections might be misleading in the case where collections are related to more than one topic. It is important required.

In addition, peers can organize the different topics in a hierarchy (cf. Fig. 4 ) with edge weights correspond-ing to topic-subtopic similarities. The edge weights can be interpreted as some kind of confidence measure that gives weight to the semantic query routing. Thus, peers can leverage the semantic routing table even in the case where the query does not correspond directly to a topic, by considering SRTs from related topics.

In addition, when the query fits to a specific topic, SRTs from sub-topics or from more general topics can be incorporated into query routing using a weighted quality assessment. This decreases the risk of querying the wrong (or not perfectly matching) peers caused by a sub-optimal query to topic classification.
In addition to these peer-to-peer quality assessments, we can also think of having global measures about the top queries, top terms, top authoritative peers, etc. This is particularly of interest in search engines, where query results for very frequent queries can be cached.

The following section describes some of the possible measures that can be used to identify good friends in the network. 4.1. Quality/usefulness measures
In this Section we give more details about some measures that can be use to assess the quality/usefulness of a peer. We also present some ideas about how to use such measures. 4.1.1. History
In any application that requires collaboration among peers, the presence of malicious peers can pose a problem. Creating a Semantic Overlay Network is not an exception. By sending false content, a peer X  X  quality is measured wrongly, and peers might be induced to chose peers which in reality have poor content, instead of real good ones.

As recently proposed by Tempich et al. (2004) , remembering excellent behavior in the past is a natural way to find friends. We can, for instance, give credit-points to peers for good cooperation. The history of a peer is very useful when combined with other measures. We could for instance, weigh every other measure, based on the past cooperation of a peer. This would decrease the impact of malicious peers and can be seen as an incen-tive mechanism as it can be used to prioritize incoming queries from friends.

What a peer stores and how it behaves can vary over time. For instance, it can find and store new files about its topics of interest or change its preferences and start storing files about a different topic. Furthermore, non-collaborative peers might become more collaborative in order to have access to network resources.
The number of credit points should reflect these changes. One solution is to reset the points counter, at regular intervals. The time space between two resets can be tuned by observing the frequency at which peers change.
Another alternative is to specify a time window, such that credit-points are given based only on the observa-tion made inside this time window. The size of a time window can be defined based also on the peers X  dynam-ics. Keeping track of the behavior in the past can be seen as a utility to predict the usefulness of a peer in the future. 4.1.2. Overlap
Avoiding the retrieval of duplicate documents is a crucial issue in large scale distributed information system. We consider autonomous peers having their own local collection, generated by focused Web crawls.
The problem inherently associated with this scenario is that collections can have a high mutual overlap, thus, it is likely that the query initiating peer will retrieve documents that it already knows from its local collection.
High quality documents are useless if there are already known: there is no need in querying a peer when it is known before hand that this peer has an extremely high overlap with regard to the own collection. The mutual overlap between peers has to be taken into account while selecting promising peers for a particular query.
Overlap aware techniques ( Bender, Michel, Triantafillou, Weikum, &amp; Zimmer, 2005; Michel, Bender, Trianta-reached by querying fewer peers, compared to the non-overlap-aware approach. For instance, if the most promising peers have exactly (or nearly) the same collections only the first peer can deliver valuable results whereas the following peers will not contribute with any new documents. Note that we consider parallel query execution, where the query is sent to multiple peers in parallel, since sequential query execution will cause additional latency.

The overlap between two collections can be estimated using min-wise independent permutations ( Broder, 1998; Broder, Charikar, Frieze, &amp; Mitzenmacher, 2000 ) that have proved to offer an accurate overlap estima-tion and at the same time are pre-computable and require only small bandwidth consumption. Using Bloom filters ( Bloom, 1970 ), compressed Bloom filters ( Mitzenmacher, 2002 ) or some random sampling technique might be another option. Recent work ( Bender et al., 2005 ) proposes a technique for predicting the query-spe-cific mutual overlap between peers.

We can also consider the notions of Containment and Resemblance as appropriate measures of mutual set correlation ( Broder, 1998 ).

Fundamentals for statistical synopses of sets and multisets have a rich literature, including work on Bloom filters ( Bloom, 1970; Fan, Cao, Almeida, &amp; Broder, 2000 ), hash sketches ( Flajolet &amp; Martin, 1985 ), and min-wise permutations ( Broder, Charikar, Frieze, &amp; Mitzenmacher, 1998, 2000 ).

A Bloom filter (BF) ( Bloom, 1970 ) is a simple data structure that represents a set as a bit vector in order to efficiently (in time and space) support membership queries. With bit vectors being a very compact represen-tation of a set, Bloom filters are an ideal representation in our environment where storage and bandwidth con-sumption is an issue.

For a particular set, a Bloom filter is a bit map of length m and is created by applying k hash functions on each member document, each yielding a bit location in the vector. Exactly (and only) these positions of the
Bloom filter will be set to 1. To check if a given element is in the set, the element is hashed using the same hash function and the corresponding k bits of the Bloom filter are examined. If there is at least one of these
There is a non-zero probability that the examined k bit positions were set by other documents, thus, creating a false positive . The probability of a false positive can be calculated by pfp (1 e of items in the original set.
 Min-Wise Independent Permutations, or MIPs for short, have been introduced in Broder et al. (1998),
Broder et al. (2000) . This technique assumes that the set elements can be ordered (which is trivial for integer keys) and computes N random permutations of the elements. Each permutation uses a linear hash function of the form h i ( x ) :  X  a i * x + b i mod U where U is a big prime number and a ordering the resulting hash values, we obtain a random permutation. For each of the N permutations, the
MIPs technique determines the minimum hash value, and stores it in an N -dimensional vector, thus capturing the minimum set element under each of these random permutations. The technique is illustrated with an exam-ple in Fig. 5 . Its fundamental rationale is that each element has the same probability of becoming the minimum element under a random permutation. By using sufficiently many different permutations, we can approximate the set cardinality.

An unbiased estimate of the pair-wise resemblance of sets using their N -dimensional MIPs vectors is obtained by counting the number of positions in which the two vectors have the same number and dividing this by the number of permutations N ( Byers, Considine, Mitzenmacher, &amp; Rost, 2004 ). Essentially, this holds as the matched numbers are guaranteed to belong to the intersection of the sets.
 In prior work ( Michel et al., 2006 ) we have conducted a performance/accuracy evaluation of MIPs, Bloom
Filters, and hash sketches with the result that MIPs provide high accuracy with only little overhead in network resource consumption. Hash sketches are also an appropriate technique but inferior compared to the MIPs.
Bloom Filter suffer from the inability to efficiently describe small collections but at the same time accurately describing large collections. Nevertheless, if tuned to a particular set, a Bloom Filter can provide an accurate and efficient way to support membership queries. A property that MIPs and hash sketches do not have.
Another measure to predict collection overlap is to compare the bookmark collections. If the local docu-ment collections have been generated by Web crawls we can treat bookmarks as crawl seeds. Thus, assuming roughly the same crawling strategy, comparing two sets of bookmarks can provide a good approximation of the collections X  content overlap. 4.1.3. Semantic similarity
We can measure the thematic similarity between two peers by comparing their bookmarks or their complete document collections. More specifically, we can compare the peers in three aspects: (1) regarding their URL sets, (2) regarding the term frequency distributions in the documents referenced by the bookmark lists, or (3) the term frequency distributions in their complete collections.

As for term distributions, we could use the relative entropy, also called the Kullback X  X eibler distance ( Kullback, 1959 ), as a measurement for information inequality. It is defined by where f and g are discrete probability distributions. The relative entropy has important mathematical proper-term frequency distributions in all documents in a collection or the documents referenced by the bookmark lists (and optionally also all hyperlink successors of these pages).

Note that the bookmark-based measure ( Bender et al., 2004 ) does not only reflect the similarity between the bookmark lists themselves, but also the similarity between the index contents of peer A with the index contents of peer B , if the index of a peer has been constructed by crawling the Web with the local bookmarks as crawl seeds. Clearly, comparing bookmarks is much more efficient than comparing entire indexes. 4.1.4. Link distribution inside semantic communities
Moreover, Flake, Lawrence, Giles, and Coetzee (2002) show that the Web is self-organizing in the sense that Web communities are formed automatically. These communities can be easily identified by considering the link distribution within these pages. It is shown that Web pages have more links to other pages inside their community than to pages that are outside their community.

To give an example, we have created 10 topic specific collections by conducting a Web crawl and in parallel classifying the collected documents into classes using our focused Crawler BINGO ( Sizov et al., 2003 ). The classifier has been trained manually and the resulting collections are strongly related to the topics. Overall, the 10 collections contain 253,875 documents and 2,416,910 links between these pages. Fig. 6 shows the dis-tribution of outgoing links from sport pages. As we can see, sport pages mainly point to other sport pages, an observation that one can support by personal experiences when surfing through the world wide Web. Please note that this skewed link distribution in inherently associated with the WWW.

In the following two sections we describe two applications that can benefit from our semantic overlay networks. 5. SONs for the JXP decentralized authority ranking 5.1. Reviewing the JXP algorithm
JXP ( Parreira &amp; Weikum, 2005 ) is an algorithm for dynamically computing, in a decentralized P2P manner, global authority scores when the Web graph is spread across many autonomous peers. The algorithm runs at every peer and it combines local computations of the standard PageRank algorithm ( Brin &amp; Page, 1998 ) and random meetings between peers in the network, to update peers X  local knowledge. The main idea is as follows. global graph that is not stored at and not known to the peer.
This world node has special features, regarding its own authority score and how it is connected to the local graph (cf. Fig. 7 ). As it represents all the pages that are not stored at the peer, we take all the links from local pages to external pages and make them point to the world node. In the same way, as the peer learns, during the random meetings, about external links that point to one of the local pages, these links are assigned to the world node. Every link from the world node is weighted based on how much of the authority score is received from the original page that owns the link. This gives a better approximation of the total authority score mass that is received from external pages. The world node also contains a self-loop link, representing links between pages that are not stored at the local graph. The authority score of the world node is defined as the sum of the scores of all external pages. After the addition of the world node, the Page-
Rank algorithm is performed on this extended graph and a first approximation to the authority scores of the pages is obtained.

The next step consists of meeting other peers in the network to exchange local knowledge and improve the local scores. Whenever two peers meet, they combine both local and external information. This is done at both peers independently of each other, so that the autonomy of peers and the asynchronous nature of communi-cation and computation in a P2P network is preserved.

Local graphs are combined by simply forming the union between them. Combining the world nodes con-sists of merging their list of outgoing links, removing links that originally come from a page that is already represented in the combined graph, and adjusting the authority score of the combined world node to reflect the sum of scores of pages that do not belong to the combined graph. The new world node that results from this merging is then connected to the combined graph and the PageRank power iteration algorithm is again performed, yielding updated authority scores. The graphs are then disconnected and the local world node is recreated from the combined world node, by keeping only the links that point to a page in the local graph. The authority score of the world node is also recomputed. Everything else is then discarded. Fig. 8 illustrates the process of combining and disconnecting local graphs and world nodes.

The algorithm is scalable, as the algorithm always runs on relative small graphs, independent of the number of peers in the network and the storage requirements are low, as peers do not keep the graphs of the peers they have met.

Experiments have shown that, as a peer learns (through meetings) about other peers in the network, the locally computed authority scores converge to the true global PageRank scores and after a moderate number of meetings, a good approximation to the global PageRank is obtained. 5.2. Benefits from p2pDating
It is clear that, in the JXP algorithm, peers do not contribute with each other in the same way. How much a peer A will benefit from peer B for improving its local authority scores heavily depends on the degree of con-nectivity between their two local graphs and the level of overlap between them. Peers will not gain much from meetings when there are only a few links between the local graphs or if the local graphs are almost identical.
The performance of the algorithm could be improved in the presence of a overlay network where peers are clustered together according to their degree of connectivity (which is an indicator of semantic similarity as well). For creating the semantic overlay network we first need to compute at each peer two sets: one containing its local pages IDs and another containing the IDs of all the successors from all local pages. The network bandwidth consumption is kept small by using min-wise independent permutations vectors for representing these two sets, which makes the messages for transmitting this information small, such that they can be pig-Assuming that Peer A has received information from Peer B , the p2pDating algorithm decides whether Peer pages in Peer A that has inlinks from local pages in Peer B . If the value is above some pre-defined threshold, Peer A adds Peer B in its friend list.

For finding potential candidates for being a friend, the algorithm computes the overlap between the local as well. If the overlap is relatively high, friends of B will be inserted into A  X  X  candidate list.
With the semantic overlay network, the JXP algorithm can identify the best peers to exchange information, instead of choosing peers at random, which leads to fewer meetings to reach a good approximation of the glo-bal authority scores.

To confirm this idea, we performed experiments in which the JXP and the p2pDating algorithms are com-bined such that whenever two peers meet in the network this meeting is used to improve both the JXP scores and the peer X  X  friend and candidate lists. We used two datasets: a collection of pages from the Amazon.com website and a partial crawl of the Web graph. The Amazon data contains information about products (mostly books) offered by Amazon.com. The data was obtained in February 2005, and the graphs were created by con-sidering the products as nodes in the graph. For each product, pointers to similar recommended products are available in the collection. These pointers define the edges in our graphs. Products are also classified into one or more categories. We have thematically grouped together some of the original categories, so in the end we had a total of 10 categories (e.g.,  X  X  X omputers X  X ,  X  X  X cience X  X , etc.).

The Web collection was obtained in January 2005, using the Bingo! focused crawler ( Sizov et al., 2003 ). We first trained the crawler with a manually selected set of pages and after that, new pages were fetched and auto-matically classified into one of 10 pre-defined categories such as  X  X  X ports X  X ,  X  X  X usic X  X , etc.
Pages were assigned to peers by simulating a crawler in each peer, starting with a set of random seeds pages from one of the thematic categories and following the links and fetching nodes in a breadth-first approach, up to a certain predefined depth. The category of a peer is defined as the category to which the initial seeds belong. During the crawling process, when the peer encounters a page that does not belong to its category, it randomly decides to follow links from this page or not with equal probabilities. In both of the two setups we have 100 peers, with 10 peers per category. In the Amazon setup there is a total of 55,196 pages and 237,160 links, and in the Web crawl setup we have 103,591 pages and 1,633,276 links.

For evaluating the performance we compare the authority scores given by the JXP algorithm against the true PageRank scores of pages in the complete collection. Since, in the JXP approach, the pages are distrib-uted among the peers and for the true PageRank computation the complete graph is needed, in order to com-pare the two approaches we construct a total ranking from the distributed scores by essentially merging the score lists from all peers (this is done for the experimental evaluation only). We do this periodically after a fixed number of meetings in the network. Since overlaps are allowed and no synchronization is required, it ranking is considered to be the average over its different scores.
 The total top-k ranking given by the JXP algorithm and the top-k ranking given by traditional, centralized PageRank are compared using Spearman X  X  footrule distance ( Fagin, Kumar, &amp; Sivakumar, 2003 ), defined as
F  X  r ranking. In case a page is present in one of the top-k rankings and does not appear in the other, its position in the latter is considered to be k + 1. Spearman X  X  footrule distance is normalized to obtain values between 0 and 1, with 0 meaning that the rankings are identical, and 1 meaning that the rankings have no pages in common. We also use a linear score error measure, which is defined as the average of the absolute difference between the JXP score and the global PageRank score over the top-k pages in the centralized PageRank ranking.

Figs. 9 and 10 present the performance comparison between the standard JXP algorithm and the combi-nation of the JXP with our p2pDating algorithm, for the Amazon data and the Web crawl, respectively.
For the Web crawl we considered the top-1000 pages, and for the Amazon data we compared the top-10,000 pages.

We can see that during the first meetings both approaches perform similarly, but as the semantic overlay networks are being formed the JXP algorithm is able to find the most promising peers, reducing the number of meetings needed for a good approximation to the global PageRank scores. For instance, in the Amazon data, to make the footrule distance drop below 0.2 we needed a total of 1770 meetings without the SONs. With the
SONs this number was reduced to 1250. In the Web crawl setup, for a footrule distance of 0.1, the number of meetings was reduced from 2480 to 1650.
It becomes clear the advantage of constructing semantic overlay network to improve the convergence speed of the JXP algorithm. This also has a big impact on the network load. By finding the most promising peers, many meetings with peers that would contribute only little useful information are avoided. 6. SONs for Minerva 6.1. Revisiting the Minerva System Design
We briefly introduce Minerva, 3 a fully operational distributed search engine that we have implemented and that serves as a valuable testbed for our work ( Bender, Michel, Weikum, &amp; Zimmer, 2005a, 2005b ). It assumes a P2P collaboration in which every peer is autonomous and has a local index that can be built from the peer X  X  own crawls or imported from external sources and tailored to the user X  X  thematic interest profile. The index contains inverted lists with URLs for Web pages that contain specific keywords.
 A conceptually global but physically distributed directory, which is layered on top of a Chord-style
Dynamic Hash Table (DHT), holds compact, aggregated information about the peers X  local indexes and only to the extent that the individual peers are willing to disclose. Minerva only uses the most basic DHT function-such that every peer is responsible for a randomized subset of terms within the global directory. For failure resilience and availability, the entry for a term may be replicated across multiple peers.

Directory maintenance, query routing, and query processing work as follows (cf. Fig. 11 ). In a preliminary function is applied to the term in order to determine the peer currently responsible for this term. This peer maintains a PeerList of all postings for this term from peers across the network. Posts contain contact infor-mation about the peer who posted this summary together with statistics to calculate IR-style measures for a term (e.g., the size of the inverted list for the term, the maximum average score among the term X  X  inverted list entries, or some other statistical measure). These statistics are used to support the query routing process, i.e., determining the most promising peers for a particular query using database selection methods ( Bender et al., 2005a ) from distributed IR ( Callan, 2000 ).

This step, called query routing , yields a number of promising peers for the complete query. Subsequently, the query is forwarded to these peers and executed based on their local indexes ( query execution ; step 2). Note that this communication is done in a pairwise point-to-point manner between the peers, allowing efficient com-munication and limiting the load on the global directory. Finally, the results from the various peers are com-have to retrieve the complete PeerLists. Instead, it can run a distributed top-k algorithm to efficiently figure out the k most promising peers. 6.2. Benefits from p2pDating
The proposed techniques to created semantic overlay networks can be easily integrated into Minerva as most of the information can be piggybacked onto the existing communication.

Random peer dates can be implemented by conducting a random lookup executing a random query. This can be implemented as a background process or be combined with the tradi-tional query processing. 6.2.1. Firework query routing
As described above, the query routing in Minerva uses standard IR techniques to figure out the most prom-ising peers for a given user query. The selected peers are then queried by sending to them the complete query that will then be locally executed at the peers X  collections. In our approach of semantic overlay networks, each peer is considered to have a logic neighborhood consisting of high quality and semantically related peers that can deliver reasonably good answers. The proposed notion of a Firework Query Model ( Ng, Sia, &amp; King, 2003 ), seems to be excellently suitable here. We can use Minerva X  X  query routing process to find a few suitable peers for a given query and, subsequently, forward the query to their neighborhood. The query is routed straight into a small, but high quality community established by the primarily selected peer(s). In contrast to the work presented in Ng et al. (2003) , we do not have to rely on random links to find the appropriate peer cluster, as we can use Minerva for that issue. This decreases the network resource consumption, as only a small number of lookups/hops are needed to determine appropriate peers inside such a semantic cluster. 6.2.2. Increased query-result quality through noise reduction
Assume a local query execution strategy that ranks local documents using a TFIDF-style scoring function, as in many of today X  X  P2P Web search approaches. As a document X  X  TFIDF score for a particular query depends inversely on the df values for the query terms (i.e., the higher the df values, the lower a document X  X  score), documents from peers with very few documents for at least one query term receive unjustified high local score values. If the result merging process uses these local scores from different peers, the scores are inherently incomparable, as documents from peers with low df values have received high local score values used logarithmic dampening) differences for low absolute df values have a higher impact on the idf -subscore than the same difference for higher values. Thus, peers with very low df values tend to place their few results into the top portion of the final result list. The benefit from p2pDating is that peers from outside a community are usually not considered during query routing, thus they do not contribute with low-quality documents that have an unjustified high score, and thus do not mess up the final document ranking that is presented to the user. 6.2.3. Pseudo-relevance feedback
Recently, we have worked on pseudo-relevance feedback in the context P2P Web Search ( Chernov, Serdyu-kov, Bender, Michel, &amp; Weikum, 2005), where the query is first executed at the most promising peer that returns query results, and an expanded query created by pseudo-relevance feedback using its collection. Sub-sequently, the query initiator can forward the expanded query to the other promising peers, or can conduct an additional query routing step, thus using statistics for the new query terms that have not been retrieved earlier.
Using SONs and the above discussed  X  X  X irework approach X  X  we can easily employ pseudo-relevance feed-back: the most promising peer receives the query, expands it, and forwards it to the peers in its neighborhood that return the query results directly to the query initiator. 6.2.4. Caching and Pro-Active dissemination
To further enhance query efficiency, statistical summaries and also the query results may be cached within a semantic cluster in a way that allows other peers not only to instantly benefit from the existing query results but also to benefit from click streams that were recorded on the occasion of similar queries. Statistical sum-maries may also be disseminated pro-actively among thematically related peers. Inside a semantic community, peers could use the cached information together with the disseminated statistics to find appropriate peers, without stressing the global Minerva directory.

Moreover, using a hybrid routing strategy that uses the Minerva directory, and the semantic routing table (cf. Fig. 2 ) seems to be a promising approach that combines the robustness of the Minerva directory with the, for thematically related queries, high accuracy of peers within a semantic community. The lazily, or pro-actively learned information about friends evolves over time and it is self adapting to the peers X  peculiarities whereas the Minerva directory is rather static as it relies on the published statistics, and thus, might be influ-enced by malicious peers posting invalid statistics about their local content. 6.2.5. Decrease storage load
In addition, Minerva can benefit from semantic classification of the peers X  content if we restrict the post process to the terms that are related to the peers X  topics.

This reduces the number of overall posts and thus increases system performance and decreases the overall storage consumption and computational load. 6.2.6. Query routing inside SONs: overlap awareness for free
In Minerva we support different collection selection strategies where quality is measured in terms of an IR relevance metric. One of the most popular approaches, CORI ( Callan, Lu, &amp; Croft, 1995; Callan, 2000 ), com-putes the collection score s i of the i th peer with regard to a query Q ={ t s
The computations of T i , t and I i , t use the number of peers in the system, denoted np , and the document fre-quency ( cdf ) of term t in collection i for any term t in collection i : where the collection frequency cf t is the number of peers that contain the term t . The value a is chosen as a = 0.4 ( Callan et al., 1995 ).
 holds in its local index) and the average term space size j V the absence of global knowledge, we replace j V avg j (as defined in Callan, 2000 ) by the average term space size over all peers that contain term t as this information is available from the distributed metadata directory during query routing.

In practice, it is difficult to compute the average term space size over all peers in the system (regardless of whether they contain query term t or not). We approximate this value by the average over all collections found in the PeerLists (cf. Section 6 .1).

Existing collection selection approaches taking into account only the expected result quality of a collection will inevitably lose some of their power in a P2P setting. For example, consider a scenario where two peers with high interest in current affairs have (independently of each other) crawled large fractions of a popular news site, such as cnn.com. A traditional approach to collection selection is likely to rank both peers high given a related query, even though their results may highly overlap. Because the second peer is likely to not add many new documents to the query result, the performance of the query can be improved if another, complementary peer is chosen instead.

In Bender et al. (2005), Michel et al. (2006) we have presented an overlap aware routing technique that uses published data synopses to identify overlapping resources. The idea is to have two components, a quality based, and a overlap based measure. The combined value is denoted as usefulness. We envision the overlap statistics being published along with the quality descriptions (a.k.a posts). During query routing, as described above, the peer retrieves the published statistics for the involved query terms and tries to identify the most promising peers, i.e., the overlap statistics have to be transferred. Using our approach of semantic overlay net-works we get the overlap awareness for free since the overlay has been built up partially according to the mutual overlap. This decreases the directory load and also the overall network load as peers do not have to retrieve the data synopses of peers they are not at all interested in. The synopses exchange inside semantic communities consumes only a small fraction of the total network consumption. 6.3. Experiments 6.3.1. Collection and queries
For experiments on real Web data we have created 10 topically focused collections using a focused crawler (see Table 1 ). Subsequently, we have created 40 peers out of the 10 collections by splitting each collection into 4 fragments. Each of the 40 peers hosts 3 out of 4 fragments from the same topic, thus forming high overlap among same-topic peers. To evaluate our approach we have used 30 popular Google queries taken from Zeit-geist; 5 they are shown in Table 2 . For every number of queried peers we compare the documents obtained so far to the top-k documents derived from a global collection formed by the union of all collections. This pro-vides us with a relative recall measure that we use to assess the search result quality. 6.3.2. Routing strategies and different measures of defining friends
To assess the query routing inside the semantic overlay network, we have build different semantic overlay networks by p2p meetings (as described above) where the quality of peers is assessed in different ways: an over-lay measure, a similarity measure, and hybrid measures. We consider the suitability of the SON structures by on the metadata directory of Minerva:
CORI : This is the collection selection strategy as proposed in Callan et al. (1995), Callan (2000) (cf. Section 6.2.6 for more details).

Overlap-aware CORI : This strategy uses the technique we have presented in Michel et al. (2006) . We use a combination of a quality based query routing strategy with an overlap prediction method to form a selec-tion strategy that reflects the relative usefulness of a peer w.r.t. the query initiating peer. The strategy that we employ here is based on (i) the collection selection strategy CORI ( Callan et al., 1995; Callan, 2000 ), and (ii) an overlap estimator based on min-wise independent permutations (MIPs). Section 4.1.2 presents MIPs in more detail.

There are different ways to define what a good friends is. Here, we consider the following usefulness measures: Overlap Only Here we consider only the overlap between peers.
 where A is the query originator.
 Similarity Only
Here we consider only the similarity between peers. This measure uses the Kullback X  X eibler distance ( Kull-back, 1959 ) to assess the semantic similarity, based on the term occurrence distributions.
 where A is the query originator.
 Overlap * Similarity This is a combination of overlap and similarity.
 where A is the query originator.
 Weighted Sum This is a weighted sum of the overlap and the similarity measure.
 where A is the query originator. 6.3.3. Experimental results
Fig. 12 shows the relative recall for the above mentioned routing strategies and overlay networks. As expected, the two metadata directory based strategies offer the best result quality. In particular, the overlap-aware CORI strategy outperforms standard CORI due to the presence of overlap between peers with similar content. Furthermore, query routing in SONs that have been created using a similarity measure or a weighted sum of similarity and overlap measures provide very good result quality, whereas the overlap-only measure creates overlay networks that are inappropriate for query routing. The product of similarity and overlap as a measure of usefulness seems to be inappropriate too. In our setup, this can be explained with the skewed distribution of mutual overlap. While thematically similar peers have highly overlapping collec-tions there is no overlap between peers that have different topics. We have also conducted experiments for different values of the coefficient a in the weighted sum of similarity and overlap; these showed comparable behavior and are thus omitted.

Table 3 shows the total number of messages, and transferred bytes at query execution time for the complete benchmark of 30 Zeitgeist queries. As suggested in Wang, Min, Zhu, and Hu (2002) we consider the average
URL length to be 70 bytes. The number of messages consists of the messages to retrieve the published meta-data (if needed), and the messages to actually execute the query by sending it to the selected peers. As the SON based routing does not use the metadata directory, there are only 150 messages required, as we consider the top-5 peers for 30 queries, whereas the directory based strategies involve 57 additional messages to retrieve the published per-term peer lists. Network traffic consists of the number of bytes for sending the query (on average 2 terms each of length 10 bytes), and retrieving the top-20 result URLs plus scores (floating point numbers) from the queried peers. This is the communication cost for the query execution. In addition, the directory peers cause network traffic when retrieving the peer lists from the metadata directory. Whereas CORI uses only the standard statistical information along with peer information (IP address and port), the overlap aware
CORI strategy additionally requires the MIP vectors (cf. Section 4.1.2 ) to assess the overlap. In our experi-ment we use 64 permutations since it has been shown in prior experiments that 64 permutations already pro-vide a reasonably good approximation of the desired overlap measures.

In summary, we have observed that query routing based on semantic overlay networks offers a excellent ratio between result quality and communication overhead. 7. Discussion and outlook
In this work we have presented an approach to create and maintain semantic overlay networks based on the notion of p2pDating, where peers maintain information about their friends to form a semantic network. Friends are chosen based on a variety of usefulness estimators, like overlap and semantic similarity. We have shown how we can leverage these friends networks in JXP and Minerva. In particular, the experiment results that we presented have shown the feasibility and the benefits of integrating JXP and Minerva with the p2pDat-ing algorithm.

Ongoing work includes more experiments with different Web-data collections. With Minerva we want to compare the traditional query routing approaches to a hybrid approach that is based on Minerva X  X  metadata directory and on the information given from the semantic-overlay-network. Ultimately, we want to refine the ingful cost measure, however, is a challenging issue. While there are techniques for observing and inferring network bandwidth or other infrastructural information, expected response times, depending on the current system load, change over time. One approach is to create a distributed Quality-of-Service directory that, for example, holds moving averages of recent peer response times.
 References
