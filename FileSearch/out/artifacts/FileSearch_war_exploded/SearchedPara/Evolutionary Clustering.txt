 We consider the problem of clustering data over time. An evolutionary clustering should simultaneously optimize two potentially conflicting criteria: first, the clustering at any point in time should remain faithful to the current data as much as possible; and second, the clustering should not shift dramatically from one timestep to the next. We present a generic framework for this problem, and discuss evolutionary versions of two widely-used clustering algorithms within this framework: k -means and agglomerative hierarchical cluster-ing. We extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously at-tain both high accuracy in capturing today X  X  data, and high fidelity in reflecting yesterday X  X  clustering.
 Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval ]: Information Search and Re-trieval General Terms: Algorithms, Experimentation, Measure-ments Keywords: Clustering, Temporal Evolution, Agglomera-tive, k -means
Evolutionary clustering is the problem of processing times-tamped data to produce a sequence of clusterings; that is, a clustering for each timestep of the system. Each clustering in the sequence should be similar to the clustering at the previous timestep, and should accurately reflect the data arriving during that timestep.

The primary setting for this problem is the following. Ev-ery day, new data arrives for the day, and must be incorpo-rated into a clustering. If the data does not deviate from historical expectations, the clustering should be  X  X lose X  to that from the previous day, providing the user with a famil-iar view of the new data. However, if the structure of the data changes significantly, the clustering must be modified to reflect the new structure. Thus, the clustering algorithm Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. must trade off the benefit of maintaining a consistent clus-tering over time with the cost of deviating from an accurate representation of the current data.

The benefits of evolutionary clustering compared to tra-ditional clustering appear in situations in which the current (say, daily) clustering is being consumed regularly by a user or system. In such a setting, evolutionary clustering is useful for the following reasons: (1) Consistency: A user will find each day X  X  clustering familiar, and so will not be required to learn a completely new way of segmenting data. Similarly, any insights derived from a study of previous clusters are more likely to apply to future clusters. (2) Noise removal: Providing a high-quality and histori-cally consistent clustering provides greater robustness against noise by taking previous data points into effect. As we de-scribe later, our method subsumes standard approaches to windowing and moving averages. (3) Smoothing: If the true clusters shift over time, evo-lutionary clustering will naturally present the user with a smooth view of the transition. (4) Cluster correspondence: As a side effect of our frame-work, it is generally possible to place today X  X  clusters in correspondence with yesterday X  X  clusters. Thus, even if the clustering has shifted, the user will still be situated within the historical context.
 Overview of framework. Formally, let C t be the cluster-ing produced by the algorithm for data arriving at timestep i .The snapshot quality of C t measures how well C t repre-sents the data at timestep t .The history cost of the clus-tering is a measure of the distance between C t and C t  X  1 the clustering used during the previous timestep. Typically, the snapshot quality is defined in terms of the data points themselves, while the history cost is a function of the clus-ter models. The overall cost of the clustering sequence is a combination of the snapshot quality and the history cost at each timestep.

For intuition, we consider an extreme example to show why the introduction of history cost may have a signifi-cant impact on the clustering sequence. Consider a data set in which either of two features may be used to split the data into two clusters: feature A and feature B. Each fea-ture induces an orth ogonal split of the data, an d each split is equally good. However, on odd-numbered days, feature A provides a slightly better split, while on even-numbered days, feature B is better. The optimal clustering on each day will shift radically from the previous day, while a consistent clustering using either feature will perform arbitrarily close to optimal. In this case, a clustering algorithm that does not take history into account will produce a poor clustering sequence.

This approach may be contrasted to incremental cluster-ing , in which a model is incrementally updated as new data arrive, primarily to avoid the cost of storing all historical similarities [9]. In evolutionary clustering, however, the fo-cus is upon optimizing a new quality measure which incor-porates deviation from history.
 Summary of Contributions. We consider two classical clustering algorithms and extend them to the evolutionary setting: (1) the traditional k -means algorithm that provides a flat clustering of points in a vector space, and (2) a bottom-up agglomerative hierarchical clustering algorithm. These represent two major categories of clustering methods and we chose to use them to demonstrate the generality of our framework. These algorithms are applied to a large evolv-ing dataset of user-tags placed on images from flickr.com tracked over 68 weeks. Our experiments demonstrate all the advantages of evolutionary clustering mentioned previ-ously, namely, stable and consistent clusterings are obtained even when the data exhibits noisy behavior, and a smooth sequence of clusters with very low history cost can be ob-tained for only a small reduction in total snapshot quality.
Our framework for evolutionary clustering is in fact quite general. With suitable definitions of history cost and snap-shot quality, many other static clustering algorithms can now be extended to perform evolutionary clustering under our framework. Furthermore, we focused on a setting in which today X  X  data must be clustered before tomorrow X  X  data is available. However, there are other natural settings. For example, the entire sequence may be available to the algorithm at once, but the algorithm must again produce a sequence of clusterings that accurately reflect the data at each timestep while shifting as gently as possible over time. This formulation applies when the data should be clustered retroactively but interpretability across time is an important consideration. Similarly, there are settings in which the nu-merical variable over which we track evolution is not time; for example, we may cluster products by dollar value, and ask that the clusterings for similar price buckets be as simi-lar as possible. Our formulation can capture such directions as well, but we will not explore them in this paper.
Clustering is a well-studied problem; see, for instance [16, 8]. However, to the best of our knowledge, evolutionary clustering has not been considered before.

In the following, we briefly review the interplay of time and notions related to clustering, including classification and online topic detection and tracking.

Temporal aspects have been considered in some classi-fication problems. Some work in online machine learning considers learning tasks such as classification in which the model evolves over time, and the algorithm is penalized for both mistakes and shifts in the model from one timestep to the next [11, 3]. However, it is unclear how this could be used in the unsupervised learning setting.

Temporal aspects have also been considered in online doc-ument clustering setting. Online document clustering ap-plies clustering sequentially to a time series of text docu-ments, in order to detect novelty in the form of certain types of outliers [4]. Zhang et al. [18] proposed a probabilistic model for online document clustering, where the emphasis is again on detecting novelty. Clustering was also used in automatic techniques for discovering and retrieving topically related material in data streams, and to detect novel events from a temporally ordered collection of news stories [17, 2]. However, the main goal of topic detection and tracking is to detect new events in a time-line using methods such as clustering, and not to produce clusterings that incorporate history into the clustering objective.

In some data stream applications, time has played a dif-ferent role with respect to clustering. Aggarwal et al. [1] study the problem of clustering data streams at different time horizons using an online statistical aggregation stage and then an offline clustering stage. For more details on clustering from a data stream analysis perspective, see [10].
The notion of clustering time-series has been considered in statistics, data mining, and machine learning. Tempo-ral correlation is perhaps the best-known approach to time-series similarity [5]. Smyth [14] considers general cluster-ing of sequence data using a probabilistic approach based on hidden Markov models; see also [12]. Immorlica and Chien [6] propose a low-dimensional representation of time-series for clustering. They use a variety of basis functions in-cluding piecewise-constant, piecewise-linear, and piecewise-aggregate approximations. Vlachos et al. propose a Fourier approach to this problem [15]. Our work, however, has much broader scope; we must consider object feature similarity in addition to the similarities in their time series, as will be explained in the next section.
In this section, we present our framework for evolutionary clustering. We begin by differentiating between algorithms that must cluster data as it arrives, versus algorithms that have access to the entire time series before beginning work. An evolutionary clustering algorithm is online if it must provide a clustering of the data during timestep t before seeing any data for timestep t +1. If the algorithm has access to all data beforehand, it is offline . Offline algorithms may  X  X ee the future, X  and should perform at least as well as their online counterparts. However, the online setting is arguably more important for real-world applications, and there are no natural, efficient offline algorithms that may be easily employed as lower bounds. We therefore leave the offline problem for future work, and focus on the online version.
Recall that an evolutionary clustering algorithm must pro-duce a sequence of clusterings, one for each timestep. The clustering produced during a particular timestep should mea-sure well along two distinct criteria: it should have low history cost, meaning it should be similar to the previous clustering in the sequence, and it should have high snapshot quality, meaning it should be a high-quality clustering of the data that arrived during the current timestep. The snapshot quality simply reflects the underlying figure of merit driving a particular clustering algorithm in a non-evolutionary set-ting. The history cost, however, must allow a comparison of clusterings, in order to determine how much the later one has shifted from the earlier. This comparison must address the issue that some data will appear for the first time in the later clustering, while some data will be seen in the earlier clus-tering but not the later one, and so forth. There are many examples of measures for comparing clusterings  X  see [13] for a discussion of the complexities that arise even in the case of flat clusterings. But this comparison may be more sophisticated than simply a comparison of two partitions of the universe. The comparison may occur at the data level, for instance by comparing how similar pairs of data objects are clustered, or at the model level, for instance by compar-ing the best matching between two sets of centroids in the k -means setting. In our setting, comparisons at the model level make the most intuitive sense, and this is what we use.
First, we require a few high-level definitions. Let U = { 1 ,...,n } be the universe of objects to be clustered. Let U t  X  X  be the set of all objects present at timestep t ,and let U  X  t =  X  t  X  t U t be the set of all objects present in any timestep up to and including step t .

An evolutionary clustering algorithm must behave as fol-lows. At each timestep t , it should produce a clustering C U  X  t , the objects seen by the algorithm so far. The distance from C t to C t  X  1 will be evaluated with respect to U  X  t  X  1 order to determine the historical accuracy of C t .Specifi-cally, if new data arrived for the first time during timestep t , the clustering will not be penalized for deviating from the previous clustering with respect to the new data unless this deviation also impacts the clustering of historical data. The history cost is computed by projecting C t onto U  X  t  X  1
At the same time, the snapshot quality of C t will be eval-uatedwithrespectto U t , the objects that actually appear during step t . These two measures, over all timesteps, will provide an overall evaluation of the entire cluster sequence.
Observe that, in order to perform well, clustering C t must include objects in U  X  t \ U t , that is, the objects that have been seen in the past but have not appeared during the current timestep. So either implicitly or explicitly, an evolutionary clustering algorithm must  X  X arry along X  information about the appropriate location of historical information.
Recall that U = { 1 ,...,n } is the universe of objects to be clustered. At each timestep t where 1  X  t  X  T ,anew set of data arrives to be clustered. We assume that this data can be represented as an n  X  n matrix M t that ex-presses the relationship between each pair of data objects. The relationship expressed by M t can be either based on similarity or based on distance depending on the require-ments of the particular underlying clustering algorithm. If the algorithm requires similarities (resp., distances), we will write sim( i, j, t ) (resp., dist( i, j, t )) to represent the similar-ity (resp., distance) between objects i and j at time t .
At each timestep t , an online evolutionary clustering algo-rithm is presented with a new matrix M t , either sim(  X  , or dist(  X  ,  X  ,t ), and must produce C t , the clustering for time t , based on the new matrix and the history so far.
A user of the framework must specify a snapshot quality function sq( C t ,M t ) that returns the quality of the clustering C t at time t with respect to M t . The user must also provide a history cost function hc( C t  X  1 ,C t ) that returns the historical cost of the clustering at time t . The total quality of a cluster sequence is then where the  X  X hange parameter X  cp &gt; 0 is a user-defined pa-rameter which trades-off between the two. As cp increases, more and more weight is placed on matching the historical clusters.

Given this framework, an evolutionary clustering algo-rithm takes as input M 1 ,...,M t and produces C 1 ,...,C We can compute the quality of the resulting cluster se-quence, and hence we may compare evolutionary cluster-ing algorithms and speak of optimal algorithms and optimal quality.
 As we stated earlier, our focus is on the online setting. Clearly a sequence of online decisions to find C 1 ,...,C not be the best offline solution to (1), but nevertheless, it is a defensible alternative given no knowledge about the future data. Our algorithms therefore try to find an optimal cluster sequence by finding at each timestep t a clustering C t that optimizes the incremental quality
In a traditional clustering setting, the data objects are all available at once to be clustered, and some measure of sim-ilarity or distance between objects may be applied. This is also true in our setting: all other timesteps may be ignored, and local information may be used to compute similarity be-tween objects during a particular timestep. However, there is also another type of similarity which is unique to our set-ting  X  temporal similarity. If objects recur over time, the algorithm to cluster them during a particular timestep may make use of their historical occurrence patterns. This type of similarity should not be confused with similarity between time series, as in a time series clustering problem: the ob-ject in our universe is the point of the time series, rather than the series itself. Below, we describe these, and how to combine them to form the input matrix M t .
 Local information similarity. In some cases, the input to evolutionary clustering might already be in the form of an inter-object similarity matrix S t ; in other cases, we must infer it. One common scenario involves input as a graph, for example, a bipartite graph linking the data objects of interest to a set of features. The n, similarity between objects is related to the number of features they share.
 Let B ( t ) represent this bipartite graph at time t ,andlet R ( t )= r j,j encode the number of features shared by both objects j and j . Then, we can say However, notice that a similarity measure based purely on R ( t ) is  X  X emoryless, X  i.e., if an object fails to appear on one snapshot, all information about its similarities with other objects is lost completely. This is not desirable since we wish to capture history to some extent. Therefore, we use where the parameter  X  is chosen to retain enough infor-mation on relative similarities of old objects with all other objects, but not swamp the data from the current snapshot. In our experiments, we set  X  =0 . 1.

Observe that this approach incorporates an exponentially decaying moving average of data, and could easily be mod-ified to support other windowing techniques.

Now, we can use cosine similarity between objects to gen-erate the local similarity matrix S t : Similar steps could be followed if the underlying clustering algorithm required a distance matrix instead of a similarity matrix.
 Temporal similarity. This is given by the standard defi-nition of correlation of the two time series up to and includ-ing time t 0 : where x i,t represents the number of occurrences of data ob-ject i in timestep t , and the means and variances are defined Total similarity. We combine these two types of similar-ity information into the final similarity matrix M t ,taking the overall similarity between two objects at time t to be where  X  controls the contribution of correlation and tempo-ral similarities.
We now present two instantiations of our framework. Sec-tion 4.1 describes an evolutionary version of the bottom-up agglomerative hierarchical clustering algorithm and Sec-tion 4.2 discusses an evolutionary version of the traditional k -means. The above two choices were motivated by the sig-nificant differences between the underlying clustering algo-rithms: k -means produces a flat rather than a hierarchi-cal clustering, implicitly requires the data to lie in a vector space, and creates a model based on pseudo-objects that lie in the same space as the actual objects being clustered. These two very different approaches thus show the generality of our framework.
To develop an evolutionary hierarchical clustering, we first describe a standard agglomerative clustering at a particular fixed timestep t .Let M = M t =sim(  X  ,  X  ,t ) ,U = U  X  t we select the pair i, j of objects that maximizes M ( i, j ). Next, we merge these two objects, creating a new object; we also update the similarity matrix M by replacing the rows and columns corresponding to objects i and j by their average that represents the new object. We then repeat the procedure, building a bottom-up binary tree T whose leaves are the objects in U ;thetree C t = T t = T represents the clustering of the objects at timestep t .

Let the internal nodes of T be labeled m 1 ,...,m | U | X  1 and let sim M ( m i ) represent the similarity of objects that were merged to produce the internal node m i .Letin( T )be the set of all internal nodes of T . For an internal node m , let m be the left child of m , m r be the right child of m ,and leaf ( m ) be the set of leaves in the subtree rooted at m .Let d ( i, j ) be the tree distance in T between nodes i and j .If T ,T are binary trees with leaf( T )  X  leaf ( T ), then the tree T |
T is the projection of T onto T , i.e., T | T is a binary tree obtained by first removing all leaves in leaf( T ) \ leaf ( T )and then collapsing all unary internal nodes.

We define the snapshot quality of T to be the sum of the qualities of all merges performed to create T :
We now define the history cost, which is the distance be-tween two trees T,T with leaf( T )  X  leaf( T ). First, we define the distance between objects i, j  X  leaf( T )tobethe squared-error distance: Then, the distance between T and T is defined to be the average distance between all pairs of objects As stated earlier, the goal is to find a clustering C t that minimizes (2). To do this, we first note that (4) can be rewritten as a sum of contributions from each internal node, where the contribution covers all pairs of points for whom that internal node is the least common ancestor. Thus, Using this reformulation of history cost, we may write the incremental quality in (2) as
X We propose four greedy heuristics to choose the order of merges. Let T = T t and T = T t  X  1 .

In the first heuristic , we choose the merge m whose con-tribution to this expression is maximal. In other words, pick the merge m that maximizes We refer to this heuristic as Squared , since it greedily mini-mizes the squared error in Equation 3.

However, we observe that a merg e with a particular squared errormaybecomebetterorworseifitisputoffuntillater. To wit, if two objects are far away in T , then perhaps we should delay the merge until they are similarly far away in T . However, if two objects are close in T but merging them would already make them far in T then we should encour-age the merge despite their high cost, as delaying will only make things worse. Based on this observation, we consider the cost of merge based on what would change if we de-layed the merge until the two merged subtrees became more distant from one another (due to intermediate merges). Thus, consider a possible merge of subtrees S 1 and S 2 . Performing the merge incurs a penalty for nodes that are still too close, and a benefit for nodes that are already too far apart. The benefit and penalty are expressed in terms of the change in cost if either S 1 or S 2 participates in another merge, and hence the elements of S 1 and S 2 in-crease their average distance by 1. This penalty may be written by taking the partial derivative of the squared cost with respect to the distance of an element to the root. At any point in the execution of the algorithm at time t ,let root( i ) be the root of the curre nt subtree containing i .For i  X  S 1 and j  X  S 2 ,let d m T ( i, j )bethe merge distance of i and j at time t , i.e., d m T ( i, j ) is the distance between i and j at time t if S 1 and S 2 are merged together. Then, d ( i, j )= d T ( i, root( i )) + d T ( j, root( j )) + 2 The benefit of merging now is given by: sim M ( m )  X  We refer to this heuristic as Linear-Internal .Noticethat, as desired, the benefit is positive when the distance in T is large, and negative otherwise. Similarly, the magnitude of the penalty depends on the derivative of the squared error (Equation 3).

As another heuristic, we may also observe that our deci-sion about merging S 1 with S 2 may also depend on objects that do not belong to either subtree. Assume that elements of S 1 are already too far apart from some subtree S 3 .Then merging S 1 with S 2 may introduce additional costs down-stream that are not apparent without looking outside the potential merge set. In order to address this problem, we modify (6) to penalize a merge if it increases the distance gap (i.e., the distance at time t versus the distance at time t  X  1) between elements that participate in the merge and el-ements that do not. Similarly, we give a benefit to a merge if it decreases the distance gap between elements in the merge and elements not in the merge. The joint formulation is then as follows: This heuristic considers the internal cost of merging ele-ments i  X  S 1 and j  X  S 2 ,andthe external cost of merging elements i  X  S 1  X  S 2 and j  X  S 1  X  S 2 ; therefore, we refer to it as Linear-Both . For completeness, we also consider the external cost alone: We refer to this final heuristic as Linear-External .
Let the objects to be clustered be normalized to unit vec-tors in the Euclidean space, i.e., the objects at time t are given by U t = { x 1 ,t ,... } where each x i,t  X  and the dis-tance matrix M t ( i, j )=dist( i, j, t )= || x i,t  X  x j,t instance, [7].)
We begin with a description of the traditional k -means algorithm. Let t be a fixed timestep and let U = U  X  t ,x x i,t ,M = M t . The algorithm begins with a set of k cluster centroids, c 1 ,...,c k ,with c i  X  ;thesecentroidscanbe initialized either randomly, or by using the results of the previous clustering C t  X  1 (which is exactly  X  X ncremental k -means X ). Let closest( j )bethesetofallpointsthatare closest to centroid c j , i.e., The algorithm proceeds during several passes, during each of which it updates each centroid based on the data elements currently assigned to that centroid: after which c j is normalized to have unit length. The al-gorithm terminates after sufficiently many passes and the clustering C t = C is given by the set { c 1 ,...,c k } of k cen-troids.

We define the snapshot quality of a k -means clustering to be (Since all points are on the unit sphere, distances are bounded above by 1.)
We define the history cost, i.e., the distance between two clusterings, to be where f is a function that maps centroids of C to centroids of C . That is, the distance between two clusterings is com-puted by matching each centroid in C toacentroidin C in the best possible way, and then adding the distances for these matches.

As stated earlier, we use a greedy approximation algo-rithm to choose the next cluster in the sequence. How-ever, in the case of k -means, the greedy algorithm becomes particularly easy. At time t , for a current centroid c t c | closest( j ) | be the number of points belonging to cluster j at time t ;let n t  X  1 f ( j ) be the corresponding number for c Let  X  = n t j / In words, the new centroid c t j lies in between the centroid suggested by non-evolutionary k -means and its closest match from the previous timestep, weighted by the cp and the rel-ative sizes of these two clusters. Again, this is normalized to unit length, and we continue with the usual k -means it-erations.
In this Section, we perform an extensive study of our al-gorithms under different parameter settings. We show how distance from history can be reduced significantly while still maintaining very high snapshot quality. For our experi-ments, we use the collection of timestamped photo X  X ag pairs from flickr.com indicating that at a given time, a certain tag was placed on a photo. A bipartite tag-photo graph is formed for each week, and two tags are considered to be sim-ilar if they co-occur on the same photo at the same timestep, as described before in Section 3. Our goal is to apply evo-lutionary clustering algorithms to this space of tags. k -means clustering over time. For this experiment, we selected the most commonly occurring 5000 tags that in the Flickr data and proceeded to study their clustering. We ran k -means with k = 10 centroids over time t =0 ... 67, for several values of cp. Recall that cp = 0 is exactly the same as applying k -means independently to each snapshot, but with the clusters found in the previous step as the starting seed; it is  X  X ncremental k -means, X  in other words. Figure 1 shows the results. We observe the following: Both the snapshot quality and the distance from history de-crease as cp increases. In fact, incremental k -means (cp = 0) gives the best snapshot quality and worst distance from his-tory. This is to be expected since clustering each snapshot independently should give the best quality performance, but at the cost of high distance from history. Also, even low val-ues of cp lower the distance from history significantly. For example, even when cp is as low as 0.125, k -means incorpo-rates history very well, which results in a significant drop in distance from history.
 Agglomerative clustering over time. We empirically find that Linear-Both and Linear-Internal significantly out-perform both Linear-External and Squared, so in Figure 2, we plot only the performance of Linear-Both and Linear-Internal over the top 2000 tags. The plots for Linear-Both are smoother than those for Linear-Internal, for all values of the change parameter cp. This demonstrates that the ex-tra processing for Linear-Both improves the cluster tracking ability of the algorithm. Also note that the distance from history plot shows very high values for a few timesteps. We suspect this is due to increased activity during that time-frame; that was when Flickr  X  X ook off. X  Note that this peak also appears during k -means clustering (Figure 1(b)), rein-forcing the idea that this is an artifact of the data. Effect of cp on snapshot quality. Figure 3(a,b) shows the dependence of snapshot quality on cp. The snapshot quality values at time t are normalized by the corresponding value for cp = 0 to remove the effects of any artifacts in the data itself. We observe that the snapshot quality is inversely related to cp. I.e., higher the cp, more the weight assigned to the distance from history, and thus worse the performance on snapshot quality.

However, while the snapshot quality decreases linearly and is well-behaved as a function of cp for k -means, the situa-tion is different for agglomerative clustering. The snapshot quality takes a hit as soon as history is incorporated even a little bit, but the degradation after that is gentler. This suggests that k -means can accommodate more of history without compromising the snapshot quality.
 Effect of cp on distance from history. Figure 3(c,d) shows the dependence of distance from history on the change parameter cp. The y-axis values are normalized by the cor-responding value for cp = 0 at that timestep to remove any data artifacts. We see that the distance from history is inversely related with cp. I.e., as the value of cp is in-creased, our algorithms weigh the distance higher, and re-ducing the distance from history becomes relatively more important than increasing snapshot quality. Thus, higher cp leads to lower distance from history.

While k -means gets closer to history for small values of cp, the situation is more dramatic with agglomerative cluster-ing. Even values of cp as small as 0.05 reduce the distance from history in a dramatic fashion. This suggests that the agglomerative clust ering algorithm is easily i nfluenced by history.
We considered the problem of clustering data over time andproposedanevolutionaryclusteringframework. This framework requires that the clustering at any point in time should be of high quality while ensuring that the clustering does not change dramatically from one timestep to the next. We presented two instantiations of this framework: k -means and agglomerative hierarchical clustering. Our experiments on Flickr tags showed that these algorithms have the desired properties  X  obtaining a solution that balances both the current and historical behavior of data.

It will be interesting to study this framework for a larger family of clustering algorithms. It will also be interesting to investigate tree-based clustering algorithms that construct non-binary and weighted trees. those of Linear-Internal.
