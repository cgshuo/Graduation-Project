 can be defined to impose various sparsity patterns.
 While most recent work has focused on studying the properties of simple well-known norms, we design a norm that will enforce it? the different weight vectors, or (c) to an unknown low-dimen sional subspace [6, 7]. which results in an efficient algorithm. training set of input/output pairs ( x training example ( x f the d  X  m matrix whose columns are the successive vectors we want to es timate. regression l ( u, y ) = 1 defined as the average loss over the training set: functional, i.e. , we consider the problem: tween tasks. For example, [5] suggests to penalize both the n orms of the w i.e. , to consider a function of the form: where  X  w = ( P n where  X  solution in W , i.e. , constrains the different w given cluster c  X  [1 , r ] , let us denote J ( c )  X  [1 , m ] the set of tasks in c , m for the m tasks, i.e. , E (
P We note that both  X  equivalently of M . We now propose to consider the following general penalty fu nction: where  X  where projections onto orthogonal supplementary subspaces, we e asily get from (7): By choosing particular values for  X  take  X  E objective function (6) both in W and M , i.e. , to consider the problem: where M into r clusters and  X ( M ) is defined in (8). Denoting by S ing set of positive semidefinite matrices, we can equivalent ly rewrite the problem as: The objective function in (10) is jointly convex in W  X  R d  X  m and  X   X  S m semidefinite matrices, however the (finite) set S semidefinite matrices that contains S structure only contributes to the second and third terms  X  Indeed, it is easy to check that M  X  U = M  X  =  X  M  X  , and that I  X  M = I  X  U  X  ( M  X  U ) =  X  therefore { f M : 0  X  f M  X  I, tr f M = r  X  1 } . This gives an equivalent convex set S with  X  =  X   X  1 penalty (12) into the empirical risk term by defining  X  now ready to state our relaxation of (10): 3.1 Reinterpretation in terms of norms We denote k W k 2 on the set S be cast in this way, i.e. , by choosing a specific set of positive matrices S Note that we have selected a simple spectral convex set S get back the formulation of [5]. 3.2 Reinterpretation as a convex relaxation of K-means In this section we show that the semi-norm k W  X  k 2 to decompose it in the form W =  X E  X  where  X   X  R d  X  r are cluster centers and E represents a partition. Given E ,  X  is found by minimizing min relaxing.
 By translation invariance, this is equivalent to minimizin g min respect to  X  ( i.e. , after optimization of the cluster centers) is equal to 3.3 Primal optimization could be easily derived following [8], a direct approach is t o rewrite (15) as which, if  X  k W  X  k 2 c = min  X  term tr W  X  X   X  1 min  X  tors of W  X  W  X  , and we obtain the value of the objective function at the opti mum: simply need to be able to compute this function of the singula r values. ing to this constraint is: For  X   X  0 , this is a decreasing function of  X  The dual function is then a linear non-decreasing function o f  X  (since  X   X   X /m  X   X  from the now consider the dual for  X   X  0 . (17) is then a convex function of  X  respect to  X  in the constraint set (  X ,  X  ) , so if  X  for  X  Reporting this in (17), the dual problem is therefore Algorithm 1 Computing k A k 2 Require: A,  X ,  X ,  X  .
 Ensure: k A k 2
Compute the singular values  X  for all interval ( a, b ) of I do end for value canceling the derivative belongs to ( a, b ) , i.e. , constraints on  X  for F . We followed an alternative direction, using only the  X  4.1 Artificial data 30 . For each cluster, a center  X  w precisely, each  X  w of For each task, 2000 points were generated and a normal noise of variance  X  2 In a first experiment, we compared our cluster norm k . k 2 will always be between the performance of the single task and the performance of CN. in approach), and a run of k-means starting from the relaxed solution ( CNinit approach). without any knowledge of the clustering structure of the tas ks. useful anymore to use a multi-task approach. since the are the same as for the reprojected method. 4.2 MHC-I binding data Method Pooling Frobenius norm Multi-task kernel Trace norm Cluste r norm involving 35 different molecules. We chose a logistic loss for  X  ( W ) . into empirically defined supertypes known to have similar binding behaviors. of clusters (here we limited the choice to 2 or 10 clusters).
 to improve performance in other settings [6].

