 Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns in-cluding itemsets, sequences, and graphs. However, the bot-tleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process.

In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily re-covered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K . Polynomial time algorithms are developed together with several opti-mization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: frequent pattern, summarization, probabilistic model
Mining frequent patterns is an important data mining problem with broad applications, including association rule mining, indexing, classification, and clustering (see e.g., [2, 26, 25, 8, 14]). Recent studies on frequent-pattern mining  X 
This work was supported in part by the U.S. National Sci-ence Foundation NSF IIS-02-09199 and IIS-03-08215. Any opinions, findings, and conclusions or recommendations ex-pressed here are those of the authors and do not necessarily reflect the views of the funding agencies.
 have seen significant performance improvements on efficient identification of various kinds of patterns, e.g., itemsets, se-quences, and graphs ([2, 3, 13, 7]). Patterns with other interesting measures were also examined extensively (see e.g., [10, 5, 19, 24, 18]). However, the major challenge of frequent-pattern mining is not at the efficiency but at the interpretability : the huge number of frequent patterns makes the patterns themselves difficult to explore, thus hampering the individual and global analysis of discovered patterns. There are two sources leading to the interpretability issue. First, the rigid definition of frequent patterns often gener-ates a large number of redundant patterns, most of which are slightly different. A pattern is frequent if and only if it occurs in at least  X  fraction of a dataset. According to this definition, any subset of a frequent itemset is frequent. This downward closure property leads to an explosive num-ber of frequent patterns. For example, a frequent itemset with n items may generate 2 n sub-itemsets, all of which are frequent. The introduction of closed frequent itemsets [19] and maximal frequent itemsets [10, 5] can partially allevi-ate this redundancy problem. A frequent pattern is closed if and only if a super-pattern with the same support does not exist. A frequent pattern is maximal if and only if it does not have a frequent super-pattern. Unfortunately, for any pattern  X  , as long as there is a small disturbance on the transactions containing  X  , it may generate hundreds of subpatterns with different supports. We term the original pattern,  X  , a master pattern and its deviated subpatterns, derivative patterns . Intuitively, it is more interesting to ex-amine the master patterns, rather than the derivative pat-terns. Unfortunately, there is no clear boundary between master patterns and their derivatives.

Secondly, as long as the number of discovered patterns is beyond tens or hundreds, it becomes difficult for a user to examine them directly. A user-friendly program should present the top-k distinct patterns first and arrange the re-maining patterns in a tree structure so that a user can start quickly from a small set of representative patterns. The patterns delivered by the existing top-k mining algorithms such as [11] are the most frequent closed itemsets, but not distinct ones. Users often prefer distinct patterns with littl e overlap for interactive exploration.

In this paper, we intend to solve the pattern interpretabil-ity issue by summarizing patterns using K representatives. There are three subproblems around this summarization task: what is the format of these representatives? how can we find these representatives? and what is the measure of their quality?
A number of proposals have been made to construct a concise and lossless representation of frequent patterns. For example, Pasquier et al. [19] introduced the concept of closed frequent patterns, and Calders et al. [6] proposed mining all non-derivable frequent itemsets. These kinds of patterns are concise in the sense that all of the frequent patterns can be derived from them. Unfortunately, the number of patterns generated in these two approaches is still too large to handle.
Researchers have also developed lossy compression meth-ods to summarize frequent patterns: maximal patterns by Gunopulos [10], top-k patterns by Han et al. [11], error-tolerant patterns by Yang et al. [27], and condensed pattern bases by Pei et al. [21]. Nevertheless, the discrimination be-tween patterns is not emphasized in these studies. Generally speaking, a user may not only be interested in a small set of patterns, but also patterns that are significantly differ-ent. A recent approach proposed by Afrati et al. [1] uses K itemsets to cover a collection of frequent itemsets. Their so-lution is interesting but leaves the support integration issue open: It is unknown how to cover the support information in a summarization. In this paper, we investigate this issue and further advance the summarization concept.
 Given a set I of items o 1 , . . . , o d and a transaction dataset D = { t 1 , . . . , t n } , where each transaction is a subset of I . The pattern collection F is a set of patterns  X  1 , . . . ,  X   X  i  X  I . We are interested in partitioning the pattern set into K groups such that the similarity within each group is maximized and the similarity between the groups is mini-mized. Frequent patterns are distinguished from each other not only because they have different composition, but also because they have different supports. Suppose  X  i and  X  j exhibit strong similarity on these two criteria. It is likely that  X  i and  X  j can be merged to one pattern,  X  i  X   X  j . This is the intuition behind merging two patterns (or clustering them in the same group). We develop a generative model M to measure these two similarity criteria simultaneously. Based on this model, we are able to evaluate the quality of such merging by measuring the probability that  X  i and  X  j are generated by M .

Using the above generative model, we can arrange all of the patterns in a tree structure using a hierarchical agglom-erative clustering method, where patterns with the highest similarity are grouped together first. In this hierarchical tree, a user can flexibly explore patterns with different sum-marization granularity. Our methods can successfully com-press thousands of frequent patterns into hundreds or even tens of distinct patterns.

Our major contributions are outlined as follows. 1. We propose a statistical model which is good not only 2. A principled similarity measure based on Kullback-3. We use K representatives to summarize the pattern set 4. Empirical studies indicate that the method can build
The rest of the paper is organized as follows. Section 2 introduces the concept of pattern profile for similar frequent patterns. The details of the similarity measure, the quality evaluation function, as well as the summarization algorithms are introduced in Section 3. We report our experimental results in Section 4, discuss related work in Section 5, and conclude our study in Section 6.
Let I be a set of items o 1 , o 2 , . . . , o d . A subset of I is called an itemset . A transaction dataset is a collection of itemsets, D = { t 1 , . . . , t n } , where t i  X  I . For any itemset  X  , we write the transactions that contain  X  as D  X  = { t t and t i  X  D } .

Definition 1 (Frequent Itemset). For a transaction dataset D , an itemset  X  is frequent if | D  X  | | D |  X   X  , where is called the support of  X  in D , written s (  X  ) , and  X  is the minimum support threshold , 0  X   X   X  1 .

Frequent itemsets have the Apriori property: any subset of a frequent itemset is frequent [2]. Since the number of subsets of a large frequent itemset is explosive, it is more efficient to mine closed frequent itemsets only.

Definition 2 (Closed Frequent Itemset). A fre-quent itemset  X  is closed if there does not exist an itemset  X  such that  X   X   X  and D  X  = D  X  .

Figure 1 shows a sample dataset, where the first column represents the transactions and the second the number of transactions. For example, 50 transactions have only items a , c , and d ; and 100 transactions have only items b , c , and d . There are 1 , 150 transactions in D 1 . If we set the minimum support at 40%, itemset h abcd i is frequent, and so are its sub-itemsets. There are 15 frequent itemsets, among which 4 are closed. As one can see, the number of closed frequent itemsets is much less than that of frequent itemsets.
Note that in Definition 1, for a transaction that con-tributes to the support of a pattern, it must contain the entire pattern. The rigid definition of closed frequent pat-terns causes a severe problem. A small disturbance within the transactions may result in hundreds of subpatterns that could have different supports. There exists significant pat-tern redundancy since many patterns are actually derived from the same pattern. On the other hand, a pattern could be missed if its support is below  X  while its derivative sub-patterns pass the threshold. In this situation, it is necessary to assemble small correlated subpatterns together to recover it. If we relax the exact matching criterion in the support definition and allow one item missing in a pattern, h abcd i becomes the only pattern in D 1 .

Inspired by this example, we find that it is very important to group similar itemsets together to eliminate redundant patterns. We can merge the itemsets in each group to a master pattern. A master pattern is the union of all the itemsets within a group . Before we formalize the format of pattern summarization, let us first examine what a good summarization means.

Suppose  X  and  X  can be grouped together to form a master pattern,  X   X   X  . That means the supports of  X  ,  X  , and  X   X   X  should not be significantly different from each other. Or more fundamentally, D  X  and D  X  should be highly correlated so that the transactions containing  X  will likely contain  X  , and vice versa.
Although the support similarity is one of the major crite-ria in determining whether we should summarize two pat-terns into one, there is another measure that determines how good a summarization is. Let us compare D 1 with another dataset D 2 shown in Figure 2. For these two datasets, we can summarize all the subpatterns of h abcd i as h abcd i be-cause their supports are very close to each other. However, the quality of these two summarizations is different. If we allow approximate matching between a pattern and a trans-action, pattern h abcd i is more likely contained by transac-tions  X  X bc X  in D 1 (one item missing) than by transactions  X  X  X  in D 2 (three items missing). That means, the summa-rization of D 1 as h abcd i has better quality. This intuition will be clarified when we explain our pattern profile model below.

According to the above discussion, we propose using a probability profile to describe a representative, instead of using an itemset only. The profile has a probability distri-bution on items.
 Definition 3 (Bernoulli Distribution Vector).
 Let I = { o 1 , . . . , o d } be a set of items, and x i be a boolean random variable indicating the selection of o i . p ( x ) = [ p ( x . . . , p ( x d )] is a Bernoulli distribution vector over d dimen-sions, where x 1 , . . . , and x d are independent boolean random variables.

Suppose patterns  X  1 ,  X  2 , . . . ,  X  l are grouped together to form a master pattern  X  1  X   X  2  X  . . .  X   X  l . We can esti-mate the distribution vector that generates the dataset D S where t i j is the value of x i in the j th transaction and  X  is a set of probability { p ( x i ) } . When t i j = 1, it means that the j th transaction has item o i .

According to maximum likelihood estimation (MLE), the  X  X est X  generative model should maximize the log likelihood L(  X  | D  X  ) = log P ( D  X  |  X  ), which leads to The well-known result is That is, p ( x i = 1) is the relative frequency of item o
We use a probability distribution vector derived from MLE to describe the item distribution in a set of transactions. Here we formulate the concept of pattern profile.
Definition 4 (Pattern Profile). Let  X  1 ,  X  2 , . . . ,  X  a set of patterns and D  X  = S i D  X  i . A profile M over  X  tribution vector learned through Eq. (3). Pattern  X  =  X  i is taken as the master pattern of  X  1 , . . . ,  X  l regarded as the support of the profile, also written as s ( M ) .
If we want to build a pattern profile for h abcd i in D 1 D , we can derive the distribution vectors for the sample datasets in Figures 1 and 2. The resulting profiles are shown in Table 1. For example, p ( a ) = 50+1000 50+100+1000 = 0 . 91. Table 1 demonstrates that the profile derived from D 1 has higher quality since it is much closer to a perfect profile , a profile with p ( x i = 1) = 1 . 0 for o i  X   X  . In addition, without accessing the original dataset, we can conclude that h bcd i in D 1 is more frequent than the other size-3 subpatterns of h abcd i . Pattern profile actually provides more information than the master pattern itself; it encodes the distribution of subpatterns. The key difference between our profile model and the model proposed by Afrati et al. [1] is that our profile model can accommodate the patterns themselves as well as their supports.

The support of a pattern in a dataset D can be regarded as the average probability of observing a pattern from a transaction, where p ( t ) = 1 | D | and p (  X  | t ) = 1 if  X   X  t , 0 otherwise.
In our profile model, we can regard the probability of ob-serving a pattern as the probability that the pattern is gen-erated by its corresponding profile times the probability of observing this profile from a transaction, where we assume the conditional independence p (  X  | M , t ) = p (  X  | M ). According to this model, we can estimate the sup-port for a given pattern  X  from the profile it belongs to.
Definition 5 (Estimated Support). Let M be a pro-support of  X  k is written as  X  s (  X  k ) , of M and x i is the boolean random variable indicating the selection of item o i in pattern  X  k .

Surprisingly, the calculation of an estimated support is only involved with d + 1 real values: the d -dimensional dis-tribution vector of a profile and the number of transactions that support the profile. This result becomes one of the most distinguishing features in our summarization model. It means that we can use very limited information in a pro-file to recover the supports of a rather large set of patterns.
Our pattern profile model shows how to represent a set of patterns in a compact way and how to recover their supports without accessing the original dataset. However, the prob-lem of selecting a set of similar patterns for summarization is not yet solved. We formalize this summarization problem as follows.

Definition 6 (Pattern Summarization). Given a set of patterns F = {  X  1 ,  X  2 , . . . ,  X  m } that are mined from a database D = { t 1 , t 2 , . . . , t n } , pattern summarization is to find K pattern profiles based on the pattern set F . A potential solution to the summarization problem is to group frequent patterns into several clusters such that the similarity within clusters is maximized and the similarity between clusters is minimized. Once the clustering is done, we can calculate a profile for each cluster.

We can construct a specific profile for each pattern that only contains that pattern itself. Using this representation, we can measure the distance between two patterns based on the divergence between their profiles. The distance be-tween two patterns should reflect the correlation between the transactions that support these two patterns. Namely, if two patterns  X  and  X  are correlated, D  X  and D  X  likely have large overlap; and the non-overlapping parts exhibit high similarity. Several measures are available to fulfill this requirement. A well-known one is the Kullback-Leibler di-vergence between the distribution vectors in the profiles of  X  ( M  X  ) and  X  ( M  X  ), where p is the distribution vector of M  X  and q is the dis-tribution vector of M  X  .

When p ( x i ) and q ( x i ) have zero probability, KL( p || q ) =  X  . In order to avoid this situation, we smooth the proba-bility of p ( x i ) (and q ( x i )) with a background prior, where  X  is a smoothing parameter, 0 &lt;  X  &lt; 1, and u could be the background distribution of item o i .

When KL( p || q ) is small, it means that the two distribu-tion vectors p and q are similar, and vice versa. This could be justified by Taylor X  X  formula with remainder, KL( p ( x i ) || q ( x i )) =  X log  X  where  X  = p ( x i = 1) and  X  = q ( x i = 1). When 0 &lt;  X  &lt; 1 and 0 &lt;  X  &lt; 1, it implies that
Note that for any o i  X   X  , p ( x i = 1) = 1 and for any o  X   X  , q ( x i = 1) = 1 (after smoothing, both of them are close to 1, but not equal to 1). When two distribution vec-tors p and q of patterns  X  and  X  are similar, transactions containing  X  will likely contain  X  too, and vice versa. It in-dicates that these two patterns are strongly correlated, i.e., p ( x i )  X  q ( x i )  X  D  X   X  D  X  . Likely, patterns  X  and  X  are derivative patterns of the same pattern  X   X   X  . Therefore, KL-divergence can serve as a distance measure for the pat-tern summarization task. This conclusion can further be justified by the connection of KL-divergence with the fol-lowing generative model.

Given several profiles { M  X  } , and a pattern  X  , we have to decide how likely  X  is generated by a profile M  X  , or how likely D  X  is generated by M  X  . If D  X  is generated by M then we cannot tell the difference between D  X  and the trans-actions D  X  covered by M  X  . In this case, we can put patterns  X  and  X  in one cluster. The best profile in { M  X  } should maximize where p is the distribution vector of M  X  . It is equivalent to maximizing L( M  X  ) = log P ( D  X  | M  X  ). Let q be the distri-bution vector of pattern  X  .

L( M  X  ) where n = | D  X  | and n i is the number of transactions (in D  X  ) having item o i . We may add the entropy of q , on the left and right side of Eq. (7). Hence,
L( M  X  ) =
X =  X  KL( q ( x ) || p ( x )) (8)
According to Eq. (8), the best profile maximizing P ( D  X  is the profile that minimizes KL( q ( x ) || p ( x )). That means that KL-divergence can measure how likely a pattern is gen-erated from a profile, indicating that it is a reasonable dis-tance measure for grouping profiles.

In the rest of this section, we will introduce our sum-marization methods based on hierarchical clustering and K-means clustering, as well as the potential optimization heuristics. We are not going to explore the details of clus-tering algorithms, since essentially any clustering algorithm based on KL-divergence can be used. We will focus on the practical issues raised by pattern summarization.
Hierarchical clustering produces a dendrogram where two clusters are merged together at each level. The dendrogram allows a user to explore the pattern space in a top-down manner and provides a global view of patterns.
 Algorithm 1 Pattern Summarization: Hierarchical Clus-tering Input: Transaction dataset D , Output: A set of pattern profiles M C 1 , . . . , M C K . 1: initialize k = m clusters, each of which has one pattern; 2: compute the pairwise KL divergence among C 1 , . . . , C 3: while ( k &gt; K ) 4: select d st such that s, t = argmin i,j d ij ; 5: merge clusters C s and C t to a new cluster C ; 8: update the profile of C over D C by Eq. (3); 9: calculate the KL-divergence between C and 10: k = k  X  1; 11: return ;
Algorithm 1 outlines the pattern summarization process using a hierarchical clustering approach. At the beginning, it calculates the KL-divergence between any pair of patterns. In the following iterations, it repeats Lines 3-10 by select-ing the two clusters which have the smallest KL-divergence (Line 4) and merging them into one cluster. The iteration procedure terminates when the total number of clusters be-comes K .
 A cluster C is defined as a collection of patterns, C  X  F . The set of transactions that contain a pattern in C is written as D C =  X   X  D  X  ,  X   X  C and the master pattern in C is written as I C =  X   X   X ,  X   X  C . The newly merged cluster inherits the transactions that support the original clusters and the patterns that are owned by the original clusters. It has a newly built profile over the merged transactions (Line 8).

The profiling construction has to scan the dataset once, thus taking O ( nd ) for each merge operation, where n is the number of transactions in D , and d is the size of the global itemset I . The initial KL-divergence construction (Line 2) takes O ( m 2 d ), where m is the number of patterns. For each cluster C i , we can maintain a distance list between C i other clusters and sort them in increasing order. Whenever a new cluster C is generated, the two merged clusters are deleted from the distance lists in time O ( m ). A new distance list is created for C and sorted in time O ( m log m ). Note that we need not insert the distance from C in the existing distance lists. The minimum KL-divergence can be found by checking the first element in O ( m ) distance lists. There-fore, hierarchical clustering itself can be done in O ( m 2 log m ). Hence, Algorithm 1 can finish in O ( m 2 log m + m 2 d + mnd ).
When the dataset is very large, it may be expensive to recompute the profile by scanning the whole dataset. For-tunately, it is effective to profile a new cluster through sam-pling based on Hoeffding bound [12].
One major computation cost in hierarchical clustering is the pair-wise KL-divergence calculation. In each iteration, Algorithm 1 has to calculate O ( m ) times of KL-divergence. In total, Algorithm 1 has to do O ( m 2 ) KL-divergence com-putation. One approach to eliminate the quadratic cost is to adopt K-means clustering. Using K-means, we can achieve very fast clustering for a large number of patterns. Algorithm 2 Pattern Summarization: K-means Input: Transaction dataset D , Output: A set of pattern profiles M C 1 , . . . , M C k . 1: randomly select K patterns as the initial clusters; 2: for each pattern  X  do 3: update the profiles of newly formed clusters by Eq. (3); 4: repeat Lines 2-3 until small change in M C 1 , . . . M the summarization quality does not increase; 5: return ;
Algorithm 2 outlines the major steps of the K-means al-gorithm. In the initial step, Algorithm 2 randomly selects K patterns as the initial cluster centers. In the following iterations, it reassigns patterns to clusters according to the KL-divergence criterion. The profiles of newly formed clus-ters are then updated (Line 3 Algorithm 2). This procedure will terminate until there is only small change in M C 1 , . . . , and M C K or it meets other stop conditions, e.g., the sum-marization quality does not increase any more (see Section 3.4).

Conceptually, Algorithm 2 is similar to distributional clus-tering [4] and divisive clustering [9]. The first difference is that we treat each dimension as a separate distribution while other approaches put all dimensions together and create a multinomial model. The second difference is that our qual-ity evaluation function (see Section 3.4) is different from the mutual information loss function proposed by Dhillon et al. [9]. The third difference is that distributional clustering or divisive clustering mixes the profiles directly by assigning equal weight to each instance, while we prefer to recompute the profiles through the original dataset. In the experiment section, we will illustrate the performance difference between these two strategies.

The time complexity of Algorithm 2 is O (( mkd + knd ) r ), where m is the number of patterns, k is the number of clus-ters, d is the number of distinct items, n is the number of transactions, and r is the number of iterations. Generally K-means clustering can complete clustering faster than hierar-chical clustering. However, it cannot provide a hierarchical clustering tree for pattern navigation. Another drawback of K-means is that its output is highly related with the seed selection (Line 1, Algorithm 2), which is undesirable in some applications.
We develop two optimization heuristics to speed up the clustering process.
We can start the clustering process either from closed fre-quent itemsets or from frequent itemsets. The following lemma shows that either way generates the same result.
Lemma 1. Given patterns  X  and  X  , if  X   X   X  and their supports are equal, then KL( M  X  || M  X  ) = KL( M  X  || M  X  Proof. If  X   X   X  and s (  X  ) = s (  X  ), then D  X  = D  X  , which leads to the above lemma.

Any frequent itemset must have a corresponding closed frequent itemset. According to Lemma 1, their profiles have zero KL-divergence, indicating that the clustering based on frequent itemsets will be the same as the clustering based on closed frequent itemsets. Therefore, we can summarize closed frequent itemsets instead of frequent itemsets. Since the number of closed frequent itemsets is usually less than that of frequent itemsets, summarizing closed itemsets can significantly reduce the number of patterns involved in clus-tering, thus improving efficiency.
Another issue is whether we should rebuild the profile from scratch as Algorithm 1 (Line 8) and Algorithm 2 (Line 3) do. The profile updating dominates the computation in both algorithms since it has to access the original dataset. A potential solution is to mix two profiles directly without checking the original dataset in Algorithm 1, or weigh each pattern X  X  profile equally in Algorithm 2,
This approximation can significantly improve clustering efficiency. However, it may affect the summarization qual-ity since the mixed profile may no longer reflect the real dis-tribution. Traditional clustering algorithms usually assume the instances are sampled independently (i.i.d). However, in our case, the i.i.d. assumption for frequent patterns is not valid. Most of the frequent patterns are not independent at all. It may be incorrect to have an equal weight for each pattern or weigh two clusters according to their sizes.
One way to evaluate the quality of a profile is to calcu-late the probability of generating its master pattern from its profile, p (  X  ) = Q o better the quality. Because the master patterns from differ-ent profiles have different sizes, it is pretty hard to combine multiple p (  X  )s to give a global quality assessment. We are going to examine an additional measure in this section.
Through Eq. (5) in Section 2, we show that the support of a pattern covered by one profile can be estimated through its distribution vector directly. A high quality profile should have this estimation as close as possible to the real sup-port. When we apply this measure to a set of profiles, we encounter an ambiguity issue since one pattern may have different estimated supports according to different profiles. Suppose we are given minimum information: a set of profiles, each of which is a triple h distribution vector, master pattern, support i . The information about which pattern belongs to which profile is not given. For any pattern  X  , it could be a subset of several master patterns. In this situation, we may get multiple support estimations for  X  . Which one should we select? A simple strategy is to select the maximum one, This strategy is consistent with the support recovery for a frequent pattern given a set of closed frequent patterns. Let F be a set of closed frequent patterns. For any frequent pattern  X  , its support is the same as the maximum support of its super-pattern  X  ,  X   X   X  and  X   X  F .

Definition 7 (Restoration Error). Given a set of profiles M 1 , . . . , M K and a testing pattern set T = {  X  . . . ,  X  l } , the quality of a pattern summarization can be evalu-ated by the following average relative error, called restora tion error,
Restoration error measures the average relative error be-tween the estimated support of a pattern and its real sup-port. If this measure is small enough, it means that the estimated support of a pattern is quite close to its real sup-port. A profile with small restoration error can provide very accurate support estimation.

The measure in the above definition is determined by the testing pattern set. We may choose the original patterns (which have been summarized into K master patterns) as the testing case. We can also assess the quality over the itemsets that are estimated to be frequent, i.e., where T  X  is the collection of the itemsets generated by the master patterns in profiles and  X  s (  X  k )  X   X  .
The measure J tests  X  X requent patterns X , some of which may be estimated as  X  X nfrequent X , while J c tests  X  X stimated frequent patterns X , some of which are actually  X  X nfrequent X . Therefore, these two measures are complementary to each other. As long as J c is relatively small, we can obtain the support of a generated itemset with high accuracy and de-termine whether it is frequent or not. The following lemma shows that if we summarize closed frequent itemsets using K profiles, the subsets generated by the master patterns in these K profiles will cover all of the frequent itemsets.
Lemma 2. Let { M 1 , . . . , M K } be a set of profiles learned over a collection of closed frequent itemsets {  X  1 using hierarchical clustering or K-means clustering. For any frequent itemset  X  , there must exist a profile M k such that  X   X   X  k , where  X  k is the master itemset of M k . Proof. For any frequent itemset  X  , there exists a closed frequent itemset  X  i such that  X   X   X  i .  X  i must belong to one cluster, say M k . Hence,  X  i  X   X  k , where  X  k is the master itemset of M k . Therefore,  X   X   X  k .
The summarization quality is related to the setting of K , i.e., the number of profiles. A smaller K is always preferred. Nevertheless, when the summarization is too coarse, it may not provide any valuable information. In order to determine the optimal number of profiles, we can apply a constraint on the summarization result. For example, for any profile M = ( p ,  X ,  X  ), we require p ( x i )  X  0 . 9 for any i such that o  X   X  . The optimal number is the smallest K that does not violate this constraint. In this section, we examine the sum-marization quality change to determine the optimal value of K .

When two distribution vectors p and q calculated from patterns  X  and  X  are close to each other, transactions con-taining  X  will likely contain  X  too, and vice versa. Thus, D is similar to D  X  and D  X   X  D  X  is similar to both D  X  and D Let r be the probability distribution vector over D  X   X  D When p and q are close, the mixture r will be close to them too. Therefore, the support estimation of any pattern ac-cording to Eq. (11) will not change much when we merge  X  and  X  . It implies that the merge of two similar profiles will not significantly change the summarization quality.
On the other hand, if a clustering algorithm has to merge two profiles M  X  and M  X  that have a large KL-divergence, it may dramatically change the estimated support of a given pattern. Therefore, when we gradually decrease the value of K , we will observe the deterioration of the summarization quality. By checking the derivative of the quality over K , we can find the optimal value of K practically: If J increases suddenly from K  X  to K  X   X  1, K  X  is likely to be a good choice for the optimal number of profiles.

Figure 3 shows the summarization quality along the num-ber of profiles for a real dataset, Mushroom. The support threshold is set at 25%. We use hierarchical clustering to summarize this pattern set. The curve indicates that there are three optimal candidates to choose: 30, 53, and 76. A user can select one of them for examination based on their need on the summarization quality. Figure 4 shows the qual-ity change along the number of profiles. The derivative of J clearly indicates three huge quality changes.
In this section, we provide the empirical evaluation for the effectiveness of our summarization algorithm. We use two kinds of datasets in our experiments: three real datasets and a series of synthetic datasets. The clustering algorithms are implemented in Visual C++. All of our experiments are performed on a 3.2GHZ, 1GB-memory, Intel PC running Windows XP. Mushroom. The first dataset, Mushroom, is available in the machine learning repository of UC Irvine. We obtained a variant from FIMI repository. This dataset consists of 8124 hypothetical mushroom samples with 119 distinct fea-tures. Each sample has 23 features. A support threshold of frequent itemsets).

Figure 5 shows the average restoration error over the closed frequent patterns ( J ) and the average restoration error over the frequent itemsets generated by the resulting profiles ( J The two restoration errors J and J c are quite close. This indicates that we can use the K representative profiles to properly estimate the supports of the original closed pat-terns as well as the supports of the patterns not in the origi-nal set but derivable from the profiles. We also examined the standard deviation of the two restoration errors and found they are pretty close to J or J c . From this aspect, our sum-the reported [1]; our results are verified by several public softwares in FIMI repository, http://fimi.cs.helsinki.fi/src. marization method is stable and accurate in the restoration of the patterns and their supports.
Figure 6 shows the average restoration error ( J ) over hier-archical clustering and K-means clustering with or without applying the profile approximation heuristics. In KMeans-Apx and Hierarchical-Apx, we do not go back to the dataset to rebuild the cluster profilers. Instead, we directly interpo-late the existing profiles as described in Section 3.3.2. Over-all, the 688 closed patterns can be successfully summarized into 30 profiles with good quality  X  the average restoration error at 30 profiles is less than 0 . 1. In other words, the error of estimating the support for a pattern is less than 10% of that pattern X  X  real support, and even as low as 5% when we summarize them into 53 profiles or over.
 BMS-Webview1. The second dataset, BMS-Webview1, is a web click-stream dataset from a web retailer company: Gazelle.com. The data was provided by Blue Martini Soft-ware [16]. In this experiment, we set the minimum sup-port threshold at 0 . 1% and got 4,195 closed itemsets. BMS-Webview1 is completely different from the Mushroom dataset. It consists of many small frequent itemsets over a large set of items (itemsets of size 1 X 3 make up 84.55% of the total 4,195 patterns versus 14.10% for Mushroom), which makes the summarization more difficult.

Figure 7 shows the average restoration error over hier-archical clustering and K-means clustering with or without applying the profile approximation heuristics. As shown in the figure, when we use the profile approximation heuristics in K-means, the summarization quality is much worse than that of building the profiles from scratch. The restoration at K = 100 is 259% for K-means with the profile approxi-mation while it is around 60% for hierarchical clustering or K-means without profile approximation.

Overall, the summarization quality for BMS-Webview1 patterns is worse than that of Mushroom. When we use 1,200 profiles to summarize the patterns, the restoration error is 17%. The difference is due to the underlying dis-tribution of patterns. There is much redundancy between patterns in Mushroom. By examining the 688 closed pat-terns of Mushroom, we can easily identify  X  X ough X  groups of patterns, where patterns in each group look very similar in composition and differ in support by only a very small number. Intuitively, these patterns can be summarized ac-curately. In BMS-Webview1, patterns are much shorter and sparser. Apparently, it is not good to put two itemsets into one cluster while they have very little overlap in composi-tion. Such pattern distribution can be also explained from the data characteristics, i.e., the click-stream dataset usu-ally contains random and short user access sessions, where the pattern explosive problem is not as serious as in dense datasets like Mushroom. Replace. The third dataset, Replace, is a program trace dataset collected from the  X  X eplace X  program, one of Siemens Programs , which are widely used in software engineering re-search [15]. We recorded the program call and transition information of 4,395 correct executions. Each type of pro-gram calls and transitions is taken as one item. Overall, there are 66 kinds of calls and transitions. The frequent patterns mined in this dataset may reveal the normal pro-gram execution structures, which can be compared with the abnormal executions for bug isolation.

We set the minimum support threshold at 3% in this experiment and obtained 4 , 315 closed frequent execution structures. Figure 8 shows the average restoration error over hierarchical clustering and K-means clustering.

All methods except K-means clustering with profile ap-proximation achieve good summarization quality. The qual-ity of the three methods is quite close and the restoration error is about 6% when we use 200 clusters. The curves indicate that there are two optimal K values to choose: 40 and 80, since the error decreases significantly at these points compared with their neighboring points. For K-means with profile approximation, we further examine the summariza-tion quality. Though the average restoration error does not decrease as we increase the number of profiles, the standard deviation of the error does lower  X  the standard deviation is 21.37% at K = 20 versus 7.39% at K = 200. It means that the summarization quality improves as we use more profiles.
In this experiment, we want to study how the underlying distribution in patterns can affect the summarization qual-ity. We used a series of synthetic datasets with different distributions to test it. The synthetic data generator is pro-vided by IBM and is available at http://www.almaden.ibm. com/software/ quest/Resources/index.shtml. Users can spec-ify parameters like the number of transactions, the number of distinct items, the average number of items in a transac-tion, etc., to generate various kinds of data.

We generated seven transaction datasets, where we vary the number of items to control the distribution of patterns in these datasets. Each dataset has 10,000 transactions, each of which has an average of 20 items. The number of distinct items in each dataset varies from 40, 60, up to 160. Since it may not be fair to compare the result using a fixed sup-port threshold in these datasets, we intentionally obtained the top-500 frequent closed patterns from each dataset and summarize them into 50 and 100 profiles using hierarchical clustering. Figure 9 shows the average restoration error J . Figure 9: Synthetic Data: Hierarchical Clustering
As shown in Figure 9, the summarization quality deteri-orates as the number of distinct items in the datasets in-creases. When the number of items is small, the dataset has a dense distribution. There exists much redundancy between patterns. So, they can be summarized with small restoration errors; while for a dataset with a large num-ber of items, the patterns are sparsely distributed. Thus, it is harder to summarize them with reasonably good qual-ity. This experiment shows that the summarization quality has close relation with the patterns themselves. This result is also observed in the previous real datasets. Dense data with high redundancy can be summarized with good quality while sparse patterns with little overlap cannot be grouped together very well.
We also tested the running time of our pattern summa-rization methods over six synthetic datasets by varying the number of transactions from 1,000, 10,000 up to 50,000. A set of about 1,100 closed patterns is obtained from each dataset using a minimum support of 10%. We tested hi-erarchical clustering and K-means clustering with or with-out applying the profile approximation heuristics over these datasets. Figure 10 shows the running time. The running time of Hierarchical-Apx and KMeans-Apx does not change with the transaction number because we simply interpolate the profiles as described in Section 3.3.2. The running time of hierarchical clustering and K-means clustering without using profile approximation heuristics increases linearly with the number of transactions. This figure shows that profile approximation can really improve the efficiency.
Lossless methods have been proposed to reduce the output size of frequent itemset patterns. Pasquier et al. [19] devel-oped the concept of closed frequent patterns, and Calders et al. [6] proposed mining non-derivable frequent itemsets. These kinds of patterns are concise in the sense that all of the frequent patterns can be derived from these represen-tations. Lossy compression methods were also developed in parallel: maximal patterns by Gunopulos [10], error-tolerant patterns by Yang et al. [27] and Pei et al. [22], and Top-k patterns by Han et al. [11]. These methods can reduce the pattern set size further. For example, in maximal pattern mining, all of the frequent subpatterns are removed so that the resulting pattern set is very compact. Besides lossless and lossy methods, other concepts like support envelopes [23] were also proposed to explore association patterns.
Our pattern approximation model is also related to the probabilistic models developed by Pavlov et al. [20] for query approximations, where frequent patterns and their supports are used to estimate query selectivity. Mielik  X ainen and Mannila [17] proposed an approximation solution based on ordering patterns. The closest work to our study is a novel pattern approximation approach proposed by Afrati et al. [1], which uses k frequent (or border) itemsets to cover a collection of frequent itemsets. Their result can be re-garded as a generalization of maximal frequent itemsets. In [1] Afrati et al. mentioned the support integration issue: It is unknown how to integrate the support information with the approximation. In this paper, we solved this problem, thus advancing the summarization concept. Interestingly, the K representatives mined by our approach can be regarded as a generalization of closed frequent itemsets.
We have examined how to summarize a collection of item-set patterns using only K representatives. The summariza-tion will solve the interpretability issue caused by the huge number of frequent patterns. Surprisingly, our profile model is able to recover frequent patterns as well as their supports, thus answering the support integration issue raised by Afrati et al. [1]. We also solved the problem of determining the op-timal value of K by monitoring the change of the support restoration error. Empirical studies indicate that we can obtain very compact summarization in real datasets. Our approach belongs to a post-mining process; we are working on algorithms that can directly apply our profiling model to the mining process.
