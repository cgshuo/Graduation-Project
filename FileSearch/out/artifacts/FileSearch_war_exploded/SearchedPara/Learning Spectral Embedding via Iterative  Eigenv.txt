 Learning data representation is a fundamental problem in data mining and machine learning. Sp ectral embedding is one popular method for learning effective data representations. In this paper we propose a novel framework to learn enhanced spectral embedding, which not only consider s the geometrical structure of the data space, but also takes advantage of the given pairwise constraints. The proposed formulation can be solved by an iterative eigenvalue thresholding (IET) algorithm. Specially, we convert the problem of learning spectral embedding with pairwise constraints into the one of completing an  X  X deal X  kernel matrix. And we introduce the spectral embedding of graph Laplacian as the auxiliary information and cast it as a small-scale positive semidefinite (PSD) matrix optimization problem with nuclear norm regularization. Then, we develop an IET algorithm to solve it efficiently. Moreover, we also present an effective semi-supervised clustering (SSC) approach with learned spectral embedding (LSE). Finally, we validate the proposed IET algorithm and LSE approach by extensive experiments on real-world data sets. I.5.2 [ Pattern Recognition ]: Design Methodology; G.1.6. [ Numerical Analysis ]: Optimization Learning spectral embedding, Matrix completion, Nuclear norm minimization, Fixed point method. Learning data representation is a fundamental problem in modern machine learning and data mining. An effective data representation can help us to be tter understand the intrinsic data properties such as geometric dist ribution and cluster structure. The existing methods for learning data representation can be categorized into two different types: unsupervised , such as PCA, NMF and manifold learning, or supervised , such as linear discriminant analysis (LDA). All the above mentioned approaches modeling, clustering, classificati on, and visualization problems. However, the results of unsupervis ed methods are not reliable due to the lack of supervision information. Obtaining the complete supervision for all the data is, on the other hand, expensive and time consuming. In recent years, researchers have been working on semi-supervised data representation learning methods [1, 2, 3], which utilizes partial supervision in formation, such as labels or pairwise constraints, during the learning process. In this paper, we will focus on learning effective data representations with the given pairwise constraints . Compared to the class labels of data, pairwise constraints could be easier to collect. People usually consider two types of constraints, must-link (ML for short), which specifies that two instances belong to the same cluster, or cannot-link (CL for short), which indicates that two instances should not belong to the same cluster [4]. These constraints can also be induced by labeled data where objects with the same label are ML while objects with different labels are CL, which is thus weaker than the label information from this sense. There are also some works combining those pairwise constraints with other domain knowledge to fulfill the learning process, e.g., Wang and Davidson [5] utilized pairwi se constraints together with real-valued degree-of-belief constraints to semi-supervised clustering (SSC) tasks. Spectral embedding [1-3] is one of the most popular data representation methods, where the embedded coordinates of the data points are obtained from the eigenvectors of a specifically designed matrix (e.g., similarity matrix and graph Laplacian). For example, the result of PCA is obtained by performing eigenvalue decomposition on the data covariance matrix, while the result of ISOMAP is obtained by perfo rming eigenvalue decomposition on a properly centralized pairwise data geodesic distance matrix. Spectral embedding can be used as an intermediate step for clustering. For example, in spectral clustering [6], because of the discrete nature of the cluster assignment matrix, it is impossible to exactly find the optimal solution. A popular tradeoff method is to first relax the solution space of the original problem into a continuous space, so that the so lution can be viewed as the embeddings of the data with the spectrum of graph Laplacian. Then simple clustering methods such as K -means can be performed on the embeddings to get the clustering results. The aim of this paper is to learn the enhanced spectral embedding towards an ideal representation as consistent with pairwise learn the enhanced spectral embedding via completing a low-rank  X  X deal X  kernel matrix. Recently, a large amount of low-rank matrix recovery methods [7-9] have been proposed for matrix completion problems. Among of them, there are some representative methods for the matr ix completion problem such as SVT [7] and FPCA [8]. And several works also provide theoretical guarantee that the task of the rank minimization problem can be accomplished via solving nuclear norm (also known as the trace norm) minimization under some reasonable conditions. Specifically, Cand X s and Recht [9] proved that a given low-rank matrix  X   X  nn Z satisfying certain incoherence conditions can be exactly recovered by the following model (1) with probability at least 3 1  X   X  cn from a subset  X  of uniformly form 5/4 log Crn n , where r is the rank of the desired matrix, and c and C are two positive constants. where * || ||  X  denotes the nuclear norm of a matrix, i.e., the sum of operator defined by ( )  X   X  ij ij A A if ( , ) ij  X  X  X  and 0 otherwise. Generally, the number of the given pairwise constraints is far less than the one which is sufficient to complete a low-rank kernel matrix with high probability. Ther efore, we also consider the spectral embedding of graph Laplacian as the auxiliary information as in [1-3], and then only need to solve a small-scale positive semidefinite (PSD) matrix with nuclear norm regularization . The proposed model for leaning the enhanced matrix is a nuclear norm regular ized least squares problem. We also develop a modified fixed point continuation algorithm with an eigenvalue thresholding (EVT) ope rator to efficiently solve it, also called iterative eigenvalue thresholding (IET) algorithm. Finally, we provide a formal proof that the proposed IET algorithm converges to an optimal solution. Moreover, we also extend the proposed IET algorithm to tackle semi-supervised clustering problems. In the matrix completion (MC) problem, we would like to recover a low-rank matrix from incomplete samples of its entries. Such a MC problem has a number of interesting applications such as identification. Let F  X  be the desired data representation, and where () i lx is the label of the data point i the dot product. Obviously, the kernel matrix K is possible to recover since it holds two important properties [10]: low-rank and incoherent properties require d for matrix completion. Our low-rank kernel matrix comp letion model can be formulated as follows: where rank( ) K represents the rank of the desired kernel matrix is the set of the symmetric PSD matrices, and instance i x and j x should be in different clusters. Rank minimization has been pr oven to be a strong global constraint and good measure of 2D sparsity. However, the optimization problem (3) is generally NP-hard due to the discrete nature of the rank function. The main solution strategy for the rank minimization problem is the convex relaxation replacing the acknowledged that the nuclear norm as a convex surrogate for the matrix rank is powerfully capable of inducing low-rank [7-9, 11]. Then, the optimization problem for learning a low-rank kernel matrix in Eq. (3) can be reformulated as follows: where  X  is a positive regularization parameter. Generally, the number of the given pairwise constraints is far less than the one which is sufficient to complete the low-rank kernel matrix with high probability. Motivated by recent progress in graph Laplacian regularized kernel learning [2] and SSC [1, 3], we consider the spectral embedding of graph Laplacian as the auxiliary information to learn the enhanced spectral embedding. embedding may be reformulated as follows: where dd M  X   X  () dn is the desired symmetric PSD matrix to enhance the original spectral embedding, F . Considering rank rank rank () ( ) ( )  X  X  T K FMF M , the optimization problem (5) can be reformulated by As a result, the problem of learning spectral embedding is converted into the one of learning a small-scale symmetric PSD matrix M with nuclear norm regularization. While the nuclear norm minimi zation problem (6) can be converted into a semidefinite programming (SDP) problem, the time complexity of each iteration of standard SDP solvers based on the interior-point method could be at least as 6 () Od . To overcome this issue, many firs t-order algorithms have been developed to solve nuclear norm minimization problems, such as FPCA [8], which is a fixed poi nt continuation algorithm with approximate singular value decomposition (SVD). Furthermore, the FPCA method provably converges to the globally optimal solution and has been shown to outperform SDP solvers in terms of matrix recoverability. Our model (6) is also a nuclear norm minimization problem with a PSD c onstraint. In this section, we propose a modified fixed point continuation algorithm to learn the enhanced matrix that incorpor ates an eigenvalue thresholding (EVT) operator, also called IET. Inspired by the fixed point continuation algorithm proposed by algorithm with an EVT operator to solve the proposed nuclear norm minimization problem (6). the function ( ) g  X  with respect to M is given by where * || || M  X  is the set of the subgradients of the nuclear norm, and ( ) : ( )  X   X   X  X   X  TT hM F FMF Z F . The operator ( ) T  X  is defined as * (): || || () Th  X   X  . And ( ) T  X  can be split into two parts: where 1* ( ) || || ( ) TI  X   X   X  X   X   X   X   X  , 2 () () () TI h identity operator. Let 2 () YTM  X  , then * () || || TM M M Y  X   X   X  X  X   X  X  the following nuclear norm minimization problem, The convex optimization problem (7) has a closed-form optimal solution [12], and the optimal solution is given by the eigenvalue thresholding operator which will be defined later: Thus, our modified fixed point scheme for solving the problem (7) can be expressed by the following two-step iteration as follows: Definition 1 (Eigenvalue thresholding operator). Assume 
M VV , where  X   X  dr V , and r  X   X   X  . Given 0  X  v , ()  X   X  is defined as where max{ , }  X  X  X  should be understood element-wise. Lemma 1 . For any square matrix dd Y  X   X  , the unique closed-form solution to the optimization problem takes the form whereby T YUU  X  X  X   X  X  is the eigenvalue decomposition of the symmetric matrix ( ) / 2 T YYY  X  X   X  , and diag( )  X   X  X   X   X  only if ( ( )) v MsM  X   X  X   X  , where v  X   X   X  and ( ) { ( ) ( ) [() ()]}/2  X   X  X  X   X  T
Ih . We develop a modified fixed poi nt iterative scheme for learning the enhanced matrix with a PSD c onstraint. As suggested in [8], the continuation technique can accelerate the convergence of the fixed point iterative method, and the parameter  X  determines the rate of reduction of consecutive k  X  , where  X  is a moderately small constant. Thus, the continuation strategy is also adopted by our modified fixed point algorithm, which solves a sequence of the problem (6), easy to difficult, corresponding to a sequence of large to small values of k defined by To avoid the BB step size k  X  being either too small or too large, we take where min max 0  X   X   X   X  X  X  X  are two constants. Based on the previous analysis, we develop a modified fixed point continuation algorithm with an eigenvalue thresholding operator thresholding (IET) algorithm. Algorithm 1 : IET algorithm Output : The learned matrix  X  M . For 1 ,, ,  X   X  X   X   X  L do 
While not converged do 1. Choose the BB step size  X  k by Eq. (12). 
Check the convergence condition 
End For Let M  X  be the optimal matrix obtained by the proposed IET algorithm. Then we can achieve the enhanced spectral embedding, 1/ 2 ()  X   X   X  FFM , and apply the K -means algorithm on it to form k clusters. Based on the previous analysis, we also propose a SSC approach listed in Algorithm 2 , also called semi-supervised clustering with learned spectral embedding (LSE). Algorithm 2: LSE algorithm 
Input : A data set of n instances 1 {, , }  X   X  n X xx , the set of must-link constraints ML {( , )} =ij , the set of cannot-link constraints CL {( , )} =ij . The number of nearest neighbors t and a constant d . 
Output : Cluster labels of all the data points. 1. Construct the t -NN graph and compute the normalized graph 2. Compute the d eigenvectors 1 ,,  X   X   X  d of L corresponding 3. Obtain the matrix  X  M by solving the problem (6) via 5. Apply K -means to the rows of 1/ 2 ()  X  FM to form k clusters. In this section, we evaluate the performance of the proposed IET algorithm and its SSC applications. All experiments were performed with Matlab 7.1 on a Pentium-IV 3.20 GHz PC running Windows XP with 1-GB main memory. In this part, we perform an experiment to demonstrate the computational efficiency of the proposed IET algorithm for learning low-dimensional embedding tasks. In this experiment, we use a subset of the USPS data set used in [2] with 2000 image training set. For the pairwise c onstraints, we randomly generate 20 ML constraints for each cluster, and 20 CL constraints for every two clusters, with a total of 1100 constraints. We compare the computational efficiency of the proposed IET algorithm against the Schur complement based SDP (SCSDP) [1] and semidefinite-quadratic-linear programs (SQLP) [2]. And two standard software packages: CSDP 6.0.1 [14] and SDPT3 [15] are used to solve the latter two algor ithms, respectively. For all these number of dimensions of the low-dimensional embeddings d to range over {10, 20, ,100}  X  . The time consumption of three algorithms for learning low-dimens ional representations is shown in Table 1, from which we can see that in most cases the proposed IET algorithm is much faster than SCSDP and SQLP. Particularly, the larger the nu mber of dimensions, the more obvious is its improvement. For the SSC task, we present th e results of the proposed LSE approach and four most related SSC algorithms including spectral learning (SL) [16], constrained clustering through affinity propagation (CCAP) [17], constrained clustering via spectral regularization (CCSR) [1], and semi-supervised kernel K -means algorithm (SSKK) [18]. The results of Normalized Cuts (NCuts) [6] are shown for reference. We use two categories of real-world data sets in our experiments: UCI data and image data . We perform experiments on 3 UCI da ta sets: Wine, Iris and WDBC, and an artificial dataset, G50c, and three image data sets: USPS, COIL20, and YaleB3. In these experiments, we set the number of clusters equal to the true number of classes for each data set, and use Rand index [17] to evaluate the accuracy of the resultant clustering for all the clustering algorithms. In addition, we generate a varying number of pairwise constraints randomly for each data set. To evaluate all these SSC approaches under different settings of pairwse constraints, the clustering performance of four existing state-of-the-art SSC algorithms, NCuts and the proposed LSE method on real-world data sets are shown in Figures 1 and 2, and the results reported are averaged over 50 independent runs. From the results shown in Figures 1 and 2, we can observe the following:  X  In most cases, these sophisticated SSC approaches  X  In most cases, the performance of SSC methods consistently  X  SL, CCAP and SSKK usually perform poorer than CCSR  X  CCSR and the proposed LSE approach mostly outperform In this paper, we proposed a novel learning spectral embedding approach via an iterative eigenvalue thresholding (IET) algorithm. The proposed approach not only exploits the structure information contained in data sets but also takes advantage of the given pairwise constraints. Specially, we first converted learning the enhanced spectral embedding as consistent with the pairwise constraints as possible into a low-rank kernel matrix completion problem. Then, we developed a mo dified fixed point continuation algorithm with an EVT operator to solve it. Moreover, we presented an effective SSC approach with learned spectral embedding (LSE). Experimental results on many real-world data sets demonstrate that the proposed LSE approach outperforms the state-of-the-art SSC algorithms. [1] Z. Li, J. Liu, X. Tang. Constrained Clustering via Spectral [2] X.-M. Wu, A. So, Z. Li and S. Li. Fast graph Laplacian [3] F. Shang, Y. Liu, F. Wang. Learning spectral embedding for [4] K. Wagstaff, C. Cardie. Clustering with instance-level [5] X. Wang, I. Davidson. Flexible constrained spectral [6] J. Shi, J. Malik. Normalized cuts and image segmentation. [7] J. Cai, E. J. Cand X s, Z. Shen . A singular value thresholding [8] S. Ma, D. Goldfarb, L. Chen. Fixed point and Bregman [9] E. J. Cand X s, B. Recht. Exact matrix completion via convex [10] R. Keshavan, A. Montanari, S. Oh. Matrix completion from [11] E. J. Cand X s, T. Tao. The power of convex relaxation: near-[12] Y. Ni, J. Sun, X. Yuan, S. Yan, L. Cheong. Robust low-rank [13] J. Barzilai, J. Borwein. Two-po int step size gradient methods. [14] B. Borchers. CSDP, a C library for semidefinite [15] R. H. T X t X nc X , K. C. Toh, and M. J. Todd. Solving [16] S. Kamvar, D. Klein, C. Ma nning. Spectral learning. In [17] Z. Lu, M.  X . Carreira-Perpi X  X n. Constrained spectral [18] B. Kulis, S. Basu, I. Dhillon, R. Mooney. Semi-supervised 
