 Today web search engine has become one of the most useful tools for web surfers, and many search models have been built up. In recent years, language modeling based information retrieval has been successfully applied to traditional document retrieval search. However, numerous problems stem from the miscellaneous content of the whole document as a single model. Second, not all blocks focus on the same topic and noisy information like navigation bars, copyrights, contact information etc., is usually embedded in Web pages. The two problems both challenge the validity of the statistic language modeling and degrade the retrieval performance. In order to solve them, we should fully exploit the block information within a Web page.

To segment a Web page into regions or blocks effectively, some methods are pro-posed under different scenarios [2-8]. These approaches mostly use HTML tag and DOM tree analysis with some heuristic rules. Especially, VIPS [12] tried to learn the importance of blocks using both spatial and content features. Furthermore, based on previous work, some research efforts have shown that segmenting a Web page into several relatively independent blocks can enhance web search [10], web link analysis benefit language modeling of web documents. 
Language modeling approaches to information retrieval are attractive and promis-ing because they connect the problem of retrieval with that of language model estima-each document, and then rank documents by the likelihood of the query according to the estimated language model [1]. Zhai and Lafferty [14] studied the smoothing method for language modeling based information retrieval. Berger and Lafferty [15] proposed a more advanced model which char acterized the retrieval process as a user translating a document into a query. However, all above approaches did not explore how the block information embedded in the web documents would affect the per-formance of language modeling-based information retrieval. modeling and Web page segmentation, which can improve the search for the Web page that contains several topics or noisy information. We first devise a DOM tree-enabled Structure indUction-based Page Segmentation (SUPS) algorithm to partition the page, and then a Block-based Language Modeling (BLM) approach is proposed. BLM is constructed based on following three assumptions: first, a Web page can be segmented into independent parts of blocks and different blocks may represent differ-ent topics; second, query terms are regarded as more relevant to the Web page when they co-occur in the same block than when they are distributed over different blocks; third, the block which has more similar language model with the Web page is more relevant with the major idea of the whole page. And such blocks should contribute more for the relevance of the Web page to a query. 
The major contributions of this paper are: z A block based language modeling approach is proposed towards web search for z A flexible segmentation algorithm, SUPS, is devised to automatically partition z A user study is performed to evaluate the performance of the proposed approach, 
The remainder of the paper is organized as follows. In section 2, we review some related work. Section 3 briefly introduces SUPS page segmentation algorithm. Sec-tion 4 describes the details of our BLM approach. Experimental results and some rection of future work in Section 6. Many research efforts [2-4] divided the page based on the type of the tags including DOM analysis. Chen [5] attempted to understand authors X  intention by identifying Object function instead of semantic understanding. Yang [6] aimed to automatically analyze semantic structure of HTML pages based on detecting visual similarities of from the Web page. Our SUPS algorithm integrates DOM analysis, visual cues and use the spatial and content features of each block to learn their importance. Different with it, our segmentation method does not try to measure the importance of block. 
How to exploit segmentation information to help web mining and information re-trieval tasks has been studied in recent years[9-11, 13]. Yi et al.[11] make use of the common presentation to map the styles of a page and its owner site. Noisy informa-showed that block information can improve link analysis like HITS[17] and PageR-ank[18]. By extracting the page-to-block, block-to-page relationships from link struc-ture and page layout analysis, the authors constructed a semantic graph over the WWW so that each node exactly represents a single semantic topic. 
As to web information retrieval, some researches are made on improving tradi-tional approaches. Yu et al.[9] detected th e semantic content structure in a Web page. terms in pseudo-relevance feedback. Cai et al.[10] expanded the idea in [9] and con-ducted comparative experiments on block-level query expansion and retrieval using four segmentation approaches. To our best knowledge, most block-enhanced retrieval methods focus on using block information to perform query expansion, or using blocks-retrieval based ranking score to revise the original document-retrieval based score. BLM revises the model estimation and query generation of language modeling for information retrieval, while no retrieval model is modified in previous work. 
Ponte and Croft [1] proposed a non-parametric language model which integrated document indexing and document retrieval. It inferred a language model for each document and estimated the probability of query generation in each model. Zhai and adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. Berger and Lafferty [15] proposed a simple, well motivated model of the document-to-query translation process, and described an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. According to the experimental reports in those papers, language model really outper-formed other models on some data sets. However, all these approaches took the document as atomic unit for statistics and came up against the difficulties mentioned before. We claim that the blocks with sing le topic are more precise for model estima-tion. Thus in BLM, the model is estimated on a much smaller block granularity. Before explaining the idea of BLM, we briefly introduce our algorithm for page seg-mentation. Given that Document Object Model (DOM) provides each Web page a block layout of Web page by constructing a DOM style HTML tag tree. Then we devise a heuristic rule based reasoning engine to induce the intention of the Web page designer. Two measurements, Degree of Isolation (DoI) and Degree of Coherence viewed as an integral block. DoI is calculated by a top-town scan that begins with the size etc. DoC is calculated by a bottom-up s can that begins with the leaf elements, and the coherence of parent node is adjusted ba sed on the coherence of its children, which is also learned from visual cues. We call this method Structure indUction-based Page Segmentation (SUPS) . Some details will be depicted in Section 5.1. In this section, we first describe the basic idea of language modeling for information retrieval. Next, we explain how to improve such idea using block information. 4.1 Basic Language Modeling for Information Retrieval To retrieval task, Language Modeling approach (LM) treats each document as a lan-guage model and the generation of queries as a random process. In [1], it is assumed that query terms occurred independently in a particular language model. If l (| ) d pQ M is the probability of the query given the language model of document d and then l (| ) d pQ M can be estimated by l (| ) d pt M as following 1 : and the second item is the probability of not producing other terms. Maximum Likeli-hood Estimate (MLE) is used to estimate l (| ) d pt M using the document collection: tokens in d. However, two practical problems exist: first, a probability of zero will be assigned to a document that is missing one or more of the query terms; second, only one document sample is used and the confidence for the maximum likelihood estima-term probability in the whole collection to a document that is missing one or more of duces another estimator from a larger amount of data: where t df is the document frequency of t . This is a robust statistic in the sense that it is estimated from a lot of more data. However, we cannot assume that every docu-using the mean and it should be minimized. The risk l (, ) Rtd for a term in a document can be modeled using the geometric distribution [16], then we can use this risk func-tion as a mixing parameter in our calculation of l (| ) d pt M . 4.2 Language Modeling Improved by Block Information In the language modeling mentioned above, it is assumed that a whole document talks about a unique topic and can be used to generate the query. However, with the Web page getting more and more miscellaneous, the performance of this statistical model tends to be degraded. We notice two common problems for the normal language mod-eling when it is applied to web documents containing multi-blocks. First, let X  X  suppose the query is  X  X merican film X  and the Web pages discussing Web page as a single model, it may easily generate the query  X  X merican film X  and be judged as relevant. This is what we really don X  X  expect because  X  X merican X  here is a occurs in different blocks is used, this page will not be considered relevant any more. Actually, in such scenario, different models should be estimated for each block, in-stead of a single one for the whole document. The second problem is about the multi-topics of one Web page. See the sample in (Block2), wresting (Block3) and a navigation bar (Block4). Such documents should be less relevant than those only talk about basketball star. In another word, we want to measure the  X  X opic consistence X  of the blocks in one page. 
After these observations, we propose Block-based Language Modeling (BLM) . The assumptions behind such approach are: z Web pages contain several blocks and they may discuss different topics. z Query terms are regarded as more relevant to the Web page when they co-occur z The block which has more similar language model with the Web page is more z The probability of a query occurring in a block l (| ) z The probability of the block occurring in the Web page l (| ) the model of B i estimated using language model. Actually, we find that the formula can also be derived with Bayesian rules: document. Compared with the normal method, we use language modeling to estimate formula (6), which is consistent with formula (4). more  X  X onsistent X  with the whole page, which contradicts with our intuition that longer block will dominate the content of a document. So we propose another version of BLM, in which l (| ) d pB M is normalized using the length of block: normalization . In this section, we first briefly introduce the segmentation algorithm SUPS. Then the data set, score method and evaluation metrics are presented. Finally, the performance of BLM is compared with that of the normal language modeling approach. 5.1 Page Segmentation within different &lt;TD&gt; tags or separated by different &lt;HR&gt; tags. Our SUPS approach explores the web content structure and divide s it into different topics. It is composed of following four steps: 1. Convert the HTML based Web page to XML based DOM tree. The major rules 2. Calculate Degree of Isolation (DoI). DoI is used to describe the difference be-3. Calculate Degree of Coherence (DoC). DoC is used to measure the confidence 4. Extract blocks from DOM tree. We define two set, Candidate Block Set S1 grates heuristic rules and structure induction in a single model. Using this algorithm, Web page can be automatically partitioned into isolated blocks. The granularity of partition can be controlled by PDoI and PDoC. 5.2 Experiment Setup We got the data set in following way: first 9 queries are excerpted from a query log 2 . from top 120 results ranked by Google. The random selection ensures that some pages are relevant while some are less or not relevant. Then a web crawler tried to download these 360 pages. A embed DOM builder dynamically translated such HTML pages to well-formed DOM trees which were later parsed and partitioned into blocks by SUPS algorithm. Because some URLs were not available at download time and some not well-formed pages cannot be translated to DOM tree, we got only 296 pages and 1949 blocks finally. Table 1 shows more details. 
We ask 7 volunteers to mark a score for each Web page in terms of its relevance to corresponding query. These volunteers are all graduate students engaging in informa-instructions can help volunteers provide objective evaluation as much as possible: z Rank 10: relevant, the page exactly matches the query. z Rank 5-7: some relevant, the page refers to some aspects of the query. z Rank 1-3: less relevant, only a tiny aspect of the query is mentioned. z Rank 0: not relevant at all. Query Page 
American film 39 317 292 7.5 42.4 basketball star 40 243.2 281 7 34.7 Apple corporation 33 246.1 202 6.1 40.3 Web service 39 227.1 247 6.3 36 Web service enhancement 31 223.8 194 6.3 35.5 IIS 5 isolation mode 22 271.6 141 6.4 42.4 Web mining 33 222.6 202 6.1 36.5 Harvard university 30 111.8 164 5.5 20.3 Car manufacturer 29 262.7 226 7.8 33.7 
We use the average scores to rank the pages and call the final ranking  X  X tandard ranking X , which will be used to evaluate both LM and BLM approaches. 5.3 Evaluation Metrics evaluate the quality of whole ranking, we measure the average  X  X istance X  between  X  X tandard ranking X  and the ranking generated by different language modeling ap-proaches. The KDist distance measure, based on Kendall X  X  - X  rank correlation and used for comparing induced rank orders is defined as follows: Consider two partially ordered lists of Web pages, 1  X  and 2  X  , each of length m. Let  X  analogously to yield ' 2  X  . KDist is then defined as: work, we only compare lists containing the same sets of Web pages, so that KDist is  X  , and BLM ranking vector as better the model performs. also very important because in most cases, web surfers only care about the top ranked pages. The scores of top n pages ranked by one model are summed up and the sum is compared with that of  X  X tandard ranking X . The ratio  X  is given out as following: 5.4 Results and Analysis The baseline, LM, is implemented using the unigram language model introduced in [1] because of its simplicity and effectiveness. Our BLM improves this model using block information and two versions are implemented: BLM with no normalization (BNN) and BLM with block length normalization (BBN). Figure 3 shows the KDist results and Figure 4 presents the improvements over baseline: (BBN), the results are polarized, i.e. for some queries BBN obtains much greater improvement than BNN, while for others, BBN worsen the ranking result. We observed the result data and found two potential reasons for the polarization of BBN: z For longer block, while l (| ) z Short block is more prone to be dominated by noisy terms. If block normalization theless, we can see that in Figure 5, BBN can obtain about 5.06% while BNN achieve 8.34% improvement when the improvement averaged on the whole dataset. 
As to the other metric, Figure 6 presents the average improvement on the whole dataset and Figure 7 shows the improvement of top 10 score for each topic. Compared with LM, BBN and BNN obtain 5.48% and 16.28% improvements respectively. Here the polarization problem also exists for BBN. 
In summary, our experiment shows BLM approach outperforms the normal one in most cases. Especially, BNN improves 8.34% and 16.28% according to KDist and top 10 score respectively. In this paper, we proposed Block-based Language Modeling (BLM), a novel approach to integrate Web page segmentation and language modeling. It contains two major components: The probability of a query occurring in a block and the probability of the block occurring in the Web page. The first one means that we estimate the model not cuses on a single topic. Experimental results show this approach is promising and some interesting issues can be further investigated. 
In future, we try to find more sophisticated block normalization method and the idea of BLM will be tested on larger collections like TREC 2001. 
