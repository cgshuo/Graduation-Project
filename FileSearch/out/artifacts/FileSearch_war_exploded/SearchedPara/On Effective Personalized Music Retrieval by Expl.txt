 In this paper, we study the problem of personalized text-based music retrieval which takes users X  music preferences on songs into account via the analysis of online listening behaviours and social tags. Towards the goal, a novel Dual-Layer Music Preference Topic Model (DL-MPTM) is pro-posed to construct latent music interest space and character-ize the correlations among (user, song, term). Based on the DL-MPTM, we further develop an effective personalized mu-sic retrieval system. To evaluate the system X  X  performance, extensive experimental studies have been conducted over two test collections to compare the proposed method with the state-of-the-art music retrieval methods. The results demonstrate that our proposed method significantly out-performs those approaches in terms of personalized search accuracy.
 Topic Model, Semantic Music Retrieval, Personalized
Over the past decades, empowered by fast advances in digital storage and networking, we have witnessed an ever-increasing amount of music data from various domain ap-plications. Meanwhile, with the proliferation of mobile de-vices (e.g., mobile phones and laptops) and cloud-based mu-sic service, the development of personalized music informa-tion retrieval techniques has gained greatest momentum as a means to assist users to explore large-scale music collec-tions based on  X  X ersonal preference X . In music information retrieval, there are two widely accepted and yet indepen-dent paradigms: content-based music retrieval [33] and text-based music retrieval [31, 28]. Due to a wide range of real applications, text-based music retrieval has been recently emerging as a popular paradigm. With this technique, users can compose several keywords to describe their music in-formation needs and current contexts, with the expecta-tion that the music search engine returns a list of suitable songs. However, existing methods in this paradigm only consider the relevance between songs and search keywords, while largely ignoring user X  X  personal music preference. In fact, how an individual perceives a song is very subjective, heavily depending on his/her emotional and cultural back-ground [30]. For example, given a query  X  sad  X , whether a song is relevant or the relevance level of the song with re-spect to  X  X ad X  is dependent on the user X  X  personal percep-tion on this song. Thus, for music retrieval, it is crucial to take user X  X  personal music preference into account and effec-tively model the correlations among (user, song, term). In fact, the significance of leveraging user music preference has been widely recognized in the development of smart music information systems [8, 27]. However, few researches fo-cus on 1) investigating the effects of user music preferences on search performance improvement; and 2) designing ad-vanced schemes to catch and model such effects and exploit them in personalized music search systems.

Indeed, effective integration of user X  X  music preference to improve retrieval performance generally requires a compre-hensive understanding of user X  X  music preference on songs with respect to search keywords. A naive approach is to leverage the assistance of end users to manually label songs with various music concepts. However, this approach could be very expensive in terms of time and expertise. In re-cent years, the rapid growth and popularity of online social music services such as Last.fm 1 and Pandora 2 provide excel-lent sources to harvest large-scale user behavior information. When interacting with the social music portals, users leave rich digital footprints, which contain the details of personal music listening history, such as which song was played by which user at what time for how long . Through analyzing user X  X  listening behaviors, we could obtain comprehensive information related to user music preference or taste, e.g., which songs are played frequently by a certain kind of users and what are the favorite levels of a user on different kinds of songs . Besides, in those social music portals, songs are tagged by users with different types of concepts, which reveal the semantic contents of songs. The social tags in Last.fm al-most cover all the concepts that users usually use to describe songs, and have been used for text-based music search [17, 22]. The listening history of users and social tags provide us reliable sources to learn the correlations among (user, song, term), which can be used to support music search at the personal level. http://www.lastfm.com http://www.pandora.com/
Motivated by discussions above, in this work, we focus on designing a music retrieval system to facilitate personalized music search by jointly exploiting user listening behaviors and music tags extracted from popular social music portals. To achieve the goal, we propose a novel dual-layer topic model called D ual L ayer M usic P reference T opic M odel (DL-MPTM), which discovers two sets of latent topics -la-tent music dimensions and latent semantic subtopics . In this model, user X  X  music preference is represented as mixtures of latent music dimensions , which are discovered based on the co-occurrence of songs in playlists and co-occurrence of la-tent semantic subtopics across songs. The latent semantic subtopics are represented as the mixtures of terms. Ac-cordingly, the correlations among (user, song, term) can be captured by the associations of the two sets of latent top-ics. Based on the model, we further develop a personalized text-based music retrieval system. Comprehensive experi-ments have been conducted to examine the performance of the method by comparing with a set of competitors over two test collections. The results demonstrate the effectiveness and robustness of our proposed method on different types of queries and datasets. In summary, the main contributions of our work are as follows.
The remainder of this article is organized as follows: Sec-tion 2 gives an overview of related work. In Section 3, we introduce the proposed personalized music search system, including the DL-MPTM topic model and retrieval method. Section 4 introduces the experimental configuration, and Section 5 reports experimental results and main findings. Finally, Section 6 concludes the paper.
In this section, we review the literature in two closely re-lated domains: personalized music retrieval and topic model.
Driven by numerous real applications, personalized infor-mation retrieval has attracted lots of research attentions in text retrieval community, and thus various approaches have been proposed in last decades [6, 19, 29]. However, very few works have been reported in the domain of personalized text-based music retrieval. Hoashi et al. [12] leveraged rel-evance feedback methods to refine users profiles for search performance improvement, while the method was designed for content-based music retrieval systems. In [34], Wang et al. proposed a tag query interface which enables users to specify their queries using multiple tags and with multiple levels of preferences. This method relies on user X  X  efforts to specify the importance of query tags in each query session. In [30], Symeonidis et al. applied the high order singular value decomposition (SVD) method to capture the associa-tions between ( user,tag,item ). Based on the likeliness that user u will tag musical item i with tag t , musical items are recommended to user u . However, this method suffers from the high time complexity and thus is only applicable for small scale data. Hariri et al. [11] considered the problem of personalized text-based music retrieval, where users X  history of preferences are taken into account in addition to their is-sued textual queries. The proposed system has not been compared with other music retrieval methods and evaluated under the standard information retrieval evaluation frame-work.
This section reviews hierarchical and multi-modal topic models, which are closely related to our work.

Hierarchical Topic Model. Latent Dirichlet Alloca-tion (LDA) [5] is an unsupervised algorithm to discover the  X  X atent topics X  underlying a large scale of text collections. Each document is modeled as a mixture of the topics and each topic is a mixture of words. In recent years, several hi-erarchical topic models were proposed to gain the relations between topics, such as nested Chinese Restaurant Process (nCRP) [3], tree-informed LDA [15] and nHDP [23]. A com-mon feature of these hierarchical topic models is that they all focus on modeling the parent-child and sliding relations between topics. In these models, all topics (parent topics and child topics) are represented as the mixture of words and thus in the same semantic space. Distinguished from these models, the proposed model in this paper discovers two sets of latent topics under two different latent spaces: the latent topics in a high-level latent space are the mixtures of the latent topics in a low-level latent space.

Multi-modal LDA. Due to the success of LDA in single modality scenarios, it is extended to support multi-modal case, such as mmLDA [1], Corr-LDA [4], tr-mmLDA [24], MDRF [14], and factorized multi-modal topic model [32]. The basic philosophy behind these multi-modal LDA models is the existence of shared latent topics that are the common causes of the correlations between different modalities. In mmLDA [1], the image and text words are generated from two non-overlapping sets of hidden topics. For an image, the two sets of topics follow the same topic distribution. Corr-LDA [4] was designed so that image is the primary modality and is generated first, and each caption word is forced to be associated with an image region and is generated based on the topic of this image region. Tr-mmLDA [24] uses a latent variable regression approach to learn a linear mapping be-tween the topic distributions of two modalities. Factorized multi-modal topic model [32] and Multi-modal document random field (MDRF) [14] generalize the modeling of two modalities to multiple modalities. In our personalized mu-sic retrieval system, there are two modalities -audio and text. Besides, because social tags are usually incomplete, the text document (formed for a song) is not complete as a corresponding document to the audio content of the song. The Corr-LDA has the merits that the topics of text words are indeed a subset of topics that occur in the corresponding image (song in our context), and an audio segment could be associated with multiple text words, which is reasonable for the annotations of an audio segment. Thus, we use the Corr-LDA as a basic component in our model. Obviously, our model is very different from these multi-modal LDA models in terms of the dual-layer structure. This section presents a detailed introduction of the DL-MPTM model and the associated retrieval method.
In this study, we aim at designing a personalized text-based music retrieval system for searching songs, which are not only relevant to the query but also effectively satisfy user X  X  personal music information needs. Consequently, the core research problem is how to effectively model user X  X  mu-sic preference on songs with respect to the search keywords. Users usually prefer different types of music tracks, which can be reflected from the songs they often listen to. Mean-while, people X  X  music preferences on songs are highly associ-ated with the semantics embodied by the audio contents of songs. Based on the semantics, user X  X  music preferences can be extracted by analyzing the semantics of songs listened by the users. Further, given that the semantics of songs are modeled by song X  X  contents and user-generated annotations (e.g., social tags), the correlations among (user, song, term) can be estimated. To achieve the goal, we propose a dual-layer LDA model, which characterizes the song X  X  semantics based on the associations between audio contents and tags and models user X  X  music interests based on the songs and their semantics . To ease understanding of the model, we firstly introduce two important concepts. Figure 1: The graphical model representation of the DL-MPTM model. Note that the variable y are conditioned on V , the number of audio words. Figure 1 illustrates the graphical representation of D ual L ayer M usic P reference T opic M odel (DL-MPTM). The model consists of two main components: Part A (the first layer) and Part B (the second layer). The second layer (Part B) is a Corr-LDA model [4], which discovers subtopics based on the co-occurrence of music contents (audio words and text words) in the same song. Besides, this model discovers the associations between audio contents and text words. The first layer (Part A) is a topic model to explore music dimen-sions  X  based on the co-occurrences of songs in the same user X  X  profile and the subtopics associated with these songs. Each subtopic z is represented by a multinomial of audio words and a multinomial distribution of text words; each music dimension  X  is represented by a multinomial distri-bution of songs and a multinomial distribution of subtopics. The set of subtopics in the second level is shared across different music dimensions. These subtopics, which are rep-resented by the distribution of text words or audio words, are used to characterize the music dimensions. User music interests are represented by the multinomial distribution of music dimensions. Because the music dimension is discov-ered based on the co-occurrence of subtopics of songs and the subtopics are discovered based on the co-occurrence pat-terns of songs X  contents, the dual-layer topic model discovers the latent music dimensions and subtopics in a mutual rein-forcement process.

From the generative perspective, a song s with text words w 3 and audio words v s preferred by a user u , namely, an
In the paper, a notation in bold type denotes a vector or matrix. observation of ( u,s, w s , v s ), is assumed to be generated by first choosing a music dimension  X  (e.g., a certain music style) from music interest  X  u of user u . Then based on the selected topic  X  , song s is drawn according to  X   X  ,s , which represents the likelihood for user u to select song s in the music dimension  X  . The audio words v s and text words w of song s are generated according to the subtopic distribu-tions  X   X  of the music dimension  X  . The generation process of audio words and text words is to firstly generate all the audio words, and then subsequently generate all the text words. Specifically, for each audio word v s , a subtopic z is sampled and the audio word is generated accordingly based on  X  z,v s . After obtaining all the audio words, for each text word, an audio word v s is first selected and the text word w s is generated, conditioned on the subtopic that generated the audio word. For details about the sampling process of the second layer (Part B), please refer to [4]. More formally, the process of user X  X  profile generation is as follows: 1. For each music dimension  X   X  X  1 ,...,L } , draw a multi-2. For each subtopic k  X  X  1 ,...,K } : 3. For each user u , draw a multinomial distribution  X  u 4. For each music dimension  X  , draw a multinomial dis-5. For each user u : Based on the connection of two layers of topic models, DL-MPTM thus specifies the conditional joint distribution on song s and a term t given a user u and the latent variables:
This equation estimates how correlative user u , song s , and term t could be, and thus can be used for personalized text-based music retrieval, which is introduced in Sect. 3.2.
Unif(1 , 2 ,...n ) denotes the sampling of a value from 1 to n with equal probability
In the DL-MPTM model,  X , X ,  X  s ,  X  v , and  X  t are Dirich-let priors and pre-defined. The parameters needed to be estimated include: (1) user interest (user-music dimension) distribution  X  u , (2) music dimension -subtopic distribution  X  , (3) music dimension -song distribution  X  s , (4) subtopic-term distribution  X  t and (5) subtopic-audio word distribu-tion  X  v . Several algorithms have been developed to approxi-mate the parameters in variants of LDA. In our implementa-tion, collapsed Gibbs sampling [10] is used to estimate these parameters, as this method has been successfully applied in many large scale applications of topic models [9, 10]. Notice that in the learning of a model, Gibbs sampling iteratively updates each latent variable given the remaining variable until it converges.

Preliminary. Given a user music profile corpus D with user set U , for each user u  X  U , a playlist { s 1 records his/her playing behaviors or music profile. Each song s contains a sequence of text words w s and a sequence of audio word v s . In the Gibbs sampling process, the playlists of users are sampled in sequence. Let S be the sampling se-quence in the Gibbs sampling process, which is the concate-nation of songs in the playlists of all the users. Similarly, let V and W denote the corresponding sampling sequences of audio words and text words.  X  and Z denote the set of latent music dimensions and subtopics corresponding to the song sequence and audio words sequence, respectively. Be-sides, Y is the assignment indicators of the word sequence W . S  X  i denotes S excluding the i -th song s i in S . Similar notation is used for other variables.

Music Dimension  X  Sampling for a Song For the sampling of latent music dimension  X  i = l for s i , the prob-ability is where N l u denotes the number of times that music dimension l is observed in u  X  X  playlist. N k l is the number of times that subtopic k is observed in music dimension l . Notice that the exclusion of  X  = l will cause the changes of N k l for all k = [1 ,K ]. N k l,  X  i denotes the number of times latent subtopic k is observed in latent music dimension l by excluding l the number of times the subtopic k is observed in music dimension l due to s i . PLS ( l,s i ) denotes the effects of the exclusion of  X  = l on the distribution of subtopics in the music dimension l .  X (  X  ) is the Gamma function.

Subtopic Sampling of Audio and Text Word: Next we introduce the sampling of subtopic z j = k for an audio word v j = v in s i and the sampling of all text words of the song s i . Notice that the text words in s i are sampled after sampling all the audio words in s i , as the assignment of z the words in a song is dependent on the subtopic sequence of audio words in this song. The probability of z j = k to an audio word v j = v is: where N v k is the number of times that subtopic z j assigned to audio word v j = v . N t k,  X  j denotes the number of times that t is assigned to subtopic k before assigning k to the j -th audio word of song s i , and N t k,  X  j n t denotes the number of times that term t is assigned to the subtopic of the j -th audio word in the current song s Notice that the exclusion of z j = k for audio word v j may influence the assignment of z j = k to multiple text terms and multiple times. Similar to PLS ( l,s i ), PZ ( k ) denotes the effects of the exclusion of z j = k on the distribution of text terms in the subtopic k .

Parameter Estimation. Based on the state of the Markov chain  X  and z , we can estimate the parameters:
The goal of the retrieval model is to search a subset of songs that are relevant to a particular query. Let q = { t 1 ,t 2 ,...,t n } represent user u  X  X  query consisting of n terms. The retrieval algorithm aims at ranking songs based on their relevance to the query according to u  X  X  music preference on the songs. Notice that the relevance level of a song with re-spect to a query is dependent on user X  X  music taste. Given a query q issued by user u , for a song s , P ( s | q,u ) denotes the likelihood or probability of user u preferring this song s with respect to the query q . Thus, candidate songs can be ranked in the descending order of their probabilities P ( s | q,u ) with respect to the user and query ( u,q ). According to Bayes rule, P ( s | q,u ) can be computed as: where P ( q,s | u ) represents the relevance of song s to query q based on user u  X  X  opinions on the song.

With the posterior estimation of  X  u ,  X   X  ,  X  s , and  X  the DL-MPTM, we have: where P (  X  | u,  X  u ) is the probability of user u selecting music dimension  X  , and P ( q,s |  X  ,  X   X  ,  X  s ,  X  t ) is the joint probability of query q and s in the music dimension  X  . In the deriva-tion, we assume the query terms are independent from each other under this specific music dimension. Given the music dimension  X  , s and t are independent, the joint probability of term t i and song s in the music dimension  X  can be esti-mated by multiplying the the probability of s and t i in the music dimension  X  : P ( s |  X  ,  X  s ) and P ( t i |  X  ,  X 
The probability of term t i in music dimension  X  can be obtained by the generative probability of term t i in the subtopic space: P K z =1 P ( t i | z,  X  t ) P ( z |  X  ,  X  and Eq. 11, the probability of user u selecting s for query q can be estimated:
Intuitively, for a specific music dimension  X  , P (  X  | u,  X  denotes the preference of user u in this dimension; P ( s |  X  ,  X  denotes the likelihood of song s in this dimension; P K z =1  X  ,  X   X  ) P ( t | z,  X  t ) denotes the likelihood of a term t in this di-mension. Thus, P (  X  | u,  X  u ) P ( s |  X  ,  X  s ) P K z =1  X  ) indicates the likelihood for user u to consider song s is relevant to term t in this music dimension.

Algorithm 1 summarizes the whole procedure of personal-ized text-based retrieval method. DL-MPTM training pro-cess can be carried out in offline phase (line 1 -2). Per-sonalized music search is based on the obtained parameters in DL-MPTM, for a given query q , a rank list L can be returned (line 3 -4).
 Algorithm 1 DL-MPTM based personalized text-based music retrieval 1: Train the DL-MPTM model using the collapsed Gibbs sam-3: Compute P ( q,s | u ) using Eq. 12 based on the estimate pa-4: Sort the songs into a ranking list L in the descending order
In this section, we present the experimental settings for the performance evaluation, including test collections, query set with corresponding ground truth, competitors and per-formance metrics.
In order to achieve good repeatability of the experiments, test collections are developed based on two public datasets. Their details are as follows,
In order to ensure the quality of test collections, the p -core filtering method [2] is used to filter users and songs. The p -core of level k has the property, that each song was listened to by at least k users and each user listened to at least k songs. In the experiments, k is set to 20. For the remaining songs, the 30 seconds audio samples were downloaded from 7digital 7 , and their tags were crawled from Last.fm. Table 2 summarizes the details about the two datasets used in exper-iments. It is worth mentioning that two datasets have very different properties. Comparing with TPS, Lastfm-1K con-tains fewer users while each user has richer listening records. Thus, two datasets are used to examine the performances of personalized music retrieval systems in two scenarios: (1) with rich users X  listening records available (Lastfm-1K), and (2) with limited users X  listening records available (TPS), re-spectively.

The training of DL-MPTM model needs the played records of songs by users and the songs X  contents, including textual content (e.g., textual words describing the song) and music content (e.g., audio words of the song). To facilitate the DL-MPTM training, we organize the related data into three types of documents. The description and generation process of the three types of documents are presented below.
User-Song Document For each user, a user-song doc-ument is generated based on his/her played records. The document is comprised by the concatenation of the songs (a  X  X ong X  in a document is indexed by a unique ID) played by the users. For example, if a user u with profiles ( u,s ( u,s 2 , 3), ( u,s 3 , 1), the user X  X  user-song document is { s http://labrosa.ee.columbia.edu/millionsong/tasteprofile http://www.dtic.upf.edu/ ocelma/MusicRecommendation Dataset/lastfm-1K.html https://www.7digital.com/ s , s 2 , s 2 , s 3 } . It is worth noticing that the songs in the doc-uments can be in any order of sequence. To accelerate the training process, the user-song document for each user is cre-ated by concatenating the songs that were played more than 2 times by the user, and each song only appears once. Thus, for each user, the user-song document is actually a playlist consisting of the songs that were preferred by the user in the past. For the users who are used as query users in ex-periments, half of the songs in their playlists are randomly selected as test songs and thus removed from the user-song document used in the training stage (see Sect. 4.1.1).
Song-Text Document The document contains the tex-tual contents of the song, namely, the text words of a song used in the DL-MPTM. In our implementation, social tags are used to represent the text documents of songs. Our model is to capture the correlation of user, song, and term to facilitate personalized search. The tags of each song are col-lected from Last.fm using public API (Track.getTopTags). In our implementation, for each dataset, we filtered the tags that appeared in less than 10 songs. Besides, we also remove the tags which express personal preferences on the songs, such as  X  X avorite songs X ,  X  X avorite X ,  X  X est song forever X , etc. The remaining tags of a song are concatenated together and tokenized with a standard stop-list to form the text docu-ment for the song.

Song-Audio Document The document contains the au-dio content of a song, namely, the audio words used in the DL-MPTM. The audio contents of one song are represented by  X  X ag-of-audio-words X  document. An audio word is a rep-resentative short frame of audio stream in a music corpus. The general procedures to generate the audio words consists of three steps: (1) segment the audio track of each song in a corpus into short frames; (2) extract acoustic features from each short frame; and (3) apply a clustering algorithm (e.g., k -means) to group the short frames into n clusters based on their acoustic features. The cluster centers are the audio words generated for the corpus. By encoding each short frame of a song with the nearest cluster center (or au-dio word), then the song is indexed as a sequence of audio words. In our implementation, we segment each song into 0.05s short frames without overlapping. Also, each song is converted to a standard mono-channel and 22,050 Hz sam-pling rate WAV format. Mel Frequency Cepstral Coefficients (MFCCs) [18] feature is used to generate the audio words. For each frame, a 13-d MFCCs vector with its first and sec-ond instantaneous derivatives are extracted, achieving a final 39-d MFFCs feature. We use K-means to generate the audio words. And for each dataset, we generate a vocabulary of 4096 audio words.
In personalized music retrieval, a positive result should not only be relevant to the query but also be preferred by the query user 8 . In other words, to evaluate personalized music retrieval systems, we need to know (1) whether the result is relevant to the query, and (2) whether the user prefers the result. Therefore, user X  X  preferences on all the songs in the test collection should be available in the evalu-The user who submits the query is called the query user. In personalized information retrieval, user and query should be in pairs. Afterward, we use  X  X uery users X  to refer to the users used in the search stage. ation. To achieve the goal, we create the query set and the test collection specific to each individual user. Firstly, a set of users is randomly selected from the datasets (Lastfm-1K and TPS) as query users. Then, for each user, a set of text queries are generated and a test collection for this specific user is created by randomly sampling half of the songs from his/her user-song document. In the user-specific test col-lection, the played times of songs can be used to estimate the user X  X  preferences on these songs. Specifically, the rele-vance levels of a song with respect to a user-specific query are defined as follows,
The definitions of relevance levels are based on the as-sumption that more times a user listen a song, higher pref-erence level the user have on the song. The evidence that a user listened to a song more than two times indicates that the user shows some interests in the song. The songs lis-tened to only once are regarded as irrelevant, since it could be a variety of reasons why users listen to a song only once. Notice that for a user, his/her listened songs, which are used in the user-song document in the topic model training stage, are removed from the test collections in the retrieval stage.
To test the performance of queries used in real scenar-ios, three types of text queries are developed for evaluation purpose: one-, two-and three-word queries, as users sel-dom issue long queries for music search in reality [21]. This strategy is also often applied in previous text-based music retrieval studies [21, 31]. For the one-word queries in each dataset, the most frequently used words are used as candi-dates. For the two-and three-word queries, the most fre-quent co-occurrent two and three words in tags are used as candidates, respectively. The query users and user-specific queries are carefully selected from these candidates to en-sure that, for each user, the user-specific test collection con-tains sufficient relevant songs for his/her queries (for the fair comparisons of different retrieval methods) [21]. The query words cover the commonly used music concepts, such as genre , instrument , mood , and era . Table 3 shows the query examples used in the experiments.

Since the average number of songs listened to by users in two datasets are very different, different numbers of users and queries can be generated in two datasets. The details about users and queries in both datasets are as below.
This section introduces the details about competitors, eval-uation metrics and system parameters. To verify the ef-fectiveness of the proposed personalized text-based music retrieval system, we compare it with popular and state-of-the-art text-based music retrieval methods, as well as the existing personalized music retrieval methods:
Evaluation Metrics In information retrieval, users are more interested in results in the top positions. Therefore, we focus on the evaluation of top results in terms of accuracy. Several standard information retrieval metrics are used, in-cluding precision at k (Precision@k), Mean Average Preci-sion (MAP) and Normalized Discounted Cumulative Gain at k (NDCG@k) [13]. The relevance levels (i.e., 0, 1, and 2) are used to compute NDCG. For Precision@k and MAP, both relevant (i.e., 1) and highly-relevant (i.e., 2) results are regarded as positive results.

Parameter Setting In our implementation, the Dirich-let hyper-parameters of both topic models (DL-MPTM and PRM) are empirically set:  X  = 1 . 0,  X  = 1 . 0,  X  s =  X  t = 0.01. We carefully tune the latent topic numbers in both topic models. In DL-MPTM, the number of latent music di-mension is tuned in { 5 , 10 , 20 , 30 , 40 , 50 , 60 } and the number of latent sub-topics is tuned in { 20, 40, 60, 80, 100, 150 } . The number of latent topics in PRM is tuned in { 20, 40, 60, 80, 100, 150 } . Besides, the combination weight w in WLC retrieval methods are both tuned from 0 to 1 in steps of 0.1.
This section reports the experimental results of our meth-ods and other competitors on retrieval performance. The reported results of DL-MPTM and PRM are based on the optimal numbers of latent topics in each dataset. The re-ported results based on MAP and NDCG in all the tables are truncated at 10, namely, MAP@10 and NDCG@10. The symbol (*) after a numeric value denotes significant differ-ences ( p &lt; 0 . 05, a two-tailed paired t-test) with the cor-responding second best measurement. All the results pre-sented in below are the average values of queries over all the users. Effectiveness Table 4 reports the retrieval performance on different queries composed of one, two and three words on the two datasets. As can be seen, the proposed model out-performs all the other algorithms over both datasets. Larger performance gain can be achieved when considering more re-sults in the top positions; in particular, the improvements in P@10, MAP and NDCG are statistically significantly compared to other algorithms. After comparing the results gained using TMR, WLC, and PAR, it is easy to find that for Lastfm-1K dataset, the consideration of audio features with text can improve the search results. Besides, PAR obtains better results than WLC does. However, in TPS dataset, the performance decreases when using acoustic fea-tures for re-ranking. It is worth noticing that the results of WLC in TPS are the same to TMR. It is mainly because the best performance of WLC can be achieved by only us-ing text feature in WLC. Notice that the search datasets of query users in Lastfm-1K is much larger than that of users in TPS (see Sect. 4.1.1). Generally, it is difficult for the content-based method to achieve better search results over a smaller dataset, because finding songs with similar con-tents in smaller datasets is harder. Thus the audio feature does not work well when being applied to support search over the TPS dataset.

On the other hand, PRM and DL-MPTM achieve much better performance than the other three methods (TMR, PAR, and WLC), which have not taken the personal music preferences into account. It demonstrates the importance of user X  X  music preference in facilitating effective music re-trieval. DL-MPTM X  X  performance improvement over PRM on both datasets demonstrates the effectiveness of our pro-posed dual-layers topic model in capturing the correlation of user, song, and term. In PRM, the correlation is mod-eled using the same latent space, which is discovered based on both the co-occurrence of songs in playlists and the co-occurrence contents of songs. In DL-MPTM, the correla-tion is captured by two layers of connected latent spaces: the low-level latent space (constructed by latent semantic subtopics ) is discovered based on the co-occurrence contents of songs, the high-level latent space (constructed latent mu-sic dimensions ) is discovered based on the co-occurrence of songs in playlists and the co-occurrence of latent subtopics across songs.
 Table 5 compares the performances of TMR, PAR, and DL-MPTM based on the top five search results in the rank-ing lists of one representative query in each type. The rel-evance level of each song in the top five positions is also shown. The results demonstrate that DL-MPTM achieves much better performance in task of searching user preferred songs with respect to the queries, comparing to TMR and PAR methods. For example, in response to the query  X  gui-tar, pop  X , DL-MPTM places three high-relevant songs at the top rank, compared with only one ranked by the TMR model at the 5th position and two ranked at the 3rd and 4th positions by the PAR model.
 Robustness By comparing the results of different query types (one-, two-and three-word queries), we can observe that the search performance is slightly decreased when the query complexity increases. For different types of queries, DL-MPTM achieves significant and consistent improvement over all metrics, showing a superior robustness across multi-word queries.

Music is usually described by different categories of mu-sic concepts, such as mood , instrument , genre , and vocals , which have been widely studied in music retrieval related research, such as classification and annotation. We examine the search performance of our method over other methods on different categories of music concepts. Table 6 presents evaluation results. One-word queries are classified into dif-ferent music concept categories as shown in the table. We focus on the one-word queries, since the two-and three-word queries could be the combination of different categories. The category  X  X ther X  contains queries, such as  X  driving  X ,  X  slow  X ,  X  sexy  X , which cannot be classified into other four categories. high relevance (see Sect. 4.1.1).
 Table 6: Retrieval results for query categories. The best results for each category are indicated in bold.
 The significant improvements over other methods on P@10, MAP and NDCG show the effectiveness and robustness of DL-MPTM over different music concept categories.

Comparing the search performances of all the methods on the two datasets, they cannot achieve good performance when searching over the TPS dataset, because of the limited size of relevant results in each user X  X  specific dataset. Notice that the number of training samples in the TPS dataset is also much smaller than that in the Last.fm-1K dataset. On the TPS dataset, the absolute performance gain achieved by DL-MPTM over other methods for all the metrics are at least comparable to those in the Lastfm-1K dataset. This demonstrates a strong robustness of DL-MPTM on rela-tively small training datasets.
In this section, we investigate the effects of parameters on the proposed retrieval method. In topic models, it is hard to accurately pre-define the number of topics, which has impor-tant effects on the results. In the DL-MPTM model, there are two sets of latent topics: the number of latent music dimensions in the first layer, and the number of subtopics in the second layer. Fig. 3a and Fig. 3b illustrate the effects of the two parameters, respectively. From the results, it can be observed that the number of latent music dimensions has strong impacts on the final performance, and it is optimal to set the number of music dimensions to [5, 20]. In contrast, we can observe the minor effects of sub-topic number, es-pecially for Lastfm-1K dataset. Fig. 3c shows the effects of weights in the combination of acoustic similarity and textual similarity. From the results, we can find that for Lastfm-1K, the combination of acoustic similarity and textual similar-ity can slightly improve the performance when w is set to [0.6, 0.8]. However, the performance degradation is observed when the same similarity combination is applied to the TPS dataset. Notice that in the WLC method, the acoustic sim-ilarity is computed based on the first search result of TMR. Thus, the accuracy of the first search result has an impor-tant impact on the WLC performance. When the search accuracy of TMR is relatively high, WLC can improve the TMR performance further, such as the results observed in the Lastfm-1K dataset. This also suggests that when search-ing music using both text and audio query examples (using relevance feedback), performance could be improved by the combination of acoustic similarity and text similarity.
In this paper, we present a personalized text-based mu-sic retrieval system which exploits the user listening behav-iors in social music services. The system can accurately estimate the relevance of a song with respect to a term subject to user X  X  music preference. To achieve the goal, a Dual-Layer Music Preference Topic Model (DL-MPTM) is proposed to leverage the user listening logs and social tags to learn the interactions among (user, song, term), which are applied for personalized text-based music search. To evaluate the performance of the personalized retrieval sys-tem, comprehensive experiments have been conducted on two public datasets. The comparisons with the state-of-the-art text-based retrieval methods and existing personal-ized music retrieval methods in experiments show that our method can significantly improve the search performance in terms of accuracy. The results also demonstrate the impor-tance of effective integration of personal music preference in developing high-performance music search engines, and verify the effectiveness of our proposed retrieval model.
