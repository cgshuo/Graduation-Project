 For the development of large-scale soft ware systems, project team members do not usually work together. They are distributed all over the world and commu-nicate each other via various tools. In addition to e-mail, instant messaging soft-wares, web forums and wikis have become common as a communication medium for users. Moreover, Web-based software engineering tools are also widely used, such as bug tracking systems. Users and their various tools constitute a web-based development community.

A bug tracking system is designed to keep track of reported software bugs in a development community. Users can submit bug reports to the system which describe the software problems, and comments upon existing bug reports. Each bug report is triaged to a suitable developer who will in charge to address the re-lated problems. Usually, there are thousands of developers in a large development community, and hundreds of bugs will be reported daily. Different QAs might report the same problem many times res pectively. This causes an issue as du-plicate bug reports. The phenomenon of duplicate bug reports is very common. If duplicate bug reports of the same problem are triaged to different developer, lots of efforts will be wasted and potential conflicts will be made.
There have been a number of works which focus on the duplicate bug reports detection issue in a bug tracking system. Existing methods [8][11][10][13][5][12][7] can be divided into two different categories: relevant bug report retrieval and du-plicate bug report identification. Given a new submitted bug report, the former methods will return a sorted list of existing reports that are similar to it. How-ever, the latter methods will identify that it is a duplicate report or not. As far as we know, the state-of-the-art search approach[7] can guarantee that the re-lated bug reports will be found in the top-10 search results with 82% probability in some dataset if the new bug report is really duplicate. It is already a very good result. However, in actual developme nt environment, it still requires users to spend lots of efforts to review the search results manually. From a practical point of view, duplicate bug reports identification can really reduce the devel-oper X  X  efforts. Yuan Tian etc.[10] reported their progress in the field of duplicate bug reports identification. The true positive rate was 24% while keeping the false positive rate of 9%. It still need to be improved in order to meet the needs of practical application. In this paper, we propose PDB, a practical duplicate bug reports detection method, which will help developers to reduce the efforts on processing bug reports by combining the relevant bug report retrieval and duplicate bug report identification.

For duplicate bug reports issue, any type of existing methods need to compute the similarity between reports. Different methods will choose different features to compute the similarity. such as textual similarity[8][11], surface fea-tures(timestamp, severity, product and component etc.)[10], execution trace[13] etc. Almost all of these features are ext racted from the bug reports themselves. Existing works have tried almost all possible methods in the field of informa-tion retrieval and machine learning to address the duplicate bug reports issue. It is hard to improve the performance only depending on these features. We propose some new features extracted from comments and user profiles according to the observation on bug reports. In a development community, comments are necessary complement to bug reports. Furthermore, The reporter X  X  knowledge and experience will affect the quality of the bug reports he submitted. Exper-iments show that these new features can e ffectively help us to further improve the duplicate bug report s detection performance.

The contributions of this paper include:  X  We propose a practical duplicate bug reports detection method, which adopts  X  We propose some new features from co mments, user profiles and query feed- X  Our extensive experiments on actua l dataset shows PDB method outper-The rest of this paper is organized as follows: section 2 gives a background knowledge on web-based bug tracking system, bug report, and duplicate bug report detection. Section 3 presents our p ractical solution to identify duplicate bug reports in a large web-based development community. The experimental study is shown in section 4. We discuss th e related works in section 5. Section 6 concludes and describes some potential future works. 2.1 Web-Based Bug Tracking System Nowadays web-based development comm unities become grown up for its remov-ing the geographical restrict, fostering communication and collaboration in group members. Various kinds of web-based dev elopment communities are widely used: Web forums provide place for users to submit and discuss the program questions. Distributed revision control system like GitHub facilitates sharing codes with other people and code management for multiple contributors.

Web-based bug tracking system also is one important kind of web-based de-velopment communities. It is designed t o keep track of reported software bugs. It allows remote access of resources anytime, anywhere. It engages all the roles including QAs, developers and project managers on a single collaboration plat-form over the web. Users could review bugs and share comments and resources freely. All the knowledge are stored and centralized. Currently many web-based bug tracking systems such as Bugzilla and JIRA have been widely used in soft-ware projects, especially in large open sou rce software projects like Firefox and Eclipse.

Figure 1 displays a general workflow for processing a bug in web-based bug tracking system. At first a reporter i ssues a bug for a special component. Then the bug is automatically sent to triager to check the quality of bug. If the triager judges that bug information is insufficient, the bug will be sent back to submit-ter to complete. If the bug is invalid, never fixed or duplicate, triage will set resolution and close bug. If triage have completed the entire checklist for a bug, bug will be marked as triaged and assign ed to related developer. Next, bug is fixed by developer. After the bug is resolved, QAs verify the resolution of the bug, and close the bug when it is fixed. 2.2 Bug Report A bug report is a structured file which is composed by multiple fields. These fields can be roughly categorized into three types: text data, metadata and at-tachments.

Text data include summary , description and contents . Summary is a one-sentence statement tha t presents problem; Description outlines problem in de-tail. It usually includes detailed steps to reproduce issue, the actual and expected result after performing the above steps; Comment is the opinion for this bug ex-pressed by bug owner or others.
 Metadata usually have several fields and generally contain following fields: Product and component fields indicate the artifact and module where the fault exist. Assignee is the person who is assigned to fix the bug. Version field shows the versions of product which the bug appears. Priority is the importance of the bug. Reporter is the person who submits the bug. Create time is when the bug is submitted. Status field indicates the processing state which the bug stays at. Resolution shows the result of bug, once it is resolved.

Attachments are usually added to bug reports in order to provide more data help to understand bug. They usually are screenshots, patches or logs. 2.3 Duplicate Bug Report Detection As figure 1 shows, triage is a key procedure to assess the quality of bugs and prefilter some unnecessary bugs, which reduces nonessential effort, and improves the efficiency of project team. The checklist for bug triage usually contain several tasks. One main task is to mark duplicate bugs.

Bugs are duplicate when they have the same root cause. When labeling them duplicate, engineers will set one of them as master bug and others as duplicate ones. The master bug represents the whole group of duplicate ones with it.
Duplicate bugs occupy a large proportion in many projects, especially in large open source projects for its loose management. As [3] reported, issued from November 2006 to March 2008, about 38% of bugs are duplicate out of 60233 bugs in Firefox; about 49% of bugs are duplicate out of 19204 bugs in Thunderbird; Generally these duplicate bugs cause redundant work and increase workload for engineers. Meanwhile, as [1] pointed out that duplicate bug pro-vides additional information about original bug, which is useful for developers to understand the bug. So identifying duplicate bugs and aggregating them will significantly improve the efficiency of bug processing.

However, it is not trivial to detect duplicate bugs manually. Several factors hinder the feasibility and efficiency of this activity in the large projects:  X  Firstly, in general it costs much time to analyze a bug report and judge  X  Secondly, same error causes failure at different level of program. In some  X  Thirdly, a bug is not filed against appropriate component, which might mis-Therefore, it need an automa tical duplicate bug detection tool to help triagers. We propose PDB, a practical duplicate bug detection method. It is designed to complete two tasks and combine them: one is to make prediction whether one bug is duplicate, the other is to return top-k most possible duplicate bugs given a bug. In this section we describe the techniques in PDB in details; Overall, PDB adopts three stages of classification in duplicate bug detection. The first one is to predict how much contribution a comment makes to define a bug. The second one is to predict the possibility that two bugs are relevant and duplicate. The possibility is used to be score for ranking. The third one is to predict whether one bug is duplicate. 3.1 Modeling Comment Similarity Comments in bugs are the points of view and discussions about the bugs issued by owner or others. Among these, a large proportion of comments are supplement to the original content, and help people better understand the appearing result and find root cause of bug. In some cases when the summary and description in a bug are vague and ambiguous, some key comments are critical to define to bug, and also contain important clues in finding duplicate bugs. So PDB puts comments into consideration when calculating the degree of relevance between bugs.

Comments in a bug report are issued by various roles with various kinds of intents. Some comments make some supplements to the description of bug phenomenon or possible rule causes, which can assist in identifying the relevance of bugs. But other comments just talks about bug fixing procedure or the results of bug processing, which deviate from the topic of main body for bug report and are not crucial in identifying the relevance of bugs. So it is intuitive to put comments weights differentially based on the type of comments, and set the former type of comments higher weight.
In order to measure the weight of comment, we divide all the comments into two classes: usefulness and less usefulness. The first class of comment is the one which describes bug phenomena, discusses root cause of bugs and solution of bug. The remaining of comments are ca tegorized to the second class. We set comment weight as probability that the classifier put the content into usefulness class. For comment c i in bug Y , we define the weight of this comment as follows: SVM with RBF kernel is adopted as classifer. There are four types of features to classify the types of comments: average-idf, bug status, length of comment text, and unigrams of a term within comments.
 Average-IDF: From our observation, those useful comments usually talks about Bug status: For a comment, the bug status indicates current state when the Length of comment text: In generally, the length of comment text is also a Unigrams of a term within comments: If a comment contains  X  X oot X , Because summary and description field of a bug are representative of defining bug and tend not to contain unessential information, The weights of summary and description are set re spectively with 2 and 1.

As discussed above, usually a comment in a bug report has its own intent and topic, the text content of comments in one report differs a lot. So it is common that it is only one comment has high similarity given a text content, and others comment in this bug are unmatched. We assume a new submitted bug have no comment. Given a query bug Q and a candidate bug Y ,wepropose W eighted Comment similarity P s ( Q, Y ) to calculate the similarity between summary and description in query bug and comments in candidate bug. Given a set of comments C in Bug Y and query text q , similarity function between them, Score Y ( q, C ), is the maximum score in similarity between text and comment. It is defined as follows: Where c i denotes the ith comment in a bug report, c i and query text q is modeled as vector space model, and Sim ( q, c i ) is cosine similarity between query text q and comment c i .

Given query bug X , text set ( X ) denotes the text set of bug X including summary and description. C denotes the set of comments in one bug Y , Ps ( Q, Y ) is defined as follows: Where W Q ( q i ) denotes weight of q i in bug Q . It has been discussed above. 3.2 Duplicate Bug Reports Retrieval Model PDB utilizes a discriminative model to calculate the possibility that two bug reports are duplicate. The possibility is used as score in ranking. Apart from weighted comment similarity and features proposed in [10], here we introduce several new features:
Topic model for document representation has widely used in machine learning and information retrieval. It makes excellent performance in many applications [14]. Topic model represents a document as a mixture of multiple topics. For all bugs report in a project, we learn LDA[2] topics from the corpus of past submit-ted bugs through gibbs sampling[4]. We put summary and description together as a bug document, and measure topic distance between two bug documents via both Jensen-Shannon and symmetric KL divergences[9] between two topic dis-tributions. Finally, binary match/nonmatch of the most probable topic in each distribution is also feature. That is, 1 if the match of the most probable topic of two reports are the same; 0, otherwise.

Generally, the author of bug is much familiar with the bugs which are issued by him, and can roughly find the cause of bugs. Accordingly, same author is not likely to issue duplicate bugs. We use an indicator of the same author relationship as a feature, that is, 1 if the authors of two posts are the same, 0, otherwise.
Table 1 shows all the features in our algorithm. 3.3 Duplicate Bug Classifier Model PDB leverages classifier to predict whether query bug is duplicate. The features in use consist of two aspects: user profile and query feedback.

User profile includes both demographic information, eg. name, age, etc, and his individual preference(technical background, interests and working styles). The latter is hard to get, but valuable for predicting the resolution of submitting bugs. Intuitively novice users as well as expert or advanced users tend to submit bugs with different quality.

PDB takes some user profiles features into consideration. They include the total number of issuing bugs, the portion of issuing bugs with resolution as FIXED and the portion of issuing bugs with resolution as DUPLICATE. The total number of issuing bugs tries to identify the participation and familiarity of submitter in a project. Two portions shows submitter X  X  habit of issuing bugs. These features are helpful to predict the resolution of a new query bug. For example, high total number of issuing bugs indicates that the submitter might have profound knowledge of project and program, and thus is less likely to submit duplicate bugs; A low portion of bugs with resolution as FIXED and a high portion of bugs with resolution as DUPLICATE mean the submitter might have not habit of searching relevant bugs before issuing a new bug, the resolution of the bug which this author issues has good chance of being duplicate.
The idea of query feedback is to judge the features of query from the returning sorted results by retrieval engine. Suppose that a user issues query bug Q to a duplicate bug retrieval system and a ranked list L of candidate bugs is returned. Intuitively, the more similar top one candidate bug is, the more probably query bug Q is duplicate bug. However, Due to convenience, submitters usually employ many same words and very similar writing style when filling in some relevant bug reports. It will lead to very high similarity score between these bugs which are submitted by same author, but not duplicate. In order to catch these situ-ations, PDB consider several query feedback information. The features include the similarity score with first, fifth, tenth bug report, whether the first, fifth, tenth returning bug is submitted by same author, the portion of same author in top five returning bug reports, the portion of same author in top ten returning bug reports, the average similarity score for in top five returning bug reports and the average similarity score for in top ten returning bug reports. Similarity score of bugs is the probability that two bugs are duplicates, described in previous subsection.
 4.1 Dataset MeeGo is a Linux-based free mobile oper ating system project. We crawled a dataset of bugs report from public MeeGo bugzilla 1 throughopenRPC/XML API. The dataset contains 22486 bugs, which are issued from March 2010 to August 2011. There are 1613 bugs with the resolution as DUPLICATE. 4.2 Evaluation and Discussion The first experiment concerns at the performance of PDB X  X  finding duplicate bug reports We compare PDB with the REP[10]. Several standard measures (MAP, Precision, Recall) are used in evaluation.

PDB adopts SVM classifer 2 with linear kernel. The number of topics in topic model is set to be 100. The ids of bugs for training set range from 1 to 5000, and the others are used for test. The query bug set contain both duplicate and master bugs. The result set are expected to contain all the duplicate bug reports.
Table 2 displays the performance on r ecall, precision and MAP respectively for these two approaches. PDB outperforms REP in all measures. PDB achieves higher recall rate by about 1%-8% for the resulting lists of top 1-20 bug reports. The gain gap grows as the size of list increases. In precision, PDB outperforms REF by 1%-8% in top 1-20 resulting bug reports. PDB improves MAP from 36% to 46%.

We analyze the contribution of the various features to the model by measuring the F-score through python script in LIB SVM. From all new features, weighted comment similarity has highest F-score, which means that comments in a bug play a major role in duplicate bug retrieval. The features related to topic model follow. Reporter distance is lowest-ranking.

The second experiment is to evaluate the performance of identifying whether one bug is duplicate bug. We randomly picked up 534 duplicate and non-duplicate bugs respectively, and then apply the Naive Bayes, Decision Tree 3 and SVM for classification. RBF kernel is used in SVM classifier. 10-fold cross validation is proceeded to measure accuracy, true pos itive rate and true negative rate. The parameters are grid searched to maximize accuracy.

We make comparison on the accuracy of three classifiers with PDB and state-of-the-art[12]. TIAN stands for [12] in the following. The best classifier refers to classifier that achieves the highest accuracy. Figure 2 shows the results. PDB outperforms TIAN consistently in all three classifiers. SVM and Decision Tree achieve close accuracy with both PD B and TIAN, but make increase about 10% on accuracy compared to the Naive Bayes.

We define true positive(TP) rate as the fraction of duplicate bugs which are correctly classified, and true negative(TN) rate as the fraction of non-duplicate bug which are correctly classified. The higher are both these two rates, the better the classifier works.
 Table 3 shows the comparison on the TP and TN rate for three classifiers with PDB and TIAN. In all three classifiers, PDB outperforms TIAN by 2%-31% on both TP rate and TN rate, except on TN rate in Decision Tree. Naive Bayes classifier shows best performance on TP rate, but get worst result on TN rate in three classifers. Similar with accuracy, SVM and Decision Tree achieve close performance on TP rate and TN rate overall. SVM outperforms Decision Tree on TP Rate and TN rate by a narrow margin. Therefore, Overall SVM do most stable and accurate work in three classifiers. In recent years, there have been a number of literatures on detecting duplicate bugs. These literatures can be roughly divided into two categories: one is to find most relevant bugs in bug report collection given a bug; The other aims to identify whether one bug is duplicate.

In the first category, Runeson et al.[8] work is the first one to address this problem. They make natural language process on the raw text data, use vec-tor space model as computational framework, and set term weighting based on term frequency. Wang X et al.[13] combine text similarity and execution trace between bugs to rank bug reports. However, as Sun et al.[11] point out, the proportion of bugs contains execution information is fairly low in some projects like OpenOffice, Firefox. So is MeeGo dat aset. Sun et al.[10] propose to train a support vector machine classifier to detect duplicate bugs with 54 text simi-larities feature including multiple text field. Sun et al.[10] propose REP, which combines BM25F with some meta-data lik e reported product, component, ver-sion. They make extensive experiment to show that REP outperforms the other works in effectiveness. Kaushik, N. et al.[6] make extensive comparison on dif-ferent IR models. The experiment shows Log-Entropy based weighting scheme outperformed other models in effectiveness.

In second category of works, Jablert et a l.[5] use surface features, textual se-mantics, and graph clustering to predic t duplicate status. More recently, Tian, Y et al.[12] work based on [5]. They used maximum REP similarity, and add an-other category information, the product difference. Also they bring into relative similarity to determine the importance of the text similarity between bugs.
Our work aims to solve both two categories. In finding relevant bug report, we adopt discriminative model, and be first one to utilize comments to improve the effectiveness of duplicate bug detection. In identifying duplicate bug, we leverage user profile and query feedback to help predict query bug report resolution. Detection of duplicate bugs is a necessary step to optimize resource allocation and improve working efficiency in the procedure of triage. In this work, we propose a practical duplicate bug reports detection method, which will help triagers to reduce their workload by combining existing two categories of meth-ods. PDB adopts three stage to detect duplicate bug reports. In modeling the similarity of two bugs, PDB is the first one to utilize comments to enhance the effectiveness. In identifying duplicate bug, PDB makes use of user profiles and query feedback. We have evaluate our algorithm on MeeGo X  X  bug reports which is issued in about one year. The experiment shows that PDB outperforms existing works [10] and state-of-the-art[12] in respective category of thread.
In future, we plan to investigate more bug reports from various software projects. Another idea is to utilize other text data like use cases and posts on online technical forums to enhance effectiveness of duplicate bug detection. Some of these are closely associated to and supplement to bug reports. Acknowledgements. This work is partially supported by National Science Foundation of China under grant number 60803022.

