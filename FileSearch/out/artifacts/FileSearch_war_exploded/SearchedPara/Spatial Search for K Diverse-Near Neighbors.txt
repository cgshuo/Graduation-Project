 To many location-based service applications that prefer di-verse results, finding locations that are spatially diverse and close in proximity to a query point (e.g., the current loca-tion of a user) can be more useful than finding the k nearest neighbors/locations. In this paper, we investigate the prob-lem of searching for the k Diverse-Near Neighbors ( k DNNs) in spatial space that is based upon the spatial diversity and proximity of candidate locations to the query point. While employing a conventional distance measure for proximity, we develop a new and intuitive diversity metric based upon the variance of the angles among the candidate locations with respect to the query point. Accordingly, we create a dynamic programming algorithm that finds the optimal k DNNs. Un-fortunately, the dynamic programming algorithm, with a time complexity of O ( kn 3 ), incurs excessive computational cost. Therefore, we further propose two heuristic algorithms, namely, Dist ance-based Brow sing (DistBrow) and Div ersity-based Brow sing (DivBrow) that provide high effectiveness while being efficient by exploring the search space prioritized upon the proximity to the query point and spatial diversity, respectively. Using real and synthetic datasets, we conduct a comprehensive performance evaluation. The results show that DistBrow and DivBrow have superior effectiveness com-pared to state-of-the-art algorithms while maintaining high efficiency.
 H.2.8 [ Database Applications ]: Spatial databases and GIS Algorithms, Experimentation K Diverse-Near Neighbor, K Nearest Neighbor, Spatial Di-versity, Location-Based Search  X  Gregory Ference is now affiliated with Google, Inc.
Owing to the maturity in wireless communications, mobile computing and positioning technologies, location-based ser-vice (LBS) applications have received tremendous market growth in the past decade. Various spatial queries, along with their supporting index structures and algorithms, have played a key role in the provisioning of these LBSs. Partic-ularly, the k nearest neighbor ( k NN) query, which finds a set of k location points closest to a given query point, has been widely used due to its large base of applications. How-ever, as LBSs grow, the conventional k NN query does not meet the new requirements of many LBS applications. For example, research on web search [2, 15, 17], recommender systems [1, 7] and databases [18, 19] has shown a trending need for factoring diversity in the returned query results. In recent years, the spatial database community has also be-gun to investigate the importance of querying for not only the nearest neighbors but also diversified neighbors [10, 11]. However, much effort from the research community is still needed in order to fully understand spatial diversity and develop efficient search algorithms for it. (a) Locations and query point
In this paper, we investigate the problem of searching for the k Diverse-Near Neighbors ( k DNNs) in spatial space. Given a set of location data points 1 and a query point q , a k DNN query searches for k locations that are close in prox-imity to q as well as spatially diverse with respect to q . Con-sider the example illustrated in Figure 1(a), which consists
A location data point is a data point associated with a location. In this paper, we also refer to it as a location point or location for simplicity. of eight locations denoted by p 1 ,p 2 ,  X  X  X  ,p 8 and a query point q . A 3NN query returns { p 3 ,p 4 ,p 5 } as the answer set. While this answer set is optimal in terms of proximity, those loca-tions are spatially clustered, representing homogeneous (i.e., spatially similar) and even redundant choices. On the other hand, a 3DNN query that returns { p 5 ,p 6 ,p 7 } could be very useful for a user looking for potential alternatives in different areas, as those locations, without compromising much of the proximity requirement, are more spatially diverse and infor-mative than the 3NNs. The k DNN query not only can be employed for LBSs (e.g., finding restaurants/gas stations or determining spatially distributed surveillance cameras [11]) but also can be potentially generalized for data mining, text summarization, and medical diagnosis, where unveiling di-verse data points close (similar) to a query point is preferred.
To achieve our goal, a key issue is to seamlessly accommo-date both factors of proximity and diversity into the query. For simplicity and clear semantics, existing works mostly adopt a linear weighting approach to combine proximity (or relevance/similarity  X  depending on the nature of their applications) and diversity metrics (usually defined based on certain measures of distance ) into one objective func-tion [10]. For example, Figure 1(b) shows a type of diversity based upon the pairwise distance between locations in an answer set, with farther apart locations being more diverse. Meanwhile, as shown in Figure 1(c), an angular-based diver-sity considers a pair of locations with wide-spread directions to have more diversity than those that are in close directions. For example, with respect to q , p 1 and p 4 are more diverse than p 4 and p 3 because  X  4 &gt;  X  3 . Note that the measure of angles between locations (i.e., angular diversity) is indepen-dent of the distance from the locations to the query point and thus can be used for the k DNN query seamlessly. Re-cently, the idea of angular similarity has been adopted to represent the diversity in the k DNN query [11]. Neverthe-less, as we will discuss later, the angular diversity measure proposed in [11] has deficiencies. In this paper, we develop a new diversity metric, called partitioning angular diversity , which is based upon the variance of the angles among the considered locations w.r.t. the query point q . The lower the variance of the angles, i.e., the sizes of the angles are more even, implies that the locations are better spread around q .
Accordingly, we define the k DNN query as a problem of finding k locations that aim to minimize their total distance to the query point while maximizing their diversity w.r.t. the query point. By exploring properties of the k DNN prob-lem, we develop a dynamic programming algorithm, which finds the optimal set of k locations from a set of n locations in polynomial time, i.e., O ( kn 3 ). Unfortunately, this dy-namic programming algorithm, requiring excessive visits of locations, incurs significant computational overhead, espe-cially for large datasets. Therefore, we propose two heuris-tic algorithms. First, Dist ance-based Brow sing (DistBrow) explores locations in the search space in increasing order of distance from the query point to increase the diversity of the solution. Second, Div ersity-based B rowsing (DivBrow) draws k evenly spread vectors originating from the query point and finds a location close to each vector as well as to the query point in order to find a solution with high diversity and proximity scores.

We conduct a comprehensive evaluation on our proposed algorithms in comparison with two state-of-the-art solutions, including distance browsing [14] and Index-Based Diverse Browsing [11]. The results for each dataset show that Dist-Brow and DivBrow are able to always achieve better effec-tiveness while maintaining high efficiency.

The contributions of this paper are summarized below.
In this section, we first review some relevant research work and then discuss metrics on angular diversity.
The topic of diversity has been widely studied for non-spatial items in web search [2, 4], information retrieval [5, 17], databases [18, 19] as well as recommender systems [1, 3, 20]. However, these prior works do not consider spatial aspects. For spatial diversity, Paramita et al. investigate result diversification in spatial image search [13]. Tang et al. show users X  preference for spatial diversity in search results [15]. Recently, Fraternali et al. consider spatial diversity for a location recommender system [7]. However, these works do not factor their results X  proximity to the query point.
Research on considering both the proximity and diversity in a spatial query has been reported in the literature [10, 11, 12, 16]. Lee et al. introduce the problem of finding the near-est spatial objects that completely surround a query point [12]. However, this work considers only objects with spatial extent instead of points. Moreover, it does not necessar-ily return k objects. Van Kreveld et al. consider diversity in ranking methods for spatial object retrieval [16], but it does not scale for LBSs due to ranking every location in the database for each query. To our knowledge, Harista is the first to consider the k DNN problem [10], which aims to find k nearest objects satisfying a diversity constraint, i.e., the objective function of its search is based only on proximity. Thus, the problem is different from our study. Accordingly, the authors develop two greedy-based algorithms that use the Gower coefficient [8] for diversity calculations, which is a variant of distance diversity.

Recently, Kucuktunc et al. explore the k DNN problem using angular diversity [11]. Among the methods proposed, the Index-Based Diverse Browsing (IBDB), based upon the R-Tree [9], is the most efficient. The idea is to explore the distance browsing feature of the best-first k NN search al-gorithm [14] by finding diversified objects near the query point. While traversing the R-tree to choose locations for its solution, IBDB prunes locations that are close (i.e., not diverse enough) to those locations already selected. How-ever, the proposed pruning strategy is not effective since it starts to discard locations even before a full initial solution is obtained, resulting in bad decisions. For example, for a query that weights proximity more than diversity, IBDB may prune a location p near the query point q simply because it is relatively close to an already chosen location. However, p may still be a better choice due to its closeness to q than another location which is diverse but far away from q .
Here we discuss the notion of angular diversity due to its important role played in the k DNN query under investiga-tion. Given a set of locations R and a query point q , we need an intuitive measure, in terms of angles among the locations, to represent the diversity among locations in R w.r.t. q .
Several simple approaches can be employed. For example, we may consider (i) the worst diversity (denoted as Div min i.e., adopt the minimum angle between two locations in R or (ii) the average pairwise diversity (denoted as Div i.e., obtain the angles between every pair of locations and average them, as the measure. Either way, the larger the angle, the greater the diversity because the locations are angularly farther apart. (a) Example loca-tions
Alternatively, an angular diversity technique, based upon finding the mean of all normalized location vectors (denoted as Div mean ), is proposed in [11]. In the following, we use Fig-ure 2 to illustrate the measure of Div mean . Consider the four locations in R and the query point q as shown in Figure 2(a). Each location p i  X  R is normalized to p 0 i  X  R 0 on the unit circle (such that its distance to the query point is 1) and can be treated as unit vectors from the query point towards the locations p i  X  R (see Figure 2(b)). Finally, as shown in Figure 2(c), the length of the mean location vector ~p m (de-rived as the average of the location unit vectors in R 0 ) can serve as the measure of diversity (i.e., Div mean = 1  X  X | ~p Thus, the diversity is maximized when || ~p m || = 0. The far-ther away ~p m is from q , the more locations are likely to be clustered in one direction, leading to lower diversity. Figure 3: Cases of low, medium and high angular diversity While the aforementioned angular diversity measures, i.e., Div min , Div apir and Div mean , all seem to be acceptable, we examine their feasibility with three test cases, intuitively representing low, medium and high diversity. Consider the locations p 1 ,p 2 ,p 3 ,p 4 and a query point q in Figure 3: Case (a) has all four locations located at the same angle w.r.t. q (see Figure 3(a)) and thus should result in the lowest diver-sity score; Case (b) has two groups of two locations at the opposite angles w.r.t. q (see Figure 3(b)) and should have a medium diversity score; and Case (c) has four locations equally spread w.r.t. q (see Figure 3(c)) and should have the highest diversity score. However, all the aforementioned angular diversity measures do not follow this intuition. For Case (a), all three measures can naturally obtain the lowest diversity, i.e., 0. However, for Case (b), Div min , Div Div mean are derived as 0, 1 3 and 1; and for Case (c), Div Div pair and Div mean are derived as 1, 1 3 and 1, respectively. In summary, the three diversity measures fail to differenti-ate the three cases in Figure 3 as low, medium, and high, because the returned values for some cases are the same. Therefore, we strive to develop a new measure for angular spatial diversity.
In this section, we first detail the proximity and diversity metrics and then formally define the k DNN query.
To address the deficiencies of the angular diversity metrics discussed previously, we introduce the partitioning angular diversity metric, denoted as Div part . We observe that the lo-cations of a given candidate set partition the angular space with the query point q as the pivot. Given k locations, the average angle between the locations is 2  X  k . If the k loca-tions are evenly spread from their neighbors in the angular space (i.e., each angle is 2  X  k ), this intuitively represents the maximal angular diversity (as exemplified by the test case (c) discussed earlier). Thus, our idea behind Div part measure the diversity based on the variance of the angles partitioning the space. Notice that the variance is 0 if the angular space is partitioned evenly (which has the maxi-mal diversity). Thus, we use the variance to measure how far/different a given set of angles is from the even partition scenario. Given k locations evenly partitioning the angular space around the query point, their angles tend to be near . However, when the locations are clustered (i.e., not di-verse), the partitioning angles vary a lot, which results in a large variance. We formally define the partitioning angular diversity and other relevant notions leading to it below.
Definition 1. Partitioning Angles. Given a query point q and k locations R , let p i ( i = 1 ,  X  X  X  ,k ) denote the loca-tions in sorted counter-clockwise order w.r.t. q (with p 1 trarily designated). We denote the counter-clockwise angle  X  ( p k ,q,p 1 ) as  X  k . Thus, the set of angles {  X  1 ,  X  X  X  , X  the partitioning angles of R w.r.t. q and P k i =1  X  i = 2  X  . Recall Figure 1(c). The partitioning angles of { p 1 ,p 2 w.r.t. q in the example form the set {  X  1 , X  2 , X  3 , X  4 we define the variance of the partitioning angles w.r.t. q and then proceed to define Div part .

Definition 2. Variance of the Angles. Given a set of k locations R and a query point q , the variance of the angles (or simply variance ) of R w.r.t. q is as follows:
Definition 3. Partitioning Angular Diversity. Given a set of locations R and a query point q , the partitioning angular diversity is as follows: where Var min ( k ) is the minimum possible variance of any k locations and Var max ( k ) is the maximum possible variance of any k locations.

Intuitively, Var min ( k ) = 0 when all the partitioning an-gles are the same, i.e., 2  X  k . To derive Var max the following lemma showing a partitioning angle set where Var max ( k ) is reached.

Lemma 1. The maximum possible variance for any k lo-cations w.r.t. a query point q can be reached when the par-titioning angles  X  1 = 2  X  and  X  i = 0 for i = 2 ,  X  X  X  ,k .
Proof. The basic idea of the proof is to convert it into a quadratic programming problem with objective function Var ( q,R ) and constraint P k i =1  X  i = 2  X  and then prove max-imality by contradiction. Due to the space constraint, we refer readers interested in the formal proof to [6].
Accordingly, we derive the maximum variance for k loca-tions as follows.
 Theorem 2. For k locations, the maximum variance is Var max ( k ) = 4  X  2 ( k  X  1) /k 2 .

Proof. From Lemma 1, the set of locations R with  X  1 = 2  X  and  X  i = 0 for i = 2 , 3 ,  X  X  X  k maximizes variance. This yields: Var max ( k ) = By Var min ( k ) and Var max ( k ), we rewrite the definition of Div part in Eq. (2) as follows:
Div part ( q,R ) = 1  X  Var ( q,R )
Unlike the other angular diversity metrics, Div part agrees with the intuition behind the three test cases in Figure 3 with the diversity scores strictly increasing from 0 , 2 3 corresponding to Cases (a), (b) and (c). For the rest of this paper, we refer to Div part as Div .
As discussed in the Introduction, a k DNN query shall also consider the proximity of searched locations to the query point q . Thus, we define a proximity metric Prox for a po-tential answer set of locations by their average distance to the query point. Based on Definition 4, a larger average dis-tance causes the Prox score (with range [0 , 1]) to be lower.
Definition 4. Proximity Metric. Given a query point q and a candidate set of locations R , the proximity metric is defined by the following equation: Prox ( q,R ) = 1  X  where dist () is a distance function and  X  is an application (or user-specified) parameter for normalization.

We aim to normalize the Prox score within [0 , 1] and thus choose to model  X  in Eq. (4) as an application-dependent constant (which can also be a user-specified parameter). Take a typical restaurant search application as an example. Having  X  = 50 km is quite reasonable, as using the maximal distance in the database (e.g., 10 , 000 km ) for normalization may render very high Prox scores for many location candi-dates. Thus, our formulation of Prox provides flexibility for various applications to allow a sizable difference in proxim-ity scores for different sets of candidate locations. Moreover, the result set with the average distance smaller than  X  is pre-ferred. Later in Section 7, the impact of  X  on performance of k DNN algorithms will be studied in detail.
The primary goal of the k diverse-near neighbors ( k DNN) query is to find a set of k  X  X patially diverse X  locations which are  X  X lose in proximity X  to a query point q from a location dataset P . To integrate the measures of diversity and prox-imity for selecting a candidate set of locations, we first define in Definition 5 a metric DivProx by introducing a parameter  X  to control the weights of Div and Prox in the query. Then we formally define the k DNN query in Definition 6.
Definition 5. DivProx. Given a candidate result set of locations R and a query point q , the integrated diversity-proximity measure, denoted by DivProx , of R w.r.t. q is defined as follows:
DivProx ( q,R, X  ) =  X   X  Div ( q,R ) + (1  X   X  )  X  Prox ( q,R ) (5) where  X   X  [0 , 1] is a user-specified weighting parameter.
In Eq. (5),  X  controls whether diversity or proximity is more important, which is similar to the problem definition in [11]. If the user wants highly diversified results that are not clustered together,  X  should be set to a large number. Conversely, if the user prefers results that are close to the query point,  X  should be set to a small number.
 Definition 6. k DNN Query. Given a location dataset P , a query point q , and a weighting parameter  X  , a k diverse-near neighbors ( k DNN) query returns a result set of k lo-cations R (  X  P ) that maximizes DivProx ( q,R, X  ), i.e., with | R 0 | = k such that DivProx ( q,R 0 , X  ) &gt; DivProx ( q,R, X  ).
To answer the k DNN query, a straightforward approach is to try all possible combinations of k locations in order to obtain the solution R with maximal DivProx ( q,R, X  ). How-ever, this approach needs to examine C n k combinations for n candidate locations in P , which is computationally intensive. Thus, we develop a dynamic programming (DP) algorithm to solve the k DNN problem.

The basic idea of our DP algorithm is to build up a solu-tion from smaller solutions. This is achieved by exploiting a property that the locations in a k DNN solution partition the whole angular space. Logically, the locations in the uni-versal set of locations P = { p 1 ,p 2 ,  X  X  X  ,p n } can be circularly sorted w.r.t. the query point q . By considering one of the locations in P at a time as the start point (denoted as p the DP algorithm iteratively finds the optimal solution of size x  X  1 for starting location p 1 and each location p  X  P being the last location. Let R ( x  X  1) p denote this optimal par-tial solution described above. Then, for each p i  X  P , we can derive R ( x ) p i from the optimal partial solutions R ( p = p 1 ,p 2 ,  X  X  X  ,p i  X  1 ) obtained previously. By incrementing x (i.e., the size of the optimal partial solution) until x = k , DP obtains the optimal solution w.r.t. p 1 . Finally, the re-sult is returned by choosing the best among the optimal solutions w.r.t. various p 1 .
 Algorithm 1 Two-Phase Dynamic Programming Algo-rithm 1: //Input: query point q , no. of DNN k , set of candidate 2: //Output: set of locations R 3: R 1 = DP( q,k,P, X  ) 4: R 2 = DP( q,k,P, 1) 5: if DivProx ( q,R 1 , X  )  X  DivProx ( q,R 2 , X  ) then 6: return R 1 7: else 8: return R 2 Algorithm 2 DP 1: //Input: query point q , no. of DNN k , set of candidate 2: //Output: set of locations R max 3: Let R max be an empty set of locations 4: DivProx max = 0 5: for all p  X  P do 6: Organize locations in P in counter-clockwise order 8: for x = 2 to k do 9: for i = x to | P | do 10: Calculate f ( p i ,x ) and store the best p j for back-11: Identify max i  X  [1 ,n ] f ( p i ,k ) and backtrack to obtain R 12: if DivProx ( q,R, X  ) &gt; DivProx max then 13: R max = R 14: DivProx max = DivProx ( q,R, X  ) 15: return R max
A technical challenge faced in the DP algorithm is that the proximity component Prox ( q,R ) in our objective func-tion is not additive since we have Prox ( q,R ) = 0 when avgdist ( q,R ) = P p  X  R dist ( q,p ) /k &gt;  X  for every possible R , i.e., Prox ( q,R ) no longer decreases when avgdist ( q,R ) &gt;  X  . To overcome this challenge, as shown in Algorithm 1, we run the DP algorithm twice, one with the proper  X  setting and the other with  X  = 1 . 0. Let R opt be the optimal solu-tion. If avgdist ( q,R opt )  X   X  , the first DP run will find R However, if avgdist ( q,R opt ) &gt;  X  , i.e., Prox ( q,R second DP run (with  X  = 1 . 0) will find the optimal solution. The proof of correctness is discussed later in the section.
Algorithm 2 details the idea of the dynamic programming algorithm discussed earlier. Given a location set P and a query point q , for each p  X  P , we set it as p 1 and rela-bel the locations in P in counter-clockwise order w.r.t. q that includes p = p 1 in the solution. The algorithm de-rives and maintains f ( p i ,x ) as well as the maximum DivProx score corresponding to the partial optimal solution R where x locations are selected from location p 1 ,  X  X  X  ,p tion. When x = 1, the only valid solution is p 1 , which is ity score contribution of p 1 to DivProx . For x = 2 to k  X  1, we calculate f ( p i ,x ) using solutions of f ( p j ,x  X  1) for different j as follows: f ( p i ,x ) = max It calculates corresponding DivProx according to f ( p j ,x  X  1) associated with p j . Every j in [1 ,i ) is examined to ensure that f ( p i ,x ) indeed is the best solution in this iteration. In gle variance between p j and p i , while (1  X   X  )  X  dist ( q,p corresponds to the change of proximity in selecting p i . No-tice that Prox ( q,R ) here is allowed to be negative (but the scenario is taken care of by Algorithm 1).

Finally, when x = k , the variance of the angle between the last selected location and p 1 is necessary to be incorporated. Thus f ( p i ,k ) is derived according to the following equation: f ( p i ,k ) = max The solution is obtained by comparing all f ( p i ,k ) and re-trieving the p i with the largest f ( p i ,k ). In addition, we record the selected p j (from the chosen f ( p j ,x  X  1)) for each f ( p i ,x ) for backtracking to obtain the set of selected loca-tions. After repeating this process for all different assign-ments of the first location p 1 , we choose the solution R with the largest DivProx score.

Next, we prove the correctness of the two-phase DP algo-rithm with Lemma 3 and Theorem 4.

Lemma 3. The k DNN solution R  X  obtained by the dy-namic programming module (i.e., Algorithm 2) is optimal if R  X  satisfies avgdist ( q,R  X  )  X   X  .

Proof. Let R  X  denote the solution obtained by the DP module. Assume that R  X  is not optimal. There must exist at denotes a location considered in the x th iteration of the algo-the backtracking continues, there must exist a location p such that f ( p (1) i , 1) is not optimal. Nevertheless, since there is only one solution for f ( p (1) i , 1) = 1  X  (1  X   X  ) dist ( q,p it contradicts that f ( p (1) i , 1) must be optimal.
Theorem 4. The two-phase dynamic programming algo-rithm obtains the optimal k DNNs.

Proof. We denote the solution of the optimal, the first, and the second dynamic programming algorithm solutions as R opt , R 1 and R 2 , respectively. The algorithm returns R or R 2 depending on which has the largest DivProx score, so the optimal answer will be obtained if it is either R 1 or R Consider the two cases for R opt : (1) avgdist ( q,R and (2) avgdist ( q,R opt ) &gt;  X  . For Case (1), Lemma 3 states that R 1 returns the optimal solution, so the two-phase al-gorithm is optimal. For Case (2), R 2 is the solution with the highest diversity because only diversity is considered when  X  = 1 . 0, which means Div ( q,R 2 )  X  Div ( q,R opt avgdist ( q,R opt ) &gt;  X  , Prox ( q,R 2 )  X  Prox ( q,R opt Prox ( q,R 2 , X  )  X  Prox ( q,R opt , X  ), which means R 2 has to be an optimal solution.

For complexity analysis, denote the number of locations in location set P as n . For each possible p 1 in Phase 1, the number of f ( p i ,x ) involved in the algorithm is O ( nk ), and it takes O ( n ) time to find each f ( p i ,x ). Afterward, we extract the maximum of f ( p i ,k ) for i  X  [1 ,n ] in O ( n ) time. Since we have to repeat the above process for each location being assigned as p 1 , the time complexity is O ( kn for Phase 1. Similarly, the time complexity of Phase 2 is O ( kn 3 ), so the proposed two-phase dynamic programming algorithm requires O ( kn 3 ) time.
Due to the excessive computational overhead of the DP algorithm, in this section, we propose a heuristic algorithm, namely, Dist ance-based Brow sing (DistBrow) , which is built upon the R-Tree. By distance browsing [14], DistBrow first obtains an initial result set R of k NNs, which guarantees the Prox component in DivProx to have the highest score. Next, it iteratively visits locations with increasing proximity from q , hoping to increase DivProx by trading some Prox score for better Div . The efficiency of query processing is achieved by pruning some redundant visits of the R-tree entries (based on their estimated Prox and Div ) until the DivProx score does not improve for a certain number of steps (controlled by a predetermined threshold as the stop condition). Un-like IBDB [11], the pruning operation occurs after an initial result set is selected, which allows DistBrow to more accu-rately determine if an R-tree entry should be pruned and whether a location should be inserted into the result set. Algorithm 3 Distance-based Browsing 1: //Input: query point q , no. of DNN k , R-Tree root page 2: //Output: set of locations R 3: Let R be an empty set and counter = 0 4: Let PQ be the priority queue (ordered by increasing 5: while PQ 6 = {} and counter &lt;  X  do 6: Pop the head entry e from PQ 7: if e is a location then 8: if | R | &lt; k then 9: R = R  X  e 10: else 11: R 0 = R  X  e 12: Find r such that max r  X  R 0 DivProx ( q,R 0 \ r, X  ) 13: R = R 0 \ r 14: if r equals e then 15: counter = counter + 1 16: else 17: counter = 0 18: else 19: if | R | &lt; k or prune( q,e,R, X  ) is false then 20: Push entries in the bounding box e into PQ 21: return R
Algorithm 3 shows DistBrow based on the basic idea dis-cussed above. One point worth noting is that when location p is visited (i.e., dequeued), we evaluate the objective func-tion for each possible combination of k locations in R  X  p to determine which location should be removed (see Line 12), which intuitively requires O ( k 2 ) computation. However, by first computing a total DivProx cost based on R  X  p (an O ( k ) operation) and then refining the DivProx cost for each com-bination (incurs O (1) computation for each of them), the total time complexity is actually O ( k ).

The pruning of minimal bounding boxes (MBBs) in the R-tree and locations inside is another important point explored in DistBrow. Let R denote the current solution set. An MBB (denoted by M ) under examination and its associated R-tree page can be pruned, if replacing any location(s) in R with any possible locations in M does not improve DivProx . As the R-tree is traversed in increasing proximity to q , all the locations within M will only be farther away than the locations in R . Thus, access to the page associated with M could be avoided if the gain in Div for visiting locations in M does not outweigh the loss in Prox . Accordingly, we define MaxProx to measure the upper bound of Prox (in case that some locations in M would replace locations in R ). Definition 7. MaxProx. Given a candidate result set R = { p 1 ,  X  X  X  ,p k } , a query point q , and an MBB M , the upper bound of Prox for a location in M to replace a location in R is: MaxProx ( q,R,M ) = where p 1 ,  X  X  X  ,p k  X  1 are the k  X  1 closest locations in R to q and mindist ( q,M ) is the minimum distance between q and M .
 In other words, the upper bound of Prox is obtained when there exists a point p  X  M (with distance mindist ( q,M )) to replace p k (the farthest location in R from q ).

We also want to estimate an upper bound for Div . Since the partitioning angles of R is known, the issue becomes how much we could make the partitioning angles more evenly spaced. Take Figure 4 as an example. Let R = { p 3 ,p 4 be the current result set. As shown, the partitioning angles of R are  X  ( p 4 ,q,p 5 ) , X  ( p 5 ,q,p 3 ) and  X  ( p 3 ,q,p MBB R 1 in the figure. The first two angles are completely covered by R 1 and the last angle, i.e.,  X  ( p 3 ,q,p 4 ), is partially overlapped. If we are to use locations in R 1 to make the angles more evenly spaced than the partitioning angles of R , a good strategy is to use the lower-left and lower-right corners of R 1 to replace p 3 and p 4 . As such,  X  ( p 4  X  ( p 5 ,q,p 3 ) are replaced by larger angles and  X  ( p replaced by a smaller angle, resulting in higher diversity.
Based on the above observation, an idea is to explore the  X  X arge partitioning angles X  (i.e., with an angle size greater than 2  X  k ) of R that do not overlap with M . Since these large angles may contribute to the increase of angle vari-ance, we find a lower bound for the angle variance score to derive the upper bound for Div . Let non-overlap-angle(  X ,M ) denote the largest portion of an angle  X  that does not over-lap M , e.g., as shown in Figure 4, the shaded portion of  X  ( p 3 ,q,p 4 ) represents non-overlap-angle(  X  ( p 3 ,q,p cause the non-shaded portion in  X  ( p 3 ,q,p 4 ) overlaps R
Let  X  1 , X  2 ,  X  X  X  , X  k be the partitioning angles of R . We use LA to denote the  X  X arge X  non-overlapped-angles of these partitioning angles w.r.t. M , i.e.,  X  i  X  [1 ,k ] , { non-overlap-angle(  X  i ,M ) | non-overlap-angle(  X  i ,M )  X  2  X  k } . As it is not possible for any location in M to make an angle in LA smaller, these angles in LA contribute to the upper bound of Div . Also note that we do not conversely penalize for an angle  X  i being smaller than 2  X  k , because either location p or p i +1 can be removed, which could replace  X  i with a new angle closer to 2  X  k , causing less variance. Moreover, the remaining angles may help to maximize the diversity when they have the same size. Therefore, we define MaxDiv to measure the upper bound of Div as follows.

Definition 8. MaxDiv. Given a candidate result set R , the query point q , and an MBB M , the upper bound of Div for having locations in M to replace locations in R is:
The first summation in Eq. (7) takes into account the larger angles while the second summation incorporates the remaining angles by assuming they have equal size. Ac-cordingly, we define MaxDivProx and the pruning rule for DistBrow below.

Definition 9. MaxDivProv. Given a candidate result set R , the query point q , the control parameter  X  and an MBB M , the upper bound of DivProx is:
Pruning Rule 1. Given MBB M , the DistBrow algorithm prunes M if MaxDivProx ( q,R,M, X  ) &lt; DivProx ( q,R, X  ).
Next, we introduce another heuristic algorithm, Div ersity-based Brow sing (DivBrow) , which aims to find k DNNs by exploring locations close to a given set of k vectors evenly spaced w.r.t. the query point q . Contrary to DistBrow, which explores locations in increasing distance from q , Div-Brow incorporates diversity into search. The idea can be illustrated by Figure 5, which shows 3 vectors that have the  X  X erfect X  diversity. For each vector, our goal is to choose a location near the vector that is close to q . In addition, we take the distance threshold  X  to filter locations/MBBs far away from q to enhance efficiency. By considering locations close to q , the proximity score will be high. By choosing a location nearby the vector, the k locations in the result set will ideally have a high diversity score. To improve the effectiveness, we run the algorithm for different sets of k vec-tors by rotating the vectors for nr times. Finally, the best result set with the highest DivProx score is returned. The pseudocode is shown in Algorithms 4 and 5.
 Algorithm 4 Diversity-based Browsing 1: //Input: Query point q , No. of DNN k , R-Tree root 2: //Output: Result set of k DNN R 3: Initialize R = {} 4: for i = 0 to nr do 5: Search R-tree to find ( i + 1)th NN p i 6: Let  X  init be the polar angle of p i w.r.t. q 7: Initialize R 0 = { p i } 8: for j = 1 to k  X  1 do 9: Let  X  j = (  X  init + j  X  2  X  k ) mod 2  X  10: Add BestLocationToAngle( q,root, X  j , X  ) to R 0 11: if DivProx ( q,R 0 , X  ) &gt; DivProx ( q,R, X  ) then 12: Let R = R 0 13: return R Algorithm 5 Best Location to Angle 1: //Input: query point q , R-Tree root page root , angle  X  , 2: //Output: Location l with largest heuristic score 3: Let PQ be the priority queue (ordered by increasing 4: while PQ is not empty do 5: Pop the head entry e from PQ 6: if e is a location then 7: return e 8: else 9: for all entries of bounding box c of e do 10: if mindist( c )  X   X  then 11: Push c into PQ
To choose and place the sets of evenly spaced vectors, our strategy is to iterate over the top nr nearest neighbors as the anchor vector to determine the rest of vectors. For example, in Figure 5, p 5 is used to determine the (bold) anchor and other vectors. Alternatively, p 3 (the second nearest neighbor to q ) may serve as the anchor vector to decide other vectors. In DivBrow, the anchor vectors are chosen based on the locations nearest to the query point because they have high proximity and diversity scores (due to the  X  X erfect X  match with the anchor vector).

After a set of vectors is heuristically determined, the task of choosing a location for a given vector and q can be simpli-fied. Consider a vector ~v  X  where  X  denotes the angle between ~v  X  and the horizontal vector w.r.t. q . Notice that to esti-mate the potential contribution of a candidate location p to DivProx , we need to consider both components of Div and Prox . While Prox between p and q can be easily derived, Div can only be accurately determined when a set of locations R is obtained. Thus, we use the angular difference w.r.t. q between a location p and ~v  X  to estimate the potential contri-bution of p to Div . Here, we introduce a heuristic measure, namely, M inD ivP rox ( MDP ) , as follows: where  X   X  X  X  qp,~v  X  denotes the angle between  X  X  X  qp and ~v between [0, 1] and favor locations very close to the vector. To minimize MDP , DivBrow sets  X  ( q,p,~v  X  ) = 1 (i.e., worst score) if the location is too far away from the vector. Note that MDP guides the traversal of the R-tree in Div-Brow by ordering entries in the priority queue based on as-cending order of MDP . This allows DivBrow to take priority to visit a location/MBB that is both close to q and ~v  X  . Thus, we further extend MDP for an MBB M as follows: MDP ( q,M,~v  X  ) =  X   X  min Since MDP heuristically captures both proximity and di-versity (via angular difference to ~v  X  ), DivBrow is able to quickly find excellent candidate locations. Moreover, MDP facilitates efficient pruning in DivBrow as the MBBs with the MDP measure larger than the best location visited are all pruned without accessing corresponding R-tree pages.
Here, we conduct comprehensive experiments using two datasets collected from Foursquare and Gowalla as well as synthetic datasets to evaluate the proposed algorithms.
In our evaluation, we implement an R-Tree to index the spatial data. For the evaluation, we use a server that allo-cates each process a 3.0 Ghz Intel Xeon E5450 Quad-Core processor with 4 GB memory. We perform experiments to compare our optimal dynamic programming (DP) algo-rithm, our heuristic algorithms, DistBrow and DivBrow, as well as two state-of-the-art algorithms, distance browsing (KNN) [14] and Index-Based Diverse Browsing (IBDB) [11]. While using DivProx as the main metric for effectiveness along with Div and Prox , we use both execution time and number of disk accesses to evaluate the efficiency of algo-rithms. In the experiments, we set i) the control parameter  X  (chosen uniformly in the range [0 , 1]) to decide the weight of proximity and spatial diversity, ii) the number of loca-tions k (default = 5), iii) the distance for normalization  X  (default= 50km), and iv) the query point (chosen in ran-dom) to run each algorithm to find k DNNs. We first evaluate different algorithms under different  X  . As the DP algorithm does not scale well (due to its O ( kn time complexity), we first use a subset of the Foursquare dataset (locations in New York City, USA) and show the results in Figure 6. Note that, due to limited space, Fig-ure 6 shows both the number of disk accesses (in form of curves) and the execution time (in form of bars, following the order of methods listed in the legend), corresponding to  X  = 0 . 3 and 0 . 7. Since DP returns the optimal solution for DivProx , it obviously outperforms all the other algorithms in terms of the DivProx metric. However, its efficiency is much worse than others under various  X  . Note that our heuristic algorithms, DistBrow and DivBrow, perform very well  X  returning locations with high DivProx scores (com-parable to DP) and incurring low execution time and disk access. IBDB and KNN tend to have tolerable effectiveness when  X  is smaller, but their inability to choose very diverse locations (evident by Figure 6(b)) hurts their performance. Since KNN chooses the k NNs for each query, its solutions will maximize the proximity metric (see Figure 6(c)), which allows it to obtain quality solutions for smaller  X  but not for larger  X  . Compared to the optimal solution, DistBrow and DivBrow have very similar curves while KNN and IBDB have different curves. This demonstrates the suitability of using DistBrow and DivBrow for searching k DNN.

When analyzing diversity and proximity (see Figure 6(b)-(c)), we find that DP obtains a better solution because its solution has a larger diversity score. This can also be seen in Figure 6(a) because when diversity is more important (i.e., larger  X  ), the distance between DP and the other algorithms increases. For diversity, all but KNN tend to obtain solu-tions with larger diversity scores when  X  increases, as all methods (except for KNN) factor in diversity in their algo-rithms. Typically, DP has the highest diversity followed by DistBrow (except for large  X  ), DivBrow, IBDB and KNN. For proximity, we see that all of the algorithms obtain re-sults with a large score. When  X  gets larger, we see DP, DistBrow and DivBrow sacrifice some of its proximity score to choose a more diverse solution, which helps the DivProx metric. KNN and IBDB typically have the best proximity metric, but its low diversity score hurts the overall score. When  X  = 1 . 0, we see that the algorithms X  proximity scores typically has a large drop because proximity is not a factor and the algorithms are searching for a very diverse solution.
For efficiency, Figure 6(d) shows the execution time and disk accesses for the different algorithms. The figures shows that DP performs worst since it has to access every disk page to perform its operations and take a very large time per query (about 1 second). When we choose a reasonably large dataset, DP does not scale well so we do not show the result of DP in other experiments (which use a larger data-set). Among other algorithms, KNN has the best efficiency followed by IBDB. DistBrow and DivBrow have compara-ble efficiency. With our experimental system, all execution times are within 7 ms per query, which are quite reasonable.
To see how the algorithms scale for a more realistic num-ber of locations in a LBS, we use a Foursquare dataset that includes 155,321 locations from around the world (mostly USA). Also, we performed experiments using a Gowalla data-set of 1,070,338 locations. Due to space constraints and very similar results, the Gowalla dataset is omitted.

The result is shown in Figure 7. The effectiveness met-rics show a similar phenomenon in comparison to the small Foursquare dataset with the exception that IBDB tends to perform noticeably better than KNN due to its more diverse locations. However, the effectiveness does not match Dist-Brow and DivBrow, with the latter two performing about 20% better in terms of the DivProx metric.

For efficiency, all algorithms start very similar for small  X  , but as  X  increases, the execution time and disk accesses for DivBrow noticeably increase at a faster rate than the other algorithms. DistBrow tends to have slightly worse efficiency than IBDB and KNN, but the difference is very small, showing that DistBrow maintains high efficiency and effectiveness. Overall, DistBrow and DivBrow show supe-rior effectiveness in comparison to IBDB and KNN while maintaining competitive efficiency.
In this section, we investigate performance sensitivity of algorithms to a number of query and system parameters. First, by varying the query parameter k from 5 to 50, we plot the experimental results in Figure 8. For all k , DistBrow and DivBrow show superior effectiveness over IBDB and KNN. Looking into the components of DivProx , we observe that for all algorithms, the Prox of the selected locations decreases linearly, because the average distance of the locations from q increases along with k . Among them, IBDB decreases at a larger rate than the other algorithms. For diversity, IBDB initially does not perform well, but quickly starts choosing diverse locations, surpassing both DistBrow and DivBrow. This allows IBDB to get close to the results of DistBrow and DivBrow, but the latter two always outperform IBDB. For disk accesses, we see DistBrow and DivBrow having slightly worse performance than IBDB when k is small. However, as k increases, IBDB starts increasing exponentially for the number of disk accesses, which shows that it may not be able to scale to a large k . The execution time (shown for k = 15 , 40) for IBDB also increases largely as k increases, but the execution time for DivBrow is still worse. However, DistBrow along with KNN consistently has low execution time and disk accesses. Along with its superior effectiveness, DistBrow performs best for larger k .

Next, we investigate the impact of  X  , which controls the normalization of the proximity metric. The results are shown in Figure 9. As  X  increases, the overall DivProx of all algo-rithms increase. Among them, both DistBrow and DivBrow consistently and significantly outperform IBDB followed by KNN. Overall, changing  X  does not affect diversity but pos-itively affects proximity (since  X  makes the number we sub-tract from the optimal score smaller). For efficiency (bars showing  X  = 50 , 125 for execution time), KNN has the best efficiency followed by IBDB, DistBrow and DivBrow.

Lastly, we test the sensitivity of the algorithms to the data distribution using synthetic datasets based on the Zipf distribution [21]. By varying the skewness of the locations in the datasets from 0 to 1 (where 0 represents a uniform distribution while 1 represents high skewness of the data), we plot the experimental results in Figure 10. In all exper-iments, DistBrow and DivBrow consistently outperform all algorithms in terms of effectiveness. Overall, the DivProx score increases as the skew of the dataset increases, which is caused by an increase in the proximity scores. All the algorithms choose very similar locations in terms of proxim-ity, but the difference in the algorithms appears in diversity. DistBrow has the best effectiveness followed by DivBrow, IBDB, and KNN. For efficiency, the algorithms have sim-ilar number of disk accesses for small skewness, but Dist-Brow rises slightly with the increase in skewness while Div-Brow shows a large increase. For execution time (shown for  X  = 0 . 3 , 0 . 7), IBDB, DistBrow and KNN perform queries quickly while DivBrow also increases with skewness.
This paper explores the k Diverse-Near Neighbor ( k DNN) problem, which differs from the conventional k nearest neigh-bor problem by exploring the spatial diversity of the candi-date locations. We develop a new angular diversity met-ric, which is more natural than alternative angular metrics, and integrate it with a proximity metric defined by conven-tional distance measure to formulate the k DNN query. We first propose a dynamic programming algorithm that opti-mally answers the k DNN problem and then introduce two heuristic algorithms, Distance-based Browsing (DistBrow) and Diversity-based Browsing (DivBrow), to find close-to-optimal solutions efficiently. We conduct a comprehensive performance evaluation using real and synthetic datasets. The results show the superiority of the candidate locations chosen by DistBrow and DivBrow.

For future work, we plan to explore the issues of non-spatial diversity using semantic tags, e.g.,  X  X estaurant X . [1] G. Adomavicius and Y. Kwon. Improving aggregate [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [3] K. Bradley and B. Smyth. Improving recommendation [4] G. Capannini, F. M. Nardini, R. Perego, and [5] C. L. Clarke, M. Kolla, G. V. Cormack, [6] G. Ference. Location recommendation for mobile users [7] P. Fraternali, D. Martinenghi, and M. Tagliasacchi. [8] J. C. Gower. A general coefficient of similarity and [9] A. Guttman. R-trees: A dynamic index structure for [10] J. R. Haritsa. The kndn problem: A quest for unity in [11] O. Kucuktunc and H. Ferhatosmanoglu.
 [12] K. C. K. Lee, W.-C. Lee, and H. V. Leong. Nearest [13] M. L. Paramita, J. Tang, and M. Sanderson. Generic [14] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest [15] J. Tang and M. Sanderson. Evaluation and user [16] M. Van Kreveld, I. Reinbacher, A. Arampatzis, and [17] R. H. van Leuken, L. Garcia, X. Olivares, and R. van [18] M. R. Vieira, H. L. Razente, M. C. N. Barioni, [19] M. R. Vieira, H. L. Razente, M. C. N. Barioni, [20] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and [21] G. Zipf. Human behaviour and the principle of
