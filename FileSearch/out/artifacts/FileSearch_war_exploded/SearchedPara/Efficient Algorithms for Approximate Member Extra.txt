 document that approximately ma tch some strings in a given dictionary. This problem is import ant in a variety of applications such as named entity recognition and data cleaning. We solve this problem in two steps. In the first st ep, for each substring in the text, from the substring. In the second step, each candidate string is verified to decide whether the substring should be extracted. We develop an incremental algorithm using signature-based inverted lists to minimize the duplicate li st-scan operations of overlapping windows in the text. Our experimental study of the proposed algorithms on real and synthetic data sets showed that our solutions significantly outperform existing methods in the literature. H.2.4 [ Database Management ]: Systems  X  Textual Databases Algorithms Approximate Member Extractio n, Filtration-verification, Approximate string matching , Incremental Computation text document M that approximate ly match (e.g. having similarity scores above a given threshold  X  ) some strings in a given dictionary R of strings. This problem, called AME (short for A pproximate M ember E xtraction ), arises in many applications, as illustrated by the following examples. Named Entity Recognition : With a given document, we want to locate pre-defined entities such as person names, conference names, and company names. We want this extraction to be appro-ximate , i.e., we allow slight mismat ches in the substrings. For instance, as shown in Figure 1, suppose we have a collection of conference names, such as  X  X CM SIGMOD X  and  X  X IKM Conference. X  We want to extract all conferen ce names from a given document. We want to find matches such as  X  X IKM 2009 Conference X  and  X  X IKM International Conference X , even though they do not match the string  X  X IKM Conference X  in the dictionary exactly. Data Cleaning : Documents in many applications could be  X  X irty X  when it contains inconsistencies. Often we need to clean the inconsistencies. We need to perform data cleaning and integration by identifying the dirty words based on an existing dictionary. each substring m of M and check if m matches strings in R approximately. Se veral algorithms have been proposed for doing the checking efficiently using two steps. In the first step, we filter the dictionary strings that are very different from m. In the second step, we compute the similarity between the remaining candidate strings and the string m to verify its approximate membership. is based on an inverted index on the dictionary R. In this method, given string m, we can find ca ndidate similar strings in the dictionary by accessing the lists of the tokens in m and finding those string ids that have enou gh occurrences on the lists. The second method is based on signatures generation [1, 4]. This method focuses on exploring signa ture schemes that convert a their signatures. methods. For instance, Wang et al. propose a method called NGPP [13] to solve the AME problem by assuming the edit-distance variations (an implicit signature). Then by generating the neighborhood of partitions of document substrings and probing the index, the algorithm filters most irrelevant strings and performs verifications for the remaining strings. Chakrabarti et al. [2] studied how to solve the AME problem with other similarity score functions such as Jaccard coefficient using a method called ISH (Inverted Signature-based Hashtabl e), which encodes a string and its signatures into a 0-1 matrix. After generating a 0-1 matrix by computing the  X  X itwise-or X  of all small matrices encoded with dictionary strings, ISH converts the problem of searching possible evidence into finding a certain 0-1 submatrix in the big matrix. observations: (1) The ISH method ge nerates signatures for strings and prune strings that share signatures with a total weight under a certain lower bound (see Para 8, Page 4 of [2]). Unfortunately, the lower bound may be too high, ca using false negatives (see Example 1 for a counter-example). (2) The filtration processing of ISH involves an NP-Complet e problem requiring finding solid submatrix (containing only 1) from a given 0-1 matrix under certain constraints. We give the reduction proof in this paper from an NP-Complete problem: Balanced Complete Bipartite Subgraph (see Appendix). Due to the intrinsi c complexity, [2] solves this problem by a simple heuristics, which significantly increases the number of false positives, and thus incurs numerous I/Os in the verification phase and deteriorates overall performance. But our research in this paper shows that we can avoid the NP-complete complexity to efficiently solve AME problem by adopting a novel index structure, which effectivel y controls the number of false positives and consequently improves the overall performance. (3) NGPP[13] and ISH[2] both require that  X  is provided when preprocessing the data and generating an index for the filter. Once thresholds other than  X  unless the index is re-generated. Under Jaccard similarity measur ement, with weight assignment r= X  X cde X  matches m= X  X bef X  unde r the similarity threshold  X  =1/3. But from the following table we see that [2] incorrectly threshold 1/3, though their real similarity is 1/3. String r= X  X cde X  {b,c,d}  X (r)= 1.187 m= X  X bef X  {a,b,e}  X (m)= 3.67 y We adopt the prefix filtering technique [4], and propose new y We utilize an inverted-list-based filtering index SIL and y We modify our SIL to answer queries with dynamic similarity y We provide detailed and accurate experimental results to Section 3 complements the theory of prefix filtering and applies them to build the SIL structure and EvSCAN algorithm. Section 4 introduces the EvITER incremen tal algorithm. Section 5 shows how to support dynamic thres holds. Section 6 reports our experimental results. Section 7 disc usses related work. Section 8 is the conclusion of our study. sets of tokens. For any token t, we denote wt(t) as the weight of t as the above notations we define Jacc ard similarity of any strings s and s 2 as: Example 1)Under weighted Jaccard similarity, the two strings m= X  X bef X , r= X  X cde X  have a similarity of wt({b, e})/wt({a, b, c, d, e, f})=(3.52+3.48)/(6+3.52+3.51+3.49+3.48+1)=1/3. description of AME as follow: is up to a length threshold L, so we may as well require that |m|  X  L. Here any extracted m is called approximate member of R, and corresponding evidence for r in R. Note that our algorithms we present later can handle any similarity function that satisfies the following properties: y Sim (m,r) is symmetric, i.e., Sim (m,r) = Sim (r,m).  X  X  [0,1], then a query M is submitted. Here M represents a relatively long string (e.g. a text file). The task of AME is to extract all M X  X  substrings m, su ch that there exists some r  X  R satisfying Sim(m,r)  X   X  . y (Symmetrically we have many similarity functions [2]. Fo r example, for any m and r, we have  X  wt(m), so the Jaccard similarity is capable of serving as a similarity function in our discussion. researchers have been designing methods following two phases of filtration and verification [1, 2, 4, 7, 13]. In the AME problem, employing this framework usuall y requires building an indexing structure for the dictionary R. Recall that for each approximate member m extracted, we define the string r in R that is similar enough with m to be m X  X  evidence. Thus, the task of extracting all approximate members from M can be simply reduced to determining whether there exists any evidence for each substring of M, and filtration-verification is actually referred to as evidence filtration and evidence verification. necessary condition (denoted as NC) of our matching criterion Sim  X   X  , that is, if some candidate evidence is real evidence, it must satisfy NC. With the dictionary R given offline, we build an index that quickly recommends for a query m ALL potential evidence that meets NC, so that true evidence is never missed. Then the evidence is verified against the actual matching criterion to determine whether the string m is a true approximate member. ensures the correctness of the whole algorithm. Moreover, it determines how balanced our fr amework is. We can evaluate it through: y How powerful is it? That is, does it eliminate as much false filtration-verification. For example, with all dictionary strings being potentially possible evidence, the most easy-going filtration approach for them is obviously  X  X o filtration X , which leads to a brute-force method of scanning the whole dictionary. On the other produces no false positive and achieves the smallest verification time; it will be expensive to perform such filtering. As a matter of fact, we need to obtain a beneficial compromise between two phases. In the following sections, we will intentionally highlight this issue through our theoretical and experimental analysis. we briefly introduce some key definitions as follows: two tokens appear with the same weight, we sort them lexicographically), and choose the first few tokens to get a subset Sig(s), such that  X  (s)=wt(Sig(s))-(1- X  )wt(s)  X  0. We call Sig(s) a signature set from now on. Example 1) Let  X  =0.6, then {a,b} is a signature set of m because  X  (m)= wt({a,b})-(1-0.6)*wt({a,b,e,f})=9.52-5.6=3.92  X  0. prefix signature set: y For any string s, Sig(s) always exists because we can let y A string may have more than one signature set with different parameter k to determine which se t we choose for a string s, where k-signature set, denoted as Sig k (s). When k is fixed, we select Sig k (s) among all available signa ture sets as follows: y If all signature sets X  sizes are bigger than k, choose the y Else if there is any signature set whose size is exactly k, y Else, choose the largest one, i.e. the string itself. signature scheme as min-signature scheme. When k is set to be  X  , we will get the string itself to be its signature set. list the different Sig(m) under di fferent k settings as below: of signature set, so we may rando mly set k for our filter, and even regard signature set as a compre ssion of information in strings, then k is the parameter for global compression rate tuning. That is, k only influences the performance of our filter. We will further discuss its role in Section 3.3. signature set is actually controlled by  X  and k, thus should be before Section 5 we also consider that  X  is static, so when the context is clear, we still use Sig(s) to denote the signature set of s. In Section 5, we use Sig(s,  X  ) because we start discussing how to support dynamic  X  during query processing. signature set which will be used in our algorithms. For instance, if String m and r meet the matching condition Sim(m,r)  X   X  , m must necessary condition for matching and can be utilized to build a conditions. all s X  X  signature tokens. Then for any string s: have larger weight th an unselected tokens. wt(Sig(m)  X  Sig(r))  X  wt(Sig(m))-wt(m-r). Here m-r refers to the minus set of m and r. Proof : We transform it to an equivalent form as wt(m-r)  X  wt(Sig(m)-Sig(r)), and prove this by showing that Sig(m)-Sig(r) is a subset of m-r .For any t  X  Sig(m)-Sig(r) , we have t  X  Sig(m) , so t  X  m .
  X  minsigwt(m)  X  minsigwt(r) . From t  X  r , wt(t)  X  minsigwt(r) and Lemma 1, we conclude that t  X  Sig(r). This is inconsistent with the fact that t  X  Sig(m)-Sig(r) . So t  X  r . From t  X  r and t  X  m , it X  X  obvious t  X  m-r. So Sig(m)-Sig(r) is a wt(Sig(m)-Sig(r)) .  X  relationship between matching stri ngs. Intuitively this inequality condition is tighter than other conditions proposed in [1], and we believe this property is also useful in other researches involving prefix signatures. However, the set minus operator seems costly to handle, so we need to make this condition more easy-going. We solve this by introducing Theorem 1 as follow: satisfy Sim(m,r)  X   X  , wt(Sig(m)  X  Sig(r))  X  min{  X  (m),  X  (r) }. Proof: If minsigwt(m))  X  minsigwt(r), according to Lemma 2, we have wt(Sig(m)  X  Sig(r))  X  wt(Sig(m))-wt(m-m  X  r)= wt(Sig(m))-wt(m)+wt(m  X  r)  X  wt(Sig(m))-wt(m)+  X  *wt(m  X  r)  X  wt(Sig(m))-wt(m)+  X  *wt(m)= wt(Sig(m))-(1- X  )wt(m)=  X  (m)  X  min{  X  (m),  X  (r) }. we have the same result. So in conclusion we have wt(Sig(m)  X  Sig(r))  X  min{  X  (m),  X  (r) }.  X  Suppose k=2, we have Sig(m)={a,b},  X  (m)= 3.92 and Sig(r) ={b,c},  X  (r)=(3.52+3.51)-0.4*14=1.43. So wt(Sig(m)  X  Sig(r))= wt({b})=3.52  X  min{  X  (m),  X  (r) } = min{3.92, 1.43}=1.43. easy to discover that when the threshold  X  (r) is computed offline, this filtering condition only involves the signature set of all strings, indicating the fact that the time and space requirement of our filter the dictionary R, which is controlled by the parameter k. Moreover, different k provides different fi ltering conditions. Among them we need to decide which one to choose. it X  X  easy to come up with the idea of building an inverted index structure for the dictionary R, a nd filtering by merging inverted lists and accumulating weights. After this index is built up offline, fact that these lists only involves si gnatures of all strings in R, we Algorithm 1 below shows the method to generate an SIL index. E XAMPLE 6. Suppose we have a dictionary R={r[1]= X  X IL X  X  filtering power X , r[2]= X  X he power of filtering by SIL X  X , the =0.55, then we have each strings X  signature set in Table 2 and the SIL built as Figure 2. 2  X  X he power of filtering by SIL X  { X  X iltering X , X  X ower X  X  
Signature  X  String rids rid wt(r)  X  (r) Figure 2. SIL and additional information for dictionary R simply compute the signature set of m, denoted as {t Then we scan all n lists that is indexed by list[t 1 ], list[t while aggregating the weight of t i to all rid whose record contains t as one of its signatu re. To record the aggregated weight, we may use an array Sum[] for convenience or a hash table to save memory space. With all lists scanned, the aggregated weight of rid appears satisfying the filtering condition of Theorem 1 , i.e. with an aggregated weight larger than min{  X  (m),  X  (r)}, we can store it for later verification to determine whether it is the one that makes m a true member. A LGORITHM 1: BuildSIL ( R,  X  , k) 1 for each r  X  R do 2 Sig  X  GenSig (r,  X  , k); 3 for each t  X  Sig do 4 list[t]=list[t]  X  { rid (r)};//insert rid of r into list 5 return list; Note that  X  (r) and wt(r) for any dictionary string r is computed in the signature generating step of Algorithm 1 and they are stored in the main memory for the later use of our algorithms (See Figure 2). the parameter k. This again proves the fact that with any assigned k, our algorithm will run correctly. Since the signature set is a compression of information in a stri ng, we will certainly get more information if we choose a rela tively large k, through which we can target potential matching evidences to a smaller scope, thus reducing the cost of verifying these evidences. cost on targeting possible evidences. For instance, if we set k =  X  , *wt(s), we interestingly find that our method degrades into a common inverted-list based solution. Chakrabarti [2] first analyzed this problem and show ed that k=3 is good on average situation, which is also proved in our experimental study. approximate membership of each single substring m in a document M, it ignores the overlapping between shifting substring windows and consequently takes a lot of time on duplicate computations. Another way to solve AME is to reduce this problem to set similarity join, which is already well studied by researchers [1, 4, 6]. In set similarity join, we are given SA and SB -two columns of sets, a similar ity function Sim, and a threshold  X  . The task of set similarity join is to join the two columns, where the joining condition is Sim(SA, SB)  X   X  . In AME, if we set SA=R, SB={all substrings of M}, run set similarity join between SA and SB and project the result set along SB, we will get the result of AME. Though the problem of set similarity join is explored and optimized in many papers, this method still doesn X  X  other  X  they are substrings of a long text. that the unique property of AME should be exploited separately, and optimized method could be de signed accordingly. So we study the incremental property of Theorem 1, and demonstrate Theorem 2, in purpose of decreasing the duplicate list-scanning when examining all substrings of M. concatenating token t to the tail of string m. Consider the process of checking m and m  X  t: we compute the si gnature set of m and verify the condition in Theorem 1, then we do the same job for m  X  t. Intuitively the signature set of m and m  X  t are much alike. We observe that: if some r cannot match m and does not contain t, it is not likely to match m  X  t. Before we formalize our intuition into new theorem and algorithm, we introduce Evidence Superset by the lemma and definition below: wt(m  X  Sig(r))  X  min{  X  *wt(m),  X  (r)}}. previous sections we mention that Theorem 1 remains correct even if we set different k for different strings, Lemma 3 is in fact obtained by setting k=  X  for m in Theorem 1 (so Sig(m) is replaced by m and  X  (m) by  X  *wt(m)). With Lemma 3 we define Evidence Superset for any query substring m as follow: ES(m)= {r  X  R| wt(m  X  Sig(r))  X  min{  X  *wt(m),  X  (r)}}, we call ES(m) an Evidence Superset of m. Based on Lemma 3 it X  X  obvious that any true evidence for m must be contained in ES(m). evidence matching m will be included in ES(m). If we can efficiently compute ES(m) for an y substring m, we can further filter elements in ES(m) to pick out all true evidences and check m X  X  approximate membership. Our intuition is formalized below. r  X  ES(m) and t  X  Sig(r), then r  X  ES(m  X  t). Proof: We prove r  X  ES(m  X  t) by showing that wt((m Sig(r)) &lt;min{  X  *wt(m  X  t),  X  (r)}: wt((m  X  t)  X  Sig(r))=wt(m  X  Sig(r)) (Because t  X  Sig(r)) &lt;min{  X  *wt(m),  X  (r)}(because r  X 
ES(m))&lt; min{  X  *wt(m  X  t),  X  (r)}.  X  the coming token t, then it cannot be evidence when the substring window moves to m  X  t. However this lemma still cannot serve as a method for efficiently computing ES(m), we further generalize Lemma 4 to obtain Theorem 2 to demonstrate the incremental property of ES(m). A LGORITHM 2: EvSCAN ( M,  X  , k, L) 1 ResultSet  X  X  ;// for storing approximate members 2 for each m of M X  X  substrings (|m|  X  L) do 3 Sig  X  GenSig (m,  X  , k); 4 Initialize Sum[];//for weight aggregating 5 CandSet  X  X  ;// for storing candidate evidence 6 for each t  X  Sig do 7 for each rid  X  list[t] do 8 Sum[ rid ]+=wt( rid );//aggregating weight 9 if Sum[ rid ]&gt;=min{  X  (m),  X  ( rid )} then 10 CandSet  X  CandSet  X  { rid }; 11 for each rid  X  CandSet do //verification 12 if Sim(m, r(rid))  X   X  then //true evidence found 13 ResultSet  X  ResultSet  X  { m }; 14 break; 15 return ResultSet; T HEOREM 2. (I NCREMENTAL P ROPERTY ). Suppose  X  , k are fixed, it holds for any string m and token t that ES(m ES(m)  X  list[t]. Sig(r) means r  X  list[t], so Theorem 2 is obviously based on Lemma 4 . This theorem indicates an efficient iterative approach of maintaining ES() for the varying substring when the right boundary of the substring windows m oves by a token. That is, we check all elements in ES(m)  X  list[t] and pick out proper ones into ES(m  X  t). Note that this process requires maintaining another field recording wt(m  X  Sig(r)) in the summing table sum[], so it can be combined with the process of filtering by SIL. substrings, that is, we fix the left boundary of the substring window and shift the other boundary to the right. While the substring varies we iterativel y maintain corresponding ES() to process, with an instance of M=t 1  X  t 2 . For convenience, we denote this incremental filtering algorithm as EvITER (Evidence Iterating), while EcSCAN in Section 3.2 is short for Evidence Scanning. denote the process of verification, by which one determine if m is really a true member. Since not all r X  X  in CandSet are the ones that  X  X erifyAllCandidate() X  a small filter-verification process, via introducing some other simple ye t effective filtering conditions. The following is exactly one of such conditions we want: states that if any two strings have too much difference in length, they are not likely to match each other. To apply this filtering condition, we need only store in memory the weight value of all can quickly narrow our scope to fewer possible r, thus avoiding more disk accessing and making computation more efficient. algorithms, and demonstrate the advantages of EvITER over EvSCAN. The notations to be used are listed in Table 3. |M| The length of text used as the input of AME L r Average length of dictionary strings L m Average substring length 
L list Average length of inverted lists  X  Similarity threshold 
C v Time cost of verifying an evidence (including disk longer than k. (2) all tokens ha ve the same weight (e.g. 1). Therefore, we obtain an upper bound for the signature set size of any string. L EMMA 5. Suppose k and  X  are fixed, then for any string m with length L, |Sig(m)|  X  max{k,(1- X  )L}.
 Proof: Because for any string m, |Sig(m)|=wt(Sig(m))  X  (1- X  ) *wt(m)= (1- X  )L, the smallest signat ure set size is (1- X  )L. smallest signature set of size (1- X  )L. Therefore we have |Sig(m)|  X  max{k,(1- X  )L}.  X  A LGORITHM 3: EvITER ( M={t[1],t[2],...,t[n]},  X  , k, L) 1 ResultSet  X  X  ;// for storing approximate members 2 for i =1 to n do 3 Initialize Sum[]; 4 LastES  X  X  ;//for iterating ES 5 for j=i to min{n,i+L-1} do //current m= t[i]...t[j] 6 ES  X  X  ; 7 CandSet  X  X  ;// for storing candidate evidence 8 update Sig(m) and maintain sum[]; 9 for each rid  X  list[t]  X  LastES do 10 if Sum[ rid ].s2  X  min{  X  *wt(m),  X  (r( rid) )} then 11 ES  X  ES  X  { rid }; 12 if Sum[ rid ].s1  X  min{  X  (m),  X  ( rid )} then 13 VerifyAllCandidate() ; //verification 14 LastES  X  ES; //iteration for the next window 15 return ResultSet; SIL can be expressed as the time cost of our algorithms, we introduce another lemma as below: L EMMA 6. For each substring m, the number of inverted lists that EvSCAN and EvITER scan are respectively |Sig(m)| and 2. Proof (Outline): It X  X  obvious for EvSCAN and we only give proof for EvITER. Consider the moment we finish checking m and prepare for m  X  t, we have to scan list[t] once to iterate ES(m) into ES(m  X  t). Moreover, it X  X  possible that t replace some signature token t X  to be a new signat ure, so we must scan list[t X  X  to maintain the summing table sum[] for next iteration.  X  respectively |M|L m L list max{k, (1- X  )L m } and 2|M|L verification cost of EvSCAN can be estimated as EC of EvITER is E (1+C v ) because of the O(1) evidence iteration cost for every evidence. In summary we have introducing the cost of evidence iteration, so it may have some advantage when k is large or E is small, which will be demonstrated by our experimental results later. as F(R,  X  0 ), given a dictionary R and a fixed threshold  X  meaning that the threshold  X  0 is undesirably static. If users want to submit a query with other thresholds, the filter has to be re-initialized. This apparently l eads to much inconvenience in with a little modification, our SI L can handle this problem well. Note that in this section, the notation Sig(s) is replaced by Sig(s,  X  ) to add a dynamic threshold  X  . threshold is caused by the definition of prefix signatures. Recall that satisfies wt(Sig(s))  X  (1- X ) wt(s). Therefore, with different  X  we need different number of sign atures to build various filters. any string s, if a token t is selected as a signature under some threshold, it will also be in the signature set of s when the threshold gets lower. That is, if we initialize the filter at a relatively low threshold  X  0 , when a query comes with a higher threshold  X   X   X  0 , those rids whose string contains t as a signature should be included in some nodes on list[t] of the current filter. For simplicity we call these nodes active nodes . All we need is to discriminate active nodes , and use them to perform filtration. nodes as follow: T HEOREM 3 (S UFFICIENT A ND N ECESSARY C ONDITION O F M IN -S IGNATURE ). Under min-signature schema, for any string s={t 1 ,t 2 ...t n }, where wt(t 1 )  X  wt(t 2 )  X  ...  X  wt(t wt(t 2 )+...+ wt(t i ))/wt(s) for any i  X  1 and U 0 following conclusion: in every node of all inverted lists, we add a field to record U order to test the condition  X  X  [0,U i-1 ) to decide whether this is an descending order of corresponding U i-1 . In this way, for any threshold  X  , all active node in a list must form a prefix of the list. Therefore, we may stop our scan once an inactive node is found, by which we avoid scanning the whole list and enhance the performance. min-signature scheme. In fact, in Section 6 we will show that under most cases, min-signature sc hema is enough to serve as a good choice. Suppose we initialize the modified filter with min-signature schema (k=1) and  X  0 =0.55 as Figure 4. We have a query with  X  =0.7, then all nodes in Figure 4 that is circled out become active nodes and should be scanned. 
Signature  X  String rids and U  X  X iltering X   X  (2, 1.0), (1, 0.6) modification only requires a little more space and additional sorting in the filter-building phase. For queries with various similarity thresholds, the modifi ed SIL successfully solves the static threshold problem, without visiting any additional list nodes or trading query performance. section: is a filter proposed in [2], whose idea is to optimize the query range of length filtering. In our experiment, we set the inverted hashtable length b to be 11 (8 is enough according to [2]). scanning the S ignature-based I nverted L ists. EvITER is an optimized version of EvSCAN , which aims at reducing unnecessary list scanning. site. We extracted 274,788 paper titles with a total size 17.8MB as the dictionary. The query text to this dictionary is 40 web pages from CiteSeer, each containing the title, abstract, citation, etc. of a random paper. Tokens are separa ted by spaces and punctuations. an URL dataset. The query text is 40 text files, each containing 50 random URLs from the rest of th e dataset. Tokens are separated by slashes. tests were conducted under the weighted Jaccard similarity measurement. We mainly judge the performance of all filters via analyzing the filtering power a nd overall running time of them. We evaluate the power of filters by the candidate evidence they produce since the size of candidate evidences has a great influence on overall performance. section. We first performed expe riments on DBLP data (dictionary size: 274,788 records), and our re sults show that ISH produced a large amount of candidate evidences and disk-accessing, thus spending much time on verification and could not terminate in one hour. Therefore, we had to reduce the size of dictionary using the first 1000 records in DBLP. of ISH: for every query substrin g m, ISH optimizes the existing length filtering condition mentioned in Section 4.3, and uses a new range (denoted as [a,b], a =  X  *wt(m), b  X  wt(m)/  X  ) as the SQL querying condition at evidence record retrieving phase. across the weight axis (see sub-figure (a) and (b)), where all records distribute densely, with at most 50k and on average 10k in a unit length of weight range (DBLP dataset). This implies that it X  X  unwise to retrieve all evidence w hose weight is in certain range, unless we can make our query ra nge desirably small or far from those regions with crowded records. represented by a characteristic point (mid-point, length) or ( (a+b)/2, b-a) ) from 2,608 SQL queries ISH launches when processing a random webpage. We also flagged some areas as  X  X esirable area X , where  X  X esirable queries X  appear (queries possessing small [a,b] ranges or a voiding the most frequent weight of all records, i.e. x-coordinate of the peak in Figure 6(a) and (b) ). vary averagely from 0 to 70, wh ich include the most frequent weight in both datasets (15 for Figure 6(a) and 20 for (b)). Moreover, though ISH sometimes successfully confirms of no matching (denoted by ranges with negative length in Figure 6(c) ), the range length in many queries is not desirably short. Therefore, there exist too many queries, whose characteristic point is located far from our  X  X esirable area X . These insufficiently optimized queries lead to tons of I/Os an d verification computations, thus deteriorating the overall performance of ISH. of SIL is mainly influenced by the following aspects: the  X  X ompressing rate X  parame ter k, dictionary size, query text length, similarity threshold, and subs tring length threshold. We run EvITER on different parameter settings and record the results, from which we have the following observations: y The parameter k is tightly related to every aspect of the filter. y Though the performance of SIL depends much on the inherent performed a comparison between the two algorithms we propose: EvSCAN and EvITER. Instead of dictionary size and query length, in this subsection we mainly focus on the two varying threshold parameters: similarity threshold a nd substring length threshold. y The similarity threshold significantly affects the time y When L=10, k=3 and  X  =0.85, EvITER shows a performance y To our surprise, when L is above 15, EvITER is gradually problem of finding a pattern string approximately in a text. There have been many studies on this problem. See [9] for an excellent survey. The problem of AME is different: searching in a long text to approximately match a string from a dictionary. In addition, AME is also different to the problem of text document indexing (finding dictionary documents a pproximately containing a query string) and string similarity joins (identifying approximate matching string pairs, each from one of two columns of strings). similarity functions can be ca tegorized as token-based and character-based, depending on what they regard strings as: sets of tokens, or sequences of characters. be straightforwardly reduced to set similarity join [1, 4, 6, 10]. Paper [4] discussed the framework and implements of a primitive operator SSJoin for performing simila rity joins, on which a variety similarity join problem by converting set-based similarity distance into hamming distance between bina ry vectors, and studying the number of shared segments of two divided vectors. In [2], Chakrabarti et al. proposed a 0-1 ma trix-based AME filter. In this paper we showed that their appr oach touches upon a NPC decision problem, whose intractability we briefly prove in the Appendix. character-based approximate st ring-matching problem has been well studied by researchers [9]. Early methods handling the edit distance and gram sharing [12]. Due to the dilemma in choosing gram length, [8] proposes VGRAM, namely variable-length gram to address the problem. For non-gram-based approaches, Wang et al. uses inverted lists to index the neighborhood of dictionary strings, and enhances previous neighborhood generation methods by reducing the upper bound of the neighborhood size [13]. because the filtration phase needs inverted list processing. In [7], this problem is formalized into T-occurrence problem, and three efficient algorithms are proposed. T-occurrence problem requires that the threshold T should be independent from any list nodes, which is not satisfied by our method (our threshold min{  X  (m),  X  (r)} technique in [7] is orthogonal to our solution here, and can be used (by some modification) on SIL index in a complementary manner. filtration-verification, we analyzed the issue of trading between the two phases, and proposed a new filtering condition and corresponding filter called SIL. Th en we designed two algorithms for SIL: EvSCAN and its incrementally optimized version EvITER, which saves the cost of scanning some inverted lists by progressively maintaining a candida te evidence set of the current substrings. We also addressed th e static threshold problem of reported the performance of ou r filtering algorithms through theoretical and experimental analysis. it to a decision problem about 0-1 matrices. Here, we give the proof about its intractability. For convenience we call it Constrained Solid Submatrix problem and describe it as below: w I={ i 1 , i 2 , ..., i r } from the rows and a subset J={ j the columns such that for any i X   X  I and j X   X  J, A[i X  X [j X  X =1, and w (i 1 )+ w 1 (i 2 )+...+ w 1 (i r )  X   X  , w 2 (j 1 )+ w and  X  are two given thresholds) problem) Constrained Solid Submatr ix problem is NP-Complete. Proof Outline: We prove by reducing to Balanced Complete Bipartite Subgraph problem, which requires finding a K*K complete bipartite subgraph in a given bipartite B=&lt;V It is already proven to be NP-Complete (see page 196 in [5]). Subgraph problem, let w 1 (i)=1,w 2 (j)=1,  X  =  X  = K, and construct a |V 1 |*|V 2 | 0-1 matrix, whose elements are assigned as follow: and only if the corresponding Ba lanced Complete Bipartite Subgraph problem has a solution. This concludes the reduction. motivate this work and thanks anonymous reviewers for their constructive comments. This res earch was partially supported by the grants from 863 National High-Tech Research and Development Plan of China (No: 2009AA01Z133, 2007AA01Z155, 2009AA011904), Nati onal Science Foundation of China (NSFC) under the number (No.60833005) and Key Project in Ministry of Education (No: 109004). [1] A. Arasu, V. Ganti, R. Kaushik. Efficient exact set-similarity [2] K. Chakrabarti, S. Chaudhuri, V. Ganti, D. Xin. An efficient [3] A. Chandel, P. C. Nagesh, a nd S. Sarawagi. Efficient batch [4] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive operator [5] M.R.Garey and D.S.Johnson. Co mputers and Intractability: [6] L. Gravano, P. G. Ipeirotis, H. V. Jagadish, N. Koudas, S. [7] C. Li, J. Lu, and Y. Lu. Ef ficient merging and filtering [8] C. Li, B,Wang, X. Yang, VGRAM: Improving performance [9] G. Navarro. A guided tour to approximate string matching. [10] S. Sarawagi, A.Kirpal, Efficient set joins on similarity [11] A. Singhal. Modern information retrieval: A brief overview. [12] E. Sutinen and J. Tarhio. On using q-grams locations in [13] W. Wang, C. Xiao, X. Lin, C. Zhang. Efficient approximate [14] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: [15] A. C. Yao and F. F. Yao. Di ctionary loop-up with small 
