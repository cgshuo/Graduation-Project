 Classification (i.e. supervised learning) is a fundamental task in data mining. A classi-fier, built from the labeled train samples described by a set of features/attributes, is a ples. Major classification algorithms include Artificial Neural Network (ANN) [2, 3, 11], Nearest Neighbor [17, 13], Na X ve Bayes [1, 20], etc. Clustering (i.e. unsupervised samples in one group are similar while samples belonging to different groups are not similar [15, 16, 19]. Many clustering algorithms have been proposed, among which k -means is one of the most popular [27]. 
Particle classification is an important analysis in particle physics experiments. Tra-which act on projections of high-dimensional event parameter space onto orthogonal axes [11]. This procedure often fails to yield the optimum separation of distinct event analysis, and then present a classification framework combining ANNs and FVC to improve the high-energy particle classification performance. 
The remainder of this paper is organized as follows: We first describe an ANN classifier in Section 2. Section 3 describes the clustering method FVC. Section 4 de-scribes the classification system combining ANNs and clustering. Section 5 describes methods for comparison. Section 6 presents the dataset, four evaluation measures and the experiment results. Conclusions are presented in Section 7. Artificial Neural Networks (ANN) is a network of perceptrons, which computes an many proposed ANN models, MLP, the multilayer feedforward network with a back-propagation-learning mechanism, is the most widely used [6]. MLP consists of an input layer of source nodes, one or more hidden layers of computation nodes, and an output layer of nodes. Data propagates through the network layer-by-layer. Fig. 1 shows the data flow of a MLP with two hidden layer. 
Define X as a vector of inputs and Y as a vector of outputs. Y , which may also be a 1-dimension vector, is typically obtained by: W layer. W 2 and B 2 are for the output layer. f a denotes the activation function. 
The classification problem of the MLP can be defined as follows: Given a training the dependency between them by adapting the weights and biases to their optimal the criterion to be optimized. 
MLP consists of iteration of two steps: (1) Forward -the predicted class corre-cost function with respect to the different parameters are propagated back through the network. The process stops when the weights and biases converge. In this section we describe a partitional clustering method Frequent Variable Sets particle data. It X  X  based on frequent itemset mining and is based on the work of Fung B. et al [15], who developed a hierarchi cal document clustering using frequent item-different algorithms have been developed for that task, including the well known Apriori [10] and FP-growth [9]. 
Frequent item-based high-energy particle clustering is based on partitioning the other than transactions, we will use the notion of variable sets instead of item sets. A variable is any attribute describing a particle within physics experiments (high-energy particles collision, for example), and a particle can have some variables detected and others undetected due to inevitable changes in experimental environments or other reasons. Therefore, even the same kind of particles may have different set of detecting variables. Thus we assume that if we can cluster particles into different groups where each group has its own specific experimental environment (please note that the parti-cles within each group will be in different classes because the group forming process is not based on the classes of the particles, particles of different classes may be under the same experimental situation thus have the same set of detected and undetected variables), then the classification model built from the particles in the same group will tional clustering method, k -means for example, just group distance similar points thus is not suitable to find variable-oriented groups. this paper, the original space is 78-dimensional), FVC considers only the low-able set (or variableset) is actually not a cluster (candidate) but only the description of a cluster (candidate), or the representational centroid of the cluster. The corresponding variable set. 3.1 Binarizing Original particle data have numeric attributes/variables; in order to find frequent vari-able sets we need to first convert them to binary attributes (1 for detected variable and 0 for undetected variable). For a particle, if one variable has a value other than 0, then we believe that the variable is detected for th e particle (or we can say that it occurs in particle, then we believe that it is undetected and is set to 0 (unchanged). Some vari-close to 0 (0.0001 for example) for one particle and 0 for another particle, then for the latter particle is hard to know whether the 0-value means detected or not. We simply to 1. Table 1 shows an example data compose of four particles with five attrib-utes/variables. Table 2 shows the converted data and its transaction representation. 3.2 Representational Frequent Variableset Assuming some variables occur in some particles, others occur in all particles. Let P in the particles of P . Each particle P i can be represented by the set of variables occur-in it, then the particle will not be in C ( S ). 
Define F i as a representational frequent variableset, which is such kind of variable-maximum fraction of the whole particle set P . A minimum support (minsupp, in a percentage of all particles) and a maximum support (maxsupp, in a percentage of all representational frequent variablesets in P with respect to minsupp and maxsupp, the particles: where | P | is the number of particles. 
A representational frequent variable is a variable that belongs to representational frequent variableset containing k variables. of frequent in association rule mining where only minsupp is used. We introduce maxsupp in order to avoid too frequent vari able sets because these variables occur in so many particles that they are not suitable for representing different kinds of particles (i.e., not representational). variablesets and then remove those whose support is beyond maxsupp and those who have any item/variable whose support is beyond maxsupp. For example, we define minsupp to be 10% and maxsupp 35%, suppose that variable V 1 X  X  support is 90%, will not be representational frequent since { V 1} is not representational frequent. 
The method described above is simple but not optimized, we also provide an opti-mized way of mining representational frequent variablesets: to modify Apriori by adding a maxsupp threshold when finding frequent itemsets/variablesets. At the steps remove those frequent ( k -1)-variablesets whose support is beyond maxsupp. This variablesets. 3.3 Obtaining Clusters tain all the particles that contain this variableset. One property of initial clusters is that all particles in a cluster contain all the items in the representational frequent variable-set that defines the cluster, that is to say, these variables are mandatory for the cluster. We use this defining representational frequent variableset as the representational centroid to identify the cluster. 
Initial clusters are not hard/disjoint because one particle may contain several repre-The following are two steps for merging. 
The overall FVC clustering algorithm proceeds as follows. 1. Data binarizing. 2. Mining all representational frequent variablesets as the initial representational centroids and construct initial clusters. 3. Assign all points/particles to their representational centroids. 4. Merge overlapped clusters to disjoint clusters. We design a classification system combining ANNs and FVC. The system, we call Clustering-ANNs, involves classification by a set of ANNs, each using distinct sub-case is a two-step process. First, the nearest cluster is found for the case, and then the decision is based on the ANN classifier trained on the specific cluster. The reason for using FVC before ANN is that FVC can partition particles into several groups accord-ing to their different experimental situatio n. Particles of different classes may be un-der the same experimental situation thus ha ve the same set of detected and undetected variables. So each group will have particles within different classes. In this section we describe several classification methods for comparison. 5.1 Probability Learning Na X ve Bayes is a successful probability learning method which has been used in many applications [24, 25, 26]. For the task of Na X ve Bayes based particle classification, we assume the particle data is generated by a parametric mixture model. Na X ve Bayes the number of training samples}, Na X ve Bayes use maximum likelihood to estimate the class prior parameters as the fraction of training particles is c i . 
We describe the particle classification problem can be generally described as fol-applying Bayes rule. The method assumes that the features of a particle are independ-m -feature particle data. 5.2 Memory Learning Memory based learning is a non-parametric inductive learning paradigm that stores rather than on the knowledge, such as models, abstracted from experience. The simi-larity between the new instance and a sample in the memory is computed using a distance metric. We use the nearest neighbor (NN) classifier, a memory based learn-ing method, that uses Euclidian distance metric [23] in the experiment. m -dimensional space (where m is the number of variables) and given an unseen parti-cle, the algorithm classifies it by the nearest training particle. 5.3 Hard Partit ional Clustering find all k clusters at once. There are many such kind of techniques, among which the that a center point can represent a cluster. Particularly, we use centroid, which is the mean (or median) point of a group of points. 
The basic k -means clustering technique is summarized below. 1. Select k points as the initial centroids. 2. Assign all points to the closest centroid. 3. Re-compute the centroid of each cluster. 4. Repeat steps 2 and 3 until the centroids don X  X  change or change little. 6.1 Datasets The high-energy particle physics dataset we used are publicly available on KDD web-site [7]. There are 50000 binary-labeled par ticles, 78 attributes for each particle. Since degrade the classification performance, we simply ignore these attributes. These particles fall into two classes: positive (1) and negative (0). 6.2 Evaluation Methods We use four performance measures [12] for the particle classification problem: Accuracy (ACC, to maximize): the number of cases predicted correctly, divided by the total number of cases. Area Under the ROC Curve (AUC, to maximize): ROC is a plot of true positive rate vs. false positive rate as the prediction threshold sweeps through all the possible val-ues. AUC is the area under this curve. AUC can measure how many times one would have to swap samples with their neighbors to repair the sort. AUC = 1 indicates per-fect prediction, where all positive samples sorted above all negative samples. AUC = 0.5 indicates random prediction, where there is no relationship between the predicted values and actual values. AUC below 0.5 indicates there is a relationship between predicted values and actual values. SLAC Q-Score (SLQ, to maximize): Researchers at the Stanford Linear Accellerator performance of predictions made for particle physics problems. SLQ breaks predic-tions interval into a series of bins. For the experiments we are using 100 equally sized bins within the 0 to 1 interval. Cross-Entropy (CXE, to minimize): CXE measures how close predicted values are to actual values. It assumes the predicted values are probabilities on the interval of 0 to 1 that indicate the probability that the sample is with a certain class. where A is the actual class (in our case, 0 or 1) and P is the predicted probability that the sample is with the class. Mean CXE (the sum of the CXE for each sample divided by the total number of samples) is used to make CXE independent of data set size. 6.3 Results 6.3.1 Illustration with Random Subset of Data figure corresponds to a variable, and the rows represent particles, there are 65 columns found that the original particles (as shown in Fig. 3) will be partitioned into three natural groups as shown in Fig.4. We can see that FVC found natural groups of the dataset. 6.3.2 Results on the Whole Dataset Full experiments are done on the whole dataset which has 50000 samples. We use 10-fold cross-validation for estimating classification performance, so the four measures, ACC, AUC, SLQ and CXE, are averaged on the 10 runs. Table 3 shows the results. 
The results show that ANN is better than Nearest Neighbor and Naive Bayes for particle classification. By combining clustering and ANNs, the proposed scheme Clustering-ANNs can get even better performance than ANN. Kmeans-ANNs is slightly better than ANNs for ACC and SLQ. By using clustering algorithm FVC which is especially designed for particles, we can get the best performance for all four measures. 
The reason that FVC-ANNs is better than using a single ANN is that FVC can cluster particle data into different groups according to different experimental charac-teristics showed in the high-energy physics experiments. Different groups found by FCV have different set of variables. So more appropriate ANN can be trained for each group, this is better than just use a uniform ANN for all particles. In this paper we describe a particle-oriented clustering method Frequent Variable Set based Clustering (FVC), and a framework Clustering-ANNs for the high-energy par-artificial neural networks (ANNs), each usin g distinct subsets of samples selected the train samples into several subsets, then standard back-propagation ANNs are trained on them. Comparisons with other popular classification methods, Nearest Neighbor and Naive Bayes, show that ANN is the best for particle physics classifica-tion, and the proposed method FVC-ANNs can get even better performance. The authors gratefully acknowledge the support of the National Science Foundation of China (Grant No. 60273015 and No. 10001006). 24. George H. John and Pat Langley: Estimating Continuous Distributions in Bayes-
