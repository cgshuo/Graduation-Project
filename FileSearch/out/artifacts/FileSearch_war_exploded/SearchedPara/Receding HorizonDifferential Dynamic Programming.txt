 We are interested in learning controllers for high-dimensional, highly non-linear dynamical systems, continuous in state, action, and time. Local, trajectory-based methods, using techniques such as Dif-ferential Dynamic Programming (DDP), are an active field of research in the Reinforcement Learn-ing and Control communities. Local methods do not model the value function or policy over the entire state space by focusing computational effort along likely trajectories. Featuring algorithmic complexity polynomial in the dimension, local methods are not directly affected by dimensionality issues as space-filling methods.
 In this paper, we introduce Receding Horizon DDP (RH-DDP), a set of modifications to the classic DDP algorithm, which allows us to construct stable and robust controllers based on local-control trajectories in highly non-linear, high-dimensional domains. Our new algorithm is reminiscent of Model Predictive Control, and enables us to form a time-independent value function approximation along a trajectory. We aggregate several such trajectories into a library of locally-optimal linear controllers which we then select from, using a nearest-neighbor rule.
 Although we present several algorithmic contributions, a main aspect of this paper is a conceptual one. Unlike much of recent related work (below), we are not interested in learning to follow a pre-supplied reference trajectory. We define a reward function which represents a global measure of performance relative to a high level objective, such as swimming towards a target. Rather than a reward based on distance from a given desired configuration, a notion which has its roots in the control community X  X  definition of the problem, this global reward dispenses with a  X  X ath planning X  component and requires the controller to solve the entire problem.
 We demonstrate the utility of our approach by learning controllers for a high-dimensional simulation of a planar, multi-link swimming robot. The swimmer is a model of an actuated chain of links in a viscous medium, with two location and velocity coordinate pairs, and an angle and angular velocity for each link. The controller must determine the applied torque, one action dimension for each articulated joint. We reward controllers that cause the swimmer to swim to a target, brake on approach and come to a stop over it.
 We synthesize controllers for several swimmers, with state dimensions ranging from 10 to 24 dimen-sions. The controllers are shown to exhibit complex locomotive behaviour in response to real-time simulated interaction with a user-controlled target. 1.1 Related work Optimal control of continuous non-linear dynamical systems is a central research goal of the RL moved, the exponential dependence of computational complexity on the dimensionality of the do-main remains a major computational obstacle. Methods designed to alleviate the curse of dimen-sionality include adaptive discretizations of the state space [1], and various domain-specific manip-ulations [2] which reduce the effective dimensionality.
 Local trajectory-based methods such as DDP were introduced to the NIPS community in [3], where a local-global hybrid method is employed. Although DDP is used there, it is considered an aid to the global approximator, and the local controllers are constant rather than locally-linear. In this decade DDP was reintroduced by several authors. In [4] the idea of using the second order local DDP models to make locally-linear controllers is introduced. In [5] DDP was applied to the challenging a minimax variant of DDP is used to learn a controller for bipedal walking, again by designing a reference trajectory and rewarding the walker for tracking it. In [7], trajectory-based methods including DDP are examined as possible models for biological nervous systems. Local methods have also been used for purely policy-based algorithms [8, 9, 10], without explicit representation of the value function.
 The best known work regarding the swimming domain is that by Ijspeert and colleagues (e.g. [11]) using Central Pattern Generators. While the inherently stable domain of swimming allows for such open-loop control schemes, articulated complex behaviours such as turning and tracking necessitate full feedback control which CPGs do not provide. 2.1 Definition of the problem In this context we assume F ( x k , u k ) = x k + approximating the continuous problem and identifying with it in the  X  t  X  0 limit. Given some The quantity maximized on the RHS is the value function , which solves Bellman X  X  equation: mization subproblem from k to N . This is a manifestation of the dynamic programming principle . If N =  X  , essentially eliminating the distinction between different time-steps, the sequence collapses to a global, time-independent value function V ( x ) . 2.2 DDP Differential Dynamic Programming [12, 13] is an iterative improvement scheme which finds a locally-optimal trajectory emanating from a fixed starting point x 1 . At every iteration, an approx-which is formed by iterative application of F using the current control sequence { u k } N k =1 . Every iteration is comprised of two sweeps of the trajectory: a backward and a forward sweep. In the backward sweep , we proceed backwards in time to generate local models of V in the following the unmaximised value function, or Q -function, as a quadratic model around the present state-action pair ( x k , u k ) : Where the coefficients Q ?? are computed by equating coefficients of similar powers in the second-order expansion of (2) Once the local model of Q is obtained, the maximizing  X u is solved for and plugged back into (3) to obtain a quadratic approximation of V k : (5) and (6) iterate in the backward sweep, computing a local model of the Value function along can be viewed as dual to the Extended Kalman Filter (though employing a higher order expansion of F ).
 In the forward sweep of the DDP iteration, both the open-loop and feedback terms are combined to We note that in practice the inversion in (5) must be conditioned. We use a Levenberg Marquardt-like scheme similar to the ones proposed in [14]. Similarly, the u -update in (7b) is performed with an adaptive line search scheme similar to the ones described in [15]. 2.2.1 Complexity and convergence The leading complexity term of one iteration of DDP itself, assuming the model of F as required for inverting Q uu . In practice, the greater part of the computational effort is devoted to the measurement of the dynamical quantities in (4) or in the propagation of collocation vectors as described below. DDP is a second order algorithm with convergence properties similar to, or better than Newton X  X  method performed on the full vectorial u k with an exact Nm  X  Nm Hessian [16]. In practice, convergence can be expected after 10-100 iterations, with the stopping criterion easily determined as the size of the policy update plummets near the minimum. 2.2.2 Collocation Vectors We use a new method of obtaining the quadratic model of Q (Eq. (2)), inspired by [17] 2 . Instead of using (4), we fit this quadratic model to samples of the value function at a cloud of collocation approximated value function at the next time step, we can estimate the value of (2) at every point: the cloud is in general configuration, the equations are non-singular and can be easily solved by a generic linear algebra package.
 There are several advantages to using such a scheme. The full nonlinear model of F is used to construct Q , rather than only a second-order approximation. F xx , which is an n  X  n  X  n tensor need not be stored. The addition of more vectors can allow the modeling of noise, as suggested in [17]. In addition, this method allows us to more easily apply general coordinate transformations in order to represent V in some internal space, perhaps of lower dimension.
 The main drawback of this scheme is the additional complexity of an O ( Np  X  2 ) term for solving the p -equation linear system. Because we can choose { x k i , u k i } in way which makes the linear system sparse, we can enjoy the  X  2 &lt;  X  1 of sparse methods and, at least for the experiments performed here, increase the running time only by a small factor.
 In the same manner that DDP is dually reminiscent of the Extended Kalman Filter, this method bears a resemblance to the test vectors propagated in the Unscented Kalman Filter [18], although we use a quadratic, rather than linear number of collocation vectors. 2.3 Receding Horizon DDP When seeking to synthesize a global controller from many local controllers, it is essential that the different local components operate synergistically. In our context this means that local models of the value function must all model the same function, which is not the case for the standard DDP solution. The local quadratic models which DDP computes around the trajectory are approximations to V ( x, k ) , the time-dependent value function. The standard method in RL for creating a global value function is to use an exponentially discounted horizon. Here we propose a fixed-length non-discounted Receding Horizon scheme in the spirit of Model Predictive Control [19].
 Having computed a DDP solution to some problem starting from many different starting points x x  X  X . Although in this way we could accumulate a time-independent approximation to V ( x, N ) only, starting each run of N -step DDP from scratch would be prohibitively expensive. We therefore propose the following: After obtaining the solution starting from x 1 , we save the local model at k = 1 and proceed to solve a new N -step problem starting at x 2 , this time initialized with the policy obtained on the previous run, shifted by one time-step, and appended with the last control u second-order convergence of DDP is in full effect and the algorithm converges in 1 or 2 sweeps. Again saving the model at the first time step, we iterate. We stress the that without the fast and exact convergence properties of DDP near the maximum, this algorithm would be far less effective. 2.4 Nearest Neighbor control with Trajectory Library A run of DDP computes a locally quadratic model of V and a locally linear model of u , expressed by inside of which a basin-of-attraction is formed. Having lost the dependency on the time k with the receding-horizon scheme, we need some space-based method of determining which local gain model we select at a given state. The simplest choice, which we use here, is to select the nearest Euclidian neighbor. Outside of the basin-of-attraction of a single trajectory, we can expect the policy to perform very poorly and lead to numerical divergence if no constraint on the size of u is enforced. A possible trajectories [20], and consider all of them when selecting the nearest linear gain model. 3.1 The swimmer dynamical system We describe a variation of the d-link swimmer dynamical system [21]. A stick or link of length l , lying in a plane at an angle  X  to some direction, parallel to  X  t =  X n = is modeled as a chain of d such links of lengths l i and masses m i , its configuration described by the generalized coordinates q = ( x cm  X  ) , of two center-of-mass coordinates and d angles. Letting  X x i = x i  X  x cm be the positions of the link centers WRT the center of mass , the Lagrangian is express the joining of successive links, and by the equation Figure 1: RH-DDP trajectories. (a) three snapshots of the receding horizon trajectory (dotted) with the current finite-horizon optimal trajectory (solid) appended, for two state dimensions. (b) Projections of the same receding-horizon trajectories onto the largest three eigenvectors of the full state covariance matrix. As described in Section 3.3, the linear regime of the reward, here applied to a 3-swimmer, compels the RH trajectories to a steady swimming gait  X  a limit cycle. definition of the  X x i  X  X  relative to the center-of-mass. The function lated frictional forces. With these in place, we can obtain  X  q from the 2+ d Euler-Lagrange equations: with u being the external forces and torques applied to the system. By applying d  X  1 torques  X  j in action-reaction pairs at the joints u i =  X  i  X   X  i  X  1 , the isolated nature of the dynamical system is preserved. Performing the differentiations, solving for  X  q , and letting x = dimensional state variable, finally gives the dynamics  X  x = (  X  q  X  q ) = f ( x , u ) . 3.2 Internal coordinates relative to an external coordinate system, which the controller should not have access to. We make a coordinate transformation into internal coordinates, where only the d  X  1 relative angles {  X   X  j =  X  on one of the links. This makes the learning isotropic and independent of a specific location on the plane. The collocation method allows us to perform this transformation directly on the vector cloud without having to explicitly differentiate it, as we would have had to using classical DDP. Note also that this transformation reduces the dimension of the state (one angle less), suggesting the possibility of further dimensionality reduction. 3.3 The reward function The reward function we used was target (the origin in internal space), and c x and c u are positive constants. This reward is maximized when the nose is brought to rest on the target under a quadratic action-cost penalty. It should not be confused with the desired state reward of classical optimal control since values are specified only for 2 out of the 2 d + 4 coordinates. The functional form of the target-reward term is designed to be linear in || x nose || when far from the target and quadratic when close to it (Figure 2(b)). Because Figure 2: (a) A 5-swimmer with the  X  X ose X  point at its tip and a ring-shaped target. (b) The func-tional form of the planar reward component r ( x nose ) =  X  X | x nose || 2 / translates into a steady swimming gait at large distances with a smooth braking and stopping at the goal. of the differentiation in Eq. (5), the solution is independent of V 0 , the constant part of the value. Therefore, in the linear regime of the reward function, the solution is independent of the distance manifold in state-space which describes steady-state swimming (Figure 1(b)). Upon nearing the target, the swimmer must initiate a braking maneuver, and bring the nose to a standstill over the target. For targets that are near the swimmer, the behaviour must also include various turns and jerks, quite different from steady-state swimming, which maneuver the nose into contact with the target. Our experience during interaction with the controller, as detailed below, leads us to believe that the behavioral variety that would be exhibited by a hypothetical exact optimal controller for this system to be extremely large. target with a cursor, a user can interact with controlled swimmers of 3 to 10 links with a state di-mension varying from 10 to 24, respectively. Even with controllers composed of a single trajectory, the swimmers perform quite well, turning, tracking and braking on approach to the target. All of the controllers in the package control swimmers with unit link lengths and unit masses. The normal-to-tangential drag coefficient ratio was k n /k t = 25 . The function F computes a single 4th-order Runge-Kutta integration step of the continuous dynamics F ( x k , u k ) = x k + with  X  t =0 . 05 s . The receding horizon window was of 40 time-steps, or 2 seconds. When the state doesn X  X  gravitate to one of the basins of attraction around the trajectories, numerical divergence can occur. This effect can be initiated by the user by quickly moving the target to a  X  X urprising X  location. Because nonlinear viscosity effects are not modeled and the local controllers are also linear, exponentially diverging torques and angular velocities can be produced. When adding as few as 20 additional trajectories, divergence is almost completely avoided.
 Another claim which may be made is that there is no guarantee that the solutions obtained, even on the trajectories, are in fact optimal. Because DDP is a local optimization method, it is bound to stop in a local minimum. An extension of this claim is that even if the solutions are optimal, this has to do with the swimmer domain itself, which might be inherently convex in some sense and therefore an  X  X asy X  problem.
 While both divergence and local minima are serious issues, they can both be addressed by appealing to our panoramic motivation in the biology. Real organisms cannot apply unbounded torque. By local minima exist even in the motor behaviour of the most complex organisms, famously evidenced by Fosbury X  X  reinvention of the high jump.
 Regarding the easiness or difficulty of the swimmer problem  X  we made the documented code avail-able and hope that it might serve as a useful benchmark for other algorithms. The significance of this work lies at its outlining of a new kind of tradeoff in nonlinear motor control design. If biological realism is an accepted design goal, and physical and biological constraints taken into account, then the expectations we have from our controllers can be more relaxed than those of the control engineer. The unavoidable eventual failure of any specific biological organism makes the design of truly robust controllers a futile endeavor, in effect putting more weight on the mode, we gain very high performance in a small but very dense sub-manifold of the state-space. Since we make use of biologically grounded arguments, we briefly outline the possible implications of this work to biological nervous systems. It is commonly acknowledged, due both to theoretical arguments and empirical findings, that some form of dimensionality reduction must be at work in neural control mechanisms. A common object in models which attempt to describe this reduction is the motor primitive , a hypothesized atomic motor program which is combined with other such programs in a small  X  X lphabet X , to produce complex behaviors in a given context. Our controllers imply a different reduction: a set of complex prototypical motor programs, each of which is near-optimal only in a small volume of the state-space, yet in that space describes the entire complexity of the solution. Giving the simplest building blocks of the model such a high degree of task specificity or context, would imply a very large number of these motor prototypes in a real nervous system, an order of magnitude analogous, in our linguistic metaphor, to that of words and concepts.
