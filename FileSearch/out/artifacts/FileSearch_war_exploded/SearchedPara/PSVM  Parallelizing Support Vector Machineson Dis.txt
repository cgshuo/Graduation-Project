 Let us examine the resource bottlenecks of SVMs in a binary classification setting to explain our train a binary classifier. SVMs aim to search a hyperplane in the Reproducing Kernel Hilbert Space (RKHS) that maximizes the margin between the two classes of data in X with the smallest train-ing error (Vapnik, 1995). This problem can be formulated as the following quadratic optimization problem: function which maps x i to an RKHS space. The decision function of SVMs is f ( x ) = w T  X  ( x )+ b , where the w and b are attained by solving P in (1). The optimization problem in (1) is the primal formulation of SVMs. It is hard to solve P directly, partly because the explicit mapping via  X  (  X  ) can make the problem intractable and partly because the mapping function  X  (  X  ) is often unknown. The method of Lagrangian multipliers is thus introduced to transform the primal formulation into the dual one variable). The weighting vector w is related with  X  in w = definite), the dual problem D (  X  ) is a convex Quadratic Programming (QP) problem with linear constraints, which can be solved via the Interior-Point method (IPM) (Mehrotra, 1992). Both the computational and memory bottlenecks of the SVM training are the IPM solver to the dual formu-lation of SVMs in (2).
 Currently, the most effective IPM algorithm is the primal-dual IPM (Mehrotra, 1992). The principal idea of the primal-dual IPM is to remove inequality constraints using a barrier function and then resort to the iterative Newton X  X  method to solve the KKT linear system related to the Hessian matrix Q in D (  X  ) . The computational cost is O ( n 3 ) and the memory usage O ( n 2 ) . In this work, we propose a parallel SVM algorithm (PSVM) to reduce memory use and to parallelize both data loading and computation. Given n training instances each with d dimensions, PSVM first loads the training data in a round-robin fashion onto m machines. The memory requirement per machine is O ( nd/m ). Next, PSVM performs a parallel row-based Incomplete Cholesky Factor-ization (ICF) on the loaded data. At the end of parallel ICF, each machine stores only a fraction of the factorized matrix, which takes up space of O ( np/m ), where p is the column dimension of the factorized matrix. (Typically, p can be set to be about ing accuracy.) PSVM reduces memory use of IPM from O( n 2 ) to O ( np/m ) , where p/m is much in (2). The computation time is improved from about O ( n 2 ) of a decomposition-based algorithm (e.g., SVMLight (Joachims, 1998), LIBSVM (Chang &amp; Lin, 2001), SMO (Platt, 1998), and Sim-pleSVM (Vishwanathan et al., 2003)) to O ( np 2 /m ). This work X  X  main contributions are: (1) PSVM achieves memory reduction and computation speedup via a parallel ICF algorithm and parallel IPM. (2) PSVM handles kernels (in contrast to other algorithmic approaches (Joachims, 2006; Chu et al., 2006)). (3) We have implemented PSVM on our parallel computing infrastructures. PSVM effec-tively speeds up training time for large-scale tasks while maintaining high training accuracy. PSVM is a practical, parallel approximate implementation to speed up SVM training on today X  X  distributed computing infrastructures for dealing with Web-scale problems. What we do not claim are as follows: (1) We make no claim that PSVM is the sole solution to speed up SVMs. Algorithmic approaches such as (Lee &amp; Mangasarian, 2001; Tsang et al., 2005; Joachims, 2006; Chu et al., 2006) can be more effective when memory is not a constraint or kernels are not used. (2) We do not claim that the algorithmic approach is the only avenue to speed up SVM training. Data-processing approaches such as (Graf et al., 2005) can divide a serial algorithm (e.g., LIBSVM) into subtasks on subsets of training data to achieve good speedup. (Data-processing and algorithmic approaches complement each other, and can be used together to handle large-scale training.) The key step of PSVM is parallel ICF (PICF). Traditional column-based ICF (Fine &amp; Scheinberg, 2001; Bach &amp; Jordan, 2005) can reduce computational cost, but the initial memory requirement is O ( np ) , and hence not practical for very large data set. PSVM devises parallel row-based ICF (PICF) as its initial step, which loads training instances onto parallel machines and performs factor-ization simultaneously on these machines. Once PICF has loaded n training data distributedly on m machines, and reduced the size of the kernel matrix through factorization, IPM can be solved on par-allel machines simultaneously. We present PICF first, and then describe how IPM takes advantage of PICF. 2.1 Parallel ICF ICF can approximate Q ( Q  X  R n  X  n ) by a smaller matrix H ( H  X  R n  X  p , p  X  n ), i.e., Q  X  HH T . ICF, together with SMW (the Sherman-Morrison-Woodbury formula ), can greatly reduce the computational complexity in solving an n  X  n linear system. The work of (Fine &amp; Scheinberg, 2001) provides a theoretical analysis of how ICF influences the optimization problem in Eq.(2). The where C is the hyperparameter of SVM, l is the number of support vectors, and  X  is the bound of Algorithm 1 Row-based PICF set to Our row-based parallel ICF (PICF) works as follows: Let vector v be the diagonal of Q and suppose equations: iterations (or say, the desired rank of the ICF matrix) p is reached.
 As suggested by G. Golub, a parallelized ICF algorithm can be obtained by constraining the par-allelized Cholesky Factorization algorithm, iterating at most p times. However, in the proposed algorithm (Golub &amp; Loan, 1996), matrix H is distributed by columns in a round-robin way on m machines (hence we call it column-based parallelized ICF). Such column-based approach is opti-mal for the single-machine setting, but cannot gain full benefit from parallelization for two major reasons:
Therefore, each machine must be able to store a local copy of the training data. 2 . Limited parallelizable computation. Only the inner product calculation ( summation of local inner product result, column calculation in (4), and the vector update in (5) must be performed on one single machine.
 To remedy these shortcomings of the column-based approach, we propose a row-based approach to parallelize ICF, which we summarize in Algorithm 1. Our row-based approach starts by initializing variables and loading training data onto m machines in a round-robin fashion (Steps 1 to 5 ). The algorithm then performs the ICF main loop until the termination criteria are satisfied (e.g., the rank of matrix H reaches p ). In the main loop, PICF performs five tasks in each iteration k :
Notice that PICF computes only needed elements in Q from training data, and it does not store Q .  X  Set the machine where the pivot resides as the master (step 11 ).  X  On the master , PICF calculates H ( i k , k ) according to (3) (step 12 ).  X  The master then broadcasts the pivot instance x i  X  Distributedly compute (4) and (5) (steps 14 and 15 ).
 At the end of the algorithm, H is stored distributedly on m machines, ready for parallel IPM (pre-sented in the next section). PICF enjoys three advantages: parallel memory use ( O ( np/m )), parallel communication overhead, its fraction of the entire computation time shrinks as the problem size grows. We will verify this in the experimental section. This pattern permits a larger problem to be solved on more machines to take advantage of parallel memory use and computation. 2.2 Parallel IPM As mentioned in Section 1, the most effective algorithm to solve a constrained QP problem is the primal-dual IPM. For detailed description and notations of IPM, please consult (Boyd, 2004; Mehro-tra, 1992). For the purpose of SVM training, IPM boils down to solving the following equations in the Newton step iteratively. where  X  and z depend only on [  X  ,  X  ,  X  ,  X  ] from the last iteration as follows: The computation bottleneck is on matrix inverse, which takes place on  X  for solving 4  X  in (8) and 4 x in (10). Equation (11) shows that  X  depends on Q , and we have shown that Q can be approximated through PICF by HH T . Therefore, the bottleneck of the Newton step can be sped up from O ( n 3 ) to O ( p 2 n ), and be parallelized to O ( p 2 n/m ).
 Distributed Data Loading To minimize both storage and communication cost, PIPM stores data distributedly as follows:  X  Distribute matrix data . H is distributedly stored at the end of PICF.  X  Distribute n  X  1 vector data . All n  X  1 vectors are distributed in a round-robin fashion on m machines. These vectors are z ,  X  ,  X  ,  X  ,  X  z ,  X   X  ,  X   X  , and  X   X  .  X  Replicate global scalar data . Every machine caches a copy of global data including  X  , t , n , and  X   X  . Whenever a scalar is changed, a broadcast is required to maintain global consistency. Parallel Computation of 4  X  According to SMW (the Sherman-Morrison-Woodbury formula ), we can write  X   X  1 z as 1 . Compute D  X  1 z . D can be derived from locally stored vectors, following (9). D  X  1 z is a n  X  1 vector, and can be computed locally on each of the m machines. of D  X  1 z . This step can be computed locally on each machine. The results are sent to the master (which can be a randomly picked machine for all PIPM iterations) to aggregate into t 1 for the next step. data. G can be obtained from H in a straightforward manner as shown in SMW. Computing t t 2 . The master then broadcasts t 2 to all machines. 4 . Compute D  X  1 Ht 2 All machines have a copy of t 2 , and can compute D  X  1 Ht 2 locally to solve Similarly,  X   X  1 y can be computed at the same time. Once we have obtained both, we can solve  X   X  according to (8). 2.3 Computing b and Writing Back When the IPM iteration stops, we have the value of  X  and hence the classification function Here N s is the number of support vectors and s i are support vectors. In order to complete this classification function, b must be computed. According to the SVM model, given a support vector s , parallel using MapReduce (Dean &amp; Ghemawat, 2004). We conducted experiments on PSVM to evaluate its 1) class-prediction accuracy, 2) scalability on large datasets, and 3) overheads. The experiments were conducted on up to 500 machines in our data center. Not all machines are identically configured; however, each machine is configured with a CPU faster than 2GHz and memory larger than 4GBytes. 3.1 Class-prediction Accuracy PSVM employs PICF to approximate an n  X  n kernel matrix Q with an n  X  p matrix H . This experiment evaluated how the choice of p affects class-prediction accuracy. We set p of PSVM to n t , that achieved by LIBSVM. The first two columns of Table 1 enumerate the datasets and their sizes with which we experimented. We use Gaussian kernel, and select the best C and  X  for LIBSVM and PSVM, respectively. For CoverType and RCV , we loosed the terminate condition (set -e 1, default 0 . 001 ) and used shrink heuristics (set -h 0 ) to make LIBSVM terminate within several days. The table shows that when t is set to 0 . 5 (or p = that of LIBSVM.
 We compared only with LIBSVM because it is arguably the best open-source SVM implementa-tion in both accuracy and speed. Another possible candidate is CVM (Tsang et al., 2005). Our experimental result on the CoverType dataset outperforms the result reported by CVM on the same dataset in both accuracy and speed. Moreover, CVM X  X  training time has been shown unpredictable hyper-parameters. For how we position PSVM with respect to other related work, please refer to our disclaimer in the end of Section 1. 3.2 Scalability For scalability experiments, we used three large datasets. Table 2 reports the speedup of PSVM on up to m = 500 machines. Since when a dataset size is large, a single machine cannot store the factorized matrix H in its local memory, we cannot obtain the running time of PSVM on one machine. We thus used 10 machines as the baseline to measure the speedup of using more than 10 machines. To quantify speedup, we made an assumption that the speedup of using 10 machines is 10 , compared to using one machine. This assumption is reasonable for our experiments, since PSVM does enjoy linear speedup when the number of machines is up to 30 .

Table 2: Speedup ( p is set to We trained PSVM three times for each dataset-m combination. The speedup reported in the table is the average of three runs with standard deviation provided in brackets. The observed variance in speedup was caused by the variance of machine loads, as all machines were shared with other tasks running on our data centers. We can observe in Table 2 that the larger is the dataset, the better is the speedup. Figures 1(a), (b) and (c) plot the speedup of Image , CoverType , and RCV , respectively. All datasets enjoy a linear speedup when the number of machines is moderate. For instance, PSVM achieves linear speedup on RCV when running on up to around 100 machines. PSVM scales well till around 250 machines. After that, adding more machines receives diminishing returns. This result led to our examination on the overheads of PSVM, presented next. (a) Image (200k) speedup (b) Covertype (500k) speedup (c) RCV (800k) speedup (d) Image (200k) overhead (e) Covertype (500k) overhead (f) RCV (800k) overhead (g) Image (200k) fraction (h) Covertype (500k) fraction (i) RCV (800k) fraction 3.3 Overheads PSVM cannot achieve linear speedup when the number of machines continues to increase beyond a data-size-dependent threshold. This is expected due to communication and synchronization over-heads. Communication time is incurred when message passing takes place between machines. Syn-chronization overhead is incurred when the master machine waits for task completion on the slowest machine. (The master could wait forever if a child machine fails. We have implemented a check-point scheme to deal with this issue.) The running time consists of three parts: computation (Comp), communication (Comm), and syn-chronization (Sync). Figures 1(d), (e) and (f) show how Comm and Sync overheads influence the speedup curves. In the figures, we draw on the top the computation only line (Comp), which ap-proaches the linear speedup line. Computation speedup can become sublinear when adding ma-chines beyond a threshold. This is because the computation bottleneck of the unparallelizable step 12 in Algorithm 1 (which computation time is O ( p 2 )). When m is small, this bottleneck is insignif-icant in the total computation time. According to the Amdahl X  X  law; however, even a small fraction of unparallelizable computation can cap speedup. Fortunately, the larger the dataset is, the smaller is this unparallelizable fraction, which is O ( m/n ). Therefore, more machines (larger m ) can be employed for larger datasets (larger n ) to gain speedup. When communication overhead or synchronization overhead is accounted for (the Comp + Comm line and the Comp + Comm + Sync line), the speedup deteriorates. Between the two overheads, the synchronization overhead does not impact speedup as much as the communication overhead does. Figures 1(g), (h), and (i) present the percentage of Comp, Comm, and Sync in total running time. The synchronization overhead maintains about the same percentage when m increases, whereas the percentage of communication overhead grows with m . As mentioned in Section 2.1, the communi-node decreases as m increases, the fraction of the communication overhead grows with m . There-fore, PSVM must select a proper m for a training task to maximize the benefit of parallelization. In this paper, we have shown how SVMs can be parallelized to achieve scalable performance. PSVM distributedly loads training data on parallel machines, reducing memory requirement through ap-proximate factorization on the kernel matrix. PSVM solves IPM in parallel by cleverly arranging computation order. We have made PSVM open source at http://code.google.com/p/psvm/. The first author is partially supported by NSF under Grant Number IIS-0535085.
 Bach, F. R., &amp; Jordan, M. I. (2005). Predictive low-rank decomposition for kernel methods. Pro-ceedings of the 22nd International Conference on Machine Learning .
 Boyd, S. (2004). Convex optimization . Cambridge University Press.
 Chang, C.-C., &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines . Software avail-able at http://www.csie.ntu.edu.tw/ cjlin/libsvm .
 Chu, C.-T., Kim, S. K., Lin, Y.-A., Yu, Y., Bradski, G., Ng, A. Y., &amp; Olukotun, K. (2006). Map reduce for machine learning on multicore. NIPS .
 OSDI X 04: Symposium on Operating System Design and Implementation .
 Fine, S., &amp; Scheinberg, K. (2001). Efficient svm training using low-rank kernel representations. Journal of Machine Learning Research , 2 , 243 X 264.
 Ghemawat, S., Gobioff, H., &amp; Leung, S.-T. (2003). The google file system. 19th ACM Symposium on Operating Systems Principles .
 Golub, G. H., &amp; Loan, C. F. V. (1996). Matrix computations . Johns Hopkins University Press. machines: The cascade svm. In Advances in neural information processing systems 17 , 521 X 528. Joachims, T. (1998). Making large-scale svm learning practical. Advances in Kernel Methods -Support Vector Learning .
 Joachims, T. (2006). Training linear svms in linear time. ACM KDD , 217 X 226.
 Lee, Y.-J., &amp; Mangasarian, O. L. (2001). Rsvm: Reduced support vector machines. First SIAM International Conference on Data Mining . Chicago.
 Loosli, G., &amp; Canu, S. (2006). Comments on the core vector machines: Fast svm training on very large data sets (Technical Report).
 Mehrotra, S. (1992). On the implementation of a primal-dual interior point method. SIAM J. Opti-mization , 2 .
 machines (Technical Report MSR-TR-98-14). Microsoft Research.
 Tsang, I. W., Kwok, J. T., &amp; Cheung, P.-M. (2005). Core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research , 6 , 363 X 392.
 Vapnik, V. (1995). The nature of statistical learning theory . New York: Springer. Vishwanathan, S., Smola, A. J., &amp; Murty, M. N. (2003). Simplesvm. ICML .
