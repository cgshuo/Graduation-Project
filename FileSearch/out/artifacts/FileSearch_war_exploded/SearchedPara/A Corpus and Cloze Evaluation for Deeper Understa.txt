 Story understanding is an extremely challenging task in natural language understanding with a long-running history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). Recently, there has been a renewed interest in story and narrative understanding based on progress made in core NLP tasks. This ranges from generic story telling models to building systems which can com-pose meaningful stories in collaboration with hu-mans (Swanson and Gordon, 2008). Perhaps the biggest challenge of story understanding (and story generation) is having commonsense knowledge for the interpretation of narrative events. The question is how to provide commonsense knowledge regard-ing daily events to machines.

A large body of work in story understanding has focused on learning scripts (Schank and Abel-son, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference res-olution, question answering, etc.) can hugely ben-efit from the rich inferential capabilities that struc-tured knowledge about events can provide. Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automati-cally induced scripts. Most relevant to this issue is work on unsupervised learning of  X  X arrative chains X  (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are fore-most interested in a resource that is full of temporal and causal relations between events because causal-ity is a central component of coherency. Personal stories from daily weblogs are good sources of com-monsense causal information (Gordon and Swan-son, 2009; Manshadi et al., 2008), but teasing out useful information from noisy blog entries is a prob-lem of its own. Consider the following snippet from ICWSM 2011 Spinn3r Dataset of Weblog entries (Burton et al., 2009): This text is full of discourse complexities. A host of challenging language understanding tasks are re-quired to get at the commonsense knowledge em-bedded within such text snippets. What is needed is a simplified version of these narratives. This pa-per introduces a new corpus of such short common-sense stories. With careful prompt design and mul-tiple phases of quality control, we collected 50k high quality five-sentence stories that are full of stereotypical causal and temporal relations between events. The corpus not only serves as a resource for learning commonsense narrative schemas, but is also suitable for training story generation models. We de-scribe this corpus in detail in Section 3.

This new corpus also addresses a problem facing script learning over the past few years. Despite the attention scripts have received, progress has been in-hibited by the lack of a systematic evaluation frame-work. A commonly used evaluation is the  X  X arra-tive Cloze Test X  (Chambers and Jurafsky, 2008) in which a system predicts a held-out event (a verb and its arguments) given a set of observed events. For example, the following is one such test with a missing event: { X threw, pulled X, told X, ???, X completed } 1 . As is often the case, several works now optimize to this specific test, achieving higher scores with shallow techniques. This is problematic because the models often are not learning common-sense knowledge, but rather how to beat the shallow test.

This paper thus introduces a new evaluation framework called the Story Cloze Test. Instead of predicting an event, the system is tasked with choos-ing an entire sentence to complete the given story. We collected 3,742 doubly verified Story Cloze Test cases. The test is described in detail in Section 4.
Finally, this paper proposes several models, in-cluding the most recent state-of-the-art approaches for the narrative cloze test, for tackling the Story Cloze Test. The results strongly suggest that achiev-ing better than random or constant-choose perfor-mance requires richer semantic representation of events together with deeper levels of modeling the semantic space of stories. We believe that switching to the Story Cloze Test as the empirical evaluation framework for story understanding and script learn-ing can help direct the field to a new direction of deeper language understanding. Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Cham-bers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the  X  X rotagonist X . A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Cham-bers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Che-ung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learn-ing. A major shortcoming so far is that these models are mainly trained on news articles. Little knowl-edge about everyday life events are learned.
Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, oth-ers have shown language-modeling techniques per-form well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation.
There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frame-works for evaluating language comprehension (Her-mann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story compre-hension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific do-mains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restau-rant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M  X  endez et al., 2014; Riedl and Le  X  on, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding. We aimed to build a corpus with two goals in mind: 1. The corpus contains a variety of commonsense 2. The corpus is a high quality collection of non-
In order to narrow down our focus, we carefully define a narrative or story as follows:  X  X  narrative or story is anything which is told in the form of a causally (logically) linked set of events involv-ing some shared characters X . The classic definition of a story requires having a plot, (e.g., a charac-ter following a goal and facing obstacles), however, here we are not concerned with how entertaining or dramatic the stories are. Instead, we are con-cerned with the essence of actually being a logi-cally meaningful story. We follow the notion of  X  X toriness X  (Forster, 1927; Bailey, 1999), which is described as  X  X he expectations and questions that a reader may have as the story develops X , where expectations are  X  X ommon-sense logical inferences X  made by the imagined reader of the story.

We propose to satisfy our two goals by asking hundreds of workers on Amazon Mechanical Turk (AMT) to write novel five-sentence stories. The five-sentence length gives enough context to the story without allowing room for sidetracks about less im-portant or irrelevant information in the story. In this Section we describe the details about how we col-lected this corpus, and provide statistical analysis. 3.1 Data Collection Methodology Crowdsourcing this corpus makes the data collec-tion scalable and adds to the diversity of stories. We tested numerous pilots with varying prompts and in-structions. We manually checked the submitted sto-ries in each pilot and counted the number of sub-missions which did not have our desired level of co-herency or were specifically fictional or offensive. Three people participated in this task and they iter-ated over the ratings until everyone agreed with the next pilot X  X  prompt design. We achieved the best re-sults when we let the workers write about anything they have in mind, as opposed to mandating a pre-specified topic. The final crowdsourcing prompt can be found in supplementary material.

The key property that we had enforced in our final prompt was the following: the story should read like a coherent story, with a specific begin-ning and ending , where something happens in be-tween. This constraint resulted in many causal and temporal links between events. Table 1 shows the examples we provided to the workers for instruct-ing them about the constraints. We set a limit of 70 characters to the length of each sentence. This prevented multi-part sentences that include unnec-essary details. The workers were also asked to pro-vide a title that best describes their story. Last but not least, we instructed the workers not to use quo-tations in their sentences and avoid using slang or informal language.

Collecting high quality stories with these con-straints gives us a rich collection of commonsense stories which are full of stereotypical inter-event re-ending, and (3) not stating anything irrelevant to the story. Figure 1: An example narrative chain with charac-ters X and Y. lations. For example, from the good story in first row of Table 1, one can extract the narrative chain represented in Figure 1. Developing a better se-mantic representation for narrative chains which can capture rich inter-event relations in these stories is a topic of future work.

Quality Control: One issue with crowdsourcing is how to instruct non-expert workers. This task is a type of creative writing, and is trickier than classifi-cation and tagging tasks. In order to ensure we get qualified workers, we designed a qualification test on AMT in which the workers had to judge whether or not a given story (total five stories) is an accept-able one. We used five carefully selected stories to be a part of the qualification test. This not only elim-inates any potential spammers on AMT, but also pro-vides us with a pool of creative story writers. Fur-thermore, we qualitatively browsed through the sub-missions and gave the workers detailed feedback be-fore approving their submissions. We often bonused our top workers, encouraging them to write new sto-ries on a daily basis.

Statistics: Figure 2 shows the distribution of number of tokens of different sentence positions. The first sentence tends to be shorter, as it usually introduces characters or sets the scene, and the fifth sentence is longer, providing more detailed conclu-sions to the story. Table 2 summarizes the statistics of our crowdsourcing effort. Figure 3 shows the dis-tribution of the most frequent 50 events in the cor-pus. Here we count event as any hyponym of  X  X vent X  or  X  X rocess X  in WordNet (Miller., 1995). The top two events,  X  X o X  and  X  X et X , each comprise less than 2% of all the events, which illustrates the rich diversity of the corpus.
 Figure 2: Number of tokens in each sentence posi-tion.
Figure 4 visualizes the n-gram distribution of our story titles, where each radial path indicates an n-Figure 3: Distribution of top 50 events in our corpus. gram sequence. For this analysis we set n=5, where the mean number of tokens in titles is 9.8 and me-dian is 10. The  X  X nd X  token distinguishes the actual ending of a title from five-gram cut-off. This fig-ure demonstrates the range of topics that our workers have written about. The full circle reflects on 100% of the title n-grams and the n-gram paths in the faded 3/4 of the circle comprise less than 0.1% of the n-grams. This further demonstrates that the range of topics covered by our corpus is quite diverse. A full dynamic visualization of these n-grams can be found here: http://goo.gl/Qhg60B .
 3.2 Corpus Release The corpus is publicly available to the com-munity and can be accessed through http: //cs.rochester.edu/nlp/rocstories , which will be grown even further over the coming years. Given the quality control pipeline and the creativity required from workers, data collection goes slowly.

We are also making available semantic parses of these stories. Since these stories are not newswire, off-the-shelf syntactic and shallow semantic parsers for event extraction often fail on the language. To address this issue, we customized search param-formance on our corpus. TRIPS parser (Allen et al., 2008) produces state-of-the-art logical forms for input stories, providing sense disambiguated and ontology-typed rich deep structures which enables event extraction together with semantic roles and coreference chains throughout the five sentences. 3.3 Temporal Analysis Being able to temporally order events in the stories is a pre-requisite for complete narrative understand-ing. Temporal analysis of the events in our short commonsensical stories is an important topic of fur-ther research on its own. In this Section, we sum-marize two of our analyses regarding the nature of temporal ordering of events in our corpus.

Shuffling Experiment: An open question in any text genre is how text order is related to tempo-ral order. Do the sentences follow the real-world temporal order of events? This experiment shuf-fles the stories and asks AMT workers to arrange them back to a coherent story. This can shed light on the correlation between the original position of the sentences and the position when another human rearranges them in a commonsensically meaningful way. We set up this experiment as follows: we sam-pled two sets of 50 stories from our corpus: Good-Stories 50 and Random-Stories 50 . Good-Stories 50 4 is sampled from a set of stories written by top workers who have shown shown consistent quality through-out their submissions. Random-Stories 50 5 is a ran-dom sampling from all the stories in the corpus. Then we randomly shuffled the sentences in each story and asked five crowd workers on AMT to rear-range the sentences.
 Table 3 summarizes the results of this experiment. The first row shows the result of ordering if we take the absolute majority ordering of the five crowd workers as the final ordering. The second row shows the result of ordering if we consider each of the 250 (50 stories x 5 workers ordering each one) ordering cases independently. As shown, the good stories are perfectly ordered with very high accuracy. It is im-portant to note that this specific set rarely had any linguistic adverbials such as  X  X irst X ,  X  X hen X , etc. to help human infer the ordering, so the main factors at play are the following: (1) the commonsensical temporal and causal relation between events (narra-tive schemas), e.g., human knows that first some-one loses a phone then starts searching; (2) the nat-ural way of narrating a story which starts with intro-ducing the characters and concludes the story at the end. The role of the latter factor is quantified in the misplacement rate of each position reported in Table 3, where the first and last sentences are more often correctly placed than others. The high precision of ordering in sentences 2 up to 4 further verifies the richness of our corpus in terms of logical relation between events.

TimeML Annotation: TimeML-driven analysis of these stories can give us finer-grained insight about temporal aspect of the events in this corpus. We performed a simplified TimeML-driven (Puste-jovsky et al., 2003) expert annotation of a sample of annotated, 62% were  X  X efore X  and 10% were  X  X imul-taneous X . We were interested to know if the actual text order mirrors real-world order of events. We found that sentence order matches TimeML order 55% of the time. A more comprehensive study of temporal and causal aspects of these stories requires defining a specific semantic annotation framework which covers not only temporal but also causal re-lations between commonsense events. This is cap-tured in a recent work which introduces a Causal and Temporal Relation Scheme (CaTeRS) for semantic annotation of event structures (Mostafazadeh et al., 2016). As described earlier in the introduction, the common evaluation framework for script learning is the  X  X ar-rative Cloze Test X  (Chambers and Jurafsky, 2008), where a system generates a ranked list of guesses for a missing event, given some observed events. The original goal of this test was to provide a compara-tive measure to evaluate narrative knowledge. How-ever, gradually, the community started optimizing towards the performance on the test itself, achiev-ing higher scores without demonstrating narrative knowledge learning. For instance, generating the ranked list according to the event X  X  corpus frequency (e.g., always predicting  X  X  said X ) was shown to be an extremely strong baseline (Pichotta and Mooney, 2014b). Originally, narrative cloze test chains were extracted by hand and verified as gold chains. How-ever, the cloze test chains used in all of the most recent works are not human verified as gold.
It is evident that there is a need for a more system-atic automatic evaluation framework which is more in line with the original deeper script/story under-standing goals. It is important to note that reorder-ing of temporally shuffled stories (Section 3.3) can serve as a framework to evaluate a system X  X  story un-derstanding. However, reordering can be achieved to a degree by using various surface features such as adverbials, so this cannot be a foolproof story un-derstanding evaluation framework. Our ROCStories corpus enables a brand new framework for evalu-ating story understanding, called the  X  X tory Cloze Test X  . 4.1 Story Cloze Test The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce  X  X tory Cloze Test X , in which a system is given a four-sentence  X  X ontext X  and two alternative endings to the story, called  X  X ight ending X  and  X  X rong end-ing X . Hence, in this test the fifth sentence is blank. Then the system X  X  task is to choose the right end-ing. The  X  X ight ending X  can be viewed as  X  X ntailing X  hypothesis in a classic Recognizing Textual Entail-ment (RTE) framework (Giampiccolo et al., 2007), and  X  X rong X  ending can be seen as the  X  X ontradict-ing X  hypothesis. Table 4 shows three example Story Cloze Test cases.

Story Cloze Test will serve as a generic story understanding evaluation framework, also applica-ble to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story genera-tion model), which does not necessarily imply re-quirement for explicit narrative knowledge learning. However, it is safe to say that any model that per-forms well on Story Cloze Test is demonstrating some level of deeper story understanding. 4.2 Data Collection Methodology We randomly sampled 13,500 stories from ROCSto-ries Corpus and presented only the first four sen-tences of each to AMT workers. For each story, a worker was asked to write a  X  X ight ending X  and a  X  X rong ending X . The workers were prompted to sat-isfy two conditions: (1) the sentence should follow up the story by sharing at least one of the characters of the story, and (2) the sentence should be entirely realistic and sensible when read in isolation. These conditions make sure that the Story Cloze Test cases are not trivial. More details on this setup is described in the supplementary material.
 Quality Control: The accuracy of the Story Cloze Test can play a crucial role in directing the research community in the right trajectory. We im-plemented the following two-step quality control: 1. Qualification Test: We designed a qualification 2. Human Verification: In order to further validate
Statistics: Table 5 summarizes the statistics of our crowdsourcing effort. The Story Cloze Test sets can also be accessed through our website. In this Section we demonstrate that Story Cloze Test cannot be easily tackled by using shallow tech-niques, without actually understanding the underly-ing narrative. Following other natural language in-ference frameworks such as RTE, we evaluate sys-tem performance according to basic accuracy mea-sure, which is defined as # correct following baselines and models for tackling Story Cloze Test. All of the models are tested on the vali-dation and test Story Cloze sets, where only the val-idation set could be used for any tuning purposes. 1. Frequency : Ideally, the Story Cloze Test cases should not be answerable without the context. For example, if for some context the two alternatives cheerful after he won X , the first alternative is sim-ply less probable in real world than the other one. This baseline chooses the alternative with higher Table 5: Statistics for crowd-sourcing Story Cloze Test instances. with its semantic roles (e.g.,  X  X *poison*flowers X  vs  X  X *nourish*flowers X ). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap : Simply chooses the alterna-tive which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec : Choose the hy-pothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec em-bedding of the context. This is basically an en-hanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full : Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last : Choose the hypothesis that matches the sentiment of the last context sentence. 6. Skip-thoughts Model : This model uses Skip-thoughts X  Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels. This model is trained on the  X  X ookCorpus X  (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model. 7. Narrative Chains-AP : Implements the standard approach to learning chains of narrative events based on Chambers and Jurafsky (2008). An event is rep-resented as a verb and a typed dependency (e.g., the subject of runs ). We computed the PMI between all event pairs in the Associate Press (AP) portion of the English Gigaword Corpus that occur at least 2 times. We run coreference over the given story, and choose the hypothesis whose coreferring entity has the highest average PMI score with the entity X  X  chain in the story. If no entity corefers in both hypotheses, it randomly chooses one of the hypotheses. 8. Narrative Chains-Stories : The same model as above, but trained on ROCStories. 9. Deep Structured Semantic Model (DSSM) : This model (Huang et al., 2013) is trained to project the four-sentences context and the fifth sentence into the same vector space. It consists of two separate deep neural networks for learning jointly the em-bedding of the four-sentences context and the fifth sentence, respectively. As suggested in Huang et al. (2013), the input of the DSSM is based on context-dependent characters, e.g., the distribution count of letter-trigrams in the context and in the fifth sen-tence, respectively. The hyper parameters of the DSSM is determined on the validation set, while the model X  X  parameters are trained on the ROCStories corpus. In our experiment, each of the two neural networks in the DSSM has two layers: the dimen-sion of the hidden layer is 1000, and the dimension of the embedding vector is 300. At runtime, this model picks the candidate with the largest cosine similarity between its vector representation and the context X  X  vector representation.
 The results of evaluating these models on the Story Cloze validation and test sets are shown in Ta-ble 6. The constant-choose-first (51%) and human performance (100%) is also provided for compari-son. Note that these sets were doubly verified by human, hence it does not have any boundary cases, resulting in 100% human performance. The DSSM model achieves the highest accuracy, but only 7.2 points higher than constant-choose-first. Error anal-ysis on the narrative chains model shows why this and other event-based language models are not suf-ficient for the task: often, the final sentences of our stories contain complex events beyond the main verb, such as  X  X ill was highly unprepared X  or  X  X e had to go to a homeless shelter X . Event language models only look at the verb and syntactic relation like  X  X as-object X  and  X  X o-to X . In that sense, going to a homeless shelter is the same as going to the beach. This suggests the requirement of having richer se-mantic representation for events in narratives. Our proposed Story Cloze Test offers a new challenge to the community. There are three core contributions in this paper: (1) a new corpus of commonsense stories, called ROC-Stories, (2) a new evaluation framework to evalu-ate script/story learners, called Story Cloze Test, and (3) a host of first approaches to tackle this new test framework. ROCStories Corpus is the first crowd-sourced corpus of its kind for the community. We have released about 50k stories, as well as valida-tion and test sets for Story Cloze Test. This dataset will eventually grow to 100k stories, which will be released through our website. In order to continue making meaningful progress on this task, although it is possible to keep increasing the size of the training data, we expect the community to develop models that will learn to generalize to unseen commonsense concepts and situations.

The Story Cloze Test proved to be a challenge to all of the models we tested. We believe it will serve as an effective evaluation for both story understand-ing and script knowledge learners. We encourage the community to benchmark their progress by re-porting their results on Story Cloze test set. Com-pared to the previous Narrative Cloze Test, we found that one of the early models for that task actually performs worse than random guessing. We can con-clude that Narrative Cloze test spurred interest in script learning, however, it ultimately does not eval-uate deeper knowledge and language understanding. We would like to thank the amazing crowd workers whose endless hours of daily story writing made this research possible. We thank William de Beaumont and Choh Man Teng for their work on TRIPS parser. We thank Alyson Grealish for her great help in the quality control of our corpus. This work was sup-ported in part by Grant W911NF-15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA), the Army Research Office (ARO) and the Office of Naval Research (ONR). Our data collec-tion effort was sponsored by Nuance Foundation.
