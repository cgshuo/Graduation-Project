 Data mining has attracted a great deal of attention in the industry in recent years due to the wide availability of huge volume of data and the imminent need for turning such data into useful information and knowledge [1]. Data min-ing generally refers to the process of ext racting models or determining patterns from large observed data [2]. It involves an integration of techniques from mul-tiple disciplines such as database technology, statistics, machine learning, high-performance computing, spatial data analysis, neural network and others.
Recently, researchers have started usin g data mining techniques in the emerg-ing field of information and system securi ty and specially in intrusion detection systems. An intrusion is defined as any set of actions that attempt to compromise the integrity, confidentiality or availability of a resource. Intrusion Detection is the process of monitoring the events o ccurring in a computer system or network and analyzing them for signs of intrusions [3].

Intrusion detection has been discussed in public research since the beginning of the 1980s. In the last few years, it became an active area of research and commercial IDSs started emerging [4]. Several research works also have been proposed that apply data mining for intrusion detection. Lee et al [5] have sug-gested data mining techniques for netwo rk intrusion detect ion. They consider several categories of data mining algorithms, namely, classification, link analysis and sequential analysis along with their applicability in the field of intrusion detection. Barbara et al [6] have built a testbed using data mining techniques to detect network intrusions . Though intrusion detection is a well researched area, only a few researches have focused on database intrusion detection. Chung et al [7] use the idea of  X  X orking scope X  to find the frequent itemsets referenced to-gether and used this information for anomaly detection. Lee et al [8] propose an intrusion detection system in real-time da tabases using time signatures. Lee et al [9] have suggested a method for fingerprin ting the access patterns of legitimate database transactions and using them to identify database intrusions. Barbara et al [10] use hidden markov model (HMM) and time series to find malicious corruption of data. They use HMM to build database behavioral models that capture the changing behavior over time, and uses them to recognize malicious patterns. Zhong et al [11] have proposed an algorithm to mine user profiles based on the queries submitted by the user. Hu et al [12] have proposed an idea of de-termining dependency among data items in databases. The transactions that do not follow the mined data dependencies are identified as malicious transactions.
In this paper, we propose an algorithm for database intrusion detection us-ing a data mining technique, which takes the sensitivity of the attributes into consideration. Sensitivity of an attribute signifies how important the attribute is for tracking against malicious modifications. This approach mines dependency among attributes in a database. The transactions that do not follow these de-pendencies are marked as malicious transactions.

The rest of the paper is organized as follows. In Section 2, we describe weighted data dependency rule mining ( W DDRM ) algorithm with an exam-ple. We present details of our experim entsandprovideresultsinSection3. Finally, we conclude the paper with some discussions. 2.1 Intuition Databases are increasing in size in two ways: the number N of records, or objects in the database, and the number d of fields, or attributes, per object. Databases containing of the order of N =10 9 objects are increasingly common nowadays. The number d of attributes can easily be of the order of 10 2 or even 10 3 in various applications [2]. With the number of attributes increasing at such a high rate, it is very difficult for administrators to keep track of attributes whether they are accessed or modified correctly or not. By dividing the attributes into different categories based on their relative importance or sensitivity, it is comparatively easier to track only those attributes whose unintended modification can have the largest impact on the application or the system.

Practitioners as well as researchers have observed that IDS can easily trigger thousands of alarms per day, a number of which are triggered incorrectly by benign events [13]. Categorization of attributes helps the administrator to check only those alarms, which are generated due to malicious modification of sensitive data instead of checking all the attributes. Since the main objective of a database intrusion detection system is to minimize the loss suffered by the owner of the database, it is important to track high sensitive attributes with more accuracy.
If sensitive attributes are to be tracked for malicious modifications then we need to generate data dependency rules for these attributes. Unless there is a rule for an attribute, the attribute cannot be checked. If high sensitive attributes are accessed less frequently, then there m ay not be any rule generated for these attributes. The motivation for dividing a ttributes in different sensitivity groups and assigning weights to each group is to bring out the dependency rules for possibly less frequent but more important attributes. Once we have rules for these sensitive attributes, we can check them in each transaction and if any transaction does not follow the mined rules, it will be marked as malicious.
We discuss the main components of an IDS in the following subsections. 2.2 Security Sensitive Sequence Mining The problem of finding sequences among the attributes along with the opera-tions { read,write } is similar to the problem of mining sequential patterns. Mining sequences from large sets of data is a kn own problem. Agrawal et al [14] have proposed an algorithm for finding sequential patterns from data. In this algo-rithm, all the data items are considered at the same level without any weightage. We modify an existing sequential mining algorithm and make it security sensi-tive sequential mining by introducing weights for each attribute based on the sensitivity group. Higher the sensitivity of an attribute, higher is its weight. We have categorized the attributes in three sets : High Sensitivity (HS) attribute set, Medium Sensitivity (MS) attribute s et and Low Sensitivity (LS) attribute set. The sensitivity of an attribute is dependent on the particular database ap-plication. Also, modification of the sensitive attributes are more important than reading those attributes from the point of view of integrity. For the same at-tribute say x ,if x  X  HS then W ( x w ) &gt;W ( x r ), where W is a weight func-tion, x w denotes writing or modifying attribute x and x r denotes reading of attribute x .

Given a schema, we categorize all the attributes into the above mentioned three sets based on their sensitivities and assign numerical weights to each set. Let w 1 ,w 2 ,w 3  X  R ,where R is the set of real numbers and w 3  X  w 2  X  w 1 are the weights of HS,MS and LS , respectively. Let d 1 ,d 2 ,d 3  X  R be the additional weights of the write operations for each category such that d 3  X  d 2  X  d 1 .Let x  X  HS be an attribute which is accessed in a read operation. Then the weight given to x is w 1 . If it is accessed in write operation then the weight given to x is w 1 + d 1 .
For security sensitive sequence mining, we assign weights to each sequence based on the sensitivity groups of the attributes present in the sequence. The weight assigned to a sequence is the same as the weight of the most sensitive attribute present in that sequence. Th e weight assigned to each sequence also depends on the operation applied on the attributes.

The weights assigned to all the sequen ces are used in the second pruning step which calculates the support of each sequence in the transaction. If support value for any sequence is above the minimum support, the sequence is considered to be a frequent sequence. Let us assume that there is a sequence s with weight w s . Let N be the total number transactions. If s is present in n transactions out of N transactions, then the support of sequence s would be:
The effect of this weighted approach on sequence mining algorithm is sig-nificant. With this approach, sequences containing high sensitive attributes but accessed less in the transa ctions can become frequent sequences because each such sequence X  X  count is enhanced by multiplying with its weight. The weighted support can now exceed the minimum support.

Consider the example transactions shown in Figure 1. There are 10 transac-tions. These transactions are generated from the bank database schema shown in Figure 2 with attributes encoded into i ntegers. In Figure 3, the weight of each attribute is shown. These attributes are categorized into HS,MS and LS groups depending upon the sensitivity. First, these transactions are given input to a se-quential pattern mining algorithm [14] for extracting the sequences using normal definition of support. These transactions and weights are also given as input to the proposed weighted sequential mining algorithm. Here, support values of the sequences are calculated using equation (1). In both the cases, minimum support is set to 25%. The sequences generated from the two algorithms are shown in Figure 4. 2.3 Read-Write Sequence Generation In this subsection, we first define some of the terminologies used in the rest of the paper.
 Definition 1. A read sequence denoted as ReadSeq of attribute a j is the se-tributes a 1 to a k that are read before attribute a j is written. All such sequences formasetnamedasreadsequencesetdenotedby ReadSeqSet .
 Definition 2. A write sequence denoted as W riteSeq of attribute a j is the se-tributes a 1 to a k that are written after attribute a j is written. All such sequences form a set named as write sequence set denoted by W riteSeqSet .
 The sequences shown in Figure 4 are nex t used to generate read and write se-quences. As per the definitions, ReadSeq and W riteSeq must contain at least one write operation. All the sequences that do not have any attribute with write oper-ation, are not used for read and write sequence generation. A sequence that con-tains a single attribute does not contribute to the generation of dependency rules and hence will be ignored too. The read-w rite sequences are generated as follows. For each write operation a j w in a sequence, add &lt;a 1 r ,a 2 r ...a k r ,a j w &gt; to ReadSeqSet where a 1 r ,a 2 r ...a k r are the read operations on attributes a 1 to a k before the write operation on attribute a j . To generate write sequences, for each where a 1 w ,a 2 w , ....a k w are write operations on attributes a 1 to a k after the write operation on attribute a j . The read-write sequences generated from the mined sequences of Figure 4 are shown in Figure 5. 2.4 Weighted Data Dependency Rule Generation There are two types of data dependency rules, namely, read rules and write rules. These rules are generated from the rea d and write sequences. Weighted data dependency rule generation uses weighted confidence. The confidence of the read and write rules are calculated by the following method.

Let R be a read rule of the form a j w  X  a 1 r ,a 2 r , ....a k r , generated from the read sequence rs  X  ReadSeqSet .LetCount( a j w ) and Count( rs ) be the total count of the attribute a j w and that of rs among the total number of transactions. The weighted confidence of the rule R is defined as: Count( a j w ) is defined as follows: Count( rs ) is defined as: The rules generated from the read-write sequences are shown in Figure 6.
After the rules are generated, they are used to verify whether the incoming transactions are malicious or not. If an incoming transaction has a write oper-ation, it is checked whether there are any corresponding read or write rules. If the write operation violates these rules, it is marked as malicious and an alarm is generated. Otherwise, normal operat ion proceeds. The compl ete algorithm for the weighted data dependency rule mining is shown in the Figure 7. We have carried out several experiments to show the efficacy of the developed method. The system has been developed using Java as front end and MS SQL 2000 Server as the back end database. We have used the bank database of Fig-ure 2 for our experiments. Volunteers fro m our institute were invited to interact with the system and make malicious transactions. This was beneficial because the interaction by the volunteers helped us to capture real data that would be expected in a normal application. They we re provided the schema and the infor-mation on sensitive attributes. The volunteers tried novel ways of committing malicious transactions since it was announced that scores would be awarded based on the total weight of attributes they could modify.

In the learning phase, we have generat ed a number of sets of training data with each set of size 10,000 transactions having different distributions. In one experiment, we have used the following distributions. Insert/Update=90%, Se-lect=10%. We also choose the number of transactions containing most sensitive attributes in the training data as a parameter. We used 20% of the transactions with highly sensitive attributes in the training data. All these parameters are varied for different experiments. The support and confidence values are .25 and .70, respectively. Once the transactions are generated, we have run the non-weighted algorithm to generate the data dependency rules. After that, we have used WDDRM algorithm on the training data with weight ratios 1:2:3 for LS, MS and HS groups, respectively. In the e xperiments, we have taken additional weight of write operation as 0.25 for all the three categories. We used weights for different groups as another parameter. Dependency rules for each set of weights were finally generated.

In order to study relative performance, we have compared our work with the non-weighted dependency rule mining approach. We call this method as DDRM and use it for comparison. Figure 8(a) shows a comparison of DDRM and W DDRM . The percentage of malicious tran sactions detected is plotted against the sensitivity ratio. When the weights of all three groups are equal, then W DDRM reduces to DDRM . However, when distinct weights are as-signed to the three groups, W DDRM detects higher percentage of malicious transactions. DDRM cannot be effectively applied in this situation. In Figure 8(b) , comparative performances is shown for each sensitivity group. It is seen that W DDRM outperforms DDRM for more sensitive attributes.

Figure 9(a) shows the effect of the number of write operations on the per-formance of the intrusion detection system. As the number of write operations increases, the effectiveness of the syst em also increases. This is because write operations are required to generate the data dependency rules. If there are more write operations on attributes in the transactions, more rules are generated. Hence, detection rate increases if more in sert/update statements are present in the transactions. Figure 9(b) shows the loss suffered by the intrusion detection system in terms of weight unit using both the approaches. The ratio of weights used for the experiment is 3:2:1 for HS , MS and LS , respectively and distribu-tion of Insert/Update=90% and Select=10%. Loss is computed by adding the weights of all the attributes whose malicious modifications are not detected by the IDS. It is evident from the figure that W DDRM outperforms DDRM .This is because W DDRM tracks the sensitive attributes in a much better way than DDRM and hence overall loss is minimized. In this paper, we have identified some of the limitations of the existing data mining based intrusion detection systems, in particular, their incapability in treating database attributes at different levels of sensitivity. We proposed a novel weighted data dependency rule mining algorithm that considers the sensitivity of the attributes while mining the dependency rules. Experimental results show that our proposed algorithm performs better than some of the previous work done in this area. The sensitivity levels can be syntactically captured during data modeling through the E-R diagram notations.
 This work is partially supported by a research grant from the Department of In-formation Technology, Ministry of Communication and Information Technology, Government of India, under Gra nt No. 12(34)/04-IRSD dated 07/12/2004.
