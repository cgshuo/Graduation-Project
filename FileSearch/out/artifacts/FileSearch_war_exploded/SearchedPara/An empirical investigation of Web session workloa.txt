 1. Introduction
By one estimate, over two billion people now use the Internet ( Staff, 2010 ). This user population now demands rich-media content in their Web experience. They want to play massively-multiplayer online games, receive IPTV, and store their data in the  X  X  X loud X  X   X  and all of this consumes huge amounts of bandwidth. Estimates of Internet traffic growth continue to show large year-over-year compound growth, reaching 1300 exabytes by 2016. Web traffic continues to be a large segment of this traffic, accounting for 5.4 exabytes of traffic per month in 2012 and is expected to rise to 17.5 exabytes per month in 2016, a compound annual growth rate of 35% ( peak traffic is growing even faster) ( Staff, 2012 ). Moreover, the number of web users fic volumes mean that performance testing of network systems using realistic data is essential, and yet infeasible; it is simply not possible to indefinitely store a sample of network data covering an operationally significant period of time. Instead, mod-ern performance testing relies on traffic generators to produce synthetic network traffic with the same characteristics as mark paper ( Leland et al., 1994 ), network traffic in general has been known to be self-similar; informally, this means that observations of traffic intensity will tend to be identically distributed irrespective of the time scale of these observations.  X 
More recent findings also extend this result to Web sessions (a measure that captures user behavior), which have also been found to be self-similar ( Arlitt &amp; Jin, 2000; Arlitt &amp; Williamson, 1997 ).

Leland et al. X  X  classic explanation for self-similarity in network traffic rests on two points: first, network traffic had been viewed as the aggregation of many renewal X  X eward processes ( Mandelbrot, 1969; Taqqu &amp; Levy, 1986 ) whose inter-renewal times exhibit infinite variance (i.e. follow a heavy-tailed distribution). An aggregate of many of these processes is known to 2000 ). However, this account neglects a possible alternative explanation for self-similarity: deterministic chaos . It is well-known that Web traffic (indeed all network traffic) has periodic components that are driven by the time of day, the day of the week, etc.  X  a phenomenon popularly referred to as the  X  X  X eekend effect. X  X  These deterministic dynamics also operate at a range of time scales (albeit tending towards longer scales), and could potentially play a significant or even dominant role in causing Web sessions to appear self-similar. Chaotic maps have been previously proposed as an alternative explanation for self-similarity in network traffic (see for instance Erramilli et al., 1995; Mondragon et al., 2001 ), but have not been examined for Web sessions.

Web session generators are used in much the same way as network traffic generators; that is, they provide a data source for load testing of Web applications. The risk in leaving the possibility of nonlinear determinism unexplored is that simulated session data may in fact diverge from reality much more than is currently expected. Given the traffic and user-population growth described above, it seems plain that any widely-used Web application will face an increasingly challenging load pro-file as time progresses. Thus, load-testing of Web applications will only grow more stringent with time. However, as the fre-quency of Web sessions in a test increases, any discrepancies between the Web session generator and real-world behavior will be magnified. If self-similarity in Web sessions arises from deterministic components, then the traffic models used in load testing are ultimately misleading. Thus, until and unless the alternative of deterministic chaos is investigated, the model for Web session self-similarity proposed in Arlitt and Williamson (1997) and employed in Goseva-Popstojanova et al. (2006a)  X  and thus all session X  X eneration systems based on them  X  will remain debatable. Such models are heavily used during the development of web services to analyze the performance and capacity of various server configurations and soft-ware alternatives (push vs. pull services). These models become even more critical when we consider the next generation of cloud-based infrastructure and services based upon service-orchestrated architecture (SOA) models.

Our goal in this article is to provide a rigorous empirical investigation of the possibility of deterministic chaos in Web session durations. Using the methods of nonlinear time series analysis ( Kantz &amp; Schreiber, 2003 ), we will investigate web server logs from two organizations (one public university, one private company) that cover much longer periods of time than earlier studies of self-similarity in Web sessions. We will first test the supposition that deterministic, rather than stochastic, processes (a necessary precondition to being chaotic in nature) best explain the datasets. We will then attempt to extract a chaotic invariant from these datasets, and thus evaluate this alternative explanation of Web session self-similarity. Finally, we will compare two forecasting algorithms (one deterministic, one stochastic) on these datasets, to determine which ap-proach more accurately models network traffic.

The remainder of this article is organized as follows. In Section 2 , we provide essential background. In Section 3 , we de-scribe the methodology for our data collection and analysis, and we attempt to extract chaotic invariants in Section 4 .We describe our predictive modeling experiments in Section 5 , and close with a summary and discussion of future work in
Section 6 . 2. Background and related work
In the classic study by Leland et al. (1994) , the number of Ethernet packets per time unit on a busy network was plotted for time periods ranging from 100 s to 0.01 s; the resulting time series seemed to have identical characteristics at all length scales, i.e. the traffic was self-similar. Multiple subsequent studies have confirmed this result. This fact has several implica-tions for network engineers. Firstly, source models for network traffic can be very simply constructed using heavy-tailed dis-tributions; indeed, Leland et al. proposed that the reason why network traffic was self-similar was that file sizes (and hence transfer sizes) on the network followed a heavy-tailed distribution. Secondly, common measures of the  X  X  X urstiness X  X  of net-work traffic become heavily dependent on the time scale of the observation if traffic is self-similar, which is a highly unde-sirable characteristic ( Leland et al., 1994 ). However, our concern in this article is Web traffic and Web system workloads, which are strongly influenced by user behaviors. Merely recording the number of bytes sent or received gives little insight into user interaction dynamics, and so a different metric is required. 2.1. The Web session metric Modern Web applications are complex pieces of software that are often mission-critical (e.g. for e-commerce vendors).
Today, browsers must render a huge variety of active content (Javascript code, Flash movies, ActiveX controls, etc.), hosted from a wide variety of software platforms. Users navigate through this tidal wave of information (or at least attempt to) by following links within a website X  X  structure; user navigation patterns, and the activities they engage in while on a site, are a prime focus of the discipline of Web usage mining ( Bordogna &amp; Pasi, 2010; de Campos et al., 2010; Mobasher et al., 2000;
Srivastava et al., 2000; Tseng et al., 2008 ). They are also a key focus of the literature on web systems reliability. Unlike more traditional software systems, the primary workload for Web systems is interactive search and retrieval, rather than compu-tation. Furthermore, the user population is ill-defined; every person on the planet with an Internet connection is a potential user, and so no  X  X  X tandard X  X  user profile exists. Thus, Web systems engineers must use fundamentally different metrics to measure their workloads, and to establish design goals for their systems ( Tian et al., 2004 ).

Some of the metrics proposed for website workload measurement are drawn from the network engineering literature: the number of requests made to the server ( hits ), the number of bytes transferred, and the number of users (actually the number of unique IP addresses) would be familiar to any network engineer. They are measured from the server logs kept by each individual Web server. However, these are not particularly effective in understanding the workload of a Web system: The number of hits means little without knowing what objects were requested.

The number of bytes transferred illustrates bandwidth usage but tells us nothing about the (interactive) operations lead-ing to those transfers.

The  X  X  X umber of users X  X  is misleading because most individual users do not have a static IP address. Usually, IP addresses are dynamically assigned from a limited pool, and so one address may correspond to several users.

A more effective metric (which captures aspects of both traffic volume and user behavior) is the number of Web sessions and their duration ( Tian et al., 2004 ). A Web session is defined as a sequence of actions taken contemporaneously by a par-ticular user at a particular website. The length of a session thus depends on the nature of the users X  interaction with a web-site; for example, a user watching a feature-length movie streamed from a website will have a much longer interaction (and receive a much greater volume of data) than a user who merely checks the current weather on a news site. This metric is unique to Web traffic, because it captures the notion of user behavior, as recorded by the sequence of HTTP requests made to the site by the user.

The Web session is one of the most popular units of measure for website traffic analysis, workload analysis, and user behavior modeling, and has been utilized by numerous researchers ( Arlitt &amp; Jin, 2000; Arlitt &amp; Williamson, 1997; Arlitt et al., 1998; Goseva-Popstojanova et al., 2004, 2006a, 2006b ). Cherkasova and Phaal (1998) proposed the Web session metric, with further discussion in Menasce et al. (1999, 2000) ; note, however, that Web sessions were discussed in an earlier paper by Crovella and Bestavros (1997) . Since that time, the Web session metric has been employed in Web mining studies to cre-ate personalization algorithms ( Eirinaki &amp; Vazirgiannis, 2003; Mobasher et al., 2000 ); studies of search engine utilization ( Jansen &amp; Spink, 2003 ); and was the proposed unit of measure in a load-managing algorithm for improved quality of service ( Cherkasova &amp; Phaal, 2002 ). Sessions offer much finer-grained information than the standard number of users metric, and can be the basis for much more detailed investigations (e.g. studying the number of bytes transferred, errors and types of errors per session ( Goseva-Popstojanova et al., 2006a; Huynh &amp; Miller, 2009 ), the number of requests, the length of the ses-sion, and inter-session arrival times ( Arlitt &amp; Jin, 2000 ).

Despite its popularity, the Web session is a difficult metric to capture. HTTP is a stateless protocol, meaning that no history of actions is maintained between one user request and the next. Web applications that require state information, for in-stance, often use session cookies to simulate a stateful connection (although this workaround introduces security vulnerabil-Time-out Threshold (STT), which defines a period of user inactivity that signals an end to a given session. In recent work,
Goseva-Popstojanova et al. (2006a, 2006b) use STT = 30 min; this is a common value used by other researchers ( Berendt, 2001; Mahoui &amp; Cunningham, 2000; Mat-Hassan &amp; Levene, 2005; Spiliopoulou et al., 2003 ). This value was rounded up from a suggested STT of 25.5 min due to Catledge and Pitkow (1995) , who claimed that the most  X  X  X tatistically significant X  X  events occurred within 1.5 standard deviations (2.5 min) from the mean time between each user event (9.3 min). By contrast,
Huynh and Miller (2009) proposed a probabilistic model to determine the STT from the Web logs of a site, meaning that the STT would be site-dependent. Although this approach is attractive as websites vary greatly, the current study will em-ploy the 30-min threshold for comparability with previous studies. 3. Methodology 3.1. Chaotic invariants
Determining whether or not a given time series is chaotic is a complex process, in which human interpretation plays a significant role. There is no test that determines whether a finite time series arises from a stochastic, or a nonlinear deter-ministic, process. The current literature allows us to conclude that deterministic chaos is present if a chaotic invariant can be measured from the time series and has certain values; such invariants include Lyapunov exponents and the correlation dimension. The correlation dimension has been used to investigate a wide variety of complex systems (e.g. machining pro-( Dick et al., 2007 ). The correlation dimension is currently considered to be the most robust chaotic invariant in any time ser-ies with noise contamination, which perhaps helps explain its popularity. The correlation dimension D is used to determine if technically has a fractal geometry if its topological dimension D computing D H is extraordinarily difficult for an arbitrary set, and so the criterion D The correlation dimension D is an estimate of D H , but is defined as a limit; this means that we cannot directly compare D to D and obtain a valid result for a finite time series. We can, however, investigate the values of D ; the topological dimen-sion is always integer-valued, while D and D H are real-valued. Thus, if D is clearly measured to be a non-integer, we can con-clude that the time series appears to be chaotic ( Kantz &amp; Schreiber, 2003; Yamaguti et al., 1997 ).

We begin our analysis by reconstructing the state space of the process that generated the time series; while we cannot do so uniquely, the delay embedding technique allows us to create an equivalent state space (i.e. the two are related by a smooth, invertible mapping). We would then typically use a noise-reduction technique, as the correlation dimension only tolerates a noise amplitude of 2 X 3% of the signal. We then calculate the correlation sum, given by where N is the number of delay vectors in the time series, e is a neighborhood, H represents the Heaviside step function, and x , x j are delay vectors with i  X  j . The correlation sum counts the number of pairs of delay vectors that are within an e -neigh-borhood of each other. For small values of e and infinite N , C ( e ) / e
With N being finite for any real-world time series, we cannot directly use this definition. Instead, we plot the local slopes of the correlation sum against the neighborhood e on a semi-logarithmic scale for several values of the embedding dimen-sion. If for all embedding dimensions m &gt; m 0 there is region where the curves all clearly plateau at a single value (or more commonly within a tolerance of a value), then that value is the correlation dimension. The extent of this plateau is consid-real-world data is noisy (denying us access to infinitesimal length scales), while data points become too sparse at large length scales for the correlation sum to be valid. Note that the correlation sum can be computed automatically, while the correlation dimension has to be determined through interpretation of this plot ( Kantz &amp; Schreiber, 2003 ).
There are some important caveats to using the correlation dimension. First, it is only intended as a measure of spatial correlation, and can be confounded by temporal correlations (which should occur in any dataset). Thus, data points sepa-rated by less than some threshold length of time should be excluded from the correlation sum; this threshold can be deter-mined from the space X  X ime separation plot. Secondly, the time series is assumed to be stationary; this assumption can also be checked using the space X  X ime separation plot (a stationary time series will exhibit a plateau or level oscillation; a non-stationary one will not have this behavior). Finally, the correlation dimension can only quantify determinism that is known attractor in phase space, but it could seem to in a finite time series. Thus, a test for deterministic behavior is needed. In the literature, the most general alternative that can be tested is that the time series arises from a linear Gaussian process that has been distorted by a nonlinear observation function. This is the null hypothesis in the method of surrogate data , which consists of an elaborate shuffle of the elements of the time series to match the rank statistics of a linear Gaussian process. We gen-erate (2/(1 v )) 1 realizations of a Gaussian process, and create a shuffled dataset matching each one. This gives us a con-fidence level of v (typically 95%). The shuffled datasets and the original are then compared using a test statistic that measures some aspect of determinism; time reversal asymmetry, or a nonlinear prediction error, are typical possibilities.
If the statistic for the original dataset is maximal or minimal, we can reject the null hypothesis; this rank-based approach is necessary as the distribution of the test statistics is unknown ( Kantz &amp; Schreiber, 2003 ). 3.2. Data collection
Server logs from the websites of two separate organizations are investigated in the current paper. The first is the website of the Department of Electrical and Computer Engineering at the University of Alberta the public presence and operation of the Department, but is not mission critical. The website is dynamic, using the ColdFusion scripting language, as well as the Apache HTTP Daemon. Log files covering a period of 11 months (albeit not continuously) were obtained from the system administrators. These logs record approximately 2.42 million hits with 203,896  X  X  X nique X  X  visitors and ied in Goseva-Popstojanova et al. (2006a) ; a high-volume, non-commercial, non-mission-critical website.

The second website studied ( X  X  X ite A X  X ) is operated by a company that specializes in online databases. This is a commercial website that is crucial to the operator X  X  business. The website charges customers for the time used to access its online data-base; hence, an outage directly translates into lost revenue. Users of this website can be characterized as customers intend-ing to purchase a product or register for a training course. The website is one of the company X  X  core revenue streams, and thus clearly qualifies as a mission-critical system. Pages are dynamically generated; technologies deployed in the delivery of this website include the PHP 3 scripting language, MySQL 4 operation (again, not continuously) were obtained. These logs record 1.9 million hits, with 63,500  X  X  X nique X  X  visitors and 33.5 GB reported within the research literature. Table 3.1 provides a summary of the log data used in previous studies, as well as the current study. Note that a heavy-tail analysis of the ECE and Site-A data has been performed in Miller and Huynh (2010) , which found that this model did not fit the data.

Although the websites examined by previous studies have higher traffic density, the periods covered are shorter. The long collection period for our data may provide several benefits over shorter periods:
User behavior can be expected to evolve over time, and thus only an incomplete snapshot of a user X  X  behavior would be captured in a short period. For instance, a new user on a Wiki may simply read articles; once familiar with the site, the user may choose to post comments, provide feedback or even participate in editing articles.

The website also evolves over time. Advertising campaigns or public announcements can create time-dependent changes in website traffic. An example is the 1500% traffic increase GoDaddy.com observed following a Super Bowl ad campaign. collection periods will reveal these patterns for what they are. In addition, major Web events can create short-term traffic spikes; popular YouTube videos are known to generate millions of hits in a short time before the site returns to normal traffic.

Any quality-of-service (QoS) problems on the website can significantly impact short-term traffic. Users will only wait for a matter of seconds before navigating away if QoS is poor. 3.3. Data preparation
The log files are stored in the Common Log Format 6 for the ECE website, and the Combined Log Format log parser was created in Ruby to extract the log data and store it in a relational database. This approach to data collection can be interpreted as a deep log analysis technique ( Mahoui &amp; Cunningham, 2000; Mat-Hassan &amp; Levene, 2005; Mobasher et al., 2000 ). An estimate of session length requires both a start time and an end time; these times are determined from the first and last requests deemed to be a part of the session. We thus remove all sessions that consist of only one request from the data-set. Furthermore, both the ECE and Site-A datasets contain periods of more than a day where no data is available. We cannot simply delete these intervals, as this would cause a spurious high-frequency spike in the time-series data. Instead, we split ECE into four segments (around the three gaps in the data), and split Site-A into two segments around the single gap in that dataset.
The log files contain requests from robots and other automated systems that should be removed as they are not actual requests from web users. Automated systems are classified as systems that repeatedly request a resource from the website after a set period. For example, upon investigation of Site A X  X  logs, requests from two monitoring services are identified. The first service requests a resource from Site A every 30 min while the second service requests a resource from Site A every 66 min. The resources these services request are unique and not publicly available; hence removing them simply involves identifying requests for these resources in the log. Robots that automatically request the  X  X  X obots.txt X  X  resource are also re-moved from both Site A and ECE log files.

We finally construct a time series from the  X  X  X leaned X  X  log data. This time series records the number of sessions active on the website during an interval of time. The length of this interval is 1 min, meaning that our longest time series consists of over 1.3 million observations. The count-per-interval data representation also prevents complications in the data analysis that would arise if the observation intervals in the time series were not uniform (we would have to transform the data to uniform sampling using a Poincar X  section ( Kantz &amp; Schreiber, 2003 ). Algorithms for nonlinear time series analysis always assume that the observations are real-valued; integer-valued data requires special handling, as data points in phase space fall exclusively on a grid (which will appear deterministic, even if the data arise from a purely random process!) Following techniques in Kantz and Schreiber (2003) , we thus add white noise in the interval [ 0.5, 0.5] to each data point. This results
After testing for determinism in the resulting dataset, we then process the dataset with a nonlinear noise reduction tech-nique before attempting to extract chaotic invariants. 4. Data analysis 4.1. Delay embedding
Figs. 4.1 X 4.6 present the number of Web sessions per minute in our datasets. Figs. 4.1 X 4.4 present datasets ECE1 to ECE4, respectively, while Figs. 4.5 and 4.6 present datasets Site A-1 and Site A-2, respectively. One particular concern emerged on analysis of the Site A-2 data: there was a period of anomalous data during which the number of sessions was identical for an extended period. As we believe that this was a data collection problem, we removed the first 300,000 data points from Site A-2; the reduced dataset in Fig. 4.6 covers the period 4/27/2006 to 5/1/2008. The peak session rate for each dataset is presented in Table 4.1 .

To determine the embedding delay and dimension for each dataset, we have used the autocorrelation plot, the mutual information statistic and the technique of false nearest neighbors. For the sake of brevity, we omit the details of this process, noting merely that we follow the approach described in Kantz and Schreiber (2003) . We summarize the delay embeddings we have selected in Table 4.2 . 4.2. Test for deterministic behavior
With the delay embeddings determined for all six datasets, we can now use the method of surrogate data to determine if any of these datasets appear to be predominantly deterministic in nature. We will proceed with noise reduction and extracting nonlinear invariants only from those datasets that pass this test. In all experiments, we aim for a two-sided con-fidence of 95%, and so 39 surrogates are generated each time.

We obtain clear signatures of deterministic behavior in datasets ECE-1 and ECE-2, while we were not able to rule out sto-chastic behavior as an explanation for datasets ECE-3 and ECE-4.

Due to system limitations, we were only able to conduct surrogate data tests on segments of 100,000 time series elements at a time, corresponding to about 2 X  months of data. We have therefore divided the datasets that are larger than this limit into continuous segments of 100,000 elements each. (A small number of data points at the extreme ends of each data set/ segment are also discarded in order to have the first and last elements of the segments closely match; this avoids a spurious high-frequency spike when the surrogate datasets are generated using the Fourier transform ( Kantz &amp; Schreiber, 2003; Press not entirely definitive; in the latter case we can only say that large segments, up to 2 X  months in length, of the data sets show possible deterministic behavior. We will only proceed to extract nonlinear invariants if deterministic behavior is dem-onstrated for all segments.
 4.3. Noise reduction, stationarity, chaotic invariants
Datasets ECE-1, ECE-2 and Site A-1 appear to be generated via deterministic processes. Before attempting to extract cha-otic invariants from these datasets, we need to reduce the noise in these datasets as much as possible. As discussed in Sec-tion 4.2 , the noise that is known to be present in these datasets is discretization noise, which we have alleviated by adding white noise in the interval [ 0.5, 0.5] to each time series, to avoid biasing the results of the surrogate data test. We then employ a locally constant noise-reduction scheme to reduce the noise in the datasets. In this technique, we select a radius e , and find all the neighbors for a given delay vector x within an e -neighborhood of x . Then, x is replaced by the neighborhood mean. The process is repeated for all points in the datasets. The neighborhood radius e is a free parameter, which obviously has a considerable effect on the outcome of noise reduction. Guidance for choosing the radius includes being larger than the apparent noise extent, while still being smaller than the radius of curvature of a  X  X  X ypical X  X  segment of the phase-space attrac-tor. In our work, we can definitely say that the noise radius needs to be greater than 1.0 to cover discretization noise. An upper limit cannot be determined at this point, and so we undertake parameter exploration to find a  X  X  X est X  X  value. This is judged by visual inspection of the phase portrait of the noise-reduced dataset. We want the largest possible structures to work with, while also ensuring that the neighborhood size is not excessive compared to the size and curvature of these structures.

We next check for stationarity in the noise-reduced datasets using the space X  X ime separation plot. Each of these datasets reaches a level oscillation, and can thus be considered stationary. We can avoid temporal correlations in computing the cor-relation dimension if we exclude points closer than 1500 time steps in ECE-1; closer than 500 time steps in ECE-2; and closer than 700 time steps in Site A-1.

Finally, we compute and plot the slopes of the correlation sum for each dataset. In these plots, we are searching for a re-gion of the x -axis where all the curves for embedding dimensions greater than the actual dimensionality of the attractor pla-teau at a common value. We do not observe any common plateaus in these plots. We must therefore conclude that the phase-space attractors of these datasets do not appear to have a fractal geometry, and thus the datasets underlying them do not exhibit chaotic behavior. 5. Predictive modeling experiments
Section 4 described evidence that some of our datasets appear to be predominantly deterministic, using the method of surrogate data. In this section, we will conduct a parallel test of this hypothesis, using predictive models based on the two explanations for self-similarity (heavy-tailed distributions vs. nonlinear determinism) through a predictive modeling task. If our hypothesis is correct (Web session data is predominantly deterministic), then a deterministic, nonlinear prediction algorithm should be significantly more accurate compared to a stochastic model. Likewise, if the hypothesis is incorrect, then the stochastic model should be significantly more accurate. 5.1. Prediction models
The two models we propose to compare are: for the deterministic model, radial basis function networks (RBF networks); and for the stochastic model, fractional auto-regressive integrated moving average (FARIMA, or sometimes ARFIMA) models.
RBF networks are a well-known and widely-used neural network architecture, which are known to be universal approxima-tors ( Haykin, 2009 ). They have been frequently used as time-series prediction algorithms (see e.g. Chng et al., 1996; Harp-ham &amp; Dawson, 2006; Lian et al., 2008 ); an RBF network is depicted in Fig. 5.1 . It is a layered feedforward network in which the first layer neurons merely distribute the inputs to the next layer, while the second layer implements a set of radial-basis functions ( Haykin, 2009 ): neuron. The third layer neurons output a weighted sum of their inputs. Training this network consists of determining the centers of the layer-2 radial basis functions (via k -means clustering) and then optimizing the layer-3 weight vector (via least-squares regression). The number of RBF neurons, and the maximum spreads of the radial basis functions, are input parameters to the training algorithm (we use the implementation in the WEKA software package).
 For the stochastic alternative, we seek a model that closely matches the characteristics of the datasets (i.e., self-similarity).
As self-similarity in network traffic is usually measured by the Hurst parameter, the class of Fractional ARIMA models are a natural choice. Recall that an ARIMA model is a generalization of the auto-regressive moving average (ARMA) model, given by: where x t is the observed value of the time series at time t , e vation lag, h j is the coefficient of the j th prediction lag, and p and q are the model orders (this is usually denoted as an AR-
MA( p , q ) model). ARMA models are only appropriate for stationary time series; ARIMA models are a generalization that accomodates a non-stationary mean. The time series is differenced as: orino, 2006 ).

In a FARIMA model, the difference parameter d can be a non-integer, and will be equal to H 0.5 where H is the Hurst parameter. FARIMA models have been used for a variety of datasets exhibiting long-range dependencies (a known property of heavy-tailed distributions); and in particular have been found to be effective models for self-similar network traffic ( Gorst-Rasmussen et al., 2012; Ilow, 2000; Liu et al., 1999 ). Based on these characteristics, we select FARIMA models as our stochastic alternative; we will use Hyndman X  X   X  X  X orecast X  X  package for R ( Hyndman &amp; Khandakar, 2008 ) to carry out our FARIMA experiments. 5.2. Methodology
Our experiments follow a common design from time-series forecasting: the dataset is split into an estimation or  X  X  X raining X  X  set, and a holdout sample that is chronologically later than all data points in the training set (i.e. a chronologically-ordered single-split design). The models will be trained for the basic one-step-ahead prediction task (given a history of the dataset, predict the next observation). The models will be parameterized on the training set, and then their out-of-sample prediction accuracy will be compared on the holdout sample. In all of our experiments, we reserve 1/3rd of the dataset in the holdout sample.

Applying RBF networks to time series forecasting requires some preprocessing; an RBF network is designed to learn a sta-tic mapping from an input  X  X  X eature X  X  space to an output space, rather than a time series. However, the method of delay embeddings reviewed in Section 3 provides an elegant method for converting time-series data into a set of feature vectors; we then need only append the next observation to this vector (as our  X  X  X utput X  X ), and the one-step-ahead prediction can now be determined by an RBF network (or indeed, any other function X  X pproximation algorithm). In addition, the time dependen-cies between observations in the time series have now been subsumed in the delay vectors; this means that we can random-ize the presentation of delay vectors to the network, allowing for a cross-validation design in our training.
Our approach for training the RBF network is therefore as follows. We first construct a delay embedding of each of the training set and holdout sample time series, using the dimensions and delays determined in Table 4.2 . We conduct a param-eter exploration using tenfold cross-validation on the training set. We randomly split the training set into ten partitions, combining nine into a smaller  X  X  X arameterization X  X  set and holding the remaining one out as a validation set. The RBF network is then trained on the parameterization set with a given parameter vector and its out-of-sample accuracy determined on the validation set. We repeat this process ten times, with each partition used as the validation set exactly once. We repeat this process for different parameter vectors, performing a grid search of the parameter space (in this case, the number of radial basis functions, and their spreads). We select the parameterization with the lowest mean RMS error across the ten validation partitions as the  X  X  X est X  X  parameterization on the dataset. We then train a new RBF network on the entire training set using this  X  X  X est X  X  parameterization, and finally determine its out-of-sample accuracy on the holdout sample.

For the FARIMA model, we use Hyndman X  X  forecast package in the R environment to determine the model order (the parameters p , q , and d ) on the training set. However, we are not able to compute the out-of-sample forecast directly, due to a memory leak in forecasting FARIMA models. Instead, we first obtain a model from the  X  X rfima X  procedure on the training set. We then performed fractional differencing (using the  X  X racdiff X  routine) on the training and holdout sets using the d deter-mined from  X  X rfima. X  We then trained an ARMA model (using the model orders p and q determined from ARFIMA on the training set) on the differenced training set, and tested it (using the  X  X  X orecast X  X  package) on the differenced holdout set. 5.3. Experimental results
In Table 5.1 , we present the root-mean-square (RMS) prediction errors for both models on our six holdout samples. For the ECE datasets, our results are plainly mixed. The RBF network and FARMIA are very close on ECE-1 and ECE-2, but RBF networks are substantially better on ECE-3, and FARIMA seems substantially better on ECE-4. On the Site-A data, the RBF network and FARIMA models are very close. Overall, we cannot favor one model or the other across the six datasets. We therefore find that our hypothesis (the RBF Network would be substantially more accurate than FARIMA) is not supported.
While it is interesting that a purely deterministic model performs as well as a FARIMA model (and supports our earlier find-ing that deterministic components play an important role in these datasets), in the end we do not find sufficient evidence to overturn the existing consensus that Web sessions are best modeled as stochastic, heavy-tailed processes.

We can also say that the overall prediction quality for both models was good. The relative absolute error for the RBF net-works (the ratio of the absolute error of the model vs. the absolute error obtained by simply predicting the mean value of the time series) was under 10% for all datasets, meaning an order-of-magnitude improvement over a na X ve model. 6. Conclusions
Our goal in this article was to provide a rigorous empirical investigation of the possibility of deterministic chaos in Web session durations. We collected web access logs from two separate organizations: one is a department at a very large public university; the other is a private company. The university website is important to the operation of the department, but is not mission-critical; the corporate website, by contrast provides one of the core revenue streams for the company, and is thus a mission-critical website. From these logs, we extract a total of six datasets, each covering one continuous period of operation (none of which are less than 1 month). Each dataset is a time series that records the number of open Web sessions in 1-min intervals. We then employ the methods of nonlinear time series analysis to test for the presence of chaotic behavior in these datasets. In three of the datasets (two from the university web site, one from the corporate site), we determined that the deterministic components of the signal (i.e. the well-known weekend effect ) appear to dominate the stochastic components of the signal. However, a subsequent check for the presence of deterministic chaos was negative; we were not able to extract a correlation dimension for any of these datasets. We then contrasted deterministic and stochastic models in a time-series prediction experiment on all six datasets, finding that the two models performed very similarly. Ultimately, we found that there was insufficient evidence to favor deterministic models over the existing model of heavy-tailed processes. In future work, we plan to replicate our predictive modeling study using web access logs from a broader array of websites.
We first want to check if RBF networks and FARIMA models are still equally effective across a broad range of websites. If they are, we will seek to understand why two such radically different models would have such similar performance on these data-sets. One intriguing possibility is that the mix of stochastic and deterministic components might be nearly  X  X  X alanced; X  X  if so,
Web session models for load testing should incorporate this characteristic (perhaps as a fine-grained hybridization of RBF networks and heavy-tailed processes). We will also investigate intelligent algorithms for determining the length of a session.
The use of a simple timeout threshold for capturing such a complex idea as user behavior is almost certainly less than opti-mal, and this will directly impact the accuracy of the Web session metric.
 Acknowledgements A, X  X  for providing access to their web server logs. This research was supported in part by the Natural Sciences and Engineering Research Council of Canada under Grant Nos. G121210906 and G121210670.
 References
