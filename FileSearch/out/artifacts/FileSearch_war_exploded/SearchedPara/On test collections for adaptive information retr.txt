 1. Introduction
Adaptive information retrieval (AIR) is defined as a search process that adapts toward the user X  X  needs and context. The goal of AIR research is to develop retrieval technology that can predict what information a searcher will need and decide how and when to present that information to the user.

Unfortunately, the current method by which much retrieval technology is developed, the Cranfield para-digm, provides little support for AIR research. In the Cranfield paradigm system developers use retrieval results from test collections to compare alternative system designs. A test collection is an abstraction of a retrieval task that provides more control over experimental variables at the cost of less realism. The abstract task is considered to be a necessary, though not sufficient, proxy for one or more real user tasks; Sparck Jones (2001) calls such an abstraction a core competency. The Cranfield test collection abstraction was conceived specifically as a means to perform retrieval experiments that were  X  X  X reed as far as possible from the contam-ination of operational variables  X  (Cleverdon, 1991 )  X  including the particular user. As such, Cranfield test collections are not a good match for AIR goals. But since the Cranfield methodology has been instrumental in advancing the state of the art in retrieval effectiveness ( Buckley &amp; Voorhees, 2005 ), it is worth considering how the paradigm might be changed to better support AIR research.

This paper examines the prospects of an  X  X IR test collection X . I first presented the ideas in this paper in a talk at the Adaptive Information Retrieval Workshop held at the University of Glasgow on October 14, 2006. As manager of the Text REtrieval Conference (TREC) 1 project for many years, I am known as a strong advocate of the Cranfield tradition. Workshop organizers invited me to speak on  X  X  X est collection building for the eval-uation of adaptive systems  X  . Initially, I viewed the theme as an oxymoron: Cranfield test collections are by def-inition static structures designed to standardize the comparison of ranked retrieval results. Turning the question around seemed more fruitful. Given the need to evaluate adaptive systems, what new abstraction retains as many of the benefits of Cranfield test collections as possible while representing the salient aspects of adaptation?
I do not have an answer to that question. It is very much an open research problem. But reflecting on it brought me to two conclusions. First, all the evidence to date suggests that even tiny extensions to Cranfield col-lections imply either a huge increase in cost or a huge loss of power for retrieval experiments. Second, there is no consensus (or data) as to what constitutes a core competency for AIR. I argue that since the increased cost and decreased power of experiments are proportional to the complexity of the task being evaluated, building effective
AIR test collections will critically depend on identifying just those factors that represent the essence of adaptive retrieval behavior, and explicitly not attempting to model all possible factors that can affect retrieval behavior.
The paper is organized as follows. The next section provides background on the Cranfield methodology as currently practiced. Section 3 demonstrates the centrality of the user effect in IR experimentation, even within the Cranfield paradigm. The current means of incorporating user behavior into retrieval experiments is to per-form user studies or interactive experiments, and the cost of these studies is the primary motivation for an AIR test collection abstraction. Section 4 looks at two existing efforts to make interactive experiments less expen-sive. The final section summarizes the lessons existing practice in retrieval experimentation has for AIR. 2. The Cranfield paradigm
A Cranfield test collection is a set of documents, a set of information need statements (called  X  X  X opics  X  in the remainder of this paper), and a set of relevance judgments that list which documents should be returned for which topic. In the simplest and most common case, relevance judgments are binary, either a document is rel-evant to the topic or it is not, and are based on topicality, a document is relevant if its subject matter matches that of the topic.

In current practice, a researcher runs a retrieval system on a test collection to produce a ranked list of doc-uments in response to each topic (a  X  X  X un  X  ). A ranking reflects the system X  X  idea of which documents are likely to be relevant to the topic; documents it believes more likely to be relevant are ranked ahead of documents it believes are less likely to be relevant. Using the relevance judgments, various evaluation metrics can be com-puted for the ranked list, each of which reflects some aspect of the goal of ranking all relevant documents ahead of all nonrelevant documents. Scores for individual topics are averaged over the set of topics in the test collection. Average scores for one run are compared to other runs using the exact same test collection. Retrie-val systems producing runs with better scores are considered more effective retrieval systems.

A Cranfield test collection is a highly abstracted surrogate for real user retrieval tasks that has been con-troversial since it was first introduced ( Taube, 1965; Cuadra &amp; Katter, 1967; Harter, 1996 ). The main issue critics have had with the methodology has been with  X  X elevance X . First, being on-topic does not necessarily mean that a document is useful; perhaps the user already knows the information in that document, or does not believe the source is credible. Second, relevance, however defined, is clearly not binary; some documents are more apropos than others. Third, relevance, however defined, is highly subjective ( Schamber, 1994 ); dif-ferent users will have legitimate differences of opinion as to whether some documents are relevant to the topic.
There are also very many factors that affect the user X  X  experience in a real task that are not addressed at all by the methodology. It completely ignores the user interface. Search efficiency is not accounted for. It provides no representation of the user X  X  prior knowledge or motivation for searching. The list can go on and on.
While all of these criticisms are true, they do not negate the fact that the Cranfield methodology has been extraordinarily useful. The Cranfield methodology captures fundamental functionality that would seem to be required by all retrieval systems: if a retrieval method cannot at least rank relevant documents before nonrel-evant documents (for some reasonable definition of relevance), it is hard to imagine how it can be successful at any real user task. By eliminating anything that does not directly contribute to this core competency, Cranfield test collections lose realism but gain experimental power since there are fewer variables to control. The collec-tions are thus convenient laboratory tools that allow system developers to weed out less successful retrieval approaches early and inexpensively, saving more costly testing of more realistic functionality for only the most promising approaches. Experience has shown that technology developed using test collections does transfer to real tasks. Basic components of current web search engines and other commercial retrieval systems, such as full text indexing, term weighting schemes, and relevance feedback, were first developed using test collections.
Nonetheless, Cranfield test collections are too abstract to be a useful model of the adaptive retrieval task which needs a more nuanced representation of the user. The challenge of creating an AIR test collection, therefore, is defining an abstraction that maintains the benefits of Cranfield collections  X  experimental power, relatively low cost, generalizability of results, broad applicability  X  while including sufficient detail to study adaptation. But this is far easier said than done. As described in the next section, users are the dominant source of variability in retrieval experiments even within the Cranfield paradigm. Changing to include more of a user focus can only increase the variability. 3. User effect and variability in IR experiments
Variability and experimental power (i.e., the ability to draw valid conclusions) are inversely related in any experimental design. An experimental design that provides a given level of power will be increasingly expensive as variability increases since more factors need to be controlled for.

Despite the fact that a Cranfield test collection is a stark abstraction of search behavior that represents a user simply as the combination of a topic statement and corresponding set of static relevance judgments, the user is the single biggest variable of retrieval system effectiveness in Cranfield-style experiments. An analysis of variance model fitted to the TREC-3 results demonstrated that the topic (i.e., the representation of the user) and system effects, as well as the interaction between topic and system, were all highly significant, but the topic effect was the largest ( Banks, Over, &amp; Zhang, 1999 ).

Indeed, the large variability in topic performance is precisely why the Cranfield paradigm uses average scores over a set of topics in comparisons. Seasoned experimenters have long known that the topic set needs to be relatively large. For example, ( Sparck Jones &amp; van Rijsbergen, 1976 ) suggested 75 topics as a minimum number of topics for an  X  X deal X  test collection. This intuition arose from observing the behavior of individual topics as demonstrated in Fig. 1. The figure shows an interpolated precision-recall graph for an example
TREC run. The heavy solid line is the average recall-precision curve over the 50 topics in the test set, while the dotted lines are the curves for 15 individual topics within the test set.
 Since the cost of building a test collection is proportional to the number of topics contained in it, Chris Buckley and I used the archive of TREC results to investigate how many topics are sufficient for a reliable
Cranfield test collection. In particular, we studied how often a single Cranfield-style comparison of two runs would incorrectly indicate that one run was better than the other using different topic set sizes and different measures ( Buckley &amp; Voorhees, 2000; Voorhees &amp; Buckley, 2002 ). The first investigation showed that some evaluation measures are more stable than others; specifically, that mean average precision (MAP) is (quite a bit) more stable than precision at a given cut-off. 2 In the other study we established an empirical relationship between the number of topics in a test set, the size of the difference in retrieval scores used to decide two runs are different ( d ), and the error rate. For all measures looked at, the error rate decreased when either the topic set size or d increased (though a larger d implies a corresponding loss of sensitivity to detect differences that do exist). For the most stable measures and for d  X  X  of the size commonly observed in the retrieval literature, 25 topics was clearly too few to have confidence in the conclusion, and even 50 topics was somewhat iffy (a d of between 0.03 and 0.04 in MAP scores, equivalent to about a 10% relative difference in MAP scores for current systems, had an error rate of about 11%). Less stable measures had higher error rates for equal topic set sizes.
Sakai (2006) later reached similar conclusions using the NTCIR archive and more theoretically well-founded mathematical underpinnings to the computation of the error rates.

These are sobering conclusions for the prospects of building an AIR test collection. They say that even in the most controlled environment  X  the Cranfield task and using MAP as the measure  X  you still need on the order of 50 topics to control for user variability sufficiently to have confidence in the conclusion. Modifications as small as moving from MAP to a more user-focused measure such as precision at ten documents retrieved require larger topic sets for a similar level of confidence. More radical departures are sure to require even lar-ger topic sets and corresponding expense, as has already been observed in interactive (user-in-the-loop) stud-ies. Robertson (1990) calculates hundreds of topics per user would be needed to obtain significance in non-matched-pair tests. And while Hersh and Turpin conclude that relevance ranking in the manner of Cranfield is not a trustworthy means to find differences in systems that matter to users, I contend their results are actu-ally a prime example of the necessity of using many topics to gain sufficient power to distinguish among sys-tems in interactive experiments ( Hersh et al., 2000; Turpin &amp; Hersh, 2001 ). 4. Reducing the cost of interactive experiments
The desire for an AIR test collection abstraction is motivated by the expense required to perform interac-tive studies, the current means for incorporating user behavior in retrieval experiments. If building an AIR test collection is expensive relative to a Cranfield collection, but its use allows valid conclusion to be drawn at a cost lower than an interactive study, the construction will be well worthwhile.
Interactive studies are expensive for a variety of reasons. There is a large start-up cost. Regardless of what particular functionality is the actual object of interest, a good interactive experiment requires complete systems that support the functionality in its best light for all alternatives. Large numbers of topics are required to reach statistically significant conclusions. Large numbers of topics imply large numbers of subjects and sophisticated experimental designs to balance subjects across conditions (any given subject can search a given topic no more than once). The more specific the design is to a given operational setting (i.e., the more  X  X  X eal  X  the experiment) the less generalizable the findings are to other environments so the more cases that need to be examined. Tague-Sutcliffe (1992) lists a myriad of factors that must be considered in designing retrieval experiments.
There have been attempts at mitigating the costs of interactive experiments. The TREC-6 interactive track used an experimental design especially constructed to test the viability of performing cross-site comparisons by comparing to a common baseline system ( Lagergren &amp; Over, 1998 ). In the design, each participating site ran the experiment comparing their system to a common baseline system. The assumption was that this set-up would allow the effect of performing all n 2 comparisons among the n participating sites for a total cost of only n comparisons (one at each site) since the common system would control for inter-site variance. Unfortu-nately, subsequent analysis of the results did not support the assumption. Further, the design incurred its own costs: participating sites had to obtain, install, and support the common system; and precious human sub-jects had to be devoted to the common system, a system that was incidental to the main purpose of the exper-iment being run at that site. As a result of these limitations, the design could not be recommended.
The TREC HARD ( X  X  X igh Accuracy Retrieval from Documents  X  ) track was another explicit attempt at defining a light-weight interactive task. In fact, the HARD track could have been called the AIR track. Its overall goal was to improve ad hoc retrieval by customizing the search to the particular user. The motivation for the track was that current retrieval systems return results for the  X  X  X verage  X  user and this necessarily limits their effectiveness for the particular user. The task in the track was an ad hoc document ranking task but with extra information available at search time.

The TREC 2004 instantiation of the track ( Allan, 2005 ) provided two different sources of additional infor-mation, metadata supplied in the topic statement and information collected from so-called clarification forms.
There were five categories of metadata: the (putative) user X  X  familiarity with the subject area specified as either little or much ; the genre of the documents sought specified as either news-report , opinion-editorial , other ,or any ; the geographical location of the source of the document ( US , non-US ,or any ); a short free-text descrip-tion of the subject domain; and text from related documents if available. Clarification forms were HTML-based forms that contained a task for the user (in this case, the TREC assessor) to perform. Track participants were free to implement any task they thought would help retrieval in the form, subject to the constraints that the assessor would spend no more than three minutes per topic responding to a form and the form had to be completely self-contained. Examples of clarifying forms were asking the user to resolve the senses of ambig-uous topic words or obtaining relevance judgments on terms or document snippets.
The track protocol required participants to perform baseline runs using only the traditional topic state-ment. If desired, they could then submit clarification forms to receive the results of the assessor using the forms. Finally they performed additional (non-baseline) runs using the metadata from the extended topic statements and/or the clarification form results. This protocol allowed direct comparison between standard and extended information runs for a single participant as well as comparisons among participants. Fig. 2 gives a precision-recall graph of the top-performing TREC 2004 HARD track runs including one baseline and one additional run per top group. A pair of runs plotted with the same symbol are a pair submitted by a single group. Baseline runs are plotted using a dotted line, and additional runs are plotted using a solid line.
In general (though not invariably) the additional information did improve retrieval effectiveness over the corresponding baseline run. But understanding what factors actually contributed to the improvement is diffi-cult. There are too few topics in a single metadata category (for example, familiarity) to draw reliable conclu-sions regarding category-specific retrieval techniques. Use of clarification form data by researchers other than the original submitters is complicated by needing to understand the specifics of the forms and the inability to repeat the experiment with slight variations. Little reuse means the costs of creating the resource is not lever-aged over the wider community. Since the construction of the HARD collection was already more expensive than standard Cranfield collections (the metadata categories needed to be decided on and then topics created that populated those categories; researchers needed to create clarification forms and assessors had to fill them out), the lack of reusability is a double penalty. 5. Current lessons for future collections
The development of retrieval tools that can adapt to users and their current context is an important chal-lenge for the retrieval community. Successful pursuit of this goal depends on the availability of reliable mech-anisms to compare the relative quality of different retrieval approaches. And the goal will be obtained significantly faster if such comparison mechanisms are convenient to use and relatively inexpensive. Cranfield test collections have traditionally filled this role for the retrieval community, to good effect. But by design
Cranfield test collections represent too little of the user to support adaptive information retrieval research, so a new tool, an  X  X IR test collection X , is needed.

While the form an AIR test collection should take is an open problem, the Cranfield tradition can provide useful guidance. The Cranfield paradigm is successful because of its carefully calibrated level of abstraction.
The document ranking task has sufficient fidelity to real user tasks to be informative, but is sufficiently abstract to be broadly applicable, feasible to implement, and comparatively inexpensive. AIR collections need a new but also carefully selected abstract task. The past few years have seen a variety of workshops focused on topics broadly related to AIR such as the Information Retrieval in Context (IRiX) workshops ( Ingwersen, Ruthven, &amp; Belkin, 2007 ), and the workshop on Evaluating Exploratory Search Systems ( White, Muresan, &amp; Marchio-nini, 2006 ). The literature associated with these and similar efforts contain a wealth of information about fac-tors that can affect a user X  X  search behavior. It is important to appreciate the range of factors that affect search behavior, but it will also be important to not include all such factors in an AIR test collection abstraction. An essential first step in creating AIR test collections will be the definition of the truly distinguishing features of AIR and the development of a protocol that captures just those features.

To the best of my knowledge, the TREC HARD track (which I can take no credit for) was the first explicit attempt to create the equivalent of an AIR test collection. It was a good effort, and parts of the track X  X  design, such as clarification forms, continue to be used. But it also illustrates the prime challenge that needs to be faced in building AIR test collections: the number of topics needed to control for user variability to reach reli-able conclusions is daunting.

Throughout this paper my description of the Cranfield methodology has implied that a comparison on a single test collection is sufficient. But that is not so. Since any test collection is going to have its own peculiar-ities based on the specific documents and topics used, researchers (and paper reviewers) require that a candi-date retrieval technique be consistently better than the alternatives on a variety of test collections before concluding the effect is real. Turned the other way, the fact that diverse techniques that improve retrieval effec-tiveness in operational settings all consistently work well on a variety of test collections is one validation of the
Cranfield abstraction itself. Similar behavior on future AIR abstractions will be one way to validate them.
One final lesson can be learned from the existing tradition of experimentation in IR. Results from user stud-ies in operational setting are difficult to generalize in large part because the studies are subject to biases that are difficult to assess or control. Bias in Cranfield test collections, as can arise when relevance judgments are skewed in some way (such as the way judgment pools are built ( Buckley, Dimmick, Soboroff, &amp; Voorhees, 2006 )), can make comparisons invalid. Since AIR protocols are likely to be subject to many of the biases found in operational settings, bias effects will need to be carefully considered for those protocols. Acknowledgement
I am thankful to the organizers of the Glasgow Adaptive Information Retrieval Workshop for inviting me to speak and thus encouraging me to think about these issues. Chris Buckley read an early draft of the paper and made helpful suggestions to improve it.
 References
