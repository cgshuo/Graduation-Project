 Many data are modeled as tensors, or multi dimensional arrays. Examples include the predicates (subject, verb, object) in knowl-edge bases, hyperlinks and anchor texts in the Web graphs, sen-sor streams (time, location, and type), social networks over time, and DBLP conference-author-keyword relations. Tensor decompo-sition is an important data mining tool with various applications including clustering, trend detection, and anomaly detection. How-ever, current tensor decomposition algorithms are not scalable for large tensors with billions of sizes and hundreds millions of nonze-ros: the largest tensor in the literature remains thousands of sizes and hundreds thousands of nonzeros.

Consider a knowledge base tensor consisting of about 26 mil-lion noun-phrases. The intermediate data explosion problem, as-sociated with naive implementations of tensor decomposition algo-rithms, would require the materialization and the storage of a ma-trix whose largest dimension would be  X  7 10 14 ; this amounts to  X  10 Petabytes, or equivalently a few data centers worth of storage, thereby rendering the tensor analysis of this knowledge base, in the naive way, practically impossible. In this paper, we propose G
T ENSOR , a scalable distributed algorithm for large scale tensor decomposition. G IGA T ENSOR exploits the sparseness of the real world tensors, and avoids the intermediate data explosion problem by carefully redesigning the tensor decomposition algorithm.
Extensive experiments show that our proposed G IGA T ENSOR solves 100  X  bigger problems than existing methods. Furthermore, we employ G IGA T ENSOR in order to analyze a very large real world, knowledge base tensor and present our astounding findings which include discovery of potential synonyms among millions of noun-phrases (e.g. the noun  X  X ollutant X  and the noun-phrase  X  X reenhouse gases X ).
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Design, Experimentation Tensor, Distributed Computing, Big Data, MapReduce, Hadoop
Tensors, or multi-dimensional arrays appear in numerous appli-cations: predicates (subject, verb, object) in knowledge bases [9], hyperlinks and anchor texts in the Web graphs [20], sensor streams (time, location, and type) [30], and DBLP conference-author-keyword relations [22], to name a few. Analysis of multi-dimensional arrays by tensor decompositions, as shown in Figure 1, is a basis for many interesting applications including clustering, trend detection, anomaly detection [22], correlation analysis [30], network forensic [25], and latent concept discovery [20].
There exist two, widely used, toolboxes that handle tensors and tensor decompositions: the Tensor Toolbox for Matlab [6], and the N-way Toolbox for Matlab [3]. Both toolboxes are considered the state of the art; especially, the Tensor Toolbox is probably the fastest existing implementation of tensor decompositions for sparse tensors (having attracted best paper awards, e.g. see [22]). How-ever, the toolboxes have critical restrictions: 1) they operate strictly on data that can fit in the main memory, and 2) their scalability is limited by the scalability of Matlab. In [4, 22], efficient ways of computing tensor decompositions, when the tensor is very sparse, are introduced and are implemented in the Tensor Toolbox. How-ever, these methods still operate in main memory and therefore cannot scale to Gigabytes or Terabytes of tensor data. The need for large scale tensor computations is ever increasing, and there is a huge gap that needs to be filled. In Table 1, we present an indicative sample of tensor sizes that have been analyzed so far; we can see that these sizes are nowhere near as adequate as needed, in order to satisfy current real data needs, which call for tensors with billions of sizes and hundreds of millions of nonzero elements.
In this paper, we propose G IGA T ENSOR , a scalable distributed algorithm for large scale tensor decomposition. G IGA T ENSOR handle Tera-scale tensors using the M AP R EDUCE [11] framework, and more specifically its open source implementation, H ADOOP [1]. To the best of our knowledge, this paper is the first approach of deploying tensor decompositions in the M AP R EDUCE framework. The main contributions of this paper are the following. subjects Figure 1: P ARAFAC decomposition of three-way tensor as sum of R outer products (rank-one tensors), reminiscing of the rank-R SVD of a matrix (top), and as product of matrices A , B , C , and a super-diagonal core tensor G (bottom).
 G I GA T ENSOR ( This work ) 26 M  X  26 M  X  48 M 144 M Table 1: Indicati ve sizes of tensors analyzed in the data mining literature. Our proposed G IGA T ENSOR analyzes tensors with  X  1000  X  larger sizes and  X  500  X  larger nonzero elements.
The rest of paper is organized as follows. Section 2 presents the preliminaries of the tensor decomposition. Sections 3 describes our proposed algorithm for large scale tensor analysis. Section 4 presents the experimental results. After reviewing related works in Section 5, we conclude in Section 6.
In this section, we describe the preliminaries on the tensor decomposition whose fast algorithm will be proposed in Sec-tion 3. Table 3 lists the symbols used in this paper. For vector/matrix/tensor indexing, we use the Matlab-like notation: A ( i, j ) denotes the ( i, j ) -th element of matrix A , whereas A (: , j ) spans the j -th column of that matrix.
 Matrices and the bilinear decomposition. Let X be an I  X  J matrix. The rank of X is the minimum number of rank one matrices that are required to compose X . A rank one matrix is simply an outer product of two vectors, say ab T , where a and b are vectors. (Given) (Discovered)
Noun Phrase Potential Contextual Synonyms pollutants dioxin, sulfur dioxide, disabilities infections, dizziness, injuries, diseases, vodafone verizon, comcast Christian history European history, American history, disbelief dismay, disgust, astonishment cyberpunk online-gaming soul body Table 2: (Left:) Given noun-phrases; (right:) their potential con-textual synonyms (i.e., terms with similar roles). They were au-tomatically discovered using tensor decomposition of the NELL-1 knowledge base dataset (see Section 4 for details).
 Symbol Definition k M k F Frobenius norm of M bin ( M ) function that converts non-zero elements of M to 1 then we can write which is called the bilinear decomposition of X . The bilinear decomposition is compactly written as X = AB T , where the columns of A and B are a r and b r , respectively, for 1  X  r  X  R . Usually, one may truncate this decomposition for R  X  rank ( X ) , in which case we have a low rank approximation of X .
 One of the most popular matrix decompositions is the Singular Value Decomposition (SVD): where U , V are unitary I  X  I and J  X  J matrices, respectively, and  X  is a rectangular diagonal matrix, containing the (non-negative) singular values of X . If we pick A = U X  and B = V , and pick R &lt; rank ( X ) , then we obtain the optimal low rank approximation of X in the least squares sense [13]. The SVD is also a very power-ful tool used in computing the so called Moore-Penrose pseudoin-verse [28], which lies in the heart of the PARAFAC tensor decom-position which we will describe soon. The Moore-Penrose pseu-doinverse X  X  of X is given by We now provide a brief introduction to tensors and the PARAFAC decomposition. For a more detailed treatment of the subject, we refer the interested reader to [21].
 Introduction to PARAFAC. Consider a three way tensor X of di-mensions I  X  J  X  K ; for the purposes of this initial analysis, we restrict ourselves to the study of three way tensors. The generaliza-tion to higher ways is trivial, provided that a robust implementation for three way decompositions exists.
 way outer product of vectors a , b , c is defined as PARAFAC [14, 8] (also known as CP or trilinear) tensor decomposition of X in R components is
The PARAFAC decomposition is a generalization of the matrix bilinear decomposition in three and higher ways. More compactly, we can write the PARAFAC decomposition as a triplet of matrices A , B , and C , i.e. the r -th column of which contains a respectively.

Furthermore, one may normalize each column of the three factor matrices, and introduce a scalar term  X  r , one for each rank-one factor of the decomposition (comprising a R  X  1 vector  X  ), which forces the factor vectors to be of unit norm. Hence, the PARAFAC model we are computing is: We may unfold/matricize the tensor X in the following three ways: X ( 1 ) of size ( I  X  JK ) , X ( 2 ) of size ( J  X  IK ) and X ( K  X  IJ ) . The tensor X and the matricizations are mapped in the following way.
 We now set off to introduce some notions that play a key role in the computation of the PARAFAC decomposition.
 product of A and B is:
If A is of size I 1  X  J 1 and B of size I 2  X  J 2 , then A  X  B is of size I 1 I 2  X  J 1 J 2 .
 product (or column-wise Kronecker product) ( A  X  B ) , where A , B have the same number of columns, say R , is defined as: If A is of size I  X  R and B is of size J  X  R then ( A  X  B ) is of size IJ  X  R .
 The Alternating Least Squares Algorithm for PARAFAC. The most popular algorithm for fitting the PARAFAC decomposition is the Alternating Least Squares (ALS). The ALS algorithm consists of three steps, each one being a conditional update of one of the three factor matrices, given the other two. The version of the algo-rithm we are using is the one outlined in Algorithm 1; for a detailed overview of the ALS algorithm, see [21, 14, 8].

Algorithm 1 : Alternating Least Squares for the PARAFAC de-composition.
 Input: T ensor X  X  R I  X  J  X  K , rank R , maximum iterations T . Output: PARAFAC decomposition  X   X  R R  X  1 , A  X  R I  X  R , 10: break for loop; 11: end if 12: end for 13: return  X  , A , B , C ;
The stopping criterion for Algorithm 1 is either one of the fol-lo wing: 1) the maximum number of iterations is reached, or 2) the cost of the model for two consecutive iterations stops changing sig-nificantly (i.e. the difference between the two costs is within a small number  X  , usually in the order of 10  X  6 ). The cost of the model is simply the least squares cost.

The most important issue pertaining to the scalability of Algo-rithm 1 is the  X  X ntermediate data explosion X  problem. During the life of Algorithm 1, a naive implementation thereof will have to ma-terialize matrices ( C  X  B ) , ( C  X  A ) , and ( B  X  A ) , which are very large in sizes.
 problem of having to materialize ( C  X  B ) , ( C  X  A ) , ( B  X  A ) is defined as the intermediate data explosion.
 In order to give an idea of how devastating this intermediate data explosion problem is, consider the NELL-1 knowledge base dataset, described in Section 4, that we are using in this work; this dataset consists of about 26 10 6 noun-phrases (and for a moment, ignore the number of the  X  X ontext" phrases, which account for the third mode). Then, one of the intermediate matrices will have an explosive dimension of  X  7 10 14 , or equivalently a few data cen-ters worth of storage, rendering any practical way of materializing and storing it, virtually impossible.

In [4], Bader et al. introduce a way to alleviate the above prob-lem, when the tensor is represented in a sparse form, in Matlab. This approach is however, as we mentioned earlier, bound by the memory limitations of Matlab. In Section 3, we describe our pro-posed method which effectively tackles intermediate data explo-sion, especially for sparse tensors, and is able to scale to very large tensors, because it operates on a distributed system. In this section, we describe G IGA T ENSOR , our proposed M
AP R EDUCE algorithm for large scale tensor analysis. G IGA T ENSOR provides an efficient distributed algorithm for the PARAFAC tensor decomposition on M AP R EDUCE . The major challenge is to design an efficient algorithm for updating factors (line 3, 5, and 7 of Algorithm 1). Since the update rules are simi-lar, we focus on updating the A matrix. As shown in the line 3 of Algorithm 1, the update rule for A is R especially in real world tensors, while A , B , and C are dense. There are several challenges in designing an efficient M AP DUCE algorithm for Equation (4) in G IGA T ENSOR : 1. Minimize flops. How to minimize the number of floating 2. Minimize intermediate data. How to minimize the inter-3. Exploit data characteristics. How to exploit the data char-
We have the following main ideas to address the above chal-lenges which we describe in detail in later subsections. 1. Careful choice of order of computations in order to mini-2. Avoiding intermediate data explosion by exploiting the 3. Parallel outer products to minimize intermediate data (Sec-4. Distributed cache multiplication to minimize intermedi-
Equation (4) entails three matrix-matrix multiplications, assum-ing that we have already computed ( C  X  B ) and ( C T C  X  B Since matrix multiplication is commutative, Equation (4) can be computed by either multiplying the first two matrices, and multi-plying the result with the third matrix: or multiplying the last two matrices, and multiplying the first matrix with the result: The question is, which equation is better between (5) and (6)? From a standard result of numerical linear algebra (e.g. [7]), the Equation (5) requires 2 mR + 2 IR 2 flops, where m is the num-ber of nonzeros in the tensor X , while the Equation (6) requires 2 mR + 2 JKR 2 flops. Given that the product of the two dimen-sion sizes ( JK ) is larger than the other dimension size ( I ) in most practical cases, Equation (5) results in smaller flops. For example, referring to the NELL-1 dataset of Table 7, Equation (5) requires  X  8 10 9 flops while Equation (6) requires  X  2 . 5 10 17 flops. For the reason, we choose the Equation (5) ordering for updating fac-tor matrices. That is, we perform the following three matrix-matrix multiplications for Equation (4):
As introduced at the end of Section 2, one of the most important issue for scaling up the tensor decomposition is the intermediate data explosion problem. In this subsection we describe the problem in detail, and propose our solution.

Problem. A naive algorithm to compute X ( 1 ) ( C  X  B ) is to first construct C  X  B , and multiply X ( 1 ) with C  X  B , as illustrated in Figure 2. The problem ( X  X ntermediate data explosion ") of this algo-rithm is that although the matricized tensor X ( 1 ) is sparse, the ma-trix C  X  B is very large and dense; thus, C  X  B cannot be stored even in multiple disks in a typical H ADOOP cluster.
 Figure 2: The  X  X ntermediate data explosion " problem in comput-ing X ( 1 ) ( C  X  B ) . Although X ( 1 ) is sparse, the matrix C  X  B is very dense and long. Materializing C  X  B requires too much storage: e.g., for J = K  X  26 million as in the NELL-1 data of Table 7, C  X  B explodes to 676 trillion rows.

Our Solution. Our crucial observation is that X ( 1 ) ( C  X  B ) can be computed without explicitly constructing C  X  B . 1 Our main idea is to decouple the two terms in the Khatri-Rao product, and perform algebraic operations involving X ( 1 ) and C , then X B , and then combine the result. Our main idea is described in Al-gorithm 2 as well as in Figure 3. In line 7 of Algorithm 2, the Hadamard product of X ( 1 ) and a matrix derived from C is per-formed. In line 8, the Hadamard product of X ( 1 ) and a matrix de-rived from B is performed, where the bin () function converts any nonzero value into 1 , preserving sparsity. In line 9, the Hadamard product of the two result matrices from lines 7 and 8 is performed, and the elements of each row of the resulting matrix are summed up to get the final result vector M 1 (: , r ) in line 10. The following Theorem demonstrates the correctness of Algorithm 2.

T HEOREM 1. Computing X ( 1 ) ( C  X  B ) is equivalent to com-puting ( N 1  X  N 2 ) 1 JK , where N 1 = X ( 1 )  X  ( 1 I  X  ( C (: , r ) 1 )) , N 2 = bin ( X ( 1 ) )  X  ( 1 I  X  ( 1 T K  X  B (: , r ) all-1 vector of size JK .
Bader et al. [4] has an alternative way to avoid the intermediate data explosion. perform algebraic operations using X ( 1 ) and C , and then X and the largest dense matrix is either the B or the C matrix.
Algorithm 2 : Multiplying X ( 1 ) and C  X  B in G IGA T ENSOR
Output: M 1  X  X ( 1 ) ( C  X  B ) . 10: M 1 (: , r )  X  N 3 1 JK ; 11: end for 12: return M 1 ; P R OOF . The ( i, y ) -th element of N 1 is given by The ( i, y ) -th element of N 2 is given by The ( i, y ) -th element of N 3 = N 1  X  N 2 is
Multiplying N 3 with 1 JK , which essentially sums up each row to the following: M 1 ( i, r ) = which is exactly the equation that we want from the definition of X
Notice that in Algorithm 2, the largest dense matrix required is either B or C (not C  X  B as in the naive case), and therefore we have effectively avoided the intermediate data explosion problem. Discussion. Table 4 compares the cost of the naive algorithm and G
IGA T ENSOR for computing X ( 1 ) ( C  X  B ) . The naive algorithm requires total JKR +2 mR flops ( JKR for constructing ( C  X  B ) , and 2 mR for multiplying X ( 1 ) and ( C  X  B ) ), and JKR + m intermediate data size ( JKR for ( C  X  B ) , and m for X ( 1 ) the other hand, G IGA T ENSOR requires only 5 mR flops ( 3 mR for three Hadamard products, and 2 mR for the final multiplication), and max ( J + m, K + m ) intermediate data size. The dependence on the term JK of the naive method makes it inappropriate for real world tensors which are sparse and the sizes of dimensions are much larger compared to the number m of nonzeros ( JK  X  m ). On the other hand, G IGA T ENSOR depends on max ( J + m, K + m ) which is O ( m ) for most practical cases, and thus fully exploits the sparsity of real world tensors.
 Table 4: Cost comparison of the naive and G IGA T ENSOR for com-puting X ( 1 ) ( C  X  B ) . J and K are the sizes of the second and the third dimensions, respectively, m is the number of nonzeros in the tensor, and R is the desired rank for the tensor decomposition (typ-ically, R  X  10 ). G IGA T ENSOR does not suffer from the interme-diate data explosion problem, and is much more efficient than the naive algorithm in terms of both flops and intermediate data sizes. An arithmetic example, referring to the NELL-1 dataset of Table 7, for 8 bytes per value, and R = 10 is: 1 . 25 10 16 flops, 100 PB for the naive algorithm and 8 . 6 10 9 flops, 1.5GB for G IGA
In this subsection, we describe M AP R EDUCE algorithms for computing the three steps in Equations (7), (8), and (9).
The first step is to compute M 1  X  X ( 1 ) ( C  X  B ) (Equa-tion (7)). The factors C and B are given in the form of &lt; X is stored in the format of &lt; i, j, k, X ( i, j, k ) &gt; , but we as-sume the tensor data is given in the form of mode-1 matricization use Q i and Q j to denote the set of nonzero indices in X and X ( 1 ) (: , j ) , respectively: i.e., Q i = { j | X ( 1 ) Q j = { i | X ( 1 ) ( i, j ) &gt; 0 } .

We first describe the M AP R EDUCE algorithm for line 7 of Algo-rithm 2. Here, the tensor data and the factor data are joined for the Hadamard product. Notice that only the tensor X and the factor C are transferred in the shuffling stage.
In the second M AP R EDUCE algorithm for line 8 of Algorithm 2, we perform the similar task as the first M AP R EDUCE algorithm but we do not multiply the value of the tensor, since line 8 uses the binary function. Again, only the tensor X and the factor B are transferred in the shuffling stage.
Finally, in the third M AP R EDUCE algorithm for lines 9 and 10, we combine the results from the first and the second steps using Hadamard product, and sums up each row to get the final result.
Note that the amount of data traffic in the shuffling stage is small (2 times the nonzeros of the tensor X ), considering that X is sparse. The next step is to compute ( C T C  X  B T B )  X  (Equation (8)). Here, the challenge is to compute C T C  X  B T B efficiently, since once the C T C  X  B T B is computed, the pseudo-inverse is trivial to compute because matrix C T C  X  B T B is very small ( R  X  R where R is very small; e.g. R  X  10 ). The question is, how to compute C T C  X  B T B efficiently? Our idea is to first compute C T C , then B T B , and performs the Hadamard product of the two R  X  R matrices. To compute C T C , we express C T C as the sum of outer products of the rows: where C ( k, :) is the k th row of the C matrix. To implement the Equation (10) efficiently in M AP R EDUCE , we partition the fac-tor matrices row-wise [24]: we store each row of C into a line in the H ADOOP File System (HDFS). The advantage of this approach compared to the column-wise partition is that each unit of data is self-joined with itself, and thus can be independently processed; column-wise partition would require each column to be joined with other columns which is prohibitively expensive.

The M AP R EDUCE algorithm for Equation (10) is as follows.
Since we use the combiner as well as the reducer, each mapper computes the local sum of the outer product. The result is that the size of the intermediate data, i.e. the number of input tuples to the reducer, is very small ( d R 2 where d is the number of mappers) in G
IGA T ENSOR . On the other hand, the naive column-wise partition method requires KR (the size of C T ) + K (the size of a column of C ) intermediate data for 1 iteration, and thereby requires K ( R R ) intermediate data for R iterations, which is much larger than the intermediate data size d R 2 of G IGA T ENSOR , as summarized in Table 5.
 Table 5: Cost comparison of the naive (column-wise partition) method and G IGA T ENSOR for computing C T C . K is the size of the third dimension, d is the number of mappers used, and R is the desired rank for the tensor decomposition (typically, R  X  10 ). Notice that although the flops are the same for both methods, G
T ENSOR has much smaller intermediate data size compared to the naive method, considering K  X  d . The example refers to the intermediate data size for NELL-1 dataset of Table 7, for 8 bytes per value, R = 10 and d = 50 .
The final step is to multiply X ( 1 ) ( C  X  B )  X  R I  X  R ( C
T C  X  B T B )  X   X  R R  X  R (Equation (9)). We note the skew-ness of data sizes: the first matrix X ( 1 ) ( C  X  B ) is large and does not fit in the memory of a single machine, while the second ma-trix ( C T C  X  B T B )  X  is very small to fit in the memory. To ex-ploit this into better performance, we propose to use the distributed cache multiplication [16] to broadcast the second matrix to all the mappers that process the first matrix, and perform join in the first matrix. The result is that our method requires only one M DUCE job with smaller intermediate data size ( IR 2 ). On the other hand, the standard naive matrix-matrix multiplication requires two M
AP R EDUCE jobs: the first job for grouping the data by the col-umn id of X ( 1 ) ( C  X  B ) and the row id of ( C T C  X  B second job for aggregation. Our proposed method is more efficient than the naive method since in the naive method the intermediate data size increases to I ( R + R 2 ) + R 2 (the first job: IR + R the second job: IR 2 ), and the first job X  X  output of size IR be written to and read from discs for the second job, as summarized in Table 6.
To evaluate our system, we perform experiments to answer the following questions: Table 6: Cost comparison of the naive (column-wise partition) method and G IGA T ENSOR for multiplying X ( 1 ) ( C  X  B ) and ( C
T C  X  B T B )  X  . I is the size of the first dimension, and R is the desired rank for the tensor decomposition (typically, R  X  10 ). Al-though the flops are the same for both methods, G IGA T ENSOR smaller intermediate data size compared to the naive method. The example refers to the intermediate data size for NELL-1 dataset of Table 7, for 8 bytes per value and R = 10 .
 Q1 What is the scalability of G IGA T ENSOR compared to other Q2 What is the scalability of G IGA T ENSOR compared to other Q3 How does G IGA T ENSOR scale with regard to the number of Q4 What are the discoveries on real world tensors?
The tensor data in our experiments are summarized in Table 7, with the following details. NELL-1 26 M 26 M 48 M 144 M NELL-2 15 K 15 K 29 K 77 M Random 10 K  X  1 B 10 K  X  1 B 10 K  X  1 B 100  X  20 M Table 7: Summary of the tensor data used. B: billion, M: million, K: thousand. We compare the scalability of G IGA T ENSOR and the Tensor Toolbox for Matlab [6] which is the current state of the art in terms of handling fast and effectively sparse tensors. The Tensor Toolbox is executed in a machine with a quad-core AMD 2.8 GHz CPU, 32 GB RAM, and 2.3 Terabytes disk. To run G IGA T ENSOR , we use CMU X  X  OpenCloud H ADOOP cluster where each machine has 2 quad-core Intel 2.83 GHz CPU, 16 GB RAM, and 4 Terabytes disk.

Scalability on the Size of Tensors. Figure 4 (a) shows the scal-ability of G IGA T ENSOR with regard to the sizes of tensors. We fix the number of nonzero elements to 10 4 on the synthetic data while increasing the tensor sizes I = J = K . We use 35 machines, and report the running time for 1 iteration of the algorithm. Notice that for smaller data the Tensor Toolbox runs faster than G IGA due to the overhead of running an algorithm on distributed systems, including reading/writing the data from/to disks, JVM loading type, and synchronization time. However as the data size grows beyond 10 7 , the Tensor Toolbox runs out of memory while G IGA T ENSOR continues to run, eventually solving at least 100  X  larger problem than the competitor. We performed the same experiment while fix-ing the nonzero elements to 10 7 , and we get similar results. We note that the Tensor Toolbox at its current implementation cannot run in distributed systems, thus we were unable to compare G
T ENSOR with a distributed Tensor Toolbox. We note that ex-tending the Tensor Toolbox to run in a distributed setting is highly non-trivial; and even more complicated to make it handle data that don X  X  fit in memory. On the contrary, our G IGA T ENSOR can handle such tensors.

Scalability on the Number of Nonzero Elements. Figure 4 (b) shows the scalability of G IGA T ENSOR compared to the Tensor Toolbox with regard to the number of nonzeros and tensor sizes on the synthetic data. We set the tensor size to be I  X  I  X  I , and the number of nonzero elements to be I/ 50 . As in the previous experi-ment, we use 35 machines, and report the running time required for 1 iteration of the algorithm. Notice that G IGA T ENSOR decomposes tensors of sizes at least 10 9 , while the Tensor Toolbox implemen-tation runs out of memory on tensors of sizes beyond 10 7
Scalability on the Number of Machines. Figure 5 shows the scalability of G IGA T ENSOR with regard to the number of machines. The Y-axis shows T 25 /T M where T M is the running time for 1 iteration with M machines. Notice that the running time scales up near linearly. Figure 5: The scalability of G IGA T ENSOR with regard to the num-ber of machines on the NELL-1 data. Notice that the running time scales up near linearly.
In this section, we present discoveries on the NELL dataset that was previously introduced; we are mostly interested in demonstrat-ing the power of our approach, as opposed to the current state of the art which was unable to handle a dataset of this magnitude. We perform two tasks: concept discovery, and (contextual) synonym detection.
 Concept Discovery. With G IGA T ENSOR , we decompose the NELL-2 dataset in R = 10 components, and obtained  X  i , A , B , C (see Figure 1). Each one of the R columns of A , B , C represents a grouping of similar (noun-phrase np1, noun-phrase np2, context words) triplets. The r -th column of A encodes with high values the noun-phrases in position np1, for the r -th group of triplets, the r -th column of B does so for the noun-phrases in position np2 and the r -th column of C contains the corresponding context words. In or-der to select the most representative noun-phrases and contexts for each group, we choose the k highest valued coefficients for each column. Table 8 shows 4 notable groups out of 10, and within each group the 3 most outstanding noun-phrases and contexts. Notice that each concept group contains relevant noun phrases and con-texts. Table 8: F our notable groups that emerge from analyzing the NELL dataset.

Contextual Synonym Detection. The lower dimensional em-bedding of the noun phrases also permits a scalable and robust strat-egy for synonym detection. We are interested in discovering noun-phrases that occur in similar contexts, i.e. contextual synonyms . Using a similarity metric, such as Cosine Similarity, between the lower dimensional embeddings of the Noun-phrases (such as in the factor matrix A ), we can identify similar noun-phrases that can be used alternatively in sentence templates such as np1 context np2 . Using the embeddings in the factor matrix A (appropriately column-weighted by  X  ), we get the synonyms that might be used in position np1 , using B leads to synonyms for position np2 , and us-ing C leads to contexts that accept similar np1 and np2 arguments. In Table 2, which is located in Section 1, we show some exemplary synonyms for position np1 that were discovered by this approach on NELL-1 dataset. Note that these are not synonyms in the tra-ditional definition, but they are phrases that may occur in similar semantic roles in a sentence.
In this section, we review related works on tensor analy-sis (emphasizing on data mining applications), and M AP R
Tensor Analysis. Tensors have a very long list of applications, pertaining to many fields additionally to data mining. For instance, tensors have been used extensively in Chemometrics [8] and Sig-nal Processing [29]. Not very long ago, the data mining commu-nity has turned its attention to tensors and tensor decompositions. Some of the data mining applications that employ tensors are the following: in [20], Kolda et al. extend the famous HITS algorithm [19] by Kleinberg et al. in order to incorporate topical informa-tion in the links between the Web pages. In [2], Acar et al. an-alyze epilepsy data using tensor decompositions. In [5], Bader et al. employ tensors in order perform social network analysis, us-ing the Enron dataset for evaluation. In [31], Sun et al. formulate click data on the Web pages as a tensor, in order to improve the Web search by incorporating user interests in the results. In [10], Chew et al. extend the Latent Semantic Indexing [12] paradigm for cross-language information retrieval, using tensors. In [33], Tao et al. employ tensors for 3D face modelling and in [32], a supervised learning framework, based on tensors is proposed. In [25], Maruhashi et al. present a framework for discovering bipar-tite graph like patterns in heterogeneous networks using tensors.
M AP R EDUCE and H ADOOP . M AP R EDUCE is a dis-tributed computing framework [11] for processing Web-scale data. M
AP R EDUCE has two advantages: (a) the data distribution, repli-cation, fault-tolerance, and load balancing is handled automati-cally; and furthermore (b) it uses the familiar concept of functional programming. The programmer needs to define only two functions, a map and a reduce . The general framework is as follows [23]: (a) the map stage reads the input file and outputs (key, value) pairs; (b) the shuffling stage sorts the output and distributes them to reduc-ers; (c) the reduce stage processes the values with the same key and outputs another (key, value) pairs which become the final result. H ADOOP [1] is the open source version of M AP R EDUCE . H
ADOOP uses its own distributed file system HDFS, and provides a high-level language called PIG [26]. Due to its excellent scalabil-ity, ease of use, cost advantage, H ADOOP has been used for many graph mining tasks (see [18, 15, 16, 17]).
In this paper, we propose G IGA T ENSOR , a tensor decomposition algorithm which scales to billion size tensors, and present interest-ing discoveries from real world tensors. Our major contributions include:
Future work could focus on related tensor decompositions, such as PARAFAC with sparse latent factors [27], as well as the TUCKER3 [34] decomposition.
 Funding was provided by the U.S. ARO and DARPA under Con-tract Number W911NF-11-C-0088, by DTRA under contract No. HDTRA1-10-1-0120, and by ARL under Cooperative Agreement Number W911NF-09-2-0053, The views and conclusions are those of the authors and should not be interpreted as representing the offi-cial policies, of the U.S. Government, or other funding parties, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] Hadoop information. http://hadoop.apache.org/. [2] E. Acar, C. Aykut-Bingol, H. Bingol, R. Bro, and B. Yener. [3] C.A. Andersson and R. Bro. The n-way toolbox for matlab. [4] B. W. Bader and T. G. Kolda. Efficient MATLAB [5] B.W. Bader, R.A. Harshman, and T.G. Kolda. Temporal [6] B.W. Bader and T.G. Kolda. Matlab tensor toolbox version [7] S. Boyd and L. Vandenberghe. Convex Optimization . [8] R. Bro. Parafac. tutorial and applications. Chemometrics and [9] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, [10] P.A. Chew, B.W. Bader, T.G. Kolda, and A. Abdelali. [11] J. Dean and S. Ghemawat. Mapreduce: Simplified data [12] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, [13] C. Eckart and G. Young. The approximation of one matrix by [14] R.A. Harshman. Foundations of the parafac procedure: [15] U. Kang, D. H. Chau, and C. Faloutsos. Mining large graphs: [16] U. Kang, B. Meeder, and C. Faloutsos. Spectral analysis for [17] U. Kang, H. Tong, J. Sun, C.-Y. Lin, and C. Faloutsos. [18] U. Kang, C. E. Tsourakakis, and C. Faloutsos. Pegasus: A [19] J.M. Kleinberg. Authoritative sources in a hyperlinked [20] T.G. Kolda and B.W. Bader. The tophits model for [21] T.G. Kolda and B.W. Bader. Tensor decompositions and [22] T.G. Kolda and J. Sun. Scalable tensor decompositions for [23] R. L X mmel. Google X  X  mapreduce programming model  X  [24] C. Liu, H. c. Yang, J. Fan, L.-W. He, and Y.-M. Wang. [25] K. Maruhashi, F. Guo, and C. Faloutsos.
 [26] C. Olston, B. Reed, U. Srivastava, R. Kumar, and [27] E.E. Papalexakis and N.D. Sidiropoulos. Co-clustering as [28] R. Penrose. A generalized inverse for matrices. In Proc. [29] N.D. Sidiropoulos, G.B. Giannakis, and R. Bro. Blind [30] J. Sun, S. Papadimitriou, and P. S. Yu. Window-based tensor [31] J.T. Sun, H.J. Zeng, H. Liu, Y. Lu, and Z. Chen. Cubesvd: a [32] D. Tao, X. Li, X. Wu, W. Hu, and S.J. Maybank. Supervised [33] D. Tao, M. Song, X. Li, J. Shen, J. Sun, X. Wu, C. Faloutsos, [34] L.R. Tucker. Some mathematical notes on three-mode factor
