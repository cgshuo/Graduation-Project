
University of Illinois at Urbana-Champaign, USA
Advanced Digital Sciences Center, Singapore Singapore Management University, Singapore As labeled data is often scarce, semi-supervised learning (SSL) can be beneficial by exploiting unlabeled data. Con-sider a random tuple ( X,Y ) , where a data point X  X  X  = { x 1 ,...,x |X| } has a label Y  X  X  . We observe labeled data L comprising i.i.d. samples of ( X,Y ) , and unlabeled data U comprising i.i.d. samples of X . Typically |U| |L| . Potentially, we may only observe a partial X via L and U . The task is to predict the label for every x i  X  X  . Towards effective SSL, graph-based smoothness has at-tracted much research interest. In particular, the smooth-ness statement is central to SSL (Zhu, 2005; Chapelle et al., 2006): if two points x i ,x j are close, their respective la-bels y i ,y j are likely to be the same . The literature further suggests that it is more effective to consider smoothness on the low-dimensional manifold, where the high-dimensional data roughly live. As widely recognized, graphs are often used as a proxy for the manifold (Blum &amp; Chawla, 2001; Zhu et al., 2003; Zhou et al., 2003; Belkin et al., 2006; Johnson &amp; Zhang, 2008; Subramanya &amp; Bilmes, 2011). Specifically, each point x i is a node on a graph, and two points x i ,x j may be connected by an edge weighted W ij The weight matrix W aims to capture the pairwise geodesic distance on the manifold. In other words, the graph reveals the structures on the manifold.
 Unfortunately, although graph-based methods universally hinge on smoothness, their realizations fall short. As the thesis of this paper, we advocate that smoothness shall be pointwise in nature and probabilistic in modeling. Nature: Pointwise smoothness. Smoothness shall inher-ently occur  X  X verywhere, X  to relate the behavior of each point to that of its close points. We call this the point-wise nature of smoothness. As recently identified (Rigol-let, 2007), precisely expressing the pointwise nature boils down to two aspects:  X  How do we decide if a data point is close to another?
The smoothness statement lacks a concrete definition of closeness. Thus, we need a data closeness model to de-fine this. (P1)  X  How is the behavior of two close points related? The smoothness statement requires that  X  X heir labels are likely to be the same X , which is rather vague. Thus, we need a label coupling model to explicitly relate their la-bel behavior. (P2) Surprisingly, to date, no existing graph-based method re-alizes pointwise smoothness. While it has been studied in non-graph based settings (Rigollet, 2007; Lafferty &amp; Wasserman, 2007; Singh et al., 2008), previous graph-based methods treat smoothness in an aggregate , rather than pointwise, manner. Specifically, they optimize an energy function in a random field (Zhu et al., 2003; Zhu &amp; Ghahramani, 2002; Getz et al., 2005) or a cost func-tion (Zhou et al., 2003; Belkin et al., 2006; Subramanya &amp; Bilmes, 2011) over the graph. An energy or cost function aggregates all pairwise differences between neighboring points across the entire graph. By minimizing the aggre-gated difference, some  X  X verage X  smoothness is achieved. However, such aggregation is not designed for and thus does not necessarily enforce smoothness at every point X  X t is unclear how an aggregate function can precisely express the pointwise nature of smoothness, in terms of the two aspects (P1 &amp; P2). After all, there exist different cost func-tions varying greatly in actual forms ( e.g. , squared error, soft margin loss, or probability divergence), with limited justification to favor one over another.
 Modeling: Probabilistic smoothness. Pointwise smooth-ness shall be modeled probabilistically in both aspects (P1 &amp; P2), to ultimately infer p ( Y | X ) . First, how close is suffi-ciently close is difficult to be reliably captured by determin-istic binary decisions (P1). Second, the smoothness state-ment that  X  X heir labels are likely to be the same X  is mean-ingless (Rigollet, 2007) unless it is exploited in probabilis-tic terms (P2). Within a probabilistic framework, eventu-ally each point can be classified based on p ( Y | X ) , given i.i.d. samples. Furthermore, probabilistic modeling con-veys some concrete benefits, such as integrating class priors p ( Y ) in a more principled way, naturally supporting multi-class tasks, and facilitating client applications that require probabilities as input.
 We note that existing probabilistic modeling in graph-based settings (Subramanya &amp; Bilmes, 2011; Das &amp; Smith, 2012; He et al., 2007; Azran, 2007) only supports aggregate, but not pointwise, smoothness.
 Our proposal. We propose the framework of Probabilistic Graph-based Pointwise Smoothness (PGP), hinging on two foundations that address the pointwise nature of smooth-ness probabilistically on a graph.
 To begin with, we need a data closeness model to determine if a point is close to another (P1). Since the graph captures the pairwise geodesic distance on the manifold, a random walk on the graph X  X hich moves from X to X 0 in each step X  X aturally  X  X onnects X  X and X 0 as close points on the manifold. Hence, for a pair of random points ( X,X 0 ) such that X is close to X 0 , we can describe their distribution p ( X,X 0 ) using the second-order stationary distribution of the random walk. In contrast, the distribution of a single point p ( X ) has been traditionally represented by the first-order stationary distribution.
 Next, we also need a label coupling model to relate the la-bel behavior of a point x i to that of its close points (P2). We leverage the notion of statistical indistinguishability (Gol-dreich, 2010). In particular, whether X is x i , or X is some point close to x i , the label Y of X shall be produced in an indistinguishable manner. In other words, we cannot tell apart the distributions of Y in these two cases.
 Together, these two foundations constitute our smoothness framework, which further entails a solution to SSL. While the given labels naturally constrain the labeled data, our smoothness framework axiomatizes a set of probability constraints on the unlabeled data. Solving these constraints eventually infers p ( Y | X ) for class prediction. Note that the constraints can be either discriminative over p ( Y | X ) , or generative over p ( X | Y ) . Although the ultimate goal is p ( Y | X ) , generative models that learn p ( X | Y ) and p ( Y ) are often favorable in SSL (Chapelle et al., 2006). Thus, although our framework can accommodate both forms, we adopt the generative form here and leave the discriminative counterpart to future work 1 .
 Finally, we present a theoretical analysis of our solution. First, to see that PGP can utilize both labeled and unlabeled data, we derive a generalization error in L and U . Second, to show that PGP is not sensitive to noisy input graphs, we assess the robustness of our solution.
 Our contributions. We summarize the contributions in this paper as follows.  X  We propose PGP, the first work to realize pointwise smoothness on a graph probabilistically.  X  We conduct an error and robustness analysis of PGP.  X  We demonstrate the advantages of PGP through exten-sive experiments. To express the pointwise nature of smoothness, we must address its two aspects. Under a probabilistic graph-based framework, we propose a data closeness model to capture how a point is close to another (Sect. 2.1), as well as a label coupling model to conceptualize how the label behavior of a point is related to that of its close points (Sect. 2.2). 2.1. Data Closeness Model (P1) We first propose a probabilistic model for capturing data closeness on the graph.
 Graph. For a set of points X = { x 1 ,...,x |X| } , we con-struct a graph G to capture the pairwise geodesic distance on the underlying manifold. Each point x i  X  X is a ver-tex of G , and each pair of points ( x i ,x j ) form an edge of G with a weight W ij . W ij is also known as the affin-ity between x i and x j , an approximate description of the geodesic distance between the two points. W is often de-fined as follows: where k X k is a symmetric distance function, and  X  is a scal-ing parameter. In our notation, unquantified indices such as i,j belong to { 1 ,..., |X|} , unless stated otherwise. Random walk-based closeness. As argued in Sect. 1, it is difficult to reliably capture how close is sufficiently close in a deterministic manner. In order to develop probabilistic closeness, we need to represent the event that two points, say x i and x j , are close.
 We capture the closeness event based on a random walk on the graph. Consider a random walk on G visiting a se-quence of points { V t : t = 0 , 1 ,... } . While traditionally a visit at x i ( V t = x i ) models a single point x i , a traversal walking from a point x i to another x j ( V t = x i ,V t +1 x ) naturally  X  X onnects X  x i and x j to imply that x i is close to x j on the underlying manifold.
 Note that our use of random walk serves a novel purpose. It specifically models the first pointwise aspect (P1) of re-lating the points X through the closeness event, which, to-gether with the second aspect (P2) of relating the labels Y in Sect. 2.2, is necessary for pointwise smoothness. On the contrary, existing use of random walk in SSL (Szummer &amp; Jaakkola, 2001; Azran, 2007; Wu et al., 2012) models the  X  X ropagation X  of label Y among X altogether, without treating the two aspects explicitly.
 Formally, let ( X = x i ,X 0 = x j ) denote the event that x is close to x j , which follows the distribution of observing a random walk traversal from x i to x j in the long run as t  X   X  . Hence, ( X,X 0 ) is a pair of limiting random vari-ables in the sense that a traversal ( V t ,V t +1 ) converges in distribution to ( X,X 0 ) jointly: In other words, ( X,X 0 ) describes the closeness between two points with the joint second-order limit, while X de-scribes a single point with the marginal first-order limit. Their convergence will be shown later.
 Probability space of closeness. We describe the probabil-ity space of the random walk-based closeness model. Sample space. An outcome that x i is close to x j is a pair of points ( x i ,x j ) , which corresponds to a random walk traver-sal from x i to x j . Hence, the sample space is  X  = X An outcome can be denoted by a pair of random variables ( X,X 0 )  X   X  , as defined in Eq. 2.
 Events. As discussed, each outcome ( x i ,x j ) is an event that x i is close to x j , i.e. , { ( X,X 0 )  X   X  : X = x x } or denoted ( X = x i ,X 0 = x j ) . In order to relate the behavior of two close points, we are also interested in the events that X is x i or X is close to x i .
 First, { ( X,X 0 )  X   X  : X = x i } , or denoted X = x i , is the event of observing X as a point x i . It corresponds to a traversal from x i to some point.
 Second, { ( X,X 0 )  X   X  : X 0 = x i } , or denoted ( X, X x ) , is the event of observing X as some point close to x i ( i.e. , X is implicitly constrained by X 0 = x i ). It corre-sponds to a traversal from some point to x i .
 Without loss of generality, here we treat X as the random variable of interest, and our ultimate goal is to estimate p ( Y | X ) . However, we could also treat X 0 as the variable of interest, and find p ( Y 0 | X 0 ) in a symmetric manner given that W is symmetric.
 Probability measure. Finally, we evaluate the probability of the events. The random walk can be formally repre-sented by a transition matrix Q such that As ( V t ,V t +1 ) converges in distribution to ( X,X 0 closeness event ( X = x i ,X 0 = x j ) obeys the second-order stationary distribution of the random walk: p ( X = x i ,X 0 = x j ) = lim t  X  X  X  p ( V t = x i ,V t +1 As established in Proposition 1, a unique second-order sta-tionary distribution exists. As a further consequence, the probability of the events can also be computed 2 . P (a) The limit of p ( V t ,V t +1 ) as t  X  X  X  exists uniquely. (b) Given that ( V t ,V t +1 ) d  X  X  X  ( X,X 0 ) , Intuitively, Eq. 5 means that the stronger affinity W ij be-tween x i and x j , the more likely they are close. Second, Eq. 6 implies that observing x i is as likely as observing a point close to x i , which is not surprising given that two close points lie near each other on the manifold. 2.2. Label Coupling Model (P2) Next, we propose a label coupling model to relate the la-bel behavior of two close points. In our realization, the label Y of X distributes similarly whether X is x i itself, or X is some point close to x i . That is, p ( Y | X = x i p ( Y | X, X 0 = x i ) shall be alike.
 Indistinguishability. We leverage the concept of statis-tical indistinguishability (Goldreich, 2010): two distribu-tions are statistically indistinguishable if they cannot be told apart to some extent.
 D tions D 1 and D 2 are -statistically indistinguishable if and only if 1 2 k D 1  X  D 2 k 1  X  .
 In our context, p ( Y | X = x i ) and p ( Y | X, X 0 = x i be statistically indistinguishable. In other words, the label Y of X is produced in an indistinguishable manner regard-less of X being x i or a point close to x i .
 Label Coupling. To achieve indistinguishability, x i  X  X  label shall distribute similarly to that of a point close to x i the same time, some  X  X istrust X  of the close points shall be allowed, as small variances in their labels are still expected. These factors can be accounted for by a simple mixture: p ( Y | X = x i ) = (1  X   X  ) p ( Y | X, X 0 = x i ) +  X D, (7) where  X   X  (0 , 1) is a parameter, and D is the distribution to fall back on when the close points are not trusted. In the distrust case, we assign x i to an  X  X nknown X  class  X  /  X  Y , i.e. , D ( y ) = 0 ,  X  y  X  Y and D (  X  ) = 1 . Our label cou-pling model represented by this mixture formally satisfies statistical indistinguishability.
 P label distribution of x i , p ( Y | X = x i ) , is  X  -statistically in-distinguishable from the label distribution of some point close to x i , p ( Y | X, X 0 = x i ) .
 Note that Eq. 7 couples the label distributions in a discrim-inative form of p ( Y | X ) . To model the generative proba-bility p ( X | Y ) as Sect. 1 motivated, we also derive its gen-erative counterpart.  X  y  X  X  ,  X  x i  X  X  , p ( X = x i | Y = y ) = p ( Y = y | X = x i ) p ( X = x i ) /p ( Y = y ) = (1  X   X  ) p ( Y = y | X, X 0 = x i ) p ( X, X 0 = x i ) /p ( Y = y ) = (1  X   X  ) p ( X, X 0 = x i | Y = y ) . (8) In particular, D is eliminated since D ( y ) = 0 ,  X  y  X  X  , i.e. , points of class y  X  Y cannot be generated from D . The intuition is that indistinguishability slowly  X  X ades X  along a  X  X hain X  of close points due to the 1  X   X  factor.
 Implication. Eq. 8 implies that the first-order condi-tional distribution p ( X = x i | Y = y ) can be related to the sum of the second-order (joint) conditional distributions p ( X = x j ,X 0 = x i | Y = y ) over x j  X  X  . The association of the first-order or point distribution, to the second-order or edge distribution, is expected, as the pointwise nature of smoothness is to relate the behavior of a point x i to that of its close points x j , which we shall see next. Under the smoothness framework in Sect. 2, we develop a set of generative probability constraints in terms of p ( X | Y ) , and show that a unique solution satisfying the constraints exists. Next, we use an iterative algorithm to find the solution and predict classes accordingly. 3.1. Generative Probability Constraints For each y  X  X  , we aim to learn the generative distribution where  X  yi , p ( X = x i | Y = y ) . To find  X  y , we develop and solve a set of constraints on  X  y . On the one hand, for x i  X  L the constraints can be modeled using the known labels. On the other hand, while there is no known label for x i /  X  L , the constraints can be modeled using points close to x i , based on our smoothness framework.
 Labeled points. We rewrite p ( X = x i | Y = y ) for x i  X  L , relating it to p ( Y = y | X = x i ) which can be estimated from the known labels in Sect. 3.3. For a given y  X  X  , The proportionality follows from p ( X = x i )  X  Z i (Propo-sition 1). We can transform this result into a constraint on  X  y below, where K is the sum of  X  yi for labeled points, and  X  yi is the proportion each  X  yi gets from the sum K accord-ing to Eq. 10. Note that we write p ( y | x i ) as a shorthand for p ( Y = y | X = x i ) if there is no ambiguity. Constraint on Labeled Data: Unlabeled points. We also rewrite p ( X = x i | Y = y ) for unlabeled points x i /  X  L , relating it to that of its close points. Specifically, for a given y  X  X  , p ( X = x i | Y = y ) 1 = (1  X   X  ) p ( X, X 0 = x i | Y = y ) = (1  X   X  ) P j p ( X = x j ,X 0 = x i | Y = y ) = (1  X   X  ) P j p ( X 0 = x i | X = x j ,Y = y ) p ( X = x = (1  X   X  ) P j p ( X 0 = x i | X = x j ) p ( X = x j | Y = y ) = (1  X   X  ) P j W ji /Z j  X  p ( X = x j | Y = y ) (12) Step 1 is the generative form of smoothness (Eq 8). In step 2, we relate x i to each x j through their second-order (joint) distribution, where each x j has a different probability of being close to x i . In step 3, based on our closeness model, given X = x j , X 0 = x i only depends on W and is condi-tionally independent of Y . In Step 5, p ( X 0 = x i | X = x is simply the transition probability Q ji = W ji /Z j . This result imposes another constraint on  X  y .
 Constraint on Unlabeled Data: 3.2. Solving the Constraints The goal is to solve  X  y that satisfies the constraints on la-beled and unlabeled data. In particular, we can show that  X  y is the stationary distribution of some Markov chain with X as its state space. Intuitively, the unlabeled constraint (Eq. 13) already tells us how state x j transitions to each x i /  X  L . Thus, we only need to deduce the transition to each state x i  X  X  . Proposition 3 establishes the exact tran-sition between the states.
 P
ROPOSITION 3 (S OLUTION ):  X  y  X  Y , if  X  y satisfies the constraints in Eq. 11 and 13, then: (a)  X  y is the stationary distribution of a Markov chain C with states X and transition matrix P , where P (b) The stationary distribution of C exists uniquely. In fact, Eq. 14 means that  X  y =  X  y P . If we rewrite it as element-wise operations, we see that a constraint is placed on every individual point.
 Class prediction. Given  X  y (which will be solved in Sect. 3.3), we predict the label y i for x i as follows: y i = arg max y  X  X  p ( Y = y | X = x i ) Here p ( X = x i | Y = y ) is simply  X  yi , and p ( Y = y ) is the class prior which can be estimated from L . 3.3. Solution Computation and Estimation Next, we discuss how  X  y can be computed.
 Iterative algorithm. Proposition 3 entails that  X  y can be found iteratively, if the transition matrix P is known : where  X  ( t ) y converges uniquely as t  X   X  for an arbitrary initial distribution  X  (0) y .
 Solution estimation. We can only find a solution estimator  X   X  y since P is unknown  X  P is a function of W and  X  y , both of which can only be estimated, resulting in two types of er-ror. First, data sampling error : W is defined on X , but only a partial  X  X is observed through L and U . Thus, we can only construct an estimator  X  W using the incomplete  X  X . Second, label sampling error:  X  y is defined by p ( y | x i ) ,  X  x but p ( y | x i ) is unknown. We can only estimate it from the given labels,  X  p ( y | x i ) = |L y  X  X  x i | / |L x i | where L set of all samples with y in L , and L x i is the set of all sam-ples with x i in L . Subsequently, we obtain an estimator based on  X  p ( y | x i ) .
 Efficiency. While efficiency is not our focus, PGP can be solved efficiently using standard iterative techniques, and its complexity is comparable to most existing SSL meth-ods. In terms of time , if we use a widely accepted k NN graph, the cost is O ( k |X| s ) , where s is the number of itera-tions till convergence (typically k ~ 10 , s ~ 100 ). Although constructing an exact k NN graph can be quadratic, an ap-proximate graph is often adequate (Chen et al., 2009). In terms of space , we can store the k NN graph sparsely, thus needing only O ( k |X| ) space. 3.4. Discussion: Comparison to Existing Methods Our constraints on unlabeled points (Eq. 13) may appear similar to existing works, in particular GRF (Zhu et al., 2003) of the following formulation: where F i  X  [0 , 1] is the label function at x i . Although they resemble in the surface form, their exact forms are still disparate. We stress that such resemblance X  expressing x i  X  X  label as some function of its neighbors x is quite expected, since it is a common insight of graph-based SSL to relate a point and its neighbors on the graph (Zhou et al., 2003; Subramanya &amp; Bilmes, 2011). Nonethe-less, our exact function still differs in that PGP normalizes each x j differently by Z j and has a damping factor 1  X   X  , whereas GRF normalizes each x j by the same Z i and has no damping factor. Beneath the surface resemblance, there also exist some fundamental differences.
 First, most existing cost function (Zhou et al., 2003; Belkin et al., 2006) or random walk (Szummer &amp; Jaakkola, 2001; Azran, 2007; Wu et al., 2012) approaches, including GRF, do not correspond to an explicit formulation of pointwise smoothness. For instance, GRF boils down to the energy function of a Gaussian field, which is the aggregated sum of pairwise losses. Such aggregation is not designed for or derived from requiring smoothness at every individual point. Thus, smoothness does not necessarily occur  X  X v-erywhere. X  Even though GRF eventually leads to a local weighted average of neighbors (Eq. 17), it is a consequence of minimizing an aggregate loss function, rather than origi-nating from the pointwise nature in terms of the two aspects P1 &amp; P2. In contrast, PGP is not derived from an aggregate function, but directly builds on the data closeness model (for P1) and label coupling model (for P2).
 Second, while many approaches (Zhu et al., 2003; Wu et al., 2012) have probabilistic interpretations, they do not explicitly model p ( Y | X ) or p ( X | Y ) . Taking GRF as an example, { F i } represents the most probable configuration of a Gaussian field. Equivalently, F i is the random walk probability that a particle starting from x i first hits a labeled point. Both interpretations do not explicitly correspond to p ( Y | X ) or p ( X | Y ) . In contrast, in PGP,  X  y directly cor-responds to p ( X | Y = y ) .
 Finally, we use a two-spiral dataset (Singh, 1998) to il-lustrate that PGP indeed results in better smoothness. As shown in Fig. 1(a), the dataset consists of two spiral struc-tures as the two classes ( i.e. , Y = { 1 , 2 } ). We compare the smoothness of PGP with the well-known GRF and the state-of-the-art MP (Subramanya &amp; Bilmes, 2011), which are both graph-based methods albeit with different energy or cost functions.
 Smoothness essentially implies that the decision function of a classifier changes slowly on a coherent structure (Zhou et al., 2003). A previously proposed decision function is h ( x i ) , ( H 1 i  X  H 2 i ) / ( H 1 i + H 2 i ) , where H  X  X core X  for class y , as assigned by a method. Then, the decision rule is sign ( h ( x i )) , which is equivalent to the de-cision rule of every method compared here.
 In Fig. 1(b1 X 3), we visualize the predictions made by the three methods, respectively. All methods use the same four points marked by  X   X   X  as labeled, whereas the rest are un-labeled. Their respective optimal parameters are adopted. The color of a point x i represents the predicted class y and the size of a point x i represents the magnitude of the decision function at x i , | h ( x i ) | . Thus, a smoother decision function shall result in a sequence of points in more uni-form sizes over each structure. Clearly, among the three methods, PGP generates points of the most uniform sizes, and is smooth nearly everywhere. Alternatively, we plot the decision function over the sequence of points in one of the structures in Fig. 1(c), which shows that PGP has a smoother decision function.
 With better smoothness, PGP achieves a perfect result against the ideal classification in Fig. 1(a) (where the size of each point has no significance). In contrast, GRF and MP misclassify 4 and 2 points, respectively. Error in  X   X  y . It is crucial that we can bound the error in the solution estimator  X   X  y , which is estimated from the samples L and U .
 We show that the expected error, E k  X   X  y  X   X  y k 1 , can be bounded by two terms, corresponding to the two types of error discussed in Sect. 3.3. Formally, as our solution is the stationary distribution of a Markov chain, the proof can be established based on the perturbation theory of Markov chains (Cho &amp; Meyer, 2001; Seneta, 1993).
 P
ROPOSITION 4 (E RROR ): Given the two constraints (Eq. 11 and 13), for any constant  X  (0 , 1) , where  X  1 = min x min x This result presents two major implications. First, both la-beled and unlabeled data can help, as the bound improves when L or U grows. Second, the bound is fundamentally limited by L . Given a fixed set of L , even as |U|  X   X  , we can achieve no better than the second error term. In other words, unlabeled data can only help so much. While our analysis is tailored to PGP, the result is consistent with previous analysis (Rigollet, 2007).
 Robustness of  X  y . Until now, we have assumed that the graph construction function (Eq. 1) is perfect. If the graph were constructed differently ( i.e. , perturbed), can we assess the robustness of our solution? In other words, do small perturbations only cause a small change in the solution? In our perturbation model, every pairwise affinity W ij can be perturbed by some scale factor s &gt; 1 . The goal is to show that the solution derived from the perturbed affinity matrix  X  W does not change much if s is small.
 P
ROPOSITION 5 (R OBUSTNESS ): Suppose a matrix  X  W is perturbed from W , such that for some s &gt; 1 , W ij /s  X   X  W ij  X  W ij  X  s ,  X  ij . Let  X   X  y be the the solution vector based on  X  W . It holds that k  X   X  y  X   X  y k 1  X  O ( s 2  X  1) . Here s is the degree of perturbation on W . The result im-plies that our solution is robust, for changes in the solution can be bounded by the degree of perturbation. We empirically compare PGP with various SSL algorithms, and validate the claims in this paper.
 Datasets. We use six public datasets shown in Fig. 2. Three of them, Digit1 , Text and USPS , come from a bench-mark (Chapelle et al., 2006). As the benchmark datasets are mostly balanced, we also use three datasets from UCI repository (Frank &amp; Asuncion, 2010), namely, Yeast , ISOLET and Cancer 3 . Only a subset of Yeast (classes cyt , me1 , me2 , me3 ) and of ISOLET (classes a , b , c , d ) are used. The benchmark datasets are taken without further process-ing. For the UCI datasets, feature scaling is performed so that all features have zero mean and unit variance. Graph. We construct a k NN graph (Chapelle et al., 2006), where k is a parameter to be selected. To instantiate Eq. 1, we use Euclidean distance for all datasets except Text , and Cosine distance for Text .  X  is set to the average distance of all neighboring pairs on the graph.
 Labeling. For a given |L| , we sample 200 runs, where in each run |L| points are randomly chosen as labeled, and the rest are treated as unlabeled. The sampling ensures at least one labeled point for each class. 5% of the runs are reserved for model selection, and the remaining are for testing. Evaluation. We evaluate the mean performance over the testing runs on each dataset. As classification accuracy is not a truthful measure of the predictive power on imbal-anced datasets, we adopt macro F-measure (Forman, 2003) as the performance metric. 5.1. Comparison to Baseline Algorithms We compare PGP to five state-of-the-art SSL algorithms, which have been shown (Zhu et al., 2003; Belkin et al., 2006; Subramanya &amp; Bilmes, 2011) to significantly out-perform earlier ones such as TSVM (Joachims, 1999) and SGT (Joachims, 2003).  X  Gaussian Random Fields (GRF) (Zhu et al., 2003): a pi-oneering method based on Gaussian fields, equivalent to optimizing the squared loss.  X  LapSVM (LSVM) (Belkin et al., 2006): an effective graph-based extension of SVM.  X  Graph-based Generative SSL (GGS) (He et al., 2007): a probabilistic generative approach.  X  Measure Propagation (MP) (Subramanya &amp; Bilmes, 2011): a divergence-based optimization formulation over probability distributions.  X  Partially Absorbing Random Walk (PARW) (Wu et al., 2012): a random walk method on graphs.
 An existing implementation (Melacci &amp; Belkin, 2011) is used for LSVM, whereas our own implementations are used for the others. Each algorithm integrates class priors as suggested in their respective work, if any.
 Model selection is performed on the reserved runs. For each algorithm, we search k  X  { 5 , 10 , 15 , 20 , 25 } to con-struct the k NN graph. GRF and GGS has no other parame-ters. For LSVM, we search  X  A  X  X  1 e X  6 , 1 e X  4 ,. 01 , 1 , 100 } , v  X  { 1 e X  8 , 1 e X  6 , 1 e X  4 ,. 01 ,. 1 } . For PARW, we search  X   X  { 1 e X  8 , 1 e X  6 , 1 e X  4 ,. 01 , 1 , 100 } . For PGP, we search  X   X  X  . 01 ,. 02 ,. 05 ,. 1 ,. 2 ,. 5 } .
 The mean macro F-measures on the testing runs are re-ported in Fig. 3, leading to the following findings. First, PGP performs the best or not significantly different from the best in 15 out of the 18 cases ( i.e. , columns), whereas GRF, LSVM, GGS, MP and PARW perform as such in only 3, 6, 4, 7, 5 cases, respectively.
 Second, while PGP has relatively stable performance across all the cases, the baselines can be volatile. In partic-ular, when PGP is not the best, there is no consistent best method, which varies between LSVM, GGS and MP.
 Third, PGP is especially advantageous with limited labeled data ( e.g. , |L| = 10 ), which is the very motivation of SSL. In contrast, when abundant data are labeled ( e.g. , |L| = 150 ), all algorithms perform better, and thus not sur-prisingly, the margin between them becomes smaller. 5.2. Integrating Class Priors A concrete benefit of probabilistic modeling is to enable better integration of class priors, which is also probabilis-tic in nature. We demonstrate that principled integration of class priors is more effective than heuristics, and integrat-ing more accurate priors helps.
 Integration of priors. We compare two different methods of integrating class priors:  X  BAYES: integrating in PGP in a Bayesian way (Eq. 15).  X  CMN: integrating in GRF using the popular heuristic Class Mass Normalization (Zhu et al., 2003).
 Note that BAYES and CMN respectively apply to a differ-ent algorithm as they are originally intended for. The priors are approximated in the same way for both methods, using the labeled points with add-one smoothing.
 We study the corrective power of each method: integrat-ing priors can be seen as  X  X orrections X  to the base model that does not incorporate priors. Directly assessing the im-provement over the base model is unfair, since the base per-formances of PGP and GRF differ. Instead, we compute the F-score from the precision and recall of the corrections: precision = #true corrections / #corrections made (19) The results are presented in Fig. 4(a) on the imbalanced datasets, which are more interesting given their non-uniform class priors. In all but one case, BAYES possesses much better corrective power than CMN.
 More accurate priors. If class priors are integrated ap-propriately, using more accurate priors is expected to im-prove the performance. Suppose we know the exact priors by considering the labels of all points. We then apply the approximate and exact priors to PGP. We directly measure the performance with or without priors, given the same base model. The results are presented in Fig. 4(b), which illus-trate that, while the estimated priors are effective in most cases, the supposedly more accurate exact priors can fur-ther improve the performance. We proposed a novel framework of Probabilistic Graph-based Pointwise Smoothness (PGP), hinging on the foun-dational data closeness and label coupling models. We further transformed such smoothness into a set of prob-ability constraints, which can be solved uniquely to infer p ( Y | X ) . We also studied the theoretical properties of PGP in terms of its generalization error and robustness. Finally, we empirically demonstrated that PGP is superior to exist-ing state-of-the-art baselines.
 This material is based upon work partially supported by NSF Grant IIS 1018723, the Advanced Digital Sci-ence Center and the Multimodal Information Access and Synthesis Center of University of Illinois at Urbana-Champaign, and Agency for Science, Technology and Re-search of Singapore. Any opinions, findings, and conclu-sions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies.
 Azran, Arik. The rendezvous algorithm: Multiclass semi-supervised learning with markov random walks. In ICML , pp. 49 X 56, 2007.
 Belkin, M., Niyogi, P., and Sindhwani, V. Manifold reg-ularization: A geometric framework for learning from labeled and unlabeled examples. J. of Machine Learning Research , 7:2399 X 2434, 2006.
 Blum, A. and Chawla, S. Learning from Labeled and Un-labeled Data using Graph Mincuts. In ICML , pp. 19 X 26, 2001.
 Chapelle, O., Sch  X  olkopf, B., and Zien, A. (eds.). Semi-supervised learning . MIT Press, Cambridge, MA, 2006. Chen, Jie, Fang, Haw-ren, and Saad, Yousef. Fast approxi-mate k NN graph construction for high dimensional data via recursive lanczos bisection. J. of Machine Learning Research , 10:1989 X 2012, 2009.
 Cho, Grace E and Meyer, Carl D. Comparison of pertur-bation bounds for the stationary distribution of a Markov chain. Linear Algebra and its Applications , 335(1):137 X  150, 2001.
 Das, D. and Smith, N.A. Graph-based lexicon expansion with sparsity-inducing penalties. In NAACL-HLT , 2012. Forman, George. An extensive empirical study of feature selection metrics for text classification. J. of Machine Learning Research , 3:1289 X 1305, 2003.
 Frank, A. and Asuncion, A. UCI machine learning repos-itory. http://archive.ics.uci.edu/ml, 2010. University of California, Irvine, School of Information and Computer Sciences.
 Getz, Gad, Shental, Noam, and Domany, Eytan. Semi-supervised learning X  X  statistical physics approach. In ICML Workshop on Learning with Partially Classified Training Data , 2005.
 Goldreich, Oded. A primer on pseudorandom generators , volume 55. American Mathematical Society, 2010.
 He, J., Carbonell, J., and Liu, Y. Graph-based semi-supervised learning as a generative model. In IJCAI , 2007.
 Joachims, T. Transductive inference for text classification using support vector machines. In ICML , pp. 200 X 209, 1999.
 Joachims, Thorsten. Transductive learning via spectral graph partitioning. In ICML , pp. 290 X 297, 2003.
 Johnson, Rie and Zhang, Tong. Graph-based semi-supervised learning and spectral kernel design. IEEE
Transactions on Information Theory , 54(1):275 X 288, 2008.
 Lafferty, John and Wasserman, Larry. Statistical analysis of semi-supervised regression. In NIPS , 2007.
 Melacci, Stefano and Belkin, Mikhail. Laplacian Support Vector Machines Trained in the Primal. J. of Machine Learning Research , 12:1149 X 1184, March 2011.
 Rigollet, Philippe. Generalization error bounds in semi-supervised classification under the cluster assumption. J. of Machine Learning Research , 8:1369 X 1392, 2007.
 Seneta, E. Sensitivity of finite markov chains under pertur-bation. Statistics &amp; probability letters , 17(2):163 X 168, 1993.
 Singh, Aarti, Nowak, Robert, and Zhu, Xiaojin. Unlabeled data: Now it helps, now it doesn X  X . In NIPS , pp. 1513 X  1520, 2008.
 Singh, Sameer. 2D spiral pattern recognition with possi-bilistic measures. Pattern Recognition Letters , 19(2): 141 X 147, 1998.
 Subramanya, A. and Bilmes, J. Semi-supervised learning with measure propagation. J. of Machine Learning Re-search , 12:3311 X 3370, 2011.
 Szummer, Martin and Jaakkola, Tommi. Partially labeled classification with Markov random walks. In NIPS , pp. 945 X 952, 2001.
 Wu, Xiao-Ming, Li, Zhenguo, So, Anthony M, Wright,
John, and Chang, Shih-Fu. Learning with partially ab-sorbing random walks. In NIPS , pp. 3086 X 3094, 2012. Zhou, Dengyong, Bousquet, Olivier, Lal, Thomas Navin,
Weston, Jason, and Sch  X  olkopf, Bernhard. Learning with local and global consistency. In NIPS , pp. 321 X 328, 2003.
 Zhu, Xiaojin. Semi-supervised learning literature sur-vey. Technical Report 1530, University of Wisconsin-Madison, 2005.
 Zhu, Xiaojin and Ghahramani, Zoubin. Towards semi-supervised classification with markov random fields.
Technical Report CMU-CALD-02-106, School of Com-puter Science, Carnegie Mellon University, 2002.
 Zhu, Xiaojin, Ghahramani, Zoubin, and Lafferty, John.
Semi-supervised learning using Gaussian fields and har-
