 Many keyword queries issued to Web search engines tar-get information about real world entities, and interpreting these queries over Web knowledge bases can often enable the search system to provide exact answers to queries. Equally important is the problem of detecting when the reference knowledge base is not capable of answering the keyword query, due to lack of domain coverage.

In this work we present an approach to computing struc-tured representations of keyword queries over a reference knowledge base. We mine frequent query structures from a Web query log and map these structures into a refer-ence knowledge base. Our approach exploits coarse linguis-tic structure in keyword queries, and combines it with rich structured query representations of information needs. H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Query formulation, Retrieval models
As the amount of structured data on the Web contin-ues to grow, the ability to exploit this data for keyword search becomes increasingly important. Efforts such as DB-pedia 1 , Freebase 2 , and Linked Data 3 have produced large heterogeneous knowledge bases that encode great amounts of information about many real world entities. Web search queries seeking information about these entities could be better served by interpreting the query over a knowledge base in order to provide an exact answer to the query, or to enhance document retrieval by understanding the entities described by the query.

For example, consider a user searching for  X  X ongs by jimi hendrix. X  While a text search may retrieve relevant docu-http://dbpedia.org http://www.freebase.com http://linkeddata.org ments to the query terms, it leaves the user with the task of scouring textual results in search of the information they are seeking. This user X  X  query could be better served by return-ing a list of particular songs by the musician Jimi Hendrix (precise answers to the query), possibly along with informa-tion about each song (e.g., structured facts from a knowledge base or documents resulting from a text search for the par-ticular song). Similarly, the keyword query  X  X uthor of moby dick X  could be better answered if a search system under-stood that  X  X oby dick X  describes a particular book,  X  X uthor of X  describes a relationship, and the query intent is to find the unspecified entity that has an  X  X uthor of X  relationship to  X  X oby dick. X 
We refer to the process of interpreting keyword queries over a knowledge base as semantic query understanding . This problem has the following characteristics that pose dif-ficult technical challenges.
Query understanding is a broad phrase used to describe many techniques applied to keyword queries in order to Figure 1: Overview of the query understanding pro-cess. represent the underlying information need in a way that a search system can exploit. Some popular approaches include topic classification, in which the topic of a query is deter-mined using a statistical classifier. Topic classification aids a search system by giving it context, or by allowing the sys-tem to select the appropriate data source to search. For example, the query may be classified as a music query. However query classifica-tion does not give any insight into precisely what the query is looking for within the domain of music.

Another query understanding approach that has received recent attention from researchers is term annotation. Term annotation labels individual query terms with annotations that describe the role these terms play with respect to an un-derlying information system. Following our example query, the terms  X  X imi hendrix X  may be annotated as a name , and the term  X  X ongs X  annotated as an entity type . While query annotations can aid a search system in understanding the meaning of individual terms, the annotations do not describe the underlying structure of the query as a single coherent query interpretation. Continuing the example, we want to model the latent query structure that expresses the query intention as finding entities of type song that are created by an entity named  X  X imi hendrix. X  (as opposed to finding, for example, information about the person named  X  X imi hen-drix X  who is known to have written songs, or a song named  X  X imi hendrix X  ).

The goal of semantic query understanding is to compute a formal interpretation of an ambiguous information need. Figure 1 shows our proposed semantic query understanding process for a variation of our running example information need. Our example keyword query can be structurally un-derstood as finding all entities of a type described as  X  X ongs, X  that have a  X  X y X  relationship to an entity described as  X  X imi hendrix X . This is given as the following conjunctive query. q ( x ):- X  y. SONG ( x )  X  createdBy ( x,y )  X  y = Jimi_Hendrix (2) Where the predicates SONG and createdBy , and the entity Jimi_Hendrix are part of a Web knowledge base. The dif-ficulty in mapping a keyword query to a formal interpreta-tion lies in the inherent ambiguity in keyword queries, and the many possible mappings query terms can have over very large reference data collections. For example, the term  X  X en-drix X  matches 68 data items in our data set, and the term  X  X ongs X  matches 4,219 items, and the term  X  X y X  matches 7,179 items (see Section 5 for details on the data set used for our experiments). This yields a space of over two billion possible conjunctive queries constructed by mapping each query term to a syntactically similar predicate. This does not count the possibility of additional predicates existing in the underlying conjunctive query that are not explicitly rep-resented by a term in the keyword query. There is also great variation in how queries are expressed; many Web queries are very short and underspecified. For example, the query  X  X endrix songs X  shown in Figure 1 shares the same logical structure as example query 1, even though the createdBy relation is not explicitly represented by any query terms. It is the job of the query understanding system to infer the existence of such latent relations.

In this work we propose a method for interpreting key-word queries over Web knowledge bases. We use an an-notated Web search query log as training data to learn a mapping between keyword queries and their underlying se-mantic structures. Because of the very high cost in creating training data, we design an approach that maps high level representations of keyword queries to schema-level represen-tations of structured queries, greatly reducing the amount of training data needed to accurately learn these mappings. Our approach integrates state-of-the-art techniques in nat-ural language processing with top-k search over structured data and knowledge base query processing, bridging the gap between language models for statistical representations of keyword queries and database formalisms for structured rep-resentations of information needs.
In this work, we make the following contributions. We demonstrate the viability of our proposed approach with an experimental evaluation of both the effectiveness and ef-ficiency of a prototype implementation of the system.
The remainder of the paper is organized as follows. Sec-tion 2 gives an overview of the keyword query understanding problem, outlines the basis of our formalism for modeling query intentions, and gives an overview of our approach to semantic query understanding. The details of our approach are presented in Section 3. Section 4 describes the results of our analysis of a Web query log. In Section 5 we empirically evaluate our proposal. We review relevant work in Section 6 and conclude in Section 7.
We adopt a formal model of knowledge representation as a collection of assertions based on the OWL2 EL Profile extended with unary entity sets (allowing an entity to rep-resent a class containing only itself), and with attributes that map to values in a concrete domain such as strings, numbers, or dates.

Definition 1. (Concept) A concept C is defined by the following grammar, where A is an entity type (a.k.a, entity class), R is a relation, R  X  is the inverse of relation R , e is an entity, f is an attribute, k is a constant, and u denotes concept conjunction (intersection) C ::= A |  X  R ( C ) |  X  R  X  ( C ) | C 1 u C 2 | f = k | { e } .
Definition 2. (Knowledge base) Facts about entities are represented as assertions. An assertion has one of the following forms: This first construct asserts that an entity e has type A , for example: GUITARIST ( Jimi_Hendrix ) . The second construct asserts that a relation R holds between two entities, for example: createdBy ( Foxy_Lady , Jimi_Hendrix ) . This also implies the inverse createdBy  X  ( Jimi_Hendrix , Foxy_Lady ) . The last construct asserts that an entity has a particular value for a given attribute. This is a special case of a re-lation assertion, where the entity relates to a value. For example, dateOfBirth ( Jimi_Hendrix , 1942-11-27) . The as-sertion subClassOf ( A 1 , A 2 ) is used to encode transitive class hierarchies among types.
 A knowledge base K is represented as a set of assertions. We write K | = C to denote that concept C is consistent with K and K| = C ( e ) to denote that the knowledge base K entails that entity e can be inferred to be an instance of C . We will ultimately use concepts in the given KB formalism to represent query intentions using the following definition of a search query over a knowledge base.

Definition 3. (Concept Search Query) A concept search query is given by a concept C expressed using Definition 1. The answer to the query C over a given knowledge base K is the set of entities inferred to be instances of C Our example query from Equation 2 can be modeled as the following: SONG u X  createdBy ( { Jimi_Hendrix } ) .
We can abstract a concept search query C as a conjunc-tive query  X  C consisting of u unary predicates and b binary predicates with a single distinguished variable. (This equiv-alence is straightforward, and can be seen in the relationship http://www.w3.org/TR/owl-profiles/#OWL 2 EL between assertions and concept expressions defined in Sec-tion 2.1.) Let K P denote the set of predicates occurring in a knowledge base K , and  X  C P denote the set of predicates (unary and binary) occurring in a conjunctive query  X  Let L ( P ) denote the set of textual labels that can be used to describe a predicate P . The space of possible queries  X  for a given keyword query Q =  X  q 1 q 2 ...q n  X  is given by the following expression. Equation 3 defines a space of all possible conjunctive queries with u unary predicates and b binary predicates. Equation 4 constrains all predicates to be part of the given knowledge base, ensuring the query is safe. Equation 5 ensures cov-erage of all query terms and Equation 6 ensures that the query is consistent with the knowledge base. Ideally we ex-pect that L ( P ) contains all possible representations of P , including synonyms and linguistic variations of terms (e.g., lemmatized forms). In practice, we approximate this by re-laxing the constraint q  X  L ( P ) to allow fuzzy and partial string matching.

The query understanding problem is to find the most prob-able concept search query C , subject to the constraints in Equation 3, for a given keyword query Q : such that C represents the intention of Q with respect to a knowledge base K . In the rest of this paper, we propose methods for estimating the quantity in Equation 7 by learn-ing from a Web query log.
Estimating the distribution in Equation 7 directly would require a large amount of labeled training examples. The space of possible keyword queries mapping to possible KB queries is very large, and estimating such a mapping directly is not feasible when training data is limited. Because of the manual effort required in creating labeled training data, we need to design an approach that can maximize the utility of a small collection of training examples.

Our semantic query understanding approach is summa-rized as a sequence of four steps, as illustrated in Figure 1. 1. Keyword Query Annotation Queries are first an-2. Keyword Query Structuring Annotated queries 3. Knowledge Base Mapping Semantically annotated 4. Knowledge Base Query Evaluation Concept search
The main focus of this paper is on the first two steps, we build on existing methods to address the final two steps. Note that at steps 1 and 2 of the process, we are not yet con-cerned with how query terms map to particular knowledge base items as would be done in traditional keyword search over graphs. Inferring probable structures of the query terms first will allow us to constrain the possible mappings into the knowledge base. This can improve performance by fixing the structure of possible mappings into the knowledge base, and improve effectiveness by only allowing structures that have a high probability of being representative of query intentions. Inferring query structures also gives us the ability to know precisely what piece of information is being requested, much like a projection in structured query languages.
As a first step to modeling the semantics of a keyword query, we map keyword sequences to structures represent-ing their intents, known as structured keyword queries , ad-dressing steps 1 and 2 of Figure 1. A structured keyword query describes a query in two ways: it breaks it into seg-ments that represent particular semantic constructs, and it describes how these constructs relate to each other. In or-der to form structured keyword queries from a given keyword query, we will have to address these two problems.
We define a keyword query Q to be a sequence of query terms Q = q 1 ,q 2 ,...,q n over a vocabulary V . A semanti-cally annotated keyword query assigns semantic constructs from a schema language to sequences of keyword query terms.
Definition 4. (Semantically Annotated Query) Given a vocabulary V , a keyword query Q and the knowledge repre-sentation language defined in Definition 1, a semantically an-notated keyword query AQ (or simply annotated query when clear from context) is a sequence of keyword phrase-semantic construct pairs
AQ =  X  q 1 q 2 ...q i  X  : a 1  X  q i +1 ...q j  X  : a 2 . . .  X  q where each q i  X  V and each a i  X  X  ent,type,rel,attr,val } . A semantically annotated version of our example query is given by the following. An algorithm that annotates keyword queries with semantic constructs must solve both the segmentation problem (the problem of determining the boundaries that separate multi-term phrases) and the annotation problem. We want to compute the probability of an annotated query given a key-word query, Pr( AQ | Q ).

Research has shown that part-of-speech (POS) tagging can be accurately performed over keyword queries [2]. Our approach to annotating queries exploits query terms, their POS tags, and sequential relationships between terms and tags to concurrently infer a segmentation and semantic an-notation of a part-of-speech annotated keyword query. To do this, we use a conditional random field (CRF) [12], a state-of-the-art machine learning method for labeling sequence data. As a baseline method for comparison, we also try a technique that directly classifies terms independently with semantic constructs. We then segment the query by joining any adjacent terms sharing the same semantic construct.
Baseline term classification Our baseline term clas-sification aims to exploit the relationship between part-of-speech tags and semantic constructs by using a term X  X  part-of-speech as a proxy for the term. Intuitively, there is a relationship between semantic constructs (e.g., entities, re-lations) and the parts-of-speech used in describing instances of those constructs. For example, entities are generally ex-pressed using proper nouns like  X  X imi Hendrix X  or  X  X ew York. X  Relations are often described by prepositions, such as  X  X n X  or  X  X y. X  The mapping between parts-of-speech and semantic constructs however, is not always so clear. Rela-tions can sometimes be described by parts-of-speech other than prepositions (e.g., the noun  X  X irthplace X ); nouns can often describe many different semantic constructs, such as types, relations, and attributes; and perhaps the most chal-lenging side of using part-of-speech tags to infer semantic constructs is that many entities, types, and relations are made up of multi-word phrases that can contain many dif-ferent parts-of-speech (e.g., the relation  X  X as won prize, X  or the type  X  X hancellors of Germany X ).

We model an annotated query log as set of triples L = { X  Q, X , X   X  X  i where Q is a keyword query,  X  is a function mapping query terms to POS tags, and  X  is a function map-ping query terms to semantic constructs. To classify terms via their part of speech tags, we use the standard na  X   X ve Bayes classification method. We perform na  X   X ve Bayes clas-sification by directly estimating the joint probability dis-tribution of POS tags and their semantic constructs from the query log. The conditional probability of a particular semantic construct C given a POS tag P is then the fre-quency of that query term X  X  POS tag, P, mapping to C, versus the frequency of P mapping to any semantic con-struct, Pr(C | P) = Pr(C , P) / Pr(P) which is estimated from the query log by the following frequencies.
 Pr(C | P) =
With a distribution over semantic constructs given the part-of-speech of a query term, we can estimate the prob-ability of assignments of semantic constructs to individual part-of-speech tagged query terms for a whole query. As-suming independence of query terms for tractability, this yields the following equation.

CRF feature design A CRF is an undirected proba-bilistic graphical model that models sequential data. Given a trained model and an input sequence, a CRF enables the computation of the most probable, or k most probable la-belings according to the model. Specifically, a labeling is an assignment of state labels y 1 ,...,y n , to an input sequence x ,...,x n , where each y i corresponds to a state in the model and each x i corresponds to a feature vector.

We base our CRF model on the design for noun-phrase detection proposed by Sha and Pereira [21] since our prob-lem shares similarities. For input position x i corresponding to query term q i , we define a feature vector containing the following features: all query terms and POS tags in a size five window around position x i ; all query term bigrams in a size three window; all POS tag bigrams in a size five win-dow; and all POS tag trigrams in a size five window. We include the actual query terms as features to allow impor-tant repetitive terms to be captured (e.g.,  X  X n X  describing a relation such as  X  X estaurants in barcelona X  ), but discard any generated feature that appears only once to avoid overfitting the particular terms in the training data.

We deviate from the model of Sha and Pereira in label de-sign. The labels must encode both the semantic constructs we want to annotate as well as the boundaries between multi-term semantic constructs. We create two output la-bels for every semantic construct in our chosen knowledge representation language: a  X  X egin X  (B) and a  X  X ontinue X  (C) label. This is the minimal encoding that allows us to iden-tify segmentation boundaries as well as semantic constructs. To generate training data, we label each multi-term phrase in the training data with the begin and continue labels cor-responding the phrase X  X  semantic construct. For example, the correct labeling of our running example is the following.  X  X ongs X  :type-B  X  X y X  :rel-B  X  X imi X  :ent-B  X  X endrix X  :ent-C Here, the query term  X  X endrix X  is annotated as a continua-tion of the entity starting with  X  X imi X  , yielding the following annotated query. Unlike our baseline term classification approach, the CRF model can distinguish between multiple instances of the same semantic construct occurring in succession. We train our model using the Broyden-Fletche-Goldfarb-Shanno (BFGS) algorithm, a type of hill climbing approach for solving non-linear optimization problems. To avoid overfitting we use L2 regularization. The probability Pr( AQ | Q ) is then given directly by the CRF model.
An annotated query reveals part of the latent structure of an entity-based keyword query by indicating the seman-tic role represented by various parts of the query. However query annotation alone does not describe how these various recognized semantic constructs interact to model the under-lying query intention. In our running example query, we know (after annotation) that the query contains a type, a relation, and an entity. However there is still ambiguity in what the query is seeking. Is it ultimately describing entities of the given type that are related to the given entity? Or is it seeking information about the given entity within the context of the given type?
To illustrate the ambiguity in query structure, consider the following two queries:  X  X ohn smith dentist X  and  X  X ew york restaurants X  . Both queries contain an entity followed by a type. The first query seeks information about the given entity (  X  X ohn smith X  ), with a type (  X  X entist X  ) given as context to disambiguate among possible interpretations of the entity. Whereas the second query is seeking instances of the given type (  X  X estaurants X  ), within the context of the given entity (  X  X ew york X  ).

To model high level query structure, we follow the logical connectives from the knowledge representation language.
Definition 5. (Structured Query Template) A struc-tured query template T is a schema-level description of a concept search query, expressed in the following grammar where node  X  X  ent,type,val } and edge  X  X  rel,attr } . A structured query template describes the overall graph structure of the query as well as the node and edge types of the query predicates. For example, the structured query template for our example query is type u rel ( ent ) .
We want to estimate the probability of a query template given a semantically annotated keyword query, Pr( T | AQ ). We assume access to an annotated query log containing both semantically annotated queries and their structured query templates, L = { ( AQ i ,T i ) } .

Our structuring approach directly estimates the probabil-ity of a structured template given an annotated query by aggregating over all queries in the training log that share the same high-level summary of semantic annotations.
Definition 6. (Semantic Summary) Given an anno-mary is an ordered list of semantic constructs occurring in AQ , and is given by the function S : AQ  X  C n s.t. S ( AQ ) =  X  a 1 ,a 2 ,...,a n  X  For example, S (  X  X ongs X  :type  X  X y X  :rel  X  X imi hendrix X  :ent) =  X  type,rel,ent  X  .

We directly estimate Pr( T | AQ ) from labeled training ex-amples in our query log from the definition of conditional probabilities, using the semantic summary as a high level representation of the annotated query.
 Pr( T | AQ ) = The probability of a query template given an annotated query is estimated by the proportion of queries with the same semantic summary that are structured using that tem-plate, versus the total number of queries with the same se-mantic summary and any structuring.
Combing an annotated query with a structured query tem-plate yields a structured version of the keyword query. Com-bining the annotated running example query with its tem-plate gives the following query  X  X ongs X  u  X  X y X  (  X  X imi hendrix X  ) by swapping the annotated constructs into their respective positions in the template. We allow relations and attributes POS Ex. Query Token Sem Freq.
 Proper noun waterloo 77.1% 28.7% ent 99.1% type 0.9%
Noun musician 42.6% 15.9%
Plural noun songs 13.6% 5.1% Adjective big 11.6% 4.3% ent 60.0% type 40.0% Preposition in 7.8% 2.9% rel 55.6% ent 44.4% Number 2008 6.6% 2.5% ent 53.8% val 46.2% URI yahoo.ca 5.0% 1.9% ent 100% Verb run 4.7% 1.7% ent 100% Determiner the 3.1% 1.2% ent 100% Gerund winning 2.7% 1.0% rel 66.7% type 33.3% Figure 2: The ten most frequently occurring part-of-speech tags among entity-based queries, with the distribution of how frequently that tag mapped to various semantic constructs. to be unmapped (e.g., see Figure 1), however we do not consider mappings in which entities or types are unmapped.
This type of expression is called a structured keyword query , and methods to map these expressions into Web knowledge bases have been developed [18]. The method proposed in [18] first generates a list of possible KB items for each keyword phrase in the query. These items are sorted by syntactic similarity, forming ordered input lists. The inputs are then processed using a variation of a top-k threshold algorithm.
In this work we extend the the method proposed in [18] in order to support the class of query structures discovered in our query log analysis. Our variation of structured keyword queries is given by the following grammar.
 SKQ ::= k | k ( SKQ ) |  X  ( SKQ ) | SKQ 1 u SKQ 2 Where k is a sequence of keywords and u denotes conjunc-tion. This grammar follows the knowledge representation language defined in Section 2.1. We extend the original structured keyword query framework by adding the con-struct  X  ( SKQ ) which allows nested queries with unknown relations. For this construct we must consider all possible relations in the knowledge base. We accomplish this by mod-ifying the top-k algorithm used in [18] to dynamically add all relations that have non-zero knowledge base support to any KB item in adjacent input lists. We also add support to the underlying KB formalism to account for inverse relations and an explicit model of attributes.

Given a structured keyword query SKQ , the KB map-ping system will return the most probable (or the top-k most probable) concept search query C = argmax C 0 Pr( C 0 | SKQ ). The probability of a structured keyword query (represented as a semantically annotated query and a query template SKQ =  X  AQ,T  X  ) is given by Pr( SKQ | Q ) = Pr( T | AQ )  X  Pr( AQ | Q ). We can compute the first term using the tech-Figure 3: The ten most frequently occurring tem-plates among entity-based queries. niques discussed in Section 3.1 and the second term using the method presented in Section 3.2. Our overall goal is to approximate the probability distribution given in Equa-tion 7. Given a keyword query Q , the concept search query C representing the intent of Q is estimated as: for some structured keyword query SKQ . Our produced concept search queries satisfy the problem definition in Equa-tion 3. The queries are safe since all predicates are retrieved from the KB, they cover all query terms by only considering complete mappings into the KB, and they are guaranteed consistent as we evaluate resulting concept search queries and only include those that are non-empty.
We analyzed a sample of keyword queries available from the Yahoo WebScope program [25]. We inspected queries keeping only those that have a semantic construct as the pri-mary query intention, following the classification proposed in [19]. We annotated 258 queries with the part-of-speech tags and semantic constructs. We did not consider mis-spelled, non-English, or other queries that were not clearly understandable. Among the annotated queries, 156 queries had some semantic construct as their primary intention ( entity-based queries ). That is approximately 60% of queries having a semantic construct as the primary intent of the query. This is consistent with the analysis done in [19] which was per-formed over a different Web query log and reported 58% of queries having a semantic construct as the primary intent.
Our part-of-speech tag set is based on the analysis in [2], which is a reduced tag set designed for annotating Web queries. Figure 2 shows the distribution of part-of-speech tags over query tokens as well as the percentage of entity-based queries that contain that part-of-speech. The figure also illustrates the distribution of terms with the given part-of-speech over semantic constructs. This distribution cap-tures the relationship between part-of-speech tags and se-mantic constructs, which plays a key role in our annotation methods described in Section 3.1. Not surprisingly, there is a very strong correlation between proper nouns and entities. Interestingly, plural nouns are a strong indicator of a term representing a type , while regular nouns are split between denoting type and entity labels.

From the 156 entity-based queries, we also annotated the structured query template underlying our interpretation of the intent of the query. In the cases where there were multi-ple possible structurings, we annotated all of them and count them as individual queries when computing the frequencies for the equations described in Section 3.

Figure 3 shows the ten most frequent structured query templates in our training query log, along with the percent-age of queries exhibiting that structure (over all entity-based queries). Many queries tend to repeat the same structures.
We implemented all of the techniques described in Sec-tion 3. We use CRF ++ 5 to learn the CRF models and use the tool for mapping structured keyword queries into a KB described in [18] with the modifications described in Sec-tion 3.3. We consider the top-10 structures (unless otherwise specified) and union the results of the best KB mapping for each structure. For unary queries (not having multiple terms to perform disambiguation) we employ a heuristic requiring 0.95 syntactic similarity for an interpretation to qualify.
We create two system configurations, the first using the na  X   X ve Bayes (NB) term classifier for query annotation and the second using the CRF approach (CRF). Both systems use the direct approach (Drct) to structuring (Section 3.2) and the same KB mapping tool described in Section 3.3 to map the computed structured keyword queries into the knowledge base and evaluate the resulting concept search queries. We omit structures containing more than one un-known relation to avoid the exponential blow-up caused by matching over all possible pairs of relations. We have found this does not have a significant impact on the quality of re-sults. We implement a simple POS-tagger that builds on the Brown and Wall Street Journal POS tagged lexicon. Our tagger assigns the most frequent POS tag for each known word (suitably mapped to our reduced tag set as described in Section 4), and assigns proper noun to unseen tokens (the most frequent tag in our query log). We find this to be suf-ficiently accurate for our purposes. Building and evaluating a more sophisticated POS tagger is an orthogonal problem beyond the scope of this paper.

For comparison, we also implement two alternative ways of mapping keyword queries into knowledge base entities. The first method implements traditional keyword search over graphs, with a number of heuristics taken from the litera-ture. The process of keyword search over graphs is to find all nodes and edges that match query terms, then search among these seed matches to see if they can be connected. A result graph is a subgraph of the data graph that spans all query terms. This can be viewed as an instance of the Steiner tree problem. Our implementation uses a breadth-first search approach to finding Steiner trees. We use the same Lucene index and graph database used by the KB mapping tool. Seed nodes are processed in order of syntactic similarity to the query term they match. The graph search will gener-ate a given parameter number of results and return the k highest scoring. The parameter is set to 10,000 in the ef-fectiveness experiments and is set to 10 in the performance experiments. Scoring is based on the same syntactic scoring (3-gram distance) used by the KB mapping tool, normalized by the answer graph size to promote more compact answers.
The heuristics used in our graph search implementation include traversal of outgoing edges only, incoming edges http://crfpp.sourceforge.net/ only, bi-directional edge traversal [10], search depth limit-ing as recommended in [24] (we use a depth of three), and a last heuristic that forces type nodes to be leaves in an-swer graphs. Note that this system returns subgraphs of the knowledge base, and cannot precisely interpret queries in order to find specific answers . In our evaluation, we give this system the benefit of considering any answer graph as correct if it contains the answer anywhere in the graph.
The second comparison approach builds a text representa-tion of each node in the knowledge base, then performs tra-ditional keyword search over the text representations. We use Lucene to create an index over all labels in a one hop radius around each node (including edge labels).
We use YAGO [22] as our knowledge base, a high quality fact collection with a rich type hierarchy over entities and a part of the Linked Data Web. Our training data is the Yahoo query log described in Section 4. To evaluate our approaches, we use both queries from the query log and a hand-crafted workload designed with specific properties we wish to evaluate. The query workload consists of 96 queries, half of which have an underlying query intention that can be modeled with the data in YAGO, and half taken from the Yahoo log that describe data not occurring in YAGO. The half that have YAGO answers, called positive queries , are created to exhibit the 12 most frequent structures found in our analysis, accounting for over 90% of all entity-based queries. We create four examples of each of these struc-tures. The second half are entity-based queries taken from the Yahoo log for which we have manually verified that no interpretation exists in YAGO. We refer to these queries as negative queries .

The gold standard result for positive queries is defined by manually constructing a structured query over YAGO, and the correct answer for negative queries is defined as the empty set. This query workload explicitly exercises both of the problems of interpreting queries when possible, and rec-ognizing when an interpretation is not possible. We man-ually create the queries to ensure all structures are repre-sented, and to control for KB coverage. Finding examples of queries with all of the desired structures in a Web query log, such that they also have intentions existing in YAGO would require a huge manual effort, and possibly a larger query sample. Existing benchmarks also do not capture all of the structures we want to evaluate while controlling for KB coverage. The positive queries from the workload are available online 6 .
To evaluate effectiveness we conducted a direct evaluation of the query annotations, an evaluation of the computed KB interpretations, and an end-to-end evaluation of the full system in finding exact answers to keyword queries.
Precision is defined as the fraction of returned results that are correct. Recall is defined as the fraction of all possible correct results that get returned. MRR measures the (recip-rocal) average rank where the (first) correct answer occurs. Given a set of queries Q and a function rank ( i ) that returns the rank of the first correct answer for query i , MRR is given by the following. http://cs.uwaterloo.ca/  X  jpound/cikm2012/workload.txt System k=1 k=5 k=10 k=1 k=5 k=10 CRF 0.461 0.703 0.832 0.626 0.797 0.923 NB 0.432 0.500 0.547 0.616 0.711 0.736 Figure 4: Average number of queries with the cor-rectly annotated query occurring in the top-k anno-tations (query recall), and average number of cor-rectly annotated tokens for the best annotation oc-curring in the top-k (token recall).
 For all measures, we count a value of 0 if a system does not return any result for a positive query or if any result is returned for a negative query.
Figure 4 shows the results of both the NB and CRF anno-tation approaches. Results are averages using 10-fold cross validation over the 156 entity-based queries from the Yahoo query log. We measure average query recall, the frequency in which the top-k annotations of a query generates the cor-rect complete annotation. The figure also shows average token recall for the best annotation in a query X  X  top-k an-notations. This is the fraction of query terms annotated correctly for the best annotation produced for each query. This is relevant because our structuring and KB mapping tools are tolerant to errors, meaning an incorrect segmenta-tion or annotation can still produce a correct concept search query. While both approaches perform similarly at k =1, the superiority of the CRF approach is clear as we consider the top-5 to top-10 annotations. There are four possible outcomes when interpreting a query. The first is that the query is interpreted over the KB, and the interpretation is correct (true positive); the second is that the query is interpreted but the interpretation is in-correct (false positive); the third is that the query does not get interpreted though an interpretation does exist in the data (false negative); and the last is that the query does not get interpreted because no interpretation exists in the data (true negative). Figure 5 shows the confusion matrices for these outcomes for both the CRF and NB approaches when considering the top-10 structures over the workload outlined in Section 5.2. The figure shows that the CRF approach is superior with respect to both types of error, and exhibits a higher overall true positive rate (correct interpretations) than the NB approach. Overall the CRF-based approach produces a correct interpretation or recognizes an uninter-pretable query almost 93% of the time. We found the CRF X  X  ranked list of query interpretations to have an MRR of 0.698 on the positive query set. This means that the correct in-terpretation is found generally in the first or second position on average. The NB approach had an MRR of 0.589, also slightly better than the second position on average.
We used the workload outlined in Section 5.2 to evaluate the ability of the proposed approaches to interpret and an-
CRF.Drct f (Incorrect) t (Correct) Total +/  X  + (Interp) 3 (3.1%) 45 (46.9%) 48 (50%)  X  (No Interp) 4 (4.2%) 44 (45.8%) 48 (50%) Total f/t 7 (7.3%) 89 (92.7%)
NB.Drct f (Incorrect) t (Correct) Total +/  X  + (Interp) 3 (3.1%) 36 (37.5%) 39 (40.6%)  X  (No Interp) 12 (12.5%) 45 (46.9%) 57 (59.4%) Total f/t 15 (15.6%) 81 (84.4%) Figure 5: Confusion matrices for the query interpre-tation task. Raw counts and percentages are shown for the true/false positive/negatives, as well as the marginals. The 96 query workload contains 48 pos-itive queries and 48 negative queries.
 Figure 6: Average Precision, MRR, and Recall over the combined positive/negative query workload. swer keyword queries. Figure 6 shows the results of each of the systems, as well the best run for each measure by any of the graph search system X  X  configurations. The su-perior CRF-based annotator paired with the the semantic summary-based structuring approach has the best perfor-mance across all metrics. While the NB approach is com-parable, the improved annotation accuracy of the CRF ap-proach yields better results. The graph search approach achieves reasonable recall and MRR, meaning it can find many correct answers and return good answers at decent ranks, however the answer set also becomes polluted with many non-relevant results hurting precision. The approach has no capacity to determine which results are proper an-swers and which are coincidental keyword matches. Also, the graph search is given the advantage of not having to find the precise answer to queries (any answer graph containing the query answer is considered correct). Not surprisingly, the text graph approach has very low precision as it is de-signed to find resources related to the query terms and does not attempt to interpret queries in order to find exact an-swers. Precision is never higher than 0.1 for any value of k . While the unbounded text graph approach achieves 97% recall on the positive query set, it has no means of detecting the negative queries and answers each incorrectly, bringing down the average considerably.
Precision and recall can be traded off for run-time perfor-mance by varying the number of candidate structures. Fig-ure 7 shows the effect of varying the number of structures considered from 1 to 10 for the CRF.Drct approach. As the data points move from left to right, the total number of structures is increasing. This gives an increase in precision and recall as we explore more of the search space to find the correct query interpretation, but at the expense of greater Figure 7: Average precision and recall vs. average run time for varying number of KB interpretations. run times. Occasionally an incorrect interpretation is found causing a dip in precision. It is important to note that the lower precision configurations of our system are most often due to queries not being answered , as opposed to being an-swered incorrectly. The highest precision (0.789) is obtained for a configuration that takes 1.5 seconds per query on av-erage. An average precision of approximately 0.74 can be achieved in sub-second query processing time.

Figure 8 shows a breakdown of the average run-time for each component of the system when considering the top-10 structures. The actual annotation and structuring time accounts for only a small percent of the total run time, with the majority of time spent mapping candidate structures into the knowledge base. The graph search approach ( k = 10) took around 67 seconds on average, often hitting an imposed two minute timeout. This is often due to the graph search attempting to walk the entire search space if it is unable to find k results. The text graph approach is very efficient at k = 10, but at the cost of effectiveness.
Tran et al. [24] have explored mapping keyword queries into conjunctive queries by matching terms against knowl-edge base items and searching for connections among the matches. This approach is similar to our graph search ap-proach used in the evaluation, though their top-k algorithm may explore less of the total search space than our BFS approach. Zhou et al. have also proposed a similar data driven approach [26]. These techniques are in contrast to our query-log driven approach which elicits structures from the information needs of real users. Lei et al. use manually defined templates to map query terms into formal structured queries over a knowledge base, and employ heuristics to re-duce the search space when the query contains more than two terms [13]. Fagin et al. [5] use user specified gram-mars and vocabularies to annotate query terms with type and attribute labels. Our proposed approach could possibly be used to learn these types of grammars from a query log.
Many recent works have also looked at annotating individ-ual terms in a keyword query with various levels of semantic meaning, such as part-of-speech tags [2], and attribute la-bels from a relational schema [20]. These works do not ad-dress how terms relate to form single coherent expressions of the underlying query intent. Manshadi and Li have also proposed query interpretation using pre-defined grammars. They also annotate terms that specify operations such as  X  X ortOrder X  [17]. Li proposed a method of determining the intent head of a query, the part of the query that represents what the user is looking for (e.g.,  X  X ongs X  in our example System Struct Map Exec Total CRF.Drct 0.002s 1.23s 0.32s 1.55s
NB.Drct 0.001s 0.94s 0.29s 1.24s graph search ---67.27s text graph, k = 10 ---0.33s text graph, k unbounded ---17.33s Figure 8: Avg. time for annotation and structuring (Struct), KB mapping (Map) and execution (Exec). query) [14]. Our approach implicitly identifies the intent head of the query by the root of the concept search query. Agarwal et al., mined query templates from a query log [1]. Templates would generalize terms that match KB items, for example  X  X obs in #location X  . These templates were used to find relevant websites based on clicks. Cheng et al. allowed user queries to specify a similar template, which explicitly specifies the intended return type [4]. Our template mining is similar in nature that of Agarwal, though our templates fully specify semantic structure and not just types occurring in keyword queries. Also, we do not rely on input query terms exactly matching a set of known types, but instead match against syntactically similar types and resolve am-biguity from the query context. Our Web knowledge base setting also yields over 250,000 possible types.

Search over relational data can be viewed as a form of semantic query understanding. Query terms are mapped into structured data producing structured queries that can be used to retrieve data that contains the queried terms. There is a long line of research for search over structured data, starting with the DISCOVER [8] and BANKS systems [9] that aim to efficiently find tuples that span all query terms, as well as extensions that aim to integrate IR style ranking [7] or augment the graph traversal algorithm [10]. The heuristics used in our graph search approach are based on this line of research. Tata et. al, extend keyword search over structured data to include aggregates and top-1 queries [23]. Queries are translated to SQL with the allowance of various aggregation keywords such as num or avg .

Blanco et al. have used relevance based retrieval to search for nodes in large knowledge bases [3]. This approach how-ever does not attempt to interpret the semantics of queries, and thus does not necessarily provide exact answers.
Semantic parsing of natural language questions shares a similar high level goal with semantic query understanding. Both tasks are to construct a logical representation of a query. Semantic parsing aims to parse natural language queries to lambda calculus expressions of the intention (see for example [15]). Semantic parsing approaches depend on fully specified domain-relevant questions, as opposed to short underspecified keyword queries. Also, these approaches are applied to small controlled domains with relatively small homogeneous knowledge bases, as opposed to the massive heterogeneous knowledge bases found on the Web.

Exploiting Web knowledge bases for question answering (QA) has also been explored. Fernandez et al. propose the PowerAqua system [6, 16] that integrates a front-end QA system on a semantic search back-end to answer questions over Linked Data knowledge bases. Katz et al. have built the OMNIBASE+START system that answers questions us-ing a knowledge base extracted from the Web [11]. While QA has a similar high level goal to our task, QA approaches generally depend on either having properly formatted ques-tions that can be parsed to take advantage of the grammat-ical structure, or they depend on having the relatively large amount of text (as compared to keyword queries) from the question in order to match against answer candidates.
We have shown how keyword queries can be interpreted over large scale heterogeneous Web knowledge bases by learn-ing semantic structures from an annotated query log.
Future directions include exploring refinements on the gran-ularity of semantic annotations. For example, distinguishing locations from other types of entities could provide addi-tional structuring hints, since locations often appear as con-text information in keyword queries. We are also interested in applying these techniques to other data sets and search verticals such as product and medical knowledge bases. [1] G. Agarwal, G. Kabra, and K. C. Chang. Towards rich [2] C. Barr, R. Jones, and M. Regelson. The linguistic [3] R. Blanco, P. Mika, and S. Vigna. Effective and [4] T. Cheng, X. Yan, and K. C. Chang. EntityRank: [5] R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and [6] M. Fernandez, V. Lopez, M. Sabou, V. Uren, [7] V. Hristidis, L. Gravano, and Y. Papakonstantinou. [8] V. Hristidis and Y. Papakonstantinou. Discover: [9] A. Hulgeri and C. Nakhe. Keyword searching and [10] V. Kacholia, S. Pandit, S. Chakrabarti, S. Sudarshan, [11] B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. J. Lin, [12] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [13] Y. Lei, V. S. Uren, and E. Motta. SemSearch: a search [14] X. Li. Understanding the semantic structure of noun [15] P. Liang, M. I. Jordan, and D. Klein. Learning [16] V. Lopez, M. Fernndez, E. Motta, and N. Stieler. [17] M. Manshadi and X. Li. Semantic tagging of web [18] J. Pound, I. F. Ilyas, and G. Weddell. Expressive and [19] J. Pound, P. Mika, and H. Zaragoza. Ad-hoc object [20] N. Sarkas, S. Paparizos, and P. Tsaparas. Structured [21] F. Sha and F. Pereira. Shallow parsing with [22] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A [23] S. Tata and G. M. Lohman. SQAK: doing more with [24] T. Tran, H. Wang, S. Rudolph, and P. Cimiano. Top-k [25] Yahoo! Academic Relations. [26] Q. Zhou, C. Wang, M. Xiong, H. Wang, and Y. Yu.
