 Society is experiencing exponential growth in the number and variety of data col-lections containing person-specific information as computer technology, network connectivity and disk storage space become increasingly affordable[9]. Many data holders publish their microdata for different purposes. However, they have diffi-culties in releasing information which does not compromise privacy. The difficulty is that data quality and data privacy conflict with each other.

Recently, a new approach of protecting d ata privacy called k-anonymity[8] has gained popularity. In a k -anonymized dataset, quasi-identifier attributes that leak information are suppressed or generalized so that, each record is indistin-guishable from at least ( k -1) other records with resp ect to the quasi-identifier. Since the k -anonymity is simple and practical, so a number of algorithms have been proposed[5][6].

The objective of this paper is to develop a new approach to achieve k -anonymity, where quasi-identifier attribute values are clustered and then published with these clusters. We view the k -anonymity problem as a clustering issue, and we add a constraint that each cluster contains at least k records, so that it satisfies k -anonymity requirements. The key idea i s to cluster records based on density which is measured by the k -Nearest-Neighbor distan ce. We develop an algorithm to come up with such a clustering. To measure the information loss, we give some data quality metrics which are suitable in both numeric and categorical attributes.
 eralize/suppress the quasi-identifiers by which most individuals may identified[8], and finally release the modified dataset which satisfies the k -anonymity constraint. For example, Table 1(left) is a raw mircodata of hospital and Table 1(right) is a 2-anonymity view of (left).
 Definition 1 (Quasi-Identifier). A quasi-identifier is a minimal set Q of at-tributes in table T that can be joined with external information to re-identify individual records(with sufficiently high probability)[8].
 Definition 2 (Equivalence Class). An equivalence class of a table with re-spect to the quasi-identifier is the set of all records in the table containing iden-tical values for the quasi-identifier attributes.
 For example in Table 1, the attribute set { Zip, Gender, Age } is the quasi-identifier. Record 1 and 2 form an equival ence class in Table1(b), with respect to quasi-identifier { ZIP, Gender, Age } and their corresponding item values are identical.
 Definition 3 ( k -Anonymity). Table T is said to satisfy k-anonymity if and only if each set of values in Q appears at least k times in T[8].
 For example, Table 1(b) is a 2-anonymity view of Table 1(a) since the minimum size of all equivalence classes is great than 2. So it can ensure that even though an intruder knows a particular individual is in the k -anonymous table T ,hecan not infer which record in T corresponds to the individual with a probability greater than 1 /k .

Clustering techniques used in k -anonymity issue do not require the number of clusters; instead, they need to satisfy a constraint that each cluster contains at least k records[1][3]. We define k -anonymity clustering issue as follow: Definition 4 ( k -Anonymity Clustering Issue). The k-anonymity clustering issue is to cluster n points into a set of clu sters with an information loss metric, such that each cluster contains at least k ( k  X  n ) data points and that the sum of information loss for all clusters is minimized.
 The distance metrics measure the dissimilarities among data points and minimiz-ing the information loss for published microdata is the objective of anonymiza-tion issue. Distance metrics should handle records that consist of both numeric and categorical attributes. The earlier works[5][6] described generalizations for a categorical attribute by a taxonomy tree. Consider some sample in Table 2 and a taxonomy tree of attribute workclass in Fig.1. The leaf nodes depict all the distinct value of attribute workclass . These leaf nodes can be generalized at next level into self-employed , government ,and unemployed . The level of a leaf node is 0 and the level of a root node is h w , based on the notion tree height, [3] gives a distance definition between two categorical values.

The priority of generalization should be considered such that the generaliza-tion near to the root should give greater information loss compared with the generalization far from the root[7]. Thus we reformulate the level weight scheme based on [3]. We define the weight distance between two categorical values as follow: Definition 5 (Weight Distance Between Two Categorical Values). Let C be a categorical attribute, and let h w be the height of weight taxonomy tree of between two values v i ,v j  X  C is defined as: where l 12 is the level of the closet common ancestor of v 1 and v 2 . For example, the weight distance in Fig.1 between of Federal and Local is 1 / (1 + 2) = 0 . 33, while the distance between Inc and Without pay is (1 + 2) / (1 + 2) = 1.
Generalizing a numeric attribute (such as age in Table 2) is done by discretiz-ing values into a set of disjoint intervals. How to choose possible end points determines the granularity of the intervals. Intuitively, the difference between two numeric values indeed repr esents their distances on the k -anonymity clus-tering problem. We define the distance between two numeric values as follow: Definition 6 (Distance between Two Numeric Values). Let N be a finite numeric attribute domain. The distances between two numeric values v 1 , v 2 is defined as:[3] where | N i | is the size of numeric attribute | N i | .
 For example, we consider the Age attribute in Table 2. The distance between the first two records in the Age attribute is | 37  X  22 | / | 54  X  18 | =0 . 42. Definition 7 (Distance between Two Records). Let C 1 ,C 2 ,...,C m ,N 1 ,N 2 ,...,N n be the quasi-identifier attributes in table T. C i ( i =1 ...m ) is the cat-egorical attribute and N j ( j =1 ...n ) is the numeric attribute. The distance between two records is defined as: distance ( r 1 ,r 2 )= For example, the distance between th e first two records from Table 2 is 1 / 3+ 0 . 25 = 0 . 58.

Based on the above distance definition between records, information loss for the anonymous table can be defined as follow: Definition 8 (Information Loss). Let C 1 ,C 2 ,...,C m ,N 1 ,N 2 ,...,N n be the where ilC i is the information loss for categorical attribute C i and ilN j is the information loss for numeric attribute N j . v all is the value of the closest common loss of cluster c.
 Thus, the total information loss of all clusters for the released microdata is: where R is a set of clusters. The choice of cluster center points can be based on the distribution density of data points. We pick a record whose density is the maximal and make it as the center of a cluster center c . Then we choose k-1 records to c that make the information loss minimal. We note that there are two important issues in the algorithm: 1. The effect of clustering. We introduce a density metric called k -nearest-neighbor distance which is defined as follow: Definition 9 ( k -Nearest-Neighbor Distance). Let R be a set of records and follow: where | R | represents the size of R.
 Definition 10 (Density). Let distKNN ( r ) be the k-nearest-neighbor distance of record r, we define the density of r as follow: The larger the density of r is, the smaller the distances between r and other records around it are. The r ecord with larger density will be made as a cluster center with high probability because the cluster has a smaller information loss. 2. The process of clustering. How to choose the next cluster center is another important issue when one iteration has finished, because we consider that the next cluster center is a record which has the maximal density in remainder records. And the next cluster center is not in the k -nearest-neigh bor records of this center, thus we define a principle as follow: Definition 11 (Principle of Choosing the Next Cluster Center). Let R be a set of records, r c be a center of cluster c and r c Next be the next cluster center. The r c Next  X  X  R-c } chosen must satisfy the follow two requirements at thesametime: So we propose an algorithm called density-based k-anonymity clustering (DBKC). We provide the pseudo code of the algorithm as follow: Density-Based K-Anonymity Clustering (DBKC) 1: compute density of each record in R and sort all records in a decrease order according to density; 2: choose the first records r (with the maximal density) in R and make it as a cluster e 1  X  X  center; 3: while the size of R &gt; k ,do 4: delete r from R ; 5: find k -1 best records in R and add them to cluster e 1 and delete them from R ; 6: find the next cluster center r in R and make it as a new cluster e 1  X  X  center; 7: end while; 8: while the size of R &gt;0,do 9: insert each remainder record into best cluster; 10: end while;
In line1-2, we compute the density of each record and sort them. The density of each record is computed with Definition 10. Sorting algorithm chosen here is quick-sort()[4] because its time complexity is smallest.

Line3-7 we form one cluster whose size is k in each iteration . For one cluster center, we find k -1 best records to add them to cl uster in line 5. The best record cluster center according to Definition 11.

After all iterations in line3-7, there are fewer than k records in R and these remainder records will be handled in line 8-10. We insert each remainder r j into the best cluster in line 9. The best cluster here is a cluster e 1 from the set of clusters formed in line3-7 such that IL ( e 1  X  r j ) is minimal.
 For the sake of space, we do not provide the source codes of DBKC algorithm. We analysis the time complexity based on the source codes. Computing the density of all records in R needs O (( k +log k +1) n 2  X  O ( n 2 )(when k n ); Sorting all records with quick-sort() needs O ( n log n ). In line 3-7, the execution 8-9 need fewer than k passes. As a result of analysis above, the time complexity of density-based k -clustering algorithm is O ( n 2 ), when k n .
 For experiments, we adopted the Adult dataset from the UC Irvine Machine Learning Repository[2]. Before the experiments, the Adult dataset was prepared similarly to[1][6]. Eight attributes were chosen as the quasi-identifier and two of them were treated as numeric attributes while the other were treated as cate-gorical attributes.

We evaluate the algorithm in terms of two measurements: information loss and execution time, and compare DBKC algorithm with the k -means which was added only one constraint that each cluster contains at least k records. Fig.2 reports the results of these algorithms and shows that the total information loss in DBKC algorithm is 2.82 times lower than that in k -means algorithm for all k values on average. This result can be explained with the following reasons. First, the choice of the cluster center points in DBKC algorithm is based on the density, while the k -means algorithm used in our experiments chooses center points randomly. Secondly, DBKC algorithm chooses the closest point to the cluster in order to make information loss lowest, while k -means algorithm chooses the point to a cluster in order to make the distance between center point and it the shortest.

As shown in Fig.2, the execution time of both algorithms decreases with the value of k . Although the execution time of the DBKC algorithm is larger than that of k -means algorithm, the time complexity of DBKC algorithm is O ( n 2 )(as discussed in Section 4)and that of k -means algorithm is also O ( n 2 ). The execu-tion time of DBKC algorithm is acceptabl e in most cases considering its better performance on information loss, but it is not fully optimized and this is our future work.

The experiment result shows that th e DBKC algorithm is acceptable on in-formation loss and execution time. It is feasible to solve k -anonymity on using clustering methods based on density. In this paper, we study the k -anonymity as a clustering problem and propose an algorithm based on density. We define the distance and information loss metrics, especially we discuss the advantage of weight distance in categorical attributes. We experimentally show that our algorithm causes significantly less information loss than traditional k -means clustering algorithm and we analyze the difference between two algorithms.

Our future work includes the following. Although the experiment result shows that DBKC algorithm has a better compromise between data quality and data privacy conflict. We believe that we can improve DBKC algorithm on time com-plexity. The key idea of DBKC algorithm is based on the density and we use k -nearest-neighbor dista nce to measure it, there may be a better density metric emergence in the future work. Because k -anonymity ensures relatively weak pri-vacy protection, DBKC method should co nsider new privacy requirements such as l -diversity, personalized privac y preservation etc. in future.
 Acknowledgement. This work was supported by NSFC 60673140 and NORPC 2004C B719400.

