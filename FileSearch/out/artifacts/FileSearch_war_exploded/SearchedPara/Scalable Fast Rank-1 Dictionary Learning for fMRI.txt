 It has been shown from various functional neuroimaging studies that sparsity-regularized dicti onary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem ha s been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) m odel and apply it for fMRI big data analysis. The model estimate s one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We impl ement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed datase t (RDDs) abstracts for the data distribution and operations. Experi mental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus en abling data-driven neuroscientific discovery from massive fMRI big data in the future. methodologies~Machine learning Algorithms, Performances fMRI; Sparse coding; Distributed computation; Algorithm parallelization In recent years, the field of neuroimaging studies based on functional magnetic resonance imaging (fMRI) has featured unprecedented large-scale data availability thanks to the efforts from a series of data collection works including Human Connectome Project [26], 1000 Functional Connectomes [19] and OpenfMRI Project [20]. The rapi dly growing data characterized different aspects of brain cognitive processes as well as various disorders, thus providing a great opportunity for decoding and identification of potential functional biomarkers for brain diseases. Consequently, there is an urgent call for more efficient and scalable data analytics and knowledge discovery methods, especially for dealing with fM RI big data. Functional network analysis has become an impor tant and popular approach for discovering the underlying organiza tion structures and meaningful dynamic patterns from the vast and noisy functional brain signals. Focusing on understanding the functional aggregation/co-activation among brain regions th rough quantitative and data-driven approaches, researchers have employed various types of matrix decomposition methods for the functional network analysis studies, including Independent Component Analysis (ICA) [24], Principal Component Analysis (PCA) [23] and Dictionary Learning [11]. Dictionary learning in particular has been shown to be a powerful tool in image compressed sensing [21], classification [14] and de-noisi ng [8], and has shown superior performance in decom posing the meaningful and comprehensive functional networks from various types of fMRI signals [15]. However, the computational cost for solving the sparse coding problem for dictionary learning has been known to be very demanding [12], especially when dealing with large-scale data sets. Furthermore, most of current dictionary learning methods for fMRI data analysis are only implemented for local application without any parallel ization scheme. Facing with the rapidly growing fMRI data and the need s for population-level analysis with terabytes or even petabytes of data size [23], the computation power limit of a single machine will eventually become the bottleneck for efficient and ef fective knowledge discovery from the fMRI big data. Following the previous success in using dictionary learning for functional network decomposition [15], in this work we devolved a novel distributed rank-1 dictionary learning (D-r1DL) model, leveraging the power of distributed computing for handling large-scale fMRI big data. Compared to the gradient-based dictionary learning algorithms such as the on line dictionary learning [18] (based on stochastic gradient descent) and the K-SVD [4] (based on gradient descent), the proposed rank-1 dictionary learning algorithm has a few critical advant ages: 1) The learning process is a fix-point algorithm by alternating least squares updates, thus avoiding the tuning of the learning rate/step size while also avoiding the slow convergence n ear the solution; 2) The memory cost of the proposed algorithm is very low because it needs not to maintain the potentially large gradient matrix in the memory. The intermediate results will be disc arded after each rank-1 basis is learned and stored, which further reduce the memory cost. More importantly, the rank-1 dictionary learning algorithm is very light-weighted regarding to the operational complexities: besides the input data, most of the routines in the algorithm will only take one vector as input and one vector as output. This feature helps the r1DL algorithm to be easily paralleli zed to its distributed version. For the algorithm parallelization, in this work we used the Spark engine [3] to implement the D-r1DL algorithm. Spark is a high-performance distributed compute en gine for large-scale data processing. It is similar to MapR educe, but has se veral distinct advantages that make it ideal fo r the deployment of large-scale analytics frameworks. First, its ba sic abstraction for distributed data, the resilient distributed dataset (RDD) [27], combines robust fault-tolerance with highly effici ent data layout strategies. RDDs track their computation lineage as a directed acyclic graph; therefore, if a segment is lost, it can be easily recomputed from the lineage. These lineages can be optimized on-the-fly to minimize the overhead of the prescribed computations. Second, all operations in Spark ar e performed in-memory, thus significantly improving throughput of data pipelines. This is a departure from Hadoop MapReduce, in which data are serialized to disk in between map and reduce steps. Third, the Spark compute engine is much more ge neralizable than MapReduce, and can efficiently support highly di verse workloads. While Spark supports the map and reduce primitives from Hadoop MapReduce, it also supports graph processing [10] and streaming [28] APIs on the same compute engine, in addition to numerous functional primitives beyond map and reduce . This structural flexibility is crucial to the efficient implementation of a wide variety of distributed algorithms. An illustration for the operational and algorithmic pipeline consisting of three layers of model specification is shown in Fig. 1. The first and foremost deliverabl e of this work is to provide an integrated solution for the large-scale fMRI big data analysis. Therefore, we initially deploy ed the proposed D-r1DL model on our in-house server (termed  X  X n-house solution X ) with an integrated neuroinformatics system [17]. The neuroinformatics system provides a web-based user interface for fMRI data uploading, hosting and result post -processing [17], as illustrated in Fig. 1(a). Alternatively, we also tested deploying the D-r1DL model on the cloud computing se rvice provided by Amazon Web Services Elastic Compute Cloud (AWS-EC2) [1], which has been widely applied for biomedical im aging researches due to its resource flexibility and ease of use. For the  X  X WS-EC2 solution X , the data preprocessing was performed before running the D-r1DL model on it. Subroutines of the r1DL algorithm and its logic flow are illustrated in Fig. 1(b). The pa rallelization subroutines and its relationship with the r1DL algorithm are illustrated in Fig. 1(c). Figure 1. (a): Operations on the neuroinformatics system for preparing the application of D-r1DL and post-analysis. (b): Algorithm pipeline of the rank-1 dictionary learning. (c): Parallelization subroutines of the D-r1DL model derived from the corresponding subroutines of the r1DL using Spark. The distribution of input data S is based on RDDs. Functional network analysis based on matrix decomposition methods has the basic premise: the observed functional signals are the result of the linear combination from the signals of many latent source (i.e. functional networ ks), plus noises and/or artifacts signals [5]. The methods then aim to identify the latent source signals as well as the loading matrix based on various learning priors including spatial/temporal independence [5; 24] (for ICA), or the sparsity in the loading coefficients [11; 15] (for dictionary learning). The decomposition resu lts consist of two parts: the signals of the latent sources (i.e. temporal pattern of the functional networks) that are regarded as ba sis activation patterns, and the loading matrix characterizing how each source contributes to the formation of each observed sign al across voxels/ brain regions (i.e. spatial pattern of the functio nal networks). The spatial and temporal pattern of a sample network decomposed result is shown in Fig. 2. Several sets of consistent and meaningful functional networks have been identified in the previous literature: including the 10 well-established resting-state networks (RSNs) [24] obtained using ICA, the task-related patte rns obtained using PCA [25], and the HAFNI (holistic atlases of functional networks and interactions) atlases [15] featuring 32 group-wise consistent patterns during both task and resting-state using dictionary learning [2]. Visualization of one functional network from the matrix decomposition results using the proposed r1DL method is shown below, to illustrate the t emporal (the time series curve) and spatial (on cortical volumetric space) patterns of the brain functional networks. Figure 2. Top: spatial pattern visualized on cortical volumetric space of one decomposed network. Bottom: visualization of its temporal pattern. While dictionary learning in general is an active area of research, there has been significantly less effort in scaling the algorithm. One of the few methods proposed was [22], in which the authors designed a sparse coding framework on Hadoop MapReduce. The method was parallelized by splitting the core operations in two main phases: the sparse coding phase, in which the loading weights were learned in parallel ; and the dictionary learning phase, in which the dictionary atoms were updated. By taking advantage of hard sparsity constraints, the authors avoided materializing the entire data matrix in memory at once, instead operating on blocks of the matrix in parallel and constructing the loading matrix row by row. In this work we undertook the similar dataflow optimization techniques: the sparse coding and dictionary atoms are assumed to fit easily in memory for fast and efficient computation. Howeve r, using the Spark framework instead of Hadoop MapReduce provides us intrinsic speedups. Where Hadoop MapReduce excels in batch processing, Spark is optimized for iterative computation: intermediate results are cached in-memory on the workers and re-used in subsequent iterations, and the updates are efficiently broadcasted to the workers. Most importantly, the R DDs abstraction pipelines the requested operations and lazily executes them after determining the optimal computational path using the least amount of resources. We leverage these adva ntages to provide a substantial performance gain in our D -r1DL dictionary learning implementation. The rank-1 dictionary learning algorithm aims to iteratively estimate multiple rank-1 basis vector u ( T  X 1 vector with unit decompose the input signal matrix S of dimension T  X  P , by minimizing the following energy function L ( u , v ): Eq. 1 indicates that the product of u and v is supposed to well-fit the input S while the total number of non-zero elements in v should be smaller than or equal to the given sparsity constraint parameter r . The minimization problem in Eq. 1 can be solved by alternatively updating u (randomly initialized before the first iteration) and v until convergence: Eq. 2 involves multiplication between input matrix S and vector u , followed by setting all elements in the resulting vector smaller than its r -th largest value to zero, essentially performing the vector partition operation. One rank-1 basis [ u , v ] can be estimated in each step; afterwards the input matrix S will be deflated to its residual R : where K is the total number of expected basis (i.e. dictionary atoms) to be discovered from the in put data. It could be seen that the formulation of the proposed rank-1 dictionary learning algorithm is similar to the sparse PCA problem [6]. However, the goal of PCA and sparse PCA is to derive a low-dimensional basis contrast, the goal in dictionary learning is to learn an over-complete dictionary set [1 2]. Regarding the algorithm convergence, it is easy to show that the value of the energy convergence), thus the objective is guaranteed to converge. The convergence of the learning in Eq. 2 was also empirically tested in this work. The deflation operation in Eq. 3 is based on Hotelling X  X  deflation method for estimating the eigenvectors, where each step of deflation leads to the corresponding eigenvalue replaced by zero. The validity of using Hotelling X  X  deflation for sparse PCA was provided in [6], while better deflation methods were also discussed in [16]. Fig. 3 shows a running example illustrating the data preparation and networks decomposed by r1DL. Figure 3. Illustration of the r1DL model applied on fMRI data as a running example. (a) 4D fMRI data represented as a series of 3D volume images. (b ) Converting 4D data into 2D input matrix S . (c) Spatial ( v 1 ... v K ) and temporal ( u patterns of the decomposed functional networks from S using r1DL dictionary learning. In this work, the rank-1 dictionary learning algorithm introduced above was implemented and para llelized on the Spark engine. Specifically, the vector-matrix multiplication and the matrix-vector multiplication steps in Eq. 2 were implemented by their corresponding distributed primitives in Spark. Reading and partitioning the input data S is supported by the RDD abstraction; therefore, the distribution of S to each node as a series of key-value pairs is inherently straight forward: data formation of the current work is based on row-vectors. In other words, each column in S contains the T number of observations for one specific feature, to the total of P features. While S was maintained as an RDD, the vectors u and v were broadcast to all nodes. Thus during the vector-matrix multiplica tion, each node will use its portion of the updated u vector, and then estimate the v vector based on the multiplication of portions of S and u . The resulting v vectors from all the nodes will be then map-reduced by the summation operation. The ma trix-vector multiplication is relatively easier, where each node will use all the updated v vector then estimate its corresponding portion of the u vector. The resulting u vector is just the collec tion of the results from each node. In addition, the matrix deflation operation in Eq. 3 was also parallelized by broadcasting both the u and v vectors learned from Eq. 2, and then estimating the outer produce between portion of u subtracted by the results of each node through mapping over each row and deflating it in parallel. This implementation of the r1DL algorithm after parallelization is termed as the  X  X istributed rank-1 dictionary learning X  (D-r1DL) model. The computational complexity of the original (un-parallelized) rank-1 dictionary learning algor ithm is quite obvious: all the major subroutines including matrix-vector multiplication, vector-matrix multiplications and deflation have complexity of T * P , essentially traversing through input matrix S . However, the distributed primitives added for th e parallelization in D-r1DL will potentially cause large extra com putations and/or data transfers across nodes. The problem will be magnified when such transfer occurs over a network (e.g. worker nodes are distributed across Internet). Thus we analyzed the extra complexity induced by the parallelization of the three subro utines, assuming that there are M number of nodes. It should be noted that the estimations are upper bounds for the complexity; the empirical performance will be largely dependent on how Spa rk optimizes the transformation lineage for the RDD, and how the RDD is distributed across the cluster. total complexity is ( T * M + P * M + T log( T ) + P ): T * M caused by the broadcasting, P * M + T log( T ) caused by the map reduce and network shuffle, and P caused by the updating of v . complexity is ( P * M + T ): P * M caused by the broadcasting, and 
T caused by the updating of u . ( P * M + T * M ): both u and v will b e broadcasted to all M nodes. The D-r1DL model was deployed on two different sets of server clusters, leading to two solution s for the data analysis. The first set is based on the in-house server, called the  X  X n-house solution X . Spark version 1.5.2 pre-built for Hadoop 1.X and python version 2.7.11 with all the required dependencies were installed on the in-house server. We setup one standalone spark cluster on the server with a master node consisting of 16 cores and 16GB RAM. The in-house solution also featured an integrated neuroinformatics system named HELPNI (HAFNI-e nabled largescale platform for neuroimaging informatics) as introduced in [17]. Authorized users of the system can upload, mana ge, and perform the preparations of the fMRI data for the model an alysis. For running the D-r1DL model, the preparation steps incl ude fMRI signal preprocessing (gradient distortion correction, motion correction, bias field reduction, and high pass filtering) [9], converting the 4D fMRI images to 2D data matrix, as well as the generation of shell scripts according to user specifications. When the computations by D-r1DL are finished, the reports of the results (consisting of the statistics and visualizations of the decomposed functional networks) will be au tomatically generate d by the HELPNI neuroinformatics system [17] and uploaded to the server, accessible via web interface for viewing and sharing. The in-house solution is mainly used for testing the single-server (on the local threads), standalone mode performance of the D-r1DL algorithm. The second set was based on the computational resources provided by AWS-EC2 service. For the AWS-EC2 implementations, we used the pr ovided EC2 deployment scripts with the Spark distribution. These created a Spark cluster on EC2 with one master node and 16 workers. Each worker consisted of 2 cores and 7.5GB memory. EC2 clusters are highly scalable, as the number of workers recruited could be adjusted within the cluster. The preprocessed and converted fMRI data was stored at the cloud through Amazon S3 and accessible by the EC2 cluster. The nodes featured Hadoop distribu ted file system (HDFS), which ensured data consistency and improved I/O speed. Hadoop version 1.0, Spark version 1.5.2, and Python version 2.7.11 with the Anaconda scientific programming stack (e.g. NumPy, SciPy) were installed on EC2 cluster. An illustrative diagram showing the organization and execution architecture of the two solutions are shown below. Figure 4. Illustrative diagram showing the organization and execution architectures for the standalone local mode and the multi-worker cluster mode. To validate the effectiveness of the proposed D-r1DL model in terms of its capability of decompos ing fMRI data into meaningful functional networks, we applied th e framework on the fMRI data acquired during multiple tasks from the Human Connectome Project (HCP) Q1 release data set [26]. The HCP dataset is advantageous in its high t emporal and spatial resolution (TR=0.72s, varied temporal le ngth from 176 to 1200 volumes; 2mm isotropic voxels, to the total of over 200,000 voxels), which enables more detailed characterization of the brain X  X  functional behavior. Additionally, the HCP dataset includes acquisitions of fMRI data during 7 tasks and res ting-state from 68 subjects, to the total of over 500 individual data, which matches the aim of the proposed framework for population-level fMRI big data analysis. The learned collection of functi onal networks represented by rank-1 dictionary basis was then compared with the HAFNI atlases [15]. The HAFNI atlas wa s obtained by applying online dictionary learning method [18] on all the individual fMRI data in the same HCP Q1 dataset. Subs equently, 32 group-wise consistent networks were identified throug h manual inspection over more than 200,000 decomposed networks [15]. Thus our aim in this validation was to identify the presence (or absence) of those atlas networks from the results of D-r1DL. The main rationale of performing comparison with network templates and atlas network for the model validation in this work was due to the lack of ground truth in fMRI data. At this stage, the capability and accuracy of the proposed model could only be validated by comparing with the previously-reported and well-established results from the same dataset. Figure 5. Spatial maps obtained from applying the dictionary learning method on the same fMRI dataset implemented by SPAMS, r1DL in C++, and D-r1DL in Spark. In addition, the D-r1DL model was compared with the rank-1 dictionary learning algorithm implemented in C++ without any parallelization, in order to inves tigate whether the parallelization would affect the model performance. Both of the implementations were deployed on the same in-house server and applied on the HCP Q1 individual resting-state fMRI data as well as two task fMRI datasets ( X  X motion X  an d  X  X orking Memory X ) using the same parameter setting ( r =0.07, K =80). An illustration for the spatial maps of two sample networks decomposed by r1DL implemented in C++ and D-r1DL, as well as the corresponding individual-level atlas network in HAFNI, is shown in the three panels in Fig. 5 (a)-(c), respectively. The obtained functional networks from D-r1DL were then matched to the individual-level atlas networks on the same subjects of the same tasks (or re sting-state) by maximizing the spatial similarity between them: where P 1 and P 2 are the spatial map vectors of the two networks. In this work P 1 is the network decomposed by D-r1DL and P the atlas network. Operator | X | counts the total number of voxels with non-zero values in the given spatial pattern. R ranges from 0 (no voxels overlapping) to 1 (ex actly the same maps). The spatial similarity results show that a ll of the atlas networks defined in HAFNI could be found from the results of D-r1DL. Specifically, the 3 atlas networks from Emotion dataset were identified from D-r1DL results with average spatial similarity of 0.82. The 6 atlas networks from WM dataset were id entified with average spatial similarity of 0.79. The 10 re sting-state networks, originally reported in [7] and later included in the HAFNI atlas were identified from the results of ap plying D-r1DL on resting state fMRI data with average spatial similarity of 0.70. The spatial similarity results also show that the networks decomposed by r1DL implemented in C++ and D-r1DL implemented in Spark are almost identical, with average spatial similarity of 0.99. As discussed in section 3 for the algorithm description, we have performed extensive experiments for testing the convergence of alternating least squares with l -0 constraint problem in Eq. 2. By running the r1DL algorithm on each individual fMRI dataset in the HCP Q1 release across 7 tasks with dictionary size parameter K =400 (i.e. decomposing 400 functional networks from each fMRI data), we found that the learning of u and v converged for all the 190,400 networks decomposed. The average number of alternative updates needed for le arning one network for different datasets of 5 randomly selected sample subjects is listed in Table 1 below. It can be seen that regardless of the input data size, the majority of the learning would be finished within only a few iterations. Table 1. Average number of iterations needed for convergence One of a major premise of the proposed r1DL algorithm is that because of its smaller memory cost and robust learning mechanism (no need to set learning rate), the algorithm should have similar or faster running speed, compared with other dictionary learning methods, even without the parallelization. Based on the performance statis tics from running r1DL over the whole HCP task fMRI (tfMRI) dataset as introduced above, we compare the r1DL with the other two dictionary learning algorithms: online dictionary learning implemented in SPAMS package [18] and the stochastic coordinate coding (SCC) algorithm introduced in [13], by applying these two methods on the same HCP Q1 dataset using the same in-house server. The performance comparison is shown in Fig. 6 (averaged across all 68 subjects). From the comparison, it can be seen that r1DL has exhibited improved computational speed over the other two methods in all the 7 tfMRI datase ts. It should be noted that we used the r1DL implemented in C++ for the testing in this experiment, and in the same wa y the other two methods were implemented to ensure cons istency in the comparison. Figure 6. Average time cost (measured in seconds) for functional network decompositio n from individual tfMRI data during 7 tasks across 68 subjects, using the three dictionary learning methods. As introduced in 5.1, in this work we applied the D-r1DL model on the three types of datasets for functional network decompositions: tfMRI data of Emotion task with dimension of 176*2M, tfMRI data of Working Memory (WM) task with dimension of 405*2M, and resting state fMRI (rsfMRI) data with dimension of 1200*2M. The testing input files sizes of the three types of dataset were 300MB, 700MB and 2GB, respectively. Using the in-house solution as specified in 4.2, we firstly analyzed the performance of the D-r1DL model using different numbers of cores on a single machine. The pe rformance statistics measured in time cost are shown in Fig. 7. Figure 7. Time cost for decomp osing one functional network from three different fMRI datasets by recruiting varying number of cores, using the in-house solution. For all the three datasets, there exists clear logarithmic relationship ( R 2 =0.84, 0.89 and 0.92) between the number of cores recruited and the total time cost for the decomposition. The speed boosts by recruiting more cores for the computation comparing with the baseline (1-core) config uration for the three datasets using the in-house solution are listed in Table 2, showing the ratio between the time cost using 1 core and the time cost using multiple cores. Table 2. Ratios of time cost decreases by recruiting more cores As the configuration for using only one core for D-r1DL is equivalent to the non-pa rallel algorithm, the performance statists indicate that the parallelizatio n based on Spark could greatly improve the performance of the rank-1 dictionary learning algorithm. Specifically, the better performance gain on larger dataset indicates that the parallelization of the rank-1 dictionary learning could potentially overcome the computational bottleneck for analyzing big neuroimaging data, potentially enabling high-throughput analysis on a loca lly-deployed high-performance computation cluster in the future. Another analysis of the performance of D-r1DL on the in-house server was aiming at investigating the relationship between dictionary size K (i.e. number of functional networks to be decomposed) and the time/memory cost. The rationale is that, as discussed in 3.1, the rank-1 dictionary learning algorithm has advantages in the iterative estimation of the basis vectors [ u , v ]. Thus the program does not need to maintain the learning results in the memory. As shown in Fig. 8, the average time cost for estimating one dictionary only marginally increased when using larger dictionary sizes K . Further, the total memory cost of running the D-r1DL was independent with K . This feature is especially useful when the spatial dimension of the input data is large, either because of the higher spatial resolution or because of the aggregated dataset from multiple subjects in a population. As in such cases, the size of all decomposed networks, which equals to P * K , could be very large. 0 200 400 600 800 Time Cost (seconds/dicitonary) Figure 8. Time and memory costs for decomposing Emotion tfMRI datasets with varying dictionary size K , recruiting 16 cores, using the in-house solution. In addition to the experiments of the single-machine multi-core configurations conducted using th e in-house solution, we have also applied the D-r1DL on the same datasets using the cloud computing service provided by AWS-EC2 as introduced in 4.2. We aimed to investigate the perf ormance of D-r1DL when applied over multiple machines through a ne twork interface. Specifically, as the Spark Python architecture and the resilient distributed dataset abstracts have been de signed for supporting large-scale, high efficient analytic framework, we are interested to test its capability of utilizing the distributed computational resources provided by AWS-EC2. In this w ork, we tested the performance in terms of time and memory cost of the D-r1DL model using 1, 2, 4, 8 and 16 workers on three datasets, while each worker has two cores for the computation. It should be noted that for normal usage, 16 is the maximum number of workers allowed for one instance for the EC2 configuration. The D-r1DL would be running in stand-alone mode under single -worker configuration, similar to the configuration used in the in-house solution. As discussed in 4.1, the communications through network interfaces caused by the parallelization of computa tion (e.g. the broadcasting of u and v ) will potentially increase the time cost mainly due to latencies. Thus the single-worker configurati on serves as the baseline for testing whether recruiting more workers will be beneficial from the performance perspective. Th e performance results of the time and memory cost are summarized in Fig. 9, with the baseline results from single-worker configuration highlighted. Figure 9. Time cost for decomp osing one functional network from three different fMRI datasets by recruiting varying numbers of workers, using the AWS-EC2 solution. The results from single-worker (standalo ne mode) configuration are highlighted as blocked markers. First of all, from the results it can be seen that the AWS-EC2 solution recorded faster computation speed (10%~80% faster) comparing with the in-house solution, especially on larger dataset, when both of them use two cores. Considering the fact that the hardware configuration of AWS-EC2 features larger memory capacity better optimized for computation purposes, such difference in performance is within our expectation. On the other hand, it is interesting to observe that for AWS-EC2 solution, there exists the break-even point at which the multiple-worker mode outperformed the stand-alone mode, but only for the two larger datasets. The ratio between the time cost using standalone mode and the time cost using multiple workers are for the three datasets summarized in Table 3. It can be observed that, for the 700MB WM and the 2GB RS dataset, using 4 or more workers could lead to faster spee d comparing with the standalone mode using 1 worker. While for th e smaller 300MB Emotion data, the standalone mode is the fastest among all experiments. Thus it can be concluded that the multi-worker configuration will be more suitable for analyzing larger datasets, while standalone mode or the simpler in-house server solution might be preferred for datasets with typically smaller sizes. 
Table 3. Ratios of time cost changes by recruiting various The memory cost on each worker as summarized in Fig. 10 indicates that the multi-worker mode under AWS-EC2 solution scales good with the increasing in put file size, as it maintains reasonable small (~100MB) memory cost for all configurations including the single-worker standa lone mode. That is the major advantage of using Spark Python model and its resilient distributed dataset for the parallelization: one or multiple workers need not to load the whole dataset at once, but only its corresponding portion of the data according to the data partitioning strategy implemented in the RDDs abstract. Time Cost (seconds/dicitonary) Memory Cost (MB) Time Cost (seconds/dicitonary) Figure 10. Memory cost for d ecomposing three different fMRI datasets by recruiting varyin g number of workers, using the AWS-EC2 solution. The results from single-worker (standalone mode) configuration are highlighted as blocked markers. In this paper, we proposed a novel and effective distributed dictionary learning model based on iterative rank-1 basis estimation. The model was implemented and parallelized in Spark, and then deployed using the in-house solution as well as the AWS-EC2 solution. The aim of this work is to meet the challenges posed by fMRI big data for more efficient and scalable data analytics methods. The testing results from running both solutions on the HCP Q1 dataset show that functional network decomposition using rank-1 dictionary learning could benefit from parallelization for both single-worker multi-core configuration and the multi-worke r cluster configuration, with significant performance improveme nt especially on larger datasets. In the current work, we have only analyzed individual-level fMRI data, with the largest da ta size of 2GB. As it has been shown from the performance statis tics that the Spark engine supported by RDDs abstract coul d effectively perform the data partition and reduce the memory cost for large-scale input data, we will test the model perform ance on larger, population-level datasets (e.g., the HCP full dataset) with the size of dozens or hundreds of terabytes in the near future. The ultimate goal of the proposed D-r1DL model with the HELPNI neuroinformatics system is to provide an integrated solution for functional neuroimaging big data management and analysis, enabling high-throughput neuroscientific knowledge discovery. In addition, the similar parallelization scheme used in this work for D-r1DL could be implemented on other algorithms as well. Thus the experience of this work also offers a practical perspective for improving the efficiency and scalability of ge neral machine learning and data mining algorithm developments. [1] https://aws.amazon.com/ec2/ [2] http://hafni.cs.uga.edu [3] https://spark.apache.org [4] Aharon, M., Elad, M., and Bruckstein, A., 2006. K-SVD: An [5] Biswal, B.B. and Ulmer, J.L., 1999. Blind Source Separation [6] D'aspremont, A., Ghaoui, L.E., Jordan, M.I., and Lanckreit, [7] Damoiseaux, J.S., Rombouts , S.A.R.B., Barkhof, F., [8] Elad, M. and Aharon, M., 2006. Image Denoising Via Sparse [9] Glasser, M.F., Sotiropoulos, S.N., Wilson, J.A., Coalson, [10] Gonzalez, J.E., Xin, R.S., Dave A., Crankshaw, D., Franklin, [11] Kangjoo, L., Sungho, T., and Jong Chul, Y., 2011. A Data-[12] Lee, H., Battle, A., Raina, R., and NG, A.Y., 2006. Efficient [13] Lin, B., Li, Q., Sun, Q., Lai, M.-J., Davidson, I., Fan, W., [14] Liu, B.-D., Wang, Y.-X., Zhan g, Y.-J., and Shen, B., 2013. [15] Lv, J., Jiang, X., Li, X., Zhu, D., Zhang, S., Zhao, S., Chen, [16] Mackey, L.W., 2008. Deflation Methods for Sparse PCA. In [17] Makkie, M., Zhao, S., Jiang , X., Lv , J., Zhao, Y., Ge, B., Li, Memory Cost (MB) [18] Mairal, J., Bach, F., Ponce, J. , and Sapiro, G., 2010. Online [19] Mennes, M., Biswal, B.B., Castellanos, F.X., and Milham, [20] Poldrack, R.A., Barch, D.M., Mitchell, J.P., Wager, T.D., [21] Ravishankar, S. and Bres ler, Y., 2011. MR Image [22] Sindhwani, V. and Ghoting, A. , 2012. Large-scale distributed [23] Smith, S.M., Hyvarinen, A., Varoquaux, G., Miller, K.L., [24] Smith, S.M., Miller, K.L., Moeller, S., XU, J., Auerbach, [25] Thirion, B. and Faugeras, O. , 2003. Dynamical components [26] Van Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E.J., [27] Zharia, M., Cho wdhury, M., Da s, T., Dave, A., MA, J., [28] Zaharia, M., Das, T., Li, H., Hunter, T., Shenker, S., and 
