 Matt Wytock mwytock@cs.cmu.edu J. Zico Kolter zkolter@cs.cmu.edu Carnegie Mellon University, Pittsburgh PA Sparse inverse covariance estimation using ` 1 methods (Banerjee et al., 2008), also known as the graphical lasso (Friedman et al., 2008), enables convex learn-ing of high-dimensional undirected graphical models. These methods estimate the inverse covariance of a zero-mean Gaussian distribution while penalizing the ` 1 norm of the off-diagonal entries; since the entries in the inverse covariance correspond to edges in a Gaussian Markov random field, this method learns a sparsely connected graphical model. In recent years, many algorithms have been proposed for this problem, including projected gradient methods (Duchi et al., 2008), smoothed optimization (Lu, 2009), alternating linearization methods (Scheinberg et al., 2010), and quadratic approximation (Hsieh et al., 2011).
 However, in many prediction tasks we may not want to model correlations between input variables. This is the familiar generative/discriminative contrast in machine learning (Ng &amp; Jordan, 2002), where it has been re-peatedly observed that in terms of predictive accuracy, discriminative approaches can be superior (Sutton &amp; McCallum, 2012). This has lead several researchers within the past year to (independently) propose a gen-eralization of the Gaussian MRF, which we refer to as the sparse Gaussian conditional random field (CRF), that allows for discriminative modeling between input and output variables (Sohn &amp; Kim, 2012), (Yuan &amp; Zhang, 2012), and our own work in (Wytock &amp; Kolter, 2012). 1 Although previous papers all showed signifi-cant promise to the model, they employed off-the-shelf optimization methods (significantly limiting the size of potential applications) and/or had theoretical results that did not fully highlight the advantages of sparsity. In this paper, we make three contributions. First, we develop a specialized second-order active set method for estimating sparse Gaussian CRF parameters, which we show to be several orders of magnitude faster than previously proposed algorithms. Second, we de-velop convergence bounds for the algorithm that estab-lish conditions for exact recovery of underlying mod-els, with rates that specifically highlight the graph de-gree, improving upon the results in (Yuan &amp; Zhang, 2012) in many settings. Third, we present extensive experimental results on large-scale synthetic data and two real-world energy forecasting tasks. Here we show improvement over state-of-the-art methods for wind power and electrical demand forecasting; these prob-lems are of substantial practical interest, as even small advances in forecasting accuracy can have notable im-pact on the efficiency and costs of large power systems. Let x  X  R n and y  X  R p denote input and output vari-ables for a prediction task. A Gaussian CRF is a log-linear model with where the quadratic term models the conditional de-pendencies of y and the linear term models the de-pendence of y on x . The model is parameterized by  X   X  R p  X  p , which corresponds to the inverse covariance matrix, and  X   X  R n  X  p , which maps the inputs to the outputs; an illustration of the model is shown in Fig-ure 1. Since the CRF is a Gaussian distribution with mean  X   X   X  1  X  T x , the partition function is given by For m data samples, arranged as the rows of X  X  R m  X  n and Y  X  R m  X  p , the negative log-likelihood f ( X  ,  X ) =  X  log p ( Y | X ;  X ;  X ) is given by f ( X  ,  X ) =  X  log |  X  | + tr S yy  X +2 S yx  X + X   X  1  X  T S xx (omitting the constant term c term), where the S terms are empirical covariances Without regularization, it is straightforward to verify that this optimization problem is simply a reparam-eterization of the least squares problem. We can ad-ditionally add ` 2 regularization by adding  X  2 to the diagonal elements of S (formally, this corresponds to a Normal-Wishart prior on  X  and the columns of  X ), but again this just corresponds to the regularized least-squares solution. However, the total number of param-eters in this problem (for estimating both  X  and  X ) is np + p ( p + 1) / 2, and thus model can overfit when the number of examples m is relatively small.
 To address this concern, we regularize the maximum likelihood estimate by adding ` 1 regularization to  X  and the off-diagonal elements of  X ; since the ` 1 norm encourages sparsity of the parameters, this directly corresponds to learning a sparse set of edges in our graphical model. Our final optimization problem is then given by minimizing the composite objective where k X k 1 denotes the elementwise ` 1 norm, k X k 1 ,? de-notes the elementwise ` 1 norm on off-diagonal entries, and  X   X  R + is the regularization parameter. 2 This is a convex objective, following from the convexity of the ` 1 norm and the fact that the log-partition function of an exponential family graphical model is concave. Furthermore, the gradients of f are given by which in previous work has motivated the use of first-order non-smooth optimization methods. Previous work on the sparse Gaussian CRF (SGCRF) model has proposed using off-the-shelf algorithms to solve the above optimization problem, including or-thantwise quasi-Newton methods (Sohn &amp; Kim, 2012) (specifically, the OWL-QN method of (Andrew &amp; Gao, 2007)), and accelerated proximal gradient methods (Yuan &amp; Zhang, 2012) (specifically, the FISTA algo-rithm of (Beck &amp; Teboulle, 2009)). These methods are attractive due to their simplicity, and since the gradi-ents can be efficiently computed using (6). Unfortu-nately, the algorithms still suffer from relatively slow convergence (even though they are faster than many alternative non-smooth first-order methods), and thus quickly become computationally impractical for large output and input dimensions.
 In this section, we propose a new second-order active set method for solving the sparse Gaussian CRF. Such algorithms have previously been applied to the Gaus-sian MRF (Hsieh et al., 2011; Olsen et al., 2012), and a general analysis of such methods (showing quadratic convergence) is presented in (Tseng &amp; Yun, 2009). The method here largely mirrors in the approach in (Hsieh et al., 2011) for the Gaussian MRF, but the precise formulation is significantly more involved, owing to the complexity of gradient term of the  X   X  1  X  T S term in the likelihood. Despite being a second-order method, we show that the resulting algorithm is faster (to reach any accuracy) than previously proposed ap-proaches, and several orders of magnitude faster at achieving solutions to high accuracy. 3.1. A second-order active set approach The basic idea of our method is to iteratively form a second-order approximation to the objective function (without the ` 1 regularization term), and then solve an ` regularized quadratic program to find a regularized analog of the Newton step. In general notation, to minimize some objective f ( x ) +  X  k x k 1 , we form the Taylor expansion f ( x +  X )  X  g ( X )  X  f ( x ) +  X  x f ( x ) T  X  + where  X  x f ( x ) and  X  2 x f ( x ) denote the gradient and Hessian respectively. To compute the regularized New-ton step d , we solve and update the parameters x  X  x +  X d for some step-size  X  , determined by backtracking line search. In our setting, precise formulations of the gradient and Hessian terms are cumbersome, due to the fact that all parameters involved are matrices, but we can concisely express this second order Taylor expansion using dif-ferentials. In particular, (see Appendix A for a full derivation) the second order Taylor expansion is f ( X  +  X   X  ,  X  +  X   X  )  X  g ( X   X  ,  X   X  )  X  f ( X  ,  X ) + As above, we compute the Newton steps D  X  , D  X  by where we use a coordinate descent algorithm to opti-mize this ` 1 regularized QP. Since ` 1 regularization on the Newton direction tends to push the Newton up-dates in a direction that increases sparsity and since Algorithm 1 Newton Coordinate Descent for SGCRF Input: Input features X  X  R m  X  n and outputs Y  X  R m  X  p ; regularization parameter  X  Output: Optimized parameters  X ,  X 
Initialize :  X   X  I ,  X   X  0 while (not converged) do end while the line search provably converges to step sizes with  X  = 1 (Tseng &amp; Yun, 2009), the number of nonzero elements tends to increase as the optimization pro-gresses. For the line search, we additionally need to ensure that  X  is positive definite, which we ensure by a common technique of simply defining  X  log | X | to be infinite if X 6 0. A generic pseudo-code description of the algorithm is shown in Algorithm 3.1, and a more detailed presentation is given in Appendix B. Further-more, a C++ and MATLAB implementation is avail-able at http://www.cs.cmu.edu/ ~ mwytock/gcrf/ . 3.2. Computational speedups In order to make the Newton method efficient numer-ically, there are a number of needed optimizations. Again, these mirror similar optimization presented in (Hsieh et al., 2011), but require adaptations for the CRF case. In practice, the majority of the compu-tational work of the Newton CD method comes from computing the regularized Newton step via coordinate descent; even though coordinate descent is known to be an efficient method for solving ` 1 regularized prob-lems, in our setting we have a total of p ( p + 1) / 2 + np different variables, and it would be infeasible to op-timize over them all. Thus, at each iteration of the algorithm we use an active set method, and only op-timize over a variable ( X   X  ) ij or respectively ( X   X  ) ij i.e., if the optimally conditions for that parameter are violated for the current iterate of the parameters. Be-cause the sparsity resulting from the ` 1 constraint re-sults in a relatively small active set, this provides a substantial speedup, especially when the optimal so-lution has high sparsity. We also keep the active set small by using warm starts; solving the optimization problem for a decreasing a sequence of the regulariza-tion parameter and initializing each successive prob-lem with the previous optimal solution.
 Second, in the coordinate descent loop, it is impor-tant to cache and incrementally update certain matrix products, such that we can evaluate subsequent coor-dinate updates efficiently. This requires that we main-tain an explicit form of the matrix products  X   X   X  and  X   X   X   X  1 ; crucially, when we update a single coor-dinate of the  X   X  or  X   X  , we only need to update a single row of these matrix products, and we can sub-sequently use only certain elements of these products to compute each coordinate descent step. Details are given in Appendix B.
 Third, since each step of our Newton method involves solving an ` 1 regularized problem itself, it is impor-tant that we solve this regularized Newton step only to an accuracy that is warranted by the overall accu-racy of algorithm as a whole. Although more involved approaches are possible, we simply require that the inner loop makes no more than O ( t ) passes over the data, where t is the iteration number, a heuristic that is simple to implement and works well in practice. Last, in cases where n m (which is a setting that we are crucially interested in for motivating ` 1 reg-ularization), by not forming the S xx  X  R n  X  n matrix explicitly, we can reduce the computation for products involving X T X from O ( n 2 ) to O ( mn ). Note that the same considerations do not apply to S yy , since we need to form an invert the p  X  p matrix  X  to compute the gra-dients. Thus, the algorithm still has complexity O ( p 3 ), as in the MRF case. However, this highlights another advantage of the CRF over the MRF: when n is large, just forming a generative model over x and y jointly is prohibitively expensive. Thus, the sparse Gaussian CRF significantly improves both the performance and the computational complexity of the generative model. As for ` 1 regularized linear regression and the sparse Gaussian MRF, it is of significant interest to know when, if data is generated from a sparse underly-ing model, the sparse Gaussian CRF is able to re-cover this model with high probability. In this sec-tion, we develop theoretical results that show the sample complexity of the SGCRF grows slower than  X ( d 4 (log p + log n )) where d is the maximum degree of the output variables in the underlying graph; impor-tantly, this term grows logarithmically in the input and output dimensions p and n ; relative to the best known bounds for the special cases of the Gaussian MRF and linear regression, our bound has a worse dependence on d , which arises in bounding the error of the Taylor expansion. This element can likely be improved with more refined analysis, but we focus here on obtaining a bound that is logarithmic in p and n , and otherwise does not depend on on the total number of nonzeros in the true parameters.
 The proof proceeds in the primal-dual witness (PDW) framework of Wainwright (2009) (that is, we are con-cerned with the setting of recovering the sparsity of the true underlying model) and our analysis mirrors much of the Gaussian MRF case (Ravikumar et al., 2011); however, as with the optimization, the addi-tional terms in the gradient of the CRF introduce sub-stantial added complexity, which for instance result in the worse dependence on d . We operate under the following assumptions, similar to assumptions for the Gaussian MRF and least-squares settings. 1. True underlying model . The data is generated 2. Column normalization . The columns of the 3. Restricted convexity . Letting S i denote the 4. Mutual incoherence . This is the most subtle Theorem 1. Under the above assumptions, given a sample size and regularization parameter then with probability at least 1  X  c 3 exp(  X  c 4 m X  2 ) 1. The solution to the ` 1 regularized optimization 2. The solution satisfies the elementwise bounds where c 1 ,...,c 5 denote constant terms.
 The proof of this theorem is quite lengthy and deferred to Appendix C (where we also provide an explicit defi-nition of the constants and a lengthier definition of the assumptions). Intuitively, this theorem shows that a sample size of m =  X ( d 4 (log p + log n )) is sufficient to guarantee with high probability that solving the ` 1 regularized MLE recovers a subset of the true edge structure, and that the recovered parameters are close to the true parameters. Note that in many settings this is an improvement over the bound in (Yuan &amp; Zhang, 2012), which effectively requires a sample size  X ( s (log p + log n )) where s is the total number of edges in the graph; for graphs with a fixed low degree (such as a chain grain) s can grow linearly in p or n , whereas d remains constant, and so this represent a significant improvement X  X ndeed, as we show in our experimental results, the empirical scaling does indeed depend only logarithmically on p , even if s increases linearly in p . Here we experimentally evaluate several aspects of the proposed model and algorithm on synthetic data and two real-world energy forecasting problems, the tasks of predicting upcoming wind power from multiple wind farms and the task of predicting upcoming electrical demand over multiple utility zones. For the latter two cases, we demonstrate state-of-the-art results. The wind prediction task is from the 2012 Global Energy Forecasting Competition (Hong, 2012), a contest re-cently held on Kaggle to forecast wind power; here our algorithm improves upon our own submission to this contest by 5.5% (our entry was a top-5 entry that used least-squares with the same features and was 2.5% worse than the winning entry). For load forecast-ing, we use real-world load data from the PJM system operator (available at http://www.pjm.com/ ) and im-prove upon the deployed PJM forecasts by 19%. We also highlight the performance of the algorithm rel-ative to its theoretical bounds, and the optimization performance of our Newton method (which in all cases substantially improves upon previous methods). 5.1. Synthetic data Exact subset recovery. Our first experiments illus-trate when the model is able to exactly recover the underlying graph structure of a true model, and illus-trates that the overall dependence given in our theory looks to fit the observed results. Specifically, we gener-ate data from a chain CRF, where each output variable is connected to two others, and each input variable is connected to one output. To represent the chain we use the true parameter  X  ? with  X  ? ii = 1 on the diag-onal,  X  ? ij = 0 . 2 on the super diagonal and a diagonal  X  ? with  X  ? In Figure 2 (top), we vary m for different choices of p and observe that once m passes a certain threshold we recover the support of the true parameters with high probability X  X caling the x-axis by log p demonstrates the same theoretical dependence on p as shown in our theory. Importantly, note that in this case, the to-tal number of edges in the graph, s , increases linearly in p whereas the maximum vertex degree is fixed at d = 3. Thus, our bound captures the overall scaling of the model, whereas the bound of (Yuan &amp; Zhang, 2012) would be significantly looser in this case. For the Figure 2 (bottom), we increase n by adding irrelevant features (features that are not connected to the output variables); again, we observe a logarithmic dependence on the input dimension.
 Optimization performance. Because the chain CRF is a rather limited example, for the remaining synthetic examples we generate data from more com-plex model. In particular, follow a similar procedure as in (Yuan &amp; Zhang, 2012), and generate  X  and  X  with 5( n + p ) random unity entries (the rest being zero), and set the diagonal of  X  such that the condi-tion number of  X  equals n + p . We sample x from a zero-mean Gaussian with full covariance, square half the entries, and then normalize the columns to have unit variance. We use this same process for the next three experiments, but vary problem size to make the experiments computationally feasible in all cases. Figure 3 shows the suboptimality of each method in terms of the objective function f  X  f ? (where f ? is com-puted by running our Newton CD approach to numeri-cal precision) versus execution time on a 2.4GHz Xeon processor; this problem has size p = 1000, n = 4000, and m = 2500. On this problem the Newton CD ap-proach converges to high numerical precision within about 81 seconds, while FISTA and OWL-QN still don X  X  approach this level of precision after two hours. It is also important to note that the Newton CD ap-proach also reaches all intermediate levels of accuracy faster than the alternative approaches, so that the al-gorithm is preferable even if only intermediate preci-sion is desired. Indeed, we note previous works (Sohn &amp; Kim, 2012; Yuan &amp; Zhang, 2012) considered max-imum problem sizes of np  X  10 5 due to the time re-quired for training; since much of the appeal of ` 1 ap-proaches lies precisely in the ability to use large feature sizes, this has significantly limited the applicability of the approach. We thus believe that our proposed al-gorithms opens the possibility of substantial new ap-plications of this sparse Gaussian CRF model.
 Comparison to MRF. Our next experiment com-pares the discriminative CRF modeling to a generative MRF model. In particular, an alternative approach to our framework is to use a sparse Gaussian MRF to jointly model x,y as a Gaussian, then compute y | x . Figure 4 shows the performance of the Gaussian MRF versus CRF, measured by mean squared error in the predictions on a test set, over a variety of different  X  parameters. The CRF substantially outperforms the MRF in this case, due to two main factors: 1) the x variables as generated by the above process are not Gaussian, and thus any Gaussian distribution will model them poorly; and 2) the x variables are cor-related and have dense inverse covariance, making it difficult for the MRF to find a sparse solution. Finally, we note again that in addition to the per-formance benefits, the CRF has substantial compu-tational benefits. Modeling x and y jointly requires computing and inverting their joint covariance, which takes time O (( n + p ) 3 ); in contrast, the corresponds operations for the CRF case are O ( np 3 ), which is sub-stantially faster for even modestly large n . Indeed, for the two real-world experiments below, we were unable to successfully optimize a joint MRF using the QUIC algorithm of (Hsieh et al., 2011) (itself amongst the fastest for solving the sparse Gaussian MRF), after running the algorithm for 20 hours.
 Sample size. Finally, to illustrate the benefit of ` 1 regularization over traditional ( ` 2 regularized) multi-ple least-squares estimation, we evaluate generaliza-tion performance versus sample size, shown in Fig-ure 5. This figure shows performance measured by mean squared error of the ` 1 regularized sparse Gaus-sian CRF versus traditional least-squares with ` 2 reg-ularization; here, for each m we choose the ` 1 and ` 2 regularization parameters using a cross validation set, then evaluate the MSE on a test set. As the sample size increases, the performance of the two methods be-comes similar (in the limit of infinite data with fixed n and p , they will of course be equivalent); however, as expected, for small samples sizes the ` 1 regulariza-tion method performs much better, being able to take advantage of the sparsity in the underlying model. 5.2. Application to energy forecasting Wind power forecasting. We here apply the sparse Gaussian CRF model to the wind power forecasting task from the Global Energy Forecasting 2012 com-petition (Hong, 2012), a recent Kaggle competition for predicting upcoming wind power at seven differ-ent nearby wind farms for a time horizon of 48 hours. The input data for this problem consisted of previous power outputs for the wind farms (going as far back as the past 36 hours), and wind speed forecasts for the upcoming 48 hours. From this input we gener-ated features that consisted of: 1) the past 8 hours of power for each wind farm, and 2) 10 RBF features placed around each forecasted wind speed, to capture non-linear dependencies on the wind speed itself. In total, this lead to p = 336 dimensional outputs and n = 3417 dimensional inputs. We heavily optimized these features for the competition, and using these fea-tures with ordinary least-squares resulted in a top-5 finish in the competition (out of 134 entrants). Figure 6 shows the performance of the sparse Gaussian CRF on the wind forecasting task, analyzed across sev-eral dimensions. First, the figure on the left shows per-formance of method for varying  X  ; also shown in the best performance of ` 2 regularized least-squares. For properly chosen  X  , the algorithm outperforms least-squares (using the exact same features), by 5.5%. For a domain such as wind power forecasting, where there is a limit to the possible performance (wind is an in-herently stochastic phenomenon, so exact forecasts are not possible), and since the least-squares solution in this case is already using highly optimized features, this represents a substantial improvement. The differ-ence in performance become even more pronounced for smaller sample sizes, as shown in the Figure 6 (center), which shows MSE (using  X  chosen by hold out cross validation), for a variety of sample sizes. Finally, to highlight the optimization performance on real data as well, we shown in Figure 6 (right) the optimization objective versus training time for the different opti-mization algorithm. Again, the Newton CD algorithm vastly outperforms FISTA and OWL-QN, converging to high accuracy after 160 minutes, whereas the latter do not reach reasonable accuracy after several hours. Finally, a significant advantage of the sparse Gaus-sian CRF approach is that the sparsity pattern of the resulting model can be interpreted directly as condi-tional dependencies between variables, and thus the sparsity pattern itself can be very informative. Figure 7 shows the sparsity patterns in  X  and  X  for the wind forecasting task; they illustrate a clear temporal and spatial dependence between the different wind farms. Electrical demand forecasting. We further apply the model to the task of predicting future electrical demand for zones operated by PJM (a system operator for coordinating electricity generation and delivery for several Eastern U.S. states). In particular, the goal is to forecast upcoming electrical demand for the next 24 hours over 15 different zones in the system.
 Electricity forecasting is a well-studied problem (Soli-man &amp; Al-Kandari, 2010), and PJM already employs a sophisticated forecasting system in its operation (Vari-ous, 2012) to predict a subset of the zones; rather than try to build an entirely new forecast, we use these pre-vious point forecasts as input features (along with past energy consumption and time-of-day features) to pre-dict future demand. The goal is thus to use a combi-nation of the existing predictions to predict even more accurately, and if we can improve upon the PJM fore-casts, this means that we are effectively combining ex-isting information to ultimately deliver a better pre-diction. For this problem, the dimension of input and output are p = 350 and n = 860. We present these results here more briefly, but the key performance el-ement we want to highlight is in Figure 8; the figure shows that by jointly predicting over all the zones, we are able to improve substantially upon PJM X  X  already state-of-the-art point forecasts. The sparse Gaussian conditional random field enjoys many benefits of existing methods for learning high-dimensional Gaussian graphical models; we believe that the advances put forward in this paper make the model significantly more practical for large-scale prob-lems, and also significantly advance our theoretical un-derstanding of the method. Furthermore, the empiri-cal results presented here on wind power and demand forecasting are of substantial practical interest, and the improvements presented here have the potential for notable effects on power system efficiency. Two future directions seem particularly promising. First, it would be worthwhile to use regret-based ap-proaches to develop alternate convergence rates under weaker assumptions than those we use. Although ex-act feature selection is not possible even for the least-squares case when inputs are very highly correlated, it is nonetheless possible to obtain regret bounds that bound the loss versus that of the true model, e.g. (Bartlett et al., 2012); such directions are likely to be of substantial interest here, since we do expect to often be in situations where input features are correlated. Sec-ond, from an application standpoint in energy systems in particular, there are a huge number of forecasting problems that share similar properties as wind power and demand; of crucial importance, however, is devel-oping control algorithms that can exploit these prob-abilistic forecasts in the planning stage. Developing such algorithms will allow high-dimensional graphical models such as the Gaussian CRF to have an immedi-ate impact on these globally important domains. Andrew, Galen and Gao, Jianfeng. Scalable training of l 1-regularized log-linear models. In Proceedings of the 24th international conference on Machine learn-ing , pp. 33 X 40. ACM, 2007.
 Banerjee, O., Ghaoui, L. El, and d X  X spremont, A.
Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data, 2008.
 Bartlett, Peter L, Mendelson, Shahar, and Neeman,
Joseph. ` 1 -regularized linear regression: Persistence and oracle inequalities. Probability theory and re-lated fields , pp. 1 X 32, 2012.
 Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences , 2 (1):183 X 202, 2009.
 Duchi, J. C., Gould, S., and Koller, D. Projected sub-gradient methods for learning sparse Gaussians. In
Proceedings of the Conference on Uncertainty in Ar-tificial Intelligence , 2008.
 Friedman, J., Hastie, T., and Tibshirani, R. Sparse in-verse covariance estimation with the graphical lasso. Biostatistics , 9(3):432 X 441, 2008.
 Hong, T. Global energy forecasting competition, 2012. URL http://www.gefcom.org .
 Hsieh, C.-J., Sustik, M. A., Dhillon, I. S., and Raviku-mar, P. Sparse inverse covariace matrix estimation using quadratic approximation. In Neural Informa-tion Processing Systems , 2011.
 Lu, Z. Smooth optimization approaches for sparse in-verse covariance selection. SIAM Journal on Opti-mization , 19(4):1807 X 1827, 2009.
 Ng, A. Y. and Jordan, M. I. On discriminative vs. gen-erative classifiers: a comparison of logistic regression and naive Bayes. 2002.
 Olsen, Peder A, Oztoprak, Figen, Nocedal, Jorge, and
Rennie, Stephen J. Newton-like methods for sparse inverse covariance estimation. Optimization Online , 2012.
 Ravikumar, Pradeep, Wainwright, Martin J, Raskutti,
Garvesh, and Yu, Bin. High-dimensional covari-ance estimation by minimizing 1-penalized log-determinant divergence. Electronic Journal of Statistics , 5:935 X 980, 2011.
 Scheinberg, K., Ma, S., and Goldfarb, D. Sparse in-verse covariance selection via alternating lineariza-tion methods. In Neural Information Processing Systems , 2010.
 Sohn, Kyung-Ah and Kim, Seyoung. Joint estima-tion of structured sparsity and output structure in multiple-output regression via inverse-covariance regularization. In Proceedings of the Conference on Artificial Intelligence and Statistics , 2012. Soliman, S. A. and Al-Kandari, A. M. Electrical Load
Forecasting: Modeling and Model Construction . El-sevier, 2010.
 Sutton, C. and McCallum, A. An introduction to con-ditional random fields. Foundations and Trends in Machine Learning , 4(4):267 X 373, 2012.
 Tseng, Paul and Yun, Sangwoon. A coordinate gradi-ent descent method for nonsmooth separable mini-mization. Mathematical Programming , 117(1):387 X  423, 2009.
 Various. PJM Manual 19: Load Forecasting and Anal-ysis . PJM, 2012. Available at: http://www.pjm. com/planning/resource-adequacy-planning/ ~ / media/documents/manuals/m19.ashx .
 Wainwright, M.J. Sharp thresholds for high-dimensional and noisy sparsity recovery using ` 1 -constrained quadratic programming (Lasso). IEEE
Transactions on Information Theory , 55(5):2183 X  2202, 2009.
 Wytock, Matt and Kolter, J. Zico. Sparse conditional gaussian random fields. In NIPS Workshop on Log Linear Models , 2012.
 Yuan, Xiao-Tong and Zhang, Tong. Partial gaussian graphical model estimation. CoRR , abs/1209.6419,
