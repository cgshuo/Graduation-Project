 Ji Liu ji-liu@cs.wisc.edu Lei Yuan lei.yuan@asu.edu Jieping Ye jieping.ye@asu.edu The sparse signal recovery problem has been well stud-ied recently from the theory aspect to the application aspect in many areas including compressive sensing (Cand`es &amp; Plan, 2009; Cand`es &amp; Tao, 2007), statistics (Ravikumar et al., 2008; Bunea et al., 2007; Koltchin-skii &amp; Yuan, 2008; Lounici, 2008; Meinshausen et al., 2006), machine learning (Zhao &amp; Yu, 2006; Zhang, 2009b; Wainwright, 2009; Liu et al., 2012), and sig-nal processing (Romberg, 2008; Donoho et al., 2006; Zhang, 2009a). The key idea is to use the ` 1 norm to relax the ` 0 norm (the number of nonzero entries). This paper considers a specific type of sparse signal recovery problems, that is, the signal is assumed to be sparse under a linear transformation D . It includes the well-known fused LASSO (Tibshirani et al., 2005) as a special case. The theoretical property of such prob-lem has not been well understood yet, although it has achieved success in many applications (Chan, 1998; Tibshirani et al., 2005; Cand`es et al., 2006; Sharpnack et al., 2012). Formally, we define the problem as fol-lows: given a measurement matrix  X   X  R n  X  p ( p n ) and a noisy observation vector c  X  R n constructed from c =  X   X   X  + where  X  R n is the noise vector whose entries follow i.i.d. centered sub-Gaussian dis-tribution 1 , how to recover the signal  X   X  if D X   X  is sparse where D  X  R m  X  p is a constant matrix dependent on the specific application 2 ? A natural model for such type of sparsity recovery problems is: The least square term is from the sub-Gaussian noise assumption and the second term is due to the spar-sity requirement. Since this combinatorial optimiza-tion problem is NP-hard, the conventional ` 1 relax-ation technique can be applied to make it tractable, resulting in the following convex model: Such model includes many well-known sparse formu-lations as special cases:  X  The fused LASSO (Tibshirani et al., 2005; Fried- X  The general K dimensional changing point de- X  The second term of (4), that is, the total vari-This paper studies the theoretical properties of prob-lem (2) by providing an upper bound of the estimate error, that is, k  X   X   X   X   X  k where  X   X  denotes the estimation. The consistency property of this model is shown by assuming that the design matrix  X  is a Gaussian ran-dom matrix. Specifically, we show 1) in the noiseless case, if the condition number of D is bounded and the measurement number n  X   X ( s log( p )) where s is the sparsity number, then the true solution can be recov-ered under some mild conditions with high probabil-ity; and 2) in the noisy case, if the condition number of D is bounded and the measurement number increases faster than s log( p ), that is, n = O ( s log( p )), then the estimate error converges to zero with probability 1 un-der some mild conditions when p goes to infinity. Our results are consistent with those for the special case D = I p  X  p (equivalently LASSO) and improve the ex-isting analysis in (Cand`es et al., 2011; Vaiter et al., 2013). To the best of our knowledge, this is the first work that establishes the consistency properties for the general problem (2). The condition number of D plays a critical role in our analysis. We consider the condi-tion numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if m p (that is, #edge #vertex ) is larger than a certain constant. Numerical simulations are consistent with our theoretical results. 1.1. Notations and Assumptions Define where l 1 and l 2 are nonnegative integers, Y is the dic-tionary matrix, and H ( Y,l 2 ) is the union of all sub-spaces spanned by l 2 columns of Y : Note that the length of h is the sum of l 1 and the dimension of the subspace H (which is in general not equal to l 2 ). The definition of  X  +  X  ,Y ( l 1  X 
 X  ,Y ( l 1 ,l 2 ) is inspired by the D-RIP constant (Cand`es et al., 2011). Recall that the D-RIP constant  X  d defined by the smallest quantity such that (1  X   X  d ) k h k 2  X k  X  h k 2  X  (1 +  X  d ) k h k 2  X  h  X  X  ( Y,l One can verify that  X  d = max {  X  +  X  ,Y (0 ,l 2 )  X  1 , 1  X   X 
 X  ,Y (0 ,l 2 ) } if  X  satisfies the D-RIP condition in terms of the sparsity l 2 and the dictionary Y . Denote  X  spectively for short.
 Denote the compact singular value decomposition (SVD) of D as D = U  X  V T  X  . Let Z = U  X  and its pseudo-inverse be Z + =  X   X  1 U T . One can ver-ify that Z + Z = I .  X  min ( D ) denotes the minimal nonzero singular value of D and  X  max ( D ) denotes the maximal one, that is, the spectral norm k D k . One  X  max ( Z ) =  X  Let T 0 be the support set of D X   X  , that is, a subset of mentary index set with respect to { 1 , 2 ,  X  X  X  ,m } . With-out loss of generality, we assume that D does not con-tain zero rows. Assume that c =  X   X  + where  X  R n and all entries i  X  X  are i.i.d. centered sub-Gaussian random variables with sub-Gaussian norm  X  (Read-ers who are not familiar with the sub-Gaussian norm can treat  X  as the standard derivation in Gaussian random variable). In discussing the dimensions of the problem and how they are related to each other in the limit (as n and p both approach  X  ), we make use of order notation. If  X  and  X  are both positive quantities that depend on the dimensions, we write  X  = O (  X  ) if  X  can be bounded by a fixed multiple of  X  for all suffi-ciently large dimensions. We write  X  = o (  X  ) if for any positive constant  X  &gt; 0, we have  X   X   X  X  for all suffi-ciently large dimensions. We write  X  =  X (  X  ) if both  X  = O (  X  ) and  X  = O (  X  ). Throughout this paper, a Gaussian random matrix means that all entries follow i.i.d. standard Gaussian distribution N (0 , 1). 1.2. Related Work Cand`es et al. (2011) proposed the following formula-tion to solve the problem in this paper: where D  X  R m  X  p is assumed to have orthogonal columns and  X  is taken as the upper bound of k k . They showed that the estimate error is bounded by C  X  + C 1 k ( D X   X  ) T c k 1 / p | T | with high probability if  X  n  X   X  R n  X  p is a Gaussian random matrix 3 with n  X   X ( s log m ), where C 0 and C 1 are two constants. Letting T = T 0 and  X  = k k , the error bound turns out to be C 0 k k . This result shows that in the noise-less case, with high probability, the true signal can be exactly recovered. In the noisy case, assume that i  X  X  ( i = 1 ,  X  X  X  ,n ) are i.i.d centered sub-Gaussian random variables, which implies that k k 2 is bounded by  X ( n ) with high probability. Note that since the measure-ment matrix  X  is scaled by 1 / of  X  X aussian random matrix X  in (Cand`es et al., 2011), the noise vector should be corrected similarly. In other words, k k 2 should be bounded by  X (1) rather than  X ( n ), which implies that the estimate error in (Cand`es et al., 2011) converges to a constant asymptotically. Nama et al. (2012) considered the noiseless case and analyzed the formulation assuming all rows of D to be in the general position, that is, any p rows of D are linearly independent, which is violated by the fused LASSO. An sufficient condition was proposed to recover the true signal  X   X  using the cosparse analysis.
 Vaiter et al. (2013) also considered the formulation in Eq. (2) but mainly gave robustness analysis for this model using the cosparse technique. A sufficient con-dition [different from Nama et al. (2012)] to exactly re-cover the true signal was given in the noiseless case. In the noisy case, they took  X  to be a value proportional to k k and proved that the estimate error is bounded by  X ( k k ) under certain conditions. However, they did not consider the Gaussian ensembles for  X ; see (Vaiter et al., 2013, Section 3.B).
 The fused LASSO, a special case of Eq. (2), was also studied recently. The sufficient condition of detecting jumping points is given by Kolar et al. (2009). A spe-cial fused LASSO formulation was considered by Ri-naldo (2009) in which  X  was set to be the identity matrix and D to be the combination of the identity matrix and the total variance matrix. Sharpnack et al. (2012) proposed and studied the edge LASSO by let-ting  X  be the identity matrix and D be the matrix corresponding to the edges of a graph. 1.3. Organization The remaining of this paper is organized as follows. To build up a uniform analysis framework, we sim-plify the formulation (2) in Section 2. The main result is presented in Section 3. Section 4 analyzes the value of an important parameter in our main results in two cases: the fused LASSO and the random graph. The numerical simulation is presented to verify the rela-tionship between the estimate error and the condition number in Section 5. We conclude this paper in Sec-tion 6. All proofs are provided in the long version of this paper (Liu et al., 2013). As highlighted by Vaiter et al. (2013), the analysis for a wide D  X  R m  X  p (that is, p &gt; m ) significantly differs from a tall D (that is, p &lt; m ). To build up a uniform analysis framework, we use the singular value decomposition (SVD) of D to simplify Eq. (2), which leads to an equivalent formulation.
 Consider the compact SVD of D : D = U  X  V T  X  where U  X  R n  X  r ,  X   X  R r  X  r ( r is the rank of D ), and V R is a unitary matrix. Let  X  = V T  X   X  and  X  = V T  X   X  . These two linear transformations split the original signal into two parts as follows: min Z = U  X   X  R m  X  r . Let  X   X ,  X   X  be the solution of Eq. (10). One can see the relationship between  X   X  and  X   X  : 4 which can be used to further simplify Eq. (10): min Let and We obtain the following simplified formulation: Denote the solution of Eq. (2) as  X   X  and the ground truth as  X   X  . One can verify  X   X  = V [  X   X  T  X   X  T ] fine  X   X  := V T  X   X   X  and  X   X  := V T  X   X   X  . Note that un-like  X   X  and  X   X  the following usually does not hold:  X   X  =  X  ( A T A )  X  1 A T ( B X   X   X  c ). Let h =  X   X   X   X   X  d =  X   X   X   X   X  . We will study the upper bound of k  X   X   X   X  in terms of k h k and k d k based on the relationship This section presents the main results in this paper. The estimate error by Eq. (2), or equivalently Eq. (11), is given in Theorem 1: Theorem 1. Define where  X   X  + ( p  X  r,. ) and  X   X   X  ( p  X  r,. ) denote  X  +  X  &gt; 2 k ( Z + ) T X T k  X  in Eq. (2) , we have if A T A is in-vertible (apparently, n  X  p  X  r is required) and there ex-ists an integer l &gt; 9  X  2 s such that W Xh, 1  X  W Xh, 2 0 , then where One can see from the proof that the first term of (12) is mainly due to the estimate error of the sparse part  X  and the second term is due to the estimate error of the free part  X  .
 The upper bound in Eq. (12) strongly depends on pa-rameters about X and Z + such as  X  + X,Z + (  X  ),  X   X  X,Z +  X   X  X and Z + are fixed, it is still challenging to evaluate these parameters. Similar to existing literature like (Cand`es &amp; Tao, 2005), we assume  X  to be a Gaus-sian random matrix and estimate the values of these parameters in Theorem 2.
 Theorem 2. Assume that  X  is a Gaussian random matrix. The following holds with probability at least 1  X  2 exp { X   X ( k log( em/k )) } : q  X  + q  X   X  The following holds with probability at least 1  X  2 exp { X   X  (( k + p  X  r ) log( ep/ ( k + p  X  r ))) } : q  X   X  Now we are ready to analyze the estimate error given in Eq. (12). Two cases are considered in the following: the noiseless case = 0 and the noisy case 6 = 0. 3.1. Noiseless Case = 0 First let us consider the noiseless case. Since = 0, the second term in Eq. (12) vanishes. We can choose a value of  X  to make the first term in Eq. (12) arbitrar-ily small. Hence the true signal  X   X  can be recovered with an arbitrary precision as long as W  X  &gt; 0, which is equivalent to requiring W Xh, 1  X  W Xh, 2 W  X  &gt; 0. Actu-ally, when  X  is extremely small, Eq. (2) approximately solves the problem in Eq. (7) with  X  = 0.
 Intuitively, the larger the measurement number n is, the easier the true signal  X   X  can be recovered, since more measurements give a feasible subspace with a lower dimension. In order to estimate how many mea-surements are required, we consider the measurement matrix  X  to be a Gaussian random matrix (This is also a standard setup in compressive sensing.). Since this paper mainly focuses on the large scale case, one can treat the value of l as a number proportional to  X  2 s . Using Eq. (13) and Eq. (14), we can estimate the lower Lemma 1. Assume  X  to be a Gaussian random ma-trix. Let l = (10  X  ) 2 s . With probability at least 1  X  2 exp { X   X (( s + l ) log( em/ ( s + l ))) } , we have From Lemma 1, to recover the true signal, we only need To simplify the discussion, we propose several minor conditions first in Assumption 1.
 Assumption 1. Assume that  X  p  X  r  X   X n (  X  &lt; 1) in the noiseless case and  X  the condition number  X  =  X  max ( D )  X  m =  X ( p i ) where i &gt; 0 , that is, m can be a poly-One can verify that under Assumption 1, taking l = (10  X  ) 2 s =  X ( s ), the right hand side of (17) is greater than  X ( n )  X   X ( p ns log( em/s )) =  X ( n )  X   X ( p ns log( ep/s )) . Letting n  X   X ( s log( ep/s )) [or  X ( s log( em/s )) if with-out assuming m =  X ( p i )], one can have that
W Xh, 1  X  W Xh, 2 W  X   X   X ( n )  X   X ( p ns log( ep/s )) &gt; 0 holds with high probability (since the probability in Lemma 1 converges to 1 while p goes to infinity). In other words, in the noiseless case the true signal can be recovered at an arbitrary precision with high prob-ability.
 To compare with existing results, we consider two spe-cial cases: D = I p  X  p (Cand`es &amp; Tao, 2005) and D has orthogonal columns (Cand`es et al., 2011), that is, D
T D = I . When D = I p  X  p and  X  is a Gaussian ran-dom matrix, the required measurements in (Cand`es &amp; Tao, 2005) are  X ( s log( ep/s )) , which is the same as ours. Also note that if D = I p  X  p , Assumption 1 is satisfied automatically. Thus our result does not en-force any additional condition and is consistent with existing analysis for the special case D = I p  X  p . Next we consider the case when D has orthogonal columns as in (Cand`es et al., 2011). In this situation, all condi-tions except m =  X ( p i ) in Assumption 1 are satisfied. One can easily verify that the required measurements to recover the true signal are  X ( s log( em/s )) without assuming m =  X ( p i ) from our analysis above, which is consistent with the result in (Cand`es et al., 2011). 3.2. Noisy Case 6 = 0 Next we consider the noisy case, that is, study the up-per bound in (12) while 6 = 0. Similarly, we mainly focus on the large scale case and assume Gaussian en-sembles for the measurement matrix  X . Theorem 3 provides the upper bound of the estimate error under the conditions in Assumption 1. Theorem 3. Assume that the measurement matrix  X  is a Gaussian random matrix, the measurement satis-fies n = O ( s log p ) , and Assumption 1 holds. Taking  X  = C k ( Z + ) T X T k  X  with C &gt; 2 in Eq. (2) , we have with the probability at least 1  X   X ( p  X  1 )  X   X ( m  X  1  X (  X  s log( ep/s )) .
 One can verify that when p goes to infinity, the upper bound in (18) converges to 0 from n = O ( s log p ) and the probability converges to 1 due to m =  X ( p i ). It means that the estimate error converges to 0 asymp-totically given the measures n = O ( s log p ). This result shows the consistency property, that is, if the measurement number n grows faster than s log( p ), the estimate error will vanish. This consistency prop-erty is consistent with the special case LASSO by tak-ing D = I p  X  p (Zhang, 2009a). Cand`es et al. (2011) considered Eq. (6) and obtained an upper bound for the estimate error  X (  X / the consistency property like ours since  X  =  X ( k k ) =  X (  X  timation error bound converges to a constant given n = O ( s log p ).
 In addition, from the derivation of Eq. (18), one can simply verify that the boundedness requirement for  X  can actually be removed, if we allow more obser-vations, for example, n = O (  X  4 s log p ). Here we en-force the boundedness condition just for simplification of analysis and a convenient comparison to the stan-dard LASSO (it needs n = O ( s log p ) measurements). Since  X  is a key factor from the derivation of Eq. (18), we consider the fused LASSO and the random graphs and estimate the values of  X  in these two cases. Let us consider the fused LASSO first. The transfor-mation matrix D is One can verify that and which implies that  X  min ( D )  X  1 and  X  max ( D )  X  3. Hence we have  X   X  3 in the fused LASSO case.
 Next we consider the the random graph. The trans-formation matrix D corresponding to a random graph is generated in the following way: (1) each row is in-dependent of the others; (2) two entries of each row are uniformly selected and are set to 1 and  X  1 re-spectively; (3) the remaining entries are set to 0. The following result shows that the condition number of D is bounded with high probability.
 Theorem 4. For any m and p satisfying that m  X  cp where c is large enough, the following holds: with probability at least 1  X  2 exp { X   X ( p ) } . From this theorem, one can see that  X  If m = cp where c is large enough, then  X  If m = p ( p  X  1) / 2 which is the maximal possible In this section, we use numerical simulations to verify some of our theoretical results. Given a problem size n and p and condition number  X  , we randomly generate D as follows. We first construct a p  X  p diagonal matrix D 0 such that Diag( D 0 ) &gt; 0 and then construct a random basis matrix V  X  R p  X  p , and let D = D 0 V . Clearly, D has independent columns and the condition number equals to  X  . Next, a vec-tor x  X  R p is generated such that x i  X  N (0 , 1), i = 1 ,..., p 10 and x j = 0, j = p 10 + 1 ,...,p .  X   X  then obtained as  X   X  = D  X  1 x . Finally, we generate a matrix  X   X  R n  X  p with  X  ij  X  N (0 , 1), noise  X  R n with i  X  X  (0 , 0 . 001) and y =  X   X   X  + .
 We solve Eq. (2) using the standard optimization pack-age CVX 6 and  X  is set as  X  = 2 k ( Z + ) T X T k  X  as sug-gested by Theorem 1. We use three different sizes of problems, with n  X  { 40 , 100 , 200 } , p  X  { 50 , 150 , 300 } and  X  ranging from 1 to 1000. For each problem set-ting, 100 random instances are generated and the av-erage performance is reported. We use the relative mance with respect to different condition numbers in Figure 1. We can observe from Figure 1 that in all three cases the relative error increases when the condi-tion number increases. If we fix the condition number, by comparing the three curves, we can see that the rel-ative error decreases when the problem size increases. These are consistent with our theoretical results in Sec-tion 3 [see Eq. (18)]. This paper considers the problem of estimating a spe-cific type of signals which is sparse under a given linear transformation D . A conventional convex relaxation technique is used to convert this NP-hard combina-torial optimization into a tractable problem. We de-velop a unified framework to analyze the convex for-mulation with a generic D and provide the estimate error bound. Our main results establish that 1) in the noiseless case, if the condition number of D is bounded and the measurement number n  X   X ( s log( p )) where s is the sparsity number, then the true solution can be recovered with high probability; and 2) in the noisy case, if the condition number of D is bounded and the measurement number grows faster than s log( p ) [that is, s log( p ) = o ( n )], then the estimate error converges to zero when p and s go to infinity with probability 1. Our results are consistent with existing literature for the special case D = I p  X  p (equivalently LASSO) and improve the existing analysis for the same formu-lation. The condition number of D plays a critical role in our theoretical analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph. The condition number in the fused LASSO case is bounded by a constant, while the con-dition number in the random graph case is bounded with high probability if m p (that is, #edge #vertex ) is larger than a certain constant. Numerical simulations are consistent with our theoretical results.
 In future work, we plan to study a more general for-mulation of Eq. (2): where D is an arbitrary matrix and f (  X  ) is a convex and smooth function satisfying the restricted strong convexity property. We expect to obtain similar con-sistency properties for this general formulation. This work was supported in part by NSF grants IIS-0953662 and MCB-1026710. We would like to sincerely thank Professor Sijian Wang and Professor Eric Bach of the University of Wisconsin-Madison for useful dis-cussion and helpful advice.
 Bunea, F., Tsybakov, A., and Wegkamp, M. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics , 1:169 X 194, 2007.
 Cand`es, E. J. and Plan, Y. Near-ideal model selection by ` 1 minimization. Annals of Statistics , 37(5A): 2145 X 2177, 2009.
 Cand`es, E. J. and Tao, T. Decoding by linear program-ming. IEEE Transactions on Information Theory , 51(12):4203 X 4215, 2005.
 Cand`es, E. J. and Tao, T. The Dantzig selector: Sta-tistical estimation when p is much larger than n . Annals of Statistics , 35(6):2313 X 2351, 2007.
 Cand`es, E. J., Romberg, J., and Tao, T. Robust uncer-tainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE
Transactions on Information Theory , 52(2):489 X  509, 2006.
 Cand`es, E. J., Eldar, Y. C., Needell, D., and Randall,
P. Compressed sensing with coherent and redundant dictionaries. Applied and Computational Harmonic Analysis , 31:59 X 73, 2011.
 Chan, T. F. Total variation blind deconvolution.
IEEE Transactions on Image Processing , 7(3):370 X  375, 1998.
 Donoho, D. L., Elad, M., and Temlyakov, V. N. Sta-ble recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on In-formation Theory , 52(1):6 X 18, 2006.
 Friedman, J., Hastie, T., Hofling, H., and Tibshirani, R. Pathwise coordinate optimization. Annals of Applied Statistics , 1(2):302 X 332, 2007.
 Kolar, M., Song, L., and Xing, E.P. Sparsistent learning of varying-coefficient models with struc-tural changes. NIPS , pp. 1006 X 1014, 2009.
 Koltchinskii, V. and Yuan, M. Sparse recovery in large ensembles of kernel machines on-line learning and bandits. COLT , pp. 229 X 238, 2008.
 Liu, J., Wonka, P., and Ye, J. A multi-stage framework for Dantzig selector and Lasso. Journal of Machine Learning Research , 13:1189 X 1219, 2012.
 Liu, J., Yuan, L., and Ye, J. Guaranteed sparse recov-ery under linear transformation. arXiv:1305.0047 , 2013.
 Lounici, K. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig esti-mators. Electronic Journal of Statistics , 2:90 X 102, 2008.
 Meinshausen, N., Bhlmann, P., and Zrich, E. High dimensional graphs and variable selection with the Lasso. Annals of Statistics , 34(3):1436 X 1462, 2006. Nama, S., Daviesb, M. E., Eladc, M., and Gribonvala,
R. The cosparse analysis model and algorithms. Ap-plied and Computational Harmonic Analysis , 34(1): 30 X 56, 2012.
 Ravikumar, P., Raskutti, G., Wainwright, M. J., and
Yu, B. Model selection in gaussian graphical mod-els: High-dimensional consistency of ` 1 -regularized MLE. NIPS , 2008.
 Rinaldo, A. Properties and refinements of the fused-
Lasso. The Annals of Statistics , 37(5B):2922 X 2952, 2009.
 Romberg, J. The Dantzig selector and generalized thresholding. CISS , pp. 22 X 25, 2008.
 Sharpnack, J., Rinaldo, A., and Singh, A. Sparsistency of the edgeLasso over graphs. AISTAT , 2012.
 Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. Sparsity and smoothness via the fused-Lasso. Journal of the Royal Statistical Society Series B , pp. 91 X 108, 2005.
 Vaiter, S., Peyre, G., Dossal, C., and Fadili, J. Robust sparse analysis regularization. IEEE Transaction on Information Theory , 59(4):2001 X 2016, 2013.
 Wainwright, M. J. Sharp thresholds for high-dimensional and noisy sparsity recovery using ` 1 -constrained quadratic programming (Lasso). IEEE
Transactions on Information Theory , 55(5):2183 X  2202, 2009.
 Zhang, T. Some sharp performance bounds for least squares regression with ` 1 regularization. Annals of Statistics , 37(5A):2109 X 2114, 2009a.
 Zhang, T. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Research , 10:555 X 568, 2009b.
 Zhao, P. and Yu, B. On model selection consistency of Lasso. Journal of Machine Learning Research , 7:
