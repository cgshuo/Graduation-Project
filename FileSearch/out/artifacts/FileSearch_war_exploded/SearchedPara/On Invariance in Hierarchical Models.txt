 Several models of object recognition drawing inspiration from visual cortex have been developed over the past few decades [3, 8, 6, 12, 10, 9, 7], and have enjoyed substantial empirical success. A central theme found in this family of models is the use of Hubel and Wiesel X  X  simple and complex cell ideas [5]. In the primary visual cortex, simple units compute features by looking for the occur-rence of a preferred stimulus in a region of the input ( X  X eceptive field X ). Translation invariance is then explicitly built into the processing pathway by way of complex units which pool locally over simple units. The alternating simple-complex filtering/pooling process is repeated, building increas-ingly invariant representations which are simultaneously selective for increasingly complex stimuli. In a computer implementation, the final representation can then be presented to a supervised learning algorithm.
 Following the flow of processing in a hierarchy from the bottom upwards, the layerwise representa-tions gain invariance while simultaneously becoming selective for more complex patterns. A goal of central importance in the study of such hierarchical architectures and the visual cortex alike is that of understanding quantitatively this invariance-selectivity tradeoff, and how invariance and selectivity contribute towards providing an improved representation useful for learning from examples. In this paper, we focus on hierarchical models incorporating an explicit attempt to impose transformation invariance, and do not directly address the case of deep layered models without local transformation or pooling operations (e.g. [4]).
 In a recent effort, Smale et al. [11] have established a framework which makes possible a more pre-cise characterization of the operation of hierarchical models via the study of invariance and discrim-ination properties. However, Smale et al. study invariance in an implicit, rather than constructive, fashion. In their work, two cases are studied: invariance with respect to image rotations and string reversals, and the analysis is tailored to the particular setting. In this paper, we reinterpret and ex-tend the invariance analysis of Smale et al. using a group-theoretic language towards clarifying and unifying the general properties necessary for invariance in a family of hierarchical models. We show that by systematically applying algebraic tools, one can provide a concise set of conditions which must be met to establish invariance, as well as a constructive prescription for meeting those condi-tions. We additionally find that when one imposes the mild requirement that the transformations of interest have group structure, a broad class of hierarchical models can only be invariant to orthog-onal transformations. This result suggests that common architectures found in the literature might need to be rethought and modified so as to allow for broader invariance possibilities. Finally, we show that our framework automatically points the way to efficient computational implementations of invariant models.
 The paper is organized as follows. We first recall important definitions from Smale et al. Next, we extend the machinery of Smale et al. to a more general setting allowing for general pooling func-tions, and give a proof for invariance of the corresponding family of hierarchical feature maps. This contribution is key because it shows that several results in [11] do not depend on the particular choice of pooling function. We then establish a group-theoretic framework for characterizing invariance in hierarchical models expressed in terms of the objects defined here. Within this framework, we turn to the problem of invariance in two specific domains of practical relevance: images and text strings. Finally, we conclude with a few remarks summarizing the contributions and relevance of our work. All proofs are omitted here, but can be found in the online supplementary material [2] . The reader is assumed to be familiar with introductory concepts in group theory. An excellent reference is [1]. We first review important definitions and concepts concerning the neural response feature map pre-sented in Smale et al. The reader is encouraged to consult [11] for a more detailed discussion. We will draw attention to the conditions needed for the neural response to be invariant with respect to a family of arbitrary transformations, and then generalize the neural response map to allow for arbitrary pooling functions. The proof of invariance given in [11] is extended to this generalized setting. The proof presented here (and in [11]) hinges on a technical  X  X ssumption X  which must be verified to hold true, given the model and the transformations to which we would like to be invariant. Therefore the key step to establishing invariance is verification of this Assumption. After stating the Assumption and how it figures into the overall picture, we explore its verification in Section 3. There we are able to describe, for a broad range of hierarchical models (including a class of convolutional neural networks [6]), the necessary conditions for invariance to a set of transformations. 2.1 Definition of the Feature Map and Invariance First consider a system of patches of increasing size associated to successive layers of the hierarchy, v 1  X  v 2  X   X  X  X   X  v n  X  S , with v n taken to be the size of the full input. Here layer n is the top-most layer, and the patches are pieces of the domain on which the input data are defined. The set S could contain, for example, points in R 2 (in the case of 2D graphics) or integer indices (the case of strings). Until Section 4, the data are seen as general functions, however it is intuitively helpful to think of the special case of images, and we will use a notation that is suggestive of this particular case. Next, we X  X l need spaces of functions on the patches, Im( v i ) . In many cases it will only be necessary to work with arbitrary successive pairs of patches (layers), in which case we will denote by u the smaller patch, and v the next larger patch. We next introduce the transformation sets H i ,i = 1 ,...,n intrinsic to the model. These are abstract sets in general, however here we will take them to be comprised of translations with h  X  H i defined by h : v i  X  v i +1 . Note that by construction, the functions h  X  H i implicitly involve restriction. For example, if f  X  Im( v 2 ) is an image of size v 2 and h  X  H 1 , then f  X  h is a piece of the image of size v 1 . The particular piece is determined by h . Finally, to each layer we also associate a dictionary of templates, Q i  X  Im( v i ) . The templates could be randomly sampled from Im( v i ) , for example.
 defined as follows.
 Definition 1 (Neural Response) . Given a non-negative valued, normalized, initial reproducing ker-nel b K 1 , the m -th derived kernel b K m , for m = 2 ,...,n , is obtained by normalizing K m ( f,g ) =  X  N response decomposes the input into a hierarchy of parts, analyzing sub-regions at different scales. The neural response and derived kernels describe in compact, abstract terms the core operations built into the many related hierarchical models of object recognition cited above.
 We next define a set of transformations, distinct from the H i above, to which we would like to be invariant. Let r  X  R i , i  X  { 1 ,...,n  X  1 } , be transformations that can be viewed as mapping degenerate translations and transformations, h or r mapping their entire domain to a single point. When it is necessary to identify transformations defined on a specific domain v , we will use the notation r v : v  X  v . Invariance of the neural response feature map can now be defined. Definition 2 (Invariance) . The feature map N m is invariant to the domain transformation r  X  R if N In order to state the invariance properties of a given feature map, a technical assumption is needed. Assumption 1 (from [11]) . Fix any r  X  X  . There exists a surjective map  X  : H  X  H satisfying for all h  X  H .
 This technical assumption is best described by way of an example. Consider images and rotations: the assumption stipulates that rotating an image and then taking a restriction must be equivalent to first taking a (different) restriction and then rotating the resulting image patch. As we will describe below, establishing invariance will boil down to verifying Assumption 1. 2.2 Invariance and Generalized Pooling We next provide a generalized proof of invariance of a family of hierarchical feature maps, where the properties we derive do not depend on the choice of the pooling function. Given the above assumption, invariance can be established for general pooling functions of which the max is only one particular choice. We will first define such general pooling functions, and then describe the corresponding generalized feature maps. The final step will then be to state an invariance result for the generalized feature map, given that Assumption 1 holds.
 Let H = H i , with i  X  { 1 ,...,n  X  1 } , and let B ( R ) denote the Borel algebra of R . As in Assump-tion 1, we define  X  : H  X  H to be a surjection, and let  X  : B ( R ++ )  X  R ++ be a bounded pooling function defined for Borel sets B  X  B ( R ) consisting of only positive elements. Here R ++ denotes the set of strictly positive reals. Given a positive functional F acting on elements of H , we define the set F ( H )  X  X  ( R ) as Note that since  X  is surjective,  X  ( H ) = H , and therefore ( F  X   X  )( H ) = F ( H ) . With these definitions in hand, we can define a more general neural response as follows. For H = H m  X  1 and all q  X  Q = Q m  X  1 , let the neural response be given by where Given Assumption 1, we can now prove invariance of a neural response feature map built from the general pooling function  X  .
 for all r  X  X  , f  X  Im( v 1 ) , then for all r  X  X  , f  X  Im( v m ) and m  X  n .
 We give a few practical examples of the pooling function  X  .
 Maximum: The original neural response is recovered setting  X ( B ) = sup B .
 Averaging: We can consider average pooling by setting  X ( B ) = R x  X  B xd X . If H has a measure  X 
H , then a natural choice for  X  is the induced push-forward measure  X  H  X  F  X  1 . The measure  X  H may be simply uniform, or in the case of a finite set H , discrete. Similarly, we may consider more general weighted averages. This section establishes general definitions and conditions needed to formalize a group-theoretic concept of invariance. When Assumption 1 holds, then the neural response map can be made in-variant to the given set of transformations. Proving invariance thus reduces to verifying that the Assumption actually holds, and is valid. A primary goal of this paper is to place this task within an algebraic framework so that the question of verifying the Assumption can be formalized and explored in full generality with respect to model architecture, and the possible transformations. For-malization of Assumption 1 culminates in Definition 3 below, where purely algebraic conditions are separated from conditions stemming from the mechanics of the hierarchy. This separation re-sults in a simplified problem because one can then tackle the algebraic questions independent of and untangled from the model architecture.
 Our general approach is as follows. We will require that R is a subset of a group and then use algebraic tools to understand when and how Assumption 1 can be satisfied given different instances of R . If R is fixed, then the assumption can only be satisfied by placing requirements on the sets of built-in translations H i ,i = 1 ,...,n . Therefore, we will make quantitative, constructive statements about the minimal sets of translations associated to a layer required to support invariance to a set of transformations. Conversely, one can fix H i and then ask whether the resulting feature map will be invariant to any transformations. We explore this perspective as well, particularly in the examples of Section 4, where specific problem domains are considered. 3.1 Formulating Conditions for Invariance Recall that v i  X  S . Because it will be necessary to translate in S , it is assumed that an appropriate notion of addition between the elements of S is given. If G is a group, we denote the (left) action of G on S by A : G  X  S  X  S . Given an element g  X  G , the notation A g : S  X  S will be utilized. Since an arbitrary pair of successive layers with associated patch sizes u and v , with u  X  v  X  S . Recall that the definition of the neural response involves the  X  X uilt-in X  translation functions h : u  X  v , for h  X  H = H u . Since S has an addition operation, we may parameterize h  X  H explicitly as h ( x ) = x + a for x  X  u and parameter a  X  v such that ( u + a )  X  v . The restriction behavior of the translations in H prevents us from simply generating a group out of the elements of H . To get around this difficulty, we will decompose the h  X  H into a composition of two functions: a translation group action and an inclusion.
 Let S generate a group of translations T by defining the injective map That is, to every element of a  X  S we associate a member of the group T whose action corresponds to translation in S by a : A t a ( x ) = x + a for x,a  X  S . (Although we assume the specific case of translations throughout, the sets of intrinsic operations H i may more generally contain other kinds of transformations. We assume, however, that T is abelian. ) Furthermore, because the translations H can be parameterized by an element of S , one can apply Equation (2) to define an injective map  X  : H  X  T by h a 7 X  t a . Finally, we define  X  u : u ,  X  S to be the canonical inclusion of u into S . We can now rewrite h a : u  X  v as Note that because a satisfies ( u + a )  X  v by definition, im ( A t a  X   X  u )  X  v automatically. In the statement of Assumption 1, the transformations r  X  R can be seen as maps from u to itself, or from v to itself, depending on which side of Equation (1) they are applied. To avoid confusion we denoted the former case by r u and the latter by r v . Although r u and r v are the same  X  X ind X  of transformation, one cannot in general associate to each  X  X ind X  of transformation r  X  R a single element of some group as we did in the case of translations above. The group action could very well be different depending on the context. We will therefore consider r u and r v to be distinct transformations, loosely associated to r . In our development, we will make the important assumption that the transformations r u ,r v  X  R can be expressed as actions of elements of some group, and denote this group by R . More precisely, for every r u  X  R , there is assumed to be a corresponding R , there is assumed to be a corresponding element  X  v  X  R whose action satisfies A  X  for all x  X  v . The distinction between  X  u and  X  v will become clear in the case of feature maps defined on functions whose domain is a finite set (such as strings). In the case of images, we will see that  X  u =  X  v .
 Assumption 1 requires that r v  X  h = h 0  X  r u for h,h 0  X  H , with the map  X  : h 7 X  h 0 onto. We now restate this condition in group-theoretic terms. Define  X  T =  X  ( H u )  X  T to be the set of group elements corresponding to H u . Set h = h a ,h 0 = h b , and denote also by r u ,r v the elements of the group R corresponding to the given transformation r  X  R . The Assumption says in part that r  X  h = h 0  X  r u for some h 0  X  H . This can now be expressed as for some t b  X   X  T . In order to arrive at a purely algebraic condition for invariance, we will need to understand and manipulate compositions of group actions. However on the right-hand side of Equation (3) the translation A t b is separated from the transformation A r u by the inclusion  X  u . We will therefore need to introduce an additional constraint on R . This constraint leads to our first condition for invariance: If x  X  u , then we require that A r u ( x )  X  u for all r  X  R . One can now see that if this condition is met, then verifying Equation (3) reduces to checking that and that the map t a 7 X  t b is onto.
 The next step is to turn compositions of actions A x  X  A y into an equivalent action of the form A xy . Do do this, one needs R and T to be subgroups of the same group G so that the associativity property of group actions applies. A general way to accomplish this is to form the semidirect product Recall that the semidirect product G = X o Y is a way to put two subgroups X,Y together where X is required to be normal in G , and X  X  Y = { 1 } (the usual direct product requires both subgroups to be normal). In our setting G is easily shown to be isomorphic to a group with normal subgroup T and subgroup R where each element may be written in the form g = tr for t  X  T,r  X  R . We will see below that we do not loose generality by requiring T to be normal. Note that although this con-struction precludes R from containing the transformations in T , allowing R to contain translations is an uninteresting case.
 Consider now the action A g for g  X  G = T o R . Returning to Equation (4), we can apply the associativity property of actions and see that Equation (4) will hold as long as for every r  X  R . This is our second condition for invariance, and is a purely algebraic requirement concerning the groups R and T , distinct from the restriction related conditions involving the patches u and v .
 The two invariance conditions we have described thus far combine to capture the content of Assump-tion 1, but in a manner that separates group related conditions from constraints due to restriction and the nested nature of an architecture X  X  patch domains. We can summarize the invariance conditions in the form of a concise definition that can be applied to establish invariance of the neural response feature maps N m ( f ) , 2  X  m  X  n with respect to a set of transformations. Let  X  R  X  R be the set of transformations for which we would like to prove invariance, in correspondence with R . Definition 3 (Compatible Sets) . The subsets  X  R  X  R and  X  T  X  T are compatible if all of the following conditions hold: The final condition above has been added to ensure that any set of translations  X  T we might construct satisfy the implicit assumption that the hierarchy X  X  translation functions h  X  H are maps which respect the definition h : u  X  v .
 If  X 
R and  X  T are compatible, then for each t a  X   X  T Equation 3 holds for some t b  X   X  T , and the map t 7 X  t b is surjective from  X  T  X   X  T (by Condition (1) above). So Assumption 1 holds.
 provide insight into the structure of compatible sets. 3.2 Orbits and Compatible Sets Suppose we assume that  X  R is a subgroup (rather than just a subset), and ask for the smallest com-patible  X  T . We will show that the only way to satisfy Condition (1) in Definition 3 is to require that  X  T be a union of  X  R -orbits , under the action for t  X  T , r  X   X  R . This perspective is particularly illuminating because it will eventually allow us to view conjugation by a transformation r as a permutation of  X  T , thereby establishing surjectivity of the map  X  defined in Assumption 1. For computational reasons, viewing  X  T as a union of orbits is also convenient.
 If r v = r u = r , then the action (7) is exactly conjugation and the  X  R -orbit of a translation t  X  T classes induced by  X  R .
 The following Proposition shows that, given set of candidate translations in H , we can construct a conjugation.
 Proposition 1. Let  X   X  T be a given set of translations, and assume the following: (1) G  X  = T o R , (2) For each r  X  R , r = r u = r v , (3)  X  R is a subgroup of R . Then Condition (1) of Definition 3 is satisfied if and only if  X  T can be expressed as a union of orbits of the form An interpretation of the above Proposition, is that when  X  T is a union of  X  R -orbits, conjugation by r can be seen as a permutation of  X  T . In general, a given  X  T may be decomposed into several such orbits and the conjugation action of  X  R on  X  T may not necessarily be transitive. We continue with specific examples relevant to image processing and text analysis. 4.1 Isometries of the Plane Consider the case where G is the group M of planar isometries, u  X  v  X  S = R 2 , and H involves translations in the plane. Let O 2 be the group of orthogonal operators, and let t a  X  T denote a translation represented by the vector a  X  R 2 . In this section we assume the standard basis and work with matrix representations of G when it is convenient.
 We first need that T C M , a property that will be useful when verifying Condition (1) of Definition 3. Indeed, from the First Isomorphism Theorem [1], the quotient space M/T is isomorphic to O 2 , giving the following commutative diagram: where the isomorphism  X   X  : M/T  X  O 2 is given by  X   X  ( mT ) =  X  ( m ) and  X  ( m ) = mT . We recall that the kernel of a group homomorphism  X  : G  X  G 0 is a normal subgroup of G , and that normal subgroups N of G are invariant under the operation of conjugation by elements g of G . That is, gNg  X  1 = N for all g  X  G . With this picture in mind, the following Lemma establishes that T C M , and further shows that M is isomorphic to T o R with R = O 2 , and T a normal subgroup of M . Lemma 1. For each m  X  M , t a  X  T , mt a = t b m for some unique element t b  X  T .
 We are now in a position to verify the Conditions of Definition 3 for the case of planar isometries. Proposition 2. Let H be the set of translations associated to an arbitrary layer of the hierarchical feature map and define the injective map  X  : H  X  T by h a 7 X  t a , where a is a parameter char-acterizing the translation. Set  X  = {  X  ( h ) | h  X  H } . Take G = M  X  = T o O 2 as above. The sets are compatible.
 This proposition states that the hierarchical feature map may be made invariant to isometries, how-ever one might reasonably ask whether the feature map can be invariant to other transformations. The following Proposition confirms that isometries are the only possible transformations, with group structure, to which the hierarchy may be made invariant in the exact sense of Definition 2. Im( v n ) by restriction. Then at all layers, the group of orthogonal operators O 2 is the only group of transformations to which the neural response can be invariant. The following Corollary is immediate: Corollary 1. The neural response cannot be scale invariant, even if b K 1 is.
 We give a few examples illustrating the application of the Propositions above.
 Example 1. If we choose the group of rotations of the plane by setting  X  R = SO 2 C O 2 , then the translations, then Assumption 1 is verified, and we can apply Theorem 1: N m will be invariant to rotations as long as b K 1 is. A similar argument can be made for reflection invariance, as any rotation can be built out of the composition of two reflections.
 Example 2. Analogous to the previous example, we may also consider finite cyclical groups C n describing rotations by  X  = 2  X /n . In this case the construction of an appropriate set of translations is similar: we require that  X  T include at least the conjugacy classes with respect to the group C n , C C n ( t ) for each t  X   X  =  X  ( H ) .
 Example 3. Consider a simple convolutional neural network [6] consisting of two layers, one filter at the first convolution layer, and downsampling at the second layer defined by summation over all distinct k  X  k blocks. In this case, Proposition 2 and Theorem 1 together say that if the filter kernel is rotation invariant, then the output representation will be invariant to global rotation of the input image. This is so because convolution implies the choice K 1 ( f,g ) =  X  f,g  X  L 2 , average pooling, and H = H 1 containing all possible translations. If the convolution filter z is rotation invariant, invariance of the initial kernel. 4.2 Strings, Reflections, and Finite Groups We next consider the case of finite length strings defined on a finite alphabet. One of the advantages group theory provides in the case of string data is that we need not work with permutation repre-sentations. Indeed, we may equivalently work with group elements which act on strings as abstract objects. The definition of the neural response given in Smale et al. involves translating an analysis window over the length of a given string. Clearly translations over a finite string do not constitute a group as the law of composition is not closed in this case. We will get around this difficulty by first considering closed words formed by joining the free ends of a string. Following the case of circular data where arbitrary translations are allowed, we will then consider the original setting described in Smale et al. in which strings are finite non-circular objects.
 Taking a geometric standpoint sheds light on groups of transformations applicable to strings. In followed by truncation outside of a fixed window. The cyclic group of circular shifts of an n -string is readily seen to be isomorphic to the group of rotations of an n -sided regular polygon. Similarly, reversal of an n -string is isomorphic to reflection of an n -sided polygon, and describes a cyclic group of order two. As in Equation (5), we can combine rotation and reflection via a semidirect product where C k denotes the cyclic group of order k . The resulting product group has a familiar presen-tation. Let t,r be the generators of the group, with r corresponding to reflection (reversal), and t corresponding to a rotation by angle 2  X /n (leftward circular shift by one character). Then the group of symmetries of a closed n -string is described by the relations These relations can be seen as describing the ways in which an n -string can be left unchanged. The is the same as reflecting and then right-shifting. In describing exhaustively the symmetries of an n -string, we have described exactly the dihedral group D n of symmetries of an n -sided regular polygon. As manipulations of a closed n -string and an n -sided polygon are isomorphic, we will use geometric concepts and terminology to establish invariance of the neural response defined on strings with respect to reversal. In the following discussion we will abuse notation and at times denote by u and v the largest index associated with the patches u and v .
 usual reflection of an v -sided regular polygon, whereas we would like r u to reflect a smaller u -sided polygon. To build a group out of such operations, however, we will need to ensure that r u and r v both apply in the context of v -sided polygons. This can be done by extending A r u to v by defining r u to be the composition of two operations: one which reflects the u portion of a string and leaves the rest fixed, and another which reflects the remaining ( v  X  u ) -substring while leaving the first u -substring fixed. In this case, one will notice that r u can be written in terms of rotations and the usual reflection r v : This also implies that for any x  X  T , where we have used the fact that T is abelian, and applied the relations in Equation (10). We can now make an educated guess as to the form of  X  T by starting with Condition (1) of Definition 3 and applying the relations appearing in Equation (10). Given x  X   X  T , a reasonable requirement is that there must exist an x 0  X   X  T such that r v x = x 0 r u . In this case where the second equality follows from Equation (11), and the remaining equalities follow from the relations (10). The following Proposition confirms that this choice of  X  T is compatible with the reflection subgroup of G = D v , and closely parallels Proposition 2.
 Proposition 4. Let H be the set of translations associated to an arbitrary layer of the hierarchical feature map and define the injective map  X  : H  X  T by h a 7 X  t a , where a is a parameter charac-terizing the translation. Set  X  = {  X  ( h ) | h  X  H } . Take G = D n  X  = T o R , with T = C n =  X  t  X  and R = C 2 = { r, 1 } . The sets are compatible.
 One may also consider non-closed strings, as in Smale et al., in which case substrings which would wrap around the edges are disallowed. Proposition 4 in fact points to the minimum  X  T for reversals in this scenario as well, noticing that the set of allowed translations is the same set above but with the illegal elements removed. If we again take length u substrings of length v strings, this reduced set of valid transformations in fact describes the symmetries of a regular ( v  X  u + 1) -gon. We can thus apply Proposition 4 working with the Dihedral group G = D v  X  u +1 to settle the case of non-closed strings. We have shown that the tools offered by group theory can be profitably applied towards understand-ing invariance properties of a broad class of deep, hierarchical models. If one knows in advance the transformations to which a model should be invariant, then the translations which must be built into the hierarchy can be described. In the case of images, we showed that the only group to which a model in the class of interest can be invariant is the group of planar orthogonal operators. Acknowledgments This research was supported by DARPA contract FA8650-06-C-7632, Sony, and King Abdullah University of Science and Technology. [1] M. Artin. Algebra . Prentice-Hall, 1991. [2] J. Bouvrie, L. Rosasco, and T. Poggio. Supplementary material for  X  X n Invariance [3] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of [4] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural net-[5] D.H. Hubel and T.N. Wiesel. Receptive fields and functional architecture of monkey striate [6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document [7] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scal-[8] B.W. Mel. SEEMORE: Combining color, shape, and texture histogramming in a neurally [9] T. Serre, A. Oliva, and T. Poggio. A feedforward architecture accounts for rapid categorization. [10] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with [11] S. Smale, L. Rosasco, J. Bouvrie, A. Caponnetto, and T. Poggio. Mathematics of the neu-[12] H. Wersing and E. Korner. Learning optimized features for hierarchical models of invariant
