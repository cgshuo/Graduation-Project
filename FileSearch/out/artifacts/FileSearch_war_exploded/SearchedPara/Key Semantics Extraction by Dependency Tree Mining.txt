 We propose a new text mining system which extracts char-acteristic contents from given documents. We define Key semantics as characteristic sub-structures of syntactic de-pendencies in the given documents, and consider the fol-lowing three tasks in this paper: 1) Key semantics extrac-tion : extracting characteristic syntactic dependency struc-tures not only as ordered trees but also as unordered trees and free trees, 2) Redundancy reduction : from the result of extraction, deleting redundant dependency structures such as sub-structures or equivalent structures of the others, and 3) Phrase/sentence reconstruction : generating a phrase or sentence in a natural language corresponding to the ex-tracted structure.

Our system is a combination of natural language pro-cessing techniques and tree mining techniques. The sys-tem consists of the following five units: 1) syntactic de-pendency analysis unit, 2) input filters, 3) characteristic ordered subtree extraction unit, 4) output filters, and 5) phrase/sentence reconstruction unit. Although ordered trees are extracted in the third unit, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the in-put filters is/are applied in the second step. The output filters delete redundant trees from the extraction result for efficient knowledge discovery. Finally, phrases or sentences corresponding to the extracted subtrees are reconstructed by utilizing the i nput documents.

We demonstrate the validity of our system by showing experimental results using real data collected at a help desk and TDT pilot corpus.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms, Languages Keywords: text mining, syntactic dependency, tree enu-meration, redundancy reduction, phrase/sentence reconstruc-tion
Text mining technologies have been receiving much inter-est, and applications of them have also been growing in a broad rage of business areas, including CRM (customer re-lationship management), knowledge management, and Web monitoring. Among many text mining technologies, that of extracting characteristic contents such as keywords or key phrases from the given documents[19] realizes one of the most important basic functions, by which a user can ana-lyze complains about a specific product from contact center data[8], overview contents at a specific Internet site[9], etc.
Generally, this function can be expressed as  X  X rom given positive and negative example documents 1 ,extractingaset of features which have strong association with the positive examples. X  Traditionally, keywords have been extracted as the features by bag-of-words representation approach[19]. However, a word is generally too small as a unit of extraction to catch semantic information about the positive documents, and a user sometimes misses the point of characteristics. As expansions of keyword extraction, extraction of fixed-length n-grams (e.g., word bigram or trigram) or pairs of words with syntactic relations (e.g., modifier-head relation) were also proposed (for example, [17]) in the hope of supplying more semantic information by combinations of words to a user.

Recently, Kudo and Matsumoto proposed a boosting al-gorithm for text classification that captures sub-structures embedded in texts[6]. They enumerate subtrees of syntac-tic dependency structures of sentences, and for each subtree they construct a decision stump of which condition part re-quires the presence of the subtree. Since the gain function of each decision stump calculated in the boosting algorithm is the strength of association between the subtree in the stump and the positive examples, their algorithm can be considered as characteristic subtrees extraction for the positive example documents.

However, their approach has several problems in cases where a user wants to read the results as characteristics of the documents. First, since each of the decision stumps requires the presence of a tree as an ordered tree , structures with different word occurrenc e order do not share a charac-teristic in a large structure. For example  X  X ecord data care-fully X  matches  X  X  recorded data very carefully X  but does not match  X  X arefully, I recorded data X  or  X  X  have data recorded
Note that the negative example set can be empty, where only one set of documents is given. carefully. X  (Fig. 1). Therefor e, the sub-structure  X  X ecorded data carefully X  is not considered to be a common structure among these sentences, and it is not extracted as a charac-teristic from them. Algorithms which extracts characteristic sub-structures as unordered trees (ignoring the orders within siblings) and free trees (ignoring the direction of edges) are necessary. Second, combinatory explosion usually occurs in the result (not in the process) generally. The output list contains too much features, and many of them are sub-structures of others or equivalent to others. A user must spend a long time to read and understand the result, and the efficiency of KDD is decreased deeply. Third, tree type data itself is not user friendly. A wide region is necessary to display tree structures, and the tree, where each node has a label describing the corresponding word and syntac-tic dependencies are expressed by directed edges, is hard to understand intuitively. This also decreases the efficiency of KDD.

Here, we define Key semantics as characteristic sub-structures of syntactic dependencies, and consider the following three tasksinthispaper: 1) Key semantics extraction :extract-ing characteristic structures of syntactic dependencies not only as ordered trees but also as unordered trees and free trees, 2) Redundancy reduction : deleting redundant depen-dency structures from the extracted results such as sub-structures or equivalent structures of the others, and 3) Phrase/sentence reconstruction : generating a phrase or sen-tence in a natural language corresponding to the extracted structure.

The main purpose of this paper is to propose a new sys-tem to extract key semantics for knowledge discovery from text data, and to demonstrate its effectiveness through its experimental evaluations for real data sets.
Related to tree mining, Asai et. al. proposed a method for ordered tree enumeration, also independently introduced by Nakano [10] and Zaki [21]. This technique, called right-most expansion technique [1], has been applied for more gen-eral classes. For unordered tree mining, Asai et. al. [2] and Kok, Nijssen [11] presented the first output polynomial time enumeration methods by generalizing the rightmost expan-sion for unordered trees. Chi et. al. [3] extended this ap-proach for frequent free tree mining. For general classes of graphs, Apriori-like algorithms have been widely used [4, 7], however, Yan and Han [20] recently proposed an efficient general graph mining method based on rightmost expansion applied to DFS-trees and obtained good performance. How-ever, since the above algorithms for unordered and free tree mining are still inefficient or too complicated in practice, we have determined to develop input filters which translate in-put data before sub-structure enumeration and to use the algorithm of ordered tree enumeration in [1] so that the im-plementation of the system can be simplified but still kept efficient. The use of input filters for unordered or free tree mining is our original idea.

Related to redundancy reduction in tree type data or phrase/sentence reconstruction from syntactic dependency subtrees for the efficiency of KDD frameworks, such tasks have never been reported before to the best of our knowl-edge.
Here, we introduce basic notations and definitions on tree type data according to [1].
Let L be a set of node labels. A labeled ordered tree (an ordered tree, for short) is a 5-tuple T =( V, E, label, r, &lt; ), where V is the set of nodes, E is the set of edges, G = ( V, E, r ) is a tree with the root r  X  V , labeling function label : V  X  E assigns a label to each node v  X  V ,and the binary relation &lt;  X  V 2 represents a sibling relation .If ( u, v )  X  E then we say that u is a parent of v or that v is a child of u . In terms of the sibling relation represented by &lt; ,wesaythatanode s is an elder brother of a node t iff s and t are children of the same parent and s&lt;t .
A labeled unordered tree (an unordered tree, for short) is a 4-tuple T =( V, E, label, r ) without the sibling relation &lt; .
Also, a labeled free tree (a free tree) is a 3-tuple T = ( V, E, label ), where E is the set of undirected edges and G = ( V, E ) is a graph without cycles. Intuitively, an unordered tree is an ordered tree without the sibling relation and a free tree is an unordered tree without a specified root. We denote by
OT , UT , FT the classes of ordered trees, unordered trees, and free trees, respectively.
First, we define the notion of matching functions as fol-lows. Let D and T be ordered trees, where A function g : V D  X  V T is called an ordered matching func-tion of T into D if it satisfies the following conditions: (i) g is a one-to-one function. (ii) For all x  X  V T , label T ( x )= label D ( g ( x )). (iii) For all x, y  X  V T ,( x, y )  X  E t iff ( g ( x ) ,g ( y )) (iv) For all x, y  X  V T , x&lt; T y iff g ( x ) &lt; D g ( y ).
A function g is called an unordered matching function of an ordered or unordered tree T into an ordered or unordered tree D if it satisfies the conditions (i) to (iii), but not nec-essarily (iv) above, and g is called a free matching function of an ordered or unordered or free tree T into an ordered or unordered or free tree D if it satisfies the condition (i), (ii), and (v) For all x, y  X  V T ,( x, y )  X  E T
Atree T matches atree D ,or T occurs in D ,or T is a subtree of D as an ordered, unordered, and free tree, if there exists an ordered, unordered, and free matching function g of T into D , respectively.

Atree T agrees with atree D as an ordered, unordered, and free tree, if T matches D and D matches T as an or-dered, unordered, and free tree, respectively.
Let F sibling : OT X  2 OT be a function of an ordered tree to a set of ordered trees, such that, C  X  F sibling ( D )iff C agrees with D as an unordered tree and the siblings in C are sorted in alphabetical order 2 .Wecall F sibling as a sibling sort filter . Basically, the sibling sort filter sorts siblings in D in alphabetical order. We say that an ordered tree T is sibling sorted if the siblings in T are sorted in alphabetical order by their labels 3 . The following theorem shows that the discrimination of matching as an unordered tree can be reduced to that of matching as an ordered tree. (We omit the proof because of the limited space.)
Theorem 1. Let T and D be ordered trees and we as-sume that T is sibling sorted. The necessary and sufficient condition for T to occur in D as an unordered tree is that there exists an ordered tree C  X  F sibling ( D ) such that T oc-curs in C as an ordered tree.

Let F root : OT X  2 OT be a function of an ordered tree to a set of ordered trees, such that, each C  X  F root ( D ) agrees with D as a free tree, and for each node x in D there exists C  X  F root ( D ) with the root node r C = g ( x ), where g is the free matching function of D into C .Wecall F root as a root expansion filter . Intuitively, the root expansion filter generates ordered trees which agree D as a free tree and of which root nodes differ from each other.

The following theorem shows that the discrimination of matching as a free tree can be reduced to that of matching as an ordered tree. (We omit the proof.)
Theorem 2. Let T and D be ordered trees and we as-sume that T is sibling sorted. The necessary and sufficient condition for T to occur in D as a free tree is that there exists an ordered tree C  X  F sibling ( F root ( D )) such that T occurs in C as an ordered tree. Note that, if some siblings have the same label then F sibling ( D ) contains the trees obtained by permuting these siblings in all possible orders. This behavior is necessary for correct discrimination of unord ered and free tree matching. The order of siblings with the same labels can be arbitrary.
From given sets of positive and negative example docu-ments, our system extracts characteristic dependency struc-tures (key semantics) of the positive examples.

Let d = { ( d 1 ,x 1 ) ,..., ( d n ,x n ) } be input data to the sys-tem, where d i is the i -th document and x i  X  X  1 ,  X  1 } positive/negative indicator of d i .Adocument d i is a set of example document if x i =1 /  X  1.

Our system is a combination of natural language process-ing techniques and tree mining techniques. The system con-sists of a dependency analysis unit, input filters, a charac-teristic subtree extraction unit, output filters, and a sen-tence reconstruction unit. The procedure of key semantics extraction is described as follows: 1) Syntactic dependency analysis of positive/negative document set, 2) Translation of the dependency trees by the input filters, 3) Extraction of characteristic ordered subtrees from the translated trees, 4) Redundancy reduction by the output filters, and 5) Recon-struction of phrases or sentences from the extracted subtrees (Fig. 2). Although ordered trees are extracted in the third step, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the input filters is/are applied in the second step. The output filters delete redundant trees from the extracted subtrees for efficient knowledge discovery. Finally, phrases or sentences corresponding to the subtrees which are not deleted by the output filters are reconstructed by utilizing the i nput document set.

Details of each process are given in the following subsec-tions.
For each sentence in the input documents, the dependency analysis unit performs morphological and syntactic analysis, and constructs a syntactic dependency tree[13][14].
In the dependency tree, each node corresponds to each word in the sentence, each sibling relation between nodes corresponds to words occurrenc e order in the sentence, and the direction of each edge corresponds to the direction of each dependency. The base form or the stem of each word is assigned to the corresponding node as the node label so that the variation of word representations can be concealed and absorbed in the process of charac teristic subtree extraction.
Let DA ( s ) be the dependency tree generated by this unit from sentence s ,andlet D i j be the corresponding depen-dency tree to the j -thsentenceinthe i -th document s i j is, D i j = DA ( s i j ). Let  X  d i be the result of the dependency analysis of the i -th document, that is,  X  d i = { D i j ,...,D where m(i) is the number of sentences in the i -th document.  X  d ,  X  X  X  ,  X  d n are sent to the next unit as the results of depen-dency analysis of the input documents.

We say a tree T matches a sentence s as an ordered, un-ordered, free tree if T matches DA ( s ) as an ordered, un-ordered, free tree respectively. Also, we say a tree T matches adocument d as an ordered, unordered, free tree if there ex-ists a sentence s in d such that T matches s as an ordered, unordered, free tree respectively.

Figure 1 shows examples of dependency (sub)trees. As an ordered tree, a subtree (d) matches (a) but does not match (b) or (c). As an unordered tree, however, (d) matches (a) and (b) but does not match (c). As a free tree, (d) matches all (a), (b), and (c).
For each  X  d i sent from the previous unit, trees in it are translated by the input filters here, and the result  X   X  d to the characteristic subtree extraction unit.

For each of the sibling sort filter and the root expansion filter defined in the section of preliminary, whether it is ap-plied here or not is switched according to a mining mode of the system specified by a user.

Ordered tree mining mode : None of the filters are applied here. The results of dependency analysis are input to the characteristic subtree extraction unit as they are (  X  d ), and characteristic ordered subtrees of positive  X  d i s( with positive x i ) are extracted in the unit.

Unordered tree mining mode: The dependency anal-ysis results are translated by the sibling sort filter; In this mode, extraction of characteristic ordered trees of to extraction of characteristic unordered subtrees of posi-tive  X  d i s (dependency trees before the translation) because of Theorem 1.

Free tree mining mode: The dependency analysis re-sults are translated by the root expansion filter and the sib-ling sort filter in this order here, and they are sent to the next unit. Then, In this mode, extraction of characteristic ordered trees of positive  X   X  d i s in the next unit leads to extraction of charac-teristic free subtrees of positive  X  d i s because of Theorem 2.
This unit extracts characteristic ordered subtrees of posi-tive  X   X  d i s.

At first, all trees that match positive  X   X  d i s as ordered trees are enumerated by rightmost-expansion method[1]. The method is an efficient technique to enumerate all labeled ordered sub-trees, which is independently introduced in Asai et al. [1], Nakano [10] and Zaki [21]. In our system, the enumeration algorithm is realized in a depth-first manner so that the pro-cess can be executed with less main memory.

Then, for each tree T enumerated by the above method, the information gain G ( T )iscalculatedas G ( T )= ESC  X  ( ESC 1+ ESC 0), which was proposed as information gain of characteristic word[18][19], where
ESC = a + l  X  ESC 1= d + l  X  ESC 0=( a  X  c )+ l  X  a and b are the number of positive  X   X  d i s and negative with negative x i ) respectively, c and d are the number of positive and negative  X   X  d i swhich T matches as an ordered tree, and l is a constant value. Note that from Theorem 1 or Theorem 2, c and d fall into the number of positive and negative  X  d i swhich T matches in the specified mining mode respectively, that is, c / d is the number of positive/negative example documents which T matches as an unordered tree and free tree in the unordered tree and free tree mining mode respectively. This guarantees correct extraction of characteristic trees in each mining mode.

The higher the information gain G ( T ) is, the higher the degree of how T is characteristic as a subtree of the positive examples is. This formula corresponds to defining informa-tion gain using extended stochastic complexity [18] where the uniform distribution is assumed on the space of all trees.
A user specifies the level of thresholds such as the min-imum value of information gain, the minimum number of matched  X   X  d i s, the lowest ranking of the information gain, etc., and trees that exceed the level are sent to the next unit as characteristic trees. Let U = { U 1 ,  X  X  X  U k } be a set of the characteristic trees. U is sent to the output filters.
We developed the following three output filters to delete trees considered as redundant in many cases.

Equivalent tree deletion filter F uniq checks all trees in U one by one, and deletes a tree T iff T agrees with at least one tree in U excepting T itself as free trees.
Matching tree deletion filter F match deletes a tree T in U iff T matches another tree D in U as an ordered tree and D does not match T .
 Low gain matching tree deletion filter F lgmatch deletes T in U iff T matches another tree D and the information gain of D is higher than T .
For each characteristic subtree that was not deleted by the output filters, the phrase/sentence reconstruction unit recovers relevant information lost in the processes of depen-dency tree construction and input filters, and constructs a phrase or a sentence corresponding to the subtree.
The unit utilizes th e input documents d 1 ,...d n so that the reconstructed phrases or sentences fit to typical expressions in them. Let T be an input subtree to the unit, and S = {
S 1 ,...S l } be a set of sentences which T matches in the input documents.

At first, reconstruction candidates are generated by con-necting the original words in each S i preserving the word occurrence order in the sentence. Then, the occurrence like-lihood of each candidate is calculated based on the bigram language model, and the candidate with the highest likeli-hood is selected as the result of phrase/sentence reconstruc-tion, that is; R =argmax W result of reconstruction, W = w 1  X  X  X  w n is a candidate, and P ( w | w ) is the bigram probability that the word w succeeds the word w . The bigram probabilities are estimated from large corpus and/or the input documents in advance.
For example, Tree (d) in Fig. 1 matches Sentence (a) to (c) in the free tree mode, and three candidates: (a)recorded data carefully, (b)Carefully, recorded data, and (c)data recorded carefully, are generated from them. For each of the candi-dates, the likelihood is calculated, and one with the highest value is selected as the reconstruction result.
We conducted experiments on two sets of data; contact data at a help desk for an internal e-mail service (in Japanese), and TDT pilot corpus (in English).
Data records in the help desk data has the field of con-tact date/time, question/request, answered date/time an-swer, way of contact and so on. One record corresponds to one contact, and the field of question/request and an-swer are text data. For key semantics extraction, we tar-geted the field of question/request of which contact way is  X  X elephone X . We composed the questions/requests field of records of which contact duration (answered data/time mi-nus contact date/time) is less than five minutes into a set of positive examples, and those with longer duration into a set of negative example. One contact record corresponds to one example document in the system, and the number of positive and negative example documents are 274 and 592 respectively.

We used a Japanese parser proposed in [13] and [14] as the dependency analysis unit. We extracted key semantics in the free tree mode and reduced the redundancy by the equivalent tree deletion and low gain matching tree deletion filters. Our system ran on an NEC EXpress5800 with 2GHz Pentium III and 1GB memory. It took about ten minutes to analyze syntactic dependencies and one minute to process the residual steps in all.

Table 1, 2, and 3 are the output of the units in the sys-tem. In them, I.G. and P and N are the information gain, the number of matched positive example documents, and the number of matched negative example documents. A parent/elder brother of a node is located in the right/upper side of the node here. Also, English translation of the labels and key semantics are attached in this subsection.
Table 1 shows the extracted subtrees by the extraction unit. Subtrees with the highest information gains are listed in the table. We can see that some of the extracted trees have many nodes. Obviously, they carry richer semantic information than single words, trigrams, or modifier-heads. Table 1: Characteristic subtrees before redundancy reduction Table 2: Characteristic subtrees after redundancy reduction Moreover, by looking into original documents in detail, we found that the fifth tree matches several sentences some of which have different word orders, and the information gain for the tree decreases in the ordered tree mining mode. This kind of key semantics cannot be extracted with a high in-formation gain until the unordered/free subtrees extractions are realized.

However, the list conains many redundant results. A user must read similar trees many times. Also, many other re-sults, which might be informative, escape from the list (and the user X  X  field of vision) because these similar trees occupy the upper reach of the rankings extensively. These redun-dancy damages the efficiency of a KDD process seriously.
Table 2 shows the result of redundancy reduction. There does not exist a tree which matches other trees in the list. The trees which occupied the lower reach of the fifth in Table 1 were unified into the fourth tree in this table, and many new key semantics came in the list. Redundancy in Table 1 was reduced sufficiently, and the KDD efficiency fairly improves.

However, Table 2 still has a problem of readability since the key semantics are depicted as tree-structured data. It is difficult to imagine original semantics (even if a user could understand Japanese :) from big trees such as the fourth and the seventh in Table 2, where even dependency directions are lost in the free tree mining mode.
Table 3 shows the result of phrase/sentence reconstruc-tion namely the final output of the system. Key semantics are output in a natural language. The readability of key semantics in this table is much higher than that of Table 2, and the efficiency of KDD improves furthermore.
From TDT pilot corpus (see [16]), we picked up the con-tents of  X  X ummary X  field of which  X  X udgment X  fields are  X  X ES X , that is, summary of documents which were manu-ally classified into one of pre-defined events with confidence. We extracted key semantics for events which have the largest number of documents.

We used an English PCFG/dependency parser proposed in [5] and [15] as the dependency analysis unit, and se-lected the unordered tree mining mode with the equiva-lent tree deletion and low gain matching tree deletion fil-ters For Event 18 (with 273 documents), key semantics with the highest information gain were,  X  X erry Nichols X ,  X  X he bombing in Oklahoma City X ,  X  X he FBI X ,  X  X f the Oklahoma City bombing X ,  X  X ames Nichols X ,  X  X odies X , and  X  X he Mur-rah building X  etc. For Event 9 (114 documents), key se-mantics were  X  X udge Lance Ito X ,  X  X he prosecution X ,  X  X n the O.J. Simpson case X ,  X  X NA testing X ,  X  X lood samples X ,  X  X hal-lenge X ,  X  X udge Ito is X ,  X  X odayfs hearing X , and  X  X n the hear-ing X  etc. Note that easily readable results, including large structures are output, while the redundancy is appropriately reduced.

In this way, we can extract key semantics in an efficient manner of KDD processes.
In this paper, we have proposed a new text mining sys-tem which extracts characteristic sub-structures of syntactic dependencies, which we defined as key semantics , not only as ordered trees but also as unordered trees and free trees. It enables extracting character istic structures irrespective of the occurrence order of words o r the direction of syntactic dependencies between words. Also, redundancy reduction and reconstruction of a phrase or sentence are conducted in the system for the efficiency of KDD processes.

Through the experiments using both Japanese and En-glish data, it was demonstrated that our system works well in the sense that key semantics were extracted and presented in a desirable manner for efficient KDD processes.
In our system, we adopted input and output filters as a preprocess and a postprocess for an ordered tree miner to re-alize flexible mining. However, this approach has an obvious limitation on its performance. In the studies of constrained pattern mining, a number of techniques have been devel-oped for pushing a set of constraints deep into the mining process [12]. Hence, it will be an interesting research prob-lem to generalize this method to compile given sequences of input and output filters into a tree mining algorithm.
