 dataset [3]), and natural language processing [4]. CNNs tak e translated versions of the same ba-capture more complex invariances, such as out-of-plane rot ations.
 particular, we present tiled convolutional networks (Tiled CNNs), which use a novel weig ht-tying based on only constraining weights/basis functions k steps away from each other to be equal (with the special case of k = 1 corresponding to convolutional networks).
 same map; within each map, units with the same fill texture hav e tied weights. (Network diagrams in the paper are shown in 1D for clarity.) robust to local transformations [9]. We show in this paper ho w TICA can be efficiently used to pretrain Tiled CNNs through the use of local orthogonality.
 The resulting Tiled CNNs pretrained with TICA are indeed abl e to learn invariant representations, tion performance, enabling Tiled CNNs to be competitive wit h previously published results on the NORB [3] and CIFAR-10 [10] datasets. the same weights (see Figure 1-Left). This reduces the numbe r of learnable parameters, and (by pooling over neighboring units) further hard-codes transl ational invariance into the model. units that have different basis functions, and hence learn a more complex range of invariances. constrain only units that are k steps away from each other to be tied. By varying k , we obtain a spectrum of models which trade off between being able to lear n complex invariances, and having other, we have fully untied simple units.
 Next, we will allow our model to use multiple  X  X aps, X  so as to l earn highly overcomplete repre-(see Figure 1-Right). When varying the tiling size, we change the degree of weight tying within each map; for example, if k = 1 , the simple units within each map will have the same weights. In maps enjoy the twin benefits of (i) being able to represent com plex invariances, by pooling over (partially) untied weights, and (ii) having a relatively sm all number of learnable parameters. Figure 2: Left: TICA network architecture. Right: TICA first layer filters (2D topography, 25 rows of W ).
 Unfortunately, existing methods for pretraining CNNs [11, 12] are not suitable for untied weights; for example, the CDBN algorithm [11] breaks down without the weight-tying constraints. In the following sections, we discuss a pretraining method for Til ed CNNs based on the TICA algorithm. A TICA network [9] can be described as a two-layered network ( Figure 2-Left), with square and layer are learned, while the weights V in the second layer are fixed and hard-coded to represent layer hidden unit p and p p sparse feature representations in the second layer, by solv ing: where the input patterns { x ( t ) } T is the size of the input and m is the number of hidden units in a layer. V is a fixed matrix ( V 1 or 0) that encodes the 2D topography of the hidden units h 2D grid, with each p each p W W T = I provides competitiveness and ensures that the learned feat ures are diverse. central to feature invariance, which is in turn essential fo r recognition tasks [13]. we can view the Tiled CNN as a special case of a TICA network, wi th the topography of the pooling an important role in speeding up TICA. We discuss this next. Tiled CNNs typically perform much better at object recognit ion when the learned representation expensive and have hyperparameters which need to be extensi vely tuned. Much of this tuning can be avoided by using score matching [16], but this is computatio nally even more expensive, and while orthogonalization can be avoided altogether with topograp hic sparse coding, those models are also a decoder unit at training time [17].
 We can avoid approximate orthogonalization by using local r eceptive fields, which are inherently built into Tiled CNNs. With these, the weight matrix W for each simple unit is constrained to be receptive fields is not necessary for learning distinct, inf ormative features either. For l maps, our computational cost is O ( ls 2 n ) , compared to standard TICA X  X  O ( l 2 n 3 ) . k to its maximum value of n  X  s + 1 gives exactly the untied local TICA model outlined in the Algorithm 1 Unsupervised pretraining of Tiled CNNs with TICA (line sear ch) shown in Algorithm 1. The innermost loop is a simple implemen tation of backtracking linesearch. In orthogonalize local RF ( W new ) , we only orthogonalize the weights that have completely ove r-weights.
 optimization parameters. This is because TICA X  X  tractable objective function allows us to monitor convergence easily. In contrast, other unsupervised featu re learning algorithms such as RBMs [6] and autoencoders [18] require much more parameter tuning, e specially during optimization. 6.1 Speed-up We first establish that the local receptive fields intrinsic to Tiled CNNs allows us to imple-ment TICA learning for overcomplete represen-tations in a much more efficient manner.
 Figure 3 shows the relative speed-up of pre-training Tiled CNNs over standard TICA us-ing approximate fixed-point orthogonalization (
W = 3 2 W  X  1 2 W W T W )[15]. These experi-ments were run on 10000 images of size 32x32 or 50x50, with s = 8 .
 We note that the weights in this experiment were left fully untied, i.e., k = n  X  s +1 . Hence, the speed-up observed here is not from an effi-cient convolutional implementation, but purely due to the local receptive fields. Overcoming this computational challenge is the key that al-lows Tiled CNNs to successfully use TICA to 6.2 Classification on NORB Next, we show that TICA pretraining for Tiled CNNs performs w ell on object recognition. We start with the normalized-uniform set for NORB, which consists of 24300 training examples and 2 4300 test examples drawn from 5 categories. In our case, each exam ple is a preprocessed pair of 32x32 that each pooling unit p without wraparound at the borders. The number of pooling uni ts in each map is exactly the same as units per map in our experiments on 32x32 images.
 A summary of results is reported in Table 1. 6.2.1 Unsupervised pretraining with which to learn the weights W of the Tiled CNN. We call this initial phase the unsupervised pretraining phase.
 while the lower weights of the Tiled CNN model remained fixed.
 purely on unsupervised data compare favorably to many state -of-the-art algorithms on NORB. 6.2.2 Supervised finetuning of W supervised training of the classifier.
 Using softmax regression to calculate the gradients, we bac kpropagated the error signal from the CNN model. During the finetuning step, the weights W were adjusted without orthogonalization. validation set comprising 10% of the training data for model selection. Models with larger numbers of maps tended to overfit and hence performed poorly on the val idation set. The best performing fine-tuned model on the validation set was the model with 16 ma ps and k = 2 , which achieved published results on NORB to this date (see Table 1).
 6.2.3 Limited training data To test the ability of our pretrained features to generalize across rotations and lighting con-ditions given only a weak supervised signal, we limited the labeled training set to comprise only examples with a particular set of view-ing angles and lighting conditions. Specifically, NORB contains images spanning 9 elevations, 18 azimuths and 6 lighting conditions, and we trained our linear classifier only on data with elevations { 2, 4, 6 } , azimuths { 10, 18, 24 } and Figure 4: Left: NORB test set accuracy across various tile si zes and numbers of maps, without finetuning. Right: NORB test set accuracy, with finetuning. images, making for a total of 675 out of the possible 24300 tra ining examples. examples. We obtained an accuracy of 72.2% on the full test se t using the model with k = 2 and 22 maps. A smaller, approximately 2.5x overcomplete model w ith k = 2 and 4 maps obtained an accuracy of 64.9%. In stark contrast, raw pixel performance dropped sharply from 80.2% with a full supervised training set, to a near-chance level of 20.8% on t his limited training set (Figure 5). reducing the need for large amounts of labeled data. 6.3 Classification on CIFAR-10 The CIFAR-10 dataset contains 50000 training images and 100 00 test images drawn from 10 cate-6.3.1 Unsupervised pretraining and supervised finetuning 48-map convolutional model performed the best on our 10% hol d-out validation set, and achieved a maps and k = 1 , only achieves 65 . 1% . 6.3.2 Deep Tiled CNNs fashion, similar to models such as DBNs [6] and stacked autoe ncoders [26, 18]. We constructed this network by stacking two Tiled CNNs, each with 10 maps and k = 2 . The resulting four-layer network has the structure W of size 4x4, and W [27] on the hold-out validation set. The number of maps in the third and fourth layer is also 10. After finetuning, we found that the deep model outperformed a ll previous models on the validation to learn more complex representations. 6.4 Effects of optimizing the pooling units with a method such as ICA. This method is computationally mor e attractive and probably easier to implement. Here, we investigate if such benefits come at the e xpense of classification accuracy. features than just na  X   X vely approximating the first layer weights. find that selecting a tile size of k = 2 achieves the best results for both the NORB and CIFAR-10 datasets, even with deep networks. More importantly, untyi ng weights allow the networks to learn invariant. 9 We note that a standard CNN is unlikely to be invariant to thes e transformations. on networks pretrained using 250000 unlabeled images from t he Tiny images dataset [30] show that is sufficient data to avoid overfitting, setting k = p can be a very good choice. parametrization of a spectrum of models which includes both fully-convolutional and fully-untied models to scale up well, producing massively overcomplete r epresentations that perform well on tially benefit a wide range of other feature learning models.
 Acknowledgements: We thank Adam Coates, David Kamm, Andrew Maas, Andrew Saxe, S erena Yeung and Chenguang Zhu for insightful discussions. This wo rk was supported by the DARPA Deep Learning program under contract number FA8650-10-C-7020.
