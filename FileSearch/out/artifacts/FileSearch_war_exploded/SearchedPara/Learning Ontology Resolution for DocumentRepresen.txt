 It is well known that synonymous and polysemous terms often bring in some noises when calculating the similarity between documents. Existing ontology-based document rep-resentation methods are static, hence, the chosen semantic concept set for representing a document has a fixed resolu-tion and it is not adaptable to the characteristics of a doc-ument collection and the text mining problem in hand. We propose an Adaptive Concept Resolution (ACR) model to overcome this issue. ACR can learn a concept border from an ontology taking into consideration of the characteristics of a particular document collection. Then this border can provide a tailor-made semantic concept representation for a document coming from the same domain. Another advan-tage of ACR is that it is applicable in both classification task where the groups are given in the training document set, and clustering task where no group information is avail-able. Furthermore, the result of this model is not sensitive to the model parameter. The experimental results show that ACR outperforms an existing static method significantly. H.3 [ Information Storage and Retrieval ]: Miscellaneous Algorithms, Theory  X  ontology, adaptive concept resolution
Some carefully edited ontologies include WordNet [7], Cyc [6], Mesh [9] and etc. Previous empirical results have shown improvement in some cases utilizing these ontologies [4, 8, 9]. However, the existing works have an obvious shortcom-ing: the strategies they adopted are static. For example, one strategy is to use each synonym set in the WordNet as one dimension in the representation vector of the documents. Therefore, the resolution for representing the documents be-longing to different collections is the same. Suppose we have two document collections, the first one has coarse granular-ity categories, such as sports, military and etc, while the second one has finer granularity categories, such as football, basketball and etc. In the first collection, we should consider football players and basketball players are related, while in the second one they should be unrelated. So an adaptive strategy should outperform the static one.

In this paper, the proposed Adaptive Concept Resolution (ACR) model can learn a concept border from an ontology taking into consideration of the characteristics of a partic-ular document collection. Then this border can provide a tailor-made semantic concept representation for a document coming from the same domain. The structure of an ontology is a hierarchical directed acyclic graph 1 (refer to the example in Figure 1), and the border is a cross section in the graph. All the concepts located below the border will be merged into one of the concepts on the border. We use a gain value to measure whether a concept is a good candidate for the border. The gain value is calculated based on the character-istics of the given document collection. Therefore, our model can generate different tailor-made borders for different col-lections adaptively. Another advantage of ACR is that it is applicable in both classification task where the groups are given in the training document set, and clustering task where no group information is available. To do so, we only need to change the granularity, that is either cluster or in-dividual document, for calculating the gain value. So ACR can be applied to both classification and clustering. The ex-perimental results show that our model can outperform an existing static method in almost all cases.
A hierarchical directed acyclic graph is a directed acyclic graph with the layer information on each node. The head node of an edge must have a higher layer than the tail. Figure 1: A fragment of WordNet structure. Each node is a concept, and the label is its synonym set.
Concept is the basic component of an ontology, and each concept refers to an abstract entity or a real entity. We give a formal definition of a concept:
Definition 2.1. Concept : A concept  X  is a quadruple  X  id,  X  , X ,  X   X  , and the items indicate its ID, synonym set, gloss, and hyponym concept set respectively. We refer to the items with  X .id ,  X .  X  ,  X . X  and  X .  X  .
 In this paper, we select WordNet2.1 as an instance of the ontology. In Figure 1, a fragment of WordNet is given. Take the concept  X  X sland X  as an example,  X  is { island } ,  X  is  X  X  land mass that is surrounded by water X , and its hyponym concepts are shown in the dashed box. The first term in a synonym set can be used to refer to the concept.

Some concepts may have more than one hypernyms, as exemplified by  X  X ight X  at the bottom right in Figure 1. We call this kind of concept ambiguous concept . Therefore, on-tology X  X  structure is a Hierarchical Directed Acyclic Graph (HDAG). The depth d (  X  ) for the  X  is defined as follows: d (  X  ) = 0 if  X  = root, max
Figure 2 depicts an overview of the Adaptive Concept Res-olution (ACR) model, which has two main parts, namely, the learning part on the left and the utilizing part on the right. In the learning part, given a document collection and an ontology graph, the algorithm learns which concepts have better information gain and generates the elements for the border B . Each concept in B encapsulates all its descen-dants shown in the original ontology graph. For example, the terms in  X  1 and  X  2 are added into  X .  X  to get a derived concept  X  b . Thus, the border is tailor-made for the given document collection. In the border utilizing part, B is used to represent a document coming from the same domain col-lection, and each concept in B is one dimension in the vector. These two parts will be discussed in detail in the following subsections. Other technical details, such as virtual concept for solving the unbalance problem, recursive calculation of the gain value and time complexity can be found in our technical report [1].

To generate the concept border, the leaf concept is merged into its hypernym  X  recursively, and we define gain (  X  ) to measure whether the merging is profitable: in which Gentropy (  X  ) is defined as: descendant set of  X  . D = { u 1 ,u 2 ,  X  X  X } is a document set. If there exist clusters in D , each u i represents a cluster. Oth-erwise, each u i represents a single document. And p ( u is the probability of u i given  X  g  X  , which is calculated as in Equation 4: where w i,j is the weight of t j in u i .

It can be observed that 0 &lt; gain (  X  )  X  1. The larger the gain value is, the less the noise is brought in because of merging  X  0 into  X  . If gain (  X  )  X   X  (profitable threshold), the merging will be performed. Until now, we discuss the prob-lem in a bottom-up fashion (generalization). We can also consider in a top-down fashion (specialization) using merg-ing operation instead of splitting operation, and gain (  X  ) can still be used in the same way. We name these two methods as GBG-g and GBG-s respectively.
GBG-g is summarized in Algorithm 1. In each loop, we attempt to merge the deepest leaves into their hypernyms. First we get the deepest leaf  X  l (line 5), and locate  X  hypernym  X  . If  X  l is an ambiguous concept, we select its hypernym which has the largest depth (line 6). If  X  con-tains non-leaf and leaf hyponyms at the same time, these hyponyms will not be merged into  X  , and set the border flag under  X  (line 8). Otherwise, if it is profitable to merge  X   X  X  leaves into it, all leaves X  synonym sets will be added into  X .  X  (line 11), then delete these leaves from G to make  X  become a leaf (line 12). If the merging is not profitable, we set the border flag under  X  (line 14). Finally, the border is composed of all leaves with flg = true . In the subpro-cedure set border flag , the unambiguous leaves of  X  become the members in B (line 8), while the ambiguous leaves will be removed from  X .  X  and its depth is reset based on the depth definition (Equation 1). Suppose an ambiguous leaf  X  has two hypernyms. After removed from  X .  X ,  X  0 becomes an unambiguous leaf, and can be treated as an ordinary leaf hereafter in the remaining processing of GBG-g. Note that the depth of  X  0 should be reset based on its remaining hyper-nym. The larger the depth of  X  0  X  X  hypernym is, the earlier the hypernym is considered. Thus, we always try to merge  X  into its more specialized hypernym.

GBG-s is summarized in Algorithm 2. A recursive split-ting operation is performed from the root. If using  X  to represent all of its descendants is profitable enough, we will encapsulate its descendants into  X  first (line 3 of top down ), then add  X  into B (line 4 of top down ). Otherwise each of Algorithm 1 GBG-g  X   X  X  hyponyms will be used as the parameter to invoke the top down procedure (line 8 of top down ). Once an ambigu-ous concept is merged into any one of its hypernyms (direct or undirect), it will be removed from G (line 5 of top down ).
Before  X  is added into B , all terms contained by  X   X  X  de-scendants are encapsulated into  X  , see line 11 in Algorithm 1 and line 3 of top down in Algorithm 2. As a result, the descendants X  semantic meanings are merged into  X  . This merging is performed under the guidance of gain (  X  ), which guarantees the trade-off is profitable.
Before feeding documents into border learning part or rep-resenting a document into a concept vector, it is necessary to extract concepts from the documents. We propose a forward maximum cutting method to extract the concepts. After that, a context matching method is used to find the correct matching for an ambiguous concept. The details of concept extraction and matching are given in the technical report [1].
In the vector space model, each concept  X  i in B is one dimension. Similar to TF-IDF, we introduce CF-IDF to in-dicate the importance of  X  i in a certain document d j , calcu-lated as cfidf i,j = cf i,j  X  idf i , in which cf i,j = f Algorithm 2 GBG-s frequency of  X  i in d j ( n l,j is the frequency of the term t d ),  X  i  X  d means that at least one term in  X  i .  X  is contained by the document d .
A previous method, known as the  X  X nly X  strategy, in [4] is implemented for conducting the comparison. In this strat-egy, each concept is used as one dimension in the document vector, and its weight is decided by the terms in the synonym set. The strategy is called  X  X otho X  in this paper.
Four data sets are used in the experiments: 20 Newsgroups (NG20), TREC data extracted from the document collection Disc 5, ODP page set and OHSUMED (MED). The details are given in Table 1.
In document classification, LibSVM [2] with linear kernel is employed to conduct the classification, and 5-fold cross-validation is adopted. Note that the gain calculation only needs the training set. Because there exist many small clus-ters in MED, we do not use this data set in classification. In document clustering, K-Means algorithm is used to perform the clustering. The entire document collection is used as the input information of the gain value calculation. Each u refers to an individual document.

The commonly used F-measure (including macro-and micro-average) and purity are used to evaluate the results of clas-sification and clustering respectively.
The results are given Table 2. We can see that except for the F ma of GBG-s on NG20, ACR dominates all other cases in classification. Especially on the TREC data, both of our methods can improve the existing method Hotho about 6%. GBG-g performs better than GBG-s on NG20 and TREC, while GBG-s achieves a better result on the ODP data. In clustering experiment, considering NG20, TREC and ODP, the improvements are more significant, about 4% to 8%. Of the first three data sets, the performances of GBG-g and GBG-s are similar. For the fourth data MED, GBG-g out-performs GBG-s by more than 6%. So the tailer-made bor-der for document representation is much better than the static one in both kinds of text mining task.

The effect of  X  in the clustering is shown in Table 3. The performance of GBG-g algorithm is not sensitive to  X  . It is because the GBG-g algorithm merges the concepts in a bottom-up fashion, and each merging is performed among the concepts with high semantic relation to each other. There-fore, the consistency of the semantic meaning of a derived new concept can be guaranteed, even when the  X  value is small. When the  X  value is too large, say 1.0, the con-straint becomes too strict, and the related concepts cannot be merged sufficiently. Consequently, the result is not as good as the result under a smaller  X  , say 0.7. GBG-s is rela-tively more sensitive to  X  than GBG-g. When  X  is small, the top-down splitting will stop early at some general concepts, and these concepts are added into B . As a result, the gen-eral meaning of the concepts in B brings in more noises to the similarity calculation. So in GBG-s, a larger  X  value can achieve better results than a smaller value in general. We use 0.7 and 0.9 as  X  values for GBG-g and GBG-s respectively. Generally speaking, GBG-g is better and more stable than GBG-s. It is because GBG-g considers the specialized con-cepts first in generating the concept border, which are more important than the general ones from the semantic point of view. Furthermore, GBG-g can deal with the unbalanced structure more effectively, because it does not merge the leaf concepts with the non-leaf concepts.
 The effect of  X  in the classification is shown in Table 4. Again we find that GBG-s is more sensitive to  X  than GBG-g because of the same reasons discussed above. Without exception, the best results for both GBG-g and GBG-s are achieved when  X  is 1. Under the predefined cluster granu-larity of calculating the gain value, a larger  X  can prevent the concepts which may bring in much noise to be added into B . At the same time, because the gain value is calcu-lated considering the cluster information, the related seman-tic meaning in the same cluster can still be merged. Thus, the value of  X  used in both GBG-g and GBG-s is 1 in the classification. Interestingly, we find that on NG20, GBG-g can perform slightly better with both small and large  X  val-ues than with the medium values. One possible reason is that after the concepts are sufficiently merged under a small  X  , the benefit obtained for calculating the similarity within a cluster overwhelms the noises brought in at the same time. While a larger  X  achieves a better result by suppressing the amount of noise. For other data sets, this exceptional situa-tion does not happen. Therefore, we adopt a larger  X  value for all data sets.
As an important expert-edited ontology, WordNet has been used to improve the performance of clustering and clas-sification. Hotho et al. [3, 4] showed that incorporating the synonym set and the hypernym as background knowledge into the document representation can improve the cluster-ing results. Jing et al. [5] constructed a term similarity Table 3: Parameter  X   X  X  effect in the clustering.
 Table 4: Parameter  X   X  X  effect in the classification. matrix using WordNet to improve text clustering. However, their approach only uses synonyms and hyponyms, and fails to handle polysemy, and breaks the multi-word concepts into a group of single words. In Recupero X  X  work [8], two strate-gies, namely, WordNet lexical categories (WLC) technique and WordNet ontology (WO) technique, are used to create a new vector space with low dimensionality for the documents. In WLC, 41 lexical categories for nouns and verbs are used to construct the feature vector. As a result the vector has 41 dimensions. In WO, the hierarchical structure is used as the ontology information. Then words are grouped based on the ontology they are related to.
In this paper, we propose an adaptive concept resolution model to adaptively learn a concept border from an ontol-ogy taking into consideration of the characteristics of a par-ticular document collection. Then this border can provide a tailor-made semantic concept representation for a docu-ment coming from the same domain. Another advantage of ACR is that it is applicable in both classification task where the groups are given in the training document set, and clustering task where no group information is available. Two algorithms are proposed, namely, GBG-g and GBG-s, to generate the concept border. In the experiments, GBG-g performs better and is more stable than GBG-s.
