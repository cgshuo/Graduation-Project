 USC/Information Science Institute Google Inc.
 University of Rochester USC/Information Science Institute
Systems based on synchronous grammars and tree transducers promise to improve the quality between the two languages. We develop a theory of binarization for synchronous context-free grammars and present a linear-time algorithm for binarizing synchronous rules when possible. based machine translation system. We also discuss the more general, and computationally more approximate polynomial-time algorithm for this problem. 1. Introduction
Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars decoding can be thought of as parsing problems, whose complexity is in general ex-
To alleviate this problem, we investigate bilingual binarization as a technique to fac-tor each synchronous grammar rule into a series of binary rules. Although mono-for all synchronous rules; we investigate algorithms for non-binarizable rules as well.
In particular: level, showing the importance and difficulty of binarization for efficient synchronous from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.
In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical.
 models and affects decoding for machine translation in Section 2. We define binarization formally in Section 3, and present an efficient algorithm for the problem in Section 4.
Experiments described in Section 5 1 show that binarization improves machine trans-lation speed and quality. Some rules cannot be binarized, and we present a decoding retical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our
Chinese X  X nglish data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation sys-tems. However, the algorithms presented may become relevant as machine translation systems improve. 560 2. Motivation
Consider the following Chinese sentence and its English translation: (1)  X 
Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2)
Decoding can be cast as a (monolingual) parsing problem because we only need to parse the source-language side of the SCFG, as if we were constructing a CFG by projecting the SCFG onto its Chinese side: (3) when parsing the source-language input, as shown in Figure 1.

Chomsky Normal Form as required by the CKY algorithm, or implicitly into a dotted representation as in the Earley algorithm. To simplify the presentation, we will focus on the former, but the following discussion can be easily adapted to the latter. Rules can be binarized in different ways. For example, we could binarize the first rule left to right or right to left (see Figure 2):
We call these intermediate symbols (e.g., PP-VP) virtual nonterminals and correspond-ing rules virtual rules , whose probabilities are all set to 1.
 described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n -gram), which has been shown language boundary words u and v to produce a bigram-item which we denote bine NP with PP. This step is written as follows in the weighted deduction notation of
Nederhof (2003): where p and q are the scores of antecedent items.
 contiguous so we cannot apply language model scoring when we build the NP-PP item.
Instead, we have to maintain all four boundary words (rather than two) and postpone the language model scoring till the next step where NP-PP is combined with to form an S item. We call this binarization method monolingual binarization because constraints from the other side.
 rule, and the decoder conservatively assumes nothing can be done on language model nonterminal has been recognized. In other words, target-language boundary words 562 derived from this rule. In the case of m -gram integrated decoding, we have to maintain 2( m  X  1) boundary words for each child nonterminal, which leads to a prohibitive over-
Gildea 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality.
Here, because PP and VP are contiguous (but swapped) in the target language, we can include the language model score by multiplying in Pr( with | meeting ), and the resulting item again has two boundary words. Later we multiply in Pr( held | Powell ) when the
PP-VP has contiguous spans on both source-and target-sides, so that we can generate a binary-branching SCFG: (4)
In this case m -gram integrated decoding can be done in O ( | w | much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of
Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules.
 malism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise.
 (5) where the reorderings of nonterminals are denoted by variables x transducer formalism of Rounds (1970), the right-hand (target) side subtree can have transformations on English parse trees to  X  X it X  another language, learning, for example, that the ( V S O ) structure in Arabic should be transformed into a ( S ( V O )) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. tween two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings.
The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing chronous parsing for the original grammar (a) is O ( | w | four indices on either side, giving a total of eight; parsing the monolingually binarized glish side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as  X  X KY in two dimensions. X  An efficient alignment used for decoding and alignment. We show how to find optimal alignment algorithms for non-binarizable rules in Section 7; in this case different grammar factorizations may be optimal for alignment and for decoding with n -gram models of various orders.
Handling difficult rules may in fact be more important for alignment than for decoding, because although we may be able to find good translations during decoding within the space permitted by computationally friendly rules, during alignment we must handle the broader spectrum of phenomena found in real bitext data.
 what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section. 3. Synchronous Binarization Definition 1
A synchronous CFG (SCFG) is a context-free rewriting system for generating string pairs. Each rule ( synchronous production ) 564 rewrites a pair of synchronous nonterminals ( A , B ) in two dimensions subject to the constraint that there is a one-to-one mapping between the nonterminal occurrences in  X  and the nonterminal occurrences in  X  . Each co-indexed child nonterminal pair is a pair of synchronous nonterminals and will be further rewritten as a unit.
 in the previous section, in the sense that we can allow different symbols to be synchro-nized, which is essential to capture the syntactic divergences between languages. For example, the following rule from Chinese to English notation by creating a compound nonterminal alphabet:
We define the language L ( G ) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start nonterminal pair.
 we now write rules in the following notation: where X i and Y i are variables ranging over nonterminals in the source and target pro-jections of the synchronous grammar, respectively, and  X  is the permutation of the rule.
For example, in rule (6), we have n = 2, X = Y = VP, X 1 = VB, X
Y 2 = NNS, and  X  is the identity permutation. Note that this general notation includes cases where a nonterminal occurs more than once in the right-hand side, for example, when n = 2, X = Y = A , and X 1 = X 2 = Y 1 = Y 2 = B , we can have the following two rules: input n -ary SCFG.
 resistant to factorization (Aho and Ullman 1972). We now turn to rigorously defining the binarizability of permutations.
 Definition 2
A permuted sequence is a permutation of consecutive integers. If a permuted sequence a can be split into the concatenation of two permuted sequences b and c , then ( b ; c ) element in c . whereas (3, 5; 4) is not. A proper split has the following property: Lemma 1 Proof The  X  direction is trivial by the definition of proper split.
 quence, i.e., the set of b  X  X  elements is not consecutive, then there must be some x  X  c contradiction if c is not a permuted sequence. Now that both b and c are permuted sequences, ( b ; c ) is a proper split. The case when b &gt; c is similar.  X  Definition 3
A permuted sequence a is said to be binarizable if either 1. a is a singleton, that is, a = ( a ), or 2. there is a proper split a = ( b ; c ) where b and c are both binarizable. We call pattern associated with each binarizable sequence, which we now rigorously define. Definition 4
A binarization tree bi ( a ) of a binarizable sequence a is either 1. a if a = ( a ), or tively, following the ITG notation of Wu (1997). Note that a binarizable sequence might have multiple binarization trees. See Figure 4 for a binarizable sequence (1, 2, 4, 3) with its two possible binarization trees and a non-binarizable sequence (2, 4, 1, 3). Definition 5
An SCFG is said to be binarizable if the permutation of each synchronous production is binarizable. We denote the class of binarizable SCFGs as bSCFG .
 (for example, parsable in O ( | w | 6 )) and covers many interesting longer-than-two rules.
The goal of synchronous binarization , then, is to convert a binarizable grammar G in bSCFG, which might be n -ary with n  X  2, into an equivalent binary grammar G generates the same string pairs (see Figure 5). This is always possible because for each 566 decomposes the original permutation into a set of binary ones. All that remains is to decorate the skeleton binarization tree with nonterminal symbols and attach terminals following theorem: Theorem 1
For each grammar G in bSCFG, there exists a binary SCFG G 4. Binarization Algorithms
We have reduced the problem of binarizing an SCFG rule into the problem of binarizing its permutation. The simplest algorithm for this problem is to try all bracketings of a permutation and pick one that corresponds to a binarization tree. The number of all possible bracketings of a sequence of length n is known to be the Catalan Number (Catalan 1844) which grows exponentially with n . A better approach is to reduce this problem to an instance of synchronous ITG parsing (Wu 1997). Here the parallel string pair that we the ITG parsing is to find a synchronous tree that agrees with the alignment indicated by the permutation. Synchronous ITG parsing runs in time O ( n to O ( n 4 ) because there is no insertion or deletion in a permutation. trees for many permutations whereas we just need one. We would prefer a consistent pattern of binarization trees across different permutations so that sub-binarizations (vir-tual nonterminals) can be shared. For example, permutations (1, 3, 2, 5, 4) and (1, 3, 2, 4) can share the common sub-binarization tree [1,  X  3, 2  X  ]. To this end, we can borrow the non-ambiguous ITG of Wu (1997, Section 7) that prefers left-heavy binarization trees so that for each permutation there is a unique synchronous derivation. definition of binarization trees accordingly.
 Definition 6 tree is the rightmost binarizable split.
 binarizable sequence.
 4.1 The Linear-Time Skeleton Algorithm
Shapiro and Stephens (1991, page 277) informally present an iterative procedure that, in each pass, scans the permuted sequence from left to right and combines two adja-cent subsequences whenever possible. This procedure produces canonical binarization trees and runs in O ( n 2 ) time because we need n passes in the worst case. Inspired by the Graham Scan Algorithm (Graham 1972) for computing convex hulls from computa-tional geometry, we modify this procedure and improve it into a linear-time algorithm that only needs one pass through the sequence.
 shift-reduce algorithm. It maintains a stack for contiguous subsequences discovered so far; for example: 2 X 5, 1. In each iteration, it shifts the next number from the input and repeatedly tries to reduce the top two elements on the stack if they are consecutive. See
Algorithm 1 for the pseudo-code and Figures 6 and 7 for example runs on binarizable and non-binarizable permutations, respectively.
 Lemma 2
If c is a permuted sequence (properly) within a binarizable permuted sequence a , then c is also binarizable.
 Proof
We prove by induction on the length of a . Base case: | a | = 2, a (proper) subsequence of 568
Algorithm 1 The linear-time binarization algorithm. split be ( b 1 , c 1 ; c 2 , b 2 ), where c = ( c 1 ; c 2 we have c 1 &lt; c 2 or c 1 &gt; c 2 . By Lemma 1 again, we have that ( c i.e., both c 1 and c 2 are themselves permuted sequences. We also know both ( b ( c 2 , b 2 ) are binarizable. By the induction hypothesis, c we conclude that c = ( c 1 ; c 2 ) is binarizable (See figure 8).  X  Theorem 2
Algorithm 1 runs in time linear to the length of the input, and succeeds (i.e., it reduces binarizable, in which case the binarization tree recovered is canonical. Proof 1. If Algorithm 1 succeeds, then a is binarizable because we can recover a 2. If a is binarizable, then Algorithm 1 must succeed and the binarization tree 570 3. The running time of Algorithm 1 (regardless of success or failure) is linear  X  4.2 Dealing with Terminals and Adapting to Tree Transducers
Thus far we have discussed how to binarize synchronous productions involving only nonterminals through binarizing the corresponding skeleton permutations. We now turn to technical details for the implementation of a synchronous binarizer in real MT systems. We will first show how to deal with the terminal symbols, and then describe how to adapt it to tree transducers.
 Figure 10(a)). The alignment matrix is shown in Figure 11.
 from both languages when appropriate. It turns out we can do this quite freely as long as we can uniquely reconstruct the original rule from its binary parse tree. We use the following rules for this step: 1. Attach source-language terminals to the leaf nodes of the skeleton tree. 2. Attach target-language terminals to the internal nodes (virtual
For example, at the leaf nodes, the Chinese word # / f `uz  X e is attached to RB 1 , and  X 
PP- X  / de , we also include the English-side string responsible for the (Figure 10(c)). In decorated binarization tree gives us the following binary SCFG rules: (8) 572 where the virtual nonterminals (illustrated in Figure 11) are: we create for the virtual nonterminals reflect the underlying sub-alignments, ensuring intermediate states can be shared across different string-to-tree rules without causing ambiguity.
 in the rule (including both terminals and nonterminals).
 monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: two-step SCFG derivation. For example, consider the following tree transducer rule: side subtree. This gives the following two SCFG rules:
The newly created nonterminals ensure that the newly created rules can only combine with one another to reconstruct the original rule, leaving the output of the transducer, we solved previously. 5. Experiments binarization. 5.1 How Many Rules are (Synchronously) Binarizable? binarizable cases over all permutations of length n quickly approaches 0 as n grows (see Figure 12). However, for machine translation, the percentage of synchronous rules that are binarizable is what we care about. We answer this question in both large-scale automatically aligned data and small-scale hand-aligned data.

Automatically Aligned Data. Our rule set here is obtained by first doing word alignment using GIZA++ on a Chinese X  X nglish parallel corpus containing 50 million words in
English, then parsing the English sentences using a variant of the Collins parser, and finally extracting rules using the graph-theoretic algorithm of Galley et al. (2004). We did a spectrum analysis on the resulting rule set with 50,879,242 rules. Figure 12 shows how the rules are distributed against their lengths (number of nonterminals). We can see that the percentage of non-binarizable rules in each bucket of the same length does not exceed 25%. Overall, 99.7% of the rules are binarizable. Even for the 0.3% of rules that are not binarizable, human evaluations show that the majority are due to alignment errors. Because the rule extraction process looks for rules that are consistent with both from GIZA++, errors in either parsing or word-level alignment can lead to noisy rules being input to the binarizer. It is also interesting to know that 86.8% of the rules have monotonic permutations, i.e., either taking identical or totally inverted order. 5.2 Hand-Aligned Data
One might wonder whether automatic alignments computed by GIZA++ are system-atically biased toward or against binarizability. If syntactic constraints not taken into account by GIZA++ enforce binarizability, automatic alignments could tend to contain spurious non-binarizable cases. On the other hand, simply by preferring monotonic alignments, GIZA++ might tend to miss complex non-binarizable patterns. To test this, we carried out experiments on hand-aligned sentence pairs with three language pairs: Chinese X  X nglish, French X  X nglish, and German X  X nglish.

Chinese X  X nglish Data. For Chinese X  X nglish, we used the data of Liu, Liu, and Lin (2005) which contains 935 pairs of parallel sentences. Of the 13,713 rules extracted using the same method described herein, 0.3% (44) are non-binarizable, which is exactly the 574 same ratio as the GIZA-aligned data. The following is an interesting example of non-binarizable rules: where ... in with ... Mishira is the long phrase in shadow modifying Mishira . Here the non-binarizable permutation is (3, 2, 5, 1, 4), which is reducible to (2, 4, 1, 3). The SCFG version of the tree-transducer rule is as follows:
It is interesting to examine dependency structures, as some authors have argued that they are more likely to generalize across languages than phrase structures. The Chinese but the dependency structures on both sides are isomorphic (i.e., this is an extremely literal translation).
 (10) ... where the SCFG version of the tree-transducer rule (in the same format as the previous example) is: dependency structures are not isomorphic between the two languages (Eisner 2003). cases, at least in  X  X ixed-word-order languages that are lightly inflected, such as English and Chinese. X  Our empirical results not only confirm that this is largely correct (99.7% in our data sets), but also provide, for the first time,  X  X eal examples X  between English and
Chinese, verified by native speakers. It is interesting to note that our non-binarizable examples include both cases of isomorphic and non-isomorphic dependency structures, indicating that it is difficult to find any general linguistic explanation that covers all such examples. Wellington, Waxmonsky, and Melamed (2006) used a different measure of non-binarizability, which is on the sentence-level permutations, as opposed to rule-level permutation as in our case, and reported 5% non-binarizable cases for a different hand-aligned English X  X hinese data set, but they did not provide real examples. French X  X nglish Data. We analyzed 447 hand-aligned French X  X nglish sentences from the
NAACL 2003 alignment workshop (Mihalcea and Pederson 2003). We found only 2 topicalization: The second instance is due to movement of an adverbial:
German X  X nglish Data. We analyzed 220 sentences from the Europarl corpus, hand-aligned by a native German speaker (Callison-Burch, personal communication). Of 2,328 rules extracted, 13 were non-binarizable, or 0.6%. Some cases are due to separable German verb prefixes:
Here the German prefix auf is separated from the verb auffordern (request). Another cause of non-binarizability is verb-final word order in German in embedded clauses: we studied, German X  X nglish had the highest percentage with 0.6%. The fact that the
German X  X nglish examples are due to syntactic phenomena such as separable prefixes and verb-final word order may indicate that an MT system would have less freedom to adverbial placement, heavy NP shift, and topicalization that we see in the Chinese X  576
English and French X  X nglish data. The results on binarizability of hand-aligned data for the three language pairs are summarized in Table 1.

Chinese X  X nglish Example (9), we can move the English PP on the same day to the first larly, we can avoid non-binarizability in French X  X nglish Example (12) by moving the
Example (13) would also become binarizable by replacing call on with a single word hypothesis by attempting to explain existing real data (the hand-aligned parallel text), the subsequent decoding experiment. This subsection not only provides the first solid confirmation of the existence of linguistically-motivated non-binarizable reorderings, but also motivates further theoretical studies on parsing and decoding with these non-binarizable synchronous grammars, which is the topic of Sections 6 and 7. 5.3 Does Synchronous Binarization Help Decoding?
We did experiments on our CKY-based decoder with two binarization methods. It is the responsibility of the binarizer to instruct the decoder how to compute the language model scores from children nonterminals in each rule. The baseline method is mono-lingual left-to-right binarization. As shown in Section 2, decoding complexity with this method is exponential in the size of the longest rule, and because we postpone all the language model scorings, pruning in this case is also biased.
 baseline system without the 0.3% of rules that are non-binarizable and did not observe any difference in BLEU scores. This indicates that we can safely focus on the binarizable rules, discarding the rest.
 synchronous binarizer. As shown in Section 1, this results in a simplified decoder with a polynomial time complexity, allowing less aggressive and more effective pruning based on both translation model and language model scores.
 has 116 Chinese sentences of no longer than 15 words, taken from the NIST 2002 test set. Both systems use trigram as the integrated language model. Figure 13 demonstrates that decoding accuracy is significantly improved after synchronous binarization. The number of edges (or items , in the deductive parsing terminology) proposed during decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system.
 state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system
Machine Translation Team 2006), achieving the best overall BLEU scores in the Chinese-to-English track among all participants. 4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system. 6. One-Sided Binarization that are not binarizable. This is primarily of theoretical interest, as we found that they constitute a small fraction of all rules, and removing these did not affect our Chinese-to-
English translation results. However, non-binarizable rules are shown to be important in explaining existing hand-aligned data, especially for other language pairs such as
German X  X nglish (see Section 5.2, as well as Wellington, Waxmonsky, and Melamed [2006]). Non-binarizable rules may also become more important as machine translation 578 systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strate-gies for general SCFGs, which we believe is an important problem.
 the strong  X  X ontinuity X  constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where binarized rules are always continuous in one of the two languages, but may be discontinuous in the other. We will present a CKY-style algorithm (Section 6.2) for finding the best parsing strategy under this new constraint, which we call one-sided binarization . In practice, this factorization has the advantage that we need to maintain only one set of language model boundary words for each partial hypothesis. complexity within this constraint. But most importantly, as the synchronous binariza-tion algorithm covers most of the SCFG rules in real data, the one-sided binarization the non-binarizable rules in real data. So this section can be viewed as a middle step optimal factorization coming in Section 7, and also a trade-off point between simplicity and asymptotic complexity for parsing strategies of SCFGs. Table 3 summarizes this incremental structure of the whole paper. 6.1 Formalizing the Problem
The complexity for decoding given a grammar factorization can be expressed in terms of the number of spans of the items being combined at each step. As an example, Figure 14 shows the three combination steps for one factorization of the non-binarizable rule:
At each step, we consider all positions in the Chinese string as possible end-points for the rule X  X  child nonterminals. Each step combines two dynamic programming items covering disjoint spans of the Chinese input, and creates a new item covering the union nonterminals A and B are combined, A has one span in Chinese, from position y y 2 in the string, and B has one span from y 3 to y 4 . The chart entry for the nonterminal pair { A , B } must record a total of four string indices: positions y Chinese string.
 the spans of each subset. However, some of the indices are tied together: If we are joining two spans into one span in the new item, one of the original spans X  end-points must be equal to another span X  X  beginning point. For example, the index y
A in Chinese, as well as the beginning position of D . In general, if we are combining a subset B of nonterminals having b spans with a subset C having c spans, to produce a spans for a combined subset A = B  X  C , the number of linked indices is b + c  X  a . In the example of the first step of Figure 14, subset { A } has two spans (one in each language) so b = 1, and { B } also has two spans, so c = 1. The combined subset { A , B } has two spans, so a = 2. The total number of indices involved in a combination of two subsets is number of shared indices. In the first step of Figure 14, a + b + c = 1 + 1 + 2 = 4 total indices, and therefore the complexity of this step is O ( | w | input Chinese strings, and we ignore the language model for the moment. Applying this formula to the second and third step, we see that the second is O ( | w | third is again O ( | w | 4 ).
 search over possible orders in which partial translation hypotheses can be built by suc-cessively combining nonterminals. Any strategy we find can be used for synchronous parsing as well as decoding. For example, the strategy shown in Figure 14 can be used to parse an input Chinese/English string pair. The complexity of each step is determined by the total number of indices into both the Chinese and English strings. Each step in 580
Algorithm 2 An O ( n 3 ) CKY-style algorithm for parsing strategies, keeping continuous complexity of the best parsing strategy found. the diagram has three indices into the English string, so the complexity of the first step is O ( | w | 4 + 3 ) = O ( | w | 7 ), the second step is O ( | w | 6.2 A CKY-Style Algorithm for Parsing Strategies
The O ( n 3 ) algorithm we present in this section can find good factorizations for most non-binarizable rules; we discuss optimal factorization in the next section. This algorithm, shown in Algorithm 2, considers only factorizations that have only one span in one of the two languages, and efficiently searches over all such factorizations by combining adjacent spans with CKY-style parsing. 5 The input is an SCFG grammar rule in its abstract form, which is a permutation, and best is a dynamic programming table used to store the lowest complexity with which we can parse a given subset of the input rule X  X  child nonterminals.
 ing continuous spans in one of the two dimensions, in general the best factorization may require discontinuous spans in both dimensions. As an example, the following parses across:
This pattern, shown graphically in Figure 15 for n = 16, can be parsed in time O ( | w | by maintaining a partially completed item with two spans in each dimension, one terminal at a time to the partially completed item, as shown in Figure 15 (right). How-ever, our CKY factorization algorithm will give a factorization with n / 2 discontinuous cubic-time algorithm grows with n , even when a constant number of spans is possible, implying that there is no approximation ratio on how close the algorithm will get to the optimal solution. 7. Optimal Factorization because in some cases it is better to maintain multiple spans in the output language (despite the extra language model state that is needed) in order to maintain continuous strategies that are guaranteed to be optimal in their asymptotic complexity. (alignment) using complex rules. This answers a question left open by earlier work in synchronous grammars: Although Satta and Peserico (2005) show that tabular parsing of a worst-case SCFG can be NP-hard, they do not give a procedure for finding the com-plexity of an arbitrary input grammar. Similarly, Melamed (2003) defines the cardinality of a grammar, and discusses the interaction of this property with parsing complexity, but does not show how to find a normal form for a grammar with the lowest possible cardinality.
 rule in Section 7.1, and then present an exponential-time dynamic programming algo-rithm for finding the best strategy in Section 7.2. We prove that factorizing an SCFG rule into smaller SCFG rules is a safe preprocessing step for finding the best strategy in Section 7.4, which leads to much faster computation in many cases. First, however, we take a brief detour to discuss modifying our a + b + c formula from the previous section in order to take the state from an m -gram language model into account during
MT decoding. 582 7.1 Taking the Language Model into Account
First, how do we analyze algorithms that create discontinuous spans in both the source
For synchronous parsing, if we are combining item B with b source spans with item C having c e target spans and c item A having a e target spans and a f source spans, the complexity of the operation is m -gram language model. At first glance, because we need to maintain ( m  X  1) boundary words at both the left and right edges of each target span, the total number of interacting variables is: we can optimize the decoding algorithm by factorizing the dynamic programming combination rule into two steps. One step incorporates the language model probability, and the other step combines adjacent spans in the input language and incorporates the
SCFG rule probability. The hook trick for a bigram language model and binary SCFG of the input foreign sentence, and u , v , v 1 , u 2 (or u , v , v words, which we assume to take O ( | w | ) possible values. There are seven free variables related to input size for doing the maximization computation, hence the algorithmic complexity is O ( | w | 7 ).
 variables enclosed in the innermost max operator, we get five: i , k , u , v where decomposition eliminates one free variable, v 1 . In the outermost level, there are six free variables left. The maximum number of interacting variables is six overall. So, we have reduced the complexity of ITG decoding using the bigram language model from O ( | w | to O ( | w | 6 ).
 complex SCFGs, each left boundary for a substring of an output language hypothesis contains m  X  1 words of language model state, and each right boundary contains a  X  X ook X  specifying what the next m  X  1 words must be. This yields a complexity analysis similar to that for synchronous parsing, based on the total number of boundaries, but now multiplied by a factor of m  X  1: for translation from source to target.
 Definition 7
The number of m -gram weighted spans of a constituent, denoted a number of source spans plus the number target spans weighted by the language model factor ( m  X  1): in Equation 17 as a simple sum of the numbers of weighted spans of constituent subsets
A , B , and C : and more generally when k  X  2 constituents are combined together:
It can be seen that, as m grows, the parsing/decoding strategies that favor contiguity on the output side will prevail. This effect is demonstrated by the experimental results in Section 7.5.
 parsing or decoding. A strategy for parsing (or decoding) the entire rule must build up the complete set of the rule X  X  children through a sequence of such combinations. Thus a parsing strategy corresponds to a recursive partitioning of the rule X  X  children, that is, an unordered rooted tree having the child nonterminals as leaves. Each node in the partition tree represents a subset of nonterminals used as a partial result in the chart for parsing, built by combining the subsets corresponding to the node X  X  children. This combination step at each node has complexity determined by the number of spans, worst combination step. We wish to find the recursive partition with the lowest overall complexity. Unfortunately, the number of recursive partitions of n items grows super-584 exponentially, as 0 . 175 n ! n  X  3 / 2 2 . 59 n =  X  (  X  ( n + 1)2 . 59
More formally, the optimization over the space of all recursive partitions is expressed as: that we can solve the optimization problem using dynamic programming techniques. 7.2 Combining Two Subsets at a Time Is Optimal in our recursive partitions, by showing that any ternary combination can be factored into two binary combinations with no increase in complexity. This fact leads to a more SCFG rule.
 Theorem 3 enables tabular parsing of an input sentence w in time O ( | w | recursive binary partition whose corresponding parser is also O ( | w | Proof
We use the notion of number of weighted spans (Equation (20)) to concisely analyze the complexity of synchronous parsing/decoding. For any fixed m , we count a constituent X  X  number of spans using the weighted span value from Equation (18), and we drop both the adjective  X  X eighted X  and the m subscript from this point forward.
 A = where b i is the number of spans for B i and a is the number of spans for the resulting item A .
 spans, and C has c spans. In the example shown in Figure 17, x = 2, a = 2, b = 1, and c = 2. The complexity for parsing this is O ( | w | a + b + c + x and refer to the number of spans in partial constituent Y as y . Parsing Y  X  AB takes time O ( | w | y + a + b ), so we need to show that to show that we can parse this new rule in no more time than the original ternary rule.
Subtracting a + b from both sides, we need to prove that
Each of the y spans in Y corresponds to a left edge. (In the case of decoding, each edge has a multiplicity of ( m  X  1) on the output language side.) The left edge in each span of Y corresponds to the left edge of a span in X or to the right edge of a span in C . Therefore, Y has at most one span for each span in C  X  X , so y  X  c + x .
O ( | w | x + y + c ). We know that since Y was formed from A and B . Therefore so parsing X  X  YC also takes no more time than the original rule X  X  ABC . By induction over the number of subsets, a rule having any number of subsets on the right-hand side can be converted into a series of binary rules.  X  mal implies that we need consider only binary recursive partitions, which correspond to unordered binary rooted trees having the SCFG rule X  X  child nonterminals as leaves. The total number of binary recursive partitions of n nodes is (Schr  X  oder 1870, Problem III). Note that this number grows much faster than the Catalan
Number, which characterizes the number of bracketings representing the search space of synchronous binarization (Section 4).
 the binary branching property also enables a straightforward dynamic programming algorithm, shown in Algorithm 3. The same algorithm can be used to find optimal strategies for synchronous parsing or for m -gram decoding: for parsing, the variables a , b , and c in Line 9 refer to the total number of spans of A , B , and C (Equation (16)), 586
Algorithm 3 An O (3 n ) search algorithm for the optimal parsing strategy that may contain discontinuous spans. while for decoding, a , b , and c refer to weighted spans (Equation (19)). The dynamic optimal strategy has already been computed. In each iteration of the algorithm X  X  inner loop, each of the child nonterminals is identified as belonging to B , C , or neither B nor
C , making the total running time of the algorithm O (3 n n , it is a significant improvement over considering all recursive partitions. 1977), in which dynamic programming items are placed on a priority queue sorted according to their complexity, and only used to build further items after all items of lower complexity have been exhausted. This technique, shown in Algorithm 4, guar-antees polynomial-time processing on input permutations of bounded complexity. To string to represent the spans X  boundaries. For each index we must specify whether the corresponding nonterminal either starts a span of subset B , starts a span of subset C , or ends a span of B  X  C . Therefore there are O ((3 n ) k ) rules of complexity no greater than algorithm will find it after, in the worst case, popping all O ((3 n ) less than or equal to k off of the heap in the outer loop, and combining each one with all other O ((3 n ) k ) such rules in the inner loop, for a total running time of O (9 correlated to n ), the best-first behavior makes it much more practical for our empirically observed rules. 7.3 Adding One Nonterminal at a Time Is Not Optimal
One might wonder whether it is necessary to consider all combinations of all subsets of nonterminals, or whether an optimal parsing strategy can be found by adding one nonterminal at a time to an existing subset of nonterminals until the entire permutation has been covered. Were such an assumption warranted, this would enable an O ( n 2 dynamic programming algorithm. It turns out that one-at-a-time parsing strategies in Figure 18, can be parsed in time O ( | w | 8 ) using unconstrained subsets, but only in time O ( | w | 10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time. 7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable Permutations considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time after factorizations, implying that k c  X  ( n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules.
 itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block of numbers that are permuted within a permutation, the
Algorithm 4 Best-first search for the optimal parsing strategy. 588 optimal parsing strategy for the entire permutation does not have to involve interactions between subsets of numbers inside and outside the block. We call filled entries in the permutation matrix pebbles ; the contiguous blocks are shaded in Figure 19, and form submatrices with a pebble in each row and column. We can first decompose a given permutation into a hierarchy of smaller permutations as the tree shown in Figure 19 and then apply the discontinuous strategy to the non-decomposable permutations in the tree. So, in this example, we just need to focus on the optimal parsing strategy for minimization, we can effectively reduce the search space without losing optimality of the parsing strategy for the original permutation.
 Theorem 4 enables tabular parsing of an input sentence w in time O ( | w | of child nonterminals forming a single continuous span in each language, then there exists a recursive partition containing S as a member whose corresponding parser is also O ( | w | k ).
 factorizing the permutation into smaller permutations [Section 7.4]). 7.5 Experiments
The combination of minimizing SCFG rule length as a preprocessing step and then applying the best-first version of Algorithm 3 makes it possible to find optimal parsing strategies for all of the rules in the large Chinese X  X nglish rule set used for our decoding experiments. For the 157,212 non-binarizable rules (0.3% of the total), the complexity takes approximately five minutes of CPU time to analyze this single rule, but processes all others in less than one second.
 rules extracted from the Chinese X  X nglish data. The CKY-on-English method found an optimal parsing strategy for 98% of the rules, and its worst-case complexity over the entire ruleset was O ( | w | 15 ), rather than the optimal O ( | w | from two directions (one for the permutation  X  and the other for the permutation  X  and take the minimum of both, we can get an even better approximation. In Table 4, we compare the approximate strategy which takes the minimum of CKY runs for two languages, which we call CKY-min, with the optimal strategy. For synchronous parsing, for 99.77% of the rules, the CKY-min method found an optimal strategy. When generalized for m -gram integrated decoding, CKY maintains continuous spans on the output language and allows for discontinuous parsing on the input sentence. The differ-ence between CKY-on-output and the optimal decoding strategy was negligible in the situation of trigram-integrated decoding for the given rules. The worst-case complexity for decoding into English by CKY-on-English was O ( | w | optimal strategy. The CKY-on-English approach found an optimal decoding strategy for 99.97% of the non-binarizable rules. The CKY-min strategy was even better, only finding sub-optimal results for six rules out of all rules, which translates to 99.996%. In
Table 4, we have also included the comparison for translating into Chinese, in which case the inverted permutations are used and the language model weight is put on the
Chinese side. A similar approximation accuracy was achieved. 7.6 Bounds on Complexity of Factorization ask whether the problem is provably NP-complete. Gildea and the treewidth of a graph derived from the rule X  X  permutation. Computing treewidth of arbitrary graphs is NP-complete (Arnborg, Corneil, and Proskurowski 1987), but the graphs derived from SCFG permutations have a restricted structure that it might be computing treewidth for graphs of bounded degree nine was shown to be NP-complete by Bodlaender and Thilikos (1997), whether the treewidth problem for graphs of degree between three and eight is NP-complete is not known. Thus, whether computing the optimal parsing strategy for an SCFG rule is NP-complete remains an interesting open problem. 590 8. Conclusion
This work develops a theory of binarization for synchronous context-free grammars. We present a technique called synchronous binarization along with an efficient binariza-tion algorithm. Empirical study shows that the vast majority of syntactic reorderings, at least between languages like English and Chinese, can be efficiently decomposed into hierarchical binary reorderings. As a result, decoding with n -gram models can be fast and accurate, making it possible for our syntax-based system to overtake a comparable phrase-based system in BLEU score.
 vide, for the first time, real examples verified by native speakers. For these remaining rules, we have shown an exponential time algorithm for finding optimal parsing strate-gies, which runs quite fast with the help of two optimality-maintaining operations and the A* search strategy. We also provide an efficient approximation, which usually finds translation system, these parsing strategies are primarily of theoretical interest, though they may become more important in future systems.
 Acknowledgments Appendix A. Proof of Theorem 4 permutation P splits a contiguous block S of P into two subtrees T at the top of Figure 20, in either or both of which there are some pebbles from outside Equation (18) to account for m -gram language model state. We use d number of spans of the pebbles outside of S in T L . d LI achieved by merging the pebbles inside and outside of S for T of the root of T L is d LO + d LI  X  r L . We have symmetric notions for T the number of spans of the root of the subtree T after merging T of spans is annotated for each node in Figure 20.
 inside pebbles and outside pebbles in each language. Each boundary in the source corresponds to the reduction of one span. Each boundary in the target corresponds to the reduction of one weighted span of ( m  X  1). In total, we can reduce the number of r
R  X  m or r L  X  m . Without loss of generality, we assume
T is best ( T ) = max { best ( T L ), best ( T R ), (( d LO + d T and T R . The improved strategy on the bottom merges the pebbles inside S together before making combinations with pebbles outside S .
 592 ing each of T L and T R into two trees involving pebbles purely inside or outside of S , as shown at the bottom of Figure 20. The separation works by simply ignoring the pebbles that are not inside when creating the inside half of the tree or outside when doing the outside half throughout T L and T R . Then we have four elementary subtrees T T RI , and T RO . In our new strategy, we recombine the four elementary trees by merging T LI and T RI to create a pebble first and merging the resulting pebble back into T make a T 0 LO , and finally merging T 0 LO with T RO .
 node in these trees is reduced or not changed as compared to that before separation. Using the a + b + c formula with reduced a , b , and c will produce lower complexity.
Roughly speaking, the reason is the inside pebbles and outside pebbles are positioned combining both sides is upper-bounded by 2 m , considering there are two boundaries in each language. At the same time, the number of spans of either the inside pebbles or the outside pebbles is lower-bounded by 2 m because both T cover S . Hence, we have the following set of inequalities: created from T L by pruning away the pebbles that are inside S , the pebble of S can join T
LO by taking the place of any trace of the pruned leaves and making the number of spans from the bottom up to the root no greater than in the counterpart nodes in T than the original T L : number of spans for each node in the reorganized tree is shown in Figure 20 (bottom), where r (  X  2 m ) is the reduction in spans after combining the new pebble S with T sums up the reductions achievable on the four boundaries of S with T up the reductions on some of the four boundaries. Thus, yield of the old strategy (Equations [A.3 X  X .6]). We need to bound the remaining terms.
Both of them can be bounded by the third term inside the maximization of Equa-tion (A.2). The first inequality is which is equivalent to which is true because d  X  m (since T has at least one pebble), and d (since the number of reducible spans is less than the total number of outside spans).
This simplifies to This inequality is true because m  X  d LI , since there is at least one inside pebble in T d  X  r R because d RI  X  m  X  r R , referring to Equation (A.1), and finally r  X  r in Equation (A.7).
 ganization. In the example, the updated parsing/decoding complexity is O ( | w | better than before ( O ( | w | 7 m  X  3 )).
 boundaries cannot be better than an optimized strategy that respects such boundaries.  X  References 594
