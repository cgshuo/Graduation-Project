 Collective classification in relational data has become an im-portant and active research topic in the last decade. It ex-ploits the dependencies of instances in a network to improve predictions. Related applications include hyperlinked docu-ment classification, social network analysis and collaboration network analysis. Most of the traditional collective classifi-cation models mainly study the scenario that there exists a large amount of labeled examples (labeled nodes). However, in many real-world applications, labeled data are extremely difficult to obtain. For example, in network intrusion de-tection, there may be only a limited number of identified intrusions whereas there are a huge set of unlabeled nodes. In this situation, most of the data have no connection to labeled nodes; hence, no supervision knowledge can be ob-tained from the local connections. In this paper, we propose to explore various latent linkages among the nodes and ju-diciously integrate the linkages to generate a latent graph. This is achieved by finding a graph that maximizes the link-ages among the training data with the same label, and max-imizes the separation among the data with different labels. The objective is further cast into an optimization problem and is solved with quadratic programming. Finally, we apply label propagation on the latent graph to make prediction. Experiments show that the proposed model LNP ( L atent N etwork P ropagation) can improve the learning accuracy significantly. For instance, when there are only 10% of la-beled examples, the accuracies of all the comparison models are less than 63%, while that of the proposed model is 74%. H.2.8 [ Database Management ]: Database applications X  Data mining Algorithms
In relational datasets and information networks, instances are not independent nor identically distributed. Instead, they are correlated with complex dependencies. For exam-ple, in collaboration networks, the researchers who collabo-rate with each other are more likely to share similar research topics than those researchers without any collaboration. To exploit these data dependencies and improve predictions, collective classification ( e.g. ,[12,6,13])isproposedandit receives considerable attentions in the last decade. One of the key ideas of collective classification is to construct new feature vectors by appending a new set of attributes that summarizes the relational information ( e.g. , [6, 13, 15, 9, 12]). For instance, [12] is among the first works that summa-rize the labels of neighboring nodes and treat the summary as new features. The approach is illustrated in Fig. 1(a), where node  X  X  X  has three positive neighboring nodes and one negative neighboring node. As a result,  X  &lt; 3, 1 &gt;  X  X sap-pended onto the original feature vector as new features as in Fig. 1(a). Note that most of the traditional collective classification models mainly study the scenario where there are a lot of labeled nodes. In this scenario, the supervision knowledge can be effectively propagated in the network and improve the learning accuracy. However, in many applica-tions, the number of training data is actually very limited. The following provides some examples: We illustrate in Fig. 1(b) to show the drawback of traditional collective classification when there are a limited number of training examples. Note that traditional models will first train a classifier based on the new feature vectors such as the one in Fig. 1(a). However, since there are only a limited number of training data, most of the nodes may not directly link to the labeled nodes, such as node A in Fig. 1(b). In this situation, the relational summary will become all 0 X  X ; thus, no local supervision knowledge can be used. As a result, the prediction accuracy drops significantly (more empirical results are studied in Section 4).

To tackle the label deficiency problem, we propose to ex-plore the latent linkages among the nodes, and judiciously integrate them to construct a latent graph. For example, in a citation network, in addition to the citation relations, those papers with similar keywords or concepts may also be sim-ilarly. This kind of latent analysis can be obtained through the node attributes, e.g. , paper title. If more information, such as abstract or the paper, is available, one can also derive some other latent relations such as semantic similarity. We call such indirect linkages latent linkages . Note that these latent linkages are already embedded in the dataset but may need some efforts to dig them out. By adding the latent link-ages, the supervision knowledge may be able to propagate more effectively as illustrated in Fig. 1(c). In this paper, we give five examples of latent linkages, and propose a general framework to construct the latent graph by integrating the latent linkages. The objective is to find a graph where the data with the same class label are linked together and those with different class labels are disconnected. It is further formulated as an optimization problem and is solved with quadratic programming. With the optimal latent graph, label propagation is applied to pass the supervision infor-mation throughout the network and make predictions. Ex-periments include three real-world graph datasets, and the proposed model can improve the accuracy by as much as 15% as compared with the traditional collective classifiers. The most important contributions of the proposed approach can be summarized as follows: The rest of the paper is organized as follows. We start by a formal description of collective classification. The proposed latent network propagation model (abbreviated as  X  X NP X ) is described in Section 3, followed by the experiment in Section 4. A brief introduction of the related work is given in Section 5. In Section 6, we conclude the paper.
In this section, we formulate the problem of collective clas-sification, and discuss the reason why it may induce poor performance with insufficient training examples. We first in-troduce the notations (Table 1) that will be used throughout this paper. Suppose we are given a relational dataset rep-resented as a graph G ( V , E , X , Y )where V = { v 1 ,  X  X  X  is a set of nodes, and E  X  R n  X  n is the adjacency ma-trix where E ( i, j ) is the weight of the edge between node v i and v j . For each node v i  X  V , it has a class label y  X  X  c 1 ,  X  X  X  ,c k } drawn from the k labels c 1 ,  X  X  X  ,c k a vector of attributes x i  X  R d in the d -dimensional input space. Let X = { x 1 ,  X  X  X  , x n } and Y = { y 1 ,  X  X  X  ,y denote L  X  V as the set of labeled examples and U = V  X  L as the set of unlabeled data. Our objective is to give pre-dictions on the unlabeled nodes V U by making use of both the attribute vectors X as well as the network E . The key idea of collective classification is to generate a new feature vector f  X  i for each data v i where x i = &lt;f 1 ,f 2 ,  X  X  X  ,f d &gt; is the original feature vector and c 1 ,c 2 ,  X  X  X  ,c k are the summary of neighboring node class labels. For example, in a binary classification problem, sup-pose v i is linked to 4 labeled examples where 3 are positive illustrated in Fig. 1(a). In other words, the new feature vec-tor can be generated by appending the class label summary to the original feature vector. With the new feature vectors, many traditional classification models [12, 6, 13] can be ap-plied to give prediction. However, the above schema may not work well when there are a limited number of labeled data. This is because if | L || V | , most of the nodes may not connect to labeled examples as illustrated in Fig. 1(b). As such, the class summary part of the new feature vector will be all 0 X  X : Hence, the relational models degenerates to the unrelational classifiers which only use the original feature vector f  X  f ,f 2 ,  X  X  X  ,f d &gt; . Thus the graph information does not con-tribute to the modeling, and the accuracy may drop. To tackle the label deficient problem, we propose LNP to gener-ate a latent graph by exploring various latent linkages among the nodes. The objective is as follows: Proposition 1 It is our objective to find a transformation G (
V , E , X , Y )  X  G  X  ( V ,  X  E , X , Y ) where  X  E  X  R n  X  n jacency matrix of the latent graph (weighted directed graph). And it is desirable that the latent graph has the following properties: With the latent graph, we can use various algorithms to give prediction. In this paper, we take label propagation as an example.
In this section, a general LNP framework is first proposed to judiciously integrate different kinds of latent linkages to generate a latent graph. We then explore various kinds of latent linkages that can be directly obtained from the rela-tional dataset. The challenge is how to combine the different latent linkages, and we tackle the problem by solving an opti-mization problem and solve it with quadratic programming. The optimization can automatically determine the optimal weights to combine the latent linkages.
In collective classification and other graph applications, there may exist various informative latent relations among the nodes as discussed in the introduction section. More for-mally, denote the set of latent relations as S = { E 1 , E where E i  X  R n  X  n is the adjacency matrix of the i -th latent graph. For example, E 1 may describe the links of docu-ments with similar topics and E 2 may describe the links of papers with similar semantic meanings. We will introduce in the next section how to find the latent linkages based on the given dataset, but at the moment, we focus on how to use the latent linkages. Recall that we aim at finding a transformation G ( V , E , X , Y )  X  G  X  ( V ,  X  E , X , Y the adjacency matrix of the target latent graph constructed from the set of latent linkages S = { E 1 , E 2 ,  X  X  X  , E In other words, the similarity matrix of the target latent graph  X  E is a linear combination of all the latent relations. The key is how to find the optimal combination weights  X  t in order to construct a latent graph with good properties as described in Proposition 1. We present in Section 3.3 about how to solve the above objective. At this moment, we assume that the latent network G  X  ( V ,  X  E , X , Y )isalready found, and we apply label prop agation to make prediction.
Inference via Label Propagation The basic intuition of label propagation is to perform random walk on the net-work when time goes to infinity. As a result, each node will gain a vector of probabilities indicating how likely the la-beled examples will visit the node. At last, each node is assigned to the label that are most likely to be visited from. More formally, it contains the following steps [23]: 1. Let D be the diagonal matrix where D ( i, i )= 2. Calculate F =( D  X   X   X  E )  X  1 Y where  X  is a parameter 3. Assign v i toalabel y i =argmax j F ( i, j ).
 Unlike previous label propagation methods, we do not con-struct the similarity matrix solely from the attribute vectors. Instead, the relational network as well as various kinds of la-tent linkages are also considered.
For LNP, we present five examples of latent linkages which can be easily obtained from the original dataset.
The first latent graph is called K -step graph. It generates links between the nodes that can be reached within k steps. An example of 2-step graph around the center node is given in Fig. 2(b) where the latent linkages are highlighted with dotted red lines. It is formally defined as follows: Definition 1 ( K -step Graph) Given node v i and v j ,the adjacency matrix of the K-step graph is defined as follows E In other words, K -step graph increases the connectivity of the original graph by linking the indirect neighbors. And k is set to be 3 in the experiment. We further denote E ( L ) as the K -step graph especially for the training data L .It will be used in Section 3.3 to train the weights of the latent linkages.
Given the original graph as in Fig. 2(a), we further ex-tend the connectivity of the graph by adding links to the data with the same class label. An example is presented in Fig. 2(c) and it is defined as follows: Definition 2 (Label Graph) Given node v i and v j ,the adjacency matrix of the label graph is defined as follows As before, we denote E ( L ) LABEL as the label graph of the training data.
Another straightforward type of latent linkages is the at-tribute similarity. In other words, we can generate the ad-jacency matrix by evaluating the pairwise similarity among the nodes. For example, in a citation network, we can apply cosine similarity between each pair of documents to generate the matrix. Hence, similar documents are assigned higher weights. An example is illustrated in Fig. 2(d) and the for-mal definition is as follows: Definition 3 (Attribute Similarity Graph) For node v i and v j , the attribute similarity is the cosine similarity: where x i and x j are the attribute vectors of v i and v j respondingly, &lt;  X  X  X  X  &gt; is the dot product and  X  is the Euclidean norm. Note that we denote E ( L ) ATT as the attribute similarity graph especially for training data. More details canbefoundinthenextsection.
Recall that we have a set of labeled examples. Then we can build a classifier with the labeled data, and give predic-tion to all the data. Hence each node v can be represented as a classification confidence vector p = { p 1 ,p 2 ,  X  X  X  ,p p = p ( c i | v ) is the classification confidence that v belongs to class c i . As such, the data with similar confidence vectors are considered to be similar. An example can be found in Fig. 2(e). More formally, it can be defined as follows Definition 4 (Prediction Confidence Graph) Given node v and v j and their corresponding classification confidence vectors p i and p j defined above, their similarity is defined as The similarity matrix on the labeled data is E ( L ) CON ( i, j ) .
In this paper, we also consider the original network struc-ture. This is because the original graph will more or less reflect the similarity of the connected nodes. Hence, we de-fine the structure similarity matrix as follows However, it is difficult to capture the structure similarity among the labeled examples by the above definition. This is because there may be no connection among the few and ran-domly distributed labeled examples. To tackle this problem, we use random walk similarity to approximate the structure similarity among the labeled examples: Definition 5 (Structure Similarity on Labeled Data) Given node v i  X  V L and v j  X  V L , their structure similarity is defined as where R = l  X  =1 c (1  X  c )  X  P  X  is the random walk probability matrix after l step, and P is the transition probability matrix constructed from the adjacency matrix E .
 As in [22], we set c =0 . 2and l = 10. Hence, even if v i and v j has no direct connection, we can still obtain their similarity from the random walk probability matrix.
It is important to note that the above five kinds of latent graphs are just a subset of potential latent linkages. They can be easily obtained from the original dataset. We show in the experiment section that even by using such straight-forward latent graphs in the proposed framework, one can improve the prediction accuracy significantly. We next in-troduce how to judiciously integrate these different latent linkages to find the optimal latent graph. Input :
Relational Graph G ( V , E , X , Y )where V L  X  V is the labeled nodes and V U = V  X  V L is the unlabeled ones. Output : The class labels for V U  X 
Perform label propagation on the latent graph  X  E to give 4 predictions of the unlabeled nodes V U .

Algorithm 1 : LNP: Latent Network Propagation
In this section, we introduce how to find the best strategy to integrate the latent linkages to infer the best graph as described in Eq. 1. The key idea is to learn the best weights from the training examples (only) such that the data with the same label are linked together and those with different labels are disconnected. Hence, during the optimization, we only consider the latent linkages among the labeled examples and try the learn the weights from it. Denote a t  X  k matrix Y as the label matrix:
Y ( i, j )= 1 if the node v i belongs to the j -th class where t is the number of labeled examples and k is the num-ber of class labels. Hence the optimal similarity matrix for the labeled examples should be M = YY T where M is a k  X  k matrix and M ( i, j )=1iff v i and v j have the same class label. Hence, we want to find a linear combination of the latent linkages so that the combining matrix approaches to the optimal matrix M . The objective function in Eq. 1 can thus be written as where E ( L ) i is the i -th type of latent linkages of the la-beled examples L discussed in the last section, and  X  = &lt;  X  , X  2 ,  X  X  X  , X  m &gt; T and  X  is a smoothing parameter with de-fault value 0.01. It can also be learnt from the training data with cross-validation.
 Theorem 6 The optimization in Eq. 9 is equivalent to the following quadratic programming problem: where I is the identity matrix and K is a m  X  m matrix and u is a m  X  1 vector such that
Proof. The optimization function in Eq. 9 can be writ-ten as Note that tr ( YY T YY T ) is a constant. Hence, the opti-mization in Eq. 9 is equivalent to that Eq. 10.
 It is important to note that the optimization in Eq. 10 is a quadric programming problem with conventional format and can be solved efficiently with general solver such as [1]. In our algorithm, since we have only five unknown  X   X  X  (corre-sponding to the five latent similarity matrices), the optimal solution can be obtained in constant time. After obtaining the best weights, we can perform label propagation on the latent network to make prediction. The algorithm is de-scribed in Algorithm 1. Step 1 is to generate the five latent graphs and Step 2 solves the optimization problem to obtain the optimal weights. Step 3 is to construct the target latent graph , and in Step 4, label propagation is applied to give prediction. It is important to emphasize again that since the optimization takes constant time, the complexity of the whole algorithm is the same as the traditional label propa-gation. The most expensive step is to calculate the inverse of the matrix, which is O ( n 2 . 4 )[2].
In this section, the proposed model LNP is evaluated on three real world network datasets. The descriptions of the datasets are summarized in Table 2. As it can be observed, all three datasets have multiple classes with two types of features. One is the conventional vector-based feature and the other is the relational fea ture (network). Hence, a well developed relational model is supposed to make good use of both of the attributes and the networks to give accurate predictions.
 Three algorithms are compared with LNP: Data set #Instances #Features #Edges #Classes Cora 2708 1433 5429 7 Citeseer 3312 3703 4832 6
Gene 672 461 910 15 Note that these are some of the most obvious choices of com-parison methods, commonly adopted in the research commu-nity. In the discussion section, we also study some modifica-tions of the above models, in order to analyze their behaviors with limited number of training data.
 In the experiment, all results are summarized on 10 runs. In each run, we apply the snowball sampling technique [12] to select certain proportion of data as the training exam-ples. This sampling technique isolates the effect of class bias and make the evaluation focus more on a  X  X air X  scenario with balanced classes. Another important thing is that the experiment will focus more on the situation when the pro-portion of training examples is not significantly large. As described in the introduction section, there are many appli-cations that are in such situation, and it is interesting to observe how the collective classifiers work. For evaluation, we use accuracy as a criterion: Accuracy = #Test data that are labeled correctly
In this section, we present the experiment results on three real world datasets: Citeseer, Cora and Gene.
 The Citeseer dataset is a publication dataset 1 and we use a processed version presented in [14]. It is a collection of re-search publications draw from Citeseer, divided into one of the 6 classes: AI, Agents, DB, HCI, IR and ML. There are 3312 instances and 4832 citation links in the dataset. Each instance is described with the  X  X ag of words X  representation. More specifically, the features are binary vectors, which in-dicate whether the instance (paper) contains the word or not. The vocabulary size here is 3703. It is important to mention that the direction of the citation links are ignored as in [12] to simplify the problem. And we also delete the isolated nodes which do not have connection to any other nodes since the isolated nodes have no contribution to the relational network.

The experiment results summarized on 10 runs are pre-sented in Fig. 3. It can be observed that the comparison http://www.cs.umd.edu/projects/linqs/projects/lbc models do not perform well. This is because most of the in-stances do not have edge connecting to the limited number of labeled data; hence the label information cannot be prop-agated through the network effectively. This phenomena is also studied in the introduction and Section 2. It is also important to note that the proposed model substantially outperforms the comparison models by propagating super-vision knowledge through the latent linkages. For example, when there are only 10% of training examples, the proposed LNP model can obtain an accuracy of 74% whereas the ac-curacies of all the comparison models are less than 64%; the improvement is above 15%. This shows the effectiveness and necessity of exploring latent linkages to tackle the label defi-ciency problem. Another interesting observation is that the equal weight LNP (ELNP) performs a lot worse than LNP. This is because LNP assigns higher weights to the informa-tive latent linkages to find a better latent graph. The Cora dataset is also a publication dataset used in [14], which is a collection of machine learning publications cate-gorized into one of the seven classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Rein-forcement Learning, Rule Learning and Theory. It contains 2708 instances and 5429 undirected ci tation links in total. Each instance is also represented as X  X ag of words X  X nd is de-scribed as a 0/1 feature vector. The vocabulary size is 1433.
We present the results in Fig. 4. In this experiment, it can also be observed that traditional collective classifiers ICA, LPP, and the equal weight LNP (ELNP) do not perform well. The proposed model LNP, instead, outperforms all the comparison models substantially and clearly. For instance, when there are 15% of labeled data, the accuracy of LNP is above 70%, while all the others are below 60%. It is also important to note that in this dataset, the advantage of LNP over the comparison model ICA diminishes as the number of training examples increases. This is because if there are sufficient labeled data, ICA can also propagate the supervision knowledge effectively. But it is usually difficult to estimate how many training data are needed for ICA to get a good result. For example, in the Citeseer dataset and the next dataset Gene, ICA does not perform better even if there are 30% of training examples. This also shows the necessity of LNP to handle label deficiency. The Gene dataset is a gene network of the yeast genome, which is downloaded from the KDD cup 2001 2 .Weuse the training set provided by the web as the whole network dataset. After pruning those isolated genes that do not have connection to other genes, we get 672 genes and 910 interac-tion links in total. As the previous two datasets, we ignore the directions of the links. The 15 classes represent the pos-sible localizations of the genome: cytoplasm, cytoskeleton, ER, golgi, mitochondria, nucleus, peroxisome, plasma mem-brane, transport vesicles and vacuole.

The experiment results are summarized in Fig. 5. Note that this dataset contains 15 class labels, which is essentially a difficult multi-class classification problem. We note that in this situation traditional approach like LPP does very poorly. This is because the traditional collective classifiers require a lot of labeled examples to handle the complicated multi-class classification problem. However, the proposed LNP model beats the comparison models consistently. It is also important to note that the performance differences of LNP, ICA and ELNP are not very significant when there are only 5% of training examples. The reason is that all the approaches cannot obtain enough supervision knowledge from the labeled data to tackle the essentially complicated multi-class classification problem. But with a bit more la-beled examples, the accuracy of LNP increases faster and it stays above the accuracies of all the other approaches. An-other observation is that LNP beats the equal weight strat-egy (ELNP) substantially. It reveals the necessity of training unequal weights to emphasize the informative latent linkages and deemphasize those noisy ones.
In this section, we aim at studying the following questions: www.cs.wisc.edu/?dpage/kddcip2001 Figure 6: Why we need to integrate latent linkages? In Section 3.2, five different types of latent linkages are in-troduced. It is thus interesting to study how these different latent linkages contribute to the model and why we need to integrate them. We study the problem on the citeseer dataset where we use the five kinds of latent linkages indi-vidually to generate five different latent graphs. They are 1. K -step graph: the graph links nodes together if they 2. Label graph: the graph links nodes together if they are 3. Structure similarity graph: the network generated from 4. Attribute similarity graph: the network generated from 5. Prediction similarity graph: the network links the data
We compare the inference performances of the five in-dependent graphs with LNP. The results are presented in Fig. 6. It can be observed that the performances of the five kinds of latent linkages are similar. However, when we apply the proposed LNP model to integrate the five, the accuracy increases by as much as 40%. This is because each type of (a) Traditional collective Figure 7: A straightforward modification of tradi-tional collective classifiers into semi-supervised ver-sion the latent linkages only provides the node relations from a specific aspect, but the combination of them can lead to a more comprehensive relation network.
 Note that traditional collective classifiers such as ICA only train on the labeled examples. Hence, if there are only a limited number of training data, the label information can-not be propagated through the network effectively. In this experiment, we make a little modification on ICA to make use of the unlabeled nodes. To do so, we first use a tradi-tional non-relational classifier, such as logistic regression, to train a model based on the attribute vector only. All nodes are assigned pseudo class labels by the non-relational clas-sifier as in Fig. 7(b). In the second step, we apply ICA to train the model with both attribute vectors and the network summary of the pseudo labels obtained in the last step. It is important to note that in this modification, the whole graph can be viewed as a fully labeled graph with both true labels and pseudo labels. As such, ICA can collect more informa-tion from the network summary as illustrated in Fig. 7.
In this experiment, we compare the proposed model with the semi-supervised collective classifier described above. The experiment result on the citeseer dataset is presented in Fig. 8. It is interesting to see that the semi-supervised modi-fication cannot improve the accuracy of ICA too much. This is because when there are a limited number of training exam-ples, the first step of the semi-supervised ICA may generate a lot of incorrect pseudo labels. As such, the performance cannot be improved too much. Hence, it is not trivial to de-vise a new mechanism to tackle the label deficiency problem. From Fig. 8, it can also be observed that the proposed LNP model outperforms both methods significantly. For instance, when the accuracies of both comparison methods are around 70%, the proposed model can achieve an accuracy of 80%. The above question includes two sub-issues: (1) why is label propagation necessary? (2) why is the latent network impor-tant? We design four models to answer the above question: 1. Label propagation (abbreviated as LPP in Fig. 9) on 2. Label propagation on the latent network (the proposed 3. ICA on the original network; 4. ICA on the latent network.
 The experiment result on the citeseer dataset is presented in Fig. 9. There are two things that can be observed. First, tra-ditional label propagation LPP does not perform well on the original network. It has the worst result among the four ap-proaches. The reason is that with a limited number of train-ing examples, the original network is not effective enough to propagate the supervision knowledge. Another reason is that traditional label propagation or other graphical infer-ences models ( e.g. , [5, 11, 15]) do not take the attribute vector into consideration. They only depend on the given network to give prediction. Without the ability of using vector-based features, the performance of label propagation is even worse than that of ICA as in Fig. 9. However, when we apply label propagation on the latent network (LNP), it obtains the best accuracy among the four models. This shows the importance of the latent network to make good use of the vector-based feature (through Attribute similar-ity graph) and integrate different supervision knowledge to improve the accuracy. The second observation is that ICA does not perform well on the latent network. It has a similar result with that on the original network. This is because the latent network is generated from both the original network and the node attributes. But ICA already has the same in-formation, and thus the latent network essentially does not provide any additional supervision knowledge to ICA. In summary, first, label propagation does not perform well on the original network because it cannot propagate the label information effectively and cannot use the attribute vector; second, although traditional collective classifiers such as ICA consider both the network and the attributes, they are not very effective when there are a limited number of training data. As such, the proposed LNP model can be viewed as a bridge of collective classification (such as ICA) and tra-ditional graphical inference (such as label propagation) by introducing the latent network.
With the explosive development of social network, graph based approaches attract intensive attentions in recent years. Several approaches have been proposed to perform inference Figure 9: The importance of the latent network and label propagation on a given network ( e.g. , [24, 18, 19, 21, 10, 20]). For in-stance, [23] proposes to propagate the labels through the network, and assigns each node with the label that is most likely to be visited from the node. However, this category of algorithms only considers the graph structure or only con-sider the feature vectors, and it cannot handle the case di-rectly when there is feature vector in a graph. For example, in a citation network, each node is a paper and the links represent a citation relationship. In addition to the link-age information among the nodes, we also have the  X  X ag of words X  feature vector for each node. Traditional graph based models cannot directly handle this kind of network data with feature vectors.

Collective classification is then proposed to solve the prob-lem. Its key idea is to combine the supervision knowledge from the traditional tuple-based feature vectors, as well as the linkage information from the network. It has been ap-plied to various applications such as part-of-speech tagging [7], classification of hypertext documents using hyperlinks [15, 3], link prediction in friend-of-a-friend networks [17], pre-dicting disulphide bonds in protein molecules [16], etc. .
Iterative Classification Algorithm (ICA [12]) is among the first works proposed to tackle collective classification. The key step is to transform the network summary into feature vectors and treat the network as ordinary features. It is reported in [4] that ICA is a fairly accurate method with robust performance to different strategies of updating the labels. Moreover, Gibbs sampling [6] is further integrated into the ICA framework to enrich the statistical foundation of the algorithm. This is the major reason that we choose ICA and Gibbs sampling as the major collective classifiers to be compared with. In recent years, there are several works proposed to use a similar schema as ICA but with different basic classifiers or in different scenarios [6, 13, 15, 9]. In [9], it is reported that Logistic Regression works the best among these different interpretations. Hence, we set the basic classifier of all the comparison methods as Logistic Regression in the experiment.

It is important to note that most of the above works are mainly studied in the scenario where there are a lot of la-beled examples in the network. For example, the models in [15, 9, 12] are all studied and evaluated with k-fold cross-validation where over half of the data are training examples. Although it has been shown that traditional collective clas-sifiers work well in this situation, it is not sure how they behave when the number of training examples is limited. In this paper, we study their behaviors in this situation and analyze the reason why they do not perform well. Further-more, a general model is proposed to judiciously integrate different latent linkages to generate a latent network to make good use of the limited training examples.
In many social graph applications, training examples are time-consuming and expensive to obtain. Without sufficient labeled data, traditional collective classifiers and their semi-supervised modifications do not perform well. In this paper, we first explore various kinds of latent linkages from the existing dataset without using the information from other sources. Then, the latent linkages are judiciously integrated to change the network structure in order to allow for effective supervision propagation. The aim is to find a latent graph that can link the data together if they have the same class label and disjoin those with different class labels. The objec-tive is formulated as an optimization problem and is solved via quadratic programming. We perform experiments on three sets of graph datasets. The performances of the pro-posed model and those of the comparison methods are stud-ied with different sample rates. In most cases, the proposed model can outperform the comparison algorithms substan-tially. For example, when there are only 15% of training examples in the cora dataset, the accuracies of the compar-ison methods are below 60% whereas the proposed model achieves an accuracy above 70%.
This work is supported in part by NSF through grants IIS 0905215, DBI-0960443, OISE-0968341 and OIA-0963278. [1] T. F. Coleman and Y. Li. A reflective newton method [2] J. Demmel, I. Dumitriu, O. Holtz, and R. Kleinberg. [3] B. Gallagher and T. Eliassi-Rad. Leveraging network [4] L. Getoor. Advanced Methods for Knowledge [5] W. R. Grilks, S. Richardson, and D. J. Spiegelhalter. [6] D. Jensen, J. Neville, and B. Gallagher. Why [7] J.D.Lafferty,A.McCallum,andF.C.N.Pereira.
 [8] J. Leskovec, L. A. Adamic, and B. A. Huberman. The [9] Q. Lu and L. Getoor. Link-based classification. In [10] S. A. Macskassy and F. Provost. A simple relational [11] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [12] J. Neville and D. Jensen. Iterative classification in [13] J. Neville and D. Jensen. Collective classification with [14] P. Sen, G. M. Namata, M. Bilgic, L. Getoor, [15] B. Taskar, P. Abbeel, and D. Koller. Discriminative [16] B. Taskar, V. Chatalbashev, D. Koller, and [17] B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. [18] Y. Weiss. Advanced Mean Field Methods. MIT Press. [19] Jonathan S. Yedidia, William T. Freeman, and Yair [20] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [21] D. Zhou and B. Scholkopf. Learning from labeled and [ 2 2] Y. Zhou, H. Cheng, and J. X. Yu. Graph clustering [23] X. Zhu. Semi-supervised learning with graphs. [24] X. Zhu, Z. Ghahramani, and J. Lafferty.

