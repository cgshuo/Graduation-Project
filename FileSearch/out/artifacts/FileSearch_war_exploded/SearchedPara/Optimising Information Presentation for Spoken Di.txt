 Work on evaluating SDS suggests that the Infor-mation Presentation (IP) phase is the primary con-tributor to dialogue duration (Walker et al., 2001), and as such, is a central aspect of SDS design. During this phase the system returns a set of items ( X  X its X ) from a database, which match the user X  X  current search constraints. An inherent problem in this task is the trade-off between presenting  X  X nough X  information to the user (for example helping them to feel confident that they have a good overview of the search results) versus keep-ing the utterances short and understandable.
In the following we show that IP for SDS can be treated as a data-driven joint optimisation prob-lem, and that this outperforms a supervised model of human  X  X izard X  behaviour on a particular IP task (presenting sets of search results to a user).
A similar approach has been applied to the problem of Referring Expression Generation in di-alogue (Janarthanam and Lemon, 2010). 1.1 Previous work on Information Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structur-ing information (see examples in Table 1). For ex-ample, the SUMMARY strategy is used to guide the user X  X   X  X ocus of attention X . It draws the user X  X  at-tention to relevant attributes by grouping the cur-rent results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strat-egy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strat-egy, e.g. (Young et al., 2007). In a previous proof-of-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the partic-ular context in which information needs to be pre-sented to a user. Here, we will also explore pos-sible combinations of the strategies, for example et al., 2002), see Figure 1.

Prior work on Content or Attribute Selection has used a  X  X ummarize and Refine X  approach (Po-lifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utility-based attribute selection with respect to how each attribute (e.g. price or food type in restaurant search) of a set of items helps to narrow down the user X  X  goal to a single item. Related work ex-plores a user modelling approach, where attributes are ranked according to user preferences (Dem-berg and Moore, 2006; Winterboer et al., 2007). Our data collection (see Section 3) and training en-vironment incorporate these approaches.

The work in this paper is the first to ap-ply a data-driven method to this whole decision space (i.e. combinations of Information Presenta-tion strategies as well as attribute selection), and to show the utility of both lower-level features (e.g. from the NLG realiser) and higher-level features (e.g. from Dialogue Management) for this prob-lem. Previous work has only focused on individual aspects of the problem (e.g. how many attributes to generate, or when to use a SUMMARY ), using a pipeline model for SDS with DM features as input, and where NLG has no knowledge of lower level features (e.g. behaviour of the realiser). In Section 4.3 we show that lower level features significantly influence users X  ratings of IP strategies. In the fol-lowing we use a Reinforcement Learning (RL) as a statistical planning framework (Sutton and Barto, 1998) to explore the contextual features for mak-ing these decisions, and propose a new joint opti-misation method for IP strategies combining con-tent structuring and attribute selection. We follow the overall framework of NLG as plan-ning under uncertainty (Lemon, 2008; Rieser and Lemon, 2009; Lemon, 2010), where each NLG ac-tion is a sequential decision point, based on the current dialogue context and the expected long-term utility or  X  X eward X  of the action. Other re-cent approaches describe this task as planning, e.g. (Koller and Petrick, 2008), or as contextual de-cision making according to a cost function (van Deemter, 2009), but not as a statistical planning problem, where uncertainty in the stochastic envi-ronment is explicitly modelled. Below, we apply this framework to Information Presentation strate-gies in SDS using Reinforcement Learning, where the example task is to present a set of search results (e.g. restaurants) to users. In particular, we con-sider 7 possible policies for structuring the content (see Figure 1): Recommending one single item, comparing two items, summarising all of them, or ordered combinations of those actions, e.g. first summarise all the retrieved items and then recom-mend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating.
 Figure 1: Possible Information Presentation struc-tures (X=stop generation) In an initial Wizard-of-Oz (WoZ) study, we asked humans (our  X  X izards X ) to produce good IP ac-tions in different dialogue contexts, when interact-ing in spoken dialogues with other humans (the  X  X sers X ), who believed that they were talking to an automated SDS. The wizards were experienced re-searchers in SDS and were familiar with the search domain (restaurants in Edinburgh). They were in-structed to select IP structures and attributes for NLG so as to most efficiently allow users to find a restaurant matching their search constraints. They also received prior training on this task.
 The task for the wizards was to decide which IP structure to use next (see Section 3.2 for a list of IP strategies to choose from), which at-tributes to mention (e.g. cuisine, price range, lo-cation, food quality, and/or service quality), and whether to stop generating, given varying num-bers of database matches, varying prompt reali-sations, and varying user behaviour. Wizard ut-terances were synthesised using a state-of-the-art text-to-speech engine. The user speech input was delivered to the wizard using Voice Over IP. Figure 2 shows the web-based interface for the wizard. 3.1 Experimental Setup and Data collection We collected 213 dialogues with 18 subjects and 2 wizards (Liu et al., 2009). Each user performed a total of 12 tasks, where no task set was seen twice by any one wizard. The majority of users were from a range of backgrounds in a higher educa-tion institute, in the age range 20-30, native speak-ers of English, and none had prior experience of Spoken Dialogue Systems. After each task the user answered a questionnaire on a 6 point Lik-ert scale, regarding the perceived generation qual-ity in that task. The wizards X  IP strategies were highly ranked by the users on average (4.7), and users were able to select a restaurant in 98.6% of the cases. No significant difference between the wizards was observed.

The data contains 2236 utterances in total: 1465 wizard utterances and 771 user utterances. We au-tomatically extracted 81 features (e.g #sentences, #DBhits, #turns, #ellipsis) 1 from the XML logfiles after each dialogue. Please see (Rieser et al., 2009) for more details. 3.2 NLG Realiser In the Wizard-of-Oz environment we implemented a NLG realiser for the chosen IP structures and attribute choices, in order to realise the wizards X  choices in real time. This generator is based on data from the stochastic sentence planner SP a RK y (Stent et al., 2004). We replicated the variation ob-served in SP a RK y by analysing high-ranking ex-ample outputs (given the highest possible score by the SP a RK y judges) and implemented the vari-ance using dynamic sentence generation. The real-isations vary in sentence aggregation, aggregation operators (e.g.  X  X nd X , period, or ellipsis), contrasts (e.g.  X  X owever X ,  X  X n the other hand X ) and referring expressions (e.g.  X  X t X ,  X  X his restaurant X ) used. The length of an utterance also depends on the num-ber of attributes chosen, i.e. the more attributes the longer the utterance. All of these variations were logged.

In particular, we realised the following IP strate-gies (see examples in Table 1):  X  SUMMARY of all matching restaurants with  X  COMPARE the top 2 restaurants by Item (i.e.  X  RECOMMEND the top-ranking restaurant (ac-
Note that there was no discernible pattern in the data about the wizards X  decisions between the UM/no UM and the byItem/byAttribute ver-sions of the strategies. In this study we will therefore concentrate on the higher level decisions ( these different realisations as noise in the realiser. 3.3 Supervised Baseline strategy We analysed the WoZ data to explore the best-rated strategies (the top scoring 50%, n = 205 ) that were employed by humans for this task. Here we used a variety of Supervised Learning meth-ods to create a model of the highly rated wizard behaviour. Please see (Rieser et al., 2009) for fur-ther details. The best performing method was Rule Induction (JRip). 2 The model achieved an accu-racy of 43.19% which is significantly ( p &lt; . 001 ) better than the majority baseline of always choos-ing SUMMARY (34.65%). 3 The resulting rule set is shown in Figure 3. Figure 3: Rules learned by JRip for the wizard model ( X  dbHits  X = number of database matches,  X  prevNLG  X = previous NLG action)
The features selected by this model were only  X  X igh-level X  features, i.e. the input (previous ac-tion, number of database hits) that an IP module receives as input from a Dialogue Manager (DM). We further analysed the importance of different features using feature ranking and selection meth-ods (Rieser et al., 2009), finding that the human wizards in this specific setup did not pay signifi-cant attention to any lower level features, e.g. from surface realisation, although the generated output was displayed to them (see Figure 2).

Nevertheless, note that the supervised model achieves up to 87.6% of the possible reward on this task, as we show in Section 5.2, and so can be considered a serious baseline against which to measure performance. Below, we will show that Reinforcement Learning (RL) produces a signifi-cant improvement over the strategies present in the original data, especially in cases where RL has ac-cess to  X  X ower level X  features of the context. Here we  X  X ootstrap X  a simulated training environ-ment from the WoZ data, following (Rieser and Lemon, 2008). 4.1 User Simulations User Simulations are commonly used to train strategies for Dialogue Management, see for ex-ample (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. 4 However, this NLG predicted user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, the NLG user simulation tells us what the user is most likely to do next, if we were to stop generat-ing now .

We are most interested in the following user re-actions: 1. select : the user chooses one of the pre-2. addInfo : The user provides more at-3. requestMoreInfo : The user asks for 4. askRepeat : The user asks the system to 5. silence : The user does not say anything. 6. hangup : The user closes the interaction.
We build user simulations using n-gram mod-els of system ( s ) and user ( u ) acts, as first introduced by (Eckert et al., 1997). In or-der to account for data sparsity, we apply dif-ferent discounting ( X  X moothing X ) techniques in-cluding back-off , using the CMU Statistical Lan-guage Modelling toolkit (Clarkson and Rosen-feld, 1997). We construct a bi-gram model 5 for the users X  reactions to the system X  X  IP struc-ture decisions ( P ( a u,t | IP s,t ) ), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system X  X  com-bined IP structure and attribute selection deci-sions: P ( a u,t | IP s,t , attributes s,t ) .
We evaluate the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) diver-gence, as also used by, e.g. (Cuay  X  ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compare the raw probabilities as ob-served in the data with the probabilities generated by our n-gram models using different discounting techniques for each context, see table 2. All the models have a small divergence from the origi-nal data (especially the bi-gram model), suggest-ing that they are reasonable simulations for train-ing and testing NLG policies.

The absolute discounting method for the bi-gram model is most dissimilar to the data, as is the WittenBell method for the tri-gram model, i.e. the models using these discounting methods have the highest KL score. The best performing methods (i.e. most similar to the original data), are linear discounting for the bi-gram model and GoodTur-ing for the tri-gram. We use the most similar user models for system training, and the most dissimi-lar user models for testing NLG policies, in order to test whether the learned policies are robust and adaptive to unseen dialogue contexts. discounting method bi-gram US tri-gram US WittenBell 0.086 0.512
GoodTuring 0.086 0.163 absolute 0.091 0.246 linear 0.011 0.276 Table 2: Kullback-Leibler divergence for the dif-ferent User Simulations (US) 4.2 Database matches and  X  X ocus of An important task of Information Presentation is to support the user in choosing between all the available items (and ultimately in selecting the most suitable one) by structuring the current infor-mation returned from the database, as explained in Section 1.1. We therefore model the user X  X   X  X o-cus of attention X  as a feature in our learning ex-periments. This feature reflects how the differ-ent IP strategies structure information with dif-ferent numbers of attributes. We implement this shift of the user X  X  focus analogously to discover-ing the user X  X  goal in Dialogue Management: ev-ery time the predicted next user act is to add in-formation ( addInfo ), we infer that the user is therefore only interested in a subset of the previ-ously presented results and so the system will fo-cus on this new subset of database items in the rest of the generated utterance. For example, the user X  X  focus after the SUMMARY (with UM) in Table 1 is DBhits = 10 , since the user is only interested in cheap, Indian places. 4.3 Data-driven Reward function The reward/evaluation function is constructed from the WoZ data, using a stepwise linear regres-sion, following the PARADISE framework (Walker et al., 2000). This model selects the features which significantly influenced the users X  ratings for the NLG strategy in the WoZ questionnaire. We also assign a value to the user X  X  reactions ( valueU serReaction ), similar to optimising task success for DM (Young et al., 2007). This reflects the fact that good IP strategies should help the user to select an item ( valueU serReaction = +100 ) or provide more constraints addInfo ( valueU serReaction =  X  0), but the user should not do anything else ( valueU serReaction =  X  100 ). The regression in equation 1 ( R 2 = . 26 ) indicates that users X  ratings are influenced by higher level and lower level features: Users like to be focused on a small set of database hits (where # DBhits ranges over [ 1-100 ] ), which will enable them to choose an item ( valueU serReaction ), while keeping the IP utterances short (where # sentence is in the range [ 2-18 ] ): Note that the worst possible reward for an NLG move is therefore (  X  1 . 20  X  100)  X  ( . 121  X  100)  X  (18  X  1 . 43) =  X  157 . 84 . This is achieved by pre-senting 100 items to the user in 18 sentences 6 , in such a way that the user ends the conversation un-successfully. The top possible reward is achieved in the rare cases where the system can immedi-ately present 1 item to the user using just 2 sen-tences, and the user then selects that item, i.e. Re-ward =  X  (1 . 20  X  1)+( . 121  X  100)  X  (2  X  1 . 43) = 8 . 06 We now formulate the problem as a Markov De-cision Process (MDP), where states are NLG di-alogue contexts and actions are NLG decisions. Each state-action pair is associated with a transi-tion probability, which is the probability of mov-ing from state s at time t to state s 0 at time t +1 af-ter having performed action a when in state s . This transition probability is computed by the environ-ment model (i.e. the user simulation and realiser), and explicitly captures the uncertainty in the gen-eration environment. This is a major difference to other non-statistical planning approaches. Each transition is also associated with a reinforcement signal (or  X  X eward X ) r t +1 describing how good the result of action a was when performed in state s . The aim of the MDP is to maximise long-term ex-pected reward of its decisions, resulting in a policy which maps each possible state to an appropriate action in that state.

We treat IP as a hierarchical joint optimisation problem, where first one of the IP structures (1-3) is chosen and then the number of attributes is decided, as shown in Figure 4. At each genera-tion step, the MDP can choose 1-5 attributes (e.g. cuisine, price range, location, food quality, and/or service quality). Generation stops as soon as the user is predicted to select an item, i.e. the IP task is successful. (Note that the same constraint is op-erational for the WoZ baseline.) Figure 4: State-Action space for the RL-NLG problem
States are represented as sets of NLG dia-logue context features. The state space comprises  X  X ower-level X  features about the realiser behaviour (two discrete features representing the number of attributes and sentences generated so far) and three binary features representing the user X  X  predicted next action, as well as  X  X igh-level X  features pro-vided by the DM (e.g. current database hits in the user X  X  focus ( dbHitsFocus )). We trained the policy using the SHARSHA algorithm (Shapiro and Langley, 2002) with linear function approximation (Sutton and Barto, 1998), and the simulation envi-ronment described in Section 4. The policy was trained for 60,000 iterations. 5.1 Experimental Set-up We compare the learned strategies against the WoZ baseline as described in Section 3.3. For attribute selection we choose a majority baseline (randomly choosing between 3 or 4 attributes) since the at-tribute selection models learned by Supervised Learning on the WoZ data didn X  X  show significant improvements.

For training, we used the user simulation model most similar to the data, see Section 4.1. For testing, we test with the different user simulation model (the one which is most dissimilar to the data).

We first investigate how well IP structure (with-out attribute choice) can be learned in increas-ingly complex generation scenarios . A genera-tion scenario is a combination of a particular kind of NLG realiser (template vs. stochastic) along with different levels of variation introduced by cer-tain features of the dialogue context. In general, the stochastic realiser introduces more variation in lower level features than the template-based re-aliser. The Focus model introduces more varia-tion with respect to #DBhits and #attributes as de-scribed in Section 4.2. We therefore investigate the following cases: 1.1. IP structure choice, Template realiser: 1.2. IP structure choice, Stochastic realiser:
We then investigate different scenarios for jointly optimising IP structure (IPS) and attribute selection (Attr) decisions. 2.1. IPS+Attr choice, Template realiser: 2.2. IPS+Attr choice, Template realiser+Focus model: 2.3. IPS+Attr choice, Stochastic realiser: Tri-2.4. IPS+Attr choice, Stochastic realiser+Focus: 5.2 Results We compare the average final reward (see Equa-tion 1) gained by the baseline against the trained RL policies in the different scenarios for each 1000 test runs, using a paired samples t-test. The results are shown in Table 3. In 5 out of 6 scenar-ios the RL policy significantly ( p &lt; . 001 ) outper-forms the supervised baseline. We also report on the percentage of the top possible reward gained by the individual policies, and the raw percentage improvement of the RL policy. Note that the best possible (100%) reward can only be gained in rare cases (see Section 4.3).

The learned RL policies show that lower level features are important in gaining significant im-provements over the baseline. The more complex the scenario, the harder it is to gain higher rewards for the policies in general (as more variation is in-troduced), but the relative improvement in rewards also increases with complexity: the baseline does not adapt well to the variations in lower level fea-tures whereas RL learns to adapt to the more chal-lenging scenarios. 7
An overview of the range of different IP strate-gies learned for each setup can be found in Table 4. Note that these strategies are context-dependent: the learner chooses how to proceed dependent on better than the Baseline policy. the features in the state space at each generation step.
 Table 4: RL strategies learned for the different sce-narios, where ( n ) denotes the number of attributes generated.

For example, the RL policy for scenario 1 . 1 learned to start with a SUMMARY if the initial num-ber of items returned from the database is high ( &gt; 30). It will then stop generating if the user is predicted to select an item. Otherwise, it contin-ues with a RECOMMEND . If the number of database items is low, it will start with a COMPARE and then continue with a RECOMMEND , unless the user selects an item. Also see Table 4. Note that the WoZ strat-egy behaves as described in Figure 3.

In addition, the RL policy for scenario 1 . 2 learns to adapt to a more complex scenario: the number of attributes requested by the DM and produced by the stochastic sentence re-aliser. It learns to generate the whole sequence ( low ( &lt; 3), because the overall generated utterance (final #sentences) is still relatively short. Other-wise the policy is similar to the one for scenario 1 . 1 .

The RL policies for jointly optimising IP strat-egy and attribute selection learn to select the num-ber of attributes according to the generation sce-narios 2 . 1 -2 . 4 . For example, the RL policy learned for scenario 2 . 1 generates a RECOMMEND with 5 at-tributes if the database hits are low ( &lt; 13). Oth-erwise, it will start with a SUMMARY using 2 at-tributes. If the user is predicted to narrow down his focus after the SUMMARY , the policy continues with a COMPARE using 1 attribute only, otherwise it helps the user by presenting 4 attributes. It then continues with RECOMMEND (5), and stops as soon as the user is predicted to select one item.
The learned policy for scenario 2 . 1 generates 5.85 attributes per NLG turn on average (i.e. the cumulative number of attributes generated in the whole NLG sequence, where the same attribute may be repeated within the sequence). This strat-egy primarily adapts to the variations from the user simulation (tri-gram model). For scenario 2 . 2 the average number of attributes is higher (7.15) since the number of attributes helps to narrow down the user X  X  focus via the DBhits/attribute relationship specified in section 4.2. For scenario 2 . 3 fewer attributes are generated on average (3.14), since here the number of attributes influences the sen-tence realiser, i.e. fewer attributes results in fewer sentences, but does not impact the user X  X  focus. In scenario 2 . 4 all the conditions mentioned above influence the learned policy. The average number of attributes selected is still low (3.19).
In comparison, the average (cumulative) num-ber of attributes for the WoZ baseline is 7.10. The WoZ baseline generates all the possible IP struc-tures (with 3 or 4 attributes) but is restricted to use only  X  X igh-level X  features (see Figure 3). By beat-ing this baseline we show the importance of the  X  X ower-level X  features. Nevertheless, this wizard policy achieves up to 87.6% of the possible reward on this task, and so can be considered a serious baseline against which to measure performance.
The only case (scenario 2 . 2 ) where RL does not improve significantly over the baseline is where lower level features do not play an important role for learning good strategies: scenario 2.2 is only sensitive to higher level features (DBhits). We have presented a new data-driven method for Information Presentation (IP) in Spoken Dialogue Systems using a statistical optimisation frame-work for content structure planning and attribute selection. This work is the first to apply a data-driven optimisation method to the IP decision space, and to show the utility of both lower-level and higher-level features for this problem.
We collected data in a Wizard-of-Oz (WoZ) experiment and showed that human  X  X izards X  mostly pay attention to  X  X igh-level X  features from Dialogue Management. The WoZ data was used to build statistical models of user reactions to IP strategies, and a data-driven reward function for Reinforcement Learning (RL). We show that lower level features significantly influence users X  ratings of IP strategies. We compared a model of human behaviour (the  X  X uman wizard baseline X ) against policies optimised using Reinforcement Learning, in a variety of scenarios. Our optimised policies significantly outperform the IP structuring and attribute selection present in the WoZ data, es-pecially when performing in complex generation scenarios which require adaptation to, e.g. number of database results, utterance length, etc. While the human wizards were able to attain up to 87.6% of the possible reward on this task, the RL poli-cies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward.
We have also shown that adding predictive  X  X ower level X  features, e.g. from the NLG realiser and a user reaction model, is important for learn-ing optimal IP strategies according to user pref-erences. Future work could include the predicted TTS quality (Boidin et al., 2009) as a feature.
We are now working on testing the learned poli-cies with real users, outside of laboratory condi-tions, using a restaurant-guide SDS, deployed as a VOIP service. Previous work in SDS has shown that results for Dialogue Management obtained with simulated users are able to transfer to eval-uations with real users (Lemon et al., 2006).
This methodology provides new insights into the nature of the IP problem, which has previously been treated as a module following dialogue man-agement with no access to lower-level context fea-tures. The data-driven planning method applied here promises a significant upgrade in the perfor-mance of generation modules, and thereby of Spo-ken Dialogue Systems in general.
 The research leading to these results has received funding from the European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 216594 (CLASSiC project www.classic-project.org ) and from the EPSRC, project no. EP/G069840/1.

