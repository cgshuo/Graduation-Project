 University of Stuttgart University of Stuttgart model performance. It presents a new language model that combines clustering and half-clustering algorithm for class-based language models that compares favorably to the exchange algorithm. When interpolated with a Kneser-Ney model, half-context models are shown to have better perplexity than commonly used interpolated n-gram models and traditional class-based the model performs well and those which are better treated by existing non-class-based models. 1. Introduction
Stochastic language models are a crucial component of many speech and language technology applications. The key problem encountered by these models is that sparse data make the accurate estimation of the probability of novel and rare word sequences difficult.
 strategies. Of particular interest in this article are the following four: 1. Context length. Careful selection of the length of the history or context 2. Interpolation. Models typically interpolate several predictions, for 3. Classes. In a class-based model, prediction is (partially) based on classes 4. Similarity. Similarity models smooth predictions with predictions for The language models that are most commonly used today, in particular the modified
Kneser-Ney (KN) model (Chen and Goodman 1998), are based on the first two strate-gies, context length and interpolation X  X hat is, they interpolate distributions of different history lengths. We will call such models history-length interpolated models .Theset of contexts that history-length-interpolated models base their prediction on is limited to those whose history is identical for the history length considered. For example, the length-2 component of the model will compute the probability of P ( w contexts in the training corpus with identical history w 1 estimates rely on contexts in the training data that are similar to (or in the same class as) the new sequence whose probability is to be estimated. Thus, for example, in attempting to estimate a probability for the bigram black cloud , unseen in training, the transition probability associated with the class to which black belongs being followed by the class to which cloud belongs can be used. The intuition is that although black cloud might not have been seen in training, the class sequence containing related bigrams like gray cloud ,or black mist ,or gray mist , that is, combinations of other members of the two classes seen in training, can offer a reasonable estimate. In principle, this type of generalization is more powerful than history-length interpolation and has been, and continues to be, used to good effect in a variety of domains. However, the model must be a good model of the distribution of sequences of strings; if its assumptions are too unrealistic or approximate, then class-based generalization will perform worse than history-length interpolation.
 models in recent years, no such model has been widely adopted as superior to history-length-interpolated models. We believe one reason for this is that the granularity of context that is optimal for generalization has not been investigated sufficiently. Conse-quently, in this article, we present the following contributions: 844
These contributions, particularly our analyses, offer a richer understanding of the rel-ative characteristics of history-length interpolation and class-based generalization and should lead to more powerful language models that combine class-based and history-length generalization mechanisms.
 representation and puts forward the half-context hypothesis. Section 3 develops a half-context language model in the context of a specific subset of related prior work on language modeling. Additional related work is discussed in a subsequent subsection.
Parameter estimation is described in Section 4. A variety of models and interpolations are evaluated, and fine-grained results, significance tests, and context-specific analyses are discussed in Section 5. Conclusions and opportunities for future work are presented in Section 6. 2. Half Contexts
The representation employed in this article builds on a specification used in our earlier work (Sch  X  utze 1993, 1995; Sch  X  utze and Walsh 2008), motivated by Exemplar Theory (Hintzman 1986; Nosofsky 1986; Pierrehumbert 2001), where rich exemplar represen-tations facilitated the acquisition of local grammatical knowledge and outperformed a categorical representation in the same task. Specifically, each word was represented in terms of its immediate left and right neighborhood context. These neighborhoods were treated separately for two reasons: (1) separate treatment of left-neighbor infor-mation and right-neighbor information resulted in reduced complexity in the model and better generalization, and (2) right and left context behavior can differ consider-ably, for example, him and her would have very similar left contexts but could have garden ).
 known as half-words but can in fact be viewed as a word-level instantiation of a broader representational formalism which we term half-contextualization . Accord-ing to this schema a given unit (word, n -gram, class, etc.) is represented in terms of half-context (HC) distributions over its immediate left and right neighborhoods.
Hence, for example, at the bigram level, each bigram type is specified by two distri-butions, namely a left HC distribution P l and right HC distribution P the bigram X  X  behavior to the immediate left/right. For example, given walk home early twice, and drive home early once, then the left HC distribution of the bigram home other words. These HC distributions underpin the HC language models presented in Section 3.
 as possessing two separate directional behaviors, in the experiments that follow we compare our HC language model against a whole-context (WC) model where a given word X  X  WC distribution is a single distribution which combines the word X  X  left and right HC distributions. For a clear statement of the contrast HC vs. WC, we define inward and outward distributions. For the estimation of P ( w based on a training set S ,the inward distributions IW w of right contexts of w 1, n and left contexts of w n + 1 in S ;the outward distributions
OW w in S . language modeling than WC-based classes as follows.

An example for the intuition behind the HC hypothesis is the him/her example given earlier. When estimating P ( him | Mary helped ), a context like Mary helped her should also be considered as evidence because even though the right contexts of him and her in the corpus are dissimilar, their left contexts are similar. The HC hypothesis states that we should only worry about similarity of the  X  X elevant side X  of the n -grams involved, that is we should only consider inward distributional information. Most clustering algo-rithms used for class-based language models, notably the exchange algorithm (Brown et al. 1992; Kneser and Ney 1993; Martin, Liermann, and Ney 1998), are  X  X hole-context X  clustering algorithms that violate the hypothesis.
 models. In general, in designing a language model only information sources that are relevant for the task to be solved should be included. Adding additional complexity or nonrelevant additional features increases the variance of predictions without improving their accuracy. We can view this as a type of bias X  X ariance tradeoff. Half-context models are simpler and have less variance because they only use one half of the available context information, the half that is actually useful for prediction. The experimental results that we report later in this article confirm this by demonstrating that half-context models perform significantly better than whole-context models.
 context hypothesis is that we need two different types of classes: one set of classes for the predictors and another set of classes for the predictees. The reason is that we use two distinct and unrelated representations, left-context distributions to induce classes of pre-dictees and right-context distributions to induce classes of predictors. In other words, half-context models are inherently asymmetric, reflecting the fact that language models are inherently asymmetric: The role of the predictor and the predicted are different.
This asymmetry shows up in word-based models to a limited extent: In most models the unit of prediction is a word; predictors include n -grams of any size in principle, not just words. However, in a class-based model the asymmetry between predictor and predicted is more important: There is no justification for the premise (made, for example, in the Brown model) that the classes that are optimal for predictors are also the classes that are optimal for predictees. This observation has also been made by Gao et al. (2002), albeit without explicit reference to half contexts. We view our approach as better motivated since the asymmetry of our model is not posited, but follows from an analysis of the information sources needed for probabilistic inference in language modeling.
 there is a single formal concept that can be instantiated either by arguments of prepositions or by arguments of transitive verbs. For example, there are few if any syntactic differences between the arguments that can appear after a preposition like by and after a transitive verb like brought . Thus, the predicting histories by and brought should be treated alike in a class-based model. But that is not possible in a 846 whole-context model. We can interpret this as a syntactic justification for half-context models. Referring back to our earlier allusion to the bias X  X ariance tradeoff, if we had unlimited data, then estimating separate distributions for by and brought would be un-problematic; but because training sets are not unlimited, we can improve generalization by assigning the two linguistically identical contexts to the same right-context class. tering and storage requirements are cut in half by omitting those parts of the context that are nonrelevant. The time complexity of many clustering algorithms depends on the number of different types of features that occur in a particular cluster as opposed to the number of tokens. The number of feature types occurring in a cluster is reduced substantially in half-context models.
 once in the training set. Suppose for the sake of argument that is after nouns is more likely than is after adjectives (because phrases like yellow is the new black are infrequent). If strilp occurred in the context a very strilp car , then it is likely to be an adjective and noun (as in the bakery car or the wedding ring )and P ( is higher. In this case, it is the outward distribution of strilp that helps us to arrive at an accurate estimate. However, our hypothesis is not that there are no such cases; rather, we believe that as a generalization mechanism, only inward distributional information is useful in improving performance. This is borne out by the experiments reported herein.
 ences for language modeling. Parsing-based language models (e.g., Hall and Johnson 2003) are a first step in this direction. The hypothesis would probably not apply to such non-distributional models. 3. Half-Context Language Model 3.1 Half-Contextualization
Our starting point is the model by Brown et al. (1992). It models the probability of class c following class c 1 where c 2 emits ( e in the diagram) the member word w belongs to (  X  , a deterministic process) class c 1 :
This model has been frequently investigated and discussed. Recent examples include its successful application in word co-occurrence and sentence retrieval investigations (Momtazi and Klakow 2009; Momtazi, Khudanpur, and Klakow 2010), and polarity classification of movie reviews (Wiegand and Klakow 2008). language models can be half-contextualized by replacing classes that model right and left contexts simultaneously by right half-context classes c r
Words are assigned to HC classes and these HC classes then generate words: 3.2 Mixed n -gram Classes
A second modification of the Brown model we propose is motivated by the fact that trigram models perform better than bigram models because a sequence of two words significantly limits the possible ways of continuing. For this reason, we condition the sequential continuation on a mixed n -gram class of both bigrams and unigrams instead of on a class of unigrams alone. The resulting model, the HC model, is depicted in
Figure 1. We show the bigram w 1 w 2 as the member of the class c the class of w 2 if w 1 w 2 was not frequent enough to be included in the clustering (criteria for inclusion are discussed in Section 4).
 class c r 12 to which the bigram w 1 w 2 belongs generates a unigram left-context class c which generates w 3 . As will become apparent from the description of parameter esti-mation and the clustering algorithm in Section 4, the HC classes in the model are based 848 on inward (IW) distributions only. The corresponding outward (OW) distributions are not taken into account in accordance with the HC hypothesis.
 classes employed are mixed classes of unigrams and bigrams rather than of unigrams only. To our knowledge this represents the first such usage and can be motivated by the fact that frequent bigrams in language often behave similarly whereas the constituent unigrams do not. For example, the right HCs of the bigrams University of and based in are similar because both are often followed by locations; but the right HCs of of and in are much more diffuse and there are many prepositional objects that occur more often with of than with in (e.g., names of people) and others that occur more often with in than with of (e.g., response ). 3.3 Discounting
In initial experiments, we found that it was difficult to achieve an improvement using class-based generalization because for many contexts history-length interpolation is the better strategy for estimation. For a high-frequency event, it can be best to base estimates on instances of this event with identical history only X  X nstead of smoothing them with other contexts that are in the same class.
 our corpus of Wall Street Journal (WSJ) articles (consisting of 40 million words), it is followed by Kong . In this case, redistributing probability mass to other members of the class that Kong is a member of will decrease the estimate for P ( Kong that should be close to 3,998/4,045) and decrease the model X  X  performance. On the other hand, we have H ( P ( w | Mr. ))  X  11 . 9 in our WSJ training set. Any of a large number of first and last names can occur after Mr. and a language model should reallocate some probability mass from names that did occur in this environment in the training set to those names that did not.
 (Ney, Essen, and Kneser 1994). Following the notation of Chen and Goodman (1998), in the training set: where C ( w 1, n ) is the frequency of w 1, n in the training set.

The discount D is a parameter of the model that controls how much of each count is redistributed to the class-based model. In a way that is similar to other discounting methods, this formalization satisfies the two desiderata stated earlier: The estimates of high-frequency events are, in relative terms, much less affected than those of low-frequency events. below. We use an analogous model for WC distributions. In that case, P replaced by P WC in Equation (1).

HC classes instead of WC classes, (2) the use of mixed classes of unigrams and bigrams (instead of classes of unigrams), and (3) the use of absolute discounting to concentrate the effect of class-based generalization on rare hard-to-estimate events while leaving robust estimates based on frequent events largely unchanged. 3.4 Additional Related Work
In addition to the motivating articles discussed earlier, other relevant work includes the randomization techniques applied by Emami and Jelinek (2005) to class-based n -gram language models. Half-context clusters are not at odds with a randomized approach as they could easily be implemented in such a fashion.

Brants (2008), in which a word bigram (as opposed to a class of bigrams) proba-bilistically generates a class. They use, in our terminology, whole-context classes. The experiments reported subsequently suggest that HC classes are preferable to WC classes in the Brown-type set-up (classes generating classes); we plan to investigate whether this is also true in a mixed model in future work.
 operate on long-distance bigram probabilities (of varying distances) within a context and on interpolated long-distance bigram probabilities, both with a view to captur-ing long-distance dependencies. Evaluation of both clustering techniques X  X ierarchical clustering exploiting Mahalanobis distances to form compact clusters and Probabilistic
Latent Semantic Analysis X  X emonstrates that the use of long distance bigrams or their interpolated varieties yield more compact and meaningful (in the case of interpolated long distance bigrams) word clusters than the use of the traditional bigram (and bi-grams which employ trigger pairs over various histories). This research demonstrates an interesting avenue for contemporary models of word clustering and it would be no doubt interesting to see how such clustering strategies might contribute to half-context clustering, how their clusters would compare to those produced via bisecting k -means (though we cluster bigrams also), as proffered here, and indeed how long distance bigrams could be half-contextualized; these questions, however, are beyond the scope of the current article which seeks primarily to investigate the potential merits of half-contextualization.
 employ classes containing phrases. They describe their models as two-level because specific language models act within the classes. Their first approach takes into account the probabilities between words which constitute the different phrases of a given class, that is, phrases are sequences of unconnected words and words are considered the basic lexical unit, and their second approach considers phrases to be indivisible lexical units. The first model is also interpolated with a standard word-based language model, and the second model is interpolated with a standard phrase-based model. Word-error-rate analyses in an ASR system indicate that these models are better than their traditional counterparts. These results provide useful motivation for extending class-based language models from classes of isolated words to classes of longer sequences, such as the classes of bigrams employed in the half-context model. Their research 850 does not, however, consider the different directional behaviors of words or bigrams as we do.
 models (and a back-off variety [Zitouni 2007]) where each vocabulary item constitutes a leaf node in a word-tree, words are clustered into classes, and, in a recursive process, classes are clustered into more general classes until the root is reached. The tree root is a class containing all vocabulary items. In attempting to estimate the likelihood of an n -gram event they linearly interpolate over different language models, each one of which is trained on one level of the tree. In this way they seek to strike a balance between specificity and generalization. In constructing the class hierarchy, words are represented by their probability given their left and right neighboring words over a vocabulary (equivalent to the whole-context representation discussed in this article) and similarity between words is established using the Kullback-Leibler distortion measure.
Words occurring frequently in similar contexts should be clustered together with a view to finding a set of clusters that minimizes global discriminative information (see also
Bai et al. 1998). The clustering algorithm is based on k -means. The use of a hierarchical tree, and interpolating over it, represents an interesting approach not at odds with our research (i.e., that is, half-context classes could form nodes in the tree), although our approach differs in the separate treatment of word contexts, the use of bigrams as class members, and in the clustering methodology.
 class-based models using the k -means algorithm and words represented in terms of vectors where each vector element corresponds to the number of times the word had a particular part of speech tag given a tagged corpus. This approach would typically yield much shorter feature vectors than approaches (including our own) which have vectors matching the vocabulary size, thus leading to lower time com-plexity. They do not, however, avail of classes of bigrams as we do, nor look at di-rectional behavior of words (though half-contextualization using part of speech tags would be an interesting extension of both our model and theirs). Abdoos and Naeini (2008) use a clustering ensemble approach to categorize words, although it is unclear from their evaluation how such an approach compares, in terms of performance, to others in the literature. Gao et al. (2002) propose an asymmetric clustering model (ACM) grounded upon the apt observation that different clusters for predicted and conditional words should be employed, a view shared here. Their research does not present an explicit treatment of half-contextualization, however, nor considers half-contextualization and the significance of inward distributional information as insights which meet language modeling needs. Furthermore, our evaluation also differs in that it involves comparison against a whole-context model and a modified KN tri-gram model, rather than a simple word trigram model. Our use of a mixed n -gram class of both bigrams and unigrams also represents a marked difference between approaches.
 contexts similarly to our approach. However, they do not compare half-context with whole-context approaches and they pursue a less efficient similarity-based approach in contrast to the class-based approach proposed here.
 similar word to the observed word as the conditioning context used to generate the next word in the sequence. Again, no comparison to whole-context approaches is made. A similarity-based approach is also difficult to use for large corpora as it would necessitate the calculation of similarity of every word to every other word in the corpus. Similarities can be computed more efficiently for a subset of words on a smaller corpus, but then many of the rare events that class and similarity based methods are most beneficial for will not be covered. Our analyses in Section 5.2 and Section 5.3 demonstrate that half-context modeling is most beneficial for rare events. Similar concerns apply to other similarity-based models, such as those proposed by Bengio et al. (2003) and Schwenk and Koehn (2008). 4. Parameter Estimation In this section we describe how the parameters of the model in Figure 1 are estimated.
These parameters belong to two broad categories, namely, those which model the HC distributions ( P r and P l ) and are used in the construction of clusters, and those which capture emission probabilities P e and sequence probabilities P model is applied. Estimates were calculated on the basis of the training set part of a corpus of WSJ articles, 1987 X 1989, consisting of almost 50 million words, which will be described in more detail subsequently. 4.1 Clustering of HC Distributions
In the clustering, n -grams are represented as HC distributions. These distributions are estimated using maximum likelihood as follows: unigrams that occur more than 10 times in the corpus as well as the unknown word
UNK. For mixed clusterings of unigrams and bigrams we include all 378,109 unigrams and bigrams that occur more than 10 times and the unknown word UNK (thus, the unigram set is a subset of the mixed set). We call these sets S used for all HC and WC models herein, including the Brown model.
 distributions. The distance measure employed is Euclidean distance because the formal properties of k -means, including convergence, only apply to Euclidean spaces. Bisecting k -means is applied to a small random sample of the set of items: k -means first splits this random sample in two, then the largest existing cluster is split and so on until k = 512 (or k = 1,024, depending on the experiment) clusters have been found. The size of the random sample is then doubled, items in the enlarged sample are assigned to cluster 852 centroids, and centroids are recomputed. The size of the sample is doubled again and so on until all items have been assigned.
 reassignment and recomputation of centroids are performed (thus producing centroids that are good representatives of the overall distribution of items); and that at the same time the total number of assignments that needs to be computed is bounded by 2 M where M is the number of items. Computing the assignments is responsible for almost all the computation time of k -means and more than 90% of the time needed to estimate the parameters of the exemplar-theoretic model.
 class models in this article. As the default we chose k = 1,024, similar to Brown et al. X  X  experiments. Note that we have 512 left HC clusters and 512 right HC clusters, a total of 1,024 in the experiments with k = 512. We also experiment with 2 because one could also argue that this is the setting that is most comparable to Brown et al. We choose the powers of 2, k = 512 and k = 1,024 (instead of 500 and 1,000), for optimal compression and compact storage.
 and their sizes are given in Table 1. The three most frequent words in the left HC cluster have similar left contexts (dominated by forms of to be ) and different right contexts (large variety of part of speech forms). The three most frequent bigrams in the right HC cluster have similar right contexts (dominated by gerunds) and dissimilar left contexts (again a large variety of possibilities). In traditional whole-context clusters one would need to split the two clusters at least in two. For example, the right HC cluster in Table 1 would have to be split into one subcluster containing without-first / of-improperly and one subcluster containing pain-and . However, this presents two distinct disadvantages: (1) The extra clusters would require the estimation of more parameters, each based on fewer data points and hence less reliable, and (2) The left-context generalization, whereby of-improperly and pain-and have similar right contexts, would be lost. the relevant right HC cluster c r 12 and left HC cluster c
P ( w 3 | w 1 w 2 ) according to the model in Figure 1. We do this as follows: 1. If w 1 w 2  X  S mixed ,weusetherightHCclusterthat P r 2. Otherwise, if w 2  X  S mixed ,weusetherightHCclusterthat P 3. Otherwise, we use the right HC cluster that P r UNK was assigned to. 4. If w 3  X  S uni ,weusetheleftHCclusterthat P l w 5. Otherwise, we use the left HC cluster that P l UNK was assigned to.
 4.2 Emission and Sequence Probabilities
Emission probabilities need only be estimated for left HC clusters in the exemplar-theoretic model. They are estimated by maximum likelihood: Cluster sequence probabilities are additively smoothed: where  X  = 0 . 1, B  X  X  512, 1,024 } is the number of HC clusters, C ( c trigrams w 1 w 2 w 3 occurring in the training set, where w c ,and C ( c r ) is the number of bigrams w 1 w 2 occurring in the training set, where w was assigned to c r .
 two HC distributions, its left HC distribution and its right HC distribution. Clus-tering, membership assignment, and probability estimation are the same in all other respects. 5. Experiments and Analysis
A corpus of WSJ articles, 1987 X 1989, consisting of almost 50 million words, was ran-domly split into training set (80%), validation set (10%), and test set (10%). validation, and test sets. A modified KN model (Chen and Goodman 1998), termed
P (KN) , was estimated on the training set count files and applied to the test set using srilm, the SRI language modeling toolkit (Stolcke 2002). The same count files were the input to the HC and exemplar-theoretic model estimation and application procedure.
Vocabulary size was the same for both KN and exemplar-theoretic models: 256,874 (the 256,873 words occurring in the training set and the unknown word). A total of 70.8% of tokens w 3 in the test set occur in a context w 1 w 2 w 22.2% of tokens only w 2 w 3 occurs in the training set; and for 6.7% only w the training set. The out-of-vocabulary rate is 0.27%. All validation and test set words that do not occur in the training set are mapped to the special unknown token UNK.
In all interpolation experiments, the weight of the P (KN) of the model with which P (KN) is interpolated is  X  . The validation set was employed to determine the optimum interpolation weight  X  and discount D for each case. 15 in Table 4, subsequently) was less than 3 . 5 hours on an Opteron 8214 processor. other class-based models. Consequently, the SRI toolkit was also used to construct a class bigram language model, following the incremental version of the algorithm proposed by Brown et al. (1992), which we simply term the P 854  X  validation test 1,024 classes (the same number of classes as the combined left and right context clusters in the 2  X  512 HC model) were derived from the training data. when applied to the validation set over a number of interpolation weights, followed by results from the test data using the optimum weight for the P found during the validation phase.
 with a KN bigram model does offer some benefit, this benefit is slight (perplexity = 164.80 for P (KN) alone, versus 164.33 using the optimum interpolation weight on the test set). It is also clear that the traditional class-based model operating by itself (  X  =1.0, perplexity = 245.45) performs poorly relative to P (KN) .
 context distributions which consider behavior to the left and right separately.
Kneser-Ney ( P (KN) ); exemplar-theoretic half-context ( P context ( P (Whole) ); exemplar-theoretic Brown ( P (ET-Brown)
P (KN-Brown) , the interpolations of Kneser-Ney with exemplar-theoretic half-context, whole-context, and Brown, respectively. 2 Perplexity results, for each of these models, from the validation and test sets, are presented in Table 4. Order-2 in Table 4 implies that only unigrams are clustered in the exemplar-theoretic models and the Kneser-Ney model is a bigram model. As for order-3, this implies that both unigrams and bigrams are clustered together in the exemplar-theoretic models and that the Kneser-Ney model is a trigram model.
 tion set are given. For lines 2 X 6 and 13 X 14, the optimal value of D on the validation set for  X  = 1 (that is, 0 weight for the P (KN) model) was chosen.
 models are as valuable, or nearly so, as the KN models: The interpolation weight of half/whole-context models is either 0.4 or 0.5. In contrast, the Brown class model (line 7) receives a lower weight of 0.2, indicating that it is less valuable in the interpolation with KN.
 in the overall model. Again, the Brown model receives the smallest weight (line 7). For both D and  X  , the lowest half/whole-context model values are those for the KN order-3 interpolations on lines 15 X 16: D = . 5,  X  = . 4 (value of  X  tied with KN order-2 interpo-lation on line 9). This may be a reflection of the fact that class-based generalization is contributing more to better performance in order-2 models because order-2 models have a much lower baseline performance.
 exemplar-theoretic whole-context (lines 13 vs. 14), although that difference is reduced in the two interpolated models P (KN-Half) and P (KN-Whole) (lines 15 vs. 16). On this evidence it would appear that the combination of left and right context information into a single context distribution (i.e., a whole-context approach) is redundant, if not harmful. This 856 is evidence for the half-context hypothesis put forward at the beginning of the article:
Outward distributions, present in WC representations but absent in HC representations, do not seem to be helpful in class-based generalization, and are perhaps even harmful in order-3.
 that unigrams and bigrams are clustered together for the order-3 models. Although it makes sense to treat, say, the right contexts of from Mark and Martin as similar, the distributional patterns of the two n -grams are very different if the left context is also taken into account, which is the case for WC models.
 for class-based language modeling to half-context clustering. This is beyond the scope of this article, however. Instead, we compare the order-2 WC experiments directly with the Brown classes. We do this to make sure that our good results for HC models are not due to the fact that we use a weak WC baseline. As we will argue now, our WC baseline is at least as good or even better than Brown clustering.
 experiments reported here: either 512 left HC classes and 512 right HC classes (lines 2 X 4, 7 X 9, and 13 X 16); or 1,024 left HC classes and 1,024 right HC classes (lines 5 X 6 and 10 X 11). In the Brown experiments (lines 2 and 7), Equation (1) is used in the same way as in the HC/WC experiments except that class membership is based on the classes induced by srilm (corresponding to the experiments in Table 2). The comparisons on lines 2 vs. 4 and 6 and 7 vs. 9 and 11 clearly show that the quality of bisecting k -means whole-context clustering is comparable to, if not better than, Brown-type whole-context clustering. That is, keeping the representation constant in both cases (i.e., whole-context) enables us to see the algorithmic benefits of bisecting k -means as it appears to offer more useful clusters than those produced by the exchange algorithm.

P (KN) model (lines 1 vs. 3 X 6, 12 vs. 13 X 14), it is important to note that the combination of the P (KN) model and the exemplar-theoretic models outperforms the stand-alone P model (lines 1 vs. 8 X 11, 12 vs. 15 and 16). This is strong evidence that a combined class-based and history-length-interpolated model is superior to history-length interpolation by itself. 5.1 Establishing Significance
Although the perplexity results documented here provide tangible support in favor of the half-context hypothesis, it would nevertheless be desirable to establish if the perplexity scores are indicative of improvements that are statistically significant. To this end, the following significance test was performed. The test set has a length of 2,800,613 words. These 2,800,613 positions are divided into 47 bins, corresponding to the part-of-speech of the word at that position that is most frequent in the training set. based on a tagging of the training set with TreeTagger (Schmid 1994). One additional bin contains all positions with a number of rare tags (e.g., FW,  X  X oreign word X ) and unknown words. Two models are then compared by computing perplexity separately for each bin, counting the number of bins where the first model performs better than the second, and testing the significance of this count using the exact binomial test. This significance test is not very sensitive in some cases because the positive effect of class generalization can be concentrated on a few parts of speech. To the extent that half-context and whole-context classes approximate part-of-speech information, this makes it more difficult to show significance because a number of bins may not be affected by the model. However, as we will see subsequently the test is sufficiently sensitive for the key results of the article.
 significance tests. As all models of order-3 significantly (and unsurprisingly) outper-form those of order-2, both order types are considered separately in the discussion that follows. All significant results are with respect to p = 0.05.
 iments indicate no significant improvement between models, with the exception that all models ( P (KN) , P (Half) ,and P (Whole) 512 classes, and P are significantly better than P (ET-Brown) . Although such a result sheds no light on the veracity of the half-context hypothesis, it nevertheless demonstrates that our exemplar-theoretic models are competitive at order-2 and are superior to P ing the interpolated order-2 models (Table 4 models 7 X 11), a similar story presents itself, that is, there is no significant difference between the interpolated models with the exception that all interpolated models ( P (KN-Half) and P
P be noted, however, that the better performance of the five order-2 class-based models (lines 7 X 11), including P (KN-Brown) , compared to P (KN) in keeping with previous findings in the literature and demonstrates that class-based generalization can complement history-length modeling.
 demonstrate that half-contextualization yields statistically significant improvements over whole-context models. Specifically, P (Half) significantly outperforms P
P (KN-Half) significantly outperforms P (KN-Whole) . In addition, although P superior performance to P (Whole) and P (Half) , interpolation with either of our exemplar-theoretic models yields significantly better performance over P
P four exemplar-theoretic models outperform the Brown varieties and the models offer significant improvements versus P (KN) when interpolated at orders 2 and 3. Indeed,
P (KN-Half) significantly beats every other model. Crucially, however, in our view, are the results of order-3 which demonstrate the significant benefits of half-contextualization as these lend considerable corroborative weight to our half-context hypothesis. 5.2 Context-Specific Analysis
In order to better understand the relative strengths and weaknesses of the P
P (KN-Whole) ,and P (KN) models, Table 5 illustrates their performance in fine-grained (order-3 P (KN-Half) ), 16 (order-3 P (KN-Whole) ), and 12 (order-3 P w w 2 w 3 , in the validation set according to length | h | 858 and whole-context models (0, 1, or 2), frequency f 3 of w explain the statistics for the example of stratum 13. This stratum contains all positions w 3 in the validation set that satisfy the following three conditions: w half-and whole-context models use for class-based prediction ( in the training set is at least 10; and the trigram w 1 w training set. There are 259,856 validation set positions in this stratum, corresponding to 241,428 different trigram types w 1 w 2 w 3 . The log likelihood of this subset of the validation set for P (KN-Half) is  X  724,464. This log likelihood of than that of P (KN-Whole) and better by 109,446 that that of P that of P (KN-Whole) and better by 0.42 that that of P (KN) guish the models. These are the contexts that contain a history that is used by the class models (lines 4 X 9 and 12 X 17). The other five strata (1, 2, 3, 10, 11) are comparatively small and have a small impact on overall difference in log likelihood. The KN model interpolates predictions for histories of different lengths. In general, this will include the history that the class-based models use, but also include other lengths. For example, in cases where the class-based model is using a length-2 history, the KN model interpolates length-2, length-1, and length-0 histories.
 if a history of length 2 is available for prediction and the predictee w once in the identical context in the training set (lines 11, 13 X 17,  X  strata). Stratum 11 contains a small number of positions and consequently contributes little to  X  (KN) , but the per position difference is the largest of any stratum (0.48). In contrast, the per-position difference for stratum 17 is small, but the overall contribution length-2 history strata to  X  (KN) is 113,689 (the sum of rows 10 X 17 of the corresponding column) and is thus responsible for the majority of the perplexity improvement due to half-context modeling.
 large for half-context modeling and the overall impact is also large for stratum 12. In
P (KN-Half) uses only one context length for prediction, the longest that is available. So on line 12, P (KN-Half) underpredicts the next word w 3 using a bigram w not occur in that position in the training set ( f 1,3 = 0). P w for prediction and computes better estimates in cases where w training set. We are planning to address this problem in future work on the half-context model.
 use both the unigram w 2 and the bigram w 1 w 2 for prediction. Note that the values for f indicate that the trigram w 1 w 2 w 3 occurred at least twice in the training set. P uses the unigram w 2 because the bigram w 1 w 2 was not frequent enough to be included in the model as a bigram. The results on lines 6 X 9 suggest that further improvements of
P (KN-Half) are possible by interpolating predictions of histories of different lengths. the trigram w 1 w 2 w 3 has frequency 0 or 1 and therefore the length-2 component of P does not predict w 3 well. P (KN-Half) achieves good predictions because the single word w used for prediction occurred frequently ( f 3  X  10) and its class therefore is likely to reflect the distributional properties of w 2 well.
 class-based generalization for rare events. However, for a number of strata (6 X 9, 10, 12)
P (KN-Half) only uses one context for prediction, which in many cases is an inferior choice compared to the predicting contexts used by P (KN) . As a result, the averages in these strata are negative. We plan to address this problem in future work (see Section 6). ferences are quite small X  X ome positive, some negative X  X or length-1 histories (strata 2 X  9). Only for length-2 histories do we find larger differences: strata 12, 13, and 17 for total differences on the validation set and strata 10, 12, and 13 for average differences.
Length-2 differences are consistently positive for P (KN-Half) ences are small.
 by the fact that the difference between half-context and whole-context models increases asthesizeof n -grams being clustered grows. For unigrams, there is significant cor-relation between left and right half-contexts: If two words have the same type of right context, then they often also have the same type of left context. For bigrams, this correlation is smaller. As an illustration consider the bigram underwriter was . 860 It occurs four times in the validation set, followed by the words Dillon , Hambrecht ,
Merrill ,and Nesbitt , none of which occur in this context in the training set. For all four
P (KN-Half) has a large advantage compared to P (KN-Whole) . The reason is that underwriter was is in the same right half-context bigram class as many bigrams that are followed by Dillon , Hambrecht , Merrill ,or Nesbitt in the training set. Examples of such bigrams similar right contexts, but dissimilar left contexts. As a result, the whole-context model groups underwriter was with other bigrams that do not support good generalization.
The pattern of consistent improvements of P (KN-Half) compared to P (lines 11 X 17) indicates that half-context clustering is able to capture useful generaliza-tion for language modeling that whole-context clustering cannot capture.
 unigram persuades is followed by them in the validation set, again a context unseen in the training set. The half-context model groups persuades with n -grams like They told , and prevented ,and both of  X  X issimilar on the left, but similar on the right X  X hat support a high estimate for P (KN-Half) (them | persuade). The class of persuade of the whole-context model is more diffuse, generally containing n -grams that are followed by a noun phrase, model leads to better generalization than class memberships in the half-context model because the whole-context model can exploit the correlation of left and right contexts.
For example, the whole-context model assigns the unigram 367,000 to a class that con-sists almost exclusively of numbers whereas the half-context model assigns it to a class that is more mixed. Because 367,000 occurs in only 11 distinct contexts in the training set, its syntactic behavior can be better characterized if both left and right contexts are exploited.
 for length-1 histories although there are large differences between the two models for individual 1-word histories. For length-2 histories, the half-context model is superior due to its ability to group histories according to the relevant half-context only X  X he right half-context X  X n accordance with the half-context hypothesis. 5.3 Objective Function of n -gram Clustering
As we argued in Section 3.3 when introducing the exemplar-theoretic model, class-based generalization is most useful for unseen and for infrequent events. This basic insight motivates two differences between our class-based model and previous work. class-based generalization is given: Weights for unseen and infrequent events are higher than weights for frequent events. As a result, the model X  X  estimates are close to maxi-mum likelihood estimates for frequent events because the maximum likelihood estima-tor is appropriate in these cases. In contrast, the model X  X  estimate of the probability of a word occurring in an unattested context is closer to the estimate of the class-based model.
 objective function of clustering. Most previous work has employed objective functions that optimize a quantity on the entire training set. For example, Brown et al. (1992) maximize mutual information and Gao et al. (2002) minimize perplexity on the entire training set. In contrast, the objective function of our clustering is similarity of half-contexts to cluster centroids or, more precisely, minimizing the residual sum of squares of differences between half-context vectors and cluster centroids. This criterion is much less sensitive to frequency than previously used criteria. In the extreme case, it may be optimal on the global criteria to put a very frequent idiosyncratic word in its own class.
This is so because even slightly better improvements of the class model for a frequent word will affect many positions in the training set and have a large cumulative effect. ized. In principle, the gain from finding an appropriate cluster for a rare word is as large as the gain from finding an appropriate cluster for a frequent word. In particular, frequent idiosyncratic words have no advantage compared to infrequent idiosyncratic words and very frequent idiosyncratic words are less likely to be assigned to singleton clusters or small clusters dominated by them. This clustering set-up may not be optimal for achieving good results with a cluster-only language model, that is, a model that does not contain a  X  X exical X  component similar to the maximum likelihood estimates in our exemplar-theoretic model. But if we acknowledge that class-based generalization is not useful or is even harmful for frequent events, then this should not be our goal. that both the design of the discounting mechanism and the k -means objective function target a different subspace of the space of all events: those that are unseen or infrequent. 5.4 Efficient Clustering
The focus of this article is the comparison of HC and WC classes and our investigations into the context-specific characteristics of history-length interpolation and class-based generalization. However, we also want to point out that the clustering algorithm we are using is very efficient, thus removing a potential obstacle to the widespread use of class-based language models. In total, the clustering algorithm requires fewer than two assignments per item on average (see Section 4.1). A single assignment requires computing the distance between an HC distribution and each of k centroids. The time necessary for computing one distance is a function of the number of nonzero entries in the distribution. 4 The total number of nonzero entries for any given bigram is the number of distinct trigrams in which it occurs. Thus, the total number of operations for performing all assignments necessary for the clustering of the bigrams is less than b times the number of distinct trigrams in the corpus where b is a small constant. This number scales linearly with the number of distinct trigrams, which in turn scales sub-linearly with the length of the training corpus. Thus, although the estimation procedure is expensive compared to standard trigram models like KN, it has desirable properties compared to other clustering algorithms, in particular the exchange algorithm. Even though there exist fast implementations for the exchange algorithm (Martin, Liermann, and Ney 1998; Uszkoreit and Brants 2008), it has worse than linear complexity. 6. Conclusions and Future Work
In this article we introduced a new representational formalism for language modeling known as half-contextualization . Half-contextualization employs only inward contextual 862 information in estimation and prediction X  X here we defined the inward distributions as the conditioning context X  X  right-context distribution and the predicted word X  X  left-context distribution. Our hypothesis was that only inward context is helpful for accurate prediction.
 is correct and that the use of outward directed information is not only redundant but also, in the case of order-3, damaging. We believe this is a particularly noteworthy discovery as it is essentially tantamount to requiring only half of the available dis-tributional information in order to achieve an equivalent, and often better, result.
Furthermore, from the outset we argued that the lack of adoption of class-and similarity-based approaches was, in part, because the granularity of contexts best suited for generalization and history-length interpolation have yet to be established; the novel context-specific analysis we presented here, which goes beyond traditional perplex-ity comparisons between models, illustrates the specific context scenarios where half-contextualization is particularly beneficial.
 words as atomic units that are best characterized by taking into account all information available about them in the training set, including what we call outward context. The model we have proposed uses different parts of the available contextual information for different inference tasks. While it may seem surprising that contextual information can be redundant or harmful for class-based generalization, we have argued that direction-ally nonrelevant information for a particular inference task can be noisy and misleading. class-based language models. First, we defined classes as mixed classes of bigrams and unigrams and argued that this flexible granularity gives rise to better classes. Second, we successfully employed a discounting method which focuses the impact of general-ization onto rare events while leaving frequent events to better-suited history-length in-terpolation. This addresses the problem that class-based generalization is often harmful for high frequency events that are best estimated by maximum likelihood on identical contexts. Third, we presented a new clustering algorithm for class-based language mod-els that has linear time complexity and is more efficient than the exchange algorithm. ous avenue, given our analyses, is to incorporate the ability to interpolate distributions of different-length conditioning contexts into the model. By incorporating such an inter-polation mechanism, we anticipate an amelioration in performance further supporting the use of half-context in language models. However, crucially, the primary endeavor in this article is not simply to promote the merits of half-contextualization, nor to establish how to build a better exemplar-theoretic model, but rather to develop and promote a deeper understanding of the relationship between history-length interpolation, class-based generalization, and context, in order to construct and combine language models, of varying varieties, in a more targeted fashion.
 Acknowledgments 864
