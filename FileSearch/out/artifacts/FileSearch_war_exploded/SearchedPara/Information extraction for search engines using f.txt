 1. Introduction
The extraction of relevant data from a target source is called Information Extraction. The target source can be a natural language source or structured records (data records) which usually contain important information. Therefore, there is a need to develop wrappers to extract these structured records. Wrappers developed recently are mostly fully automated and they could have significant speed advantages when processing large volumes of web site data, therefore they could be helpful in meta search engine development [56,34] and in comparing and evaluating shopping lists [52].

A wrapper needs to go through a few stages to complete its operation. These include collecting and labeling training pages, creating and generalizing extraction rules from the set of labeled pages, extracting the relevant data and using them as output in a suitable format for further processing [10]. Research works on wrapper design focus mostly on the interme-diate operation, that is the extraction of relevant data from a set of sample pages, although some of the works provide a solu-tion for collecting training data such as a crawler and others provide output in XML format or relational database format for further integration of data. Generally, the labeling phase specifies the output of the extraction task and this requires the involvement of a user [2,9,13] . However, some contemporary systems do not require such labeling, instead the labeling and annotation of data are usually carried out after the generation of extraction rules [3,11,7,33,52] . Data extracted can be aligned and used for further processing [33,52] . Data alignment is optional as they may not be needed by a user in some cases. Once data records are properly aligned and tabulated, they are labeled so that they can be easily distinguished [30,50] .
In general, wrappers can be classified in a number of ways. The simplest way is to classify wrappers based on their data extraction ability [26]. Some wrappers are designed to extract data records at record level, which only involves the extraction of relevant data records without any data alignment [6,7,25,27,37] , while other wrappers are developed to extract data re-cords at data unit level which is also known as data alignment. These wrappers split the extracted data records into smaller attributes called data items and rearrange them in a tabular form [30,33,52] . Data items have several atomic entities which can be separated further into smaller components, known as data units [26]. For example, a data record  X  X  book  X  may contain smaller attributes such as author, title, price and ISBN.

The other way to distinguish a wrapper is to look at the principles of operation of the wrappers to determine whether they are manual, semisupervised, supervised or automated wrappers. Manual wrappers are the earliest wrappers developed and these wrappers need knowledgeable and trained users familiar with the underlying structure of the web pages in their operation [31,36,5,45,24] . By observing the HTML page, the users will find the particular patterns of data records and they will be able to hand code the wrapper based on these patterns. However, this method soon became impractical, virtually every type of pages needs its own wrapper, and the wrappers need maintenance and update should their target web pages change their layout. Thus, this wrapper is not easily scalable for large web sites.

Supervised wrapper requires the user to label the HTML pages and the wrapper will automatically extract the information based on the labeled instances [2,9,13,18,29,39,41] . However, the involvement of a user is still needed for the operation of the wrapper and the labeling of web page is time consuming.

Similar to supervised wrapper, semisupervised wrapper requires the user to label the HTML pages [3,11,12,54]. However, once labeling is carried out, the wrapper will automatically predict the set of extraction rules for extracting data from other similar HTML pages. The semisupervised wrapper still requires human intervention, thus, it is labor intensive and not suit-able for large-scale web comparisons.

To overcome this drawback, fully automatic wrappers were developed [46,7,8,37,48,33,30,52,55,57,25 X 27,38] . The advantage of automatic wrappers is that they are able to extract relevant data from different web pages, provided that the data records in the sample pages are similar in structure. Automatic wrappers work by checking the pattern and struc-ture of data records, typical examples are the detection of repetitive sequence of HTML Tags (Mining Data Region (MDR) wrapper [7]), the space occupied by data records (Visual Segmentation based Data Records (VSDR) wrapper [37]), the use of semantic properties in data records (Ontology Assisted Data Extraction (ODE) [50] and the works of [17,40] ), the grouping of data records into clusters [22,38] , the extraction of data tables [14] and news articles [28,32] . Automatic wrappers are able to cover a larger domain but they are not as accurate as their manual counterparts as they are designed based on a number of assumptions. As early automatic wrappers use HTML Tags to determine the structure of data records, the accuracy and pre-cision of these methods are affected by ambiguities in the HTML language, furthermore if the information in the web page is not uniformly presented [6,7,30,46] . To overcome these limitations, recent wrappers use additional visual cues, such as con-itively, the argument of using visual cues seems appealing and one that seems adequately supported by comparative results between existing state of the art non-visual wrappers [7,55,57,30] . Using additional visual cues will result in increase in complexity and decrease in the speed of the wrapper. For more information on wrappers, the readers are encouraged to refer to the surveys by Laender et al. [1] and Chang et al. [10].

In this paper, we focus on developing an automated non-visual wrapper for the extraction of data records at record and data unit levels, particularly the search engine results pages. Our aim is to improve on current non-visual based wrapper performance and demonstrate that our wrapper, Wrapper Incorporating Set of Heuristic Techniques (WISH) performs equally as well, and in many cases, better than the current state of the art automatic visual wrappers. Our results question how effective visual information is currently being used and that there is at least a need to rethink the way in which visual information is currently used.

This paper is divided into several sections. Section 2 reviews the existing problems in current state of the art wrappers and our proposed solutions to these problems. Section 3 describes the work relevant to our research. In Section 4 we discuss our proposed methodology in detail. Section 5 discusses the result of our experimental tests while Section 6 summarizes our work. It is worth noting that data labeling is outside the scope of this paper. 2. Problem formulation and proposed solutions
A review of previous work shows that the problems generally related to wrapper design are: 1. Current non-visual wrappers rely on HTML Document Object Model (DOM) Tree are unable to locate and extract relevant data region (groups of data records [52]) from search engine results pages [6,7,30] . A DOM Tree is the underlying code of a
HTML page that has a  X  X  tree  X  like structure rendered by an internet browser. Visual based wrappers [25 X 27,33,37] are developed to overcome this problem using additional visual cues from HTML page. However, these visual steps incur an additional computational overhead, and make the running process slow because they require rendering information from the web browser and additional parsing to extract the visual information [56]. 2. Non-visual wrappers use tree matching algorithm to check the similarity of data records by comparing the position and identity of each node in the trees (data structure to represent the data records X  structure in a tree form) to remove irrelevant data records with dissimilar structure [8,52,53] . However, the implementation and coding of the algorithm are complicated [52,53] . This algorithm also runs in a time complexity of O( n tree and n 2 is the number of nodes in the second tree. In general, most web pages consist of complex trees with a large number of nodes. Therefore, these complexities slow down the current tree matching algorithm. 3. Iterative data items are data items which are similar and occur repetitively and disjunctive data items are data items which may exist in some data records but not in all the data records. For example, in a web site showing a pet shop with have similar format and structure. This pet shop may also sell other pets as part of their items, for example cats. There-fore, data item  X  X  persian cat  X  is a disjunctive data item as this pet shop may not have this data item as part of their contents shown in the web site. Current state of the art wrappers are unable to align iterative data items [33,52] because they treat the detected data items as separate entities without further checking whether these data items are having similar parent  X  X  siberian husky  X  should be aligned under three columns with similar column name under  X  X  pet  X  but current wrappers will align them under three separate columns with different names. For disjunctive data items, current wrappers are unable to align them correctly because they are optional items in some data records and thus, the position to which they are to be inserted into the template cannot be determined. 4. Current wrappers are having difficulties in solving the HTML Tag (web page syntax) and Text (webpage content) mis-matches problem efficiently as they make invalid assumptions on the formation of data units from the HTML Tag and Text nodes [30,33,52] . Tag and text mismatches problem involves the identification of atomic elements (data units) to cor-rectly segment the data. Previous work on wrapper design assumes that the entire text node is the smallest entity and this entity is a non-divisible text. Therefore, for each of the text nodes encountered in a data record, it will be aligned in a particular column of a table according to its correct position. However, a text node may be stored under a parent
HTML Tag and may consist of several data units which themselves can be smaller sub texts. In actual case, these smaller sub texts can be treated as separate individual text units. Furthermore, if a text is separated by a decorative tag (a HTML formatting code), for example HTML Tags representing bold , italic text, the text will be separated into smaller units and represented separately by several texts. We use Fig. 1 to illustrate the problem of identifying the separate entities of a text node. There are two data records in Fig. 1 and each of these has two text nodes. In each data record, one of the text node matic wrappers (excluding Search Result Records (SRR)) will not be able to identify  X  X  Price  X  and  X  X  ISBN  X  as two data units considered as the smallest entity. Therefore, they are unable to segment these data units correctly. In Fig. 2 , we demon-strate the problem of identifying text separated by a decorative tag. In this example, there are also two data records. In the
HTML page displayed, each data record has three data units. However, when shown in a DOM Tree form, each data record contains six data units due to the presence of decorative tags. For instance,  X  X  Title: World Wide Web X  will appear as two texts as they are separated by the bold tag &lt; B &gt;. These data records cannot be separated correctly using current wrappers.
In the tables at the bottom of Figs. 1 and 2 , we also illustrate, in tabular form, the traditional way of segmentation of data units and our proposed method of identifying the data units. In these tables, the first row represents the label assigned to each of the entity, while the remaining rows are the contents of the data units. Once the data units are clearly identified, they can be aligned properly.

The techniques we propose to improve the speed and accuracy of our wrapper in extracting and aligning data records are discussed below: 1. Filtering Techniques
We incorporate a series of data filters to remove irrelevant data records from the HTML page. These filters are designed based on heuristic techniques, each of them works based on the observations made by authors of [7,33,52] . The idea is to reduce the  X  X  X oise  X  or irrelevant data records in each filtering stage so that the wrapper can be more efficient in extracting the correct data region containing data records. 2. Similarity check algorithm
We propose a Dummy Tree Matching algorithm based on the frequency measures of a tree structure as part of the filter-ing stages to check the similarity of data records. This algorithm does not actually match two tree structures and find their similarity by checking the identity of each node, but uses the number of nodes in a tree to determine the similarity of two trees. As our method does not require the comparison of all the nodes in a tree structure, it will reduce significantly the computational overhead. Our algorithm works in a time complexity of O( n )( n is the number of nodes in the tree), and is faster than the current tree matching algorithms. This increase in speed is useful when our wrapper is used in large-scale web comparisons. 3. Scoring function
We use the number of occurrence of text and image to develop a scoring function for detecting relevant data region. Cur-rent visual based wrappers such as Visual Segmentation based Data Records (VSDR) [37], Visual information aNd Tags (ViNT) [27], and Visual Perception based Extraction of Records (ViPER) [33] use visual boundary to locate and extract the relevant data region. Our study shows that it is possible to achieve the same objective using an alternative way which is more accurate and yet simple and fast in operation. We use Fig. 3 to demonstrate the correct way of measuring the size of data records using the bounding box method of VSDR. In Fig. 3 b, there are two data records with a huge portion of unoccupied space. To measure the size of the bounding box of individual HTML Text, we consider appropriate to take into account the bounding box of individual data records as shown in Fig. 3 a. For this case, unoccupied space will not be taken into account in calculating the size of the data records. Fig. 4 illustrates a case where the visual calculations of VSDR cause it to incorrectly identify data records by over estimating the valid data record area. VSDR will treat the size of the right data region (menu bars surrounded by a rectangle) in Fig. 4 as slightly larger than that of the data region in the center (search results surrounded by a rectangle). In actual case, the data region containing search results is larger because half of the data region in the menu bars is occupied by empty space (rectangle with round corners). Therefore, the technique used to measure the size of data records proposed for Fig. 3 a is preferred. To detect the data record size accurately with-out reducing the accuracy of data extraction, the alternative way is to take into account the number of occurrence of text and image. This information is readily available in HTML text and &lt;IMG&gt; tag in a given HTML page. As data region con-taining search results has more text and image than data region containing menu bars, data region containing search results is the correct data region. Distinguishing correct data region in this way has more advantages than boundary detection as the space occupied by data records is measured more efficiently. 4. Data alignment with template provided
In this study, the template detection technique is used to accurately align data items and for solving the problems encountered in aligning disjunctive and iterative data. A web template is a server script used to generate fragments of
HTML code to be embedded in a HTML page which can be displayed for human consumption [58]. This code fragment is a tree that is used to represent the structure of a group of data records. It is usually defined and implemented by the programmer of the database server. When a user enters a query, the database server will process the query and pre-pare the necessary information to be sent to the user. This information is encoded and returned to the user based on a template predefined in the server. The wrapper can be used to extract data records from HTML pages, decode and return them to the original template provided by the database server. Our wrapper aligns the extracted data records, with a new format to identify the various data items presented in a data record. Template of data records is used as it could represent all the data records in a single form, assuming that data records contain nearly similar format and structure. Therefore, it will have the advantage of a general view of the data items X  position stored in the data records. The tree structure of data records is fully utilized for aligning data items. Making use of tree structure has the advantage of cutting down the time complexity of the algorithm as trees can be pruned and traversed. We also strictly adhere to the atomic properties of a text node, therefore for every text node encountered in a HTML DOM Tree, we align it as one entity instead of having more than one text node as an entity. Our algorithm is able to make a wiser and more flexible decision when handling disjunc-tive and iterative data by taking into consideration the adjacent data items. 5. Solving the imperfect segmentation problem
To solve the imperfect segmentation problem, we propose a data merging algorithm that can detect decorative tags (HTML Tags that contains tags such as &lt;b&gt;, &lt;i&gt; tags) and merge them accordingly. We use a clustering technique to par-tition and separate a text into several data units. Our approach is different from previous work on merging and partition-ing data units as the previous methods use a simple rule that measures only the token X  X  frequency and its absolute position [26]. A token is a word stored in a text node of a DOM Tree separated by spaces.We also propose a method to detect high frequency tokens regardless of their positions in the data records. We achieve this by partitioning data records into data items in the first phase of our data alignment process. We then further partition these data items into smaller data units. Our algorithm measures the relative position of similar tokens instead of their actual position. Thus, this allows us to determine the correct sequence of the data units. In addition, we treat a number of tokens in a text as one group irrespective of their frequency of occurrence in that particular text. If these tokens occur with the same number of times as the number of occurrence of data records, data partitioning will be carried out.
 gles) occurs twice in the data records and the relative position (calculated based on absolute positions of  X  X  Title  X  token, the first  X  X  Title  X  token in Data Record 1 will have relative position of 1, the second  X  X  Title  X  token in Data Record 2 will have relative position of 1, and so on) of the two  X  X  Title  X  tokens is the same for the two texts, therefore they are chosen as and second data records would have absolute positions of 4 and 5. Due to separation of one position, previous wrappers would not partition the text using  X  X  Price  X  token as a cut-off point. 3. Related work
In this section, we describe the current work related to ours, which is data extraction at record and data unit levels. 3.1. Data extraction at record level
The key component of a wrapper is the algorithm that checks the similarity of data records. Data records are retained and considered valid if they are similar and discarded if they are dissimilar. Current wrappers such as Data Extraction based on
Partial Tree Alignment (DEPTA) [52] and Mining Data Region (MDR) [7] use edit distance techniques to check the similarity of the structure of data records. Common edit distance techniques in such area are string edit distance and tree edit distance [42,15] . For more information on edit distance techniques the readers are encouraged to refer to the surveys of Baeza-Yates [42], Gusfield [15] and Navarro [23].

The string edit distance algorithm involves matching two strings and the determination of how the first string is to be transformed into the second string. String edit distance algorithms are generally fast in operation and run in a time complex-ity of O( m )( m is the number of tags in a data record). However, these algorithms are unable to compare two trees having nearly similar tree structures, with iterative and disjunctive data. This is because this algorithm matches flat level data, which occur in single level (strings) rather than tree structures. String edit distance algorithms are also unable to distinguish
HTML Tag as a single entity (they tend to compare strings by examining the characters in these strings), therefore this may result in inaccurate matching. For example, when two HTML Tags &lt;P&gt; and &lt;NOBR&gt; are matched, we assume that this mismatch is counted as one (one mismatch of two HTML Tags), but string edit distance algorithms consider this mismatch as 4 (4 characters in the second string do not match with the 1 character in the first string). There are several variants of string edit distance algorithms, some common ones are Levenshtein distance [47], Hamming distance [16], Episode distance [21], and Longest common subsequence distance [43,4] .
 The tree edit distance algorithm uses two tree structures and matches them by comparing the node identity and position.
Tree matching algorithms developed are the tree edit distance [35], alignment distance [44], isolated-subtree distance [19], top down distance [35,51] , and bottom up distance [20]. Tree edit distance algorithm is quite similar to string edit distance, except that it includes tree nodes matching. Tree edit distance algorithm for unordered tree is NP Complete . The top down algorithm was proposed in [51]. For this algorithm, two trees are matched in O( n the first tree and n 2 is the number of nodes in the second tree). The bottom up approach was introduced by [20] and the time complexity for it is O( n 1 + n 2 )( n 1 is the number of nodes in the first tree and n tree). The top down and bottom up approaches are restricted versions of tree matching algorithm.

DEPTA [52] uses a bottom up tree matching algorithm to match tree structures of data records. A tree matching algorithm matches two tree structures and determines how the first tree can be transformed into the second tree. DEPTA X  X  tree match-ing algorithm determines the maximum matches between two trees by comparing the location and identity of the nodes in the tree structures. Although this algorithm solves the problem emerged in data matching successfully, the algorithm re-quires a complex data structure for its implementation. Therefore, an algorithm that could simplify the implementation pro-cess will be helpful.

DEPTA checks the similarity of two trees using the percentage similarity of the trees. In this context, DEPTA may not be able to match two trees having particular elements (HTML Tags) which occur iteratively in the two trees. This is due to the fact that the number of occurrence of the particular element (HTML Tags) in a tree might not be the same as it occurs in the other tree. Fig. 6 shows such a case. The upper left and right trees are of different sizes, DEPTA will assume that the upper left tree is 3/((5 + 3)/2) = 3/4 = 75% similar to that of the upper right tree. Basically, the two trees have a similar template (the bottom tree of Fig. 6 ), DEPTA treats the two trees as dissimilar because they have different numbers of iterative data. A rea-sonable way to check the similarity of two trees is to calculate the difference in the number of nodes of the two trees. As an example, given two trees with 4 and 5 nodes each, and assume that they have 3 similar nodes, DEPTA will assume the first tree is 3/4 = 75% similar to the second tree. However, for large trees say with 50 and 51 nodes each, assuming they have 49 similar nodes, then DEPTA will assume the first tree is 49/50 = 98% similar to the right tree. We consider the trees of the two examples as nearly similar as in each case, the difference between the total number of nodes and the total number of similar nodes is only 1. DEPTA X  X  tree matching algorithm works in a time complexity of O( n the first tree and n 2 is the number of nodes in the second tree).

A wrapper is also designed to locate and extract the correct data region. Visual based wrappers such as ViNT [27], VSDR [37], and ViPER [33] use visual cue to locate and extract correct data region. These wrappers calculate the boundary and loca-tion of a data region, and take data region which is large and centrally located as the correct data region. For example, VSDR uses the Vision-based Page Segmentation Algorithm (VIPS) which segments a HTML page content into several regions while
ViPER uses the boundary of a HTML Tag to determine data region which is centrally located. 3.2. Data extraction at data unit level
For data extraction at data unit level, a wrapper usually aligns extracted data records in a tabular form. Each column of the table contains individual data items and these data items are used for further processing.
 Zhai and Liu [52] proposes a partial tree alignment algorithm to insert data into the tree accordingly where necessary.
Before data alignment is carried out, the tree structure of data records are matched to determine the template for all the data records. Whenever two nodes are identical, they are considered as matched and their contents will be used for further matching.

Zhai X  X  data alignment algorithm assumes that a data could find a match in a tree structure for insertion. If this location could not be found, the algorithm will search further to match the data with a second data for insertion. This process will be repeated until the last data is chosen for insertion. This procedure becomes impractical if there is no match for all the pos-sible data chosen. Furthermore, early insertion of data will affect the state of the tree for future insertions of data. Fig. 7 shows a case where a partial tree cannot find a match. Assuming that there are three partial trees, the partial tree AM is un-able to merge with the first partial tree EB as there is no correct sequence for the match. Partial tree AM will then be used to match with partial tree HCO (second partial tree) and they cannot be merged for the same reason.
 Simon and Lausen [33] proposes Multiple Sequence Alignment (MSA) algorithm to align data based on Maximal Unique
Matches (MUM). MSA could efficiently align data records in a polynomial time complexity, but to find the MUM requires extensive checking on the DOM Tree structure. MUM is also not suitable to represent data which is iterative. For example, and this sequence of HTML Tags/Text is the smallest non-divisible unit (Fig. 8). MSA also assumes that if a MUM is created, it may contain more than one text elements. Text nodes in a HTML DOM Tree are atomic entities and therefore they should be considered as separate entities when used for data alignment.
 Extraction of data records at data unit level also involves the correct identification of atomic entities in the data items.
Zhao et al. [26] develops Search Results Record (SRR) wrapper for solving imperfect segmentation problem by merging and partitioning data items into data units using clustering and voting strategies. Solving the imperfect segmentation prob-lem will help the user to differentiate real world entities rather than looking at the tag structure of data records to determine their various entities.

There are several limitations in Zhao X  X  algorithm when it is used to detect high frequency tokens to partition data into smaller units. In SRR, tokens located in the same position are given a larger weight than others. This assumption is partially true as a text may have a larger content than other texts. A text may also have more similar tokens than other texts. For example, a book web site may have Price and Discounted Price as two entities in the first text and may contain another three away from the price entity in the first text, therefore Zhao X  X  algorithm will fail to recognize Final Price as the third entity. 4. The implementation of WISH 4.1. Overview of WISH
In this section we discuss the requirements and the assumptions made for WISH. For WISH to work successfully the sam-ple pages used for data extraction should be obtained from a search engine query and each of these sample pages must con-tain at least three data records. WISH however, does not require the HTML page to be converted to XHTML format as the parser can recognize the HTML format.

The three main components of WISH are shown in Fig. 9 . The first component involves parsing the HTML page and orga-nizing it into the DOM tree representation. In the second component, WISH extracts data records using heuristic techniques.
WISH then determines the template of the data records and aligns the data using the template. WISH further checks the indi-vidual data unit and partitions them accordingly to their patterns. Finally, WISH stores each of the individual data records in a XML file. The sub components of Component2 are shown in Fig. 10 . Component 1 is described in Section 4.2.1. Detailed description of Component 2 is presented in Section 4.2.2 which includes set of filtering rules. Section 4.2.3 provides the implementation work for Component 3. 4.2. Components of WISH 4.2.1. Component 1: Parser for WISH
A search engine result page is required as input for a parser to parse this web page. We experimented with several open source HTML to DOM Tree parsers, and settled on the parser called  X  X  HTML Parser  X (http://htmlparser.sourceforge.net/ ). This parser will read the sample pages and divide them into tokens. There are two types of tokens, the HTML command tag are assumed to be text token. Once the sample web pages are parsed, WISH stored and arranged the contents in a DOM Tree, which will be used for further processing in the subsequent component.
 4.2.2. Component 2: Data extraction at record level in WISH 4.2.2.1. Breadth First Search (BFS) Extraction technique. Once a web page is parsed and represented in a DOM Tree structure, our wrapper needs to traverse through the DOM Tree and identify the various data regions in the web page. To achieve this, we use Breadth First Search (BFS) technique to detect and label the different data regions. Our BFS technique developed is based on the improved and modified version used in MDR. A data region can be defined as a set of data records. Data records in turn can be defined as any records that have similar parent HTML Tag, contains repetitive sequence of HTML Tags and are located in the same level of the DOM Tree.

The nodes in the same level are checked to determine their similarities. In the case where none of the nodes can satisfy this criterion, then the search will go one level lower and perform the search again on all the lower level nodes. WISH takes all the nodes in the same level having similar HTML Tag as a potential group of data records regardless of the distance be-tween them (Fig. 11 ). As long as there is a repetitive sequence of HTML Tags, WISH treats and labels these similar tag nodes as one group ( Fig. 11 ). Fig. 11 shows two cases, where the first case has three A Nodes which are separated by a same dis-tance of 2 while the second case has two A Nodes separated by distance of 2 and a third A Node separated by a distance of 3.
In WISH, potential data records are treated as containing two or more nodes in one group. Fig. 12 a depicts 4 data records, as shown by the rectangles. These data records appear at least twice in the same level of the tree, and have similar HTML Tag identity. Fig. 12 b shows the data detection algorithm in MDR. Unlike our wrapper, MDR is able to group nodes which occur repetitively (e.g. Nodes A and B). However, this procedure is time consuming as additional processing is required to locate and group these nodes. Our wrapper does not require this step, thus reduces the running time required. Data records that are not relevant are removed in the filtering stages accordingly. 4.2.2.2. Filtering stages 4.2.2.2.1. Overview. Automatic wrappers require certain assumptions and observations that may at first appear limiting [33,52] . However, structured records by their very nature have repetitive patterns and follow certain website design rules as they are automatically generated by search engines. Thus, automatic wrappers take advantage of these programming practices to uncover those patterns.

To locate and extract the relevant data region from a pool of available data regions, WISH uses four heuristic techniques for data extraction, each of them is related to the definition of a data record. The authors of the papers [30,33,37,52,48,7,50] on Information Extraction in Web Pages have pointed out several unique features inherent to a data record. We have also made several observations on the constitution of a data record. Based on these observations, we come out with a way to ap-ply heuristic techniques to correctly extract a data region. In general, data records usually occur in repetitive sequence and located in the center of a HTML page. These records are usually large in size and contain large number of text and images. The following are the observations made by several authors as presented in their papers: Observation 1 [37,48,50] : The size of the data records is usually large in relation to the size of the whole page.
 Observation 2 [52,7,33,50] : Data Records usually occur more than three times in a given web page.
 Observation 3 [33,52,30,50] :
Data Records usually conform to a specific regular expression rule to represent their individual data, hence they have nearly similar tree structure.
 Our Observation 4: Data Records usually consist of three HTML Tags that make up their tree structure.

In this paper, four stages of filtering rules are proposed, each of them considers the above observations. After the com-pletion of BFS Extraction, WISH will have a list of data regions. Our examination shows that data regions fall into one of the several groups. We group the first set of potential data regions as menus, these typically determine the layout of HTML pages and are usually large in size and highly dissimilar. The second data region group is advertisements, regions of this group are highly similar but with simple structures. The third group of data regions consists of menu bars, these are simple but are nearly similar in structure. The fourth and last group in these groups of data records is relevant to our work, they are the search engine results output. This group of data records is highly similar in structure and large in size. We aim to design our wrapper so that it can extract this last group of data regions, while removing the other irrelevant ones. We used filtering stage 1 to remove advertisements, filtering stage 2 to remove menus which determine the layout of the HTML page, and fi-nally filtering stage 4 to remove the remaining irrelevant data records. Filtering stage 3 is designed to remove data records which occur less frequently, as observed by author of [52]. 4.2.2.2.2. Stage 1: HTML Tag structures. In this rule, WISH performs the filtering process based on Observation 4. Once the list of the data regions are obtained from BFS Extraction, Stage 1 involves removing data records that have less than three
HTML Tags in each and every group. The purpose of this filtering stage is to remove advertisement related information. We observe that advertisement usually contains simple structure to present its content (usually a list of hyperlinks as its con-tent). Removing these data records will result in faster execution time and more accurate data extraction as there will be fewer irrelevant data records for the other components to consider. 4.2.2.2.3. Stage 2: Similarity. In this section, we introduce the Dummy Tree Matching Algorithm which is developed to check the similarity of data records. We derive this method based on Observation 3 and our finding that data records share an important characteristic, i.e. the distinct tags of a tree and the total number of distinct tags in each level of the tree are nearly similar to those of the other trees of the group. Thus we are able to formulate a similarity check algorithm which can mimic the behavior of a full tree matching algorithm. Our approach is to carry out the similarity check of two trees by exam-ining the distinct tags and comparing the total number of distinct tags in all levels of the trees. Our algorithm is simple but efficient and it can obtain similar results as those of a tree matching algorithm but it has a reduced time complexity. Details of our algorithm and its use in detecting similarity of data records and filtering dissimilar data regions are presented in the following subsections. 4.2.2.2.3.1. Dummy Tree Matching algorithm: Our Dummy Tree Matching algorithm consists of a two stage screening pro-cedure to check the similarity of a group of trees. Given a number of trees, our algorithm first examines the distinct tags of the first tree and those of the second tree. If almost all the distinct tags occur concurrently in the two trees (overall with say only one element different), then the trees pass the similarity test of the first stage and they are used for the second stage the second tree. If the first two trees have almost equal number of distinct tags in all levels of the trees (overall with a dif-ference of only one tag), then the two trees are considered similar according to the stage two criterion. The first two trees are further processing and the second tree is then compared with the third tree of the group to check their similarity using
Stages 1 and 2 of our screening algorithm. On the other hand, if the first two trees are not similar, the first tree will be re-moved and the second tree will be compared with the third tree to check their similarity. The screening procedures for both the above cases are repeated until the last tree is used for comparison.

Fig. 13 gives the complete algorithm derived for similarity check in WISH. As can be seen in Fig. 13 , a data record contains n nodes, therefore the algorithm will run in n times to calculate the similarity of two trees. This implies that Dummy Tree Matching algorithm is able to determine the similarity of two trees in O( n ) time.

Figs. 14 X 16 show data records presented in a tree form obtained from the DOM Tree of HTML pages. For simplicity, we show only two trees in each figure. We calculate the similarity of the two trees of Figs. 14 X 16 using our Dummy Tree Match-tree, so the left tree is similar to the right tree according to the rules of Stage 1 of our similarity check. Further check using level 4 of the tree). The overall similarity checks considering rules of both Stage 1 and Stage 2 indicate that the two trees are b&gt; for both the left and the right trees. The first screening procedure shows that the trees are similar. The total number of procedures will be repeated using the second tree and third tree and so on until the last tree of the group is used if there are more than 2 trees.

Calculations using the tree matching algorithm (e.g. DEPTA) show that trees of Fig. 15 are similar and those of Figs. 14 and 16 are dissimilar. This algorithm gives results consistent with our Dummy Tree Matching algorithm. In general, single data record is usually represented by a regular expression which is applicable to all the data records.

In summary, the procedures used in our Dummy Tree Matching algorithm to check the similarity of a group of trees are: will be used for the second test. 2. Calculate and compare the number of distinct tags in all levels of the trees passing the first test, the trees are considered to pass second test if they have the same number of distinct tags in all levels of the trees. 3. The first tree and second tree are considered similar if they pass both the tests, for such a case, the first tree will be retained for further use. The trees are considered not similar if they fail to pass one of the tests carried out in Steps 1 and 2, therefore the first tree will be removed from the group. For both cases, the second tree will be compared with the third tree and the similarity tests are repeated for tree 2 and tree 3 and so on until the last tree in the group is used for comparison.
 4.2.2.2.3.2. Filtering dissimilar data regions: In general, there are two types of data regions left after the BFS stage, namely data regions with similar data records and data regions with entirely dissimilar data records. Dummy Tree Matching algo-rithm is designed to work by checking the data records of a data region and if they are not similar, they will be removed one by one and thus a data region with dissimilar data records will finally be filtered out. For data regions with similar data re-cords, all these data records will be retained for further processing. The aim of this filtering stage using Dummy Tree Match-ing algorithm is to detect data regions with structurally similar data records normally exist in search engine results page, which are relevant to our study. Dissimilar data regions such as menus which determine the layout of a HTML page have structurally dissimilar data records and will be removed by our filtering algorithm.

We use Figs. 17 and 18 to demonstrate how our Dummy Tree Matching algorithm is used to remove dissimilar data re-gions and retain the similar data regions. Fig. 17 shows the Lycos search engine results page. Fig. 18 is the similar page pre-from those of second and third &lt;table&gt; tags) because they have subtree structure with different sizes. Data records in Data
Region 2 ( Figs. 17 and 18 ), which are represented by the dotted rectangles in Fig. 17 are similar because they have subtree structures of similar sizes. The same applies to Data Regions 3 X 5. Using the Dummy Tree Matching algorithm, Data Region 1 is removed while other data regions (Data Regions 2 X 5) are retained. 4.2.2.2.4. Stage 3: Number of nodes. In this stage, WISH will filter out irrelevant data records based on Observation 2. Data records occurring less than 3 times will be filtered out and excluded for further processing. The purpose of this filtering stage is to reduce the number of irrelevant data regions, thus the data extraction in the final filtering stage (scoring function) can be more accurate. 4.2.2.2.5. Stage 4: Scoring function. After the completion of Stage 3, WISH will have a filtered list of data regions. From the list of available data regions, only one data region is chosen based on the scoring function of this stage assigned to each of the data regions. Filter Rule in Stage 4 is the most important component of the data extraction phase because a good scoring function is needed to differentiate the correct data region from incorrect ones.

This filter rule is derived based on Observations 1 and 2. Since data records occupy most of the space in a web page, we need to represent this property in our implementation. The best way to deal with this is to look at the text and images of the data records. It is noted that correct data records have more text and images than the rest of the data records. Therefore we take into account the total length of the text and the number of images.

A constant value of 15 is added to the scoring function for every image detected in the data records. We also add a value of 1 to the scoring function for every character encountered in the data records. We decide to normalize the size of images with respect to the size of a character. Therefore, we choose a value of 15 to be added to the scoring function for each image de-tected assuming that one image has the size of 15 characters on the average.

We notice that correct data records usually have more parent nodes than the rest of the potential data records. Therefore, we give a value of 150 for every parent node of the data records. We choose a value of 150 for the parent node after exper-iments were carried out on the set of possible values in our training data and find that this is the best value for our evalu-ation. When the total number of images, text length and parent nodes have been determined, a final value of the scoring function is calculated to represent a data region.

There are several reasons for the adoption of the various values for the scoring function. A value of 150 is assigned for the data records X  parent nodes as these nodes occur less frequently than the total text length and number of images. A relatively much smaller value is assigned for every character encountered in data records as characters tend to occur in large quanti-ties. Images are generally larger than character, hence they are given a value of 15 instead of 1. WISH also recognizes sep-arator tags such as &lt;br&gt; and &lt;hr&gt; that tend to occupy space in data records. Therefore, whenever WISH encounters these tags, it will assign a value of 50 to them, assuming that each tag contains 50 characters on the average.

The aim of choosing various parameters is to achieve a balance on the number of occurrences of data records and also the space occupied by the data records. A too high score given to parent node will result in extraction of incorrect potential data records such as menu bar that has larger number of occurrences than the correct data records. However, this menu bar has significantly fewer text and images than the correct data records. On the other hand, a high text and image score will result in WISH extracting incorrect potential data records such as menus which determine the layout of the HTML page.
WISH calculates the value of the scoring function according to the following equation: a=NumParentNodesLevel b=TotalTextLength c=NumImages d=NumSeparatorTags x=Data Region
With a list of available candidates, WISH will locate the data region with the highest score value and used it as input for the next stage, i.e. data alignment. 4.2.3. Component 3: Data extraction at data unit level in WISH 4.2.3.1. Data alignment. Once the relevant data region is extracted, the data records in this data region can be aligned for fur-ther use. The data items in each of the data records need to be rearranged and presented in a tabular form for the user. WISH checks the patterns of data records to determine the template to be used for the data records, with consideration also given to the sub template of a subtree unlike other current wrappers [26]. DeLa [30] uses string matching to determine template for data records. However, DeLa considers only the single level string matching, hence it fails to consider the sub tree struc-tures of data records. We use the tree structure of data records to merge similar tags in the tree located next to each other. Our observations indicate that data records contain nearly similar tree structures, therefore we find it useful to match these tree structures and create a template based on the regular expression rule inferred to generate the data records. Further observations show that an iterative statement (e.g. for, while) in the server scripts generates iterative data, while a selective statement (e.g. if) generates disjunctive data. Based on these observations, we use repetitive HTML Tags and sub tree struc-tures of these HTML Tags to check for iterative data and string alignment to check for disjunctive and optional data. Fig. 19 gives a simple example to show how our template detection algorithm is used in our wrapper.

As shown in Fig. 19 , WISH uses the first data record to create an initial template. WISH will further enhance the template by adding dissimilar elements from the subsequent data records to the template. When WISH encounters two different nodes located in the same position in two different trees, it will treat these nodes as disjunctive provided that these nodes have the same previous and subsequent elements.

The general rules incorporated in WISH to generate a data template for different types and groups of data items are as follows: 1. Iterative data : In response to a user X  X  queries, a database server usually generates a set of data records and embeds them in a HTML page for the user. As the server usually uses the same code to generate these data records, it allows us to use a generalized rule to represent these data records. Our study shows that data records appear contiguously and we are able to use the symbol * to represent repetitive pattern for the regular expression of data records. 2. Optional and disjunctivedata: We need to specially treat data records with optional and disjunctive data items. WISH takes into consideration the characteristics of special data items such as optional and disjunctive data items. For example, given four data records with elements ABCCCD, ACCE, ABCCE, and ACE, where element B is the optional data item and elements
D and E are the disjunctive data items, we will be able to create three regular expressions ABC the three regular expressions, we can further generalize them to form a final regular expression AB?C the regular expression rule, we use string alignment [15] to detect data records with disjunctive and optional patterns and data merging to handle iterative patterns. An alignment of two strings is carried out by appending the HTML Tags in a particular level of a tree. Referring to Fig. 19 , the first level of the tree in Record 1 contains HTML Tags of the sequence two sequences, we can then determine the optional and disjunctive patterns by examining the position of the individual
HTML Tag. Take Fig. 20 as an illustrative example, the HTML Tag &lt;I&gt; occurs in the first string but do not occur in the sec-ond string. Therefore, we use the symbol  X  X ? X  for HTML Tag &lt;I&gt; as an optional attribute. The resulting regular expression after applying the string alignment algorithm is &lt;P&gt;&lt;DIV&gt;&lt;B&gt;&lt;P&gt; &lt;DIV&gt;&lt;B&gt;&lt;P&gt;&lt;I&gt;?. 3. Groups of HTML Tags which are iterativedata: To determine iterative patterns, we need to generalize the regular expression rules by detecting repetitive HTML Tags (see point 1 above). However, our observations on data records X  structure indi-cate that not only HTML Tags occur repetitively, but a group of HTML Tags may also occur repetitively. Referring to Fig. 19 , incorrect pattern. To determine the correct pattern, our template detection algorithm analyzes the tree structures of the HTML Tags in the patterns to check for regularity in their tree structures. For our example, we know that the string has tree structure of &lt;A&gt;, which is similar to the second HTML Tag &lt;DIV&gt;). However, after checking the string sequence second HTML Tag &lt;P&gt; has tree structure of HTML Tag &lt;A&gt; containing subtree structure of HTML Text node (Add to cart).
Due to the difference in tree structures of these two HTML Tags, we can then conclude that the string sequence does not have iterative data. 4. Sub tree of HTML Tags: Once the iterative, optional, and disjunctive data are considered, we can generalize our regular expression rule and apply to the remaining tree structures of the first level HTML Tags (i.e. applying the rules to the remaining levels of HTML Tags in data records).

In WISH, an element which appears more than once but if the two same elements are not located next to each other in a template, they are treated differently. For example, for the data records ABBCDBE and ABCFBBE, WISH will take same ele-ments located next to each other as similar and the regular expressions for the data records are AB*CDBE and ABCFB the two data records are merged, the result is a new template AB ferent elements as they are located in positions not next to each other, even though they contain similar identities.
Once the template has been obtained as described in the previous section, data alignment is carried out. The nodes of a tree are labeled using the notation [ A 1 , A 2 , A 3 , ... , A leftmost node in level l, A b 1 is the position of the parent node in a higher level, A level higher than the parent node etc.

In Fig. 21 , there are two trees with two different data records and four text elements. WISH will use the template detec-tion algorithm to generate a template for the two data records as shown in Fig. 22 . Nodes A X  X  are determined from the tem-plate generated.

The results of data alignment are summarized in Table 1 . Row 1 of Table 1 shows the columns X  name for each of the text nodes of the data records. Row 2 is the aligned data for data record 1 and row 3 shows the aligned data for data record 2.
WISH aligns each of a set of data records starting from the first data record, referring to the template generated from all the data records to be aligned.

Take for example Text 1 (Fig. 22 ) the column name assigned to it is (1:1:2:1) as it is in the first position in level 1 (node A), first position in level 2 (node B), second position in level 3 (node E, there is a node D on the left of E in level 3), and first position in level 4. The other elements are aligned using the same principles. 4.2.3.2. Data merging and partitioning in WISH. After the data items are rearranged and presented in a tabular form, they can also be partitioned and merged further into data units. The data merging and partitioning algorithm was first proposed by
Zhao et al. [26]. Zhao proposes a method to identify the decorative tags and merge them. He also solves the imperfect seg-mentation problem by partitioning data items into multiple data units. Solving the imperfect segmentation problem is important as real world entities could be identified instead of using the HTML web page format and structure. For example, a text token might contain the author X  X  name, title, price and ISBN of a book. This token contains four different entities, but they are located under the same text token. Separating these four entities is helpful in meta search application.
In this paper, we check the decorative tags and merge them into a new text token as shown in Fig. 23 . This process is carried out recursively in a depth first manner. We also use the frequency of text to partition data items into several data units.

Data partitioning can be carried out once data records are aligned and tabulated. Take Figs. 21 and 22 as examples, WISH will match the text tokens located in the same column of the table. Thus text 1 in column 1 will be matched and text 1 and text 5 cannot be matched as they are located in different columns (Table 1 ). To partition data items into data units, the texts are separated to form tokens. A token from the first text is then compared with all the tokens of other data records. The above step is repeated for the remaining tokens of the first text. The purpose of this process is to determine the frequency of occur-rence of the tokens in each data record and also in different data records.

WISH uses a predefined library of keywords to filter out tokens that are not relevant. This predefined library contains key-are treated as tokens that are of no value to the wrapper.

Tokens which are similar are grouped and identified. The number of occurrence of a particular group of tokens in the data records is then recorded. If the frequency of occurrence of a group of tokens is almost the same as the number of data records (within the range of  X 1), WISH will then use the identified token for data partitioning. This particular token is used as a cut-off point of the text in a column of a table which contains the token.

For a particular token to be chosen as a cut-off point, it is required that: 1. The token should appear in all the texts used for partitioning. The frequency of occurrence of the token should be the same in each individual text (within the range of  X 1). 2. The relative position of the token should be the same in all the texts. 3. The tokens in all the texts should occur in the same sequential order.

The absolute position of the first token is {1,1,1} for all the texts, the second token is {4,4,4}, and the third token is {8,11,10}, {11,14,13}, {14,17,16}. From the absolute positions, we can calculate the values for relative positions using their order in the respective text. For example, in Text 2, Author X  X  absolute position is 1, Title is 4, while Date is 11, 14, 17. Using some simple sorting algorithm, we can deduce that the sequential order of the tokens in Text 2 is {Author,Title,Date,Date,-position of the first  X  X  date  X  is 3, that of the second  X  X  date  X  is 4 and that of the final  X  X  date  X  is 5 for all the texts.
Therefore, text 1 will be partitioned as  X  X  Will Smith  X  for the first subtext, followed by  X  X  Adaptive Web Survey  X  as the second 13/1/08  X  as the last subtext. As the date entity could be subdivided further into several entities, it is partitioned into three Fig. 26 shows the result of data partitioning.

It is noted that the words  X  X  Web  X  and  X  X  Survey  X  appear at least once in the text. However, as explained earlier, these words will not be selected as cut-off points for data partitioning as they do not occur throughout the whole three texts. 5. Experimental tests 5.1. Preparation of datasets
The datasets used in this study are taken from web pages that contain search engine results. These datasets are divided into seven groups: Training Set, Dataset 1, Dataset 2, Dataset 3, Dataset 4, Dataset 5, and Dataset 6 with size of 50 web pages for Training Set, 150 web pages for Dataset 1, 119 web pages for Dataset 2, 50 web pages for Dataset 3, 51 web pages for
Dataset 4, 80 web pages for Dataset 5, and finally 100 web pages for Dataset 6. The data distribution for each of the datasets varies, ranging from academic sites, general sites to governmental sites.

The training and first datasets are prepared by the authors. The sample web pages for the training dataset are randomly chosen from the web sites. The first dataset is randomly chosen from the internet. Dataset 1, Dataset 6, and the training data-set are publicly available at http://geocities.com/dtmwrapper/. The second and third datasets were taken from ViNT testbed, available at http://www.data.binghamton.edu:8080/vints/testbed.html . The fourth dataset is the TDBW v1.02, obtained from http://daisen.cc.kyushu-u.ac.jp/TBDW/ .

Before WISH is used to test Datasets 1 X 6, we used the training dataset to optimize the parameters of WISH. We carry out sensitivity tests to gauge the performance of our wrapper using several sets of parameter values (HTML Tags, Parent Nodes,
Image Score, Text Score). Once the optimal parameters are obtained, we used the same parameters for Datasets 1 X 6. It is worth noting that for all the datasets, each web page belongs only to a single web site. For example, Dataset 1 has 150 web pages, therefore there are 150 distinct web sites in it.

The datasets contain different web pages, where none of the web pages chosen for one of the datasets will occur in any other datasets. Datasets 2 and 3 are the data originally used to test the performance of ViNT wrapper. These datasets are then used to test our wrapper as a useful indicator to see the accuracy and reliability of our wrapper.

The fourth dataset taken is the one used for testing in ViPER [33]. The purpose of using this dataset is to see the perfor-mance of our wrapper compared to ViNT [27] and DEPTA [52] when tested against a neutral publicly available dataset.
For the fifth dataset, 80 web pages were chosen from Dataset 1. This dataset is chosen in such a way that both DEPTA and our wrapper are able to extract the data from all the 80 web pages. The main purpose of choosing the data in this way is to evaluate the performance of the data alignment algorithms for our wrapper and DEPTA. Data alignment is usually carried out after the data extraction phase, therefore we make it vital that WISH and DEPTA are able to extract data records for the 80 pages chosen.
 We used Datasets 1, 2, 3, 4 and 6 to test the efficiency of our data merging and partitioning algorithm. The sample pages in
Dataset 6 contain data records with a high number of atomic entities. This dataset provides a useful indicator to test the reli-ability of our wrapper in merging and partitioning data. The total number of web pages used to test our wrapper amounts to 520. For all the datasets chosen, all the web pages contain semistructured data records.

We also evaluate the time complexity of our wrapper with respect to DEPTA and ViNT. We use datasets 1 X 4 to measure the running time used to extract data records for ViNT, DEPTA and our wrapper. The average time to perform data extraction for the respective wrappers is recorded. We do not compare our wrapper with MDR as studies shown in ViNT [27] and DEPTA [52] indicate that both ViNT and DEPTA can perform better than MDR [7] and our experiments show that WISH is compar-atively better than ViNT and DEPTA.

We further evaluate the robustness of our wrapper by replacing several components of our wrapper by available conven-tional techniques. This step is carried out to measure the reliability of our wrapper when these components are replaced. For example, we use string and tree edit distance algorithms as a replacement for Dummy Tree Matching for measuring the sim-ilarity of the structure of data records while we use visual cue (rectangular bounding box of a data region) as a replacement for heuristic scoring function to determine the relevant data region. Experimental results indicate that when using Dummy Tree
Matching and heuristic scoring function, our wrapper is able to run faster and more accurately. We also test the reliability of our wrapper by removing the Stage 1 (Filter HTML Tags) and by changing the number of nodes for Stage 3 (Filter Number of
Nodes) of our filtering modules. Experimental tests indicate that the above do not affect the accuracy of our wrapper. 5.2. Method of evaluation
HTML web page parsing is a difficult task. Therefore, it is very unlikely that a publicly available parser can achieve 100% parsing rate. For those web pages that the parser failed to parse, we rule them out from consideration in our evaluation.
For all the datasets, we compared our work with ViNT [27] and DEPTA [52], two of the current publicly available wrap-pers. Unlike some of the wrappers (EXALG [6], and ROADRUNNER [46]), our wrapper is able to identify data records on a single HTML web page generated by a search engine. EXALG [6] and ROADRUNNER [46] require multiple web pages to carry out data extraction tasks. Due to the fact that our wrapper only needs to work on a single sample page, we randomly choose one web page from each of the 10 sample pages available in Dataset 2 and Dataset 3. For the fourth dataset, there are five sample pages for every web site. For the similar reason, we randomly select one web page from each web site for our evaluation.

The experiment was conducted with a PC specification of Pentium 4 2.4 GHz, with 1 GB of RAM memory. The measures of wrapper X  X  efficiency are based on three factors, the number of actual data records to be extracted, the number of extracted data records from the test cases, and the number of correct data records extracted from the test cases. Based on these three values, precision and recall are calculated according to the formulas:
The experiments were carried out in two parts. Training Set and Datasets 1, 2, 3, 4, and 6 were used in data extraction and data partitioning while Dataset 5 was used for data alignment.
 5.3. Data extraction results 5.3.1. Training Set
Detailed results obtained from the Training Set are presented in Table 2 . In the Training Set, we manage to obtain the set of optimal parameter values for WISH. The set of parameter values are presented in Table 3 . These four parameters are trade off such that the correct data records will end up having the largest score value at the end of the extraction phase, hence chosen for extraction. The parent nodes X  score value and the number of occurrence of images and text must balance each other so that the final score value is able to differentiate correct data records from the incorrect ones. We also carry out sen-sitivity tests by varying the value of each optimized parameter. When a particular parameter value is changed, the remaining parameters are set at the optimal values. These tests are used to check the stability and validity of the optimal parameter values. Results of these tests are shown in Table 4 . Test results indicate that by increasing or decreasing the value of an opti-mal parameter does not improve the precision and recall rates. 5.3.2. Dataset 1
WISH outperforms DEPTA and ViNT both in terms of recall and precision rates. (Table 5 ). The strength of our wrapper lies in the testing of Dataset 1. The result in Table 5 shows that our wrapper significantly outperforms the works of ViNT and
DEPTA. Our wrapper incorporates an accurate tree matching algorithm which could detect similarity of data records. Besides, the filtering technique used in Component 2, Stage 4 allows extraction of correct data records. However, there are several odd cases which our wrapper did not consider for. Some search engine results have search identifier (e.g. Search query  X  X  Web  X  returns 10 results) which also has similar parent node to that of relevant data records. In some cases, this identifier also has similar tree structures as data records X  tree structures. This search identifier will eventually pass through all the filtering stages successfully and extracted as data records. 5.3.3. Dataset 2
The test on Dataset 2 shows that WISH has improvements over the works of DEPTA and ViNT ( Table 6 ). Our result shows that we obtained better recall and precision rates than that of DEPTA and ViNT. This could be attributed to the fact that our
Dummy Tree Matching algorithm is more efficient in detecting the similarity of structured data records than the algorithms in DEPTA and ViNT. 5.3.4. Dataset 3
Similar to Dataset 2, test on Dataset 3 shows improvements in our work compared to the works of DEPTA and ViNT ( Table 7). 5.3.5. Dataset 4
WISH wrapper performs better than DEPTA and ViNT on this dataset (Table 8). As shown in the table, WISH produces a recall value of over 10% higher than ViNT. 5.3.6. Running time of WISH compared to DEPTA and ViNT We evaluate the running time of our wrapper with respect to DEPTA and ViNT on Datasets 1 X 4. We compare Dummy Tree
Matching algorithm with the tree matching algorithm of DEPTA. There are two wrappers (DEPTA [52] and NET [8]) which use tree matching algorithms to detect similarity of data records, but only DEPTA is publicly available. Experimental results show that our wrapper runs faster than DEPTA (Table 9 ). This result signifies that our Dummy Tree Matching algorithm runs in a time complexity smaller than that of DEPTA. Our wrapper also runs faster than ViNT, a visual wrapper using visual cue and
DOM Tree structure of data records. 5.3.7. Evaluation of the Filter HTML Tags
To evaluate the stability and effect of Filter HTML Tags in our wrapper, we test our wrapper on Datasets 1 X 4 by removing the HTML Tags Filter. Results are presented in Table 10 . Our results show that removing the HTML Tags Filter has no marked effect on the overall performance of our wrapper. Slightly higher accuracy rates are noted in Datasets 1 and 2. The higher accuracy rate could be attributed to the presence of web pages containing search results with simple structures, hence our wrapper is able to extract these data records. However, as shown in Table 10 , with HTML Tags Filter included, there is a decrease in running time of our wrapper although the precision and recall rates are slightly reduced. This additional increase in speed will be helpful in meta search engine application and large-scale web comparisons. 5.3.8. Evaluation of the similarity check of data records
As stated earlier, we test the performance of our wrapper by replacing Dummy Tree Matching with String Edit Distance algorithms. We incorporate two types of string edit distance techniques (the simple Euclidean distance and the more com-mon Levenshtein edit distance [47]) using the work of http://www.dcs.shef.ac.uk/~sam/stringmetrics.html and test our wrapper on Datasets 1 X 4. Experimental results indicate that String Edit Distance algorithms cannot improve the perfor-mance of our wrapper as can be seen from the recall and precision rates obtained ( Table 11 ). This test also shows that Dum-my Tree Matching algorithm is an important component of our wrapper as it contributes in the accuracy of data extraction.
We also test the performance of our wrapper by replacing our Dummy Tree Matching algorithm by the tree matching algorithm of DEPTA [52]. Results indicate that although the tree matching algorithm of DEPTA is accurate, it is slow in oper-ation as can be seen from the running time results (Table 12 ). Our Dummy Tree Matching algorithm is not only accurate, but it could also perform the function of a tree matching algorithm at a reduced running time complexity of O(n) (n is the num-ber of nodes in the tree). 5.3.9. Evaluation of Filter Number of Nodes
We also test our wrapper by changing the number of data records to be filtered out from 3 to 2 in the Stage 3 filtering process: Number of nodes (see Table 13 ). Results show that the recall and precision rates will decrease slightly when two data records instead of three data records are used. This is because the menus, a data region containing highly dissimilar data records are extracted instead of relevant data region. As relevant data region which contains only two data records is smaller in size compared to other data region such as menu bars, they are more difficult to be extracted compared to other data region. 5.3.10. Evaluation of locating correct data region
To further measure the reliability of our wrapper, we test our wrapper by replacing the component which determines the correct data region. We used the algorithm of VSDR (determine the large and centrally located data region) as a replacement for Component 2 Stage 4 of our wrapper (Largest Score Filtration). As the algorithm in VSDR requires visual cue for its imple-mentation, we used ICE browser available at http://www.icesoft.com/ as part of our wrapper design. This browser is able to parse a HTML page and provide visual cue in addition to DOM Tree. Experimental results show that our technique is able to obtain more accurate results than the technique using the rectangular bounding box to locate the relevant data region ( Table 14). However, our test also indicates that the technique using rectangular bounding box is slow in operation, thus it is a lim-itation for large-scale web comparisons. The increase in running time is due to the extra processing needed to obtain visual information from the underlying browser rendering engine during the parsing phase. Experimental results also indicate non-visual wrappers like WISH which use fast heuristic techniques could also attain similar performances as visual wrappers. 5.4. Data alignment results
We evaluate the efficiency of data alignment for our wrapper using recall and precision rates. Several algorithms have been used to align data records [30,33,52] . However, only few of these works evaluate their data alignment algorithm.
We measured the performance of our data alignment algorithm using Dataset 5. We compared the data items for each col-umn in the table to determine the recall and precision rates. If a column has its content aligned correctly, we take that col-umn as correctly aligned and extracted [52]. Otherwise, we assume that the column contains data items that are correctly extracted but not correctly aligned. Actual value indicates the number of columns present in a web page.

Experimental results of our data alignment algorithm show that our wrapper could achieve high recall and precision val-ues when used to align data records (Table 15 ). This success could be attributed to the fact that our algorithm uses template detection algorithm to determine the data records X  items in a tree structure, therefore these items will likely to be correctly positioned in the result output of the data alignment algorithm. Our wrapper also takes a wiser decision when applying dis-junctive and iterative operator for data records X  items. We also evaluate our data alignment by comparing the results of our work with those of DEPTA [52]. Experimental results show that our data alignment algorithm is more accurate than that of
DEPTA, with higher recall and precision rates obtained. 5.5. Data merging and partitioning results
As data merging is a straightforward and simple process, our wrapper is able to achieve 100% accuracy in recall and pre-cision rates for all the datasets. We also evaluate our data partitioning algorithm as shown in Table 16 . A data item is con-sidered as correctly extracted and partitioned if WISH manages to identify the group of similar tokens in this data item and partition them accordingly. Otherwise, we consider it as correctly extracted but not correctly partitioned. Actual value indi-cates the number of data items that should be partitioned. Experimental result shows that our wrapper could partition data item into smaller data units efficiently. Dataset 6 contains data records with a high number of atomic entities. As shown in
Table 16 , our wrapper is able to achieve good results for data partitioning. 6. Conclusions
In this study we propose a non-visual wrapper WISH which is able to extract data records from structured web pages and align them using a regular template. Our results show that our wrapper is able to obtain results as well as and in most cases better than the current state of the art visual wrappers such as ViNT and DEPTA. Our approach uses a set of filtering methods based on the DOM Tree structure of data records and a more accurate algorithm to calculate the space occupied by the data region. Our results question how effective visual information is currently used and it also shows that there is a need to re-think the way on how the visual information is used.

Our Dummy Tree Matching algorithm simplifies the complicated process of comparing every node of each tree to check the similarity of two trees as used in the tree matching algorithm. We use the number of nodes of a tree to compare the similarity of two trees. This procedure improves the overall running time without compromising the accuracy, making it suitable for large-scale web comparisons. Our stability tests on each of the heuristic data extraction components of our wrapper also show that Dummy Tree Matching and scoring function are the most important components for extracting data records.

We propose a template that detects the structure of the data records and then aligns them into data items. We believe using a tree structure is more representative of the actual structure of data records than flattening the data records into a single level string. We also merge any repetitive tags before our tree matching algorithm is applied. Thus, our data alignment algorithm is able to align data records efficiently, with support for data records containing iterative and disjunctive data.
To partition data items further into smaller data units, our wrapper also solves the imperfect segmentation problem by using a more reasonable approach. We believe that data items have text with different length, therefore using relative posi-tion instead of absolute position can solve the problem more efficiently. We also believe that data units occur in a predefined sequence, therefore we make this condition as a rule for data partitioning.

We believe this sets a new standard for non-visual wrappers for data extraction at record and data unit levels. Future works include extending WISH to handle web pages containing multiple sections data records and a review on the emerging ontological techniques for data extraction and data alignment.

References
