 change in some specific way is an important question in several fields. A first common situation is when we want to find change-points in a multidimensional signal, for instance, we may want to auto-matically detect changes from human speech to other sound in a movie, based on data representation of features coming from both the audio and visual tracks [1]. Another important situation is when we are confronted with several 1-dimensional signals which we believe share common change-points, and medicine, in particular for the detection of copy-number variation along the genome, though it is also useful for microarray and genetic linkage studies [2]. The common thread in all of these is sudden changes in measurement. As opposed to the segmentation of multi-dimensional signals such as speech, the length of the signal (i.e., the number of probes along the genome) is fixed for a given technology while the number of signals (i.e., the number of patients) can increase. It is therefore of interest to develop method to identify multiple change-points shared by several signals which can benefit from increasing the number of profiles.
 There exists a vast literature on the change-point detection problem [3, 4]. Here we focus on the problem of approximating a multidimensional signal by a piecewise-constant one, using quadratic error criteria. It is well-known that the optimal segmentation of a p -dimensional signal of length n into k segments can be obtained in O ( n 2 pk ) by dynamic programming [5, 6, 7]. The quadratic complexity in n 2 is however prohibitive in applications such as genomics, where n can be in the or-der of 10 5 to 10 7 with current technology. An alternative to such global procedures, which estimate change-points as solutions of a global optimization problem, are fast local procedures such as binary segmentation [8], which detect breakpoints by iteratively applying a method for single change-point detection to the segments obtained after the previous change-point is detected. While such recursive methods can be extremely fast, in the order of O ( np log( k )) when the single change-point detector is O ( np ) , quality of segmentation is questionable when compared with global procedures [9]. For p = 1 (a single signal), an interesting alternative to these global and local procedures is to express the optimal segmentation as the solution of a convex optimization problem, using the (con-vex) total variation instead of the (non-convex) number of jumps to penalize a piecewise-constant function, in order to approximate the original signal [10, 11]. The resulting piecewise-constant ap-proximation of the signal, defined as the global minimum of the objective function, benefits from theoretical guaranties in terms of correctly detecting change-points [12, 13], and can be implemented efficiently in O ( nk ) or O ( n log( n )) [14, 12, 15].
 multidimensional setting, in order to approximate a multidimensional signal by a piecewise con-stant signal with multiple change-points. We define the approximation as the solution of a convex optimization problem, which involves a quadratic approximation error penalized by the ` 1 norm of increments of the function. The problem can be reformulated as a group LASSO problem, which we propose to solve approximately with a group LARS procedure [16]. Using the particular structure to the multidimensional setting.
 Unlike most previous theoretical investigations of change-point methods, we are not interested in situation where n is fixed and p increases. Indeed, this corresponds to the case in genomics where, for example, n would be the fixed number of probes used to measure a signal along the genome, and p the number of samples or patients analyzed. We want to design a method that benefits from increasing p in order to identify shared change-points, even though the signal-to-noise ratio may be very low within each signal. As a first step towards this question, we give conditions under which our method is able to consistently identify a single change-point as p increases. We also show by simulation that our method is able to consistently identify multiple change-points, as p  X  +  X  , validating its relevance in practical settings. To conclude, we present possible applications of the method in the study of copy number variations in cancer. norm in the case of vectors). For any subsets of indices A = a 1 ,...,a | A |  X  [1 ,u ] | A | and B = b [1 matrix. (
Y corrupted by noise, and that change-points locations tend to be shared across profiles. Our goal is to detect these shared change-points, and benefit from the possibly large number p of profiles to increase the statistical power of change-point detection.
 When p = 1 (single profile), a popular method to find change-points in a signal is to approximate it by a piecewise constant function using total variation (TV) denoising [10], i.e., to solve For a given  X  &gt; 0 , the solution U  X  R n of (1) is piecewise-constant and its change-points are predicted to be those of Y . Adding penalties proportional to the ` 1 ot ` 2 norm of U to (1) does not change the position of the change-points detected [11, 17], and the capacity of TV denoising to correctly identify change-points when n increases has been investigated in [12, 13].
 Here we propose to generalize TV denoising to multiple profiles by considering the following convex optimization problem, for Y  X  R n  X  p : a time-dependent multidimensional vector. Intuitively, this penalty will enforce many increments U provides an approximation of the profiles Y by a n  X  p matrix of piecewise-constant profiles U which share change-points. In the following, we propose a fast algorithm to approximately solve (2) 5), and provide an empirical evaluation of the method (Section 6). Although (2) is a convex optimization problem that can in principle be solved by general-purpose solvers [18], we are often working in dimensions that can reach millions, making this approach impractical. Moreover, we would ideally like to obtain solutions for various values of  X  , corre-sponding to various numbers of change-points, in order to be able to select the optimal number of fast coordinate descent-like method, [12] showed how to find the first k change-points iteratively in O ( nk ) , and [15] proposed an O ( n ln( n )) method to find all change-points. However, none of these p is monotically decreasing with  X  .
 a group LASSO regression problem [16]. To this end, we make the change of variables (  X , X  )  X  R immediately get an expression of U as a function of  X  and  X  : This can be rewritten in matrix form as we can re-express (2) as follows: (3), we get that the matrix of jumps  X  is solution of where  X  Y and  X  X are obtained from Y and X by centering each column.
 Equation 4 is a group LASSO problem, with a particular design matrix and particular groups of features. Since existing methods to exactly solve group LASSO regression problems remain difficult to apply here  X  in particular we do not want to store in memory the n  X  ( n  X  1) design matrix when n is in the millions  X  we propose to approximate instead the solution of (4) with the group LARS strategy, which was proposed by [16] as a good approximation to the regularization path of the group LASSO. More precisely, the group LARS approximates the solution path of (4) with a piecewise-affine set of solutions, and iteratively finds change-points. While the original group LARS method requires storing and manipulation of the design matrix [16], which we can not afford here, we can efficient computation of matrix inverses and products. Lemma 1. For any R  X  R n  X  p , we can compute C =  X  X &gt; R in O ( np ) time and memory. matrix  X  X &gt;  X  ,A  X  X  X  ,A is invertible, and for any | A | X  p matrix R , the matrix can be computed in O ( | A | p ) time and memory.
 Proof of these results can be found in Supplementary Materials.
 Algorithm 1 describes the fast group LARS method to approximately solve (4). At each subse-quent iteration to find the next change-point, we follow steps 3 X 8 which have maximum complexity O ( np ) , resulting in O ( npk ) complexity in time and O ( np ) in memory to find the first k change-points with the fast group LARS algorithm.
 Algorithm 1 Fast group LARS algorithm Require: centered data  X  Y , number of breakpoints k . 1: Initialize r = 2: for i = 1 to k do 3: Compute  X  c = 4: If i = 1 , find the first breakpoint :  X  a = argmin j  X  [1 ,n ] k  X  c j,  X  k , A = {  X  a } . 5: Descent direction: compute w = 6: Descent step: for each u  X  [1 ,n ] \A , find if it exists the smallest positive solution  X  u of the 7: Find the next breakpoint:  X  u = argmin [1 ,p ] \A  X  u . 8: Update A = A X  X   X  u } and r = r  X  a  X  u u A . 9: end for The vast majority of existing theoretical results for offline segmentation and change-point detection consider the setting where p is fixed (usually p = 1 ), and n increases. This typically corresponds to a setting where we can sample a continuous signal with increasing density, and wish to locate more precisely the underlying change-points as the density increases.
 Here we propose a radically different analysis, motivated by applications in genomics. Here, the length of profiles n is fixed for a given technology, but the number of profiles p can increase when more biological samples or patients are analyzed. The property we would like to study is then, for a given change-point detection method, whether increasing p for fixed n allows us to locate more precisely the change-points. While this simply translates our intuition that increasing the number of profiles should increase the statistical power of change-point detection, and while this property was empirically observed in [2], we are not aware of previous theoretical results in this setting. 5.1 Consistent estimation of a single change-point As a first step towards the analysis of this "fixed n increasing p " setting, let us assume that the observed centered profiles  X  Y are obtained by adding noise to a set of profiles with a single shared change-point between positions u and u + 1 , for some u  X  [1 ,n  X  1] . In other words, we assume that  X  entries are assumed to be independent and identically distributed with respect to a centered Gaussian by our procedure is the correct one, when p increases. We therefore consider an infinite sequence of same formula.
 Lemma 3. Assume, without loss of generality, that u  X  n/ 2 . When p  X  +  X  , the first change-point selected is with probability tending to 1 .
 From this we easily deduce under which condition the correct change point is selected, i.e., when  X  u = u : Theorem 4. Let  X  = u/n and p  X  +  X  . When  X  2 &gt;  X   X  2  X  , it is not the correct one with probability tending to 1 . This theorem, whose proof along with that of Lemma 3 can be found in Supplementary Materials, deserves several comments. 5.2 Consistent estimation of a single change-point with fluctuating position An interesting variant of the problem of detecting a change-point common to many profiles is that of detecting a change-point with similar location in many profiles, allowing fluctuations in the precise location of the change-point. This can be modeled by assuming that the profiles are random, and that and identically distributed according to a distribution P = P  X   X  P U (i.e., we assume  X  i independent support of P U is [ a,b ] with 1  X  a  X  b  X  n  X  1 , the following result extends Theorem 4 by showing that, under a condition on the noise level, the first change-point discovered is indeed in the support of P U : Theorem 5. Let  X  = U/n be the random position of the change-point on [0 , 1] and  X  m = a/n and  X  also If 1 / 2  X  (  X  one with probability tending to 1 .
 This theorem, whose proof is postponed to Supplementary Materials, illustrates the robustness of the method to handle fluctuations in the precise position of the change-point shared between the profiles. Although this situation rarely occurs when we are considering classical multidimensional signals such as financial time series or video signals, it is likely to be the rule when we consider profiles coming from different biological samples. Although the theorem only gives a condition on the noise level to ensure that the selected change-point lies in the support of the distribution of change-point locations, a precise estimate of the location of the selected change-point as a function of P U , which generalizes Lemma 3, is given in the proof. 5.3 The case of multiple change-points While the theoretical results presented above focus on the detection of a single change-point, the real interest of the method is to estimate multiple change-points. The extension of Theorem 4 to this setting is beyond the scope of this paper, and is postponed for future efforts. We nevertheless conjecture here that we can consistently estimate multiple change-points under conditions on the level of noise (not too large), the distance between them (not to small), and the correlations between their jumps (not too large). Indeed, following the ideas in the proof of Theorem 4, we must analyze precisely at the true change-points. The situation is more complicated than in the single change-remain strictly within the hypersphere between consecutive change-points. This can be ensured if the noise level is not too high (like in the single change-point case), and if the positions corresponding to successive change-points on the hypersphere are far enough from each other. In practice this translates to conditions that two successive change-points should not be too close to each other, and that profiles should have, if possible, independent jumps (direction, etc.). We provide experimental results below that confirm that, when the noise is not too large, we can indeed correctly identify several change-points, with a probability of success increasing to 1 as p increases. In this section we give experimental evidence both for theoretical O ( npk ) complexity and Theorem confirming the O ( npk ) complexity.
 To test Theorem 4, we considered signals of length 100 , each with a unique change-point located at position u . We fixed  X  = 0 . 8 ; assuming for simplicity that each signal jumps a height of 1 at convergence in accuracy to zero. This is indeed what is seen in Figure 2 (left panel), with u = 80 the limit case between the two different modes of convergence. Figure 1: Speed trials. (a) CPU time for finding 50 change-points when there are 2000 probes and the Figure 2: Single change-point accuracy. Accuracy as a function of the number of profiles p when the The right-hand-side panel of Figure 2 shows results for the same trials except that change-point locations can vary uniformly in the interval u  X  2 . As predicted by Theorem 5, we see that the location.
 change-points, we further simulated profiles of length 100 with a change-point at all of positions 10 , 20 ,..., 90 . The jump at each change-point was drawn from a centered Gaussian with variance 1. We then fixed various values of  X  2 and looked at convergence in accuracy as the number of signals increased. One thousand trials were performed for each  X  2 , and results are presented in convergence to zero.
 An interesting application of the fast group LARS method is in the joint segmentation of copy-number profiles. For a set of individuals with the same disease (e.g. a type of cancer), we expect there to be regions of the genome which are frequently gained (potentially containing oncogenes) or lost (potentially containing tumor suppressor genes) in many or all of the patients. These regions are separated by change-points. Figure 4 shows Chromosome 8 of three bladder cancer copy-number profiles. We see that in the region of probe 60, a copy number change occurs on all three profiles. Though it is not in exactly the same place on all profiles, the sharing of information across profiles Figure 3: Multiple change-point accuracy. Accuracy as a function of the number of profiles p when allows the approximate location to be found. The bottom right panel shows the smoothed profiles superimposed on the same axes. A promising use of these smoothed signals, beyond visualization of many profiles simultaneously, is to detect regions of frequent gain of loss by testing the average profile values on each segment for significant positive (gain) or negative (loss) values. Preliminary experiments on simulated and real data suggest that our method is more accurate and two orders of magnitude faster than the state-of-the-art H-HMM [19] method for that purpose. Figure 4: Segmented and smoothed bladder cancer copy-number profiles. Probes shown are mensional setting, developed a fast algorithm to approximately solve it, shown theoretically that the method can consistently estimate change-points, and validated the results experimentally. We have not discussed the problem of choosing the number of change-points, and suggest in practice to use existing criteria for this purpose [6, 7]. We observed both theoretically and empirically that increasing the number of profiles is highly beneficial to detect shared change-points Acknowledgements We thank Zaid Harchaoui and Francis Bach for useful discussions. This work was supported by ANR grants ANR-07-BLAN-0311-03 and ANR-09-BLAN-0051-04. [1] Z. Harchaoui, F. Vallet, A. Lung-Yut-Fong, and O. Cappe. A regularized kernel-based approach [3] M. Basseville and N. Nikiforov. Detection of abrupt changes: theory and application . Infor-[4] B. Brodsky and B. Darkhovsky. Nonparametric Methods in Change-Point Problems . Kluwer [5] Y. C. Yao. Estimating the number of change-points via schwarz criterion. Stat. Probab. Lett. , [6] L. Birg X  and P. Massart. Gaussian model selection. J. Eur. Math. Soc. , 3:203 X 268, 2001. [7] M. Lavielle and G. Teyssi X re. Detection of multiple change-points in multivariate time series. [8] L. J. Vostrikova. Detection of disorder in multidimensional stochastic processes. Soviet Math-[9] M. Lavielle and Teyssi X re. Adaptive detection of multiple change-points in asset price volatil-[10] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. [11] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the [12] Z. Harchaoui and C. Levy-Leduc. Catching change-points with lasso. In J.C. Platt, D. Koller, [13] A. Rinaldo. Properties and refinements of the fused lasso. Ann. Stat. , 37(5B):2922 X 2952, [14] J. Friedman, T. Hastie, H. H X fling, and R. Tibshirani. Pathwise coordinate optimization. Ann. [15] H. Hoefling. A path algorithm for the Fused Lasso Signal Approximator. Technical Report [16] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. [17] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse [18] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, New York, [19] S.P. Shah, W.L. Lam, R.T. Ng, and K.P. Murphy. Modeling recurrent DNA copy number
