 In recent years, with the widespread usage of Web 2.0 tech-niques, crowdsourcing plays an important role in offering human intelligence in various service websites, such as Ya-hoo! Answer and Quora. With the increasing amount of crowd-oriented service data, an important task is to ana-lyze latest hot topics and track topic evolution over time. However, the existing techniques in text mining cannot ef-fectively work due to the unique structure of crowd-oriented service data, task-response pairs , which consists of the task and its corresponding responses. In particular, existing ap-proaches become ineffective with the ever-increasing crowd-oriented service data that accumulate along the time. In this paper, we first study the problem of discovering topics over crowd-oriented service data. Then we propose a new probabilistic topic model, the Topic Crowd Service Model (TCS model), to effectively discover latent topics from mas-sive crowd-oriented service data. In particular, in order to train TCS efficiently, we design a novel parameter inference algorithm, the Bucket Parameter Estimation (BPE), which utilizes belief propagation and a new sketching technique, called Pairwise Sketch (pSketch). Finally, we conduct ex-tensive experiments to verify the effectiveness and efficiency of the TCS model and the BPE algorithm.
 H.2.8 [ Database Applications ]: Data Mining Design, Experimentation, Performance Crowd-oriented Service, Probabilistic Topic Model, Crowd-sourcing
Crowdsourcing refers to outsourcing works traditionally performed by an employee to an  X  X ndefined, generally large group of people in the form of an open call X  X 13]. In gen-eral, crowdsourcing-based service has a common framework: each employer (a.k.a the task publisher) poses a task, and then this task is responded or finished by many different and unknown crowd employees. Thus, the  X  X ask-response pairs X  is the unique structure of crowdsourcing data. In this pa-per, we call information services provided by crowdsourcing, which include massive task-response pairs, as crowd-oriented services . Examples of crowd-oriented services include Ya-hoo! Answers, Quora, and Baidu Recommender, etc.

Given a crowd-oriented service system, one of the most significant problems is the X  X ssignment problem X  X 11], namely how the crowd-oriented service system assigns a task to a right employee or helps an employee to find a right task. In order to enhance the accuracy of assignments, an effective method is to match them in terms of the semantic topic distribution of tasks and historical responses of employees. Thus, the problem of discovering topic is a fundamental task for crowd-oriented service systems. For example, stackover-techniques. By means of discovering the topic distributions of tasks and responses in stackoverflow, we can know which programming technique is the most popular one in current communities of programmers and guide the crowdsourcing platform to find hidden speciality of crowd employee accord-ing to their responses.

Although topic discovery is a basic operator for crowd-oriented service data, it is not trivial to capture hidden top-ics over crowd oriented service data using existing techniques of text mining. Back to the example about stackoverflow, Table 1 includes several real tasks and responses downloaded from the stackoverflow. The crowd-oriented service data in Table 1 contains infinite entries that are chronologically or-dered and each entry contains a task and its correspond-ing responses (if any). We can observe that the first and second task-response entries discuss the topic related with Android-based developing techniques, and the third one is about iPhone techniques. If we use existing text mining techniques, such as Latent Dirichlet allocation (LDA) model, to discover the hidden topic distribution, a straightforward solution is to consider each task and response as a docu-ment and applies LDA model. For the first two entries in Table 1, this method can capture the topic about Android-based technique in the tasks and their responses since  X  X n-
ID Tasks Responses droid X  occur in both of them. However, for the third entry, LDA model might not return similar topic distribution be-cause there is no common words appeared in both of the task and the response.

Thus, for crowd-oriented service data, it is the neces-sary to discover hidden topics by considering both the task-response pair correlation and the specific semantic feature of each task and response. Furthermore, a real-time crowd-oriented service should be viewed as an online updateable data of crowd-oriented task-response pairs, which is referred as a series of consecutively submitted crowd-oriented tasks (task for short) and corresponding responses from crowds. Therefore, for these online service data, the efficiency of training the model is another major concern. The latent topics need to be discovered from massive crowd-oriented service data in real time. To achieve these goals in discov-ering topics over crowd-oriented service data, we have to address the following two challenges.
The aforementioned challenges reflect the high demand of an advanced probabilistic model which is tailored to meet the crowd data form and large data size. In answering this demand, we design and present the Topic Crowd Service Model (TCS), which features the capability of discovering latent topics from massive crowd-oriented service data with high accuracy. One essential niche of TCS is that it cap-tures the the structure of task-response pair by preserv-ing the mutual correlation. The massive size of incoming data, which is continuously increasing everyday, is the night-mare for most conventional parameter estimation methods such as collapsed Gibbs Sampling(GS)[20] and Variational Bayes(VB)[1], due to the high computational cost while travers-ing the dataset. It is observed that such computational cost attributes to the iterative behaviour of scanning the entire corpus and visit the complete topic space. This procedure leads to a linearly increasing computational time cost of the size of the data, the number of topics and the number of training iterations. In answering such challenge, we pro-pose the Block Parameter Estimation (BPE), which is a fast parameter estimation method. The BPE features the Pair Sketch (pSketch) technique, which is a storage space reduc-tion approach that alleviates the pain of updating new data trunks. Moreover, in order to achieve the convergence of parameter inference process, BPE integrates belief propaga-tion[18] into the stochastic gradient descent framework [3], in which a series of online gradient updates lead to the sta-tionary point of the likelihood function of TCS. Such design of the BPE brings in significantly speedup for the parameter inference process by simplifying the updates of TCS during the iterations: in each iteration, only a small portion of the crowd-oriented service data are selected as well as a part of topic space for message updating and passing. In sum, we make following major contributions:
The rest of the paper is organized as follows. Our prob-lem formulation is introduced in Section 2. In Section 3, we discuss the assumptions and generative process of the Topic Crowd Service Model (TCS). In addition, to train T CS efficiently, we present a new pairwise data-based sketch (pSkech) to quickly select the significant words in crowd-oriented service data in Section 4. Based on the significant words, we propose an effective parameter estimation algo-rithm, called Bucket Parameter Estimation (BPE), to train TCS in a specific bucket and multiple consecutive buckets of crowd-oriented service data in Section 5. We present the experimental results, related work and conclude the paper in Sections 6, 7, and 8, respectively.
In this section, we formally define the related concepts and the problem of discovering hot topics over crowd-oriented service data. Let us begin with defining a few basic concepts as follows.

Definition 1 (Task-Response Pair). Given a crowd-oriented task T i , a set of corresponding responses { R i, 1 R i,m } , the arbitrary pair ( T i , R i,j ) for j  X  [1 , m ] is called a task-response pair. Definition 2 (Crowd-oriented Service Data). Let C S= { ( T 1 , R 1 , 1 ) , , ( T n , R n, 1 ) , , ( T n , R task-response pairs, where each task and response is a docu-ment. Each document d is represented by a subset of the col-lection of words W= { w 1 , , w | W | } . Given arbitrary task-response pair, a word-pair includes two words, one word is from the document of the task, the other word is from the document of the response.

Definition 3 (Topic). A semantically coherent topic  X  is a multinomial distribution of words { p ( w |  X  ) } w  X  W the constraint P w  X  W p ( w |  X  ) = 1 .

According to the definitions of related concepts, we can now formally define the major task in the problem of dis-covering topics over crowd-oriented service data as follows. Discovering Topics over Crowd-oriented Service Data Problem : Given the input of a crowd-oriented ser-vice data CS , we are required to infer the latent topics  X  over in CS .

Moreover, we list notations used in this paper in Table 2. In this section, we present a novel probabilistic model, Topic Crowd Service Model (TCS), for discovering topics over crowd-oriented service data. As shown in Table 1 and aforementioned definitions, each document is either a task or a response from crowd employees . Then, we illustrate the underlying logic of TCS. The generative process of TCS is shown in Algorithm 1.

Each document has a topic distribution. According to the pairs generated by pSketch, which will be introduced in Sec-tion 4, the topically coherency of each document is related. When composing the documents, the user first decides the topic that is aligned with his or her current task requirement and then selects some words according to the chosen topic. Notably, the topic distribution is determined by the docu-ment itself and the documents related by the correspond-ing task-response pairs. Furthermore, since each sentence
Algorithm 1: Gen erative process of TCS 1 for each topic k  X  X  1 , , K } do 2 draw a word distribution  X  k  X  Di richlet(  X  ); 3 for each document d  X  X  1 , , D } do 4 draw topic distribution  X  d  X  Di richlet(  X  ); 5 if d is a task then 6 sample a response d  X  wi th regard to the number 7 if d is a response then 8 select the corresponding task d  X  ; 9 generate new document topic distribution  X   X  by 10 for each sentence s  X  d do 11 cho ose a topic z  X  Multinomial(  X   X  ); 12 generate words w  X  Multinomial(  X  z ); is usually topically coherent, we impose the constraint that the words in each sentence should share the same topic, in order to capture semantic coherency.
 Although existing parameter inference techniques, such as Gibbs sampling and variational Bayes, can be used to train the TCS model according to Algorithm 1, they have to spend higher computational cost. In order to enhance the efficiency of training process significantly, we will consider the problem of training the TCS model as a labelling problem. In other words, the training objective is equal to assign a set of topic labels to explain the observed data.
As discussed in Section 3, the latent topic distribution is influenced by corresponding task-response pairs in the pro-posed TCS model. Thus, it is crucial for the training pro-cess to calculate frequencies of word pairs from task-response pairs. However, due to high-volume of online crowd-oriented service data, it is infeasible to count and store all word pairs due to the excessively high cost. Fortunately, word pairs with significant frequencies provide most information for discovering latent topics. So we propose to give pri-ority to capture the contents of these pairs. Since finding significant word pairs is the foundation of our parameter es-timation algorithm, we present a novel sketching structure, called Pairwise Sketch ( pSketch ), to efficiently select the sig-nificant word pairs from massive online task-response pairs. Before discussing the new data structure and algorithm, we first introduce several basic concepts and notations.
Given the set of all words of the given crowd-oriented ser-vice data W , it can be partitioned into two disjoint subsets, denoted by W T and W R , which consist of all words from tasks and responses, respectively. Moreover, the sets of dis-tinct words in W T and W R are denoted by T and R , respec-tively. Then, a significant word pair is defined as follows.
Definition 4 (Significant Word Pair). Given a crowd-oriented service data CS , two significant thresholds  X  T  X 
R where  X  T ,  X  R  X  (0 , 1) , a word pair (t,r) is a significant where f(t,r) means the frequency of the word pair (t,r), f(t) is the sum of frequencies of all word pairs with t as the word in tasks, and f(r) is similar.

Please note that duplicates of a task in CS is only counted (a) Primary Hashing (PH) (b) Secondary Hashing (SH) ferent significant thresholds for them. In order to discover all significant word pairs for online crowd-oriented service data efficiently, a straightforward idea is to utilize existing streaming algorithms of finding frequent items, such as the CM-Sketch algorithm[7], the lossy-counting algorithm[15], the space-saving algorithm[17], and etc. However, the ex-isting researches only handle single item rather than corre-lated pairs. Hence, we have to design a novel solution to find all significant word pairs. Inspired by the space-saving algorithm[17], which is one of the fastest streaming algo-rithms, we propose a novel sketching structure, called the P airwiseSketch ( pSketch ) and algorithm to find all signifi-cant word pairs. The pSketch is defined as follows.
Pairwise Sketch: It consists of two hashing structure components: the primary hashing structure (denoted PH ) and the secondary hashing structure (denoted SH ).
Primary Hashing Structure (PH) : It includes | T | primary hashing units since the crowd-oriented service data has | T | distinct words from tasks. Each primary hashing unit (de-noted PH( t i )) uses t i as the hashing key and maps a space-saving structure, which is also called the Stream-Summary in [17], to maintain the information about which words from responses are approximate significant for t i . In other words, r j is approximate significant for t i if f ( t i , r j ) &gt;  X  (  X   X  ) f ( t )  X  .  X  is the error ratio for  X  T . Moreover, based on [17], the number of elements of each space-saving structure is equal to  X  1  X   X  , na mely m =  X  1  X   X  . Fi gure 1(a) shows a pri-mary hashing structure.

Secondary Hashing Structure (SH) : Similar to the Pri-mary Hashing Structure , it consists of the set of secondary hashing units . Similarly, the number of elements of each space-saving structure, n =  X  1  X   X  whe re  X  is the error ra-tio for  X  R . However, there are two differences between the two hashing structures. On the one hand, the number of secondary hashing units is scalable rather than | R | . As dis-cussed above, we choose the set of words from tasks as the primary hashing set since | W R | &gt;&gt; | W T | . Thus, our ba-sic idea is to construct secondary hashing units only for the words from responses in current significant word pairs. On the other hand, an additional element, called the signifi-cant counter (denoted Sig( r j )), appears in each secondary hashing units SH( r j ) and is used to filter out redundant secondary hashing units , where Sig ( r j ) records the num-ber of currently significant word pairs from tasks with r Once Sig( r j )=0, r j can be safely deleted from SH . Finally, in practice, in order to discover potential significant words from responses, we relax the monitoring requirement to the concept of  X  -significant words from responses . Namely, a word r j is  X  -significant for a word t i if f ( t i , r Figure 1(b) shows a secondary hashing structure.

According to the aforementioned pSketch structure, we design the significant word pair sketching algorithm in Al-gorithm 2. For each word pair (t,r), the algorithm first checks whether t constructs its primary hashing unit P H ( t ) in lines 1-2. Then, the algorithm calls the space-saving algo-rithm subroutine to maintain frequencies of the word pairs Algorithm 2: Sig nificant Word Pairs Sketching
Input : a crowd-oriented service data CS , a error ratio 1 for each word pair (t,r)  X  CS do 2 if t does not construct its PH(t) in PH th en 3 Con struct PH(t); 4 Call Space-Saving(PH(t),r,  X  ); 5 if r becomes  X  -significant for F(t) then 6 if SH( r) exists in SH then 7 Sig (r)  X  Sig(r)+1; 8 else 9 Con struct SH(r) and Sig(r)  X  1; 10 if r is not  X  -significant for F(t) then 11 Sig (r)  X  Sig(r)-1; 12 if Sig(r)=0 then 13 Del ete SH(r); 14 if SH( t)  X  SH then 15 Cal l Space-Saving(SH(t),t,  X  ); including t in l ine 4. If r becomes  X  -significant for t and has SH ( r ) in the current secondary hashing structure, the algo-rithm pluses one for Sig(r) in lines 5-7. Otherwise, SH ( r ) is constructed in lines 8-9. When r is not  X  -significant for t , Sig(r) is decreased by 1. In particular, the SH ( r ) will be deleted from current secondary hashing structure if Sig(r) is equal to zero. In other words, the current r is never in-cluded by any significant word pairs. Finally, the algorithm also call the space-saving algorithm subroutine to maintain frequencies of the word pairs including r if SH ( r ) is not deleted from SH . Hence, the pSketch structure not only maintains all significant word pairs but also returns them while traversing the sketch structure.

Computational Complexity Analysis: The time com-plexity and space complexity of Algorithm 2 are shown as follows. Since crowd-oriented service data is online in most cases, the number of word pairs may be currently unknown. Hence, we mainly analyze the time of processing each word pair in Algorithm 2. The processing time for each word pair in Algorithm 2 is an amortized constant time, which can be directly obtained because Algorithm 2 calls the space-saving subroutine in constant time, and the space-saving subrou-tine has the amortized constant processing time[17]. the following, the space usages of primary and secondary hash structures are analyzed, respectively. For primary hash structures, Algorithm 2 assigns a space-saving structure for each distinct word from task. Each space-saving structure spends O ( 1  X  ) ac cording to its definition[17]. Hence, the space usage of primary hash structures is totally O ( | T |  X  ). F urther-more, according to the definition of  X  -significant word pairs, the maximum number of words that come from responses and satisfy f ( t, r ) &gt;  X  f ( t ) for any word t from tasks is 1 Thu s, the total space usage of secondary hash structures is  X  X  . To sum up, the total space complexity of Algorithm 2
In this section, we discuss the details of the Bucket Param-eter Estimation (BPE). Section 5.1 first shows the procedure of utilizing BPE to train TCS with crowd-oriented service dat a in a specific bucket. Section 5.2 also extends the BPE approach to the scenario of multiple consecutive buckets.
Section 4 introduces a novel sketching structure to select the significant task-response pairs in crowd-oriented service data. According to these task-response pairs, we propose a new latent parameter estimation approach in a bucket in this subsection, a. Since each sentence is the basic unit for topic assignment, we first aim to infer the probability that a sentence s of the document d is assigned to the topic k , namely the following formula, where l d denotes the related documents which include a word occurring with the other word of the current document d in at least a significant word pair in pSketch,  X  s means all sentences in the current document d without s , and z d,  X  s and z ,  X  s,w are all possible topic assignments of neighbouring variables.

Therefore, we denote the belief message by k d,s , indicating a sentence s of the document d is generated by the topic k . It is represented by the following formula: where  X  denotes a ratio to influence the intensity between the current document d and the related documents l d . In addition, belief message k d,s,w that a word w in the sentence s can be also updated by k d,s,w = k d,s . .

According to Equation 2, we need to perform a series of iterations to update beliefs and infer parameters. In order to guarantee the efficiency and effectiveness of training the TCS model, there are two crucial challenges: 1) What is the best strategy for the training process? 2) when should the iteration process terminates?
For the first challenge, our main idea is to choose the largest belief residual, which is denoted by r k d,s = k d,s d,s ( t  X  1) , as the updating goal in the updating process from the ( t  X  1) th iteration to the t th iteration. It is reason-able for the updating method in each iteration because the largest belief residuals speed up the convergence of iterations as much as possible. Based on the aforementioned updat-ing strategy, we have to update all non-zero belief residuals, whose number is too huge. Hence, to further enhance the efficiency of the training process, we select only significant task-response pairs, topics, and documents in the training process according to the pSketch. Before introducing more details of our selection strategy, we first extend the concept of r k d,s to the residuals of higher levels. We denote the resid-ual of the document d on the topic k as r k d = P s r k d,s other words, the residual of the document d on the topic k is equal to the sum of all residuals of sentences of the docu-ment d in the topic k . Similarly, we also denote the residual of the document d as r d = P s r k d . Then, we define two selected proportions, denoted by  X  D and  X  K , of documents and topics for message passing in each iteration.
Based on the above concepts, our selection updating strat-egy in each iteration is shown as follows.
According to the above selection updating strategy, we can obtain the normalized estimated messages as follows. where ( t  X  1) and t are the previous iteration and the cur-rent iteration, respectively. Based on the aforementioned belief messages, we can infer the following parameters: the distribution of topics,  X  , and the distribution of words,  X  , respectively. Namely,
For the second challenge, the iteration process also plays a important role. We give two termination conditions for the iteration process. The first condition is a predefined number of iterations. The second condition is when the convergence of the iteration process occurs.

To sum up, based on the solutions to the above two chal-lenges, Algorithm 3 presents the Bucket Parameter Esti-mation (BPE) algorithm. According to the pseudocode in Algorithm 3, BPE algorithm first initializes and normalizes d,s (0) and k d,s,w (0) in line 2. The pSketch stores the fre-quencies of significant task-response pairs and assists the initialization of k d,s,w (0). Then, in each iteration, BPE al-gorithm sorts the documents according to corresponding r d and selects  X  D  X  D documents having the largest residu-als in lines 3-10. For each selected documents, BPE algo-rithm also sorts topics and selects  X  K  X  K topics having the largest residuals in lines 4-8. After the selection, this algo-rithm updates and normalizes k d,s ( t ) and k d,s,w ( t ) in line 5. In particular, please note that BPE algorithm has to sort and calculates all residuals of all documents and topics in the first iteration because there is no previous information. Finally, BPE algorithm terminates when one of two termi-nation conditions, the maximum iteration number and the convergence of iteration, is satisfied.
Section 5.1 have introduced the Bucket Parameter Esti-mation algorithm within a bucket, we extend the BPE al-gorithm to the scenario of multiple consecutive buckets. In
Algorithm 3: Buc ket Parameter Estimation (BPE) 1 for each task-response pair in each document d an d each topic k do 2 Random initialization and normalization k d,s ( 0) 3 for each iteration do 4 for each document do 5 compute k d,s ( t ) and k d,s,w ( t ); 6 compute r k d,s ( t ), r k d ( t ) and r d ( t ); 7 Sort r k d ( t ) in a descending order; 8 Select the  X  K  X  K documents having the largest 9 Sort r d ( t ) i n a descending order; 10 Select the  X  D  X  D documents having the largest fact, there are many real applications for the scenario of mul-ti ple consecutive buckets. For example, incremental crowd-oriented service data, updating crowd-oriented service data, crowd-oriented service data steams, and so on. For such data, we can partition the complete data into multiple buck-ets, then extend BPE algorithm to infer latent topics and capture the evaluation of topics.

First of all, we define several new input parameters for the scenario of multiple consecutive buckets. We denote m as the index of multiple buckets and D m as the number of documents in the m th bucket. Please note that m  X  [1 ,  X  ], d  X  [1 , D m ], and w  X  [1 ,  X  ]. The indexes of multiple buck-ets and words reach infinity because infinity crowd-oriented service data are considered in the scenario of multiple con-secutive buckets.

Then, we introduce how to extend BPE algorithm to the scenario of multiple consecutive buckets. Different from the updating strategy in the single bucket, the basic idea is that the updating parameters not only consider the information on the current bucket but also integrate the information on the previous buckets. Specifically, in the m th bucket, the extended BPE algorithm first randomly initializes and nor-malizes the belief messages, denoted by k d,s [ m ], then ini-tializes the sufficient statistics, denoted by  X  k w [ m  X  1] = n , ,w [ m  X  1] k , ,w [ m  X  1]. Hence, the belief message that a sentence s of the document d on the topic k in Equation 2 should be extended as follows,
According to the aforementioned belief message updating strategy, the BPE algorithm (Algorithm 3) can be extended to perform until at least one of the two termination condi-tions is satisfied. In this section, we evaluate the performance of TCS and BPE with a large-scale crowd-oriented service data. Section 6.1 describes the experimental setup. Section 6.2 presents the experimental results of BPE in terms of the efficiency. Section 6.3 measures the memory cost of BPE. Section 6.4 evaluates the effectiveness of TCS model with several stan-dard metrics. Section 6.5 illustrates some results of topics and analyzes topic evolution in multiple consecutive buckets.
A crowd oriented service data (a.k.a question-answer data) from Yahoo is used as our experimental data. The data con-tains 142,612 questions and corresponding answers. Simi-lar to [20], we set the hyperparameters as  X  = 2 /K and  X  = 0 . 01. Furthermore, for sampling based parameter infer-ence methods, we report the topic modeling results after 300 iterations, which practically ensures convergence in terms of perplexity that is a standard measure for evaluating the gen-eralization of a probabilistic model [21].
We first demonstrate the TCS performance concerning the efficiency aspect. Specifically, we compare BPE with a set of state-of-the-art practices including variational Bayes [1] (VB) and collapsed Gibbs sampling (GS) [10, 20]. We train all the models on the same dataset from pSketch in order to achieve a fair evaluation. Please refer to Figure 2 for the performance evaluation results. In Figure 2(a), the training time with the increase of the data size of a bucket is illus-trated, where we set the topic amount K = 300. We sum-marize two findings via this experiment: 1) As shown in the figure, the training time of TCS(GS) slightly decreases with the data size of a bucket even it involves the additional cost of residual sorting, while that of TCS(VB) and TCS(BPE) increases. This is because the larger data size of a budget leads to a slightly faster convergence of the sampling based parameter inference methods. 2)TCS(BPE) shows less sen-sitivity to the change of the size of a bucket, which is a great character concerning online service with dynamic changes.
We then increase the topic amount K while fixing the data size of a bucket at 512MB, and we record the time cost of the three parameter inference methods. The results are shown in Figure 2(b), in which the running time of all three algo-rithms increase with K . However, since GS and VB require visiting all documents and the entire topic space, their time cost increase quickly when the data size grows larger. On the contrary, BPE only entails a subset of documents and a fraction of topic space, which qualifies itself as a fast topic modeling of massive crowd oriented service data.

We then answer the question that TCS(BPE) outperforms the other topic models in terms of training efficiency. We vary the data size of a bucket and the topic amount, and then we evaluate the time consumption of different models and present the result in Figure 2(c) and 2(d). Here all baseline methods are trained on full data while TCS(BPE) is trained selectively. In Figure2(c) we set the topic amount to be 300 and the result demonstrates the superiority of TCS(BPE) over other models: 1) TCS is a relatively light-weight topic model and does not involve much complicated calculation; 2) TCS(BPE) utilizes pSketch to reduce the crowd oriented ser-vice data that need to be digested by the downstream topic modeling process; and 3) TCS(BPE) reduces the amount of documents and the scope of the topic space that need to be (a) Time vs Size (Different Parameter
Inf erence Approaches) (d) Time vs # of Topics (Different Topic
Mod els) (g) Memory vs Size (Different Topic Mod-(j) Perplexity2 vs. Observed Percentage (m) Perplexity1 vs. Size (Different Pa-ram eter Inference Approaches) scanned in each iteration. We then fix the data size of a buc ket to be 512MB while varying the topic amount K in-creasingly and evaluate the time cost. The results are shown in Figure 2(d), where the TCS(BPE) exhibits significantly better scalability over other methods with a rather moderate linear increase.
Memory cost is another significant concern when evalu-ating topic modeling techniques. In this subsection, Fig-ure 2(e) and Figure 2(f) show the results of comparing the memory cost of different parameter inference methods for training TCS. The BPE approach always outperforms both GS and VB in terms of memory cost.

Moreover, we also evaluate the performance of different topic models by varying the data size of a bucket. Please refer to Figure 2(g) as the results, where TCS(BPE) con-sumes much less memory resource than LDA and TOT. The reason is two-fold, firstly, classical topic models need to pro-cess all data and inevitably occupy large memory resources; secondly, small data size of each bucket helps other three topic models exploit the memory more effectively. For ex-ample, when the bucket size is set to 512MB, the typical memory cost of TCS(BPE) is only 617MB, which is much less than those consumed by the representative topic mod-els. And this value even outperforms the result of Online-LDA, which demonstrates the merit of pSketch in Section 4. Then we demonstrate the memory cost in terms of a varying amount of topics while the data size of a bucket is set to be 512MB. The results are depicted in Figure 2(h), where lin-ear increasing behavior is observed for every topic models. However, Online-LDA and TCS(BPE) shows lowest mem-ory usage when the amount grows larger, which indicates its potential as a scalable practice.
In this subsection, we discuss the effectiveness of TCS model. Specifically, the evaluation are conducted based on the concept of perplexity measurement[21]. Before we elab-orate the details of the experiment, we first generalize its definition as below:
The difference between P erplexity 1 and P erplexity 2 is that the former one is used to describe the held  X  out per-plexity on the learned model  X  , and the latter one is used to evaluate the effectiveness of prediction of the T CS model.
The design of TCS aims to achieve better generalization performance, which can be associated to a lower value of perplexity. In order to facilitate the empirical study, we integrate about ten thousand questions and corresponding responses into a dataset. We then evaluate the TCS ca-pability of predicting unknown task and response words on such dataset, and we notice that the perplexity has a mono-tonically decreasing relationship with the likelihood of the dataset. Moreover, we adopt LDA [20] and TOT [25](both classical topic models) and Online-LDA [12](dynamic topic model) as the baseline approaches while following their cor-responding parameter estimation methods. We summarize the results in Figure 2(i), where the lowest line(TCS) shows lower perplexity and therefore better capability to predict-ing unseen data comparing with the baselines. Note that for the online models(Online-LDA and TCS(BPE)), we assume the crowd oriented service data of each hour as a bucket.
Moreover, we aim to measure how effective the proposed models is in terms of predicting the future task and response words based on a portion of available tasks and responses. Specifically, given the task words t 1: P from a user X  X  log of tasks and response, we try to find out which model provides a better predictive distribution p ( t | t 1: P ) of the remaining words. In particular, eighty percent of tasks and responses data are set as the training data and the remaining twenty percent as the test data. The calculation of the perplexity is shown in Equation (9) and we summarize the comparison results in Figure 2(j). As shown in the figure, TCS(BPE) shows good capability to predict the future released tasks and responses given the historical tasks and responses.
We then present Figures 2(k) and (l) to show the perplex-ity1 and perplexity2 measurements while increasing the data size of a bucket. In Figure 2(k) the topic amount is fixed to 300. We can observe that all topic models is not sensi-tive for changing the data size of a bucket since they show stable value of the perplexity1 in terms of increasing data size of a bucket. Then, Figure 2(l) shows the perplexity2 measurement with the increase of the data size of a bucket. Not surprisingly, all the topic models remains stable on the perplexity2 measurement, and TCS(BPE) shows obvious su-periority over other methods in terms of perplexity.
We further demonstrate in Figure 2(m) the perplexity1 measurement with increasingly varying data size of a bucket, while the topic amount K is set to 300. TCS(VB) exhibits lower perplexity when the data size of a bucket increases, because larger data size of bucket ushers in more robust online gradient descents for higher accuracy. On the con-trary, TCS(GS) and TCS(BPE) often perform worse when the data size of a bucket increases, because smaller data size of a bucket helps correct the global biases. In all cases, TCS(BPE) achieves the lowest predictive perplexity, indi-cating the highest topic modeling accuracy. Figure 2(n) shows the performance of the perplexity2 measurement with the same setting above. We report similar result to that in Figure 2(m), where TCS(BPE) achieves highest topic mod-eling accuracy.

These experimental results above verify that TCS is a robust and effective topic model for crowd oriented service data in terms of the topic modeling accuracy.
Based on the topic modeling results of TCS, it is observed that TCS is designed to discover semantically meaningful topics by different parameter inference methods. The top ten words of four topics extracted by VB, GS and BPE on the same dataset are summarized in Table 3. All three pa-rameter inference methods exhibit effectiveness in grouping semantically coherent task words together as topics, where their results observe high level overlapping of task words except slightly different word ranking. For instance, task words  X  X unning X ,  X  X wimming X  X  X ym X ,  X  X ootball X  and  X  X asket-ball X  are all contained in the topic Sport . And the rank-ings of these task words also show great resemblance with each other. Therefore we conclude that the discovered top-ics are comparable among all the three parameter inference algorithms, which means that using BPE to train TCS can achieve paramount topic modeling accuracy while signifi-can tly better efficiency is observed.

The evolution of each topic is another informal but im-portant measure of the success of topic models. We com-pare the topics that are discovered from consecutive differ-ent buckets and show the results in Figure 2(o),. Again we take the topic Sport as an example of topic evolution. In the first bucket, the task word  X  X wimming X (solid line with circle marker) does not exist in the top five words. Then after receiving more data from the crowd oriented service data flow, the rank of  X  X wimming X  climbs from the sixth to the second in the following several buckets. The task words  X  X ym X  and  X  X ootball X  present similar trajectories where they gradually become more and more important in the topic of Sport . Word emergence and word perishment, in the mean-time, is another important phenomenon in topic modeling results. For example, in the fourth bucket, the word X  X njury X  appears in the topic for the first time and its rank reaches the 7th position in the fifth bucket. On the other hand, the word  X  X hoe X (dotted lines) loses its importance to the topic as more and more crowd oriented service data are processed, and in the fourth bucket it eventually disappeared. All these results demonstrate the capability of TCS via BPE to detect the topic evolution.
In this section, we review the related work in two cate-gories, crowd-oriented service computation and topic mod-eling problems.
Crowd-oriented service computation is a long-existing con-cept and has been practiced for centuries. With the emer-gence of Internet web service, especially the one that facil-itates online question-and-answer websites like Yahoo! An-swer and Baidu Recommender, crowd-oriented service starts to experience a new age where the source of human is broad-ened to a vast pool of crowds, instead of designated experts. This type of outsourcing to crowds, i.e. crowdsourcing, is now receiving countless success in many areas such as fund raising, logistics, monitoring and so on.

In crowd-oriented service applications, human cognitive abilities are mainly exploited in two types: voting among many options, and providing contents according to certain requirements. Most of basic queries in database [8] can be decomposed into simple voting as human tasks: such as fil-tering [5, 6, 19] into two-option voting (Yes or No), entity resolution [24], join [16], data cleaning[23], ranking [9], etc.
Since Blei et al. proposed the concept of topic model-ing[2], topic modeling has attracted tremendous attention in both academic and industrial areas since its emergence. Especially along with the development of Web2.0 techniques, textual data plays a more and more important role in real-life application. Among these textual data, social network or social media like Facebook or Twitter exhibits great value due to their popularity and diversity in contents. Topic mod-eling is thus adopted to track emerging events in social com-munities [14] and to capture geographical topics [29].
Besides aforementioned wide applications, one of impor-tant issues is how to efficiently train the probabilistic mod-els. The collapsed variational Bayesian inference for latent Dirichlet Allocation(LDA)[22] is proposed to beat its coun-terpart in terms of both computation cost and training ac-curacy. The work in [30, 31] then enables the classic loopy belief propagation for parameter estimation by considering the LDA as a factor graph. Topic distribution for new doc-uments can also be inferred without retraining[28]. These parameter inference methods tackle the efficiency issue in training probabilistic topic models in different angles, but none of them are able to be easily adapted to meet the re-quirements from massive crowd oriented service data.
Moreover, another set of related researches with our work is the Community Question Answering (CQA). In this field, most previous work focused on bridging the lexical gap be-tween the queried question and the historical questions [26]. Recently, a few work has studied how to utilize latent infor-mation to fill up the lexical gap [32, 4] and how to discover latent topics[27]. However, the biggest difference between researches of CQA and our work lies in the research objec-tive. Our work focus on enhancing the efficiency of training process and save the space cost as much as possible through the proposed sketching techniques. However, the main goals of CQA are to bridge the lexical gap between the queried question and the historical questions and to recommend best answers to new questions.

To sum up, to the best of our knowledge, the TCS model together with the BPE algorithm and the pSketch struc-ture is the first technique that systematically investigates the problem of topic discovery over massive crowd-oriented service data and provides solutions with solid performance.
In this paper, we study the problem of discovering la-tent topics over massive crowd-oriented service data effi-ciently. In order to guarantee the efficiency and effectiveness of the mining process, we design a novel probabilistic topic model, called Topic Crowd Service Model , which seamlessly incorporates a new data structure, called Pairwise Sketch ( pSketch ) and an efficient parameter estimation algorithm, called Bucket Parameter Estimation ( BPE ). We conduct ex-tensive experiments in real data to verify the effectiveness and efficiency of TCS and BPE. In particular, we verify that BPE algorithm not only significantly enhances the efficiency of topic modeling but also decrease the memory cost than that of existing approaches.
The authors thank the anonymous reviewers for their in-sightful and constructive comments. This work is supported in part by the Hong Kong RGC Project N HKUST637/13, Nat ional Grand Fundamental Research 973 Program of China under Grant 2012-CB316200, National Natural Science Foun-dation of China (NSFC) Grant No. 61232018, Microsoft Re-search Asia Gift Grant, Microsoft Research Asia Fellowship 2012 and Google Faculty Award 2013.
