 Recommender system has become an important component in modern eCommerce. Recent research on recommender systems has been mainly concentrating on improving the relevance or profitability of individual recommended items. But in reality, users are usually exposed to a set of items and they may buy multiple items in one single order. Thus, the relevance or profitability of one item may actually depend on the other items in the set. In other words, the set of rec-ommendations is a bundle with items interacting with each other. In this paper, we introduce a novel problem called the Bundle Recommendation Problem (BRP). By solving the BRP, we are able to find the optimal bundle of items to recommend with respect to preferred business objective. However, BRP is a large-scale NP-hard problem. We then show that it may be sufficient to solve a significantly smaller version of BRP depending on properties of input data. This allows us to solve BRP in real-world applications with mil-lions of users and items. Both offline and online experimen-tal results on a Walmart.com demonstrate the incremental value of solving BRP across multiple baseline models. H.3.3 [ Information Storage and Retrieval ]: Information Filtering; G.1.6 [ Numerical Analysis ]: Optimization Bundle Recommendation; Recommender System; eCommerce; Quadratic Knapsack Problem
Recommender system is one of the key components in eCommerce. It allows eCommerce company to provide per-sonalized service to individual users, increase order size by recommending accessories at checkout, and enhance user X  X  loyalty and engagement [21]. Recent research on recom-mender systems has been mainly focusing on improving the relevance of individual recommended items [16, 24]. How-ever, in scenarios such as personalized email marketing and webpage personalization, a user is usually exposed to mul-tiple items at one time. The relevance or attractiveness of one item may actually depends on the other items shown to the user. Therefore, we argue that we should consider the whole set of recommended items as a bundle instead of treating them individually and independently. Here, we re-fer bundle to a set of items that customers consider or buy together. Examples of bundles are, just to name a few, lap-top &amp; accessories, mattress &amp; bed frame &amp; other bedroom furnitures, and movies directed by the same director. Below we list a few reasons why bundle recommendation matters.
Customer buy bundles. It has been reported that the average order size for Amazon.com is 1 . 5 and 2 . 3 for Wal-mart.com. 1 Using the transaction data of Walmart.com, we found over 1 / 3 of orders contain two or more distinct items as shown in Figure 1.

Contextual influence. It is well known in classic mar-keting research that there is an  X 1+1 &gt; 2 X  effect in carefully-designed product bundles because they may generate con-text that influences a customer X  X  evaluation and choice [4, 28]. For example, IKEA promotes their products (furni-tures) by displaying multiple products together in a single showroom to simulate an idea of perfect home. It is hard to deny that a mediocre product may be more tempting when displayed in the showroom. http://www.edatasource.com/Resources/Press_ Releases/release.aspx?id=125
Product compatibility or consistency. It is a better experience for customers to see items which are compatible and consistent with each other. For example, given that a customer is interested in bedroom furnitures, it is not very wise to recommend a double-size bed frame and a queen-size mattress to the customer at the same time.

Cost saving. Taking advantage of economies of scale, buying a bundle of products may cost less than buying each product separately for both customers and retailers. For example, it is a common practice for online retailers to waive shipping fee if an order exceeds certain amount.

To give a palpable understanding, we illustrate the impor-tance of considering bundles using a real-world example as shown in Figure 2. Given a customer and his past activities on a eCommerce website, a set top-8 relevant items are gen-erated from a recommendation engine that is optimized for relevance, which is shown in column (a). Those items with a check mark are items bought by the customer. Column (b) are what this customer eventually bought in the upcoming week. Comparing columns (a) and (b), we observe that two items (a crib and a dresser) are missing from the recommen-dation list in column (a). Note that customers interested in a crib mattress are very likely to be interested in cribs and other furnitures as well. By considering the bundling ef-fect, column (c) show the items selected using our proposed approach (more details later). Note that all the items pur-chased by the customer are captured in the recommendation list. This case study just shows the potential if we consider bundling effect properly. However, deciding which items to put into a bundle is non-trivial. Typically, the number of items to recommend is bounded by a constant. Take the case study above as an example. We need to decide, not only which items in Figure 2(a) to replace, but also which items to bundle with other ones.

In this paper, we introduce a bundle recommendation problem (BRP) that can promotes bundling effect during recommendation. Its solution is a set of items that maximize some total expected reward. Examples of the reward func-tion include the conversion rate of items in the bundle, and total revenue or profit of the bundle. The item-to-item cross-dependencies are estimated from historical data and should be able to incorporate contextual influence, product com-patibility and cost saving automatically if their effects exist. This also saves the trouble to estimate these cross-item de-pendencies directly which can be both time-consuming and ambiguously defined. Technically, the BRP is essentially a type of Quadratic Knapsack Problem [6] and NP-hard in general. We show that solving the BRP for all the items is equivalent to solving it for a carefully-constructed candidate set. When the estimated expected reward of items satisfies Pareto Principle (also known as the 80-20 rule ), which is valid in most cases, the size of the constructed candidate set will be significantly smaller than the total number of all items. This allows us to solve millions of NP-hard problems in less than an hour on a few hundreds of nodes. We also present several practical implementations of the bundle rec-ommendation algorithm which makes the model very con-venient to be applied to any existing recommender system. In the evaluation session, we first test the BRP model on a real-world date set from Walmart.com. We then conduct an online A/B testing using the BRP model. Both the offline and online results show that the BRP can consistently boost the performance of the baseline methods.

The contribution of this paper is threefold. Firstly, we propose a novel bundle recommendation problem which con-siders the set of recommendations as a whole. It takes into account cross-item dependencies explicitly and rigorously. To the best of our knowledge, the BRP model is the first model that considers the contextual influence of bundles in recommender system; Secondly, we show that it may be suf-ficient to solve BRP for a substantially smaller candidate set, making it tractable and scalable; Thirdly, we justify the efficacy of BRP by applying the BRP model to real-world applications under both offline and online scenarios.
Recommender system has attracted a lot of attention since the Netflix prize [12, 24, 20]. Majority of research on recom-mender system has been focused on improving the accuracy of predicting the relevance of individual item with respect to a user [11, 23]. Item bundling in recommender systems has been considered in vocation package recommendation [27, 15], but it is modeled as a (linear) knapsack problem and no item-to-item pairwise dependency is considered in their work. Item bundling is also considered in curriculum rec-ommendation because of the sequential ordering of courses [18] and time constraints. In these works, cross-item depen-dencies are modeled as hard constraints which makes the problem difficult to solve and unnecessary in our context. In eCommerce, it is difficult to draw a hard constraint be-tween two items, not to mention the size of catalog can be overwhelming. More likely, users have different preferences over item sets, rather than a clear yes or no. Therefore, those methods proposed in other domains cannot be applied directly to eCommerce.

One seemingly relevant work is [8]. Indeed, Garfinkel et al. studies a different problem. The bundles are predefined by retailers with different cost/price. And the shopper has his own request of products and thus has to choose a set of bundles to satisfy his demand while minimizing the overall cost. A greedy method is introduced to select bundles one by one for the shopper. By contrast, in our application, we have to decide which items to be put into a bundle and recommend to customers.

On the other hand, researches on product/price bundling have a longer history in traditional marketing and economics, see e.g., [5, 26, 2]. Product/price bundling refers to the of-fering of two or more products as one combined product at one price. The psychological impact and economical reasons of bundling products are well understood [5, 26], and the techniques of designing bundles are well studied [2]. How-ever, those bundling techniques are not personalized thus not applicable in our case. Moreover, the bundling method in traditional marketing is usually considering hard bundles where products in bundle cannot be sold separately, while we are considering soft bundles in recommendation where the bundling is implicit and products in bundle can be sold separately.

Another related problem is Quadratic Knapsack Prob-lem (QKP). It is a general optimization model that can take cross-dependency into account [6]. One simple ap-plication of QKP is the Max-Clique problem [10] where the cross-dependency is represented as unweighted edge in graph. A more practical application of QKP is portfolio se-lection problem in finance where the cross-dependency is the covariance of two securities [13]. Although QKP is powerful optimization model, it is rarely used for large-scale problem in practice because QKP is NP-hard in general and only small or medium scale problem is tractable by modern mixed integer solver [14, 17].
In this section, we first define a general version of recom-mendation problem where all kinds of cross-item dependen-cies (e.g., contextual influence, cost dependency) are con-sidered. With some assumption and approximation, the so-called Bundle Recommendation Problem (BRP) is then in-troduced.

In scenarios such as personalized email ad-campaigns and product suggestions on product webpage, online customers are usually exposed to a variety of relevant items. Typically, the number of items to recommend at one time is bounded by a constant, say k . Here, we consider the problem of select-ing a set of k items from a given list of relevant items. More precisely, given a user u , we want to recommend k items from a catalog containing n items whose relevances with respect to the user u are known. Note that the relevance score can be imputed through any existing recommendation techniques.

Let R i 1 ...i k denote the reward if items i 1 ...i k are bought by user u at the same time. The reward can be revenue, profit, number of conversions etc. Because of economics of scale, the reward usually satisfies the property R i 1 ...i k  X  P buys items i 1 ,i 2 ,...,i s given that user u will buy s distinct items. For presentation convenience, the notation of u is skipped unless necessary, i.e., we simply use P ( i 1 ,i 2 Let x i denote the decision variable where x i = 1 means item i is shown to the user u and 0 otherwise. Then, the total expected reward (denoted by R total ) of recommending a set of k distinct items to user u can be written as The goal of general bundle recommendation is to find a set of k items such that the total expected reward R total is max-imized, i.e.,
Based on the distribution of order sizes from the Wal-mart.com (see Figure 1), we found most online customers purchase only one or two items in one shopping session. Thus, it is usually sufficient to consider only the first and the second terms and ignore the other higher order terms of R Let it follows that
Since P ( s = 1) is a constant given one user, the general bundle recommendation problem (1) can be approximated by From now on, we refer problem (5) as the Bundle Recom-mendation Problem , or BRP. We call the data r and Q the expected reward vector and cross-dependency matrix .
Constructing the expected reward vector r and cross de-pendency matrix Q is nontrivial. In this subsection, we present one approach to estimate them.

First of all, the rewards R i and R ij are not user-specific and are given depending on the business requirement. For instance, if the recommendation is optimized for number of conversions (purchases), then R i = 1 and R ij = 2. Simi-larly, the reward can be easily modified if revenue or profit is emphasized.

On the contrary, the probabilities P ( i | s = 1) and P ( i,j | s = 2) are essentially user-specific , and thus it is very challeng-ing to estimate them accurately. However, the probability P (buy i | user u ) or its proxy is usually available from any recommender system. Meanwhile, the transition probability P (buy i | buy j ) can be estimated from historical transaction data using the following formula: P (buy i | buy j ) = # users who bought items i &amp; j in one order Then one way to approximate the probabilities P ( i | s = 1) and P ( i,j | s = 2) is to use the following Plugging into Eqs. (2) and (3), we construct expected reward vector r and cross-dependency matrix Q . Because of the scaling effect in constructing r and Q , we leave the parame-ter  X  as a tuning parameter to control the level of bundling effect in final recommendations.

We remark that Eqs. (6) and (7) are just one way to ap-proximate probabilities P ( i | s = 1) and P ( i,j | s = 2). We choose them because of their availability and ease to com-pute. The technique discussed later in this paper applies to any r,Q and does not depend on this approximation.

Caveat : Following Eq. (6), one may propose an alterna-tive estimation of P ( i,j | s = 2) as follows: But this is not recommended in general. Because it essen-tially assumes the independence of purchases between items i and j , which is against the original motivation of bundling. It can be shown that the BRP in Eq. (5) in this case reduces to maximizing only the first term, i.e., r T x , which does not capture any cross-dependency at all. The details are skipped due to space limit.
In previous section, we have defined and instantiated the bundle recommendation problem in eCommerce setting. In this section, we discuss properties of BRP and derive an efficient algorithm to solve the BRP with millions of items. It is not hard see the BRP is a special case of Quadratic Knapsack Problem [6]. It includes the k -clique problem [7] as a special case if we let r = 0 and Q to be the adjacency matrix of a given graph, thus we have Proposition 4.1. The general BRP is NP-hard.
 Moreover, the dimensionality of item space could be on the order of millions in practice. If the dimensionality is one mil-lion, there are roughly 2 . 48  X  10 43 possible combinations of 8 items. And we need to solve the BRP for hundreds of mil-lions of users. This renders the problem highly intractable.
Based on the formulation, heuristics can be developed to solve it, such as greedy method, or constraint relaxation and then rounding. But those methods have no guarantee of arriving at a global optimal solution. Below we show several techniques to obtain global optimal of BRP for all users in a reasonable amount of time. All the techniques are based on the key idea of constructing a candidate set, i.e., a subset of items, which contains the optimal item selection.
In this subsection, we define a candidate set and show why it suffices to consider only items in the candidate set. It is observed in practice that majority of items in the cat-alog is irrelevant to a given customer. Figure 3 shows the value of sorted r of one randomly picked customer, with respect to reward being conversion, revenue and profit, re-spectively. Apparently, no matter which reward function it is, only a few items have a high score, while the remaining have a value close to zero. Intuitively, it seems unnecessary to consider those items when selecting the optimal item rec-ommendation. This motivates us to construct a candidate set of items which the optimal item recommendations can only belong to.

We first introduce the dominance relationship between items. This dominance relationship allows us to differen-tiate the items that have the potential to be contained in the optimal bundle from the ones that do not.

Definition 4.2. An item  X  is dominated by item  X  if the following condition is satisfied r  X  + Q  X  X  + min where U denote the index set of all the items in the catalog.
If an item in a recommendation list is dominated by an-other item which is not in the list, it is always better to switch these two items, as stated in the lemma below.
Lemma 4.3. If item  X  is dominated by item  X  then we have
Proof. The main idea is to first decompose the objective into terms that contain  X  and terms that do not. Then bound the terms that contain  X  by the terms that contain  X  using Definition 4.2. = max  X  max &lt; max = max = max Now, we are ready to present our main theorem.

Theorem 4.4. Item dominated by k or more items will not appear in an optimal selection.

Proof. Suppose there is an item i dominated by k or more items in the optimal selection. Then, there is at least one item which dominate item i but not in the optimal se-lection; let j denote the index of this item. By Lemma 4.3, swapping item i and j will increase the objective value which contradicts the fact the current selection is optimal.
Thoerem 4.4 implies that we only need to solve the BRP for the set of items that are not dominated by k or more items, which we shall refer as the candidate set (denoted by V ).

Corollary 4.5. Given BRP with data r,Q and  X  , we have where V is the set of indices corresponding to the items that are dominated by no more than k  X  1 items, i.e., where  X   X , X  are n -dimensional vectors such that  X  i  X  X  and r V ,Q V denote the sub-vector and sub-matrix of r,Q with rows and/or columns corresponding to the items that belong to V and m = |V| .

For the bundle recommendation problem, the size of can-didate set is usually significantly smaller than n and only a few times larger than k in practice. Here show some em-pirical results for the sizes of candidate set. The expected reward vector r is estimated using the category-based rec-ommendation model described in Subsection 5.1.1 and Q is estimated using (7). Table 1 shows how the size of candidate set m changes as  X  varies. The histogram of m for different users with  X  = 0 . 8 is plotted in Figure 4.

Table 1: Candidate set size versus  X  with k = 8 Mean of m 8 11.5 13.2 14.8 16.5 18.1 19.5 STD of m 0 7.4 7.7 8.0 8.4 8.7 9.0
From Table 1 and Figure 4, we can see that the size of the candidate set is indeed much smaller than n . This is because the largest few r i  X  X  is so dominant that the items in the tail will never be the optimal selection (see Figure 3). As the size of BRP is significantly reduced, global search algorithm such as Branch and Bound [19] can be applied. Plenty of existing software packages like MINOTAUR [14] can be used.
Though the candidate set size is usually much smaller than the total number of items n , which makes the BRP tractable, constructing the candidate set V will take O ( n each user, which could be time-consuming. To construct the candidate set using Eq. (10), we first need to compute the upper and the lower bounds in Eqs. (11) and (12) respec-tively for each item. This will cost in total O ( n 2 log k ) time if heap sort is used. Then, we can resort to heap sort again to find out the k -th largest element in ( r +  X  X  ) and construct Figure 4: Distribution of candidate set size m , for different users with  X  = 1 . 2 the corresponding candidate set, which will take O ( n ) time. In sum, the time complexity to construct the candidate set is O ( n 2 ) when k is small. This is still time consuming if the dimensionality of item space n is huge. Keep in mind that the cross-dependency matrix Q is different with respect to different users, we have to do this computation for every single user, which makes it even worse. We have to develop a more efficient method to obtain the candidate set in rea-sonable time.

As shown in the previous section, the sizes of the con-structed candidate set using Eq. (10) are typically very small. This motivates us to relax the condition in Eq. (10) so we may get a slightly larger candidate set but in less amount of time. We notice that the term r +  X  is mainly contributed by the expected reward r , we may construct the candidate set as follows:
V = { i | r i +  X   X   X  max  X  the k -th largest element in r } (13) where  X   X  max = max i  X   X  i .
 Any item picked by Eq. (10) will definitely picked by Eq. (13). Therefore, we have the following corollary:
Corollary 4.6. Given BRP with data ( r,Q ) and  X  , Eq. (9) also holds true for V defined in Eq. (13).

Eq. (13) does not save much computational cost, as com-puting  X   X  max is still O ( n 2 ) for each user. But if we can esti-mate or approximate  X   X  max cheaply, then we only need O ( n ) to construct the candidate set. Figure 5 shows a distribu-tion of  X   X  max for a subset of different users. We can see that majority of  X   X  max  X  X  are small except a few in the tail. If we assume  X   X  max for each user is a random variable following the distribution plotted in Figure 5, then a reasonable approxi-mation of  X   X  max is to use p -quantile of  X   X  i  X  X  (denoted by of the elements in vector  X   X  . Examples of p could be 0.80, 0.90, 0.99.

Now, if using a small subset of users to estimate advance, then constructing the candidate set V using Eq. (14) only need O ( n ) for each user. If the candidate sets
Figure 5: Distribution of  X   X  max for different users constructed using (14) contains at most m items for all users, we can simply choose the largest m elements in r to form the candidate set instead of having variable sizes of candidate set for each user, i.e., And a larger p we choose for  X   X  p , the larger m will be. There is a one-to-one relationship between p and m . Therefore, choosing p level for  X   X  p is equivalent to choosing the corre-sponding m .

Essentially, one can use anyone of (10), (13), (14) and (15) to construct the candidate set. A good choice will depend on the data set and the tolerance of accuracy. The candidate set by (14) and (15) will take less time to construct but contain more items and thus it takes more time to solve the reduced BRP. In general, Eqs. (14) and (15) should be chosen if the bottleneck is constructing the candidate set and vice verse.
In the subsection, we summarize the bundle recommenda-tion algorithm. Let R (1) denote the one-item reward vector where R (1) i = R i , R (2) denotes the two-item reward matrix dation algorithm is summarized in Algorithm 1:
We remark that the for-loop in Algorithm 1 is embar-rassingly parallel as users are independent of each other. Modern large scale distributed computing framework such as MapReduce [3] can be used.
 We give a toy example to demonstrate how to use the BRP algorithm for recommendation. Given items 1, 2, 3 and 4 with R (1) i = 1 ,  X  i , R (2) ij = 2 ,  X  i,j and
P Algorithm 1: Bundle Recommendation Algorithm
Result : indices of item to recommend for user u ; for i = 1 , 2 ,...,u do end We need to recommend 2 items to this user. Apply Al-gorithm 1 to the above data R (1) ,R (2) ,P (1) ,P (2) , X  . Using (10), (13), (14) or (15) with m = 3, we obtain the candi-date set because item 4 will never be the optimal solution. Then, we have By solving the BRP with ( r V ,Q V , X  ), we obtain the optimal selection (items 1 and 3). In contrast, items 1 and 2 will be selected if we use traditional recommender system because they are the two most relevant items. Intuitively, choosing items 1 and 3 makes sense because user may buy item 3 as well if he/she decides to buy item 1 which is very likely.
In this section, we first test the BRP model using three baseline models on a 2-month transaction data collected from Walmart.com. We then conduct an online A/B test where the recommended items are sent to customer as an email campaign. We would like to emphasize that all the ex-periments and analysis are privacy-friendly as all customer ids are anonymized into random integers. In our experi-ments, the objective is to maximize conversion, thus the rewards are set as R (1) i = 1 and R (2) ij = 2. Unless otherwise specified, all the reported metrics are relative improvement over the baseline models, i.e., where x 1 ,x 0 are the metrics of the BRP combined with base-line model (usually denoted by  X  X RP+baseline X ) and the sole baseline model respectively.
We first briefly review three baseline models used for of-fline evaluations. These baseline models are also used to estimate the expected reward vector r for the BRP model. We want to emphasize that our bundle recommendation al-gorithm works as a post-processing step, and thus is orthog-onal to the baseline model used to compute relevance.
The fundamental assumption of category-based recom-mendation is that a user is more likely to purchase a product within the same category if he has already indicated an in-terest in a category. In particular, P (buy i | user u ) = X The interest categories of one user P (buy in c | user u ) is learned through his past actions. Each user is represented as a multinomial distribution of interest categories, by map-ping each of his actions to its corresponding category in a carefully curated product taxonomy.

To estimate P (buy i | buy in c ), we examine the popular-ity of each product among existing transactions. In order to capture the recent trend, we restrain ourselves to look at transactions only within the past few days/weeks at rec-ommendation time. Therefore, this category-based recom-mendation prefers to pick those recent best-selling products given one user X  X  personal interest categories.
Another commonly used approach for recommendation in eCommerce is to model user actions as a markov chain [20]. That is, one X  X  current action depends only on his most recent action (denoted by a ( t ) ), i.e., In order to estimate the transtition probability from one action to another, we simply count the coocurance of two actions within a fixed time window:
P (buy i | bought j ) = # users who bought j then i As for prediction, a user X  X  profile is represented by the most recent few transactions ( T ):
Matrix factorization has been shown to be a great success for Netflix-prize competition [12]. Due to the scale of our data set, we implemented a randomized matrix factoriza-tion detailed in [25]. Basically, the method approximately factorize a user-item matrix with a product of two lower-rank matrices, i.e., In order to handle huge numbers of rows and columns while minimizing the excess overhead in MapReduce iterations, it proposes to use a randomized SVD [9] to compute Q which can be accomplished through a simple random pro-jection. Then it applies one iteration of alternating least squares to obtain P m  X  k given approximate Q n  X  k .
In this subsection, we test the BRP model using two base-line models -the category-based model and the Markov model. The purpose of offline experiment is more about testing the predicting power of the BRP model and verify-ing our methodologies. We test our method on a subset of active customers from Walmart.com. The transaction data from 6/1/2013 to 6/30/2013 are used to generate the expected reward vector r for all cus-tomers and the conditional transition probability P which are the input for Algorithm 1. Based on our experience, it is reasonable to choose m in Eq.(15) to be 40 (this is further justified by Table 3). For each customer, we recom-mend 8 items. Without showing our recommendations to the customer, we see how many of our recommendations ac-tually appeared in the corresponding customer X  X  transaction between 7/1/2013 and 7/20/2013. Please note that offline evaluation is a conservative evaluation of the BRP model as the customers did not actually see our recommendations. We hope that a model with better accuracy in modeling users X  purchase intention will incur more transactions on-line. Such a setup for offline evaluation has been widely used in recommender systems.

Different metrics, including precision, average order size are reported. The precision of a recommender system is defined as follows: And the average order size is defined for those customers with at least one successful recommendation, order size = Mean(# of successful recs for customer u ) . We also evaluate how different the BRP recommends with respect to baseline models. Note that the reported overlap is the absolute value given by the above formula.

We randomly divide all the customers into 5 groups, num-bered from 1 to 5. In our experiment, group 1 is used to tune the parameter  X  and group 2 to 5 are exploited for offline test. Performance metrics of different groups are reported.
Precision improvement: Table 2 presents the precision improvement over three baseline models. It is observed that the positive improvement is quite consistent across all groups and all baseline models. The same observation applies to av-erage order size. The improvement may look marginal, be-cause there is still a decent amount of customers who con-sider only one item during a shopping session online (see Figure 1). Note that most of the time, our BRP would not change the recommendation list substantially, as indicated by the column of overlap. This suggests that the BRP model is able to strike a good balance to recommend highly relevant items with a few bundling items.

Sensitivity to candidate set size: In our BRP algo-rithm, we exploit Eq. (15) to construct candidate set effi-ciently. One natural question is how sensitive our algorithm is with respect to the candidate set size. Table 3 shows the performance of group 2 with different candidate size. The same pattern is observed for all other groups. As seen in the table, all the performance metrics stabilizes for m  X  30. Note that increasing the candidate set size generally will lead to better performance, as shown in the last row of the table. However, as we discussed, a larger candidate set will Table 2: Relative improvement of BRP using 3 base-line models Table 3: Relative improvement of precision and av-erage order size with different candidate set size, m , in (15) for Group 2 incur more computational time to solve BRP. Given this diminishing return, we have to balance between accuracy and efficiency depending on business priorities and compu-tational resources. In our application, it is reasonable to choose m = 40.

Sensitivity to parameter  X  : We use users in group 1 for tuning the parameter  X  . The results of parameter tuning are listed in Table 4. Note that when  X  = 0, it is essentially relying on baseline models alone without any bundling consideration. Almost in all cases, a non-zero  X  , which encourages bundling, yields better performance with respect to baselines. The empirical curve of precision with respect to  X  is concave, so an optimal  X  can be located. With increasing  X  , i.e., stronger and stronger emphasize over bundling, it is expected that the order size increases while the overlap is getting smaller and smaller.

In summary, by considering the bundling effect, our BRP model is able to capture user purchase intension more ac-curately and those orders with multiple items. Next, we apply our model to online setting, in which customers will be influenced by our recommendations. Table 4: Relative improvement of precision and av-erage order size using different  X  for Group 1
In this subsection, we present an online A/B test result for the BRP model. We compared the three baseline models used in the offline evaluation and the performance of Markov model is the best of the three. Matrix factorization performs the worst in our particular application even after all kinds of parameter tuning and twists. Its inferior performance may be due to the extreme data sparsity and highly skewed distribution in purchases [1]. Therefore, the winner, i.e. the Markov model is chosen as the baseline model in our online evaluation. We remark that there is a very small amount of customers who have bought few items and the Markov model cannot generate sufficient recommendations. In such a case, the Category-based model (see Subsection 5.1.1) is used.

A personalized email campaign was launched on Septem-ber 25, 2013 for our online evaluation. Each email contains 8 recommendations which are randomly distributed to a 2 by 4 matrix in the email. All emails were sent out within the same time period. We randomly select a small percentage of customers from the eCommerce company for our email campaign. The subset of customers are randomly divided into test and control groups. For the customers in the con-trol group, we send an campaign email with 8 item recom-mendations generated by the baseline model only. For the customers in the test group, we send 8 item recommenda-tions generated by the baseline model with BRP model as post-processor.

To evaluate the quality of recommendations, we exploit click-to-open rate and conversion-to-open rate as our evalu-ation metrics. They are defined as follows: where a  X  X onversion X  corresponds to the event that a cus-tomer clicked an item in the email and eventually placed an order in the same browsing session. Even though customers who clicked one item in the email may buy it later, we de-cide not to count those cases as conversions due to various co-existing targeting channels like search engine marketing or retargeting. The conversion attribution problem itself is Table 5: Relative improvement of click-to-open rates (click) and conversion-to-open rates (conversion) us-ing BRP + Markov model after campaign conversion(2 days after) 54 . 67% 0 . 191 click(1 week after) 7 . 43% 0 . 093 conversion(1 week after) 48 . 92% 0 . 230 tricky and worth further research. Here we use a much more conservative approach to count conversions.

The numbers of 2 days and one week after campaign are listed in Table 5. Actually, the numbers do not change much since two days after the email campaign. The Mann-Whitney (or Mann-Whitney-Wilcoxon) Test described in [22] is used to test the significance of the improvement using BRP and the p -value is reported. From Table 5, we see that BRP does improve both click-to-open rate and conversion-to-open rate. The difference is significant for click-to-open rate, indicating that our bundling recommendation algo-rithm are more intriguing for customers to click. The im-provement of bundle recommendation is more observable in online setting than in offline setting because users were in-deed exposed to the recommendation and thus contextual influence could take effect.

The resultant conversions also jump, but the p -value is larger due to data sparsity. Because the intrinsic nature of purchases and our conservative way to count conversions, the number of conversions is several orders smaller than email opens and clicks. We believe the numbers here are a positive sign and we hope to further verify the improvement signifi-cance using a larger scale online bucket test in the future.
In this paper, we introduced a novel bundle recommen-dation model. The bundle recommendation model can be considered as a post-processor, thus can be applied to any existing recommender system. By ignoring the higher order terms of the total expected reward function, the general bun-dle recommendation model can be formulated as a quadratic knapsack problem which we call the Bundle Recommenda-tion Problem (BRP). However, the BRP is a large-scale NP-hard problem in general and we need to solve it for each in-dividual user. This renders the problem highly intractable. To circumvent this, we showed that it is equivalent to solv-ing BRP for only a subset of items called candidate set. In our application, the size of the candidate set is signifi-cantly smaller than n , thus the BRP can be easily solved by any Branch and Bound algorithm. We then tested our bundle recommendation algorithm in both offline and on-line experiments. Both the offline and online results showed our bundle recommendation algorithm can consistently im-prove the baseline models in terms of predefined reward like conversions or revenues.

In this work, we have shown bundle recommendation is able to trigger more transactions in eCommerce, without even leveraging the price advantage of bundles. It would be interesting to integrate dynamic pricing techniques with bundle recommendation. In some applications where order sizes are usually greater than 2, the higher order terms in the objective function may play an important role. The tech-niques of solving a reduced small-scale problem can be gener-alized to the higher order bundle recommendation problem, but the difficulty will be estimating and storing those higher order tensors in practice. On the other hand, we notice that some research has proposed to maximize diversity of rec-ommendation [29]. Diversity can also be considered as one type of cross dependency. We hope to integrate both diver-sity maximization and bundling effect for recommendation in the future. We believe that the bundle recommendation problem can be generalized to other domains beyond eCom-merce, and most techniques proposed in this work are still valid. It boils down to constructing proper r and cross-dependency Q for the particular application. [1] F. Aiolli. Efficient top-n recommendation for very [2] Y. Bakos and E. Brynjolfsson. Bundling information [3] J. Dean and S. Ghemawat. Mapreduce: simplified [4] M. E. Drumwright. A demonstration of anomalies in [5] G. J. Gaeth, I. P. Levin, G. Chakraborty, and A. M. [6] G. Gallo, P. L. Hammer, and B. Simeone. Quadratic [7] M. R. Garey and D. S. Johnson. Computers and [8] R. Garfinkel, R. Gopal, A. Tripathi, and F. Yin. [9] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding [10] J. Hastad. Clique is hard to approximate within [11] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [12] Y. Koren, R. Bell, and C. Volinsky. Matrix [13] D. Laughhunn. Quadratic binary programming with [14] S. Leyffer, J. Linderoth, J. Luedtke, A. Mahajan, and [15] Q. Liu, Y. Ge, Z. Li, E. Chen, and H. Xiong.
 [16] P. Melville and V. Sindhwani. Recommender systems. [17] G. Optimization. Gurobi optimizer reference manual. [18] A. Parameswaran, P. Venetis, and H. Garcia-Molina. [19] P. M. Pardalos and G. P. Rodgers. Computational [20] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. [21] J. B. Schafer, J. Konstan, and J. Riedi. Recommender [22] G. Shani and A. Gunawardana. Evaluating [23] B. Shapira. Recommender systems handbook . Springer, [24] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. [25] L. Tang and P. Harrington. Scaling matrix [26] G. Tellis and S. Stremersch. Strategic bundling of [27] M. Xie, L. V. Lakshmanan, and P. T. Wood. Breaking [28] M. S. Yadav. How buyers evaluate product bundles: A [29] M. Zhang and N. Hurley. Avoiding monotony:
