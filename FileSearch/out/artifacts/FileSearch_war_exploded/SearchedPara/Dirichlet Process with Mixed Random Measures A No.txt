 Dongwoo Kim dw.kim@kaist.ac.kr Computer Science Department, KAIST, Daejeon, Korea Suin Kim suin.kim@kaist.ac.kr Computer Science Department, KAIST, Daejeon, Korea Alice Oh alice.oh@kaist.edu Computer Science Department, KAIST, Daejeon, Korea Topic models such as latent dirichlet allocation (LDA) (Blei et al., 2003) have been extended to incorpo-rate side information such as authorship (Rosen-Zvi et al., 2004), spatial or temporal coordinates (Wang &amp; Grimson, 2007; Wang et al., 2008), and document labels (Ramage et al., 2009). Most of these models are parametric topic models, and they cannot be simply converted to nonparametric counterparts which gen-erally have various advantages over parametric mod-els. In the Bayesian nonparametric (BNP) literature on Dirichlet processes (DP), modeling unknown den-sities with covariates has been often done with depen-dent Dirichlet Processes (DDP), but extending DDP for topic modeling requires more complex model set-tings and posterior inferences (Srebro &amp; Roweis, 2005). In this paper, we propose a novel nonparametric topic model, Dirichlet process with mixed random measures (DP-MRM) for documents with an arbitrary amount of discrete side information such as labels. DP-MRM can be seen as a nonparametric extension of Labeled-LDA (L-LDA) (Ramage et al., 2009) in terms of defin-ing topic distributions over labels. Recent research shows that incorporating label information into topic models has advantages for topic interpretation as well as other practical uses such as user profiling in social media (Ramage et al., 2010). However, L-LDA as-sumes that each label corresponds to a single multino-mial (i.e., topic), and a document is only generated by the topics of the observed labels. Consequently, the model imposes an overly limiting restriction on the topics with which to represent the documents. While L-LDA models each label with a single multinomial, DP-MRM models each label with a random measure which is defined over the entire topic space.
 There are several supervised topic models including sLDA (Blei &amp; McAuliffe, 2007), discLDA (Lacoste-Julien et al., 2008), and medLDA (Zhu et al., 2009), that also model data with labels. There are two major differences between those models and DP-MRM. First, in the former models which are designed specifically for classification, each label acts as the supervisor for learning. In L-LDA and DP-MRM which are designed with the focus on understanding the meaning of each label in terms of the latent topics, each label actually is the label for one (in L-LDA) or a set of (DP-MRM) topic(s). Second, the former models are restricted to modeling data with one label per document and can-not model documents with multiple labels. To illus-trate this second point, we evaluate DP-MRM on data with single labels as well as multiple labels. Another view of DP-MRM is that it is a more gen-eral case of the HDP (Teh et al., 2006). Modeling the corpus with our model using a single label for all doc-uments would produce the same results as the HDP. Viewed this way, DP-MRM can be used instead of the HDP in many BNP models that are extensions of HDP. We show an example of this by incorporat-ing the ddCRP (Blei &amp; Frazier, 2011) into our model for the task of image segmentation as done in rddCRP (Ghosh et al., 2011).
 The paper is organized as follows. In section 2, we describe DP-MRM along with the stick-breaking and P  X olya urn perspectives. In section 3, we derive a sampling method for the latent variables based on Gibbs sampling. In section 4, we demonstrate our approach on labeled documents for single-labeled and multi-labeled corpora and compare the performance of our model by label prediction and heldout likeli-hood against LDA-SVM and L-LDA. In section 5, we present a modification of our model for image segmen-tation and compare the performance with nCuts (Shi &amp; Malik, 2000) and rddCRP (Ghosh et al., 2011) quan-titatively and qualitatively. In this section, we describe our model, Dirichlet pro-cess with mixed random measures (DP-MRM) model. We first review the generative process of L-LDA, and then we show how DP-MRM incorporates label infor-mation within the BNP framework based on Dirichlet Processes (DP). Lastly, we present the stick breaking process and the P  X olya urn scheme for DP-MRM. 2.1. Model Definition L-LDA is a supervised version of LDA for modeling multi-labeled documents. The generative process of L-LDA starts with a definition of a document specific function label ( j ), which returns a set of observed label indices for document j . Then, for each document j , a multinomial distribution  X  j over topics is randomly sampled from a Dirichlet with parameter r j  X  , where r j is a K dimensional vector whose k th value is 1 if k  X  label ( j ) and 0 if k /  X  label ( j ). Then, to generate the word i , a topic z ji is chosen from this topic dis-tribution, and a word, x ji , is generated by randomly sampling from a topic-specific multinomial distribu-tion  X  z ji . By using a document specific indicator vec-tor r j , the model can specify the topic proportion of document  X  j over the | label ( j ) | X  1 dimension simplex. We now describe the generative process of Dirichlet process with mixed random measures. First, we define a DP distributed random measure G 1 0 ,...,G K 0 over K possible labels with a base distribution H as follows: where the base distribution H is assumed to be a sym-metric Dirichlet distribution over the entire vocabulary dimension, and  X  k controls the variability of G k 0 . By defining one random measure per label, we place an in-finite topic space for each label. For each document j , another DP distributed random measure G j is defined with a mixture of labeled-random measure as follows: where  X  is a concentration parameter,  X  jk is a mix-ing proportion of G k 0 , and  X  controls the sparsity of  X  . DP-MRM uses a mixture of random measures, P k  X  jk G k 0 , as the base distribution of G j , the docu-ment specific measure. For the mixing proportion  X  jk of each G k 0 , we sample  X  j from a symmetric Dirichlet prior parameterized by r j and  X  . Hence, with the ob-served labels label ( j ), r j selectively specifies the mix-ing proportions of G k 0 over the | label ( j ) | X  1 dimen-sional simplex.
 For each word x ji in document j , the probability of drawing a word x ji is parameterized by a random vari-able  X  ji drawn from G j with some family of distribu-tion F . It is typically assumed to be a multinomial distribution, which makes F to be conjugate to the base distribution H , and so it is possible to integrate out the factors  X  ji As a result of the construction, the model chooses an appropriate number of topics for each label. Note that HDP can be viewed as a specialized instance of our model (Teh et al., 2006), where we assume there is a single  X  X nknown X  label for all documents. Then the overall corpus is defined by a set of topics from the single  X  X nknown X  label, G unknown 0  X  DP(  X ,H ), and the random measure for document j is drawn from G j  X  DP(  X ,G unknown 0 ). A similar idea of using a mix-ture of random measures was proposed in (Antoniak, 1974), but our model extends that idea into a hierar-chical construction for the grouped clustering problem. 2.2. Construction and Predictive Distribution We now describe two perspectives that are important for the inference algorithms for DP-MRM: the stick breaking process and the P  X olya urn scheme.
 Stick breaking process The stick breaking process is a constructive definition for generating a Dirichlet process (Sethuraman, 1991). Same as the model defini-tion in the previous section, the stick breaking process can be divided into two level DPs. For the first level random measure G k 0 , we follow the general stick break-ing process, which is given by the following conditional distributions: where  X  is a Dirac delta measure. A general stick breaking process can be seen as two independent se-quences of deciding the stick length  X  l by samples from i.i.d. Beta trials and deciding the atom of the l th stick  X  l by i.i.d. samples from H .
 The second level stick breaking construction is given by the following conditional distributions: Deciding the length of each stick is the same as the general stick breaking process, but assigning atoms for each divided stick must be changed because there are K random measures for drawing  X  jt . We introduce k jt as an indicator to G k 0 where atom  X  jt is drawn. We let  X  ji denote the random variable drawn from G j ,  X  jt the atom of G j , and  X  k l the atom of G k 0 . Note that each  X  ji is associated with one  X  jt (i.e.,  X  ji =  X  jt ), and each  X  jt is associated with one  X  k l , thus they form a shared structure across the corpus. Figure 1 visualize a sharing structure between first and second level DPs. P  X olya urn scheme A posterior perspective of the DP is the P  X olya urn scheme which shows that draws from the DP are discrete and exhibit a clustering prop-erty. As Blackwell and MacQueen showed (Blackwell &amp; MacQueen, 1973), our model can also be formed as a successive conditional distribution of  X  ji given  X  Let n jt be the number of words for which factor  X  corresponds to  X  jt in document j , and m jkl be the number of  X  jt such that  X  jt =  X  k l . Then the condi-tional distribution of  X  ji given  X  j 1 ,..., X  ji  X  1 ,G 1 and  X  , with G j and  X  j marginalized out, is  X   X  X where | r j | is the number of 1 X  X  in r j , and r jk if label k has been observed in document j .  X  ji can be sampled from the first term of RHS or the second term of RHS. When  X  ji is sampled from the first term, then it corresponds to one existing  X  jt , and when it is sampled from the second term, we choose G k 0 to draw  X  ji with probability proportional to m jk  X  +  X  . After that, we can marginalize out G k 0 to proceed further and get the conditional distribution We propose a Gibbs sampler for DP-MRM, a P  X olya urn scheme based on the marginalization of unknown dimensions (Escobar &amp; West, 1995). For the collapsed Gibbs sampler, we marginalize out factors,  X , X , X  , mixing proportions,  X  , and random probability mea-sures, G j ,G k 0 . As a result, we only need to sample the index of each latent variable. Let t ji be the index variable such that  X  jt =  X  ji , and k jt be the index vari-able such that  X  jt  X  G k 0 , and l jt be the index variable such that  X  jt =  X  k jt l . Let n jt be the number of  X  such that  X  ji =  X  jt , and let m jkl be the number of  X  jt such that  X  jt =  X  k l . We use f kl ( x ji ) to denote the conditional density of x under mixture component l of random measure G k 0 , given all items except x ji , f Sampling t : The conditional density of word x ji being assigned to  X  jt is Sampling k and l : When new t is sampled, we need to sample k jt new and l jt new . However, sampling k and l cannot be done independently because given l the probability of k is always zero except one. The joint conditional density of k and l is p ( k jt = k,l jt = l | k  X  jt , l  X  jt ,rest ) (9) Sampling k and l of existing t changes the component memberships of all data items x jt = { x ji ; t ji = t } , and this sampling can be done with the conditional distribution of k and l given x jt . We measure the performance of DP-MRM with three experiments. First, we compare the label prediction performance of DP-MRM and LDA-SVM on single-labeled documents. Then, we compare the label pre-diction performance of DP-MRM and L-LDA on multi-labeled documents. Finally, we compare the predictive performance of DP-MRM and L-LDA on heldout data. For the label prediction experiments, we take a semi-supervised approach: divide the corpus into training and test sets, infer the posterior distribution of the training set with the observed labels (i.e. r jk = 1 only when k  X  label ( j )), and infer the posterior distribution of the test set with all possible K labels (i.e. r jk = 1 for all k ).
 For all evaluations, we run each model ten times with 5,000 iterations, the first 3,000 as burn-in and then using the samples thereafter with gaps of 100 itera-tions. For sampling the hyperparameters, we place Gamma(1,1) priors for  X  k , and  X  , and set  X  to 0.5. 4.1. Single-Labeled Documents DP-MRM was designed to model multi-labeled docu-ments, but it assumes that a label generates multiple topics, so this flexible assumption allows DP-MRM to be used for modeling single-labeled documents as well. Note that L-LDA for single-labeled documents would assign every word in a document to a single topic, and the document would thus be modeled as a mixture of unigrams (i.e., naive Bayes).
 To measure the classification performance, we trained our model with five comp subcategories of newsgroup documents (20NG) 1 . Table 1 shows the details of our datasets. 90% of the documents were used with the la-bels, and the remaining 10% of documents were used without the labels. We classified each of the test doc-uments by the label with the most number of words assigned. As a baseline, we trained a multi-class SVM with the topic proportions inferred by LDA (Blei et al., 2003). MedLDA (Zhu et al., 2009), one of supervised topic model, also used for the comparision. The re-sults, shown in Figure 2, display a significant improve-ment of our model over the LDA-SVM approach and MedLDA. 4.2. Multi-Labeled Documents We compared the performance of L-LDA and DP-MRM using two multi-labeled corpora: the Ohsumed dataset 2 , which is a subset of the MEDLINE corpus consisting of medical journals, and RCV1-V2 dataset (Lewis et al., 2004), a corpus of Reuters news articles. We randomly sampled a subset of each corpus, and the detailed descriptions are shown in Table 1. Again, 90% of documents were used with the labels, and the rest 10% of documents were used without the labels. L-LDA provides a systematic way of naming the dis-covered topics, and thus increases the interpretability of them. However, the assumption that a document is generated from a subset of topics specified by the observed labels limits the expressiveness of the model. DP-MRM was designed to keep the benefits of L-LDA while increasing the expressiveness, and we can see the consequences of the design in the discovered top-ics shown in Table 2. The table shows one label from each corpus and the corresponding topics. DP-MRM discovered multiple topics for the labels  X  X nfant X  and  X  X orporate/Industrial X , and these are more detailed topics than the single topics discovered by L-LDA. For the classification of multi-labeled documents based on the posterior samples, we counted the number of words assigned to each measure G k 0 and classified as label k with various threshold cuts based on normal-ized counts. We scored each model based on Micro F1 and Macro F1 measures. Micro F1 accounts for the proportion of each class, so large classes affect its re-sults, whereas macro F1 assigns equal weights to all classes. Table 3 shows the classification results with different cuts, and our model performs better than L-LDA in terms of micro average, but in macro average, there are inconsistencies between the different cuts. In general, DP-MRM shows more stable performance with respect to the cuts, whereas L-LDA shows vari-able results depending on the cut. 4.3. Predictive Performance To compare the model fit, we measure the predictive performance of our model and L-LDA with heldout likelihood of the test set. For each model, posterior sampling was done with 90% of the words in each doc-ument while the test set performance was evaluated on the remaining 10% of the words. Given S samples from the posterior, the test set likelihood for our model is computed as follows: where n jklx is the number of words x corresponding to l in document j , and W is the vocabulary size. The test set likelihood for L-LDA was computed as follows: where K is the total number of labels. Figure 3 shows the test set per-word log likelihood of both model with RCV dataset, our model performs better than L-LDA across ten folded dataset consistently. We describe an extension of DP-MRM, built by incor-porating ddCRP, a nonparametric Bayesian prior that accounts for spatial dependencies, into DP-MRM. This illustrates the generality of DP-MRM that it may serve as a replacement for HDP for data with side informa-tion. We test this DP-MRM-ddCRP model on the task of image segmentation for multi-labeled images without manually segmented training data.
 Image segmentation is often done with manually seg-mented and labeled data (He et al., 2004; Gould et al., 2009). DP-MRM can also perform supervised segmen-tation, but such data are harder to obtain, whereas image collections with multiple labels and no segmen-tation are relatively easy to obtain (e.g., Picasa or Flickr). One recent paper has shown a Bayesian model for simultaneous image segmentation and annotation (Du et al., 2009) using a logistic stick-breaking process. While that model is specialized for image understand-ing, DP-MRM is a general framework for modeling multi-labeled data including documents and images. 5.1. Incorporating ddCRP into DP-MRM The Chinese restaurant process (CRP) is an alterna-tive formulation of the DP. CRP forms a clustering structure of customers by assigning each customer to an existing or a new table. ddCRP, however, forms a clustering structure of customers by linking customers, accounting for the distances between them; customers who are relatively close to each other are likely to be linked together than those who are far apart. Let c i be the assignment of customer i to the other customers, then the distribution of the customer assignment is where d ii 0 is the distance between customer i and i 0 , and f ( d ii 0 ) is a decay function of the distance which mediates how the distances affect the resulting distri-bution over the partitions. There are many possible ways of defining the decay function, and in this pa-per, we follow (Ghosh et al., 2011) and use a window decay function which measures the distance between superpixels as a hop distance between them.
 Based on the conditional distribution of assignments, the P  X olya urn scheme for the combined model is:  X   X  to Equation (6), but we modify the equation based on the window decay function.
 For posterior inference, we modify the posterior sam-pling Equation (8) based on the customer assignment scheme, but the changes only affect the local sampling results (within the document level), and can be em-ployed by the algorithm for ddCRP mixture in previ-ous work. The sampling scheme based on link struc-ture among customers enhances the rapid mixing of sampler. See (Blei &amp; Frazier, 2011) for a more detailed explanation of posterior inference. 5.2. Image Segmentation with Multiple Labels For image segmentation, we use the eight scene cate-gories in (Oliva &amp; Torralba, 2001) which are fully seg-mented and labeled by human subjects and available from the LabelMe dataset (Russell et al., 2008). A widely used method for representing images for infer-ence is a codebook of images (Fei-Fei &amp; Perona, 2005). To generate the codebook, each image is first divided into approximately 1,000 superpixels using the normal-ized cut algorithm (Shi &amp; Malik, 2000). Each super-pixel is described via local texton histogram (Martin et al., 2004) and HSV color histogram. By using k -means, we quantize these histograms into 128 bins, and superpixel i in image j is summarized via these codewords x ji = { x t ji ,x c ji } indicating its texture x t and color x c ji . The base distribution H should be de-fined as H  X  Dir(  X  t )  X  Dir(  X  c ) for image segmentation. Figure 4 shows some examples of the labeled objects from posterior samples where DP-MRM segments im-ages into objects and labels each object. We note again that we do not give any pixel-level information for each object during the posterior inference, but our model can successfully segment images and label segments simultaneously. The results indicate that DP-MRM succeeds in inferring both the segments and the cor-responding labels by capturing the co-occurrence pat-terns of superpixels and labels.
 Figure 6 shows some examples of the image segmen-tation results comparing the original images, human segmented images, and DP-MRM segmented images. Figure 5 shows the quantitative performance of the segmentation via Rand Index, comparing DP-MRM with rddCRP (Ghosh et al., 2011) and normalized cuts (nCuts) (Shi &amp; Malik, 2000), varying the number of segments from two to ten. We also vary the number of segments for each image, denoted as nCuts(*), where the number of segments are given as the number of labeled objects in each image. The result shows DP-MRM performs better than both rddCRP and nCuts. In this paper, we presented our new model, DP-MRM, in which the base distribution of DP is a mixture of random measures. The applications with multi-labeled documents and images are shown with label predic-tion and image segmentation experiments. The re-sults show that DP-MRM for labeled data produces interpretable topics with more flexibility than the La-beled LDA. One promising extension of our model is to incorporate prior knowledge of external sources or domain experts into a Bayesian nonparametric topic model. It is beyond the scope of this paper, but our model can use different  X  k for each base distribution H k , therefore using the structualized prior  X  k from do-main experts (Andrzejewski et al., 2009) can be easily incorporated into our model.
 This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Tehcnology (2011-0026507).

