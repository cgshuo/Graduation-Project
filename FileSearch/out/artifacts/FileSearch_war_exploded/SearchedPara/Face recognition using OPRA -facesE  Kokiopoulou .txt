
This paper presents a method named  X  X rthogonal Pro-jection Reduction by Affinity X , or OPRA -faces, for face recognition. As its name indicates, the method consists of an (explicit) orthogonal mapping from the data space to the reduced space. In addition, the method attempts to preserve the local geometry, i.e., the affinity of the points in a ge-ometric representation of the data. The method starts by computing an affinity mapping W , of the data, which opti-mally expresses each point as a convex combination of a few nearest neighbors. This mapping can be viewed as an op-timal representation of the intrinsic neighborhood geome-tries and is computed in a manner that is identical with the method of Locally Linear Embedding (LLE). Next, and in contrast with LLE, the proposed scheme computes an ex-plicit linear mapping between the high dimensional sam-ples and their corresponding images in the reduced space, which is designed to preserve this affinity representation W . OPRA -faces shares some properties with Laplacianfaces, a recently proposed technique for face recognition, which computes the linear approximation of the Laplace-Beltrami operator on the image manifold. Laplacianfaces aims at preserving locality but does not explicitly consider the in-trinsic geometries of the neighborhoods as does OPRA . As a result of the preservation of the affinity mapping W , OPRA will tend to produce a linear subspace which cap-tures the essential geometric characteristics of the dataset. This feature, which appears to be crucial in representing images, makes the method very effective as a tool for face recognition. OPRA is tested on standard face databases and its effectiveness is compared with that of Laplacianfaces, Eigenfaces and Fisherfaces. The experimental results indi-cate that the proposed technique produces results that are sharply superior to the other methods, at a comparable or lower cost.
Face recognition [11] is one of the most challenging problems in computer vision and has numerous applications ranging from security to contactless human-machine inter-action. One of the most successful class of methods for face recognition is the class of appearance-based [11] methods. In these methods, facial images are often represented lex-icographically as vectors in a high dimensional space and a lower dimensional linear subspace which captures certain properties of the dataset is constructed. Then, linear dimen-sionality reduction is invoked in order to project both train-ing and test data in the lower dimensional space. Recog-nition is then performed among the projected data in the reduced space using a simple classifier such as the near-est neighbor classifier. Well known methods of this class include Eigenfaces [9], Fisherfaces [1] and Laplacianfaces [5, 4].

Eigenfaces employ Principal Component Analysis (PCA) [10] for constructing the linear subspace, which is usually called face space. PCA aims at preserving the global structure and seeks orthogonal axes of maximum variance. These are obtained by computing t he principal eigenvectors of the sample covariance matrix. PCA is appropriate when the data samples (approximately) lie on a linear subspace. However, it has been observed that the manifold of facial images is intrin sically nonlinear [7] and this can render PCA ineffective in capturing facial images manifolds. The method of Fisherfaces employs Linear Discriminant Analysis (LDA) [10] for computing the dimensionality re-duction matrix. Its basis is to compute a set of directions which are optimal for discriminating information. These directions are obtained by solving a generalized eigenprob-lem and as a result they are not mutually orthogonal.
Note that both PCA and LDA consider only the Eu-clidean structure and do not take into account the data topol-ogy. Recently, a method named Laplacianfaces was intro-duced which models explicitly the data topology, by means of a weighted graph. It was shown [5] that Laplacianfaces is able to capture the nonlinear structure of the image manifold and yield an effective method for face recognition. Lapla-cianfaces builds a linear subspace for face representation, which is designed to preserve the locality of the data sam-ples. Similarly to Fisherfaces, the dimensionality reduction matrix is obtained by solv ing a generalized eigenproblem which involves the Laplacian matrix of the graph and hence, the resulting axes are not mutually orthogonal.

OPRA -faces, the method proposed in this paper, also models explicitly the data topology by a weighted graph. The graph used by OPRA -faces expresses in some least-squares sense, each point as a convex combination of a few nearest neighbors. This weight ed graph can be viewed as an optimal representation of the intrinsic neighborhood ge-ometries and it is computed in a manner that is identical with the method of Locally Linear Embedding (LLE) [7, 8]. In this paper we refer to this graph as the affinity graph . A major difference with the standard LLE where the map-ping between the input and the reduced spaces is implicit, is that OPRA -faces is an appearance-based method which employs an explicit linear mapping between the two. The embedding of LLE is defined only on the training points and it is cumbersome to extend it to handle new data samples. In contrast, treating new data sa mples is straightforward in our algorithm, as this amounts to a simple linear transformation. The proposed method shares some properties with Laplacianfaces, since they both rely on a k-nearest neigh-bor graph in order to capture the data topology. How-ever, our algorithm inherits the characteristics of LLE in preserving the geometric st ructure of local neighborhoods, while Laplacian-faces aims at pr eserving only locality with-out specifically aiming at preserving the geometry. Exper-iments suggest that it is important to try to preserve the affinity graph when one uses nearest neighbor classifiers for recognition in the reduced space. An additional advantage of OPRA -faces is that it employs mutually orthogonal axes in contrast with Laplacianfaces, where the projection axes are not orthogonal. OPRA -faces is able to capture the non-linear structure of the manifold and experimental evidence suggests that it is an effective method for face recognition.
This section gives a brief review of the most repre-sentative related methods: Eigenfaces [9], Fisherfaces [1], and Laplacianfaces [5, 4]. Consider a collection of fa-cial images represented by the columns of a matrix X = [ x 1 ,x 2 ,...,x n ]  X  R m  X  n ,wherethe i -th image is lexico-graphically represented by the data sample x i . All the above methods are characterized by a common framework. First, they compute a dimensionality reduction matrix V .Next, this matrix is used for projecting the data samples onto the reduced space by computing y i = V x i ,i =1 ,...,n .Fi-nally, recognition is performed in the reduced space (among y  X  X ) using a simple classifier. The methods are differenti-ated by the way in which the matrix V is determined.
The method of Eigenfaces employs PCA to deter-mine V . In PCA, the matrix V is computed such that the variance of the projected vectors is maximized i.e, constraints V V = I . It turns out that the column vectors of the solution V to this problem are the principal eigenvec-tors of the sample covariance matrix [10].

Fisherfaces determines V by using Linear Discrimi-nant Analysis (LDA). LDA works by extracting a set of  X  X ptimal X  discriminating axes. Assume that we have c classes and that class i has n i data points. Define the between-class scatter matrix S B = c i =1 n i (  X  ( i  X  )(  X  ( i )  X   X  ) and the within-class scatter matrix S W the centroid of the i -th class. In LDA the columns of V are the eigenvectors associated with largest eigenvalues of the generalized eigenvalue problem S B w =  X S W w .
Laplacianfaces [5] constructs the weighted k nearest neighbor ( k -NN) graph and builds a similarity matrix S , whose entry S ij represents the edge weight between nodes x and x j . The authors in [5] propose the use of Gaussian weights, where S ij = e  X  x i nearest neighbors of x j (orviceversa)and0otherwise.The selection of the parameter  X  is crucial for the performance of the algorithm. Laplacianfaces employs the following ob-jective function which is identical with that of the method of Laplacian Eigenmaps [2], a nonlinear technique for dimensionality re-duction. The main difference with Laplacian Eigenmaps is that Laplacianfaces is linear and employs an explicit linear mapping X  X  Y . The objective function (1) captures the locality of the data samples and results in the generalized eigenproblem where D = i S ij and L = D  X  S is the Laplacian matrix. The eigenvectors of the above problem corresponding to the smallest eigenvalues yield the dimensionality reduction ma-trix V used by Laplacianfaces.
The process of OPRA -faces consists of two parts. The first part is identical with that of LLE [7, 8] and consists of computing some optimal weights in each neighborhood. The basic assumption is that each data sample along with its k nearest neighbors (approximately) lies on a locally linear manifold. Hence, each data sample x i is reconstructed by a linear combination of its k nearest neighbors. The recon-struction errors are minimized via the objective function The weight W ij represent the linear coefficient for recon-structing the sample x i from its neighbors { x j } .Thefol-lowing constraints are imposed on the weights: 1. W ij =0 ,if x j is not one of the k nearest neighbors of 2. j W ij =1 ,thatis x i is approximated by a convex
Note that the optimization problem (3) can be recast in matrix form as min W X ( I  X  W ) F ,where W is an n  X  n sparse matrix which has a specific sparsity pat-tern (condition (1)) and satisfies the constraint that its row-sums be equal to one (condition (2)). The weights for a specific data point x i are computed as follows. Define C containing the pairwise i nner products among the neighbors of x i , given that the neighbors are centered with respect to x i . It can be shown that the weights of the above con-strained least squares problem are given in closed form [7] using the inverse of C , where w i represents the i -th column of W . The weights W ij satisfy certain optimality proper ties. They are invariant to rotations, scalings, and translations. As a consequence of these properties the affinity graph preserves the intrinsic geometric characteristics of each neighborhood.

Consider now the second part of projecting the data sam-ples X to the reduced space Y =[ y 1 ,y 2 ,...,y n ]  X  R d OPRA -faces imposes an explicit linear mapping from X  X  Y such that y i = V x i ,i =1 ,...,n for an appropri-ately determined matrix V  X  R m  X  d . In order to determine the matrix V , OPRA -faces imposes the constraint that each data sample y i in the reduced space is reconstructed from its k neighbors by exactly the same weights as in the input space. This leads to the solution of the following optimiza-tion problem, where we set M =( I  X  W )( I  X  W ) and  X  M = XMX min Algorithm OPRA-FACES
Input : Dataset X  X  R m  X  n and d : dimension of reduced space, k : number of NN, c : number of classes and : class labels.

Output : Dimensionality reduction matrix V  X  R m  X  d and projected vectors Y =[ y 1 ,y 2 ,...,y n ]  X  R d  X  n . 1. Employ PCA projection on X to reduce dimension 2. Compute the k nearest neighbors of each data point 3. Compute the weights using equation (4) that give 4. Compute the projected vectors If we impose the additional constraint that the columns of V are orthonormal, i.e. V V = I , then the solution V to the above optimization problem is the basis of the eigenvectors associated with the d smallest eigenvalues of  X  M . We ob-served in practice that ignoring the smallest eigenvector of  X  M is helpful. This is an issue to be investigated in future work. Note that the embedding vectors of LLE are obtained by computing the eigenvectors of the matrix M associated with its smallest eigenvalues.

Consider now a new facial test point x t which must be recognized. The test vector is projected onto the subspace y t = V x t using the dimensionality reduction matrix V . Next, it is compared to the training samples y i ,i =1 ,...,n and recognition is performed using a nearest neighbor (NN) classifier based on the Euclidean distance.
OPRA -faces can be implemented in either a supervised or an unsupervised setting. In the supervised case where the class labels are available, OPRA -faces can be modi-fied appropriately and yield a projection which carries not only geometric information but discriminating information as well. The method starts by building the affinity graph G =( N, E ) , where the nodes N correspond to data sam-ples and an edge e ij =( x i ,x j ) exists if and only if x x j belong to the same class. In other words, we make ad-jacent those nodes (data samples) which belong to the same class. Notice that in this case one does not need to set the pa-rameter k , the number of nearest neighbors, so the method becomes fully automatic.

Denote by c the number of classes and n i the number of data samples which belong to the i -th class. The data graph G consists of c cliques, since the adjacency relationship be-tween two nodes reflects their class relationship. This im-plies that with an appropriate reordering of the columns and rows, the weight matrix W will have a block diagonal form where the size of the i -th block is equal to the size n i i -th class. In this case W will be of the following form, The weights W i within each class are computed in the usual way, see eq. (4). The rank of W defined above, is restricted as is explained by the following proposition.
 Proposition 1 The rank of W is at most n  X  c .
 Proof Recall that the row sum of the weight matrix W i is equal to 1, because of the constraint (2). This implies that W i e i =0 ,e i =[1 ,..., 1] vectors  X  are linearly independent and belong to the null space of W . Therefore, the rank of W is at most n  X  c .

Consider now the case m&gt;n where the number of samples ( n ) is less than their dimension ( m ). This case is known as the undersampled size problem and occurs very often in face databases. A direct consequence of the above proposition is that in this case, the matrix  X  M  X  R m  X  have rank at most n  X  c . In order to ensure that the result-ing matrix  X  M will be nonsingular, we may employ an ini-tial PCA projection that reduces the dimensionality of the data vectors to n  X  c .Call V PCA the dimensionality re-duction matrix of PCA. Then the OPRA -faces algorithm is performed and the total dimensionality reduction matrix is given by V = V PCA V OPRA , where V OPRA is the dimen-sionality reduction matrix of OPRA -faces. The main steps of the OPRA -faces algorithm are summarized in Table 1.
PCA and LDA are traditional linear techniques which consider only the Euclidean structure. They do not take into account the nonlinear structure of the image mani-folds. On the other hand OPRA -faces and Laplacianfaces explicitly model the data structure and topology by means of a weighted k -NN graph. Moreover, PCA and LDA are global methods which do not aim at preserving locality. On the other hand, OPRA -faces and Laplacianfaces aim at preserving local geometry and locality respectively. This last feature is very important especially when one performs recognition in the reduced space using NN classifier (as is usually done in appearance-based methods).

OPRA -faces shares some properties with Laplacian-faces. Note that the former inherits the optimal weights from LLE which represent the intrinsic local geometries. In contrast, Laplacianfaces aims at preserving only local-ity and does not consider the geometric structure explicitly. Thus, the geometric structure of the neighborhoods in the reduced space may be perturbed. Note also that the Gaus-sian weights used in Laplacianfaces are somewhat artificial and may not reflect the underlying geometry. In addition, the selection of the parameter  X  , the width of the Gaussian envelope, is crucial for the performance of the algorithm. This issue is often overlooked, but it is an important weak-ness associated with the use of Gaussian weights. The su-pervised version of OPRA -faces is fully automatic. Indeed, the only parameter, the number of nearest neighbors k ,is implicitly determined by the corresponding class size n i Finally, the dimensionality reduction matrix V of OPRA -faces has orthonormal columns. This is very helpful in pre-serving angles as much as possible in the reduced space. This is to be contrasted with Laplacianfaces where the ma-trix V is not orthogonal since its columns are eigenvectors of a generalized eigenproblem. We demonstrate the advantageous characteristics of OPRA -faces over the other methods by applying it on two popular artificial datasets: the s-curve and the swissroll [7, 8]. The results are illustrated in Figure 1. We uni-formly sample n =1 , 000 data points from the s-curve and the swissroll and the discretized manifold is illus-trated in the left panels. The number of neighbors is k =12 . Each data point is projected in the two-dimensional space using the corresponding dimensionality reduction matrix V of each algorithm. Observe that OPRA -faces preserve lo-cality (indicated by the color shading) since nearby points in the input space are mapped nearby in the output two di-mensional space. In addition, notice that the angles are pre-served as much as possible and the projection at the reduced space is faithful and conveys meaningful information about how the manifold is folded in the higher dimensional space.
Figure 1. Results of applying all methods on the s-curve and the swissroll .Fromleftto right: OPRA -faces, Laplacianfaces and PCA. We used two datasets that are publically available: UMIST [3], and AR [6]. For computational efficiency the images in both databases were downsampled to size 38  X  31 . Thus, each facial image was represented lexicographically as a high dimensional vector of length 1,178. In order to measure the recognition performance, we use a random sub-set of facial expressions/poses from each subject as training set and the remaining as test set. In order to ensure that our results are not biased from a specific random realization of the training/test set, we perform 20 different random real-izations of the training/test sets and we report the average error rate.

Note that in what follows, we test with the supervised version of OPRA -faces (see Section 4 for more details) and Laplacianfaces. In the latter algorithm, we employ Gaus-sian weights. We determine the value of the width  X  of the Gaussian envelope as follows. First, we sample 1000 points randomly and then compute the pairwise distances among them. Then  X  is set equal to half the median of those pair-wise distances. This gives a good and reasonable estimate for the value of  X  .
The UMIST database [3] contains 20 people under dif-ferent poses. The number of different views per sub-ject varies from 19 to 48. We used a cropped version of the UMIST database that is publically available from S. Roweis X  web page 1 . Figure 2 illustrates a sample subject from the UMIST database along with its first 20 views. We form the training set by a random subset of 15 different poses per subject (300 images in total) and use the remain-ing poses as a test set. We experiment with the dimension of the reduced space d = [10 : 5 : 70] (in MATLAB nota-tion) and for each value of d we plot the average error rate across 20 random realizations of th e training/set set. The re-sults are illustrated in Figure 3. Concerning the method of Fisherfaces note that there are only c  X  1 generalized eigen-values, where c is the number of subjects in the dataset. Thus, d cannot exceed c  X  1 and so we plot only the best achieved error rate by Fisherfaces across the various val-ues of d . Observe that OPRA -faces outperforms the other methods across all values of d . We also report the best er-ror rate achieved by each method and the corresponding di-mension d of the reduced space. The results are tabulated in Table 2. Both Eigenfaces and OPRA -faces perform very well, with OPRA -faces showing a clear margin of superi-ority over the other methods. contains 126 subjects under 8 d ifferent facial expressions and variable lighting conditions for each individual. Fig-ure 4 depicts two subjects randomly selected from the AR database under various facial expressions and illumination. We form the training set by a random subset of 4 different facial expressions/poses per subject and use the remaining 4 as a test set. We plot the error rate across 20 random real-izations of the training/test set, for d = [30 : 10 : 100] .The results are illustrated in Figure 3. Observe that OPRA -faces
Figure 3. Error rate with respect to the re-duced dimension d . Top panel: UMIST database and bottom panel: AR database. outperforms its counterparts across all values of d .Alsoit seems that Laplacianfaces compete with Fisherfaces. Fur-thermore, Table 2 reports the best achieved error rate and the corresponding value of d . Again, OPRA -faces outper-forms its competitors.
OPRA -faces, a fully automatic face recognition algo-rithm aims at preserving the affinity graph, i.e., the local ge-ometries of the data samples in the high dimensional space. The method is able to capture the nonlinear features of the dataset by means of the affinity data graph. OPRA -faces was tested for face recognition using a few well known, and extensively studied, facial dat abases and was shown to out-performe three popular rival methods on these test cases.
Figure 4. Sample face images from the AR database.
 [1] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigen-[2] M. Belkin and P. Niyogi . Laplacian eigenmaps for di-[3] D. B Graham and N. M Allinson. Characterizing Vir-[4] X. He and P. Niyogi. Locality preserving projections. [5] X. He, S. Yan, Y. Hu, P. Niyogi, and H-J Zhang. [6] A.M. Martinez and R. Benavente. The AR Face [7] S. Roweis and L. Saul. Nonlinear Dimensionality [8] L. Saul and S. Roweis. Think Globally, Fit Locally: [9] M. Turk and A. Pentland. Face Recognition using [10] A. Webb. Statistical Pattern Recognition . Wiley, 2nd [11] W. Zhao, R. Chellapa, P. Phillips, and A. Rosenfeld.
