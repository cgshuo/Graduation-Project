 The success of any web portal or social network service (SNS) lies with the num-ber of users registered with that service. Registration with a SNS requires a user to explicitly input his/her personal information which is termed as user profile. The information contained in a user profile is more or less static. Other than the user X  X  personal information, the systems are more interested in users X  preferences or users X  interests that they can use for recommendation of their services. Search engines and e-commerce portals mine user logs to generate user preferences or a profile of users X  interests. We define such profile as the User Interest Profile ( UIP ); it consists of terms or topics that represent users X  interests. The information in a UIP , unlike user profile, is dynamic; it keeps changing with time.

Current research studies[1,2,3,5,6,7] highlight the use of folksonomies for con-structing and managing UIP . A very simplistic approach, to build a UIP ,could be to collect all the tags that the user input to annotate the resources of interest. The frequency of tag usage, its tag weight, indicates the importance of a tag in the UIP .A UIP is defined as a set of tags and their tag-weights. Undoubtedly, a
UIP is immensely useful to the web portals for product recommendation, tag suggestion, search results re-ranking, query expansion, etc. Nonetheless, there is still some scope of improvement that we discuss next from Noll et al.[1] that uses folksonomies for building the UIP and use it for personalized search. Authors mention that the search result URL of US Security and Administration is pro-moted, by re-ranking based on the terms in the UIP , even though it is not related to the input query keyword security . There exists no established reasoning that can explain the quantitative effect of the terms, in the UIP , on the re-ranking of the search results, i.e., why does the re-ranking process, based on the UIP , promotes a particular URL more than the other URL? We offer the following reasoning. Some terms, in the UIP , even though not related to the user query security , but because they are present in the UIP , contributes to the re-ranking score of the search results.The term, in this case insurance in the UIP , has a false positive effect on the re-ranking of the URL of US Security and Administration; this is because of the incapability of the system to judge the context of user query. Note that the terms in the UIP can have, uncalled for, a false positive effect or a false negative effect on the re-ranking of the search results.
In our endeavour to build the CUIP , we propose two approaches: svdCUIP based on Singular Value Decomposition (SVD); modsvdCUIP based on a variation of SVD termed as modSvd. A CUIP is defined as a collection of clusters, where each cluster consists of related terms a nd their term weights. Determining the right contextual cluster from user X  X  CUIP for the given user query and using that cluster for re-ranking search results, or recommending products would result in more user satisfaction. It will also help in disambiguating the context of user query by finding related terms to the user query from the clusters in user X  X  CUIP . We run experiments to evaluate which of the two methods have the acceptable or strong clustering tendency and best clustering accuracy. The results obtained in our experiments show that modsvdCUIP clearly outperforms svdCUIP .
The rest of the paper is organized as follows. In the next section, we discuss about the current research work that uses folksonomy for building the CUIP . Section 3 presents the 2 pr oposed approaches and their evaluation in Section 4. Finally, we conclude this work in Section 5 followed by references. Two key papers exist in the literature that are based on building term cluster clusters for modelling user profile. Shepstein et al.[6] clusters the entire tag space of a folksonomy system to obtain sets of semantically related tags. It models users and resources as vectors over the set of tags. The proposed modified Hierarchical algorithm selects a subset of potential c lusters related to user X  X  current naviga-tional activity and use that cluster for recommendation. This way, clusters serve as an intermediate between users and resources. This algorithm has high time complexity, because it calculates the user  X  X  interest in each cluster, and each re-source X  X  closest clusters-both computations are based on a variation of Jaccard coefficient. Note that the number of clusters could be in millions. Our proposed approaches are centred on user; we propose to discover clusters in user X  X  UIP . Second difference between our work and their work is the use of Latent Semantic Analysis (LSA) for generating similarity matrix. Authors in their future work mentioned that they would like to analyse other approaches like LSA. They also mentioned that they would like to cluster tags in user profiles. This is exactly what we are presenting in this work.

Simpson et al.[7] approach to build CUIP uses cosine similarity measure for discovering term-term similarity matrix. For clustering,authors have used graph based approaches, such as centrality and betweenness. We are using Hierarchical Clustering Algorithm (HCA) that works best when the number of clusters are unknown apriori . Moreover, no experiments appear to have been carried out that could verify clustering accuracy, rather they provided some example visualization of cluster structure. We report a more de tailed set of experiments that measures the clustering tendency and clustering accuracy of CUIP . This section describes two methods for gen erating Clustered User Interest Profile ( 3.1 User X  X  Context and Term Weighting This work uses Twitter 1 as a source for obtaining terms of user X  X  interest. Tweets inputted by a user can be thought of as texts of user X  X  interest. The first step involves processing tweets to obtain ter ms by removing stop words, stemming, and extracting nouns 2 .

Let Tw be a set of tweets inputted by a user, and let T be a list of terms obtained after processing Tw . A user X  X  context Tw,T,R is defined as a 3-tuple, where R is a binary relation between Tw and T . In order to express that a tweet 1  X  Tw is in a relationship with a term t  X  T ,wewrite tItw 1 or ( t, tw 1 )  X  I , which can be read as  X  X he term t is a topic of tweet tw 1  X . Focusing only on the relations between tweets and terms i n Table 1, based on a dummy example, a user X  X  context in Table 2 is derivable. N ote that, in a real scenario, the actual term-values may be greater than 1. This section describes how the terms and their term weights in a UIP are combined, aggregated, and normalized.Each term t in a tweet tw i has some weight w ( t, tw i ) defined as term weight. For ex: the term-weight of term java in tweet tw 1 is 1. Term weight of a term in the CUIP is aggregated from from multiple tweets; it is very much possible that the same term may originate from multiple tweets, each with a potentially different weight. We use the standard result set fusion technique to aggregate the weight where w ( t, tw i ) is the term-weight of the term t retrieved from the tweet tw i .For example, the UIP for the user context in Table 2 would be { java:2, game:1, appli-cation:2, travel:1, iPhone:2 } . Using raw frequency of term-weight is problematic; extremely popular terms are used many times, so their term weights are really high. To circumvent this limitation, we propose to normalize the term-weights of terms using the following equation nw ( t i ,up )= w ( t i ,up ) 3.2 Latent Semantics in User Interest Profile Latent semantics means those hidden relationship between terms that exists but is not explicitly visible. We propose to build a system that could discover related terms, even though the terms are not identical or do not belong to the same tweet. The approaches to establishing latent structures in a UIP are based on the assumption that more similar terms are more closely related. In this section, we propose two similarity measures to calculate the similarity between terms, and using them for the clustering algorithm.
 Term-Term Similarity -Co-occurrence similarity derives similarity between two or more terms that belong to the same tweet. The degree of their relationship is calculated using the frequency of term co-occurrence, called as 1 st order co-occurrence similarity. Another type of co-occurrence similarity is 2 nd order co-occurrence similarity that derives the similarity between two terms that do not belong to the same tweet, but they have a common term that relates to both of them. For example, refer Table 2 the term application is related to term iphone in tweet tw 5 ,andthesameterm application is also related to term java in tweet 1 ; this suggests an indirect relationship or 2 nd order co-occurrence relationship between terms iphone and java . It is analogous to finding friend of a friend and also quantifying the degree of friendship relationship.
 The proposed methods employ matrix factorization on user X  X  context or term-Tweet matrix to discover 1 st order and 2 nd co-occurrence similarity between terms. Latent Semantic Analysis (LSA)[4] uses a matrix factorization technique, Singular Value Decomposition (SVD), to find hidden relationship between terms which is not apparent, otherwise. In the preliminary step, the user context in Table 2 is represented as the term by Tweet matrix, let it be A. The SVD technique decomposes th e term-tweet matrix, A , into three matrices, A = USV T ,: a term by dimension matrix, U ; a diagonal matrix of singular values, S ; and, a tweet by dimension matrix V . SVD translates the term and tweet vectors into a space determined by the rank r of matrix A . When we select the k largest singular values from S and their corresponding singular vectors from U and V ,wegetthe rank k approximation A k , A k = U k S k V k T . To compute the term-term similarity matrix, we would like to compute, U k , a low-rank approximation of U matrix. The term-term similarity matrix, Sim k , is computed using Sim k = U k S k ( U k S k ) T =
S k S k T U k T = U k S 2 k U k T . Determining the value of k is an essential part of generating a good similarity matrix and hence a good cluster structure; we show, in the experiment section, how to d etermine the appropriate value of k that has the strongest or acceptable clustering t endency. The example below shows the term-term similarity matrix sim , generated using SVD, for the user context in Table 2.
 This example is based on a small dummy corpus, therefore the sparseness is low (approx. 40%). In real scenarios, sparseness could be as high as 99% which seriously effects the ability of SVD to calculate the similarity between terms. Our experiments vouch for this statement. To rightly capture the similarity matrix, we go a step further by calculating the co sine similarity between term vectors of similarity matrix sim produced from the SVD process. This method is called as modded Svd (modSvd). Each term vector represents the projection of a term in the term space. For instance, each term t i in similarity matrix sim has an entry for each term t j that co-occurs with it. The similarity matrix modSim calculates the similarity between all pair of term vectors in the entire term space of user; this results in discovering 2 nd order co-occurrence similarity between terms. The following example shows the modSim matrix for the matrix Sim , illustrated above, calculated using equation 1. The higher value of cosine signifies grea ter overlap between two vectors across n dimensions, where n is the number of terms in the term space of the user. Clustering -The input to the clustering algorithm is the similarity matrix calculated in the present section. The output of the clustering algorithm is the clusters of semantically related terms. Since, the number of clusters are unknown before hand, we use Hierarchical Clustering Algorithm (HAC) which works best when the number of clusters are unknown, apriori . At the outset, HAC treats each term as a singleton cluster and then successively merge pair of clusters until all clusters have been merged into a singl e cluster that contain all the terms. The output of HAC is a hierarchy of clusters. We don X  X  need a single hierarchy of cluster, refer fig 1, but l number of clusters. Note that, the value of l is not specified apriori . This requires cutting the single cluster; We use distinctness, , as a measure to cut the single hierarchy of clusters to obtain l clusters. For instance, Table 3 shows the clusters in svdCUIP and modsvdCUIP generated using HAC for sim and modSim matrix illustrated above. svdCUIP has 4 clusters, it fails to identify that the terms iphone and game should belong to one cluster, whereas modsvdCUIP didn X  X  make that mistake. It is very important to choose the right value of d that could generate appropriate clu sters matching user X  X  perspective. Fig. 1 shows the role of distinctness parameter d ;if d&gt; =1 . 4 , it would result in only 1 cluster that consist of complete hierarchy of terms. Instead, if d =0 . 4 , then there would be 3 clusters. For value of d&lt; 0 . 3 , there would be a plain list of terms. The experiment section shows how to determine the right value of d , to generate crisp clusters, without com promising on the clustering accuracy. The CUIP that results from the application of HAC on Sim matrix obtained by applying SVD on term-tweet matrix is called as svdCUIP (read as, SVD based CUIP). Whereas, the CUIP that results from the application of HAC on modSim matrix obtained by calculating the cosine similarity of every pair of term vectors in Sim matrix is called as modsvdCUIP (read as, modSVD based CUIP).
 The experiments in this section e xamine the clustering accuracy of svdCUIP and modsvdCUIP . In particular, it addresses the following issues: How does the dimen-sionality reduction parameter k and distinctness parameter d effects clustering tendency and clustering accuracy?; Which of the two proposed methods have better clustering accuracy? The data se t is created from the tweets posted by 10 volunteers during the period July to September 2010, refer Table 4. Unlike other evaluations, it is hard to to measu re the accuracy of clusters because for clustering no body knows what the corr ect clusters are. The users themselves are the best judge. To establish the ground truth, we asked each user to group related terms that were extracted from his/her tweets. Generating ground truth manually for evaluation is the normal procedure used in many research papers[9]. Since this process is subjective, we take the average of the scores from all the users as the final score of the clustering. The manual procedure to generate ground truth is obviously a labour intensive and time consuming task; this is the primary reason that forced us to only experiment with a small set of users. For each user, 2 sets of several CUIPs were generated. In each set, a CUIP is generated for each combination of value of dimension reduction parameter k and distinctness parameter d . The value of k varies from 10 to 110, in steps of 10. This results in 11 sim k and 11 modSim k similarity matrices. Similarly, the dis-tinctness parameter d is initialized to 0.03, and it increases in steps of 0.02 until 0.13, after which it increases in steps of 0.1 until 0.93 (total of 14 values). This resulted in a total of 154 svdCUIPs and the same number of modsvdCUIP s .The objective of experiments is to determine the value of k and d that has the most accurate and acceptable clu ster structure gen erated using each method. Table 4 displays the characteristics of collected tweets and similarity matrix. The aver-age number of collected tweets are 134, and the average number of terms are 621. On average, 62.8% of values in the similarity matrix sim are less than or equal to 0, whereas, the average number of values less than or equal to 0 in the similarity matrix modSim are 47.53%. Also, the average number of values greater than 0.75 in the similarity matrix, sim , are 0.335% compared to 7.2% in the similarity matrix modSim . Overall, the number of values less than or equal to 0.5 in the similarity matrix sim are 99.6%. On the other hand, the number of values less than or equal to 0.5 in the similarity matrix, modSim , is 88.5%. These results suggest that the similarity matrix, modSim , is less sparse than the simi-larity matrix, Sim .Wechoose silhouettecoefficient (unsupervised evaluation) [8] to judge the cluster tendency,and FScore (supervised evaluation) to compare the clustering accuracy. A very useful over all quality measure of a given clustering is its average silhouette coefficient, interpretation of average silhouette coefficient, support of cluster structure: the value of idence, between ]0.5 ,0.7] suggests reasona ble evidence, ]0.25, 0.5] suggests weak evidence, and between [-1, 0.25] suggests no evidence. 4.1 Evaluating Clustering Tendency The experiments in this s ection determine, for both svdCUIP and modsvdCUIP , the various value(s) of dimensionality reduction parameter k and distinctness parameter d that show(s) strong clustering tendency. The assessment of pres-ence of clusters in a data set is an important step in cluster analysis. The plot of average silhouette coefficient vs. numb er of clusters in Fig. 2 helps in visual-izing cluster tendency, if any. Fig. 2 shows that that modSvdCUIP has stronger evidence of cluster tendency, whereas svdCUIP exhibits weak or no evidence of clustering tendency. We observed that the clustering tendency in a user X  X  CUIP is affected by the ratio of number of zero val ues to the number of positive values; the lower the better. The average ratio for modsvdCUIP is 0.9, and for svdCUP is 1.68. The maximum and minimum ratio for modsvdCUIP is 3.2 and 0.6, re-spectively. Whereas, the maximum and minimum ratio for svdCUIP is 6.2 and 1.0, respectively. These numbers explain the reason behind the lack of cluster tendency in svdCUIP .
The plot also depicts that the value of average silhouette coefficient ( creases as the number of clusters incr eases beyond 50. This suggests that the best cluster structure is obtained when the number of clusters are less than 50. This is acceptable because the average number of terms is 594, which could pos-sibly result in 30-50 clusters. However, what is surprising is that even when the number of clusters is less than 10, the plot shows strong clustering tendency. To try to find the natural number of clusters in a user X  X  CUIP , one should look for a knee, a peak, or dip in the plot of aver age silhouette coefficient vs. number of clusters [10]. The plot for modsvdCUIP clearly shows a rise and then a dip, and hence a peak, occurring around when the number of clusters are in the range of 30-40. However, it is difficult to find a peak for svdCUIP , because there are several such peaks.
 Fig. 4 presents 3-dim plot that shows how the value of average silhouette coeffi-cient changes as the value of k and d changes. The average silhouette coefficient vs. k and d plot for svdCUIP exhibits a clear pattern; for low values of k and no matter what the value of d is, there is no evidence of clustering tendency. For high values of k , close to 100 and 110, and low values of d , there is a reasonable evidence of clustering tendency. svdCUIP shows reasonable clustering tendency for high values of k =100 and small values of d , this implies that the dimension reduction step decreases the clustering tendency in this case. In other words, it also means, CUIP based on SVD performs best when the number of dimensions encompasses the whole feature space and not a reduced space.

The average silhouette coefficient vs. k and d plot for modsvdUIP exhibits a clear pattern; unlike svdCUIP , modsvdCUIP plot exhibits a strong evidence of clustering tendency for intermediate values of k = 30 and 40, and small values of d . It ascertains the fact th at increasing the value of d decreases the clustering tendency. The modSvdCUIP overcomes the limitation of sparseness of similar matrix, and hence get benefited from the dimension reduction step. 4.2 Clustering Accuracy This experiment aids in determining the appropriate value of d for the cluster structure that has the highest accuracy . Fig. 3 shows the clustering accuracy for each method. It can be clearly seen that modsvdCUIP has better clustering accuracy than svdCUIP . The average clustering accuracy for modsvdCUIP and svdCUIP cluster structure is 0.58 and 0.16, respectively; there is a 244% in-crease in average clustering accuracy. T his indicates that the cluster structure produced by modsvdCUIP is more accurate than the c luster structure produced by svdCUIP . When the method is modSvd , the dimension reduction parameter =30 has better clustering accuracy than k =40. Also, the difference in clustering accuracy between k =30 and k =40 is very small. Moreover, both the curves fol-low the same pattern, which means the clustering accuracy estimated by modSvd for k =30 and k =40 is nearly identical with a slightly better performance for =30. The highest clustering accuracy for modsvdCUIP is 0.75, which is obtained when k =30 and distinctness parameter d =0.07. The highest clustering accuracy for svdCUIP is 0.55 which is obtained when k =110 and distinctness parameter =0.03. We proposed 2 approaches to build Clustered User Interest Profile (CUIP) from user X  X  tweets: SVD based CUIP, ( svdCUIP ), is generated by applying Hierarchical Agglomerative Clustering (HAC) on the term-term similarity matrix computed using SVD; modded Svd based CUIP, modsvdCUIP , is generated by applying HAC to the second order co-occurrence matrix modSim obtained from comput-ing the cosine similarity be tween the term-vectors in the sim matrix. The experi-ments show that the svdCUIP fails to cluster semantica lly related terms, however it can, to some extent, cluster co-located terms. To circumvent this limitation, we proposed to generate the cluster structure modsvdCUIP . Experimental results suggest that the cluster structure modsvdCUIP has higher clustering tendency and clustering accuracy as comp ared to the clusters structure svdCUIP .The poor clustering tendency of the cluster structure svdCUIP is due to the spar-sity(92%) of similarity matrix sim . The best clustering accuracy(0.75) for the cluster structure modsvdCUIP is achieved for dimension reduction parameter k = 30 and distinctness parameter d = 0.07. In contract, the cluster structure svdCUIP showed best clustering accura cy(0.57), though lower than modsvdCUIP , for k = 110 and d = 0.03. In the future, we would like to put the CUIP into practice with a search system to develop personalized search.

