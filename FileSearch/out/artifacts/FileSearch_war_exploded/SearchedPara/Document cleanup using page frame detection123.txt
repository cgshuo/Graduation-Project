 ORIGINAL PAPER Faisal Shafait  X  Joost van Beusekom  X  Daniel Keysers  X  Thomas M. Breuel Abstract When a page of a book is scanned or photo-copied, textual noise (extraneous symbols from the neighbor-ing page) and/or non-textual noise (black borders, speckles, ...) appear along the border of the document. Existing docu-ment analysis methods can handle non-textual noise reason-ably well, whereas textual noise still presents a major issue for document analysis systems. Textual noise may result in undesired text in optical character recognition (OCR) out-put that needs to be removed afterwards. Existing document cleanup methods try to explicitly detect and remove marginal noise. This paper presents a new perspective for document image cleanup by detecting the page frame of the document. The goal of page frame detection is to find the actual page contents area, ignoring marginal noise along the page bor-der. We use a geometric matching algorithm to find the opti-mal page frame of structured documents (journal articles, books, magazines) by exploiting their text alignment prop-erty. We evaluate the algorithm on the UW-III database. The results show that the error rates are below 4% for each of the performance measures used. Further tests were run on a dataset of magazine pages and on a set of camera cap-tured document images. To demonstrate the benefits of using page frame detection in practical applications, we choose OCR and layout-based document image retrieval as sample applications. Experiments using a commercial OCR system show that by removing characters outside the computed page frame, the OCR error rate is reduced from 4.3 to 1.7% on the UW-III dataset. The use of page frame detection in layout-based document image retrieval application decreases the retrieval error rates by 30%.
 Keywords Document analysis  X  Marginal noise removal  X  Document pre-processing 1 Introduction Paper positioning variations is a class of document degra-dations that results in skew and translation of the page con-tents in the scanned image. Document skew detection and correction has received a lot of attention in last decades and several skew estimation techniques have been proposed in the literature (for a literature survey, please refer to [ 1 ]). However, estimating the global position of the page has been largely ignored by the document analysis community. This is perhaps due to the fact that most of the layout analysis meth-ods are robust to global translation of the page and would producethesamesegmentationofthepagefordifferenttrans-lations as long as all page contents are visible. Hence the OCR output is usually not affected by global translation of the page. This effect can be seen in Fig. 1 , where a page seg-mentation algorithm is shown to correctly identify the page segments irrespective of the translation of the page in each image.

Different amount of noise can be present along the border of a document image depending on the position of the paper on the scanner. Figure 1 shows the effect of paper positioning variations on the amount of marginal noise in the resulting scanned image. In general, marginal noise along the page border can be classified into two broad categories based on its source:  X  non-textualnoise(blackbars,speckles,...)resultingfrom  X  textual noise coming from the neighboring page
An example image showing textual and non-textual noise along the page border is shown in Fig. 2 .

The most common approach to deal with non-textual noise is to perform document cleaning by filtering out connected components based on their size and aspect ratio [ 2  X  4 ]. This usually works out quite well in removing black bars and isolated specks. However, when characters from the adja-cent page are also present, they cannot be filtered out using this approach. Therefore, state-of-the-art page segmentation algorithms report a number of false alarms originating from textual noise regions [ 5 ]. When these textual noise regions are fed to a character recognition engine, extra characters appear in the output of the OCR system along with the actual contents of the document. Hence the edit distance between the OCR output and the ground-truth text increases resulting in decreased OCR accuracy.

Textual noise can be avoided altogether by scanning only the page contents area. Typical desktop scanners come with a graphical user interface to allow the users to conveniently mark the region to be scanned. This allows the user to man-ually select the page frame during document scanning. The resulting document image is then free of textual noise. How-ever, if alargenumber of documents havetobescanned, man-ually defining the page frame for each one of them becomes quite cumbersome.

Researchers have also tried to explicitly detect and remove marginal noise in scanned documents. For example, Le et al. [ 6 ] have proposed a rule-based algorithm using several heuristics to detect the page borders. The algorithm relies upon the classification of document rows and columns into blank, textual or non-textual classes. Then, an analysis of projection profiles and crossing counts is done to detect the marginal noise. Their approach is based on the assump-tion that the marginal noise is very close to the edges of the image and borders are separated from image contents by a large whitespace, i.e. the borders do not overlap the edges of an image content area. However, this assumption is often violated when pages from a thick book are scanned (see Fig. 6 ). Avila et al. [ 7 ] and Fan et al. [ 8 ] propose tech-niques for removing non-textual noise overlapping the page content area, but do not consider textual noise removal. Cinque et al. [ 9 ] propose an algorithm for removing both textual and non-textual noise from grayscale images based on image statistics like horizontal/vertical difference vectors and row luminosities. However, their method is not suitable for cleaning binary images. Also, their approach is very sen-sitive to the amount of noise present in the document image and the error rates increases monotonically with the artifact area.

Other more recent attempts for border noise removal were by Peerawit et al. [ 10 ] and Stamatopoulos et al. [ 11 ]. Both these approaches try to remove textual and non-textual noise from document images. The approach in [ 10 ] tries to identify bordersofnoiseregionsbasedonananalysisoftheprojection profiles of the edges in the image. Their technique is based on the observation that non-textual marginal noise areas have much higher density of edges than normal text. Again, this observation may not hold for all documents (see Fig. 9 ). The approach in [ 11 ] tries to detect page borders based on an analysis of the projection profiles of the smeared image com-bined with a connected component labeling process. They have demonstrated their technique on flat camera captured documents. A common feature of these techniques is that they try to design some rules to detect noisy regions along the page border. However, in practice such rules tend to work only on a small collection of documents or on documents cap-tured under similar scanning conditions. None of the above mentionedapproacheshasbeentestedonlargepubliclyavail-able datasets. So it is hard to judge their performance under real-world circumstances.

This paper presents a new approach for dealing with paper positioning variations in scanned documents. Instead of iden-tifying and removing noisy components themselves, the pro-posed method focuses on identifying the actual content area. This is accomplished by using a geometric matching algorithm. Including page frame detection as a document pre-processing step can help to increase OCR accuracy by removing textual noise from the document. Also in applica-tions like document image retrieval based on layout infor-mation [ 12 ], noise regions result in incorrect matches. Using the page frame to reject zones originating from noise can therefore reduce the retrieval error rates.

Our method for page frame detection takes advantage of the structure in a printed document to locate its page frame. This is done in two steps. First, a geometric model is built for the page frame of a scanned document. Then, a geometric matching method is used to find the globally optimal page frame with respect to a defined quality function.
The use of geometric matching for page frame detection has several advantages. Instead of devising carefully crafted rules, the page frame detection problem is solved in a more general framework, thus allowing higher performance on a more diverse collection of documents. Additionally, the use of geometric model for page frame detection makes the pre-sented approach very robust to the amount of noise present in a document image and can find the page frame even if noise overlaps some regions of the page content area.

Part of the work presented in this paper was published in [ 13 ] for timely dissemination of this work. This paper is a substantially extended version of the previous conference publication.

The rest of this paper is organized as follows. Section 2 describes in detail the method for page frame detection. In Sect. 3 , several error measures to evaluate the performance of a page frame detection algorithm are proposed. Section 4 presents the experimental protocol and discusses the results obtained, followed by the conclusion in Sect. 5 . 2 Geometric matching for page frame detection 2.1 Document model For structured documents, like technical journals and busi-ness letters, the structure can be described as a hierarchy, where entities at each level of the hierarchy represent a partic-ular level of information, like zones, text-lines, or connected components. Different hierarchical models representing doc-ument structure have been proposed in the past [ 14  X  16 ]. One common shortcoming of these models is that they only repre-sent the contents of the document and do not specify how to represent textual and/or non-textual noise added to the docu-ment by the photocopying or scanning process. For instance, in the hierarchical model of Liang et al. [ 15 ], all polygonal regions in the page are organized in a hierarchy of zones, text blocks, text-lines, etc. and a reading order is defined between them. However, in the presence of regions consist-ing of noise, it is not clear how the reading order should be defined among the zones in the page. To address this prob-lem, we extend the definition of the hierarchical document model in this work and another add another level of hierarchy that represents the actual page content area. In this way it is then possible to define a unique reading order of zones within the page content area, ignoring textual and non-textual noise along the page border. The definitions of different levels of the hierarchy are as follows.  X  A binary document image D is defined as the union of  X  The set of foreground pixels can then be partitioned into  X  X hesetof text-lines L ={ L 1 ,..., L N } is viewed as a  X  X hesetof zones Z ={ Z 1 ,..., Z R } is defined such that  X  X he page frame F is defined as the minimum rectangle
Note that other levels of the hierarchy are also possible (e.g. word-level, character-level), but the above-mentioned levels are sufficient to describe a document for the purpose of page frame detection.

In order to extract the document structure at different lev-els of the hierarchy, the page frame detection system uses a different algorithm at each level. A fast labeling algorithm is used to extract connected components from the docu-ment image. The constrained text-line finding algorithm [ 3 ] is used to extract text-lines, whereas the Voronoi-diagram based algorithm [ 17 ] is used to extract zones from the doc-ument. These algorithms were chosen since recent evalua-tions of page segmentation algorithms [ 5 , 18 ] show that they work well on standard document collections like UW-III. The text-line extraction algorithm was used with a high threshold for the quality of the extracted text-lines to avoid text-lines generated from non-textual noise components. We used the implementations of connected component analysis and text-line extraction algorithms from the OCRopus open source OCR system [ 19 ], and the implementation of the Voronoi algorithm from the PSET toolkit [ 20 ]. 2.2 Page frame model The page frame of a scanned document is parameterized as a rectangle described by five parameters  X  ={ l , t , r , The parameters { l , t , r , b } represent the left, top, right, and bottom coordinates, respectively, whereas  X  represents the skew angle of the page frame. The page frame detection sys-tem takes skew corrected documents as input; standard skew correction methods [ 21 , 22 ] can be used for this purpose. Hence, the page frame is modeled as an axis-aligned rectan-gle, described by four parameters  X  ={ l , t , r , b } .Giventhe sets of connected components C , text-lines L , and zones Z , the goal of page frame detection is to find the maximizing set of parameters  X  with respect to the sets C , L , and Z :  X   X (
C , L , Z ) := arg max where Q ( X , C , L , Z ) is the total quality for a given parameter set, and T is the parameter space. The design of the quality function is described in detail in Sect. 2.3 ,followedbythe description of the algorithm for finding the optimal set of parameters in Sect. 2.4 . 2.3 Design of quality function The design of the quality function in Eq. ( 1 ) is done by exploiting the text-alignment property of structured docu-ments. In such documents, text-lines are usually printed in justified or left-aligned style. Hence, a large number of con-nected components are aligned with the page frame of the document. At first glance, it may seem like a good idea to use the number of character bounding boxes touching the page frame as the quality of the page frame. The charac-ter bounding boxes could be obtained from C by filtering out noise and non-text components based on their area and aspect ratio. However, such an approach does not work well in practice because: 1. The top and bottom text-lines do not necessarily contain 2. The parameters l and r can only be reliably estimated for
Therefore, instead of using connected component level information, text-lines can be used. The quality function can then be a function of the number of text-lines that touch the page frame from inside. Based on this idea, the parame-ters  X  can be decomposed into two parts:  X  h ={ l , r } and  X  v ={ t , b } . Although  X  h and  X  v are not independent, such a decomposition can still be done because of the nature of the problem. First, the parameters  X  v are set to their extreme values ( t = 0 , b = H where H is the page height) and then optimal  X  h is searched. This setting ensures that none of the candidate text-lines is lost based on its vertical position in the image. The decomposition not only helps in reducing the dimensionality of the searched parameter space from four to two, but also prior estimates for  X  h make the estimation of  X  v a trivial task, as will be seen later in Sect. 2.5 . Hence the optimization problem of Eq. ( 1 ) is reduced to  X   X  ( L ) := arg max The total upper bound of the quality Q can be written as the sum of local quality functions Q ( X 
An upper and lower bound for local quality function q is computed. Given a line bounding box  X  L ={ x 0 , y 0 , x from the parameter intervals l and r , respectively, are deter-mined. The local quality function q for a given line and a parameter range  X  h can then be defined as q ( X  where defines the distance up to which a text-line can con-tribute to the page frame. Text-lines may have variations in their starting and ending positions within a text column depending on text alignment or paragraph indentation. A value of = 150 pixels is used in this work in order to cope with such variations for documents scanned at 300-dpi. This quality function alone already works well for single column documents, but for multi-column documents it may report a single text-column (with the highest number of text-lines) as the optimal solution. In order to discourage such solutions, a negative weighting for text-lines on the  X  X rong X  side of the page frame (that is x 1 of a text-line contributing to parameter l or x 0 of a text-line contributing to parameter r ) is introduced in the form of the quality function q ( X  The overall local quality function is then defined as q ( X  The quality function in Eq. ( 6 ) will yield the optimal parame-ters for  X  h even if there are intermediate text-columns with larger number of text-lines. However, if the first or last col-umn contains very few text-lines, the column can possibly be ignored. The search space for the parameters  X  h can be limited to certain regions of the document image to solve this problem. In this work the value of the parameter l was constrained to lie within the first half of the page, whereas the value of the parameter r was limited to the second half of the page. 2.4 Branch-and-bound optimization The RAST (Recognition by Adaptive Subdivision of Trans-formation Space) technique [ 23 ] is employed to perform the maximization in Eq. ( 2 ). RAST is a branch-and-bound algo-rithm that guarantees to find the globally optimal parame-ter set by recursively subdividing the parameter space and processing the resulting parameter hyper-rectangles in the order given by an upper bound on the total quality. During the search, each partition of the search space is described by a Cartesian product of intervals for the parameters, i.e. a set of the form T =[ l 0 , l 1 ] X [ r 0 , r 1 ] . The upper bound on the quality of the page frame with parameters in the rectangular region T is calculated using interval arithmetic [ 24 ]. Given a computation of an upper bound on the quality, the search can be organized as follows (for details see [ 23 , 25 ]): 1. Pick an initial region of parameter values T such that it 2. Maintain a priority queue of regions T i , where the upper 3. Remove a region T i from the priority queue; if the upper 4. If the region is small enough to satisfy the accuracy 5. Otherwise, split the region T i along the dimension fur-
This algorithm will return the parameter set that maxi-mizes the quality of the match function in Eq. ( 2 ). To make the approach practical and avoid duplicate computations, a match-list representation [ 23 ] is used. That is, with each region kept in the priority queue in the algorithm, a list (the match-list) of all and only those text-lines is maintained that havethepossibilitytocontributewithanon-zerolocal quality to the global quality. These match-lists shrink with decreas-ing size of the regions T i . It is easy to see that the upper bound of a parameter space region T i is also an upper bound for all subsets of T i . Hence, when a region is split in Step 5 , the text-lines in the children that have already failed to con-tribute to the quality computation in the parent never have to be reconsidered. Thus the match-lists can be reused in the children thereby allowing a very fast computation of quality for the children. 2.5 Parameter refinement The RAST algorithm returns the optimal parameters for  X  h in terms of mean square error with respect to the quality function in Eq. ( 3 ). However, if the text is not aligned in the justified style or if different paragraphs have different indentation, parameters  X  h returned by the RAST algorithm may cut through some text-lines as shown in Fig. 3 .Sothe parameters are refined to adjust the page frame according to different text alignments. If the bounding box of a text-line overlaps with the page frame by more than half of its area, the page frame parameters  X  h are expanded to include the complete text-line, as shown in Fig. 3 .

The use of match-lists gives the list of text-lines bound-ing boxes which contributed positively to the quality func-tion Q ( X  h , L ) . All these text-lines are sorted with respect to the top of each text-line X  X  bounding box ( y 0 ). This gives an initial estimate for parameters  X  v by simply setting t = min ( y 0 , j ), j = 1 ,..., N and b = max ( y 1 , j ), j = 1 ,..., N . A page frame detected in this way is shown in Fig. 3 . Although the detected page frame is correct for most of the documents, it fails in these cases: 1. If there is a non-text zone (images, graphics, logo, ...) 2. If there is an isolated page number at the top or bottom An example illustrating these problems is shown in Fig. 4 .In order to estimate the final values for  X  v ={ t , b } , document zones are used as given by the Voronoi algorithm [ 17 ]. The Voronoi algorithm performs document cleaning as a part of zoning process and successfully removes most of the non-textual noise. The output of the Voronoi algorithm for an example image is shown in Fig. 4 . Textual noise usually appears only along the left or the right side of the docu-ment. Based on this observation, filtering is performed on the zones obtained by the Voronoi algorithm, such that all the zones that lie completely inside, or do not overlap hori-zontally with the detected page frame are removed. Then, all of the remaining zones are included into the page frame. An example result is shown in Fig. 4 . 3 Performance measures To determine the accuracy of the presented page frame detec-tion algorithm, performance measures are needed that not only reflect the accuracy of the algorithm, but also quantify its usefulness in practical document analysis systems. There-fore, the error measures are categorized into two parts. 3.1 Page frame detection accuracy The goal of the performance measures in this section is to determine the accuracy with which the page frame is located. Previous approaches for marginal noise removal [ 6  X  9 ]use manual inspection to decide whether noise regions have been completely removed or not. Then, the error rate is defined as the percentage of documents on which the noise was not completely removed. While these approaches might be use-ful for small scale experiments, an automated way of eval-uating border noise removal is needed for evaluation on a large sized dataset. In the following, performance measures based on area overlap, connected components classification, and ground-truth zone detection are introduced to evalu-ate different aspects of the presented page frame detection algorithm. 3.1.1 Area overlap Let F g be the ground-truth page frame and F d be the detected page frame. Then the area overlap between the two page frames can be defined as A = 2
The amount of area overlap A will vary between zero and one depending on the overlap between ground-truth and detected page frames. If the two page frames do not overlap at all A = 0, and if the two page frames match perfectly, i.e. | F g  X  F d |=| F g |=| F d | , then A = 1. This gives a good measure of how closely the two page frames match. However, the area overlap A does not give any hints about the errors made by the algorithm. Secondly, a small error like including a noise zone near the top or bottom of the page into the page frame may result in a large error in terms of area overlap. To evaluate the page frame detection algorithm in more detail, a performance measure based on connected component classification is defined. 3.1.2 Connected components classification Defining components detected as lying within the page frame as  X  X ositive X , the performance of page frame detection can be measured in terms of four quantities:  X  X rue positive X ,  X  X alse positive X ,  X  X rue negative X , and  X  X alse negative X . The error rate can then be defined as the ratio of incorrectly classified con-nected components to the total number of connected compo-nents.

The error measure based on classification of connected componentsgivesequalimportancetoallcomponents,which may not be desired. For instance, if the page number is not included in the detected page frame, the error rate will still be very low because page number comprises a very small fraction (typically about 0.03 X 0.1%) of the total number of connected components in the page frame. However, the page number carries important information for the understanding of the document. To compensate this shortcoming, a perfor-mance measure based on detection of ground-truth zones is introduced. 3.1.3 Ground-truth zone detection For the zone-based performance measure, three different val-ues are determined:  X  Totally in: Ground-truth zones lying completely inside  X  Partially in: Ground-truth zones lying partially inside the  X  Totally out: Ground-truth zones lying totally outside the Using this performance measure, the  X  X alse negative X  detec-tions are analyzed in more detail. Since, the page numbers are considered an independent zone, missing page numbers will have a higher impact on the error rates in this performance measure. 3.2 Performance gain in practical applications In order to demonstrate the usefulness of page frame detec-tion in practical applications, we chose OCR and layout-based document image retrieval applications. 3.2.1 OCR accuracy The OCR accuracy is determined by the percentage of char-acters correctly recognized in a document image. Many extra characters (false alarms) may appear in OCR output if tex-tual noise is present in the document. Current commercial OCR systems have their own noise removal techniques to deal with marginal noise. The edit distance [ 26 ] between the OCR output and the ground-truth text is used as the error measure for determining OCR accuracy. Edit distance is the minimum number of point mutations (insertion, deletions, and substitutions) required to convert a given string into a target string. The goal of performance measure based on edit distance is to determine whether the performance of existing OCR systems improves if page frame detection is used as a pre-processing step. 3.2.2 Layout-based document image retrieval In layout-based retrieval, the purpose is to query document image databases by layout, in particular by measuring the similarity of different layouts in comparison to a reference or query layout. Blocks originating from marginal noise result in incorrect matches, thereby increasing the error rates of the retrieval system. Different layout analysis or page segmen-tation algorithms use different methods to deal with noise in a document image. The goal of this performance measure is to determine the decrease in retrieval error rates when page frame detection is used as a pre-processing step. 4 Experiments and results The evaluation of the page frame detection algorithm was done on the University of Washington III (UW-III) data-base [ 27 ]. The dataset was divided into 160 training and 1,440 test images. In order to make the results replicable, every tenth image (in alphabetical order) from the dataset was included into the training set. Hence the training set consists of images A00A, A00K, ..., W1UA. The training images were used to design the quality function (Sect. 2.3 ) and to find suitable values for parameters (e.g. ). The post-processing steps (Sect. 2.5 ) were also introduced based on results on the training images to cope with different layout styles and the presence of non-textual content at the top or bottom of a page image.

The evaluation of our page frame detection system was done on the remaining 1,440 test images. Some examples of page frame detection for documents from the UW-III dataset are shown in Fig. 5 . Figure 6 shows an example where mar-ginal noise overlaps with some text-lines at the bottom of the page. The use of page frame detection successfully detects the page contents region and removes the border noise from the image while keeping the page contents intact. 4.1 Page frame detection accuracy The evaluation of page frame detection on the basis of over-lapping area (Eq. 7 ) showed a page frame detection accuracy of 91%. An inspection of the UW3 ground-truth page frame showed that it does not tightly enclose the page contents area as shown in Fig. 7 . Hence, the correct page frame of docu-ments in the test set was computed by finding the bounding box of all ground-truth zones for each document. Testing with the corrected ground-truth page frame gave an overall mean area overlap of 96%. In the following, when mention-ing the ground-truth page frame, this corrected ground-truth page frame is meant.

The result for the connected component based measure is given in Table 1 . The high percentage of true positives shows that the page frame mostly includes all the ground-truth components. The percentage of true negatives is about 73.5%, which means that a large part of noise components are successfully removed. The results for the N th genera-tion photocopies show that the percentage of true negatives goes down to 42.8% which may lead to the conclusion that the computed page frames for this subset are typically bigger than the ground-truth page frame. The total error rate defined as the ratio of  X  X alse X  classifications to the total number of connected components is 1.6%. Since the test set contains only 19 images in the N Gen category, the total results do not reflect the performance on such severely degraded doc-uments. A detail study of the performance of the proposed page frame detection method on documents with different noise levels is presented later in this section.
 TheresultsforthezonebasedmeasurearegiveninTable 2 . Compared to the number of missed connected components, it can be seen that the percentage of missed zones is slightly higher than the corresponding percentage of false negatives on the connected component level. One conclusion that can be drawn from this observation is that the zones missed do not contain a large number of components, which is typically true for page numbers, headers and footers of documents. These zones have a few components and therefore do not contribute much to the mean false negative errors on the connected com-ponent level. In some cases, the text-line finding algorithm merges the text-lines consisting of textual noise to those in the page frame. In such cases, a large portion of textual noise is also included in the page frame.

A box plot of the run times of different steps of the pro-posed method is shown in Fig. 8 . The execution times were computed on an AMD Athlon 1.8GHz machine running Linux. The worst case running time of the unmodified RAST algorithm is exponential in the problem size [ 28 ]. However, in practice such a case rarely appears. For page frame detec-tion, the RAST algorithm took less than 10ms. per page on the average. Hence if it is integrated with a document analysis systemthatalreadycomputestextlinesandzones,pageframe detection will not add a significant increase in the computa-tion time. When page frame detection is used as a monolithic system for document image cleanup, the total running time is of interest, which is 3 X 4s per page.

A particular advantage of our method is that it is robust to skew between the main page frame and the textual noise. An example image is shown in Fig. 9 where our method success-fully finds the main page content area despite the presence of textual noise with a different skew angle. Also note that there is very little non-textual noise in this page. Therefore the assumption by Peerawit et al. [ 10 ] that noisy regions can be separated from text regions based on edge density, will not work here.

In order to quantify the amount of marginal noise in a doc-ument image, the noise ratio of a document image is defined as Noise ratio = where n pb is the number of foreground pixels outside the ground-truth page frame, and n p is the total number of fore-ground pixels in a document image. A histogram of the noise level of the documents in the test set is shown in Fig. 10 . Inter-estingly, there are many documents with noise levels above 50%. The mean error rate obtained for each of these noise level based document categories is plotted in Fig. 11 .The plot shows that the algorithm works well even on documents with very high amount of noise. The error rates on all three performance measures used are below 10% for noise levels up to 80%.

Some limitations of the presented page frame detection algorithm were also revealed during the course of evalua-tion. Although the algorithm works very well for most of the layouts even under large amount of noise, yet for a few lay-outs the algorithm does not give 100% result even for noise-free documents. This happens for documents with very few text-lines beside the margin of the document and there is no text-line that spans across the main content area and the page margin. In this case, these text-lines lie completely outside the computed parameters  X  h (Eq. 2 ). So the parameter refine-ment step (Sect. 2.5 ) fails to include these text-lines into the page frame. To deal with such layouts, the quality function can be modified to include an offset between the page frame parameters and the main content area of the page. 4.2 Performance gain in practical applications The use of page frame detection in an OCR system showed significant improvement in the OCR results. For this pur-pose Omnipage 14 X  X  commercial OCR system X  X as cho-sen. The ground-truth text provided with the UW-III dataset has several limitations when used to evaluate an OCR sys-tem. First, there is no text given for tables. Secondly, the formatting of the documents is coded as latex commands. When an OCR system is tested on this ground-truth using error measures like the Edit distance, the error rate is unjustly too high. Also, our emphasis in this work is on the improve-ment of OCR errors by using page frame detection, and not on the actual errors made by the OCR system. Hence, the UW-III documents are first cleaned using the ground-truth page frame, and then the output of Omnipage on the cleaned images was used as the ground-truth text. This type of ground-truth gives us an upper limit of the performance of a page frame detection algorithm, and if the algorithm works perfectly, it should give 0% error rate, independent of the actual error rate of the OCR engine itself.
 First, OCR was performed on the original images and the Edit distance to the estimated ground-truth text was com-puted. Then, the computed page frame was used to remove marginal noise from the documents, and the experiments was run again. The results (Table 3 ) show that the use of page frame detection for marginal noise removal reduced the OCR error rate from 4.3 to 1.7%. The insertion errors are reduced by a factor of 2.6, which is a clear indication that the page frame detection helped in removing a lot of extra text that were treated previously as part of the document text. There are also some deletion errors, which are a result of the changes in the OCR software X  X  reading order determination. One example is shown in Fig. 12 for which the reading order changed after document cleaning.

The effect of using page frame detection on the perfor-mance of a layout based document image retrieval applica-tion showed a significant decrease in retrieval error rates. van Beusekom et al. [ 12 ] introduced several similarity measures for layout based document image retrieval. They evaluated the performance of these similarity measures on the MARG database.Theexperimentsshowedthatthebestdistancemea-sure for this task is the overlapping area combined with the Manhattan distance of the corner points as block distance together with the minimum weight edge cover matching for establishing correspondences between the matched layouts. The documents in the MARG database are categorized with respect to 9 different layout types, 59 publishers, and 161 journals. Given a query document, the target is to retrieve a document of the same class based on layout information only. The error rates are then determined as the percentage of correctly retrieved documents using leave-one-out cross validation.

In this work retrieval experiments were performed both with and without using page frame detection. Since each method for page segmentation has a different way of deal-ing with noise, four well-known page segmentation algo-rithms were compared for use in layout based retrieval: the X X  X  cut [ 29 ], Docstrum [ 4 ], whitespace analysis [ 2 ], and the Voronoi-diagram based approach [ 17 ].

In the first experiment, the document images were used directly for page segmentation without any page frame detection. The blocks extracted from the documents were then used for the purpose of layout based retrieval.
In the second experiment, the document images in the database were cleaned by performing page frame detection and removing all the foreground pixels outside the detected page frame. Following the document cleaning, page seg-ments were extracted from the cleaned images and then retrieval experiment was repeated. The decrease in error rates for each of the three subdivisions of the data set (according to type, publisher, and journal) was used as a performance measure.

The use of page frame for layout-based document image retrieval resulted in lower error rates on all three classes of layouts for each algorithm as shown in Table 4 . These results show that the Voronoi-diagram based approach performs bet-ter than other algorithms both with and without page frame detection. The use of page frame detection with the Voronoi algorithm lowers the retrieval error rates by 4% for the cor-rect journal, 30% for the correct type, and 20% for the correct publisher. These results clearly demonstrate the usefulness of page frame detection in practical applications. 4.3 Experiments on magazine pages The UW-III dataset contains scanned pages from journal articles with Manhattan layouts. In order to test the perfor-mance of our approach on non-Manhattan layouts that are more representative of layouts in magazines, we chose the ICDAR 2007 page segmentation contest dataset [ 30 ]. The dataset contains 23 magazine pages, among which 6 are in thetrainingsetand17areinthetestset.Weusedall23images as test images and used our method without any parameter tuning on these images. Some of the results are shown in Fig. 13 . It can be seen that the page frame detection works correctly for most of these pages. In Fig. 13 a X  X  our algorithm found the correct page frame without making any error. In Fig. 13 d one vertical text-line was missed by our algorithm since it was not detected by the text-line detection algorithm. Another error is shown in Fig. 13 e where the last column consisted only of a few lines and hence was excluded from the detected page frame. Note that in Fig. 13 d, margin notes were included in the computed page frame because the title lines were spanning across the main text content area and the margin notes area. A similar error is shown in Fig. 13 f where the last column consists only of images and hence was not included in the page frame.

To evaluate the performance of our algorithm quantita-tively, we used the ground-truth zone detection measure (see Sect. 3.1.3 ). Since ground-truth information was not avail-able, we counted the number of text-lines in each document and calculated the percentage of totally in, partially in, and totally out text-lines. The results are shown in Table 5 .The results show that the page frame detection correctly included 98.23% text-lines from the test images. The partially missed lines were those with inverted text and were on the top of the page. The text-line extraction algorithm detected only parts of these text-lines and hence the remaining part was not included in the detected page frame. The missed errors were due to margin notes and isolated vertical lines. 4.4 Extension to camera-captured documents In this experiment we extended the proposed approach to work on camera-captured documents. A document captured with a hand-held camera undergoes several distortions like page curl or perspective distortion. In the case of perspective distortion, the page frame can be represented by four straight lines. Under page curl distortion, the top and bottom of the page frame get distorted with respect to the amount of curl on the page, whereas the geometry of the left and right borders is usually not affected. Based on this observation, we focus on adapting the proposed method to find left and right page frame border on camera captured documents as these are needed to remove textual noise from neighboring pages.
To find the left and right border of the page, the preceding method was modified as follows:  X  X he model is adapted from { l , t , r , b } to {  X  l , o  X  X s feature points for the skew-corrected rectangular case,  X  the quality function was modified to use the length of a  X  the parameter refinement step was confined to adjusting We evaluated our method on the publicly available CBDAR 2007 document dewarping dataset [ 32 ] consisting of 10 training images and 92 test images. The training images were used to develop the algorithm and to test different para-meters. For these documents, = 50 was found to be a good choice. Evaluation based on the percentage of totally in, partially in, and totally out text-lines showed that 95.6% of the text-lines were completely inside, 2.3% text-lines were partially inside, and 2.1% text-lines were completely outside the page frame. Some example results are shown in Fig. 14 . Our method correctly found the page frame in Fig. 14 a X  X  despite the large variety of page contents and border noise. In Fig. 14 d one text-line was not completely included in the page frame since its center was outside the computed page frame. 5 Conclusion In this paper we presented an algorithm for page frame detection using a geometric matching method. The presented approach does not assume the existence of whitespace between marginal noise and the page frame, and can detect the page frame even if the noise overlaps some regions of the page content area. Several error measures were defined based on area overlap, connected component classification, and ground-truth zone detection accuracy for determining the accuracy of the presented page frame detection algo-rithm. It was shown that the algorithm performs well on all three performance measures with error rates below 4% in each case. It was also demonstrated that the presented method can handle documents with a very large amount of noise with reasonable accuracy. The error rates on all three performance measures used are below 10% for noise levels up to 80%. The major source of errors was missing iso-lated page numbers. Locating the page numbers as a sep-arate process and including them in the detected page frame may further decrease the error rates. The benefits of the page frame detection in practical applications were highlighted by using it with an OCR system and a layout-based docu-ment image retrieval system, where it showed a significant decrease in the error rates in both applications. We have released an open source implementation of our page frame detection algorithms as part of the OCRopus open source OCR system.
 References
