 Biological data mining aims to extract signi can t informa-tion from DNA, RNA and proteins. The signi can t infor-mation ma y refer to motifs, functional sites, clustering and classi cation rules. This pap er presen ts an example of bio-logical data mining: the classi cation of protein sequences using neural net w orks. W e prop ose new tec hniques to ex-tract features from protein data and use them in com bina-tion with the Ba y esian neural net w ork to classify protein sequences obtained from the PIR protein database main-tained at the National Biomedical Researc hF oundation. T o ev aluate the p erformance of the prop osed approac h, w ecom-pare it with other protein classi ers built based on sequence alignmen t and mac hine learning metho ds. Exp erimen tal re-sults sho w the high precision of the prop osed classi er and the complemen tarit y of the to ols studied in the pap er. Bioinformatics, biological data mining, mac hine learning, neural net w orks, sequence alignmen t, feature extraction from protein data As a result of the Human Genome Pro ject and related ef-forts, DNA, RNA and protein data accum ulate at an accel-erating rate. Mining these biological data to extract useful kno wledge is essen tial in genome pro cessing. This sub ject has recen tly gained signi can t atten tion in the data mining comm unit y[8]. W e presen t here a case study of the sub-ject: the application of neural net w orks to protein sequence classi cation.
 The problem studied here can b e stated formally as follo ws. Giv en are an unlab eled protein sequence S and a kno wn sup erfamily F . W e w an t to determine whether or not S b elongs to F . (W e refer to F as the tar get class and the set of sequences not in F as the non-target class.) In general, a sup erfamily is a group of proteins that share similarit y in structure and function. If the unlab eled sequence S is detected to b elong to F , then one can infer the structure and function of S . This pro cess is imp ortan t in man y asp ects of computational biology . F or example, in drug disco v ery , if sequence S is obtained from some disease X and it is determined that S b elongs to the sup erfamily F , then one ma ytrya com bination of the existing drugs for F to treat the disease X . F rom a one-dimensional p oin tof view, a protein sequence con tains characters from the 20-letter amino acid alpha-bet A = f A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y g . An imp ortan t issue in applying neu-ral net w orks to protein sequence classi cation is ho wtoen-co de protein sequences, i.e., ho w to represen t the protein sequences as the input of the neural net w orks. Indeed, se-quences ma y not be the best represen tation at all. Good input represen tations mak e it easier for the neural net w orks to recognize underlying regularities. Th us, go o d input repre-sen tations are crucial to the success of neural net w ork learn-ing.
 W e prop ose here new enco ding tec hniques that en tail the ex-traction of high-lev el features from protein sequences. The b est high lev el features should b e \relev an t". By \relev an t," w e mean that there should b e high m utual information b e-tw een the features and the output of the neural net w orks, where the m utual information measures the a v erage reduc-tion in uncertain t y ab out the output of the neural net w orks giv en the v alues of the features.
 Another w a y to lo ok at these features is that they capture b oth the global similarit y and the lo cal similarit y of protein sequences. The global similarit yrefers totheo v erall simi-larit y among m ultiple sequences whereas the lo cal similarit y refers to motifs (or frequen tly o ccurring substrings) in the sequences. Sections 2 and 3 elab orate on ho w to nd the global and lo cal similarit y of the protein sequences. Section 4 presen ts our classi cation algorithm, whic h emplo ys the Ba y esian neural net w ork originated from Mac k a y [5]. Sec-tion 5 ev aluates the p erformance of the prop osed classi er. Section 6 compares our approac h with other protein classi-ers. Section 7 concludes the pap er.
 permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 T o calculate the global similarit y of protein sequences, w e adopt the 2-gram enco ding metho d prop osed in [9]. The 2-gram enco ding metho d extracts and coun ts the o ccurrences of patterns of t w o consecutiv e amino acids (residues) in a protein sequence. F or instance, giv en a protein sequence PVKTNVK , the 2-gram amino acid enco ding metho d giv es the follo wing result: 1 for PV (indicating PV o ccurs once), 2 for VK (indicating VK o ccurs t wice), 1 for KT , 1 for TN ,1for NV . W e also adopt the 6-letter exc hange group f e 1 ;e 2 ;e e g to represen t a protein sequence,where e 1 2f H ; R ; K g , e f D ; E ; N ; Q g , e 3 2 f C g , e 4 2 f S ; T ; P ; A ; G g , e e 6 2f F ; Y ; W g . Exc hange groups represen t conserv ativ ere-placemen ts through ev olution. F or example, the ab o v epro-tein sequence PVKTNVK can b e represen ted as e 4 e 5 e 1 The 2-gram exc hange group enco ding for this sequence is: 1 for e 4 e 5 , 2 for e 5 e 1 , 1 for e 1 e 4 ,1for e 4 e 2 F or eac h protein sequence, w e apply b oth the 2-gram amino acid enco ding and the 2-gram exc hange group enco ding to the sequence. Th us, there are 20 20 + 6 6 = 436 p ossible 2-gram patterns in total. If all the 436 2-gram patterns are c hosen as the neural net w ork input features, it w ould require man yw eigh t parameters and training data. This mak es it dicult to train the neural net w ork|a phenomenon called \curse of dimensionalit y ." Di eren t metho ds ha v e b een pro-p osed to solv e the problem b y careful feature selection and b y scaling of the input dimensionalit y . W e prop ose here to select relev an t features (i.e. 2-grams) b yemplo ying a dis-tance measure to calculate the relev ance of eac h feature. Let X b e a feature and let x be its v alue. Let P ( x j Class =1) and P ( x j Class = 0) denote the class conditional densit y functions for feature X , where Class 1 represen ts the tar-get class and Class 0 is the non-target class. Let D ( X ) denote the distance function b et w een P ( x j Class = 1) and P ( x j Class =0). The distance measure rule prefers feature X to feature Y if D ( X ) &gt; D ( Y ), b ecause it is easier to distinguish b et w een Class 1 and Class 0b y observing fea-ture X than feature Y . In our w ork, eac h feature X is a 2-gram pattern. Let c denote the o ccurrence n um b er of the feature X in a sequence S . Let l denote the total n um ber of 2-gram patterns in S and let len ( S ) represen t the length of S . W eha v e l = len ( S ) 1. De ne the feature v alue x for the 2-gram pattern X with resp ect to the sequence S as x = c= ( len ( S ) 1). F or example, supp ose S = PVKTNVK . Then the v alue of the feature VK with resp ect to S is 2/(7-1) =0.33.
 Because a protein sequence ma y b e short, random pairings can ha v e a large e ect on the result. D ( X ) can be ap-pro ximated b y the Mahalonobis distance [6] as D ( X ) = ( m tiv ely) are the mean v alue and the standard deviation of the feature X in the p ositiv e (negativ e, resp ectiv ely) train-ing dataset. The mean v alue m and the standard deviation d of the feature X in a set S of sequences are de ned as m =( x is the v alue of the feature X with resp ect to sequence S 2S , and N is the total n um b er of sequences in S . Let X 1 ;X 2 ;:::;X N (2-gram patterns) with the largest D ( X ) v alues. 1 In tu-itiv ely , these N g features o ccur more frequen tly in the p os-itiv e training dataset and less frequen tly in the negativ e training dataset. F or eac h protein sequence S (whetheritis a training or a test sequence), w e examine the N g features in S , calculate the feature v alues, and use the N g feature v al-ues as input feature v alues to the Ba y esian neural net w ork for the sequence S .
 T o comp ensate for the p ossible loss of information due to ignoring the other 2-gram patterns, a linear correlation co ef-cien t( LCC )bet w een the v alues of the 436 2-gram patterns with resp ect to the protein sequence S and the mean v alue of the 436 2-gram patterns in the p ositiv e training dataset is calculated and used as another input feature v alue for S . Sp eci cally , the LCC of S is de ned as: where x j is the mean v alue of the j th 2-gram pattern, 1 j 436, in the p ositiv e training dataset and x j is the feature v alue of the j th 2-gram pattern with resp ect to S . The lo cal similarit y of protein sequences refers to frequen tly o ccurring motifs in the protein sequences. Let T p = f S , S k g b e the p ositiv e training dataset. W e use a previously dev elop ed sequence mining to ol Sdiscover [7] to nd the reg-ular expression motifs of the forms X and X Y where eac h motif has length Len and appro ximately matc hes, within Mut m utations, at least Occur sequences in T p . Here, a m utation could be a mismatc h, an insertion, or a deletion of a letter (residue); Len , Mut , and Occur are user-sp eci ed parameters. X and Y are segmen ts of a sequence, i.e., substrings made up of consecutiv e letters, and is a v ariable length don't care (VLDC) sym b ol. The length of a motif is the n um b er of the non-VLDC letters in the mo-tif. When matc hing a motif with a sequence S i , a VLDC sym b ol in the motif is instan tiated in to an arbitrary n um ber of residues in S i at no cost. F or example, when matc hing a motif VLHGKKVL with a sequence MNVLAHGKKVLKWK , the rst is instan tiated in to MN and the second is instan ti-ated in to KWK . The n um ber of m utations b et w een the motif and the sequence is 1, represen ting the cost of inserting an A in the motif.
 Often, the n um b er of motifs returned b y Sdiscover is enor-mous. It's useful to dev elop a measure to ev aluate the signif-icance of these motifs. W e prop ose here to use the minim um description length (MDL) principle [3, 8] to calculate the sig-ni cance of a motif. The MDL principle states that the b est mo del (a motif in our case) is the one that minimizes the sum of the length, in bits, of the description of the mo del and the length, in bits, of the description of the data (the p ositiv e training sequences in T p in our case) enco ded b y the mo del. 1 Our exp erimen tal results sho wthatc ho osing N g 30 can yield a reasonably go o d p erformance pro vided one has suf-cien t (e.g. &gt; 200) training sequences. W e adopt information theory in its fundamen tal form[3, 8] to measure the signi cance of di eren t motifs. The theory tak es in to accoun t the probabilit y of an amino acid in a motif (or sequence) when calculating the description length of the motif (or sequence). Sp eci cally , Shannon sho w ed that the length in bits to transmit a sym bol b via a c hannel in some optimal co ding is log 2 P x ( b ), where P x ( b ) is the probabilit y with whic h the sym bol b o ccurs. Giv en the probabilit ydis-tribution P x o v er an alphab et x = f b 1 ;b 2 ;:::;b n calculate the description length of an y string b k o v er the alphab et x b y In our case, the alphab et x is the protein alphab et A con taining 20 amino acids. The probabilit y distribution P can b e calculated b y examining the o ccurrence frequencies of amino acids in the p ositiv e training dataset T p . One straigh tforw ard w a ytodescribe(or enco de) the sequences in T p , referred to as Sc heme 1, is to enco de sequence b yse-quence, separated b y a delimiter $. Let dlen ( S i ) denote the description length of sequence S i 2 T p . Then dlen ( S i n um b er of o ccurrences of a j in S i . F or example, supp ose S = MNVLAHGKKVLKWK is a sequence in T p . Then dlen ( S i )= ( log 2 P ( M )+ log 2 P ( N )+2 log 2 P ( V )+2 log 2 P ( L )+ log log 2 P ( H )+ log 2 P ( G )+4 log 2 P ( K )+ log 2 P ( W )) Let dlen ( T denote the description length of T p = f S 1 ;:::;S k g . If w e ignore the description length of the delimiter $, then the de-scription length of T p is giv en b y dlen ( T p )= Another metho d to enco de the sequences in T p , referred to as Sc heme 2, is to enco de a regular expression motif, sa y M and then enco de the sequences in T p based on M j . Sp ecif-ically , if a sequence S i 2T p can appro ximately matc h M then w e enco de S i based on M j . Otherwise w e enco de S using Sc heme 1. Let us use an example to illustrate Sc heme 2. Consider, for example, M j = VLHGKKVL . W eencode M j tion is allo w ed in matc hing M j with S i , and $0 is a delimiter to signal the end of the motif. Let bet f a 1 ;a 2 ;:::;a 20 ; ; $0 g , where a 1 ;a 2 ;:::;a amino acids. Let P 1 denote the probabilit y distribution o v er the alphab et cipro cal of the a v erage length of motifs. P 1 ( )= n ( P P ( a i )=(1 ( n +1) P 1 ($0)) P ( a i ), where n denotes the n um-b er of VLDCs in the motif M j . F or a motif of the form X , n is 2; for a motif of the form X Y , n is 3.
 Giv en P 1 ,w e can calculate the description length of a motif as follo ws. Let M j = a j note the description length, in bits, of the motif M j . Then, dlen ( M j )= (2 log 2 P 1 ( )+ log 2 P 1 ($0) + F or instance, consider again M j = VLHGKKVL . W e ha v e dlen ( M j )= (2 log 2 P 1 ( )+ log 2 P 1 ($0) + 2 log 2 P 2 log 2 P 1 ( L )+ log 2 P 1 ( H )+ log 2 P 1 ( G )+2 log that are appro ximately matc hed b y the motif M j can be enco ded with the aid of the motif. F or example, consider again M j = VLHGKKVL and S i = MNVLAHGKKVLKWK . M j matc hes S i with one m utation, represen ting the cost of in-serting an A in the third p osition of M j . The rst VLDC sym b ol is instan tiated in to MN and the second VLDC sym bol is instan tiated in to KWK .W e can th us rewrite S i as MN SS
KWK where SS i is VLAHGKKVL and denotes the concate-nation of strings. Therefore w e can enco de S i as M , N ,$1; 1, ( O I ,3, A ); K , W , K ,$1. Here $1 is a delimiter, 1 indicates that one m utation o ccurs when matc hing M j with S i and ( O
I , 3, A ) indicates that the m utation is an insertion that adds the letter A to the third p osition of M j . In general, the m utation op erations in v olv ed and their p ositions can b e ob-serv ed using appro ximate string matc hing algorithms. The description length of the enco ded S i based on M j , denoted dlen ( S i ;M j ), can b e calculated easily .
 Supp ose there are h sequences S p training dataset T p that can appro ximately matc h the mo-tif M j . The w eigh t (or signi cance) of M j , denoted w ( M is de ned as w ( M j )= dlen ( S p pro ximately matc hing M j and the less bits w e use to enco de M j and to enco de those sequences based on M j , the larger w eigh t M j has.
 Using Sdiscover , one can nd a set S of regular expression motifs of the forms X and X Y from the p ositiv etrain-ing dataset T p where the motifs satisfy the user-sp eci ed parameter v alues Len , Mut and Occur . W ec ho ose the top N l motifs with the largest w eigh t. Let R denote this set of motifs. Supp ose a protein sequence S (whether it is a train-ing sequence or a test sequence) can appro ximately matc h, within Mut m utations, m motifs in R . Let these motifs be M 1 ;:::;M m . The lo cal similarit y( LS )v alue of S ,de-noted LS ( S ), is de ned as LS ( S )=max 1 i m f w ( M i m 6 = 0; LS ( S ) = 0, otherwise. This LS v alue is used as an input feature v alue of the Ba y esian neural net w ork for the sequence S . Note that w e use the max op erator here to maximize discrimination. In general, p ositiv e sequences will ha v e large LS v alues with high probabilities and ha v e small LS v alues with lo w probabilities. On the other hand, negativ e sequences will ha v e small LS v alues with high prob-abilities and ha v e large LS v alues with lo w probabilities. W e adopt the Ba y esian neural net w ork (BNN) originated from Mac k a y [5] to classify protein sequences. There are N g +2 input features, including N g 2-gram patterns, the LCC feature describ ed in Section 2 and the LS feature de-scrib ed in Section 3. Th us, a protein sequence is represen ted as a v ector of N g + 2 real n um bers. The BNN has one hid-den la y er con taining m ultiple hidden units. The output la y er has one output unit, whic h is based on the logistic activ ation function f ( a )=1 = (1 + e a ). The BNN is fully connected bet w een the adjacen tla y ers.
 Let D = f x ( m ) ;t m g ; 1 m N , denote the training dataset including b oth p ositiv e and negativ e training sequences. is an input feature v ector including the N g + 2 input feature v alues, and t m is the binary (0/1) target v alue for the out-put unit. That is, t m equals 1 if x ( m ) represen ts a protein sequence in the target class, and 0 otherwise.
 Let x denote the input feature v ector for a protein sequence, whic h could b e a training sequence or a test sequence. Giv en the arc hitecture A and the w eigh ts w of the BNN, the output v alue y can be uniquely determined from the in-put v ector x . Because of the logistic activ ation function f ( a ) of the output unit, the output v alue y ( x ; w ; in terpreted as P ( t = 1 j x ; w ; A ), i.e., the probabilit y that represen ts a protein sequence in the target class giv en , A . The lik eliho o d function of the data D giv en the mo del is calculated b y P ( Dj w ; A )= N m =1 y t m (1 y ) 1 t exp ( G ( Dj w ; A )). Here G ( Dj w ; A ) is the cross-en trop yer-ror function, i.e. G ( Dj w ; A ) = t ) log (1 y ).
 The G ( Dj w ; A ) is the ob jectiv e function in a non-Ba y esian neural net w ork training pro cess and is minimized. This pro-cess assumes all possible w eigh ts are equally lik ely . The w eigh t deca y is often used to a v oid o v er tting on the train-ing data and p o or generalization on the test data b y adding a term w eigh t deca y parameter (h yp erparameter), sum of the square of all the w eigh ts of the neural net w ork, and q is the n um ber of w eigh ts. This ob jectiv e function is minimized to p enalize the neural net w ork with w eigh ts of large magnitudes. Th us, it p enalizes an o v er-complex mo del and fa v ors a simple mo del. Ho w ev er, there is no precise w a y to sp ecify the appropriate v alue of ,whic h is often tuned oine. In con trast, in the Ba y esian neural net w ork, the h y-p erparameter is in terpreted as the parameter of a mo del, and is optimized online during the Ba y esian learning pro-cess. W e adopt the Ba y esian training of neural net w orks describ ed in [5] to calculate and maximize the evidence of , namely P ( Dj ; A ). The training pro cess emplo ys an it-erativ e pro cedure; eac h iteration in v olv es three lev els of in-ference.
 In classifying an unlab eled test sequence S represen ted b y its input feature v ector x , the output of the BNN, P ( t = 1 j x ; w ; A ), is the probabilit y that S b elongs to the target class. If the probabilit y is greater than the decision b ound-ary 0.5, S is assigned to the target class; otherwise S is assigned to the non-target class. W e carried out a series of exp erimen ts to ev aluate the p er-formance of the prop osed BNN classi er on a P en tium I I PC running the Lin ux op erating system. The data used in the exp erimen ts w ere obtained from the In ternational Protein Sequence Database [2], release 62, in the Protein Informa-tion Resource (PIR) main tained b y the National Biomedi-cal Researc h F oundation (NBRF-PIR) at the Georgeto wn Univ ersit y Medical Cen ter. This database, accessible at sequences.
 F our p ositiv e datasets w ere considered; they w ere globin, ki-nase, ras, and ribitol sup erfamilies, resp ectiv ely , in the PIR protein database. The globin sup erfamily con tained 831 protein sequences with lengths ranging from 115 residues to 173 residues. The kinase sup erfamily con tained 350 pro-tein sequences with lengths ranging from 151 residues to 502 residues. The ras sup erfamily con tained 386 protein sequences with lengths ranging from 106 residues to 322 residues. The ribitol sup erfamily con tained 319 protein se-quences with lengths ranging from 129 residues to 335 residues. The negativ e dataset con tained 1,650 protein sequences, also tak en from PIR protein database, with lengths ranging from 100 residues to 200 residues; these negativ e sequences did not b elong to an y of the four p ositiv e sup erfamilies. Both the p ositiv e and negativ e sequences w ere divided in to train-ing sequences and test sequences where the size of the train-ing dataset equaled the size of the test dataset m ultiplied b y an in teger r . With the same training data, w e tested sev eral BNN mo dels with di eren tn um b ers of hidden units. When there w ere 2 hidden units, the evidence obtained w as the largest, so w e xed the n um b er of hidden units at 2. Mo dels with more hidden units w ould require more training time while ac hieving roughly the same p erformance.
 The parameter v alues used in the exp erimen ts w ere as fol-lo ws. N g (n um b er of 2-gram patterns used b yBNN) w as 60; N l (n um b er of motifs used b yBNN)w as 20; Len (length of motifs for Sdiscover )w as 6; Mut (m utation n um b er for cover )w as 2; Occur (o ccurrence frequency of motifs for cover )w as 1/10; r (size ratio) w as 2. The measure used to ev aluate the p erformance of the BNN classi er is pr e cision , PR , whic h is de ned as PR =( NumCorrect= NumTotal ) 100%, where NumCorrect is the n um b er of test sequences classi ed correctly and NumTotal is the total n um ber of test sequences. W e presen t the results for the globin su-p erfamily only; the results for the other three sup erfamilies w ere similar. In the rst exp erimen t, w e considered only 2-gram patterns and ev aluated their e ect on the p erformance of the pro-p osed BNN classi er. The p erformance impro v es initially as N g increases. The reason is that the more 2-gram pat-terns w e use, the more precisely w e represen t the protein sequences. Ho w ev er, when N g is to o large (e.g. &gt; 90), the training data is insucien t and the p erformance degrades. In the second exp erimen t, w e considered only motifs found b y Sdiscover and studied their e ect on the p erformance of the classi er. 1,597 motifs w ere found, with lengths ranging from 6 residues to 34 residues. The more motifs one uses, the b etter p erformance one ac hiev es. Ho w ev er, that w ould also require more time in matc hing a test sequence with the motifs. W e exp erimen ted with other parameter v alues for Len , Mut and Occur used in Sdiscover . The results didn't c hange as these parameters c hanged. W e also tested the e ects made b y eac h individual feature and their com bina-tions. W e found that the b est p erformance is ac hiev ed when all the features are used, in whic h case the PR of the BNN classi er is 98%. The purp ose of this section is to compare the prop osed BNN classi er with the commonly used BLAST classi er [1] built based on sequence alignmen t and the SAM classi er [4] built based on hidden Mark o v mo dels. The BLAST v ersion n um-ber w as 2.0.10. W e used default v alues for the parameters in BLAST. The SAM v ersion n um ber w as 3.0; w eusedin ternal w eigh ting metho d 2 as suggested in [4].
 F or BLAST, w e aligned an unlab eled test sequence S with the p ositiv e training sequences (i.e. those in the target class, e.g., the globin sup erfamily) and the negativ e training se-quences in the non-target class using the to ol. If S 's score w as b elo w the threshold of the e v alue whic hw as xed at 10, then S w as undetermined. Otherwise, w e assigned S to the class con taining the sequence b est matc hing S . F or SAM, w eemplo y ed the program buildmo del to build the HMM mo del b y using only the p ositiv e training sequences. W e then calculated the log-o dds scores for all the training sequences using the program hmmsco re . The log-o dds scores w ere all negativ e real n um b ers; the scores (e.g. -100.3) for the p ositiv e training sequences w ere generally smaller than the scores (e.g. -4.5) for the negativ e training sequences. The largest score S p for the p ositiv e training sequences and the smallest score S n for the negativ e training sequences w ere recorded. Let B hig h = max f S p ;S n g and B low =min f S p ;S n g . W e then calculated the log-o dds scores for all the test sequences using the program hmmsco re . If the score of an unlab eled test sequence S w as smaller than B low , S w as classi ed as a mem b er of the target class, e.g., a globin sequence. If the score of S w as larger than B hig h , S w as classi ed as a mem b er of the non-target class. If the score of S w as b et w een B low and B hig h , S w as undetermined. In addition to the three basic classi ers BNN, BLAST and SAM, w e dev elop ed an ensem ble of classi ers, called COM-BINER, that emplo ys an un w eigh ted v oter and w orks as follo ws. If the BNN, BLAST, and SAM agree on the classi-cation results, the result of COMBINER is the same as the results of the three classi ers; if t w o classi ers agree on the classi cation results, the result of COMBINER is the same as the results of these t w o classi ers; if none of the classi ers agrees on the classi cation results, the result of COMBINER is undetermined. W e found that in comparison with BLAST and SAM, the BNN classi er is faster, without yielding an y undetermined sequence. COMBINER ac hiev es the highest precision ( &gt; 99%) among all the classi ers for all the four sup erfamilies globin, kinase, ras, and ribitol. In this pap er w eha v e presen ted a Ba y esian neural net w ork approac h to classifying protein sequences. The main con-tributions include (i) the dev elopmen t of new algorithms for extracting the global similarit y and the lo cal similar-it y from the sequences that are used as input features of the Ba y esian neural net w ork; (ii) the dev elopmen tof new measures for ev aluating the signi cance of 2-gram patterns and frequen tly o ccurring motifs used in classifying the se-quences; (iii) exp erimen tal studies in whic hw e compare the p erformance of the prop osed BNN classi er with t w oother classi ers, namely BLAST and SAM, on four di eren tsu-p erfamilies of sequences in the PIR protein database. The main ndings of our w ork include the follo wing. (1) The three studied classi ers, BNN, SAM and BLAST, comple-men t eac h other; com bining them yields b etter results than using the classi ers individually . (2) The training phase, whic h is done only once, of the t w o learning-based classi ers BNN and SAM ma y tak e some time. After the classi ers are trained, they run signi can tly faster than the alignmen ttool BLAST in sequence classi cation. This w ork w as supp orted in part b y NSF gran ts IRI-9531548 and IRI-9531554, and b y a gran t from No v artis Pharmaceu-ticals Corp oration. W e thank SIGKDD review ers for their though tful commen ts. W e also thank Dr. Da vid Mac k a y for sharing the Ba y esian neural net w ork soft w are with us, and Dr. Ric hard Hughey for pro viding us with the SAM soft w are. [1] S. F. Altsc h ul, T. L. Madden, A. A. Sc ha er, J. Zhang, [2] W. C. Bark er, J. S. Gara v elli, D. H. Haft, L. T. Hun t, [3] A. Brazma, I. Jonassen, E. Ukk onen, and J. Vilo. [4] R. Karc hin and R. Hughey .W eigh ting hidden Mark o v [5] D. J. C. Mac k a y . The evidence framew ork applied to [6] V. V. Solo vy ev and K. S. Mak aro v a. A no v el metho d of [7] J. T. L. W ang, G. W. Chirn, T. G. Marr, B. A.
 [8] J. T. L. W ang, B. A. Shapiro, and D. Shasha (eds.). [9] C. H. W u and J. McLart y . Neur al Networks and
