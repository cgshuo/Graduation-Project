 Data clustering is a fundamental problem arising in many practical applications. In this paper, we consider the density-based data clustering problem on spatial data, defined as follows [7,3]. We are given a set S of n points in the d -D space R d ( d  X  2 is a constant integer) and two parameters  X &gt; 0and  X &gt; 1. For any point p in R d ,wedenoteby N  X  ( p ) the sphere in R d centered at p with radius  X  (based on a given distance function such as an L c metric). The sphere N  X  ( p )is called the  X  -neighborhood of p . 1. For a point p  X  S ,ifthereareatleast  X  points of S (including p )contained 2. For two subsets C 1 and C 2 of S ,ifeachof C 1 and C 2 belongs to a cluster 3. A cluster of S is a maximal set satisfying the two conditions above. All points
The density-based clustering (DBC) problem is to identify all clusters of S and all noise of S .Let S  X  ( p ) denote the point set S  X  N  X  ( p ) for any point p in R d . Apoint p  X  S is called a dense point if | S  X  ( p ) | X   X  ;otherwise, p  X  S is a sparse point .

Considerable work on solving the DBC problem has been done. One of the most well known DBC algorithms is (G)DBSCAN [7]. (G)DBSCAN uses a spa-tial data structure, R  X  -tree, to identify points within a distance  X  from the dense points of the clusters. A heuristic algorithm for determining the param-eters  X  and  X  was given in [7]. OPTICS [1] creates an augmented ordering of the database based on the local densities. DBCLASD [9] defines clusters based on the expected distribution of the distances to the nearest neighbors; hence no parameters are needed in the cluster search. FDC [10] defines clusters by an equivalence relationship on the objects in the database using a cell-based method. DENCLUE [5] models the overall point density analytically as the sum of the influence functions of the data points.

With the rapid increase in data volumes today, data sets are often too large to entirely fit in a computer X  X  internal memory (i.e., the main memory), and instead must be stored in external storage devices (e.g., disks). In this paper, we consider using disk (or interchangeabl y, secondary memory or external memory, and they all mean the same thing) for data storage. A major performance bot-tleneck in this setting is the cost of input/output (I/O) communication between the external and internal memories, since such I/O operations are very time-consuming. One promising approach for dealing with this I/O difficulty is to design algorithms and data structures that bypass the virtual memory system and explicitly manage their own I/O. Vitter [8] considered this problem, and referred to such algorithms and data structures as external memory algorithms and data structures (see [8] for more details).

In this paper, we present the following results: 1. We developed two algorithms (called Algorithms A and B) that compute 2. Besides packing data structures into buckets and using block access tech-3. We evaluated the effects of various space-filling curves and identified the 4. We implemented our algorithms and conducted extensive performance eval-Like many other geometric algorithms, the CPU time and I/O bounds of Algorithms A and B have a constant factor of 3 d in the cluster search. Algorithms A and B are efficient for d  X  10. Algorithm A focuses on handling not highly clustered input data, while Algorithm B focuses on highly clustered input data. We use hashing-based external data structures for both Algorithms A and B. There are three reasons for our choices of the external data structures: (1) They are cheap to construct; (2) they have low I/O cost for searching operations; (3) we can further reduce the I/O cost by applying space-filling curve techniques to these external data structures. Compared to using the general high dimen-sion indexing methods based on space-filling curves to solve the density-based clustering, the external data structures and the cluster search algorithm in our algorithms are more tightly integrate d to achieve high efficiency in both CPU time and I/O cost, and this is done at the cost of significant loss of generality of our external data structures. 2.1 Decomposing the Space Given a set S of n points in R d and parameters  X &gt; 0and  X &gt; 1, we partition the space containing S into a set of cells, called the basic cells , so that for any point p of S in a basic cell c , we need to search only c  X  X  3 d neighboring basic cells for the point set S  X  ( p ). The length of a basic cell in any dimension is 2  X  .
To save disk space and reduce disk access operations, we need to store in one page the data points in many basic cells. Thus, we combine as many basic cells as possible into a larger cell, called new cell , such that the average number of points in each new cell is no bigger than  X  ,where  X  is the capacity of one page. The decomposition of the space produces the smallest hypercube C that contains S , and the edge length of a new cell. 2.2 The Sequence Numbers of New Cells We store all input points in each new cell as a collection. Clearly, we need to decide the order in which the new cells are to be stored. A space-filling curve helps create a mapping of the new cells from a d -D space to a single dimension (thus defining an order). It is desirable that the new cells that are close together in the d -D space be also close together in the mapped 1-D space [6]. Each new cell is mapped to a unique new cell sequence number , and we store input points based on this new cell sequence number.
 We consider six space-filling curves [4,6]: Row-wise, snake row-wise, Z-ordering, Z-ordering with Gray code, Gray ordering, and Hilbert curve. We need to deter-mine which curve is most suitable for our DBC algorithms. Due to the page limit, we refer to [4,6] for the details of these six space-filling curves.
Both our algorithms efficiently search the clusters by gradually  X  X xpanding X  the clusters instead of  X  X umping X  everywhere. Thus, we need to use a space-filling curve which helps  X  X xpand X  the clusters efficiently. The Hilbert curve has the least  X  X umps X  comparing to other space-filling curves [6], and is the best choice for our cluster search algorithms, as shown by our experimental results.
The mapping from each new cell to its new cell sequence number based on any of the six space-filling curves mentioned above takes O (1) CPU time. Thus, the overhead of this mapping is very small comparing to the savings in I/O costs. 2.3 External Data Structure of Algorithm A Suppose there are totally N new cells. The first N data pages as shown in Figure 1(b) correspond to the N new cells, in the ascending order of the new cell sequence numbers. We call these N pages the starting pages because if one page cannot hold all input points in one new cell, we will allocate one or more extra pages for this new cell, and these N pages are always the starting pages for finding the extra pages. There are K extra pages in Figure 1(b). As shown in Figure 1(a), each data page contains the information of the row-wise sequence number of the corresponding new cell (say, c ), the number of points in c ,the points in c and their cluster IDs, and the pointer to c  X  X  next extra page.
We omit the construction of the external data structure of Algorithm A due to the page limit, and summarize it in the following lemma. Lemma 1. Suppose we are given a set S of n points in the d -D space R d (for any constant integer d  X  2 ) and two parameters  X &gt; 0 and  X &gt; 1 . By scanning the input data twice, we can create the data file for Algorithm A in O ( n ) CPU time, space, and I/Os if there are O ( n ) new cells and each new cell contains a constant number of points. 2.4 External Data Structure of Algorithm B The external data structure of Algorithm B has directory pages and data pages. The address of a new cell in the data pages can be located by consulting the directory.

Figure 2(a) shows a directory page. Each new cell has an entry (called a directory node or D -Node ) in directory. Each D -Node contains information of the row-wise sequence number of the corresponding new cell (say, c ), the number of points in c , the pointer to the data page, and the offset in the data page. Figure 2(b) shows the structure of a data page. Figure 2(c) shows the external data structure of Algorithm B. New cells are saved consecutively along the ascending order of the new cell sequence numbers in the both directory pages and data pages. The space utilization is 100% , i.e., there are no useless directory and data pages.

We omit the construction of the external data structure of Algorithm B due to the page limit, and summarize it in the following lemma.
 Lemma 2. Suppose we are given a set S of n points in the d -D space R d (for any constant integer d  X  2 ) and two parameters  X &gt; 0 and  X &gt; 1 . By scanning the input data three times, we can create the external data structure for Algorithm Bin O ( n ) CPU time, space, and I/Os if there are O ( n ) new cells. Below is the main procedure for cluster search in Algorithms A and B. Input: External data structure F , and parameters  X  and  X  .
 Output: External data structure F with cluster IDs assigned. 1. For each new cell c along the new cell number sequence, do the following: 2. Eliminate the equivalent cluster IDs. We search for clusters along the new cell sequence numbers in ascending order. In this way, we can reduce the swaps of data pages between the main memory and the disk, and operate on more consecutive pages. Given the new cell sequence number s of a new cell c , we can calculate the new cell sequence numbers of its neighboring new cells in R d easily since we keep the row-wise sequence number of c in the external data structures of Algorithms A and B.

We use a  X  X irst-in-first-out X  queue Q to record the new cells that are in the main memory, with the corresponding cell sequence numbers as keys. Q has a prescribed maximum length L . If we have more main memory, we can set L bigger to reduce the swap between the main memory and the secondary memory. When we read new cells into the main memory, we add new cells to Q . For those new cells that need to be read into the main memory, we first sort their new cell sequence numbers in ascending order, and then read them along this order. In this way, we can save much I/O cost by reading consecutive pages. When Q exceeds the length limit L , the new cells which came earlier will be removed, and these removed new cells will be written back to the secondary memory. We also use the ascending order of the new cell sequence to write back these new cells for reducing I/O cost.

We only need to search a point p  X  X  3 d neighboring basic cells to find the point set S  X  ( p ). If p is a dense point, we need to assign all points in N  X  ( p )toasame cluster. But, some of the points in N  X  ( p ) may have already been assigned different cluster IDs, while these points in fact should belong to the same cluster. We call different cluster IDs which are actually for the same cluster the equivalent cluster IDs [10]. We reduce the number of equivalent cluster IDs by doing the following. For a dense point p , if there is a point in N  X  ( p ) already having a cluster ID, we just use this ID (instead of creating a new ID) to label the points in N  X  ( p ). In this way, we can reduce a great deal of equ ivalent cluster IDs. We formulate the problem of eliminating equivalent clust er IDs as one of computing the connected components to completely eliminate any equivalent cluster IDs.
 We have the following theorem as the summary of Algorithms A and B.
 Theorem 1. Given n points in R d (for any constant d  X  2 )andparameters  X &gt; 0 and  X &gt; 1 , Algorithms A and B can compute all density-based clusters and noises in O ( n ) CPU time, space, and I/Os if there are O ( n ) new cells and each new cell contains a constant number of data points.
 For an input data set, if we like to use different values of  X  , Algorithms A and B need to reconstruct the ex ternal data structures. This overhead actually does not have a significant impact on the o verall efficiency of Algorithms A and B since the time for constructing the external data structures is much less than the time for searching clusters, especially on large data sets. We conducted extensive experiments on our two DBC algorithms, using an Intel Pentium 4 (1.4 GHZ with 512M main memory, running MS Windows 2000).

For each data set, we apply the heuristic algorithm [7] to determine the pa-rameters  X  and  X  . Note that in specific applications, we can adjust the values of  X  and  X  to find the types of clusters interesting to us, such as low density sparse clusters. The experiments have shown that our algorithms find the same clusters as DBSCAN when using the same values of  X  and  X  on input data sets. We used the library ANN [2] to generate the synthetic data sets in the following way: 15  X  X ore X  points were first chosen from the uniform distribution (on the interval [0 , 1]) in the unit hypercube, and then many points based on a Gaussian distribution with standard deviation  X  centered around each core point were generated in the unit hypercube. In the experiments below, we use two  X  values to create two kinds of data sets: We use  X  =0 . 05 to create data sets that repre-sent not highly clustered data, and  X  =0 . 005 to create data sets that represent highly clustered data. Due to the page limit, we omit the experiments with real data sets and big data sets, and those about relation between the execution time and dimension, etc.

For all the experiments below, the execution time of Algorithms A and B includes the time of both the external data structure construction and cluster search, since we need to reconstruct the extern al data structures when  X  changes. The execution time of DBSCAN only includes the cluster search time. 4.1 Random Disk Access Operations and Space-Filling Curves We use six different space-filling curves and record the total random disk access operations for cluster search. When sev eral disk accesses are on consecutive data pages, we count them as one random disk access. This is because consecutive I/Os is much faster than the discrete I /Os. We use random disk accesses instead of disk accesses to better reflect the r eal I/O costs in cluster search. In Figure 3, we use 2-D data sets of sizes from 10M to 40M with  X  =0 . 005. We can see that the random disk access ope rations in cluster search with the Hilbert space-filling curve are the smallest for all data sizes. When data size is 40M, other space-filling curves, Gray code, row-wise, snake row-wise, Z-ordering, and Z-ordering with Gray code, are 1 . 5, 1 . 1, 1 . 0, 0 . 7, and 0 . 6 times bigger than Hilbert, respectively.
Based on many experiments with data se ts of different sizes, dimensions, and  X  values, we conclude that the Hilbert curve is the best choice for Algorithms A and B on reducing the I/O costs. We use the Hilbert curve in all the experiments below. 4.2 Experiments with Different  X  Values Figure 4 shows the relation between the e xecution time and data size with dif-ferent  X  values. In Figure 4(a), the input data is not highly clustered (  X  =0 . 05). For data sizes of 1M, 2M, 3M, and 4M, DBSCAN uses 2 . 9, 4 . 5, 8 . 9, and 15 . 7 times as much time as Algorithm A, 1 . 4, 2 . 5, 4 . 9, and 9 . 3 times as much time as Algorithm B, respectively. For the data set of size 4M, DBSCAN needs 87,962 seconds (i.e., over 1 day) for the cluster search, while Algorithms A and B use 1.3 and 1.6 hours of execution time, respectively. Algorithms B uses around 1.8 times as much time as Algorithm A for the data sizes.

In Figure 4(b), the input data is highly clustered (  X  =0 . 005). For data sizes of 1M, 2M, 3M, and 4M, DBSCAN uses 1 . 7, 3 . 0, 5 . 9, and 11 . 1timesasmuch time as Algorithm A, 2 . 0, 3 . 6, 7 . 1, and 13 . 3 times as much time as Algorithm B, respectively. For the data set of size 4M, DBSCAN needs 89,543 seconds (i.e., over 1 day) for the cluster search, while Algorithms A and B use 2.2 and 1.8 hours of execution time, respectivel y. Algorithms A uses around 1.2 times as much time as Algorithm B for the data sizes.
