 University of the Basque Country University of the Basque Country Universitat Polit ` ecnica de Catalunya University of Arizona
This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research: the limited influence and sparseness of lexical features. We mitigate this problem using models that integrate automatically learned selectional preferences (SP). We explore a range of models based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC task is better modeled by SP models centered on both verbs and prepositions, rather than verbs alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical baseline with 20 F 1 points in domain and almost 40 F 1 points out of domain. Furthermore, we show that a state-of-the-art SRC system extended with features based on selectional preferences performs significantly better, both in domain (17% error reduction) and out of domain (13% error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain small but statistically significant improvements, even though our modified SRC model affects only approximately 4% of the argument candidates. Our post hoc error analysis indicates that insufficient to disambiguate the correct role. 1. Introduction
Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by identifying arguments and tagging them with semantic labels indicating the role they play with respect to the predicate. Such sentence-level semantic analysis allows the determination of who did what to whom , when and where , and thus characterizes the consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles. 1 spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems.

Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification . Whereas the former is mostly a syntactic recognition task, the latter usually requires semantic knowledge to be taken into account. The semantic knowledge that most current systems capture from text is basically limited to the predicates and the lexical units contained in their arguments, including the argument head. These  X  X exical features X  tend to be sparse, especially when the training corpus is small, and thus SRL systems are prone to overfit the training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).
As a simplified example of the effect of sparsity, consider the following sentences occurring in an imaginary training data set for SRL: words Dallas , New York , November ,and winter ) represent the most relevant knowledge for discriminating between the Location and Temporal adjunct labels in learning. 632 encounter similar expressions with new words like Texas or December , which the classifiers cannot match with the lexical features seen during training, and thus become useless for classification: new domains where the number of new predicates and argument heads increases considerably. The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling (Carreras and M ` arquez 2004, 2005) reported a significant performance degradation of around 10 F 1 points when applied to out-of-domain texts from the Brown corpus.
Pradhan, Ward, and Martin (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons.
 fight sparseness and domain shifts, and improving SRC results. Selectional preferences have been widely used in computational linguistics since the early days (Wilks 1975).
Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models.
 1. We formalize and implement a method that applies several selectional 2. We show that the selectional preference models are able to generalize 3. We integrate the information of several SP models in a state-of-the-art SRL 4. We present a manual analysis of the output of the combined role sented in Zapirain, Agirre, and M ` arquez (2009), and later extended in Zapirain et al. (2010) to a full-fledged SRC system. In the current paper, we provide more detailed background information and details of the selectional preference models, as well as complementary experiments on the integration in a full-fledged system. More impor-tantly, we incorporate a detailed analysis of the output of the system, comparing it with that of a state-of-the-art SRC system not using SPs.
 automatic acquisition of selectional preference, and its recent relation to the semantic plained in all their variants. The results of the SP models in laboratory conditions are presented in Section 4. Section 5 describes the method for integrating the SP models in a state-of-the-art SRL system and discusses the results obtained. In Section 6 the qualita-tive analysis of the system output is presented, including a detailed discussion of several examples. Finally, Section 7 concludes and outlines some directions for future research. 2. Background
The simplest model for generating selectional preferences would be to collect all heads filling each role of the target predicate. This is akin to the lexical features used by current
SRL systems, and we refer to this model as the lexical model . More concretely, the lexical model for verb -role selectional preferences consists of the list of words appearing as heads of the role arguments of the predicate verb . This model can be extracted automatically from the SRL training corpus using straightforward techniques. When using this model for role classification, it suffices to check whether the head word of the argument matches any of the words in the lexical model. The lexical model is the baseline for our other SP models, all of which build on that model.
 principle any lexical resource listing semantic classes for nouns could be applied, most of the literature has focused on the use of WordNet (Resnik 1993b). In the WordNet-based model , the words occurring in the lexical model are projected over the semantic hierarchy of WordNet, and the semantic classes which represent best those words are selected. Given a new example, the SRC system has to check whether the new word matches any of those semantic classes. For instance, in example sentences (2) X (5), the semantic class &lt;time period&gt; covers both training examples for Temporal (i.e., November and winter ), and &lt;geographical area&gt; covers the examples for Location . When test words Texas and December occur in Examples (6) and (7), the semantic classes to which they belong can be used to tag the first as Location and the second as Temporal . distributional similarity thesauri. Distributional similarity methods analyze the co-occurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998).
In the Distributional similarity model , when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York , in contrast to the lower similarity to November and winter , would be used to label the argument with the Location role.
Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic X  X emantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and
Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007;
Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task.
 have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called  X  X emantic fields X  used to classify all nouns in WordNet (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011). The classification could be more fine-grained, as defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy and Carroll 2003), and other lexical resources could be used as well. Other authors have used automatically induced hierarchical word classes, clustered according to occurrence information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009). model, but one could also model selectional preference using distributional similarity (Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel 2008). In this paper we will focus on WordNet-based models that use the whole hierarchy and on distributional similarity models, and we will use the lexical model as baseline. 2.1 WordNet-Based Models
Resnik (1993b) proposed the modeling of selectional preferences using semantic classes from WordNet and applied the model to tackle some ambiguity issues in syntax, such as noun-compounds, coordination, and prepositional phrase attachment. Given two alternative structures, Resnik used selectional preferences to choose the attachment maximizing the fitness of the head to the selectional preferences of the attachment points. This is similar to our task, but in our case we compare the target head to the selec-tional preference models for each possible role label (i.e., given a verb and the head of an argument, we need to find the role with the selectional preference that fits the head best). erence of an argument position r of a governing predicate p , noted as R ( p , r ). For that, given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-tropy or Kullback-Leibler distance between the prior distribution P ( C ) and the posterior distribution P ( C | p , r ): and using maximum likelihood estimates. The frequencies for classes cannot be directly observed, but they can be estimated from the lexical frequencies of the nouns under the class, as in Equation (2). Note that in WordNet, hypernyms ( X  X yp X  for short) correspond to superclass relations, and therefore hyp ( n ) returns all superclasses of noun n . noun counted once in all classes that its senses belong to, polysemous nouns would account for more probability mass than monosemous nouns, even if they occurred the same number of times. As a solution, the frequency of polysemous nouns is split among estimated according to the frequencies of nouns like November , spring , and the rest of nouns under it. November has a single sense, so every occurrence counts as 1, but spring has six different senses, so each occurrence should only count as 0.16. Note that with this method we are implicitly dealing with the word sense ambiguity problem. When encountering a polysemous noun as an argument of a verb, we record the occurrence intended sense of the nouns will gather more counts than competing classes. In the elastic device meaning of spring ). Researchers have used this fact to perform Word Sense Disambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003).
 rences of the nouns in the required argument position of the predicate, and thus requires a corpus annotated with roles.
 argument, noted as SP Res ( p , r , w 0 ), is formulated as follows: contains w 0 . The hypernym (i.e., superclass) of w 0 chosen. The denominator models how restrictive the selectional preference is for p and r , as modeled in Equation (1).
 explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children.

Resnik X  X  selectional preferences) on a syntactic plausibility judgment task for German. 636
The models return weights for (verb, syntactic function, noun) triples, and correla-tion with human plausibility judgment is used for evaluation. Resnik X  X  selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and representative among WordNet-based methods (Pad  X  o, Pad  X  o, and Erk 2007; Erk, Pad  X  o, and Pad  X  o 2010; Baroni and Lenci 2010). We also chose to use Resnik X  X  model in this paper.
 butional similarity models, is that they require that the heads are present in WordNet.
This limitation can negatively influence the coverage of the model, and also its general-ization ability. 2.2 Distributional Similarity Models
Distributional similarity models assume that a word is characterized by the words it co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size context window. Each word w would be represented by the set of words that co-occur with it, T ( w ). In a more elaborate model, each word w would be represented as a vector of words T ( w ) with weights, where T i ( w ) corresponds to the weight of the i th word in the vector. The weights can be calculated following a simple frequency of co-occurrence, or using some other formula.
 larity measure between their co-occurrence sets or vectors. For instance, early work by
Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T ( w )and T ( w (cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions, including Jaccard and the cosine between two vectors T ( w )and T ( w in Figure 1).
 has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account syntactic dependencies ( d ) in its co-occurrence model. In this case, the set T ( w )ofco-occurrences of w contains pairs ( d , v ) of dependencies and words, representing the fact that the corpus contains an occurrence of w having dependency d with v . For instance, if the corpus contains John loves Mary , then the pair (ncsubj, love) wouldbeintheset
T for John . The measure uses information-theoretic principles, and I ( w , d , v ) represents the information content of the triple (Lin 1998).
 standard practice, some authors have argued for more complex uses. Sch  X  utze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical co-used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method.
In this paper, we will also use second order to refer to a new method which goes beyond the usual co-occurrence vectors (cf. Section 3.3).
 terested in showing that some of those models can be used successfully to improve SRC.
Pad  X  o and Lapata (2007) present a review of distributional models for word similarity, and a study of several parameters that define a broad family of distributional similarity models, including Jaccard and Lin. They provide publicly available software, we have used in this paper, as explained in the next section. Baroni and Lenci (2010) present a framework for extracting distributional information from corpora that can be used to build models for different tasks.
 instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the distributional similarity measure defined by Lin (1998). Distributional similarity was used to overcome sparsity problems: Alongside the counts in the training data of the target words, the counts of words similar to the target ones were also used. Although not made explicit, Lin was actually using a distributional similarity model of selectional preferences.
 posed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results.
 the evaluation of artificial tasks or human plausibility judgments. Erk (2007) introduced a distributional similarity X  X ased model for selectional preferences, reminiscent of that of Pantel and Lin (2000). Her approach models the selectional preference SP of an argument position r of governing predicate p for a possible head-word w follows: 638 set of heads of role r for predicate p seen in the training data set (as in the lexical model), and weight ( p , r , w ) is the weight of the seen head word w . Our distributional model for selectional preferences follows her formalization.
 measures, including Lin X  X  similarity, Jaccard, and cosine (Figure 1) among others, and several implementations of the weight function such as the frequency. The quality of each model instantiation, alongside Resnik X  X  model and an expectation maximization (EM)-based clustering model, was tested in a pseudo-disambiguation task where the goal was to distinguish an attested filler of the role and a randomly chosen word. The results over 100 frame-specific roles showed that distributional similarities attain similar error rates to Resnik X  X  model but better than EM-based clustering, with Lin X  X  formula having the smallest error rate. Moreover, the coverage of distributional similarity mea-sures was much better than Resnik X  X . In a more recent paper, Erk, Pad  X  o, and Pad  X  o (2010) extend the aforementioned work, including evaluation to human plausibility judgments and a model for inverse selectional preferences.
 selectional preference models in a setting directly related to semantic role classification, best the given head word. The problem is indeed qualitatively different from previous work in that we do not have to choose among the head words competing for a role but among selectional preferences of roles competing for a head word.
 discriminative models (Bergsma, Lin, and Goebel 2008) and topical models ( 2010; Ritter, Mausam, and Etzioni 2010). These models would be a nice addition to those implemented in this paper, and if effective, they would improve further our results with respect to the baselines which don X  X  use selectional preferences.
 hand-built resource. Their coverage and generalization ability depend on the corpus from which the distributional similarity model was computed. This fact makes this approach more versatile in domain adaptation scenarios, as more specific and test-set focused generalization corpora could be used to modify, enrich, or even replace the original corpus. 2.3 PropBank
In this work we use the semantic roles defined in PropBank. The Proposition Bank (Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research in SRL. It provides semantic role annotation for all verbs in the Penn Treebank corpus. PropBank takes a  X  X heory-neutral X  approach to the designation of core semantic roles.
Each verb has a frameset listing its allowed role labelings in which the arguments are designated by number (starting from 0). Each numbered argument is provided with an English language description specific to that verb. The most frequent roles are Arg0 and
Arg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to the prototypical patient or theme of the proposition. The rest of arguments (Arg2 to Arg5) do not generalize across verbs, that is, they have verb specific interpretations.
AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD (modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),
AM-REC (reciprocal), and AM-TMP (temporal). 3. Selectional Preference Models for Argument Classification
Our approach for applying selectional preferences to semantic role classification is discriminative. That is, the SP-based models provide a score for every possible role label given a verb (or preposition), the head word of the argument, and the selectional preferences for the verb (or preposition). These scores can be used to directly assign the most probable role or to codify new features to train enriched semantic role classifiers. used in our study, and then present the method to apply them to semantic role classifi-cation. We selected several variants that have been successful in some previous works. 3.1 Lexical SP Model
In order to implement the lexical model we gathered all heads w of arguments filling arole r of a predicate p and obtained freq ( p , r , w ) from the corresponding training data corpus for the verb write . The lexical SP model can be simply formalized as follows: 3.2 WordNet-Based SP Models
We instantiated the model based on (Resnik 1993b) presented in the previous sec-tion ( SP Res , cf. Equation (3)) using the implementation of Agirre and Martinez (2001).
Tables 2 and 3 show the synsets 5 that generalize best the head words in Table 1 for write -Arg0 and write -Arg1 , according to the weight assigned to those synsets by
Equation (1). According to this model, and following basic intuition, the words attested as being Arg0s of write are best generalized by semantic classes such as living things, 640 entities, physical objects, and human beings, whereas Arg1s by communication, mes-sage, relation, and abstraction.
 that it tends to overgeneralize. For instance, in Table 2, the concept for  X  X ntity X  (one of the unique beginners of the WordNet hierarchy) has a high weight. This means that a head like  X  X rant X  would be assigned Arg0. In fact, any noun which is under concept n#00001740 ( entity ) but not under n#04949838 ( message ) would be assigned Arg0. This observation led us to speculate on an alternative method which would try to generalize as little as possible.
 same time. For instance, the &lt;entity&gt; class, as a superclass of most words, would be a correct generalization for the selectional preferences of all agent , patient ,and instrument roles of a predicate like break . On the contrary, specific concepts are usually more useful for characterizing selectional preferences, as in the &lt;tool&gt; class for the instrument role of break . The priority of using specific synsets over more general ones is, thus, justified in the sense that they may better represent the most relevant semantic characteristics of the selectional preferences.
 hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model the specificity of concepts (the deeper the more specific) is not new (Rada et al. 1989;
Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect to generalization: When we check which SP is a better fit for a given target head, we always prefer the SP that contains the most specific generalization for the target head (the lowest synset which is a hypernym of the target word). predicate, that is: all the synsets and hypernyms of w , including hypernyms of hypernyms recursively up to the top synsets.
 and let 1 S tained in the multiset S mul ( p , r ). We define a partial order among synsets a , b as follows: ord ( a ) &gt; ord ( b )iff d ( a ) &gt; d ( b )or d ( a ) = d ( b )
Tables 4 and 5 show the most specific synsets (according to their depth) for write -Arg0 and write -Arg1 .
 the rank in the partial order of the first hypernym of the head that is also present in the selectional preference. For that, we introduce SP wn ( p , r , w ), which following the previous notation is defined as: 642
In case of ties, the role coming first in alphabetical order would be returned. Note that, similar to the Resnik model (cf. Section 2.1), this model implicitly deals with the word ambiguity problem.
 depth has some issues, as some deeply rooted stray synsets would take priority. For instance, Table 4 shows that synset n#01967203 for human being is the deepest synset. In practice, when we search the synsets of a target word in the SP (10), the most specific synsets (specially stray synsets) are not found, and synsets higher in the hierarchy are used. 3.3 Distributional SP Models
All our distributional SP models are based on Equation (7). We have used several vari-ants for sim ( w 0 , w ), as presented subsequently, but in all cases, we used the frequency freq ( p , r , w ) as the weight in the equation. Given the availability of public resources for distributional similarity, rather than implementing sim ( w compiled similarity measures by Lin (1998), 7 and (2) the software for semantic spaces by Pad  X  o and Lapata (2007).
 parsed corpus comprising journalism texts from different sources: WSJ (24 million words), San Jose Mercury (21 million words) and AP Newswire (19 million words).
The resource includes, for each word in the vocabulary, its most similar words with the similarity weight. In order to get the similarity for two words, we can check the sim pre Lin . Table 6 shows the most similar words for Texas and December according to this resource.
 extract co-occurrences, using the optimal parameters as described in Pad  X  o and Lapata (2007, page 179): word-based space, medium context, log-likelihood association, and 2,000 basis elements. We tested Jaccard, cosine, and Lin X  X  measure for similarity, yielding sim Jac , sim cos ,and sim Lin , respectively.
 occurrence vectors of each word as in Section 2, we also tried a variant which we will call second-order similarity . In this case each word is represented by a vector which contains all similar words with weights, where those weights come from first order similarity. That is, in order to obtain the second-order vector for word w , we need to compute its first order similarity with all other words in the vocabulary. The second-order similarity of two words is then computed according to those vectors. For this, we just need to change the definition of T and T in the similarity formulas in Figure 1: Now
T ( w ) would return the list of words which are taken to be similar to w ,and T ( w ) would return the same list but as a vector with weights.
 square matrix of similarities for all word pairs in the vocabulary, which is highly time-consuming. Fortunately, the pre-computed similarity scores of Lin (1998) (which use sim
Lin ) are readily available, and thus the second-order similarity vectors can be easily computed. We used Jaccard and cosine to compute the similarity of the vectors, and we will refer to these similarity measures as sim pre Lin  X  Jac computational complexity, we did not compute second order similarity for the semantic space software of Pad  X  o and Lapata (2007).
 pre-computed similarity list used to build them. 3.4 Selectional Preferences for Prepositions
All the previously described models have been typically applied to verb -role selectional preferences for NP arguments. Applying them to general semantic role labeling may not be straightforward, however, and may require some extensions and adaptations.
For instance, not all argument candidates are noun phrases. Common arguments with other syntactic types include prepositional, adjectival, adverbial, and verb phrases. Any candidate argument without a nominal head cannot be directly treated by the models described so far. 644 relations between the preposition attachment point and the preposition complement.
Prepositions are ambiguous with respect to these relations, which allows us to talk about preposition senses . The Preposition Project (Litkowski and Hargraves 2005, 2006) is an effort that produced a detailed sense inventory for English prepositions, which was later used in a preposition sense disambiguation task at SemEval-2007 (Litkowski and Hargraves 2007). Sense labels are defined as semantic relations, similar to those of semantic role labels. In a more recent work, Srikumar and Roth (2011) presented a joint model for extended semantic role labeling in which they show that determining the sense of the preposition is mutually related to the task of labeling the argument role of the prepositional phrase. Following the previous work, we also think that prepositions define implicit selectional preferences, and thus decided to explore the use of preposi-tional preferences with the aim of improving the selection of the appropriate semantic roles. Addressing other arguments with non-nominal heads has been intentionally left for further work.
 would be to add the preposition as an extra parameter of the SP. Initial experiments revealed sparseness problems with collecting the verb, preposition, NP-head, role 4-tuples from the training set. A simpler approach consists of completely disregarding the verb information while collecting the prepositional preferences. That is, the selec-tional preference for a preposition p and role r is defined as the union of all nouns w found as heads of noun phrases embedded in prepositional phrases headed by p and labeled with semantic role r . Then, one can apply any of the variants described in the previous sections to calculate SP ( p , r , w ). Table 8 shows a sample of the lexical model for the preposition from , organized according to the roles it plays.
 still being able to capture relevant information to distinguish the appropriate roles in many PP arguments. In particular, they proved to be relevant to distinguish between adjuncts of the type  X  X  in New York] Location  X  X s. X  X  in Winter] are aware that not taking into account verb information also introduces some lim-itations. In particular, the simplification could damage the performance on PP core arguments, which are verb-dependent. 9 For instance, our prepositional preferences would not be able to suggest appropriate roles for the following two PP arguments:  X  X ncrease [ from seven cents ashare] Arg 3  X  and  X  X eceive [ from the funds ] the two head nouns ( cents and funds ) are semantically very similar. Assigning the
Arg3 is the starting point for the predicate increase , whereas Arg2 refers to the source for receive .
 just a starting point to play with the argument preferences introduced by prepositions.
A more complex model, distinguishing between prepositional phrases in adjunct and core argument positions, should be able to model the linguistics better yet alleviate the sparseness problem, and would hopefully produce better results.

Depending on the syntactic type of the argument we apply one or the other model, both in learning and testing: possibilities like doing mixtures of both SPs are left for future work. 3.5 Role Classification with SP Models
Selectional preference models can be directly used to perform role classification. Given a target predicate p and noun phrase candidate argument with head w , we simply select selection rule is formalized as: the goodness of fit of the selectional preference model for the head w , which can be instantiated with all the variants mentioned in the previous subsections, including the lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)), and distributional SP models (Equation (7)), using different similarity models as in
Table 7. Ties were broken returning the role coming first according to alphabetical order. Note that in the case of SP wn (Equation 10) we need to use arg min rather than arg max. 646 and embedded NP head word w , the classification rule uses the prep -role SP model, that is: 4. Experiments with Selectional Preferences in Isolation
In this section we evaluate the ability of selectional preference models to discriminate among different roles. For that, SP models will be used in isolation, according to the clas-pairs. That is, we are interested in the discriminative power of the semantic information carried by the SPs, factoring out any other feature commonly used by the state-of-the-art SRL systems. The data sets used and the experimental results are presented in the following. 4.1 Data Sets
The data used in this work are the benchmark corpus provided by the CoNLL-2005 shared task on SRL (Carreras and M ` arquez 2005). The data set, of over 1 million tokens, comprises PropBank Sections 02 X 21 for training, and Sections 24 and 23 for develop-ment and testing, respectively. The Selectional Preferences implemented in this study are not able to deal with non-nominal argument heads, such us those of NEG, DIS,
MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the same evaluation conditions of typical PropBank-based SRL experiments all arguments evaluation penalizes them accordingly.
 are extracted from the arguments of the training set, yielding 71,240 triples, from which 5,587 different predicate-role selectional preferences ( p , r ) are derived by instantiating the different models in Section 3. Tables 9 and 10 show additional statistics about some of the most (and least) frequent verbs and prepositions in these tuples.
 into the appropriate role label. In order to study the behavior on out-of-domain data, we also tested on the PropBanked part of the Brown corpus (Marcus et al. 1994). This corpus contains 2,932 ( p , w ) pairs covering 491 different predicates. 4.2 Results
The performance of each selectional preference model is evaluated by calculating the customary precision (P), recall (R), and F 1 measures. ported in this paper, we checked for statistical significance using bootstrap resampling (100 samples) coupled with one-tailed paired t-test (Noreen 1989). We consider a result significantly better than another if it passes this test at the 99% confidence interval. for the combination of verb -role and preposition -role SPs as described in Section 3.4.
It is worth noting that the results of Tables 11 and 12 are calculated over exactly the 648 same example set. PP arguments are treated by the verb -role SPs by just ignoring the preposition and considering the head noun of the NP immediately embedded in the PP. facing a head word missing from the model. This is especially noticeable in the lexical model, which can only return predictions for words seen in the training data and is penalized in recall. WordNet based models, which have a lower word coverage com-pared to distributional similarity X  X ased models, are also penalized in recall. The following rows correspond to the WordNet-based selectional preference models.
The distributional models follow, including the results obtained by the three similarity formulas on the co-occurrences extracted from the BNC ( sim results obtained when using Lin X  X  pre-computed similarities directly ( sim second-order vector ( sim pre Lin  X  Jac and sim pre Lin  X  preposition -role SPs yields better results. The comparison of Tables 11 and 12 shows that the improvements are seen for both precision and recall, but especially remarkable for recall. The overall F 1 improvement is of up to 10 points. Unless stated otherwise, the rest of the analysis will focus on Table 12.
 underscores the importance of the lexical head word features in argument classification. Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,
Ward, and Martin (2008), who also report a similar performance drop for argument classification on out-of-domain data. All our selectional preference models improve over the lexical matching baseline in recall, with up to 24 absolute percentage points in the WSJ test data set and 47 absolute percentage points in the Brown corpus. This comes at the cost of reduced precision, but the overall F-score shows that all selectional preference models are well above the baseline, with up to 13 absolute percentage points on the WSJ data sets and 39 absolute percentage points on the Brown data set. sparseness problem. 12 verb wear found in the test set: doctor , men , tie , shoe . None of these nouns occurred as heads of arguments of wear in the training data, and thus the lexical feature would be unable to predict any role for them. Using selectional preferences, we successfully assigned the A0 role to doctor and men ,andthe A1 role to tie and shoe .
 butional similarity models attain similar levels of precision, but the former have lower recall and F 1 . The performance loss on recall can be explained by the limited lexical coverage of WordNet when compared with automatically generated thesauri. Examples of words missing in WordNet include abbreviations (e.g., Inc. , Corp. ) and brand names (e.g., Texaco , Sony ).
 lighter method of WordNet-based selectional preference was successful, as our simpler variant performs better than Resnik X  X  method. In manual analysis, we realized that
Resnik X  X  model tends to always predict the most frequent roles whereas our model covers a wider role selection. Resnik X  X  tendency to overgeneralize makes more frequent roles cover all the vocabulary, and the weighting system penalizes roles with fewer occurrences. 650 and Lapata 2007) calculated over the BNC ( sim Lin ) in both Tables 11 and 12. This might be due to the larger size of the corpus used by Lin, but also by the fact that Lin used a newspaper corpus, compared with the balanced BNC corpus. Further work would be needed to be more conclusive, and, if successful, could improve further the results of some SP models.
 seems to perform consistently better. Regarding the comparison between first-order and second-order using pre-computed similarity models, the results indicate that second-order is best when using both the verb -role and prep -role models (cf. Table 12), although the results for verb -roles are mixed (cf. Table 11). Jaccard seems to provide slightly better results than cosine for second-order vectors.
 results, and second-order similarity is highly competitive. As far as we know, this is the first time that prep -role models and second-order models are applied to selectional preference modeling. 5. Semantic Role Classification Experiments
In this section we advance the use of SP in SRL one step further and show that selec-tional preferences are able to effectively improve performance of a state-of-the-art SRL system. More concretely, we integrate the information of selectional preference models in a SRL system and show significant improvements in role classification, especially when applied to out-of-domain corpora. 13 section. We will focus on the combination of verb -role and prep -role models. Regarding the similarity models, we will choose the best two performing models from each of the three families that we tried, namely, the two WordNet models, the two best models based on the BNC corpus ( sim Jac , sim cos ), and the two best models based on Lin X  X  precom-future work. 5.1 Integrating Selectional Preferences in Role Classification
For these experiments, we modified the SwiRL SRL system, a state-of-the-art semantic role labeling system (Surdeanu et al. 2007). SwiRL ranked second among the systems that did not implement model combination at the CoNLL-2005 shared task and fifth overall (Carreras and M ` arquez 2005). Because the focus of this section is on role classi-fication, we modified the SRC component of SwiRL to use gold argument boundaries, that is, we assume that semantic role identification works perfectly. Nevertheless, for a realistic evaluation, all the features in the role classification model are generated using actual syntactic trees generated by the Charniak parser (Charniak 2000).
 base models using all resources available and we combine their outputs using multi-ple strategies. Our pool of base models contains 13 different models: The first is the unmodified SwiRL SRC, the next six are the selected SP models from the previous section, and the last six are variants of SwiRL SRC. In each variant, the feature set of the unmodified SwiRL SRC model is extended with a single feature that models the choice of a given SP, for example, SRC+SP res contains an extra feature that indicates the choice of Resnik X  X  SP model. 14 majority voting, which selects the label predicted by most models, and (b) meta-classification, which uses a supervised model to learn the strengths of each base model.
For the meta-classification model, we opted for a binary classification approach: First, for each constituent we generate n data points, one for each distinct role label proposed by the pool of base models; then we use a binary meta-classifier to label each candidate role as either correct or incorrect. We trained the meta-classifier on the usual PropBank training partition, using 10-fold cross-validation to generate outputs for the base models that require the same training material. At prediction time, for each candidate confidence.
 652 5.2 Results for Semantic Role Classification
Table 13 compares the performance of both combination approaches against the stand-alone SRC model. In the table, the SRC+SP  X  models stand for SRC classifiers enhanced with one feature from the corresponding SP. The meta-classifier shown in the table com-bines the output of all the 13 base models introduced previously. We implemented the meta-classifier using Support Vector Machines (SVMs) 15 with a quadratic polynomial kernel, and C = 0 . 01 (tuned in the development set). 16 of the voting strategy, over the same set of base models.
 arguments (Core) and adjunct arguments (Adj.). Note that for the overall SRC scores, we report classification accuracy, defined as ratio of correct predictions over total number of arguments to be classified. The reason for this is that the models in this section always return a label for all arguments to be classified, and thus accuracy, precision, recall, and
F 1 are all equal.
 standalone SRC model in domain (WSJ), and all of them outperform SRC out of domain (Brown). The improvements are small, however, and, generally, not statistically signifi-cant. On the other hand, the meta-classifier outperforms the original SRC model both in domain (17.4% relative error reduction; 1.60 points of accuracy improvement) and out of domain (13.4% relative error reduction; 2.42 points of accuracy improvement), and the differences are statistically significant. This experiment proves our claim that
SPs can be successfully used to improve semantic role classification. It also underscores the fact that combining SRC and SPs is not trivial, however. Our hypothesis is that this is caused by the large performance disparity (20 F domain) between the original SRC model and the standalone SP methods.
 proach in domain and slightly worse out of domain. We believe that this is another effect of the above observation: Given the weaker SP-based features, the meta-classifier does not learn much beyond a majority vote, which is exactly what the simpler, unsuper-vised voting method models. Nevertheless, regardless of the combination method, this experiment emphasizes that infusing SP information in the SRC task is beneficial. core and adjunct arguments. Out of domain, we see a bigger accuracy improvement voting model). This is to be expected, as most core arguments fall under the Arg0 and
Arg1 classes, which can typically be disambiguated based on syntactic information (i.e., subject vs. object). On the other hand, there are no syntactic hints for adjunct arguments, so the system learns to rely more on SP information in this case.
 (e.g., SRC+SP Res ), the differences among SP models in Table 13 are much smaller than in Table 12. SP sim pre distributional methods are slightly stronger than WordNet-based methods. SP
SP wn perform similarly when combined, with a small lead for Resnik X  X  method. The smaller differences and changes in the rank among SP methods are due to the complex interactions when combining SP models with the SRC system.
 654 inal SRC model and the meta-classifier (results are also presented over all numbered arguments, Core, adjuncts, and Adj). This comparison emphasizes the previous obser-vation that SPs are more useful for arguments that are independent of syntax than for arguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1). For example, in domain the meta-classifier improves Arg0 classification with 1.1 F but it boosts the classification performance for causative arguments (AM-CAU) with 13 absolute points. A similar behavior is observed out of domain. For example, whereas
Arg0 classification is improved with 1.7 points, the classification of manner arguments (AM-MNR) is improved by 11 points. All in all, with two exceptions, selectional prefer-ences improve classification accuracy for all argument types, both in and out of domain. over a battery of base models improves over the performance of each individual clas-sifier. Given that half of our base models are all relatively minor changes of the same performance gain of the meta-classification system is due to the infusion of semantic information that is missing in the baseline SRC, and not to a regularization effect coming reinforce this hypothesis. 5.3 Results for End-to-End Semantic Role Labeling
Lastly, we investigate the contribution of SPs in an end-to-end SRL system. As discussed before, our approach focuses on argument classification, a subtask of complete SRL, because this component suffers in the presence of lexical data sparseness (Pradhan,
Ward, and Martin 2008). To understand the impact of SPs on the complete SRL task we compared two SwiRL models: one that uses the original classification model (the SRC line in Table 13). To implement this experiment we had to modify the publicly down-loadable SwiRL model, which performs identification and classification jointly, using a single multi-class model. We changed this framework to a pipeline model, which first performs argument identification (i.e., is this constituent an argument or not?), followed by argument classification (i.e., knowing that this constituent is an argument, what is original model to identify argument boundaries. This pipeline model allowed us to easily plug in different classification models, which offers a simple platform to evaluate the contribution of SPs in an end-to-end SRL system.
 model where the classification component was replaced with the meta-classifier previ-ously introduced ( SwiRL w/ meta). The latter model backs off to the original classifi-cation model for candidates that are not covered by our current selectional preferences second child). We report results for the test partitions of WSJ and Brown in the same table. Note that these results are not directly comparable with the results in Tables 13 and 14, because in those initial experiments we used gold argument boundaries whereas
Table 15 shows results for an end-to-end model, which includes predicted argument boundaries.
 using predicted argument boundaries as well. Selectional preferences improve F for four out of five core arguments in both WSJ and Brown, for six out of nine modifier arguments in WSJ, and for seven out of nine modifier arguments in Brown. Notably, the SPs improve results for the most common argument types (Arg0 and Arg1). All in all, SPs yield a 0.4 F 1 point improvement in WSJ and 0.6 F 1 point improvement in Brown.
These improvements are small but they are statistically significant. We consider these re-sults encouraging, especially considering that only a small percentage of arguments are actually inspected by selectional preferences. This analysis is summarized in Table 16, which lists how many argument candidates are inspected by the system in its different out by the argument identification component, which does not use SPs. Because of this, even though approximately 50% of the role classification decisions can be reinforced with SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ and
Brown, respectively, are actually inspected by the classification model that uses SPs. 6. Analysis and Discussion
We conducted a complementary manual analysis to further verify the usefulness of the semantic information provided by the selectional preferences. We manually inspected 100 randomly selected classification cases, 50 examples in which the meta-classifier is 656 correct and the baseline SRC ( SwiRL ) is wrong, and 50 where the meta-classifier chooses the incorrect classifier and the SRC is right. Interestingly, we observed that the majority of cases have a clear linguistic interpretation, shedding light on the reasons why the meta-classifier using SP information manages to correct some erroneous predictions of the original SRC model, but also on the limitations of selectional preferences. to low frequency verb X  X rgument head pairs, in which the baseline SRC might have had problems with generalization. In 29 of the cases (  X  58%), the syntactic information predictions are correct, however, so the meta-classifier does select the correct role label.
In another 15 cases (  X  30%) the source of the baseline SRC error is not clear, but still, several SP models suggest the correct role, giving the opportunity to the meta-classifier to make the right choice. Finally, in the remaining six cases ( observed: The failure of the baseline SRC model does not have a clear interpretation and, moreover, most SP predictions are actually wrong. In these situations, several labels are predicted with the same confidence, and the meta-classifier selects the correct one by chance.
 mation provided by the selectional preferences. In example (a), the verb flash never occurs in training with the argument head word news . The syntactic structure alone strongly suggests Arg0, because the argument is an NP just to the left of a verb in active form. This is probably why the baseline SRC incorrectly predicts Arg0. Some semantic information is needed to know that the word news is not the agent of the predicate work perfectly, because all variants predict the correct label by signaling that news is much more compatible with flash in Arg1 position rather than Arg0.
 Recipient) and an action as Arg2 ( promised action , Theme). Moreover, the presence of
Arg2 is obligatory. The syntactic structure is correct but does not provide the semantic required an additional Arg2 ) needed to select the appropriate role. SwiRL does not have it either, and it assigns the incorrect Arg1 label. Most SP models correctly predict that investigation is more similar to the heads of Arg2 arguments of promise than to the heads of Arg1 arguments, however.  X  from ayear ago X  (Surdeanu et al. 2003). In PropBank,  X  X ear X  is a strong indicator of a temporal adjunct (AM-TMP). The predicate double , however, describes the Arg3 argument as  X  X tarting point X  of the action and it is usually introduced by the preposition from . This is very common also for other motion verbs ( go , rise , etc.), resulting in the from -Arg3 selectional preference containing a number of heads of temporal expressions, in particular many more instances of the word year than the from -AM-TMP selectional preference. As a consequence, the majority of SP models predict the correct Arg3 label. front of parsing errors. In this example, the NP  X  X  superco. detergent  X  is incorrectly attached to  X  X egin  X  instead of the predicate testing by the syntactic parser. This produces many incorrect features derived from syntax (syntactic frame, path, etc.) that may con-fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.
Most of the SP models, however, predict that detergent is not a plausible Agent for test ( X  X xaminer X ), but instead it fits best with the Arg1 position ( X  X xamined X ).
 model syntactic structures, which often give strong hints for classification. In fact, the vast majority of the situations where the meta-classifier performs worse than the origi-nal SRC model are cases that are syntax-driven, hence situations that are incompletely addressed by the current SP models. Even though the SRC and the SRC+SP models have features that model syntax, they can be overwhelmed by the SP features and standalone models, which leads to incorrect meta-classification results. Figure 3 shows a few representative examples in this category. In the first example in the figure, the meta-classifier changes the correctly assigned label Arg2 to Arg1, because most SP models favor the Arg1 label for the argument  X  X est. X  In the PropBank training corpus, however, the argument following the verb fail is labeled Arg2 in 79% of the cases. Because the
SP models do not take into account syntax or positional information, this syntactic preceded by an Arg1 argument, or the argument immediately following the verb precede tends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c). All these 658 examples are strong motivation for SP models that model both lexical and syntactic preferences. We will address such models in future work. 7. Conclusions
Current systems usually perform SRL in two pipelined steps: argument identification and argument classification. Whereas identification is mostly syntactic, classification requires semantic knowledge to be taken into account. In this article we have shown and that selectional preferences are able to generalize those lexical heads. In fact, we preference models with a state-of-the-art SRC system yields significant improvements in both in-domain and out-of-domain test sets. These improvements to role classification translate into small but statistically significant improvements in an end-to-end semantic role labeling system. We find these results encouraging considering that in the complete semantic role labeling task only a small percentage of argument candidates are affected by our modified role classification model. The experiments were carried out over the well-known CoNLL-2005 data set, based on PropBank.
 tional similarity. Our experiments show that all models outperform the pure lexical matching approach, with distributional methods performing better that WordNet-based methods, and second-order similarity models being the best. In addition to the tradi-tional selectional preferences for verbs, we introduce the use of selectional preferences for prepositions, which are applied to classifying prepositional phrases. The combi-nation of both types of selectional preferences improves over the use of selectional preferences for verbs alone.
 bined system differed showed that the selectional preferences are specially helpful when syntactic information is either incorrect or insufficient to disambiguate the correct role.
The analysis also highlighted that the limitations of selectional preferences for modeling syntactic structures introduce some errors in the combined model. Those errors could be addressed if the SP models included some syntactic information.
 information for Semantic Role Labeling. We introduced selectional preferences in the
SRC system as simple features, but models which extend syntactic structures with selectional preferences (or vice versa) could overcome some of the errors that our system introduced. Extending the use of selectional preferences to other syntactic types beyond noun phrases and prepositional phrases would be also of interest. In addition, the method for combining selectional preferences for verbs and prepositions was naive, and we expect that a joint model of verb and preposition preferences for prepositional phrases would improve results further. Finally, individual selectional preference meth-ods could be improved and newer methods incorporated, which could further improve the results.
 Acknowledgments References 660 662
