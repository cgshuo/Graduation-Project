 We present a simple and efficient external perfect hashing scheme (referred to as EPH algorithm) for very large static key sets. We use a number of techniques from the liter-ature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magni-tude increase in the size of the problem to be solved com-pared to previous  X  X ractical X  methods. We demonstrate the scalability of our algorithm by constructing minimum per-fect hash functions for a set of 1.024 billion URLs from the World Wide Web of average length 64 characters in approxi-mately 62 minutes, using a commodity PC. Our scheme pro-duces minimal perfect hash functions using approximately 3.8 bits per key. For perfect hash functions in the range { 0 ,..., 2 n  X  1 } the space usage drops to approximately 2.7 bits per key. The main contribution is the first algorithm that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage care-fully analyzed without unrealistic assumptions.
 E.1 [ Data Structures ]: Graphs and networks; E.2 [ Data Storage Representations ]: Hash-table representations Algorithms, Design, Performance, Theory minimal, perfect, hash, functions, large, key sets
Perfect hashing is a space-efficient way of creating com-pact representation for a static set S of n keys. For appli-cations with successful searches, the representation of a key x  X  S is simply the value h ( x ), where h is a perfect hash function for the set S of values considered. A perfect hash function (PHF) maps the elements of S to unique values. A minimal perfect hash function (MPHF) produces values that are integers in the range [0 ,n  X  1], which is the smallest possible range. More formally, a PHF maps a static key set S  X  U of size | S | = n to unique values in the range [0 ,m where m  X  n and U is a key universe of size u .If m = n we have a MPHF.

MPHFs are used for memory efficient storage and fast retrieval of items from static sets, such as words in nat-ural languages, reserved words in programming languages or interactive systems, univers al resource locations (URLs) in web search engines [7], item sets in data mining tech-niques [8, 9], sparse spatial data [22], graph compression [3], routing tables and other network applications [28].
Some types of databases are updated only rarely, typi-cally by periodic batch updates. This is true, for example, for most data warehousing applications (see [31] for more examples and discussion). In such scenarios it is possible to improve query performance by creating very compact repre-sentations of keys by MPHFs. In applications where the set of keys is fixed for a long period of time the construction of a MPHF can be done as part of the preprocessing phase.
APHFcanalsobeusedtoimplementadatastructure with the same functionality as a Bloom filter [26]. In many applications where a set S of elements is to be stored, it is acceptable to include in the set some false positives 1 with a small probability by storing a signature for each perfect hash value. This data structure requires around 30% less space usage when compared with Bloom filters, plus the space for the PHF. Bloom filters have applications in dis-tributed databases and data mining [8, 9].

Perfect hash functions have also been used to speed up the partitioned hash-join algorithm presented in [24]. By using a PHF to reduce the targeted hash-bucket size from 4 tuples to just 1 tuple they have avoided following the bucket-chain during hash-lookups that causes too many cache and translation lookaside buffer (TLB) misses.

Though there has been considerable work on how to con-struct PHFs, there is a gap be tween theory and practice among all previous methods on minimal perfect hashing. On one side, there are good theoretical results without ex-perimentally proven practicality for large key sets. On the other side, there are the theoretically analyzed time and space usage algorithms that assume that truly random hash functions are available for free, which is an unrealistic as-sumption.
False positives are elements that appear to be in S but are not.
In this paper we attempt to bridge this gap between the-ory and practice, using a number of techniques from the literature to obtain a novel external memory based perfect hashing scheme, referred to as EPH algorithm . The EPH al-gorithm increases one order of magnitude in the size of the greatest key set for which a MPHF was obtained in the lit-erature [4]. This improvement comes from a combination of a novel, theoretically optimal perfect hashing scheme that greatly simplifies previous methods, and the fact that our algorithm is designed to make good use of the memory hi-erarchy (see Section 4.2 for details).

The minimum amount of space to represent a MPHF for a given set S is known to be approximately 1 . 4427 bits per key. We present a scalable algorithm that produces MPHFs using approximately 3.8 bits per key. Also, for applications that can allow values in the range { 0 ,..., 2 n  X  1 } , the space usage drops to approximately 2.7 bits per key. We demonstrate the scalability of the EPH algorithm by considering a set of 1.024 billion strings (URLs from the world wide web of average length 64), for which we construct a MPHF on a commodity PC in approximately 62 minutes. If we use the range { 0 ,..., 2 n  X  1 } , the space for the PHF is less than 324 MB, and we still get hash values that can be represented in a 32 bit word. Thus we believe our MPHF method might be quite useful for a number of current and practical data management problems.
There is a gap between theory and practice among mini-mal perfect hashing methods. On one side, there are good theoretical results without experimentally proven practical-ity for large key sets. We will argue below that these meth-ods are indeed not practical. On the other side, there are two categories of practical algorithms: the theoretically an-alyzed time and space usage algorithms that assume truly random hash functions for their methods, which is an unre-alistic assumption because each truly random hash function h : U  X  [0 ,m  X  1] require at least u log m bits of storage space, and the algorithms that present only empirical evi-dences. The aim of this section is to discuss the existent gap among these three types of algorithms available in the literature.
In this section we review some of the most important the-oretical results on minimal perfect hashing. For a complete survey until 1997 refer to Czech, Havas and Majewski [11].
Fredman, Koml  X  os and Szemer  X  edi [15] proved, using a counting argument, that at least n log e +loglog u  X  O (log n ) bits are required to represent a MPHF 2 , provided that u  X  n 2+ j for some j&gt; 0 (an easier proof was given by Radhakrishnan [29]). Mehlhorn [25] has made this bound almost tight by providing an algorithm that con-structs a MPHF that can be represented with at most n log e +loglog u + O (log n ) bits. However, his algorithm is far away from practice because its construction and eval-uation time is exponential on n (i.e., n  X  ( ne n u log u
Schmidt and Siegel [30] have proposed the first algorithm for constructing a MPHF with constant evaluation time and description size O ( n +loglog u ) bits. Nevertheless, the scheme is hard to implement and the constants associated
Logarithms are in base 2. with the MPHF storage are prohibitive. For a set of n keys, at least 29 n bits are used, which means that the space usage is similar in practice to schemes using O ( n log n )bits. One important theoretical result was proposed by Hagerup and Tholey [17]. The MPHF obtained can be evaluated in O (1) time and stored in n log e +loglog u + O ( n (log log n ) 2 / log n + log log log u ) bits. The construction time is O ( n +loglog u )using O ( n ) computer words of the Fredman, Koml  X  os and Szemer  X  edi [16] model of computa-tion. In this model, also called the Word RAM model, an element of the universe U fits into one machine word, and arithmetic operations and memory accesses have unit cost. The Hagerup and Tholey [17] algorithm emphasizes asymp-totic space complexity only. As discussed in [5], for n&lt; 2 the scheme is not even defined, as it relies on splitting the key set into buckets of size  X  n  X  log n/ (21 log log n ). By let-ting the bucket size be at least 1, then for n&lt; 2 300 buckets of size one will be used, which means that the space usage will be at least (3 log log n + log 7) n bits. For a set of a bil-lion keys, this is more than 17 bits per element. In addition, the algorithm is also very complicated to implement, but we will not go into that. Thus, it is safe to conclude that the Hagerup-Tholey algorithm will not be space efficient in practical situations.
Let us now describe the main practical results analyzed with the unrealistic assumption that truly random hash functions are available for free.

The family of algorithms proposed by Czech et al [23] uses random hypergraphs to construct order preserving MPHFs. APHF h is order preserving if the keys in S are arranged in some given order and h preserves this order in the hash table. One of the methods uses two truly random hash func-tions h 1 ( x ): S  X  [0 ,cn  X  1] and h 2 ( x ): S  X  [0 ,cn generate MPHFs in the following form: h ( x )=( g [ h 1 ( x )] + g [ h 2 ( x )] mod n where c&gt; 2. The resulting MPHFs can be evaluated in O (1) time and stored in O ( n log n ) bits (that is optimal for an order preserving MPHF). The resulting MPHF is generated in expected O ( n ) time. Botelho, Ko-hayakawa and Ziviani [4] improved the space requirement at the expense of generating functions in the same form that are not order preserving. Their algorithm is also linear on n , but runs faster than the ones by Czech et al [23] and the resulting MPHF are stored using half of the space because c  X  [0 . 93 , 1 . 15]. However, the resulting MPHFs still need O ( n log n )bitstobestored.

Since the space requirements for truly random hash func-tions makes them unsuitable for implementation, one has to settle for a more realistic setup. The first step in this direction was given by Pagh [27]. He proposed a family of randomized algorithms for constructing MPHFs of the form h ( x )=( f ( x )+ d [ g ( x )]) mod n ,where f and g are chosen from a family of universal hash functions and d is a set of displacement values to resolve collisions that are caused by the function f . Pagh identified a set of conditions concern-ing f and g and showed that if these conditions are satisfied, then a MPHF can be computed in expected time O ( n )and stored in (2 + ) n log n bits.

Dietzfelbinger and Hagerup [12] improved the algorithm proposed in [27], reducing from (2+ ) n log n to (1+ ) n log n the number of bits required to store the function, but in their approach f and g must be chosen from a class of hash functions that meet additional requirements. Woelfel [33] has shown how to decrease the space usage to O ( n log log n ) bits asymptotically. However, there is no empirical evidence on the practicality of this scheme.

Botelho, Pagh and Ziviani [5] presented a family F of practical algorithms for construction and evaluation of PHFs of a given set that uses O ( n ) bits to be stored, runs in lin-ear time and the evaluation of a function requires constant time. The algorithms in F use r -uniform random hyper-graphs given by function values of r hash functions on the keys of S .For r = 2 they obtained a space usage of (3 + ) n bits for a MPHF, for any constant &gt; 0. For r =3they obtained a space usage of approximately 2 . 62 n bits for a MPHF, which is within a factor of 2 from the information theoretical lower bound of approximately 1 . 4427 n bits. For m =1 . 23 n they obtained a space usage of 1 . 95 n bits, which is slightly more than two times the information theoretical lower bound of approximately 0 . 89 n bits.
In this section we discuss results that present only empir-ical evidences for specific applications. Fox et al. [14] cre-ated the first scheme with good average-case performance for large data sets, i.e., n  X  10 6 . They have designed two algorithms, the first one generates a MPHF that can be eval-uated in O (1) time and stored in O ( n log n )bits. Thesecond algorithm uses quadratic hashing and adds branching based on a table of binary values to get a MPHF that can be eval-uated in O (1) time and stored in c ( n +1 / log n )bits. They argued that c would be typically lower than 5, however, it is clear from their experimentation that c grows with n and they did not discuss this. They claimed that their algorithms would run in linear time, but, it is shown in [11, Section 6.7] that the algorithms have exponential running times in the worst case, although the worst case has small probability of occurring.

Fox, Chen and Heath [13] improved the result of [14] to get a function that can be stored in cn bits. The method uses four truly random hash functions to construct a MPHF. Again c is only established for small values of n .Itcould very well be that c grows with n . So, the limitation of the three algorithms from [14] and [13] is that no guarantee on the size of the resulting MPHF is provided.

Lefebvre and Hoppe [22] have recently designed MPHFs that require up to 7 bits per key to be stored and are tai-lored to represent sparse spatial data. In the same trend, Chang, Lin and Chou [8, 9] have designed MPHFs tailored for mining association rules and traversal patterns in data mining techniques.
In this work we propose a practical algorithm that is the-oretically well-understood and performs efficiently for very large key sets. To the best of our knowledge the EPH al-gorithm is the first one that demonstrates the capability of generating MPHFs for sets in the order of billions of keys, and the generated functions require less than 4 bits per key to be stored. This increases one order of magnitude in the size of the greatest key set for which a MPHF was obtained in the literature [4], mainly because the EPH algorithm is designed to make good use of the memory hierarchy, as dis-cussed in Section 4.2. We need O ( N ) computer words, where N n , for the construction process. Notice that both space usage for representing the MPHF and the construction time are carefully proven. Additionally, our scheme is much sim-pler than previous theoretical well-founded schemes.
Our algorithm uses the well-known idea of partitioning the key set into a number of small sets 3 (called  X  X uckets X ) using a hash function h 0 .Let B i = { x  X  S | h 0 ( x )= i denote the i th bucket. If we define offset [ i ]= let p i denote a MPHF for B i then clearly is a MPHF for the whole set S . Thus, the problem is reduced to computing and storing the offset array, as well as the MPHF for each bucket.

The EPH algorithm is essentially a two-phase multi-way merge sort with some nuances to make it work in linear time. Figure 1 illustrates the two steps of the EPH algorithm: the partitioning step and the searching step . The partitioning step takes a key set S and uses a hash function h 0 to par-tition S into 2 b buckets. The searching step generates a MPHF p i for each bucket i ,0  X  i  X  2 b  X  1 and computes the offset array. To compute the MPHF of each bucket we used one algorithm from the family of algorithms proposed by Botelho, Pagh and Ziviani [5] (see Section 3.3 for details on the algorithm).
We will choose h 0 such that it has values in { 0 , 1 } b some integer b . Since the offset array holds 2 b entries of at least log n bits we want 2 b to be less than around n/ log n , making the space used for the offset array negligible. On the other hand, to allow efficient implementation of the func-tions p i we impose an upper bound onthesizeofany bucket. We will describe later how to choose h 0 such that this upper bound holds.

To create the MPHFs p i we could choose from a number of alternatives, emphasizing either space usage, construc-tion time, or evaluation time. We show that all methods based on the assumption of truly random hash functions can be made to work, with explicit and provably good hash functions. For the experiments we have implemented the Used in e.g. the perfect hash function constructions of Schmidt and Siegel [30] and Hagerup and Tholey [17], for suitable definition of  X  X mall X . algorithm presented in [5] (see Section 3.3 for more details). Since this computation is done on a small set, we can expect nearly all memory accesses to be  X  X ache hits X . We believe that this is the main reason why our method performs better than previous ones that access memory in a more  X  X andom X  fashion.

We consider the situation in which the set of all keys may not fit in the internal memory and has to be written on disk. The EPH algorithm first scans the list of keys and computes the hash function values that will be needed later on in the algorithm. These values will (with high probability) distin-guish all keys, so we can discard the original keys. It is well known that hash values of at least 2 log n bits are required to make this work. Thus, for sets of a billion keys or more we cannot expect the list of hash values to fit in the internal memory of a standard PC.

To form the buckets we sort the hash values of the keys according to the value of h 0 . Since we are interested in scalability to large key sets, this is done using an implemen-tation of an external memory mergesort [21]. If the merge sort works in two phases, which is the case for all reasonable parameters, the total work on the disk consists of reading the keys, plus writing and reading the hash function values once. Since the h 0 hash values are relatively small (less than 15 decimal digits) we can use radix sort to do the internal memory sorting.
 We have designed two versions of the EPH algorithm. The first one uses the linear hash function described in Sec-tion 3.4.2 that are slower and require more storage space, but guarantee that the EPH algorithm can be made to work for every key set. The second one uses faster and more compact pseudo random hash functions proposed by Jenk-ins [19]. This version is, from now on, referred to as heuristic EPH algorithm because it is not guaranteed that it can be made to work for every key set. However, empirical studies show that limited randomness p roperties are often as good as total randomness in practice and, the heuristic EPH has worked for all key sets we have applied it to.

The detailed description of the partitioning and search-ing steps are presented in Sections 3.1 and 3.2, respectively. The internal algorithm used to compute the MPHF of each bucket is from [5] and is presented in Section 3.3. The inter-nal algorithm uses two hash functions h i 1 and h i 2 to compute aMPHF p i . These hash functions as well as the hash func-tion h 0 used in the partitioning step of the EPH algorithm are described in Section 3.4. The partitioning step performs two important tasks. First, the variable-length keys are mapped to 128-bit strings by using the linear hash function h presented in Section 3.4. That is, the variable-length key set S is mapped to a fixed-length key set F . Second, the set S of n keys is partitioned into 2 b buckets, where b is a suitable parameter chosen to guarantee that each bucket has at most = 256 keys with high probability (see Section 3.4). We have two reasons for choosing = 256. The first one is to keep the buckets size small enough to be represented by 8-bit integers. The sec-ond one is to allow the memory accesses during the MPHF evaluation to be done in the cache most of the time. Figure 2 presents the partitioning step algorithm.

The critical point in Figure 2 that allows the partitioning step to work in linear time is the internal sorting algorithm. 1 . for j =1 to N do We have two reasons to choose radix sort. First, it sorts each key block B j in linear time, since keys are short integer numbers (less than 15 decimal digits). Second, it just needs O ( |B j | ) words of extra memory so that we can control the memory usage independently of the number of keys in S .
At this point one could ask: why not to use the well known replacement selection algorithm to build files larger than the internal memory area size? The reason is that the radix sort algorithm sorts a block B j in time O ( |B j | ) while the replace-ment selection algorithm requires O ( |B j | log |B j | ). We have tried out both versions and the one using the radix sort algo-rithm outperforms the other. A worthwhile optimization we have used is the last run optimization proposed by Larson and Graefe [21]. That is, the last block is kept in memory instead of dumping it to disk to be read again in the second step of the algorithm.

Figure 3(a) shows a logical view of the 2 b buckets gener-ated in the partitioning step. In reality, the 128-bit strings belonging to each bucket are distributed among many files, as depicted in Figure 3(b). In the example of Figure 3(b), the 128-bit strings in bucket 0 appear in files 1 and N ,the 128-bit strings in bucket 1 appear in files 1, 2 and N ,and so on. Figure 3: Situation of the buckets at the end of the partitioning step: (a) Logical view (b) Physical view
This scattering of the 128-bit strings in the buckets could generate a performance problem because of the potential number of seeks needed to read the 128-bit strings in each bucket from the N files on disk during the second step. But, as we show later on in Section 4.3, the number of seeks can be kept small using buffering techniques.
The searching step is responsible for generating a MPHF for each bucket and for computing the offset array. Figure 4 presents the searching step algorithm. Statement 1 of Fig-1 . for j =1 to N do { Heap construction } 2 . for i =0 to 2 b  X  1 do ure 4 constructs the heap H of size N . This is well known to be linear on N . The order relation in H is given by the bucket address i (i.e., the b most significant bits of x  X  Statement 2 has two important steps. In statement 2.1, a bucket is read from disk, as described below. In statement 2.2, a MPHF is generated for each bucket B i using the in-ternal memory based algorithm presented in Section 3.3. In statement 2.3, the next entry of the offset array is computed. Finally, statement 2.4 writes the description of MPHF i and offset [ i ] to disk. Note that to compute offset [ i +1] we just need the current bucket size and offset [ i ]. So, we just need to keep two entries of vector offset in memory all the time.
The algorithm to read bucket B i from disk is presented in Figure 5. Bucket B i is distributed among many files and the heap H is used to drive a multiway merge operation. Statement 1.1 extracts and removes triple ( i, j, x )from H , where i is a minimum value in H . Statement 1.2 inserts x in bucket B i . Statement 1.3 performs a seek operation in File j on disk for the first read operation and reads sequentially all 128-bit strings x  X  F that have the same index i and inserts them all in bucket B i . Finally, statement 1.4 inserts in H the triple ( i ,j,x ), where x  X  F is the first 128-bit string read from File j (in statement 1.3) that does not have the same bucket address as the previous keys. 1 . while bucket B i is not full do
It is not difficult to see from this presentation of the searching step that it runs in linear time. To achieve this conclusion we use O ( N ) computer words to allow the merge operation to be performed in one pass through each file. In addition, it is also important to observe that: 1. 2 b &lt; n log n (see Section 3.4), 2. N n (e.g., see Table 5 in Section 4.3) and 3. the algorithm for the buckets runs in linear time, as
In conclusion, our algorithm takes O ( n )timebecauseboth the partitioning and the searching steps run in O ( n )time. The space required for constructing the resulting MPHF is O ( N ) computer words because the memory usage in the partitioning step does not depend on the number of keys in S and, in the searching step, the algorithm used for the buckets is applied to problems of size up to 256. All together makes our algorithm the first one that demonstrates the capability of generating MPHFs for sets in the order of billions of keys.
For the buckets we decided to use one of the algorithms from the family F of algorithms 4 presented by Botelho, Pagh and Ziviani [5], because it outperforms the algorithms pre-sented in Section 2 and also is a simple and near space-optimal way of constructing a minimal perfect hash func-tion for a set S of n elements. They assume that it is pos-sible to create and access two truly random hash functions f : U  X  [0 , m 2  X  1] and f 1 : U  X  [ m 2 ,m  X  1], where m = cn for c&gt; 2. The Functions f 0 and f 1 are used to map the keys in S to a bipartite graph G =( V, E ), where V =[0 ,m  X  1] and E = {{ f 0 ( x ) ,f 1 ( x ) }| x  X  S } (i.e, | E | = | Hence, each key in S is associated with only one edge from E . Figure 6(a) illustrates this step, referred to as mapping step ,foraset S with three keys. Figure 6: (a) Mapping step generates a bipartite graph (b) Assigning step generates a labeling g so that each edge is uniquely associated with one of its vertices (c) Ranking step builds a function rank : V  X  [0 ,n  X  1]
In the following step, referred to as assigning step , a func-tion g : V  X  X  0 , 1 , 2 } is computed so that each edge is uniquely represented by one of its vertices. For instance,
The algorithms in F use r -uniform random hypergraphs given by function values of r hash functions on the keys of S .An r -graph is a generalization of a standard graph where each edge connects r  X  2 vertices. For the buckets in the EPH algorithm we used the 2-uniform hypergraph instance. in Figure 6(b), edge { 0 , 3 } is associated with vertex 0, edge { 0 , 4 } with vertex 4 and edge { 2 , 4 } with vertex 2. Then, a function  X  : S  X  V defined as  X  ( x )= f i ( x ), where i = i ( x )=( g ( f 0 ( x )) + g ( f 1 ( x ))) mod 2 is a perfect hash function on S .

The assigning step splits the set of vertices into two sub-sets: (i) the assigned ones and (ii) the unassigned ones .A vertex v is defined as assigned if g ( v ) = 2 and unassigned otherwise. Also, the number of assigned vertices is guaran-teed by construction to be equal to | S | = n . Therefore, a function rank : V  X  [0 ,n  X  1] that counts how many vertices are assigned before a given assigned vertex v  X  V is a MPHF on V . For example, in Figure 6(c), rank(0) = 0, rank(2) = 1 and rank(4) = 2, which means that there is no assigned vertex before vertex 0, one before vertex 2, and two before vertex 4. This implies that a function h : S  X  [0 ,n  X  1] de-fined as h ( x )=rank(  X  ( x )) is a MPHF on S . The last step of the algorithm, referred to as ranking step , is responsible for computing the data structures used to compute function rank in time O (1).

Botelho, Pagh and Ziviani [5] have shown that g can be generated in linear time if the bipartite graph G = G ( f is acyclic. When a cyclic graph is generated a new pair ( f 0 ,f 1 ) is randomly selected so that the values of f 0 and f are truly random and independent. Hence the number of iterations to get an acyclic random graph must be bounded by a constant to finish the algorithm in linear time. They have shown that if | V | = m = cn ,for c&gt; 2, the probability of generating an acyclic bipartite random graph is Pr a = p 1  X  (2 /c ) 2 and the number of iterations is on average N 1 /P r a .For c =2 . 09 we have Pr a  X  0 . 29 and N i  X  3 . 4. Finally, they have shown how to compress g and the data structures used to compute function rank in time O (1) so that the resulting MPHFs are stored in (3 + ) n bits for &gt; 0.
The aim of this section is threefold. First, in Section 3.4.1, we define the hash functions h i 1 and h i 2 used by the algo-rithm from [5] to generate the MPHF of each bucket, where 0  X  i  X  2 b  X  1. Second, in Section 3.4.2, we show how to efficiently implement the hash functions h i 1 , h i 2 which is used to split the key set S into 2 b buckets. Third, in Section 3.4.3, we show the conditions that parameter b must meet so that no bucket with more than keys is created by h . We also show that h i 1 and h i 2 are truly random hash functions for the buckets. This section was mostly derived from the technical report Botelho, Pagh and Ziviani [6].
The hash functions h i 1 and h i 2 will be defined based on the linear hash functions over Galois field 2 (or simply GF(2)) analyzed by Alon, Dietzfelbinger, Miltersen and Petrank [1]. For that, a key is defined as a binary vector of fixed length L . This is not a restriction as any variable-length key can be padded with zeros at the end because the ascii character 0 does not appear in any string.

We define h i 1 and h i 2 as where The functions y 1 ,...,y k are hash functions from { 0 , 1 { 0 , 1 } r  X  1 0, where 2 r and k are parameters chosen to make the algorithm work with high probability (see Sec-tion 3.4.3). Note that the range is the set of r -bit strings ending with a 0. The purpose of the last 0 is to ensure that we can have no collision between y j ( x 1 )and y j ( x 1  X  j  X  k , for any pair of elements x 1 and x 2 .Theta-bles t 1 ,...,t 2 k contain 2 r random values from { 0 ,...,p where p is a prime number much larger than | B i | (i.e., the size of the desired range of h i 1 and h i 2 ). The variable s is a random seed number and the symbol  X  denotes exclusive-or. The variable s i is specific to bucket i . The algorithm randomly selects s i from { 1 ,...,p  X  1 } until the functions h 1 and h i 2 work with the algorithm of Section 3.3, which is used to generate MPHFs for the buckets. We will sketch a proof in Section 3.4.3 that a constant fraction of the set of all functions works. The family of linear hash functions proposed by Alon, Dietzfelbinger, Miltersen and Petrank [1] enable us to im-plement the functions h 0 , y 1 ,y 2 ,y 3 ,...,y k to be computed at once. We use a function h from that family that has the following form: h ( x )= Ax ,where x  X  S and A is a  X   X  L matrix in which the elements are randomly chosen from { 0 , 1 } . The output is a bit string of an a priori defined size  X  . In our implementation  X  = 128 bits. It is impor-tant to realize that this is a matrix multiplication over GF (2). The implementation can be done using a bitwise-and operator (&amp;) and a function f : { 0 , 1 }  X   X  X  0 , 1 } parity instead of multiplying numbers. The parity function f ( a ) produces 1 as a result if a  X  X  0 , 1 }  X  has an odd number of bits set to 1, otherwise the result is 0. For example, let us consider L =3bits,  X  =3bits, x = 110 and The number of rows gives the required number of bits in the output, i.e.,  X  = 3. The number of columns corresponds to the value of L . Then, where b 1 = f (101 &amp; 110) = 1, b 2 = f (001 &amp; 110) = 0 and b = f (110 &amp; 110) = 0.

To get a fast evaluation time we use a tabulation idea from [2], where we can get evaluation time O ( L/ log  X  )by using space O ( L X  )for  X &gt; 0. Note that if x is short, e.g. 8 bits, we can simply tabulate all the function values and compute h ( x ) by looking up the value h [ x ]inanarray h . To make the same thing work for longer keys, split the ma-trix A into parts of 8 columns each: A = A 1 | A 2 | ... | and create a lookup table h i for each submatrix. Similarly split x into parts of 8 bits, x = x 1 x 2 ...x L/ 8 .Now h ( x ) is the exclusive-or of h i [ x i ], for i =1 ,..., L/ 8 . Therefore, we have set  X  to 256 so that keys of size L can be processed in chunks of log  X  = 8 bits. In our URL collection the largest key has 65 bytes, i.e., L = 520 bits.

The 32 most significant bits of h ( x ), where x  X  S ,are used to compute the bucket address of x , i.e., h 0 note the right shift of bits. The other 96 bits correspond to y ( x ) ,y 2 ( x ) ,...y 6 ( x ), taking k = 6. This would give r = 16, however, to save space for storing the tables used for com-puting h i 1 and h i 2 , we hard coded the linear hash function to make the most and the least significant bit of each chunk of 16 bits equal to zero. Therefore, r = 15. This setup en-able us to solving problems of up to 500 billion keys, which is plenty of for all the applications we know of. If our algorithm fails in any phase, we just restart it. As the parameters are chosen to have success with high probability, the number of times that our algorithm is restarted is O (1).

Finally, the last parameter related to the hash functions we need to talk about is the prime number p .As p must be much larger than the range of h i 1 and h i 2 ,thenwe set it to the largest 32-bit integer that is a prime, i.e, p = 4294967291.
In this section we show that the EPH algorithm can be made to work with high probability. For that we need to analyze the following points: 1. The function h discussed in Section 3.4.2 that was 2. We have imposed an upper bound on the size of the 3. we will now analyze the probability (over the choice 4. Finally, we need to show that it is possible to obtain,
In this section we present the experimental results. We start presenting the experimental setup. We then present the performance of our algorithm considering construction time, storage space and evaluation time as metrics for the resulting functions. Finally, we discuss how the amount of internal memory available affects the runtime of our two-step external memory based algorithm.
The EPH algorithm was implemented in the C language and is available at http://cmph.sf.net under the GNU Lesser General Public License (LGPL). All experiments were carried out on a computer running the Linux operating sys-tem, version 2.6, with a 1 gigahertz AMD Athlon 64 Pro-cessor 3200+ and 1 gigabyte of main memory.

Our data consists of a collection of 1.024 billion URLs collected from the Web, each URL 64 characters long on average. The collection is stored on disk in 60.5 gigabytes of space. We are firstly interested in verifying the claim that the EPH algorithm runs in linear time. Therefore, we run the algorithm for several numbers n of keys in S .Thevalues chosen for n were 32, 128, 512 and 1024 million. We limited the main memory in 512 megabytes for the experiments in order to show that the algorithm does not need much inter-nal memory to generate MPHFs. The size  X  of the a priori reserved internal memory area was set to 200 megabytes. In Section 4.3 we show how  X  affects the runtime of the algorithm. The parameter b (see Eq. (3)) was set to the minimum value that gives us a maximum bucket size lower than = 256. For each value chosen for n ,therespective values for b are 18 , 20 , 22 and 23 bits.

In order to estimate the number of trials for each value of n we use a statistical method for determining a suitable sample size (see, e.g., [18, Chapter 13]). We got that just one trial for each n would be enough with a confidence level of 95%. However, we made 10 trials. This number of trials seems rather small, but, as shown below, the behavior of the EPH algorithm is very stable and its runtime is almost deter-ministic (i.e., the standard deviation is very small) because it is a random variable that follows a (highly concentrated) normal distribution.

Table 1 presents the runtime average for each n ,there-spective standard deviations, and the respective confidence intervals given by the average time  X  the distance from av-erage time considering a confidence level of 95%. Observing the runtime averages we noticed that the algorithm runs in expected linear time, as we have claimed. Better still, it out-puts the resulting MPHF faster than all practical algorithms we know of, because of the following reasons. First, we make good use of the memory hierarchy because the memory ac-cesses during the generation of a MPHF for a given bucket cause cache hits, once the problem was broken down into problems of size up to 256. Second, at searching step we are dealing with 16-byte (128-bit) strings instead of 64-byte URLs. The percentages of the total time spent in the par-titioning step and in the searching are approximately 49% and 51%, respectively. Table 1: EPH algorithm: average time in minutes for constructing a MPHF with confidence level of 95% in a PC using 200 megabytes of internal mem-ory.

In Table 1 we see that the runtime of the algorithm is al-most deterministic. A given bucket i ,0  X  i&lt; 2 b ,isasmall set of keys (at most 256 keys) and, the runtime of the build-ing block algorithm is a random variable X i with high fluctu-ation (it follows a geometric distribution with mean 1 /P r 3 . 4). However, the runtime Y of the searching step of the EPH algorithm is given by Y = hypothesis that the X i are independent and bounded, the law of large numbers (see, e.g., [18]) implies that the ran-dom variable Y/ 2 b converges to a constant as n  X  X  X  .This explains why the runtime is almost deterministic.
The next important metric on MPHFs is the space re-quired to store the functions. In order to apply the algorithm used for the buckets to larger sets we randomly choose f 0 and f 1 from the family of universal hash functions proposed by Thorup [32]. Botelho, Pagh and Ziviani [5] have ana-lyzed that algorithm under the full randomness assumption so that universal hashing is not enough to guarantee that it works for every key set. But it has been the case for every key set we have applied it to. Then, we refer to this version as heuristic BPZ algorithm .

The EPH algorithm is designed to be used when the key set does not fit in main memory. Table 2 shows that it can be used for constructing PHFs and MPHFs that require on average 2.7 and 3.8 bits per key to be stored, respectively.
The lookup tables used by the hash functions of the EPH Table 2: EPH algorithm: space usage to respectively store the resulting PHFs and MPHFs. algorithm require a fixed storage cost of 1,847,424 bytes. To avoid the space needed for lookup tables we have im-plemented the heuristic EPH algorithm. It uses the pseudo random hash function proposed by Jenkins [19] to replace the hash functions described in Section 3.4. The Jenkins function just loops around the key doing bitwise operations over chunks of 12 bytes. Then, it returns the last chunk. Thus, in the mapping step, the key set S is mapped to F, which now contains 12-byte long strings instead of 16-byte long strings.

The Jenkins function needs just one random seed of 32 bits to be stored instead of quite long lookup tables, a great improvement from the 1,847,424 bytes necessary to imple-ment truly random hash functions. Therefore, there is no fixed cost to store the resulting MPHFs, but two random seeds of 32 bits are required to describe the functions h and h i 2 of each bucket. As a consequence, the MPHFs gen-eration and the MPHFs efficiency at retrieval time are faster (see Table 3 and 4). The reasons are twofold. First, we are dealing with 12-byte strings computed by the Jenkins func-tion instead of 16-byte strings of the truly random functions presented in Section 3.4. Second, there are no large lookup tables to cause cache misses . For example, the construction time for a set of 1024 million keys has dropped down to 64 . 3 minutes in the same setup. The disadvantage of using the Jenkins function is that there is no formal proof that it works for every key set. That is why the hash functions we have designed in this paper are required, even being slower. In the implementation available, the hash functions to be used can be chosen by the user.

Table 3 presents a comparison of our algorithm with the ones proposed by Botelho, Pagh and Ziviani [5] (BPZ), by Pagh [27] (Hash-displace), by Botelho, Kohayakawa and Zi-viani [4] (BKZ), by Czech, Havas and Majewski [10] (CHM), and by Fox, Chen and Heath [13] (FCH), considering con-struction time and storage space as metrics. Notice that they are the most important practical results on MPHFs known in the literature. Observing the results, the EPH algorithm is the fastest one at construction time and the heuristic BPZ algorithm builds slightly more compact func-tions.

Finally, we show how efficient are the resulting MPHFs at retrieval time for the methods aforementioned, which is as important as construction time and storage space. Table 4 presents the time, in seconds, to evaluate 2  X  10 6 keys. We group the BKZ and CHM methods together because the re-sulting MPHFs have the same form. The MPHFs generated by the EPH algorithm are slower. Nevertheless, the differ-ence is not so expressive (each key can be evaluated in few microseconds) and the EPH algorithm is the first efficient option for sets that do not fit in main memory. Table 3: Construction time and storage space with-out considering the fixed cost to store lookup tables.
It is important to emphasize that the BPZ, BKZ, CHM and FCH methods were analyzed under the full randomness assumption. Therefore, the EPH algorithm is the first one that has experimentally proven practicality for large key sets and has both space usage for representing the resulting func-tions and the construction time carefully proven. Addition-ally, it is the fastest algorithm for constructing the functions and the resulting functions are much simpler than the ones generated by previous theoretical well-founded schemes so that they can be used in practice. Also, it considerably im-proves the first step given by Pagh with his hash and displace method [27].
In order to bring down the number of seek operations on disk we benefit from the fact that the EPH algorithm leaves almost all main memory available to be used as disk I/O buffer. In this section we evaluate how much the parameter  X  affects the runtime of the EPH algorithm. For that we fixed n in 1 . 024 billion of URLs, set the main memory of the machine used for the experiments to 1 gigabyte and used  X  equal to 100, 200, 300, 400 and 500 megabytes.

In order to amortize the number of seeks performed we use a buffering technique [20]. We create a buffer j of size =  X /N for each file j ,where1  X  j  X  N .Everytime a read operation is requested to file j and the data is not found in the j th buffer, bytes are read from file j to buffer j . Hence, the number of seeks performed in the worst case is given by  X / (remember that  X  is the size in bytes of the fixed-length key set F ). Forthatwehavemadethe pessimistic assumption that one seek happens every time buffer j is filled in. Thus, the number of seeks performed in the worst case is 16 n/ , since after the partitioning step we are dealing with 128-bit (16-byte) strings instead of 64-byte URLs, on average. Therefore, the number of seeks is linear on n and amortized by . It is important to emphasize that the operating system uses techniques to diminish the number of seeks and the average seek time. This makes the amortization factor to be greater than in practice.
Table 5 presents the number of files N , the buffer size used for all files, the number of seeks in the worst case con-sidering the pessimistic assumption aforementioned, and the time to generate a PHF or a MPHF for 1 . 024 billion of keys as a function of the amount of internal memory available. Observing Table 5 we noticed that the time spent in the construction decreases as the value of  X  increases. However, for  X &gt; 400, the variation on the time is not as significant as for  X   X  400. This can be explained by the fact that the kernel 2.6 I/O scheduler of Linux has smart policies for avoiding seeks and diminishing the average seek time (see http://www.linuxjournal.com/article/6931 ). Table 5: Influence of the internal memory area size (  X  ) in the EPH algorithm runtime to construct PHFs or MPHFs for 1.024 billion keys (time in min-utes).
This paper has presented a novel external memory based algorithm for constructing PHFs and MPHFs. The algo-rithm can be used with provably good hash functions or with heuristic hash functions that are faster to compute.
The EPH algorithm contains, as a component, a provably good implementation of the BPZ algorithm [5]. This means that the two hash functions h i 1 and h i 2 (see Eq. (3)) used instead of f 0 and f 1 behave as truly random hash functions (see Section 3.4.3). The resulting PHFs and MPHFs require approximately 2.7 and 3.8 bits per key to be stored and are generated faster than the ones generated by all previ-ous methods. The EPH algorithm is the first one that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage carefully an-alyzed without unrealistic assumptions. As a consequence, the EPH algorithm will work for every key set.

The resulting functions of the EPH algorithm are approx-imately four times slower than the ones generated by all previous practical methods (see Table 4). The reason is that to compute the involved hash functions we need to ac-cess lookup tables that do not fit in the cache. To overcome this problem, at the expense of losing the guarantee that it works for every key set, we have proposed a heuristic ver-sion of the EPH algorithm that uses a very efficient pseudo random hash function proposed by Jenkins [19]. The result-ing functions require the same storage space, are now less than twice slower to be computed and are still faster to be generated.
We thank Rasmus Pagh for helping us with the analy-sis of the EPH algorithm, which was previously published in Botelho, Pagh and Ziviani [6]. We also thank the partial sup-port given by GERINDO Project X  X rant MCT/CNPq/CT-INFO 552.087/02-5, and CNPq Grants 30.5237/02-0 (Nivio Ziviani) and 142786/2006-3 (Fabiano C. Botelho). [1] N. Alon, M. Dietzfelbinger, P. B. Miltersen, [2] N. Alon and M. Naor. Derandomization, witnesses for [3] P. Boldi and S. Vigna. The webgraph framework i: [4] F. Botelho, Y. Kohayakawa, and N. Ziviani. A [5] F. Botelho, R. Pagh, and N. Ziviani. Simple and [6] F. C. Botelho, R. Pagh, and N. Ziviani. Perfect [7] S. Brin and L. Page. The anatomy of a large-scale [8] C.-C. Chang and C.-Y. Lin. A perfect hashing [9] C.-C. Chang, C.-Y. Lin, and H. Chou. Perfect hashing [10] Z. Czech, G. Havas, and B. Majewski. An optimal [11] Z. Czech, G. Havas, and B. Majewski. Fundamental [12] M. Dietzfelbinger and T. Hagerup. Simple minimal [13] E. Fox, Q. Chen, and L. Heath. A faster algorithm for [14] E. Fox, L. S. Heath, Q. Chen, and A. Daoud. Practical [15] M. L. Fredman, J. Koml  X  os, and E. Szemer  X  edi. On the [16] M. L. Fredman, J. Koml  X  os, and E. Szemer  X  edi. Storing [17] T. Hagerup and T. Tholey. Efficient minimal perfect [18] R. Jain. The art of computer systems performance [19] B. Jenkins. Algorithm alley: Hash functions. Dr. [20] D. E. Knuth. The Art of Computer Programming: [21] P. Larson and G. Graefe. Memory management during [22] S. Lefebvre and H. Hoppe. Perfect spatial hashing. [23] B. Majewski, N. Wormald, G. Havas, and Z. Czech. A [24] S. Manegold, P. A. Boncz, and M. L. Kersten. [25] K. Mehlhorn. Data Structures and Algorithms 1: [26] A. Pagh, R. Pagh, and S. S. Rao. An optimal bloom [27] R. Pagh. Hash and displace: Efficient evaluation of [28] B. Prabhakar and F. Bonomi. Perfect hashing for [29] J. Radhakrishnan. Improved bounds for covering [30] J. P. Schmidt and A. Siegel. The spatial complexity of [31] M. Seltzer. Beyond relational databases. ACM Queue , [32] M. Thorup. Even strongly universal hashing is pretty [33] P. Woelfel. Maintaining external memory efficient hash
