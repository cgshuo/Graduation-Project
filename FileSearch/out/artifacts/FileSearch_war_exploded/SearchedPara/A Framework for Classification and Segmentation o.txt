 In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to per-form quick online classification and recognition of massive voice streams. Typically such applications are encountere d in real time intelligence and surveillance. In many cases, t he data streams can be in compressed format, and the rate of data processing can often run at the rate of Gigabits per sec-ond. All known techniques for speaker voice analysis requir e the use of an offline training phase in which the system is trained with known segments of speech. The state-of-the-ar t method for text-independent speaker recognition is known as Gaussian Mixture Modeling (GMM), and it requires an iterative Expectation Maximization Procedure for trainin g, which cannot be implemented in real time. In this paper, we discuss the details of such an online voice recognition system. For this purpose, we use our micro-clustering algo-rithms to design concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such a system is that while it was origi-nally designed only for efficiency, we later discovered that i t was also more accurate than the widely used Gaussian Mix-ture Model (GMM). This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possib le to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective. H.2.8 [ Database Management ]: Database Applications Algorithms Speaker Recognition Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
The problem of speaker voice analysis and classification is useful in a number of applications such as real time monitor-ing, detection, and surveillance. In this paper, we are con-centrating on the problem of text-independent speaker clas-sification in which the actual textual content of the speech i s not available for modeling purposes. A number of statistica l and machine learning methods have been recently proposed for speaker classification. Some examples of such technique s may be found in [3].

A well known method for speaker classification and iden-tification is that of Gaussian Mixture Modeling (GMM) [4]. The first step is to extract multi-dimensional feature vecto rs in order to represent portions of sampled speech. In this method, it is assumed that each data point extracted from the speech segments from a number of known speakers are used to estimate the parameters of a GMM model. Then the data points from an unknown speech segment are applied to each speaker model in order to estimate the maximum like-lihood fit. The model with the highest fit is reported as the relevant class label. The GMM modeling method has been widely popular because of its intuitive appeal and effective -ness, and has therefore been used extensively.

In many applications, it is desirable to perform the speaker identification in real time. In such cases, we need to have a system which is adaptive enough to learn characteristics of new speakers in real time, and used these learned profiles in order to construct the final model for speaker classification . Unfortunately, popularly used models such as GMM are not very appropriate for real time speaker modeling. This is because the first step of determining the parameters of the mixture model requires an iterative computationally inten -sive approach known as the EM algorithm. The second stage of model fitting requires the evaluation of a log likelihood criterion on the data set for each model. These techniques can be computationally intensive in practice, and are of-ten difficult to use effectively for real time modeling and classification. Furthermore, we are also interested in othe r mining variations of the classification problem in which it i s desirable to model or match individual segments of speech from unknown speakers in real time. This is not possible with an iterative approach such as the EM algorithm. In addition, since we are making the stream assumption for all data processing, we assume that each point in the data can be scanned only once throughout the computation. This as-sumption is also violated by all other speaker identificatio n systems [3] known to us. Therefore, we need to construct a system which can work with the constraints of a one-pass system and still provide accurate results for speaker ident ifi-cation. In addition to speaker classification, we would like to design a system which is capable of detecting quick changes in the pattern of the underlying data stream. Such a system is useful in applications in which it is desirable to segment portions of speech into different speakers. We note that this segmentation may need to be done in an unsupervised way, since example segments of the different speakers may not be known in advance. In order to design a system which can work with such challenges, we construct a number of techniques which are designed towards stream based online processing of massive amounts of audio data. We adapt our earlier research results for stream micro-clustering [1] i n or-der to create fast cluster based signatures of the data. These signatures are constructed and used in real time in order to perform the speaker identification.

This paper is organized as follows. In the next section, we will describe the speaker recognition system. In section 3, we will present the experimental results obtained from deploying these algorithms over a number of real data sets. Section 4 contains a number of conclusions and observations .
In our online voice recognition system, we will use a non-parametric density estimation approach. The idea is to de-termine the probability distribution of the data for each speaker and map it to the behavior of unknown test seg-ments in order to perform the matching. The reason for using a non-parametric approach is that most parametric methods such as the GMM model require an iterative ap-proach to estimate the densities in the underlying data. On the other hand, a non-parametric approach adapts very well to an online application such as that of clustering data streams. A well known non-parametric method of finding the data distribution is that of kernel density estimation. However, density estimation is often an inefficient method since it requires us to estimate the data behavior over all regions of the data. In high dimensionality, most of the re-gions in the data are sparse, and the estimation will need to be performed over a very large number of data points (which is exponentially increasing with dimensionality) i n order to provide a comprehensive overview of the data be-havior. This can rapidly become untenable for non-uniform data distributions of even modestly high dimensionality. I n order to substitute for using density distributions direct ly, we will use very fine grained micro-clusters in the data. Micro-clustering is a concept which is used to track very detailed level of statistics of clusters with high granular ity. This is a desirable approach when the data sets are received in the form of massive data streams. The distribution of the data for different speakers over these fine grained clusters are used as a surrogate for the actual densities in the under-lying data. We will see that this turns out to be a practical and scalable approach in most applications, and also simu-lates the underlying densities quite well. In order to desig n the voice recognition system, we will build upon some micro-clustering machinery developed in [1]. The micro-clusteri ng framework discusses some summary statistics which are used to maintain the statistical information about the clusters . We assume that the dimensionality of the data stream is d . The definition of a micro-cluster is as follows:
Definition 1. A micro-cluster for a set of d -dimensional points C = { X i 1 . . . X i n } with time stamps T i 1 . . . T fined as the (2  X  d +2) tuple ( CF 2 x ( C ) , CF 1 x ( C ) , CF wherein CF 2 x ( C ) and CF 1 x ( C ) each correspond to a vector of d entries. The definition of each of these entries is as follows:  X  For each dimension, the sum of the squares of the data values is maintained in CF 2 x ( C ) . Thus, CF 2 x ( C ) contains d values. The p -th entry of CF 2 x ( C ) is equal to  X  For each dimension, the sum of the data values is main-tained in CF 1 x ( C ) . The p -th entry of CF 1 x ( C ) is equal to P  X  The last update time (which is max { T i 1 . . . T i n } ) is main-tained in CF t ( C ) .  X  The number of data points is maintained in n ( C ) . The design of micro-cluster statistics is chosen so as to to make some important predictions about the data points in them. We make the following two observations about the statistics [1] stored in the micro-clusters:
Observation 2.1. The mean and variance of the data points in a cluster can be determined from the micro-cluster statistics.
 In addition, the micro-clusters satisfy the additivity property . We omit the details of the proofs since they can be found in [5].

Observation 2.2. The micro-cluster statistics satisfy the additivity property. The micro-cluster statistics for the union of two sets of data points is the sum of the micro-cluster statistics of the individual sets of points.
 As discussed later, the above observations are used dur-ing the clustering process. The clustering process is used to create and store summary frequency information. This summary information is used in order to perform effective classification. A set of a clusters can be used to create a signature summary of the data. This signature summary is defined in terms of the frequencies of the different data points drawn from the segment S , in terms of their distri-bution over the clusters C 1 . . . C k . The signature summary is used as a surrogate for the probability density of the data points in the stream. As we will observe later, this is not an unreasonable assumption when a fine granularity of cluster-ing is maintained. Let us denote the data in the last segment of length S by D ( S ). We define the signature summary of the data set D ( S ) over the segment S with respect to the clusters C 1 . . . C k as follows:
Definition 2. The signature summary  X  ( C 1 . . . C k , S ) for a set of k clusters C 1 . . . C k and segment of data points S is defined as the k -dimensional vector ( f 1 . . . f k ) , where f defined as follows: It is important to note that the signature summary essen-tially defines the relative distribution of the data points across the different clusters. When the individual clusters have very small variance, they can be used as a surrogate for the true density distribution. In order to explain the rela-tionship between the probability density and the frequenci es of the data points across different clusters, we make the fol-lowing observations.

Let X ( C i ) and h ( C i ) 2 be the centroid and variance of clus-ter C i . Then, the density estimate  X  ( x ) at any point x in the space can be constructed as follows: We note that Equation 2 is a modification to the standard density function which is often used in kernel density esti-mation of individual data points. The primary difference is the cluster-specific value of the bandwidth h ( C i ). Here h  X  is an additive bandwidth which is defined in the same way as standard kernel density estimation, except that we use the number of clusters rather than the number of points to de-fine h  X  . According to the Silverman approximation rule, for a data distribution with n points (clusters) and variance  X  of the data points (cluster centroids), the value of h  X  is chosen to be 1 . 06  X   X   X  n  X  1 / 5 . We note that lim n  X  X  X  h  X   X  0. In effect, the density estimate is equal to the sum of Gaussian kernels which are centered at different clusters, and have bandwidth which is defined by the variance of the data points in the cluster. For very fine grained clusters, such an approach provides a very good estimate to the process of kernel den-sity estimation. This is because this limiting case defines a situation in which the value of h ( C i ) tends to zero. On sub-stituting the value of h ( C i ) = 0 in Equation 2, we obtain the same formula used for standard kernel density estimation. We summarize this observation as follows:
Observation 2.3. In the limiting case, when each cluster contains only one data point, the value  X  ( x ) is equal to the standard kernel density estimate.

The essential idea behind this exercise was to illustrate that the signature summary can encode very detailed infor-mation about the probability distribution of the data even for very fine grained clustering. We note that the difference in probability density between two distributions  X  1 (  X  ) and  X  (  X  ) may be defined as follows: This difference is essentially equal to the integral of the di f-ference of the two distributions over the entire space. Intu -itively, we would like to use this difference in density distr i-bution as a measure for the fit between two density distri-butions. Note that even when probability densities can be estimated accurately, this difference can be difficult to com-pute in practice. Therefore, as the surrogate, we compute the difference in signature summaries between the two data streams.

Definition 3. The distance Dist ( H 1 (  X  ) , H 2 (  X  )) between two signature summaries H 1 (  X  ) = ( f 1 . . . f k ) and H 1 . . . f  X  k ) is defined as follows: This distance is thus defined in terms of the L 1 -norm. We note that this distance is essentially a discrete analogue o f the difference in density distributions. In this case, we are Algorithm ConstructSpeechSignatures (Speech Segment: S , begin for each speaker label i for the next data point X in S do begin Determine closest micro-cluster C i to X ;
Add X to C i and update
Update signature counts for corresponding cluster in H j end end summing the discrete differences in probabilities that a voi ce packet belongs to a particular micro-cluster.

The micro-clustering algorithm is used in order to con-struct profiles from the speaker data. These profiles are essentially the signatures in the data stream. The micro-clustering algorithm uses a nearest neighbor clustering al go-rithm in which each data point is assigned to the mean of the closest cluster. As discussed in Observation 2.1, the mean o f the cluster can be computed from the micro-cluster statis-tics. Once the closest micro-cluster has been computed, we add the statistics for that data point to the micro-cluster statistics. This is possible to achieve because of the micro -cluster statistics discussed in Observation 2.2. The overa ll process for signature construction is discussed in Figure 1 .
One problem which may arise in a real application is that of cluster centroid drift. Cluster centroids can drift when the new data points which are added to the clusters have a different distribution from the original set of data points . Such a drift may affect the significance of the signatures over time, since different voice streams may be received one after another, and may have slightly different cluster cen-troids. This situation is also quite possible in real applic a-tions since the data distribution may vary over time. This is an undesirable situation, since it is difficult to fit the tes t data accurately. We note that the key function of cluster centroids is to ensure that the most relevant (dense) region s in the data are represented by these anchors. The exact location of these clusters is not very important, as long as they remain fixed over time, and represent most of the dense regions. Furthermore, we note that the only statistic which is used during the actual speaker identification process is t he relative frequencies of the data points in the different clus -ters. Therefore, it is acceptable to fix the cluster centroid s once each micro-cluster is sufficiently populated. Therefor e, we pick a small threshold (say 100 voice packets) for each micro-cluster, and apply the micro-cluster update algorit hm in a normal way till this limit. However, when the number of data points in the micro-cluster exceed this threshold, w e do not add the assigned data point to the micro-cluster, but only update the count of the number of data points in it. The other statistics are updated by the corresponding mul-tiplicative factor in order to reflect a larger number of data points. This ensures that the centroid of the micro-cluster remains fixed, but the frequency statistics are appropriate ly maintained.
 During the training phase, we also maintain the signatures
Figure 2: Training Signature for Aaron Brown for the different speaker labels. These signatures are used for the speaker classification process. Specifically, we ass ume that the k -dimensional vector H i is used in order to track the micro-cluster frequency behavior of the speaker i . Whenever a new data point is received, we add to the frequency for the corresponding micro-cluster in the signature for speaker i .
We note that the great flexibility of the scheme is that the testing phase is essentially no different from the train-ing phase, except that the micro-cluster statistics do not need to be updated. As in the training phase, we construct the signature for the test segment in order to create the summary signatures. This test segment is matched with the training segments using the relationship discussed in Equa -tion 4. The closest matching signature is returned as the speaker identification.

The method discussed in this paper can also be used for speech segmentation. The essential idea in speech segmenta -tion is to partition the conversations between one and more speakers in an unsupervised way. We note that this is often required as a pre-processing step to speaker identification in a real time application in which one does not have the time to perform the segmentation manually after using the con-tent of the speech. In order to extend the methodology to these cases, we perform the micro-clustering continuously as in the previous case, except that we save the signatures in each window at intervals which are equivalent to the window size. For each window, we determine the distance between the signatures for the current window and the window just before it. This leads to a continuous alarm function over the data stream. This alarm function can be used in order to track sudden changes in the trends of speaker behavior. This can create a clear segmentation between the different speakers in a conversation. More details and experimental tests of the approach will be described in an extended ver-sion of this paper.
In this section, we will discuss some empirical results il-lustrating the effectiveness and efficiency of the method on a variety of voice data sets. For the purpose of comparison, we will use a standard Gaussian Mixture model, which was derived from a software called netlab [2]. For each speaker, we first constructed a GMM on the training data set, and Figure 4: Testing Signature for Aaron Brown (735 records) Figure 5: Testing Signature for Aaron Brown (2932 records) Figure 6: Accuracy of Classification for Test Seg-ments of Different Sizes) calculated the log-likelihood of each speaker in the traini ng data on the test data segments. The maximum likelihood speaker was reported as the speaker for the test data set. We note that the use of GMMs is considered as state of the art for text independent speech recognition [4]. While such models often cannot be used easily for online training and detection, the accuracy provides a idea of the desired accuracy of an online system.

The data set used is the HUB-64 data set, which contains the voice signal of 64 different public personalities such as Wolf Blitzer, Bill Clinton, Al Gore, Candy Crowley etc. In each case, a training data set and test data set was con-structed from the data. In most real time applications, the size of the test data can be quite small and cannot be con-trolled. Furthermore, it is desirable to use as small a test segment as possible to perform the classification, since thi s affects the latency of speaker identification. On the other hand, very large sets of training data can often be made available in most applications for well known speakers by compiling data from multiple sources. Therefore, we chose to divide the training data such that the training data size was twice the test data in each case.

First, we will discuss some examples of signatures which were obtained by using micro-clustering on the different dat a sets. In Figures 2 and 3, we have illustrated some examples of the signatures obtained from the Aaron Brown, and Bill Clinton respectively. In each case, we have used 120 micro-clusters in order to construct the signatures. In each case, the cluster-id is illustrated on the X-axis, whereas the rel a-tive frequency of the corresponding cluster is illustrated on the Y-axis. It is clear that in each case, the corresponding signatures are quite distinct. The distinct natures of thes e signatures are very useful in performing an exact classifica -tion process on different test segments. In Figures 4 and 5, we have illustrated the signatures obtained on test segment s of different sizes for the signatures corresponding to Aaron Brown. It is clear that the signature in Figure 5 from the test segment of larger size is closer to the signature on the training data set. As a result, the accuracy of classificatio n increases with increasing size of the test segment. In Fig-ure 6, we have illustrated the accuracy on test segments of different sizes. In each case, we trained for all 64 speakers s i-multaneously, and classified a test segment into one of these 64 speakers. This is significantly more difficult than a prob-lem in which we try to distinguish a particular speaker from a control or mixed signal. This is because many speakers may have inherent similarity in their voice patterns, and it often becomes difficult to distinguish between a very large number of speakers using a single model. In Figure 6, we have illustrated the number of records on the X-axis, and the classification accuracy over the 64 speakers on the Y-axis. In the same Figures, we have illustrated the accuracy using Gaussian Mixture Models on test segments of different sizes. The accuracy of classification increases considerably with in-creasing test segment size because it is easier to build more accurate models on larger test segments. It is interesting to see that the accuracy of the micro-clustering process was significantly higher than the more computationally intensi ve (and offline) GMM approach. Our initial goals in designing this system were only to construct a system which could work in a one-pass approach for an online data stream, and we did not expect the micro-clustering approach to outper-form the GMM model in terms of accuracy. Therefore, the (substantial) qualitative advantage of the micro-cluster ing approach over the GMM model was particularly surprising. We believe that the higher accuracy is because of the more compact representation of the micro-clustering approach.
In this paper, we discussed an online system for voice recognition in data streams. We discussed a micro-clusteri ng approach which is not only efficient, but is significantly more accurate than the state-of-the-art GMM model. The greater accuracy of the micro-clustering approach is particularly surprising considering the fact that the GMM model is de-signed using the iterative EM-approach which does not have the constraints of the one-pass stream based micro-cluster ing model. While the simplicity and compactness of the micro-clustering signature representation was originally desig ned only for speed, we found that it achieves the dual purpose of avoiding the overtraining of the parameter-heavy GMM model  X  a classic example of the  X  X ess is more X  paradigm in machine learning. [1] C. C. Aggarwal, J. Han, J. Wang, P. Yu. A Framework for Clustering Evolving Data Streams. VLDB
Conference , 2003. [2] I. Nabney. Netlab: Algorithms for Pattern Recognition. Advances in Pattern Recognition , Springer-Verlag , Germany, 2001.

URL : http://www.ncrg.aston.ac.uk/netlab/down.php [3] M. Prybocki, A. Martin. NIST X  X  Assessment of Text
Independent Speaker Recognition Performance. http://www.nist.gov/speech/publications/index.html [4] D. Reynolds, T. Quateiri, R. Dunn. Speaker Verification using Adapted Gaussian Mixture Models.
Digital Signal Processing , Vol. 10, pp. 42 X 54, 2000. [5] T. Zhang, R. Ramakrishnan, M. Livny. BIRCH: An Efficient Data Clustering Method for Very Large Databases. ACM SIGMOD Conference, 1996.

