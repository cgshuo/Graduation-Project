 It is an important research problem to design efficient and effective solutions for large scale similarity search. One popular strategy is to represent data examples as compact binary codes through semantic hashing, which has produced promising results with fast search speed and low storage cost. Many existing semantic hashing methods generate binary codes for documents by modeling document relationships based on similarity in a keyword feature space. Two major limitations in existing methods are: (1) Tag information is often associated with documents in many real world applications, but has not been fully exploited yet; (2) The similarity in keyword feature space does not fully reflect semantic relationships that go beyond keyword matching. This paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling. In particular, a unified framework is designed for ensuring hashing codes to be consistent with tag information by a formal latent factor model and preserving the document topic/semantic similarity that goes beyond keyword matching. An iterative coordinate descent procedure is proposed for learning the optimal hashing codes. An extensive set of empirical studies on four different datasets has been conducted to demonstrate the advantages of the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques. Furthermore, experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Hashing, Tags, Topic Modeling
Similarity search identifies similar information objects given a query object, which has many information retrieval applications such as similar document detection, content-based image retrieval and collaborative filtering. Due to the explosive growth of the internet, a huge amount of data such as texts, images and videos has been generated, which indicates that nearest neighbor methods for similarity search are becoming more reliable. Therefore, it is important to design effective and efficient nearest neighbor methods for similarity search with large scale data.

Two major challenges have to be addressed for using similarity search in large scale datasets such as storing the data efficiently and retrieving the large scale data in an effective and efficient manner. Traditional text similarity search methods in the original keyword vector space are difficult to be used for large datasets, since these methods utilize the content vectors of the documents in a high-dimensional space and are associated with high cost of float/integer computation.

Semantic hashing (e.g., [9, 22, 34]) has been proposed as a promising technique for addressing these two challenges, which designs compact binary code in a low-dimensional space for each document so that similar documents are mapped to similar binary codes. Query documents can also be efficiently transformed into hashing codes so that similarity search can be conducted. The retrieval process of similarity search can be simply conducted by calculating the Hamming distances between the hashing codes of available documents and a query and selecting documents within small Hamming distance. Therefore, this method addresses the two major challenges of large scale similarity search in the following ways: (1) The encoded data is highly compressed within a low-dimensional binary space, and thus can often be dealt with in main memory and stored efficiently; (2) The retrieval process is very efficient, since the distance between two codes is simply the number of bits that they differ.
Existing semantic hashing approaches generate promising results and are successful in addressing the two challenges of storage and retrieval efficiency. However, two major issues are not addressed in the existing methods: (1) Tag information is not fully utilized in previous methods. Most existing methods only deal with the contents of documents without utilizing the information contained in tags. Actually, in many real-world applications, documents are often associated with multiple tags, which provide useful knowledge in learning effective hashing codes. For instance, in some big corporations, their employees often assign tags to some webpages in the intranet, such that they can find these webpages again more easily through these tags. Another example is microblog, usually each tweet is associated with multiple tags such as  X  X tock X ,  X  X usiness X ,  X  X ports X , etc.; (2) Document similarity in the original keyword feature space is used as guidance for generating hashing codes in previous methods, which may not fully reflect the semantic relationship. For example, two documents in the same topic may have low document content similarity in keyword space due to the vocabulary gap, although their semantic similarity can be high.
Based on the above observations, this paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM). The SHTTM approach integrates available tag information and document similarity in semantic topics for generating effective hashing codes. There are three main challenges for designing this method: (1) How to incorporate the tag information? (2) How to preserve the similarity between documents based on semantic topics? (3) How to obtain the hashing code for a query document as no tag information is available for the query document?
A unified framework is proposed to address the above challenges, which ensures the consistency of hashing codes with tag information by a latent factor model and preserves document semantic similarity based on topic modeling. An iterative optimization procedure is proposed for learning the optimal hashing codes. An extensive set of experiments on four real world datasets has been conducted to demonstrate the advantages of the proposed method over several state-of-the-art methods. More specifically, a set of experiments clearly demonstrates the benefits of utilizing tag information and topic modeling separately, while the combination of these two methods in the unified framework generates the best results. To our best knowledge, this is the first piece of research work on semantic hashing that integrates both tag information and semantic topic modeling, which generates more effective hashing codes than several other state-of-the-art methods.
Efficiency is a crucial issue for large scale information retrieval applications with a huge amount of text documents. When there is only a low-dimensional feature space, similarity search can be carried out by some space partitioning index structures, such as TF-IDF methods [23, 24], KD-tree, or data partitioning index structures, like R-tree [5]. Several types of structures and operations of inverted indexing are also proposed [6, 26, 40] for traditional ad-hoc text search with relatively short user queries. However, traditional similarity search may fail to work efficiently within a high-dimensional vector space [33], which is often the case for many real world information retrieval applications.

Semantic hashing [22] is proposed to address the similarity search problem within a high-dimensional feature space. In particular, semantic hashing methods try to represent each document by using a small fixed number of binary bits so that the queries can be answered in a short time [27]. The hashing based fast similarity search can be viewed as a strategy to transform documents from a high-dimensional space into a low-dimensional binary space, and at the same time preserve the semantic similarity between documents as much as possible. Hashing methods generate binary codes for efficient search, which is different from traditional dimensionality reduction methods such as Principal Component Analysis (PCA) and Latent Semantic Indexing (LSI) [8, 12].

Locality-Sensitive Hashing (LSH) [2, 7] is one of the most popularly used hashing methods. It simply utilizes random linear projections to map documents from a high-dimensional Euclidean space to a low-dimensional one. It has already been shown that the Hamming distance between different documents will asymptotically approach their Euclidean distance in the original feature space with the increase of the hashing bits. LSH has been extended to Kernelized Locality-Sensitive Hashing (KLSH) [16] by exploiting kernel similarity for better retrieval efficacy. Recently, the work in [35] further extends the KLSH to the scheme of Boosting Multi-Kernel Locality-Sensitive Hashing (BMKLSH) that improves the retrieval performance of KLSH by making use of multiple kernels.

Several machine learning approaches have been proposed to solve the hashing problem. For example, the PCA Hashing [19] method projects each example to the top principal components of the training set, and then binarizes the coefficients by setting a bit to 1 when its value is larger than the median value seen for the training set, and -1 otherwise. The work in [22] uses stacked Restricted Boltzman Machine (RBM) [10, 11] to generate compact binary hashing codes, which can be viewed as binarized LSI. Recently, Spectral Hashing (SH) [34] is proposed to learn compact binary codes that preserve the similarity between documents by forcing the balanced and uncorrelated constraints into the learned codes, which be viewed as an extension of spectral clustering [37]. A graph-based hashing method has been proposed in work [21] to automatically discover the neighborhood structure inherent in the data to learn appropriate compact codes. More recently, the work [38] proposes a Composite Hashing with Multiple Information Sources (CHMIS) method to integrate information from different sources. In another recent work [15], an isotropic hashing (IsoHash) method is proposed to learn projection functions of individual hashing codes with equal variances.

In the following two sections, we mainly discuss two state-of-the-art hashing methods that are most related to the proposed research as Self-Taught Hashing [39] and Semi-Supervised Hashing [31].
Self Taught Hashing (STH) [39] generally provides more effective hashing solutions than LSH [7] and SH [34]. STH combines an unsupervised learning step with a supervised learning step to learn hashing codes.
In the unsupervised learning step, STH constructs a similarity graph using a fix number of nearest neighbors for the given dataset, and then embeds all the documents into a k dimensional space through spectral analysis, and finally uses simple thresholding to obtain the binary hashing code for each document. This step can be formulated as follows: S S S ij is the pairwise similarity between document i and document j , y i is the hashing code for document i , k is the number of hashing bits. The objective function incurs a heavy penalty if two similar documents are mapped far away, which preserves the similarity between documents. The constraint P i y i = 0 requires each bit to be balanced and 1 n P i y i y T i = I I I forces the hashing bits to be uncorrelated with each other.

In the supervised learning step, a set of k SVM classifiers are trained based on existing documents and their binary hashing codes learned from the previous step. Then, the k classifiers can be used to generate the hashing codes for the query documents as a classification problem. STH does not assume that data are uniformly distributed in a hyper-rectangle as requested by SH, which is often too restrictive for real world applications. STH often generates more effective hashing codes than SH.
 However, there are two main limitations for STH. Firstly, STH does not utilize tag information, which is often available with documents in many real world applications. Secondly, the similarity matrix in STH is calculated in the original feature (keyword vector) space, which may not reflect the semantic similarity beyond simple keyword matching. Topic modeling (e.g., [4]) has been shown as an effective approach for capturing semantic meanings in text documents. In the proposed new research, we address both of the two problems of STH through integrating the tag information and topic modeling into one unified framework so that the semantic similarity between documents can be better preserved in the learned hashing codes.
The work in [31] proposes a Semi-Supervised Hashing (SSH) approach for incorporating the pairwise relationships between documents into the semantic hashing problem. More precisely, these pairwise similarity constraints are also called Must-Link and Cannot-Link, which could be partially generated from tags. For example, a Must-Link is created when two documents share a common tag and a Cannot-Link is created when two documents share no tag. Their basic motivation is that the hashing codes of the document pairs with Must-Link should be as close as possible, while the hashing codes of document pairs with Cannot-Link should be as different as possible. This motivation is then incorporated into the objective function for learning the hashing codes. A sequential projection method is utilized to solve the resulting optimization problem.

The SSH has shown promising results for improving hashing effectiveness by leveraging the pairwise information, but there are several limitations for SSH. Firstly, the SSH method only utilizes the pairwise similarity constraints as the summary of tag information, which is suboptimal with respect to the complete information in the tags. Secondly, the pairwise link information may not be accurately generated when tags are missing, incomplete or mismatched, which is often the case for many real world applications. Furthermore, SSH also directly works in the original keyword feature space for modeling content similarity of documents. These problems may potentially limit the performance of the hashing methods based on pairwise constraints. Different from SSH, the proposed method utilizes the complete tag information as well as semantic information from topic modeling for building more effective hashing codes.
We first introduce the problem of SHTTM. Assume there are total n training documents in the dataset, denoted as: X X X = { x 1 ,x 2 ,...,x n }  X  R R R m  X  n , where m is the dimensionality of the content feature. Denote their tags as: T T T = { t 1 ,t 2 ,...,t n } X  X  0 , 1 } l  X  n , where l is the total number of possible tags associated with each document. A tag with label 1 means a document is associated with a certain tag/category, while a tag with label 0 means a missing tag or the document is not associated with that tag/category. The main purpose of SHTTM is to obtain optimal binary hashing codes Y Y Y = { y 1 ,y 2 ,...,y n }  X  { X  1 , 1 } k  X  n for the training documents X X X , and a hashing function f : R R R m  X  { X  1 , 1 } which maps each document to its hashing code with k bits (i.e, y j = f ( x j )).
The proposed SHTTM approach is a general learning framework that consists of two stages. In the first stage, the hashing codes are learned in a unified framework by simultaneously ensuring hashing codes to be consistent with tag information by a formal latent factor model and preserving the document topic/semantic similarity. In particular, the objective function of SHTTM is composed of two components: (1) Tag consistency component, which ensures that the hashing codes are consistent with tag information; (2) Similarity preservation component, which aims at preserving the topic/semantic similarity in the learned hashing codes. An iterative algorithm is then derived based on the objective function using a coordinate descent optimization procedure. In the second stage, the hashing function is learned with respect to the hashing codes for training documents.

The rest of this section first presents the two stages of the proposed SHTTM approach respectively, and then addresses the corresponding optimization problem. Finally, this section discusses connections and distinctions of the proposed approach with some related research work.
In many real world applications, documents are associated with tag information, which can provide useful knowledge in learning effective hashing codes. There are two main challenges for utilizing tags: (1) We have no knowledge about how the tags are related to the hashing codes. Therefore, we need to explore the correlation between them in order to bridge tags with hashing codes; (2) Tags may be missing, we need to deal with the situation of incomplete tags.

The first problem of exploring the correlation between tags and hashing codes can be addressed by matrix factorization with a latent factor model. A latent variable u i for each tag t is first introduced, where u i is a k  X  1 vector indicating the correlation between tags and hashing codes. Then a tag consistency component can be formulated as follows: where T T T ij is the binary label of the i -th tag on the j -th document. u T i y j can be viewed as a weighted sum that indicates how the i -th tag is related to the j -th document, and this weighted sum should be consistent with the observed label T T T ij as much as possible. The second regularization component, P l i =1 k u i k 2 , is introduced to avoid the overfitting issue (e.g., [28, 29]).  X  is a meta parameter that explores the trade-off between the tag consistence and regularization.

The second problem of dealing with missing tags can be addressed by introducing a confidence matrix C C C  X  R R R the value of C C C ij is large, we trust the tag information T T T more. As discussed before, T T T ij = 0 can be interpreted in two ways: tag i on the j -th document is either missing or not related. We will use a similar strategy as in [14] for a different application to set C C C ij a higher value when T T T than T T T ij = 0 as follows, where a and b are parameters satisfying a &gt; b &gt; 0 1 the whole component of tag consistency becomes: The above equation can be rewritten in a compact matrix form as: where C C C 1 2 is the element-wise square root matrix of C C C , and  X   X   X  is the element-wise matrix multiplication. kk F matrix Frobenius norm. By minimizing this component, the consistency between tags and the learned hashing codes can be ensured.
One of the key problems in semantic hashing methods is similarity preserving, which indicates that semantically similar documents should be mapped to similar hashing codes within a short Hamming distance. A popular criterion of similarity preservation is to minimize the objective value in Eqn.1, where S S S is the document similarity matrix. This criterion incurs a heavy penalty if two similar documents are mapped far away in the dataset.

There are many different ways of defining the similarity matrix. In SH [34], the authors used the global similarity
In our experiments, we set the confidence parameters a=1 and b=0.01 consistently throughout all experiments. structure of all document pairs, while in STH [39] and CHMIS [38], the local similarity structure, i.e. , k nearest neighborhood, is used. However, all these methods compute the similarity matrix S S S within the original keyword feature space and thus may not reflect the document semantic similarity that goes beyond simple keyword matching.
To address the limitation of calculating semantic similar-ity in existing approaches, features from topic modeling are used to measure the semantic similarity between documents instead of features from the original keyword space. Topic modeling algorithms (e.g., [3, 36]) are used to discover a set of  X  X opics X  from a large collection of documents and provide an interpretable low-dimensional representation of the documents associated with the topics. Topic modeling has been widely used in many information retrieval applications such as document clustering and classification. Here we exploit the Latent Dirichlet Allocation (LDA) [4] approach of topic modeling to extract k latent topics from the document corpus. Each document x j corresponds to a distribution  X  j over the topics where two semantically similar documents have similar topic distributions. In this way, document semantic similarity is preserved in the extracted topic distributions  X   X   X  . Since we require the hashing codes to reflect the topic distributions, a document similarity preservation component can be naturally defined as follows: By minimizing this component, the similarity between different documents is preserved in the learned hashing codes.
The entire objective function of the proposed SHTTM approach integrates two components such as the tag consistency component in Eqn.5 and the semantic similarity preservation component given in Eqn.6 as follows: where  X  and  X  are trade-off meta parameters to balance the weight between the components. The constraint Y 1 Y 1 Y 1 = 0 requires each bit to appear 50% of the time (with equal probability as positive or negative). Note that one advantage of our method is that we do not need the bits-uncorrelated constraint Y Y Y Y Y Y T = I I I while many previous methods do. This is because we assign each hashing bit a semantic meaning (a latent topic), which is learned from topic modeling and latent topics can be assumed to be highly uncorrelated. Therefore, by imposing the semantic similarity term between Y Y Y and  X   X   X  , the uncorrelated property among Y Y Y is preserved.
Directly minimizing the objective function in Eqn.7 is intractable because of the discrete constraints. Therefore, we propose to relax this constraint and drop the constraint Y 1 Y 1 Y 1 = 0 first (we will discuss this constraint later). However, even after the relaxation, the objective function is still non-convex with respect to Y Y Y and U U U jointly, which makes it difficult to optimize. Fortunately, this relaxed problem is convex with respect to either one of the two sets of parameters when the other one is fixed, and therefore can be } associated with tags T T T = { t 1 ,t 2 ,...,t n } . solved by coordinate descent optimization with guaranteed convergence similar to [32]. In particular, after initializing Y Y Y , the optimization problem can be solved by doing the following two steps iteratively, until convergence.
Step 1: Fix Y Y Y , optimize:
By taking the derivative of Eqn.8 with respect to u i and setting it to 0, we can obtain the close form solution of this optimization below: where C C C i is a n  X  n diagonal matrix with C C C ij ,j = 1 , 2 ,...,n as its diagonal elements and T T T i = ( T T T ij ) ,j = 1 , 2 ,...,n is a n  X  1 label vector of i -th tag.

Step 2: Fix U U U , optimize:
By taking the derivative of Eqn.10 with respect to y j and setting it to 0, we can obtain the optimal y j : where C C C j is a l  X  l diagonal matrix with C C C ij ,i = 1 , 2 ,...,l as its diagonal elements and T T T j = ( T T T ij ) ,i = 1 , 2 ,...,l is a l  X  1 label vector of the j -th document. By solving Eqns.8 and 10 iteratively, optimal Y Y Y and U U U can be obtained.
In the previous section, the optimal hashing codes Y Y Y are obtained after relaxing the binary constraint and the bit balance constraint Y 1 Y 1 Y 1 = 0. In this section, we will discuss how to obtain the hashing function that maps the data into binary hashing codes and how to binarize the optimal Y Y Y in order to satisfy the constraints.

In this paper, we utilize a linear hashing function to generate binary hashing code, which is consistent with previous hashing methods (e.g., [30, 31, 38]) as: where W W W is a k  X  m parameter matrix representing the hashing function. Then the optimal hashing function can be obtained by minimizing the following objective: Here  X  is a weight parameter for the regularization term to avoid overfitting.

The binary hashing codes for the training set can be obtained by thresholding Y Y Y . Then, a natural question is how to pick these thresholds? In [19] and [37], the authors pointed out that a good semantic hashing should also maximize the entropy to ensure efficiency. Following the maximum entropy principle, a binary bit that gives balanced partitioning of the whole dataset always provides maximum information. Therefore, we set the threshold for binarizing the p -th bit to be the median of y p . We denote the median of all bits by vector m m m . Thus, if the p -th bit of document y is larger than m m m p , then y p j is set to +1, otherwise y to -1. In this way, the binary code achieves the best balance The hashing code of a query document q can be obtained by first computing the hashing function f ( q ) = W W Wq . Then, the corresponding binary hashing code can be obtained through thresholding the hashing function output, i.e. , set the j -th code for q to be +1, if f ( q ) j &gt; m m m j , and -1 otherwise.
The whole training procedure and predicting procedure of the proposed method is described in Table 1. For hashing methods, the training process is always conducted off-line. Therefore, our focus of efficiency is on the predicting process. This process of generating hashing code for a query for prediction only involves some dot products and comparisons between two binary vectors, which can be done in O ( m  X  k + k ) time.
The formulation of the proposed SHTTM approach is related to the traditional collaborative filtering methods (e.g., [14, 25, 13]) by treating each document as an item and the label of each tag as a rating from a user (i.e., tag) on this item (i.e., document). From this perspective, the proposed method is related to Collaborative Topic Modeling (CTM) [29], in which both the rating scores and the content of items are incorporated into one topic modeling framework. However, several major differences exist between the proposed work and CTM: (1). CTM is designed for collaborative filtering, while the proposed method investigates how tags and topic modeling can be integrated to improve the performance of hashing; (2). in the proposed research, one focus is on out-of-sample query examples, which do not exist in the training dataset. Therefore, the proposed research designs hashing function to convert query examples to their corresponding hashing codes, while this is not considered in CTM.

As for the speed issue, the major off-line computational costs of the proposed method come from LDA and the latent factor model parts. For LDA, we can use parallel LDA (e.g., [1]) to accelerate the computational speed. For optimizing the unified framework, it normally converges within 20 iterations and only some simple updates are necessary in each iteration. For the more important issue of calculating hashing codes of the query examples for similarity search, it is very efficient since only some linear transformations are necessary.
This section presents an extensive set of experiments to demonstrate the advantages of the proposed research.
A set of datasets are utilized in evaluation as follows: 1. ReutersV 1 (Reuters-Volume I): This dataset contains 2. Reuters (Reuters21578) 2 is a collection of documents 3. 20 Newsgroups 4 corpus is collected and originally http://daviddlewis.com/resources/textcollections/reuters2 1578/. we removed the tags which only have a limited number of examples. http://people.csail.mit.edu/jrennie/20Newsgroups/ retrieved examples within Hamming radius 2. 4. WebKB 5 consists of 6883 webpages, collected from In all datasets, term frequency (i.e. tf ) features are used as content features and also used for learning topic distributions. Note that tags in each dataset are divided into two groups, one set is used only in training and the other is treated as ground truth only for testing.
To conduct similarity search, each document in the testing set is used as a query document to search for similar documents in the corresponding training set based on the hamming distance of their hashing codes. The performance is measured with standard information retrieval performance metrics: precision as the ratio of the number of retrieved relevant documents to the number of all retrieved documents and recall as the ratio of the number of retrieved relevant documents to the number of all relevant documents. The performance is averaged over all test queries in the dataset.

There are several methodologies to determine whether a retrieved document is relevant to the given query document. In SH [34], the k closest documents in the original feature
CMU world wide knowledge base (WebKB) project. Avail-able at http://www.cs.cmu.edu/ WebKB/. space are considered as the relevant documents. In STH [39] and CHMIS [38], the documents with the same tag as the query document are considered as the most relevant ones. The former metric is not suitable since the similarity in the original feature space may not well reflect the document semantic similarity (as discussed in section 3.3). The latter metric is also questionable since documents with more than one tag are discarded in STH and CHMIS, while documents are allowed to have multiple tags in our experiments. Therefore, we adopt a similar metric as in [20] and [31] in our experiments. In particular, a retrieved document that shares any common test tag with the query document is regarded as a relevant document.
The proposed SHTTM approach is compared with five different methods on these datasets such as Semi-Supervised Hashing (SSH) [31], Self Taught Hashing (STH) [39], Spectral Hashing (SH) [34], PCA Hashing (PCAH) [19], and Latent Semantic Hashing (LSH) [7] by using the evaluation metric described above.

The parameters  X  ,  X  and  X  in SHTTM are tuned by 3-fold cross validation on the training set through the grid tuned to be 7 when constructing the graph Laplacians for STH in all experiments. For LSH, we randomly select projections from a Gaussian distribution with zero-mean and identity covariance to construct the hash tables. For SSH, we sample 2 k random points from the training set to construct the pairwise constraint matrix. We evaluate the performance of different methods by varying the number of hashing bits in the range of { 8 , 16 , 32 , 64 , 128 } and calculate the average result by repeating each experiment 10 times. Figure 2: Results of Precision-Recall curve with 32 hashing bits on four datasets.
Four sets of experiments are conducted on each dataset to measure the performance of the proposed SHTTM approach and the five alternative methods to answer the following questions: (1) Whether SHTTM can outperform other methods in high-precision results such as the precision for the top 100 retrieved documents based on the hamming distance ranking and the precision for retrieved documents within a fixed hamming distance 2; (2) How does the SHTTM compare with other methods with recall-related metrics as the precision-recall curve? In particular, for each query document, we vary the number of retrieved documents from 0 to the number of all training documents while fixing the number of hashing bits to 32; (3) How do the two components of the SHTTM approach work that only utilize the tag information or the topic modeling information? These two variants are compared with corresponding baseline methods and the SHTTM approach that combines both tag and topic modeling information; (4) Whether SHTTM has robust performance with respect to different values of meta model parameters.

In the first set of experiments, we report the precision for the top 100 retrieved documents with different numbers of hashing bits in Fig.1(a)-(d) and Table 2. The precisions for the retrieved documents within hamming radius 2 are shown in Fig.1(e)-(h). From these comparison results, it can seen that SHTTM gives the overall best performance among all six hashing methods on all four datasets.

In Fig.1(e)-(h), the precision of most compared methods decreases when the number of hashing bits increases from 16 to 128 This is because when using longer hashing bits, the Hamming space becomes increasingly sparse and very few data points fall within the Hamming ball of radius 2, resulting in even queries with precision 0. Similar behavior is also observed in [20] and [31]. In this situation, the precision results of top 100 documents from Fig.1(a)-(d) provide better performance measurement, while the precision results of SHTTM are still consistently better than other methods.
In the second set of experiments, the precision-recall curves of different methods with 32 hashing bits on different datasets are reported in Fig.2. It can be seen that among all of these comparison methods, SHTTM shows the best performance.

From these figures, we can see that LSH does not perform well in most cases, especially its precision of the top 100 documents is not satisfactory. This is because LSH method is data-oblivious and may lead to inefficient codes in practice as also observed in [22] and [34]. For methods SH and STH, although these methods try to preserve the similarity between documents in their learned hashing codes, they do not utilize the supervised information contained in tags. Moreover, the similarity matrices in both methods are computed from the original keyword feature space, which may not fully reflect the semantic similarity between documents that goes beyond keyword matching. Therefore, the SHTTM method substantially outperforms these two methods by leveraging tag information and topic modeling. SSH achieves better results than SH and STH due to the incorporation of pairwise similarity constraints. However, as pointed out in Section 2.2, these coarse pairwise constraints generated from tags may lose detailed tag information and may not be reliable. On the other hand, the tag information is fully exploited in the SHTTM approach via modeling the semantic correlation between tags and hashing codes through a latent factor model and thus SHTTM generates higher quality hashing codes than SSH.

In the third set of experiments, the effectiveness of the two components of the proposed SHTTM such as tag modeling and topic modeling are evaluated separately. In particular, the component of tag modeling is obtained by setting  X  to 0 in Eqn.7, which means we do not utilize topic models. This method is named SHTTM-TagOnly in our experiment. SHTTM-TagOnly is compared with SSH which incorporates summary tag information as pairwise constraints into learning hashing codes. the component of topic modeling is obtained by setting the tag matrix C C C = 0 0 0 in Eqn.7, which means we do not utilize any tag information in learning hashing codes. This method is called SHTTM-TopicOnly and it is compared with STH that calculates the document similarity in the original keyword feature space. The precision results of top 100 retrieved documents of SHTTM-TagOnly, SHTTM-TopicOnly, SHTTM and the corresponding baseline methods are shown in Fig.3 when different numbers of hashing bits are used. The experimental results in Fig.3 indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately as SHTTM-TagOnly is more effective than SSH while SHTTM-TopicOnly is more effective than STH. In particular, the SHTTM-TagOnly method incorporates the complete tag information into learning hashing codes via a latent factor model, while SSH only utilizes the partial tag information in pairwise constraints. The SHTTM-TopicOnly method preserves the document similarity in the topic/semantic, while the original keyword feature space used by STH may not fully reflect the semantic relationships that go beyond keyword matching. Finally, combining these two components together, the proposed SHTTM achieves even better performance, which is consistent with our expectation. Comparison results of using topic model only.
 The fourth set of experiments study the performance of SHTTM with respect to the meta parameters  X  ,  X  and  X  . To prove the robustness of the proposed method, we conduct parameter sensitivity experiments on all datasets. In each experiment, we tune only one parameter from optimal values obtained from the first set of experiments. We report the results on ReutersV 1 and 20 Newsgroups in Fig.4. It is clear from these experimental results that the performance of SHTTM is relatively stable with respect to  X  ,  X  and  X  . We also observe similar results of the proposed method in the other two datasets. But due to the limit of space, they are not presented here.

The prediction procedure for generating hashing codes is very fast. The linear hashing function allows a quick mapping of a query document from the original high-dimensional feature space to the low-dimensional space of hashing codes. We implement our method using Matlab on a PC with Intel Duo Core i5-2400 CPU 3.1GHz and 4GB RAM. It takes 0.0002 second average per query document for the prediction procedure (Table 1) on all datasets.
Similarity search has become an important technique in many information retrieval applications such as search and recommendation. Many applications with similarity search often involve a large amount of data, which demands effective and efficient solutions. Semantic hashing has been proposed for the problem to map data examples like documents in a high-dimensional space (e.g., a vector space of keywords in the vocabulary) into a low-dimensional binary vector space, which at the same time preserves the semantic Figure 4: Parameter Sensitivity for  X  ,  X  and  X  . Results of precision of the top 100 retrieved documents with 32 hashing bits. relationship of the data examples as much as possible. Valuable prior research has been conducted in this direction for learning hashing codes and mapping function with techniques such as unsupervised learning and supervised learning. However, most existing research on semantic hashing is only based on content similarity computed in the original keyword feature space. Moreover, tag information is not fully utilized in existing research, although they are often available in many information retrieval applications.
This paper proposes novel research for semantic hashing that jointly considers tag information and semantic simi-larity in generating hashing codes. The proposed research utilizes topic modeling to explore semantic similarity between documents that goes beyond keyword matching. The new SHTTM approach incorporates two components as tag consistency and topic consistency together into a joint objective function for learning desired tag representation and topic representation simultaneously. An iterative coordinate descent method is proposed to obtain the optimal hashing codes by solving the objective function. An extensive set of experiments has shown that the proposed new research can generate more accurate hashing codes than several other state-of-the-art hashing methods. In particular, the experiments clearly demonstrate the benefits of utilizing tag information and topic modeling information separately, and further show that the joint approach generates the best results by combining both two types of information.

There are several possible directions to explore in the future research. For example, the research in this paper only utilizes a fixed number of topics, which may not be optimal for different types of datasets. We plan to explore new research for automatically adjusting the number of topics with either model selection methods or a nonparametric Bayesian approach. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
