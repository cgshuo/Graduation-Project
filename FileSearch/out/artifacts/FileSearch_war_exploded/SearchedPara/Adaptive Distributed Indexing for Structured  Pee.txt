 Structured peer-to-peer networks support keyword search by building a distributed index over the collective content shared by all peers. Building the index and processing queries involve data transfer among peers, thus it is important to keep both of these activities bandwidth-efficient. However, this goal is difficult to attain, as smaller, less precise indices reduce index building and access costs but increase query processing cost, which potentially increases overall cost. We study the trade-off between indexing cost and query processing cost in a structured peer-to-peer network and propose a cost-reducing, adaptive, distributed indexing technique based on the term distributions in local shared contents and user query logs. Using this information, we reduce costs by tuning the precision of the index. The approach we take is to group local documents and to index the groups instead of either individual documents or entire peer collections. We control total cost by controlling the number and contents of groups. We propose a probabilistic model to estimate the cost of grouping, which allows us to identify the optimal number of groups to be created. In addition, we propose a cost-based distance function to guide the document grouping process. Experimental results show that our adaptive indexing technique reduces cost by up to 47% compared with peer-level grouping and by up to 73% compared with document-level grouping. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Search process, Clustering. Algorithms, Performance, Experimentation. Distributed Indexing, Peer-to-Peer, DHT-based networks. Much recent research has been conducted on efficiently searching for shared contents in peer-to-peer (P2P) networks. To support general multi-term queries, structured P2P networks (a.k.a. DHT-based networks), such as those described in [2][3][4], implement a distributed index on the collective documents shared by all peers. It turns out that the costs of creating and using these indices are interdependent (i.e., minimizing one may increase the other), a dependency that has been given little attention in previous work. We therefore propose a technique that minimizes the sum of these costs. In structured P2P networks, each term in a document collection is associated with one peer based on a hashing mechanism [2][3][4]. The peer responsible for term t maintains t  X  X  global posting list that share these documents). To create a global posting list for t , each peer builds a local posting list for t (i.e., a posting list based on local shared content) and sends this list to the peer responsible posting lists. To process a query, a peer must download all of the global posting lists of all of the query terms, and then compute their intersection to identify the peers that potentially share the relevant files. The query is then issued to all of these peers who return any matching documents. For P2P applications, the cost of transferring data among peers (e.g., in posting list retrieval or in issuing queries to peers who may share matching documents) is significant compared with local query processing costs. One way to reduce cost is by keeping posting lists short, which reduces the cost required to transfer them. Another way to reduce cost is by making the intersection of the posting lists more precise, thereby reducing the number of peers that are necessary to query for matching documents. Unfortunately, these two goals conflict as shorter posting lists lead to less precise intersections. To keep posting lists short, peers might choose a  X  X eer-level X  indexing strategy, where each peer indicates only whether a particular term occurs in its collection, excluding any indication of which particular local documents contains the term. Thus the peer level global posting list of a term contains only the IDs of the peers whose collection contains the term. In contrast,  X  X ocument-level X  indexing contains information about which document contains which term for each peer. This results in longer posting lists, but the precision of their intersections leads to more accurate query issuing. Thus, the document-level global posting list of a term contai ns the IDs of documents that contain the term as well as the IDs of the peers share those documents. For example, consider a query  X  X utomobile auction X  and a peer that shares the documents  X  X utomobile design X  and  X  fine art auction. X  By using peer-level indexing, both of the global posting lists of the terms  X  X utomobile X  and  X  X uction X  conta in the ID of the considered peer. As a result, their intersection wi ll contain this ID. Consequently, the query will be issued to this peer even though it does not share any matching documents. Document-level indexing eliminates this  X  X rroneous query issuing X  problem. Because the posting list of a ter m contains document IDs, queries are guaranteed to be forwarde d to peers who have at least one document containing all query terms. Unfortunately, this document-level approach is unsc alable, as document-level global posting lists might be too lo ng. We study the performance of keyword search in struc tured peer-to-peer networks, measured by the total network cos t, including the costs of index building, posting list retrieval and query issuing. Our approach in reducing total cost is to trade ind ex building and posting list retrieval costs for query issuing cost . For an individual peer, we propose to create groups of local shared d ocuments and represent each group by its  X  X erm set. X  Our indexin g strategy is called  X  X roup-level X  indexing, where the posting li st of a term contains pairs of peer ID and group ID. Group-level indexing is a generalization of both peer-level and document-leve l indexing. If each document forms a group, group-level indexing i s equivalent to document-level indexing and if a single group is formed, group-level indexing is equivalent to peer-level in dexing. There are two important decisions to be made in usi ng group-level indexing: the number of groups to create and the as signment of documents to groups. Creating more groups lowers th e query issuing cost, but raises the cost of index building and posting list retrieval. Assigning a document to its  X  X ppropriate  X  group reduces the likelihood of query issuing errors. First, we propose a probabilistic model to estimate the cost associated with a given number of groups based on t he term distributions derived from both user queries and lo cal contents. Using this model, we efficiently identify the numbe r of groups appropriate to each peer. Second, we propose a grouping algorithm, which is a lso based on the term distributions derived from user queries an d local contents. We reveal that grouping semantically simi lar documents together, as done by traditional text clustering al gorithms, is not appropriate, and thus, we define a new grouping tec hnique. Almost all DHT-based P2P networks employ document-l evel indexing. Associated with each term is a posting li st, which is a the peers that share the document. Each posting li st is assigned to a peer using a well-known (hash) function. Process ing a query requires retrieving all relevant posting lists, com puting their intersection, and then issuing the query to the pee rs identified in the intersection. The problem with this type of indexing is that many potentially long posting lists have to be transferred among pee rs during index building, node joining/leaving, and query processin g, resulting in the consumption of a significant amount of network bandwidth. Approaches for handling long posting lists include: (i) compress-ing the posting lists [7][12]; (ii) truncating the posting lists [10][11]; (iii) pre-computing (materializing) the p osting list intersections of popular term combinations [11][12] [13]; and (iv) pruning the documents before building the index [14 ]. To the best of our knowledge, none of the above techniques cons ider the interdependence among the costs involved in query p rocessing. Michel et al. proposed the sk -STAT technique [12] that attaches a hash sketch with each term to compactly represent i ts document-level posting list. A Bloom filter [9] can also be used instead of a hash sketch [7]. These approaches ignore the effect of the size of Bloom filters or hash sketches to the cost of index building and posting list retrieval (obviously, the larger the B loom filters/hash sketches, the lower the cost of query issuing, but the higher the cost of index-building and posting list retrieval).
 Podna et al. [10] and Skobeltsyn et al. [11] propos e to truncate the posting lists, keeping only the IDs of the most relevant documents. To avoid the deterioration of the qualit y of results due combinations must be included in the index (as done in [13]). Unfortunately, this increases the cost of index bui lding, as there are significantly more posting lists to be built. The mk -STAT technique in [12] tries to identify the most frequent term combinations in the user query log, and pre-co mputes their posting list intersections, thus avoiding posting l ist retrieval cost for those term combinations. However, this techniqu e might not work well with the heavy tailed query work load (wh ich is likely the case in practice), wherein a significantly larg e portion of queries is seldom repeated. The technique proposed in [14] removes unimportant terms from documents before building the index, reducing the n umber of posting lists, and therefore, the index-building co st. However, it makes full-text search impossible. In unstructured P2P networks, each peer builds a su mmary of its shared contents and disseminates it to other peers [27]. Upon receiving a query, a peer will make a decision of t o whom among its neighbors to forward the query based on the nei ghbors X  summaries. This is similar to the source selection problem of a meta-search engine [26]. The CORI Net approach [22] represents a text collection as a  X  X uper document X  (i.e., the document that results from combining all documents in the collect ion). The GlOSS approach [23] represents a text collection as a set of terms, each of which is associated with its document frequ ency and its average term frequency over all documents. In gener al, these  X  X anguage model X -based approaches [16][17][24][25] summarize shared contents at a peer level -they estimate the overall term distribution of the local collection, not the term distribution of each document or groups of documents. Real-world P2 P file sharing networks, such as the Gnutella network, als o use peer-level summaries: a peer X  X  content is represented by a term set, defined as the union of all its shared file X  X  descr iptors, and a query is forwarded to a peer whose term set contains all query terms [21]. For XML path query routing, the  X  X readth Bloom Filt er X  and the  X  X epth Bloom Filter X  were proposed to summarize the structure of XML documents [18]. It is unclear if it is possi ble to adapt these approaches to collections of unstructured, pu re text documents. In [31], we propose to summarize a peer X  X  content b y partitioning its local shared documents to a number of groups a nd represent-ing each group by a Bloom filter. Our technique imp roves query routing efficiency up to 60% with virtually no cost . In [30], we further improve our partitioning technique by explo iting not only term distributions of local documents, but also one s of user queries. Exploiting term distributions of user quer ies can further improve query routing performance by 30%. We adapt our previous works to structured P2P netwo rks. The differences between structured and unstructured net works require us to reconsider both our grouping technique and ou r way of identifying the optimal number of groups. In a P2P network, each peer P i shares a collection D documents. Denote the term set of a document d ij and the vocabulary of a peer P i by V i , where V i = Each term t in V i is associated with one peer in the network, called the peer responsible for t , denoted by resp ( t ). Locating the peer responsible for t in structured P2P networks is efficient, by querying O(log N P ) peers, where N P is the number of peers in the network [2]. Peer P i builds a set of posting lists, one for each term i n its vocabulary V i , based on its local shared content D local posting list of term t built by P i by l by l ( t ). The peer resp ( t ) just concatenates all received local posting lists to have the global posting (i.e., l ( t ) = to the process of building global posting lists as  X  X ndex building. X  Each local posting list is a list of postings. Depe nding on the indexing strategy, a posting might or might not con tain information about the individual documents. For exa mple, if peer-level indexing is used, a posting contains only the peer ID, and if document-level indexing is used, a posting contains both a document ID and a peer ID. Formally, for peer-level indexing, l ( t ) = { P i }; for document-level indexing, l i ( t ) = {&lt; d D , t  X  T ( d ij )}. A query q is a set of terms, q = { t 1 , ..., t QLen P sends a message to resp ( t i ) for each t i  X  q , requesting the global posting list l ( t i ). When all the global posting lists are retrieved, their intersection is computed by P intersection by I ( q ) (i.e., I ( q ) =  X  l ( t i ), t containing q is issued to each of the peers whose ID is in I ( q ). We refer to the peers in I ( q ) as matching peers . A query q that is issued to a peer P i is matched with all documents in D i . By default, queries are processed conjunctively, so a peer P i for the query q is R i ( q ) = { d ij  X  d ij event of a match, R i ( q ) and P i are returned to the client who issued q , which uses this information to decide whether or not to download the associated document. A query q is erroneously R ( q ) =  X  ). We refer to erroneously issued queries as failed queries . Given a query log Q, denote the set of query terms in Q by V Denote the frequency of query q  X  Q by qf ( q ), and the query term frequency of a term t  X  q by qtf ( t ) =  X  { q and qtf ( t ) is the number of queries in Q that contains t . We disregard the frequency of each term in the query a s well as the order of the terms since we only consider the case of conjunctive query matching. The total network cost required to index shared con tents at all peers and to process all queries in Q is where N e , a system parameter, is the maximum number of postings that one network message can contain. In the above expression, the first term is the cost of indexing local contents of the N P peers. The second term is the cost of retrieving global posting lists for all queries in Q . The third term is the cost of query issuing of all queries in Q to matching peers. In DHT-based networks, to contact resp ( t ), the peer responsible for a term t , one must look it up by a sequence of query messag es that obey the DHT routing protocol. At most O(log N are needed for this purpose.  X  X ublishing X  a local p osting list (i.e., sending a local posting list to the peer responsibl e for it), and retrieving a global posting list (when processing a query) also require looking up the responsible peer. We do not include this look-up cost in the first and the second terms in ( 1). The reason for this is look-up cost does not depend on the ind exing strategy (it just depends on the number of peers in the netw ork). For the same reason, we do not include the cost of returnin g matching results in the third term in (1). Peer-level indexing minimizes the first and the sec ond terms, as it minimizes the length of posting lists. Document-lev el indexing minimizes the third term, as it includes enough inf ormation in the posting lists to eliminate query issuing errors. Ho wever, it is not obvious which indexing approach is superior in gene ral. The total query processing cost depends on the cont ents shared by peers and the query workload. It also depends on N determines the relative contributions of index-buil ding  X  posting-list-retrieval cost and query issuing cost to total cost. Specifically, applications that use large messages are more effic ient at transmitting long posting lists and therefore it is more important to minimize query issuing cost. The opposite is tru e for applications that use small messages, so it is impo rtant to reduce the length of posting lists, possibly at the expens e of increased query issuing cost. More generally, N e serves as a parameter that allows us to tune the relative contribution of index-building  X  posting l ist retrieval cost, other P2P network performance factors. One of these factors is the  X  X hurn X  rate, the rate at which peers join and leav e the network. Each time a peer joins the network, it must send al l of its local posting lists to responsible peers. In addition, a newly joined peer takes responsibility for some terms, which requires it to retrieve the posting lists of those terms from other peers. Further, each time a peer leaves the network, another peer has to take over its term responsibilities. If the churn rate is high, t he index-building and posting list retrieval cost has more weight on the total cost, and vice versa. Thus, by setting a low value for N e , we can model a network with high churn rate, and vice versa. We propose to group the documents in D i into K i disjoint groups G , j = 1, ..., K i such that  X  G ij = D i , and G ij  X  each peer P i based on its local shared content D workload Q . The term set of a group G ij is defined as T ( G  X 
T ( d ik ), where d ik  X  G ij . Each group can be considered as one  X  X uper-document, X  and thus the local posting list o f a term t is defined as l i ( t ) = {&lt; G ij , P i &gt;  X  t  X  T ( G Our grouping technique is designed to minimize the following cost C Q qtf t qf q where Q i is defined as the set of queries that are erroneously issued to peer P i : the sum of C i ( Q ) over all peers. The first term of C i ( Q ) is the cost of indexing local content. Notice that the posting list of a term t may contain a maximum of K postings. Thus, reducing K i reduces the indexing cost. The second term of C i ( Q ) is the cost that P i contributes to the total cost of retrieving posting lists for queries in Q . Notice that this cost is V the global posting list of term t , and the higher the posting list retrieval cost imposed on the peer responsible for t , resp ( t ). By optimizing this cost, P i reduces cost of retrieving its posting lists. Other peers, likewise, help P i reduce its cost of maintaining the global posting lists assigned to it. The last term of C cost of erroneously issuing queries to P i . We do not include the cost of correctly issued queries, as this cost is t he same for all indexing strategies, and is unavoidable. Reducing K increases query issuing cost, since it increases th e number of erroneously issued queries. In general, given the number of groups K i , to find the optimum K groups of documents in D i that minimize the cost defined in (1), we need to consider all possible K i disjoint subsets of D intractable. We therefore adopt a simple greedy algorithm below to build our groups. Our DocumentGrouping algorithm aims to greedily minimize C i ( Q ) for each document assignment. The algorithm starts with an  X  X mpty X  solution (i.e., wi th K groups, where C i ( Q ) = 0). At each step, the algorithm randomly picks an unassigned document and assigns it to a gr oup so that C ( Q ) measured on the new solution increases the least. The algorithm stops when all documents are assigned to a group. Assigning a document to a group can only increases C may increase the length of some posting lists, and it may increase the number of erroneously issued queries. We call t he increase in C ( Q ) caused by the assignment of document d ik to group G  X  X ost of the assignment of d ik to group G ij , X  and denote it by cost ( d ik , G ij ). We describe our method to measure cost ( d follows. First, assigning document d ik to group G ij adding one posting in the posting lists of terms in the set T ( d ik ) \ T ( G posting lists of those terms only after the assignm ent. Second, assigning document d ik to group G ij may extend the set of erroneously issued queries, Q i . For simplicity, we consider queries of length two only. Assume that assigning document d G ij adds a failed query q = { t 1 , t 2 } to the set Q that case that either both t 1 and t 2 belong to T ( d failed query, nor both t 1 and t 2 belong to T ( G ij a new failed query. Thus we must have t 1  X  T ( G ij ) \ T ( d T ( d ik ) \ T ( G ij ), or vice versa. Therefore the cost of assigning d G can be measured as where q = { t 1 , t 2 }, t 1  X  T ( G ij ) \ T ( d ik Our grouping algorithm above takes as input the num ber of groups K i . As we stated earlier, the lower the value of K lower the index-building and posting list retrieval cost, but the higher the query issuing cost. Choosing the right v alue of K therefore crucial. In this section, we propose a si mple probabi-listic cost model that allows us to identify quickl y an appropriate value of K i for peer P i . Our model is based on both the term distributions in the query workload Q and the local shared content D . Note that K i could be any value from 1 to N documents in D i , and K i may vary from peer to peer. We build a cost model to estimate quickly the total cost of a peer given K i , according to (2), without actually grouping docum ents and calculating the cost. This allows us to identif y a good value of K to use as an input parameter for our grouping algo rithm. Clearly, running the grouping algorithm N i times to pick the best value of K i is inefficient. We first estimate how K i affects the index-building cost and posting list retrieval cost in (2). We denote the d ocument-level local posting list of a term t as dl_l i ( t ), while using l group-level posting list of t . length of their document-level posting lists, as fo llows cannot contain more than j entries. When K i  X  j , the group-level posting lists of the terms in class S j cannot contain more than K entries. To estimate the index-building and posting list retrieval cost of (2), we assume the worst case scenario, in which documents are adversarily assigned to groups. Thus if K length of group-level posting lists of terms in S j is j , otherwise, the length is K i . We now estimate how K i affects the query issuing cost in (2). We consider failed queries of length two only (i.e. q = { t R ( q ) =  X  ). Denote by m and n the lengths of document-level posting lists of t 1 and t 2 , respectively: Without loss of generality, assume that m  X  n . If m  X  to our assumption that documents are adversarily as signed to groups, any group G ij contains at least one document that contains t . Thus any group that contains document(s) from dl_l contains document(s) from dl_l i ( t 1 ). This means that the inter-section of group-level posting lists l i ( t 1 ) and l i C ( Q ) in (2). Similarly, when m &lt; K i and ( m+n )  X  K i , there are m groups containing one document from dl_l i ( t 1 ), and there are n groups containing one document from dl_l i ( t 2 ). Since ( m+n ) must be at least one group that contains both docum ents from dl_l i ( t 1 ) and dl_l i ( t 2 ). Thus, in combination, if ( m+n ) C ( Q ) in (2). Now we consider the case ( m+n ) &lt; K i . In this case, the probability that a group contains one document from dl_l i ( t probability that a group contains one document from dl_l n / K i . Thus the probability that a group contains one do cument from dl_l i ( t 1 ) and one from dl_l i ( t 2 ) is mn / K that there is no such group is therefore With the above probability, query q does not contribute to the query issuing cost in (2). Thus, we assume that que ry q contributes a portion of p of its query issuing cost to (2). To summarize, for a given value K i , we estimate costs in (2) as follows. Based on our cost model in Section 4.2, we propose to estimate the number of groups K i as follows. We first build two histograms, one for estimating i ndexing cost, called H 2 , as We classify all failed queries of length two in Q classes, defined as where k = 2, ..., N i  X  1, and For each class Q ik defined above, we build one histogram HQ The maximum number of histograms is ( N i + 1). Actually, we don X  X  have to build all ( N i + 1) histograms, as many sets Q empty. For example, all sets Q ik , with k &lt; 2 min empty. The number of buckets in H 1 and H 2 is at most N number of buckets in HQ k is at most  X  k /2  X  . We can build the histograms above efficiently. H 1 built easily based on the already-built local posti ng lists (a peer needs to build its local, document-level posting li sts for the efficient local processing of queries). The set of HQ can be built in a single pass over the query log. T hey actually can be built incrementally: each time a peer receives a failed query, it can easily update the appropriate histograms. Based on our cost model in Section 4.2 and these hi stograms, we can estimate the cost of the group-level indexing, given the number of groups desired. Our EstimateCost algorith m is specified below. The algorithm is straightforward. For two special c ases, K K = N i , we can directly calculate the associated costs as shown in Step 2 and Step 3, respectively. (In fact, for the se two cases, the  X  X rouping solutions X  are trivial.) For K i other than 1 or N we don X  X  know how documents are grouped, we based o n the histograms and the cost model to estimate the cost of the grouping solution, as in step 4a, 4b, 4c, and 4d. In step 4a , we estimate the cost of index-building and posting list retrieval r elated to terms whose posting lists contain less than K i postings. In step 4b, we estimate the same cost for terms whose posting list s contain K more postings. In step 4c (4d), we estimate query i ssuing cost for terms whose posting lists contain less than K i (exactly K postings. Given the estimated costs of grouping with differen t number of groups, our AdaptiveIndexing algorithm is specified above. We first build all histograms as required, and then es timate the cost for each possible value of K i . The value of K i that incurs the least cost is chosen, and documents are then assigned to groups by the DocumentGrouping algorithm. Our DocumentGrouping algorithm in Section 4.1 is ba sed on a  X  X ost-based X  distance function defined in (3). The distance function estimates the cost increase by assigning a document to a group. This cost-based distance function is differe nt from other distance functions that are normally used in docume nt clustering, which measure the similarity between a document and a group. As alternatives, we also consider these  X  X imilarity-ba sed X  distance functions commonly used in the field of Information Retrieval below. Cosine distance: the cosine distance of a group G document d ik is measured based on the cosine of their angular separation when G ij and d ik are represented as term frequency vectors. The cosine distance between G ij and d one minus their cosine similarity: frequency vector, (  X  ) is the dot product, and || V ( G ij of V ( G ij ). We also tried other distance functions (e.g., the K L-divergence [28] and the squared Jensen-Shannon divergence [29] ), but without significantly different results. We study the effectiveness of our adaptive indexing technique by applying it to a data set [32] of real user queries and shared data collected from the Gnutella file sharing network du ring the Spring of 2007. We randomly selected 650,000 queries and 2 ,000 peers from this data set, where each peer shares between 100 and 5,000 files. We use the terms in filenames to represent e ach file. We simulate a DHT-based peer-to-peer network of 2,0 00 peers. Query issuers are randomly picked among the peers. We assume that the DHT-based infrastructure is stable: each p eer joins the network once, and stays connected until all queries are processed. Thus, there is no cost of maintaining the network t opology. Also, there is no cost of re-transferring global posting lists due to peer joining and leaving. Notice that, as we stated earl ier in Section 3.2, we can tune the parameter N e to represent the cost of joining and leaving the network in our experiments. Whenever a peer wants to retrieve a global posting list for a term t , or wants to send its local posting list of t, it needs to locate the peer responsible for same for all indexing techniques. For the same rea son, we do not include the cost of returning matching documents. For each peer, we record the number of messages req uired for index-building cost), and the number of query messa ges received (i.e., its query issuing cost). We also record, for each query, the number of messages required to transfer all global posting lists of its terms from the responsible peers to the client.
 Our main metric is the total network cost to index the contents and to process the query workload (posting list ret rieval and query issuing) in terms of the total number of messages t ransferred among peers. We compare the cost of our adaptive in dexing technique with the costs of the peer-level indexing technique and the document-level indexing technique. We also incl ude the cost of adaptive indexing technique using cosine() as th e distance function to reveal the impact of the choice of dist ance function. The characteristics of queries and shared contents used in our experiments are summarized in Table 1 and Table 2. Number of queries 650,000 
Average (standard deviation) of number of terms per query 3.66 (1.32) Min  X  max number of terms per query 1  X  9 Number of peers 2,000 
Average (standard deviation) of number of files per peer 464 (554) Min  X  max number of files per peer 100  X  4,774 
Average (standard deviation) of number of unique terms per peer 786 (577) 
Min  X  max number of terms per peer 11  X  4,726 We first evaluate the total cost of our adaptive in dexing technique, document-level indexing technique, and peer-level i ndexing technique for various settings, characterized by N e , the number of postings a posting list can contain. With the assum ption that each posting is 2 bytes, we choose N e to be 500, 1000 and 2000, which is equivalent to a message size of 1KB, 2KB, and 4K B, respectively. Notice that in Section 3, we formally define a posting as a pair of peer ID and document ID, but a ctually a local posting list of a peer can be encoded as a list of IDs, starting with the peer ID, followed by document IDs. Thus using 2 bytes for each posting allows each peer to share a maximum of 65,536 documents. In Figure 1, we show the total cost of each indexin g technique over various message sizes. In all settings, adapti ve indexing outperforms both peer-level indexing and document-l evel indexing. Peer-level indexing outperforms document-level indexing in the first two settings. In the last set ting, peer-level indexing and document-level indexing have similar c ost, with document indexing slightly outperforming peer-level indexing. 
Figure 1. Adaptive indexing versus peer-level index ing and For the settings in which the size of message is sm all, the cost of index-building and posting list retrieval is domina nt. The dominance of index-building and posting list retrie val cost over query issuing cost is shown in Figure 2, for the se tting in which N = 500. For the document-level indexing, the index-building and posting list retrieval cost is about 93% of the total cost, while for the peer-level indexing, it is only about 26%. Thus, in this case, it is important to keep posting lists short, and therefore, peer-level indexing is better than document-level i ndexing (as showed in Figure 1). Interestingly, for the same setting ( N e = 500), adaptive indexing outperforms peer-level indexing, reducing the total cost by 26%, even though it creates longer posting lists. It doe s so by trading index-building and posting list retrieval cost for query issuing cost. In comparison to peer-level indexing, by crea ting an average of 8.45 groups per peer, adaptive indexing slightly increases the former cost, but significantly decreases the latter cost (Figure 2). The consequence is a net profit of the total cost. Document-level indexing makes use of the settings w ith large message size, while peer-level indexing does not. I n Figure 1, when N e = 2000, the total costs of peer-level indexing and document-level indexing are very similar. Adaptive indexing exploits the large size messages by creating more g roups. In this case, it creates an average of 24.78 groups per pee r, and reduces cost by 46% in comparison to document-level and pee r-level indexing. In Figure 3, we show the total cost of each indexin g technique for two extreme settings, where we set N e to the values of 200 and 5,000. In the first extreme setting ( N e = 200), the cost of peer-level indexing is significantly lower than the cost of do cument-level indexing as expected. As the index-building and pos ting list retrieval is dominant, it is better to keep posting lists short. Adaptive indexing, in this case, chooses the peer-l evel indexing approach for most of the peers (there are 1,919 pee rs that exploit the advantage of peer-level indexing). The average number of groups that adaptive indexing creates is only 1.22. However, adaptive indexing still outperforms peer-level inde xing by about 2%. In the second extreme setting ( N e = 5,000), document-level indexing outperforms peer-level indexing by 39%. By creating an average of 30.63 groups for most of the peers (ther e are 1,631 peers that have two or more groups), adaptive index ing  X  X oves X  away from peer-level indexing, and by not creating too many groups, it still outperforms document-level indexin g by 23%. Our detailed experimental results, summarized in Ta ble 3, demonstrate the adaptability of our adaptive indexi ng technique. In all settings, it always outperforms other indexi ng techniques. In one of the two extreme settings, it is as good as t he better of peer-level and document-level indexing, and in the other extreme setting, it outperforms the better of the two. In a ddition, with increasing message size, adaptive indexing appropri ately creates more groups, and there are more peers that adopt gr oup-level indexing over peer-level indexing. We now compare the effectiveness of our cost-based distance function and the standard, similarity-based, cosine distance function. Our distance function, defined in (3), is based on the term distributions of both local documents and user queries, while the cosine distance function is based on only the t erm distribution of documents. Also, the cosine distance function is designed to group semantically  X  X imilar X  documents, while our d istance function is designed to group documents in a way th at minimizes the query processing cost in (2). The question is w hether traditional text clustering techniques are appropri ate for creating groups that reduce index-building and query process ing costs in DHTs. X  Figure 4 shows that our cost-based distance functio n is more effective than the cosine distance function. It out performs the cosine distance function for all settings, reducing the total cost by 20% to 25%. We have similar results for other simil arity-based = 200 N e = 500 N e = 1,000 N e = 2,000 N e = 5,000 
Document-level indexing cost (number of messages, x 10 6 Avg. number of groups (Adaptive indexing) 1.22 8.45 17.51 24.78 30.63 Number of peers adopting adaptive indexing 81 1,115 1,467 1,578 1,631 
Number of peers adopting peer-level indexing 1,919 885 533 422 369 distance functions, such as the KL-divergence and t he squared Jensen-Shannon divergence. The reason our cost-based distance function outperf orms the similarity-based distance functions is because the groups formed by the latter are highly unbalanced in size (i.e., the number of unique terms in each group). In our experiments, th e largest group created by cosine distance, for example, contains 9 0% of the unique terms. Therefore, this solution approximates the peer-level indexing solution, which we have shown to be less e fficient for query processing than our proposed techniques. In structured P2P networks, document-level indexing has a high cost of index-building and posting list retrieval, while peer-level indexing has a high cost of query issuing. These co sts are interdependent, thus it is difficult to minimize al l of them simultaneously. To the best of our knowledge, there is no prior work that considers this interdependency. To address this issue, we propose an adaptive index ing technique for search systems built on structured P2P networks . We propose grouping the documents, considering each group as a  X  X uper-document, X  and building the distributed index based on these resulting super-documents. The correct grouping des ign effectively trades-off index-building and posting l ist retrieval cost for query issuing cost, resulting in a reduction in total cost. Our contribution is twofold. First, we develop a pr obabilistic cost model based on the term distributions of local shar ed documents and user queries. Our model allows us to estimate q uickly the total cost of the group-level indexing technique fo r a given number of groups. Based on this model, we identify the best number of groups for each peer. Second, we develop a cost-based distance function to guide document grouping proces s, which is also based on the term distributions of local docum ents and user queries. Our experimental results reveal that neither extrem e solutions of document-level indexing or peer-level indexing is g ood for all situations. In general, given particular system con ditions  X  particularly message size  X  one extreme solution wi ll be more cost-effective than the other while the best soluti on is somewhere in between. Our adaptive indexing technique identif ies the best of these solutions. The adaptive solution reduces cost by up to 46% peer-level indexing and by 73% compared with docume nt-level indexing. Our experiments also show that cost-based distance function is better than similarity-based distance f unctions, reducing cost by up to 25%. We are working on techniques that build semantic in dices over the peers X  contents, which help reduce the number o f posting lists in the network (dimension reduction), and also supp ort semantic searching. Indexing peers X  contents using each term as one dimension is unscalable approach. For example, a pe er sharing thousands terms needs to contact this number of pee rs when joining the network to index its content. This is c learly a very costly activity. Semantic indexing, such as the Lat ent Semantic Indexing technique, can relieve this problem by sig nificantly reducing the number of dimensions (i.e., identifyin g a small number of concepts, and use each concept as a dimen sion). Unfortunately, current P2P information retrieval sy stems either do not support semantic search, or require global know ledge to build the semantic index. [1] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. [2] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, an d H. [3] B. Y. Zhao, L. Huang, J. Stribling, S. C. Rhea, A. D. Joseph, [4] A. Rowstron and P. Druschel,  X  X astry: Scalable, [5] J. Li, B. T. Loo, J. Hellerstein, M. F. Kaashoek, D . R. [6] Y. Yang, R. Dunlap, M. Rexroad, and B. F. Cooper, [7] P. Reynolds and A. Vahdat,  X  X fficient peer-to-peer keyword [8] A. Crespo and H. Garcia-Molina,  X  X outing indices fo r peer-[9] B. H. Bloom,  X  X pace/time trade-offs in hash coding with [10] I. Podna, M. Rajman, T. Luu, F. Klemn, and K. Abere r, [11] G. Skobeltsyn, T. Luu, I. P. Zarko, M. Rajman, and K. [12] S. Michel, M. Bender, N. Ntarmos, P. Triantafillou, G. [13] O. D. Gnawali,  X  X  keyword set search system for pee r-to-[14] C. Tang, and S. Dwarkadas,  X  X ybrid global-local ind exing [15] C. Tang, Z. Xu, and S. Dwarkadas,  X  X 2P information [16] J. Lu, and J. Callan,  X  X ontent-based retrieval in h ybrid peer-[17] J. Lu, and J. Callan,  X  X ederated search of text-bas ed digital [18] G. Koloniari, and E. Pitoura,  X  X ontent-based routin g of path [19] A. Kurma, J. Xu, and E. W. Zegura,  X  X fficient and s calable [20] The Gnutella protocol specification v0.6. http://rf c-[21] C. Rohrs,  X  X uery routing for the Gnutella network, X  [22] J. Callan, Z. Lu, and W. B. Croft,  X  X earching distr ibuted [23] L. Gravano, C. Chang, and H. Garcia-Molina,  X  X lOSS: text-[24] L. Si, R. Jin, J. Callan, and P. Ogilvie,  X  X  langua ge modeling [25] J. Xu, and W. B. Croft,  X  X luster based language mod els for [26] W. Meng, C. Yu, and K. Liu,  X  X uilding efficient and [27] F. M. Cuenca-Acuna, C. Peery, R. P. Martin, and T. D. [28] S. Kullback,  X  X he Kullback-Leibler Distance, X  The [29] H. Schutze, and C. D. Manning, Foudations of Statis tical [30] L. T. Nguyen, W.G. Yee, and O. Frieder,  X  X uery work load [31] W. G. Yee, L. T. Nguyen, D. Jia, and O. Frieder,  X  X  fficient [32] L. T. Nguyen, W. G. Yee, D. Jia, and O. Frieder,  X  X  tool for 
