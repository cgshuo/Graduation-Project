 motivated from (1) knowledge engineering, (2) semantic computing and (3) computer-supported cooperative work. Knowledge engineering has proposed systematic approa-ches to facilitate a common understanding between disciplines, with the development of large ontologies, e.g., the Uni fi ed Medical Language System (UMLS ontologies are manually constructed, costly to build, and usually lack complete con-ceptual coverage of the respective domains of the disciplines. Few publications detail models or tools aimed at exploiting the semantics in ontologies to address the speci challenge of integration.
 plines or domains can support cross-disciplinary search with query expansion tech-niques that include new connections between concepts but also avoid terms that could have a confusing meaning. For instance, the word  X  cell  X  pretations including that of  X  the smallest block of biological unit  X  web  X  in architecture and  X  A basic unit of storage  X  in computer science. This can also fully automated cross-disciplinary tasks such as literature-based discovery. One of the basic challenges to model and use such connections between lexical domains is to measure the semantic similarity between terms from these domains. With current approaches to measuring semantic similarity, it would require either very thoroughly hand-crafted knowledge repositories or contextual vectors de or aligned word spaces [ 2 ]. The similarity of semantic spaces has been addressed in some studies [ 4 ], but is not suf fi cient to identify polysemy (terms with different meanings across domains).
 Thus, the question we address in this paper is how to compare domain concepts whose context vectors are de fi ned in different spaces. We propose an initial approach based on semantic alignment from a generic ontology, and evaluate its potential on a small hand crafted dataset with 2 tasks: identify which terms have different senses and synonyms across 2 disciplines. In IR, the notion of domain concept is generally associated with either word senses as de fi ned in some knowledge based repository, or the statistical use of terms in context based on the distributional hypothesis.
 Computing relatedness and the degree of similarity between words has often been addressed with corpus based geometric or probabilistic approaches, such as LSA, HAL, LDA and NNLM [ 8 , 10 ]. These approaches generate a vector (or probabilistic) space in space, into what is referred to as context vectors. Similarity between terms can then be established using mathematical tools to compare the vectors, provided that they are de fi ned in the same space. These models cannot directly address cross-domain envi-ronments, unless initial connections between domains are provided. WSD and Named Entity Recognition [ 6 ] techniques attempt to clarify speci where a domain is then considered as an area of knowledge that is widely accepted as unitary, and there is generally no attempt to connect domains.
 Knowledge based repositories (e.g. lexical resources) are hand-crafted and are employed for discovering semantic relations between words and as a basis for WSD tasks. WordNet provides some domain speci fi c versions [ 7 ], listing domain speci terms. Distributional approaches [ 5 ] or graph-based approaches [ 4 ] may be used to enhance the estimation of semantic similarity within domain.
 Domain adaptation methods seek to learn a common feature representation, by heuristic selections of common pivot features [ 1 ], or learning a shared latent sematic space underlying the domains [ 9 ]. However, the number of unseen patterns in these methods can impact on fi nal adaptation, these techniques require prior knowledge. A recent method proposes to use only data labeled in a source domain instead of alignment data, and uses an incremental learning algorithm [ 3 ]. In this paper, we make the assumption that common and similar terms between domains seed the notion of alignments. separate domains, and then connect them. We propose here to generate the initial representations with the HAL approach, which is a distance-sensitive word space. similarity measures will be affected by few identicalterms. We propose to use the semantic similarity provided by external resources to connect the dimensions. 3.1 EHAL: A Syntax-Sensitive Word Space HAL (Hyperspace Analog to Language) creates a word space (a square term-term matrix) by scanning a corpus of documents, and for each term t encountered, updating its context vector by adding to each dimension corresponding to terms occurring on a window of size W around t a value function of its distance to t. We additionally propose that this function should also account for the syntactic role of these sur-rounding terms. For a given term t and a context (surrounding) ter. t to the dimension corresponding to W in the context vector of t would then be: factor that is dependent on the syntactic role of the context term (PPMI), to further emphasise the importance of rare co-occurring terms, and to improve performance of all distributional semantic approaches [ 2 ]. 3.2 Semantic Similarity Between Context Vectors For two different domains  X  concepts for example,  X  Algorithm and  X  Process  X  from bio-information, the non-null part of their context vectors may consist of a few dimensions [e.g. step, sequence] and [e.g. stage, chain] respectively It means the context term  X  chain  X  in the context vector of the term feature in the context vector of  X  algorithm  X  . However, a distance of 0.66).
 vectors  X  non-null dimensions by employing the notion of similarity distance in an external resource, such as WordNet. More precisely, for each context term (non-null dimension) of a source context vector in domain D1 , we seek the closest context term in the target context vector in domain D2 . The score we propose is additionally asymmetric, and seeks to measure the similarity by a mapping of the most similar context terms of the second word to the fi rst. The similarity score is the sum of the weighted score for each non-null dimensions of the fi rst vector. This weighted score is the average of the context vector values for that dimension and the most similar non-null dimension in the second vector, multiplied by the distance between the non-null dimensions. This is illustrated by Equation which illustrates how to calculate the semantic association between two words W1 and W2 with the semantic similarity between the context vectors reduced to their non-null values W1[x ... ,y ]de fi ned in the sub-set of dimensions of domain A{a 1, ... b } respectively. The semantic similarity between the dimensions is de external lexical resource.
 4.1 Evaluation Dataset We built a small dataset to evaluate the potential for our approach to similarity mea-sure. To do this, we selected 11 pairs of synonym words, and 11 polysemy words between 2 domains which are biology and computer science as we had experts able to advise on these domains. The synonym pairs are (Process, Algorithm), (Analyse, Compute), (Study, Survey), (Reproduction, Replication), (Duplicate, Clone), (Sequence, Chain), (Reserve, Store), (Produce, Develop), (Flaw, Imperfect) and the polysemous words are (Cell, Gene, Host, Synthesize, Clone, Synthetic, Mutant, Object, Genetic, Taxonomy, Recovery, System, Benchmark). 4.2 Implementation Two different corpora belonging two different domains were implemented by crawling two publicly available corpora to build the models for our evaluation. There are PubMed 2 for the Biology domain and ACM 3 for the computer science domain. We fi rst pre-processed the corpora (stemming and removing stop words and low frequency terms). HAL and the EHAL were implemented with window size 20 (10 words each side of the target term). In EHAL, we set a  X  5 and in a short distance to the target, while the b function is based on the intuition that emphasising noun-noun and noun-verb contexts is more likely to provide the signi cant semantic context across domains.
 We used a path-distance measure of WordNet as the external similarity measure [ 5 ], which is the minimum number of edges that separate two words. 4.3 Rank-Based Evaluation We fi rst investigated the appropriateness of the similarity scores provided by each method to differentiate between synonyms and polysemous terms expressed in different domains. More precisely, we measured the proportion of synonyms that had a simi-larity score higher than the highest score between variants of polysemous terms, and higher than the second highest score, respectively.
 provides the most accurate distinctions. It also shows that it is reasonably stable when we look further than the highest score and is not only relying on chance. This suggest that a threshold could be estimated to provide a reliable way of distinguishing between low and these results recognize that it is a dif fi cult task. 4.4 Threshold-Based Evaluation In this second evaluation, we considered a threshold for separating synonyms from non-synonyms. We fi rst proposed to apply a standard threshold of 0.5 on both WordNet and cosine similarity measures and measured the accuracy for each task. We also investigated what an oracle threshold would lead to, and reported the range in which it would have to be for each method to maximize the overall accuracy across both tasks. The results are reported in Tables 2 and 3 . All methods use PPMI. We observe that not only did our approach always outperform the comparisons, but also that the range of an oracle threshold is much wider than for the other methods, suggesting a much better separation between synonyms and unrelated words. The results also show that all methods could identify differences between polyse-mous words, which suggests that indeed the vectors built from the two different corpora are quite different for these words. However, standard approaches are not able to draw enough similarities between the vectors of synonyms across the different domains, and the results suggest that using external resources is helpful in that regard. Our proposed approach can successfully distinguish similarity relations, especially between nouns, even though, the proposed implementation cannot fully account for highly speci fi c terms in context vectors, as they would be missing from the WordNet ontology. However the results on our initial dataset suggest that our method is promising to reduce the semantic gap in how the terms are contextually de ferently across different domains. The approach to measure the semantic similarity could be experimented on as a symmetric measure, by simply adding or averaging both asymmetric measures. Additionally, it would be interesting to trial the semantic mea-sure on a regular word association task or one that does require asymmetric associa-tions to be taken into account, within a single domain. We will also grow the evaluation dataset by recruiting more experts, and evaluate our approach with other representa-tions of the word spaces (e.g. NNLM) and a range of distance measures.
