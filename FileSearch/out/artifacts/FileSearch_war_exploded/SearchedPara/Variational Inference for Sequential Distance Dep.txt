 Sergey Bartunov SBOS @ SBOS . IN Dmitry P. Vetrov VETROVD @ YANDEX . RU Moscow State University, Moscow RUSSIA Higher School of Economics, Moscow RUSSIA One of the most important problems in machine learning and statistics is model selection. Well-known examples of it are choosing the number of hidden layers of neural network or the number of clusters in the mixture model. Model selection almost always leads to a tradeoff between model complexity and fit accuracy. While one may fit sev-eral models by varying the number of structural compo-nents and choosing the best one by some criteria, Bayesian nonparametric methods provide an elegant alternative solu-tion to this problem. Instead of comparing several models, nonparametric approach is to define a distribution on model structure which can adapt it X  X  complexity to data. Further-more, it may refine and even complicate its structure when new data is being observed.
 For many nonparametric models such as widely used Dirichlet process ( Ferguson , 1973 ) or Indian Buffet Pro-cess ( Griffiths &amp; Ghahramani , 2006 ), it is common to as-sume exchangeability of data, the property that every per-mutation of data points has the same probability under the model. While this assumption often holds, in many settings when data has temporal, spatial or any other internal de-pendencies, a proper non-exchangeable prior which takes such information into account could model data more ade-quately and thus fit it more accurately.
 A number of such distributions were developed to date in-cluding dependent Dirichlet process ( MacEachern , 1999 ) and other similar processes ( Duan et al. , 2005 ; Griffin &amp; Steel , 2006 ; Xue et al. , 2007 ), in which various construc-tions on top of Dirichlet process are considered. Besides that, a distance-dependent Chinese Restaurant Process (dd-CRP) was proposed recently by ( Blei &amp; Frazier , 2011 ), which takes an alternative approach to modeling depen-dencies by considering a distribution over partitions. Given pairwise distances of any nature (generally not symmetric), each data point connects itself to other ones (including it-self) with probability depending on the corresponding dis-tance; after that data points that are reachable from each other form a partition.
 Besides very general yet simple formulation which allows for many interesting applications including image segmen-tation ( Ghosh et al. , 2011 ) and natural language processing tasks ( Haghighi &amp; Klein , 2010 ), ddCRP has another advan-tage of not generally exhibiting marginal invariance , the property that a missing observation does not affect the joint distribution of data and parameters. This fact also distin-guishes ddCRP from other mentioned processes.
 As for many interesting distributions, posterior of ddCRP is intractable, and so far only Markov Chain Monte Carlo (MCMC) inference techniques were developed for it. Al-though it is theoretically guaranteed that a properly con-structed Markov chain will once converge to the true pos-terior, in practice it X  X  often hard to determine convergence, and no general methods are provided to estimate the re-quired number of samples to obtain a good approximation. Due to the fact that nature of ddCRP differs a lot from many other nonparametric distributions, prior developments of variational inference for DP, such as the one based on stick-breaking construction, are not applicable.
 In this paper, we propose variational inference algorithm for the important sequential case of ddCRP called sequen-tial distance-dependent Chinese Restaurant Process (seqd-dCRP), in which (a) data points arrive in sequential order, and (b) a data point can not be connected to those which arrived later (though the opposite is allowed). This as-sumption often holds in temporal data and is natural for many natural language processing tasks. Moreover, Chi-nese Restaurant Process ( Aldous , 1985 ) could be formu-lated as a special case of seqddCRP.
 The contributions of this paper are: 1. We introduce variational mean-field approximation 2. We show that our approximation of posterior provides 3. We develop efficient coordinate-ascent inference algo-The rest of the paper is organized as follows. We first review Dirichlet process and Chinese Restaurant process (section 2 ), then we briefly describe distance-dependent Chinese Restaurant Process (section 3 ). In section 4 we present our variational inference framework for seqddCRP mixture. We conclude with experiments on synthetic and real data (section 5 ). One of the most frequently used nonparametric models is Dirichlet process (DP) introduced by ( Ferguson , 1973 ). It could be seen as infinite-dimensional generalization of Dirichlet distribution parametrized by base measure G 0 and concentration parameter  X  . A draw from DP is a ran-dom distribution over draws from G 0 with the property that some draws may contain repetitive elements. The fre-quency of repetitions is governed by  X  .
 The infinite set of probabilities representing frequencies of unique draws from G 0 is distributed according to stick-breaking process (SBP) ( Sethuraman , 1994 ) which divides total probability mass into diminishing probabilities  X  k such that P 1 the rest of the unit-length stick 1 P v k  X  Beta (1 ,  X  ) . This allows for constructive definition of DP and is often used for variational inference.
 Thus one may define nonparametric mixture model using DP by placing SBP prior over mixture component assign-ments z i for data points 1 , 2 ,... and drawing mixture pa-rameters  X  k from appropriate base measure.
 Another representation of DP is Chinese Restaurant Pro-cess ( Aldous , 1985 ) which could be derived by integrat-ing out base measure from DP. This makes mixture assign-ments dependent on each other, but leaves them exchange-able and transforms DP into distribution over the all possi-ble partitions of natural numbers. It is defined as follows: consider a restaurant with infinite number of tables and no customers at the beginning. Customers enter the restau-rant one by one, each drawing her table assignment z i . By definition for the first customer z 1 =1 . All successive customers i =2 , 3 ,... draw z i according to the following distribution: where K is the number of ocuppied tables before i -th cus-tomers enters, and n k is a total number of customers al-ready chosen the table. After all the table assignments have been selected, they form clusters; after that cluster param-eters  X  k are being drawn. Generative process is finished by drawing each data point x i from the corresponing mixture component  X  z The generative process of table assignments in CRP de-scribed above may be reformulated using customer assign-ments . Let each customer in CRP choose not the table z i but rather exactly one customer c i to share a table with: Thus the first customer always sits with herself, and suc-cessive customers sequentially choose whether they want to join existing table (occupied by some of the previous customers), or to initiate a new one. It is easy to show that if we define z i ( c ) as the minimal index of customers that are reachable from i through directed graph of customer assignments (i.e. those which are sitting at the same table), then the induced partitioning z ( c ) based on such reachabil-ity is equivalent to the one constructed in CRP. ( Blei &amp; Frazier , 2011 ) generalized ( 2 ) to make customer assignments distance dependent : where f ( d ) is non-negative decay function such that f ( 1 )=0 , D is distance matrix and positive  X  governs for table initiation. This distribution on customer assignments is called sequential distance-dependent Chinese Restaurant Process (seqddCRP). Traditional CRP appears as a special case of seqddCRP and the latter is generally a very differ-ent distribution. At first, it accounts for prior dependencies in data which may be of any nature and hence does not assume customers to be exchangeable . In addition, seqdd-CRP is not generally a distribution over partitions induced by random measures (see ( Blei &amp; Frazier , 2011 ) for de-tails).
 It is also possible to define general ddCRP without assum-ing sequential order of customers by allowing assignments c &gt;i . This makes possible to start a new table not neces-sarily by drawing c i = i for some i . In that case, table as-signments are constructed treating customer assignments as an undirected graph. Below we focus on sequential restau-rants only.
 Now using ( 3 ) as a prior over mixture component assign-ments, we may define the seqddCRP mixture model: p ( x , c ,  X  |  X  )= p ( c |  X  ) where  X  = { f, D,  X  , G 0 } are process parameters, G 0 is a base measure generating mixture parameters  X  , and N is total number of data points.
 Here we account for all N possible tables (since all cus-tomers have non-zero probability to start a new table) and identify each table with a customer that initiated it by choosing to sit with herself. Notice that while parameters of empty tables affect joint distribution, marginal distribu-tion p ( x , c ) does not depend on empty tables, and so does the variational lower bound. Before we continue with the mean-field approximation of the posterior, let us first reformulate seqddCRP model. We denote as r ij ( c ) indicator function which is equal to and only if there exists a directed path from i -th customer to j -th customer in the graph induced by customer assign-ments c . We also expand customer assignment c i = j to one-hot vector of size N such that c ij =1 and c ik =0 for k 6 = j . This allows us to rewrite joint distribution of seqddCRP mixture in the following way: As we are interested in connected components z ( c ) one may observe that z i ( c )= j if and only if c jj =1 and r ij ( c )=1 which is still correct table assignment notation. Thus we enumerate tables not by abstract or-dering 1 , 2 ,..., | z ( c ) | , but rather by customer numbers 1 , 2 ,...,N . Below we denote z ij ( c )= c jj r ij ( c ) To derive variational inference we need to define the fac-torized approximation of the posterior of seqddCRP pa-rameters. Similarly to the variational algorithms for DP we choose the factorization that holds true when there are no data (i.e. for the priors). In DP such a property was true for the parameters responsible for the breaking ratio of the stick v k (see section 2 ). In the case of ddCRP such parameters are c i . Indeed, in the absence of any data the distribution p ( c ,  X  |  X  ) is fully factorized. We consider such prior-induced factorization to be the most natural choice. So we approximate posterior p ( c ,  X  | x ,  X  ) as a fully factor-ized distribution q ( c ,  X  )= Q Kullback-Leibler divergence between the true posterior and its approximation, which is equivalent to maximization of marginal likelihood lower bound ( Jordan et al. , 1999 ): log p ( x ) L = E q The main difficulty, both conceptual and computational in deriving variational inference for ddCRP, is in expectation of table assignments w.r.t variational distribution q ( c ) is E q z ( c ) . Table assignments z ( c ) are computed by de-terministic function which maps independent customer as-signments into table assignments. It is global in the sense that even if we are interested only whether customer i sits at the table started by customer j (that is z ij ( c ) ), in gen-eral case information about all customer assignments c is required to answer this question. The opposite is also true: changing just one customer assignment c i may cause global change of table assignments. We now describe how to com-pute this expectation for seqddCRP and provide efficient variational inference algorithm. 4.1. Expected table assignments via inverse of the Consider directed graph modeled by seqddCRP. It consists of
N vertices, one for each customer, all of them having exactly one link c i pointing to another customer j  X  i with probability q ( c i = j ) . We define then the probabilistic ad-jacency matrix A such that A ij = q ( c i = j ) for i 6 = j A ii =0 for all i .
 Further we denote R ij = E q ( c ) r ij ( c ) which is the prob-ability of existence of the directed path from customer i to customer j . One may observe that vertex i may be con-nected with j directly with probability A ij or through other vertices. By noting that R ii =1 for all i 1 for j&gt;i we get: We may vectorize this formula and obtain j -th column of R : where e j is j -th column of identity matrix. Finally, we obtain the whole matrix: Matrix L = I A is non-singular since it X  X  triangular and has ones on main diagonal. It could be viewed as expected Laplacian matrices are widely used in spectral graph the-ory for finding important graph properties, see e.g. ( Chung , 1996 ).
 To get expected table assignments, recall that z ij ( c )= c jj r ij ( c ) and use ( 5 ): Thus expected table assignments could be computed as column-weighted inverse of expected Laplacian induced by factorized distribution of customer assignments. This con-nection allows for deterministic inference algorithms and also provides lower bound for their computational com-plexity as matrix inverse operation is being involved. Note that equation ( 5 ) holds true not only for approximated posterior q ( c ) but also for prior p ( c | D,  X  , f, a ) also factorized. This allows us to analytically obtain var-ious properties of prior table assignments, e.g. expected size of j -th table P Based on those properties one may select the values of hyper-parameters for the model such as decay function f or  X  . 4.2. Coordinate ascent algorithm We iteratively perform fixed-point updates of each varia-tional distribution q ( c i ) by setting derivative of KL diver-gence with respect to corresponding distribution to zero. Updates for q (  X  j ) depend on a particular form of distribu-tions and are quite straightforward, so we focus on updates for q ( c ) .
 While we may naively use the result from the previous sec-tion to obtain the update equations, this would require a number of matrix inverse operations which is computation-ally expensive. Thus, we derived an efficient Algorithm 1 requiring only one implicit matrix inverse per iteration and relying on Woodbury ( 1950 ) matrix identity. We provide the details on derivation of efficient variational updates in the appendix.
 The algorithm has complexity O ( N 3 ) per iteration which is equal to cost of a matrix inverse required to compute ta-ble assignments and variational lower bound and thus may be considered as quite efficient. It is possible to further improve it X  X  computational performance by adding and re-moving clusters ad-hoc, i.e. clusters with probability of ex-istence q ( c k = k ) below some threshold may not be taken into account.
 Note that it is possible to update variational distributions Algorithm 1 Variational inference for seqddCRP
Input: data x , initial q ( c ) and q (  X  ) , hyperparameters
Compute reachability matrix R according to eq. ( 5 ) repeat until convergence q ( c i ) in any order and we have empirically observed that random order updates perform much better than sequential ones. In this section we empirically compare our variational al-gorithms with a set of baselines. We start from comparison with Gibbs sampler as it is currently the only available in-ference algorithm for ddCRP. Next we compare our vari-ational inference for CRP-equivalent mixture model and variational inference for Dirichlet Process. Then we eval-uate distance-dependent mixture model against CRP mix-ture. We release our software implementation used for the 5.1. Language modeling Following ( Blei &amp; Frazier , 2011 ) we model natural text as fully observed seqddCRP where individual tokens w are organized in tables and each table then draws a word from discrete distribution over words V (which is just word fre-quencies). This is very simple  X  X nigram X  model, yet it is especially suitable for comparison with Gibbs sampler, because there are no hidden variables and thus nothing to collapse out. Note that derivation of collapsed variational inference is usually non-trivial task and comparison of col-lapsed and non-collapsed model would be flawed, hence it was decided to use this task for demonstration of computa-tional efficiency of inference algorithms. Our dataset consisted from 2246 news articles from the As-sociated Press, we performed word stemming, but did not remove stop-words. Distances between tokens was defined just as number of tokens between two given. Sigmoid de-cay function was used with hyper-parameters set as for best model in ( Blei &amp; Frazier , 2011 ).
 We compared absolute convergence speed of our varia-tional algorithm comparing to Gibbs sampler by visually assessing convergence of posterior estimates, in particular, expected number of tables was chosen as statistics of inter-est since it is important quantity in nonparametric analysis. We provide such a plot for a randomly selected document on figure 6 . It was observed that our variational algorithm converged in just one iteration on all documents and esti-mates made with two considered algorithms are very close. 5.2. Mixture of Gaussians We continue our experimental study with continuous mix-ture modeling. We generated 5 datasets each drawn from mixture of 5 two-dimensional Gaussian distributions with equal weights, spherical covariance and mean located in (0 , 0) and ( R, R ) , ( R, R ) , ( R, R ) , ( R, R ) respec-tively, varying R from 1 to 5 . R =1 means that clusters are almost undistinguishable and R =5 makes them easely separable. Each dataset was split into 200 train data points and 200 test data points.
 It is common to compare different models by predictive likelihood which is p ( x test | x ) assuming that well-fitted model assigns high probability to test data which is ob-tained from the same context as train data, e.g. held-out part of the document. Unfortunately this quantity is in-tractable for ddCRP and thus we use the following estimate as test likelihood: p ( x test | x ,  X   X  ) = where  X   X  are cluster parameters estimated as posterior mean.
 Since the purpose of test likelihood is to measure how well estimated cluster parameters explain unseen data, new clus-ters emerged in test data could flaw the results. It is impos-sible in traditional parametric models and in DP mixture models based on truncated stick-breaking, however ddCRP allows to assign c test ,i = i for some i . Thus we restrict test data to start new tables by setting corresponding prior probabilities to zero and re-normalizing prior. 5.2.1. SEQ CRP AND DP VB We denote CRP-equivalent mixture model formulated as special case of seqddCRP as seqCRP below in the paper. We compare our variational inference algorithm for this model and one for DP based on truncated stick-breaking ( Blei &amp; Jordan , 2005 ), further we denote it as VB DP. We set parameter  X  =0 . 1 for both models to encourage small number of clusters and provided weak informative priors for covariance matrix to slightly suggest it X  X  spherical form. Truncation level for DP VB was set to 50.
 Since both algorithms are dependent of random initializa-tion we performed 300 runs in each setting and selected best results. We observed that seqCRP achieved higher variational lower bound and since both models actually rep-resent the same mixture model, it could be admitted that our variational approximation is tighter. Figure 4 contains histograms of variational lower bounds on datasets with R =5 and R =3 . In the first case where the mixture is easily separable, best lower bound obtained by our ap-proximation was 1881 . 59 comparing to 1886 . 29 from VB DP, and in the second case where it is harder to re-cover true number of clusters best results were 1891 . 73 and correspondingly 1892 . 66 .
 We also compared convergence rate for both algorithms (see fig. 3 ). Our variational algorithm is considerably slower because it involves matrix inverses extensively. However, it converges much faster than VB DP in number of iterations.
 This empirically demonstrates potential of our variational algorithm which could be preferred over DP VB in situa-tions when tight approximation is more important than run-ning time. Finally we evaluate seqddCRP with informative prior in the sense that data points generated from the same mixture component have lower distances than every pair of points which are not. In particular, we have used exponential de-cay function with parameter a =4 meaning moderate de-cay and sparseness inducing  X  =0 . 1 , as in language mod-eling task distances were plain and measured in number of data points between two given. Train data was generated sequentially from each Gaussian, while test data was ran-domly permuted and for them uninformative CRP distances were used.
 Clearly, such information improved test likelihood compar-ing to plain seqCRP, see table 1 . Also for both models test likelihood converged very fast, detailed graph is on fig. 5 . Note that we didn X  X  tune parameters of the process and thus even more performance increase could be achieved. This suggests seqddCRP as an alternative to various sequential mixture models such as Hidden Markov Model. 5.3. Time-dependent mixture Motivated by previous experiment we evaluate non-collapsed Gibbs sampler and our variational algorithm on modeling of time-dependent normal mixtures. Data was generated from a number of states, each associated with its own normal distribution, by a random process visualized on figure 1 . In each moment of time exactly one state is active and the the process draws a point from the corre-sponding distribution. It also may switch between states. Each normal distribution has mean 5  X  k where k is its num-ber and random precision drawn from Gamma distribution with shape 0 . 8 and scale 1 . 1 . This Gamma distribution was used as a prior for both algorithms. After generative pro-cess finished, we randomly permuted several points near state switches in order to simulate perturbations in transi-tive periods often observed in real data.
 The setting was the same as in 5.2.2 , except we used conve-nient average per-point test likelihood as evaluation metric (see e.g. ( Blei &amp; Jordan , 2005 )). Besides it is tractable for both sample and analytic computation obtained from varia-tional distribution, we empirically found that it is highly correlated with test likelihood estimate we used before (equation 7 ).
 Results of the comparison are shown on figure 2 . Clearly, variational inference outperforms Gibbs sampler in conver-gence speed although the latter may achieve better test like-lihood in time depending on the initialization. In the paper we proposed mean-field approximation for sequential distance-dependent Chinese Restaurant process and developed efficient coordinate ascent algorithm. Our inference procedure is closely connected with Laplacian of random graph modeled by seqddCRP. We used special fac-torization w.r.t. customer assignments which is not only natural for seqddCRP but is also applicable for conven-tional CRP. For the latter case it can be regarded as an alter-native to well-known stick-breaking variational approxima-tion. We showed that it might yield better lower bounds of marginal probabilities for Dirichlet process mixture mod-els. For the general case the only available inference frame-work is based Gibbs sampler. The proposed framework could serve as its faster deterministic analogue. We thank the anonymous reviewers for their helpful com-ments and suggestions. Sergey Bartunov is supported by RFBR grant 14-01-31361, Dmitry P. Vetrov is supported by RFBR grants 12-01-00938 and 12-01-33085.
 The general form of the variational update for each q ( c (with hyper-parameters and explicit dependency on c omit-ted for clarity): where we denote c \ i as a set of all customer assignments except for i -th customer.
 Now consider how E q ( c pends on c i . First note that if s&lt;i or k&gt;i then r not depend on assignment of i -th customer due to sequen-tial property of the process. Denote r \ i directed path from s to k that avoids i . Note that if r sk then it immediately implies r si =0 . Then we may write Equivalently in algebraic form In last equation only the second item depends on c i . Let c i = t . Then observing that r ik = r c t k we express the dependence on c i by explicit formula: First note that r c other. Also note that E q ( c for j&lt;i . The value of E q ( c q ( c j ) for j&gt;t , so we may compute it when updating as follows: Aldous, D.J. Exchangeability and related topics. pp. 1 X  198, 1985. Lecture Notes in Math. 1117.
 Blei, David M. and Frazier, Peter I. Distance Dependent
Chinese Restaurant Processes. J. Mach. Learn. Res. , 12: 2461 X 2488, November 2011. ISSN 1532-4435.
 Blei, David M. and Jordan, Michael I. Variational infer-ence for dirichlet process mixtures. Bayesian Analysis , 1:121 X 144, 2005.
 Chung, Fan R. K. Spectral Graph Theory (CBMS Regional
Conference Series in Mathematics, No. 92) . Amer-ican Mathematical Society, December 1996. ISBN 0821803158.
 Duan, A., Guindani, Michele, and Gelfand, Alan E. Gener-alized spatial dirichlet process models. In Duke Univer-sity , pp. 05 X 23, 2005.
 Ferguson, Thomas S. A Bayesian Analysis of Some Non-parametric Problems. The Annals of Statistics , 1(2):209 X  230, 1973. ISSN 00905364. doi: 10.2307/2958008. URL http://dx.doi.org/10.2307/2958008 .
 Ghosh, Soumya, Ungureanu, Andrei B., Sudderth, Erik B., and Blei, David M. Spatial distance dependent chi-nese restaurant processes for image segmentation. In
Shawe-Taylor, John, Zemel, Richard S., Bartlett, Pe-ter L., Pereira, Fernando C. N., and Weinberger, Kil-ian Q. (eds.), NIPS , pp. 1476 X 1484, 2011.
 Griffin, Jim E. and Steel, Mark F. J. Order-based dependent dirichlet processes. Journal of the American Statistical Association , 101(473):179 X 194, 2006.
 Griffiths, Tom L. and Ghahramani, Zoubin. Infinite la-tent feature models and the Indian buffet process. In
Advances in Neural Information Processing Systems 18 , 2006.
 Haghighi, Aria and Klein, Dan. Coreference resolution in a modular, entity-centered model. In Human Lan-guage Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Compu-tational Linguistics , HLT  X 10, pp. 385 X 393, Stroudsburg,
PA, USA, 2010. Association for Computational Linguis-tics. ISBN 1-932432-65-5.
 Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,
Tommi S., and Saul, Lawrence K. An introduction to variational methods for graphical models. Mach. Learn. , 37(2):183 X 233, November 1999. ISSN 0885-6125.
 MacEachern, S. Dependent Nonparametric Processes. In ASA Proceedings of the Section on Bayesian Statistical Science , 1999.
 Sethuraman, Jayaram. A constructive definition of Dirich-let priors. Statistica Sinica , 4:639 X 650, 1994. Woodbury, Max A. Inverting Modified Matrices . Num-ber 42 in Statistical Research Group Memorandum Re-ports. Princeton University, Princeton, NJ, 1950. Xue, Ya, Dunson, David, and Carin, Lawrence. The ma-trix stick-breaking process for flexible multi-task learn-ing. In Proceedings of the 24th international confer-ence on Machine learning , pp. 1063 X 1070, Corvalis,
Oregon, 2007. ACM. ISBN 978-1-59593-793-3. doi:
