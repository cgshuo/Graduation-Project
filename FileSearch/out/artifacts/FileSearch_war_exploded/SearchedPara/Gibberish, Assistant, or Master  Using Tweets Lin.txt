 Single-document summarization is a challenging task. In this paper, we explore effective ways using the tweets link-ing to news for generating extractive summary of each doc-ument. We reveal the very basic value of tweets that can be utilized by regarding every tweet as a vote for candidate sentences. Base on such finding, we resort to unsupervised summarization models by leveraging the linking tweets to master the ranking of candidate extracts via random walk on a heterogeneous graph. The advantage is that we can use the linking tweets to opportunistically  X  X upervise X  the summa-rization with no need of reference summaries. Furthermore, we analyze the influence of the volume and latency of tweets on the quality of output summaries since tweets come af-ter news release. Compared to truly supervised summarizer unaware of tweets, our method achieves significantly better results with reasonably small tradeoff on latency; compared to the same using tweets as auxiliary features, our method is comparable while needing less tweets and much shorter time to achieve significant outperformance.
 H.3 [ Information Storage and Retrieval ]: Miscellaneous Single-document summarization; tweets; highlights
Single-document summaries are also known as story high-lights of an article, which are provided by only a few news website such as CNN.com. A summary typically consists of three or four succinct itemized texts for readers to quickly capture the gist of the document. The highlights can dra-matically reduce reader X  X  information load, which can be seen as an example in Table 1.

Although practically very useful, generating such abstrac-tive summaries is technically challenging due to the ultimate c need for language understanding capability [17]. In prac-tice, most summarizers are based on extractive approach [8, 1, 14, 10, 17, 5]. The extractive summarization task aims at selecting a subset of textual units of the documents such as sentences, clauses and phrases that can optimize an ob-jective for sentence scoring and satisfy a length constraint. Sentence scoring can be done by learning from various sta-tistical and linguistic features [14, 17, 16], or by graph-based centrality method for capturing the relative importance of textual units [1, 10]. Another school of research intends to identify novel features for improving summary quality based on external data sources such as Web search results [7], click-through data [13], query logs and Wikipedia [14], comments from news readers [6], and recently tweets corpus [18, 3, 16].
Nowadays it becomes more a nd more common that users share interested news content via Twitter. Intuitively, the more often some part of the story is tweeted, the more salient it might be. Previous work assumed that such socially fo-cused sentences might be closely related to the reference summary [18, 3, 16]. However, there are some important questions left unanswered. Tweets content is notoriously in-formal and noisy, and Twitter users X  elusive posting behav-ior can hardly be related to summarization in the first place. Meanwhile, as a kind of third-party data source, tweets are inevitably subject to latency as tweets linking to a news come after the news exposure, which might drag the sum-marization performance.

In this paper, we intend to address three major concerns about using relevant tweets for single-document summariza-tion: (i) Are the linking tweets largely gibberish or useful for producing summaries? (ii) If being useful, do they play assistant or master roles? (iii) Is the latency of tweets a ma-jor setback or a reasonable tradeoff regarding the quality of summaries? For these questions, we first conduct empirical analysis on a public tweets-news-highlights tripling corpus to reveal the very basic value of tweets that was not discovered before; Then we naturally come with unsupervised models that leverage the linking tweets to opportunistically  X  X uper-vise X  the sentence scoring. Furthermore, by comparing with state-of-the-art baselines, we examine how the volume and latency of tweets influence the summaries to provide deeper insight into the practicality and usefulness of using tweets linking to news for single-document summarization.
Recently, much attention was paid to connecting news and microblogs for content enrichment [12, 3, 4, 15, 16]. Partic-ularly, a corpus containing news-tweets-highlights triplings based on CNN/USAToday news was used to enrich the fea-tures based on the tweets linking to news for highlights extraction in [16]. This public corpus contains 121 docu-ments, 455 highlights and 78,419 linking tweets regarding 17 world news events during July 2012-July 2013. Tweets were collected using Topsy search API 1 , and then the retrieved tweets containing URLs that point to CNN and USAToday news documents are gathered together with the documents and the associated story highlights. The corpus was pre-processed by removing the extremely short tweets and those tweets suspected duplicating the reference highlights.
We look into this data more deeply for revealing the basic relation among the highlights, tweets and news sentences. Our goal is to answer what exact basic value tweets can provide. Figure 1 exhibits some interesting findings:  X  X igure1a 2 shows the positions of sentences that are most similar 3 to each of the reference highlights. Figure 1b demonstrates the positions of sentences that receive the top-four largest number of X  X otes X  X rom tweets, where vote means that a tweet finds the sentence at that position as the best match. We observe that most of these important positions are located within the first 20 sentences. Considering aver-age document length (which is 54 sentences long), highlights are most likely originated from anterior part of an article.  X  The more anterior a sentence X  X  position, the more prob-able it is selected as the source of highlights, and likewise the more likely it is caught attention and tweeted by users. This can be seen in Figure 1c where the probability of sen-tences hit by highlights and tweets along their positions in documents appear very close except for the first sentence position, at which the probability hit by highlights (0.52) is much larger than hit by tweets (0.13).  X  Figure 1d shows that there is extremely rare case in the corpus that the tweets copy or nearly duplicate the reference
Highlight1-4: the highlights in reference summary
IDF-modified-cosine [1] is used to calculate similarity
Tweet1-4: the four largest number of  X  X otes X  from tweets highlights, which indicates that the corpus was cleansed to avoid the tweets that are biased towards the highlights al-ready shown in the webpages.

Therefore, the basic value of tweets linking to news lies in the fact that they tend to pick out sentences in similar positions of articles as reference highlights do. So, the next question is if they can only serve as auxiliary features as suggested in [16] or master the selection of sentences directly.
Most successful single-document summarizers are trained by using reference summaries and other precious resources for feature generation [14, 17, 5, 16]. Generally, this makes it impractical to adapt the model to new domains or lan-guages where such required data resources are not available. Particularly, tweets can only play less important role as as-sistant features in some approaches [14, 16]. Following our findings, we naturally come up with two unsupervised mod-els that allow tweets to directly locate the salient sentences without the need of reference summaries.  X  Social Vote (SociVote): In this model, we directly utilize the votes (or hits) of every document sentences re-ceived from all its linking tweets. Given a tweet, we say it hits a sentence when the tweet is more similar to the sentence than any other sentences in the document. Then we can simply rank the sentences according to their hit counts and extract the top-four sentences as a summary. This model, although simple, makes use of the very basic value of rele-vant tweets as described in Section 2.  X  Heterogeneous Graph Random Walk (HGRW): In this model, we create an undirected similarity graph which is inspired by LexRank [1]. However, our graph is heteroge-nous, thus becomes a joint model, where the nodes are of two types including news sentences and tweets, and the edge weights (i.e., node similarity) are defined as follows: sim ( x, y )=  X   X  idf-modified-cosine( x, y ) , if x.t = y.t where the coefficient  X  is used to control the contribution of sentence-tweet cross-type similarity ( .t indicates the type of node). With  X  , we encourage the voting effect between heterogeneous nodes to be mutually reinforced during ran-dom walk; Varying  X  also affects the overall ranking via the relation among homogenous nodes.

The node scores are updated by random walk over the het-erogeneous graph based on the above modified edge weights: while computing the score for a sentence, we make its cen-trality balance the effect from other sentences and that from the linking tweets with the similarity function; for a tweet, its centrality is determined by balancing its affinity to the centroid sentences and centroid tweets. We configure the system to output a summary with only sentences, a sum-mary with only tweets or a joint summary containing both.  X  Lead Sentences (Lead): This model simply extracts the first four sentences from each document, which was a well-known strong baseline adopted in DUC single-document summarization task [11].  X  X exRank: The original LexRank [1] algorithm that takes as input the homogenous type of texts, i.e., either news sentences or tweets separately. (c) highlights vs. tweets hit Table 2: Results on CNN/USAToday corpus. Bold :best score; Underline : p&lt; 0.05 as to the baselines except for CrossL2R based on paired two-tailed t-test; The suffixes -S , -T and -ST : output contains sentences, tweets and both, respectively; *: supervised models  X  X earningtoRank(L2R): The supervised summa-rizer based one RankBoost [2] with 27 local and cross-type features described in [16]. For training, the pairwise orders are derived from the largest ROUGE-1 F-score [9] of each in-stance between the instance and the reference highlight sen-tences. The model is configured to (1) take either sentences or tweets as input only, denoted as L2R ; (2) take both to incorporate cross-type features, referred to as CrossL2R . We conducted 5-fold cross-validation and used 3-1-1 split among the five folds for training, development and test. We use ROUGE-1 F-score rather than recall alone as main evaluation metric [9] because the output summaries in this task are limited as four sentences and/or tweets but have no length constraint. Table 2 shows the results. Our unsuper-vised models outperform the baselines most of time includ-ing L2R unaware of tweets and perform comparably well as CrossL2R using tweets as auxiliary features. Paired two-tailed t -test shows that the F-scores of HGRW-S/-T/-ST are significantly better than most of the baselines except for CrossL2R (which is comparable). Even our simplest model SociVote performs comparably well as L2R. It implies that the linking tweets are strongly indicative of candidate sen-tences, being an effective opportunistic master.

Our method performs comparably well as CrossL2R ac-cording to t-test and sometimes with even higher F-score. This is because the similarity between different type of in-stances can be directly utilized in the graph, but CrossL2R has to transform such correlation into high-level features that may not be expressive enough to capture the salient correlations.
 Figure 2a shows the impact of  X  . It is clear that larger giving higher weights to cross-type similarity, is generally helpful. When  X  increases, the joint ranking benefits from the addition of cross-type voting effect until some turning point. In fact, the turning points usually come late when close to 1, which is a good property since there is no training data for officially tuning  X  , and therefore one can basically use relatively large  X  to safeguard good performance. In our case, we empirically set  X  as 0.8. An exception is that the performance of HGRW-S drops quickly when  X &gt; 0.85. This is because excessive involvement of similarity from tweets allows noise dominating the sentence scoring.
 One of the general issues using third-party data source is caused by the volume and latency originated from the nature of the source used. Tweets are characterized by their instan-taneity and large quantity obtainable quickly. In this sec-tion, we examine how the volume and latency of tweets can influence the performance of summarization on the news.
Preprocessing: We first acquired the timestamps of the news and the associated tweets which were not provided in the original corpus [16]. We accomplished this by two steps: (1) we downloaded the news articles and captured the exact publish time of the webpages indicated by the proper meta tag, such as &lt; meta content= X 2012-07-20T09:14:52Z X  item-prop= X  X atePublished X  name= X  X ubdate X  property= X  X g:pub date X  &gt; , which was then transformed into the timestamp in milliseconds; (2) we re-downloaded the tweets according to tweet IDs together with their timestamps in milliseconds. Then we ranked the tweets of each article by their times-tamps to compare with the article timestamp.

The impact of tweets volume: Figure 2b shows that our three HGRW variants reach plateau with 250+ tweets, and HGRW-S performs better initially with fewer tweets whileHGRW-TandHGRW-STcatchuplaterwithmore number of tweets added. Therefore, more tweets are clearly advantageous. SociVote seem a little unstable with small tweets volume because there is lack of focus with not many posts, while such shortage can be compensated by HGRW with the participation of news sentences. At most of time, CrossL2R has lower performance than HGRW under differ-(c) Impact of tweets latency ent tweets volumes. This confirms that the cross-type sim-ilarity from tweets plays a master role with HGRW more than just assisting CrossL2R as features.

The impact of tweets latency: We examined the per-formance of different systems based on one-hour interval for 24 hours starting from the publish times of articles. Fig-ure 2c shows that latency of tweets indeed influences the results. With reference to Figure 2d that displays the box plot of the tweets volume at each given time, during the first hour, tweets-aware systems have only limited number of tweets (with around 100 tweets per article in average). Under such a case very close to cold start, two of our four models, i.e., HGRW-S and SociVote-S, still outperform the two LexRank baselines and L2R, where the real cold-start performance is guaranteed as effective as the best system not using tweets. Also, we find that HGRW-S and HGRW-ST reached significantly better performance over the baselines that ignore the tweets much more quickly than CrossL2R-S did. It took HGRW-S and HGRW-ST only 7 and 11 hours respectively, whereas CrossL2R-S needed 23 hours. By re-ferring to the tweets volume in Figure 2d, the mean number of tweets required is 399 for HGRW-S and 469 for HGRW-ST, but CrossL2R needs around 577 tweets in average. We believe that such relatively small cost that one needs to tol-erate some 7-11 hour latency yet with significant gains, is a generally reasonable tradeoff for most news summarization applications.
In this paper, we explored to use tweets linking to news articles for improving extractive single-document summa-rization without resorting to the reference summaries. We revealed the very fundamental merit of tweets that offers a voting effect on the important news sentences. Based on this finding, we present a heterogeneous graph model, which is simple but very effective, leveraging the linking tweets to opportunistically  X  X upervise X  sentences and tweets scor-ing. We also evaluated the influence of tweets volume and latency on the performance of summarization. Compared to the truly supervised summarizer unaware of tweets, our method achieves significantly better results with a reason-ably small trade-off on latency; compared to the same that uses tweets as auxiliary features, our method is comparable but needs less amount of tweets and much shorter time.
There are some interesting future directions. For exam-ple, we can study how the relevant tweets can be used to generate news summary even before the news were mas-sively reported so that other reporters can compose articles that more intentionally favor to or deepen the content on the already known focus of the online readers; A challeng-ing problem is that we can search for the relevant tweets that are potentially massive but not linking to the articles directly for reducing or even eliminating the latency; Also, we may cross-lingually summarize an article using relevant tweets of other languages for those who are not familiar with the original language of the news article.
