
This paper presents Online Topic Model (OLDA), a topic model that automatically captures the thematic patterns and identifies emerging topics of text streams and their changes over time. Our approach allows the topic modeling frame-work, specifically the Latent Dirichlet Allocation (LDA) model, to work in an online fashion such that it incremen-tally builds an up-to-date model (mixture of topics per doc-ument and mixture of words per topic) when a new doc-ument (or a set of documents) appears. A solution based on the Empirical Bayes method is proposed. The idea is to incrementally update the current model according to the information inferred from the new stream of data with no need to access previous data. The dynamics of the proposed approach also provide an efficient mean to track the top-ics over time and detect the emerging topics in real time. Our method is evaluated bot h qualitatively and quantita-tively using benchmark datasets. In our experiments, the OLDA has discovered interesting patterns by just analyzing a fraction of data at a time. Our tests also prove the ability of OLDA to align the topics across the epochs with which the evolution of the topics over time is captured. The OLDA is also comparable to, and sometimes better than, the orig-inal LDA in predicting the likelihood of unseen documents.
As electronic documents become available in streams over time, their content contains a strong temporal order-ing. Considering the time information is essential to better understand the underlying topics and track their evolution and spread within their domai n. In addition, instead of ana-lyzing large collections of time-stamped text documents as archives in an off-line fashion, it is more practical for gen-uine applications to analyze, summarize, and categorize the stream of text data at the time of its arrival. For example, as news arrive in streams, organizing it as threads of rele-vant articles is more efficient and convenient. In addition, there is a great potential to rely on automated systems to track current topics of interest and identify emerging trends in online digital libraries and scientific literature. Identi-fying these stemming topics is essential for selecting and establishing state-of-the-art research projects and business entrepreneurships that would be attractive.

Probabilistic topic modeling is a relatively new approach that is being successfully applied to explore and predict the underlying structure of discrete data, such as text. A topic model, such as the Probabilistic Latent Semantic Indexing (PLSI) proposed by Hofmann [9], is a statistical genera-tive model that relates documents and words through latent variables which represent the topics [14]. By considering a document as a mixture of topics, the model is able to gen-erate the words in a document given the small set of la-tent variables (or topics). Inverting this process, i.e. fitting the generative model to the observed data (words in doc-uments), corresponds to inferring the latent variables and, hence, learning the distributions of underlying topics.
Latent Dirichlet Allocation (LDA) [2] extends the gen-erative model to achieve the capacity of generalizing the topic distributions so that the model can be used to gen-erate unseen documents as well. LDA considers the top-ics to be multinomial distributions over the words, and as-sumes the documents to be sampled from a random mix-tures of these topics. To complete its generative process for the documents, LDA considers Dirichlet priors for the doc-ument distributions over topics and the topic distributions over words.

This paper presents an online version of LDA that auto-matically captures the thematic patterns and identifies top-ics of text streams and their changes over time. Our ap-proach allows LDA model to work in an online fashion such that it incrementally builds an up-to-date model (mixture of topics per document and mixture of words per topic) when a new document (or a set of documents) appears. A solu-tion based on the Empirical Bayes method is proposed. The idea is to incremen tally adjust the lear ned topics according to the dynamical changes in the data with no need to access the previously processed documents. This is achieved by sampling words in the newly a rrived documents according to the distribution represented so far by the current model. The count of words in topics, resulted from running LDA at a time instance, is used to construct (weighted) priors at the following time instance. Thus, in our method, the new topic distributions will correspond to the previous realistic text structures.

Most of the related work either processes archives in an off-line fashion (e.g. [16]), post-discretizes the time ([17, 13]), or uses unconjugated priors to multinomial dis-tributions and trained on all the previous data (e.g. [3, 15]). Our online topic model, however, makes use of the con-jugacy property of the Dirichlet distribution to keep the model X  X  structure simple, and to enable sequential infer-ence. In addition, OLDA processes small subsets of data at a time which improve its memory usage and time com-plexity. The dynamics of our proposed approach provide a natural mean to solve the task of detecting emerging trends in text streams and tracking their drift over time. The idea is to use the inferred topic description to compute the sim-ilarities between the aligned topics across time and detect the topics that appear to be outliers. This approach has the added advantage that one could compute in real time when the topic emerges and when it ceases to be an outlier.
Our method is evaluated both qualitatively and quantita-tively using benchmark datasets. The results are compared to the original LDA. We have found meaningful patterns in the discovered topics within the application domain. In ad-dition, the OLDA model is able to align the topics across the epochs and, eventually, captures the evolution of the topics over time easily. The OLDA is also comparable to, and sometimes better than, the original LDA in predicting un-seen documents as measured using perplexity.

The rest of the paper is organized as follows. Our On-line LDA approach is introduced in Section 3, following a short review of the most related work in the literature (Sec-tion 2). In Section 4, we present the experiments we per-formed on NIPS and Reuters-21578 datasets and the results we obtained. Our final conclusions and suggestions for fu-ture work are discussed in Section 5.
Considering time information for the task of identifying and tracking topics in time-stam ped text data is the focus of recent studies (e.g. [4, 7, 10, 11]). Among other approaches, statistical modeling using versions of PLSI (e.g. [5]) and
LDA (e.g. [16, 3, 6, 13, 12, 15]) have been deployed to solve this task.

In the probabilistic topic modeling that is based on LDA, the studies have examined latent topics and their changes across time in three main fold. The first, as in [16], had jointly modeled time and word co-occurrence with no
Markov dependencies such that it treated time as an ob-served continuous variable. This approach, however, works offline, as the whole batch of documents is used once to construct the model. This feature does not suit the online setting where text streams continuously arrive with time.
In addition, many methods use post-or pre-discretized time analysis. The former involves fitting a topic model with no reference of time, and then ordering the documents in time, slicing them into discrete subsets, and examining the topic distributions in each time-slice. The work in [6] is one example of such approach. On the other hand, the pre-discretaized time analysis of topic modeling pre-divides the data into discrete time slices, and fits a separate topic model in each slice. Examples of this type include th e experiments with the Group-Topic model [17] and the personal infor-mation dissemination behavior model [13]. Although our method discretize the time, it is distinguished by its ability of utilizing the newly acquired knowledge within the learn-ing process and tracking the evolution of topics over time.
The work in [3], and most recently [15], have used a time series analysis to present a dynamic topic model (DTM) which explicitly models the evolution of topics with time by estimating the topic distribution at various time instances.
To do so, the authors assume that the parameters are con-ditionally distributed by normal distributions with mean equal to the corresponding parameter at the previous time instance. However, since the normal distribution is not a conjugate to the multinomial distribution, the model does not yield a simple solution to the problems of inference and estimation. Finally, Multiscale Topic Tomography Model (MTTM) [12] is a sequential topic model which is the most relevant work to our approach. It uses conjugate priors using the Poisson distribution to model the generation of word-counts. Unlike our method, MTTM does assume the document streams to be of equal sizes.
First, before defining the online approach, we describe the statistical model of LDA [2] and the Gibbs sampling algorithm for inference in this model [6]. A glossary of notations used in the paper is given in Table 1.

LDA is a hierarchical Bayesian network that relates words and documents through latent topics. Since the words are observed, the document and the topic distributions,  X  and  X  , are conditionally independe nt. Furthermore, the doc-D total number of documents K number of topics
V total number of unique words  X  size of sliding window
N d number of word tokens in document d
S t a stream of documents arriving at time t
V
N w z  X   X   X  d K-vector of priors for document d at time t  X   X  uments are not directly linked to the words. Rather, this re-lationship is governed by additional latent variables, z ,in-troduced to represent the responsibility of a particular topic in using that word in the document, i.e. the topic(s) that the document is focused on. By introducing the Dirichlet priors  X  and  X  over the document and topic distributions, respectively, the generativ e model of LDA is complete and generalized to process unseen documents. LDA is based on the assumption of exchangeability for the words in a docu-ment and for the documents in a corpus.

The generative process of the topic model specifies a probabilistic sampling procedure that describe how words in documents can be generated based on the hidden topics. It can be described as follows: 1. Draw K multinomials  X  k from a Dirichlet prior  X  , one for each topic k ; 2. Draw D multinomials  X  d from a Dirichlet prior  X  , one for each document d ; 3. For all documents, d , in the corpus, then for all words, w , in the document: ( p ( w i | z i , X  ))
Because an exact approach to estimate  X  is intractable, sophisticated approximations are usually used. Griffiths and Steyvers in [6] proposed Gibbs sampling as a simple and effective strategy for estimating  X  and  X  . Under Gibbs sam-pling,  X  and  X  are not explicitly estimated. Instead, the pos-terior distribution over the assignments of words to topics,
P ( z | w ) , is approximated by means of the Monte Carlo al-gorithm which iterates over each word token in the text col-lection and estimates the probability of assigning the current word token to each topic ( P ( z i = j ) ), conditioned on the topic assignments to all other word tokens ( z  X  i ) as follows [6]: P ( z i = j | z  X  i ,w di , X   X   X ,  X   X   X  )  X  where C VK w  X  i ,j is the number of times word w is assigned to topic j , not including the current token instance i ;and ,j is the number of times topic j is assigned to some word token in document d , not including the current in-stance i . From this distribution, a topic is sampled and stored as the new topic assignment for this word token. Af-ter a sufficient number of sampling iterations, the approx-imated posterior can be used to get estimates of  X  and  X  by examining the counts of word assignments to topics and topic occurrences in documents .

To enable LDA to work in an on-line fashion on data streams, OLDA model considers the temporal ordering in-formation and assumes that the documents are divided in time slices. At each time slice, a topic model with K com-ponents is used to model the newly arrived documents. The generated model, at a given time, is used as a prior for
LDA at the successive time slice, when a new data stream is available for processing. The hyper-parameters  X  can be interpreted as the prior observation counts on the number of times words are sampled from a topic before any word from the corpus is observed ([14], [1]). So, the count of words in topics, resulted from running LDA on documents received at time t , can be used as the priors for the t +1 stream.
Our approach allows many alte rnatives for keeping track of history at any time t , ranging from a full memory that keeps track of the complete history to a short memory that keeps the counts of the model associated with time t  X  1 only. Such variety of solutions suits the structure of text repositories, since the flow an d nature of document streams differ according to the type of the corpus and, consequently, the role of history would be different too. While the current experiments will demonstrate some of these differences, it is part of our future work to investigate the role of history in inferring future semantics. 3.1 Generative Process and Approximate
To formulate the problem, we first assume that docu-ments arrive in ascending order of their publication date. After each time slice, t , of a predetermined size  X  ,e.g. an hour, a day, or a year, a stream of documents, S t = { d 1 ,  X  X  X  , d M t } , of variable size, M t , is received and ready to be processed. The size of the time slice,  X  , depends on the nature of the corpus on which the model is applied, and on how fine or coarse the resulted description of data is ex-pected to be. The indices of the documents within a stream, S t , preserve the order by which the documents were re-ceived during the time slice t ,i.e. d 1 is the first document to arrive and d M t is the latest document in the stream. A doc-ument d received at time t is represented as a vector of word stream S t introduces new word(s) in the vocabulary. These words are assumed to have 0 count in  X  for all topics in pre-vious streams. This assumption is important to simplify the definition of matrix B and the related computation.
Let B t  X  1 k denotes an evolutionary matrix of topic k in which the columns are the word-topic counts  X  j k , generated for streams received within the time specified by the sliding window, i.e. j  X  X  t  X   X   X  1 ,  X  X  X  ,t  X  1 } .Let  X   X  be a vector of  X  weights each of which is associated with a time slice from the past to determine its contribution in computing the priors for stream S t . We assume that the weights in  X  t sum to one. Hence, the parameters of a topic k at time t are determined by a weighted mixture of the topic X  X  past distributions as follows:
Computing the  X   X  X  in this manner ties the topic distribu-tions in the consecutive models and captures the evolution of topics in a sequential corpus. Thus, the generative model for time slice t of the proposed online LDA model is given as follows: 1. For each topic k =1 ,  X  X  X  ,K 4. For each document, d ,
At time slice =1 , the topic parameters,  X  1 k ,aredrawn from a Dirichlet prior, Dir (  X |  X  1 k ) ,where  X  1 k is initialized to some constant, b , as done in the original LDA modeling, e.g. [6].

Maintaining the models X  priors as Dirichlet is essentially useful to simplify the inference problem by making use of the conjugancy property of Dirichlet and multinomial dis-tributions. In fact, by tracking the history as prior patterns, the data likelihood and, hence, the posterior inference in the static LDA are left the same, and applying them to our proposed model is a straightforward. The main difference between the two approaches in this regard is that the in-ference problem in our online approach is solved by using chunks of the data instead of the whole set. This makes the time complexity and memory usage of OLDA efficient and applicable for genuine applications. Our model uses Gibbs sampling as an approximate inference method to estimate the word-topic assignments. The conjugacy property of our priors makes the application of the sampling method in our approach very easy. 3.2 Topic Detection and Tracking
The dynamics of our proposed approach provide a natu-ral mean to capture the topics and their evolution over time.
By constructing the priors as a weighted combination of the history, the topics are tied and automatically aligned across time. The matrix B t k can be considered as the evolution of topic k in which the topic development over time is cap-tured. Furthermore, novel concepts or topics can also be identified. We define a novel topic as the one which, when appears, is  X  X ifferent X  from the previous (or current) con-cepts, i.e. is an outlier, and with time it becomes  X  X ain-stream X  and, hence, ceases to be an outlier.

After applying the topic modeling, a topic is represented as a vector of probabilities over the space of words. The dis-similarity between two topic distributions, p and q , can be computed in such a space usin g the Kullback Leibler (KL) divergence. The KL divergence KL( p q ) represents the average additional amount of bits required to encode sam-ples from p with a code based on q [1], and is given by KL divergence is not a real metric, since it is not symmetric.
Thus, in our work, we compute the average of KL( p q ) and KL( q p ) and denote it KL distance or D KL in the rest of the paper.

An emerging topic can be viewed as the one that is dif-ferent from its peers in the same stream, or from all the topics seen so far. To quantify the difference, we define a  X   X 
K distance matrix Dist where each entry, Dist ( t, k ) ,is the D KL between the distributions of topic k at time t and t +1 .Let CL be a confidence level, and perc t be the per-centile, the value below which a CL percent of distances computed at time t fall. The identification of emerging topics can be modeled by considering different approaches to compute the percentile at time t : either to consider the
K topic distances computed at time t (current percentile -perc t ) , or to use all the  X   X  K distances computed so far (historic percentile -percALL t ). Then, if the KL distance of a topic,  X  t k , from the one that immediately precedes it,  X  , exceeds the percentile value, perc t ( percALL t ,re-spectively), the topic is flagged as a nominated emerging topic . Thus, given the evolution matrices, B t , the emerging topic detection algorithm (Edetect) at time t can be formu-lated as follows: 1. Etopics =  X  ; EtopicsALL =  X  ; 2. For each previous time slice, j =2 to  X  5. Compute perc t = percentile( Dist (  X   X  1 , :) ,CL ); 6. Compute percALL t = percentile( Dist, CL ); 7. For each topic, k =1 to K
Thus, the algorithm returns the topics that are flagged as emerging topics in stream S t . Note that the distances in Dist need not to be recomputed at every time slice and can be constructed incrementa lly to reduce time complexity. 3.3 OLDA Algorithm
An overview of the proposed Online LDA algorithm is shown in Algorithm 1. In addition to the text streams, S t , the algorithm takes as input the CL confidence level, the weight vector  X  , and fixed Dirichlet values, a and b , for ini-tializing the priors  X  and  X  , respectively, at time slice 1 . Note that b is also used to set the priors of new words that appear for the first time in any time slice. If N stream notes the number of streams processed, the output of the al-gorithm will be: N stream generative models, the evolution matrices B k for all topics, and lists of nominated emerging topics, one for each stream.
 Algorithm 1 Online LDA 14: [ Etopics ( t ) ,EtopicsA ( t )] = Edetect ( CL )
Online LDA (OLDA) is evaluated in three problem do-mains: document modeling, document classification, and emerging topic detection. The performance of the proposed method is compared to the standard version of LDA. OLDA is trained on the individual stream arriving at each time t , while the original LDA, named LDA-upto, is trained on all the streams received up to time t . Both models were run for 500 iterations and the last sample of the Gibbs sampler was used for evaluation. The number of topics, K ,isfixed across all the streams. Following the settings in [2, 6], K , a ,and b are set to 50 , 50 /K ,and 0 . 1 . For now,  X  pends on the topic distribution of the previous stream only, i.e.  X  =1 . Using different weight settings for  X  ,three variants of OLDA are considered specifically for the doc-ument modeling problem. The standard version of our ap-proach, which we call OLDA, uses the actual counts of the previous model to compute the priors.The second model, namely OLDA-fixed, ignores the history and processes the text stream using fixed symmetric Dirichlet prior. In the last version, named OLDA-norm, the counts are normalized be-tween zero and one before being used. All experiments are runon2GHzPentium(R)M-pro cessor laptop using  X  X at-lab Topic Modeling Toolbox X , authored by Mark Steyvers and Tom Griffiths 1 . 4.1 Datasets
The following is a short description of the datasets used in our experiments.

Reuters-21578 2 . The corpus consists of newswire arti-cles classified by topic and ordered by their date of issue.
There are 90 categories with some articles classified in mul-tiple topics. The ApteMod version of this database has been used in many papers. This version consists of 12,902 docu-ments, with approximately 27,000 features in total.
For our experiments, only articles with at least one topic were kept for processing. For data preprocessing, words were only down-cased and stemmed to their root source.
The resulting dataset consists of 10337 documents, 12112 unique words, and a total of 793936 word tokens. For sim-plicity, we partitioned the data into 30 slices and considered each slice as a stream.

NIPS dataset 3 . The NIPS set consists of the full text of the 13 years of proceedings from 1988 to 2000 Neu-ral Information Processing Systems (NIPS) Conferences.
The data was preprocessed for down-casing, removing stop-words and numbers, and removing the words appearing less than five times in the corpus. The data set contains 1,740 research papers, 13,649 unique words, and 2,301,375 word tokens in total. Each document has a timestamp that is de-termined by the year of the proceedings. Thus, the set con-sisted of 13 streams in total. The size of the streams, M t , varies from 90 to 250 documents. 4.2 Document Modeling timation that describes the underlying structure of data.
One common approach to measure this is by evaluating the model X  X  generalization performance on previously unseen documents. Perplexity is a canonical measure of goodness that is used in language modeling to measure the likelihood of a held-out test data to be generated from the underlying (learned) distributions of the model [8]. The higher the like-lihood is, the lower the perplexity will be, and, hence, better generalization performance can be achieved. Formally, for a test set of M documents, the perplexity is [2]: pling X  must be performed to get the document-topic counts of the unseen document which are required to compute the likelihood (refer to [8] for details).
 upto topic models on the NIPS and Reuters datasets. At every time slice, we compare t heir perplexity performance.
Figures 1 and 2 illustrate the perplexity of the models trained on Reuters and NIPS, respectively. OLDA improved the document modeling in Reuters with respect to the LDA baseline. As for Online models with normalized or fixed priors, the performance is reversed. This shows that infor-mation propagated from the past is very useful to predict future streams in Reuters.
 ior. OLDA with normalized priors performed better on the test data. LDA framework, in general, is a statistically data-dependent approach. So, the role of history would, eventu-ally, vary according to the homogeneity of the domain. This justifies the importance of the weight matrix  X  .
 data for NIPS, the perplexity noticeably decreased. Because our approach has more parameters and they are set accord-ing to the information propagated from previous streams, the online model results in better fitting of the data pro-cessed so far rather than predicting future documents. This Figure 1. Comparisons of Perplexities of OLDA and result matches similar findings in the literature [12] and sat-isfies the objective for which our model is applicable.
To verify the ability of our model of visualizing the data, we analyzed the posteriors of OLDA estimated from
Reuters and NIPS corpora. Due to lack of space, only two examples from NIPS and Reuters are listed in Table 2 and 3 respectively.

Like the standard LDA, our method is able to identify meaningful topics in NIPS su ch as classification, speech recognition, Bayesian learning, and regression. The top-ics discovered in Reuters at every stream fit well with the categories that the articles belong to. Yet, OLDA is able to find these topics with no access to the entire data. Rather, the model is generated from a small fraction of documents, which makes our model superior in terms of time and mem-ory efficiency. Figure 3 comp ares the execution time re-quired for OLDA and the standard LDA to generate the topic model at every time instance for Reuters. It can be seen that OLDA requires approximately a constant time, depending on the size of each stream, while the run time required by LDA-upto to analyze the data is accumulative.
In addition, LDA requires the whole data to be stored for fu-ture processing, however, our model stores only a metadata of the data in terms of a small number of generative models.
Furthermore, OLDA is able to identify more fine topics that may be represented by a small number of documents at a certain point of time. For example, in NIPS, the topic  X  X upport Vector Machine X  (SVM) appeared in three docu-ments in year 1995, in two documents in year 1998, in six documents in 1999, and in 9 documents in 2000.

Table 4 lists the  X  X VM X  documents that appeared in 1995 and the  X  X VM X  topic distribution over words gener-ated by LDAupto, OLDA, and OLDA-norm. The table also shows the weight of the topic  X  X VM X  in the document dis-tribution generated by the models. The number between brackets represents the rank of the topic  X  X VM X  in the doc-ument, e.g. rank (1) means the topic has the highest weight in the corresponding document. LDA-upto was not able to detect  X  X VM X  as a distinguished topic, so we report four topics that had the highest weights in  X  X VM X  documents.
On the other hand, both the OLDA models were able to detect the topic and assign high weight for it in the docu-ments X  distributions. The same observation is found in the 1998 and 1999 models. LDAupto was only able to detect  X  X VM X  in the year 2000.
 learning X  in Table 2, have a strong and constant identity over the years, while other topics were a mixture of ei-ther meaningful themes, like topic 14 which is a mixture of  X  X VM X  and  X  X haracter recognition X , or  X  X unk X  topics that are holding words like  X  X bstract X ,  X  X igure X ,  X  X ntroduction X  and so on. Our intuition is that the number and size of rel-evant documents is an important factor. By examining the distribution of topic SVM from year 1998 to 2000, it can be clearly seen how SVM related words are dominating over character recognition terms as the number of SVM articles increases. In addition, the setting of the number of com-ponents, K , has a major impact too. On inspection, we tested OLDA with different settings for K (see Figure 2).
However, detailed analysis of the effect of the number of component is part of our future work.
 Table 2. Examples of topics estimated by OLDA from Table 3. Examples of topics estimated by OLDA from
Table 4. The topic  X  X VM X  (distribution &amp; documents) from NIPS in LDA, OLDA, and OLDA-norm at year 1995.
The top lists the weight and rank of the topic SVM for each document. The bottom list the distribution of the topic from each model
It is also interesting to track the popularity of a topic as a function of time. This can be easily done by examining the topic evolution matrices. Fi gure 4 illustrates the popularity of two topics, the  X  X ayesian learning X  and  X  X ultilayer neu-ral networks X  (NN), in terms of topic probability at each year. The first topic is clearly gaining more interest in the literature while the topic  X  X N X  is declining. 4.2.1 Document Classification The distribution of a document over topics can be consid-ered a reduced description of the document in a new space spanned by the small set of latent variables [2]. So, the performance of the topic model can be evaluated by investi-gating the amount of discriminative information that is pre-served in the document distributions. One way to do this is by solving a classification problem. For evaluation, classi-fication accuracy and F1 measure are common measures.
We conducted a two class-classification problem us-ing the Reuters dataset. At each time slice, OLDA and LDAupto models were trained without using the true class labels with K set to 50 , as in [2]. Then, the document distributions,  X  t d , are used to train a Support Vector Ma-chine (SVM) to classify the  X  X arn X  class 4 .SVMwasrun five times using different 20  X  80% partitions of train-test sets. The average F1 at every time stream for both models
Figure 4. Tracking topics in NIPS over 13 years. Top: topic 32 (Bayesian Learning). Bottom: topic 8 (Multilayer neural networks -supervised learning/gradient descent) is given in Figure 5, and the performance in terms of clas-sification accuracy averaged over all the streams is given in
Table 5. While trained on a small subset of the corpus, our approach is able to generate a model that is as descriptive as the one generated using the whole data. In fact, the low
F1 obtained with OLDA were due to the random partition-ing that resulted in test sets that do not include any positive example. Figure 5. Average F1 of OLDA and LDAupto trained on
Reuters 4.2.2 Emerging Topic Detection
The objective of this set of experiments is to test the abil-ity of our method to detect novel topics at the time of their arrival. We test the emerging topic detection at two con-fidence levels: 90% and 95% . We applied the emerging topic detection method on NIPS and a number of topics were flagged at each year. For example, Topic  X  X VM X  was detected at year 1999 at both confidence levels. Figure 6 illustrates the distance and probability of topic  X  X VM X  with
CL set to 90% . The year at which the topic distance ex-ceeds the historic (current) percentile is marked by #(  X 
Because the number of components K is fixed, an emerging topic appears first with a small and/or similar topic. For ex-ample, when  X  X VM X  first appeared in 1995, it shared topic 14 with the topic  X  X haracter/digit recognition X .
 hence, the probability of the topic sharply increased while the distance from the topic distribution at year 1999 de-creased. Thus, the topic ceases from being an emerging topic and the algorithm does not consider it novel anymore.
The reason why  X  X VM X  was not detected in the year 1995 could be related to the number of documents, i.e. number of tokens, that are associated to the topic compared to other topics. As can be seen in Figure 6, the probability of the topic at that year, stream =8 , is very low. To address this behavior, we are working on a  X  X eighted KL distance X  which is invariant with respect to the number of tokens as-sociated to a topic.

Reuters data. The documents of two classes,  X  X rude X  and  X  X offee X , were held out for some number of streams. Then, at the forth (seventh) time slice, the documents of  X  X rude X  ( X  X offee X ) were released. Our emerging topic detection was able to detect both topics as emerging topics at the time of their release for both the current and historic distances. ter releasing the  X  X rude X  documents while Table 6 illus-trates the output of our method for CL= 95% using the historic percentile. The topic 28 (18) in stream 4 (7) clearly corresponds to the  X  X rude X  ( X  X offee X ) documents that were released at that time. The topic  X  X rude X , though, appeared again as a new emerging topic, at stream 10 for example (see Table 6). The words  X  X pec X ,  X  X elief X ,  X  X evenue X , and  X  X evelopment X  are clearly the cause of the flag. These words indicate new news regarding some re-lief/development efforts of the Opec.
Figure 6. Distance and probability of Topic 14 (Charac-ter recognition -SVM) over 13 years. The topic is flagged as emerging topic at years 1990 and 1996 with historic percentile (#) and at years 1994 and 1999 with current per-centile (  X  ) . The confidence level is 90% Table 6. The output of the Emerging Topic Detection on
Reuters corpus. The crude topic (28) is detected at stream 4 and the coffee topic (18) is detected at stream 7. The distributions, probability, percentage of documents of both the past and the new topics are listed
We have developed an online topic model for discrete data to model the temporal evolution of topics in data streams. Our approach is a non-Markov on-line LDA Gibbs sampler topic model (OLDA) in which the current model, along with the new data, guide the learning of a new gener-ative process that reflects the dynamic changes in the data. This is achieved by using the g enerated model, at a given time, as a prior for LDA at th e successive time slice, when a new data stream becomes available for processing.
The weight of history in the generative process can be controlled by the weight matrix depending on the homo-geneity of the domain. Our model results in an evolutionary matrix for each topic in which the evolution of the topic over time is captured. In addition, we proposed an algorithm to detect emerging topics based on the framework of OLDA.
By processing small subsets of documents only, OLDA is able to learn meaningful topics, similar and in some cases better than the LDA baseline. Our method also outperforms LDA in detecting topics represented by a small set of docu-ments at a certain point in time.

The proposed approach can be extended in many direc-tions. Examining different settings for the weight matrix is part of our future work to investigate its effect on the learned models. We are also considering the use of prior-knowledge to learn (or enhance the construction of) the parameters. In addition, different alternatives are considered for the dis-tance metric used to compute the dissimilarities between topic distributions. We plan to construct a weighted dis-tance metric that  X  X ormalizes X  the document size and dis-tinguishes between inter-topic differences and intra-topic drifts.
 [1] C. M. Bishop. Pattern Recognition and Machine Learning . [3] D. M. Blei and J. D. Lafferty,  X  X ynamic topic models, X  In [4] B. Cao, D. Shen, J. Sun, X. Wang, Q. Yang, and Z. Chen, [5] T. Chou and M. Ch. Chen,  X  X sing Incremental PLSI [6] T. L. Griffiths and M. Steyvers,  X  X inding scientific topics, X 
Proceeding of the National Academy of Sciences , pp. 5228 X  5235, 2004. [7] R. Guha, R. Kumar, D. Sivakumar, and S. Jose,  X  X nweaving a Web of Documents, X  Proceeding of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data
Mining , 2005 [8] G. Heinrich,  X  X arameter estimation for text anal-ysis, X  Arbylon publications . Retrieved from: http://www.arbylon.net/publications/, Retrieved date: 12/9/2007, Aug 2005. [9] T. Hofmann,  X  X robablistic Latent Semantic Indexing, X  Pro-ceedings of the 15th Conference on Uncertainty in Artificial
Intelligence , 1999. [10] J. Kleinberg,  X  X ursty and hierarchical structure in streams, X 
Proceeding of the 8th ACM SIGKDD International Confer-ence on Knowledge Discovery in Data Mining , 2002. [11] Q. Mei and C. Zhai,  X  X iscovering evolutionary theme pat-terns from text: an exploration of temporal text mining, X  Pro-ceeding of the eleventh ACM SIGKDD international confer-ence on Knowledge discovery in data mining , 2005. [12] R. Nallapati, S. Ditmore, J.D. Lafferty, and K. Ung,  X  X ul-tiscale topic tomography, X  Proceedings of the 13th ACM
SIGKDD international conference on Knowledge discovery in data mining , 2007. [13] X. Song, C.-Y. Lin, B. L. Tseng, and M.-T. Sun,  X  X odel-ing and predicting personal information dissemination be-havior, X  Proceedings of the 11th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Min-ing , 2005. [14] M. Steyvers and T. L. Griffiths,  X  X robabilistic Topic Mod-els, X  In T. Landauer, D McNamara, S. Dennis, and W. Kintsch (ed), Latent Semantic Analysis: A Road to Meaning ,
Laurence Erlbaum, 2005. [15] C. Wang, D. Blei, and D. Heckerman,  X  X ontinuous time dy-namic topic models, X  The 23rd Conference on Uncertainty in
Artificial Intelligence , 2008. [16] X. Wang and A. McCallum,  X  X opics over Time: A Non-Markov Continuous-Time Model of Topical Trends, X  ACM
SIGKDD international conference on Knowledge discovery in data mining , 2006. [17] X. Wang, N. Mohanty, and A. McCallum,  X  X roup and topic discovery from relations and text, X  The 11th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining Workshop on Link Discovery: Issues, Approaches and Applications , pp. 28-35, 2005.
