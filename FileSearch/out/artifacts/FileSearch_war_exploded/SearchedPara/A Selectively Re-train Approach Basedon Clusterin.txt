 Data stream classification has been wi dely studied, and there are many success-ful algorithms for coping with this problem [6,11]. However, most of these studies assume that data streams are relatively balanced and stable, which results in a failure to handle rather skewed distributions. Actually, skewed data streams are very important in many real-life data stream applications, such as credit-card fraud detection, diagnosis of rare diseases, network traffic analysis etc. In such skewed data streams, the probability that we observe positive instances is much less than the probability that we observe negative ones. For imbalanced data streams, instances from the minority class are more costly and thus are of more interest. For example, online credit-card fraud rate of US was just 2 percent in 2006 [4] which means that in credit card transactions stream, there are a lot of genuine transactions and very few fra ud transactions. Hence, it X  X  very neces-sary and important to classify such minority instances from all streams because classifying an instance of credit-card fraud (positive) as a normal transaction (negative) is very costly.

Concept-drift occurs in the stream when the underlying concepts of the stream change over time. It is difficult to predict when and how the concept changes. When any of concept changes occurs, a d ecrease in classific ation accuracy usu-ally occurs because the training data the model is built on would be carrying out-of-date concepts. Many of the previous work aimed to effectively update the classification model when stream data flows in. After some period, those approaches throw out the out-of-date examples or fade them out by decreas-ing their weights as time elapses. Skewed distribution and concept-drifting are two challenges for traditional classification algorithms. Classification algorithms are required to deal with data streams with skewed distributions, at the same time, when concept-drifting occurs, classification algorithms should be conver-gent to the up-to-date concept with hig h accuracy and speed. This method will be described in detail in section 3.

The rest of the paper is organized as follows. Section 2 gives the introduction on the related works. Section 3 shows our proposed algorithm in detail. In Section 4, we give experimental results by using a synthetic data set and real data sets which own the properties of skewed dat a streams. Section 5 summarizes our work and introduces the future work. Though there are many stream classification algorithms available, most of them assume that the streams have relatively balanced distribution of classes. In re-cent years, there have been several algorithms proposed for coping with skewed data stream classification. In these algorithms, data set usually are balanced by sampling, which is the most common method [7].

The basic sampling methods for dealing with the skewed data streams in-clude under-sampling, over-sampling and clustering-sampling [12]. Drummond et al. [1] concluded that under-sampling outperforms over-sampling through detailed experiments. However, under-sampling may lead to loss of useful in-formation. In order to use more majority class data to reduce information loss, clustering-sampling is preferred. This is because clustering can maintain more useful information, which may be thrown away by under-sampling.

Wang et al. [12] proposed an clustering-sampling based ensemble algorithm for classifying data streams with skewed di stribution. Empirical results show that clustering-sampling outperforms under-sampling. Particularly in the case of en-semble model, the proposed ensemble based algorithm gives better performance. However, this method can not handle the problem of concept-drifting.
Nguyen et al. [9] proposed a new method for online learning from imbalanced data streams. In this method, a small training set T used to initialize a clas-sification model is first collected. The cl assification model includes several base models. It is gradually updated by new training instance arriving one by one. If the class of the new training instance is positive, all the base models are updated. If the incoming instance is in negative class, these base models are updated with a certain probability. This approach also can not deal with the problem of concept-drifting.

Gao et al. [3] proposed a framework which employs both sampling and ensem-ble methods to classify skewed data streams. This paper analyzes the source of concept drifts, classification model se lection reason and why ensemble methods reduce classification errors. In the sampling phase, it randomly under-samples the negative instances from the most up-to-date chunk Q m , at the same time, it collects all the positive instances from data chunk Q 1 ,Q 2 ,  X  X  X  ,Q m and keeps them in the training set. Though the training set is balanced through sampling, all collected positive examples may not be consistent with the current concept and affect classification results if regarded as positive instances.
A variety of techniques have also been proposed in the literature for address-ing concept-drifting in data stream classification. Wang et al. [11] proposed a general framework for mining concept-drifting data streams using weighted en-semble classifiers. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approac h improves both the efficiency in learn-ing the model and the accuracy in performing classification.

Kolter et al. [13] proposed AddExp algorithm, which is similar to expert pre-diction algorithms, for discrete classes a nd continuous classes, respectively. This algorithm bounds its performance over changing concepts, not relative to the performance of any abstract expert, but relative to the actual performance of an online learner trained on each concept individually. During the online learning process, new experts can be added.

Although data stream classification algorithms coping with imbalanced distri-bution problem and concept-drifting prob lem respectively have been researched for several years, dealing with the two p roblems in a system framework is still challenging. In this paper, we propose the SeRt framework to address the non-stationary stream data with skewed distribution and concept-drifting.
Many available algorithms for classifying data streams measure their perfor-mance by considering overall classificat ion accuracy. However, such assessment metric is not suitable for assessing the performance of those algorithms which classify data streams with skewed distributions. This is because that, in such case, the overall accuracy is dominated by the majority class. Even if the classi-fication model performs well only for negative instances (eg. majority examples), the overall classification accuracy may also be high. For example, if 98% of the data is from the majority class and only 2% from the minority class, a classifi-cation model classifies every instance as majority, and thus the overall accuracy reaches up to 98% at the cost of no minority class instances correctly classi-fied. Those literatures [2] [10] [5] gives some measurements, like AUROC and G  X  Mean , to evaluate the performance of algorithms for classifying skewed data streams. Imbalanced learning problem and concept drifting phenomenon are quite pop-ular in recent years with a increased number of reports on the difficulties in many practical applications. When concept drifting occurs or class distribution changes, it definitely leads to a sudden drop of classification accuracy if we still use the classifier trained on old data points to classify new instances. 3.1 Basic Idea and Main Framework The basic idea of our method is that when there is no concept-drifting occurring in data streams with unbalanced distribution, we use the most up-to-date chunk to train a base classifier and use it to update the ensemble classifier. This may improve the accuracy of ensemble classifier, because the most up-to-date chunk contains information about the current target concepts. When concept drifting occurs, we use these data points which co nsist with the current target concepts to re-train those base classifiers in the previous ensemble classifier E j  X  1 .This way can make the algorithm converge to target concepts wi th high accuracy efficiently.

When concept-drifting occu rs, for each base classifier C i in the classifier en-semble E , we inject those data points which can represent the up-to-data target concepts into the training set T i with the injection probability p .Here T i is the training set corresponds to the base classifier C i . But the value of p has relationship with the overall accuracy and the accuracy of each base classifier. It is obvious that for greater p , more training instances are given in the model which may produce more error reduction. So, when p =1,wemayhavemax-imum reduction in prediction error for a single model. However, if the same set of instances are injected in all models, the correlation among them may in-crease, which reduces the accurate predic tion rate of the ensemble. So, we have to choose a value of p to balance the overall accura cy and the accuracy of each classification model [8]. Then we update the classification model C i . In stream applications, the training data are read in consecutive data chunks. Suppose the incoming data stream is partitioned into sequential chunks of equal size S 1 ,S 2 ,  X  X  X  ,S j ,  X  X  X  ,where S j is the most up-to-date data chunk. Each data chunk is considered as a conventional i mbalanced data set, which makes it easy to apply sampling methods to balance those skewed data chunks. In order to deal with skewed data streams, most of the available algorithms have either used oversampling or under-sampling approach for ensemble of classifiers. In our algorithm, we use clustering-sampling to balance the skewed distribution and selectively re-train approach to solve th e problem of concept-drifting. Figure 1 shows the framework of our proposed algorithm.
 3.2 How to Choose Data Point for Re-training We illustrate the idea of re-training through a simple hyperplane example in Figure 2. Figure 2( a ) shows two true models of the evolving hyperplane at dif-ferent time. A instance is positive (+) if it is above the hyperplane, otherwise it is negative (  X  ). We assume that hyperplane 1 is the previous concept and hyperplane 2 is the current concept, C 1 and C 2 stand for their true models re-spectively. In Figure 2( b )wedrawtheoptimalmodel E 1 ( ensemble classifier ) for hyperplane 1 , which are interpolated by straight lines. In Figure 2( c ), those data points in the shaded areas can be used to re-train those base classifiers in previous ensemble classifier E 1 .
 3.3 Sampling Skewed Data Streams The basic sampling methods often used are under-sampling and over-sampling. Although the study result shows that these two sampling methods can somehow address the problem of skewed distribution, they have several drawbacks[1].
In our method, we employ K  X  Means clustering algorithm, for selecting negative examples to represent majority class, to balance the skewed distribution. Algorithm 1 [12] gives clustering-sampling algorithm for sampling imbalanced data streams. In this algorithm, we set the number of clusters k to the number of positive instances in the current data chunk S j . After sampling, the negative examples in the current chunk are clustered into k clusters, and the centroid of each cluster is used as negative instance to represent majority class. Algorithm 1. Clustering-sampling algorithm for sampling imbalanced data streams 3.4 Selectively Re-train Algorithm Based on Clustering The pseudo code of our method for learning from imbalanced data streams is formulated in Algorithm 2. After receiving a data chunk S j , we firstly balance it using clustering-sampling method, thus we get the corresponding balanced training set T j . Then we train a base classifier C j , by an arbitrary successful bi-nary learner from the training set T j . Before updating the classification ensemble E  X  1 , we first check if concept-drifting has occurred by checking if E j  X  1 ( X i ) = y i holds. If the inequation holds, it means concept-drifting has occurred, or vice verse. If the concept-drift has occurred, we inject those data points which can represent the up-to-date target concepts into training set T i with a certain prob-ability, where T i corresponds to the base classifier C i ( C i  X  E j  X  1 ). The AUROC value is employed as the metric to evaluate the base classifier. Finally, we use the classification ensemble to predict the cla ss label for test instances with weighted majority voting.

In Algorithm2, Learn ( T i ) is an arbitrary learning algorithm; class ( X i )re-turns the class label of instance X i ; AUC ( C i ) returns the AUROC value of base classifier C i ; NormalizeWeight ( E j ,C i ) returns the normalized weight of base classifier C i , which satisfies C i  X  E j ; GetRandomNumber () returns a ran-dom number in [0 , 1]; numBaseClassifier is the number of the base classifier in Ensemble-classifier; and balanceRatio is a parameter that is used to judge whether the training set T h needed to be balanced again. In our experiment, we evaluate our proposed algorithm which can be abbreviated to SeRt and compare it with the approach proposed in [3]which is denoted as Algorithm 2. Selectively Re-train algorithm Based on Clustering towards Concept-drifting Data streams with Skewed Distribution SE in the simulation results on both synthetic and real data sets. To simulate data streams, we partitioned these data sets into several chunks. 4.1 Data Sets Synthetic Data Set. We create synthetic data sets with drifting concepts based on a rotating hyperplane by reprogramming the software MOA Task Launcher . The rotating hyperplane is widely used for experiments [6,11, ?? ].Ahyperplane in a d-dimensional space is the set of data instance which is denoted by the equation:
Here, x i is the i  X  th coordinate of instance x . We label instances with i =1 a i x i a 0 as positive instances, and instances with negative instances. Weights a i (1 &lt;i&lt;d ) in (1) are initialized randomly in the range of [0 , 1]. Hyperplanes have been used to simulate time-changing concepts because the orientation and the position of the hyperplane can be changed in a smooth manner by changing the magnitude of the weights[6]. We choose the value of a 0 so that the hyperplane cuts the multi-dimensional space into two parts of the different volume, eg. a 0 = r d i =1 a i where r is the skewness ratio. when r = 1 2 , the hyperplane cuts the multidimensional space into two parts of the same volume. In our experiment, r = 1 2 .

In our study, we simulate the phenomenon of concept drifting through a series of parameters. Parameter n ( n  X  N ) specifies the number of examples in each batch, and parameter k ( k  X  N ) specifies the number of dimensions whose weights are involved in concept drifting. Parameter t ( t  X  R ) indicates the magnitude of the changing of weights a 1 ,a 2 ,  X  X  X  ,a k and s i  X  X  X  1 , 1 } (1  X  i  X  k ) indicates the direction of change for each weight a i .Weight a i (1  X  i  X  k ) is adjusted by s i  X  t after each instance is generated.
 Real Data Set. We use the data set Optical Recogniti on of Handwritten Digits from the University of California, Irvine X  X  Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets.html). Although this data set doesn X  X  correspond directly to skewed data mining problem, we can convert it into skewed distribution problem by taking one small class as the minority class and the re-maining records as the majority class. So, from the original data sets, we can get 10 skewed data sets and average the results over these data sets. 4.2 Evaluation Metrics Traditionally, overall accuracy is the most commonly used measure for evaluating the performance of classifier. However, for classification with skewed distribution, overall accuracy is no longer proper since t he minority class has very little impact on it as compared to the majority class. This measurement is meaningless to some applications where the learning target is to identity the rare instances. For any classifier, there is always a trade-off between true positive rate (TPR) and true negative rate (TNR). For this end, some evaluation metrics associated with confusion matrix are used to validat e the effectiveness of those algorithms dealing with class imbalance problem. Table 2 illustrates a confusion matrix of a two-class problem.

TP and TN denote the number of positive and negative instances that are classified correctly , while FP and FN d enote the number of examples which are misclassified respectively.  X  Overall Accuracy (OA):  X  X PR( ACC + ):  X  X NR( ACC  X  ):  X  X -mean:  X  Mean Squared Error (MSE): Where T is the set of testing examples, f ( x i ) is the output of the Ensemble-Classifier, which is the estimated posterior probability of testing instance x i ; p (+ | x i ) is the true posterior probability of x i . 4.3 Experiment Configuration and Results Experiment Configuration. The synthetic data set (Moving Hyperplane Data Set) is used to validate the effectiveness and superiority of our proposed method. In our experiments, the dimensionality of the data stream is set to be 50. Only two dimensionalities are changing with time and the magnitude of the change for every example is set to be 0 . 1. The percentage of probability that the direction of change i s reversed is set to be 10%. At the same time, we set the balanceRatio to be 0 . 85 and the method applied to build base classifier is Naive Bayes .

When concept-drifting occu rs, for each base classifier C i in the classifier en-semble E , we inject those data points which can represent the up-to-date target concepts into the training set T i ,where T i corresponds to the base classifier C , with the probability p . But the value of p has relationship with the overall accuracy and the accuracy of each base classifier. If the same set of instances are injected in all the models, then the correlation among them may increase, resulting in reduced prediction accura cy of the ensemble. So, we have to choose a value of p to balance the overall accuracy and the accuracy of each model [8]. In order to find the relationship between p and the performance of our ap-proach, the number of chunks in the data stream is set to be 101, each of which carries 1000 examples and the last chunk contains the testing instances. In the data stream, there are 98064 negative instances and 2936 positive instances. 956 negative examples and 44 positive examples are used for testing.
 Figure 3 shows the relationship between p and TPR , TNR , G  X  mean and MSE ,where NS denotes selectively re-train approach based on No-Sampling, US denotes selectively re-train approach based on Under-Sampling and CS de-notes selectively re-train approac h based on Clustering-Sampling ( eg. our pro-posed method SeRt ). Through experiments, we get that, when p is between 0 . 5 and 0 . 75, TPR , TNR , G  X  mean and MSE are near optimal. And the point where p =0 . 75 is the turning point, so we set p =0 . 75.
 Experiment Results. After experiment configuration, we can conduct exper-iments based on it. The following are the experiment results.  X  The performance comparison wi th different sampling methods In order to solve the problem of imbalanced distribution of data stream, we should balance it by using sampling method. We compare the performance of different sampling methods, as shows Figure 4, where CS represents Clustering-sampling, US represents Under-sampling, and NS represents that we do not use any sampling method. From this figure, it is obvious that CS outperforms NS and US in TPR and G  X  Mean . Though CS is slightly lower than other two methods in the case of TNR , the results are very competitive, because the cost of misclassifying a positive instance is much larger than the cost of misclassifying a negative instance .
  X  The comparison between our method and the methode which was proposed Except for synthetic data set, we also employ the real data set Optical Recognition of Handwritten Digits to compare our algorithm with the approach proposed in [3] which is denoted as SE in the results. Figure 5 gives the average result of these 10 skewed data streams. From Figure 5, we find that SeRt can get higher TPR and G  X  Mean with the cost of TNR and MSE than SE . But it is worth, be-cause in the case of skewed distribution, minority class is the most significant one, and the cost of misclassifying of a positive example is more huge than the cost of misclassifying a negative example.  X  The comparison between our method and the methode which was proposed in We also use synthetic data set with concept-drifting to validate the performance of SeRt and SE , as follows Figure 6. The data set contains 101000 instances in total, 99817 negative examples and 1183 positive examples respectively. We partition the data set into 34 chunks to simulate data stream. The last chunk for testing contains 2000 examples, and the rest of each chunk for training contains 3000 examples. There are 1929 majority instances and 71 minority instances in the testing set. From Figure 6, we know that when the data stream is changing with time continuously, it is obvious that our proposed algorithm SeRt outper-forms SE .
 In this paper, we propose a new algorithm for mining data streams with skewed distribution, which can also tackle the problem of concept drifting.
The proposed algorithm could only deal with binary classification tasks. How-ever, many real-life applications, such as network intrusion detection, are charac-terized as multi-class classification tasks. In the future, we will study the problem of classifying multi-class with skewed data streams. Acknowledgement. This work is supported by National Science Foundation of China under its General Pro jects funding # 61170232 and 61100218, Fun-damental Research Funds for the Central Universities # 2012JBZ017 and # 2011JBM206, Research Initiative Grant of Sun Yat-Sen University (Project 985), State Key Laboratory of Rail Traffic Control and Safety Research Grant # RCS2012ZT011. The corresponding author is Hong Shen.

