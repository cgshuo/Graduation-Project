 Abstract This paper describes the generation of temporally anchored infobox attribute data from the Wikipedia history of revisions. By mining (attribute, value) pairs from the revision history of the English Wikipedia we are able to collect a comprehensive knowledge base that contains data on how attributes change over time. When dealing with the Wikipedia edit history, vandalic and erroneous edits are a concern for data quality. We present a study of vandalism identification in Wikipedia edits that uses only features from the infoboxes, and show that we can obtain, on this dataset, an accuracy comparable to a state-of-the-art vandalism identification method that is based on the whole article. Finally, we discuss different characteristics of the extracted dataset, which we make available for further study. Keywords Wikipedia Infobox Attributes Temporal data 1 Introduction articles, with the goal of being a comprehensive and up-to-date reference work. 1 Aside from its value as a general-purpose encycl opedia, Wikipedia has also become one of the most widely used resources to acquire, either automatica lly or semi-automatically, knowledge bases of structured data. Much res earch has been devoted to automatically building lexical resources, taxonomies, parall el corpora and structured knowledge from it.
Many Wikipedia entries contain so-called infoboxes : tabular information encoded as (attribute, value) pairs that summarize key information about a given article. It has been reported that roughly 30 % of the articles in the English Wikipedia contain an infobox (Lange et al. 2010 ). Parsing infoboxes has yielded useful knowledge bases such as DBPedia (Auer and Lehmann 2007 ; Auer et al. 2007 ) and Freebase (Bollacker et al. 2008 ). Data extracted automatically from infoboxes has been applied to various NLP tasks such as document summarization (Ye et al. 2009 ; Xu et al. 2010 ) and relation extraction (Wu and Weld 2007 , 2010 ; Hoffmann et al. 2010 ). Infoboxes have been successfully used for distant supervision , i.e. using data obtained from infoboxes to semi-automatically annotate a dataset that can be used in training a supervised machine learning (ML) algorithm (Mintz et al. 2009 ; Hoffmann et al. 2010 ).
Every editable page in Wikipedia has an associated page history, where users can view past versions and, if necessary, revert the current state of the entry to one of them. By now Wikipedia has accumulated a wealth of historical information about the last decade, encoded in its revision history. To the best of our knowledge, existing work using infoboxes to extract lexical and relational knowledge bases only uses snapshot versions of Wikipedia, containing a single (frozen) version for each article. In contrast, historic values of attributes in infoboxes can also be exploited; for example, for distant supervision in temporally-aware information extraction systems, with the goal of extracting values for attributes that change over time (Zhang et al. 2008 ; Wang et al. 2010 ). Probably, one of the reasons why the revision history has not been used before is the large size and the format of the dataset; these features render its processing a very difficult task.

In this paper, we describe the collection of a large, structured dataset of temporally anchored attributes and values, obtained from the revision history of Wikipedia, including the different steps involved in its construction, and analyze several properties of the obtained data. We call the generated dataset WHAD (Wikipedia historical attributes data).

We are releasing this dataset through Wikimedia Deutschland, 2 which proposed to distribute it from its Wikimedia Toolserver download page, 3 under the Creative Commons license that covers Wikipedia.
When the edit history is used as data source, erroneous and vandalic edits are a concern for data quality. We present a study of vandalism identification in Wikipedia edits that uses only features from the infoboxes, and show that, for the subset of Wikipedia articles that contain infoboxes, we attain results comparable to a state-of-the-art vandalism identification method that is based on the whole article.
We believe that the released corpus will be particularly useful for training distant supervision classifiers to extract temporally-anchored attribute values. If we know that a given person X was president of a country during a period of time Z (as indicated by the updates to the Wikipedia infoboxes) we should be able to identify sentences containing the person, the country and a time inside that interval, from which a classifier can be trained. Working along this line is part of our immediate future work plans.

The paper is structured as follows: first, Sect. 2 discusses related work. Section 3 presents an overview of our approach, outlining our design and implementation. The components of our system are described in detail: in Sect. 3.1, we describe how we gathered the full edit history log of Wikipedia; in Sect. 3.2 we describe how we processed this information to extract updates to relational data; and Sect. 3.3 discusses the problem of identifying vandalic edits to demonstrate how vandalism can be filtered out using the available data.

Section 4 analyzes and discusses the dataset. Details on the released dataset X  X  format and structure are provided in Sect. 4.1 . Section 4.2 explores the generation of temporally anchored relational data from the infobox attribute updates. It shows, with two experiments, evidence that WHAD data can be used as proxy for real world temporal data, with a coverage and an accuracy that has increased over time. Section 4.3 describes the results of the evaluation of vandalism detection.
Finally, our conclusions and future lines of research are reported in Sect. 5. 2 Related work Information Extraction (IE) is the task of acquiring structured information from unrestricted text or semi-structured sources such as Wikipedia. Data from infoboxes, lists, categories, and disambiguation pages has proven useful to gather semantic information (Suchanek et al. 2007 ; Auer and Lehmann 2007 ; Nguyen et al. 2007 ; Bollacker et al. 2008 ), and for many other tasks: text classification (Gabrilovich and Markovitch 2007 ); semantic similarity (Gabrilovich and Markovitch 2007 ); document clustering (Hu et al. 2009 ); ontology generation (Suchanek et al. 2007 ; Ponzetto and Strube 2007 ); entity linking (Milne and Witten 2008 ); or question answering (Ahn et al. 2004 ).

This paper is concerned in particular with Wikipedia infoboxes, whose semi-structured layout hints at their usefulness for knowledge extraction. Our approach is similar in spirit to Auer and Lehman X  X  DBPedia (Auer et al. 2007 ; Auer and Lehmann 2007 ), who proposed parsing infoboxes as a way of automatically obtaining knowledge.
 The first contribution of this paper is to extend this approach by using the full Wikipedia edit history, and not just a particular snapshot. Wikipedia infoboxes are commonly designed to display the current value of attributes. We hypothesize that values that were valid in the past lie hidden among the older revisions. Not only the surface of Wikipedia, but also the underlying layers of historical strata can be mined for knowledge.

Recent research has started tapping into the Wikipedia edit history for a variety of tasks. We can establish two categories amongst them: (a) Analyses of Wikipedia itself, the quality of its articles and the collaborative (b) Research exploiting Wikipedia content as a resource or corpus for machine
To the best of our knowledge, the edit history has not been used yet to extract relational knowledge. Recently, an open software libra ry that implements delta-compression of Wikipedia X  X  edit history, and access through a Java API might ease the burden of processing the data, and facilitate future research (Ferschke et al. 2011 ). present an in-depth survey of methods and applications that exploit Wikipedia X  X  dynamic and collaborative nature, materialized both in its edit history and its discussion pages.
The information we extract is not only interesting by itself, but also because it can be applied to other tasks. Distant supervision consists in semi-automatically annotating a dataset that can be used as training set for a supervised ML algorithm (Banko et al. 2007 ; Mintz et al. 2009 ; Hoffmann et al. 2010 ). Wu and Weld X  X  system, K YLIN (Wu and Weld 2007 ), uses attributes extracted from Wikipedia infoboxes to bootstrap a distantly supervised learning system, in order to extract new attribute pairs. In this way, two drawbacks of supervised machine learning methods are tackled: the need for labour intensive labeling of training data, and for specifying the full set of semantic relations to be extracted as an input.

Our system extracts attribute updates that are temporally anchored, which would allow expanding previous approaches with this additional temporal information. The extraction of temporal information is an important open challenge for Information Extraction. Significant research, particularly around the TempEval community (Verhagen et al. 2009 ), has focused on the classification of the temporal links between events and temporal expressions, exploiting supervised machine learning techniques enabled by the release of the TimeBank temporally annotated corpus (Boguraev et al. 2007 ). The 2011 edition of the Knowledge Base Population track 4 at the Text Analysis Conference-2011 5 included the acquisition of temporally anchored attribute values. Recent research has extracted temporal facts from infoboxes, categories and lists, to be integrated with a pre-existing ontology (Wang et al. 2010 ; Zhang et al. 2008 ). These works do not use the edit history, so the facts they can extract are those present in a particular snapshot.

Working with the full edit history of Wikipedia comes not only at the cost of processing many edits to articles, but also of dealing with many erroneous edits .Some of these errors are the result of vandalism . Other, non-vandalic, errors within Wikipedia content are out of the scope of this work. An open line of research focuses on those quality flaws (Anderka and Stein 2012 ). The pervasive nature of vandalism in Wikipedia compromises its value as a resource for ML and NLP tasks, particularly when using the edit history rather than a single snapshot. Past research trying to leverage Wikipedia edit history often overlooks this issue, or leaves it for future work (Yamangil and Nelken 2008 ; Yatskar et al. 2010 ). Manually annotating a corpus might be feasible for small datasets (Zanzotto and Pennacchiotti 2010 ), but not for large-scale knowledge acquisition, and therefore it is unsuitable for our purposes.
We address the issue of vandalism in this paper, demonstrating how the information contained in the dataset can be exploited to filter out vandalic edits without using data extrinsic to the dataset. A contribution of this paper is to analyze how vandalic edits to infoboxes can be detected. As we have described above, infobox attributes are fed into other systems, so the scenario of having to decide whether a modification of an infobox is vandalic, without relying on full-page features, is realistic. Previous detection systems deal with full-page edits, while we are interested in edits to infoboxes.
Although vandalism in Wikipedia has been observed from its inception, relevant research on this topic is quite recent. The first systems to address the issue were automated scripts, or bots , based on heuristic rules, with an eye on high precision but very poor recall; see Geiger and Ribes ( 2010 ) for a historical analysis of these bots. Much research has been encouraged by the release of the manually annotated PAN-WVC-10 English Wikipedia vandalism corpus (Potthast 2010 ), extended later to German and Spanish (Potthast and Holfeld 2011 ), and the first two editions of a vandalism detection competition, PAN 2010 (Potthast et al. 2010 ), and PAN 2011 (Potthast and Holfeld 2011 ). 6 The proposed systems can be grouped by the kind of features that they focus on: the article text content and its revision history (Potthast et al. 2008 ; Smets et al. 2008 ; Chin et al. 2010 ). A related idea is to use the compression rate of edits (Itakura and Clarke 2009 ), although such methods tend to overlook small-sized vandalic edits. The best participant of the PAN 2010 competition (Mola-Velasco 2010 ), used textual and linguistic features . Reputation , particularly of users, was used by Adler et al. ( 2010 ) and West et al. ( 2010 ). This last paper also exploited metadata features. Adler et al. ( 2011 ) used the lessons learned in the previous work to implement a classifier by merging features from previous systems. Their work also compares the relative merit of features of different nature. We will compare the performance of our own vandalism detector to that of their revised, state-of-the art system. In the second edition of the competition, the best participant system (West and Lee 2011 ) demonstrated that a significant improvement is possible using features that exploit a posteriori knowledge, that is, taking into account later revisions to the one to be classified. 3 System overview We hypothesize that Wikipedia is a useful resource for temporally-anchored knowledge. Relational facts that were valid in the past but have been overridden with more up-to-date information are hidden in the edit history. Our aim is to uncover this knowledge, tied to the time when it was valid. The reliability of this resource is affected by the presence of erroneous and vandalic edits in the edit history. In this paper, we demonstrate how information intrinsic to the dataset can be exploited in order to filter out vandalic edits without relying on extrinsic information.
The research question central to this work is: how can we generate historical data from the Wikipedia edit history in a robust way, mitigating the presence of vandalism? An outline of our approach is graphically depicted in Fig. 1 ; methodologically, it involves the following main steps: (A) Data gathering: obtain the revision history from Wikipedia. We have (B) Harvesting infobox attribute updates. The edit history has to be processed to (C) Detecting vandalic edits. Vandalic and erroneous data are a burden for data (D) Generating temporal anchoring from selected edits. The relational facts we
The implementation of each of these steps is detailed in the following subsections, where we also discuss technical and scientific challenges that we have encountered during our development, experimentation and evaluation. 3.1 Data gathering The Wikimedia foundation makes the Wikiped ia edit history available for download at http://download.wikipedia.org/ . 8 For this research, we have focused on the English edition of Wikipedia, although the techniques we employ could be readily adapted to other languages. The downloaded file 9 contains all the articles that exist in the on-line version, together with the full sequence of edits for t hose articles. For each article, and for each revision, these data includes not only the full text, but also the discussion page, infoboxes and category annotations. Note that deleted articles are excluded from this archive.
The revision history is generated periodically, although on a somewhat irregular basis. Throughout the development and experimental phases of this work, we used a dump from January 30, 2010 of the English Wikipedia containing the full edit history of the articles. This is the compressed representation of an extremely large xml file, so storing and processing the file is not trivial. The .bz2 file that we downloaded is 280.3 GB in size. It was necessary to use a program that, as it decompresses the dump, distributes the different Wikipedia entries in many smaller files to be stored in a distributed file system in order to make it usable. We processed only content articles, and rejected all disambiguation, redirect and discussion articles. All further processing of these data was performed using an implementation of the MapReduce paradigm (Dean and Ghemawat 2008 ) inside a computing cluster. The experimental results reported in Sect. 4.2 are performed on this dataset, which we will refer to as W HAD 2010.

The availability of up-to-date revision data would be limited if the processing depended on the schedule of Wikipedia database dumps. To keep our dataset up-to-date, it is necessary to be able to crawl current revisions from Wikipedia. We have worked in this direction, and have now the infrastructure capable of crawling newer revisions in place, incrementally updating the obtained dataset using the MediaWiki API. The dataset described here, which constitutes the first release, is the most up-to-date at the time of writing, dated March 23, 2012. We will refer to this dataset as W
HAD 2012, and we will describe it in detail in Sect. 4.1 We plan to produce data refreshes periodically. 3.2 Infobox update extraction In order to extract infobox updates, we follow a similar approach to Auer and Lehmann ( 2007 ), which is outlined as follows: 1. Parse the MediaWiki mark-up to identify infoboxes in all revisions for each 2. Get the infobox type and all the (attribute name, attribute values) pairs contained 3. Some of the mark-up, such as hyperlinks to other entities in Wikipedia (e.g. if
The differences with respect to previous approaches to infobox parsing are:  X  For each entry and revision-timestamp we store an infobox instance, containing where we extract from the revision the name of the attribute, the value that was edited out in that revision, and the value standing after the revision. 13 The relational fact is in this way augmented with an anchoring timestamp; we will study in Sect. 4.2 how this temporal information can be exploited. For newly added attributes, value prev is empty, and for attribute names that are removed from an infobox,  X  Most of the changes to Wikipedia are edited by other users who have established 3.3 Vandalism detection Working with the edit history as a data source, erroneous and vandalic edits are a concern for data quality. Simple heuristic filtering can be used to weed out the most obviously vandalic content, 14 but some kinds of vandalism will still not be detected. In this section, we show that it is possible to filter out vandalic edits to infoboxes in Wikipedia, and therefore to maintain a reasonable quality in the data.

Since the dataset described in the previous section includes every single modification to an infobox performed by a single user, it contains malicious, vandalic edits. Following previous research, we adopt Wikimedia X  X  definition of a vandalic edit ( http://en.wikipedia.org/wiki/Wikipedia:Vandalism ): any addition, removal, or change of content in a deliberate attempt to compromise the integrity of Wikipedia . The issue of non malicious factual errors is not the focus of our investigation; in Sect. 5, we point out possible lines of research to address it.

As an example, Fig. 2 shows the number of edits for the attribute president in the infobox of the entry France . As in most countries, the president in France is elected every few years, and therefore this should be a fairly stable attribute. On the other hand, the figure shows that between 2006 and 2010 there have been 116 edits of the name of the French president. Some of them are accessory but legitimate changes, such as adding or modifying the name of the political party to which the president is affiliated. Many of the spikes in the number of changes per month, though, were due to users adding, apparently deliberately, incorrect values. Fortunately, for popular Wikipedia entries, such as this one, vandal edits are usually reverted quickly. One particular source of extra revisions for this attribute comes from the interval of time between Sarkozy X  X  election and office assumption (around May 2007), during which contributors did not agree whether the value should be already updated or not. The next larger spike (on July 2009) comes from a single (vandalic) user insisting over and over again that the president of France is Philippe Petain. All of these edits were reverted in a matter of minutes. 3.3.1 Procedure Our aim is to show that it is possible to detect vandalic revisions using only the infobox update information that we collected. We think that this is a reasonable scenario, because of the following reasons:  X  The infobox-only corpus is much easier to handle than the full revision history,  X  Our focus is on relation extraction, and in particular on using infoboxes for this
We describe in the following the features included in our model. Some of them are similar to those used in standard full-article vandalism detection work (Mola-Velasco 2010 ; Potthast et al., 2010 ), but most are specific to our problem (updated infoboxes). We did not perform any manual feature engineering aside from putting together all the features that intuitively seemed useful for this task and were not too correlated with each other.  X  Lexical features : whether the revision is adding new sex words (e.g. sex or porn )  X  Whether the revision has been tagged as  X  X  X inor X  X , or the comment indicates that  X  Whether the contributor is identified with a user ID or an IP address.  X  The number of infobox attributes added in this revision.  X  The number of infobox attributes removed in this revision.  X  The number of infobox attributes whose value changed in this revision.  X  Statistics about how long it took for the changed attributes from this revision to  X  The number of attribute values added, deleted or changed in this revision that  X  Whether a whole infobox is being created or removed in this revision.
For classification we used an AdaBoost classifier: during our development work the choice of learning algorithm did not seem to affect the results on our development set much, and AdaBoost gives reasonably good results according to Wu et al. ( 2010 ). During early development we observed that, because of the highly imbalanced dataset, it was often the case that most ML algorithms simply learned to tag everything as non-vandalic, so we used a cost matrix penalizing false negatives ten times more than false positives. No other parameters were tuned on the development set. 3.3.2 Development and test sets PAN-WVC-10 (Potthast 2010 ), which was developed by means of crowdsourcing , is probably the most comprehensive English Wikipedia vandalism corpus publicly available. The dataset contains 32,439 manually annotated Wikipedia edits, out of which 2,394 are classified as vandalic (around 7.5 %).

The sampling procedure performed by Potthast to select the revisions that were to be annotated weighted each article with the number of times that it was edited, so as to give more importance to documents that attracted more attention from Wikipedia contributors. To use it as test set, we adapted this gold standard by removing all revisions that were not modifying any infobox (because these are not present in our dataset). After removing those, the dataset obtained has 2,839 revisions, out of which 128 are labelled as vandalic (4.5 %), a ratio slightly lower than that of the full set.
For development purposes, we created a new development set in the same spirit of PAN-WVC-10. To do this, we randomly sampled 1,000 Wikipedia edits that modify at least one attribute in an infobox. To annotate the edits, we use a proprietary crowdsourcing approach similar to, but independent from services such as Crowdflower 15 or Amazon Mechanical Turk. 16 Non-expert annotators were assigned the task of deciding whether a particular revision in our dataset was vandalic. They were provided with the following information:  X  The entry that was being changed, together with a pointer to the current version  X  The Wikipedia diff page 17 between the revision that we would like annotated,  X  The date of the revision.  X  The infobox attribute that changed, the previous value and the new value for that
The task of the raters was to annotate the revision with one of the following options: (a) It is a regular, legitimate revision. (b) It is a vandalic revision. (c) I don X  X  know.

An example of the annotation form raters were provided is shown in  X  X  Appendix  X  X  .

Three annotations were collected for each item, and each rater was set a limit of at most 30 ratings, to avoid bad raters having a large effect on the whole annotation. A total of 233 annotators participated in the task. The revisions without majority agreement or where  X  X  X on X  X  know X  X  was the majority label were discarded. The final development dataset contains 74 items marked as vandalic edits (9.6 %), and 770 items marked as legitimate edits. Observe that the percentage of vandalism present here is somewhat larger than in the test set.

In our crowdsourcing evaluation setting, three ratings are assigned to each evaluation item, and the items are annotated by different raters, from the total of 233. A suitable statistical measure of inter-annotator agreement under these conditions is Fleiss X  kappa (Fleiss et al. 2004 ). While Cohen X  X  kappa is a good estimate of agreement between 2 raters, Fleiss X  X  kappa is useful if there are more than 2 raters and/or if the ratings have been issued by different raters. This coefficient quantifies the extent to which the observed amount of agreement among raters exceeds what would be expected if ratings were completely random: where P o is the proportion of pair-wise agreements and P e is the expected proportion of such agreements under a random assignment. Considering that our task is detecting vandalic edits, we chose to aggregate the responses of the categories  X  X  X  don X  X  know X  X  and  X  X  X t is a regular, legitimate revision X  X . In this setting the Fleiss X  Kappa value is: j = 0.30436, which can be considered a  X  X  X air agreement X  X .

Furthermore, there was a majority agreement (that is, at least 2 out of the 3 annotators did agree) in 937 of the examples (93.7 %). In 488 examples (48.8 %), 2 out of three annotators agreed, while in 449 (44.9 %) examples, all three annotators agreed.

Inspecting the comments in the 63 examples where a majority agreement was not reached, we observe that one of the major causes for disagreement (25 % of the cases) was that the raters tried to verify the factual correctness of a modified value and reached different conclusions. The other 25 % of the disagreements was due to the limitations of our extractions and annotation interface, as it was sometimes difficult for the annotators to decide whether an edit is vandalic when the infobox type is changed (maybe correctly), when it is removed or recovered altogether, or when the edit introduced a syntactic error in the MediaWiki code.

Also, when an image is modified, the Wikipedia diff page that is shown to the annotators provides only the name of the image, 18 so their decision based on that was difficult. This accounts for 15 % of the disagreements.

Roughly 10 % of the disagreement was caused by rater errors, which could be detected from comparing their decision to the comments they provided. The rest of the disagreements are caused by a variety of reasons, such as the edit changing an already wrong value, disagreement over the relevance of a piece of data, or over possible spelling mistakes.

We consider this level of agreement reasonable given the nature of the task, and the observed results are consistent with those reported in Potthast ( 2010 ). The corpus described in that work, Webis-WVC-07, consists of 940 human-annotated edits of which 301 are vandalism(Potthast et al. 2008 ). Webis-WVC-07 was annotated using Amazon Mechanical Turk; with a three-raters per example setting, and the author reports three-out-of-three agreement in 58 % of the cases, and two-out-of-three in 42 %. Note that, as opposed to our setting, here the raters X  judgements are binary. 3.4 Temporally anchored relational data One of the main potential uses of revisions of Wikipedia infoboxes is to recover historical values of infobox attributes: it is possible to anchor facts to the time when they were introduced in Wikipedia. The attribute updates in W HAD are temporally anchored, and can be represented as tuples:
For instance, in order to know the GDP of the United-States in 2005, one would ideally look at the value of a revision of the attribute late in 2005 or early in 2006. The W HAD dataset can be used to enhance existing knowledge bases, such as DBPedia or Freebase, with historical values. Another possible application would be to date documents according to the age of information mentioned in their content.
In Sect. 4.2, we empirically explore whether such an approach can produce accurate and timely relational data 4 Evaluation, analysis and discussion This section describes the results of an analysis of the data produced, and discusses its usefulness. We provide a more detailed description of the released data and a study of the timeliness of manual updates to Wikipedia that affect attribute values. Finally, we report the experimental results of the automatic vandalism detection experiment. 4.1 Dataset analysis As a contribution of this work, we are releasing the full, up-to-date, dataset of Wikipedia infobox attribute updates, W HAD 2012, for further research. In this section we start by providing general descriptive statistics about the dataset and then concisely describe the format and structure of the data.

Our aim is to distribute the most recent dataset possible; as we are able to process recent versions obtained by crawling regularly Wikipedia, we have augmented the data from the 2010 dump on which we performed our experimental analyses (W HAD 2010) with more recent updates. This release dataset, updated to March 23, 2012, W HAD 2012, is the one described in this section. 4.1.1 Descriptive statistics From our store of Wikipedia X  X  full edit history updated to March 23, 2012, we filter out non-content, disambiguation, redirect and discussion articles. The total number of revisions we parse is 291,601,701. From them, we extract a total of 510,102,778 individual infobox attribute updates (IAU), that correspond to 2,040,181 articles.

Table 1 collects some relevant statistics on the extracted data. For information purposes, the table has another column where a very conservative sanity-check filtering of the dataset has been applied: we remove every revision that introduced a string value of more than 10,000 characters (being this most certainly a mistake or a vandalic edit), and those edits that were reverted within a minute from being saved.
At the dataset level, we can see that in the period 2003 X 2012, 510,889,795 infobox attribute updates (IAU) have been made to 2,040,181 different entries by over 7 million users (identified by unique username if available, or by IP address otherwise). Roughly half of the wikipedia entries have an infobox. 19
Infoboxes are classified according to the type of information they contain, indicating, for example, that they refer to a company or to a country. In our dataset there is a total of 24,028 different infobox type names. 20 The ones that appear in most entries are settlement , album , french commune , 21 film and musical artist .
At the attribute level, we report statistics on the detected typed values within attribute values. Many attribute values contain a mention of a typed value, such as a location or a date. Also, some attributes contain several typed-value mentions, of the same or different types.
 We used a combination of gazetteer and regular expression-based Named Entity recognizers with manual heuristics, developed in-house, in order to normalize the values and characterize their types. Taking as input the 38,979,871 attribute updates after simple clean-up, we computed the number of updated values that contain one among a set of potentially interesting value types: numbers, hyperlinks, geographical locations, dates, measurements, currency values, and also time expressions and temporal intervals. Table 2 reports these statistics of the detected types. Note that the list of types is not necessarily comprehensive and that a type has not been detected for every attribute value. Also, more than one value can be identified and counted for a single attribute update. For instance, the value might be a list, and we detect an entity for each of its elements: the  X  X  X eveloper X  X  field of the article Unix has value:  X  X  X en Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, and Joe Ossanna at Bell Labs X  X ; we detect and count six hyperlinks in such a value. 4.1.2 Data generated, structure and format The generation of the full dataset is indeed costly, as it involves the following steps:  X  Either download a Wikipedia database dump to obtain the full edit history up to  X  Transform this data into a usable database format, decompressing it and storing  X  Extract and collect the updates to Wikipedia infoboxes, which involves the
The size of the dataset which will be released to the community is 5.5 GB; compared to the original edit-history dump released by Wikimedia, this new corpus is much easier to handle for everyone thus facilitating research in this area. The format of the data is JSON. Each text line corresponds to one Wikipedia entry. It has as fields:  X  article_title: the name of the entry  X  attribute: a list of attribute updates, each of which has: As an illustration, Listing 1 shows a sample from the actual dataset.

It has to be noted that values of attributes are often special-purpose templates themselves. For instance, an excerpt of Alan Turing X  X  article can be seen in Listing 2. As shown, the birth and death dates are encoded using a template, with some fields denoting the year, month and day of the date. Before processing the values of the attributes, specialized parsing of these value templates has to be performed, a procedure we followed prior to the experiments described in this paper. In our dataset release, on the other hand, we aim at providing the maximum coverage of attributes, and letting the users decide which parts are important to them. The produced dataset includes the verbatim string values for all attribute values, so clients of this resource can write the simple analysis tools needed to interpret the information they are interested in. 4.2 Accuracy and timeliness of temporally anchored relational data As described above, the attribute updates in W HAD are a source of temporally anchored relational information, that can be represented as tuples:
In this section, we show to what extent this approach can produce accurate, timely relational data, and to demonstrate the kinds of analyses that the W HAD dataset enables. We focus on the W HAD 2010 compilation of Wikipedia updates we obtained by processing the dump from January 30, 2010 of the English Wikipedia, as it is described in Sect. 3.1 .

We address the following practical research questions: To what extent is the information contained in past revisions to Wikipedia infoboxes useful for knowledge extraction purposes? Is it reliable data? If the delay between an event occurring and the Wikipedia infobox being updated is short enough, it could indeed be used for event detection, so how often was the data updated? Our method of investigation is to analyze empirically these two different aspects of the quality of Wikipedia historical information: accuracy and delay.

Previous work has dealt with quality assessment of Wikipedia entries, but most has focused on a particular time, or snapshot (Stvilia et al. 2005 ; Arazy and Nov 2010 ). 22 In contrast, the analysis proposed in this paper focuses on time. The problem addressed here is to assess the quality of information over a time period .
We use the infobox types as the different categories in order to explore which attributes are more common in each type of infobox, and which ones change most often. The infobox type company is an interesting example because it has a number of attributes whose value changes over time, such as revenue or number of employees, allowing us to investigate the availability of data that was correct in the past but was later substituted by more recent values. Table 3 shows the analysis for the infobox type company . The left part of the table contains the attributes that appear most often in infoboxes of this type, and the right part contains the average number of times that the value of an attribute changed in any entry. As can be seen, the highest ranking attributes in number of revisions refer to transient properties of companies. 4.2.1 Accuracy analysis case study We define transient attributes as those whose value changes in real life, either periodically (e.g. the GDP of a country, which is generally computed on a quarterly or yearly basis) or irregularly (e.g. the number of Grand Slam victories for a tennis player). We present here a case study on a periodic attribute, the population estimate for countries, whose real value is updated every year for most countries. We used the World Bank website to acquire actual population sizes for a large number of countries from 2005 to 2009. This website provides information for all of these years for 91 countries. By evaluating how accurately Wikipedia reflects the population of these countries we can get a measure of the reliability of the information.

Country population estimates are often updated when new estimates are published by sources such as the World Bank, the International Monetary Fund, or an official authority responsible for carrying out national census. Furthermore, these sources can typically issue several population estimates for a given year as more and more data becomes available. Therefore, to estimate the population size for a given year, we define our Wikipedia-based proxy estimation as the value of the attribute at the time of the last revision in that year. Our proxy discounts revisions marked as vandalism and revisions whose values deviate by more than 1 standard deviation from the mean to avoid outliers. For example, an incorrect parse might be caused by a user using dots instead of commas to separate three-digit groups. These incorrect readings are easily discarded.

We can then compare the estimated values with the actual values for the 91 available countries. There were 400 entries in Wikipedia with a country infobox, which is more than the number of countries listed by the United Nations. The difference is due to the fact that the infobox has also been applied to regions (e.g. Ile de France). For all the 91 countries with population data from the World Bank there was a corresponding entry in Wikipedia with the country infobox.

Table 4 shows the median, mean and standard deviation of the difference between the observed and actual values, given as a percentage of the real value. For instance, in the case of the median, we see that the population included in Wikipedia differs from the real population value of these countries by roughly 2 % of its value.
To assess coverage, we study the proportion of countries for which the country infobox provides an estimate of the population size. Figure 3 indicates that from 2005 to 2009, the proportion of countries with a population attribute has steadily grown from 58 to 100 %. In the same time period, we can see that the proportion of countries providing an estimate that was updated in the previous year has also grown from 58 to 90 %.

These statistics suggest that the estimates for the population size are generally accurate and have a good coverage, which is improving over time. 4.2.2 Delay analysis Popular Wikipedia entries are generally updated almost instantly when new information becomes publicly available. For example, Wikipedia entries reporting game scores of famous soccer teams are typically updated within seconds after the end of the game. However, update delays for less popular entries may be significantly longer. Thus, to avoid selecting obsolete values, a Wikipedia-based proxy for temporal attribute values needs to take into account that some delays may happen before the real value is introduced in the dataset. This section describes an analysis of the latency of infobox date attributes.

Let us define the latency of an attribute update as the difference between the moment a piece of information was first available and when it was included as an attribute in the relevant Wikipedia infobox. Measuring the latency of an attribute is generally problematic because the exact time when the information item became available might be difficult to obtain. In particular, sources of historical data seldom report when the information was first available. In the previous experiment on population sizes, the World Bank data gives historical values but it does not indicate when exactly these values were released. We shall look for other attributes, that are more adequate for measuring latency.

Infobox attributes whose value is the date in which an event took place offer an additional temporal reference, which we can use for our purposes: we can focus on infobox attributes whose value is a date that has to be previous to the update to the corresponding infobox attribute. For instance, the infobox person has the attribute death-date; if for the entry of a person the attribute death-date was first filled in date d with value d , we can automatically compute how long it took to update the entry from the time the event happened in the real world: d u -d .

In other words, to estimate the latency of a revision of a date attribute we compute the number of days between the revision date and the new value (assumed to contain a year, month, and day). Obviously, we remove from this study old date values. In an extreme case, if we had considered the date of death of Voltaire (30 May 1778), the update delay would be due to the fact that Wikipedia did not exist back then. As a general rule, the latency is only computed for revisions whose value is later than the earliest date when the attribute was first introduced across all entries.

The distribution of delays for various date attributes as computed using this method is reported in Table 5 , organized by deciles. For example, in the case of the date of a military conflict, more than 40 % of updates are reported in &lt;2 days. The last time a television show or episode was aired is typically updated much faster than the  X  X irst time aired X  attribute. Considering the aggregate values for all date attributes in all infoboxes (the last row of Table 5 ), 20 % of these date attributes are updated within the day that the event actually happened (the second decile is 0 days). 4.3 Vandalism detection experimental results This section describes the results of the automatic evaluation of vandalic edits using only infobox data. As mentioned in Sect. 3.3, an AdaBoost classifier was trained on the development set that we have collected, and the test set used is PAN-WVC-10 (Potthast, 2010 ). As a baseline for comparison we decided to use WikiTrust (Adler et al. 2010 ), a state-of-the-art vandalism detection system. It has a public API available at http://www.wikitrust.net/vandalism-api , to which it is possible to send a document title and revision ID, getting as a response from the system backend a confidence value, between 0 and 1, of the article revision being vandalic. It is still necessary to define a threshold so that if the confidence value exceeds the threshold the revision will be considered to be vandalic. We have used our own development set to find the confidence threshold that maximizes the F-score for vandalic edits. This system ranked second in a recent vandalism detection competition (Potthast et al. 2010 ).

Figure 4 shows the ROC curves for our vandalism detection and the system described by Adler et al. ( 2010 ). The area under the curve reported by Adler et al. ( 2010 ) is 0.90, slightly higher than for our approach (0.88).

Something that can be noted is that the results are in the low end of those reported in the PAN evaluation for the system that we are using as baseline. One possible affect infoboxes. To verify this, we ran the WikiTrust API on the original PAN-WVC-10 dataset, including changes that did not affect infoboxes. With this set, the obtained area under the ROC curve is again comparable with the results obtained here, 0.88. After a personal communication with one of the authors he indicated that there may be many reasons for this to happen, such as that user reputation and other parameters also considered for classification may have changed since the PAN-WVC-10 evaluation was performed.

We conclude that in the task of vandalism identification, for those articles that contain infoboxes, using only infobox-dependent features it is possible to attain a performance comparable to that obtained using full entry features. 5 Conclusions and future work In this paper we have described our approach to extract, compile and make available, from the revision history of Wikipedia, a dataset of temporally anchored relational data. The immediate contributions of this work are the built architecture, capable of extracting infobox attribute updates from the Wikipedia historical data (from both a historical dump and single page updates), and the dataset released.
We are releasing the full, up-to-date, dataset of Wikipedia infobox attribute updates, WHAD2012, under the Creative Commons license that covers Wikipedia. For each entry and revision-timestamp we store an infobox instance extracted for that revision, containing tuples of the following form: ( attribute , previous value , current value , timestamp ), and metadata pertaining to that revision. The full dataset, containing over 510 million such revisions, has been made available. With a total size of 5.5 GB, compared to the original edit-history dump released by Wikimedia, this new corpus is much easier to handle, thus facilitating further research.
We have presented several analyses performed on the dataset, including a case study on the population attribute for countries, showing that the accuracy of the values matches the real values reported by the World Bank within an error of around 2 %, and a study on the delay with which date attributes are encoded in Wikipedia, showing that 20 % of them are updated within a day, and 50 % within 4 months.
One particular characteristic of the revision history is that vandalic content is pervasive; even though most vandalic edits are typically short-lived, working as we do with the full edit history requires us to deal with them, and vandalism identification becomes necessary. We have described a vandalism classifier using primarily features obtained from infoboxes, without relying on full-page features at all, and showed that we can attain results that are comparable to a state-of-the-art system trained on the full entries in Wikipedia.

In future work, we plan to investigate the following lines of research: 1. Extending the vandalism classifier to include more structured information about 2. Extending our accuracy analysis to more attribute types. 3. As our approach is language independent, we are considering processing and 4. Assessing the factual correctness of the information contained in Wikipedia has Appendix: Manual rating instructions Instructions Wikipedia is an on-line encyclopedia to which many users contribute editing the entries. Wikipedia entries sometimes contain one or several small boxes with structured data called Infoboxes. For example, the Wikipedia entry for United States has a small box at the right hand side containing the name of the country, its flag and seal, motto, anthem, capital, and other facts about the country. We X  X l call each of these lines in the infobox attributes . If you want to read more about Wikipedia Infoboxes, you can see this page.

Wikipedia keeps logs of all the edits done by each contributor during the past many years. This allows us to explore the past changes for each entry. For example, Confederation X  X . In this example, the contributor modified the value of the attribute  X  X  X riter X  X . This attribute is the one that is used in the infobox line specifying who the authors were. This particular contributor edited the value of the writer from just  X  X  X ontinental Congress X  X  to a new value of an insulting nature. This is a clear case of vandalism. For the purposes of this evaluation, we consider that a contribution is vandalic if either:  X  It is adding insulting or obscene content.  X  It is plainly false.

If a page contained a correct value and a user replaces it with an incorrect value, we assume that the edit is vandalism. For example, look at this page. The value of the origin (birth place) of Lil Jon was changed from Montreal to Atlanta. The correct value for this attribute is Atlanta. You can click on the  X  X  X revious edit X  X  link to see that Montreal was added in replacement of the correct value Atlanta. For these reasons, we X  X l say that the page was initially correct, Montreal was added in a vandal edit, and the change in the shown page is fixing the vandalism by reverting the value to the previous correct value Atlanta.

You will be shown below the name of an entry, the time when it was changed, name of the attribute in the infobox, the old value of the attribute, and the new value of the attribute. The task is to reply to the questions below to identify possible cases of incorrect values or vandalic actions.
 References
