 Given Q nodes in a social network (say, authorship net-work), how can we find the node/author that is the center-piece, and has direct or indirect connections to all, or most of them? For example, this node could be the common ad-visor, or someone who started the research area that the Q nodes belong to. Isomorphic scenarios appear in law en-forcement (find the master-mind criminal, connected to all current suspects), gene regulatory networks (find the pro-tein that participates in pathways with all or most of the given Q proteins), viral marketing and many more.
Connection subgraphs is an important first step, handling the case of Q =2 query nodes. Then, the connection sub-graph algorithm finds the b intermediate nodes, that provide a good connection between the two original query nodes. Here we generalize the challenge in multiple dimensions: First, we allow more than two query nodes. Second, we allow a whole family of queries, ranging from  X  X R X  to  X  X ND X , with  X  X oftAND X  in-between. Finally, we design and compare a fast approximation, and study the quality/speed trade-off.
We also present experiments on the DBLP dataset. The experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting sub-graphs agree with our intuition. Wall-clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6 : 1 speedup.
 This material is based upon work supported by the National Science Foundation under Grants No. IIS-0209107 SENSOR-0329549 EF-0331657IIS-0326322 IIS-0534205. This work is also supported in part by the Penn-sylvania Infrastructure Technology Alliance (PITA), a part-nership of Carnegie Mellon, Lehigh University and the Com-monwealth of Pennsylvania X  X  Department of Community and Economic Development (DCED). Additional funding was provided by donations from Intel, NTT and Hewlett-Packard. Any opinions, findings, and conclusions or rec-ommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Na-tional Science Foundation, or other funding parties. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. H.2.8 [ Database Management ]: Database Applications  X  Data Mining Application, experimentation Center-piece subgraph, goodness score, K softAND
Graph mining has been attracting increasing interest re-cently, for community detection, partitioning, frequent sub-graph discovery and many more. Here we introduce and solve a novel problem, the  X  center-piece subgraph  X ( CEPS ) problem: Given Q query nodes in a social network (e.g., co-authorship network), find the node(s) and the resulting subgraph, that have strong connections to all or most of the Q query nodes. The discovered nodes could contain a com-mon advisor, or other members of the research group, or an influential author in the research area that the Q nodes belong to. As mentioned in the abstract, there are multiple alternative applications (law enforcement, gene regulatory networks).

Earlier work [6] focused on the so-called  X  X onnection sub-graphs X . Although the inspiration for the current work, the connection subgraph algorithm can only handle the case of Q =2. This is exactly the major contribution of our work: we allow not only pairs of query nodes, but any arbitrary number Q of them.

Figure 1 gives screenshots of our system, showing our so-lution on a DBLP graph, with Q =4 query nodes. All 4 researchers are in data mining, but the first two (Rakesh Agrawal and Jiawei Han) are more on the database side, while Michael Jordan and Vladimir Vapnik are more on the machine learning and statistical side. Figure 1(b) gives our CEPS subgraph, when we request nodes with strong ties to all four query nodes. The results make sense: re-searchers like Daryl Pregibon, Padhraic Smythe and Heikki Mannila are vital links, because of their cross-disciplinarity and their strong connections with both the above sub-areas. Figure 1(a) illustrates an important aspect of our work, the K softAND feature, which we will discuss very soon. In a nutshell, in a K softAND query, our method finds nodes with connections to at least k of the query nodes ( k =2in Figure 1(a)).
Thus, we define the center-piece subgraph problem, as follows: Problem 1. Center-Piece Subgraph Discovery( CEPS ) Given: an edge-weighted undirected graph W , Q nodes as Find: a suitably connected subgraph H that (a) contains all
Allowing Q query nodes creates a subtle problem: do we want the qualifying nodes to have strong ties to all the query nodes? to at least one? to at least a few? We handle all of the above cases with our proposed K softAND queries. Figure 1(a) illustrates the case where we want intermediate nodes with good connections to at least k =2ofthequery nodes. Notice that the resulting subgraph is much different now: there are two disconnected components, reflecting the two sub-communities (databases/statistics).

The contributions of this work are the following
The system is operational, with careful design and nu-merous optimizations, like alternative normalizations of the adjacency matrix, a fast algorithm to compute the scores for K softAND queries.

Our experiments on a large real dataset (DBLP) show that our method returns results that agree with our intuition, and that it can be made fast (a few seconds response time), while retaining most of the accuracy (about 90%).

The rest of the paper is organized as follows: in Section 2, we review some related work; Section 3 provides an overview of the proposed method: CEPS . The goodness score calcu-lation is proposed Section 4 and its variants are presented in the Appendix. The  X  EXTRACT  X  algorithm and the speed-ing up strategy are provided in Section 5 and Section 6, respectively. We present experimental results in Section 7; and conclude the paper in Section 8.
In recent years, there is increasing research interest in large graph mining, such as pattern and law mining [2][5][7][20], frequent substructure discover y [27], influence pro pagation [18], community mining [9][11][12] and so on. Here, we make a brief review of the related work, which can be categorized into four groups: 1) measuring the goodness of connection; 2) community mining; 3) random walk and electricity re-lated methods; 4) graph partition.

The goodness of connection. Defining a goodness cri-terion is the core for center-piece subgraph discovery. The two most natural measures for  X  X ood X  paths are shortest dis-tance and maximum flow. However, as pointed out in [6], both measurements might fail to capture some preferred characteristics for social network. The goodness function for survivable network [13], which is the count of edge-disjoint or vertex-disjoint paths from source to destination, also fails to adequately model social relationship. A more related dis-tance function is proposed in [19] [23]. However, It can-not describe the multi-faceted relationship in social network since center-piece subgraph aims to discover collection of paths rather than a single path.

In [6], the authors propose an delivered current based method. By interpreting the graph as an electric network, applying +1 voltage to one query node and setting the other query node 0 voltage, their method proposes to choose the subgraph which delivers maximum current between the query nodes. In [25], the authors further apply the delivered cur-rent based method to multi-relational graph. However, the delivered current criterion can only deal with pairwise source queries. Moreover, the resulting subgraph might be sensi-tive to the order of the query nodes (See Figure 2 for an example). On the other hand, as we will show very soon, connection subgraph can actually be viewed as a special case of the proposed center-piece subgraph ( X  X ND query X  with pair source nodes ).

Random walk related methods. The proposed im-portance score calculation is based on random walk with restart. There are many applications using random walk and related methods, including PageRank [22], personalized PageRank [14], SimRank [16], neighborhood formulation in bipartite graph [26], content-based image retrieval [15], cross modal correlation discovery [24], BANKS system [1], Objec-tRank [3], RalationalRank [10] and so on.

Community detection. Center-piece subgraph discov-ery is also related with community detection, such as [9][11][12]. However, we cannot directly apply community detection to subgraph discovery especially when the source queries are remotely related or they lie in different communities.
Graph partition and clustering. There are a bunch of graph partition and clustering algorithms proposed in the literature, e.g. METIS [17], spectral clustering [21], flow simulation [8], co-clusterfing [4], betweenness based method [12]. It is worth pointing out that the proposed method is orthogonal to the specific graph partition algorithms.
Let us first define the goodness score for nodes. For a given node j , we have two types of goodness score for it:
A natural way to measure the goodness of the subgraph H is to measure the goodness of the nodes it contains: the more  X  X ood X /important nodes (wrt th e source queries) it contains, the better H is. Thus, the goodness criterion of H can be defined as:
With the above goodness criterion, a straightforward way to choose the  X  X est X  subgraph should be the one which max-imizes g ( H ):
However, no connection is guaranteed in this way and the resulting subgraph H might be a collection of isolated nodes. Thus, there are two basic problems in center-piece subgraph discovery: 1) how to define a reasonable goodness score r ( Q ,j ) for a given node j ; 2): how to quickly find a connection subgraph maximizing g ( H ). Moreover, since it might be very difficult to directly calculate the goodness score r ( Q ,j ), we further decompose it into two steps. The pseudo code for the proposed method ( CEPS ) is listed as follows:
There are two basic concepts in goodness score calcula-tion: Input : the weighted graph W , the query set Q , Output : the resulting subgraph H
Step 1: Individual Score Calculation . Calculate the
Step 2: Combining Individual Scores. Combine the
Step 3:  X  EXTRACT  X . Extract quickly a connection
These two kinds of steady probability ( r i,j and r ( Q ,j,k )) are the base of our goodness score calculation (for both r ( i, j )and r ( Q ,j )). It X  X  basic idea is that: suppose there are Q random particles doing RWR from each query node in-dependently; then after convergency, each particle has some steady-state probability staying at the node j ; and different particles have some meeting probability at the node j .The steady-state probability and the meeting probability provide some hints on how the node j is related with the source queries, and are used to compute the goodness score of node j . Moreover, by designing different meeting probability ,we can get the specific type of goodness score tailored for the specific query scenario. Table 2 lists all the symbols and definitions used throughout this paper.
Here we want to compute the goodness score r ( i, j )ofa single node j , for a single query node q i .Weproposetouse random walks with restart, from the query node q i .
Suppose a random particle starts from query q i , the parti-cle iteratively transmits to its neighborhood with the proba-bility that is proportional to the edge weight between them, and also at each step, it has some probability c to return to node q i . r ( i, j ) is defined as the steady-state probability r that the particle will finally state at node i :
More formally, if we put all the r i,j probabilities into ma-trix form R =[ r i,j ], then where E =[ e i ]( i =1 , ..., Q )isthe N  X  Q matrix, c is the fly-out probability, and  X  W is the adjacency matrix W ap-propriately normalized, say, column-normalized:
Theproblemcanbesolvedinmanyways-wechoose the iteration method, iterating Eq. 4 until convergence. For Q ,  X  Q = { q i } , ( i =1 , .., ( Q  X  1)) simplicity, in this paper, we iterate Eq. 4 m times, where m is a pre-fixed iteration number.
Here we want to combine the individual score r ( i, j )( i = 1 , ..., Q )toget r ( Q ,j ), the goodness score for a single node j wrt the query set Q . We propose to use the meeting prob-ability r ( Q ,j,k ) of random walk with restart. Furthermore, by using different softAND coefficient k , we can deal with different types of query scenario.
 The most common query scenario might be that  X  X iven Q query nodes, find the subgraph H the nodes of which are important/good wrt ALL queries X . In this case, r ( Q ,j ) should be high if and only if there is a high probability that ALL particles will finally meet at node j :
Eq. 6 actually defines a logic AND operation in terms of individual goodness scores: the node j is important wrt the query set Q if and only if it is important wrt every query node. Thus, we refer such query type as  X  X ND query X .
A complemental query scenario is  X  X R query X :  X  X iven Q queries, find the subgraph H the nodes of which are impor-tantwrtatleastONEquery X . Inthiscase, r ( Q ,j ) should be high if and only if there is a high probability that at least one particle will finally stay at node j :
Eq. 7 defines a logic OR operation in terms of individual importance scores: the node j is important wrt the source queries if and only if it is important wrt at least one source query.

Besides the above two typical scenarios, the user might also ask  X  X iven Q queries, find the subgraph H the nodes of which are important wrt at least k (1  X  k  X  Q )queries X . We refer such query type as  X  K softAND query X . In this case, r ( Q ,j ) should be high if and only if there is a high probability that at least k -out-of-Q particles will finally meet at node j . To avoid exponential enumeration (which is O (2 k )), Eq. 8 can be computed in a recursive manner: where r (  X  ,j, 0) = 1( j =1 , ..., Q ).

Intuitively, Eq. 8 defines a logic operation in terms of in-dividual importance scores that is between logic AND and logic OR. In this paper, we refer it as logic K softAND: the node j is important wrt the sourc e queries if and only if it is important wrt at least k -out-of-Q source queries.
It is worth pointing out that both  X  X ND query X  and  X  X R query X  can be viewed as special cases of  X  K softAND query X :  X  X ND query X  is actually  X  X  softAND query X ; while  X  X R query X  is actually  X 1 softAND query X 
To compute the goodness score r ( i, j )and r ( Q ,j ), we need to construct the transition matrix  X  W for random walk with restart. A direct way is to normalize W by column as Eq. 5. However, as pointed out in [6], there might be the so called  X  X izza delivery person X  problem, that is, the node with high degree is prone to receive too much attention (receiving too high individual goodness score in our case). To deal with this problem, we propose to normalize W as Eq. 10. The normalized weighted graph W will be further used to for-mulate the transition matrix  X  W by Eq. 5. for all j, l =1 , ..., N .

The motivation of normalization is as follows: for the high degree node j ,everyedge( j, l )( l =1 , ...., N ) is penalized by ( d )  X  and vice versa. The coefficient  X  control the penal-ization strength: bigger  X  indicates stronger penalization. Note that the idea of penalizing the node with high degree is similar with that of setting a universal sink node in [6].
The  X  X XTRACT X  algorithm takes as input the weighted graph W , the importance scores on all nodes, the budget b and the softAND coefficient k ; and produces as output a small, unweighted, undirected graph H . The basic idea is similar with the display generation algorithm in [6]: 1) instead of trying to find an optimal subgraph maximizing g (
H ) directly, we decompose it into finding key paths incre-mentally; 2) by sorting the nodes in order, we can quickly find the key paths by dynamic programming in the acyclic graph.

However, we cannot directly apply the original display generation algorithm since it can only deal with pair source queries (and also the resulting subgraph is sensitive to the order of the source queries). To deal with this issue, we extend the original algorithm in the following aspects: (1) Instead of finding a source-source path, at each step, (2) The order (which will be used in the dynamic program-(3) Key path discovery differs with the different query
Before presenting the algorithm, we require the following definitions:
The destination node pd can be decided by Eq. 11: where H is the partially built output subgraph.

In order to discover a new path between the source q i and the promising node pd , we arrange the nodes in descending order of r ( i, j )( j =1 , ..., n ): { u 1 = q i ,u 2 ,u (note that all nodes with smaller r ( i, j )than r ( i, pd )areig-nored). Then we fill the extracted matrix C in topological order so that when we compute C s ( t, u ), we have already computed C s ( t, v ) for all v  X  d i ,u . On the other hand, as the subgraph is growing, a new path may include nodes that are already present in the output subgraph, our algorithm will favor such paths as in [6]. The complete algorithm to discover a single path from source node q i and the destina-tion node pd is given in table 3.
 1. Let len be the maximum allowable path length 2. For j  X  [1 , ..., n ] 3. Output the path maximizing C s ( i, pd ) /s ,where s =0
Based on the previous preparations, the EXTRACT al-gorithm can be given in table 4.
 1. Initialize output graph H null 2. Let len be the maximum allowable path length 3. While H is not big enough 4. Output the final H
To compute r ( i, j ), we have to solve a linear system. When the data set is large (or more precisely, when the total num-ber of the edges in the graph is large), the processing time could be long.
 Note that Eq. 4 can be solved in closed form:
Since both  X  X ND query X  and  X  X R query X  can be viewed as special cases of  X  X  softAND query X , the number of active sources is actually k for all query types.
Thus, an obvious way to speed up CEPS is to pre-compute and store the matrix A =( I  X  c  X  W )  X  1 ,then R T =(1  X  can be computed on-line nearly real-time. However, in this way, we have to store the whole N  X  N matrix A ,whichis a heavy burden when N is big.

As suggested by [26], the goodness score r ( i, j )( j =1 , ..., N ) is very skewed, that is, most values of r ( i, j ) are near zero and only a few nodes have high value. Based on this ob-servation, we propose to pre-partition the original weighted graph W into several partitions and only use the partitions containing the source queries to run CEPS .Inthispaper, we use METIS [17] as the partition algorithm.

The pseudo code for the accelerated CEPS is summarized as follows: Input : the weighted graph W , the query set Q , Output : the resulting subgraph H Step 0: pre-partition W into p pieces (one-time cost)
Step 1: pick up partitions of W that contain
Step 2: .run CEPS as in table 1 on nW In this section, we demonstrate some experimental results. The experiments are designed to answer the following ques-tions.
Data Set We use the DBLP data set to evaluate the pro-posed method. To be specific, the author-paper information is used to construct the weighted graph W : every author is denoted as a node in W ; and the edge weight is the number of co-authored papers between the corresponding two au-thors. On the whole, there is  X  315 K nodes and  X  1 , 834 K non-zero edges in W .

Source Queries To test the proposed algorithm, we se-lect several people from different communities to compose the source-query repository: 13 people from database and mining; 13 people from statistical and machine learning; 11 people from information retrieval; and 11 people from com-puter vision. Then the source queries are generated by ran-domly selecting a small number of queries from the reposi-tory.

Parameter Setting The re-starting coefficient c in Eq. 4 is set 0 . 5 and the iteration number m is set 50 since we do not observe performance improvement with more iteration steps. The maximum allowable path length len is decided by the budget b and the number of active sources k as [ b/k ]. For normalization coefficient  X  , a parametric study is provided in Section 7.3. For other experiments,  X  =0 . 5.

Evaluation Criterion Firstly, the resulting g ( H )canbe evaluated by  X  X mportant Node Ratio ( NRatio ) X . That is,  X  X ow many important/good nodes are captured by g ( H )? X : Complementally, we can also evaluate by  X  X mportant Edge Ratio ( ERatio ) X . That is,  X  X ow many important/good edges are captured by g ( H )? X :
The goodness score r ( Q , ( j, l )) of an edge ( j, l ) is defined similarly as the goodness score for a node: what is the prob-ability that the specific edge ( j, l ) will be traversed simul-taneously by all (or at least k ) of the particles. Firstly, we calculate the goodness score r ( i, ( j, l )) for an edge ( j, l )wrt a single query node q i :
BasedonEq.15,wecaneasilydefine r ( Q , ( j, l )) according to the specific query type. For example, for  X  X ND query X , r (
Q , ( j, l )) can be computed as Eq. 16; while for  X  X R query X  and  X  X  softAND query X , r ( Q , ( j, l )) can be computed as Eq. 17 and Eq. 18, respectively. r ( Q , ( j, l )) r ( Q , ( j, l ) , 1) = 1  X  r (
Q , ( j, l )) r ( Q , ( j, l ) ,k ) where r (  X  , ( j, l ) , 0) = 1.

For all experiments except subsection 7.1, we run the pro-posed algorithm multiple times and report the mean NRatio as well as mean ERatio .
As we mentioned before, connection subgraph is a spe-cial case of center-piece subgraph ( X  X ND query X  with pair source nodes ). Figure 2 shows the connection subgraph with budget 4 for  X  X oumen Chakrabarti X  and  X  X aymond T. Ng X . It can be seen that both our method and the deliv-ered current method output somewhat reasonable results. It is worth pointing out that the subgraph by the delivered current method is very sensitive to the order of the source queries: comparing figure 2(a) and (b), there is only one common node ( X  X . Muthukrishnan X ). On the other hand, if we compare figure 2(b) and (c), while most nodes are the same for the two methods, It is clear that our method cap-tures more strong connection: compared with figure 2(b), the different node ( X  X .V. Jagadish X ) in figure 2(c), 1) has more connections (4 vs. 3) with the remaining nodes and 2) has more co-authored papers with those connected neigh-bors than the corresponding node in figure 2(b) ( X  X hiyuan Chen X ).

Figure 1 shows an example for multi-source queries. When the user asks for 2  X  SoftAND , the algorithm outputs two clear cliques (figure 1(a)), which makes some sense since  X  X ladimir Vapnik X  and  X  X ichael I. Jordan X  belong to statis-tical machine learning community; while  X  X akesh Agrawal X  and  X  X iawei Han X  are database and mining people. On the other hand, if the user asks for  X  X ND X , the resulting sub-graph shows a strong connection with all four queries. Figure 3 shows an example for  X  X ND query X , with  X  X eorge Karypis X ,  X  X ise Getoor X  and  X  X ian Pei X  as source nodes. All three researchers are working on graphs. The nodes of the retrieved  X  X enter-piece subgraph X  are all database, data mining and graph mining people, forming three groups: the nodes close to  X  X ise Getoor X  are related to the Univer-sity of Maryland ( X  X .S. Subrahmanian X  is a faculty mem-ber there and he was the advisor of  X  X aymond Ng X ). The nodes close to  X  X eorge Karypis X  are faculty members at Minnesota ( X  X ipin Kumar X ,  X  X hashi Shekar X ). The nodes close to  X  X ian Pei X  are professors at Simon Fraser (SFU) or University of British Columbia (UBC), which are geo-graphically nearby, both in Vancouver:  X  X iawei Han X  was a faculty member at SFU and thesis advisor of  X  X ian Pei X  ;  X  X aks Lakshmanan X  and  X  X aymond Ng X  are faculty mem-bers at UBC. Not surprisingly, the  X  X enter-pieces X  of the subgraph consist of  X  X aymond Ng X ,  X  X iawei Han X ,  X  X aks Lakshmanan X , which all have direct, or strong indirect con-nections with the three chosen query sources.
The performance of the  X  EXTRACT  X  algorithm is evalu-ated by measuring both NRatio and ERatio as functions of the budget b . Here, we fix the query type as  X  X ND query X .
Figure 4(a) shows the mean NRatio vs. the budget b for different numbers of source queries; while figure 4(b) shows the mean ERatio vs. the budget b for different numbers of source queries. Note that in both cases, our method captures most of important nodes as well as edges by a small number of budget b . For example, for 2 source queries, the result-ing subgraph with budget 50 captures 95% important nodes and 70% important edges on average; for 4 source queries, the resulting subgraph with budget 20 capt ures 100% im-portant nodes and 70% important edges on average. An interesting observation is that for the same budget, the sub-graph with more source queries captures higher NRatio as well as ERatio than those with less source queries. This is consistent with the intuition: generally speaking, finding people that are important wrt all source queries becomes more difficult when the number of source queries increases. In other words, r ( Q ,j ) becomes more skewed by increasing thenumberofsourcequeries.
Here we conduct the parametric study for normalization coefficient  X  .Themean NRatio vs.  X  isplottedinfigure 5(a); and the mean iERatio vs.  X  is plotted in figure 5(b).
It can be seen that in most cases, the normalization step does help to improve the performance of the resulting sub-graph g ( H ). For example, the normalization with  X  =0 . 5 helps to capture 17 . 7% more important nodes and 9 . 1% more important edges for 2 source queries on average; while for 3 source queries, it captures 18 . 1% more important nodes and 7 . 6% more important edges on average.
For large graph, the response time for importance score calculation could be long. By pre-partition the original graph and performing subgraph discovery only on the par-titions containing the source queries, we could dramatically reduce the response time. On the other hand, we might miss a few important nodes if they do not lie in these parti-tions. To measure such kind of quality loss, we use  X  X elative Important Node Ratio ( RelRatio ) X : where NRatio and NRatio are  X  X mportant Node Ratio X  for the subgraph by pre-partition and by the original whole graph, respectively.

We fix the budget 20 and the query scenario as  X  X ND query X . The mean RelRatio vs. responsetimeisshownin figure 6(a); and the mean response time vs. the number of partitions is shown in figure 6(b). It can be seen that with a little quality loss, the response process is largely speeded up. For example, with  X  10% loss, the subgraph for 2 source queries can be generated within 5 seconds on average; with  X  10% quality loss, the subgraph for 5 source queries can be generated within 10 seconds on average. On the other hand, it might take 40 s  X  60 s without pre-partition. Note that in figure 6 (b), even with a small number of partitions, we can greatly reduce the mean response time.
We have proposed the problem of  X  center-piece subgraphs  X , and provided fast and effective solutions. In addition to the problem definition, other contributions of the paper are the following:
A very promising research direction is the use of paral-lelism, to achieve fast responses on huge graphs. Another one is to extend the concepts and algorithms to  X  X ulti-graphs X , that is, graphs with different types of edges. For example, a social network, where one type of edge would in-dicate  X  X -mail correspondence X , another would mean  X  X ele-phone contact X , and so on.
The authors would give their sincere thanks to anonymous reviewers for their valuable comments; to Jimeng Sun for his help to process DBLP dataset. [1] B. Aditya, G. Bhalotia, S. Chakrabarti, A. Hulgeri, [2] R. Albert, H. Jeong, and A.-L. Barabasi. Diameter of [3] A. Balmin, V. Hristidis, and Y. Papakonstantinou. [4] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [5] S. Dorogovtsev and J. Mendes. Evolution of networks. [6] C. Faloutsos, K. S. McCurley, and A. Tomkins. Fast [7] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On [8] G. Flake, S. Lawrence, and C. Giles. Efficient [9] G. Flake, S. Lawrence, C. L. Giles, and F. Coetzee. [10] F. Geerts, H. Mannila, and E. Terzi. Relational [11] D. Gibson, J. Kleinberg, and P. Raghavan. Inferring [12] M. Girvan and M. E. J. Newman. Community [13] M. Gr  X  otschel, C. L. Monma, and M. Stoer. Design of [14] T. H. Haveliwala. Topic-sensitive pagerank. WWW , [15] J. He, M. Li, H. Zhang, H. Tong, and C. Zhang. [16] G. Jeh and J. Widom. Simrank: A measure of [17] G. Karypis and V. Kumar. Parallel multilevel k-way [18] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [19] D. Liben-Nowell and J. Kleinberg. The link prediction [20] M. E. J. Newman. The structure and function of [21] A. Ng, M. Jordan, and Y. Weiss. On spectral [22] L. Page, S. Brin, R. Motwani, and T. Winograd. The [23] C. R. Palmer and C. Faloutsos. Electricity based [24] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [25] C. Ramakrishnan, W. Milnor, M. Perry, and A. Sheth. [26] J. Sun, H. Qu, D. Chakrabarti, and C. Faloutsos. [27] D. Xin, J. Han, X. Yan, and H. Cheng. Mining [28] D. Zhou, O. Bousquet, T. Lal, J. Weston, and Here, we provide and discuss some variants on goodness score calculation.  X  Variant 1: calculate r i,j by manifold ranking
One potential problem with Eq. 4 is that such goodness score might be asymmetric, that is r i,j = r j,i . For social network, this is OK since that person X is important/good for person Y does not necessarily mean that person Y is also important/good for person X . However, in some other applications, symmetry might be a desirable property for the goodness score. To deal with this problem, we can define r as manifold ranking score [28].

Formally, r i,j in this case can be computed by replacing the transition matrix  X  W in Eq. 4 by graph Laplacian S : where S = D  X  1 / 2 WD  X  1 / 2 is graph Laplacian.
Note that since S is symmetric, the individual goodness score r i,j by Eq. 20 is always symmetric. That is, r i,j However, in this case, the resulting goodness score r i,j no longer the steady-state probability ,thatis In our experiments, we find that the resulting subgraphs by Eq. 4 and Eq. 20 are actually quite similar.  X  Variant 2: calculate r ( Q ,j ) by order statsitic 1 , ..., Q ).

Then, we can also use r ( k ) ( i, j )toget r ( Q ,j ). For exam-ple, we can use minimum order statistic as goodness score for  X  X ND query X :
The probabilistic interpretation of Eq. 21 is that the node j is important wrt the source q ueries if and only if there is at least some high probability for every particle to finally stay at node j .

Similarly, the order statistic variants for  X  X R query X  and  X  X  softAND query X  can be defined as r (1) ( i, j )and r ( respectively.
