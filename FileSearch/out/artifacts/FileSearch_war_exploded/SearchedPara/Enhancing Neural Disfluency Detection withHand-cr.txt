 Disfluencies are common in automatic speech recognition (ASR). Detecting disfluen-cies is important for natural language understanding, since most downstream NLU sys-tems are built on the fluent utterances. Disfluency of a sentence can be categorized into five classes: uncompleted words, filled pauses (e.g.  X  X h X ,  X  X m X ), editing terms (e.g.  X  X ou know X ), discourse markers (e.g.  X  X  mean X ) and repairs that are discarded, or cor-rected by its following words (see Fig. 1 for a disfluency example with filled pauses, discourse markers and repair words). The former four classes of disfluencies are easy to detect as they often consist of fixed phrases (e.g.  X  X h X ,  X  X ou know X ). While, the re-pair disfluencies (see Table 1) are more difficult to detect, because they are in arbitrary form [25]. Most of the previous disfluency detection works focus on detecting the repair disfluencies.
 lems for detecting the repair disfluencies. Previous sequence tagging methods [5, 6, 21] require carefully designed features to capture information of long distance, but usu-ally suffer sparsity problem. Another line of syntax-based disfluency detection work [9, 25] try to model the repair phrases on a syntax tree by compressing the unrelated phrases and allowing repair phrases to interact with each other. However, data that both have syntax tree and disfluency annotation is scarce. The performance of syntax parsing models (about 92% on English) also hinders the disfluency detection X  X  performance. Re-current neural network (RNN), which can capture dependencies at any length, has been successfully applied to many NLP tasks, including NER [11] and opinion mining [12]. There is also work that tried to use RNN in disfluency detection problem [10]. However, in term of solving long-range dependencies, [10] overly relies on the RNN itself and doesn X  X  adopt the hand-craft discrete features which has shown effective in previous work [26]. Further more, [10] treats sequence tagging as classification on each input token and doesn X  X  model the transition between tags which is important for recognizing the repair phrases of multi-words.
 solve the disfluency detection problem. More specifically, we use a bidirectional Long Short-Term Memory (BI-LSTM) network to encode the words and feed the output to a linear-chained CRF to model the probability of tag sequence, thus result in a BI-LSTM-CRF model. We also study the problem of adopting the hand-crafted discrete features into BI-LSTM-CRF model. We evaluate our model on the English Switchboard corpus. The results show that our model outperforms previous stat-of-the-art systems by achieving a 87.1% F-score. RNN has been employed to produce state-of-the-art results on a variety of tasks [11, in which h t encodes the information in ranging from x 1 to x t . In theory, RNN can capture dependencies of the inputs at any length with a hidden layer that memorizes the historical information. Unfortunately, in practice, it fails due to the gradient var-nishing/exploding problems [1, 20]. Long Short-Term Memory (LSTM) [8] as a variant of RNN is designed to cope with the gradient varnishing problems. It addresses the varnishing problems with an extra memory  X  X ell X  c t that is constructed as a linear com-bination of the previous state and input signal. LSTM cells process the inputs with three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. A LSTM memory cell is calculated as: where is the element-wise sigmoid function and is the element-wise product. ering its past contexts, because disfluent phrases of this word can occur before or after it. A good model should access to both the past and the future contexts for disfluency detection. In this paper, we encode the past and the future information with BI-LSTM [15]. In the BI-LSTM, the past information is represented with a forward LSTM and the future with a backward LSTM respectively. The hidden states from these two LSTMs are concatenated to form the final output. tagging. But such classification scheme is limited when there are strong dependencies between output tags. Disfluency detection is one of the tasks that strong dependencies exist between tags because one repair phase can have multi-words. Instead of modeling tagging decisions independently, we model them jointly using a CRF layer [11, 14, 15, 18]. Fig. 2 illustrates the architecture of BI-LSTM-CRF in detail. For an input sentence BI-LSTM network, where k is the number of tags and P i,j corresponds to the score of x its score is defined as and its probability is defined as where Y X represents all possible tag sequences of the input sequence X . To learn pa-rameters of the BI-LSTM and the transition matrix A , we minimize the negative log-probability of the correct tag sequence over the input data { ( X ( n ) ,y ( n ) ) } N training: While decoding, we predict the highest-scored output sequence using dynamic pro-gramming. Previous studies [5, 21] show that hand-crafted features are very effective for achiev-ing good disfluency detection performance, especially those features that capture the duplication between phrases. In this paper, We use two kinds of hand-crafted discrete features as shown in Table 2 and incorporate them into our neural networks by trans-lating them into a 0-1 vector d . The dimension of d is 78, which equals to the number templates. The duplicate features care whether x t has a duplicated word/pos in certain distance. The similarity features care whether the surface string of x t resembles its sur-rounding words.
 features d . In the first method, we treat d as an input to the LSTM along with the fin-tune word embedding w , fixed word embedding e w and POS-tag embedding p . These four parts are concatenated together, transformed by a matrix V and fed to a rectified layer to learn feature combination, as ing d into LSTM, we allow to encode a long-range of hand-crafted features. of the forward and the backward LSTMs. Formally, it can be calculated as where ! h t is the output of the forward LSTM, h t is the output of the backward LSTM. This method is shown schematically as in Fig. 3(b). 4.1 Parameters Pretrained word embedding . There are lots of methods for creating word embed-dings. As [4] does, we use a variant of the skip n -gram model introduced by [17], named  X  X tructured skip n -gram X , where a different set of parameters are used to predict each context word depending on its position relative to the target word. The hyperpa-rameters of the model are the same as in the skip n -gram model defined in word2vec [19]. We set the window size to 5, and use a negative sampling rate to 10. the AFP portion of English Gigaword corpus (version 5) is used as the training corpora. Hyper-Parameters . Our BI-LSTM and BI-LSTM-CRF models use two hidden lay-ers for the forward and backward LSTMs whose dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimensions of labels in BI-LSTM-CRF are set to 16.
 Parameter initialization . The learned parameters in the neural networks are ran-domly initialized with uniform samples from [ the number of rows and columns in the parameter structure. 4.2 Optimization Algorithm Parameter optimization . Parameter optimization is performed with stochastic gra-dient descent (SGD) with an initial learning rate of  X  0 =0 . 1 and a gradient clipping of 5.0. The learning rate is updated on each epoch of training as  X  t =  X  0 / (1 +  X  t ) , in which t is the number of epoch completed and the decay rate  X  =0 . 05 .
 Early Stopping . We use early stopping [7] based on performance on dev sets. The best parameters appear at around 12 epochs, according to our experiments.
 Dropout Training . To reduce overfitting, we apply the dropout method [24] to regu-larize our model. We apply dropout not only on input and output vectors of BI-LSTM, but also between different hidden layers of BI-LSTM. We observe a significant im-provement on model performances after using dropout.
 Unknown Word Handling . As described in section 3, the input layer contains a learned vector representation for the word w and a corresponding fixed pretrained vec-tor representation e w . We randomly replace the singleton word in the training data with the UNK token during training, but keep corresponding e w unchanged. This technique can deal with out-of-vocabulary words. 5.1 Settings Dataset . We conduct our experiments on the English Switchboard corpus. Following the experiment settings in [2, 9, 25], we use directory 2 and 3 in PARSED/MRG/SWBD as our training set and split directory 4 into test set, development set and others. We ex-tract the repair disfluencies according to the EDITED label in the Switchboard corpus. We also discard all the  X  X m X  and  X  X h X  tokens and merge  X  X ou know X  and  X  X  mean X  into single token. Automatic POS tags generated from pocket crf [21] are used as POS-tag in our experiments.
 Metric and Tagging Schemes . Following previous works [5, 25], token-based preci-sion (P), recall (R), and F-score (F1) are used as the evaluation metrics. We use BIESO tagging scheme in our experiments. 5.2 Effect of CRF-layer To investigate the effect of combining BI-LSTM and CRF, we build the following sys-tems:  X  CRF: a baseline system and the hand-crafted discrete features are the same with  X  BI-LSTM: model that encodes word and POS-tag with BI-LSTM and use the output  X  BI-LSTM-CRF: model that takes the same input as BI-LSTM but compute the Table 3 shows that BI-LSTM-CRF achieves significant improvements over BI-LSTM by adding CRF layer for joint decoding. This result demonstrates the necessary of han-dling the label bias problem in disfluency detection. We also note that the CRF with rich hand-crafted discrete features outperforms the BI-LSTM-CRF with only continuous neural features. A natural question that arises from this contrast is whether hand-crafted discrete features and continuous neural features can be integrated for better accuracies. We will study it in next section. 5.3 Effect of Hand-crafted Features Based on the BI-LSTM-CRF, we compare two methods of combining hand-crafted dis-crete features and continuous neural features. In Table 4,  X -HIDDEN X  means combin-ing hand-crafted discrete features with hidden outputs of BI-LSTM.  X -INPUT X  means combining discrete feature embeddings with POS-tag and word embedding to the input layer. From the result of Table 4, both the BI-LSTM-CRF-HIDDEN and the BI-LSTM-CRF-INPUT outperform the BI-LSTM-CRF by a large margin. The result confirms that hand-crafted discrete features and continuous neural features can be integrated to achieve better performance. By comparing two methods of combining hand-crafted discrete features, BI-LSTM-CRF-INPUT achieves better performance than BI-LSTM-CRF-HIDDEN. We attribute it to the fact that BI-LSTM-CRF-INPUT allow the dense and hand-crafted discrete features to interact with each other by non-linear transforma-tion and encode the discrete features of long range with LSTM at the same time. We also test the effect of hand-crafted features on BI-LSTM model and get similar results with BI-LSTM-CRF. 5.4 Final Result and Comparsion with Previous Work We compare our best model (BI-LSTM-CRF-INPUT) to four previous top performance systems. Our method outperforms state-of-the-art work and achieves a 87.1% F-score as shown in Table 5. Our model achieves 2 point improvements over UBT [25], which is the best syntax-based method for disfluency detection. The best performance by linear statistical sequence labeling methods is the semi-CRF method [5], achieving a 84.8% F1 score without leveraging prosodic features. Our model beats the semi-CRF model, obtaining 2.3 point improvements. Note that our method performs better than previous methods not only on precision , but also on recall. The comparison shows that our model is a good solution to disfluency detection. 5.5 Ablation Test To test the individual effectiveness of duplicate features and similarity features, we con-duct feature ablation experiments for the BI-LSTM-CRF-INPUT model. Table 6 shows the result. We can see that both the two kinds of features contribute to the performance improvements of disfluency detection and we can achieve a higher performance by in-tegrating all of them into BI-LSTM-CRF. This indicates that duplicate features and similarity features are both important to the BI-LSTM-CRF and they provide different kinds of information for disfluency detection.
 Most related works on disfluency detection are aimed at detecting repair type of disflu-encies. [13] proposed a TAG-based noisy channel model for disfluency detection. The TAG model was used to find rough copies. Following the work of [13], [27] extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. [21] proposed a muiti-step learning method using formed many other labeling models such as CRF model. [5] used the Semi-Markov CRF model for disfluency detection and achieved high F-score by integrating prosodic features.
 dency parsing and disfluency detection. [16] involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. [22] designed a joint model for both disfluency detection and dependency parsing. [9] presented a new joint model by extending the original transition actions with a new  X  X dit X  transition. This model achieved good performance on both disfluency detection and parsing. [25] proposed a right-to-left transition-based joint method and achieved the state-of-the-art performance compared with previous syntax-based approaches.
 with an objective that combines detection performance with minimal latency. This ap-proach achieved worse performance compared with other works for the latency con-straints. [3] used word embeddings learned by an RNN as features in a CRF classifiers. In this paper, we have explored an application of BI-LSTM-CRF networks to the task of disfluency detection. Our method explores the combination of hand-crafted discrete features and continuous neural features. Experimential result shows that our method achieves the best reported performance on the English Switchboard corpus.
 to sequence learning problem and incorporate character-based representations into the encoder model. We would also like to jointly model disfluency detection and automatic punctuation using some neural network. This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61133012 and 61370164.

