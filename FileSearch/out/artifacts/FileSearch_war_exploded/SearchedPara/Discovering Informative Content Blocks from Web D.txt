 SIGK.DD '02, July 23-26, 2002, Edmonton, Alberta, Canada. 
Copyright 2002 ACM 1-58113-567-X/02/0007...$5.00. rate of dot-corn pages is 54.68%. are tabular pages with 4.42 &lt;TABLE&gt; tags in average. As for pages of non-dot-com sites, 57.32% are tabular pages with 3 &lt;TABLE&gt; tags per page. That is, 69.59% of these pages are tabular structures in our search engine. Some pages even contain tens of tables. Intuitively, &lt;TABLE&gt; is easy and convenient to modularize an HTML page to several visualized content blocks. block, which is the unit of content to be justified as redundant or informative in this paper. Many studies on information extraction (or Web mining) also try to discover metadata from a set of Web documents [11][14]. However, they perform well only in specific sites based on the guidance of human knowledge. That is, these applications are not scalable in par with search engines. In the paper, we try to deal with the semantic and scalability problem with respect to search engines and information extraction systems. Hence, we focus on efficiently and automatically discovering informative content blocks instead of extracting the metadata of a page. In this way, our system can be effectively applied to search engines. It can also be a pre-process of information extraction since focusing on informative blocks rather than the whole page will reduce the complexity and increase the mining precision. In the following section of the paper, we first describe related studies. Then, we illustrate the representation of content blocks in a page, and propose a method to evaluate the information measure of a content block. Based on the information measure, we use the greedy approach to dynamically divide content blocks into either informative or redundant. Regarding the hand-coding data of NSE as the answer set, we perform several experiments to evaluate the effectiveness of our proposed method. Experiments indicate our method is perfect to discover informative content blocks from tabular pages. Finally, we conclude our contributions. This study is proposed to deal with the problem of intra-page redundancy that causes search engines to index redundant contents and retrieve non-relevant results. The problem also affects Web miners since they extract patterns from the whole document rather than the informative content. Thus, we illustrate studies of both fields. In the rest of the paper, for better understanding, we use information retrieval (IR) systems to denote search engines and information extraction (IE) systems to denote Web or text miners. Many IR systems have been implemented to automatically gather, process, index, and analyze the Web documents for serving users' information needs. IR systems (or search engines) can be divided into three automatic processes: preprocessing (crawling), indexing, and searching. In the crawling phase, the Web crawler grabs a page and its related pages by following hyperlinks of the page. It also parses contents of the page based on HTML or other markup language like XML. Then, the index engine processes and stores the parsed content as the page's index files or database indexes, which makes the following search requirements to be efficiently matched with indexed documents and retrieved in the relevant due to the fast growth of Web documents. BY analyzing the hyperlink structure of the Web, the two best-known algorithms, HITS [12] and PageRank [2], were proposed to cope with the problem. PageRank is successfully used in the Google search engine [3]. As indicated in the appendix described in [2], the ranking result will be inherently biased toward advertising pages and away from the needs of users since all search engines index the whole page content without considering the semantics of content. In fact, HITS algorithm does not give a concise Web structure, due to the many semantically redundant hyperlinks in pages. Obviously, redundant contents, such as advertisements, company loges, navigation panels, relative channels, and privacy statements, are indexed so that they will probably be retrieved. Consequently, IR systems are scalable applications, but they require automatic processes to find meaningful contents for indexing and for improving the precision of retrieval. IE systems [10] [I 1] [14] [22] have the goal of transforming a collection of documents, usually with the help of IR systems, into information that is more readily digested and analyzed [8]. In par with IR systems that retrieve relevant documents, IE systems aim to extract the structure or representation of a document. There are basically two types of IE: IE ffi'om unstructured texts and IE from semi-structured documents [13]. The former, called text mining, typically integrate with NLP works to extract the information from unstructured text. With the increasing popularity of the Web, text mining studies were shifted to the structural IE research called Web mining. Wrapper [14] and SoflMealy [11] are well known systems that extract the structural information from Web HTML documents based on manually generated templates or examples. Cardie [2] defines five pipelined processes for an IE system: tokenization and tagging, sentence analysis, extraction, merging, and template generation. SRI's FASTUS [ 1] is based on a cascade of six finite-state transducers, which are similar to that of Cardie. Machine learning is usually applied to learn, generalize, and generate rules in the last three processes. However, the domain-specific knowledge such as concept dictionaries and templates for generating rules are necessary to be manually generated. Training instances applied to learning processes are also artificially selected and labeled. For example, text miners usually learn wrapper rules from labeled training tuples. In Wrapper induction [14], the author manually defines six wrapper classes, which consist of knowledge to extract data by recognizing delimiters to match one or more of the classes. The richer a wrapper class, the more probable it will work with any new site [6]. SoftMealy [11] provides a GUI that allows a user to open a Web site, define the attributes and label the tuples in the Web page. The common disadvantages of IE systems are the cost of templates, domain-dependent NLP knowledge, or annotations of corpora generated by hand. This is why these systems are merely applied to specific Web applications, which extract the structural information from pages of specific Web sites or pages generated by CGI. Consequently, IE systems are not scalable and therefore cannot be applied to resolve the semantic deficit of search engines. In this paper, we propose an approach to discover informative contents of pages to cope with the problem of intra-page redundancy in IR systems. Also, IE systems will become more efficient by extracting structures from informative contents instead of the whole page. Our system tries to extract informative contents from Web documents (or other types of documents) to improve the precision and efficiency oflR and IE systems. Pages written in HTML are the majority in the ever-increasing Web, even though XML was proposed for several years. W3C's Document Object Model (DOM) [19] defines a tree structure for HTML [20] and XML [21 ] documents, in which tags are internal nodes of the tree, and texts or hyperlinks to other trees are leaf nodes. According to counts referred to in the Introduction, about 70% of all Web pages use HTML tag &lt;TABLE&gt;. To reduce the complexity, we concentrate on HTML documents with &lt;TABLE&gt; left-and-fight ordering in the tree. of the tree. 
Figure 1: The processes of InfoDiscoverer. 
Figure 2: Extracting content blocks with text strings. the matrix. The following is Shannon's famous general formula for uncertainty [ 17]: By normalizing the weight of the feature to be [0, 1], the feature entropy is: H(I~) = - X  we log, w~j , where we is the weight of Fj in document D r To normalize the entropy value to the range [0, 1], the base of logarithm is the number of documents, and the above equation is modified as: For the example of Figure 3, there are N pages with five content blocks in each page. Features FI to F10 appear in one or more pages according to the figure. The layout is widely used in dot-tom Web sites with the logo of a company on the top, followed by advertisement banners or texts, navigation panels on the left, informative content on the right, and its copyright policy at the bottom. Without losing generality, we consider only two pages in this figure and the feature entropy is calculated as follows. H(F~) = -X--log, 1 = H(F2 ) = H(F;) = H(F4) = H(F,) = H(F,) =1 H(F,) = -llog, 1 -0log, 0 = H(F,) = H(F,) = H(F,o) = 0 By instinct, feature entropies contribute to the semantic measure of a content block that owns these features. I.e. the entropy value of a content block is the summation of its features entropies, as shown in the following equation. H(CB~) = ~H(Fj) , where Fj is a feature of CB I with k features Since content blocks contain different numbers of features, the equation is normalized as: That is, the entropy of a content block, H(CB), is the average of all feature entropies in the block. It is feasible to assume that the average number of content blocks in a page is constant. Undoubtedly, the time complexity is o ( [ D [ ) for calculating the entropy value of each content block. The accumulated complexity is still O( ] D I niogn) up to this stage. Based on H(CB), the content block can be divided into two categories: redundant and informative.  X  If H(CB) is higher than a defined threshold or close to 1, the .......... ...... /-/-/-------------..................... b ..... ......... ~/;" ............. "I .............. --...... ----] 
Figure 5: Recall rate of each page cluster. rZ;;.Zi ....... " .......... ................... ............. ;; -.,-................. ~'~ .=:.o%'~ .... Figure 6: Precision rate of each page cluster. Table 2: Recall and precision at optimal threshold of H(CB) 
Site Optimal H(CB) 1Thome 0.7 ET 0.2 FTV O.4 CNet 0.5 TSS 0.3 CDN 0.5 TVBS ' 0.1 CTV 0.2 CAN 0.7 UDN 0.7 CTimes 0.4 CTS 0.5 Trimes 0.7 recall rate is not affected because higher threshold means more features included. According to previous experiments, we can conclude that our proposed methods are feasible to discover informative contents from Web pages of the same site. The greedy approach of InfoDiscoverer is adaptive to find the optimal threshold of block entropy for different Web sites with different templates. Based on this approach, the optimal threshold of informative content blocks is dynamically selected for different sites. The result shows that both recall and precision rates are larger than 0.956, which is very close to the hand-coding result. Obviously, the experiment results prove contributions to our news search engine since InfoDiscoverer knows how to automatically extract informative contents, i.e. news articles, from news Web pages. Evidently, it can be ~ipplied to general Web IR systems (search engines) by reducing the size of index and increasing the precision of retrieval. The complexity of the discovering process is polynomial. Most important, intermediate results, such as the page representation and keywords with weights, generated by the InfoDiscoverer are shared with the crawler and indexer of Web IR systems. Applying InfoDiscoverer to Web IE systems is also efficient as these systems simply consider smaller informative content blocks instead of the whole page content. Thus, InfoDiseoverer can be the preprocessor Web IR and IE systems. The proposed method is applied to tabular Web pages and based on the assumption of knowing page clusters. Experiments are merely evaluated for Chinese pages published by news Web sites. In the future, we will develop other methods to automatically discover page dusters from Web sites. To make our method be applicable to general Web pages instead of restricting to tabular pages, we will apply generalization and specialization processes to merge or split content blocks based on HTML document object model. Also, evaluations on English Web pages are important to polish our method. [1] Bear, J., Israel D., Petit, J., and Martin, D., "Using [l] Blahut, R. E., "Principles and Practice of Information 
