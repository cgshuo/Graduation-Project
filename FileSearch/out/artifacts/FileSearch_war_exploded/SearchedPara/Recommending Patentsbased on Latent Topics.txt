 The availability of large volumes of granted patents and ap-plications, all publicly available on the Web, enables the use of sophisticated text mining and information retrieval meth-ods to facilitate access and analysis of patents. In this pa-per we investigate techniques to automatically recommend patents given a query patent. This task is critical for a va-riety of patent-related analysis problems such as finding rel-evant citations, research of relevant prior art, and infringe-ment analysis. We investigate the use of latent Dirichlet allocation and Dirichlet multinomial regression to represent patent documents and to compute similarity scores. We compare our methods with state-of-the-art document rep-resentations and retrieval techniques and demonstrate the effectiveness of our approach on a collection of US patent publications.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models, Selection Process, Search Process Citation Recommendation, Patent Retrieval, Document Rank-ing, Topic Models, Language Models
Millions of patent documents are publicly available elec-tronically. Analyzing these documents can help in our un-derstanding of technological progress, the evolution of new language and terms, and the emergence of new products. Tools for automated patent analysis also have direct bene-fits to inventors in terms of finding relevant prior work, for companies wishing to patent new products or ideas, and for patent examiners in deciding which patents to grant.
Being able to efficiently and accurately search large patent databases has been a problem for decades. For example in the early 1950 X  X , a mechanized way of finding relevant patents based on punch cards was developed [ 3]. Since then, information retrieval techniques have been used to provide more assistance in patent search, allowing for searching of large databases in seconds across multiple languages [ 15 ]. Systems that support this type of keyword-based search include both commercial systems such as Dialog or Lexis-Nexis 1 , as well as systems used in patent offices 2 . Despite this progress, searching patent databases, either by inven-tors, lawyers, patent examiners, or analysts, still heavily re-lies on Boolean keyword queries [ 2] and often requires con-siderable legal expertise and domain knowledge.

In the context of patent retrieval, different types of search scenarios are relevant depending on the specific tasks that occur at different stages in the life of a patent [ 2, 5]:
Across all of these tasks, finding patents that are simi-lar to the query patent is a crucial component of the prob-lem. For example, accurately recommending relevant cita-tions could save significant time and resources. In contrast to recommending scientific articles, the problem of automat-ically recommending patents poses some distinct challenges. For example, language usage in patents tends to be more idiosyncratic and less consistent compared to technical lan-guage usage in scientific articles [ 1]. Furthermore, the cita-tion graph for patents conveys different information than in the case of scientific articles. For example, patent citations do not occur within the text of the patent but are provided as a separate list, often consisting of both inventor-generated and examiner-generated sublists. http://www.dialog.com , http://www.lexisnexis.com e.g. USPTO http://www.uspto.gov
We address the problem of finding related or similar patents by investigating a combination of (1) latent Dirichlet alloca-tion (LDA) [4 ], (2) Dirichlet multinomial regression (DMR) [14 ], and (3) language models [6 ]. The combination of topic mod-els and language models, especially when used for document smoothing, has been shown to be beneficial in prior work on document ranking (e.g. [18 ]). Latent topics can capture con-cepts and general terms, whereas language models are useful for capturing specific terms and details [ 10]. In the following we investigate the use of combinations of these models for the specific problem of patent retrieval.

The problem we address is as follows: given a granted patent q , return a ranked list of patents and patent applica-tions R = { d 1 ,d 2 ,...,d n } , all published before the applica-tion date of q , ordered by the degree of similarity to q . The highest ranked patents d i are considered to be most similar to q .
 Figure 1 shows the overview of our system X  X  architecture. In a manner similar to that of Mase et al. [ 13 ], we use a two-stage approach; first generating a list of candidates and then using different ranking functions to determine the most similar patents.

The first stage consists of two steps: 1. To find the candidates we use a search engine to deter-2. We use the top N terms (with N =30)of the summary In the second stage, after the generation of the candidate set in Step 2, we re-rank the candidate patent documents using combinations of language models and topic models. To provide an example of the number of patents returned in a candidate set, when using K =500 in step 1, an average candidate set produced from step 2 consists of around 2,700 patents.

We compare our algorithms with two baseline algorithms: (1) BM25 for long queries [12 ] and (2) language models (LM) [6 ]. To compute similarity scores for the test patent relative to each candidate we treat the test document as the query q and the candidates from our two-step candidate selection process as our document collection.

For BM25 the similarity score for each candidate d with test document q is computed as: BM25 d = X where N c is the number of documents in the collection, df is the document frequency of term w , and L is the length of document d divided by the average document length for all documents in the collection. tf wd is the term frequency of term w in document d . The tuning parameters k 1 , k 3 , and b are set to 1 . 5,1 . 5, and 0 . 75 respectively, as suggested in [12 ]. For LM we use a Dirichlet smoothed language model [ 19 ]: P lm ( w | d )= N d P ml ( w | d ) is the maximum likelihood estimate of word w in document d , and P ml ( w | c ) is the maximum likelihood estimate of word w in the entire collection c . N d is the number of tokens in document d and  X  =500.
We follow Wei and Croft [ 16], who proposed an LDA-based document model for ad-hoc retrieval where they used topics to smooth a language model representation. To find related documents for a given patent, we compute a similar-ity score defined as the probability of a query patent q given document d : where z is a latent topic and  X   X  and  X   X  are posterior estimates of  X  and  X   X  X he inferred topic and word distributions. We use collapsed Gibbs sampling [9 ] to infer  X  and  X  given the observed words, the model, and the priors.
The number of topics N z , as well as  X  and  X  , are set beforehand. We achieved best results when choosing N z to be collection. We also used 50 /N z for  X  and 200 /N v for  X  , with N v being the vocabulary size.

We combine topic and language models by using the latent topics to smooth the language models [16 ]. The similarity score between the test patent and document d can then be computed as: P lm ( w | d ) is the smoothed language model and P lda ( w | d ) can be computed using the inferred topic and word distributions as defined earlier.  X  is set to 0 . 3 as suggested in [ 16 ] (see Section 3.3 for a discussion of how this parameter influences the retrieval results).
Mimno and McCallum [14 ] proposed Dirichlet multino-mial regression (DMR), which extends the LDA model to allow the topics to be conditioned on arbitrary features. We can use DMR to model different topical content across dif-ferent patent sections (abstract, claims, etc.).

The language used in a patent document can vary sig-nificantly depending on the section. Different sections can be written by different individuals, e.g. the details of an invention by engineers and the claims by an attorney. By treating sections individually we hope to gain more coher-ent topics without losing the overall topical context of the patent document as a whole.

In the DMR model the document-topic prior  X  is a func-tion of the observed document features encoded in a vector x . A 1 in x indicates the presence of a feature in document d and a 0 indicates its absence.

Here we split the patent text into its four sections: ti-tle+abstract , claims , summary , and details . We model each section as an individual document with two types of meta-data information: patent id and section id, where the ids are presented as binary features in the DMR model. The similarity score is then computed as: To get P dmr ( w | d ) we combine the topic models for the dif-ferent sections s . N s is the number of words in section s ; and N d is the number of words in the whole patent d . The topic models computed in this way can be used to smooth the language models in a manner analogous to the LDA models: We use the same setting as for LM-LDA with  X  =0 . 3.
To evaluate our proposed methods, we use each algorithm to rank patents in terms of similarity to a query patent q , where the set that is ranked is (a) restricted to patents or applications published prior to the date of publication of q , and (b) restricted to the candidates generated by the two-stage candidate set generation process described earlier. We use the citations in a patent q as a proxy for ground truth in terms of determining a set of patents that are relevant to q . This strategy was also used in [17 ] and in the NTCIR workshop series.

There are a number of drawbacks to using existing cita-tions in a patent as a measure to evaluate patent similarity algorithms. For example, it is quite common in practice that truly relevant patents are not cited, potentially deliberately. In addition, if there exists a published patent application and a granted patent for the same application, only one or the other might have been cited. However, since these limi-tations affect all of the algorithms, it is reasonable to argue that a patent X  X  citations provide a useful surrogate ground truth by which to compare and evaluate algorithms in the context of patent retrieval.
For testing, we randomly picked 100 granted utility patents published on November 13th, 2012 by the USPTO 4 . For each of these 100 test patents we selected the top-K =500 patents found by the search engine as candidates (see step one in Figure 1). We expanded these candidate sets by adding the references as described in step two in Figure 1. This resulted in approximately 275,000 patents and patent applications for the 100 test patents in total. For each test patent, the average number of references to US patents published after 1975 was 25.7, of which 9.9 on average were added by the patent examiner. In addition each test patent had on av-erage 4.5 references to non-US patents, old US patents, or other non-patent literature. An average of 18.7 references were present in the citation candidate set whose average size per test patent was 2,772.
The US patents cited in the test patents act as our ground truth set. We discard references to non-US patents and non-patent literature. For computation of precision and recall we also do not include references to US patents that are not among the candidates and thus can not be retrieved and ranked by our algorithms.

For each test patent q we generate a ranking of patents to be cited using all the candidates for that patent. We use mean average precision (MAP) [ 12] to compare the rankings R i generated by different methods for all test patents q  X  Q : where R jk is the set of ranked results from the top of the list down to item k in the list, and the set of relevant items is { i 1 ...i m j } . If no relevant document is retrieved, precision is taken to be 0. http://research.nii.ac.jp/ntcir/index-en.html
USPTO patents are publicly available at http://www. google.com/googlebooks/uspto-patents.html The main results from our experiments are summarized in Table 1. We report MAP for the total number of citations and show in the third column the performance difference as a percentage relative to the language model . It is clear from these results that the combination of language model repre-sentations with topic model representations (either LDA or DMR) outperforms the individual methods, with LM-LDA yielding the best results. The last two columns in Table 1 show MAP scores considering only the references added by the patent examiner or by others (usually the patent au-thor), respectively. Note that LM on its own performs as well or better than LDA and DMR on their own, for refer-ences cited by the examiner, whereas this is not true for ref-erences cited by others. (The combination of LM with LDA or DMR is still best overall on both sets). This suggests that references added by examiners tend to contain language that more closely matches the language in the original patent, relative to the references added by non-examiners. This could be due to the fact that patent examiners make more extensive use of keyword-based search, compared to non-examiners, when determining which prior patents should be cited.
In this paper we investigated the application of language modeling and topic modeling to the problem of patent re-trieval given a query patent. We conducted experiments on a subset of the USPTO patent corpus using patent citations as ground truth. We found that language models provide systematic improvements in terms of precision and recall over simpler methods such as BM25. Furthermore, our ex-periments showed that the combination of topic modeling and language modeling provides further significant improve-ments in performance over either alone. Additional improve-ments could potentially be gained by incorporating meta-information such as the category information for a patent or citation graphs [ 7]. Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Busi-ness Center contract number D11PC20155. The U.S. gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official poli-cies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. government. [1] K. H. Atkinson. Toward a more rational patent search [2] L. Azzopardi, W. Vanderbauwhede, and H. Joho.
 [3] M. Bailey, B. Lanham, and J. Leibowitz. Mechanized [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] D. Bonino, A. Ciaramella, and F. Corno. Review of [6] W. B. Croft and J. Lafferty. Language Modeling for [7] A. Fujii. Enhancing patent retrieval by citation [8] S. Fujita. Technology survey and invalidity search: A [9] T. L. Griffiths and M. Steyvers. Finding scientific [10] R. Krestel and P. Fankhauser. Personalized [11] R. J. Mann and M. Underweiser. A new look at patent [12] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [13] H. Mase, T. Matsubayashi, Y. Ogawa, M. Iwayama, [14] D. M. Mimno and A. McCallum. Topic models [15] F. Saad and A. N  X  urnberger. Overview of prior-art [16] X. Wei and W. B. Croft. Lda-based document models [17] X. Xue and W. B. Croft. Automatic query generation [18] X. Yi and J. Allan. A comparative study of utilizing [19] C. Zhai and J. Lafferty. A study of smoothing
