 Most research in speeding up text mining involves algorithmic improvements to induction algorithms, and yet for many large scale applications, such as cla ssifying or indexing large document repositories, the time spent extracting word features from texts can itself greatly exceed the initial training time. This paper describes a fast method for text feature extraction that folds together Unicode conversion, fo rced lowercasing, word boundary detection, and string hash computa tion. We show empirically that our integer hash features result in classifiers with equivalent statistical performance to those built using string word features, but require far less com putation and less memory. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing. I.5.2 [ Pattern Recognition ]: Design Methodology X  feature evaluation and selection. Algorithms, Measurement, Perfo rmance, Experimentation. text mining, text indexing, bag-of-words, feature engineering, feature extraction, document categorization, text tokenization. Most text analysis X  X uch as document classification or clustering X  X ncludes a step of text feature extraction to determine the words or terms that occur in each document. This step is straightforward, runs in linear time, and is generally not a topic of study. Research towards fast and scalable methods tends to focus on algorithmic issues such as model induction, typically O(N the number of training cases. By contrast, scoring documents in production with a learned model is merely O(N) X  X et for a different meaning of N, the number of production documents, which can be very large for enterprise-scale applications or when analyzing the world X  X  web page s. The scoring time (aka testing time ) for large scale deployment can easily dominate the induction time, especially fo r the common case where it is difficult to obtain a large quantity of training examples labeled by domain experts. An example woul d be an Information Lifecycle Management (ILM) applicati on that periodically applies classifiers to huge document repos itories for content management, such as the automatic application of file retention, archiving and security policies. Our recent res earch in this space found that the time to extract the words from a text file can be roughly on par with the time to fetch the file from a local disk [4]. Finally, full-text indexing also must perform text feature extraction on large volumes of files or web pages. Hence, text feature extraction can take considerable computational resources for large scale systems. The tremendous increase of online te xt annually, together with the proliferation of large scale text analysis applications, yields a valuable opportunity to speed up th e core text scanning subroutine that is so ubiquitous. We dem onstrate a way to perform this scanning step up to two orders of magnitude faster, with little or no effect on the quality of the text analysis. The method, which includes character table lookup and hashing in its inner loop, inexpensively supports forced lo wercasing, multi-word phrases, and either word counts or boolean features, as desired. It can also supplant common Unicode conversi on, eliminating some buffer copying and character expansion. We demonstrate its use and speedup in applying multiple classi fiers to texts, without having to write out different feature vectors for different feature-selected classifiers. The method yields word and phrase features represented as hash integers rather than as strings. The obvious use for faster feature extraction is to process more text per second, run more classifi ers per second, or require fewer servers to handle the text processing load. Alternately, where the rate of text per second is limited, the benefit may be to lower the impact on a user X  X  machine, e.g. where text analysis agents operate constantly in the background to  X  X nderstand X  the texts the user is reading or writing .
 The following section describe s both baseline and proposed methods for text feature extraction. Section 3 provides an empirical comparison for speed and classification accuracy. Section 4 analyzes the collision behavior and memory requirements. Section 5 discusses practical matters for real-world use: the effect for end-users and an extension of our method that folds in the processing of text encoded using Unicode UTF-8 at little incremental cost. Section 6 places this work in the broader context of related work. Section 7 concludes with future work. We begin by laying out the fundamentals of text feature extraction and describe a straight forward baseline method. After this, we describe the steps of our method. An extension for processing Unicode UTF-8 texts is included in Section 5.2. Text feature extraction depends on some definition of which characters are to be treated as word characters vs. non-word characters. Besides the lette rs A X  X  (in both upper and lower cases variants), word characters may also include accented letters, digits, the underscore, and generally all Unicode characters having one of the Unicode  X  X etter X  general categories (uppercase, lowercase, titlecase, modifier, or other), depending on the application. Let the boolean function isWord(char) determine the status of any given character. Fo r this paper, we take the common approach that a maximal sequence of adjacent word characters in the text stream constitutes a word. Typical text processing applications normalize the capitalization of each word by forcing each character to lowercase. Let this conversion be determined by the character function toLowerCase(char). The mapping is non-trivial for some Unicode letters. Additionally, if undersco res or digits are included among the word characters, then their lower case mapping is the identity function. For indexing applications, one needs to determine the sequence of each word that appears, but for text classification or clustering applications, one typically dis tills the text to the well-known bag-of-words as the feature vector repr esentation X  X or each word, the number of times that it occurs, or , for some situations, a boolean value indicating whether or not the word occurs. The feature vector needed by a classifier depends only on the words that occurred in the training set, and in some cases, may use only a subset of these. Feature selecti on methods may be used to select a subset of the most predictive words in order to improve the accuracy of the trained classifier [3]. This selection depends on the class labels of the training set, and if multiple classifiers are to be applied to the same text, then each classifier may require a different selection of features for its feature vector. Text classifiers can often be made more accurate if they also include features that represent word phrases, the most benefit coming from 2-word phrases (aka bi-grams) with diminishing returns for longer phrases [11]. By including word phrases, the dictionary of potential terms is greatly increased, emphasizing the need to select only the most useful terms for the classification task at hand. It is not uncommon fo r a training set to contain hundreds of thousands of distinct terms and for the final classifier to operate with a subset of, say, 1,000 of the best terms. Table 1 presents pseudo-code that performs text word extraction for boolean features in a straightforward manner. Although many variations and minor optimiza tions are possible, the common thread of all baseline methods is to gather the contiguous sequence of word characters together in an accumulator buffer (line 5) until the end of the word is encountered, and then to register the word String with some feature vector representation (line 8). This pseudo-code computes the set of words, but if instead one required the bag-of-words, then the appropriate hash table would be a HashMap&lt;String,Integer&gt; that maps each occurring word to the number of times it occurred (o r to a position in an array of counters for faster incrementing). Note that to add a word of type String to a hash table of words w ill require a hash function to be computed over the String that is us ed as an index into the hash table array, and may require at least one String-vs-String comparison in order to verify that the correct hash table entry has been located or to differentiate this word from other words whose hash representations collide in the hash table representation. We are able to eliminate this overhead with our method. Table 2 presents equivalent pse udo-code for the basic skeleton of our method, which we call SpeedyFx. In structure it appears much the same as before, but with two key differences. Foremost is that the current word accumulator is represented by an integer hash (line 2 &amp; 6) instead of a String. (We analyze the strength of our chosen hash function in S ection 4.1.) The second major difference is that all function calls have been removed from the inner text processing loop. A lookup (line 4) into a precomputed table replaces two much slower function calls: isWord(char) and toLowerCase(char). Even if one could get these functions compiled inline, their joint processing overhead would greatly exceed a single table lookup. And the many word occurrences are simply overwritten to the boolean feature array (line 9), avoiding many calls to the hash table add () function inside the loop. Note that there is no need to check whether the particular word hash has already been noted in the feature array. (In systems that want a count of each word, the array can be an array of integers rather than an array of booleans.) In practice, N will be chosen to be a power of two, allowing the modulus operation (line 9) to be replaced by a simple bitwise AND. HashSet&lt;String&gt; extractWordSet(text): 1 HashSet&lt;String&gt; fv = new HashSet&lt;String&gt;(); 2 StringBuilder word = new StringBuilder(); 3 foreach (char ch: text) { 4 if (isWord(ch)) { 5 word.append(toLowerCase(ch)); 6 } else { 7 if (word.size() &gt; 0) { 8 fv.add(word.toString()); 9 word.clear(); 10 } 11 } 12 } 13 return fv; boolean[] extractWordSet(text): 1 boolean fv[] = new boolean[N]; 2 int wordhash = 0; 3 foreach (byte ch: text) { 4 int code = codetable[ch]; 5 if (code != 0) {// isWord 6 wordhash = (wordhash&gt;&gt;1) + code; 7 } else { 8 if (wordhash != 0) { 9 fv[wordhash % N] = 1; 10 wordhash = 0; 11 } 12 } 13 } 1 4 r eturn fv; If a more compact representation of the resulting feature vector is required, a quick, linear scan of the boolean array can be used to insert each occurring word hash into a hash table or into an array containing the word hashes found. In many cases, though, features are extracted in order to be used immediately for some task, an in such cases the boolean array representation is often sufficient. A reasonably large bits array of size N=2 20 is suggested. Larger values of N result in a greater memory demand momentarily for the bits array, whereas smaller values of N effectively restrict the size of the hash space, conflating features that have different 32-bit integer hash values together into a smaller space. Using N=2 is easily sufficient for corpora or training sets with hundreds of thousands of unique terms. For perspective, a corpus of 804,414 Reuters news articles contains 47,236 unique words, and a corpus of 62,369 Arxiv.org astrophysics papers contains 99,757 unique words [7]. Regarding the memory demands, most compilers will allocate one byte per boolean for good performance, rather than packing them tightly into bits. Thus, for N=2 20 bits table is 1 MB, which is a fraction of the L2 cache for modern CPU chips. If instead one needs to track the word count for the bag of words representation, this array can instead be used to contain 8-bit or 16-bit word frequency counters. The precomputed code table actually serves four different functions simultaneously: word character determination, lowercasing, hash randomization, and, as will be introduced later in Section 5.2, Unicode processi ng. Table 3 shows pseudo-code to prepare the table appropriate for 8-bit character streams. For any character that is not a word character, the table contains zero. For word characters, it maps the character to its lowercase equivalent and maps the result a distinct fixed random integer code. This randomization subs tantially improves the hashing function over just using the lowercase character code directly, as the Java String.hashCode() method does. (For reference, Java Strings use multiplicative hashing [9], computing their hashes by iterating hash = 31* hash + char over their characters). It should be noted that while in our tests our word characters were letters and numbers an our normaliza tion is lowercasing, this is all embodied in the creation of the code table. Any other set of rules could be used by providing a differe nt table. The only restriction is that the mapped and normalized character (if any) be decidable from a single character. This can be relaxed, but the mechanisms for doing so are beyond the scope of this paper. Our hashing function, which we call Mapped Additive Shift Hashing (MASH), consists of a simple arithmetic right shift, a table lookup (from a very small table), and an addition. As such, it is very efficient and easy fo r a compiler to optimize. Although somewhat non-intuitive, it actually produces substantially better hashes than using a left-shift. The reason is that we end up using only the low order bits of the resulting word hash (line 9), and if a left-shift were used instead, then these lowest order bits would be largely unaffected by the first characters of the word, especially for a longer word. (In the case in which the modulus is a power of two for efficiency, they would be completely unaffected by characters can contribute to the hash. By using a right-shift, on the other hand, and by ensuring that each character contributes 32 bits to the value, a character X  X  influence continues (in a degrading fashion) for at least 31 following characters. Even for the occasional word longer than 32 characters, note that we use a continues to have some effect on the hash. Of course, this influence does diminish, and so MASH is certainly not suitable for hashing long texts, which is the goal of typical hash function designs such as SHA1, MD5 and Rabin hashing, which are each much more involved to compute. If one wishes to also produce features for each phrase bi-gram, then at the moment when the hash is registered (line 9), we can also register a phrase hash consisting of the current hash combined with the previous word hash, using a symmetric combination (such as XOR or addition) if it is desired to have features that are independent of the order of the words or an asymmetric combination (such as subtraction or XOR with a shifted value) if order is to be preserved. Working in integer space, such combination is much more efficient than working in string space, in which typically a new string must be constructed (or at least a new heap-based object that refers to the two strings). So far the discussion has primarily focused on the use case of extracting a bag or set of words, which is needed for use in classification or clustering applica tions. But the speed benefits of our method can also be leveraged for the widespread use case of full-text indexing of large docum ent repositories (either large documents or many of them or bot h). In this case, the indexing engine needs to extract the sequence of words/terms in a text, and build reverse indices of them for later information retrieval. The key difference for our features would be that the term features are integers rather than Strings. Wh en later a query is submitted, the words that the user enters are submitted to the exact same hash processing, yielding the same inte ger hashes that the text has previously been indexed by. Note that for this usage, one can use the full 32-bit integer hash space (i.e. without the modulo N and the bit array on lines 1 and 9 of Table 2). In this section we present a series of experiments to evaluate the speed and other qualities of our method compared with various baselines. In all cases our compiler is Java JDK 1.6. Where speed measurements are concerned, the measurements were performed on a 2.4 GHz Intel  X  Xeon X  CPU, which is a 64-bit architecture, running a version of Linux derived from Red Hat version 4. Measuring elapsed tim e for performance can be quite tricky, since other processes and threads regularly interrupt processing for uncontrolled amounts of time, occasionally slowing down the overall or averag e measurement substantially. For this reason, we measure the throughput for each test over 50 separate iterations and use the m easurement that shows the best performance, which excludes non-essential delays for thread interruptions. To exclude the impact and variability of disk transfer speeds, we cache the file bytes in memory before beginning the series of time measurements. prepTable(): 1 int rand[256] = 256 fixed random integers; 2 foreach (ch: 0..255) { 3 codetable[ch] = isWord(ch) ? 4 rand[toLowerCase(ch)] : 0; 5 } We use two sources for publicly available benchmark texts. To measure speed for tokenization, we use the text of Leo Tolstoy X  X  War and Peace , which is 3.1 MB in ASCII and is available from Project Gutenberg. 1 To measure speed over feature extraction and classification, we use th e popular 20 Newsgroups dataset, which has roughly 1000 Usenet postings for each of 20 categories. 2 We use the  X  X ydate X  version, which has had duplicates and some headers removed and which prescribes a specific train/test split. The dataset contains 34.2 MB of mostly-ASCII text in 18,846 documents. Thus the average document is about 1.9 KB. First we measure the raw speed of text tokenization. For this measurement, we simply produce th e sequence of word Strings or word hash integers that occur in the text and discard them without further processing. This is the most basic processing skeleton that needs to be computed regardless of the text analysis application. For example, in a text indexing system, this sequence of terms hash, document ID&gt; pairs. Table 4 shows the processing speed for a variety of methods measured on the War and Peace text. The first line shows our best processing rate of 136.2 MB/second using SpeedyFx with MASH and calling down to a user-provided function on each word hash found. The same speed was attained using an inlined version of Java X  X  hash algorithm. When Rabin fingerprinting was substituted, the performance dropped to 111.9 MB/second.. Typically tokenization is performed by using an iterator-like class that returns successive tokens each time a method is called. When our algorithm was embodied in such a class, returning a word hash each time, the performance dropped to 116.0 MB/second using MASH and Ja va X  X  hashing and to 87.0 MB/second using Rabin fingerprint s. Since both approaches entail a single method call per discovered word, this speedup of approximately 17% appears to imply that the Java compiler is more aggressive about inlining ca lls to methods within the same object for (necessarily final) anonym ous classes than it is to calls of objects of different classes. All of these numbers compare very favorably with existing baseline approaches to tokeniza tion. Java provides a String-Tokenizer class, which takes a string and a string containing delimiter characters (characters to not be considered part of any word) and provides hasMoreTokens () and nextToken() methods. Besides the overhead of the extra call to find out whether more words are available, this suffers from several problems common to many approaches we looked at. work directly from a byte array. This involves converting bytes to characters and creating a string from the characters, both of which imply copying the data. Second, each of the produced tokens needs to be created as a String object. 3 And third, any Version 11, http://www.gutenberg.org/dirs/etext01/wrnpc11.txt. 
The size includes Project Gutenberg boilerplate text. In the case of the StringTokenizer class, the tokens are substrings of the input string, which are themselves small, but normalization (in this case lowercasing) needs to be done externally on each produced string. In addition, the StringTokenizer class searches in its delimiter string for each character it sees. The class is designed for use with a small set of delimiters (e.g., commas and tabs), but for our purposes, we need to include all non-alphanumeric char acters, and so this check is quite slow. The net result is that StringTokenizer processed War and Peace at a rate of only 4.1 MB/second, just 3% that of SpeedyFx. A slightly more efficient approach is to use a regular expression, in the form of a Java Pattern object matching all Unicode non-alphanumeric characters, to split each line and then lowercase each word.. This processed War and Peace at 11.1 MB/second, or 8% of SpeedyFx. Another approach is to use classes supplied by a library. The Lucene v2.2 library [6] provides two methods. The most commonly used of the two is their StandardAnalyzer class, but it is not strictly an apples-to-apples comparison, since that class incorporates a full lexical analy zer, which attempts to recognize more complex objects, such as e-mail addresses and hostnames, which ours cannot. On War and Peace , the StandardAnalyzer performed at 1.9 MB/second, 1% of SpeedyFx. Lucene also provides set of Tokenizer classes to allow the programmer to specify the rules directly, and a CharTokenizer subclass that defines tokens as se quences of allowable characters, arbitrarily normalized. We tested a subclass that allows alphanumeric characters and normalizes by lowercasing. Unlike tokenizers that require strings as input, this can read directly from a character stream and so performs much better, processing War and Peace at a rate of 25.7 MB/second (19%). The popular Weka v3.4 [15] libra ry contains a class StringTo-WordVector to convert a dataset with String attributes into bag-of-words features. At its core , it uses its own text tokenizer AlphabeticStringTokenizer, which has specialized code for recognizing sequences of (only) ASCII alphabetic characters. When put in a framework that processes a stream and does lowercasing of the words it finds, it processes War and Peace at a rate of 21.8 MB/second (16%). which refer to the larger string. This would appear to have severe memory implications if the tokens are stored in long-lived structures, as they would prevent the memory for the full text string from being garbage collected. One question that arises is the extent to which the speed-up we see is due to the table-driven appr oach rather than the decision to constructed a tokenizer that used the same tables as SpeedyFx but which constructed a string rather than computing a hash. This processed War and Peace at 64.5 MB/second, 47% of the rate at which the full algorithm worked. A similar approach, suggested by a reviewer, is to use pre-computed arrays in place of the isWord() and toLowerCase() func tions (to remove the function call overhead) and to yield the hash of the constructed string rather than the string itself. This approach processed War and Peace at 31.6 MB/second, 26% of the rate of the full algorithm. Next, we measure the text processing speed for text feature extraction, i.e. to determine the complete set or bag of terms in a text, which is a task typically performed during clustering analysis and classification, esp ecially during the training phase (i.e. before any feature selection has been performed). For these tests, we use all 18,846 articles of the 20 Newsgroups benchmark and measure the time it takes to extract a representation of the set of words contained in each article, where a word is, as before, considered to be the lowercase version of a maximal sequence of alphanumeric characters (or the closest that can be approximated using a given method). Over and above the tokenization task, this task adds the problems of (1) recognizing that a word has already been seen and (2) constructing a re presentation that can be used to enumerate the words seen and to decide whether a word of interest was seen. For methods that enumerate stri ngs, a Java HashSet&lt;String&gt; was chosen as the representation, wh ile for methods that enumerate hash values, an array was used. The size of the array was 2 Table 5 shows the text extraction speeds for extracting sets of words. Again, we see that extraction using a hash-based tokenizer runs many times faster (9 X 65x) than the existing string-based methods, and the difference is even greater than with tokenization. This is not surpri sing, as the use of the hash set imposes its own significant overh ead over simply writing a value in an array. First of all, a hash value must be computed over the string to be added, which is linear in the length of the string. Then, if there are already entries in the appropriate slot in the hash table X  X  array, each must be compared against the current string, which is a constant-time operation unless the strings X  (recorded) hash values match. And when a word is seen for the second or subsequent time, a full string comparison must be performed to determine that a new entry does not need to be added, which is again linear in the length of the string. Finally, whenever the table X  X  capacity thre shold is exceeded, it needs to be grown, which is linear in the numbe r of elements currently in the table. This additional overhead due to the hash table explains why even when using the table-based tokenizer, the performance is only marginally better (11% vs. 151% for tokenization) than that of the fastest other string-based extractor. Table 5 illustrates another interes ting point about extraction using hash-based tokenization. Speed yFx methods show up both at the top and near the bottom of the table. The numbers at the bottom are for the method essentially as described in Table 2, in which the feature vector representation is an array of boolean values which are set to true each time a hash computation is completed. When run on War and Peace , this method had essentially the same performance (116.0 MB/second vs. 120.5 MB/second) as the faster method. The problem is that while it is, indeed, inexpensive to set the array item to true each time a word is seen, in order to be usable as a feature vector, this requires that all array items are set to false before the process begins. With 2 20 elements in the array needing to be reset (or initialized) for each of 18,846 cases, this meant that 19.8 billion false values were bei ng set in order to distinguish them from the approximately 6 million hash values actually seen X  over three thousand times as ma ny. Indeed, the 18.4 GB of feature vector initialization dwarfed the 34.2 MB of text that had to be processed. Clearly, the cost of initializing the feature vector (not as big a problem with hash sets, whose internal arrays are proportional to the size of their contents) needs to be taken into account when dealing with smaller texts. We tried two approaches to get ar ound this problem. In the first, we maintained a trail  X  X  separate array of indices of the feature array values changed from false to true. This necessitated that before we wrote a feature value, we first checked to see whether we had already done so, but it also meant that to reset the feature vector to prepare it for the next case, we needed merely to walk a list proportional to the number of unique words seen. This improved the performance to 82.2 MB/second, or 70% of our eventual best speed. The second X  X nd best X  X pproach was to replace the boolean array with an array of values indicating the document in which the corresponding hash value was last encountered. The extractor keeps track of the value indicating the current case and sets the appropriate array element to that value when a hash computation is completed. To check whether the current case contained a word hash, the array element is compared against the current case indicator. If they are the same, the answer is  X  X es X . For our implementation, the case indicator was a number, incremented with each new case and allowed to wrap on overflow. Doing it this way has an interesting side effect, though. If the last case in which a particular word hash was seen was exactly 256 cases ago (for byte indicators; 65,536 cases ago for short indicators), the feature extractor would incorrectly report that the case contained that word hash. In theory, this should not unacceptable, the array can be reset every 255 (or 65,535) cases, amortizing the cost of doing the reset. Doing so with byte indicators reduced the speed to 102.1 MB/second (87%). As described earlier, often one needs to apply many text classifiers to each text, and our method can support this common use case efficiently. To test this, feature selection was performed on the training cases for each of the 20 newsgroups in the 20 Newsgroups dataset to extract the 1,000 most informative features for each using bi-normal separation as the metric and separately for word hash and string features. This resulted in 18,030 selected word hash features a nd 18,175 selected string features (some of which were selected by multiple classifiers). We then scored each article in the entire 20 Newsgroups dataset for each of the 20 classes, by summing weights associated with features seen (noting each weight only once per article), as would be the case when using an SVM or Na X ve Bayes classifier. In these experiments we compared SpeedyFx-based classifiers with string-feature-based classifiers. For the string-feature-based classifiers, we used hand-crafted classifiers based on the table-based tokenizer that consistently performed the best of the string-based methods in the tokenizati on and extraction tests. In addition, each of these methods was implemented three ways, each considering articles one at a time. The first two approaches began by performing feature extraction on the article. In the first approach, the resulting feature vector (array or HashSet) was filtered (based on the complete list of selected features) to an array of booleans. Each classifier was then asked to score the article based on this array. In the second approach, for each class, the feature vector representation was used directly to score the article. The third approach skips feature extraction entirely. Instead, a internal data structure mapping features selected to the classes that selected them and the associated weights. (This data structure is an array for SpeedyFx and a hash map for the string-based method.) For each, the main extraction loop is performed, but now when a feature is determined, rather than adding it idempotently to a feature vector, the classifier checks the target set to see whether it contains the determined feature. If so, the classifier uses the associated classes and weights to update the appropriate scores. It then rem oves it from the target set and adds the classifier for the next article. By removing the feature from same article, it will be ignored. As can be seen in Table 6, in all cases, SpeedyFx clearly dominated the string-based equivale nts. Also, for both SpeedyFx-and string-based approaches, inlin e classification overwhelmingly dominated classification that begins with feature extraction. Ultimately the superior speed of any extraction method would be unimportant if the features it produced led to poor characterization of the data. In this section, we demonstrate that, classification accuracy when using traditional bag-of-word features as opposed to us ing our hash features. Figure 1 shows the F-measure (harmonic average of precision and recall) for each of the 20 binary  X  X ne-class-vs-others X  classification tasks defined by the 20 Newsgroups dataset. The classifier used is the WEKA v3. 4 Support Vector Machine (SVM) [15] using the best 4096 binary f eatures selected separately for each training set via Bi-Normal Separation [3]. (We chose this number of features to maximize macro-averaged F-measure overall.) The results show essentially no difference in performance between bag-of-words features (BOW curve) and our hash features (hash). A pair ed t-test over the F-measures for the 20 classification tasks shows th at any apparent difference is statistically insignificant (p=0.31) . Furthermore, we next show for perspective that other common factors affect the performance much more than the type of features generated. F-measure extract, filter, classify 6.2 11% 1.2 2% extract, classify 8.1 14% 1.3 2% inline classify 57.7 100% 23.5 41% F-measure Figure 2. Macro-averaged F-measure is affected more by choice of classifier, number of features selected and the choice of feature selection metric than it is by whether we use bag-of-words features (BOW) or our hash features. Figure 2 shows the F-measure under various models macro-averaged over all 20 binary classification tasks. For each model, we see that the performance with bag-of-words features or with our hash features has a small effect (the two curves are close together). Overall, choice of feature generation has a much smaller effect on performance than the choice of classifier (SVM vs. multinomial Na X ve Bayes), the num ber of features selected, or the feature selection metric by which to judge features (Bi-Normal Separation vs. Informati on Gain). This brings up an additional point of motivation for this work: If in practice one needs to try many different model parameters and select the best via cross-validation, then it may be even more important to the practitioner to have fast text feature extraction. For a good hash function, accidental hash collisions should be as rare as or rarer than misspellings in the text or naturally occurring homonyms (two words that are written the same but have different meanings, e.g. the  X  X oxer X  dog breed vs. the  X  X oxer X  athlete). Just as misspellings and homonyms occasionally detract from our text analysis goals, so too will hash collisions of unrelated words. We show in th is section that our chosen hash function, MASH, is stronger th an the ubiquitous multiplicative hashing, used by Java X  X  String. hashCode() function, and the well-known Rabin hash function, which is generally purported to be fast to compute with the use of a pre-computed lookup table. Table 7 compares the three hash functions in terms of the collisions over words in the 20 Newsgroups dataset, considering only the bottom 20 bits of each hash value. The 20 Newsgroup dataset contains 5,984,079  X  X or ds X  (maximal sequences of alphanumeric characters) which comprise 168,461 distinct (lowercased) values. The first two rows are the number of distinct hash values seen using the give n hash function and the number of those values that are the result of hashing distinct words. (The numbers do not add up to the number of distinct words since a small number of values will be shared among more than two words). The third line is the resulting probability that a given word will result in a hash value that is shared with some other word in the corpus. This is a static probability, which does not take into account word frequency. As can be seen from the table, words are considerably less likely to share their 20-bit hash values using MASH than using either Java  X  X  string hashing or Rabin fingerprinting. Only approximate ly one in 13 words has a shared hash value for MASH, compared to nearly one in ten for Java string hashing and nearly one in three for Rabin fingerprinting. But static collisions are not the w hole story. In any corpus, the the words will tend to follow a Zipf distribution with the vast majority occurring very seldom and only a small minority occurring with noticeable frequency. Since hash collisions do not depend on the frequencies of the words, this implies that for the vast majority of collisions, one or (much more likely) both will be very infrequent. For instance, using MASH, the words  X  X it X  and  X  X x3 X  both hash to 128,278, but the former occurs in 2,597 articles, while the latter appears in only one. The implication for classification is that a colliding hash is either unlikely to be selected as a feature (since both words are infrequent) or will almost always represent the word that led the classifier to select it. To take this into account, we consider a meas urement of a probability of  X  X ord confusion X  that focuses on the dynamic impact of collisions. This is the conditional probability that if two words chosen at random from the corpus hash to the same value, they are, in fact different words. It is computed as where n w and n h are, respectively, the number of occurrences of words and hashes in the corpus. As is apparent in Table 7, this is over five times as likely to happe n with Java string hashing (and 71 times as likely to happen with Rabin fingerprinting) as it is with MASH. The actual collisions seen appear to be unlikely to cause much problem. Looking at MASH with the  X  X ec.motorcycles X  group, the most significant term is a co llision between  X  X ikes X , clearly a relevant word, which occurs in 233 articles in the corpus and  X  X om X , which occurs once. Of the top 1,024 features for the group, in only two is there a collision between words both of which occur more than ten times. The first such collision, the 1,001 st most important feature as selected by Bi-Normal Separation, is a collision between  X  X oemi X  (13 articles) and  X 8mhz X  (11). The second such co llision is an actual example of what might be considered the introduction of a homonym:  X  X in X  (692 articles) and  X  X xecuted X  (83 articles) both hash to 998,868. In the entire corpus of 168,461 words, there were only sixteen pairs of words that each occurred in at least 100 articles and which hashed to the same valu e using MASH. These, shown in Table 8, represent just 0.01% of the 155,620 hash values seen and 0.13% of all (static) collisions. If the threshold is lowered to Table 7. Comparison of hash function quality over words. Hashes seen 155,620 150,613 113,817 Colliding hashes 12,178 15,661 24,436 P(static collision) 0.0762 0.1059 0.3244 P(dynamic confusion) 0.0012 0.0065 0.0841 Table 8. All pairs of frequent words confused by MASH. key (3,395) vs (1,075) research (2,044) easy (841) heard (1,607) mu (280) 90 (712) medium (108) vms (645) floppy (482) alone (501) duke (424) those that have at least ten instances each, the number of such hashes rises to only 367 (0.24%). So far, the analyses have all used a bit table space of N=2 Naturally, the collisions will increas e if we decrease N, e.g. in order to reduce memory demands for small portable devices. Figure 3 shows the number of collisions produced by the set of words and phrases used previously, as we vary the table size modulus N from 500,000 to 2 20 . Most noticeably, the 31*h+c multiplicative hash function used by Java has erratic performance. It gets many more collisions for particular values of N X  X ultiples of 31 (the JDK 1.6 HashMap and HashSet implementations avoid such table sizes by always using a power of two). Likewise, the Rabin hash function also show s erratic performance, though it very often has fewer collisions than the multiplicative hash function. Finally, observe that MASH performs consistently well at every table size (labeled  X  X urs X ). For a near-lower bound comparison, we repeated this experiment for a random set of integers and find that their collision performance is only marginally better than MASH (labeled  X  X deal X ). For indexing applications, in which the task is simply to extract the complete sequence of tokens, it is perfectly reasonable to store much larger hashes of 4 to 8 bytes per token. Using 6-byte hashes, one would not expect to see a single collision in fewer than nineteen million distinct words, assuming hash uniformity. For a web indexing application with a vocabulary of, say, five million words, 48 bits would suffice to give you a probability of 96% of not having a single collision. In addition to processing speed, another efficiency consideration is size. In the present context th is can be interpreted in two ways: (1) the size of a representation of a classifier having a certain number of selected features and (2 ) the amount of memory that is needed to perform feature extraction or classification. In what follows, we give a summary of our results. Readers interested in the details of the analysis are i nvited to see Appendix 10.1 of our longer technical report. The anal ysis presumes a straightforward implementation using Java JDK 1.6 on a 32-bit machine. It also assumes the use of 20-bit hashes. The size of a classifier representation has implications primarily with regard to transmitting the classifier from one machine to another or writing it to permanent st orage. This will be especially important in settings in whic h there are a large number of classifiers being used. With ha sh-based methods, classifiers can be represented with 7 bytes per selected feature (4 bytes for the weight and 3 bytes sufficient to identify the feature hash). With string-based methods, the number will depend on the distribution of strings. For the 20 Newsgroups dataset, for which words average 6.5 characters, a classifier can be represented on the wire with 11.5 bytes per selected feature on average (4 bytes for the weight and 6.5 UTF-8 bytes on aver age, plus a null terminator), for an increase of 64% over the use of hash-based features. The second concern is the memory footprint used temporarily during feature extraction and classification. For SpeedyFx, the memory used will be an array of either one or two bytes per possible hash code, so a 20-bit hash space will require 1 MB or 2 MB. For string-based methods, we analyzed the memory used by the HashSet feature set representation. Our analysis implies that a HashSet of Strings will require for each word approximately 80 bytes + 2 bytes/character, or 93 bytes per word on average for the 20 Newsgroups dataset. The articles in the corpus average 317.5 distinct words apiece, and so processing them should require a HashSet whose overall memory footprint is approximately 29 KB, substantially less than SpeedyFx. For longer texts, the difference is not as clear-cut. The 17,816 different words in War and Peace average 7.5 characters long, and overall the HashSet can therefore be expected to require 1.6 MB. One further consideration is that the arrays used by SpeedyFx are contiguous and self-contained. The HashSets used by string-based methods, on the other hand, comprise hundreds or thousands of individual small objects which will not exhibit the same degree of locality and which will need to be individually garbage collected when no longer needed. In addition, while the 1 or 2 MB size of the SpeedyFx array is larger than the HashSets used by string-based methods for small texts, it is small enough to fit in cache on most modern architectures. For perspective on the speed improvement, consider the motivation of enterprise-scale te xt indexing from the introduction. For each 100GB of enterprise files to be indexed, the off-the-shelf StandardAnalyzer in the Lucene package would spend 15 hours of CPU time just to tokenize the text. Even going to the effort of hand-coding an extension to Lucene X  X  CharTokenizer would only reduce this to just over an hour. By contrast, with SpeedyFx this comes down to less than 13 minutes (from Table 4). The remainder of this section contains a discussion of the practical impact of occasional hash collisions and an extension to SpeedyFx that folds in the ability to handle Unicode UTF-8 text with minimal loss of efficiency. Hash collisions, when not too common, behave similarly to homographs. Consider first info rmation retrieval: a search for documents containing the word  X  X hift X  will return some about  X  X ear shift, X  some about  X  X ight sh ift, X  and others about  X  X hift X  in the slang sense of  X  X ove it. X  We usually discover the confusion after viewing the initial search re sults, and then use natural means to cope with these situations. We may add disambiguating search terms such as  X  X hift AND (gear OR transmission) X , we may place the word in an unambiguous search phrase such as  X  X ear shift, X  or rarely, we may give relevance fee dback to refine the hits without caring which terms are involved. The major difference for hash features is that our background knowledge does not provide intuition for which words will have troublesome pseudo-homographs due to random hash collisions. If we search for  X  X at X  and we get an equal mix of doc uments containing  X  X trawberry, X  then we may be surprised, but the same coping techniques apply as above. With our intuition, we might know that  X  X hift X  is going to be ambiguous and plan for it from the very first query, but for the  X  X at X  example, we could not know in advance. Since most words occur rarely according to a Zipf distribution, it is much more likely that the pseudo-homograph would be some more obscure word. For example, if we search for  X  X at X  and we also get documents containing  X  X chadenfreude , X  then the precision of our results may hardly be noticeably polluted, especially when considering the already difficult signal-to-noise problems in everyday, practical searches. Next, consider the homograph-collision problem for text classification: the classifier is sure to use a variety of terms together to predict the class. Thus, a small amount of term confusion (typically from rarely occurring words) is unlikely to impact classification accuracy significantly. Nevertheless, it could occasionally happen that a particularly important word for classification is conflated with a very common word, such as the common stopword  X  X he X , which ruins the predictive value of the joint feature. So, although there may be no substantial degradation for the average text classification problem, hash collisions may impair classification accuracy for a single, specific classification task at hand. A potential work-around would be to try a different initial hash seed to avoid the unfortunate collision. This workaround is not practical for the information retrieval scenario, where the queries of interest are not known until after the large document repositories have already been indexed with a fixed hash function. Although traditionally most text processing has been done on 7-bit ASCII or 8-bit characters of various encodings, more and more one must process texts encode d using the Unicode character set [14], either in the fixed-width two-byte UCS-2 encoding or in the variable-length UTF-8 enc oding. Typical software for Unicode text processing, especially in Java, first calls a function to translate a UTF-8 byte stream into a buffer of 16-bit Unicode characters, upon which all remaini ng text processing is done. But this requires a substantial amount of copying and writing to a large block of buffer RAM (writing is slower than reading for most modern CPUs). The table-driven MASH algorithm can be easily modified to handle both the 16-bit characters and the variable-length UTF-8 encoding, working directly on the byte stream and with only a minor reduction in throughput. The modifications do not entail the construction of a 65,536-element code table, but take advantage of the fact that most texts contain characters from only one or a few code pages, often AS CII or Latin-1 (on page 0) and one other, e.g., Greek characters on code page 3 or Devanagari characters on code page 9. Because of this, the table can be sparse and constructed lazily. The reader is invited to see the full description in Appendix 10.2 of our longer technical report. The restricted problem of searching text for a previously selected set of useful features for a classifier may be cast in a broader context that is generally familiar in computer science. The problem of searching for a limite d number of known query strings in large volumes of text is a traditional string search problem. The most well-known algorithms that treat multiple strings simultaneously and do not first re quire extensive indexing of the target text are the Aho-Corasick algorithm [1], the Commentz-Walter algorithm [2], and the Rabin-Karp algorithm [8]. Although they each have an expected run time linear in the size of the target text, they were designed in a historical context where the number of query patterns was in the dozens (e.g. information retrieval via UNIX grep). Research towards scalable methods that handle thousands of query patterns continues (e.g. [13]), but all these algorithms are expected to operate under general-purpose conditions, i.e. no restrictions on th e query strings. In our setting, we can be sure that a query string is a word (or a bi-gram), which eliminates a great deal of dictionary checking. Another major difference is that for many statis tical text processing applications, we can accept a certain risk of false positive matches in the form of hash collisions, considering that the natural misspellings, homographs, and variations in word choice in real-world text. Thus, for our purposes, traditional string search algorithms are not competitive, nor do they address the more general problem of text feature extraction for unrestricted words. Just recently there have been two papers that promote hash features for classification, though neither focuses on text extraction speed, supervised feature selection, or applications besides classification, as we do. A short paper by Ganchev and Dredze [5] suggests using Java  X  X  String.hashCode() function modulo N for text classification features. Their focus is to obtain small classifiers for mobile devices with little RAM: the description of a linear classifier then needs only retain an array of weights X  X he index implicitly names the associated feature rather than a String word. The main result is that the number of features N was reduced to 10,000 X 20,000 without substantial loss in classification accuracy. To this we note that feature selection methods provide a principled approach for limiting the number of features, and classifiers with the best 500 X 2000 features are often more accurate than models using all features [3]. We illustrate this point in Figure 4, which compares the F-measure of an SVM using our hash features where we select the top features via BNS (the top curve as Figure 2), vs. simply reducing the width N of the bit table N and using all the features created. We see that as N decreases, the increasing number of random collisions generally harm the predictive quality of th e features. Instead, it is much F-measure better to produce features in a large bit array and then select the best of these, sometimes re sulting in improved performance. The other recent work promoting hashes for classification is by Rajaram and Scholz [12]. In their case, their goal is to compute unsupervised, similarity-preserving hashes for texts such that 1000 X 4000 bits could be a sufficient feature vector for many heavyweight by comparison to ours, and first rely on a text word extractor. SpeedyFx could be used as their raw text scanning subroutine, since they only depend on the statistical properties of the features, not the actual letter sequences. We have shown that using SpeedyFx integer hashes in place of actual words is faster, requires less memory for transmission and use of multiple classifiers, and has an effect on classification performance that is practically noise compared to the effect of other common parameters in model selection. We showed that MASH has strong, uniform pe rformance for words, though not appropriate for long blocks of text. Moreover, we have demonstrated the ability to classify a text by many classifiers many times faster than is possible with typical code. The primary implication of this is that it allows significantly more efficient indexing and classi fication of large document repositories, e.g. to support in formation retrieval over all enterprise file servers with frequent file updates. It also enables lower impact to a user X  X  computer, e.g. when there are many classification agents examining all text the user sees or types, for the purpose of  X  X nderstanding X  what the user is thinking about and providing appropriate support. Th e savings in computation may also make it feasible to run on cer tain small mobile devices and/or consume less battery power. Whenever a CPU bottleneck is resolved by more efficient algorithms, it often exposes the I/ O transfer speed as the next bottleneck. Although with SpeedyFx we can easily process over a hundred megabytes per second per processor core, if there is only a single disk attached, the data transfer bottleneck may be just 2 X  70 MB/sec. Multiple disks or a 100 gigabit Ethernet feed from many client computers may certainly increase the input rate, but ultimately (multi-core) processing technology is getting faster faster than I/O bandwidth is getting faster. One potential avenue for future work is to push the general-purpose text feature extraction algorithm closer to the disk hardware. That is, for each file or block read, the disk controller itself could distill the bag-of-w ords representation and then transfer only this small amount of data to the general-purpose processor. This could enable much higher indexing or classification scanning rates than is currently feasible. Another potential avenue is to investigate varying the hash function to improve classificati on performance, e.g. to avoid a particularly unfortunate collision between an important, predictive feature and a more frequent word that masks it. At the least, this could amount to trying different random seeds and selecting the most favorable via cross-validation. In the limit, perfect hashing techniques could attempt to ge nerate a hash function that produces dense, unique positive inte gers for each of the selected features, and negative values fo r features to be discarded. We wish to thank Kave Eshghi, Rajan Lukose, Shyam Rajaram, Martin Scholz, and Craig Soules for discussions and motivations. [1] Aho, AV. and Corasick, MJ. 1975. Efficient string [2] Commentz-Walter, B. 1979. A string matching algorithm [3] Forman, G. 2003. An extensiv e empirical study of feature [4] Forman, G. and Rajaram, S. 2008. Scaling up text [5] Ganchev, K. and Dredze, M. 2008. Small statistical models [6] Gospodnetic, O., and Hatcher, E. 2004. Lucene in Action (In [7] Joachims, T. 2006. Training linear SVMs in linear time. In [8] Karp, RM and Rabin, MO. 1987. Efficient randomized [9] Knuth, DE. 1973. The Art of Computer Programming, [10] McKenzie, B. J., Harries, R., and Bell, T. 1990. Selecting a [11] Mladenic, D. and Grobelnik, M. 1998. Word sequences as [12] Rajaram, S. and Scholz, M. 2008. Client-friendly [13] Salmela, L., Tarhio, J., and Kyt X joki, J. 2007. Multipattern [14] The Unicode Consortium. 2006. The Unicode Standard, [15] Witten, I., and Frank, E. 2005. Data Mining: Practical 
