 Queensland University of Technology University of California, Berkeley and QUT University of California, Berkeley Consider a robot that controls an electron microscope to track a microorganism. Given the entire trajectory of the microorganism and the dynamics of the system, the opti-mal control can be computed. The trajectory, however, is not known in advance and the target might behave in an ar-bitrary fashion. In such situations, designing a controller based on some prior knowledge about the target location might be sub-optimal. It is important to take the behavior of the target into account.
 We consider problems with linear transition functions and quadratic tracking losses. When the target trajectory is known in advance, the problem is called the linear quadratic (LQ) problem in the control community. The LQ problem is one of the most studied problems in the con-trol literature and is widely applied in practice (Lai and Wei, 1982; 1987; Chen and Guo, 1987; Chen and Zhang, 1990; Fiechter, 1997; Lai and Ying, 2006; Campi and Ku-mar, 1998; Bittanti and Campi, 2006; Abbasi-Yadkori and Szepesv  X  ari, 2011). By principles of dynamic programming the optimal controller can be computed analytically. The solution is obtained by computing value functions, starting from the last round and recursing backward. This method needs to know the entire target sequence from the begin-ning and its computational complexity scales linearly with the length of the trajectory. It turns out that the optimal con-troller is linear in the state vector, and the value functions are quadratic in state and action.
 As we discussed earlier, the assumption that the entire tra-jectory is known in advance is not always realistic. But what would tracking mean without a reference trajectory? To make the problem well-defined, we fix a class of map-pings from states to actions (also known as policies) as our competitors. Our objective is to track the trajectory nearly as well as the best policy in the comparison class in hind-sight. The standard dynamic programming procedures are not applicable when the entire sequence is not known in advance. We show that we can still have an effective track-ing algorithm even if the sequence is not known in advance. The proposed algorithm is perhaps the first tracking method that can deal with infinite and unknown sequences. We study the adversarial version of the LQ problem where an adversary designs the trajectory of the target and reveals the target location only at the end of each round. Formally, we study problems with transition dynamics and loss func-tions where x t  X  R n is the state at round t ; a t  X  R d is the action; g is the target location; ` t : R n  X  R d  X  R is the loss function; and A,B,Q are known matrices. 1 The matrix Q is symmetric and positive definite. The learner observes the sequence of states ( x t ) t . We make no assumptions on the trajectory sequence ( g t ) t , apart from its boundedness. The sequence might even be generated by an adversary. An LQ problem is defined by a 4-tuple ( A,B,Q,G ) , where G is an upper bound on the norm of vectors g t .
 Let T be a time horizon, x  X  t be the state of the system if we run policy  X  for t rounds, and  X  be a class of policies. Let  X  ( g 1 ,...,g T ) = The objective of the learner is to suffer low loss. The per-formance is measured by the regret defined by R
T ( A,B,Q,G,x 1 ,  X ) = sup where k X k denotes the 2-norm. In what follows, we use R that x 1 = 0 . As the optimal policy for the classical problem with a constant target is linear (strictly speaking, affine), a reasonable choice for the class of competitors is the set of linear (affine) policies.
 The problem that we describe is an instance of Markov De-cision Process (MDP) problems with fixed and known dy-namics, and changing loss functions. Note that the loss function depends on the target location, which can change in an arbitrary fashion. Such MDP problems were pre-viously studied by Even-Dar et al. (2009) and Neu et al. (2010b;a). However, these papers provide results for fi-nite MDP problems and are not applicable to problems with large state/action spaces. Our key finding is that the algorithm of Even-Dar et al. (2009) can be modified to be applicable in adversarial LQ problems. Interestingly, the resultant algorithm is identical with a Policy Iteration method (see, for example, (Howard, 1960)) with changing loss functions. Another interesting observation is that the gain matrix is independent of target vectors (see Lemma 4). This simplifies the design and analysis of our algorithm. We prove that the regret of the algorithm is logarithmic in the number of rounds of the game.
 Finally, we also study a more adversarial problem: Here, the time-varying transition matrices A t and B t and the target vector g t are chosen by an adversary. In this prob-lem, we show that under a uniform stability assumption, an exponentially-weighted average algorithm recently pro-posed by Abbasi-Yadkori et al. (2013) enjoys an O ( regret bound with respect to the class of linear policies. We use  X  min ( M ) and  X  max ( M ) to denote the minimum and maximum eigenvalues of the positive semidefinite ma-trix M , respectively. We use k X k to denote the 2-norm of matrices and vectors, where the 2-norm of a matrix M is defined by k M k = p  X  max ( M &gt; M ) . We use M 0 to denote that M is positive definite, while we use M 0 to denote that it is positive semidefinite. We use M ij to denote a block of matrix M . The indices and the dimensionality of the block will be understood from the context. Similarly, v denotes a block of vector v . Even-Dar et al. (2009) study finite state MDP problems with fixed and known transition functions and adversar-ial loss functions. Their algorithm (MDP-E) in its present form is not applicable to our problem with a continuous state space. Somewhat surprisingly, we can design a variant of the MDP-E algorithm that is applicable to our tracking problem with continuous state and action spaces.
 The MDP-E algorithm, shown in Figure 1, maintains an expert algorithm in each state; that is, it treats each state as a separate problem of prediction with expert advice, where each action corresponds to an expert. At round t , the product of expert recommendations over the state space de-fines the policy, denoted by  X  t . The algorithm takes action a  X   X  t ( x t ) and observes the loss function ` t . It computes the value function V  X  t ,` t defined by the Bellman Optimality Equation where m defines the state transition probabilities. 2 Then, the algorithm feeds the expert algorithm in state x with V the computational cost of the MDP-E algorithm per round is O ( W + |X| ) , where W is the cost of obtaining the value function and X is the finite state space.
 Applied to the LQ problem, the value functions are defined by where we use the notation V t = V  X  t ,` t . The linear struc-ture allows us to compute V t implicitly for all states, thus overcoming the difficulty of the infinite state space. As we will show, a suitable expert algorithm for our problem is the Follow The Leader (FTL) algorithm that we define next. Consider an online quadratic optimization problem where at round t the adversary chooses a quadratic loss function f that is defined over a convex set D  X  R k . Simultaneously, the learner makes a prediction p t  X  D , suffers loss f t and observes the loss function f t . The regret of the learner The FTL algorithm makes the prediction The FTL algorithm enjoys the following regret bound for quadratic losses (Cesa-Bianchi and Lugosi, 2006, Theorem 3.1): Theorem 1 (FTL Regret bound) . Assume f t is convex, maps to [0 ,C 1 ] , is Lipschitz with constant C 2 , and is twice differentiable everywhere with Hessian H C 3 I . Then the regret of the Follow The Leader algorithm is bounded Figure 2 shows the FTL-MDP algorithm for the linear quadratic tracking problem. It corresponds to the MDP-E algorithm, with FTL as the expert algorithm for each state. The algorithm starts at state x 1 = 0 and the first policy is chosen to be  X  1 ( x ) =  X  K  X  x , where K  X  is a gain matrix that will be defined later. 3 The algorithm computes the to-tal loss in each state, shown by V 0 ( x,. ) = P t  X  1 s =1 The FTL strategy chooses the greedy action in each state, which is obtained by minimizing V 0 ( x,. ) . As the next lemma shows, value functions computed in the FTL-MDP algorithm are always quadratic and thus, the function V 0 is always quadratic in state and action. This implies that policies are linear in state.
 Lemma 2. Consider the MDP-E algorithm applied to the adversarial LQ problem ( A,B,Q,G ) . Let the expert algo-rithm be the FTL strategy. Assume the first policy is chosen to be an arbitrary linear policy,  X  1 ( x ) =  X  K 1 x + c for appropriate matrices P t and L t , the value function at time t has the form of and the policy chosen by the algorithm at time t is  X  t ( x ) = c t =  X  ( P are the ij th and i th blocks of matrix P s and vector L respectively. (Here the block structure naturally appears as components corresponding to the state and action.) The proof uses the following lemma that shows that the value of a linear policy is quadratic.
 Lemma 3. Consider the LQ problem ( A,B,Q,G ) with fixed target g  X  . Let K be a matrix such that k A  X  BK k &lt; 1 . The value function of policy  X  ( x ) =  X  Kx + c has a quadratic form where P = P ( K ) and L are solutions to equations P = and L Further, the loss in the limiting state x  X   X  is The proof can be found in Appendix A.
 Proof of Lemma 2. We prove the lemma by induction. By Lemma 3, the value of the first policy,  X  1 ( x ) =  X  K 1 has the form of for some matrices P 1 and L 1 . This establishes the base case.
 Next assume the value functions up to time t  X  1 are all quadratic. Because we use a FTL strategy as our expert algorithm in states, the policy in state x in round t can be computed by We obtain the policy by setting  X  a P t  X  1 s =1 V s Letting for s = 1 ... ( t  X  1) , we get that V ( x,a ) = gradient with respect to the second argument and setting to zero, we get that, and thus, Thus, the policy can be compactly written as linear policy, we get the quadratic value function V t from Equation (2).
 Lemma 2 implies that the MDP-E algorithm can be effi-ciently implemented in the adversarial LQ problem. Before stating the main result of this paper, we describe certain assumptions and definitions from the control litera-ture. (See, for example, (Bertsekas, 2001)).
 Definition 1. A pair ( A,B ) , where A is an n  X  n matrix and B is an n  X  d matrix, is said to be controllable if the n  X  nd matrix [ B AB ... A n  X  1 B ] has full rank. A pair ( A,C ) , where A is an n  X  n matrix and C is an d  X  n matrix, is said to be observable if the pair ( A &gt; controllable.
 Roughly speaking, controllability implies that the state can be moved arbitrarily by changing the actions, while observ-ability implies that the state can be externally measured. We assume that the system is controllable and observable. These assumptions are standard in the literature, and will allow a closed form expression for the optimal control law. Assumption A1. (Controllability and observability) The pair ( A,B ) is controllable and the pair ( A,Q 1 / 2 ) is observ-able.
 Under this assumption, the gain matrix is stable, i.e. there exists  X   X  (0 , 1) such that k A  X  BK  X  k X   X  &lt; 1 , where is the gain matrix (Bertsekas, 2001), and S is the solution of the Riccati equation Interestingly, as the next lemma shows, all gain matrices are equal. The proof can be found in Appendix A.
 Lemma 4. Consider the FTL-MDP algorithm. Let P  X  = P ( K  X  ) as defined by (3) . If we choose K 1 = K  X  , then all gain matrices are equal, K  X  = K 1 = K 2 = K 3 = ... , and hence P  X  = P 1 = P 2 = P 3 = ... .
 Lemma 4 shows that gain matrices are independent of tar-get vectors and can be computed by assuming that all target vectors are zero. Given the fixed gain matrix, the system is driven to a desired target position by changing the bias term of the linear policy.
 We represent the linear policy  X  ( x ) =  X  Kx + c by the pair  X  = ( K,c ) . The class of ( K 0 ,C 0 ) -bounded,  X  -stable linear policies is defined by  X  = {  X  = ( K,c ) : k A  X  BK k  X   X , k K k X  K 0 , k c k X  C 0 } .
 Theorem 5. For a controllable, observable LQ problem ( A,B,Q,G ) , the regret of the FTL-MDP algorithm with respect to the class of ( K bounded,  X  -stable linear policies is O (log 2 T ) , where the hidden constants are polynomials in k A k , k B k , k Q k ,G,S, k P  X  k , 1 / X  min ( P  X  ) , k K and 1 / (1  X   X  ) .
 The hidden constants are all small order polynomials. As we will show in the next section, a careful asymptotic anal-ysis gives us an asymptotic O (log T ) bound. It is an open problem to show a finite-time O (log T ) regret bound with polynomial constants. Let x  X   X  = lim t  X  X  X  x  X  t be the limiting state under ( K bounded and  X  -stable linear policy  X  = ( K,c ) . In Lemma 6 we show that this limit exists. Let  X  t (  X  ) = ` ( x  X   X  , X  ( x  X   X  )) be the loss of policy  X  in state x  X  decompose the regret Let The terms  X  T and  X  T correspond to the difference between the losses of the policies between their stationary and tran-sient states. The term  X  T measures the regret with respect to the optimal policy. The rest of this section is devoted to providing bounds on these terms. 4.1. Bounding  X  T To bound  X  T , we need to show that sum of terms ` ( x t ,a t )  X   X  t (  X  t ) is small. Let x  X  t  X  = lim We will show that this limit exists. Because  X  t (  X  ` to show that x  X  t  X  is close to x t . This is done in a number of steps. First, we obtain the limiting state x  X  t  X  (Lemma 6 and the discussion after that). Then, we show that the cho-sen policy is slowly changing. Given these two results, we bound k x t  X  x  X  t  X  k , which is then used to bound  X  T First, we study the behavior of the state vector under any bounded and stable policy. We show that the policy con-verges to its stationary state exponentially fast. Lemma 6. The limiting state x  X   X  = lim t  X  X  X  x  X  t under a ( K 0 ,C 0 ) -bounded and  X  -stable linear policy  X  = ( K,c ) exists and is equal to x  X   X  = ( I  X  A + BK )  X  1 Bc . Further, we have that k x  X  t k X k B k C 0 / (1  X   X  ) and Proof. We have that where we used x 1 = 0 in the last equality. Thus, as t goes to infinity, x  X  t  X  ( I  X  A + BK )  X  1 Bc . This also implies that k x  X  t k X k B k C 0 / (1  X   X  ) . It is also easy to see that x Note that even if  X  = ( K,c ) /  X   X  , but ( A  X  BK ) is stable, the above argument is still valid and we get a similar result. In particular, Letting C be an upper bound on k c t k for t  X  T , with a similar argument we can also show that In what follows, we use X to denote the upper bound on the chosen policy is slowly changing and the bias term in policies is bounded by C , where where P  X  denotes the solution to Equation (3), correspond-ing to gain matrix K  X  . The proof can be found in Ap-pendix A.
 Lemma 7. We have that ( i ) . k c t k X  C , ( ii ) . k c t  X  c t  X  1 k X  Next, we show that the limiting state of policy  X  lim s  X  X  X  x  X  t s ) is close to the state at time t . The proof can be found in Appendix A.
 Lemma 8. If t &gt; d log( T  X  1) / log(1 / X  ) e , then Also, we have that X Remark 9. This lemma shows an O (log 2 T ) bound on P t =1 k x t  X  x  X  t  X  k . As we will see, this leads to an O (log 2 T ) regret bound. Let t = k x t  X  x  X  t  X  k . To get an O (log T ) regret bound, we need to show that t = H 1 /t for a constant H 1 . A careful examination of proof of Lemma 8 H 2 and H 3 . Let f ( t ) = P f ( t ) = H 4 /t for a constant H 4 so that we get an O (log T ) regret bound.
 To show this, we argue as follows: first establish the sim-ple recurrence f ( t + 1) =  X f ( t ) +  X /t . This implies that f ( t )  X  0 as t  X   X  . Now, define g ( t + 1) = tf ( t + 1) . Thus g ( t + 1) =  X g ( t ) +  X f ( t ) +  X  . Since  X  &lt; 1 and lim t  X  X  X  f ( t ) = 0 , lim t  X  X  X  g ( t ) exists and a simple calcu-lation shows that this is  X / (1  X   X  ) . This in fact implies that f ( t )  X   X / ( t (1  X   X  )) asymptotically, which in turn implies that regret is O (log T ) asymptotically.
 Now we are ready to bound  X  T .
 Lemma 10. Let
Z 1 = 2( C k K  X  k + G k Q k ) + 2( k Q k + K 2  X  )
Z 2 = 4 k B k C
Z 3 = k B k ( k D k G + 2 C )(1 + log T ) Then we have that Proof. For policy  X  = ( K,c ) , we have For policy  X  t = ( K t ,c t ) = ( K  X  ,c t ) , define S K  X  K  X  and d t = 2( c  X  where we used Lemma 8 in the last step. 4.2. Bounding  X  T The term  X  T is bounded by showing a reduction to regret minimization algorithms (in this case, the FTL algorithm). To use the regret bound of the FTL algorithm (Theorem 1), we need to show boundedness of the value functions. Lemma 11. Let X 0 = k B k C 0 / (1  X   X  ) , U = max {k K  X  k ,K 0 } max { X,X 0 } + max { C,C 0 } , V = k P  X  k ( X 0 + U ) 2 + 2 1  X   X  ( G k Q k +  X C k P  X  k ) ( X 0 For any t , and any ( K 0 ,C 0 ) -bounded,  X  -stable linear pol-icy  X  = ( K,c ) , For any action such that k a k X  U , Further, V t ( x  X   X  ,. ) is Lipschitz in its second argument with constant F . Finally, the Hessians of the value functions are positive definite and H ( V t ( x  X   X  ,. )) 2 I . The proof can be found in Appendix A.
 Lemma 12. We have Proof. To bound  X  T , first note that Thus, Thus, to the FTL strategy in state x  X   X  . Lemma 11 shows that con-ditions of Theorem 1 are satisfied. Thus, we get the result from the regret bound for the FTL algorithm (Theorem 1): 4.3. Bounding  X  T Finally, we bound  X  T . The proof is similar to the proof of Lemma 10 and can be found in Appendix A.
 Lemma 13. Let Z 0 1 = ( CK 0 + G k Q k ) + ( k Q k + K 0 2 ) k B k C 0 / (1  X   X  ) . Then we have that 4.4. Putting Everything Together Proof of Theorem 5. The regret bound follows from Lem-mas 10, 12, and 13. Our results can be extended to LQ problems with adversar-ial transition matrices, Here, transition matrices A t and B t and the target vector g are chosen by an adversary. Once again, we measure the any policy  X  is identified by some pair ( K,c ) such that  X  ( x ) =  X  Kx + c .
 The only no-regret algorithm for this setting is the re-sult of Abbasi-Yadkori et al. (2013) who propose an exponentially-weighted average algorithm and analyze it under a mixing assumption. Similarly, we make the fol-lowing assumption: Assumption A2. (Uniform Stability) The choices of the learner and the adversary are restricted to sets K  X  C  X  R There exists 0 &lt;  X  &lt; 1 such that for any A  X  A and B  X  X  , and any K  X  X  , Further, there exits K 0 ,C 0 &gt; 0 such that for any K  X  K and c  X  X  , k K k X  K 0 and k c k X  C 0 .
 The proposed algorithm for the LQ problem (9) is shown in Figure 3. The algorithm maintains a distribution over policies. The distribution has the form of The following theorem bounds the regret of this algorithm. Theorem 14. Consider a uniformly stable system. The re-gret of the algorithm in Figure 3 with respect to a class of policies |  X  | is bounded by O ( p T log |  X  | + log |  X  | ) . Proof. We prove the theorem by showing that conditions of Theorem 1 in (Abbasi-Yadkori et al., 2013) are satisfied. Uniform mixing assumption : Let P (  X ,A,B ) be the transi-tion probability matrix of policy  X  = ( K,c )  X  X  X C under transition dynamics ( A,B )  X  X  X B . Let p 1 and p 0 1 be two distributions over the state space and p 2 = p 1 P (  X ,A,B ) and p 0 2 = p 0 1 P (  X ,A,B ) . Let 1  X  k  X  n be rank of M = ( A  X  BK ) . Let M 0 be a k  X  k matrix whose eigenvalues are the nonzero eigenvalues of M . For x  X  R n , let x r ( x )  X  be a parameterization of the component of x that is on the row space of M . Similarly, define x n ( x )  X  R n  X  k that cor-responds to the orthogonal component on the null space of M . For u  X  R k and v  X  R n  X  k , let x ( u,v ) be a vector in R n such that x r ( x ( u,v )) = u and x n ( x ( u,v )) = v . Fi-nally, let p r ( u ) = R by substitution, we get that k p This shows that the uniform mixing assumption of Abbasi-Yadkori et al. (2013) is satisfied with the choice of mixing time  X  = 1 / log(1 / X  ) .
 Bounded losses : With an argument similar to the proof of Lemma 6, we can show that the state is bounded. This, together with the boundedness of sets K and C , give that the action is bounded. Thus, all losses are bounded. Results of Abbasi-Yadkori et al. (2013) also apply to the simpler setting of Section 3. However, sampling from the distribution (10) can be computationally expensive, whereas the FTL-MDP algorithm is computationally effi-cient. We studied the problem of controlling linear systems with adversarial quadratic tracking losses, competing with the family of policies that compute actions as linear functions of the state. We presented an algorithm whose regret, with respect to such linear policies, is logarithmic in the num-ber of rounds of the game. An interesting direction for fu-ture work is to consider more complex families of policies, such as the class of linear policies with a limited number of switches.
 Existing tracking algorithms require the target sequence to be known in advance. Also their computational complex-main difficulty in the setting studied here, is the adversar-ial nature of target vectors, which is very different from the classical setting. The key advance is to show how the idea of Even-Dar et al. (2009) (instantiating expert algorithms in all states) can be applied to the LQ problem, which has a continuous and unbounded state space. This is done by showing that the sequence of value functions and policies will be quadratic and linear respectively, if we choose the right expert algorithm (FTL). The compact representation of value functions and policies allows an efficient imple-mentation of the FTL algorithm.
 We showed how a related approach can be applied to ad-versarially chosen changing linear dynamics. Unfortu-nately, this algorithms is computationally expensive. A more challenging problem is to design efficient algorithms for the case of adversarially chosen changing transition ma-trices. An interesting open problem is whether there is an efficient no-regret algorithm, or whether a computational lower bound can be established.
 It might be possible to extend our results to LQ problems with fixed, but unknown transition matrices of the form: where w t +1 is a sub-Gaussian noise and matrices A and B are unknown. We expect this extension to be fairly straighfoward using techniques from (Neu et al., 2012) and (Abbasi-Yadkori and Szepesv  X  ari, 2011). Our approach is similar to Neu et al. (2012), with the difference that we use FTL instead of FPL.
 Yasin Abbasi-Yadkori and Csaba Szepesv  X  ari. Regret bounds for the adaptive control of linear quadratic sys-tems. In COLT , 2011.
 Yasin Abbasi-Yadkori, Peter Bartlett, Varun Kanade,
Yevgeny Seldin, and Csaba Szepesv  X  ari. Online learning in Markov decision processes with adversarially chosen transition probability distributions. In NIPS , 2013. D. P. Bertsekas. Dynamic Programming and Optimal Con-trol . Athena Scientific, 2nd edition, 2001.
 S. Bittanti and M. C. Campi. Adaptive control of linear time invariant systems: the  X  X et on the best X  principle.
Communications in Information and Systems , 6(4):299 X  320, 2006.
 M. C. Campi and P. R. Kumar. Adaptive linear quadratic Gaussian control: the cost-biased approach revisited.
SIAM Journal on Control and Optimization , 36(6): 1890 X 1907, 1998.
 Nicol ` o Cesa-Bianchi and G  X  abor Lugosi. Prediction, Learn-ing, and Games . Cambridge University Press, New York, NY, USA, 2006.
 H. Chen and L. Guo. Optimal adaptive control and consis-tent parameter estimates for armax model with quadratic cost. SIAM Journal on Control and Optimization , 25(4): 845 X 867, 1987.
 H. Chen and J. Zhang. Identification and adaptive con-trol for systems with unknown orders, delay, and coeffi-cients. Automatic Control, IEEE Transactions on , 35(8): 866  X 877, August 1990.
 Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour.
Online Markov decision processes. Mathematics of Op-erations Research , 34(3):726 X 736, 2009.
 C. Fiechter. Pac adaptive control of linear systems. In in Proceedings of the 10th Annual Conference on Com-putational Learning Theory, ACM , pages 72 X 80. Press, 1997.
 R. A. Howard. Dynamic Programming and Markov Pro-cesses . MIT, 1960.
 T. L. Lai and C. Z. Wei. Least squares estimates in stochas-tic regression models with applications to identification and control of dynamic systems. The Annals of Statistics , 10(1):pp. 154 X 166, 1982.
 T. L. Lai and C. Z. Wei. Asymptotically efficient self-tuning regulators. SIAM Journal on Control and Opti-mization , 25:466 X 481, March 1987.
 T. L. Lai and Z. Ying. Efficient recursive estimation and adaptive control in stochastic regression and armax mod-els. Statistica Sinica , 16:741 X 772, 2006.
 Gergely Neu, Andr  X  as Gy  X  orgy, and Andr  X  as Antos
Csaba Szepesv  X  ari. Online Markov decision processes under bandit feedback. In NIPS , 2010a.
 Gergely Neu, Andr  X  as Gy  X  orgy, and Csaba Szepesv  X  ari. The online loop-free stochastic shortest path problem. In COLT , 2010b.
 Gergely Neu, Andr  X  as Gy  X  orgy, and Csaba Szepesv  X  ari. The adversarial stochastic shortest path problem with un-
