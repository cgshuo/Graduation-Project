 Univeristy of California, Berkeley keep exploring and never exploit the knowledge accumulated, we will not behave optimally. numerical reward at each time step and we measure performance by the accumulated reward over R p uncertainty X  principle [3] X  X 7].
 regret of AOA is C A ( P ) log T where can show that the constant for OLP satisfies Nevertheless, note that C ( P ) depends inversely on  X   X  ( P ) not  X   X  ( P ) 2 . { p distributions p i ( a ) .
 and starting state i 0 will be denoted by P  X ,P i N ( i, a, j ) respectively in  X  t .
 We make the following irreducibility assumption regarding the MDP. is possible to reach any state from any other state).
 Consider the rewards accumulated by the policy  X  before time T , maximum possible sum of expected rewards before time T , the above quantity, Define the long term average reward of a policy  X  as restricted set D  X  X  of actions, the gain or the best long term average performance is As a shorthand, define  X   X  ( P ) :=  X  ( P, A ) . 2.1 Optimality Equations reward optimality equations : of generality.
 optimality equations. Accordingly, define To measure the degree of suboptimality of actions available at a state, define Note that the optimal actions are precisely those for which the above quantity is zero. Any policy in O ( P, D ) is an optimal policy, i.e., 2.2 Critical pairs p look almost optimal : play a crucial role in determining the regret. We call these critical state-action pairs, Define the function, the regret, of our algorithm OLP (see Algorithm 1 and Theorem 4 below) is the following: minimum degree of suboptimality of a critical action.
 Proposition 2. Suppose A ( i ) = A for all i  X  S . Define Then, for any P , See the appendix for a proof. 2.3 Hitting times from another: The following constant is the minimum hitting time among optimal policies: time over all policies: We can now bound C ( P ) just in terms of the hitting time T ( P ) and  X   X  ( P ) . for any P , See the appendix for a proof. Algorithm 1 Optimistic Linear Programming 1: for t = 0 , 1 , 2 , . . . do 2: s t  X  current state 4: . Compute solution for  X  X mpirical MDP X  excluding  X  X ndersampled X  actions 6:  X  i  X  S, D t ( i )  X  X  a  X  A ( i ) : N t ( i, a )  X  log 2 N t ( i ) } 7:  X  h t ,  X   X  t  X  solution of the optimality equations (3) with P =  X  P t , D = D t 9: . Compute indices of all actions for the current state 12: . Optimal actions (for the current problem) that are about to become  X  X ndersampled X  13:  X  1 t  X  X  a  X  O ( s t ;  X  P t , D t ) : N t ( s t , a ) &lt; log 2 ( N t ( s t ) + 1) } 15: . The index maximizing actions 18: if  X  1 t = O ( s t ;  X  P t , D t ) then 19: a t  X  any action in  X  1 t 20: else 21: a t  X  any action in  X  2 t 22: end if 23: end for an L 1 -ball around the empirical estimate  X  p t s L ( i, a,  X  p t s the support sets of the transition distributions.
 for all P satisfying Assumption 1, where C ( P ) is the MDP-dependent constant defined in (8) .
 Proof. From Proposition 1 in [1], it follows that Define the event Define, where  X  A t denotes the complement of A t . For all &gt; 0 , letting  X  0 sufficiently slowly.
 Proposition 5. For all P and i 0  X  S , we have Proposition 7. For all P satisfying Assumption 1, i 0  X  S and &gt; 0 , we have than the analogous Proposition 4 in [1].
 Therefore for &lt; 0 / 3 , N 1 T ( i, a ; ) = 0 .
 On the event A t , we have | X  q,  X  h t  X  X  X  X  q, h  X  ( P )  X  X  X  and thus the above implies Recalling the definition (6) of J i,a ( p ; P, ) , we see that this implies We therefore have, is no more than J for some constant C 1 , and so the expectation of the second sum is no more than Combining the bounds (17) and (18) and plugging them into (16), we get Letting  X   X  0 sufficiently slowly, we get that for all &gt; 0 , Therefore, summing over ( i, a ) pairs in Crit( P ) .
 Proof of Proposition 6. Define the event so that we can write optimal actions a  X   X  O ( i ; P, A ) , we have, on the event A 0 t ( i, a ; ) , Since L  X  ( i ; P, A ) = r ( i, a  X  ) +  X  p i ( a  X  ) , h  X  ( P )  X  , this implies
A 0 t ( i, a ; )  X  Using a Chernoff-type bound, we have, for some constant C 1 , Using a union bound, we therefore have, Combining this with (19) proves the result.
 References
