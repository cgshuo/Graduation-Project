 timely generalization. Machine learning can he lp the filter to refine constantly accord-ing to user X  X  response. The other is the online application must have a high processing speed which requires the spam filter to reduce training time. So, choosing appropriate training emails is the most essential task. 
In many machine learning tasks, gathering training data is time-consuming and user for their labels and trains the filter with those labeled ones. 
There are three main approaches in the spam filtering --the IP protocol based one, the SMTP protocol based one and the content based one. The previous research shows speed and less requirements of labeled training emails. Applying ensemble learning we can divide a complex spam filtering problem into can reduce complexity efficiently in analyzing and computing each aspect. Our previ-ous research shows the SVMEL filter has the highest performance among several ensemble learning filters [1]. of filtering problem. In this paper, we use the linguistic ones ( body full word, subject result of several simple ones. Using kernel function mapping SVM is fit for the non-numerical spam confidence score (SCS) which is a real number between 0 and 1. 
The SVM ensemble method is showed below. Let the object space of the SVM SCS output by the i th simple filter. In the learning process a SVM model can be got-classify the email with the SVM model, and output a SCS. 
It is time-consuming of training a SVM model. Our experiments also show that adding a few training emails can not improve SVM performance obviously after 4,000 training emails added. In order to reduce the training time we propose a mem-ory window which denotes the number of distinct training emails. In our experiments and makes the SVM model has a strong timely generalization with less performance drop. method which can choose training samples dynamically [3]. Using current knowledge active learner does not receive training samples passively but actively select the sam-ples which can train a more optimal model. 3.1 Filtering Architecture Figure 1 shows a spam filtering architecture with active learning, in which each pair filter combines the SCS generated by those simple ones to a new SCS, with that labels the email with spam or ham by comparing the new SCS against the threshold. 
According to the new ensemble SCS of an email active learner makes a decision. If their knowledge. 3.2 Choosing Training Email There are already some methods such as uncertainty-based sampling (UBS) method [4] [5], committee method [6], version space and margin-based method [7], statistical method [8] and so on. 
UBS method selects those samples to train which are easily wrong filtered. The must have a pre-filter to estimate the likelihood of wrong filtering. This likelihood is a key factor which indicates how much possibility the email will be wrong filtered. 
Spam filter is refined in an online situation whose performance is bad early in the filter X  X  deployment. But the pre-filter may not be more tolerant of errors that are made with some early user X  X  responses and with that we apply this model in UBS method. Then the current version of filter can be used as a pre-filter to estimate the likelihood of wrong filtering. Using UBS method we choose those emails whose ensemble SCS is about 0.5. 3.3 Cache Improvement For bulk feature of spam we can improve SCS more by comparing new email content cause content comparison one by one is time-consuming. We find a principle that the same spam often appears in some period of time. So we can use a cache to memorize the last ones whose size is the previous memory window. 
For a higher content comparison speed th is paper compromises to model email SCS vector and a user response label. If output SCS vector generated by simple filters is equal to one vector in cache then the SCS is 1 or 0 according to cached spam label or ham one. 
The improved performance of cache is dependent with the quantity of repeated emails. If there are a lot of repeated emails then the cache improvement will be obvi-ous. Contrarily there will be less improvement. spam filter as a baseline system. Secondly we add active learning and cache im-in TREC07p corpus and TREC07 active learning task [9] [10]. The TREC07p corpus contains 75,419 messages: 25,220 hams and 50,199 spams. 4.1 Experiments numbers and the consume indicates the actually used query numbers. The active is No means the run uses baseline system. The Del and Par runs are baseline system run in first 10,000 feedbacks and random 30,388 feedbacks. lowances and 30,388 allowances. After that we do some performance comparison cache SVMEL filter) and Bogo-0.93.4 system 1 which had high performance in TREC06 spam track. 4.2 Results ance and the cache technique can improve more from the active filter. The UBS method is fit for spam filtering since it can choose informative emails and cache im-provement is useful since bulk feature of spam. Figure 2(a) and 2(b) also show before 30,000 messages the cache improvement is obvious and after 30,000 messages the cache improvement is unobvious because less repeated emails after 30,000 messages. that of Bogo-0.93.4. Moreover the learning speed of ActCac10000 and ActCacFul is steady-state performance more quickly. In addition, it takes our filter half of the wall clock time that Bogo-0.93.4 costs to filter. 
Our experiments results show active learning is useful to decrease computation complexity, save training time, resist useless samples and increase filtering accuracy. samples from the candidate training set to compose the training set and remove noise useful one. This paper studied the online feature of the spam filtering, analyzed various features of email and built a baseline spam filter applying SVM ensemble learning. In order to learning for spam filtering, which included choosing training email actively and cache improvement technology. Our experiment result shows the filter applying active performance more quickly. Furthermore, the cache improvement can reach higher performance. hope to apply hierarchical active learning, not only in ensemble filter level but also in semantic mining by natural language processing is also very interesting. This research is supported by the National Natural Science Foundation of China (60403050), Program for New Century Excellent Talents in University (NCET-06-0926) and the National Grand Fundamental Research Program of China under Grant (2005CB321802). Many thanks to Prof. Gordon V. Cormack for his evaluation system. 
