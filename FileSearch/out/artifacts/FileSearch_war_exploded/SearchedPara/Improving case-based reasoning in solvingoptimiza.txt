 Department of Computer Engineering, University of Isfahan, Isfahan, Iran 1. Introduction
Due to the importance of the optimization problems, many new methods have been introduced for solving them. Some examples of optimization problems are job shop scheduling, traf fi c signal timing, fi nding the shortest path for the vehicle movements on roads, sea travel scheduling, air travel scheduling and routing problem in computer networks. Most of the optimization problems are repetitive and during time several instances of them arise and should be solved [5,6]. These problems are similar in some aspects; thus, the knowledge obtained from the previously solved problems can be used to accelerate the solving process of the new problems [23,25]. For example, in a job shop scheduling, some new jobs may be added to the problem gradually and should be considered in the scheduling process; the quality of materials may change, some machinery may stop working, or some machinery may be replaced. Due to the above mentioned changes in a problem, it is necessary to solve a new optimization problem similar to the previous ones. In a routing problem in computer networks, which can be de fi ned as searching for the shortest path between two nodes in a graph, when some routers in the network turn off or on, the existing nodes in the network increase or decrease. In these situations after each change in the network, the discovered shortest routes would become annul and should be discovered again. Hence, in the routing problem we have a trail of repetitive and similar optimization problems to be solved during the time.
There are many similarities and repetitive features in these overlapping problems and only a few of these features may differ in the newly raised problems. Therefore, using the knowledge obtained from previously solved problems may be useful in solving the new optimization problems faster. This can be done by applying a case-based reasoning (CBR) system.

CBR is an automatic reasoning procedure that imitates human models in problem solving using its former experiences [11]. CBR solves a new problem by adopting the solutions of similar problems that have been solved before. Some related researches where CBR is applied in the optimization problems will be reviewed in Section 3.

As will be mentioned in Section 2, the two obstacles in using CBR are: (i) Designing a method for applying the knowledge of formerly solved problems in solving the new problems and (ii) Low fl exibility of the previously stored knowledge. In t his paper we try to pr opose a method based on combining Bayesian Optimization Algorithm (BOA) and CBR in order to remove these obstacles of CBR in solving optimization problems and improve the quality of the optimization solutions. The organization of the rest of paper will be as follows. In Section 3, some related works in applying CBR for optimization problems will be reviewed. BOA algorithm will be described in Section 4. In Section 5, our proposed method for combining BOA with CBR will be presented. The implementation will be described in Section 6 and the evaluation and results of our proposed method for the 0 X 1 knapsack and traveling salesman problems will be presented in Section 7. Finally in Section 8, the paper is concluded. 2. Case-based reasoning and its dif fi culties
Generally, the cycle of solving a new problem through CBR method includes the following four stages [1]:
Designing a proper method for the Reuse and Revise stages is one of the dif fi culties in CBR. When a new problem arises, some similar problems along with their solutions are retrieved from the case-base and it is necessary to combine the existing solutions to obtain a solution for solving the new problem. Then the newly obtained solution is adopted to be completely compatible with the new problem. These stages require problem speci fi c knowledge and each special kind of problem should be dealt with in a special manner [15]. This means that for each kind of problem these two stages should be designed in accordance with the nature of problem itself. Finding a new uni fi ed method to take care of various problems with respect to Reuse and Revise stages of CBR would pave the way for using the CBR for various problems. Another dif fi culty in CBR for solving the optimization problems is that the stored knowledge in the case-base is not fl exible enough. Therefore, sometimes adopting the solutions for solving the new problems is not straightforward. Instead of the exact solutions of previously solved problems, if we store an approximate and statistical description of the solutions in the case-base, the fl exibility of knowledge will increase and the retained knowledge will have more potential to be used in solving the new problems as they arise. 3. Related works
There are many works related to combining CBR with some optimization methods in order to solving similar optimization problems faster. In this section some important methods will be reviewed.
In [2], a combination of CBR and simulated annealing is presented for solving the optimization problems. In this method, when a new problem arises, the most similar problem in the case-base is retrieved and its solution is used as the initial point of the simulated annealing search algorithm. In [21], CBR is applied for solving the combinatorial optimization problems. For this purpose, fi rst, the problem parameters are processed and some hidden useful information is extracted. This information is used to describe the problem. For example, some features like the objects pro fi ts average as well as objects weights average and variance are extracted to describe a knapsack problem. When a new problem arises, we resort to Retrieve stage. These retrieved problems, would guide the search algorithm to some regions of the state space that are apt to fi nd the solutions. In [5,6], CBR is combined with the integer programming technique to solve optimization problems. In the method introduced in [5, 6], some mathematical relations are designed for each problem and their maximization or minimization would take us closer to the optimal solution. Then, some random problems are generated and solved by methods such as greedy algorithms. These problems and their solutions are stored in a case-base. Here, for solving each type of optimization problems, a speci fi c method is designed to combine the solutions of the retrieved problems. In [24], CBR is combined with a genetic algorithm in a way that in case a new problem is raised, similar problems are retrieved from the case-base and then evolved gradually by the genetic algorithm to become entirely compatible with the new problem. Thus, in this method a genetic algorithm is used in the Revise stage. In [7], CBR is combined with genetic algorithm and neural network in order to solve the optimization problems. Each solved problem is stored in the case-base and also is used to train a neural network. When a new problem is presented, the solution of the most similar problem is retrieved from the case-base. Moreover, the new problem is introduced to the trained neural network as an input and the network output is computed. The best solution is selected from the neural network and the CBR solutions. This solution is evolved by genetic algorithm to become more compatible with the new problem. In [20], CBR and genetic algorithm are combined and when a new problem arises, some similar problems are generated using the genetic algorithm, which are then retrieved from the case-base and are used to solve the new problem. Thus, in this method, genetic algorithm is used in the Retrieve stage. In [22], CBR is applied for adjusting the control parameters of various optimization algorithms. Here it is assumed that the control parameters of optimization algorithms should be adjusted similarly when similar problems are to be solved. Thus, when a new problem arises, similar problems and their stored control parameters are retrieved from the case-base and the values of their control parameters are used for initializing the control parameters for the optimization algorithm.

All the works reviewed above solve optimization problems using CBR but none of them tries to address the two obstacles of CBR mentioned in Section 2. In this paper we propose an optimization method based on CBR and BOA algorithm in order to address these obstacles. In Section 5 we will describe how the application of BOA helps resolve these obstacles. 4. Bayesian optimization algorithm Evolutionary algorithms are a main group of algorithms presented for solving optimization problems. In recent years the attempts to improve the existing evolutionary algorithms and introducing new versions of these algorithms are on an increase. The main feature of evolutionary algorithms is their ability in fi nding the solutions X  building blocks. Building blocks show partial solutions of the problems [4]. Genetic algorithm [9] which is a kind of evolutionary algorithm manipulates building blocks implicitly through selection and crossover operators. Crossover operators sometimes end up breaking up the partial solutions of the problem and sometimes lead to premature convergence. The problem of building blocks destruction is called linkage problem [4]. There have been extensive attempts to prevent building blocks destruction and solving the linkage problem. One of the main groups of methods for solving the linkage problem is Estimation of Distribution Algorithms (EDAs) [8]. These algorithms are presented based on the genetic algorithm concepts but they use estimation of distribution of promising solutions for creating the candidate solutions instead of standard operators of genetic algorithms [18].

Bayesian Optimization Algorithm (BOA) is a speci fi c kind of EDAs proposed by Pelikan and Goldberg to solve a group of complicated problems ef fi ciently [19]. This algorithm evolves a population of candidate solutions through constructing Bayesian Networks and sampling the constructed Bayesian networks.

A Bayesian network is a Directed Acyclic Graph (DAG) that shows the causal relations between variables in a given data set. In this graph, each node is related to a variable in the modeled data set and the edges correspond to conditional dependencie s. A set of conditional probability tables is used for presenting the conditional independencies of the variables [19].
 BOA constructs the initial population randomly with a uniform distribution for al l possible solutions. Then the population is updated in several iterations. Each iteration includes four stages [19]. First, by using a selection method (like what is used in genetic algorithm), the promising solutions are selected from the population. In the second stage, a Bayesian network compatible with the population of promising solutions is constructed. In the third stage, the new candidate solutions are created based on sampling the Bayesian network. Finally, in the fourth stage, the new candidate solutions are incorporated in pre-existing solutions and rep lace all or some of them. Until th e termination cond ition is r eached, these four stages are repeated. Termination cond ition can be either of the c onvergence to homogenous solutions, or obtaining a speci fi c iteration number [18].

By using Bayesian networks, this algorithm is able to consider multivariate interactions between genes [18] and is known as one of the best EDA algorithms in the recent years. 5. Our proposed method
In our proposed method, CBR is combined with BOA to improve the fl exibility and generality in CBR for optimization problems and also to improve the quality of optimization solutions.
 As mentioned in Section 3, up to now CBR has been combined with several optimization algorithms. However, this method has not been combined with any of EDA algorithms yet. In this paper we show that the use of BOA as an EDA algorithm can lead to a positive impact on the CBR method. In the following, in part 5.1 the advantages of using BOA in CBR will be discussed. Afterward, our proposed method will be presented in details, in Section 5.2.
 5.1. The advantages of applying BOA in CBR
One special feature of BOA that is distinctive is that after solving an optimization problem, BOA leaves a Bayesian network describing the features of the obtained solution [16]. It seems that the Bayesian networks, left after solving various problems, are knowledge fi t to be stored in the case-base of a CBR system and may resolve the two dif fi culties of CBR menti oned in Section 2. We acclaim thi s based on the following two reasons:
First, in pervious CBR methods, combination and modi fi cation and adaptation of the retrieved solutions and their application in solving the new problems are designedspeci fi cally for eachproblem, the problem-speci fi c knowledge, considered. For example, each retrieved solution in the 0 X 1 knapsack problem is a set of selected objects but in the traveling salesman problem each retrieved solution is a cycle in a graph objects in the 0 X 1 knapsack problem and using them to solve the new problem is different from the stages in the traveling salesman problem (i.e., combination of cycles speci fi ed for previous graphs and constructing a new cycle for the new graph).Thus, the process of combination and adaptation of solutions should be designed separately for each kind of problem. However, in our proposed method (using BOA algorithm and retaining the Bayesian networks), for all kinds of problems the retrieved information of previous problems are Bayesian networks. Hence, a general and multi-purpose method can be designed for combining this knowledge in all kinds of problems. In this manner, our proposed method does not need to design a new way for reuse and Revise stages for each kind of problem and, consequently it has more generality than previous CBR methods.

Second, the Bayesian networks obtained after runs of BOA, describe the features of solutions statisti-cally and in an approximate manner. Thus, storing these networks in the case-base makes the case-base knowledge more fl exible. Adapting these approximate descriptions to solve the new problems is much easier than adapting the strict solutions stored in the case-bases of the basic CBR.

Because of the above mentioned two reasons, our proposed method is more fl exible and more general than former CBR methods and has the potential for resolving the CBR dif fi culties mentioned in Section 2. 5.2. The steps in our method
In our proposedmethod, the optimization problems are solved using BOA; then, the information related to the solved problems is saved in a case-base. This information for each problem includes the problem, the obtained solution, the Bayesian network of the last stage of BOA algorithm for that problem, and the obtained solution quality. Each time a new problem arises, problems which are similar to this new problem are retrieved from the case-base, and the knowledge related to their solutions will be used for the new run of BOA algorithm. For this purpose, the Bayesian networks of these retrieved problems are combined together and the initial population of BOA is generated by sampling the compound Bayesian network. BOA will start running from this initial population. This compound Bayesian network will be used for constructing the future probabilistic models of BOA during its run and when BOA is converged the solution of the new problem is obtained. In this manner, instead of using the direct solutions of the previous problems, a probabilistic description of the solutions is used to solve the new problems.
As mentioned in Section 2, the cycle of CBR includes four stages. These stages in our method are presented in Fig. 1 as a fl owchart. The pseudo code of the entire algorithm is presented in Fig. 2. The procedure of each stage will be discussed in the rest of this section. 5.2.1. Retrieve stage
Here a criterion for computing the degree of similarity between problems is considered based on the type of the problem. When a new problem arises, this criterion will be used to choose and retrieve similar problems from the case-base. A variety of methods have been introduced for computing the degree of similarity in CBR. For example, a similarity criterion for combinatorial optimization problems is presented in [6].

When a new problem arises, from problems stored in the case-base, those closer in similarity to the newly appearing problem will be selected for the Reuse stage according to the de fi ned similarity criterion. Each retrieved problem has four parts: the problem, the problem solution, the Bayesian network of the last stage of BOA algorithm used for solving the problem at hand, and the obtained solution quality. If one of the retrieved problems is identical to the new problem, the obtained solution for that problem in the case-base will be revealed; thus, the end of the process. Otherwise, the Reuse stage will begin to act.
If no existing case in the case-base is similar enough to the new problem, the BOA algorithm will begin running for solving that problem without using the previous knowledge. After obtaining the solution, Retain stage will begin to act. 5.2.2. Reuse stage
Here, the Bayesian networks of retrieved problems are combined using some impact coef fi cients to form a new Bayesian network. The impact coef fi cient of each Bayesian network is obtained through multiplying  X  X ts degree of similarity to the new problem X  by  X  X he solution quality of that problem X . For combining Bayesian networks there are various methods, some of which are presented in [3,10,12 X 14, 26]. In this paper, the method proposed in [3] has been used to combine the Bayesian networks. After Bayesian networks are combined, the initial popula tion of BOA algorithm is generated by sampling from this compound network.
 5.2.3. Revise stage
Whether the population gen erated in the previous stage incl udes a solution with an acceptabl e quality or not is examined in this stage. If an acceptable sol ution is found in this initia l generation, it will be reported and the Retain stage will begin to act. Other wise, this population will be regarded as the initial population of BOA. BOA will start its run from this population and end in the algorithm termination condition.

As mentioned before, in each iteration, BOA selects the best solutions of the current population and creates a Bayesian network to describe them. Then it creates the next generation through sampling from this network. In our proposed method, during the run of BOA algorithm, the compound Bayesian network generated in the Reuse stage is used for biasing these probabilistic models as well. Here, it is accomplished in this way that after a certain number of iterations of the algorithm (that is denoted by T in Fig. 2), the compound Bayesian network is recombined with the Bayesian network created for the next generation in that stage, and the resulted network will be used for creation of the new generation. In this combination, impact coef fi cients of the two networks are different. The more progress toward the fi nal stages of BOA, the less the effect of compound network. 5.2.4. Retain stage
To improve the CBR method under an incremental learning process, the case-base should gradually become more complete. If the introduced problem is a new one, after the solution is found, the problem and its solution as well as the last Bayesian network of BOA algorithm and the quality of the obtained solution will be stored in the case-base as a new entry. If in the Revise stage the acceptable solution is found in the initial generation, the compound Bayesian network will be stored in the case-base instead of the last Bayesian network of BOA. 6. Implementation
BOA algorithm has been implemented in C++ language based on the method proposed in [17] and has been used in the Revise stage of CBR. Each Bayesian network saved in the case-base includes some conditional probability tables and a graph that are all implemented by 2-dimentional arrays in C++. The case-base is designed as a fi le. After each run of BOA algorithm for solving a new problem, a record is of BOA algorithm and the quality of the obtained solution for that problem.

In our proposed method, when a new problem is introduced, the degree of its similarity to any of the existing problems in the case-base is computed. Similarity criterion between problems depends on the type of the problem. For evaluating our method, as discussed in Section 7, we use some combinatorial optimization problems as benchmark problems. Combinatorial optimization problems are solved by searching for the best con fi guration of a set of variables to reach a certain goal. As mentioned in the previous section, a similarity criterion for combinatorial problems is presented in [6]. In [6] it is supposed that variables of combinatorial problems that are developedthrough time and are to be solved, are selected from a large set containing N variables; therefore, each combinatorial problem can be regarded as an N-bit vector in which some members have value 1 (that means the corresponding variable exists in the combinatorial problem) and other members have value 0 (that means the corresponding variable does not exist in that combinatorial problem). The number of common variables existing in two combinatorial problems can be de fi ned as their degree of similarity.

In our method, problems that have most similarity to the new problem are retrieved (maximum number of retrieved problems is M ). The solution quality of each problem is multiplied by its degree of similarity to the new problem and the result will be regarded as the impact coef fi cient of that Bayesian network; then, the Bayesian networks of retrieved problems are combined together. In this combination each Bayesian network will be effective according to its impact coef fi cient degree. 7. Evaluation and the results
As mentioned before, the main purpose of our proposed method is twofold: to generalize the Reuse stage of CBR and to improve the fl exibility of stored knowledge. In addition to these objectives, it is necessary to evaluate the quality of solutions in our proposed method by comparing it to that of the basic CBR. Here, we evaluate our method by solving the 0 X 1 knapsack and traveling salesman problems.
The average quality of obtained solutions for som e randomly generated problems is used as the evaluation criterion; thus, our proposed method will be compared to the basic CBR with respect to the average quality of obtained solutions.

First, 0 X 1 knapsack problem is regarded as a combinatorial benchmark problem. This problem is de fi ned as selecting a subset of some existing objects to fi ll a knapsack to reach the maximum sum of objects pro fi ts. Instances of this problem were generated randomly using method [6]. To do this, 24 objects have been presumed each with a speci fi c weight and pro fi t. The average and variance of objects X  weights are 15 and 5 respectively. Also the average and variance of objects X  pro fi ts are 30 and 10 respectively. Then, each 0 X 1 knapsack problem was generated by choosing a subset of these objects randomly. In generating each problem, the pr obability of choosing each object is 0.5. Therefore, every randomly generated problem can be considered as a 24-bit binary vector in which 1s represent the selected members. The average number of 1s existing in each vector is 12. The capacity of knapsack is 250. This capacity is assumed constant for all of these problems. Totally 600 problems have been generated randomly in this manner.

The fi rst 300 problems were solved by BOA without using proposed CBR system. Then, these solved problems along with their solutions, the quality of obtained solutions and the Bayesian network of the last stage of BOA for that problem were stored in the case-base as 300 records. After that, the next 300 random problems were solved one by one using our proposed CBR system. In solving these problems, the degree of similarity for two problems is equal to the number of common objects of two problems according to method presented in [6].

For evaluating our proposed method, the fi rst 300 knapsack problems along with their solutions were saved in the case-base of the basic CBR. Then the next 300 problems, with preserving the order of their occurrence, were re-solved using CBR system presented in [6] and the average quality of their solutions was then computed. The results of these two methods have been compared.

We have tested several values for parameters M and L that are the maximal number of similar problems retrieved from the case-base and similarity threshold respectively. Each pair of values for M and L means that at most M problems will be retrieved from the case-base provided that their degree of similarity to the new problem exceeds the threshold L. For each setting of t hese parameters, t he averag e quality of solutions for 300 random knapsack problems has been computed. All the combinations of these parameters are tested; however, only some of the combinations of the parameters are shown in Fig. 3 in order to show how the values of the parameters have affected the performance of our method. The values of parameters M and L as well as their related average percentages of improvement in solutions quality compared to the basic CBR for the next 300 random 0 X 1 knapsack problems are presented in Fig. 3(a). As shown in this fi gure, when we are strict in selecting the similar problems and we retrieve fewer problems with higher degree of similarity to be combined, the probability of sticking in local optima and fi nding solutions with lower quality will increase. On the other hand, if the number of the selected similar problems increase in number, the impact of si milar problems X  features will become smaller and the BOA initial popul ation will be created rather randomly; therefore our proposed method does not lead to signi fi cant improvement in the solutions quality. The experience shows that 7 is the best value for L, and 5 is the best value for M in this case.

We compared our method to the basic BOA. In Fig. 3(b) and 3(c) the average percentages of improve-ment in solutions quality and algorithm runtime compared to the basic BOA for the next 300 random 0 X 1 knapsack problems are presented respectively.

In addition to 0 X 1 knapsack problem, all preceding stages are re peated for travelin g salesman problem (TSP). TSP is de fi ned as fi nding the shortest Hamiltonian cycle in a graph. Here, fi rst, 20 cities are distributed randomly on a surface. The average and the variance of the distances between each two nodes is 20 and 4 respectively. For generating each one of the 600 random problems, a subset of these cities was selected randomly. Each problem is describe d by a 20-bit binary vector where 1s represent the selected nodes. Again, the fi rst 300 problems were used for initializing the case-base of our method and the basic CBR. The next 300 problems were used for evaluation and comparison of these two methods.
Several values for parameters M and L in our proposed method and the average percentages of improvement in the solutions quality compared w ith the basic CBR for the next 300 random traveling salesman problems (see Fig. 4(a)). Just like to Fig. 3(a), if we retrieve more similar problems to be combined for solving the new problem, the average of solutions quality will be improved. If the number of the selected similar problems increases in number, the impact of similar problems X  features will become smaller. Here, the experience shows that 7 is the best value for L and 5 is the best value for M.
Figure 4(b) and 4(c) show the average percentages of improvement in solutions quality and algorithm runtime compared to the basic BOA for the 300 random travelling salesman problems respectively. 8. Conclusion
In this paper we introduced a method based on BOA algorithm for improving the CBR ef fi ciency in optimization problems. In our proposed method, after solving each optimization problem using BOA, the obtained Bayesian network along with the information related to the solved problem is retained in the case-base. Storing these Bayesian networks has two bene fi ts. First, the combining and reusing the knowledge and revising the obtained solutions can be accomplished in a uniform and problem-free manner through combining the Bayesian networks. Second, describing the knowledge of previously solved problems in the form of Bayesian networks leads to an increase in the stored knowledge fl exibility. In order to evaluate the solutions quality of this proposed method, it is compared to the basic CBR. For this purpose, the 0 X 1 knapsack and traveling salesman problems have been considered as two benchmark problems. The implementation results show improvement in solutions quality.
 With respect to the two dif fi culties of CBR in all of its applications, in our paper we only focus on the CBR applications that are related to optimization. For example, problems such as Scheduling, repair-based optimization, and query optimization can be solved by our method. However, our method cannot be used to solve the problems that are not considered as optimization problems; that is, our method is not a tool for fi nance forecasting, diagnosis problems, and cost estimation. Our method can be extended to address the mentioned CBR dif fi culties for other CBR applications in the future works. References
