
Finding relevant experts in a specific field is often cru-cial for consulting, both in industry and in academia. The aim of this paper is to address the expert-finding task in a real world academic field. We present three models for expert finding based on the large-scale DBLP bibliogra-phy and Google Scholar for data supplementation. The first, a novel weighted language model, models an expert candidate based on the relevance and importance of asso-ciated documents by introducing a document prior proba-bility, and achieves much better results than the basic lan-guage model. The second, a topic-based model, represents each candidate as a weighted sum of multiple topics, whilst the third, a hybrid model, combines the language model and the topic-based model. We evaluate our system using a benchmark dataset based on human relevance judgments of how well the expertise of proposed experts matches a query topic. Evaluation results show that our hybrid model out-performs other models in nearly all metrics.
Expert finding has received increased interest in recent years since the advent of the expert search task in the TREC Enterprise track [22]. The task of expert finding is to come up with a ranked list of experts with relevant expertise in a given topic. The current developments in expert search are concentrated in the Enterprise corpora known as TREC2005 [8, 9] and TREC2006 [21]. They have provided a common platform for researchers to empirically assess methods and techniques devised for expert finding. However, little work has been done on methods of finding experts in any spe-cific academic field, which is an important practical prob-lem. Identification of the persons that have expertise on a specific academic topic could be of great value in many ap-plications, for example, determining important experts for consultation by researchers embarking on a new research field, recommending panels of reviewers for state research grant applications [11], and assigning papers to reviewers automatically in a Peer-Review Process [17, 20].

As our approach is to deal with the expert-finding task in a real-world academic field, a key component is therefore the acquisition of a dataset replete with publications from which expertise can be accessed. The DBLP bibliography 1 is a good starting point for extracting the data needed for this application, as it contains more than 955,000 articles with over 574,000 authors from conferences and journals in the Computer Science field. In scientific research, the publications of a researcher could be assumed to be repre-sentative of his/her expertise [20]. One limitation of DBLP data is that each paper record only contains the title, which is too limited to calculate the relevance of papers to queries. To address this problem, Google Scholar [2] is utilized as a data supplement.
Figure 1. A query example with documents and authors.

Previous approaches in the TREC Enterprise Track [8, 9, 21] treat expert finding as an information retrieval task. One of the state-of-the-art approaches is based on a statis-tical language model to rank experts. The basic language model measures the relevance between a query and docu-ments, then models the knowledge of an expert from the associated documents [3, 4]. An illustration of a query ex-q ample is sketched in Figure 1. Here we suppose a query q has the same relevance probability (=0.1) to two documents d 1 and d 2 ; documents ( d 1 and d 2 ) are associated with au-thors ( a 1 and a 2 ) respectively (=0.2). In addition, d by 200 documents, while d 2 is cited by 10 documents. Ac-cording to the basic language model and the above infor-mation, the author a 1 has the same expertise as the author a 2 given the query q . However, when considering the cita-tion number, the document d 1 , which has the higher citation number, would be more important than d 2 . Therefore, intu-itively, it is more reasonable that a 1 (the author of d 1 higher probability of being an expert than a 2 (the author of d ) given the query q .

To address this problem, we introduce a novel weighted model to interpret the importance of the document d by in-troducing a prior probability p ( d ) , i.e., the prior probabil-ity of a document written by an expert, as shown in Fig-ure 2 (a). Let D be the set of related documents, and ca be an expert candidate. In most existing work [3, 21], such as document-based model, this probability is ignored or as-sumed to be uniform. However, as shown in Section 6.2, a reasonable prior can help improve the retrieval accuracy. In this paper, we assume such a prior probability is related to the importance of the document. Specifically, we con-struct a weighted language model to take into consideration not only the relevance between a query and documents but also the impact of the documents. Then the expertise of the authors could be deduced based on the overall aggregation of the relevance and the document priors. Our evaluation results indicate that it is very important to consider the prior probability.

Moreover, motivated by the observation that researchers usually describe their expertise as a combination of several topics, we investigate a topic-based model to associate the query with the expert candidates. As shown in Figure 2 (b), each expert candidate is represented in terms of mixing pro-portions of multiple topics (denoted as T ). So, the exper-tise given a query could be derived based on the proportion-ate aggregation of associated topics. Furthermore, we pro-pose a hybrid model to combine the language model with the topic-based model. Experimental results show that a weighted language model can improve the performance sig-nificantly compared to the baseline language model, while the topic-based model achieves competitive results with lan-guage model. Finally, the evaluation results of the hybrid model show that it outperforms the language model and the topic-based model.

The main contribution of this paper is to propose an ef-fective weighted language model, which introduces a docu-ment prior probability p ( d ) to model the importance of the document written by an expert. Another contribution of this paper is that we investigate a topic-based model to inter-pret the expert finding task, and then integrate the language model with the topic-based model.

The rest of this paper is organized as follows. We briefly summarize related work on expertise and the topic model in Section 2. In Section 3 we provide detailed descriptions of the expertise modeling based on the language model. Ad-vanced models including the topic-based model and the hy-brid model for expertise retrieval are presented in Section 4. Next, in Section 5, we define the experimental setup of our methods. Experimental results are presented in Section 6. We conclude and discuss future work in Section 7. The inclusion of expert finding in the TREC Enterprise Track has resulted in a great deal of work in this area. There are two principal approaches to expert modeling: query-dependent and query-independent. In both cases the expert-finding system has to discover documents related to a per-son and estimate the probability of that person being an expert from the text [18]. A query-independent method [5, 15, 10] directly models the profile (builds a  X  X irtual doc-ument X ) of a candidate based on all documents associated with the candidate and estimates the ranking score accord-ing to the profile in response to a user query. On the other hand, a query-dependent approach [3, 10] first ranks docu-ments in the corpus given a query topic, and then find the as-sociated candidates from the subset of retrieved documents. Both methods have advantages and disadvantages [18]. In terms of data management, query-independent profiles can be significantly smaller in size than the original corpus. However, the contribution of each document in a profile cannot be measured individually, and as a result, this ap-proach is less effective than other subsequent approaches. On the other hand, a query-dependent approach allows the application of advanced text modeling techniques in rank-ing individual documents.

Balog et al. [3] propose two language models in expert search and extensively compare the two methods. Their first model directly models the knowledge of an expert from as-sociated documents, while their second model first locates documents on the query topic and then finds the associated experts. Their experimental results show that the second model outperforms the first model. Petkova and Croft [18] propose a hierarchical approach, based on a combination of the above first model and second model. In contrast, the probabilistic approach proposed by Cao et al. [8] uses a two-stage language model of combining relevance model and co-occurrence model. In [15], Macdonald and Ounis present yet another approach based on a voting model for expert search. Later, they apply query expansion in [16] to enhance the expert search. Nearly all of the work has been evaluated on the W3C collection [23]. Balog et al. [4] fo-cus on expertise retrieval in an intranet that differs from the W3C setting.

The use of a topic model for information retrieval tasks is described in Wei and Croft [24]. The authors find that inter-polations between Dirichlet smoothed language models and topic models show improvements in retrieval performance above language models by themselves. Probabilistic latent semantic indexing (pLSI) is a widely used document model [12, 6]. Furthermore, Mimno and McCallum [17] propose the Author-Persona-Topic model to model the expertise of a person. However, computational complexity is often a big concern for topic models. Our proposed topic-based model can be simplified using a predefined topic set.

Despite all this work in expert finding, little work has been done in a specific academic domain, in terms of re-trieving experts given the topic. Recently, Li et al. [14] build an academic expertise oriented search service, includ-ing expert finding based on the DBLP bibliography. They propose a relevancy propagation-based algorithm using the co-authorship network for expert finding. In this paper we focus on expert finding in a real world academic field based on the DBLP bibliography.
In this section we detail the expert finding models and in-troduce a set of baseline approaches based on statistical lan-guage modeling; later, in Section 4, we turn our attention to advanced modeling approaches, including the topic-based model and the hybrid model.
Using language models for information retrieval has been studied extensively in recent years [13, 19, 25, 26]. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of their matching the query according to the es-timated language model. Several different methods have been applied to compute the query likelihood, i.e., the prob-ability of generating a query given the observation of a doc-ument.

In [3], Balog et al. formulate the problem of identify-ing experts for a given topic using a generative probabilistic model: what is the probability of a candidate ca being an ex-pert given the query topic q ? Thus, the task is to determine p ( ca | q ) , and rank candidates ca according to this probabil-ity. There are no restrictions on the form of the query topic q , which could consist of any terms or concepts; for in-stance,  X  X ata mining X  is a query to search experts who have expertise on the query topic  X  X ata mining X . Using Bayes X  theorem, the probability can be formulated as follows: where p ( ca, q ) is the joint probability of a candidate and a query, p ( q ) is the probability of a query. Since p ( q ) is a constant, it can be ignored for ranking purposes. The proba-bility p ( ca | q ) can be reformulated to estimate the joint prob-ability p ( ca, q ) . The basic language model used to estimate the probability p l ( ca, q ) can be defined as follows: where p ( d ) is the prior probability of a document, and the supporting documents D act as a  X  X ridge X  to connect q and ca . Under this model, the process of finding an expert is as follows: given a collection of documents ranked accord-ing to the query, we examine each document relevant to the query, and then we note the authors associated with that document. Here, the process is taken to the extreme where we consider all documents in the collection.

To determine the probability of a query given a docu-ment, we infer a document language model  X  d for each doc-ument, where p ( t |  X  d ) is the maximum likelihood estimate of the term in a document d , and n ( t, q ) is the number of times that term t occurs in query q . This model is drawn from Balog et al. [3]. The likelihood of a query q consisting of some number of terms t for a document d under a language model with Jelinek-Mercer smoothing [26] is We follow Balog et al. in setting  X  =0 . 5 .

Suppose that we make the assumption that the candidate ca is conditionally independent of the query q given a doc-ument d ; that is In our setting it is reasonable to assume that candidate ca has knowledge about the topic described in the document d if candidate ca is an author of document d . Now, in the case of a multi-author paper, one author with many co-authors may have less association p ( ca | d ) on average than a sole author. To account for this effect, we weight the association inversely according to the number of co-authors as follows. Suppose a document has n authors in total, we assume that each author has the same knowledge about the topics de-scribed in the document, where n d is the number of authors, and p ( ca | d ) is used to measure the document-candidate association.

The document priors p ( d ) are generally assumed to be uniform and thus will not influence the ranking. The final estimation of the baseline language model is obtained by substituting p ( q |  X  d ) for p ( q | d ) into Eq. 2, where rank = means  X  X quivalence for ranking the candidates X . We refer to this method as a baseline language model for expert finding.
The language model described above calculates the rel-evance between a query and a document, but it ignores the prior of the document. As shown in Figure 1, suppose there are only two documents d 1 and d 2 , d 1 with one author a and d 2 with one author a 2 ; both documents have similar contents, i.e., the query likelihoods are almost the same ( p ( q | d 1 )= p ( q | d 2 ) ), but the two documents have differ-ent importance, I ( d 1 ) &gt;I ( d 2 ) . Which document is more reasonable to rank to the top? Which author has the higher probability of being an expert given the query topic? Obvi-ously, we would prefer to rank the more important one ( d at the top, and author a 1 would have the higher probabil-ity of being an expert than author a 2 on topic q based on the assumption that the more important document has more weight. To the best of our knowledge, the language mod-els currently do not take this factor into account. We in-troduce a weight factor w d to denote the importance of the document, which, theoretically, can be interpreted as being proportional to the document prior p ( d ) , where C ( = d  X  D w d ) is a constant normalization factor.
For our model, the weight factor is estimated using the citation number, and transformed using two logarithm func-tions: the common logarithm (B2), and the natural loga-rithm (B3). We can see that this is exactly the method of the basic language model when the uniform weight w d is set to 1 (B1). Three different methods to measure the weight are defined as follows, where c d ( c d  X  0 ) is the citation number of the document d , and the constants 10 and e are used to guarantee the weight factor not to be less than 1. The citation numbers are ob-tained from Google Scholar [2].

The final estimation of the weighted language model is In Section 6.2, we compare the performance of weighted language models with different weighting methods, namely B 1 , B 2 and B 3 .
Now that our language modeling techniques have been developed for expertise retrieval, we proceed to introduce our topic-based model. Also, in this section, a hybrid model is presented which combines the language model with the topic-based model. 4.1.1 Model Form In this approach, as illustrated by the model in Figure 2 (b), each candidate is represented as a weighted sum of multiple topics, and there is an implicit relation between the query and the topic z in terms of the probability p ( q | z ) .
In our topic-based model, a candidate ca and a query q are conditionally independent given a latent topic z : where p ( z, ca ) is the joint probability of the topic z and the candidate ca , and p ( q | z ) represents the probability of a query q generated by the topic z . Computational complexity is often a big concern for topic models, especially when the dataset is large-scale and a great number of latent variables are used. Our topic-based model can therefore be simplified if a well-defined topic set is available, and it can be viewed as query expansion.

If such a well-defined topic set is available, the proba-bility p ( z, ca ) could be estimated using the method as de-scribed in Section 3. In addition, we need to associate the well-defined topic z with the query topic q . Therefore, given a well-defined topic, we can interpret this topic by collecting a number of most relevant papers as the support-ing representation of the topic z . We denote this as  X  z substitute  X  z for z into Eq. 11. Meanwhile, the probabil-ity p ( q |  X  z ) could be measured using Eq. 3, which indicates the correlation between the query q and the topic z , and the higher probability corresponds to the stronger connec-tion. Thus, the probability can also be regarded as the sim-ilarity between each other. Now the remaining problem is transformed to find the most relevant or similar topics to the original query. 4.1.2 Topic Selection Algorithm The challenge now is to consider what similar topics the candidate would satisfy, and then use the entire subset of topics similar to the original query to measure the joint probability of a query and a candidate. We advocate three methods for selecting the similar topics (the subset of top-ics) and estimating this probability.

The first method is conceptually simpler, and assumes that those topics are independent. Let Z = { z 1 ,z 2 , ..., z be the set of all predefined topics. Then, one natural way to select topics similar to the query is to calculate the similarity score p ( q |  X  z ) between the query and topics as the ranking function, and use the top K ranked topics as the similar topics. We denote this method as T1.

By definition of Eq. 11, the topic model builds on a kind of independent relevance assumption: there is no spill-over of expertise across predefined topics. This assump-tion rarely holds in reality. Intuitively, it is desirable for the selected similar topics to include topics from many differ-ent subtopics and undesirable that they include many top-ics that redundantly cover the same subtopics. However, the first method T1 may select a subset of topics with high redundancy, which may induce expertise topic drift in the topic-based model. To take into account of the topic de-pendence, we consider another method of selecting the sub-set of topics, which approximately satisfies the independent relevance assumption, from all the predefined topics.
To obtain such similar topics, a greedy algorithm is de-signed to select topics one by one according to the given query. Suppose we have selected several topics z 1 , ..., z the next topic z i should cover many subtopics not covered by the previous topics, and few of the subtopics covered by the previous topics. It can be formulated using a conditional probability function value ( q | z i ; z 1 , ..., z i  X  1 tify the novelty and penalize the redundancy of a topic z for rank i . The detailed algorithm is shown in Algorithm 1. In each round, the algorithm tries to exhaustively search for the topic with the maximum value function. In the course of K rounds, we get K similar topics. Note that the algorithm will stop when the value function is less than or equal to 0.
Since it is difficult to represent the value function explic-itly, some kind of approximation is necessary in practice. An approach for approximating the value function is de-fined as follows: value ( q | z i ; z 1 , ..., z i  X  1 )  X  p ( q | z i )  X  max where max j&lt;i p ( z i | z j ) is the maximum similarity between z and the previously selected similar topics. We denote this method as T2 . Another way to approximate the value function is formulated as follows: value ( q | z i ; z 1 , ..., z i  X  1 )  X  p ( q | z i )  X  We denote the method using Eq. 13 as T3 . Once we obtain the subset of topics, then we use Eq. 11 to calculate the joint probability p t ( q, ca ) .
 Algorithm 1 Greedy Selection Algorithm Inputs: Predefined topic set Z , select topic size K 1: for i =1 ... K do 2: Select a well-defined topic z i from unselected topic 3: Prerequisite: value ( q | z i ; z 1 , ..., z i  X  1 ) &gt; 0 , other-4: Update the topic set: 5: end for Return the topics { z 1 ,z 2 , ..., z K } .
To improve the performance, a hybrid model is utilized to aggregate the advantage of the language model and the topic-based model. Consider the probability of a candidate ca being an expert given the query topic q : this can be mod-eled by interpolating between the language model p l ( q, ca ) and the topic-based model p t ( q, ca ) , as follows: In Section 6.4 we compare the performance of the hybrid model with the pure language model and the topic-based model, and demonstrate that the hybrid model can provide more accurate results than the pure approaches.
In the following experiments we compare the three dif-ferent expert finding models through an empirical evalua-tion. In this section we define the experimental setup, while the evaluation results are presented in Section 6.
We have defined the following task: given a query and a set of expert candidates, the system has to retrieve a list of experts that have expertise in the given area. In the rest of this section, we introduce the DBLP and topic collection, the assessments and evaluation metrics.
A key aspect of finding experts from bibliographic data is therefore the acquisition of a dataset replete with publi-cations from which expertise can be derived. As of Novem-ber 2007, DBLP XML records contain over 955,000 arti-cles related to Computer Science, originally published in conferences, journals, books etc., adding up to 414.5MB. In total we gather more than 574,000 author names from DBLP XML records, each of whom can be an expert candi-date. Although DBLP is a good starting point for obtaining expert candidates and publications, several challenges ex-ist due to its limitations. One limitation is that each DBLP record provides the paper title without the abstract and in-dex terms. The information provided by the title is too lim-ited to represent the paper; some more expanded informa-tion is required. Generally, the abstract and index terms are useful to represent the paper for estimating the probability of a query or topic given the paper.

To obtain the abstract and index terms for each DBLP record, one natural way is to fetch them automatically from digital libraries such as ACM, IEEE, Springer, etc. We note, however, that it is very hard to obtain the complete meta-data (the abstracts and index terms of publications) for all the DBLP records. Thus Google Scholar is utilized for data supplementation as shown in Figure 3: for a document d , we use the title as the query to search in Google Scholar and select the top 10 returned records which are most rel-evant to the query title; next, these records combined with the publication title are viewed as the representation of the publication d . The metadata (HTML pages) crawled from Google Scholar is up to 20GB. This process is done auto-matically by a crawler and a parser, and the citation number of the publication d in Google Scholar is obtained at the same time. The total number of valid papers after this pro-cess is 953,774, and the number of valid authors is 574,369.
For the topic-based model, an important task is to collect the predefined topics related to Computer Science. From eventseer.net [1], an interesting website that tracks upcom-ing Computer Science research events, one can obtain an updated repository of 2,498 well-defined topics. Table 1 shows a snippet of topics from eventseer.net. Working with this list of topics, we also use the method based on Google Scholar to crawl the top 100 returned records as the sup-porting representation of each topic. The statistics of DBLP and the topic collection are shown in Table 2.

Table 2. Statistics of DBLP and the topic col-lection.

It is difficult to evaluate the quality of query/expert rele-vance rankings due to the scarcity of data that can be exam-ined publicly. The ground truth is manually created through the method of pooled relevance judgments together with human judgments. For each query, the top authors from the computer science bibliography search engines (such as CiteSeer 2 ,Libra 3 , and Rexa 4 ) and the committees of the top conferences in the topic were taken to construct the pool. Some researchers were then asked to assess each of the rec-ommended candidates in context of the query. To help them in their task, those researchers were presented with publica-tions and a description relating to each author. They could access and find additional content directly on a search en-gine when needed.

Such a benchmark dataset with expert lists (for expert finding) has been collected in Tsinghua university [27]. Their assessments were carried out mainly in terms of how many publications an expert candidate has published, how many publications are related to the given query, how many top conference papers he/she has published, and what dis-tinguished awards he/she has been awarded. Four grade scores (3, 2, 1, and 0) were assigned respectively represent-ing top expert, expert, marginal expert, and not expert. Fi-nally, the judgment scores (at levels 3 and 2) were averaged to construct the final ground truth. The data set contains 7 query topics and creates 7 expert lists. Table 3 shows the details of the dataset.
For the evaluation of the task, we adopted three metrics that capture different aspects of the performance of our pro-posed models.
 Precision at rank n (P@n) Precision at rank n measures Mean Average Precision (MAP) For a single query, aver-Bpref Bpref [7] is the score function of the number of non-
The presentation of the evaluation results is organized in the following five subsections. First we evaluate the ef-fectiveness of the representation of each publication using Google Scholar. Then we report the results for the language models and compare the three weighting methods in Sec-tion 6.2. In Section 6.3, we examine the performance of the different topic-based models. Section 6.4 discusses the re-sults for the hybrid models, in comparison to the pure lan-guage models and topic models. Finally, we compare our models with other methods. The evaluation results shown in this section are the average results.
As described in Section 5.1, the DBLP records only con-tain the publication titles. We present a new and effective representation for a publication based on Google Scholar. In order to compare the performance of the two represen-tations, we set up two corpora for evaluation. One corpus (Title) is collected only using the publication title, while the other corpus (GS) is built based on the supplemental representation using Google Scholar. Preliminary experi-ments are performed on these two corpora using the basic language model (B1). The comparison results are reported in Table 4. It is clear that the results of  X  X S X  are much better than those of  X  X itle X , which indicates that it is more effec-tive to represent publications using Google Scholar as a data supplement. Thus the  X  X S X  corpus is used in the following parts.

Table 4. Evaluation results on two corpora (%).
 Title 57.14 42.86 40.00 38.65 22.92 30.05 GS 61.43 50.71 42.38 43.21 30.38 35.95
Table 5. Evaluation results of language mod-els using different weighting methods (%).
 Best scores are in boldface.
 B1 61.43 50.71 42.38 43.21 30.38 35.95 B2 65.71 53.57 48.10 44.42 32.03 37.30
B3 68.57 57.86 46.19 44.48 32.39 37.70
In this subsection we evaluate the performance of the language models and compare the three different weighting methods. Table 5 shows the results for the different meth-ods on the test collection, where B1 represents the baseline method with uniform weight w d =1 , B2 is the method with the common logarithm weight w d = log(10+ n d ) and B3 is the method with natural logarithm weight w d =ln( e + n d
First, we inspect the absolute performance of the meth-ods. For the precision P@10, the basic language model B1 only achieves 61.43%, and the weighted language mod-els B2 and B3 can enhance the precision significantly to 65.71% and 68.57%. For the mean average precision (MAP), we measure a precision of 32.39% for B3, and 30.38% for B1, which indicates that B3 improves the MAP measurement by 6.7%. When looking at the overall per-formance, we observe that weighted language models B3 and B2 outperform the basic language model B1 on all the metrics from P@10 to bpref. Comparing the two weighted language models, method B3 is better than method B2 in most cases.

According to the experimental results, we can argue that it is very important to consider the prior probability of the document; our weighted language model performs very well and achieves much better performance than the basic language model. Based on the outcomes of our experi-ments, we use the weighting method B3 in the following parts of the section.
We now turn our attention to the performance of our topic-based models. In this subsection, we evaluate and
Table 6. Evaluation results of topic-based models using different numbers of similar topics (%).
 Model: T1 K=5 62.86 52.14 43.33 40.98 29.02 34.39 K=10 62.86 50.71 43.81 39.56 28.19 33.46 K=20 58.57 48.57 42.38 37.82 26.50 31.87 K=40 57.14 47.14 39.05 37.02 24.83 29.83 K=100 50.00 40.71 36.19 33.32 21.09 26.61 Model: T2 K=5 68.57 55.71 46.19 43.40 31.45 37.00 K=10 70.00 55.71 46.19 43.40 31.51 37.01 K=20=40=100 the same results as K=10 Model: T3 K=5 70.00 56.43 46.19 44.11 31.86 37.39
K=10=20=40=100 the same results as K=5 compare the three topic-based models introduced in Sec-tion 4.1, varying the number of topics ( K ) from 5 to 100. For T1, K denotes the top ranked topics. However, in T2 and T3, K represents the number of selected topics in Algo-rithm 1, and the algorithm may stop before completing K rounds. In this event, we denote round i as a cut-off point, and the result in this point may be close to the best perfor-mance in a sense. Table 6 shows the detailed results using different values for K .

A quick scan of Table 6 reveals that T2 and T3 always outperform T1 for all settings. Figure 4 compares the per-formance of the three topic-based models with different numbers of topics, using different metrics. For T1, increas-ing the number of topics was not of benefit to the perfor-mance. In fact, the results of T1 are inversely related to the number of topics, and the best results were achieved when K was equal to 5. In contrast, for T2, we witness improve-ments with increasing number of topics in some cases, and the best results were achieved when K was 10. In terms of the precision P@10, T2 and T3 achieved higher perfor-mance than the best language model B3.

Importantly, the number of topics will be cut off auto-matically when K is larger than 10 in T2. For T3, the cut-off point is 5. The differences between T1 and T2/T3 are significant for different numbers of similar topics. As ex-pected, it may mean that T2 and T3 perform better since it reduces the redundancy between the selected similar topics.
In this section, we evaluate the hybrid model by tuning  X  from 0 to 1 with increments of 0.1. A hybrid model can
Table 7. Evaluation results of hybrid models (%). Best scores are in boldface.
 Hybrid Model H2: (B3 + T2 with K=10) B3 68.57 57.86 46.19 44.48 32.39 37.70 T2 70.00 55.71 46.19 43.40 31.51 37.01 H2 71.43 57.86 46.19 44.82 32.60 37.98
Table 8. Example results of the hybrid model showing the top five experts for several queries, based on the DBLP dataset.
 Q1: Boosting Q2: Data Mining Robert E. Schapire Jiawei Han Yoav Freund Mohammed Javeed Zaki Yoram Singer Rakesh Agrawal Manfred K. Warmuth Heikki Mannila Nader H. Bshouty Philip S. Yu Q3: Information Extraction Q4: Semantic Web Ellen Riloff Dieter Fensel Dayne Freitag James A. Hendler Stephen Soderland Katia P. Sycara Raymond J. Mooney Amit P. Sheth Andrew McCallum Ian Horrocks Q5: Support Vector Machine Q6: Computer Vision Bernhard Sch  X  olkopf Azriel Rosenfeld Vladimir Vapnik Robert M. Haralick Alex J. Smola Michael Brady Ingo Steinwart Dana H. Ballard
Thorsten Joachims Thomas S. Huang consist of any language models and topic-based models. We restrict ourselves to the combination of the best perform-ing language model B3 and the topic-based model T2 with K equal to 10, namely H2. For all the measure metrics, H2 returns the highest performance when  X  =0 . 6 .Ta-ble 7 reports detailed results of the hybrid model H2 with weight  X  =0 . 6 compared to B3 and T2. The improvement of the hybrid model H2 is relatively small, as the perfor-mances of the model T2 and the model B3 are very similar. However, for the top 10 precision (P@10), H2 outperforms both T2 and B3, improving the precision from 68.57% to 71.43%. In general, the evaluation results show that our hy-brid model outperforms the pure language model and topic-based model in most of the metrics.

For illustration, we show six examples of the top 5 ex-perts in Table 8, where the query samples are  X  X oosting X ,  X  X ata Mining X ,  X  X nformation Extraction X ,  X  X emantic Web X ,  X  X upport Vector Machine X , and  X  X omputer Vision X .

Table 9. Evaluation results of our language models and the method TS (%). Best scores are in boldface.
 TS 48.00 40.40 36.15 -11.03 16.11 B1 53.85 43.46 39.74 22.40 11.36 17.16 B2 59.23 50.77 43.33 23.93 13.33 18.94
B3 60.77 51.54 43.85 24.83 13.92 19.67
To compare with the approaches proposed by [14, 27], we set up the experiments with the same benchmark dataset, which contains 13 query topics and corresponding expert lists. We denote their method as TS . Table 9 shows the evaluation results of our language models and the method TS reported by Tsinghua. Clearly, our results are much bet-ter than the method TS for all the metrics. For our language models, we observe that method B3 outperforms B2, and B2 outperforms B1. These results are consistent with the results shown in Section 6.2.
We presented our three expert-finding models, whose purpose is to retrieve experts in specific academic domains based on the DBLP bibliography and Google Scholar for data supplementation. Our models include the statisti-cal language model, the topic-based model and the hybrid model. More specifically, we proposed a weighted language model and a topic-based model with predefined topics. We have shown in our evaluation results that, in general, the weighted language model improves the performance signif-icantly compared to the baseline language model. In terms of the topic-based model, we have proposed three methods to search for experts. As expected, the topic-based models T2 and T3, which reduce the redundancy within the subset of topics, perform much better than T1. Finally, the evalu-ation results of our hybrid model show that it outperforms the pure language model and the topic-based model.
Our current expert-finding approaches in the DBLP dataset only consider the publications of the experts. To further improve the performance of our methods, we plan to take into account other types of information in future work, such as profiles of the researchers and social information.
This work is fully supported by two grants from the Re-search Grants Council of the Hong Kong Special Admin-istrative Region, China (Project No. CUHK 4125/07E and Project No. CUHK4150/07E).

