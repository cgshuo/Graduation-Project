 A num ber of vertical mining algorithms have been pro-posed recen tly for asso ciation mining, whic h have sho wn to be very e ectiv e and usually outp erform horizon tal approac hes. The main adv antage of the vertical format is supp ort for fast frequency coun ting via intersection operations on transaction ids (tids) and automatic prun-ing of irrelev ant data. The main problem with these approac hes is when intermediate results of vertical tid lists become too large for memory , thus a ecting the algorithm scalabilit y.

In this pap er we presen t a novel vertical data repre-sen tation called Di set , that only keeps trac k of di er-ences in the tids of a candidate pattern from its generat-ing frequen t patterns. We sho w that di sets drastically cut down the size of memory required to store interme-diate results. We sho w how di sets, when incorp orated into previous vertical mining metho ds, increase the per-formance signi can tly.
 H.2.8 [ Database Managemen t ]: Data Mining Di sets, Frequen t Itemsets, Asso ciation Rule Mining
Mining frequen t patterns or itemsets is a fundamen-tal and essen tial problem in man y data mining appli-cations. These applications include the disco very of asso ciation rules, strong rules, correlations, sequen tial rules, episo des, multi-dimensional patterns, and man y This work was supp orted in part by NSF CAREER Aw ard IIS-0092978, DOE Career Aw ard DE-F G02-02ER25538, and NSF gran t EIA-0103708.
 other imp ortan t disco very tasks [10]. The problem is form ulated as follo ws: Giv en a large data base of item transactions, nd all frequen t itemsets, where a frequen t itemset is one that occurs in at least a user-sp eci ed percen tage of the data base.

Most of the prop osed pattern-mining algorithms are a varian t of Apriori [1]. Apriori emplo ys a bottom-up, breadth-rst searc h that enumerates every single fre-quen t itemset. The pro cess starts by scanning all trans-actions in the data base and computing the frequen t items at the bottom. Next, a set of poten tially frequen t candidate 2-itemsets is formed from the frequen t items. Another database scan is made to obtain their supp orts. The frequen t 2-itemsets are retained for the next pass, and the pro cess is rep eated until all frequen t itemsets have been enumerated. The Apriori heuristic achiev es good performance gain by (possibly signi can tly) reduc-ing the size of candidate sets. Apriori uses the down-war d closur e prop erty of itemset supp ort to prune the searc h space | the prop erty that all subsets of a fre-quen t itemset must themselv es be frequen t. Thus only the frequen t k -itemsets are used to construct candidate ( k + 1)-itemsets. A pass over the database is made at eac h level to nd the frequen t itemsets among the can-didates.

Apriori-inspired algorithms [18, 14, 5, 13] sho w good performance with sparse datasets suc h as mark et-bask et data, where the frequen t patterns are very short. How-ever, with dense datasets suc h as telecomm unications and census data, whic h have man y, long frequen t pat-terns, the performance of these algorithms degrades in-credibly . This degradation is due to the follo wing rea-sons: these algorithms perform as man y passes over the database as the length of the longest frequen t pattern. This incurs high I/O overhead for scanning large disk-residen t databases man y times. Secondly , it is compu-tationally exp ensiv e to chec k a large set of candidates by pattern matc hing, whic h is specially true for mining long patterns; a frequen t pattern of length m implies the presence of 2 m 2 additional frequen t patterns as well, eac h of whic h is explicitly examined by suc h algo-rithms. When m is large, the frequen t itemset mining metho ds become CPU bound rather than I/O bound.
There has been recen t interest in mining maximal fre-quen t patterns in \hard" dense databases, where it is simply not feasible to mine all possible frequen t item-sets; in suc h datasets one typically nds an exp onen-tial num ber of frequen t itemsets. For example, nding long itemsets of length 30 or 40 is not uncommon [4]. Metho ds for nding the maximal elemen ts include All-MFS [9], whic h is a randomized algorithm to disco ver maximal frequen t itemsets. The Pinc er-Se arch algo-rithm [12] not only constructs the candidates in a bottom-up manner like Apriori , but also starts a top-do wn searc h at the same time. This can help in reducing the num-ber of database scans. MaxMiner [4] is another algo-rithm for nding the maximal elemen ts. It uses e-cien t pruning based on lookaheads to quic kly narro w the searc h. DepthPro ject [2] nds long itemsets using a depth rst searc h of a lexicographic tree of itemsets, and uses a coun ting metho d based on transaction pro-jections along its branc hes. Ma a [6] also uses sev eral pruning strategies, uses vertical bit-v ector data format, and compression and pro jection of bitmaps to impro ve performance. GenMax [8] is a bac ktrac k searc h based algorithm for mining maximal frequen t itemsets. Gen-Max uses a num ber of optimizations to prune the searc h space. It uses a novel technique called progressiv e fo-cusing to perform maximalit y chec king, and uses di -set propagation to perform fast frequency computation. Finally , FPgro wth [11] uses the novel frequen t pattern tree (FP-tree) structure, whic h is a compressed repre-sen tation of all the transactions in the database. It uses a recursiv e divide-and-conquer and database pro jection approac h to mine long patterns.

Another recen t promising direction is to mine only closed sets. It was sho wn in [15, 21] that it is not necessary to mine all frequen t itemsets, rather frequen t close d itemsets can be used to uniquely determine the set of all frequen t itemsets and their exact frequency . Since the cardinalit y of closed sets is orders of magni-tude smaller than all frequen t sets, even dense domains can be mined. The adv antage of closed sets is that they guaran tee that the completeness prop erty is preserv ed, i.e., all valid asso ciation rules can be found. Note that maximal sets do not have this prop erty, since subset coun ts are not available. Metho ds for mining closed sets include the Apriori-based A-Close metho d [15], the Closet algorithm based on FP-trees [16] and Charm [23].
Most of the previous work on asso ciation mining has utilized the traditional horizon tal transactional database format. However, a num ber of vertical mining algo-rithms have been prop osed recen tly for asso ciation min-ing [22, 20, 7, 23, 8, 6] (as well as other mining tasks like classi cation [19]). In a vertical database eac h item is asso ciated with its corresp onding tidset, the set of all transactions (or tids) where it app ears. Mining algo-rithms using the vertical format have sho wn to be very e ectiv e and usually outp erform horizon tal approac hes. This adv antage stems from the fact that frequen t pat-terns can be coun ted via tidset intersections, instead of using complex internal data structures (candidate gen-eration and coun ting happ ens in a single step). The horizon tal approac h on the other hand requires com-plex hash/searc h trees. Tidsets o er natural pruning of irrelev ant transactions as a result of an intersection (tids not relev ant drop out). Furthermore, for databases with long transactions it has been sho wn using a sim-ple cost mo del, that the the vertical approac h reduces the num ber of I/O operations [7]. In a recen t study on the integration of database and mining, the Vertic al algorithm [17] was sho wn to be the best approac h (bet-ter than horizon tal) when tigh tly integrating asso ciation mining with database systems. Also, VIPER [20], whic h uses compressed vertical bitmaps for asso ciation mining, was sho wn to outp erform (in some cases) even an op-timal horizon tal algorithm that had complete a priori kno wledge of all frequen t itemsets, and only needed to nd their frequency . MAFIA [6] and SPAM [3] use ver-tical bit-v ectors for fast itemset and sequence mining resp ectiv ely.

Despite the man y adv antages of the vertical format, when the tidset cardinalit y gets very large (e.g., for very frequen t items) the metho ds start to su er, since the intersection time starts to become inordinately large. Furthermore, the size of intermediate tidsets generated for frequen t patterns can also become very large, re-quiring data compression and writing of temp orary re-sults to disk. Thus (esp ecially) in dense datasets, whic h are characterized by high item frequency and man y pat-terns, the vertical approac hes may quic kly lose their ad-vantages.

In this pap er we presen t a detailed evaluation of a novel vertical data represen tation called di set , that only keeps trac k of di erences in the tids of a candidate pattern from its generating frequen t patterns. We sho w that di sets drastically cut down (by orders of magni-tude) the size of memory required to store intermediate results. The initial database stored in di set format, in-stead of tidsets can also reduce the total database size. Thus even in dense domains the entire working set of patterns of sev eral vertical mining algorithms can t entirely in main-memory . Since the di sets are a small fraction of the size of tidsets, intersection operations are performed extremely fast! We sho w how di sets im-pro ve by sev eral orders of magnitude the running time of vertical algorithms like Eclat [22] that mines all fre-quen t itemsets. These results have not been previously published. We also compare our di set-based metho ds against Vip er [20], and against FPGro wth [11]. While we have previously used di sets for closed [23] and max-imal pattern mining [8], the detailed exp erimen tal eval-uation of di sets has not been presen ted before.
Asso ciation mining works as follo ws. Let I be a set of items, and T a database of transactions, where eac h transaction has a unique iden ti er ( tid ) and con tains a set of items. A set X I is also called an itemset , and a set Y T is called a tidset . An itemset with k items is called a k -itemset. For con venience we write an itemset f A; C; W g as AC W , and a tidset f 2 ; 4 ; 5 g as 245. The supp ort of an itemset X , denoted ( X ), is the num ber of transactions in whic h it occurs as a subset. An item-set is frequent if its supp ort is more than or equal to a user-sp eci ed minimum supp ort ( min sup ) value, i.e., if ( X ) min sup . As a running example, consider the database sho wn in Figure 1. There are ve dif-feren t items, I = f A; B; C; D; E g and six transactions T = f 1 ; 2 ; 3 ; 4 ; 5 ; 6 g . The table on the righ t sho ws all 19 frequen t itemsets con tained in at least three transac-tions, i.e., min sup = 50%. A frequen t itemset is called maximal if it is not a subset of any other frequen t item-set. A frequen t itemset X is called close d if there exists no prop er sup erset Y X with ( X ) = ( Y ).
 An asso ciation rule is an expression X s;c ! Y , where X and Y are itemsets. The rule's supp ort s is the join t probabilit y of a transaction con taining both X and Y , and is given as s = ( XY ). The con dence c of the rule is the conditional probabilit y that a trans-action con tains Y , given that it con tains X , and is given as c = ( XY ) = ( Y ). A rule is frequen t if its supp ort is greater than min sup , and strong if its con -dence is more than a user-sp eci ed minim um con dence ( min conf ).

Asso ciation mining involves generating all rules in the database that have a supp ort greater than min sup (the rules are frequen t) and that have a con dence greater than min conf (the rules are strong). The main step in this pro cess is to nd all frequen t itemsets having minim um supp ort. The searc h space for enumeration of all frequen t itemsets is given by the powerset P ( I ) whic h is exp onen tial (2 m ) in m = jIj , the num ber of items. Since rule generation is relativ ely easy , and less I/O intensiv e than frequen t itemset generation, we will focus only on the rst step in the rest of this pap er.
Figure 2 also illustrates some of the common data for-mats used in asso ciation mining. In the traditional hor-izon tal approac h, eac h transaction has a tid along with the itemset comprising the transaction. In con trast, the vertical format main tains for eac h item its tidset, a set of all tids where it occurs. Most of the past researc h has utilized the traditional horizon tal database format for mining; some of these metho ds include Apriori [1], that mines frequen t itemsets, and MaxMiner [4] and DepthPro ject [2] whic h mine maximal itemsets. No-table exception to this trend are the approac hes that 6 5 4 3 2 1 use a vertical database format, whic h include Eclat [22], Charm [21], and Partition [18]. Vip er [20] and Ma a [6] use compressed vertical bitv ectors instead of tidsets. Our main focus is to impro ve upon metho ds that utilize the vertical format for mining frequen t patterns. Let I be the set of items. De ne a function p : P ( I ) N 7! P ( I ) where p ( X; k ) = X [1 : k ], the k length pre x of X . De ne an equiv alence relation k on the subset tree as follo ws: 8 X; Y 2 P ( I ) ; X k Y , p ( X; k ) = p ( Y; k ). That is, two itemsets are in the same class if they share a common k length pre x. k is called a pre x-b ased equiv alence relation [22].

The searc h for frequen t patterns tak es place over the subset (or itemset) searc h tree, as sho wn in Figure 3 (boxes indicate closed sets, circles the maximal sets and the infrequen t sets have been crossed out). Eac h node in the subset searc h tree represen ts a pre x-based class. As Figure 3 sho ws the root of the tree corresp onds to the class f A; C; D; T; W g , comp osed of the frequen t items in the database (note: all these items share the empt y pre x in common). The leftmost child of the root con-sists of the class [ A ] of all subsets con taining A as the pre x, i.e. the set f AC ; AD ; AT ; AW g , and so on. At eac h node, the class is also called a combine-set . A class represen ts items that the pre x can be extended with to obtain a new frequen t node. Clearly , no subtree of an infrequen t pre x has to be examined.

The power of the equiv alence class approac h is that it breaks the original searc h space into indep endent sub-problems. For the subtree rooted at A , one can treat it as a completely new problem; one can enumerate the patterns under it and simply pre x them with the item A , and so on. The branc hes also need not be explored in a lexicographic order; supp ort-based ordering helps to narro w down the searc h space and prune unnecessary branc hes.

In the vertical mining approac hes there is usually no distinct candidate generation and supp ort coun ting phase like in Apriori. Rather, coun ting is sim ultaneous with generation. For a given node or pre x class, one performs intersections of the tidsets of all pairs of class elemen ts, and chec ks if min sup is met. Eac h resulting frequen t itemset is a class unto itself with its own el-emen ts that will be expanded in the next step. That is to say, for a given class of itemsets with pre x P , [ P ] = f X 1 ; X 2 ; :::; X n g , one performs the intersection of P X i with all P X j with j &gt; i to obtain a new class [ P X i ] with elemen ts X j where the itemset P X i X j is frequen t. For example, from [ A ] = f C; D; T; W g , we obtain the classes [ AC ] = f D; T; W g , [ AD ] = f T; W g , and [ AT ] = f W g (an empt y class like [ AW ] need not be explored further).

Vertical metho ds like Eclat [22] and Vip er [20] utilize this indep endence of classes for frequen t set enumer-ation. Figure 4 sho ws how a typical vertical mining pro cess would pro ceed from one class to the next using intersections of tidsets of frequen t items. For example, the tidsets of A ( t ( A ) = 1345) and of D ( t ( D ) = 2456) can be intersected to get the tidset for AD ( t ( AD ) = 45) whic h is not frequen t. As one can see in dense do-mains the tidset size can become very large. Com-bined with the fact that there are a huge num ber of patterns that exist in dense datasets, we nd that the assumption that a sub-problem can be solv ed entirely in main-memory can easily be violated in suc h dense do-mains (particularly at low values of supp ort). One way to solv e this problem is the approac h used by Vip er, where they use compression of vertical bit-v ectors to selectiv ely read/write tidsets from/to disk as the com-putation progresses. Here we o er a fundamen tally new way of pro cessing tidsets using the concept of \di er-ences".
Since eac h class is totally indep enden t, in the sense that it has a list of all possible itemsets, and their tid-sets, that can be com bined with eac h other to pro duce all frequen t patterns sharing a class pre x, our goal is to leverage this prop erty in an ecien t manner.
Our novel and extremely powerful solution (as we shall sho w exp erimen tally) is to avoid storing the en-tire tidset of eac h mem ber of a class. Instead we will keep trac k of only the di erences in the tids between eac h class mem ber and the class pre x itemset. These di erences in tids are stored in what we call the di set , whic h is a di erence of two tidsets (namely , the pre x tidset and a class mem ber's tidset). Furthermore, these di erences are propagated all the way from a node to its children starting from the root. The root node's mem bers can themselv es use full tidsets or di erences from the empt y pre x (whic h by de nition app ears in all tids). More formally , consider a given class with pre x P . Let t ( X ) denote the tidset of elemen t X , and let d ( X ) the di set of X , with resp ect to a pre x tidset, whic h is the curren t univ erse of tids. In normal vertical metho ds one has available for a given class the tidset for the pre x t ( P ) as well as the tidsets of all class mem bers t ( P X Assume that P X and P Y are any two class mem bers of P . By the de nition of supp ort it is true that t ( P X ) t ( P ) and t ( P Y ) t ( P ). Furthermore, one obtains the supp ort of P XY by chec king the cardinalit y of t ( P X ) \ t ( P Y ) = t ( P XY ).

Now supp ose instead that we have available to us not t ( P X ) but rather d ( P X ), whic h is given as t ( P ) t ( X ), i.e., the di erences in the tids of X from P . Similarly , we have available d ( P Y ). The rst thing to note is that the supp ort of an itemset is no longer the cardinalit y of the di set, but rather it must be stored separately and is given as follo ws: ( P X ) = ( P ) j d ( P X ) j . So, given d ( P X ) and d ( P Y ) how can we compute if P XY is frequen t?
We use the di sets recursiv ely as we men tioned above, i.e., ( P XY ) = ( P X ) j d ( P XY ) j . So we have to com-pute d ( P XY ). By our de nition d ( P XY ) = t ( P X ) t ( P Y ). But we only have di sets, and not tidsets as the expression requires. This is easy to x, since d ( P XY ) = t ( P X ) t ( P Y ) = t ( P X ) t ( P Y ) + t ( P ) t ( P ) = ( t ( P ) t ( P Y )) ( t ( P ) t ( P X )) = d ( P Y ) d ( P X ). In other words, instead of computing d ( P XY ) as a dif-ference of tidsets t ( P X ) t ( P Y ), we compute it as the di erence of the di sets d ( P Y ) d ( P X ). Figure 5 sho ws the di eren t regions for the tidsets and di sets of a given pre x class and any two of its mem bers. The tidset of P , the triangle mark ed t ( P ), is the univ erse of relev ant tids. The gra y region denotes d ( P X ), while the region with the solid blac k line denotes d ( P Y ). Note also that both t ( P XY ) and d ( P XY ) are subsets of the tidset of the new pre x P X .
 Example Consider Figure 6 sho wing how di sets can be used to enhance vertical mining metho ds. We can choose to start with the original set of tidsets for the frequen t items, or we could con vert from the tidset rep-resen tation to a di set represen tation at the very begin-ning. One can clearly observ e that for dense datasets like the one sho wn, a great reduction in the database size is achiev ed using this transformation (whic h we con rm on real datasets in the exp erimen ts below).

If we start with tidsets, then to compute the sup-port of a 2-itemset like AD , we would nd d ( AD ) = t ( A ) t ( D ) = 13 (we omit set notation when there is no confusion). To nd out if AD is frequen t we chec k ( A ) j d ( AD ) j = 4 2 = 2, thus AD is not frequen t. If we had started with the di sets, then we would have d ( AD ) = d ( D ) d ( A ) = 13 26 = 13, the same result as before. Even this simple example illustrates the power of di sets. The tidset database has 23 entries in total, while the di set database has only 7 (3 times better). If we look at the size of all results, we nd that the tidset-based approac h tak es up 76 tids in all, while the di set approac h (with initial di set data) stores only 22 tids. If we compare by length, we nd the average tidset size for frequen t 2-itemsets is 3.8, while the average di set size is 1. For 3-itemsets the tidset size is 3.2, but the avg. di set size is 0.6. Finally for 4-itemsets the tid-set size is 3 and the di set size is 0! The fact that the database is smaller to start with and that the di sets shrink as longer itemsets are found, allo ws the di set based metho ds to become extremely scalable, and de-liver orders of magnitude impro vemen ts over traditional approac hes.
To illustrate the power of di set-based mining, we have integrated di sets with Eclat [24, 22], a state-of-the-art vertical mining algorithms. Our enhancemen t is called dEclat . We brie y discuss this algorithm below.
Figure 7 sho ws the pseudo-co de for dEclat. Details on some optimizations, esp ecially for computing frequen t items and 2-itemsets have been omitted, whic h can be found in [22]. dEclat performs a depth-rst searc h of the subset tree. Our exp erimen ts sho w that di sets allo w it to mine on much lower supp orts than other metho ds like Apriori and the base Eclat metho d. The input to the pro cedure is a set of class mem bers for a subtree rooted at P . Frequen t itemsets are generated by computing di sets for all distinct pairs of itemsets and chec king the supp ort of the resulting itemset. A recursiv e pro cedure call is made with those itemsets found to be frequen t at the curren t level. This pro cess is rep eated until all frequen t itemsets have been enumerated. In terms of memory managemen t it is easy to see that we need memory to store intermediate di sets for at most two consecutiv e levels within a class. Once all the frequen t itemsets for the next level have been generated, the itemsets at the curren t level within a class can be deleted.
All exp erimen ts were performed on a 400MHz Pen-tium PC with 256MB of memory , running RedHat Lin ux 6.0. Algorithms were coded in C++. Furthermore, the times for all the vertical metho ds include all costs, in-cluding the con version of the original database from a horizon tal to a vertical format required for the vertical algorithms. We chose sev eral real and syn thetic datasets for testing the performance of algorithms. All datasets except the PUMS (pumsb and pumsb*) sets, are tak en from the UC Irvine Mac hine Learning Database Rep osi-tory . The PUMS datasets con tain census data. pumsb* is the same as pumsb without items with 80% or more supp ort. The mushro om database con tains characteris-tics of various species of mushro oms. Finally the con-nect and chess datasets are deriv ed from their resp ec-tive game steps. Typically , these real datasets are very dense, i.e., they pro duce man y long frequen t itemsets even for very high values of supp ort.

We also chose a few syn thetic datasets, whic h have been used as benc hmarks for testing previous asso ci-ation mining algorithms. These datasets mimic the transactions in a retailing environmen t. Usually the syn thetic datasets are sparser when compared to the real sets.

Figure 9 sho ws the characteristics of the real and syn-thetic datasets used in our evaluation. It sho ws the num ber of items, the average transaction length and the num ber of transactions in eac h database. As one can see
Database # Items Avg. Length # Records chess 76 37 3,196 connect 130 43 67,557 mushro om 120 23 8,124 pumsb* 7117 50 49,046 pumsb 7117 74 49,046 T10I4D100K 1000 10 100,000 T40I10D100K 1000 40 100,000 the average transaction size for these databases is much longer than con ventionally used in previous literature.
We also include two sparse datasets (last two rows) to study its performance on both dense and sparse data.
Our rst exp erimen t is to compare the bene ts of di -sets versus tidsets in terms of the database sizes using the two formats. We conduct exp erimen t on sev eral real (usually dense) and syn thetic (sparse) datasets (see Sec-tion 4 for the dataset descriptions). In Figure 8 we plot the size of the original vertical database, the size of the database using tidsets of only the frequen t items at a given level of supp ort, and nally the database size if items were stored as di sets. We see, for example on the dense pumsb dataset, that the tidset database at 60% supp ort is 10 4 times smaller than the full vertical database; the di set database is 10 6 times smaller! It is 100 times smaller than the tidset database. For the other dense datasets, connect and chess, the di set for-mat can be up to 100 times smaller dep ending on the min sup value.

For the sparser pumsb* dataset we notice a more in-teresting trend. The di set database starts out smaller than the tidset database, but quic kly gro ws more than even the full vertical database. For mushro om and other other syn thetic datasets (results sho wn only for T40I10-D100K), we nd that di sets occup y (sev eral magni-tudes) more space than tidsets. We conclude that keep-ing the original database in di set format is clearly su-perior if the database is dense, while the opp osite is true for sparse datasets. In general we can use as starting point the smaller of the two formats dep ending on the database characteristics.

Due to the recursiv e dep endence of a tidset or di set on its paren t equiv alence class it is dicult to obtain analytically an estimate of their relativ e sizes. For this reason we conduct an exp erimen t comparing the size of di sets versus tidsets at various stages during mining. Figure 10 sho ws the average cardinalit y of the tidsets and di sets for frequen t itemsets of various lengths on di eren t datasets, for a given minim um supp ort. We denote by db a run with tidset format and by Ddb a run with the di set format, for a given dataset db . We assume that the original dataset is stored in tidset for-mat, thus the average tidset length is the same for single items in both runs. However, we nd that while the tid-set size remains more or less constan t over the di eren t lengths, the di set size reduces drastically .
For example, the average di set size falls below 1 for the last few lengths (over the length interv al [11-16] for chess, [9-12] for connect, [7-17] for mushro om, [8-15] for pumsb*, 8 for pumsb, [9-11] for T10, [12-14] for T20). The only exception is T40 where the di set length is 5 for the longest patterns. However, over the same inter-val range the avg. tidset size is 1682 for chess, 61325 for connect, 495 for mushro om, 18200 for pumsb*, 44415 for pumsb, 64 for T10, 182 for T20, and 728 for T40. Thus for long patterns the avg. di set size is sev eral orders of magnitude smaller than the corresp onding avg. tidset size (4 to 5 orders of magnitude smaller on dense sets, and 2 to 3 orders of magnitude smaller on sparse sets). We also sho w in Table 11 the average di set and tidset sizes across all lengths. We nd that di sets are smaller by one to two orders of magnitude for both dense as well as sparse datasets.

There is usually a cross-o ver point when a switc h from tidsets to di sets will be of bene t. For dense datasets it is better to start with the di set format, while for sparse data it is better to start with tidset format and switc h to di sets in later stages, since di sets on average are orders of magnitude smaller than tidsets. In general, we would like to kno w when it is bene cial to switc h to the di set format from the tidset format. Since eac h class is indep enden t the decision can be made adaptiv ely at the class level.
 Consider a given class with pre x P , and assume that P X and P Y are any two class mem bers of P , with their corresp onding tidsets t ( P X ) and t ( P Y ). Consider the itemset P XY in a new class P X , whic h can either be stored as a tidset t ( P XY ) or as a di set d ( P XY ). We de ne reduction ratio as r = t ( P XY ) =d ( P XY ). For di sets to be bene cial the reduction ratio should be at least 1. That is r 1 or t ( P XY ) =d ( P XY ) 1. Substi-tuting for d ( P XY ), we get t ( P XY ) = ( t ( P X ) t ( P Y )) 1. Since t ( P X ) t ( P Y ) = t ( P X ) t ( P XY ), we have t ( P XY ) = ( t ( P X ) t ( P XY )) 1. Dividing by t ( P XY ) we get, 1 = ( t ( P X ) =t ( P XY ) 1) 1. After simpli ca-tion we get t ( P X ) =t ( P XY ) 2. In other words it is better to switc h to the di set format if the supp ort of P XY is at least half of P X . If we start with a tidset database, empirically we found that for all real datasets it was better to use di sets from length 2 onward. On the other hand, for the syn thetic datasets we found that the 2-itemsets have an average supp ort value 10 times smaller than the supp ort of single items. Since this re-sults in a reduction ratio less than 1, we found it better to switc h to di sets starting at 3-itemsets.
Vip er [20] prop osed using compressed vertical bitv ec-tors instead of tidsets. Here we compare the bitv ec-tors against di sets. The classical way of compressing bitv ectors is by using run-length enco ding (RLE). It was noted in [20] that RLE is not appropriate for asso ciation mining, since it is not realistic to assume man y consecu-tive 1's for a sparse dataset. If all 1's occur in an isolated manner, RLE outputs one word for the preceding 0 run and one word for the 1 itself. This results in a database that is double the size of a tidset database.
 Vip er uses a novel enco ding scheme called Skinning . The idea is to divide runs of 1's and 0's in groups of size W 1 and W 0 . Eac h full group occupies one bit set to 1. The last partial group ( R mod W i , where R is the run length) occupies lg W i bits storing the partial coun t of 1's or 0's. Finally , a eld separator bit (0) is placed between the full groups bits and the partial coun t eld. Since the length of the coun t eld is xed, we kno w that we have to switc h to 1's or 0's after having seen the coun t eld for a run of 0's or 1's, resp ectiv ely. If the minim um supp ort is less than 50% the bitv ectors for (longer) itemsets will have more 0's than 1's, thus it mak es sense to use a large value for W 0 and a small value for W 1 . Vip er uses W 0 = 256 and W 1 = 1.
Let N denote the num ber of transactions, n 1 the num ber of 1's and n 0 the num ber of 0's in an item-set's bitv ector. Assuming word length of 32 bits, a tid-set tak es 32 n 1 bits of storage. Assume (in the worst case) that all 1's are intersp ersed by exactly one 0, with a trailing run of 0's. In the skinning pro cess eac h 1 leads to two bits of storage, one to indicate a full group and another for the separator. For n 1 1's, we get 2 n 1 bits of storage. For a compressed bitv ector the num ber of bits used for isolated 1's is given as 2 n 1 For the n 1 isolated 0's we need n 1 (1 + lg W 0 ) = 9 n 1 bits. For the remaining n 0 n 1 = ( N n 1 ) n 1 = N 2 n 1 0's we need ( N 2 n 1 )(1 =W 0 ) = N= 256 n 1 = 128 bits. Since n 1 N min sup the total num ber of bits for the compressed vector is given as the sum of the num ber of bits required for 1's and 0's, given as 2 n 1 + 9 n 1 n 1 = 128 + N= 256 = 1407 n 1 = 128 + N= 256. The bene t of skinning compared to tidset storage is then given as C w 32 n 1 = (1407 n 1 = 128 + N= 256), where C w denotes worst case compression ratio, i.e., the ra-tio of the storage required for the tidset divided by the storage required for the compressed bitv ector. After substituting n 1 N min sup and simplifying, we get
C
For supp ort values less than 0.02% C w is less than 1, whic h means that skinning causes expansion rather than compression. The maxim um value of C w reac hes 2.91 asymptotically . For min sup =0.1%, C w = 2 : 14
Thus for reasonable supp ort values the compression ra-tio is exp ected to be between 2 and 3 compared to tid-sets. Supp osing we assume a best case scenario for skinning, where all the n 1 1's come before the n 0 0's (this is highly unlik ely to happ en). We would then need n 1 bits to represen t the single run of 1's, and n 0 =W 0 = n 0 = 256 bits for the run of 0's. The total space is thus n 1 + n 0 = 256 = n 1 + ( N n 1 ) = 256 = 255 n 1 = 256 + N= 256. The best case compression ration is given as C b 32 n 1 = (255 n 1 = 256 + N= 256). After sim-pli cation this yields C b 1 asymptotically reac hes a value of 32. In the best case, the compression ratio is 32, while in the worst case the compression ratio is only 2 to 3. The skinning pro cess can thus pro vide at most 1 order of magnitude compres-sion ratio over tidsets. However, as we have seen di sets pro vide anywhere from 2 to 5 orders of magnitude com-pression over tidsets. The exp erimen tal and theoretical results sho wn above clearly substan tiate the power of di set based mining! base metho d that use only tidsets. We give a thorough sets of exp erimen ts spanning all the real and syn thetic datasets men tioned above, for various values of mini-Apriori Viper
Eclat dEclat 100 1000 Apriori Viper
Eclat dEclat T10 on the syn thetic datasets are only marginal, up to a factor of 2 impro vemen t.

In Figure 13 we compare horizon tal and vertical algo-rithms for mining the set of all frequen t patterns. We compare the new dEclat metho d against Eclat [22], the classic Apriori [1] and recen t Vip er [20] algorithm. We see the dEclat outp erforms by orders of magnitude the other algorithms. One observ ation that can be made is that dEclat mak es Eclat more scalable, allo wing it to enumerate frequen t patterns even in dense datasets for relativ ely low values of supp ort. On dense datasets Vip er is better than Apriori at lower supp ort values, but Vip er is uncomp etitiv e with Eclat and dEclat. Figure 14 compares dEclat with FPGro wth [11] 1 . The results are sho wn separately since the authors pro-vided only a Windo ws executable. We tested them on a 800 Mhz, 256MB memory , Pentium III pro cessor run-ning Win98 and cygwin. We observ e that dEclat out-performs FPGro wth by a factor of 2 for all datasets except T10I4D100K whic h is very sparse and has only a few long patterns. The time di erence increases with decreasing supp ort.
In this pap er we presen ted a detailed evaluation of a novel vertical data represen tation called Di set , that only keeps trac k of di erences in the tids of a candidate pattern from its generating frequen t patterns. We sho w that di sets drastically cut down the size of memory re-quired to store intermediate results. We sho w how di -sets, when incorp orated into a previous vertical mining metho ds, increase the performance signi can tly. [1] R. Agra wal, et al. Fast disco very of asso ciation [2] Ramesh Agra wal, Charu Aggarw al, and V.V.V. [3] Jay Ayres, J. E. Gehrk e, Tomi Yiu, and Jason [4] R. J. Bayardo. Ecien tly mining long patterns [5] S. Brin, R. Mot wani, J. Ullman, and S. Tsur. [6] D. Burdic k, M. Calimlim, and J. Gehrk e. MAFIA: [7] B. Dunk el and N. Sopark ar. Data organization 1 We extend our thanks to Jiawei Han amd Jian Pei. [8] K. Gouda and M. J. Zaki. Ecien tly mining [9] D. Gunopulos, H. Mannila, and S. Saluja.
 [10] J. Han and M. Kam ber. Data Mining: Conc epts [11] J. Han, J. Pei, and Y. Yin. Mining frequen t [12] D-I. Lin and Z. M. Kedem. Pincer-searc h: A new [13] J-L. Lin and M. H. Dunham. Mining asso ciation [14] J. S. Park, M. Chen, and P. S. Yu. An e ectiv e [15] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [16] J. Pei, J. Han, and R. Mao. Closet: An ecien t [17] S. Sara wagi, S. Thomas, and R. Agra wal. [18] A. Savasere, E. Omiecinski, and S. Navathe. An [19] J. Shafer, R. Agra wal, and M. Meh ta. Sprin t: A [20] P. Sheno y, et al. Turb o-charging vertical mining of [21] M. J. Zaki. Generating non-redundan t asso ciation [22] M. J. Zaki. Scalable algorithms for asso ciation [23] M. J. Zaki and C.-J. Hsiao. ChARM : An ecien t [24] M. J. Zaki, S. Parthasarath y, M. Ogihara, and
