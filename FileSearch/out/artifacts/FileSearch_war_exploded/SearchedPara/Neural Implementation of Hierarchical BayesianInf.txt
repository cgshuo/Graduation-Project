 Living creatures occupy an environment full of uncertainty due to noisy sensory inputs, incomplete observations, and hidden variables. One of the goals of the nervous system is to infer the states of the world given these limited data and make decisions accordingly. This task involves combining prior knowledge with current data [1], and integrating cues from multiple sensory modalities [2]. Studies of human psychophysics and animal behavior suggest that the brain is capable of solving these problems in a way that is consistent with optimal Bayesian statistical inference [1, 2, 3, 4]. Moreover, complex brain functions such as visual information processing involves multiple brain areas [5]. Hierarchical Bayesian inference has been proposed as a computational framework for modeling such processes [6]. Identifying neural mechanisms that could support hierarchical Bayesian inference is important, since probabilistic computations can be extremely challenging. Just representing and updating distributions over large numbers of hypotheses is computationally expensive.
 Much effort has recently been devoted towards proposing possible mechanisms based on known neuronal properties. One prominent approach to explaining how the brain uses population activities for probabilistic computations has been done in the  X  X ayesian decoding X  framework [7]. In this framework, it is assumed that the firing rate of a population of neurons, r , can be converted to a probability distribution over stimuli, p ( s | r ) , by applying Bayesian inference, where the likelihood a distribution over stimuli, which can be recovered through Bayesian decoding. The problem of performing probabilistic computations then reduces to identifying a set of operations on firing rates [8] showed that when the likelihood p ( r | s ) is an exponential family distribution with linear sufficient statistics, adding two sets of firing rates is equivalent to multiplying probability distributions. In this paper, we take a different approach, allowing a population of neurons to encode a probability distribution directly. Rather than relying on a separate decoding operation, we assume that the activ-ity of each neuron translates directly to the weight given to the optimal stimulus for that neuron in the corresponding probability distribution. We show how this scheme can be used to perform Bayesian inference, and how simple extensions of this basic idea make it possible to combine sources of in-formation and to propagate uncertainty through multiple layers of random variables. In particular, we focus on one Monte Carlo method, namely importance sampling with the prior as a surrogate, and show how recursive importance sampling approximates hierarchical Bayesian inference. Given a noisy observation x , we can recover the true stimulus x  X  by using Bayes X  rule to compute the posterior distribution the probability of the observation x if the true stimulus value is x  X  . A good guess for the value of x  X  is the expectation of x  X  given x . In general, we are often interested in the expectation of some on the task. For example, in noise reduction where x  X  itself is of interest, we can take f ( x  X  ) = x  X  . However, evaluating expectations over the posterior distribution can be challenging: it requires com-can be approximated using a Monte Carlo method known as importance sampling. In its general form, importance sampling approximates the expectation by using a set of samples from some surro-
E [ f ( x  X  ) | x ] ' Thus, importance sampling provides a simple and efficient way to perform Bayesian inference, approximating the posterior distribution with samples from the prior weighted by the likelihood. Recent work also has suggested that importance sampling might provide a psychological mechanism for performing probabilistic inference, drawing on its connection to exemplar models [9]. The key components of an importance sampler can be realized in the brain if: 1) there are feature detection neurons with preferred stimulus tuning curves proportional to the likelihood p ( x | x  X  i ) ; 2) the frequency of these feature detection neurons is determined by the prior p ( x  X  ) ; and 3) divisive normalization can be realized by some biological mechanism. In this section, we first describe a radial basis function network implementing importance sampling, then discuss the feasibility of three assumptions mentioned above. The model is then extended to networks of spiking neurons. 3.1 Radial basis function (RBF) networks Radial basis function (RBF) networks are a multi-layer neural network architecture in which the hidden units are parameterized by locations in a latent space x  X  i . On presentation of a stimulus x , Figure 1: Importance sampler realized by radial basis function network. For details see Section 3.1. these hidden units are activated according to a function that depends only on the distance || x  X  x  X  i || , e.g., exp(  X  X  x  X  x  X  i | 2 / 2  X  2 ) , similar to the tuning curve of a neuron. RBF networks are popular because they have a simple structure with a clear interpretation and are easy to train. Using RBF networks to model the brain is not a new idea  X  similar models have been proposed for pattern recognition [10] and as psychological accounts of human category learning [11].
 Implementing importance sampling with RBF networks is straightforward. A RBF neuron is re-cruited for a stimulus value x  X  i drawn from the prior (Fig. 1). The neuron X  X  synapses are organized would be reached at preferred stimulus x = x  X  i and diminishes as || x  X  x  X  i || increases. The i th RBF neuron makes a synaptic connection to output neuron j with strength f j ( x  X  i ) , where f j is a function of interest. The output units also receive input from an inhibitory neuron that sums over all RBF neurons X  activities. Such an RBF network produces output exactly in the form of Eq. 3, with the activation of the output units corresponding to E [ f j ( x  X  ) | x ] .
 Training RBF networks is practical for neural implementation. Unlike the multi-layer perceptron that usually requires global training of the weights, RBF networks are typically trained in two stages. First, the radial basis functions are determined using unsupervised learning, and then, weights to the outputs are learned using supervised methods. The first stage is even easier in our formulation, because RBF neurons simply represent samples from the prior, independent of the second stage later in development. Moreover, the performance of RBF networks is relatively insensitive to the precise form of the radial basis functions [12], providing some robustness to differences between the Bayesian likelihood p ( x | x  X  i ) and the activation function in the network. RBF networks also produce sparse coding, because localized radial basis likelihood functions mean only a few units will be significantly activated for a given input x . 3.2 Tuning curves, priors and divisive normalization We now examine the neural correlates of the three components in RBF model. First, responses of cortical neurons to stimuli are often characterized by receptive fields and tuning curves, where receptive fields specify the domain within a stimulus feature space that modify neuron X  X  response and tuning curves detail how neuron X  X  responses change with different feature values. A typical tuning curve (like orientation tuning in V1 simple cells) has a bell-shape that peaks at the neuron X  X  preferred stimulus parameter and diminishes as parameter diverges. These neurons are effectively measure the likelihood p ( x | x  X  i ) , where x  X  i is the preferred stimulus.
 Second, importance sampling requires neurons with preferred stimuli x  X  i to appear with frequency proportional to the prior distribution p ( x  X  ) . This can be realized if the number of neurons represent-ing x  X  is roughly proportional to p ( x  X  ) . While systematic study of distribution of neurons over their preferred stimuli is technically challenging, there are cases where this assumption seems to hold. For example, research on the  X  X blique effect X  supports the idea that the distribution of orientation tuning curves in V1 is proportional to the prior. Electrophysiology [13], optical imaging [14] and fMRI studies [15] have found that there are more V1 neurons tuned to cardinal orientations than to oblique orientations. These findings are in agreement with the prior distribution of orientations of lines in the visual environment. Other evidence comes from motor areas. Repetitive stimulation of a finger expands its corresponding cortical representation in somatosensory area [16], suggesting more neurons are recruited to represent this stimulus. Alternatively, recruiting neurons x  X  i according to the prior distribution can be implemented by modulating feature detection neurons X  firing rates. This strategy also seems to be used by the brain: studies in parietal cortex [17] and superior col-liculus [18] show that increased prior probability at a particular location results in stronger firing for neurons with receptive fields at that location.
 Third, divisive normalization is a critical component in many neural models, notably in the study of attention modulation [19, 20]. It has been suggested that biophysical mechanisms such as shunting inhibition and synaptic depression might account for normalization and gain control [10, 21, 22]. Moreover, local interneurons [23] act as modulator for pooled inhibitory inputs and are good can-didates for performing normalization. Our study makes no specific claims about the underlying biophysical processes, but gains support from the literature suggesting that there are plausible neu-ral mechanisms for performing divisive normalization. 3.3 Importance sampling by Poisson spiking neurons Neurons communicate mostly by spikes rather than continuous membrane potential signals. Poisson spiking neurons play an important role in other analyses of systems for representing probabilities [8]. Poisson spiking neurons can also be used to perform importance sampling if we have an ensemble To show this we need a property of Poisson distributions: if y i  X  Poisson (  X  i ) and Y = P i y i , implies that E ( y i /Y | Y = n ) =  X  i / P i  X  i . Assume a neuron tuned to stimulus x  X  i emits spikes r expectation is
E which is thus an unbiased estimator of the importance sampling approximation to the posterior ex-pectation. The variance of this estimator decreases as population activity n = P i r i increases because var [ r i /n ]  X  1 /n . Thus, Poisson spiking neurons, if plugged into an RBF network, can per-form importance sampling and give similar results to  X  X eurons X  with analog output, as we confirm later in the paper through simulations. Inference tasks solved by the brain often involve more than one random variable, with complex dependency structures between those variables. For example, visual information process in pri-mates involves dozens of subcortical areas that interconnect in a hierarchical structure containing two major pathways [5]. Hierarchical Bayesian inference has been proposed as a solution to this problem, with particle filtering and belief propagation as possible algorithms implemented by the brain [6]. However, few studies have proposed neural models that are capable of performing hier-archical Bayesian inference (although see [24]). We show how a multi-layer neural network can perform such computations using importance samplers (Fig. 1) as building blocks. 4.1 Generative models and Hierarchical Bayesian inference Generative models describe the causal process by which data are generated, assigning a probability distribution to each step in that process. To understand brain function, it is often helpful to identify the generative model that determines how stimuli to the brain S x are generated. The brain then has to reverse the generative model to recover the latent variables expressed in the data (see Fig. 2). The direction of inference is thus the opposite of the direction in which the data are generated. Figure 2: A hierarchical Bayesian model. The generative model specifies how each variable is generated (in circles), while inference reverses this process (in boxes). S x is the stimulus presented to the nervous system, while X , Y , and Z are latent variables at increasing levels of abstraction. posterior expectation of some function f ( z ) of a high-level latent variable Z given stimulus S x , this hierarchical Bayesian inference problem can decomposed into three importance samplers with values x  X  i , y  X  j and z  X  k drawn from the prior.
 This result relies on recursively applying importance sampling to the integral, with each recursion resulting in an approximation to the posterior distribution of another random variable. This recursive importance sampling scheme can be used in a variety of graphical models. For example, tracking a stimulus over time is a natural extension where an additional observation is added at each level of the generative model. We evaluate this scheme in several generative models in Section 5. 4.2 Neural implementation of the multi-layer importance sampler The decomposition of hierarchical inference into recursive importance sampling (Eq. 5) gives rise to a multi-layer neural network implementation (see Fig. 3a). The input layer X is similar to that in Fig. 1, composed of feature detection neurons with output proportional to the likelihood p ( S x | x  X  i ) . Their output, after presynaptic normalization, is fed into a layer corresponding to the Y variables, with synaptic weights p ( x distributions. Posterior expectations involving any random variable can be computed because the neuron activities at each level approximate the posterior density. A single pool of neurons can also feed activation to multiple higher levels. Using the visual system as an example (Fig. 3b), such a multi-layer importance sampling scheme could be used to account for hierarchical inference in divergent pathways by projecting a set of V2 cells to both MT and V4 areas with corresponding synaptic weights. Figure 3: a) Multi-layer importance sampler for hierarchical Bayesian inference. b) Possible imple-mentation in dorsal-ventral visual inference pathways, with multiple higher levels receiving input from one lower level. Note that the arrow directions in the figure are direction of inference, which is opposite to that of its generative model. In this section we examine how well the mechanisms introduced in the previous sections account for human behavioral data for two perceptual phenomena: cue combination and the oblique effect. 5.1 Haptic-visual cue combination When sensory cues come from multiple modalities, the nervous system is able to combine those cues optimally in the way dictated by Bayesian statistics [2]. Fig. 4a shows the setup of an experiment where a subject measures the height of a bar through haptic and visual inputs. The object X  X  visual input is manipulated so that the visual cues can be inconsistent with haptic cues and visual noise can be adjusted to different levels, i.e. visual cue follows x V  X  X  ( S V , X  2 V ) and haptic cue follows x
H  X  X  ( S H , X  2 H ) , where S V ,S H , X  2 V are controlled parameters. The upper panel of Fig. 4d shows the percentage of trials that participants report the comparison stimulus (consistent visual/haptic cues from 45-65mm) is larger than the standard stimulus (inconsistent visual/haptic cues, S V = 60 mm and S H = 50 mm ). With the increase of visual noise, haptic input accounts for larger weights in decision making and the percentage curve is shifted towards S H , consistent with Bayesian statistics. Several studies have suggested that this form of cue combination could be implemented by popula-tion coding [2, 8]. In particular, [8] made an interesting observation that, for Poisson-like spiking neurons, summing firing activities of two populations is the optimal strategy. This model is under the Bayesian decoding framework and requires construction of the network so that these two pop-ulations of neurons have exactly the same number of neurons and precise one-to-one connection between two populations, with the connected pair of neurons having exactly the same tuning curves. We present an alternative solution based on importance sampling that encodes the probability distri-bution by a population of neurons directly.
 The importance sampling solution approximates the posterior expectation of the bar X  X  height x  X  C given S V and S H . Sensory inputs are channeled in through x V and x H (Fig.4b). Because sensory input varies in a small range (45-65mm in [2]), we assume priors p ( x C ) , p ( x V ) and p ( x H ) are uniform. It is straightforward to approximate posterior p ( x V | S V ) using importance sampling: Figure 4: (a) Experimental setup [2]. (b) Generative model. S V and S H are the sensory stimuli, X
V and X H the values along the visual and haptic dimensions, and X C the combined estimate of transparent ellipses indicate the tuning curves of high level neurons centered on values x  X  C,k over x
V and x H . The big ellipse represents the manipulated input with inconsistent sensory input and different variance structure. Bars at the center of opaque ellipses indicate the relative firing rates of x
C neurons, proportional to p ( x  X  C,k | S V ,S H ) . (d) Human data and simulation results. of spike trains is needed.
Fortunately, the experiment gives an important constraint, namely subjects were not aware of the manipulation of visual input. Thus, the values x  X  C,k employed in the computation are sampled from normal perceptual conditions, namely consistent visual and haptic inputs ( x V = x H ) and normal variance structure (transparent ellipses in Fig.4c, on the diagonal). Therefore, the random variables { x
V ,x H } effectively become one variable x V,H and values of x  X  V,H,i are composed of samples drawn from x V and x H independently. Applying importance sampling, and x  X  H,j are treaded as from one population in Eq 8. r V,i and r H,j are weighted differently only because of different observation noise. Eq. 9 is applicable for manipulated sensory input (in Fig. 4c, the ellipse off the diagonal). The simulation results (for an average of 500 trials) are shown in the lower panel of Fig.4d, compared with human data in the upper panel. There are two parameters, noise levels  X  V and  X  H , are optimized to fit within-modality discrimination data (see [2] Fig. 3a). { x
V,i } , { x of each set of neurons is limited to 30. The simulations produce a close match to human behavior. 5.2 The oblique effect The oblique effect describes the phenomenon that people show greater sensitivity to bars with hor-izontal or vertical ( 0 o / 90 o ) orientations than  X  X blique X  orientations. Fig. 5a shows an experimental setup where subjects exhibited higher sensitivity in detecting the direction of rotation of a bar when the reference bar to which it was compared was in one of these cardinal orientations. Fig. 5b shows the generative model for this detection problem. The top-level binary variable D randomly chooses a direction of rotation. Conditioning on D , the amplitude of rotation  X   X  is generated from a truncated Figure 5: (a) Orientation detection experiment. The oblique effect is shown in lower panel, being greater sensitivity to orientation near the cardinal directions. (b) Generative model. (c) The oblique effect emerges from our model, but depends on having the correct prior p (  X  ) . normal distribution ( N T ( D ) , being restricted to  X   X  &gt; 0 if D = 1 and  X   X  &lt; 0 otherwise). When combined with the angle of the reference bar r (shaded in the graphical model, since it is known),  X   X  generates the orientation of a test bar  X  , and  X  further generates the observation S  X  , both with normal distributions with variance  X   X  and  X  S  X  respectively.
 The oblique effect has been shown to be closely related to the number of V1 neurons that tuned to different orientations [25]. Many studies have found more V1 neurons tuned to cardinal orientations than other orientations [13, 14, 15]. Moreover, the uneven distribution of feature detection neurons horizontal and vertical segments exist in the natural visual environment of humans.
 neurons around 0 o / 90 o can cause the oblique effect, which becomes a question of whether the oblique effect depends on the use of a prior p (  X  ) with this distribution. The quantity of interest is: shows that detection sensitivity is uncorrelated with orientations if we take a uniform prior p (  X  ) , but exhibits the oblique effect under a prior that prefers cardinal directions. In both cases, 40 neurons are used to represent each of  X   X   X  i and  X   X  i , and results are averaged over 100 trials. Sensitivity is measured by percentage correct in inference. Due to the qualitative nature of this simulation, model parameters are not tuned to fit experiment data. Understanding how the brain solves the problem of Bayesian inference is a significant challenge for computational neuroscience. In this paper, we have explored the potential of a class of solutions that draw on ideas from computer science, statistics, and psychology. We have shown that a small number of feature detection neurons whose tuning curves represent a small set of typical examples from sensory experience is sufficient to perform some basic forms of Bayesian inference. Moreover, our theoretical analysis shows that this mechanism corresponds to a Monte Carlo sampling method, i.e. importance sampling. The basic idea behind this approach  X  storing examples and activating them based on similarity  X  is at the heart of a variety of psychological models, and straightforward to implement either in traditional neural network architectures like radial basis function networks, circuits of Poisson spiking neurons, or associative memory models. The nervous system is con-stantly reorganizing to capture the ever-changing structure of our environment. Components of the importance sampler, such as the tuning curves and their synaptic strengths, need to be updated to match the distributions in the environment. Understanding how the brain might solve this daunting problem is a key question for future research.

