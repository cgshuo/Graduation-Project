 In this paper, we study the problem of Web forum crawling. Web forum has now become an important data source of many Web applications; while forum crawling is still a challenging ta sk due to complex in-site link structures and login controls of mo st forum sites. Without carefully selecting the traversal path, a g eneric crawler usually downloads many duplicate and invalid pages fr om forums, and thus wastes both the precious bandwidth and the l i-mited storage space. To crawl forum data more effectiv ely and efficiently, in this paper, we propose an automatic approach to exploring an appropriate traversal strategy to direct th e crawling of a given target forum. In detail, the traversal strate gy consists of the identification of the skeleton links and the detection of the page-flipping links . The skeleton links instruct the crawler to only crawl valuable pages and meanwhile avoid duplicate and unin-formative ones; and the page-flipping links tell the crawle r how to completely download a long discussion thread which is usually shown in multiple pages in Web forums. The extensive experi-mental results on several forums show encouraging performance of our approach. Following the discovered traversal strat egy, our forum crawler can archive more informative pages in compa rison with previous related work and a commercial generic cr awler. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  clustering, information filtering . Algorithms, Performance, Experimentation Forum crawler, sitemap, traversal strategy. Web forum (also named bulletin or discussion board) is a W eb application for holding discussions and posting user-created con-tent (UCC). Web forum may refer to either the entire community (e.g. MSDN) or a specific sub-forum within a large Web s ite deal-ing with a distinct topic (e.g. Amazon) [1]. With millio ns of users' contribution, forum data usually has plenty of highly valuable knowledge and information, and becomes an important resource on the Web. The strong driving force behind the highly v aluable forum data is the power of users and communities. For examp le, consumers more like to share their comments and experiences to some electronic products on Web forums. In comparison with the reviews from official websites, comments from forums are rela-tively unbiased and with more perspectives. Most commerc ial search engines such as Google, Yahoo!, and Live have begun to leverage information from Web forums to improve their se arch result qualities. To do this, search engines have to first download pages from various forums and build a locally indexed reposi tory [5]. High quality search results highly depend on a high quality repository. This requires the crawler to fetch as much as possible valuable data from the infinite number of Web forum pages [3] , using the limited bandwidth and storage space . However, traditional generic crawlers [5] which basicall y adopt the breadth-first traversal strategy [4] are usually inef ficient in crawling forum sites. This is because forum sites have so me dif-ferent characteristics from other general websites. F irst, to help users conveniently browsing, a Web forum usually has many shortcut links which point to the same content but often have dif-ferent URLs. Indiscriminately following all these shortc uts will lead to many duplicate pages in forum crawling. Second, most Web forums have access control to protect privacy, and some links are forbidden for unregistered users (such as a crawle r). Rudely following these links in crawling will result in ma ny unin-formative pages such as a login portal. According to the s tatistics in [7], using a breadth-first and depth-unlimited crawler , average-ly there are more than 40% useless (uninformative or duplic ate) forum pages among all the crawled pages. These pages serio usly degrade the repository quality. Moreover, generic crawlers totally ignore the problem of page-flipping in Web forums. That is, a long discussion thread is usually divided and shown in multiple pages. Such a relationship among these pages should be preser ved in crawling to facilitate further indexing and storage. At last, as content of Web forums usually changes more frequently, a c raw-ler needs a high frequent re-crawling schedule to keep refre shing the repository, which makes the above three problems even w orse. This motivates us to explore a better traversal strateg y for Web forum crawling. In general, for forum crawling, an ideal traversal strat egy needs to answer two questions: 1) what kinds of links should be followed ? To save bandwidth and reduce redundancy, only the links that are linked to valuable pages should be kept whereas the links to un in-formative and duplicate pages should be dropped; and 2) how to follow these links ? To better schedule the crawling queue and to build more efficient index structures, the flipping relatio nships among pages should be discovered and preserved. To the best of our knowledge, little existing work in liter atures has systemically investigated the traversal strategy for forum crawl-*This work was performed when the first author was an intern in Microsoft Research Asia. ing. Most existing research works on Web crawling only utili ze out-link related information such as URL pattern and anchor t ext [8][12] to design traversal strategies, which ignore the re lation-ships among various pages but just judge each out-link on dif fer-ent pages independently. However, considering the complex in-site link structures and access controls of Web forums, s uch works cannot completely meet the requirements of forum crawling . We will provide a more detailed review to discuss these re lated re-search efforts in Section 2. Therefore, to well answer the afore-mentioned two questions, a crawler needs to utilize more informa-tion than a link itself, and should think about the travers al strategy globally. In this paper, we propose a novel solution for efficient fo rum crawling. To understand the global organization of a fo rum, we first re-construct its sitemap based on a few thousands pages ran-domly sampled from the target site [7]. The sitemap te lls us how many kinds of pages are there in the target forum, and how they are linked together. In this way, we can leverage more glo bal knowledge to explore the traversal strategy. For example, besides the URL pattern of a link, we also know the characteristi cs of both its source and target pages. Moreover, as such informatio n of the source/target pages is summarized based on the statistics of a group of pages, it is more robust than only considering a si ngle page. Given the sitemap, the traversal problem becomes to inves-tigate whether to follow and how to follow every link in the site-map. More specifically, in this paper we focus on the ident ifica-tion of two kinds of links, skeleton link and page-flipping link , from the sitemap, as described in the following.  X  S KELETON L INK I DENTIFICATION . To answer the first ques- X  P AGE-F LIPPING L INK D ETECTION . To answer the second The rest of this paper is organized as follows. First, we briefly review some related works in Section 2, and define the pro blem settings in Section 3. The system overview of the propos ed ap-proach is introduced in Section 4, and the algorithm detail s are described in Section 5. Experiment evaluations are reporte d in Section 6. And in the last section, we draw conclusions and point out some future research directions. To make a tradeoff between the "performance and cost", mos t generic Web crawlers adopt the breadth-first strategy a nd limit the crawling depth. However, in practice it is hard to select a proper crawling depth for each site. A shallow crawling strateg y cannot ensure to access all valuable content, whereas a deep cr awling strategy may result in too many duplicate and invalid pag es. To improve this, some research works tried to find more eff ective crawling strategy than the breadth-first one [2][4]. For e xample, the on-line page importance computation (OPIC) was proposed i n [2] which utilized the partial PageRank to estimate the im portance of a page, to help a crawler skip valueless pages. How ever, we argue that this approach is not suitable for forum sites , because it is hard to appropriately estimate the PageRank scores for pages in Web forums. As content in Web forums changes frequently every day, new generated pages usually have very low PageRank s cores. Deep Web (or hidden Web) crawling [14] is also a related r e-search topic. This is because forums are also a kind of deep Web which consists of dynamic pages generated from a database. However, the focuses of a forum crawler and a deep Web c rawler are different. A deep Web crawler focuses on how to prep are ap-propriate queries to probe and retrieve hidden pages; while a fo-rum crawler is interested in how to identify valuable links t o fol-low, given that most forum pages have explicit in-links. The most related work is focused crawling [8][12][13][15], which attempts to retrieve Web pages that are relevant to so me pre-defined topics or labeled examples. The target descriptions in focused crawling are quite different in various applications . In [8][12][13], the targets are usually described with a series terms or topics; while in [15] the target was described by the DOM tree of a manually selected sample page. Forum crawler is also a kind of targeted crawler as it selectively downloads informative pages containing user-created content. However, the existing metho ds are not suitable for forum crawling. First, semantic topi cs in fo-rums are too diverse to be simply characterized with a li st of terms. The structure driven-based approach in [15] seems pr omis-ing since it only cares about the layout structure of the t arget pag-es. This is suitable for forum crawling as most pages in forums are generated by some pre-defined templates; and different tem plates have different layout structures to present different co ntent such as list-of-board , list-of-thread , post-of-thread , user profile , etc. However, the strategy in [15] only focuses on finding a possible path to the target pages, but cannot guarantee that it is a n optimal one. Actually, in forums there are usually multiple paths to visit a page. Most of these paths are just shortcuts and cannot prom ise to cover all the targets. Moreover, the problem of detecti ng page-flipping links was not discussed in [15]. We will compare our approach with the structure-driven based approach in the experi-ments part. There is also one recent study investigating the problem of forum crawling [9]. Unfortunately, it mainly adopted some heuristic rules as traversal strategies, and can only deal with f orums with some specific organization structures. While in reality there are hundreds of forum structures implemented by either Website d e-velopers or different Internet forum software. Thus it is impractic-al to define universal heuristics for general forum cra wling. To make a clear presentation and to facilitate the follow ing dis-cussions, we first explain some concepts used in this paper.
 S
ITEMAP. A sitemap is a directed graph consisting of a set of ver-tices and the corresponding links . Each vertex represents a group of forum pages which have similar page layout structure; a nd each link denotes a kind of linking relationship between two vertic es. Fig. 1 (a) provides an illustration of the sitemap for the ASP.NET forum (http://forums.asp.net). For vertices, we can find that each vertex is related to one kind of pages in the forum, as show n in Fig. 1 (a) with the typical pages and labels. We can al so find that formation. For links, we should address that in this paper each link is characterized by both the pattern and the location of where the corresponding URLs are located in a page. Location inform a-tion can help distinguish different links when there is no distinct URL pattern in a forum; for more details please refer to [7]. Therefore, in Fig. 1 (a), we can find there may be mul tiple links from one vertex to another. S
KELETON L INK . Skeleton links, as the name shows, are the most important links supporting the structure of a forum site. As afore-mentioned, to facilitate users' browsing, in Web forums ther e are usually more than one path to visit a page. For example, as shown in Fig. 1 (b), to browse a post-of-thread page, we can either di-rectly click on the links in the green square in a list-of-board page; or follow the links in the red ellipse in a list-of-board page to of-thread page to read each post. Actually, links in the green square are a kind of shortcut links which are designed to help users read the newest posts more conveniently; while links in the red ellipse reflect the real organizational structure of the forum. That is, browsing the list-of-board pages, list-of-thread pages, and post-of-thread pages in a sequential manner. Although a crawler can download some post pages following the shortcut links, it takes a huge risk that most post pages will be missed i n the crawl-ing. In fact, only the newest post pages have shortcuts in the list-of-board page. Moreover, there are often multiple shortcuts point-ing to a page, which may lead to duplicate copies in cra wling. In contrast, skeleton links ensure to reach all the valuable p ages with few redundant. P
AGE-F LIPPING L INK . Page-flipping links are a kind of loop-back links in the sitemap. That is, following such kind of l ink we can reach another page in the same vertex in the sitemap gra ph. Cor-rectly dealing with page-flipping links has the following adv an-tages which were ignored in previous works: 1) it enables the crawler to completely download a discussion thread. A long thread may consists of tens or even hundreds of pages, most of which are missed in a generic crawling as their link dept hs are too deep; and 2) it also provides useful information to rank forum data. The number of flipping pages can be used to approximate the popularity (how many replies) of a discussion thread. Howev-er, it should be noticed that not all loop-back links are pag e-flipping links. Fig. 1 (c) shows such an example. In Fig. 1 ( c) there are two kinds of loop-back links: one is the "previ ous" and "next" buttons marked with the orange square, which actually a re shortcuts pointing to the posts of another two discussion thre ads; the other is the digital sequence marked with the purple elli pse, which are real page-flipping links pointing to the rest of posts in the same thread. Therefore, we need a method to detect page -flipping links from other loop-back links in the sitemap. The flowchart of our method is illustrated in Fig. 2, whi ch mainly consist of two parts: (I) sitemap recovering and (II) traversal strat-egy exploring. The goal of the first step is to estimate the sitemap structure of the target forum using a few sampled pages. Therefore, the sa mpling Fig. 1. An illustration of the (a) Sitemap, (b) Skele ton Link, and (c) Page-Flipping Link. The graph in (a) and the scr een shots in (b) and (c) have been simplified for a clear view. Fig. 2. The flowchart to the proposed approach, which con-sists of two parts: (I) sitemap recovering; and (II) t raversal strategy exploring quality is the foundation of the whole mining process. To keep the sampled pages as diverse as possible in terms of page l ayout and to retrieve pages at deep levels, we adopted a combined s trategy of breadth-first and depth-first using a double-ended queue. In the implementation, we tried to push as many as possible unsee n URLs from each crawled page to the queue, and then randomly pop a URL from the front or the end of the queue for a new sam-pling. In practice, it was found that sampling a few thousand s pages is enough to re-construct the sitemap for most forum s ites. After that, pages with similar layout structures are furt her clus-tered into groups ( i.e. vertices) using the single linkage algorithm, as marked with green dashed ellipses in Fig. 2. In our appr oach, we utilized the repetitive regions to characterize the content layout of each page. Repetitive regions are very popular i n forum pages to present data records stored in a database. Consi dering that two similar pages may still have different numbers o f adver-tisements, images, and even some complex sub-structure embed-ded in user posts, the repetitive region-based representation i s more robust than the whole DOM tree [15]. Finally, all possi ble links among various vertices are established, if in the source ver-tex there is a page having an out-link pointing to another page in the target vertex. As aforementioned, each link is describe d by both the URL pattern and the location (the region where the cor-responding out-link is located). For more details of this step, please refer to our previous work in [7]. The second part is in charge of exploring an optimal traversa l strategy on the constructed sitemap. First, through the module of skeleton link identification , the links pointing to redundant and uninformative pages are discarded and only the links pointing to valuable pages are preserved, as marked with the dark arro ws in Fig. 2. Then, all the possible loop-back links of page-flip ping, as marked with the red dashed arrows in Fig. 2, are detected f rom the skeleton links through the module of page-flipping link detection . This indicates that a page-flipping link must be a skeleton l ink first. The methods in this step are discussed in detail i n the follow-ing section. In this section, we present the algorithm details for explo ring the optimized traversal strategy in the sitemap. First, th e skeleton links are identified from the sitemap to ensure an effi cient crawl-ing. Second, the page-flipping links are detected to restor e a long discussion thread which is shown with multiple pages. To indentify skeleton links, first we summarize some co mmon characteristics of skeleton links, and then propose an appro priate strategy to distinguish skeleton links from others. After investigating the organization structures of a subst antial number of various Web forums, we found that most skeleton links have the following two characteristics.  X  Skeleton links should point to those vertices containing valu- X  Skeleton links should not introduce any redundant crawling Therefore, the characteristics of a skeleton link actually depend on the behaviors of its target vertex. To quantitatively me asure the above two characteristics, in this work, we propose two criteria based on the results of near-duplicate detection. This is beca use duplicate means redundant information in crawling. Moreover, valueless pages in forum sites are usually a group of pag es which are completely duplicated with each others. For example, al l the login prompt pages returned by a forum are exactly the sam e page. In our approach, the content-based near-duplicated detection a lgo-rithm [10][11] is employed. In the implementation, each Web page is first cleaned by removing all the HTML and scri pt tags and is then characterized with fingerprints such as Shing les [6] or SimHash [11]. At last, any two pages with a small L the fingerprint space are considered as duplicates. For each vertex in the sitemap, assuming that the number of sam-pled pages in this vertex is N , and we finally get K unique pages after the near-duplicate detection. The two criteria, coverage and informativeness , are defined as follows. 1) Retrieving all the K unique pages is the ideal result we want 2) Suppose With these two criteria, we can evaluate each link inde pendently. However, considering that the choosing of a link (or a set o f links) may affect the evaluation of other links, it still cannot g uarantee a global optimization only with the above criteria. We stil l need a strategy to search the skeleton links from a global perspec tive. Typically, a user usually starts browsing a forum from i ts portal, follows one link on the portal to visit a page in the seco nd level, and then navigates to pages in deeper levels. Inspired by th is ob-servation, the search process is carried out according t o the depths of vertices in the sitemap, as shown in Fig. 3. That is , vertices are investigated one by one from vertices in top levels to v ertices in deeper levels. The processed vertices are put into the se t PV , and the unprocessed vertices are in the set UV . For each vertex in UV , all the links from the vertices in PV pointing to this vertex (in-cluding its loop-back links) are the candidates of skeleton l inks for this vertex, denoted as procedure of GetSkeletonLinks . The goal of GetSkeletonLinks is to find out a combination of links from above two criteria. Considering each L i may be selected as a ske-leton link or not, the search space consists of 2 the search space can be represented as an m -level binary tree, in which the two sub-trees under a node denote the two decisions (selected or not), as the two dashed boxes shown in Fig. 4. To accelerate the search process, in practice a pruning is adopted with the following two rules: (1) if a link is selected but ca uses a sig-nificant drop of informativeness, it shouldn't be a skeleton l ink. The sub-tree in which the link is selected can be pruned; (2) i f a link is rejected but causes a significant drop of coverag e, it should keeps both of the two decisions and investigates the rest sea rch space recursively. The detailed algorithm of GetSkeletonLinks is shown in Fig. 5. Page-flipping link is a kind of loop-back links of a vertex. But not all the loop-back links are page-flipping links. For exampl e, in the vertex of post-of-thread , there are usually two kinds of loop-back links. One is the page-flipping link which connects multiple pages belonging to one thread, as the purple ellipse shown in Fig . 1 (c). The other is the "Previous/Next" navigation link which po ints to the first page of other threads, as the orange rectangle s hown in Fig. 1 (c). The goal of page-flipping link detection is to distin-guish the page-flipping link from other loop-back links. In this paper, we found a special characteristic for page-fli pping links. That is, for page-flipping links, if there is a pat h from a page A to another page B , there must be a path from B to A . While for other loop-back links, they cannot guarantee such a character istic. For example, we can browse a thread from its first pa ge A n page A n , and can also go back from A n to A 1 page-flipping link. However, for "Previous/Next" navigatio n links, if we jump from A n to another page B 1 in another thread, we can-not go back from B 1 to A n only with navigation links  X  we can only first jump to A 1 and follow the page-flipping link to go back to A In other words, using the navigation link there is only a path from A to B 1 , but no path from B 1 to A n . Based on this observation, for each kind of loop-back link, we define a measurement called "connectivity", as: where A and B are any two pages in the vertex. If there is a path from A to B following only this kind of link, otherwise link will receive evidently larger "connectivity" scor e than other loop-back links. In the experiments, we will exhaustively com pare this characteristic between page-flipping links and other lo op-back links on various forum sites. In practice, for each vertex, we first estimate the co nnectivity of each loop-back link as defined in Eq. 3. Meanwhile, the ave rage connectivity score of all the loop-back links on this vert ex is also estimated. Finally, the loop-back links whose connectivity is higher than the average score are selected as the page-f lipping links for this vertex. Identify the skeleton links in the sitemap 1. input: an unprocessed vertices set UV ={all the vertices 2. output: a set of skeleton links SL ; 3. begin 4. SL =  X  ; 5. while ( viiiix  X   X  ) do 6. V  X  the top most level vertex in UV ; 7. viiiix  X  viiiix X  {  X  } and iiiix  X  iiiix X  {  X  } 8. Let {  X  1 , ... ,  X   X  } be the links from a vertex in PV 9. SL  X  SL  X  GetSkeletonLinks ( {  X  1 , ... ,  X  10. end for while 11. end Fig. 4. An illustration of the search process of skel eton links. GetSkeletonLinks ( { IX  X  , ... , IX  X  } ) 1. input: links set {  X  1 , ... ,  X   X  } 2. output: a set of skeleton links; 3. Begin 4.  X  X  X  X   X  X  X  X  = 0;  X  X  X  X  X  X   X  X  X  X  = 0; // to record the coverage 5. Define  X  X  X  X  X  X  X   X  X  X  X  X  to record the best state of links in the 6.  X  X   X   X  selected, for all 1  X  i  X  m ; 7. JudgeSkeletonLink (1); 8. return the selected links in  X  X  X  X  X  X  X   X  X  X  X  X  ; 9. End JudgeSkeletonLink (  X  ) 1. 2. Begin 3. if ( i &gt;m ) 4. if (  X  X i is better than  X  X  X  X  X  X  X   X  X  X  X  X  ) 5. 6. exit; 7. Endif 8. for ( CS i in {unselected, selected}) 9.  X  X  X  X   X  X  X  X   X  coverage of  X  X i ; 10.  X  X  X  X  X  X   X  X  X  X   X  informativeness of  X  X i ; 11. if (  X  X  X  X   X  X  X  X  12. JudgeSkeletonLink ( i +1 ); 13. Endfor 14. End In this Section, we present the experimental results of the pro-posed system, including the performance analysis of our syst em, and some comparisons with a generic crawler, the structure -driven crawler in [15], in terms of both the effectiveness and efficiency. To evaluate the performance of our system on various situa tions, eight different forums were selected in diverse categ ories (includ-ing bike, photography, travel, computer technique, and some gen-eral forums) in the experiments, as listed in Table 1. Since the structure-driven-based approach in [15] seems promis-ing and is more relevant to our work, we also include it in the experiments. The original structure-driven-based approach only crawl one kind of target pages, and therefore requires o ne sample page of the target pages. But there are usually multiple k inds of valuable pages in a forum site. We have tried our best to adapt the structure-driven-based approach to forum crawling by: 1) manual -ly choosing one sample page from each vertex since pages in the same vertex have similar layout; and 2) adjusting the thr eshold of URL pattern generation in the structure-driven-based approach . To set up a consistent data collection for further evaluat ion and comparison, we first mirrored the above eight sites using a com-mercial search engine crawler. The crawler was adjusted to be domain-limited, depth-unlimited and never dropping pages. For each site, the crawler starts from the homepage and follo ws any links belonging to that domain; and a unique URL address will be followed only once. For each forum we kept the first 30,0 00 cor-rectly crawled pages, which is large enough to cover all kinds of pages in that forum and can be considered as a sketch of the origi-nal site. The following crawling experiments, including our me-thod, the structure-driven crawler, and the generic crawl er, were all simulated on this data collection. Moreover, for a quantitative evaluation, we also manuall y labeled the traversal strategy as the baseline. Since there are q uite few performance differences between the baseline and the strat egy by our automated exploring method, we will not compare them fu r-ther in the later part. The performance analysis here includes: 1) the crawling quali ty; and 2) the crawling effectiveness and efficiency. First, as introduced in Section 4 and Section 5, for eac h site we randomly sampled 5,000 pages to build the sitemap and generate the traversal strategy. Then we used our method and the st ructure-driven crawler to simulate the crawling of 30,000 pages on t he mirrored dataset which is treated as a sketch of the ori ginal sites. We first evaluated the informativeness of results using different methods. Fig. 6 illustrates the comparison of our method, th e structure-driven crawler and the mirrored sites crawled by t he general crawler. Informativeness is defined in Eq. (2) in S ection 5.1.1. It is clear that our method can effectively increas e the in-formativeness by avoiding duplicate pages in the original mirrors. On average, the informativeness score by our method can reac h 0.97; for the structure-driven crawler the number is 0.85; while for mirrored pages the number is 0.67. Moreover, it is worth noting that our method can significantly increase informativeness w hile keeping a high coverage ratio. As shown in Fig. 7, more tha n 83% valuable pages can be visited by our method. But there are only 40% valuable pages that can be visited by the structure-drive n crawler. The structure-driven crawler drops both the coverage and i nfor-mativeness because the traversal strategy for a forum site is gen-erally much more complex than that for a traditional web sit e. There are several reasons. First, even to crawl one kind o f target pages, multiple traversal paths are required in forum sit es. For example in the "Asp.Net" forum, the list-of-thread pages can be crawled from list-of-board pages. But there are a lot of successive thread list pages can be only crawled from previous list-of-thread pages by page-flipping links. But the structure-driven crawler only considers one traversal path for each target. Second, the structure-driven crawler describes a traversal path based on URL patterns which are usually not robust for forum sites. Such a s in the forums "CQZG" and "Biketo", the structure-driven crawler cannot generate reasonable URL patterns by its automatic m ethod to distinguish valuable pages from duplicate pages and wi ll drop the informativeness of results. Fig. 6. The comparison of informativeness for the mirr ored pages and, the pages crawled by our method and the struc -ture-driven crawler, on various forums, respectively.
 Fig. 7. The coverage of valuable pages retrieved by the s truc-ture-driven crawler and our method. From Fig. 6 and Fig. 7, we can find that the informative ness scores of various forums are quite different by the gener ic crawler. Some forums supported by commercial companies are well de -signed and have less valueless pages, such as Baidu. Most fo rums developed by commercial organizations are of this style . In con-trast, some forums like CQZG are of lower informativene ss due to many duplicate pages. Also, forums with restricted access controls are often of lower informativeness, such as the Asp.Net f orum. In contrast to the generic crawler and the structure-driv en crawler, our method can achieve promising performance on all these fo-rums. We also noticed that there are still some problems that should be improved. First, we should better balance the tra deoff between guaranteeing coverage and removing useless pages. Tak-ing CQZG as an example, its coverage is somewhat harmed a l-though its informativeness significantly increases. Second, we need more evidences to remove duplicates. In the current i mple-mentation, the duplicate detection is mainly based on the a lgo-rithm introduced in Section 5.1, which may be too simple t o han-dle very complex situations. Effectiveness and efficiency are two important criteria to evaluate a Web crawler. Effectiveness means that given a number o f re-trieved pages, how many of them are valuable and informati ve. Effectiveness is very important for saving network bandwidt h and storage space. Efficiency means how fast a crawler can retrieve a given number of valuable pages. Efficiency is important as it de-termines how quickly a crawler can update its repository a nd in-dexing. In this subsection, we compare our method with the ge-neric crawler and the structure-driven crawler, in terms of both effectiveness and efficiency. To evaluate the effectiveness, we retrieved additional 5,000 pages from the mirrored sites using the generic crawler, the s tructure-driven crawler and our method, respectively. We can find out how many valuable and duplicate pages were visited by the three c raw-crawled by the generic crawler, the ratio of valuable pa ges is around 59%, averaged across the eight forums. The average ratio is 69% for the structure-driven crawler. In comparison, the aver-age ratio of valuable pages crawled by our method is 91%, which is a significant improvement over the other two methods. I n other words, given a fixed bandwidth, our method can crawl almost 1.53 times valuable pages than a generic crawler and 1.32 t imes valuable pages than the structure-driven crawler. To evaluate the efficiency, we continually crawled each mi rrored site until additional 5,000 valuable pages are retrieved . Then we investigate how many pages have to be downloaded respectiv ely using the generic crawler, the structure-driven crawler and our method. The results are shown in Fig. 9 (a), (b) and (c) . From Fig. 9, it can be seen that to archive 5,000 valuable pages, a generic crawler averagely needs to crawl about 8700 pages; the struc ture-driven crawler needs to crawl about 7100 pages. In contrast, o ur method only needs to fetch about 5500 pages. Consequently, su p-posing a constant downloading time for each page, to archive the same number of valuable pages, a generic crawler will re quire 1.57 times crawling time than our method and the structure-dr iven crawler will require 1.40 times crawling time than our me thod. In this section, we evaluate the performance of the page-flipping link detection. First of all, we consider all the loop-ba ck links in a site as the candidates for page-flipping links. Then we dete ct page-flipping links among all the candidates by measuring thei r connectivity scores according to the algorithm presented i n Sec-tion 5.2. To evaluate the distinguish ability of our method on dif ferent size of dataset and find an appropriate size; we repeated the m easuring Fig. 8. The comparison of effectiveness between (a) t he generic crawler, (b) the structure-driven crawler and (c) our method, on the top 5,000 retrieved pages.
 Fig . 9. The comparison of efficiency between (a) the generic crawler , (b) the structure-driven and (c) our method , to re-trieve 5,000 valuable pages. process for each link by varying the number of sampled pages. There are 31 candidate loop-back links in all forum sites. The connectivity scores of these loop-back links are shown in F ig. 10. We manually labeled all candidates and mark in Fig. 10 t he 14 page-flipping links in blue and the other 17 non page-flipping links in orange. It can be seen that the difference betwee n page-flipping links and non page-flipping links are distinct using 5,000 pages and the difference becomes more apparent using large r datasets. With 5,000 sampled pages for loop-back links of al l sites, the minimal connectivity score for page-flipping links is higher than 70% while the maximal score for non page-flipping links is lower than 50%. The results show that the proposed crite-rion can accurately detect the page-flipping links from other loop-back links. In this paper, we have presented a complete solution to aut omati-cally explore an appropriate traversal strategy to direct the crawl-ing of a given target forum. Traditional generic crawle rs which simply adopt the breadth-first traversal strategy are usually ineffi-cient due to the complex in-site link structures and login controls in most forum sites. To understand the global organization o f a forum, we proposed to first re-construct its sitemap based on a few thousands pages randomly sampled from the target site . In this way, we can leverage more global knowledge to explor e the traversal strategy. The proposed solution mainly consists of the identification of skeleton links and the detection of page-flipping links. The skeleton links instruct the crawler to only cra wl valua-ble pages and meanwhile avoid duplicate and uninformative ones ; and the page-flipping links tell the crawler how to completel y download a long discussion thread which is usually shown in multiple pages in Web forums. To evaluate the proposed trav ersal strategy, we conducted extensive experiments on several for ums. The experimental results demonstrated promising performance in terms of both efficiency and effectiveness. Following the discov-ered traversal strategy, our forum crawler can effici ently archive more informative pages and restore a long discussion threa d which is divided into multiple pages, which is impossible in most previous works. In the future, we will study how to optimize the crawling sc hedule to incrementally update the archived forum content, and how to parse the crawled forum pages to separate replies in each post thread. This will further structuralize forum pages, and e nable many interesting and promising applications based on forum con-tent mining. [1] Internet Forum. http://en.wikipedia.org/wiki/Internet_forum . [2] S. Abiteboul, M. Preda, and G. Cobena. Adaptive on-line [3] R. Baeza-Yates and C. Castillo. Crawling the infinite We b: [4] R. Baeza-Yates, C. Castillo, M. Marin, and A. Rodrigue z. [5] S. Brin and L. Page. The anatomy of a large-scale hypertex-[6] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Z weig. [7] R. Cai, J.-M. Yang, W. Lai, Y. Wang, and L. Zhang. iRobot : [8] S. Chakrabarti, M. van den Berg, and B. Dom. Focused [9] Y. Guo, K. Li, K. Zhang, and G. Zhang. Board forum crawl-[10] M. Henzinger. Finding near-duplicate Web pages: a large-[11] G. S. Manku, A. Jain, and A. D. Sarma. Detecting near-[12] F. Menczer, G. Pant, P. Srinivasan, and M. E. Ruiz. Ev aluat-[13] S. Pandey and C. Olston. User-centric Web crawling. In [14] S. Raghavan and H. Garcia-Molina. Crawling the hidden [15] M. L.A. Vidal, A. S. da Siva, E. S. de Moura, and J. M. B. Fig . 10. The connectivity scores of all the loop-back links with different numbers of sampled pages. Page-flipping links are marked in blue and non page-flipping links are in orange . 
