 Vasil S. Denchev denchev@gmail.com Nan Ding ding10@purdue.edu S. V. N. Vishwanathan vishy@stat.purdue.edu Purdue University, West Lafayette, IN 47907, USA Hartmut Neven neven@google.com Google, Los Angeles, CA 90291, USA In recent years machine learning researchers and prac-titioners have been focusing on convex optimization methods due to their computational advantages and well understood mathematical properties. The many successes of convexity-based algorithms are witnesses to that. While it is easily recognized that allowing for non-convex objectives opens up a plethora of possibili-ties for better solutions to machine learning problems, much of the contemporary research has deliberately avoided them. The reason is the widely known fact that non-convexity often results in NP-hard problems. However this choice comes at a cost as the shortcom-Appearing in Proceedings of the 29 th International Confer-ence on Machine Learning , Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). ings of convex objectives are also well understood. Re-cent work (Long &amp; Servedio, 2010) showed that convex loss functions cannot be made robust in the presence of label noise because they cause unbounded growth of penalties for large negative margins. (Manwani &amp; Sas-try, 2011) further characterized this effect by analyzing various convex losses and found that none of them is tolerant to non-uniform label noise. In practice label noise turns out to be a serious problem due to the fact that it affects real-world data sets to a significant de-gree. Since label noise manifests itself throughout the optimization as large negative margins, the finally con-structed decision hyperplane that represents the global minimum of any convex loss tends to be pulled by the mislabeled training examples away from the minimizer of classification error. Therefore, even though solving convex losses to optimality is feasible, when label noise causes the lowest objective value to not correspond to the lowest attainable training error, the entire exer-cise misses the mark. Consequently, any approach ex-hibiting this problem does not stand to benefit from improved optimization techniques.
 Fig. 1 shows an example of the broken correspondence between training error and objective value when a con-vex loss is used in a training problem of practical significance X  X  X CR in photos X . The human task of tagging characters in photos of potentially poor qual-ity is not easy, so the presence of mislabeled exam-ples in the training set is not surprising. Even worse, routinely used semi-automatic preparation of training data is also contributing to mistakes. The problem may gradually disappear for cleaner data sets, which often happen to be the cases when convex losses pro-duce excellent classifiers. Unfortunately the nature of large-scale supervised learning does not permit elab-orate quality assurance for data sets that are handed out to training algorithms; accordingly label noise will continue to pollute real-world data sets. Moreover fu-ture intelligent systems will rely increasingly on weakly labeled data, thus increasing the need for robustness. Figure 1. Relationship between training error and inverse empirical risk produced by minimizing square loss on six  X 2 X  vs the rest, etc.) The data ( X  X CR in photos X ; 10200 dimensions; 38924 examples; 10 classes) represents a chal-lenging real-world training problem of significant practical importance. An adequate loss function should generally be decreasing training error as the empirical risk approaches global minimum (top plots). Unfortunately, the opposite effect (bottom plots) can often be observed when working with convex losses. The failures are found to be due to two factors, both of which cause square loss to be drasti-cally misled by its convexity: occasionally mistaken labels resulting from the semi-automatic process generating the data; and the presence of examples of one class that may be similar to examples of another class (e.g.  X 6 X  and  X 8 X ). (Ding &amp; Vishwanathan, 2010) and (Masnadi-Shirazi et al., 2010) took these lessons and independently stud-ied two different non-convex but seemingly well be-haved types of loss functions. (Collobert et al., 2006; Ertekin et al., 2011) also explored non-convexity in the context of SVM with ramp loss, but their focus was on achieving sparser sets of support vectors and speed of training rather than improved accuracy and robustness of the constructed classifier.
 In the present work we continue the study of non-convexity. We report on training with a non-convex objective using discrete optimization in a formula-tion adapted to take advantage of emerging hard-ware that performs adiabatic quantum optimization (AQO). AQO, first proposed in (Farhi et al., 2000), is a quantum computing model with good prospects for scalable and practically useful hardware implementa-tion. Studies of its purported computational superior-ity over classical computing have repeatedly given en-couraging results, e.g. (Dickson &amp; Amin, 2011). Sig-nificant investments are underway by the Canadian company D-Wave to develop a hardware implemen-tation. A series of rigorous studies of the quantum mechanical properties of the D-Wave processors, cul-minating in a Nature article (Johnson et al., 2011), have increased the excitement in the quantum com-puting community for this approach. This was fur-ther fueled by news of a successful collaboration with Google (Neven et al., 2009a) and of Lockheed Martin purchasing a D-Wave machine. For machine learning purposes, D-Wave X  X  implementation of AQO can be re-garded as a discrete optimization engine that accepts any problems formulated as quadratic unconstrained binary optimization (QUBO), also equivalent to the Ising model and Weighted MAX-2-SAT. It should be noted that this training formulation is a good for-mat for AQO independently of D-Wave X  X  efforts since it can be physically realized as the simplest possi-ble multi-qubit configuration X  X n Ising system (Brush, 1967). We do not claim principled superiority of q-loss over other non-convex losses. However q -loss is distin-guished by the fact that it can be formulated for AQO on quantum hardware that only supports quadratic (2-local) interactions among its qubits. To the best of our knowledge, no other non-convex loss has this prop-erty 1 . While all other non-convex losses are tackled by convex optimization with limited success, q -loss may be solvable to optimality by AQO.
 The paper is organized as follows: Section 2 defines the training problem; Section 3 introduces q -loss, de-rives its QUBO formulation, and discusses the intu-ition behind it; Sections 4 and 5 deal with choosing hyper-parameter values and discretization of variables; Section 6 presents our experiments; and Section 7 con-cludes with overview and discussion. Technical details can be found in the Appendix of (Denchev et al., 2012). We study binary classifiers y = sign w w w T x x x + b , where x x x  X  R N is an input pattern to be classified, y  X  X  X  1 , 1 } is the label associated with x x x , w w w  X  R N is a vec-tor of weights to be optimized, and b  X  R is the bias. Training, also known as regularized risk mini-mization, consists of choosing w w w and b by simultane-ously minimizing two terms: empirical risk R ( w w w,b ) = P R , via a loss function L , estimates the error that any candidate classifier causes over a set of S training ex-amples { ( x x x s ,y s ) | s = 1 ,...,S } . The argument of L is known as the margin of example s with respect to the decision hyperplane defined by w w w and b :  X  controls the complexity of the classifier and is nec-essary for good generalization because classifiers with high complexity display overfitting X  X hey can classify the training set with low error but may not do well on previously unseen data. Training amounts to solving 1 Except for 0-1 loss (Neven et al., 2009b) A natural choice for L is 0-1 loss , which simply indicates a misclassification for a negative margin: L 0-1 ( m ) = (1  X  sign ( m )) / 2. Due to the non-convexity of L 0-1 , the resulting optimization problem (2) is NP-hard (Feldman et al., 2010). To avoid dealing with NP-hard problems, in practice L 0-1 is replaced by some convex upper bound (e.g. square, logistic, exponen-tial, hinge), and  X  is usually chosen as ` 1 -or ` 2 -norm penalization of w w w . This allows arriving at convex opti-mization problems that can be rigorously analyzed and efficiently solved by classical means. However, such re-laxations are known to compromise the original goal of training because convex losses can be severely misled by label noise in the training data. Because the quantum hardware natively represents a general family of quadratic functions, the simplest loss function that would work is square loss , which is a convex upper bound to L 0-1 : However, there are two drawbacks of square loss when applied to binary classification. First, in binary clas-sification it does not make sense to penalize large pos-itive margins. Second, as mentioned earlier, square loss has the same flaw as all convex losses X  X enalties for large negative margins grow unboundedly, which can cause non-robustness with respect to label noise. With these considerations in mind, we modify square loss in order to obtain a training formulation for binary classification that is both compatible with quantum hardware and robust to label noise. The resulting loss, which we name q-loss (Fig. 2, top), is essentially a doubly truncated version of (3) with parameterization over q  X  (  X  X  X  , 0] defined as follows: Definition 1 (q-loss) Unfortunately, (4) does not lead to a QUBO. However, we can transform it into a problem that can be solved as a QUBO. The basic idea is to find a variational approximation via a family of quadratic functions that upper-bound q -loss and are governed by a variational parameter t  X  R as shown in Fig. 2, middle.
 Theorem 2 q-loss in (4) is equivalent to: L ( m ) = min Figure 2. Top : q -loss for different values of q . Middle : q -loss with three members of the quadratic upper bounds family. t  X  R is the variational parameter. Bottom : Tran-forming the y -axis for concavity.
 Proof Since q -loss is non-convex, the standard deriva-tion via convex duality (Jordan et al., 1999) dictates that we first find a new coordinate system in which q -loss is concave or convex. Then we calculate the con-jugate function for linear bounds in the transformed space and transform back to the original space where the linear bounds become the quadratic bounds shown in Fig. 2, middle. Because of the presence of two constant segments in q -loss, any coordinate system in which the two axes are independent transforma-tions of the original x and y axes clearly cannot re-sult in concavity or convexity. Thereby we are led to the transformation f ( y ) = y  X  x 2 , which gives f ( L q ( m )) = L q ( m )  X  m 2 . It can be seen (Fig. 2, bottom) that in this transformed space q -loss is con-cave and the quadratic upper bounds become tangent lines. The conjugate function in the transformed space is g (  X  ) = min m {  X m  X  f ( L q ( m )) } .
 To minimize, we seek stationary points by differenti-ating  X  (  X ,m ) =  X m  X  f ( L q ( m )) with respect to m :  X   X  X   X  (  X ,m ) =  X   X  as yielded by piecewise differentiation of L q ( m ). Set-ting to 0 gives the stationary points
Plugging them back into the conjugate function yields g (  X  ) =
In accordance with convex duality, f ( L q ( m )) = min = min Transforming back into the original space and setting t =  X   X / 2, the variational upper bound for q -loss is
L q ( m ) = f  X  1 ( f ( L q ( m ))) (10) 3.1. Latent variables view Traditionally when facing non-convex optimization problems, a viable approach is to introduce latent vari-ables that allow reformulating over a simpler family of functions. This is precisely what Theorem 2 achieves. For any fixed m , the latent variable t  X  R gives a con-vex optimization problem whose minimum is L q ( m ):
L q ( m ) = h ( m,t  X  ( m )) , where (11) t  X  ( m ) = arg min h ( m,t ) = ( m  X  t ) 2 + (1  X  q ) 2 (1  X  sign ( t  X  1)) / 2 The regularized risk minimization (2) with empirical risk over L q in the form (11) is amenable to a block coordinate descent method for jointly optimizing the model parameters ( w w w,b ) and the latent variables t for s = 1 ,...,S : similarly to EM, alternate between convex optimization runs over the latent variables ( t step ) and the model parameters ( w step ). Even though such methods do well on some problems with certain benign structure X  X .g. Gaussian mixtures (Dempster et al., 1977)) X  X hey are also known to fail on other problems that lack such structure. We believe q -loss belongs to the latter group and have verified that a block coordinate descent method is likely to be sensi-tive to initialization and is quickly terminating in bad local minima. The intuitive reason is that due to the quadratically growing penalty for mismatching a mar-gin with its latent variable, the t step tends to lock in the model parameters found during the previous w step, thus possibly preventing the next w step from moving to a different model. The impact of this effect becomes ever more severe for large data with S &gt;&gt; N . On the other hand, by transforming (4) into (11) we have made training with q -loss representable in QUBO form albeit at the expense of additional variables. Sec-tion A of (Denchev et al., 2012) explicitly shows the QUBO problem that can be derived from (11). Since the goal of AQO is to perform global optimization si-multaneously over all variables, we believe AQO is a much better candidate for training with q -loss. Besides making the QUBO formulation possible, the introduc-tion of latent variables also gives rise to an intuitive in-terpretation of the mechanism by which q -loss achieves robustness when compared to the non-robustness of square loss. While in (3) the fixed target 1 has to be matched as closely as possible by m , in (11) t plays the role of a flexible target that can change sign for a large negative margin, thereby flagging that training example as mislabeled. t  X  ( m ) in (11) is: Case I ensures zero penalty for large positive margins; Case II produces the same quadratic penalty as (3); Case III can be seen as flipping the label of a possi-bly mislabeled example but also incurring a constant penalty of (1  X  q ) 2 in order to not lose connection with the original labeling. Thus, the hyper-parameter q de-fines the largest negative margin to be tolerated. A training example that has a negative margin with some larger magnitude gets flipped with constant penalty. While it is difficult to formalize any general statements about the computational hardness of q -loss, it is easily recognized that the hardness depends on the size of the parabolic segment controlled by q . For q  X  X  X  X  , even the negative margins of highest magnitude incur the usual quadratic penalty, and the loss becomes effec-tively convex. For smaller q the loss becomes similar to 0-1 loss, so the resulting optimization problems may be approaching the hardness of the corresponding 0-1 loss problems. However, the most beneficial regime of operation is not known a-priori. This necessitates cross-validation over q , which, depending on the noise level, we expect to result in some trade-off between robustness and computational hardness. For the pur-pose of choosing values for cross-validation, we give an approximate lower bound for q as a function of our estimate of the underlying Bayes error in the data and the label noise that we might artificially insert into the training set for robustness evaluation.
 Let the effective Bayes error be  X  eff  X  [0 , 0 . 5). This should account both for the Bayes error  X  0 of the data that we are given and the additional error  X   X  [0 , 0 . 5) that we introduce by injecting label noise. Then if we wish for the entire  X  eff portion of the training set to be flagged by q -loss as mislabeled, the empirical risk is R ( w w w,b )  X   X  eff  X  (1  X  q ) 2 . But we know the trivial solution consisting of all 0 weights has R (0 0 0 , 0) = 1. Then we want  X  eff  X  (1  X  q ) 2 &lt; 1, which, together with q  X  (  X  X  X  , 0], gives q  X  (1  X  1 / p  X  eff , 0]. Usually we do not have  X  0 , but we can obtain an em-pirical estimate by training on the given data:  X  emp =  X  +  X  opt +  X  gen , where  X  opt is the additional error caused by imperfect optimization, and  X  gen represents the generalization component of the overall test error. Assuming  X  emp is sufficiently close to  X  0 and account-ing for the artificially introduced label noise  X  , we set  X  eff =  X  emp  X  2  X  emp  X  +  X  . The subtraction corrects for originally bad examples that flip under  X  . The quantum optimization processor that we aim to deploy for solving q -loss requires problems to be dis-crete and formulated as QUBO. Further, the current hardware can handle a maximum of 512 binary vari-ables, which imposes the additional requirement of be-ing frugal with the bit-depth of weights. To that end we discretize the elements of w w w to some low bit-depth d &lt; 64. While this approach is somewhat unconven-tional, (Neven et al., 2008) argued there is no funda-mental reason why the weights should need high pre-cision and in fact showed a favorable sufficiency condi-tion of d w  X  log( S/N ) in the case of binary features. Even though classifiers constructed out of more general features have not been studied in this way, our experi-ments provide support for using low-precision weights. The reason for fixing at 1 the smallest positive margin that yields zero penalty in q -loss is the same as in hinge loss SVM (Bishop, 2006): any arbitrary rescaling of the weights w w w  X   X w w w and bias b  X   X b does not change can assume a margin of 1 for the correctly classified point that is closest to the decision surface. However, this freedom of arbitrary rescaling becomes compli-cated when the bit-depth of weights is lowered. We want the intervals for weight variables to cover the maximum magnitude that the interplay between mar-gin enforcement and regularization may demand. On the other hand, a loose interval decreases the effec-tive precision in sub-intervals that may really matter. Thus we derive a  X  -dependent bound for setting the intervals in which discrete weight variables take values. F (0 0 0 , 0) =  X   X (  X  w w w ). Then, Now consider any  X  w w w 3  X (  X  w w w )  X   X (  X  w w w ): to bound the intervals in which the weight variables live while ensuring that the minimizer of F belongs to these intervals. For ` 2 -norm regularization,  X (  X  w w w ) = w  X  j only in the interval [  X   X (  X  w w w ) ,  X (  X  w w w )]. The discrete optimization problem for training with q -loss and ` 2 -norm regularization is: (  X  w w w,  X  b )  X  = arg min where  X  w w w and  X  b are the discretized w w w and b , and  X   X  controls the relative importance of regularization. While prior work on non-convex losses applied var-ious forms of convex optimization (Masnadi-Shirazi et al., 2010; Yuille &amp; Rangarajan, 2002; Liu et al., 1989) hoping they can still be solved with somewhat reasonable quality, we take the approach of directly tackling the resulting problems by discrete optimiza-tion. Admittedly, this choice makes the optimization method largely oblivious to existing benign structure and may cause us to face NP-hardness in certain situ-ations. However, we do this for the purpose of being compatible with emerging quantum hardware that can be employed as a discrete optimization engine having the potential to do well on such problems.
 Quantum hardware was already successfully deployed by (Neven et al., 2009a) on a large-scale training prob-lem with square loss and ` 0 -norm regularization. In the present work on q -loss with ` 2 -norm regularization, we only verify the validity of our approach by using Tabu search (Palubeckis, 2004) as a classical heuris-tic stand-in and leave the quantum hardware to future work. A quantum optimization with q -loss is expected to achieve in shorter time equal or better results than our classical optimization setup. We do not report CPU time comparisons because they are irrelevant in the absence of quantum hardware runs.
 In order to show robustness, we randomly flip train-ing labels and observe the worsening of test error as a function of increasing label noise. While prior work on robust classification (Ding &amp; Vishwanathan, 2010; Collobert et al., 2006; Freund, 2009) considered uni-form label noise, we note this does not adequately capture the essence of the true mechanism by which label noise trickles into real-world training tasks. In fact, recent work (Manwani &amp; Sastry, 2011) shows that even convex losses can be robust under uniform noise. Moreover, experience with practical applications con-firms that the type of label noise that affects classifica-tion accuracy is never independent of the underlying data distribution. For example, if the human taggers preparing training data for a computer vision appli-cation receive somewhat inaccurate or ambiguous in-structions affecting only one of the classes, the result-ing label noise is strongly correlated with that class. For this reason we move to a noise model in which we introduce uniformly random flips only in the labels of one class X  X ere WLOG of the negative class X  X nd keep the labels of the other class clean. In the ex-periments described below, the percentage label noise refers to the probability with which we flip labels in the negative portion of training data.
 We conduct experiments on two synthetic and four UCI data sets with data summary given in Section B of (Denchev et al., 2012). The synthetic data sets (Long &amp; Servedio, 2010; Mease &amp; Wyner, 2007) are designed to provide stark distinction between robust and non-robust losses. We compare the classification perfor-mance of q -loss to seven other convex and non-convex ` -regularized methods: liblinear ( ` 2 -loss primal SVM) (Fan et al., 2008), t-logistic regression (Ding &amp; Vish-wanathan, 2010), smoothed hinge loss (Zhang et al., 2010), logistic regression, square loss, sigmoid loss, and probit loss (Bishop, 2006). For all methods except q -loss and liblinear we use Petsc/Tao implementations with convex optimization (Balay et al., 2011; Benson et al., 2010). We do not compare against ramp loss (Collobert et al., 2006), as (Ding &amp; Vishwanathan, 2010) already attempted it in a similar setting on the majority of data sets we use and were unable to pro-duce any salient results. Not surprisingly, this is an ex-ample of the inadequacy of convex optimization meth-ods when applied to non-convex problems. Also, we do not compare against 0-1 loss because it is not margin-enforcing. It is well known that if minimized, 0-1 loss yields the lowest possible training error, but due to the lack of margin enforcement, generalization is bad even when regularization is applied (Vapnik, 1998).
 With all methods we perform a standard 10-fold cross-validation procedure (Dietterich, 1998) for locating appropriate values of parameters affecting generaliza-tion. Fig. 3 presents the main results with an empha-sis on the consistently superior performance of q -loss across all data, especially at high levels of noise. We have verified that often in the high noise cases Tabu search fails to reach the lowest attainable objective value. Therefore we believe we are looking precisely at cases of computationally hard optimizations that fail classically but may be solved successfully by quan-tum means. We note sigmoid and probit are some-times close competitors of q -loss but other times are the worst performers. This can be explained by their non-convexity, which gives them the potential for ro-bustness, but makes them hard to optimize reliably. However, unlike q -loss, we do not know of any AQO-compatible formulations for probit and sigmoid. q -loss allows identifying possibly mislabeled training examples as the points with m  X  q . We recorded the points whose labels we flipped before training ( injected flips ) and the points that q -loss flagged as mislabeled ( trained flips ). Fig. 4 summarizes the overlaps between these two sets. The sets of trained flips for covertype and adult9 are expectedly larger due to the large Bayes error of these data sets. (Denchev et al., 2012) pro-vides details on cross-validated hyper-parameter val-ues (Sections C and D) and statistical significance tests for the observed error rates (Section E). In this paper we introduced q -loss as a robust alter-native to convex losses that suffer in the presence of label noise. The QUBO format of the optimiza-tion incorporating q -loss makes this version of training an ideal candidate for applying emerging commercial AQO technology as the optimization method of choice. Moreover just by using a classical heuristic solver as a stand-in for hardware-based AQO, we were already able to show significant advantages in test error over a rich variety of data sets and across a number of exist-ing convex and non-convex losses. Our focus here was on formulating a robust loss that can be made com-patible with the engineering constraints imposed by emerging quantum hardware. Since with other non-convex losses there is no other choice but to resort to often failing convex optimization, q -loss stands out with its AQO compliance. This opens up new possibil-ities for achieving results better than ever seen before. 0 20 40 60
Mease-Wyner 0 2 4 mushrooms 1 2 3 4 web8 Given such encouraging results, we see great potential for robust classification with q -loss under AQO. Even though (11) is a QUBO, future work still needs to address the fact that on large data sets this formu-lation may result in a number of binary variables that exceeds the available physical qubits. For that reason, options for training via repeated optimization runs X  e.g. large neighborhood search X  X eed to be studied. By using suitable graph embedding techniques, we also need to address the fact that not all quadratic in-teractions between QUBO variables have correspond-ing connections between qubits on the physical device. Also, future work needs to investigate the asymptotic scaling of the time necessary for optimizing q -loss with AQO, similarly to how that was done for square loss in (Neven et al., 2009b). An open question is whether the derivation in Section 3 can be extended to expressing a more general class of functions as QUBOs.
 We thank Alessandro Bissacco for providing the  X  X CR in photos X  dataset. We acknowledge Mark Cummins, Edward Farhi, William G. MacReady, James Philbin, and Mani Ranjbar for helpful discussions.
 Figure 4. Venn diagrams showing overlap between flips in-jected in the data before training (injected flips) and flips indicated by training with q -loss (trained flips). Orange color shows portion of injected flips recovered by q -loss. References Balay, S., Brown, J., Buschelman, K., Gropp, W. D., Kaushik, D., Knepley, M. G., McInnes, L. C., Smith,
B. F., and Zhang, H. PETSc Web page, 2011. http://www.mcs.anl.gov/petsc.
 Benson, S., McInnes, L. C., Mor  X e, J., Munson, T., and
Sarich, J. TAO user manual (revision 1.10.1). Techni-cal Report ANL/MCS-TM-242, Mathematics and Com-puter Science Division, Argonne National Laboratory, 2010. http://www.mcs.anl.gov/tao.
 Bishop, C. M. Pattern Recognition and Machine Learning . Springer-Verlag New York, Inc., 2006.
 Brush, S. History of the Lenz-Ising model. Reviews of Modern Physics , 39:883 X 893, 1967.
 Collobert, R., Weston, J., and Bottou, L. Trading convex-ity for scalability. In Proceedings ICML , 2006. Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Soc. B , 39(1):1 X 38, 1977. Denchev, V. S., Ding, N., Vishwanathan, S. V. N., and
Neven, H. Robust classification with adiabatic quantum optimization. 2012. arXiv:1205.1148v2.
 Dickson, Neil G. and Amin, Mohammad H. Algorith-mic approach to adiabatic quantum optimization, 2011. arXiv:1108.3303v1.
 Dietterich, Thomas G. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation , 10:1895 X 1923, 1998.
 Ding, N. and Vishwanathan, S. V. N. t-logistic regression. In Proceedings NIPS , 2010.
 Ertekin, S., Bottou, L., and Giles, C. L. Ignorance is bliss: Non-convex online support vector machines. IEEE
Transactions on Pattern Recognition and Machine Intel-ligence , 33(2):368 X 381, 2011.
 Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., and Lin, C. J. LIBLINEAR: A library for large linear classification. JMLR , 9:1871 X 1874, 2008.
 Farhi, E., Goldstone, J., Gutmann, S., and Sipser, M.
Quantum computation by adiabatic evolution. 2000. arXiv:quant-ph/0001106v1.
 Feldman, V., Guruswami, V., Raghavendra, P., and Wu,
Y. Agnostic learning of monomials by halfspaces is hard. 2010. arXiv:1012.0729v1.
 Freund, Y. A more robust boosting algorithm. Technical report, 2009. arXiv:0905.2138v1.
 Johnson, M. W., Amin, M. H. S., Gildert, S., Lanting, T.,
Hamze, F., Dickson, N., Harris, R., Berkley, A. J., Jo-hansson, J., Bunyk, P., Chapple, E. M., Enderud, C., Hilton, J. P., Karimi, K., Ladizinsky, E., Ladizinsky, N.,
Oh, T., Perminov, I., Rich, C., Thom, M. C., Tolka-cheva, E., Truncik, C. J. S., Uchaikin, S., Wang, J.,
Wilson, B., and Rose, G. Quantum annealing with man-ufactured spins. Nature , 473(7346):194 X 198, May 2011. Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,
Tommi S., and Saul, Lawrence K. An introduction to variational methods for graphical models. Machine Learning , 37:183 X 233, November 1999.
 Liu, D. C., Nocedal, J., and Dong, C. On the limited mem-ory BFGS method for large scale optimization. Mathe-matical Programming , 45:503 X 528, 1989.
 Long, P. M. and Servedio, R. A. Random classification noise defeats all convex potential boosters. Machine Learning Journal , 783:287 X 304, 2010.
 Manwani, N. and Sastry, P. S. Noise tolerance under risk minimization. 2011. arXiv:1109.5231v2.
 Masnadi-Shirazi, H., Mahadevan, V., and Vasconcelos, N. On the design of robust classifiers for computer vision. In CVPR , pp. 779 X 786, 2010.
 Mease, D. and Wyner, A. Evidence contrary to the statis-tical view of boosting. JMLR , 9:131 X 156, 2007. Neven, H., Denchev, V. S., Rose, G., and MacReady, W. G.
Training a binary classifier with the quantum adiabatic algorithm. 2008. arXiv:0811.0416v1.
 Neven, H., Denchev, V. S., Drew-Brook, M., Zhang, J.,
Macready, W. G., and Rose, G. NIPS 2009 demonstra-tion: Binary classification using hardware implementa-tion of quantum annealing. December 2009a.
 Neven, H., Denchev, V. S., Rose, G., and MacReady, W. G.
Training a large scale classifier with the quantum adia-batic algorithm. 2009b. arXiv:0912.0779v1.
 Palubeckis, G. Multistart tabu search strategies for the unconstrained binary quadratic optimization problem. Ann. Oper. Res. , 131:259 X 282, 2004.
 Vapnik, V. Statistical learning theory . Wiley, 1998. Yuille, A. L. and Rangarajan, A. The concave-convex pro-cedure (CCCP). In Proceedings NIPS , 2002.
 Zhang, X., Saha, A., and Vishwanathan, S. V. N. Regu-larized risk minimization by Nesterov X  X  accelerated gra-dient methods: Algorithmic extensions and empirical
