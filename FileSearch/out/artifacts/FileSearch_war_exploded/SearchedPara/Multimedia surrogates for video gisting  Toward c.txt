 1. Introduction
Digital video is becoming increasingly ubiquitous but the usability of web-based video retrieval is often quite poor. One problem is that searchers need better summaries, excerpts, or other highly condensed representations of the videos to make judgments about whether to view the full videos. We refer to these human-consumable summaries as surrogates and argue that they are crucial for effective video retrieval.

There has been an enormous effort to develop and evaluate video retrieval techniques that take advantage of non-textual features (e.g., Natsev, Tesic, Xie, Yan, &amp; Smith, 2007; Ponceleon, Amir, Srinivasan, Syeda-Mahmood, &amp; Petkovic, 1999; Smith
Video Track ( Smeaton, Over, &amp; Kraaij, 2006 ). Even as better retrieval techniques emerge, ranked lists of video results must still be examined by searchers and rapid gist determinations must be made in order to select which video to watch. Thus, good video surrogates are crucial to any video retrieval system. Textual summaries such as keywords are by far the most common surrogates in music, image, and video retrieval and browsing systems, although thumbnail images are increasingly common additions to the textual summaries (e.g., YouTube). Some video retrieval systems (e.g., Open Video) also offer video fast forward surrogates, which were generally created by selecting every Nth frame of the original video and displaying the frames at normal video speed (i.e., 30 fps) or playing a slide show at 1 fps (e.g., Internet Archive), to help people quickly get the gist of full videos. Video fast forwards review the entire video at a higher frame rate, but with no audio. These quick vi-sual overviews can be quite effective (Wildemuth et al., 2003; Wittenburg, Nicol, Paschetto, &amp; Martin, 1999 ).
With advances in broadband networks, audio processing, and portable audio X  X ideo devices, audio and video surrogates be-come increasingly feasible as alternatives to or in combination with textual surrogates. The major issues with using audio sur-rogates in the context of video retrieval are twofold. First, what kinds of non-textual surrogates are effective and useful for the relevance judgment (gisting) phase of video retrieval? Second, how can these surrogates be created in cost-effective and scal-able ways? Because most video retrieval systems use keywords for search, using these keywords to create spoken keyword surrogates using speech synthesis is quite feasible. If textual descriptions do not exist, creating them is expensive to do man-ually and challenging to do automatically using automatic speech recognition (ASR) and text summarization techniques, nei-ther of which are highly reliable for web-based videos. However, if transcriptions of the video or closed captioning are already available, ASR is not needed and text summarization techniques can be used to automatically generate a short textual descrip-tion. We wondered how much additional value descriptions would have over keywords for video gisting tasks if we used auto-mated speech synthesis. We also wanted to investigate if these descriptions in synthesized speech would be more effective than visual surrogates. We used fast forwards as a benchmark visual surrogate in this study. Finally, we were interested in whether combining spoken words and fast forwards would be better or worse than the individual channels alone. 2. Research questions
This paper reports results from a laboratory user study that compared three different types of non-textual surrogates alone and in combination. One type of surrogate studied was a fast forward and the other two were spoken descriptions and spoken keywords. Combinations of the fast forward and the two spoken surrogates were also studied. The Open Video repository offers text descriptions and keywords that are manually generated by humans. The manually-generated descrip-tions and keywords used in this study were generated by taking the best two sentences of the Open Video descriptions and the best five keywords of the Open Video keywords, respectively. To generate spoken descriptions and spoken keywords completely automatically, we used a text analysis system (Neff, Byrd, &amp; Boguraev, 2004 ) that was part of a large metadata generation system (Dorai et al., 2006; Li, Dorai, &amp; Farrell, 2005 ) to generate a text summary and salient keywords from video closed captions and transcripts. We then generated spoken versions of all manual and automatic descriptions and keywords using a speech synthesizer. We then investigated how well these automatically-generated descriptions and keywords sup-ported video gisting by running two parallel studies on two groups of participants. The two studies were alike in all ways except that one study used the automatically generated surrogates and the other used manually generated surrogates.
We had four specific research questions related to spoken surrogates for video retrieval purposes: (1) Do automatically-generated descriptions and keywords surrogates approach the effectiveness of manually generated (2) How do spoken descriptions and spoken keywords compare for gisting tasks? From an information theoretic perspec-(3) How do fast forward surrogates compare with the spoken surrogates? Evidence from more than a decade of video (4) What are the effects of combining fast forwards and spoken surrogates? Our previous studies of audio surrogates
To address these questions, we conducted a user study with 48 participants divided into two groups. Both groups com-pleted multiple trials of six gisting tasks with each of the five different surrogates: fast forwards alone, spoken description alone, spoken keywords alone, fast forward combined with spoken description, and fast forward combined with spoken key-words. One group used surrogate conditions with automatically-generated descriptions and keywords and the other used surrogate conditions with manually-generated descriptions and keywords. We first describe the experimental stimuli, tasks, and procedures, and then present experimental results organized by the four research questions. Based in these results, we then discuss the design implications for video retrieval systems. 3. Experimental set up
Evaluating the effectiveness of audio and video surrogates presents significant challenges. People infer gist by combining evidence at hand (in our case, the surrogates) with personal knowledge and experience (e.g., Boguraev &amp; Neff, 2000; Ponce-leon et al., 1999; Spence, 2002 ). To evaluate these cognitive processes we have developed a series of tasks designed to reveal how different forms of evidence are used in gisting. The most direct task is generative: asking people to articulate the gist based on the evidence at hand. Other tasks are secondary: asking people to select salient features from feature sets. In order to investigate different multimedia evidence, five kinds of features are used in the tasks: keywords, keyframes, visual snip-pets, audio snippets, and textual summaries.

In the study reported here, there were five types of surrogate conditions: fast forwards alone, spoken description alone, spoken keywords alone, fast forward combined with spoken description, and fast forward combined with spoken keywords.
Participants encountered the different surrogates in a counterbalanced order. For each surrogate condition we measured task completion accuracy, confidence in task responses, task completion time, and a set of subjective measures of usability, use-fulness, enjoyment, and engagement. We also asked the participants to rate their affective responses to each surrogate con-dition in general, and elicited written comments from them. 3.1. Participants
Forty-eight participants were recruited for the study by posting mass emails to our university-wide LISTSERV. Partici-pants selected from responses were self-assessed native English speakers with adequate listening and visual abilities, who used computers daily and had experience with searching for videos using computers at least occasionally. Participants included 26 females and 22 males, ranging in age from 18 to 57 years old, and were associated with 27 different academic departments in our university. 3.2. Test videos
We selected 25 comparable videos from the open access repository of the Open Video Project ( www.openvideo.org ). lengths of the videos were similar, ranging from 28 min 15 s to 29 min 39 s, and were 28 min 36 s on average. Twenty of the 25 videos were selected from the NASA Connect collection that targets middle school science students. The other five videos were selected from the NASA Destination Tomorrow collection that targets general audiences interested in science and were used to train subject prior to the tasks (i.e., excluded from data analysis). All the 25 videos had a common structural format and were at a similar conceptual level on various middle school science topics, for example, hurricanes, aerodynamics, the Northern lights, and the global water cycle. None of the 25 videos is on the same or similar topic as the rest of the test videos. Each participant was given 25 trials with surrogates from the videos in this study. 3.3. Surrogates
Five surrogate conditions were created for each of the 25 videos: fast forward alone (FF, i.e., visual only), spoken descrip-tion alone (SD, i.e., audio only), spoken keywords alone (SK, i.e., also audio only), fast forward combined with spoken descrip-tion (FF with SD, i.e., visual and audio combined), and fast forward combined with spoken keywords (FF with SK, i.e., also visual and audio combined). For all surrogate conditions except the fast forward alone condition, two versions of each sur-rogate were created: (a) descriptions or keywords were manually generated and (b) descriptions or keywords were automat-ically generated. And in each surrogate condition, the surrogate started playing automatically when the corresponding web page was loaded. Participants could replay or interrupt the surrogates during the consumption period if desired, but could not replay once the gisting tasks had begun. Thus, for each trial, the participants were asked to work on the tasks based on their memory and understanding of the surrogate without reviewing it.

The fast forwards were created by selecting roughly every 150th frame so that each fast forward ran about 10 s to be tem-porally similar to the run times of spoken surrogates. Fast forwards, also known as rapid serial visualization presentation (Ponceleon et al., 1999 ), have been shown to be effective at very high rates (e.g., Tse et al., 1998; Wildemuth et al., 2003;
Wittenburg et al., 1999 ). The manually-generated descriptions and keywords used in the study were downloaded from the Open Video website ( www.open-video.org ).
In this study, the automatically-generated descriptions and keywords were created using the text analysis tool that is part of a system called MAGIC (Dorai et al., 2006 ). The text analysis tool integrates text summarization and keyword generation (Ferrucci &amp; Lally, 2004 ). The text summarization process extracts and ranks sentences from the video transcripts using a vari-ety of summarization techniques including discourse segmentation and topic shift detection (Boguraev &amp; Neff, 2000 ). The keyword generation process uses lexical lookup, morphological analysis, part of speech tagging, abbreviation detection, and other techniques to find domain-specific words and multi-word terms and then filters and ranks the resulting keywords (Park, Byrd, &amp; Boguraev, 2002 ). Fig. 1 presents manual and automatic descriptions and keywords for one of the videos used in the study. Several differences are apparent. First, the automatic descriptions are composed of extracted sentences rather than originally composed sentences that aim to summarize as in the case of manually-generated descriptions. Second, the video transcripts are quite different from technical or scholarly text in that visual or audio anaphora are used to provide context and motivation for the audio and visual channels rather than solely carrying the content. In this case, the automatically gen-erated keywords also contain more specific terminology, influenced by inverse term weighting, whereas the manually gen-erated keywords are more descriptive of the overall video content.

The manually and automatically generated keywords and descriptions were  X  X  X poken X  by a text-to-speech synthesizer make the spoken surrogates for the study. The text-to-speech synthesizer was used to ensure consistent pace and pronuncia-tion, and to obviate the need for manual recording. We anticipate that even better results could be obtained by having human-read spoken surrogates, but in most practical situations this would be too expensive. In this paper, we studied spoken descrip-tions and keywords instead of the textual descriptions and keywords, given the increasing feasibility of audio surrogates as ad-vances in broadband networks, audio processing, and small form factor devices. In particular, spoken surrogates will save a lot more screen real estate than their textual counterparts, and spoken surrogates are able to be used in mobile situations or with disabled users where reading the screen is not an option.

The spoken description and spoken keywords ran 14.8 s and 11.9 s on average, respectively. For the combined conditions, the fast forwards and spoken surrogates were played concurrently using the same media player, with the fast forward typ-ically finishing a few seconds before the audio stream X  X hat is, no attempt was made to synchronize the two streams except to have them play using the same media player. 3.4. Gisting tasks
Participants conducted all tasks based on experiencing these surrogates only; they did not view full videos. Six types of tasks were used in the study. Four of them were adopted from our previous studies (Marchionini, Wildemuth, &amp; Geisler, 2006; Wildemuth et al., 2003 ): a verbal gist writing task, a keywords identification task, a keyframes identification task, and a verbal gist identification task. The other two tasks were newly designed for this study: a visual excerpt identification task and an audio excerpt identification task. For all trials, the six tasks were carefully ordered to minimize potential knowl-edge gained from previous tasks. For example, the open-ended verbal gist writing task was completed first so that partici-pants would not gain extra information from the other five tasks. The verbal gist selection task was completed last because it includes the correct descriptions, which significantly deepens participants X  understanding of the video segments. For all tri-als, the tasks were given in the same order as described below.
 3.4.1. Task 1. Gist writing
In this open-ended question task, participants were asked to write a short summary of the video based on the surrogate they experienced. Note that the participant viewed only the surrogate and not the full video. To evaluate participants X  input for this task, two of the authors worked together to identify two main facets for each summary, and the summaries were scored by the two researchers independently on a four point scale: for each facet, 2 points were given if the facet was com-pletely addressed, and 1 point was given if the facet was partially addressed, and 0 if the facet was not addressed at all. The points for the two facets were added together to make the final score of each summary. Therefore,  X  X 0 X  means none of the two main facets were addressed at all;  X  X 1 X  means only one facet was partially addressed;  X  X 2 X  can either mean one facet com-pletely addressed or two facets both partially addressed;  X  X 3 X  means one facet completely addressed and the other partially addressed; and  X  X 4 X  means both facets were completely addressed. A correlation of 0.82 was found between the respective scores from the two coders, so the two sets of scores were averaged for each trial to yield reasonable and valid final scores in the 0 X 4 range for each summary. The mean of the scores for the four trials (excluding the training trial) were then computed for each surrogate condition. To compare the effects of using manually-generated text and using automatically generated text, the mean scores were computed separately for the two participant groups.

In this and the other five tasks, the participants were also asked to indicate their confidence in their answer for each task trial on a five point rating scale (i.e., 1  X  not confident, 5  X  very confident). As with the accuracy measures, the confidence ratings were averaged across four trials for comparison between surrogate conditions. The total time to complete each task trial was also recorded and averaged across the four trials. 3.4.2. Task 2. Keywords identification
In this multiple choice task, participants were asked to select keywords they believe to be appropriate for the video from a set of 10 words listed. Five of the keywords were chosen from the keywords for the video on the Open Video repository (i.e., were correct) and the remaining five were distractors (i.e., were wrong). Of the distractors, two keywords were selected from keywords for other videos in the same video collection (i.e., they had similar structures and were at a similar conceptual level, but were on different topics); two keywords were selected from keywords for videos in a different NASA video collec-tion (i.e., they also had similar structures and were at a similar conceptual level, but again were on different topics); and one keyword was selected from keywords for videos from a totally different video collection (i.e., not only the topics, but also the structures and conceptual levels of the videos were different from the target video) so that we could have near and far miss examples while including only manually generated keywords. Therefore, none of the distractors keywords should also apply to the target video being gisted. For each trial, the fraction of correctly identified keywords was measured. The scores (rang-ing from 0 to 1) across each set of four trials were averaged for comparison between different surrogate conditions, as were the confidence scores and task completion time for each trial. 3.4.3. Task 3. Keyframe identification
This is also a multiple choice task. Fig. 2 shows the online stimulus for the keyframe identification task. Participants were asked to select appropriate keyframes that they thought came from the video from a set of 10 candidate keyframes provided.
Similar to the keyword identification task, five of the key frames were selected from the keyframes for the video segment on the Open Video repository (i.e., were correct), and five keyframes were selected from keyframes for other videos in the same video collection, in the other NASA collections, and in totally different collections in the Open Video repository (i.e., were wrong). As with the selection of the distractors for the keywords identification task, none of the distractor keyframes should also apply to the target video being gisted.

For each trial, the percentage of correct keyframes was computed and the scores across each set of four trials were aver-aged. Likewise, participant X  X  confidence ratings and task completion times were computed for comparison between surro-gate conditions. 3.4.4. Task 4. Visual excerpt selection
Fig. 3 shows the online stimulus for the single choice visual excerpt identification task. Participants were asked to play four videos that were each 7 s clips from a full video segment but without an audio track and select the one visual excerpt that they thought came from the video segment that the surrogate represents. The correct visual excerpt was based on the 7-s excerpt of the video in the Open Video repository, but only the visual part was taken and the sound was not included. The three alternatives were visuals extracted from three other videos in the Open Video repository. Of those three distractors, one was selected from other videos in the same collection, one was selected from videos in similar collections, and one was from a totally different collection in Open Video. This task was scored as correct or incorrect, thus the score was either 0 or 1 for each trial. The mean of the four trials yielded a normalized score between 0 and 1. As with the other tasks, con-fidence and time to complete tasks were computed as well. 3.4.5. Task 5. Audio excerpt selection
This single choice task is the same as the visual excerpt identification task except that the participants were asked to play four audio excerpts and select one that they thought came from the video segment that the surrogate represents. The correct audio excerpt was based on the 7-s excerpt of the video in the Open Video repository, but only the audio part (without the visual) was extracted. Distractors were selected using the same strategy as was used for the visual excerpt selection task.
Similar to Task 4, this task was scored as correct or incorrect (scoring either 0 or 1) for each trial, and the average across the four trials was used to compare different surrogate conditions, along with confidence and task completion time. 3.4.6. Task 6. Verbal gist selection
In this single choice question, participants were asked to select the most appropriate textual summary for the video seg-ment from a set of four textual descriptions based on the surrogate they experienced. The  X  X orrect X  answer was the two most salient sentences taken from the description for the video available in the Open Video repository, and the three alternatives were taken from descriptions for other videos in the repository. Similar strategies for selecting distractor videos as for Task 4 and Task 5 were employed. This task was scored as correct or incorrect, thus the score is either 0 or 1 for each trial and the sum taken across the four trials and then divided by 4 to yield a normalized score between 0 and 1. Confidence and task completion time were also computed.

For each of the above six tasks, the accuracy of each trial and the participants X  confidence in their answers were recorded as participants worked. In addition, the time the participants spent experiencing the surrogates, the time they spent com-pleting each task, as well as the number of replays were also recorded. 3.5. Study protocol
An online protocol system for the study was developed using PHP/MySQL and JavaScript. The system administered the study and collected test data. It also managed the counterbalancing of the surrogate condition orderings. Note that the order-ing of the six tasks in each trial did not change across different surrogate conditions.

The 48 participants were scheduled over a 2 week period and were assigned into groups of 4 X 10 depending on participant availability. Participants were seated at identical alternating workstations in a computer laboratory so they could not see each others X  screens and were asked to wear headphones during the study. Each test session ran 1.5 h on average with a few participants taking up to 3 h.

Whenthestudystarted,theparticipantswerefirstgivenconsentformstoreadandsign,andwerethen briefedas agroupon the general overall purpose and procedure of the study. We made it clear to the participants that the purpose of our study was not to evaluate their abilities in completing the tasks, but to study the effectiveness of different surrogates. Thus, they were encouraged to work at their own pace. We also explained explicitly to the participants that actual video segments would not be played at any time during the study, so that they would have to formulate their answers solely by experiencing the surrogates.

The online protocol system first presented a pre-session questionnaire about the participants and their computer and vi-deo usage, and then assigned respective surrogate conditions in counterbalanced order for each participant. For each of the five conditions, participants experienced a surrogate and had the option to stop or replay the surrogate. When done, they performed the six tasks without going back to the surrogate. In each condition, they completed five trials, with the first trial treated as a training trial and excluded from the data analysis. When all five trials were completed, participants completed twelve 5-point Likert scales (e.g., Using this system helps me better estimate the gist of videos: 1  X  strongly disagree, 5  X  strongly agree) and eight 7-point semantic differential scales (e.g., Using the video surrogates is 1  X  not interesting, 7  X  inter-esting) related to usability, usefulness, enjoyment, and engagement (Davis, 1989; Ghani, Supnick, &amp; Rooney, 1991 ) before going on to the next surrogate condition.

After the participants finished all five surrogate conditions, they completed a final questionnaire that focused on compar-ing their experiences with the five different types of surrogates. Upon completion, participants were thanked and given $20 for their participation. 4. Results
Tables 1 X 6 show results for all six tasks for all surrogate-measure combination. Measures displayed in the tables are means and standard deviations on task completion accuracy, confidence, and task completion time. Task 7 shows results in four affective measures collected in the questionnaires after participants completed each surrogate condition. In all tables, asterisks in column headings denote statistical significance in main effects at the 0.01 probability level. 4.1. Automatic and manually-generated text
We first examine the results using the spoken descriptions by the manually-generated text group and the automatically generated text group. Automatic summarization is a difficult challenge (see Mani &amp; Maybury, 1999 for a review of classical work and the DUC 3 results for more recent advances) and as expected, manually-generated descriptions were uniformly supe-rior to automatically-generated descriptions. For all six task sets that used spoken stimuli, the manual group performed statis-maries. The manually generated summaries (or descriptions) were often crafted to capture specific instructional content. Video transcripts may require different summarization approaches to account for the extra text that necessarily is devoted to describ-ing what is depicted (e.g., people, places, actions) and the text that may not be included because objects and actions are visually or aurally present in the video or appear in the video as superimpositions. Although the expectations that manual descriptions would be better than automatically generated ones were born out, the performance with automatically-generated descriptions were better than chance for all of the tasks except gist writing. In the gist selection task, manual descriptions led to 100% accu-racy in selecting the correct summary and automatic descriptions led to 82% accuracy, whereas in the more difficult gist writing task, manual descriptions led to more than 80% accuracy (3.3 out of 4) whereas automatic descriptions led to only 25% (.98 out of 4) accuracy. Because assessing gist in actual video retrieval system will benefit from much more contextual information (e.g., the searcher will have specified a query) and be performing more of a recognition task (akin to the gist selection task), we should expect that performance in actual retrieval situations with automatically-generated descriptions will be better than not having those descriptions.

We expected that there would be less disparity between the automatically and manually generated keywords, because we inferred that keywords would not require the coherence needed for descriptions. Although participants were generally tion) were statistically reliably in favor of the manual keywords. As mentioned earlier, confidence levels were also statisti-cally reliably higher for the manual keywords in all by one task (Task 5: audio excerpt selection). These differences are reinforced and contrasted by the fact that none of the 12 T -tests for accuracy and confidence resulted in statistically reliable differences across the two groups for the fast forward surrogate conditions tasks. Thus, the differences between the perfor-mance and confidence of the manual and automatic groups are due to the differences in two types of spoken surrogates rather than the individual participants in the two groups. Although the manual keywords were better overall than the auto-matically generated keywords, the differences were not as large as those between manual and automatic descriptions. For the gist writing task, performance with manual keywords was about 60% (2.34 out of 4) while it was about 35% (1.34 out of 4) and for the gist selection task, the respective performance means were 93% and 85%, respectively.

These results and findings have implications for digital video retrieval system user interface design. First, the manually generated spoken descriptions and keywords are significantly reliably superior to their automatically generated counter-parts when used for video retrieval and sense-making purposes. The manually generated spoken descriptions and keywords should be used if available or affordable. In situations where they are not available and too expensive to create, automatically generated metadata can be inexpensively created and used, to support adequate gisting: they may not be perfect, but are much better than nothing or chance gisting. 4.2. Data analysis for research questions 2 X 4
Given these differences, the analyses for each of the other research questions were conducted separately for each of the two groups. The means and standard deviations for accuracy, confidence, and task completion time for all conditions are pre-sented in the discussions below for the other research questions. Repeated measures ANOVAs were conducted for accuracy, confidence, and surrogate consumption time for each of the six different tasks. Means and standard deviations for task com-pletion time showed that no main effects were found for any condition or across the two groups so the focus is on accuracy and confidence. Pairwise contrasts (LSD) were conducted when main effects were statistically reliable. Tables 1 X 6 show the means and standard deviations for each surrogate-measure combination. Statistically reliable main effects are denoted with asterisks in the column headings and pairwise contrasts are discussed in the text. Note that separate tables are provided for the manually and automatically generated text conditions. Repeated measures ANOVAs were also conducted across the five surrogate conditions for each of four satisfaction constructs: usefulness, usability, engagement, and enjoyment. Table 7 a pre-sents means and standard deviations. Main effects were found for each of four constructs for the manually-generated text conditions and for all but engagement in the automatically generated text conditions.

These results were strongly paralleled in the questionnaire data for each of the usefulness, usability, enjoyment, and engagement scales ( Table 7 ). ANOVAs and pairwise contrasts for the manually generated spoken text group for each of the satisfaction indicators showed statistically reliable differences with spoken descriptions having the highest mean for useful-ness, usability and enjoyment (with the exception of engagement where fast forward combined with spoken keywords was rated the highest), and spoken keywords having the lowest means for all four scales. Similarly, participants in the manually-generated text group strongly preferred the spoken descriptions in their debriefing interview comments. One participant in the manual group said:  X  X  X  did not like the keywords or just the visual. The audio description gave a clear and concise descrip-tion and the visual, when mixed with the audio, provided a peek at what you will be watching. X  The automatically generated text group, however, rated fast forwards highest and spoken keywords lowest on all four scales, with all these differences sta-tistically reliable. Note that people did reasonably well with automatic generated keywords, but did not like them. 4.2.1. Manually generated spoken descriptions and keywords
Having learned that manually generated spoken surrogates were superior to their automatically generated counterparts, in this section, we study the differences between manually generated spoken descriptions and manually generated spoken keywords.

For the manually-generated text group participants, the gist writing task (see Table 1 a) showed statically reliable differ-ences among the five surrogates ( F = 8.36, p &lt; .001). Pairwise contrasts showed that spoken descriptions yielded statistically reliably better ( p &lt; .05) performance than spoken keywords, and that combining spoken descriptions with fast forwards was likewise better than combining spoken keywords with fast forwards. The ANOVA for confidence in responses ( F = 10.72, p &lt; .001) also showed statistically reliable differences among the surrogates and pairwise comparisons directly paralleled the accuracy results. No differences were found on task completion time across the surrogates ( F = 0.33, p = .86). For the group of 24 participants using the manually-generated text, participants X  ratings on the four affective scales demonstrated strong preferences for manually generated spoken descriptions, which were rated the highest among the five surrogates for both usefulness and usability (see Table 7 a). Manually generated keywords were rated the lowest for usefulness and merely above fast forwards combined with manually generated spoken keywords for usability. Pairwise contrasts for manually gen-erated spoken descriptions and keywords were statistically reliably different on both usefulness and usability measures ( p = .000 in both cases). Spoken descriptions were rated higher than spoken keywords on engagement and enjoyment (although not higher than combined descriptions with fast forwards). Pairwise contrasts for descriptions and keywords were statistically reliably different on both engagement and enjoyment measures as well ( p = .001 and .002, respectively).
These results demonstrate that writing a brief summary based on hearing a description is much better than writing a summary based on hearing a set of keywords, if the description is good (i.e., provides highly salient concepts). This is an important qualification as illustrated by the results from the group that used the automatically-generated descriptions and keywords (see Table 1 b). The ANOVAs conducted for the automatically generated conditions showed statistically reliable differences across the five surrogate conditions ( F = 17.1, p &lt; .001). The pairwise comparisons, however, showed quite differ-ent results from the manually generated conditions. The fast forward surrogate outperformed all the others while the auto-matically generated keywords in fact outperformed the automatically-generated descriptions. Participants rated fast forwards alone the most useful and usable, and the pairwise differences between descriptions and keywords were not sta-tistically reliably different. Participants found the automatic descriptions more enjoyable than the automatic keywords but there were no statistically significant differences for engagement. Clearly, for good gisting performance we need to have good descriptions.

For the keyword selection task (see Table 2 a), no main effects were found for accuracy in the group using the manually-vored spoken descriptions over spoken keywords, and favored fast forward combined with spoken descriptions over fast for-ward combined with spoken keywords. No differences in task completion time were found.

For the keyword selection task done by the group using automatically generated text, the results (see Table 2 b) directly wise comparisons showing that fast forwards alone and combined with either spoken descriptions or keywords outper-formed spoken descriptions or keywords alone for both accuracy and confidence ratings. As with the group using manually-generated descriptions and keywords, no task completion time differences were found.

For the keyframe selection task (see Table 3 a), the results closely parallel the results for the keywords selection task for accuracy, confidence, and task completion time in each of the two group conditions. The cases for visual and audio excerpts selections (see Tables 4 and 5 ) also closely mirror each other with no statistically reliable main effects for accuracy in the manually generated spoken text group but statistically reliable main effects in the automatically generated spoken text group. Pairwise comparisons favored the fast forward and combined surrogates for the automatic group. No differences in task completion time were found in any of the five conditions.

Results for the gist selection task (see Table 6 ) mirror those for the gist writing task with statistically reliable main effects for accuracy and confidence in both the manually and automatically generated groups. Pairwise comparisons again demon-strate manually generated spoken descriptions are superior to spoken keywords, and fast forward and combined surrogates are superior to spoken surrogates that use automatically-generated descriptions or keywords. No task completion time dif-ferences were found. Thus, the accuracy and confidence results in all the tasks demonstrate that well-constructed spoken descriptions are powerful surrogates for video gisting.

From a design perspective these results suggest that if good descriptions cannot be automatically generated, it might be prudent to use automatically generated keywords instead. If an application expects to be used on a large screen display, then providing both keywords and descriptions makes sense, because the user can easily decide which to use. 4.2.2. Fast forward and manually generated spoken surrogates Five years of TREC Video results readily demonstrate the importance of linguistic data (in text format) for retrieval (e.g.,
Christel, 2007; Smeaton et al., 2006 ). Nonetheless , several of the studies noted above and in our previous work (Tse et al., 1998; Wildemuth et al., 2003 ) have demonstrated that people like to have visual surrogates regardless of their performance effects.

The fast forward surrogates used in this study were surprisingly effective. They were found to be less effective than man-ually generated spoken descriptions and manually generated keywords on some word-based tasks (gist writing task,, and verbal gist selection task), but were found to be better than automatically generated spoken descriptions and keywords on all 6 tasks. Thus, the right words for describing the videos are worth more than a lot of pictures for making quick sense of a video surrogate; however, if the description and keywords surrogates have to be generated automatically without deliv-ering good salience, a fast forward is more useful.
The fast forwards used in this experiment were very rapid, and several participants noted that they closed their eyes or looked away while listening to the audio and then played the combined surrogates again to watch the fast forward, some-times more than once. Combining the rapid fast forwards with the spoken audio seemed to have distracted the participants from concentrating on either of the two surrogate channels. However, several participants pointed out the usefulness of the visual information despite the speed. For example, one noted the distinction between the activity and the concepts in the fast forward surrogates:  X  X  X  liked the visual because it showed images of the video to make it clear what was going on, while I liked the audio because it gave insight into what was really being discussed in the video. X  Another participant noted the po-tential for the fast forward to act as an advance organizer:  X  X  X he visual features prepare the viewer for the quality of the video and what the presentation of the content will be like, such as whether there are computer modules or interviews. X  Thus, for some people, fast forwards actually added some visceral fidelity that helped them make sense of the videos.

The surrogate consumption time and the number of replays of the surrogates also reinforced the importance of the right words. On average, a single play of the fast forward took a shorter time than a single play of the spoken description or the spoken keywords. In the combined conditions, because the fast forwards and spoken surrogates were played concurrently with the fast forwards usually ending a few seconds before the spoken surrogates, a single play of the combined surrogate took the same amount of time as a single play of the spoken surrogate used. In the study, the participants were allowed to replay or interrupt the surrogates during the consumption period before the gisting tasks began. We recorded the number of replays and stops of each surrogate condition for the participants.

Results show that for the manually-generated text group, participants spent statistically reliably more time consuming the fast forward surrogates than consuming the spoken surrogates ( F = 6.36, p &lt; .001), given that the fast forwards were rapid but were replayed more times than the spoken surrogates. Similar results were also found for the automatically generated text group ( F = 5.40, p &lt; .001).

A key conclusion from this study is that manually generated spoken descriptions (and keywords) are very effective sur-rogates for making sense of digital videos, and have an even higher compaction rate than an already very rapid fast forward (as used in this study). The fast forwards, despite being very fast and having to be replayed more times than the spoken sur-rogates, provided some visceral fidelity to the participants that helped them make sense of the videos. From a design per-spective, these results suggest that both the spoken surrogates and fast forwards can be useful for video retrieval and browsing: the spoken surrogates should be provided to the users if they are manually generated with the right words deliv-ering good salience; otherwise, a fast forward, which can be automatically generated and less expensive, is more useful than automatically generated spoken text of poor summarizing quality. 4.2.3. Combining surrogates There is ample evidence that coordinated multimedia channels are desirable (e.g., Mayer &amp; Moreno, 1998; Tindall-Ford,
Chandler, &amp; Sweller, 1997 ) and the assumption is that coordinated surrogates will also be better than non-coordinated. Re-sults from the Informedia project (Christel, Smith, Taylor, &amp; Winkler, 1998; Wactlar et al., 1999 ), the CueVideo Project (Pon-celeon et al., 1999; Srinivasan, Petkovic, &amp; Ponceleon, 1999 ) and our own work (Marchionini et al., 2006; Wildemuth et al., 2003; Song &amp; Marchionini, 2007 ) suggest that combining surrogates of different modalities could be effective.
Song and Marchionini (2007) showed that spoken surrogates alone were almost as good as spoken surrogates com-bined with storyboards (i.e., a temporally static tabular display of a number of keyframes extracted from the video), and suggested that the temporal coordination between the visual and audio channels which is desirable for the primary information objects may not be necessary for condensed information such as video surrogates. In that study, a storyboard was augmented with a spoken description, which made it obvious to the users that the audio and visual channels were not synchronized. We wondered whether similar effects would be found for fast forwards that include an inherent tem-poral component.

In this study, we combined the fast forward with the spoken surrogates by playing them concurrently in the same media player, with the fast forward typically finishing slightly before the audio stream. No attempt was made to synchronize the visual and audio channels. Though it was not obvious that the visual and audio channels were not temporally coordinated at the participants X  first glance, the participants learned this quickly upon finishing the first trial with the training video. Thus, the participants were aware that the audio and visual channels were not synchronized in the combined conditions.
We asked the participants to rate how different they found the surrogates from one another. Fifteen out of the 48 partic-ipants rated the surrogates as completely different , 29 participants rated them somewhat different , and only 4 participants rated the surrogates as not different at all . Thus, almost all participants found the surrogates to be at least somewhat different no matter the descriptions or keywords were generated manually or automatically. In this section, we examine the effects of combining fast forwards and the spoken surrogates, and the results on accuracy, confidence, affective ratings, and partici-pants X  comments are discussed as follows. 4.2.3.1. Accuracy and confidence ratings. When the descriptions and keywords were generated by humans, participants per-formed significantly reliably better (i.e., with higher accuracy) using the fast forward combined with spoken descriptions than using fast forward combined with spoken keywords, for 3 of the 6 tasks (i.e., gist writing task, keyword and keyframe identification tasks). Similar results were found in the confidence ratings for all the six tasks, with the differences signifi-cantly reliably in favor of fast forwards combined with the spoken descriptions. However, combining fast forwards with either type of spoken surrogate was not statistically reliably better than any of the individual spoken surrogates in terms of confidence ratings.

With the automatically generated text group, combining fast forwards with spoken surrogates statistically reliably out-performed both of the spoken surrogates alone on accuracy and confidence, but pairwise comparisons showed no statisti-cally reliable differences between the fast forward alone and the combined surrogates. Therefore, augmenting the fast forwards with the spoken surrogates did not add much additional value when the spoken surrogates were of poor summa-rizing quality.

In summary, our study reinforced the findings in Song and Marchionini (2007) that most people were able to use the com-bined surrogates to quickly and accurately make sense of the video and also were positive in their attitudes toward these combined surrogates, yet our study strongly emphasized the importance of the summarizing quality of the spoken surro-gates. Moreover, when the spoken surrogates were well generated, the spoken surrogates were almost as good as spoken surrogates combined with the fast forwards, and were much better than fast forwards alone. When the spoken surrogates were not well generated, however, the spoken surrogates alone were of the least value for making sense of the videos, and even combining them with the fast forwards were no much better than the fast forwards alone. 4.2.3.2. Affective measurements. Participant ratings for the five surrogates on the four affective scales including usefulness, usability, engagement and enjoyment, directly parallel the accuracy results. With the manually-generated text group, their affective ratings demonstrated their strong preferences for spoken descriptions, which were rated the highest among the five surrogates for both usefulness and usability. Manually generated spoken keywords were rated the lowest for usefulness, but were rated slightly above the fast forwards combined with spoken keywords for usability. In addition, combining fast for-wards with either type of spoken surrogate was not significantly reliably more useful or usable than any of the individual spoken surrogates. In terms of engagement and enjoyment ratings, however, fast forwards combined with spoken descrip-tions were rated higher than spoken descriptions, which were rated higher than spoken keywords; and the differences were significantly reliable. In sum, combining fast forwards with spoken surrogates leads to higher engagement and enjoyment levels on average, but the combined surrogates were not uniformly more useful or usable than the individual surrogates.
Thus, spoken descriptions, if generated with high summarizing quality, were the most effective and preferred surrogate for digital videos. Combining high-quality spoken descriptions with fast forwards did add some motivational value as well as allow some useful inferences about specific attributes like age appropriateness and visual style; however, the combination also confused some people or caused them to consume the two channels separately. Thus, spoken descriptions of good sum-marizing quality can stand alone for many gisting purposes.

Quite distinct results were found in the affective responses for the automatically generated text group (see Table 7 b): the fast forward alone was rated the highest on all 4 affective measures (including usefulness, usability, engagement, and enjoy-ment), and fast forward combined with spoken descriptions was rated higher than fast forward with spoken keywords, which in turn was rated higher than spoken descriptions alone, while spoken keywords alone was given the lowest affective measures. The differences were significantly reliable in all 4 affective measures except in the engagement ratings. In a word, when the spoken descriptions and spoken keywords were generated with poor summarizing quality, fast forwards alone lead to the highest affective ratings. This result is reinforced by our analysis of the participants X  comments about combined surrogates. 4.2.3.3. Open-ended user comments. 4.2.3.3.1. What the audio features adds. We also asked participants for their comments about combined surrogates. We first asked  X  X  X hese surrogates used visual and audio features. What do you think the audio features add to help understanding of the content? X  Many participants (especially those using manually-generated text) commented that (1) the audio helped to clarify the video. They stated that the audio  X  X  X elped sometimes by giving a simple description of the video X , and  X  X  X ade it clearer what the video was about X . (2) Several participants noted that the audio was less ambiguous than the video, leaving  X  X  X o room for interpretation X . (3) Several participants commented that the audio provided key words and phrases. They stated that the audio  X  X  X elped me ... take in and internalize specific words or phrases X ,  X  X  X re useful when you can pick up on keywords X , and  X  X  X raw attention to the keywords X . (4) Some participants expressed that the audio gave them an overall understanding of the video. They stated that the audio  X  X  X rticulates features you may not be familiar with and helps to under-stand them X ,  X  X  X ave a synopsis X , and  X  X  X ave an overview. X 
On the contrary, many participants using the automatically generated text noted that the audio features were  X  X  X ery hard to understand X , and were  X  X  X ind of distracting X . One participant complained that the audio actually confused him more  X  X  X e-cause the audio wasn X  X  really commenting on what was going on in the video. X  One participant felt the audio features did not add much to help in understanding the video, and  X  X  X ould have been better selected, particularly when used in conjunction with the fast forward X , while another participant commented that  X  X  X he pictures help tell a story. Short phrases are not en-ough (for me) to create a story X .

More unexpectedly, some participants pointed out the problems with multimodal presentation. They stated that it was  X  X  X ard to watch the video and listen to the audio at the same time X . Some participants had trouble with the differing pace of the audio and video when presented together. One participant wrote that  X  X  X hen taken in conjunction with the video fea-tures, the audio features gave me a headache and really held me back from picking up on visual cues, not because the images were different than what was being talked about, but mostly because the speeds were so different X . 4.2.3.3.2. What the visual features add. We also asked the participants:  X  X  X hat do you think the visual features add? X  They identified various functions of the visual features. (1)  X  X  X isual allows you to focus X  and  X  X  X ive(s) details that the audio does not provide X . One participant stated that the visual features provided  X  X  X emembrance X  and the images  X  X  X tick out in your mind X .
The video features provided  X  X  X ocus X  and  X  X  X orce the attention X  of the viewer. (2) Two participants noted that they were  X  X  X ntertaining X . (3) Several thought they helped the viewer decide whether to watch the full video. They said it helps them  X  X  X ecide if [the video] looks interesting or not. X  Several thought the video fast forward  X  X  X rovides the overall gist of the topic X , and pointed out that the videos are  X  X  X angible X , are  X  X  X ictures to be seen X  and give what the scene is  X  X  X ctually like X  allowing them to  X  X  X ee what is going on X . (4) Others noted that the video surrogate  X  X  X elped prepare the viewer for the quality of the video and what the presentation of the content will be like, such as whether there are computer models or interviews X , and determine certain types of content very quickly, such as  X  X  X he video X  X  target audience X ,  X  X  X raphs, graphics, and general activities X ,  X  X  X omputer models or interviews X  or  X  X  X iagrams and visual explanations. X  (5) The visual features were a good complement to the audio features. Some participants stated that  X  X  X he visual features helped to see what the words were referring to and helped to connect the two together X .

Extremely different comments were given by the group using manually-generated text versus the group using automat-ically generated text. Many participants in the manually generated group said the video helped  X  X  X ery little X , and the audio was  X  X  X ore concise and easy to understand X  while the video is  X  X  X ot as helpful as ... the audio. X  One said  X  X  X apid-fire visuals made me a bit dizzy! X  Others said it was  X  X  X oving too quickly X  or  X  X  X oo fast to get a good enough understanding X . Some par-ticipants pointed out that the fast speed of the visual features made the combined surrogates difficult to follow. One said  X  X  X hey were too speeded-up to be very useful ... and ... the quick succession of images was very distracting and made it hard to concentrate on the audio X .

For the group using automatically generated text, however, the majority of the participants gave very positive comments on the visual features (probably due to their relatively poor experience with the audio features). One participant said that the visual features added  X  X  X  lot X . The participant noted  X  X  X ithout the visual features not much would be understood because to me, when I look at things I learn a lot more X . Another said,  X  X  X he visual features allow a connection with my brain to images X .
Although the fast forwards went very quickly, some participants figured out that they  X  X  X ould use the control bar to slow down the fast-forward X . One stated that  X  X  X he visual features helped the most because you could slow down if you needed and stop on frames to understand more of what was going on in the video X .

Also, it is worth noting that there is a learning curve for using the fast forwards. One participant stated that fast forwards are  X  X  X isconcerting at first but very helpful once I was accustomed to using them X .

When we asked the participants what they you liked and disliked, they commented that the fast forwards were  X  X  X nter-taining X  but  X  X  X oo fast X . One said  X  X  X  did not like the keywords or just the visual. The audio description gave a clear and concise description, and the visual, when mixed with the audio, provided a peak at what you will be watching. X  Again, their attitudes differed between the two user groups, such that the manually-generated text group tended to like the spoken descriptions, while the automatically generated text groups preferred the fast forwards alone. 4.2.3.3.3. What the combined surrogates add. Another question we asked was  X  X  X s there anything that having the audio and visual together adds that is beyond simply having the audio and visual presented separately? X 
A majority of the participants using the manually-generated text found having the audio and visual together  X  X  X ives an overall better understanding of the surrogate X , gives  X  X  X ore clarification and a clear presentation of what to expect X , and made it  X  X  X asier to understand X . One said  X  X  X he two systems together make the video much more clear than either by itself X .
Another said  X  X  X hen paired together, the audio parts seem to provide a frame of reference for what I am watching, making the video more valuable than it had been, and making the audio synopsis much less boring X .

Some participant also confirmed the value of having the audio and visual together, but suggested that the audio alone contributed the most to gisting. He said  X  X  X  thought the audio and visual together was more helpful than just the visual, but I thought just the audio was the most helpful. Having them together did help to connect the two, though. X  Thus, for many of the participants, the fast forwards were useful supplements to the audio.

A small number of participants did not find combining the visual and audio to be very helpful and commented  X  X  X t X  X  con-fusing X  and  X  X  X istracting X , and there was  X  X  X ot much help having them together X . For instance, one participant said  X  X  X he visual goes too fast, and I X  X  rather just focus on listening and then repeating it back to myself. X  Another stated  X  X  X hen combined, I found it very difficult to focus. X 
Some participants noted that the spoken keywords were less useful than the spoken descriptions when used together with the visual. One said  X  X  X aving the video and audio together helps but only if the audio is a description and not keywords.
Having just the keywords is very confusing and didn X  X  help with anything. X  Another participant stated that the visual and audio did not correspond to each other. He said  X  X  X he fast-paced visual storyboard can make it confusing because the viewer is unsure of what they X  X e looking at if it doesn X  X  correspond to what they X  X e hearing. X  This synchronization issue will be dis-cussed in more details in the next section.

Not surprisingly, the participants using the automatically generated text gave relatively more negative comments on the audio surrogates, but 10 out of the 24 participants in this group also found combining audio and visual to be very  X  X  X istract-ing X  and  X  X  X ess helpful X ,  X  X  X ecause the audio didn X  X  match what I was seeing at any particular moment X , and provided very negative comments on the combined surrogates. One participant stated  X  X  X t irritated my senses. It added nothing, and de-tracted a lot X . Another said  X  X  X  really hated having to split my attention between audio and visual X . Overall, the participants using the manually-generated descriptions and keywords liked the idea of combining the fast forward and spoken surrogates much better than the participants using the automatically-generated descriptions and keywords. 4.2.3.3.4. Synchronization. We were interested to find out whether the coordination which is necessary and desirable for the primary information objects was also necessary and desirable for combining fast forwards and spoken surrogates. We asked  X  X  X n the combined audio and visual condition, at any time did you think that the audio and visual channels were synchronized? X  Most participants were able to tell that the visual and the audio in this study were not synchronized, though one participant thought they were synchronized  X  X  X aybe initially X , but learned that they were not  X  X  X eally X  synchronized very quickly.

We also asked them whether they thought the spoken keywords or spoken descriptions should be synchronized with the visual information or sampled separately from the video and why. The majority of the participants thought they should be synchronized. They felt they could remember the things better when they were synchronized. They said  X  X  X hey should be syn-chronized because that maximizes the information presented X , and  X  X  X t X  X  very difficult to pay attention to both audio and vi-deo when the content they present is very different X .

Some participants said  X  X  X ynchronizing would be great if done well, X  because  X  X  X aving to make a story out of pictures and words that are not already connected is rather difficult. X  Some pointed out that  X  X  X he spoken keywords need to be synchro-nized with the visual information because otherwise the audio is hard to contextualize X , while  X  X  X t is less necessary to syn-chronize the spoken descriptions, since they can provide the gist of the video X  X  content on their own X . Another agreed that  X  X  X he keywords should be synchronized [with the visual] X , yet  X  X  X f the descriptions were synchronized and ended when the visuals did, then that system might be even better. X 
Therefore, synchronization between surrogates is preferred, but not always a must. When combining the spoken key-words and the fast forwards, the system works better if they are synchronized. When combining the spoken descriptions and the fast forwards, the synchronization between the two is great, but may not be necessary.

Based on all these findings, when the spoken surrogates were well generated, combing fast forwards with spoken surro-gates did not lead to better performance than spoken surrogates alone, and was less preferred by the participants than spo-ken surrogates alone in terms of affective measures. Given that the spoken surrogates alone were almost as good as spoken surrogates combined with the fast forwards, we recommend adding spoken surrogates of good summarizing quality to the video retrieval and browsing systems. However, fast forwards also provided some visceral fidelity that helped participants make sense of the videos, and they are much easier and cheaper to generate. Thus, when good spoken surrogates are not available, the fast forwards can be easily generated and should be used. 5. Discussion
Four important classes of factors interact to determine the user gisting experience: user characteristics, content charac-teristics, task requirements, and the video system. This study is limited by the variables we chose to control or manipulate for each of these factors. We aimed to control user characteristics through a laboratory study with 48 participants and sta-tistical analyses that account for individual differences. University students and staff on average had higher computer liter-acy and proficiency and were more knowledgeable about the science topics than the general public, thus the results serve as an upper bound on the performance. However, because our study aimed to investigate the effectiveness of different surro-gate conditions, higher performance on each surrogate condition would not affect the differences among the conditions. Sub-sequent studies could include more diverse sets of participants.

Content characteristics such as genre, conceptual complexity, and media effects all influence how people view and inter-pret video. Likewise, video qualities (e.g., amount of motion, color and audio feature quality) influence what kind of auto-matic summarization techniques can be leveraged. In this study, highly produced (professional production) documentary content aimed at middle school students was used. Each video addressed well-defined scientific topics, and was about 30 min long, and used fast-action, highly motivating graphics and sound effects to motivate the audience. Thus, surrogate creation techniques could leverage the instructional themes but also had to compress quite long runs into very short surro-gates. From a visual perspective, such conditions might benefit from fast forwards that sample from the entire run length, whereas talking heads lectures, news or other shorter content may be better summarized with storyboards. Likewise, a few keywords or sentences will have to be highly general to summarize 30 m of video. It is likely that the automatic keyword and description surrogates used in this study were disadvantaged by these long videos.

Determining the gist of a video in a result list is but one subtask in the overall video retrieval process and this study fo-cused specifically on this subtask without considering the context that users would have in an actual retrieval setting. This is true for all laboratory studies. Naturalistic analogs that consider the overall retrieval process in field settings will comple-ment these results. Furthermore, we used six kinds of experimental tasks to estimate actual gisting behavior. Although we have used these kinds of identification and inferential tasks in several other studies and argue that they are valid stim-ulants for gisting behavior, we acknowledge that they do not fully address all aspects of how people make sense of video summaries.

In this study, we tried to control the user, content, and task factors and manipulated the system surrogate factors, thus more attention is given to the nature of the surrogates. Most research on summarizing digital videos focuses on processing the image content. A few systems have incorporated some type of audio surrogate. For example, the CueVideo system had a compressed playback mode and Sundaram, Lexing, and Chang (2002) reported on a method that maintains the utility and coherence of an audio skim. However, neither of these systems investigated the effectiveness of audio skims or whether audio and video surrogates need be synchronized. The Informedia project found that coordination is important for their skims ( Christel et al., 1998 ), however, there are challenges in aligning openings and closings of coordinated skims and the coordination tends to increase average run times to accommodate these alignments. Skims are meant to mirror as many characteristics of video viewing as possible so that users can use familiar viewing habits to determine gist and thus it makes good sense to synchronize the audio and visual channels just as in full video. We focused on how much two specific kinds of audio alone meet the requirements of gisting and the effects of combining these audio surrogates independently (not aligned) with a visual surrogate. Our work is the first study to investigate and compare different types of audio surrogates for video. This study suggests that the well generated audio only surrogates are almost as effective as audio and visual com-bined surrogates and avoid the challenges associated with synchronizing the two media streams and thus have the potential to be used alone.

We also found significant differences in accuracy between automatic and manually-generated descriptions and keywords across a wide range of gisting tasks. Participants in our study had a positive affective response to manually-generated text surrogates. Why would this be? One explanation is that our video transcripts did not have the kinds of text structure and cues needed by our automatic summarization methods. Another explanation is that manually-generated descriptions some-times have specific elaborated content not taken directly from the video transcript, while automatic methods were indepen-dent of genre and were limited to extracted sentences from the video transcript.
 There has been considerable work on techniques for generating text summaries that do not rely on sentence extraction.
For example, Witbrock and Mittal (1999) describe a system that creates headline summaries from news stories by generating a statistical model of the relationship between document text units and summary text units. The system generated summa-ries between 4 and 10 words and achieved a 0.91 overlap with the top-ranked summary sentence of the story. However, the authors did not judge intelligibility or other subjective factors that are important to human readers. Also, a single sentence summary would be likely to be insufficient in coverage for the video transcript of a video several minutes long.
With the increased focus on video summarization research, we expect the automatically-generated descriptions to be-come more salient and useful in the future. When that day comes, the automatically generated spoken descriptions may sup-port video gisting as well as manually-generated descriptions do, and become an effective and affordable audio surrogate for video retrieval systems. 6. Conclusion
The usability of digital video libraries and video retrieval and browsing systems may be improved by careful design of video surrogates for quickly deriving gist and relevance of the available content. We have shown that spoken descriptions, even when created by a text-to-speech synthesizer, provide an effective surrogate for video gisting tasks. High summarizing quality spoken descriptions are more effective than spoken keywords and participants have higher confidence in their task responses when given these spoken descriptions as video surrogates. We recommend the use of spoken descriptions of good summarizing quality as video summaries, especially in video retrieval and browsing systems with a limited display area such as mobile applications. When high quality descriptions are not available, system designers should consider using fast for-wards or automatically generated keywords instead.

Techniques for automatically generating effective descriptions and additional investigations of the usability and satisfac-tion of unsynchronized media in surrogates deserve future attention. The gisting tasks used in this study offer new ap-proaches to evaluating retrieval techniques and may be useful to other researchers. Additionally, it may be useful in future work to consider possible differences between the style of video transcript word flow and written text when tuning summarization techniques.
 Acknowledgments
This work was supported by NSF Grant IIS 0455970 and an IBM Faculty Research Award. The study participants are grate-fully acknowledged for their diligence.
 References
