 Microsoft Research, 1 Microsoft Way, Redmond, WA 98052 USA The Hebrew University, Jerusalem 91904, Israel Supervised machine learning techniques often play a cen-tral role in solving complex real-world classification prob -lems. First, we collect a training set of labeled examples and present this set to a machine learning algorithm. Then, the learning algorithm constructs a classifier, which can be put to use as a component in a working system. The pro-cess of collecting the training set and constructing the cla s-sifier is called the training phase , whereas everything that occurs after the hypothesis has been determined is called the classification phase . In many cases, the training phase can be performed under sterile and controlled conditions, and care can be taken to collect a high quality training set. In contrast, the classification phase often takes place in th e noisy and uncertain conditions of the real world, and some of the features that were available during the training phas e may be missing or corrupted. In this paper, we explore the possibility of anticipating and preparing for this type of classification-time noise.
 The problem of corrupted and missing features occurs in a variety of different classification settings. For example , say that our goal is to learn an automatic medical diagno-sis system. Each instance represents a patient, each featur e contains the result of a medical test performed on that pa-tient, and the purpose of the system is to detect a certain disease. When constructing the training set, we go to the trouble of carefully performing every possible test on each patient. However, when the learned classifier is eventu-ally deployed as part of a diagnosis system, and applied to new patients, it is highly unlikely that all of the test re-sults will be available. Technical difficulties may prevent certain tests from being performed. Different patients may have different insurance policies, each covering a differe nt set of tests. A patient X  X  blood sample may become con-taminated, replacing the features that correspond to blood tests with random noise, while having no effect on other features. We would still like our diagnosis system to make accurate predictions. Alternatively, our goal may be to tra in a fingerprint recognition system that controls the lock on a door. After a few days of flawless operation, a user with greasy fingers comes along and leaves an oily smudge on the fingerprint scanner panel. From then on, all of the fea-tures measured from the area under the smudge are either distorted or cannot be extracted altogether. Ideally, the fi n-gerprint recognition system should continue operating. We take a worst-case approach to our problem, and assume that the set of affected features is chosen by an adversary individually per instance. More specifically, we assume that each feature is assigned an a-priori importance value and the adversary may remove or corrupt any feature sub-set whose total value is upper-bounded by a predefined pa-rameter. In many natural settings, missing and damaged features are not actually chosen adversarially, but we find i t beneficial to have our algorithm as robust as possible. We present two different learning algorithms for our prob-lem, each with pros and cons. The first approach formu-lates the learning problem as a linear program (LP), in a way that closely resembles the quadratic programming for-mulation of the Support Vector Machine (Vapnik, 1998). However, the number of constraints in this LP grows ex-ponentially with the number of features. Using tricks from convex analysis, we derive a related polynomial-size LP, and give conditions under which it is an exact reformulation of the original exponential-size LP. When these conditions do not hold, the polynomial-size LP still approximates the exponential-size LP, and we prove an upper bound on the approximation difference. Despite the fact that the distri bu-tion of training examples is different from the distributio n of examples observed during the classification phase, we prove a statistical generalization bound for this approach . Letting m denote the size of our training set and n number of features, our polynomial LP formulation uses O ( mn ) variables and O ( mn ) sparse constraints. Depend-ing on the dataset, this can still be rather large for off-the -shelf LP solvers. We see this as a shortcoming of our first approach, which brings us to our second algorithmic ap-proach. We define an online learning problem, which is closely related to the original statistical learning probl em. We devise a modified version of the Perceptron algorithm (Rosenblatt, 1958) for this online problem, and convert thi s Perceptron into a statistical learning algorithm using an online-to-batch conversion technique (Cesa-Bianchi et al ., 2004). This approach benefits from the computational ef-ficiency of the online Perceptron, and from the generaliza-tion properties and theoretical guarantees provided by the online-to-batch technique. Experimentally, we observe th at the efficiency of our second approach seems to come at the price of accuracy.
 Choosing an adequate regularization scheme is one of the keys to solving this problem successfully. Many existing machine learning algorithms, such as the Support Vector Machine, use L eralization. When L algorithm may put a large weight on one feature and com-pensate by putting a small weight on another feature. This promotes classifiers that focus their weight on the features that contribute the most. For example, in the degenerate case where one of the features actually equals the label, an L weight on that one feature. Some algorithms use L larization to further promote sparse solutions. In the con-text of our work, sparsity actually makes a classifier more susceptible to adversarial feature-corrupting noise. Her e we prefer dense classifiers, which hedge their bets as much as possible. Both of the algorithms presented in this paper achieve this density by using a L It is interesting to note that the choice of the emerges as a natural one in the theoretical analysis of our first, LP-based learning approach. 1.1. Related Work Previous papers on  X  X oise-robust learning X  mainly deal with the problem of learning with a noisy training set, a research topic which is entirely orthogonal to ours. The learning algorithms presented in (Dietterich &amp; Bakiri, 1995) and (Gamble et al., 2007) try to be robust to general necessarily to feature deletion or corruption. ( ? ) presents adversarial learning as a one-shot two-player game be-tween the classifier and an adversary, and designs a ro-bust learning algorithm from a Bayesian-learning perspec-tive. Our approach shares the motivation of ( ? ) but is oth-line learning, where the training and classification phases are interlaced and cannot be distinguished, (Littlestone, 1991) proves that the Winnow algorithm can tolerate vari-ous types of noise, both adversarial and random.
 Our work is most closely related to the work in (Globerson &amp; Roweis, 2006), and its more recent enhancement in (Teo et al., 2008). Our motivation is the same as theirs, and the approaches share some similarities. Our experiments, pre-sented in Sec. 4, suggest that our algorithms achieve con-siderably better performance, but we would also like to em-phasize more fundamental differences between the two ap-proaches: We allow features to have different a-priori im-portance levels, and we take this information into account in our algorithm and analysis. Our approach uses L ularization to promote a dense solution, where (Globerson &amp; Roweis, 2006) uses L proach, which uses online-to-batch conversion techniques , is entirely novel. Finally, we prove statistical generaliz ation bounds for our algorithms despite the change in distributio n at classification time. In this section, and throughout the paper, we use lower-case bold-face letters to denote vectors, and their plain-f ace counterparts to denote each vector X  X  components. We also use the notation [ n ] as shorthand for { 1 , . . . , n } 2.1. Feature Deleting Noise We first examine the case where features are missing at classification time. Let X  X  R n be an instance space and let D be a probability distribution on the product space X  X  { X  1 } . We receive a training set S = { ( x i , y i sampled i.i.d. from D , which we use to learn our classifier. We assign each feature j  X  [ n ] a value v mally, we think of v ture j , or as the importance of feature j to the classification task. It can also represent the cost of obtaining the feature (such as the price of a medical test). Next, we define the value of a subset J of features as the sum of values of the features in that subset, and we denote V ( J ) = P For instance, we frequently use V ([ n ]) when referring to P we fix a noise-tolerance parameter N in [0 , V ([ n ])] and de-fine P = V ([ n ])  X  N . During the classification phase, instances are generated in the following way: First, a pair ( x , y ) is sampled from D . Then, an adversary selects a subset of features J  X  [ n ] such that V ([ n ] \ J )  X  N replaces x for each instance individually, and with full knowledge of the inner workings of our classifier. The noise-tolerance pa -rameter N essentially acts as an upper bound on the amount of damage the adversary is allowed to inflict. We would like to use the training set S (which does not have miss-specific type of classification-time noise.
 We focus on learning linear margin-based classifiers. A lin-ear classifier is defined by a weight vector w  X  R n and a bias term b  X  R . Given an instance x , which is sampled from D , and a set of coordinates J left intact by the adver-sary, the linear classifier outputs b + P of b + P j  X  J w j x j constitutes the actual binary prediction, while | b + P fidence in that prediction. A classification mistake occurs if and only if y ( b + P of the linear classifier ( w , b ) as
R ( w , b ) = Pr Since D is unknown, we cannot explicitly minimize Eq. (1). Thus, we turn to the empirical estimate of Eq. (1), the empirical risk , defined as 1 m where [[  X  ]] denotes the indicator function of the predicate Minimizing the empirical risk directly constitutes a diffic ult combinatorial optimization problem. Instead, we formulat e a linear program that closely resembles the formulation of the Support Vector Machine (Vapnik, 1998). We choose a margin parameter  X  &gt; 0 and a regularization parameter C &gt; 0 , and solve the problem min The objective function of Eq. (3) is called the empirical hinge-loss obtained on the sample S . Since  X  strained to be non-negative, each training example con-tributes a non-negative amount to the total loss. Moreover, the objective function of Eq. (3) upper bounds the empiri-( w , b,  X  ) of Eq. (3),  X  i upper bounds  X  times the indicator function of the event To see this, note that for a given example ( x i , y exists a feature subset J such that V ([ n ] \ J )  X  N and y ( b + P j  X  J w j x j )  X  0 then the first constraint in Eq. (3) enforces  X  now implies that V ( J )  X  P , and therefore  X  a set J does not exist, then the second constraint in Eq. (3) enforces  X  The optimization problem above actually does more than minimize an upper bound on the empirical risk. It also re-quires the margin attained by the feature subset J to grow with proportion to V ( J ) . While a true adversary would always inflict the maximal possible damage, our optimiza-tion problem also prepares for the case where less damage is inflicted, requiring the confidence of our classifier to in-crease as less noise is introduced. We also restrict w to a hyper-box of radius C , which controls the complexity of the learned classifier and promotes dense solutions. More-over, this constraint is easy to compute and makes our algo-rithms more efficient. Although Eq. (3) is a linear program, it is immediately noticeable that the size of its constraint set may grow exponentially with the number of features n . For example, if v per example. We deal with this problem below. 2.2. A Polynomial Approximation Taking inspiration from (Carr &amp; Lancia, 2000), we find an efficient approximate formulation of Eq. (3), which turns out to be an exact reformulation of Eq. (3) when v  X  { 0 , 1 } for all j  X  [ n ] . Specifically, we replace Eq. (3) with  X  i  X  [ m ]  X  j  X  [ n ] y i w j x i,j  X   X v j P  X   X  i v j  X   X   X  i  X  [ m ]  X  j  X  [ n ]  X  i,j  X  0 , where the minimization is over w  X  R n , b  X  R ,  X   X  R m ,  X   X  R m , and  X  1 , . . . ,  X  m , each in R n . The number of variables and the number of constraints in this problem are both O ( mn ) . The following theorem explicitly relates the optimization problem in Eq. (4) with the one in Eq. (3). Theorem 1. If ( w  X  , b  X  ,  X   X  ,  X   X  ,  X   X  Eq. (3), and therefore the value of Eq. (4) upper-bounds the value of Eq. (3). Moreover, if v if it does not hold that v assuming k x i k  X  1 for all i , then the difference between the value of Eq. (4) and the value of Eq. (3) is at most C/ X  As a first step towards proving Thm. 1, we momentarily forget about the optimization problem at hand and focus on feasible point of Eq. (3) or not? More concretely, for each training example ( x i , y all J with V ([ n ] \ J )  X  N it holds that We can answer this question by comparing  X   X  value of the following integer program: For example, if the value of this integer program is less than y the set J = { j  X  [ n ] :  X   X  other hand, if there exists some J with V ([ n ] \ J )  X  N violates Eq. (5) then its indicator vector is a feasible poin t of Eq. (6) whose objective value is less than  X   X  Directly solving the integer program in Eq. (6) may be dif-ficult, so instead we examine the properties of the following linear relaxation: min To analyze this relaxation we require the following lemma. Lemma 1. Fix an example ( x i , y ( with respect to these choices. (a) If  X   X   X   X  holds. (b) In the special case where v j  X  [ n ] and where N is an integer,  X   X   X   X  i if and only if Eq. (5) holds. (c) There exists a minimizer of Eq. (7) with at most one coordinate in (0 , 1) .
 The proof of the lemma is straightforward but technical, and is omitted due to lack of space. Lemma 1 tells us that comparing the value of the linear program in Eq. (7) with  X   X  i provides a sufficient condition for Eq. (5) to hold for the example ( x i , y both sufficient and necessary in the special case where v j  X  { 0 , 1 } ing the first part of Thm. 1 using claim (a) in Lemma 1. The remaining parts of the theorem follow similarly from claims (b) and (c) in the lemma.
 Proof of Theorem 1. Let ( w  X  , b  X  ,  X   X  ,  X   X  ,  X   X  an optimal solution to the linear program in Eq. (4). Specif-ically, it holds for all i  X  [ m ] that  X   X  negative, that P  X   X  Therefore, it also holds that the value of the following op-timization problem is at least  X   X   X  gramming (Boyd &amp; Vandenberghe, 2004) states that the value of Eq. (8) equals the value of its dual optimization problem, which is: min s.t.  X  j  X  [ n ] 0  X   X  In other words, the value of Eq. (9) is also at least  X   X  Using claim (a) of Lemma 1, we have that holds for all J with V ([ n ] \ J )  X  N . The optimization problem in Eq. (4) also constrains k w k Eq. (3). Since Eq. (3) and Eq. (4) have the same objective function, the value of Eq. (3) is upper bounded by the value of Eq. (4). 2.3. Generalization Bounds We now prove a generalization bound on the risk of the classifier learned in our framework, using PAC-Bayesian techniques (McAllester, 2003). Throughout, we assume that k x k ity, we assume that the bias term b is 0 , and that v for all j . These assumptions can be relaxed at the cost of a somewhat more complicated analysis. Given a classifier w , let  X  ple ( x , y ) , defined as where [[ ]] again denotes the indicator function. Note that Theorem 2. Let S be a sample of size m drawn i.i.d from D . For any  X  &gt; 0 , with probability at least 1  X   X  , it holds for all w  X  R n with k w k with w is at most where  X  ( m,  X ,  X  ) = ln( m/ X  ) + P n and KL is the Kullback-Leibler divergence. The above is upper-bounded by the empirical  X  -loss (which equals
P m i =1  X   X  ( w , x i , y i ) ), plus the additional term Proof sketch. The proof follows along similar lines to the PAC-Bayesian bound for linear classifiers in (McAllester, 2003). First, define the axis-aligned box B = Q n upper bound E loss over D of a classifier sampled uniformly from B  X  ing the PAC-Bayesian theorem (McAllester, 2003), where the uniform distribution over B  X  [  X  C, C ] n is the poste-rior classifier distribution, and the uniform distribution over of the average empirical  X  from B , plus a complexity term dependent on the volume ratio between B and [  X  C, C ] n . Finally, this average loss can be upper bounded by the empirical  X  peating the technique of the first stage. The weaker bound stated in the theorem follows from a lower bound on the KL divergence, presented in (McAllester, 2003).
 It is interesting to note that L the most natural one in this setting, since it induces the mos t convenient type of margin for relating the  X  functions as described above. This lends theoretical sup-port to our choice of the L 2.4. Feature Corrupting Noise We now shift our attention to the case where a subset of the features is corrupted with random noise, and show that the the same LP approach used to handle missing features can also deal with corrupted features if the margin parameter  X  in Eq. (4) is sufficiently large. For simplicity, we shall assume that all features are supported on [  X  1 , 1] with zero mean. Unlike the feature deleting noise, we now assume that the each feature selected by the adversary is replaced with noise sampled from some distribution, also supported on [  X  1 , 1] and having zero mean. The following theorem expected  X  -loss in the feature deletion setting, where the latter can be bounded with Thm. 2.
 Theorem 3. Let  X  , C , and N be arbitrary positives, and let  X  be at least C p N ln(1 / X  ) / 2 . Assume that we solve Eq. (4) with parameters  X  , C , N and with v j  X  [ n ] . Let w be the resulting linear classifier, and assume for simplicity that the bias term b is zero. Let f be a random vector-valued function on X , such that for every x  X  X , f ( x ) is the instance x after the feature corruption scheme described above. Then, using  X  ( x , y ) drawn randomly from D , we have: Proof. Let ( x , y ) be an example and let J denote the fea-ture subset which remains uncorrupted by the adversary. Using Hoeffding X  X  bound and our assumption on  X  , we have that Pr y P by  X  . Therefore, with probability at least 1  X   X  over the randomness of f , y h w , f ( x ) i is equal to: y X Thus, with probability at least 1  X   X  , Pr( y h w , f ( x is upper bounded by E [  X  ability at most  X  , Pr( y h w , f ( x ) i &lt; 0)  X  1 . We conclude with an interesting observation. In the fea-ture corruption setting, making a correct prediction boils down to achieving a sufficiently large margin on the uncor-rupted features. Let r  X  (0 , 1) be a fixed ratio between N and n , and let n grow to infinity. Assuming a reason-able degree of feature redundancy, the term y P grows as  X ( n ) . On the other hand, Hoeffding X  X  bound tells us that y P for r arbitrarily close to 1 and a large enough n , the first sum in Eq. (11) dominates the second. Namely, by setting  X  =  X ( ruption matches our ability to withstand feature deletion. We now turn to our second learning algorithm, taking a different angle on the problem. We momentarily forget about the original statistical learning problem and instea d define a related online prediction problem. In online learn-ing there is no distinction between the training phase and the classification phase, so we cannot perfectly replicate the classification-time noise scenario discussed above. In -stead, we assume that an adversary removes features from every instance that is presented to the algorithm. We ad-dress this online problem with a modified version of the Perceptron algorithm (Rosenblatt, 1958) and use an online-to-batch conversion technique to convert the online algo-tour through online learning gives us efficiency while the online-to-batch technique provides us with the statistica l generalization properties we are interested in. 3.1. Perceptron with Projections onto the Cube We start with a modified version of the well-known Per-ceptron algorithm (Rosenblatt, 1958), which observes a se-quence of examples ( x i , y and incrementally builds a sequence ( w i , b ear margin-based classifiers, while constraining them to a hyper-cube. Before processing example i , the algorithm has the vector w i and the bias term b ory. An adversary takes the instance x i and reveals only a subset J cause the online algorithm to make a prediction mistake. In choosing J V ([ n ] \ J )  X  N . Next, the algorithm predicts the label associated with x i to be After the prediction is made, the correct label y and the algorithms suffers a hinge-loss  X  ( w , b, x fined as where P = V ([ n ])  X  N and [  X  ] tion, max {  X , 0 } . Note that  X  ( w i , b  X  times the indicator of a prediction mistake on the current example, for any choice of J choose to denote the loss by  X  to emphasize the close rela-tion between  X  ( w i , b choice of loss function, we can assume that the adversary chooses the subset J The algorithm now uses the correct label y pair ( w i +1 , b tion. If  X  ( w , b, x , y ) = 0 , the algorithm defines w i +1 and b ing the following coordinate-wise update and b abbreviates the function max min {  X , C } ,  X  C . This up-date is nothing more than the standard Perceptron update with constant learning rate  X  , with an added projection step onto the hyper-cube of radius C . The specific value of  X  used above is the value that optimizes the cumulative loss bound below. As in the previous section, restricting the online classifier to the hyper-cube helps us control its com-plexity, while promoting dense classifiers. It also comes in handy in the next stage, when we convert the online algo-rithm into a statistical learning algorithm.
 Using a rather straightforward adaptation of standard Per-ceptron loss bounds, to the case where the hypothesis is confined to the hyper-cube, leads us to the following the-orem, which compares the cumulative loss suffered by the algorithm with the cumulative loss suffered by any fixed hypothesis in the hyper-cube of radius C .
 Theorem 4. Choose any C &gt; 0 and let w  X   X  R n and b  X   X  R be such that k w  X  k C . Let ( x i , y ples, with k x i k quence is presented to our modified Perceptron, and let  X  ( The next step is to convert our online algorithm into a sta-tistical learning algorithm. 3.2. Converting Online to Batch To obtain a statistical learning algorithm, with risk guar-antees, we assume that the sequence of examples pre-sented to the modified Perceptron algorithm is a training set sampled i.i.d. from the underlying distribution D . We turn to the simple averaging technique presented in (Cesa-Bianchi et al., 2004) and define  X  w = 1  X  and defines our robust classifier. We use the derivation in (Cesa-Bianchi et al., 2004) to prove that the average classi -fier provides an adequate solution to our original problem. Note that the loss function we use, defined in Eq. (12), is bounded and convex in its first two arguments. This al-lows us to apply (Cesa-Bianchi et al., 2004, Corollary 2) to fered by the Perceptron. It also allows us to apply Hoeffd-ing X  X  bound to relate the expected loss of any fixed classifier ( w  X  , b  X  ) with its empirical loss on the training set. Com-bining both bounds results in the following corollary. Corollary 1. For any  X  &gt; 0 , with probability at least over the random sampling of S , our algorithm constructs (  X  w ,  X  b ) such that E ( x ,y )  X  X   X  (  X  w ,  X  b, x , y ) min where  X  =  X  max set of all pairs ( w , b ) such that k w k Using the fact that the hinge loss upper-bounds  X  times the indicator function of a prediction mistake, regardless of t he adversary X  X  choice of the feature set, we have that the ex-pected hinge loss upper-bounds  X  R (  X  w ,  X  b ) . We compare the performance of our two algorithms (LP-based and online-to-batch) with that of a linear L (Joachims, 1998) and with the results reported in (Glober-son &amp; Roweis, 2006). We used the GLPK package ( http://www.gnu.org/software/glpk ) to solve the LP formulation of our LP-based algorithm.
 We begin with a highly illustrative sanity check. We gener-ated a synthetic dataset of 1000 linearly separable instances in probability 0 . 2 . Then, we added two copies of the actual of 22 features. We randomly split the data into equally sized training and test sets, and trained an SVM classi-fier on the training set. We set v v last two features are more valuable. Using these feature values, we applied our technique with different values of the parameter N . We removed one or both of the high-fiers. With only one feature removed both SVM and our approach attained a test error of zero. With two features removed, the test error of the SVM classifier jumped to 0 . 477  X  0 . 004 (over 100 random repetitions of the exper-on the two perfect features. With the noise parameter set to N = 20 , our approach attained a test error of only 0 . 22  X  0 . 002 . This is only marginally above the best possi-ble error rate for this setting.
 we conducted experiments using the SPAM and MNIST datasets. The SPAM dataset, taken from the UCI reposi-tory, is a collection of spam and non-spam e-mails. Spam can be detected by different word combinations, so we ex-pect considerable feature redundancy in this dataset. The MNIST dataset is a collection of pixel-maps of handwritten digits. Again, following (Globerson &amp; Roweis, 2006), we focused on the binary problem of distinguishing the digit from the digit 7 . Adjacent pixels often contain redundant information, making MNIST well-suited for our needs. On each dataset, we performed 2 types of experiments. The first type follows exactly the protocol used in (Globerson &amp; Roweis, 2006). Namely, the algorithm is trained with a small training set of 50 instances, and its performance is tested in the face of random feature-deleting noise, which uniformly deletes N non-zero features from each test in-stance, for various choices of N . Notice that this setting deviates from the adversarial setting considered so far, an d the reason for conducting this experiment is to compare our results to those reported in (Globerson &amp; Roweis, 2006). A validation set is used for parameter tuning. We did not test our online-to-batch algorithm within this setting, si nce it has little advantage with such a small training set. The function of the number of deleted features. Compared to its competitors, our algorithm has a clear and substantial advantage.
 The second type of experiment simulates more closely the adversarial setting discussed throughout the paper. Using 10 -fold cross-validation, we corrupted each test instance using a greedy adversary, which deletes the most valuable features of each instance until either the limit N is reached or all useful features are deleted. 1 / 9 of the training set was used for parameter tuning. Due to computational con-siderations when running our LP-based algorithm, we per-formed a variant of bagging by randomly splitting the train-ing set into chunks, training on each chunk individually, and finally averaging the resulting weight vectors. In con-trast, our online-to-batch algorithm trained on the entire training set at once, and so did the SVM algorithm. We SPAM dataset, we repeated this entire experiment twice, once with features values v with v we set where Z is such that P v is the mutual information between the predicate [[ x and the label y , over all examples in the training set. Intu-itively, we are calculating the amount of information con-tained in each individual feature on the label, provided tha t we are looking only at linear threshold functions. When ex-perimenting with the MNIST dataset, we only used the val-ues of v the features of MNIST are of markedly different impor-tance levels. For example, the corner pixels, which are al-ways zero, are completely uninformative, while other pix-els may be very informative. The results are presented in Fig. 2, and show test error as a function of N . Clearly, our algorithms have the advantage. SVM repeatedly puts all of its eggs in a small number of baskets, and is severely pun-ished for this, while our technique anticipates the actions of the adversary and hedges its bets accordingly.
 Moreover, the results in Fig. 2 demonstrate the tradeoffs between our LP-based and online-to-batch algorithms. Al-though we have handicapped the LP-based algorithm by chunking the training set, its performance is comparable and sometimes superior to that of the online-to-batch algo-rithm. With less or without chunking, we expect its perfor-mance to be even better.
 We conclude that our proposed algorithms successfully withstand feature corruption at classification time, and co n-siderably improve upon the current state of the art. On a more general note, this work has interesting connections to a recent trend in machine learning research, which is to de-velop sparse classifiers supported on a small subset of the features. In our setting, we are interested in the exact op-posite, and the efficacy of using the L demonstrated here. The trade-off between robustness and sparsity provides fertile ground for future research. Boyd, S., &amp; Vandenberghe, L. (2004). Convex optimiza-tion . Cambridge University Press.
 Carr, R. D., &amp; Lancia, G. (2000). Compact vs. exponential-size LP relaxations SANDIA Report 2000-2170.
 Cesa-Bianchi, N., Conconi, A., &amp; Gentile, C. (2004). On the generalization ability of on-line learning algorithms .
IEEE Transactions on Information Theory , 50 , 2050 X  2057.
 Dietterich, T. G., &amp; Bakiri, G. (1995). Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research , 2 , 263 X 286. Gamble, E., Macskassy, S., &amp; Minton, S. (2007). Classifi-cation with pedigree and its applicability to record link-age. Workshop on Text-Mining &amp; Link-Analysis . time: robust learning by feature deletion. Proceedings of ICML 23 (pp. 353 X 360).
 Joachims, T. (1998). Making large-scale support vector machine learning practical. In Advances in kernel meth-ods -support vector learning . MIT Press.
 Littlestone, N. (1991). Redundant noisy attributes, attri bute errors, and linear-threshold learning using winnow. Pro-ceedings of the COLT 4 (pp. 147 X 156).
 McAllester, D. A. (2003). Simplified PAC-bayesian margin bounds. Proceedings of COLT 16 (pp. 203 X 215).
 Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review , 65 , 386 X 407.
 Teo, C.-H., Globerson, A., Roweis, S., &amp; Smola, A. (2008). Convex learning with invariances. Advances in NIPS 21 .
