 Knowledge bases capture millions of entities such as people, companies or movies. However, their knowledge of named events like sports finals, political scandals, or natural dis-asters is fairly limited, as these are continuously emerg-ing entities. This paper presents a method for extract-ing named events from news articles, reconciling them into canonicalized representation, and organizing them into fine-grained semantic classes to populate a knowledge base. Our method captures similarity measures among news articles in a multi-view attributed graph, considering textual con-tents, entity occurrences, and temporal ordering. For dis-tilling canonicalized events from this raw data, we present a novel graph coarsening algorithm based on the information-theoretic principle of minimum description length. The qual-ity of our method is experimentally demonstrated by ex-tracting, organizing, and evaluating 25 000 events from a corpus of 300 000 heterogeneous news articles.
 H.1 [ Information Systems ]: Models and Principles Knowledge Bases; Temporal Knowledge; Event Mining; In-formation Extraction; Minimum Description Length
Motivation: Large knowledge bases such as dbpedia.org , yago-knowledge.org , or freebase.com (the core of the Google Knowledge Graph), are valuable assets for entity awareness in search, summarization, analytics, and recommendations. These assets contain millions of individual entities (people, places, products, etc.), including named events such as elec-tions, revolutions, notable disasters, important concerts, etc.
However, the coverage of such events is fairly limited in today X  X  knowledge bases, the reason being that entities and facts are mostly extracted from Wikipedia and similarly cu-rated sources. None of the above projects taps into news or social media for increasing their population of named events. Thus, major events are captured only late, after being prop-erly edited in Wikipedia, and events in the long tail, such as concerts of indie rock bands, and brand-new events, such as hurricanes or political scandals, are completely out of scope. We aim to capture more events of this kind as early possible.
Problem Statement: This paper addresses the prob-lem of populating a knowledge base with fine-grained emerg-ing events, along with detailed semantic typing (e.g., using classes like rock concerts or hurricanes), relationships among events (sub-events, temporal order, etc.), as well as people and organizations participating in events. The raw input for this task is articles from newspapers, magazines, and online feeds. A seemingly obvious approach would be to run a clus-tering algorithm on a set of news articles, using a text-based contents similarity measure. Each resulting cluster would then be interpreted as a separate event. However, our goal is to go beyond this and to obtain semantically clear output that is ready for populating a high-quality knowledge base. This entails the following desiderata:  X  Canonicalized event entities: Rather than merely clus- X  Semantic labeling: Each event is labeled with one or more  X  Chronological ordering: Events must be placed on po- X  Event hierarchies: Events should be organized in a hi-From a computational perspective, we address the follow-ing problem. Given a heterogeneous collection of news arti-cles (from many different sources), group these into equiva-lence classes denoting the same events, label the equivalence classes with semantic types and participating entities, chain them in chronological order on the timeline, and organize them in a sub-/super-event hierarchy.

An example output that our method achieves, by process-ing several hundred articles about the UEFA Champions League season 2012/2013, is shown in Figure 1. The fig-ure shows three different events and the news articles from which they were inferred, together with their representative names (inside ovals), their semantic types (top line), their participating entities (bottom line), and their chronological ordering. Note that the underlying articles in each cluster are not necessarily from the same date; so different clusters can heavily overlap in their overall timespans and inferring the temporal order between events is not obvious at all.
State of the Art: There are prior methods for mining events and storylines from a batch or stream of news articles (e.g., [3, 5, 18]). These methods compute output at the level of (ambiguous) noun phrases such as news headlines rather than true entities. For example, a typical output could be events such as  X  X ayern wins Champions League X ,  X  X ay-ern beats Borussia X ,  X  X ayern X  X  triumph in London X ,  X  X ayern overcomes last year X  X  final trauma X , etc. These are semanti-cally overlapping and do not denote distinct events. Simple clusters of news articles are not sufficient for a high-quality knowledge base.

In contrast to this prior work, our method reconciles all surface cues for the same event into a single entity in a canonicalized representation. Moreover, our method assigns fine-grained class labels such as soccer finals or European sports championships to an event. So there is a fundamen-tal difference between traditional event mining and our goal of populating a knowledge base, satisfying the desiderata stated above.

Contribution: This paper presents novel methods for populating event classes (concerts, ceremonies, elections, con-flicts, accidents, disasters, tournaments, etc.) of high-quality knowledge bases by extracting, cleaning, and canonicalizing fine-grained named events from news corpora. Our method-ology involves the following contributions:  X  mapping news articles into event classes by automatically labeling them with fine-grained types (Section 3);  X  a multi-view graph model that captures relationships and relatedness measures between news articles, and a novel graph-coarsening algorithm for grouping and temporally ordering news articles based on the information-theoretic principle of minimum description length (Sections 4, 5);  X  building a high-quality knowledge base with 25 000 named events automatically derived from 300 000 news articles referenced as external links in Wikipedia (Section 6).
Our system taps online news from a heterogeneous set of sources (newspapers, magazines, other news feeds).  X  X istill-ing X  canonicalized and semantically organized events from these news articles proceeds in three steps. First, news are labeled with semantic types; these are later carried over to the distilled events. Second, news are grouped in a hierchical manner; each group will eventually be an event. Third, re-lated events are chained in chronological order. The second and the third step are carried out by the same algorithm, based on coarsening a multi-view attributed graph (MVAG). Finally, the resulting events and their semantic annotations are placed in the knowledge base, populating a taxonomy of fine-grained semantic types.
 Feature sets: We exploit four kinds of feature groups that are provided by each news article n or can be derived from it by information extraction methods:  X  the textual content C ( n ) of the article,  X  the publication date t ( n ),  X  the set of entities A ( n ) : people, organizations, countries,  X  the semantic types T ( n ) of events covered by an article,
The entity names appearing in a news article are extracted using the Stanford NER tagger [7]. Since these are used as features for subsequent processing, we do not attempt to disambiguate entities onto canonical representations in a knowledge base, but simply accept a tolerable level of am-biguity and noise. Semantic types for news articles are ob-tained in two steps: first, by constructing statistical lan-guage models (LM X  X ) [24] for articles and types; second, by mapping an article to its similar types using Kullback-Leibler divergence. This is further explained in Section 3. Distance measures: Features are used to compute differ-ent kinds of distance measures between news articles. The content distance is the cosine distance of the articles X  tf  X  idf vectors over a bag-of-words model: where V ( n i ) and V ( n j ) are the tf  X  idf vectors of news articles n i and n j , respectively; tf denotes the term frequency of words, and idf denotes the inverse document frequency of words (i.e., the inverse frequency of a word in the corpus). The temporal distance is the normalized time distance be-tween the publication dates of news articles: where H is the time horizon of the entire news corpus. We define H as the difference between the earliest and the latest timestamps appearing in the corpus.
 The attribute distance between articles is the weighted Jac-card coefficient capturing the overlap in the entity sets or in the type sets respectively: where X can be entity set A , or type set T . Entity names are weighted by their tf  X  idf values, the types are weighted by their idf values.

These are all standard measures from the information-retrieval and text-mining literature. Our specific choices are based on prevalent usage in the state-of-the-art, but could be easily replaced.
 Multi-view attributed graph (MVAG): The feature sets and the distance measures are used together to con-struct a multi-view attributed graph (MVAG) of news arti-cles; G = ( V,A,E,F ).
 Vertices and attributes: A vertex v i  X  V of the MVAG cor-responds to a news article n i . Each vertex inherits a set of attributes A from n i : its textual content C , its timestamp t , its associated entities A , and its types T .
 Edges and weights: The MVAG has two kinds of edges: undi-rected ones, edge set E , and directed ones, edge set F . All edges are weighted. Two vertices are connected by an undi-rected edge e i  X  j if they share at least one entity and at least one type: The weight of the edge is the content distance between two vertices; w ( e i  X  j ) = dist text ( n i ,n j ).

Two vertices are connected by a directed edge f i  X  j if their timestamps indicate that they are ordered on the timeline. The weight of a directed edge is the temporal distance be-tween the time stamps of vertices: w ( f i  X  j ) = dist temp
We have devised a two-step method for mapping news ar-ticles onto semantic event types in a knowledge base. First, news articles are mapped to Wikipedia categories (ca. 32 000 event categories) using statistical language models (LM X  X ) [24]. Second, Wikipedia categories are mapped to Wordnet event classes by the heuristic of Suchanek et al. [1]. Word-Net provides ca. 6 800 classes under the type label  X  X vent X  in its taxonomy, with 5-6 levels of subclasses. Examples are: final  X  match  X  contest  X  social_event  X  event , and All WordNet classes are also integrated in YAGO [1, 11], the specific knowledge base we aim to populate.
The first step is based on language models for news ar-ticles and categories. The LM of a news article captures i) the news content in terms of title and body words, and ii) all entities in the news article, including normalized date literals that appear in the article. LM X  X  are a principled approach in IR [24], widely used for query-result ranking, cross-lingual retrieval, question answering, etc. An LM is a probability distribution over words or other text features. LM X  X  are usually defined for documents and for queries, with probabilistic or information-theoretic distance measures be-tween LM X  X . For our setting, we customize and extend the notion of LM X  X  as follows.
 Document models: The document model of a news article n is constructed over keywords and entities appearing in the article, so it is a probability distribution over { w : w  X  n } X  X  e : e  X  n } , where w denotes keywords, and e denotes entities.
 For example, the news article Borussia Dortmund 1-2 Bayern Munich ( http://www.bbc.co.uk/sport/0/football/ 22540468 ), starting with  X  X rjen Robben X  X  late winner exor-cised the demons that have haunted him and Bayern Mu-nich in the Champions League as they won a pulsating all-Bundesliga encounter . . .  X , has a document LM with:  X  keywords: { winner, exorcised, demons, match, goal, . . . }  X  entities: { Arjen Robben, Bayern Munich, The LM X  X  probability distribution for news article n is where s is a word or entity, P W [ s ] and P E [ s ] are estimated probability distributions for words and entities in n , respec-tively, and  X  is a hyper-parameter that controls the rela-tive influence of each of the two aspects. The LM of a Wikipedia category is defined analogously and constructed from all Wikipedia articles that belong to the category. Estimating LM parameters: The LM parameters for both news articles and categories are estimated from fre-quencies in the underlying texts, with Jelinek-Mercer smooth-ing (using the global frequencies in the entire collection of all news articles and all categories, respectively): where  X  is the smoothing coefficient. P E [ s ] is estimated analogously.
 Comparing news and categories: To compare how close a Wikipedia category is to a news article, we use the Kullback-Leibler (KL) divergence (aka. relative entropy) between the Figure 2: Coarsening a multi-view attributed graph. corresponding LM X  X : For each news article, we compute the top-k categories based on this distance measure, and accept those categories whose KL divergence is below a specified threshold.
The second step of our two-step approach maps the ac-cepted Wikipedia categories to their lowest (i.e., most spe-cific) event type in the WordNet taxonomy DAG, by adopt-ing and adjusting the heuristic of [1]. This method uses a natural-language noun-group parser on the category name to identify its head word, and maps the head word to the best matching WordNet type. If the head word is am-biguous, the word-sense frequency information of WordNet is used to make the choice. For example, for the cate-gory name  X  General elections in Italy  X , the word  X  elections  X  is found as the head word, and it is mapped to the first one of four possible senses in WordNet: election (a vote to select the winner of a position or political office) (see http://wordnet.princeton.edu/ ).
Once we have all the features of news articles in a collec-tion, including the semantic type labels, our goal is to distill canonicalized named events from this collection and orga-nize the events into semantic classes of the knowledge base. This task entails a grouping and a chaining problem : com-bining thematically highly related news into a single event, and ordering the resulting groups along the timeline.
A straightforward approach to this problem would be to proceed in two phases: first compute clusters of news arti-cles based on similarity over all features, then infer ordering relations between clusters (e.g., by majority voting over the items in each pair of clusters). However, computing the ordering chains only after the clusters are determined may pose a poor if not unsolvable situation for the chaining step.
To overcome these issues, we designed a novel approach to this problem, by means of a graph coarsening. The rationale is that we transform the fine-grained MVAG for news articles into a coarser graph whose nodes correspond to the final event entities. Thus, our approach integrates the clustering and ordering tasks.

In order to identify the optimal coarsened graph for a given MVAG, we take a principled approach and employ Minimum Description Length (MDL) principle [9]. Given a multi-view attributed graph G with node set V , weighted undirected edges E , weighted directed edges F , entity sets A , and types T , a coarser graph G V  X   X  2 V (i.e., forming equivalence classes of nodes) and E ,F  X  , A  X  , T  X  is computed such that i) G  X  preserves the main properties and structure of G , and ii) G  X  is simpler (smaller, coarser) than G .
 The grouping of the original V nodes that leads to V  X  in-duces the undirected edges E  X  , directed edges F  X  , edge weights, and attribute sets of the coarsened graph. This is explained in the following.
 Consider the grouping function  X  for mapping V to V  X  .  X  induces edge sets E  X  and F  X  :  X  also determines the edge weights in G  X  by averaging the weights of edges between all node pairs ( x,y ) that are mapped onto coarsened nodes ( X ( x ) ,  X ( y )):
 X  induces entity sets A  X  in G  X  . It is worth noting that the entity set of a node in G can be noisy due to imperfect quality of the named entity recognition tool. In addition, there can be entities mentioned in a news article that are not relevant to the reported event. For example, the enti-ties  X  BBC Radio, BBC Sport website  X , extracted from the news article  X  Champions League: Dortmund confident Mats Hummels will be fit  X , are not relevant to the event mentioned in the article. Hence, the grouping function  X  induces the entity set of x  X  as the shared entities of the fine nodes that are mapped to x  X  . Therefore, the main participants (enti-ties) of an event are captured, whereas irrelevant entities are avoided. Formally, A ( x  X  ) = T A ( x i ), where  X ( x i induces types T  X  in G  X  in the same way as entity sets, using the intersection of type sets.

Figure 2 illustrates the MVAG coarsening. We formalize our problem using the Minimum Description Length (MDL) principle [9], which can be paraphrased as Induction by Compression . In a nutshell, it identifies the best model M  X  in a family M of models M as the model minimizing L ( M ) + L ( D | M ), where L ( M ) is the length, in bits, of the description of M , and L ( D | M ) the length of the data given M . This scheme ensures that M  X  neither overfits nor is redundant X  X therwise another M would minimize the sum. We formally define our problem as follows.

Problem: In our setting, the family M of models is the family of MVAG X  X . For a given graph G our goal is to com-pute the MDL optimal coarsened graph G  X  is minimal. Hence, our algorithms aim to find a minimum of this objective function.

To use MDL we have to define how we encode a model, i.e., a coarsened graph G  X  , and how we encode the input graph G given G  X  . For the latter the high-level idea is to encode the error of G  X  with respect to G , i.e., we encode their exclusive OR, G  X   X  G , such that the receiver can reconstruct the original graph without loss upon receiving G  X  and G  X  G . We formalize these encodings as follows.
To encode a coarse graph G  X  , we encode all its properties: its vertices V , their attributes A , the undirected edges E , and the directed edges F , The vertices. Encoding the vertices entails encoding their number (upper bounded by | V | ), and encoding per vertex v  X  V  X  to how many and which nodes in V it maps. Hence, The attributes. We encode the vertex attributes by L ( A  X  ) = P where per coarse vertex v  X  V  X  we encode how many at-tributes it has, which these are, and their weights. We en-code weights at the resolution of the data, rs = 10 # sign . digits based on the number of significant digits of the data. The undirected edges. For encoding the undirected edges we first encode their number X  X sing the upper bound ub = | V  X  | ( | V  X  | X  1) X  X hen identify which edges exist, and then encode their weights again using resolution rs . We have The directed edges. Encoding the (one-)directional edges is almost identical; in addition we only need to transmit their direction, for which we require one bit per edge. Thus,
To reconstruct the original graph, we need to transmit all information needed to interpret the coarse graph, as well as correct any errors it makes with regard to the input data. That is, we need to correct all missing and superfluous edges, attributes, and their weights. At a high level we have
L ( G | G  X  ) = L ( | V | ) + L ( A | A  X  ) + L ( E | E  X  As L ( | V | ) is constant for all models we can safely ignore it. Attributes: Starting with the node attributes, we transmit the error per node by encoding i) the number of missing and superfluous attributes, ii) which attributes these are, and iii) the correct weights. Thus, L ( A | A  X  ) = P v  X  V log( | A \ A where atr ( v ) is the set of attributes of v . We encode the at-tribute weight errors using log( rs ) bits each X  X f one is willing to make assumptions on the error distribution other choices are warranted.
 Undirected edges: To reconstruct adjacency matrix E , we transmit the edges in the (upper diagonal part) of the error matrix E c = E  X   X  E where  X  the exclusive OR. Thus, L ( E | E  X  ) = log ( ub ( E )) + log ` ub ( E ) | E We encode the weight errors using log( rs ) bits per edge in E .
 Directed edges: Last, let us write F c = F  X   X  F for the er-ror matrix for the (uni-)directed edges. As above, we define L ( F | F  X  ) analogue to L ( E | E  X  ), but in addition need to specify the direction of edges in F which are unmodelled by F  X  using one bit per edge. Hence, we have L ( F | F  X  ) = log ( | F \ F  X  | )+log ( ub ( F ))+log ` Algorithm 1 Greedy (MVAG G ) 1: Q  X   X  2: for all matched pairs ( u,v )  X  V with gain ( u , v ) &gt; 0 do 3: Q. insert (( u , v ) , gain ( u , v )) 4: while Q 6 =  X  do . Iterative coarsening phase 5: mergeSet  X  Q. popHead () 6: for all overlapping pairs ( n,m )  X  Q with mergeSet do 7: mergeSet = mergeSet  X  X  n,m } , remove ( n,m ) from Q 8: Merge ( G, mergeSet ), Update ( G ) 9: recompute ( Q )
In sum, we now have a principled and parameter-free objective function for scoring the quality of coarse graphs for a multiview attributed graph.
For an input graph G , the set of all possible models M is huge. Moreover, it does not exhibit any particular struc-ture (e.g., sub-modularity) that we can exploit for efficient pruning of the search space. Hence, we resort to heuristics. The algorithms proceed in iterations; each iteration aims to reduce the input MVAG by the following operations:  X  Match : For a given node, the match operation finds a  X  Merge : This operation merges two or more nodes into  X  Update : Following a Merge step we update the MVAG
Within this general framework, we developed two specific algorithms: a greedy method and a randomized method, which we explain next in turn.
A standard greedy method would have a Match opera-tion that, for a given node, always chooses the node(s) for which a Merge results in the largest gain of the objective function. While our method follows this general principle, it has a specific twist by performing a light-weight form of look-ahead on the options for subsequent iterations. Specif-ically, we determine in each iteration if the currently best merge can be combined with other merges whose node pairs overlap with the node pair of the best merge. Without this look-ahead, the algorithm produces many tiny event groups.
The algorithm keeps track of the candidate node pairs considered for merging, using a priority queue Q of node pairs sorted in descending order of gain (line 2-3 in Algo-rithm 1). The gain is calculated as the objective function X  X  improvement caused by a potential node pair merge. The algorithm first selects the pair at the head of the queue, which decreases the objective function the most. Next, in contrast to standard greedy techniques, our algorithm scans the queue for other pairs that have one overlapping node with the nodes of the head pair (line 6-7). If such a pair is found, it is added to  X  mergeSet  X . The algorithm then pro-ceeds further and repeats considering further merges, until it exhausts a bounded part of the queue. Algorithm 2 Randomized (MVAG G ,  X  , , T ) 1: best  X   X  2: while T &gt; do 3: pick a random node u , and its match v 4: if gain ( u , v ) &gt; 0 then 5: Merge ( G , { u,v } ), Update ( G ), best  X  G 7: Merge ( G , { u,v } ), Update ( G ) 8: T  X  T  X   X 
As an example, suppose the algorithm found &lt;n 1 ,n as  X  bestPair  X , and the next positions in the priority queue are { &lt;n 2 ,n 5 &gt; , &lt;n 4 ,n 6 &gt; , &lt;n 3 ,n 1 tify n 5 and n 3 for merging as their best matches are al-ready included in  X  mergeSet  X . The algorithm thus expands merges all these nodes in one step (line 8). The subse-quent Update operation incrementally computes the neces-sary changes of the MVAG and updates all data structures for the graph. The gain values for nodes in Q are recom-puted for the next level of coarsening. When there is no further coarsening operation that would improve the objec-tive function, the resulting MVAG is returned.
Our randomized method is based on the principle of sim-ulated annealing [13]. This is an established framework for stochastic optimization, traversing the search space (of pos-sible Merge operations) in a randomized manner. In con-trast to greedy methods, the method can accept, with a certain probability, choices that lead to worsening the objec-tive function. The probability of such choices is gradually reduced, so the algorithm is guaranteed to converge.
Our algorithm, in each iteration, performs a randomized coarsening step. It does so by picking a random node u and then identifying its best Match v for a Merge operation (line 3 in Algorithm 2). This is slightly different from stan-dard methods where both nodes of a node pair would be chosen uniformly at random. Our experiments have shown that our choice leads to faster convergence. If the consid-ered coarsening step decreases the objective function, it is accepted and the energy of the system is updated (line 5). Otherwise, it is accepted with a probability of e gain ( u , v )
After each accepted coarsening step, the Update opera-tor adjusts the graph. The algorithm maintains a so-called  X  X emperature value X  T (a standard notion in simulated an-nealing) which is gradually reduced after each iteration, by geometric progression with factor  X  . Note that T is initially chosen very high and  X  is chosen very close to 1, therefore, the temperature cools down very slowly after each iteration. The algorithm terminates when the temperature drops be-low a specified threshold . Across all iterations, the best solution is remembered, and this is the final output when the algorithm terminates.
Our overriding goal in this work is to populate a high quality knowledge base with high coverage of named events extracted from news. Thus, we performed a large-scale # of events 24 348 # of sub-events 3 926 # of follow-up events 9 067 avg # of entities per event 18 # of distinct classes 453 computation in order to populate a knowledge base with named events. We used the Greedy method to process 295 000 news articles that are crawled from the external links of Wikipedia pages. These news articles are from a highly heterogeneous set of newspapers and other online news providers (e.g., http://aljazeera.net/, etc.). The Wikipedia pages themselves were not used. Ranking event candidates: The set of extracted named events exhibit mixed quality and are not quite what a near-human-quality knowledge base would require. However, it is easy to rank the event candidates based on the following scoring model, and use thresholding for high precision.
We use the cumulative gain for a coarse node representing an event in the output graph as a measure of quality. This is defined as the total improvement of the objective func-tion after the merge operations that involved intermediate nodes that finally belong to the coarse node at hand. Thus, the score for event E is S ( E ) = P the graph, q i is an intermediate node created by a merge operation that contributes to final output node E .
The acceptance threshold for an event score is set to the 90th percentile of the score distribution for all extracted events. This is a conservative choice, motivated by the re-ported quality of 90 to 95% for knowledge bases like YAGO [11]. Note that our task here is substantially harder than that of the YAGO extractors, as the latter operate on Wiki-pedia infoboxes. Thresholding may break chaining informa-tion between some events. Thus, chaining is re-computed by transitively connecting missing links. Each event is la-beled by a  X  representative news headline  X . The representa-tive news article is chosen based on the centrality score in the original MVAG.
 Statistics: Our method computed 24 348 high-quality events from the underlying 295 000 news articles. A good portion of these events contain sub-events and follow-up events (i.e., events that appear on a chain). Table 1 shows other statis-tics about the extracted events. The most densely populated classes of events are: protest (8732), controversy (7514), con-flict (4718), musical (1846), sport event (766), war (727), etc. An event can be assigned to multiple classes.
 Assessing the KB coverage: We show that the events distilled by our methods yield much higher coverage than traditional knowledge bases which solely tap into semistruc-tured elements of Wikipedia (infoboxes, category names, ti-tles, lists). We compared two knowledge bases on their in-total # 624 6423 % year events 16% 48% % month events 10% 36% Table 2: Coverage of events in YAGO vs. Event KB. cluded events: YAGO [11], built from the 2013-09-04 dump of Wiki-pedia, vs. Event KB, the knowledge base compiled by our method as described above.

For comparing the two knowledge bases in their coverage of events, we used the events section of Wikipedia X  X  year ar-ticles as a form of ground truth. Specifically we used the pages en.wikipedia.org/wiki/2012 and en.wikipedia.org /wiki/2013 , but excluded events after 2013-09-04, the date of the Wikipedia dump for YAGO. In total, these articles contain 51 itemized snippets, each of which briefly describes a salient event. We also considered the monthly pages, e.g., en.wikipedia.org/wiki/January_2013 , for the total relevant time period, together providing 9811 events in the same tex-tual format.

Note that these text items are not canonicalized events: they mention an event but often do not point to an ex-plicit Wikipedia article on the event, and in many cases such an explicit article does not exist at all. For example, the Wikipedia text for January 11, 2013 says:  X  X he French military begins a five-month intervention into the North-ern Mali conflict, targeting the militant Islamist Ansar Dine group. X  However, the only href links to other Wikipedia ar-ticles (i.e., entity markup) here are about the French army, the Ansar Dine group, and the Northern Mali conflict in general. The event of the French intervention itself does not have a Wikipedia article.

For each of these textually described events in the year pages, we manually inspected YAGO and Event KB as to whether they cover the event as an explicit entity or not. For the 9811 event descriptions in the month pages, we inspected a random sample of 50.
 Results: The coverage figures are shown in Table 2. For both ground-truth sets ( year and month pages of Wikipedia), Event KB shows more than 3 times higher coverage. This demonstrates the ability of our approach to populate a high-quality knowledge base with emerging and long-tail events.
The total number of events that Event KB acquired for the relevant time period is 10 times higher than what YAGO could extract from semistructured elements in Wikipedia. Table 3 shows two sample Event KB events, along with their semantic annotations, entities, and time spans.
In this section, we present experiments to evaluate the output quality of the various components of our method-ology, in comparison with different baselines. We do this systematically by looking at our three main components sep-arately: labeling, grouping, and chaining.

Test Data. We prepared two test datasets: news ar-ticles from i) Wikinews and ii) news sources referenced in Wikipedia articles. Note that Wikinews ( en.wikinews.org is totally independent of Wikipedia. Moreover, we removed all semi-structured elements from Wikinews articles to cre-ate a plain text corpus. The Wikipedia-referenced news are a highly heterogeneous set, from a variety of newspapers and other online news providers. We, therefore, refer to this dataset as WildNews .

For the Wikinews dataset, we picked articles by starting from topic categories that denote named events. Such cat-egories are identified by matching years in their titles, e.g., FIFA World Cup 2006, War in South Ossetia (2008), G8 Summit 2005, etc. In total, we extracted around 70 named event categories from Wikinews, with a total of 800 articles. Some named events are simple such as 2010 Papal UK tour or Stanley Cup Playoffs 2007 , whereas others are complex and contain sub-events, e.g., 2010 Haiti earthquake or US War on Terror .

For the WildNews dataset, we start with the collection of Wikipedia articles listed in the Wikipedia Current Events portal ( http://en.wikipedia.org /wiki/Portal:Current_events All news items cited by these Wikipedia articles with ex-ternal links are crawled and together constitute the news corpus. This corpus has 800 news for 26 named events.
Ground Truth. The way we derived the news arti-cles from named event categories already provides us with ground truth regarding the grouping and chaining of articles. However, the data does not come with semantic labels for populating a knowledge base. To this end, we have randomly chosen a subset of 3 news articles per named event, a total of 210 articles for the Wikinews data and a total of 78 articles for the WildNews data. These samples were manually la-beled with one or more semantic classes from WordNet tax-onomy (which is integrated in the YAGO knowledge base). For example, the news article Bomb blasts kill several in Iran is labeled with WN_bombing, WN_death, WN_conflict .
Methods under Comparison. 1. Labeling. Our methodology that uses statistical language models to find semantic labels of news articles is compared with the tf  X  idf based cosine similarity. 2. Grouping. We compare our Greedy and Random-ized methods (see Section 5) against several baselines: k-means clustering, hierarchical clustering, METIS [12] and the Storyline detection [21] method. All features (text, time, entities, types) are fed into the following flat distance mea-sure that is used for the baselines: d ( n i ,n j ) =  X   X  d text ( n i ,n j ) +  X   X  d time ( n  X   X  d ent ( n i ,n j ) +  X   X  d type ( n i ,n j )
The reported results in the experiments are obtained by uniform weighting. We varied these parameters to study their sensitivity. This led to small improvements (up to 5% in precision) in some cases and small losses in other cases. Hence, we only report results for the uniform setting. a. k-means clustering is run over both datasets with different k values. For each k value, the algorithm is run 10 times with different random centroid initializations. k-means achieved the best results, when k is set to the real number of the named events in the datasets. Thus, we set k = 70 for the Wikinews, k = 26 for the WildNews dataset. b. Hierarchical agglomerative clustering (HAC) , unlike flat clustering algorithms, can find clusters with differ-ent levels of granularity. We tried different linkage criteria: single-link (SLINK), complete link (CLINK), unweighted pair group method average (UPGMA), and weighted pair group method average (WPGMA). WPGMA achieved the best results for our settings. c. METIS [12] is a graph partitioning tool that first coarsens a graph and then applies partitioning algorithms to it. METIS takes k number of partitions as input. We incrementally changed k to see how clustering performance change. METIS can have the best result, similar to k-means, when k is close to the number of real clusters. Thus, we set k = 70 for the Wikinews, k = 26 for the WildNews dataset. d. Storyline detection [21] is a method to find story chains in a set of news articles returned by a query. Repre-senting news articles as nodes of a graph, the method applies two steps to compute chains: It first finds the minimum set of dominating nodes of the graph. Second, it defines the dominating node with the earliest time stamp as a root. Other dominating nodes are combined by a directed Steiner tree algorithm. The edge weights in the multiview graph are induced via the flat distance measure mentioned above. Note that Storyline detection is designed to return one story chain at a time. We modified the method to apply it repeat-edly for each dominating node. Thus, it can find multiple chains in the test data without any querying. 3. Chaining. We compare the methods that can find dependencies between news articles. Among the models we introduced so far, Greedy, Randomized, and Storyline de-tection can find temporal dependencies. Thus, these are the only models used for chaining comparisons.
As the labels computed by tf  X  idf cosine similarity and our LM-based method are ranked, we use the notion of pre-cision@k to compare the k top-ranked labels against the ground-truth set of labels: precision @ k = ( P j is the position, and r j = 1, if the result at the j th is correct and r j = 0 otherwise.

We define recall@k analogously: recall @ k = ( where j is the position, and n is the number of ground-truth labels. Although n is usually close to k , it can be smaller than k for certain news articles that have only few true labels. In this case, one may suspect that the models can easily reach 100% recall. However, in practice, none of the methods compared in this paper was able to reach 100% recall for the top-5 labels.
To compare different methods for grouping news articles into events, we look at pairs of news articles. We have ground-truth statements stating which articles belong to the same group (i.e., named event) and which ones are in differ-ent groups. Thus, we can compare the output of different kinds of grouping algorithms against the ground truth. We define precision and recall as follows.

Precision: the estimated probability of pairs ( n i ,n j ) of news articles being in the same ground-truth event given that they are assigned to the same group.

Recall: the estimated probability of pairs ( n i ,n j ) of news articles being assigned to the same group given that they come from the same ground-truth event.

Let G M and G T denote the groups for method M or ground truth T , respectively. Then we compute (micro-averaged) precision and recall as:
For evaluating the quality of ordering events on the time-ordering of their corresponding groups by method M and the ordering of their true events in the ground-truth data T . We refer to these orderings as C M (chaining by M ) and C
T (chaining in T ). So ( n i ,n j )  X  C T means that there are ground-truth groups G T ,G 0 T such that n i  X  G T , n j  X  G and G T is connected to G 0 T by a directed  X  X appened-before X  edge. We compute precision and recall analogous to group-ing. Instead of using G M and G T groups for method M or ground truth T , we use C M (chaining by M ) and C (chaining in T ) in the formulas.

For all three tasks  X  labeling, grouping, chaining  X  our very primary goal is high precision as we aim to populate a high-quality knowledge base. Nonetheless, recall is also important for high coverage of events. The combined quality is usually measured by the harmonic mean of precision and recall, the F1 score: F 1 = (2  X  precision  X  recall ) / ( precision + recall ).
We compared our method to the baseline of using tf  X  idf based cosine similarity. Both methods mapped the 78 Wild-News and 210 Wikinews news articles, for which we had ground-truth labels, to Wordnet semantic classes through Wikipedia event categories (Section 3). The top-5 semantic classes of each of the two methods are compared against the ground-truth classes. Table 4 shows precision and recall for different ranking depth k .

We see that our method substantially outperforms the cosine-similarity baseline.
We ran flat clustering (k-means), multi-level clustering (METIS), hierarchical clustering (HAC), Storyline, Greedy, and Randomized on the Wikinews and the WildNews datasets. The final clusters produced by the methods are automati-cally evaluated by comparing them to the ground truth as explained in Section 7.2.

Although we gave k-means and METIS the  X  X racle X  ben-efit of choosing k to be exactly the number of true events in the ground truth, they achieve inferior precision for both datasets. k-means obtained 62% precision, METIS 59%, and HAC 58% for the Wikinews dataset. For the WildNews dataset, k-means obtained 68% precision, METIS 71%, and HAC 50%. As precision is the crucial property for knowledge base population, k-means, METIS, and HAC are not suit-able methods to populate knowledge bases for our setting.
The results for the remaining methods are shown in Ta-ble 5. Storyline prioritizes precision at the expense of poor recall. Although Storyline achieves a good precision for the WildNews dataset, it loses on F1 score due to low recall. Our methods, especially Greedy, yield similarly high preci-sion at much higher recall. This results in roughly doubling the F1 scores of Storyline.
 Storyline .79 .10 .18 . 91 .15 .26 Greedy . 80 . 31 . 45 . 91 . 38 . 54 Randomized .79 .29 .42 .77 .26 .39 Table 5: Precision, recall, and F1 scores for grouping. Storyline .94 .32 .47 .96 .29 .45 Greedy . 96 . 77 . 85 . 97 . 67 . 79 Randomized .93 .71 .81 .94 .44 .60 Table 6: Precision, recall, and F1 scores for chaining.
We compare the methods that can find ordering depen-dencies between news articles. This rules out METIS, k-means, and HAC, and leaves us with the Storyline detection method as our competitor. As Table 6 shows, all methods achieve similar precision for chaining experiments for both datasets. However, Greedy and Randomized methods attain much higher F1 scores than Storyline.
We have three main observations on our experiments: 1. The baselines METIS, k-means and HAC group news articles in an overly aggressive manner, resulting in clusters that unduly mix different events. Thus, they yield poor precision values. The Storyline method, on the other hand, favors pure clusters and is conservative in its chaining. This leads to low recall. In contrast, Greedy and Randomized methods exploit the rich features encoded in MVAG model well and jointly infer groups and chains, which results in high precision values and the best F1 scores for both datasets.
Considering that Randomized requires simulated anneal-ing parameters as input, and those parameters may vary based on the news corpus, Greedy is the most practical parameter-free method with very good overall grouping and chaining performance. 2. All methods except Randomized perform well on group-ing for the WildNews dataset, which seems surprising. The reason is that WildNews articles are longer than Wikinews articles and contain more entities. The average number of entities per article is 19 for Wikinews and 26 for WildNews. This observation suggests that the semantic features like en-tities in articles boost the grouping performance. 3. All methods perform better on the Wikinews dataset on chaining. The reason is that WildNews articles span a much shorter time period than Wikinews articles, and many articles have overlapping time stamps, which degrades the chaining performance. This observation implies that chain-ing is more difficult when news articles have nearby or over-lapping timestamps.

The data of our experimental studies is available on http://www.mpi-inf.mpg.de/yago-naga/evin/ . Ontological event extraction: Knowledge bases such as YAGO [1, 11], DBpedia [4], or freebase.com contain entities of type event, fine-grained classes to which they belong, and facts about them. However, the coverage is fairly limited. The rule-based method by [14] has recently shown how to increase the number of named events and temporal facts that can be extracted from Wikipedia infoboxes, lists, article titles, and category names. However, all these approaches can capture a new event only after it has a sufficently rich Wikipedia article (with infobox, categories, etc.). Story mining from news: There is a large amount of work on topic detection and story mining on news, e.g., [5, 18, 21, 23, 15, 6]. However, the events found by these systems are not ontological events, for which we require canonical repre-sentations and semantic typing, clean enough for populating a knowledge base.
 Graphs for events: Identifying events in news streams has been addressed by modeling news articles as graphs of enti-ties [3] or as graphs of keywords [2]. Both methods identify dense clusters in graphs. However, an event here is merely a set of temporally co-occurring entities and/or keywords. Thus, events are implicitly represented; they are not canon-icalized and cannot be used to populate a knowledge base. General graph algorithms: Attributed graphs have been used by [25, 19], for purposes unrelated to our topic. Graph coarsening has been pursued by [12, 17] for plain graphs in the context of graph-cut algorithms. Our setting is different by being based on multi-view attributed graphs (MVAG X  X ). We developed novel methods for coarsening with a loss func-tion specifically designed for our problem of grouping and chaining the nodes of an MVAG.
 Graph summarization: Graphs can be summarized in terms of motif distributions such as frequent triangles or frequent subgraphs, or by showing aggregated views of a graph. The latter is related to our coarsening method. [20] presents the Snap-k operator for summarizing an attributed graph in terms of k groups. The key differences to our setting are that this work considers only graphs with one kind of edges (as opposed to our notion of MVAG X  X ) and that the user determines the number of groups. [16] addresses the task of lossy graph compression by means of summarization using the MDL principle. In contrast to our setting, this work is limited to only plain graphs.
 Mapping documents to Wikipedia categories: There are numerous studies on using Wikipedia categories for clus-tering documents, e.g., [10, 22, 8]. These methods exploit the semantic relatedness of documents to Wikipedia con-cepts and the link structure of Wikipedia categories. We use statistical language models to map news articles to YAGO classes, via specific Wikipedia categories for events.
The methods presented here fill an important gap in the scope and freshness of knowledge bases. We tap into (the latest) news articles for information about events, and dis-till the extracted cues into informative events along with temporal ordering. Our experiments demonstrated that our methods yield high quality, compared to a variety of baseline alternatives, and can indeed populate specific event classes in a knowledge base with substantial added value. Use-cases of this contribution include strengthening the entity-aware functionality of search engines, and also using the addition-ally acquired knowledge of events for smarter recommenda-tions, e.g., when users browse social media, and for better summarization. Finally, the events distilled by our methods are themselves entities serving as background knowledge for better acquiring more events and other knowledge from news. Jilles Vreeken is supported by the Cluster of Excellence  X  X ultimodal Computing and Interaction X  within the Excel-lence Initiative of the German Federal Government. [1] F. M. Suchanek, et al. Yago: A Core of Semantic [2] M. K. Agarwal, et al. Real Time Discovery of Dense [3] A. Angel, et al. Dense Subgraph Maintenance under [4] S. Auer, et al. DBpedia: A Nucleus for a Web of Open [5] A. Das Sarma, et al. Dynamic Relationship and Event [6] Q. Do, et al. Joint Inference for Event Timeline [7] J. R. Finkel, et al. Incorporating Non-local [8] E. Gabrilovich, et al. Overcoming the Brittleness [9] P. D. Gr  X  unwald. The Minimum Description Length [10] X. Hu, et al. Exploiting Wikipedia as External [11] J. Hoffart, et al. Yago2: A Spatially and Temporally [12] G. Karypis, et al. Multilevel Graph Partitioning [13] S. Kirkpatrick, et al. Optimization by Simulated [14] E. Kuzey, et al. Extraction of Temporal Facts and [15] W. Lu, et al. Automatic Event Extraction with [16] S. Navlakha, et al. Graph Summarization with [17] I. Safro, et al. Advanced Coarsening Schemes for [18] D. Shahaf, et al. Connecting the Dots Between News [19] A. Silva, et al. Mining Attribute-Structure Correlated [20] Y. Tian, et al. Efficient Aggregation for Graph [21] D. Wang, et al. Generating Pictorial Storylines Via [22] P. Wang, et al. Using Wikipedia Knowledge to [23] R. Yan, et al. Evolutionary Timeline Summarization: [24] C. Zhai. Statistical Language Models for Information [25] Y. Zhou, et al. Graph Clustering Based on
