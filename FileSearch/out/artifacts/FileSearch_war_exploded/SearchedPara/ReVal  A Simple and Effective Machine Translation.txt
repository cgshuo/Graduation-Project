 Deep learning approaches have turned out to be successful in many NLP applications such as para-phrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine trans-lation (Mikolov et al., 2013a). While dense vec-tor space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation mea-sure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (Hochreiter and Schmidhu-ber, 1997) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed represen-tations of sentences using distributed representa-tions of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM frame-work (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. Many metrics have been proposed for MT eval-uation. Earlier popular metrics are based on n-gram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and para-phrase databases. However, system-level cor-relation with human judgements for these met-rics remains below 0.90 Pearson correlation co-efficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER-0.821).

Recent best-performing metrics in the WMT-14 metric shared task (Mach  X  acek and Bojar, 2014) used a combination of different metrics. The top performing system D ISKO TK-P ARTY -T UNED (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit (Gim  X  enez and M ` arquez, 2010). The metric com-putes the number of common sub-trees between a reference and a translation using a convolution tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhat-tacharyya, 2014), uses linear interpolation of dif-ferent metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Univer-Recently, Guzm  X  an et al. (2015) presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score.

In this paper we propose a compact MT eval-uation metric. We hypothesize that our model learns different notions of similarity (which other metrics tend to capture using different metrics) using input, output and forget gates of an LSTM architecture. Recurrent Neural Networks allow processing of arbitrary length sequences, but early RNNs had the problem of vanishing and exploding gradi-ents (Bengio et al., 1994). RNNs with LSTM (Hochreiter and Schmidhuber, 1997) tackle this problem by introducing a memory cell composed of a unit called constant error carousel (CEC) with multiplicative input and output gate units. Input gates protect against irrelevant inputs and output gates against current irrelevant memory contents. This architecture is capable of capturing important pieces of information seen in a bigger context. Tree-LSTM is an extension of simple LSTM. A typical LSTM processes the information sequen-tially whereas Tree-LSTM architectures enable sentence representation through a syntactic struc-ture. Equation (1) represents the composition of a hidden state vector for an LSTM architecture. For a simple LSTM, c t represents the memory cell and o t the output gate at time step t in a sequence. For Tree-LSTM, c t represents the memory cell and o t represents the output gate corresponding to node t in a tree. The structural processing of Tree-LSTM makes it better suited to representing sentences. For example, dependency tree structure captures syntactic features and model parameters the importance of words (content vs. function words). Figure 1 shows simple LSTM and Tree-LSTM architectures. Figure 1: Tree-LSTM (left) and simple LSTM (right) We represent both the reference ( h ref ) and the translation ( h tra ) using an LSTM and predict the similarity score  X  y based on a neural network which considers both distance and angle between h ref where,  X  is a sigmoid function,  X  p  X  is the estimated probability distribution vector and r T = [1 2 ...K ] . The cost function J (  X  ) is defined over probability distributions p and  X  p  X  using regularised Kullback-Leibler (KL) divergence.
 In Equation 3, i represents the index of each train-ing pair, n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows: for 1 j K , where, y 2 [1 , K ] is the similarity score of a training pair. For example, for y = 2 . 7 , p T = [0 0 . 3 0 . 7 0 0] . In our case, the similarity score y is a value between 1 and 5.

For our work, we use glove word vectors (Pen-nington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network im-Training is performed on the data computed in Section 5. The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The compositional parameters for our Tree-LSTM systems with memory di-mensions 150 and 300 are 203,400 and 541,800, respectively. The training is performed for 10 epochs. System-level scores are computed by ag-gregating and normalising segment-level scores. As we do not have access to any dataset which provides scores to segments on the basis of trans-lation quality, we used the WMT-13 ranks corpus to automatically derive training data. This corpus is a by-product of the manual systems evaluation carried out in the WMT-13 evaluation. In the eval-uation, the annotators are presented with a source segment, the output of five systems and a reference translation. The annotators are given the following instructions:  X  You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed)  X . Using the WMT-13 ranked corpus, we derived a corpus where the reference and corresponding translations are assigned simi-larity scores. The fact that ties are allowed makes it more suitable to generate similarity scores. If all translations are bad, annotators can mark all as rank 5 and if all translations are accurate, an-notators can mark all as rank 1. The selection of the WMT-13 corpus over other WMT workshops is motivated by the fact that it is the largest among them. It contains ten times more ranks than WMT-12 and three to four times more than WMT-14. This also makes it possible to obtain enough refer-ence translation pairs which are evaluated several times.
Our hypothesis is that if a translation is given a certain rank many times, this reflects its simi-larity score with the reference. A better ranked translation among many systems will be close to the reference whereas a worse ranked translation among many systems will be dissimilar from the reference. To remove noisy pairs, we collect ref-erence translation pairs below a certain variance only. We determined appropriate variance values using Algorithm 1 below for n = 3 , 4 , 5 , 6 , 7 and 8 , separately. The computed variance values are given in Table 1. Table 1: Variances computed using Algorithm 1 Algorithm 1 Variance Computation In Algorithm 1, the kendall function calculates Kendall tau correlation using the WMT-13 hu-man judgements. We select a set for which the correlation is computed using the annotations for which scores are available in the corpus ( prs ). In other words, the corpus acts as a scoring function for the available reference translation pairs, which gives a similarity score between a reference and a translation. We selected pairs below the variance values obtained for n = 4 , 5 , 6 , 7 and 8 . Finally, all the pairs are merged to obtain a set (L). Apart from this set, we created three other sets for our experiments. The last two also use the SICK data (Marelli et al., 2014) which was developed for evaluating semantic similarity. All four sets are described below: Table 2 shows the number of pairs extracted for We evaluate our approach trained on the four dif-ferent datasets obtained from WMT-13 (as given in Table 2) on WMT-14. Table 3 shows system-level Pearson correlation obtained on different lan-guage pairs as well as average Pearson correlation (PAvg) over all language pairs. The last column of the table also shows average Spearman corre-lation (SAvg). The 95% confidence level scores are obtained using bootstrap resampling as used in the WMT-2014 metric task evaluation. The scores in bold show best scores overall and the scores in bold italic show best scores in our variants.
In Table 3 and Table 4, the first section (L+Sick(lstm)) shows the results obtained us-ing simple LSTM (layer 1, hidden dimension 50, memory dimension 150, compositional pa-rameters 203400). The second section shows the scores of our Tree-LSTM metric trained on different training sets and dimensions. Di-mensions are shown in brackets, e.g L(50,150) shows the results on set  X  X  X  with the hidden dimension 50 and the memory dimension 150.
 L+Sick(mix) shows results of combining the two systems: L+Sick(50,150) and L+Sick(100,150). For the sentences longer than 20 words, the sys-tem uses scores of L+Sick(100,150) and scores of L+Sick(50,150) for the rest. The third sec-tion shows the best three overall systems from the WMT-14 metric task. The fourth section in Table 3 shows the systems from the WMT-14 task which obtained best results for certain languages but do not preform well overall. The last section in Tables 3 and 4 shows systems implementing BLEU (or variants for the segment level) and METEOR in the WMT-14 metric task.

Tables 3 and 4 contain a deluge of evaluation data, mainly to explore the effect of different training data and model parameter settings for our models. The main messages can be summarised as follows: 1. Tree LSTM models significantly outperform the LSTM model (L+Sick(lstm) and L+Sick(50,150) have the same data and parameter settings). 2. For Tree-LSTM models different parameter settings have only a minor impact on performance (in fact only for a few language pairs (e.g. hi-en at system-level, L+Sick(100, 300) and L+Sick(100,150)) results are statistically signifi-cantly different). This is reassuring as it indicates that the metric is not overly sensitive to exten-sive and delicate parameter tuning. 3. For the system level evaluation Tree-LSTM models are fully competitive with the best of the current com-plex models that combine many different metrics, substantial external resources and may require a significant amount of feature engineering and tun-ing. 4. For the segment level evaluation our met-ric outperforms BLEU based approaches and the approaches. We investigate this further below.
Tables 3 and 4 show that set L is able to obtain similar results compared to set LNF even though we filter out almost half of the pairs. Table 3 shows that for L+Sick(50, 150) and L+Sick(mix), we ob-tained an average second best Pearson correlation and best Spearman correlation coefficient. We also obtained better results for the Russian-English and Czech-English language pairs compared to any other systems in the WMT-14 task.

We also evaluate our setting L-Sick(50,150) on the WMT-12 task dataset. Our metric performs best for two out of four language pairs and best overall at the system level with 0.950 and 0.926 Pearson and Spearman correlation coefficient, re-spectively. At the segment level, we obtained 0.222 Kendall tau correlation which was better than seven out of the total ten metrics in the WMT-12 task.

One of the reasons for the difference in segment-level and system-level correlations is that Kendall Tau segment-level correlation is calcu-lated based on rankings and does not consider the amount of difference between scores. Here is an example similar to that given in (Hopkins and May, 2013). Suppose four systems produce the translations T0, T1, T2 and T3. Suppose we have two metrics M1 and M2 and they produce scores and rankings as follows. GS represents the correct ranking and scores; Scores are in a scale [0, 1] with a higher score indicating a better translation: M1: T0 (0.10), T3 (0.71), T1 (0.72), T2 (0.73) M2: T1 (0.71), T0 (0.72), T2 (0.73), T3 (0.74) GS: T0 (0.10), T1 (0.71), T2 (0.72), T3 (0.73)
Certainly, M1 produces better scores and rank-ing than M2. But, Kendall Tau segment-level correlation is higher for M2. (There are four concordant pairs in the M1 rank and five in the M2 rank.) Therefore, if a metric does not scale well as per the quality of translations, it may still obtain a good Kendall Tau segment-level corre-lation and a better metric may end up getting a low correlation. Another reason for the discrep-ancy between segment and system-level scores may be a low agreement on annotations. For the WMT-14 dataset, inter-annotator and intra-annotator agreement were 0.367 and 0.522. These problems should not occur with Pearson corre-lation at the system level because system-level scores are calculated using more sophisticated ap-proaches (Koehn, 2012; Hopkins and May, 2013; Sakaguchi et al., 2014). For example, Hopkins and May (2013) model the differences among annota-tors by adding random Gaussian noise. We conclude that our dense-vector-space-based ReVal metric is simple, elegant and effective with state-of-the-art results. ReVal is fully competitive with the best of the current complex alternative approaches that involve system combination, ex-tensive external resources, feature engineering and tuning.
 The research leading to these results has received funding from the People Programme (Marie Curie Actions) of the European Unions Seventh Frame-work Programme FP7/2007-2013/ under REA grant agreement no. 317471 and the EC-funded project QT21 under Horizon 2020, ICT 17, grant agreement no. 645452.
