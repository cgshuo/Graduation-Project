 Due to imprecise query intention, Web database users often use a limited number of keywords that are not directly re-lated to their precise query to search information. Semantic approximate keyword query is challenging but helpful for specifying such query intent and providing more relevant answers. By extracting the semantic relationships both be-tween keywords and keyword queries, this paper proposes a new keyword query approach which generates semantic ap-proximate answers by identifying a set of keyword queries from the query history whose semantics are related to the given keyword query. To capture the semantic relationships between keywords, a semantic coupling relationship anal-ysis model is introduced to model both the intra  X  and inter  X  keyword couplings . Building on the coupling re-lationships between keywords, the semantic similarity of d-ifferent keyword queries is then measured by a semantic ma-trix. The representative queries in query history are identi-fied and then a priori order of remaining queries correspond-ing to each representative query in an off-line preprocessing step is created. These representative queries and associated orders are then used to expeditiously generate top-k ranked semantically related keyword queries. We demonstrate that our coupling relationship analysis model can accurately cap-ture the semantic relationships both between keywords and queries. The efficiency of top-k keyword query selection al-gorithm is also demonstrated.
 H.2.4 [ Database Management ]: Systems X  Query process-ing ; H.2.8 [ Database Management ]: Database applica-tions X  Data mining Algorithms, Performance, Design, Experimentation Web database, keyword query, coupling relationship analy-sis, top-k selection
With the increasing of complexity and size of Web databa-ses 1 , keyword search plays an important role in obtaining the information needed from Web databases. In practice, however, it is difficult for lay users to obtain complete and effective information since average people usually have insuf-ficient knowledge about the structure and contents of Web databases. Accordingly, one often has imprecise ideas about what exact keywords he/she should use for searching and finds it hard to formulate an appropriate query by using on-ly a few keywords. As a result, an inadequate answer, or no answer, is often returned when the query is too selective or query keywords are not properly selected. In such a con-text, a user has to reformulate queries several times before meaningful query results are received, which is often a time-consuming exercise. Therefore, it is important to produce a list of queries that are semantically related to the original query so that a user can select and view the answers to a query by choosing it from a proper list. Additionally, pro-viding related queries is also very helpful for the scientific database users, especially the people who is unfamiliar with a new research field.

The challenge in selecting semantically related queries is to understand the semantics of the original query and to measure the semantic similarity between them. Several ap-proaches have been proposed to deal with the issue of key-word search over relational databases [1,3,14,15,20]. Their basic idea is to extract a set of joining trees of tuples. A join-ing tree of tuples is formed by matched tuples, which con-tain the specified keywords in their text attributes, are inter-connected through primary-foreign-key references. However, most of the existing work neglects the coupling relationship-s between keywords when searching the joining tuple trees, rather treating them independent. As a result, the semantic coupling between keywords is overlooked.

However, in the real world, there are various coupling re-lationships [8] existing between objects, which have been shown valuable to be incorporated into analysis such as doc-ument term semantic analysis [10], clustering [21] and clas-sification [23]. Similarly, keywords embedded in a query are
Web da tabase refers to a non-local online database that can be accessible by a web form based interface. coup led in terms of co-occurrences and semantic relation-ships. The meaning of a keyword is often associated with the meaning of the others, we call intra  X  couplings between keywords in a query. The semantically connected keyword set in a query jointly express the user query intention. In addition, coupling relationships also exist between keywords from different queries, we call inter  X  couplings of keyword-s. Such cross-query keyword relationship contributes to the matching between a query and others. On top of this obser-vation, in this paper, we propose a new keyword query ap-proach which incorporates the keyword semantic couplings for approximate query. It incorporates the coupling analy-sis [22] into keyword coupling relationship and query seman-tic similarity analysis. It then leverages the query semantic similarities to extract the top-k queries from query histo-ry that are related to a given query. We will use the toy example below to motivate and provide an overview of our approach.

Example 1. As shown in Figure 1, a DBLP database consisting of three relations: Authors , P apers and W rite , connected by primary-foreign-key relationships.
 Figure 1: Example of relational database DBLP A user may issue the following keyword queries: Q 1 :-DBLP(R. Agrawal, sequential patterns) Q 2 :-DBLP(association rules, apriori algorithm)
On receiving the query Q 1 , a classic keyword search ap-proach may provide a set of joining tree of tuples contain-ing keywords  X  X . Agrawal X  and  X  X equential patterns X  as the query results. For example, a joining tree of tuples a 2 w 2 1 p 1 is an answer for query Q 1 . It is clear that there ex-ists a relationship between keywords  X  X . Agrawal X  and  X  X e-quential patterns X  because they appear in the same query. The query mostly probably indicates that the user wants to find the paper related to  X  X equential patterns X  proposed by the author  X  X . Agrawal X . In real applications, the user who submitted query Q 1 may also be interested in tuples containing  X  X ssociation rules X  since the  X  X equential pattern-s X  and  X  X ssociation rules X  are proposed by the same author (i.e., R. Agrawal), and they are much related with each other in the data mining research field. However, the traditional keyword search approaches cannot provide the answer con-taining keywords  X  X ssociation rules X  such as a 1 1 w 1 1 p returned by Q 2 . This example shows coupling relationships exist between keywords in a query and between queries.
This paper proposes a solution for extracting keyword semantic approximate query results by selecting the top-k queries from query history that are semantically relat-ed to a given query. We first capture the intra-and inter-couplings between different pairs of keywords extracted from query history -log of past users keyword queries. The intra-and inter-couplings are then combined to generate the keyword coupling relationship. Note that, although key-word coupling relationships can be captured from other da-ta sources, in this paper we only rely on the query histo-ry that is directly related to user query intentions. With such keyword coupling relationships, we further measure the semantic similarity between different queries by building a semantic matrix, where the coupling relationships between keywords are reserved. As a result, given a query, the sys-tem provides a list of k queries from query history that are related to the given query, and a user can view the answers of related query by choosing it in the list.

Our contributions are summarized as follows: (1). A novel method is proposed to measure the keyword coupling relationships, which considers both the intra-and inter-couplings between different keywords within and across the queries. (2). A new keyword query similarity metric based on a semantic matrix is proposed, in which the keyword coupling relationships are reserved. (3). A top-k query selection algorithm, which is used to quickly select top-k related queries from query history, is presented.

The rest of this paper is organized as follows. Section 2 re-views some related work. Section 3 outlines an overview of our framework. Section 4 proposes the keyword coupling relationship measuring method while Section 5 describes the query semantic similarity measuring method. Section 6 presents a top-k query selection algorithm. The experiment results are presented in Section 7. The paper is concluded in Section 8.
Several methods have been proposed to handle keyword search in relational and XML database systems, and the popularity of keyword search is ongoing [24]. For the re-lational database, the related work can be classified into two main categories. The first is mainly based on Steiner trees, such as BANKS [1] and its extensions [12,20]. These approaches firstly model the database as a directed data graph, where nodes are tuples and the directed edges are foreign key references between tuples. A keyword query is then processed by traversing graph for searching minimal joining trees of tuples containing the query keywords. The second leverages Candidate network (CN), such as DBX-plorer [3], DISCOVER [15], and SPARK [18], to find the relevant answers. A candidate network is a joining network of tuples(JNTs), in which the tuples are inter-connected through primary-foreign-key relationships. The CN-based approaches generate all possible candidate networks follow-ing the database schema, and then identify a set of minimal total joining network of tuples (MTJNTs) based on CNs. For the XML database, the lowest common ancestors (L-CAs) [26] and its extensions [5, 17] are used for keyword search. In summary, the existing approaches mainly focus on finding MTJNTs or LCAs explicitly containing the speci-fied keywords and lack of considering the semantic relevance between answers and queries. As a result, they cannot iden-tify the results from which some MTJNTs or LCAs may also be very relevant to a query in semantic terms, even though they do not explicitly contain the query keywords.
In recent years, tentative work on keyword semantic un-derstanding and approximate query has been undertaken. In [19 ], the transformation rules are manually defined used for keyword query integration and the local results are an-alyzed used for finding relevant answers. In [6], the meta-data of database is used for translating keyword queries in-to meaningful SQL queries that describe the intended query semantics. In [24], the data structural semantics are exploit-ed and employed to reformulate the initial query. Although keyword semantics have been taken into consideration, most of the existing approaches usually assume that keywords in a query are independent of one another, but in reality cou-pling relationships exist between objects such as keywords and terms as shown in [8,10].

The concept of coupling relationship has recently been in-troduced to cater for the interactions within and between attributes. A number of studies [8, 10, 21] have proven to be very effective for capturing the implicit relationships for machine learning and data mining tasks such as clustering and document analysis. In this paper, we incorporate this idea into keyword coupling and query similarity analysis and then leverage the query similarity to find the top-k queries from query history that are related to a given query. The top-k retrieval problem has been studied in several situation-s, such as the view-based top-k query against the relational database [11] and the top-k preferences retrieval in context of high dimensions [25]. Given a set of objects O , the ba-sic idea of top-k retrieval is to quickly find the k objects in O with the highest scores with respect to a given query by considering the monotonic ranking functions defined on a subset of the attributes of the set O . Note that, keyword search issues are not investigated in this paper. Our focus is on keyword coupling relationship analysis, query semantic similarity measure and top-k related query selection.
This paper proposes a two-step processing approach to address this problem. The framework is shown in Figure 2. Figure 2: Framework of keyword semantic approxi-mate query
The first step occurs offline. It analyses query history of all users already in the system and then captures the cou-pling relationships between keywords. Firstly, all distinct keywords in query history are extracted, following which the intra-and inter-couplings between different pairs of key-words can be calculated by leveraging the correlation anal-ysis method. Consequently, the keyword intra-and inter-UID QID Keyword s U 1 Q 13 classi fication, clustering, KDD U 2 Q 24 associa tion rules, clustering, data analysis U 3 Q 35 associa tion rules, decision tree, prediction
U 4 Q 43 classi fication, decision tree, KDD coup ling is combined into a coupling relationship to reflect the semantic relevance between keywords. Based on the keyword coupling relationship, we further use the semantic matrix to measure the similarity between keyword queries. To reduce the online computation time, a few representa-tive queries are selected and the remaining queries in query history are formed into several ordered lists, hereafter called  X  X rders X . Each order corresponds to a representative query and the queries in each order are ranked according to their similarities to that representative query.
 The second step occurs online when a user makes a query. It first decomposes the input query into several distinct key-words. Based on coupling relationships between keywords, it then computes the semantic similarities between the giv-en query and representative queries. Lastly, a list of top-k related queries is returned using Threshold Algorithm (TA) and priori orders. The user can view the results of the re-lated query by selecting it from the list.
Building on the term coupling analysis in document anal-ysis [10], to evaluate the keyword coupling relationship, the query history is used as the knowledge source in this paper. This section first introduces the query history and its prune strategy, and then presents how to measure the keyword coupling relationships based on the query history. De nition 1. (Keyword query): A input keyword query Q over database D is an ordered list of distinct keywords, or a topical phrase, depending on the decomposition.
De nition 2. (Query history): A query history W is con-sisted of { ( U 1 , Q 1 , K 1 ) , ..., ( U n , Q n , K n ) der, where U i is a session ID (a session is a duration started from a user connects to the Web database and ended to the user disconnects), Q i is a query ID, and K i is a query keywords list.

To guarantee the quality of query history, the following prune strategy is used: (1) remove the queries with emp-ty results; (2) reserve only the keyword query that is most related to the query intention. A user may issue several queries in one session that progress from being general in nature to being more concrete, finally stopping at a query that returns meaningful results. Therefore, it is the last keyword query in such a refinement sequence that is most related to the query intention and should be reserved [9]; (3) decompose the remaining queries into several distinct key-words and normalize them using text split and analysis tools such as AlchemyAPI and Wikipedia [16] (although it would be in teresting to process the natural language and normal-ize the keywords, it is beyond the scope of this paper). An example of pruned query history is shown in Table 1.
In Information Retrieval, two terms are considered seman-tically related if they frequently co-occur in the same docu-ments of the document set. Similarly, each keyword query in query history is considered as a document. Then, the frequency of co-occurrence of a pair of keywords ( k i , k pearing in the same queries of query history can be measured by Jaccard coefficient [7] as follows, where, W ( k i ) and W ( k j ) represent the queries in query his-tory in which k i and k j appears, respectively. Based on Equation (1), we can give the definition of intra  X  coupling of keywords.

De nition 3. (Intra-coupling of keywords): Keywords k i and k j are intra-related if they co-occur in at least one key-word query Q i ( Q i  X  W ), the intra-coupling between them in W is defined as, where, J ( k i , k j ) is defined as Equation (1).
Since keyword k i or k j may also co-occur with other key-words in the queries, we need to normalize the intra-coupling between k i and k j by dividing the total number of intra-couplings between k i and other keywords. Thus, the intra-coupling between k i and k j is finally computed as follows, where, n is the number of all distinct keywords in W .
For each pair of keywords ( k i , k j ), we have  X  IaR ( k and  X  er due to the different dominators. The keyword intra-coupling relationship calculating algorithm is shown in Al-gorithm 1. Note that, since J ( k i , k j ) = J ( k j , k of  X  IaR ( k i , k j | W ) is symmetric, therefore we need to only compute the upper-half of the matrix of  X  IaR ( k i , k j Algorithm 1 (line 4-5). Table 2 shows the intra-coupling ma-trix of keywords extracted from Table 1. For simplicity, we use CA, CU, KD, AR, DA, DT, and PR to denote the key-words classif ication , clustering , KDD , association rules , data analysis , decision tree , and prediction , respectively.
The intra-coupling reflects the correlation between the keywords in case of they are co-occurring in the same queries. Besides the intra-coupling, the keywords may also be inter-related via their common keywords across queries. In par-ticular, the keywords, which have never co-occurred in the same queries (that means they only appeared in separate queries), may also inter-related in semantic, such as  X  X lassi-fication X  and  X  X ssociation rules X  are inter-related by the key-word  X  X lustering X  and  X  X ecision tree X . In this paper, we say this type of correlation between keywords is inter  X  coupling of keywords. We next present how to capture the inter-coupling between keywords.
 Algo rithm 1: Keyword intra-coupling calculation
Input : query history W , set of all distinct keywords K
Output : IaRMatrix. 1 IaRMatrix=null. 2 for i = 1 to n  X  1 do 3 for k = i + 1 to n do 4 IaRM atrix[ i ][ j ]=J( K [ i ] , K [ k ]). 5 IaRMatrix[ k ][ i ]=IaRMatrix[ i ][ k ]. 6 for m = 1 to n do 7 if m  X  = i then 8 Sum=S um+IaRMatrix[ i ][ m ]. 9 for j = 1 to n do 10 if j  X  = i th en 11 IaR Matrix[ i ][ j ]=IaRMatrix[ i ][ j ]/Sum. 12 Retu rn IaRMatrix.
 Table 2: Example of keyword intra-coupling matrix
The basic idea for capturing the inter-coupling between keywords is that if the sets of keywords co-occurred with k and k j are partly overlapped we say k i and k j are inter-related.

Given a keyword k i , all the keywords co-occurred with k in query history can be seen as the features associated with k . The inter-coupling between two keywords can be esti-mated by the commonality in the features associated with them. For example, given a keyword  X  X lassification X  in Ta-ble 1, a set of keywords  X  X lustering, KDD, decision tree X  is associated with it; while, a set of keywords  X  X lustering, da-ta analysis, decision tree, prediction X  is associated with the keyword  X  X ssociation rules X . Clearly, the overlapped key-words between two sets are  X  X lustering X  and  X  X ecision tree X . In this paper, we say these are common keywords, which mean that two keywords occurring in different queries are inter-related via their common keywords. According to this, the inter-coupling between k i and k j via a common keyword k can be defined as follows.

De nition 4. (Inter-coupling of keywords): Keywords k i and k j are inter-related if there is at least one common key-word k c such that  X  IaR ( k i , k c ) &gt; 0 and  X  IaR ( k hold but keywords k i and k j appear in different queries. The inter-coupling between keywords k i and k j via common keyword k c is defined as, between k i and k c , k j and k c , respectively.
Algori thm 2: Keyword inter-coupling calculating algo-rithm
Input : set of all keywords K in W , number of keywords
Output : IeRMatrix. 1 IeRMatrix=null. 2 for i = 1 to n  X  1 do 3 for j = 1 to n do 4 S  X  the set of common keywords between K [ i ] 5 m = | S | . 6 if S =  X  then 7 IeRMa trix[ i ][ j ]=0. 8 else 9 for k = 1 to m do 10 minv alue=min {  X  IaR ( K [ i ] , S [ k ]), 11  X  IaR ( K [ j ] , S [ k ]) } . 12 sum+= minvalue* w ( S [ k ]). 13 IeRMa trix[ i ][ j ]=sum/m. 14 Return Ie RMatrix.

It shou ld be pointed out that there is usually more than one common keyword between k i and k j and each one may have different importance/weight in query history. Thus, it is necessary to measure the importance of the common key-word. The intuition is that the greater the frequency of the keyword occurring in query history, the greater the number of the users interested in it, and thus the more importan-t the keyword is. A method that leverages this intuition is to count the frequencies of keywords appearing in query history, and then allow important coefficients to depend on these frequencies. Let QF ( k i ) represents the frequency of occurrence of keyword k i in query history and QF M ax the frequency of the most frequently occurring keyword. Con-sequently, the weight of k i , w ( k i ) can be defined as,
We then let S be the set of common keywords for k i and k , that is, S = { k c | (  X  IaR ( k i , k c ) &gt; 0  X   X  IaR Then, the inter-coupling between k i and k j , inter-related by all the common keywords in S , can be formalized as,  X  where, w ( k c ) is computed by Equation (5),  X  IeR ( k i is the inter-coupling between k i and k j related via the com-mon keyword k c , and | S | denotes the number of common keywords in S . Equation (6) means the inter-coupling be-tween k i and k j is measured by the average inter-couplings between k i and k j via all of their X  X  common keywords. If S =  X  , then  X  IeR ( k i , k j ) is zero. Note that, the correla-tion between two co-occurring keywords is also enhanced by their inter-coupling relationship. The keywords inter-coupling calculating algorithm is shown in Algorithm 2. Ta-ble 3 shows the inter-coupling matrix of keywords extracted from Table 1.
 Table 3: Example of keyword inter-coupling matrix Table 4: Keyword coupling relationship matrix
The coupling relationship between two keywords k i and k is the combination of intra-and inter-coupling of the two keywords, which is defined keyword coupling as follows,  X  SR ( k i , k j ) = { where,  X   X  [0 , 1] is the parameter to determine the weight of intra-and inter-coupling. It is clearly that the higher the coupling relationship, the more related is the two keywords. Note that, the Equation (7) would be intra-coupling if  X  = 0 while it would be inter-coupling if  X  = 1, that means the intra-and inter-coupling are the special cases of keyword coupling relationship.

Table 4 shows the coupling relationship matrix of all key-words extracted in Table 1. Here, we set  X  to 0.5, which means the intra-and inter-coupling play the same important role in measuring the keyword coupling relationship. From Table 4, we can see that the coupling relationship between keywords considering both of intra-and inter-coupling of keywords is more reasonable than that of only considering either intra-coupling or inter-coupling of keywords. For ex-ample, we consider a pair of keywords  X  X lassification X  and  X  X rediction X  (or  X  X ata analysis X ) in Table 1. If we only con-sider their intra-coupling, there is no relationship between them as showed in Table 2. But in reality,  X  X lassification X  and X  X rediction X (or X  X ata analysis X ) is closely related to each other in semantic and the relationship between them can be captured by our inter-coupling calculating algorithm. As a result, the coupling relationship between them would not be zero as showed in Table 4.
In information retrieval, cosine similarity is a commonly used similarity measure, defined on Vector Space Model. In this paper, each query can be treated as a document and the keyword can be treated as a term. Thus, we can adopt the cosin e similarity to quantify the semantic similarity between queries. The solution consists of the following 3 steps.
Step 1. Convert the keyword query into vector representation. Given a pair of queries Q i 1 and Q i 2 , we assume K be the set of all distinct keywords in Q i 1 and Q and n the number of keywords in K . We also let n = | K |  X  be an arbitrary but fixed order on the keywords appearing in K . K [ i ] refers to the i -th keyword of K based on the order  X . In the context of Q i 1 and Q i 2 , a vector representation of Q The i -th element of the vector corresponds to keyword K [ i ]. If K [ i ] appears among the keywords of Q i 1 then otherwise it is 0. Note that, since different pairs of queries usually contain different number of keywords, the cardinality of K is finite and varies depending on the compared queries.
For example, the queries Q 35 and Q 43 in Table 1 totally have five distinct keywords. We assume the order on them is association rules , decision tree , prediction , classif ication , and KDD . Then, the query Q 35 and Q 43 can be represented by the vector [1 1 1 0 0] and [0 1 0 1 1], respectively.
Step 2. Construct the semantic matrix. Given a pair of queries Q i 1 and Q i 2 , we also assume K be the set of all distinct keywords in Q i 1 and Q i 2 and n the number of keywords in K . The coupling relationships of all keywords in K can then be transformed into a semantic Matrix S K , which is a n  X  n matrix and each element S K ( i, j ) in it cor-responds to the coupling relationship between keywords k i and k j .

Step 3. Compute the semantic similarity between queries. The traditional VSM-based cosine similarity mea-suring method assumes the keywords are independent in queries and ignores the coupling relationships between them. To address the omission of semantic relationships between keywords in VSM, based on the semantic matrix S K con-structed in Step 2, each keyword query vector is transformed into a new feature vector query vector representation with the coupling relationships between keywords. Then, using this transformation the cor-responding kernel [4] of two query vector be written as,
Based on the query vector representations ( kernel k  X  ( Q i 1 , Q i 2 ), we can define the kernel-based cosine similarity between two queries as follows,
Usi ng Equation (9), the semantic similarity between each pair of queries in query history can be obtained. The matrix-es of similarities between different pairs of queries obtained by using traditional VSM-based cosine similarity (short for V-COS) and kernel-based cosine similarity (short for K-COS) algorithms are showed in Table 5 and 6, respectively.
From Table 5 and 6, we can find that the similarities of a specified query to other queries calculated by V-COS al-gorithm are usually same. For example, the similarities be-tween Q 24 to Q 13 and Q 35 are all 0.33 and the same to Q to Q 24 and Q 43 , and thus lead the queries are difficult to differ from each other. While, the similarities of them cal-culated by K-COS algorithm are different and more close to reality.
 Table 5: Query similarity matrix based on V-COS Table 6: Query similarity matrix based on K-COS Let Q be a given keyword query over database D and W be the query history. Based on the semantic similarities between different queries, the goal is to address the top-k query selection problem defined as, where,  X  k is a list of k keyword queries and n is the number of all queries in query history. The objective of the problem is to find a set of number k queries in query history that semantically related closely as possible to the given query.
A straightforward algorithm to find the top-k related queries is used to compare the similarities of the given query to al-l queries and then rank them according to the similarities. The time complexity is O ( n 2 logn ), where n is the number of all queries in query history. However, this complexity is un-acceptable for a large scale query history. Thus, we have to find an approximate method to expeditiously find the top-k related queries. This paper proposes a three-step solution to resolve it. The first step is to find a few representative queries in query history and the second step is to order the remaining queries corresponds to each representative query. The third step is to select the top-k related queries based on these orders. The first and second steps are processed during offline time and the third step is processed during online time.

Step 1. Find representative keyword queries. Based on the semantic similarities between different pairs of queries, we provide an algorithm (Algorithm 3), which is inspired by the furthest-first traversal algorithm [2], to find the repre-sentative queries in query history. Let m be the number of queries in query history W . Also let l be the number of representatives and W l the set of representatives. The algorithm starts by picking an arbitrary query in W as a representative query, denoted by  X  Q i , and then adds it to the set W l = {  X  Q i } , while  X  Q i is removed from W . Then, it picks query  X  Q j , which is furthest from  X  Q i that means the  X  sim (  X  Q j ,  X  Q i ) is the smallest among the {  X  sim (1 , ..., m ) and j  X  = i } in W . Algori thm 3: Representative queries finding algorithm Input : queries in W = { Q 1 , ..., Q m } , number l .
Output : the set of l representatives W l = { Q 1 , , Q l 1 W l  X   X  . 2 pick arbitrary Q 1  X  W . 3 W l  X  W l 4 W  X  W  X  X  Q 1 } 5 for i = 2 to l do 7 W l  X  W l 8 W  X  W  X  X   X  Q i } 9 Return W l = {  X  Q 1 , ..,  X  Q l } Step 2. Create orders for representative queries. For each representative query  X  Q i create an order  X  i of all remaining queries (except  X  Q i ) in query history in descending order, according to their similarities to  X  Q i . The output of this procedure is a set of l orders. According to the output orders, each query Q j has a score that is associated with the position of Q j in each order  X  i . The score of Q j in  X  corresponds to  X  Q i is: where,  X  i ( Q j ) represents the position of Q j in  X  i .
Using Algorithm 3, a few representative queries can be selected. When a given query is coming, it needs to only compute the similarities between the given query and rep-resentative queries, which is used as a weighting parameter for top-k query selection.

Step 3. Select top-k related queries. For a given query Q , using the output of Step 2, this step computes the set Q k ( W )  X  W with | Q k ( W ) | = k , such that  X  Q ( W ) and Q  X  j  X  X  W  X  Q k ( W ) } it holds that score ( Q score ( Q  X  j , Q ), with score ( Q j , Q ) = We next describe a method, which employs the Threshold Algorithm (TA) [13], to provide the top-k related queries for a given query. The TA uses sorted and random modes to access the queries in the orders. The Sorted mode obtains the score of a query in an order by scanning the order of the queries from the top to down sequentially. The Random mode finds the score of a query in an order in one access. The top-k query selection algorithm is shown in Algorithm 4, where the score of Q j found in each order  X  i to Q is computed by: where, s ( Q j , Q ) is weighted by the semantic similarity be-tween the given query Q to the representative query  X  Q i
The score of Q j in every other order is found via random access, and all these scores are summed, resulting in the final score of Q j for the given query Q :
In Algorithm 4,  X  is a threshold for the current repeat loop, for any query Q  X  j that has not yet been seen in the repeat loop access, its score is less than  X  . The time com-plexity of Algorithm 4 is O ( kl 2 ), where k is the number of queries need to be retrieved and l ( l &lt;&lt; n ) is the num-ber of representative queries. When l is small, Algorithm 4 Algo rithm 4: The top-k query selection algorithm
Input : Orders set W l = {  X  1 , ...,  X  l } , given query Q ,
Output : Top-k related queries in query history. 1 Let B = {} be a buffer that can hold k keyword queries. 2 Let L be an l size array that is used to store the score of the last visited query of each order by the end of the current round-robin cycle. 3 repeat 4 for i = 1 to l do 5 Retriev e next query Q j from  X  i using sorted 6 Compute s ( Q j , Q ) =  X  sim ( Q,  X  Q i ) s ( Q j |  X  7 Update L [ i ] with score of Q j in  X  i . 8 Get score of Q j from other orders {  X  k |  X  k  X  W l 9 score ( Q j , Q )  X  summing up of all the scores of 10 Insert  X  Q j , score ( Q j , Q )  X  into B in descending 11  X   X   X  + L [ i ]. 12 un til B [ k  X  1] .score  X   X  13 Return B . achie ves a significant decrease in complexity (In Section 7, we demonstrate that even for a small number of l , our top-k related query selection algorithm can achieve a relatively high accuracy). The experiments are conducted on a computer running Windows 2008 with Intel P4 3.2-GHz CPU, and 8 GB of RAM. We implemented all algorithms in C# and SQL. We use the following two real datasets to evaluate the perfor-mance of our methods. 1. DBLP dataset. The download DBLP XML file is de-composed into 4 relational tables, that are Authors , P apers , W rite and P ublications , respectively. We built a keyword query system based on DBLP dataset and provided a web in-terface for users to submit keyword queries that they would execute. In this way, we collected 1,600 queries in an exten-sive scope and 500 queries are finally retained after pruned as the query history. Each remained query contained 3  X  5 keywords and there are 1,674 distinct keywords in total. The keywords in query history are related to author, paper title, and conference name, and they are also inter-related within the same queries and/or across the different queries. So the query history over DBLP is very appropriate for testing the performance of our keyword coupling relationship and query semantic similarity measuring methods. 2. IMDB dataset. The Internet Movie Database (IMD-B) contains movies, directors, actors and other movie-related information. For this database, we adopt the following strat-egy to simulate the query history. We first created a data view, in which each record is formed by joining all connect-ed tuples according to the primary-foreign-key references. Then, we random selected 1,000 records from the data view and extracted keywords respectively from the movie name, actor name, genre, role, and director name of each record. Next, the keywords extracted from each record were random selected to integrate as a keyword query. Finally, we totally formed 1,000 queries as the query history.
This experiment aims to show how to determine the pa-rameter  X  in Equation(7) to get the best accuracy of cou-pling relationships between keywords. To do this, we ran-domly selected 10 keywords from DBLP and IMDB query histories, respectively. For each keyword k i , we first ob-tained the top-5 relevant keywords by using our keyword coupling relationship measuring method with respect to each value of parameter  X  in Equation (7) from 0 to 1 at the in-crements 0.1. Then, we mixed these keywords and thus a set K i of 55 keywords was generated. Next, we calculat-ed the frequency of occurrence for each distinct keyword in K i and finally marked the top 5 most frequently occurring keywords as the relevant keywords since the more frequently the keyword occurring in K i indicates the more the keyword relevant to the given keyword.

Based on the relevant keywords marked for each selected keyword k i , we then use the Recall and P recision metrics to evaluate the accuracy of coupling relationships with respect to different values of  X  . Recall is the ratio of the number of relevant keywords retrieved by the algorithm to the total number of the keywords that were marked as relevant. Preci-sion is the ratio of the number of relevant keywords retrieved by the algorithm to the total number of keywords that were retrieved. In our case, both the relevant and retrieved key-words number 5, making the Recall and Precision equal. Figure 3 shows the Recall&amp;Precision of answers obtained by using our keyword coupling relationship measuring method with respect to different values of  X  on DBLP and IMDB datasets, respectively. Note that, the Recall&amp;Precision for each value of  X  is averaged over 10 selected keywords. Figure 3: Accuracy of answers for different values of  X  on DBLP and IMDB datasets
It can be seen that the curve of Recall&amp;Precession reach-es the peak at  X  = 0 . 5 for both DBLP and IMDB datasets, which demonstrates that our method achieves the best per-formance on these two datasets when  X  is set to 0.5, and the corresponding accuracy are 0.90 and 0.83, respectively. Note that, since the keywords from different datasets may have different coupling relationships, it is necessary to opti-mize the setting of  X  to achieve the highest accuracy. It also can be seen that, the peak accuracy on IMDB is lower than that on DBLP. This is because the query history of DBLP is the real queries user submitted, in which the coupling rela-tionships between keywords of them are very strong, so that we can capture the coupling relationships more efficiently.
This experiment aims to evaluate how well our query se-mantic similarity measuring algorithm captures the user query intentions. To verify the accuracy of the semantic similari-ties between different queries, we adopt the strategy as fol-lows. We invited 10 people, which are researchers and PhD students, to choose the queries from the DBLP and IMDB query histories. For each selected query Q i , we generat-ed a set K i of 30 queries from query history that likely to contain a mix of relevant and irrelevant queries in relation to the given query. Each set K i is formed by mixing the top 10 results of each algorithm of kernel-based similarity (K-COS), traditional VSM-based cosine similarity (V-COS), and RANDOM (it selects the queries in a random order and provides a baseline to show the efficiency of the other two algorithms). Lastly, we presented the queries with their cor-responding K  X  i s to each user in our study. Each user had to mark the top 10 queries in K i that they considered se-mantically related to Q i . We then measured how closely the 10 queries marked as relevant by the user matched the 10 queries returned by each algorithm. The users were asked to describe whether they considered a query Q  X  related to a given query Q based on: (i) the keywords in Q  X  are related to that of Q . For ex-ample, the keyword  X  X equential mining X  is related to  X  X sso-ciation rules X , hence the queries contains the keywords are considered to be related to each other. (ii) the results of Q  X  are relevant to that of Q , although no keyword in Q  X  is same or related to Q . For example, the results of query  X  X -commerce, decision making X  are partly overlapped with those of query X  X ata analysis, decision tree X , but the keywords in queries are not explicit related. Figure 4: Accuracy for K-COS, V-COS, and RAN-DOM
The Recal l&amp;Precision metrics is also used to evaluate this overlap. Figure 4 shows the Recall&amp;Precision of answers for K-COS, V-COS, and RANDOM. We can see that the Re-call&amp;Precision of K-COS is much higher than V-COS over the two datasets. The averaged Recall&amp;Precision of K-COS is 0.84 for DBLP and 0.78 for IMDB while the V-COS is 0.65 for DBLP and 0.52 for IMDB. This is because V-COS learns the query similarity based on traditional VSM, which only considers the local overlapped information between the queries. In contrast, the K-COS considers both the local overlapped information between the queries and the cou-pling relationships of the keywords within/across queries. Additionally, the reasonability of keyword coupling relation-ship, which considers both the intra-and inter-coupling be-tween keywords, is demonstrated by Experiment 7.2. Hence, the answers for the given query can meet the user X  X  inten-tions more closely.
This experiment aims to test the accuracy of the top-k queries obtained using only the orders that corresponds to representative queries when compared with the top-k key-word queries obtained by computing the similarities of the given query to all queries in query history. To quantify this accuracy, we use R ( All, k ) to denote the top-k queries re-turned by computing the similarities of the given query to all queries in query history, and R ( Rep, k ) to denote the top-k queries returned using only the orders corresponding to representative queries. The overlap of two top-k answer sets is measured using the Jaccard coefficient: Figure 5: Accuracy of different l when value k varied
The coefficient falls into the interval of [0, 1] and the high-er its value the more similar the two sets of queries are. In this experiment, we use three parameters: n , l , and k , to character the dataset. Here, n is the number of queries in each of the orders, l the number of representative queries, Figure 6: Performance of top-k selection algorithm and k the number of queries needs to be selected. Figure 5 shows the value of the coefficients (averaged over 10 test queries) for different values of k , when l = { 10, 20, 40, 60, 80 } . The values of n are fixed to 500 for DBLP and 1000 for IMDB (because there are 500 and 1000 queries in their query histories, respectively), and k is varied in { 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 } .

From Figure 5 we can see the coefficients corresponding to different numbers of l are nearly identical (except the case of l =10 for DBLP) and the accuracy is relatively high, which means when only a small number of representative queries (such as l = 20) are used to find the related queries, the information lost by looking at the orders for representatives instead of computing similarities of the given query to all queries in query history is not substantial. Additionally, the accuracy of l =10 is much lower than that of other numbers of l on DBLP dataset, which indicates that the number of l should be selected appropriately for different datasets.
This experiment aims to verify the performance of the top-k selection algorithm. In this experiment, we generate 5,000 and 10,000 keyword queries as query history for DBLP and IMDB datasets, respectively. Based on these datasets, we fix the number of l to 10, 20, 40, 60, and 80, respectively and then test the execution time of top-k selection algorithm for different k values (here, we set the number of k to 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100, repectively). Figure 6 shows the execution time on DBLP and IMDB datasets for different k values when l = { 10, 20, 40, 60, 80 } .
From Figure 6, we can see that the algorithm runs fast, especially when l  X  20 and k  X  10. As demonstrated in Ex-periment 7.4, the information lost is not substantial by only using l = 20 representative queries to find related queries. Meanwhile, providing top-10 related queries are enough for most users in real applications. Therefore, our top-k selec-tion algorithm can be very well suitable for processing the large scale of query history. Additionally, the performance of th e algorithm decreases with the increasing of value l and k . The reason is that the top-k query selection algorithm needs to deal with more queries in orders as the number l and k increased. We also computed the time consumption for computing the similarities of a given query to all queries in query history. It takes approximately 48 seconds for D-BLP and 321 seconds for IMDB to obtain the similarities of a given query to all remaining queries in query history. Our top-k selection algorithm clearly outperforms existing methods and demonstrates more efficient performance.
This paper presented a novel approach to address the cou-pling relationships hidden between keywords and between keywords and queries to enhance semantic approximate key-word queries over Web databases. The techniques proposed in this paper can also be adopted in domains other than web databases. For example, scientific database users, especially the ones who are not familiar with the area, would benefit from a system that improves their original queries. Further-more, this approach can be used both at the application lev-el and be incorporated into most of existing keyword search frameworks to support the semantic approximate keyword search. The experiments on real dataset identified that the keyword coupling relationship and query semantic similarity measuring methods can capture the semantic relationships of keywords and queries more reasonable. The top-k related queries can be returned quickly and relatively high accuracy is achieved, even though only a small number of representa-tive queries are retained.

It would be interesting to investigate (i) how to minimize the updating cost when the query history is varied, and (i-i) the effect of introducing some diversity in the suggested queries.
This work is supported by the National Science Founda-tion for Young Scientists of China (No.61003162) and the Y-oung Scholars Growth Plan of Liaoning (No. LJQ2013038). [1] B. Aditya, G. Bhalotia, and S. Chakrabarti. Banks: [2] R. Agrawal, R. Rantzau, and E. Terzi.
 [3] S. Agrawal, S. Chaudhuri, and G. Das. Dbxplorer: A [4] L. AlSumait and C. Domeniconi. Text clustering with [5] Z. F. Bao, J. H. Lu, and T. W. Ling. Xreal: an [6] S. Bergamaschi, E. Domnori, and F. Guerra. Keyword [7] D. Bollegala, Y.Matsuo, and M. Ishizuka. Measuring [8] L. B. Cao, Y. M. Ou, and P. S. Yu. Coupled behavior [9] Z. Y. Chen and T. Li. Addressing diverse user [10] X. Cheng, D. Q. Miao, C. Wang, and L. B. Cao. [11] G. Das, D. Gunopulos, and N. Koudas. Answering [12] B. Ding, J. X. Yu, and S. Wang. Finding top-k [13] R. Fagin, A. Lotem, and M. Naor. Optimal [14] V. Hristidis, L. Gravano, and Y. Papakonstantinou. [15] V. Hristidis and Y. Papakonstantinou. Discover: [16] A. Huang, D. Milne, and E. Frank. Clustering [17] L. B. Kong, R. Gilleron, and A. Lemay. Retrieving [18] Y. Luo, X. M. Lin, and W. Wang. Spark: top-k [19] N. Sarkas, N. Bansal, and G. Das. Measure-driven [20] S. Tata and G. M. Lohman. Sqak: doing more with [21] C. Wang, L. B. Cao, and M. C. Wang. Coupled [22] C. Wang, Z. She, and L. B. Cao. Coupled clustering [23] X. Wang and G. Sukthankar. Multi-label relational [24] J. J. Yao, B. Cui, and L. S. Hua. Keyword query [25] A. Yu, P. K. Agarwal, and J. Yang. Top-k prefrences [26] R. Zhou, C. F. Liu, and J. X. Li. Fast elca
