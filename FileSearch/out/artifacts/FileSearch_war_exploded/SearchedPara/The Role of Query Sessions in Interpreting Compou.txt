 The meaning of compound noun phrases can be approximated in the form of lexical interpretations extracted from text. The interpre-tations hint at the role that modifiers play relative to heads within the noun phrases. In a study examining the role of query sessions in explaining compound noun phrases, candidate interpretations of compound noun phrases are extracted from pairs of queries that be-long to the same query session. Experimental results over multiple evaluation sets of noun phrases show a higher accuracy of the in-terpretations when extracted from query sessions rather than from individual queries.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing; H.3.1 [
Information Storage and Retrieval ]: Content Analysis and In-dexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Compound noun phrases, compositional concepts, open-domain in-formation extraction, knowledge acquisition Motivation : A necessary step towards understanding the meaning of compound noun phrases (e.g.,  X  X eatles songs X  ,  X  X rench mac-arons X  ,  X  X yurvedic medicinal plants X  ) is to understand the roles that modifiers ( beatles , french , ayurvedic ) play within the noun phrases. Often, the roles can be approximated lexically, in the form of string interpretations (  X  X omposed by X  ,  X  X rom X  ,  X  X sed in X  serve as connectors within what are effectively paraphrases ( composed by beatles X  ,  X  X acarons from france X  ,  X  X edicinal plants used in ayurveda X  ) of the compound noun phrases.

Evidence available within, and collected from, large text collec-tions should be helpful in the analysis of compound noun phrases, and allow for the extraction of relevant interpretations. Indeed, tex-tual document collections are a useful source for extracting para-phrases of compound noun phrases [19, 36]. As an alternative to using document collections, collections of Web search queries pro-duce interpretations that may be more accurate [28] than when ex-tracted from documents, even if the source queries are available simply as a set of queries that are independent from one another. Contributions : Expanding upon [28], the experiments reported in this paper examine the role of Web search queries vs. query ses-sions in uncovering the semantics of open-domain class labels in particular; and of compound noun phrases in general. Queries and query sessions are the source for candidate lexical interpreta-tions of compound noun phrases. The interpretations turn implicit properties or subsuming roles (  X  X omposed by X  ,  X  X rom X  ,  X  X sed in X  that modifiers ( beatles , french , ayurvedic ) play within longer noun phrases (  X  X eatles songs X  ,  X  X rench macarons X  ,  X  X yurvedic medici-nal plants X  ) into concrete strings. The roles of modifiers relative to heads of noun phrase compounds cannot be characterized in terms of a finite list of possible compounding relationships [10]. Hence, the interpretations are not restricted to a closed, pre-defined set. In experiments comparing query sessions and independent queries, over evaluation sets of noun phrases from multiple sources, more accurate interpretations are extracted from query sessions. Compounds : Let N be a compound noun phrase, containing a head H preceded by modifiers M . Each of H and M may contain one or multiple tokens. Being a compound, the sequence of mod-ifiers and head in N act as a single noun [10, 16], which refers to a concept whose meaning is based on the semantics of its compo-nents [24, 35].
 Hypothesis 1 : If N is relevant and of interest to Web users, then in a sufficiently large corpus it will eventually be referred to in rel-atively more verbose search queries, which reveal the implicit role that modifiers M play relative to the head H .
 Hypothesis 2 : If N is relevant and of interest to Web users, then in a sufficiently large corpus it will eventually be referred to in the same query session [34] as more verbose search queries, which re-veal the implicit role that modifiers M play relative to the head Extraction from Queries : Figure 1 gives an overview of the role of queries and query sessions in extracting interpretations of com-pound noun phrases. The compound noun phrases to be interpreted are available as an input vocabulary of noun phrases. For example, the vocabulary might include the noun phrases  X  X eatles songs X   X  X yurvedic medicinal plants X  . The source of candidate interpreta-tions takes the form of anonymized queries, from which possible interpretations of the noun phrases must be extracted. The queries are available as a set of pairs of anonymized queries submitted Figure 1: Extraction of interpretations of noun phrases from q ueries (H, M=head and modifier) within the same query sessions, and also as a set of independent anonymized queries, collected from the queries that form the pairs. For example, the set of queries may include  X  X ongs written by the beatles X  and  X  X edicinal plants in ayurvedic products X  . Optionally, if the queries are available as query sessions rather than merely as a set of queries, the queries  X  X eatles songs X  and  X  X ongs written by the beatles X  , or  X  X yurvedic medicinal plants X  and  X  X edicinal plants in ayurvedic products X  , may be pairs that belong to the same query sessions.

The extraction consists of several steps [28]: (1) the selection of a subset of queries that may be candidate interpretations of some yet-to-be-specified noun phrases; (2) the matching of the selected queries to the noun phrases to interpret; and (3) the aggregation of matched queries into candidate interpretations extracted for a noun phrase.
 Selecting Candidate Interpretations : As a pre-requisite, the noun phrases to interpret and queries are both part-of-speech tagged [5]. A query is deemed to be a candidate interpretation of a hypotheti-cal compound noun phrase that might consist in a modifier mediately followed by a head H (e.g.,  X (ayurvedic) M (medicinal plants) H  X  ), if the query matches one of the following patterns [28]:  X  passive constructs, for example  X (medicinal plants) H used as (ayurvedic) M drugs X  ;  X  prepositional constructs, for example  X (medicinal plants) (ayurvedic) M products X  ;  X  relative pronoun constructs, e.g.,  X (medicinal plants) H used in (ayurveda) M  X  .

The patterns effectively split matching queries into four consec-utive sequences of tokens Q =[ Q 1 Q 2 Q 3 Q 4 ], where H correspond to Q 1 and Q 3 , and Q 4 may be empty. For example, in the lower portion of Figure 1, one of the patterns matches the query  X (medicinal plants) H in (ayurvedic) M products X  , thus splitting the query into  X  X edicinal plants X  as Q 1 ,  X  X n X  as Q 2 ,  X  X yurvedic X  Q 3 and  X  X roducts X  as Q 4 .
 Mapping Noun Phrases to Interpretations : Each noun phrase to interpret is split into all possible decompositions of two consec-utive sequences of tokens N =[ N 1 N 2 ], where the two sequences correspond to a hypothetical modifier and a hypothetical head of the noun phrase. For example, the noun phrase  X  X yurvedic medic-inal plants X  is split into [  X  X yurvedic medicinal X  ,  X  X lants X  separately into [  X  X yurvedic X ,  X  X edicinal plants X  ]. If N and N 2 and Q 1 respectively, match, then the matching query (e.g.,  X (medicinal plants) H in (ayurvedic) M products X  ) is retained as a candidate interpretation of the noun phrase N (  X (ayurvedic) (medicinal plants) H  X  ), as shown in the middle portion of Figure 1. If the noun phrases are interpreted using query sessions instead of independent queries, the matching query Q and the noun phrase N must satisfy an additional constraint, namely to be one of the pairs of queries that are available as part of the same query session. Otherwise, the matching query Q is discarded.
 Mapping via Modifier Variants : At its simplest, the matching of the hypothetical modifier relies on strict string matching. Alterna-tively, original modifiers in the noun phrases to interpret may be matched to queries via expansion variants. Variants are phrases that likely play the same role, and therefore share interpretations, as modifiers relative to the head in a noun phrase. Variants allow for the extraction of candidate interpretations that may otherwise not be available in the input data. For example, the variant stones available for beatles allows for the matching of beatles the noun phrase  X (beatles) M (songs) H  X  , with rolling stones query  X (songs) H played by (rolling stones) M  X  . The candidate in-terpretation  X (songs) H played by (beatles) M  X  is extracted for the noun phrase  X (beatles) M (songs) H  X  , even though the query played by beatles X  might not be present among the input queries.
Possible sources of variants include, among others, distribution-ally similar phrases [21], where the phrases most similar to a mod-ifier would act as its variants. Mappings from adjectival modifiers in noun phrases (e.g., ayurvedic in  X  X yurvedic medicinal plants X  in Figure 1) into the nominal counterparts (e.g., ayurveda are likely to occur in interpretations (e.g.,  X (medicinal plants) that are used in (ayurveda) M  X  ) are also useful. Concretely, as described later in the experimental setting, variants are generated using WordNet [12], distributional similarities and Wikipedia [33]. Aggregation of Candidate Interpretations : Candidate interpre-tations of a noun phrase are aggregated from source queries that matched the noun phrase. The frequency score of a candidate inter-pretation is the weighted sum of the frequencies of source queries from which the candidate interpretation is collected, possibly via variants of modifiers. In the weighted sum, the weights are simi-larity scores between the original modifier from the noun phrase, on one hand, and the variant from the source query into which the modifier was mapped, on the other hand [28]. The weights for the variants beatles and rolling stones relative to the original modifier beatles are 1.0 (identity) and within [0.0, 1.0] (distribu-tional similarity), whereas the weights of adjectival modifiers such as ayurveda for ayurvedic are 1.0. Separately from the frequency score, a penalty score is computed that penalizes interpretations containing extraneous tokens. Specifically, the penalty counts the number of nouns or adjectives located outside the modifier and head. For example, the penalty scores assigned to the interpreta-tions  X (songs) H by (beatles) M  X  and  X (medicinal plants) used in (ayurveda) M  X  are 0. In comparison, the penalty scores for the interpretations  X (songs) H that (beatles) M play X  and inal plants) H in (ayurvedic) M products X  are 1. From among can-didate interpretations extracted for a noun phrase, interpretations whose penalty score is higher than 1 are discarded. Candidate in-terpretations extracted for a noun phrase are ranked in increasing order of their penalty scores or, in case of ties, in decreasing order of their frequency scores. Sources of Textual Data : The experiments rely on a random sam-ple S of around 3 billion unique pairs of fully-anonymized Web search queries in English. The sample is drawn from pairs of queries within the same query session(s), as submitted to a general-purpose Web search engine. A session contains queries submitted in rela-tively quick succession by the same anonymized user [34]. The queries from the query pairs in the sample S are separately orga-nized as a sample Q of around 1.5 billion unique queries. Each Figure 2: Acquisition of interpretations of compound noun p hrases via wiki-templ variants query in Q is available independently from other queries, and is ac-companied by its frequency of occurrence in the query logs. In this setup, each query in Q also appears as a query in some query pair in S. Conversely, each query in a query pair in S also appears as a query in Q.
 Experimental Runs : The experiments consist in extracting inter-pretations from the independent queries Q, in the case of run as introduced in [28]; or from the query sessions S, in the case of run R S , as proposed here.
 Sources of Variants : The original form of the modifiers is de-noted as orig-phrase . Three types of variant phrases are collected for the purpose of matching modifiers within noun phrases to in-terpret, with phrases from queries. Relations encoded as Value-Of, Related-Noun and Derivationally-Related relations in Word-Net [12] are the source of adj-noun variants. They map around 6,000 adjectives into one or more nouns (e.g., ( french ( electric  X  electricity ), ( aquatic  X  water )). A repository of distribu-tionally similar phrases, collected in advance [21, 29] from a sam-ple of around 200 million Web documents in English, is the source of dist-sim variants. For each of around 1 million phrases, the vari-ants consist of their 50 most similar phrases (e.g., art garfunkel { carly simon , melissa manchester , aaron neville , ..}). A snapshot of all Wikipedia [33] articles in English, as available in June 2014, is the source of wiki-templ variants. For each of around 50,000 phrases, their wiki-templ variants are collected from Wikipedia cat-egories sharing a common parent Wikipedia category (e.g., bums by artist X  ) and having a common head (  X  X rt garfunkel al-bums X  ,  X  X lack sabbath albums X  ,  X  X etallica albums X  ). The dif-ferent modifiers ( art garfunkel , black sabbath , metallica company the shared head are collected as variants of one another. Among the four types of variants, wiki-templ variants are applied only when the noun phrase to interpret, and the source Wikipedia category names from which the variants were collected, have the same head. For example, X = art garfunkel  X  { black sabbath lica , 50 cent , ..} is applied only in the context of the noun phrase  X  X  albums X  . As shown in Figure 2, the extraction of interpretations via some of the variants allows for effectively inferring interpreta-tions via other variants, as long as the noun phrases share the same head. When computing the frequency score of a candidate interpre-tation as the weighted sum of the frequencies of source queries, the weights assigned are 1.0, for orig-phrase, adj-noun and wiki-templ variants; and the available distributional similarity scores within [0.0, 1.0], for dist-sim variants. Figure 3: Distribution of the number of interpretations ex-t racted per noun phrase, over noun phrases with some ex-tracted interpretation(s). Computed over various vocabularies of noun phrases Vocabularies of Noun Phrases : The experiments acquire inter-pretations from queries, for noun phrases from three vocabularies. ListQ is a set of phrases X (e.g.,  X  X ramaic words X  ) from queries in the form [ list of X ], where the frequency of the query [ 100 times higher than the frequency of the query [ list of X the frequency of the latter is at least 5. IsA is a set of class labels (e.g.,  X  X cademy award nominees X  ), originally extracted from Web documents via Hearst patterns [15], and associated with at least 25 instances each (e.g., zero dark thirty ). WikiC is a set of Wikipedia categories that contain some tokens in lowercase beyond preposi-tions and determiners, and whose heads are plural-form nouns (e.g.,  X  X rench fiction writers X  ). Only phrases that are one of the full-length queries from the input set of Web search queries are retained in the respective sets, as vocabularies of noun phrases to interpret; other phrases are discarded. Relative Coverage : Because it is not feasible to manually compile the exhaustive sets of all string forms of valid interpretations of all (or many) noun phrases, we compute relative instead of absolute coverage. At least one interpretation is extracted from query ses-sions and independent queries for more than 500,000 of the noun phrases from all input vocabularies. Considering only that subset, Figure 3 shows the distribution of the number of candidate inter-pretations extracted per noun phrase. Given a count of extracted candidate interpretations fixed between 1 and 20, run R S tently extracts that number of interpretations for a higher fraction of the noun phrases from the input vocabularies than R Q does. The apparent advantage of run R S over R Q in fact reflects its inability to extract as many candidate interpretations as R Q does. Indeed, although not shown in the graphs, more than 100 interpretations are extracted per noun phrase for 14.5% (R Q ) vs. 5.5% (R the noun phrases in ListQ, 24.8% (R Q ) vs. 5.6% (R S ) in IsA, and 16.7% (R Q ) vs. 7.1% (R S ) in WikiC.
 Precision of Interpretations : From an input vocabulary, an ini-tial weighted sample of 150 noun phrases with some interpreta-tions extracted in both runs R Q and R S is manually inspected. The sampling weight is the frequency of the noun phrases as queries. A noun phrase from the selected sample is either retained, or dis-carded if deemed to be a non-interpretable phrase. A noun phrase is not interpretable if it is in fact an instance (  X  X ew york X  cia keys X  ) rather than a class; or it is not a properly formed noun phrase (  X  X atch movies X  ); or does not refer to a meaningful class (  X 3 significant figures X  ). The manual inspection ends, once a sam-ple of 100 noun phrases has been retained. The procedure gives weighted random samples of 100 noun phrases, drawn from each Eval Set Sample of Noun Phrases Table 1: Samples from evaluation sets of noun phrases in each v ocabulary
Correctness label: correct generic (numeric value: 1.0):
Correctness label: correct specific (numeric value: 1.0): ayurvedic medicinal plants medicinal plants used in ayurveda
Correctness label: okay (numeric value: 0.5):
Correctness label: incorrect (numeric value: 0.0): Table 2: Examples of interpretations manually annotated with e ach correctness label of the ListQ, IsA and WikiC vocabularies. The samples, shown in Table 1, constitute the evaluation sets of phrases ListQ, IsA and WikiC, over which precision of interpretations is computed.
Following evaluation methodology introduced for other open-domain information extraction tasks, the top 20 interpretations ex-tracted in an experimental run for each evaluation phrase are man-ually annotated with correctness labels. As shown in Table 2, an interpretation is annotated as: correct and generic, or correct and specific, if relevant; okay, if useful but containing non-essential in-formation; or wrong. To compute the precision score over an evalu-ation set of phrases, the correctness labels are converted to numeric values. Precision of a ranked list of extracted interpretations is the average of the correctness values of the interpretations in the list.
Table 3 provides a comparison of precision scores at various ranks in the extracted lists of interpretations, as an average over all phrases from an evaluation set. Several conclusions can be drawn from the results. First, at a given rank, the average precision of a run varies from one evaluation set to another, since the evaluation sets are selected from vocabularies from different sources. Second, pre-cision levels are encouraging for all runs. Third, with the exception of precision at rank 1 for the IsA evaluation set, run R S more accurate interpretations than R Q across all ranks and across all evaluation sets. In particular, when considering the top 20 inter-pretations extracted for all phrases from the union of all evaluation Eval Run Precision@N Set @1 @3 @5 @10 @20 ListQ R Q 0 .824 0.742 0.656 0.541 0.443 WikiC R Q 0 .747 0.649 0.586 0.448 0.336
Union R Q 0 .787 0.676 0.592 0.473 0.372 of all R S 0 .820 0.734 0.673 0.583 0.490 Table 3: Average precision, at various ranks in the ranked lists o f interpretations extracted for noun phrases from various sets of evaluation phrases. Computed for each run, as an average over each evaluation set, and as an average over the union of all evaluation sets Run: Eval Phrase (P@5)  X  [ Ranked Candidate Interpretations] R
Q : atm card (0.00)  X  [card stuck in atm, card retained by atm, card retained in atm, card captured by atm, card eaten by atm, ..] R S : atm card (1.00)  X  [card used in atm, card for atm] R
Q : ayurvedic medicinal plants (1.00)  X  [medicinal plants in ayurveda, medicinal plants used in ayurveda, medicinal plants of ayurveda, medicinal plants used by ayurveda, medicinal plants used in ayurvedic medicines, ..] R
S : ayurvedic medicinal plants (1.00)  X  [medicinal plants in ayurveda, medicinal plants used in ayurveda, medicinal plants of ayurveda, medicinal plants used by ayurveda, medicinal plants used in ayurvedic medicines, ..] R
Q : chemical processes (0.40)  X  [processes of chemical weath-ering, processes in chemistry, processes involved in chemi-cal weathering, processes of chemistry, processes in chemical weathering, ..] R
S : chemical processes (1.00)  X  [processes in chemistry, pro-cesses of chemistry, processes that involve chemistry, processes involved in chemistry, processes related to chemistry, ..] R
Q : michael jackson song (0.90)  X  [song by michael jackson, song of michael jackson, song from michael jackson, song sung by michael jackson, song written by michael jackson, ..] R
S : michael jackson song (0.80)  X  [song by michael jackson, song of michael jackson, song from michael jackson, song sung by michael jackson, song for michael jackson, ..] R
Q : special diet (0.10)  X  [diet with special k, diet of special k, diet for special forces, diet for special needs, diet before a special event, ..] R
S : special diet (0.75)  X  [diet for special needs, diet for special conditions, ..] R
Q : uk record companies (0.60)  X  [record companies in the uk, record companies in uk, record companies of uk, record compa-nies in london uk, ..] R
S : uk record companies (0.75)  X  [record companies in the uk, record companies in uk, record companies of uk, record compa-nies in london uk] Table 4: Top candidate interpretations extracted in the experi-m ental runs for a sample of noun phrases from the ListQ eval-uation set (P@5=precision at rank 5 in the ranked list of ex-tracted interpretations) sets, about half of the interpretations are correct, for R over a third, for R Q . At rank 1, the precision of R S is higher than 0.8. Within an evaluation set, the accuracy of extracted interpre-tations varies between the experimental runs, as well as from one evaluation phrase to another, as illustrated in Table 4. Presence of Relevant Interpretations : Since sometimes it is dif-ficult to even manually enumerate as many as 20 distinct, relevant Table 5: Average of scores indicating the presence or absence o f any interpretations annotated as correct. Computed over interpretations extracted up to various ranks in the ranked lists of extracted interpretations string forms of interpretations of a given noun phrase, measuring precision at a particular rank (e.g., 20) in a ranked list of interpreta-tions may be too conservative. Table 5 summarizes a different type of scoring metric, namely the presence of any relevant interpreta-tion, among the interpretations extracted up to a particular rank. Relevance is flexibly defined, by requiring the interpretations to have been assigned a certain correctness label, then computing the average number of evaluation phrases for which such interpreta-tions are present up to a particular rank. Intuitively, the resulting scores at a given rank may be lower for R S than for R Q . Indeed, as R S effectively attempts to discard incorrect interpretations that R
Q would otherwise produce, it may occasionally discard relevant interpretations. Especially when few interpretations are extracted in the first place, discarding relevant interpretations may adversely affect the scores measuring the presence of any relevant interpreta-tions. However, even as they are affected by this phenomenon, the scores in Table 5 are still competitive for R S relative to R scores indicate that at least one of the top 5 interpretations is cor-rect and specific for about 1 out of 3 noun phrases in the evaluation sets on average.
 Popular Heads, Modifiers and Interpretations : The most fre-quent heads induced by interpretations extracted in R S are shown in Figure 4. Although the incorrect head wikipedia is present in the upper graph, most of the heads and modifiers seem plausible, suggesting that the input vocabulary does contain reasonably clean compound noun phrases. In the lower graph, the prominence of location-related phrases (e.g., uk , indian ) among the most popular modifiers for the ListQ vocabulary is not unexpected, but is not characteristic for all vocabularies. For example, in the case of the IsA vocabulary, the most popular modifiers are business good , human , famous , with the earliest location-related modifier american being only the 24th most popular modifier.

Figure 5 illustrates the most popular interpretations extracted for noun phrases in the ListQ vocabulary. For example, if interpreta-tions are restricted to those containing more than just prepositions and determiners, the ninth most popular interpretation is ten by (M) X  , where H and M stand for the induced heads and mod-ifiers respectively.
 Impact of Variants : Variants of modifiers provide alternatives in identifying and extracting candidate interpretations, even when the modifiers from the noun phrases are not present in their original Figure 4: Most frequent heads and modifiers induced by inter-p retations extracted in run R S for noun phrases from the ListQ vocabulary. Computed as the fraction of noun phrases with some extracted interpretation(s), which have a particular head (first graph) or modifier (second graph) induced by at least one of their top 10 interpretations Figure 5: Most frequent interpretations extracted in run R f or noun phrases from the ListQ vocabulary. Computed over the top 10 interpretations of noun phrases with some extracted interpretation(s). Collected interpretations are either unre-stricted (A) or required (B) to include at least a token that is neither a preposition nor a determiner (second graph). (H) and (M) stand for portions of an interpretation that correspond to the head and modifier of the interpreted noun phrase form in the interpretations. For illustration, the adj-noun variant ethiopia of the modifier ethiopian leads to the extraction of the in-Table 6: Impact of various types of variants of modifiers, on the c overage of noun phrase interpretations. Computed as the frac-tion of the top 10 extracted interpretations produced by a par-ticular variant type, and possibly by other variant types (upper portion); or produced only by a particular variant type, and by no other variant types (lower portion) (Vocab=vocabulary of noun phrases) terpretation  X  X unners from ethiopia X  for the noun phrase  X  X thiopian runners X  . Similarly, wiki-templ variants metallica and 50 cent the modifier art garfunkel , in the context  X  X  albums X  , allow for the extraction of the interpretation  X  X lbums sold by art garfunkel X  for the noun phrase  X  X rt garfunkel albums X  , via the interpretations  X  X lbums sold by metallica X  and  X  X lbums sold by 50 cent X  .
Table 6 quantifies the impact of various types of variants, on the coverage of noun phrase interpretations. The scores provided for each variant type correspond to either non-exclusive (upper portion of the table) or exclusive (lower portion) contribution of that vari-ant type towards some extracted interpretations. In other words, in the lower portion, the scores capture the fraction of the top 10 in-terpretations that are produced only by that particular variant type. The results illustrate four phenomena. First, all variant types con-tribute to increasing coverage, relative to using only orig-phrase variants. Second, dist-sim variants have a particularly strong im-pact. Third, wiki-templ variants have a strong impact, but only when the contexts from which they were collected match the con-text of the noun phrase being interpreted. On the WikiC vocabulary in the lower portion of Table 6, the scores for wiki-templ illustrate the potential that contextual variants have for extracting additional interpretations. Fourth, the relative impact of the types of variants is comparable between runs R Q and R S .

Figure 6 quantifies the impact of various types of variants, on the coverage of noun phrase interpretations, and thus provides an alternative view to the lower portion of Table 6. The results illus-trate several phenomena. First, dist-sim variants have a particularly strong impact. Second, wiki-templ variants have a strong impact, but only when the contexts from which they were collected match the context of the noun phrase being interpreted. On the WikiC vo-cabulary, the scores for wiki-templ illustrate the potential of contex-tual variants in extracting additional interpretations. Third, beyond inherent variations, the relative impact of the types of variants is consistent between runs R Q and R S .
 Impact of Size of Textual Data : When more input queries or query sessions are available, better interpretations for more compound noun phrases may be extracted from the queries. Conversely, a reduction in the amount of textual data available in the form of queries may lead to interpretations of lower quality extracted for Figure 6: Impact of various types of variants of modifiers, o n the coverage of noun phrase interpretations. Computed as the fraction of the noun phrases whose top 10 interpretations include some interpretation(s) produced only by a particular variant type, and by no other variant types Table 7: Average precision of interpretations extracted in run R
S f or noun phrases from the ListQ evaluation set, at various ranks in the ranked lists of interpretations extracted for noun phrases, when changing the size of the input query data from which interpretations are extracted. The input query data is re-duced from the original data (100%) to iteratively smaller data. Computed as an average over the entire set of 100 evaluation phrases from ListQ (upper portion); and as an average over (variable) subsets E of the 100 evaluation phrases from ListQ that have some extracted interpretation(s) (lower portion) fewer compound noun phrases. Ablation experiments, whose re-sults are summarized in Table 7, quantify the impact of the size of the textual data on the extracted interpretations. To this effect, the run R S is applied to incrementally smaller input textual data. The input query session data is organized as a set of unique pairs of queries that are available as part of the same query session. From the entire query data of 100% of the query pairs, a series of itera-tively smaller query data are created, in the form of random samples of 25%, 10% etc. of the original set of query pairs. Table 8: Average of scores indicating the presence or absence o f any interpretations annotated as correct (i.e., correct generic or correct specific). Computed for run R S over the ListQ eval-uation set, when changing the size of the input query data from which interpretations are extracted. The input query data is reduced from the original data (100%) to iteratively smaller data. Computed over interpretations extracted up to various ranks in the ranked lists of extracted interpretations, as an av-erage over (variable) subsets of the 100 evaluation phrases from ListQ that have some extracted interpretation(s)
As expected, there is a clear impact of the size of the input textual data, on interpretations extracted in run R S over the ListQ evalua-tion set. The second column in Table 7 measures coverage. More precisely, it shows the number of evaluation phrases, out of the 100 evaluation phrases available in the ListQ evaluation set, for which some interpretations are extracted from query data of a particular size. When extracting from a quarter or a tenth of the original query data, some interpretations are extracted for 88 and 76 respectively of the 100 evaluation phrases.

The precision scores in the upper portion of Table 7 are com-puted as an average over all 100 evaluation phrases in ListQ. The scores quantify how precision increases, when the size of the input query data increases. But the scores are affected by both preci-sion and coverage, rather than just precision. Indeed, when less input data is used, some of the decrease in scores can be attributed to the lack of any interpretations being extracted, for some of the evaluation phrases. Such evaluation phrases are effectively penal-ized, as they contribute zeros to the averages computed in the upper portion of the table. In contrast, no penalty is given to such eval-uation phrases in the lower portion of Table 7. For a given input query data, the scores in the lower portion are computed over the subset of evaluation phrases with some extracted interpretations. Therefore, the scores in the lower portion of the table are a better indicator of the effect of a reduction in the size of the query data, on the quality of extracted ranked lists of interpretations. As the size of the textual data increases, the accuracy of the ranked lists of extracted interpretations also increases. Table 8 illustrates the same phenomenon, when considering the presence of any relevant interpretation, among the interpretations extracted up to a particular rank. As the size of the textual data increases, the interpretations extracted up to a particular rank include a relevant interpretation for more of the evaluation phrases, from among the subsets of evalua-tion phrases from ListQ for which run R S extracts some interpreta-tions for a given data size.
 Interpretations from Queries and Sessions : An additional, hy-brid run R H merges the ranked candidate interpretations produced by R S and R Q . More precisely, a ranked list of candidate inter-pretations in R H is the concatenation of the ranked list from R any, followed by the ranked list from R Q . Only the earliest occur-rence of any duplicate interpretation is retained in the merged list. Thus, run R H re-ranks interpretations that are extracted from query Vocab Noun Phrases Have Interpretation(s)? WikiC 39,432 3.6  X  z A 8 .4  X  z A 1,172 z A z M Table 9: Correlation between coverage of run R S , measured as the presence of some extracted interpretation(s) for a noun phrase, on one hand; and frequency of the noun phrase as a query, on the other hand (Vocab=vocabulary; I=subset of noun phrases that are queries and have some extracted interpreta-tion(s);  X  I=subset of noun phrases that are queries and do not have any extracted interpretation(s); A=average query fre-quency of noun phrases as queries; M=median query frequency of noun phrases as queries) Figure 7: Ability of run R S t o extract interpretations for noun phrases, as a function of the length of (i.e., number of tokens in) noun phrases. Computed as the fraction of noun phrases from an input vocabulary with a particular number of tokens, for which there are some extracted interpretation(s) sessions ahead of interpretations extracted only from independent queries, but otherwise keeps all interpretations extracted from in-dependent queries as well. The precision at rank 1 is the same for R H as for R S , over each evaluation set and also over their union. At other ranks, R H preserves the advantage (or disadvantage) in accuracy that R S has over R Q , but to a lesser degree. For example, when compared to earlier results in Table 3, the precision scores at ranks 5 and 20 for run R H are 0.707 and 0.502 over the ListQ set, 0.619 and 0.396 over IsA, and 0.640 and 0.410 over WikiC. Discussion : Independently of the source of queries (e.g., indepen-dent queries or query sessions) from which interpretations are ex-tracted, a noun phrase is intuitively more difficult to interpret if it is relatively more rare or more complex (i.e., longer). Additional experiments quantify the effect in run R S , by measuring the cor-relation between the presence of some extracted interpretations for an input noun phrase, on one hand; and the frequency of the noun phrase as a query (in Table 9), on the other hand. In Table 9, the effect is visible in that query frequency is higher for noun phrases with some extracted interpretations (in the left portion of the ta-ble) vs. noun phrases with none (in the right portion). For exam-ple, for the ListQ vocabulary, the average query frequency is 4.3 higher for noun phrases with some extracted interpretations. Sim-ilarly, in Figure 7, a larger fraction of the input noun phrases with a particular number of tokens have some extracted interpretations, when the number of tokens is lower rather than higher. The effect is somewhat less pronounced for, but still applicable to, the WikiC vocabulary. That a larger fraction of the longer noun phrases can be interpreted in the WikiC vocabulary is attributed to the role of w iki-templ variants in extracting interpretations that would other-wise not be available.
A large body of work addresses the more general task of open-domain information extraction from text [2, 6, 38, 11, 23, 17, 41]. Extracted relations are tuples of an argument ( yellow submarine a text fragment acting as the lexicalized relation ( and another argument ( beatles ) (cf. [11, 22]). A particular type of extracted relations are IsA relations, linking instances ( submarine ) with their class labels (  X  X eatles songs X  ) [2, 8, 37, 20, 7, 39, 32, 13].

Interpretations extracted from queries act as a potential bridge between facts, on one hand, and class labels, on the other hand, available for instances. The former might be inferred from the lat-ter and vice versa. There are two previous studies that are relevant to the task of extracting facts from existing noun phrases. First, [40] extract facts for attributes of instances, without requiring the presence of the verbal predicates usually employed [11] in open-domain information extraction. Second, in [26], relations encoded implicitly within Wikipedia categories are converted into explicit relations. As an example, the explicit relation &lt; deconstructing harry , directed , woody allen &gt; is obtained from the fact that structing harry is listed under  X  X ovies directed by woody allen X  in Wikipedia. The method in [26] relies on massive, manually-compiled knowledge, and does not attempt to interpret compound noun phrases.

Relevant interpretations (  X  X ongs composed by beatles X  ) repre-sent alternative class labels of instances ( yellow submarine may be originally associated with the class labels being interpreted (  X  X eatles songs X  ). They may be beneficial in typed search [9] or entity search [1, 32]. In that case, class labels submitted as queries may be answered by instances available for the class labels.
In previous work on the acquisition of paraphrases of compound noun phrases, most methods [19, 36] operate over documents, and may rely on text analysis tools including syntactic parsing [25]. In contrast to [28], the experiments reported here extract interpre-tations from query sessions, and show that the interpretations are more accurate than from independent queries. The interpretations are further related to paraphrase acquisition in that, similarly to output of other open-domain information extraction methods, they would benefit from grouping together extractions that are lexically distinct but semantically equivalent.

Queries are used as a textual data source in other tasks in open-domain information extraction [18, 31, 14]. In particular, query sessions are shown to be useful in paraphrase acquisition [42], rank-ing class labels extracted for instances [27] or instances extracted for class labels [3] and the disambiguation of instances mentioned in queries relative to entries in external knowledge repositories [30, 4].
The experiments described in this paper quantify the extent to which interpretations better explain the roles that modifiers play within longer noun phrases, when the interpretations are extracted from query sessions rather than independent queries.

Future work explores the role of human-curated resources such as dictionaries and encyclopedias, in guiding the acquisition of in-terpretations from documents and queries; and alternatives for com-bining interpretations from queries and from query sessions. [1] K. Balog, M. Bron, and M. de Rijke. Category-based query [2] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and [3] B. Billerbeck, G. Demartini, C. Firan, T. Iofciu, and [4] R. Blanco, G. Ottaviano, and E. Meij. Fast and [5] T. Brants. TnT -a statistical part of speech tagger. In [6] M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. [7] B. Dalvi, W. Cohen, and J. Callan. Websets: Extracting sets [8] D. Davidov and A. Rappoport. Enhancement of lexical [9] G. Demartini, T. Iofciu, and A. de Vries. Overview of the [10] P. Downing. On the creation and use of English compound [11] A. Fader, S. Soderland, and O. Etzioni. Identifying relations [12] C. Fellbaum, editor. WordNet: An Electronic Lexical [13] T. Flati, D. Vannella, T. Pasini, and R. Navigli. Two is bigger [14] M. Gamon, T. Yano, X. Song, J. Apacible, and P. Pantel. [15] M. Hearst. Automatic acquisition of hyponyms from large [16] I. Hendrickx, Z. Kozareva, P. Nakov, D.  X  S X aghdha, [17] J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. [18] A. Jain and M. Pennacchiotti. Open entity extraction from [19] N. Kim and P. Nakov. Large-scale noun compound [20] Z. Kozareva, K. Voevodski, and S. Teng. Class label [21] D. Lin and X. Wu. Phrase clustering for discriminative [22] Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni. [23] F. Mesquita, J. Schmidek, and D. Barbosa. Effectiveness and [24] J. Mitchell and M. Lapata. Composition in distributional [25] P. Nakov and M. Hearst. Semantic interpretation of noun [26] V. Nastase and M. Strube. Decoding Wikipedia categories [27] M. Pa  X sca. Ranking class labels using query sessions. In [28] M. Pa  X sca. Interpreting compound noun phrases using Web [29] P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and [30] P. Pantel and A. Fuxman. Jigs and lures: Associating web [31] P. Pantel, T. Lin, and M. Gamon. Mining entity types from [32] P. Pasupat and P. Liang. Zero-shot entity extraction from [33] M. Remy. Wikipedia: The free encyclopedia. Online [34] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. [35] R. Socher, J. Bauer, C. Manning, and A. Ng. Parsing with [36] T. Van de Cruys, S. Afantenos, and P. Muller. MELODI: A [37] R. Wang and W. Cohen. Automatic set instance extraction [38] F. Wu and D. Weld. Open information extraction using [39] W. Wu, H. Li, H. Wang, and K. Zhu. Probase: a probabilistic [40] M. Yahya, S. Whang, R. Gupta, and A. Halevy. ReNoun: [41] X. Yao and B. Van Durme. Information extraction over [42] S. Zhao, H. Wang, and T. Liu. User behaviors lend a helping
