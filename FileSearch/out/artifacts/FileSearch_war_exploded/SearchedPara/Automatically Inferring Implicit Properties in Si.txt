 A simile is a figure of speech comparing two essen-tially unlike things, typically using  X  X ike X  or  X  X s X  (Paul, 1970). Comparing fundamentally different types of entities is what makes a simile figurative (Israel et al., 2004). Similes may be closed or open (Beardsley, 1981). A closed simile explains the basis for a comparison by explicitly mentioning a shared property. For example, the simile  X  X y room is as cold as Antarctica X  gives  X  X old X  as the prop-erty shared by both the room and Antarctica. But most similes do not explicitly mention the basis for comparison, leaving people to infer what the enti-ties have in common. An open simile expressing the same comparison is  X  X y room feels like Antarc-tica X  , where the shared property of being cold is left implicit. In our study of similes in tweets, we found that 92% of similes are open similes so the property must be inferred. Our research tackles this problem of inferring the implicit property evoked by an open simile.

Inferring the basis of comparison in a simile is central to natural language understanding and metaphor interpretation. For example,  X  X ohn was like a lion in battle X  is probably a statement about John X  X  bravery or courage, not a description of John X  X  physical appearance. Methods to under-stand figurative similes could also be valuable to un-derstand metaphor in other linguistic constructions, such as predicate nominals (e.g.,  X  X e is a lion X  ). Furthermore, identifying the implicit property of a simile could be useful for sentiment analysis, be-cause similes are often used to express positive and negative feelings (Li et al., 2012). For example,  X  X ohn was like a lion in battle X  contains only neutral words, but inferring  X  X ravery X  as the implicit prop-erty suggests that the simile has positive polarity.
We designed a three step process to infer the im-plicit properties of open similes. First, we gen-erate candidate properties for a simile by harvest-ing words that are associated with its verb ( X  X vent X ) or object of comparison ( X  X ehicle X ) using a variety of methods, including syntactic patterns, dictionary definitions, and word embeddings. Each candidate property is generated from just one component of the simile. The second step of the process then eval-uates each property X  X  compatibility with the com-plementary component of the simile (event or vehi-cle). Finally, the third step of the process aggregates all of the candidates generated by different methods and ranks them based on collective evidence from the different sources. We evaluate the performance of our approach using gold standard properties pro-vided by seven human annotators. We also present an analysis of the similes in our data set with respect to their interpretive diversity (intuitively, a measure of how many plausible interpretations a simile has). We show that our method performs best on similes with low diversity, as one would expect since their implicit properties are most clear to humans. A simile typically consists of four key components: the topic or tenor (subject of the comparison), the vehicle (object of the comparison), the event (act or state), and a comparator (usually  X  X s X ,  X  X ike X , or  X  X han X ) (Niculae and Danescu-Niculescu-Mizil, 2014). For the simile  X  X he room feels like Antarc-tica X  ,  X  X oom X  is the tenor,  X  X eels X  is the event, and  X  X ntarctica X  is the vehicle. A property (shared at-tribute) can optionally be included to explicitly state how the tenor is being compared with the vehicle, (e.g.,  X  X he room is as cold as Antarctica X  ). Table 1 shows examples of open similes from our Twitter data set, along with several properties in-ferred by our human annotators (our data set will be described in Section 2.1). We represent each sim-ile using just the head noun of the tenor and vehicle, and the lemma of the event. Veale and Hao (2007) observed that when a property is explicitly given, it is usually a salient property of the vehicle. Table 1 illustrates some examples of inferred properties that are strongly associated with the vehicle (e.g.,  X  X elodic X  and  X  X ulcet X  are musical attributes).
We observed that implicit properties can be strongly evoked from the event as well. For ex-ample, most inferred properties for  X  X erson buzz like fridge X  emanate from the word  X  X uzz X , such as  X  X umming X ,  X  X ibrating X ,  X  X istracting X , and  X  X nnoy-ing X . Similarly, the tenor can also evoke properties, as we see with the inferred property  X  X quinty X  for the simile  X  X ye feel like clam X  although our obser-vation is that this is less common. The event and the tenor need to be semantically rich to evoke implicit properties. The event in many similes is a form of  X  X o be X  or a perception verb (e.g.,  X  X eels X ), which are semantically weak and contribute little. A tenor provides limited information when it is a pronoun or unknown entity (e.g.,  X  X ohn drives like a snail X  is understandable without knowing who John is).
Ultimately, an implicit property must be compat-ible with the vehicle, event, and the tenor in order for a simile to make sense. For example, Antarctica is strongly associated with the color  X  X hite X , but it would not make sense to infer the property  X  X hite X  for the simile  X  X y room feels like Antarctica X  be-cause of the verb  X  X eel X . Although in this example the tenor  X  X oom X  is still compatible with  X  X hite X  and will not help to eliminate  X  X hite X  as a property, in other similes it may (e.g., rivers can be  X  X ide X , but time can not be, so  X  X ide X  can be eliminated as an implicit property in the simile  X  X ime be like river X ).
A novel aspect of our work is that our architec-ture is designed to consider a property X  X  compatibil-ity with multiple components. In this research, for generating candidate properties and utilizing their influence for compatibility, we particularly focus on the vehicle and event terms. Initially, we generate candidate properties from the vehicle and the event separately. But the second step then evaluates each candidate property X  X  compatibility with the comple-mentary simile component. If a property was ini-tially generated from the vehicle, then we evaluate its compatibility with the event; if a property was ini-tially generated from the event, then we evaluate its compatibility with the vehicle. This approach em-phasizes the need to consider multiple components of a simile when inferring implicit properties. 2.1 Collecting Similes with Implicit Properties For our research, we created a new data set of open similes, where the property is implicit. Similes are common on Twitter, so we extracted similes from roughly 140 million English tweets collected dur-ing the time period 2/13/2013  X  4/15/2014. To iden-tify similes, we applied a part-of-speech tagger de-signed for Twitter (Owoputi et al., 2013) to tweets containing the word  X  X ike X  and applied rules to rec-ognize simple noun phrases and verb phrases. We then selected tweets matching the syntactic pattern: N P 1 V ERB like N P 2 , where N P 2 can contain only a noun and an optional indefinite article. We required similes to have a vehicle term with no pre-modifiers to avoid problems associated with corefer-ence (e.g.,  X  X he man X  or  X  X hat man X ) and to focus on vehicles that represent general concepts. We leave for future work the challenge of tackling multi-word vehicle phrases (e.g.,  X  X y room is like stepping into a hurricane X  or  X  X y room is like a boots store X  ).
This selection process extracted many similes, but it also extracted literal comparisons with no apparent property (e.g.,  X  X his flower smells like a rose X  ) and statements that are not comparisons (e.g.,  X  X  called like five times X  ). To focus on figurative similes with an implicit property, we further filtered the collec-tion to only retain similes with vehicle terms that had occurred in comparisons with an explicit property. Using the same Twitter data, we extracted nouns that appeared in the following syntactic patterns, which represent comparison constructions with an adjecti-val property: ADJ like [ a, an ] N OU N (e.g.,  X  X ed like a tomato X  ) and ADJ as [ a, an ] N OU N (e.g.,  X  X ed as a tomato X  ). We only kept similes whose ve-hicle occurred in these patterns. Finally, we filtered similes that contain a pronoun (except personal pro-nouns in the tenor, which we generalized to a  X  X er-son X  token), common person first names 1 , profan-with Twitter language such as misspellings, elon-gated words, etc. 2.2 Gold Standard Implicit Properties We developed a gold standard set of implicit proper-ties for each simile using Mechanical Turk. We pre-qualified 7 workers, who each annotated 700 similes with frequency  X  3 randomly selected from our col-lection. Each annotator was asked to provide up to 2 properties that best captured the most likely basis for comparison between the tenor and vehicle. We also provided the annotators with the option to label a simile as Invalid if it was not a simile at all (most commonly due to parse errors, such as  X  X e looks like ran X  ) or label a simile as having No Property (often due to literal or underspecified comparisons, such as  X  X he looks like my aunt X  ). The annotators were asked to give adjectives, adverbs, or verbs but oc-casionally they provided a noun. Table 1 presents sample annotated simile properties.

Among the 700 similes, a majority of the annota-tors labeled 59 of them as either Invalid or No Prop-erty, so we did not use these. We set aside 183 sim-iles (29%) as a development set and the remaining 458 similes (71%) as a test set. Our research tackles the problem of inferring prop-erties in open similes by decomposing the problem into three subtasks: (1) generating candidate proper-ties, (2) evaluating the candidate properties with re-spect to multiple simile components, and (3) aggre-gated ranking of the properties. Figure 1 illustrates our approach.

First, the vehicle and event components of a simile are used individually to generate candidate properties. We investigate a variety of candidate generation methods, including harvesting properties from syntactic structures and dictionary definitions, identifying relevant properties using statistical co-occurrence, and assessing similarity between word embedding vectors.

Second, the candidates generated by each method are evaluated based on their strength of association with the complementary component of the simile. For candidates generated from the vehicle term, we evaluate them based on their association with the event term, and vice versa. We explore three asso-ciation measures: point-wise mutual information to measure statistical co-occurrence, and vector simi-larity using single and composite word embeddings.
Third, we produce an aggregate ranking over the entire set of properties hypothesized by all of the candidate generation methods. Intuitively, we view each candidate generation method as an independent source, and look at the aggregate evidence across the set of different candidate generation methods (simi-lar to an ensemble). Each property is scored based on its average rank across the different methods, so that properties highly ranked by multiple methods are preferred. 3.1 Candidate Property Generation We generate candidate properties from the vehicle and event words of a simile. However when the event is a form of  X  X o be X  or a perception verb (taste, smell, feel, sound, look), we do not generate candi-date properties from the event because the verb is too general. Only 73 (16%) of the similes in our evalu-ation data have a verb other than  X  X o be X  or a per-ception verb. We restrict properties to be adjectives, adverbs, or verb forms that can function as nominal premodifiers (e.g.,  X  X rying baby X ,  X  X ilted lettuce X ). We explore a total of seven methods for generating candidate properties and generate candidates using our entire Twitter corpus.
 Modifying ADJ: Given a vehicle term, we extract pre-modifying adjectives. For example,  X  X ipe X  is extracted for the vehicle  X  X omato X  from the phrase  X  X ipe tomato X .
 Predicate ADJ: Given a vehicle term, we extract adjectives in predicate adjective constructions with the vehicle. For example,  X  X ed X  is extracted for the vehicle  X  X omato X  from the phrase  X  X omato is red X . Modifying ADV: Given an event term (verb), we ex-tract adverbs that precede or follow the verb. For ex-ample,  X  X mmaturely X  is extracted for the event  X  X ct X  due to the phrase  X  X cts immaturely X .
 Explicit Property: We extract properties mentioned explicitly in comparison phrases. For vehicle terms, we extract properties from phrases of the form:  X  X DJ/ADV like NP X  (e.g.,  X  X old like Antarctica X  ) and  X  X DJ/ADV as NP X  (e.g.,  X  X old as Antarctica X  ). For event terms, we extract properties from phrases of the form:  X  X ERB ADJ/ADV like X  and  X  X ERB as ADJ/ADV as X  (e.g.,  X  X eels as cold as X  ).
 Dictionary Definition: Dictionary definitions often mention salient properties associated with a word. We harvest adjectives, adverbs and verbs (function-ing as premodifiers) as candidate properties from the dictionary definitions of the vehicle and event terms. tains 5 source dictionaries: Heritage Dictionary of the English Language, Wiktionary, the Collabora-tive International Dictionary of English, The Cen-tury Dictionary and Cyclopedia, and WordNet 3.0 (Miller, 1995).
 PMI: Given a vehicle or event term, we compute point-wise mutual information (PMI) between that term and candidate properties (appearing in  X  100 tweets) in our Twitter corpus.
 Word Embedding: We train a word embedding model using our tweet collection, limiting the vo-cabulary to nouns, verbs, adjectives and adverbs that occurred in  X  100 tweets. For training, we use lows training for arbitrary context using the skip-gram model. We use 300 dimensions for the out-put word and context vectors. Candidate proper-ties are generated by selecting the words whose con-word vector using cosine similarity. To control for noisy candidates, we require that the property oc-curred with the vehicle (or event) as a bigram with frequency  X  10 in the Twitter corpus.

For each generation method, we rank the candi-dates and select the top 20 properties. For the four methods that use syntactic patterns, we calculate P(property | vehicle) based on the number of times the property and the vehicle appear together in that syntactic construction among all times the vehicle appear in that syntactic construction. We use this probability to rank the candidates. For the dictio-nary definition method, we sort the properties based on how many of the 5 dictionaries mention the prop-erty in the word X  X  definition. We break ties based on the frequency of the property in the definitions. For the word embedding-based method, we use cosine similarity scores. 3.2 Productivity of the Candidate Generation First we investigate how many candidates each method is able to generate. If a method generates too few candidates, it will not be very useful. Con-versely, if a method generates a large number of can-didates, then our ranking framework needs to be ro-bust to rank the plausible properties higher than the properties that do not fit.

Figure 2 presents statistics about the candidate properties generated by different methods. The PMI and Word Embedding-based methods were excluded here as these methods evaluate all words in the cor-pus. The methods that used the explicit property ex-traction patterns and dictionary definitions generate fewer candidates than the methods that used gen-eral syntactic structures. The trend lines in Figure 2 show that these methods do not generate more than 20 candidate properties for most similes. 3.3 Coverage of the Generated Candidates Next, we investigate the effectiveness of our candi-date generation methods. The last column of Table 2 shows candidate ranking results based on Mean Re-ciprocal Rank (MRR) for the top 20 properties pro-duced by each candidate generation method. MRR is calculated by: where S is the set of similes. We observe that the PMI method (for both vehicles and events) and the Dictionary Definition method (for events) produced low MRR scores &lt; 0.10. Therefore we decided not
One of our primary concerns is assessing the abil-ity of our candidate generation methods to generate at least some acceptable properties. We expect them to over-generate, but they need to produce at least one acceptable property or the downstream compo-nents will be helpless. To assess this, we evaluated the coverage of each candidate generation method based on the Top 10, Top 20, and Top 30 proper-ties that it produced. Coverage is the percentage of similes for which the method generates at least one gold standard property (from the human annota-tors). Table 2 shows that the Dictionary Definitions for vehicles was the best performing method for the Top 10 candidates, generating at least one accept-able property for 40% of the similes. The Modify-ing ADJ method performed best for the Top 30 can-didates, generating an acceptable property for 63% of similes. Note that the Explicit Property method performs reasonably well (40% coverage for Top 30 properties generated from vehicles and 6% coverage for properties generated from events), but clearly is not sufficient on its own, showing the limitation of harvesting explicitly stated properties.

The ALL rows show the coverage obtained by combining the property lists from all generation methods listed above in the table. The combined set of properties (Top 30) generated from vehicles yields 86% coverage, while the combined set of properties generated from events yields only 10% coverage (partly because these methods apply to only 16% of the similes), showing that vehicles are more effective for candidate generation. However, the TOTAL row shows that combining properties generated from both vehicles and events yields 88% coverage using the Top 30 candidates. The Top 20 candidates provide coverage that is nearly as good (86%) with substantially fewer properties to process downstream, so we use the Top 20 candidates for all 3.4 Ranking the Candidate Properties Using Next, we investigate whether the initial ranking re-sults in the previous step can be improved by con-sidering the second component of the simile. Intu-itively, suppose that  X  X reen X ,  X  X low X , and  X  X ndan-gered X  are generated as candidate properties from the vehicle  X  X urtle X  (e.g., for  X  X ad drives like a tur-tle X  ). Taking the event verb  X  X rive X  into account can help to rank  X  X low X  more highly than the other can-didates. We explore three criteria to rank candidates generated from one simile component based on its association with the second component (unless the event is  X  X o be X  in which case we retain the original candidate ranking because the verb is too general). PMI with second component (PMI): We calculate Pointwise Mutual Information between a candidate property and the second component of a simile. Embedding word vector similarity with the sec-ond component (EMB 1 ): We use our trained word embeddings model to calculate cosine similarity be-tween a candidate property and the second compo-nent of the simile. As before, for properties we use the context vectors.
 Embedding word vector similarity with compos-ite simile vector (EMB 2 ): For a given event and vehicle, we create a composite simile vector by per-forming element-wise addition of the vectors for the event and the vehicle, and calculate cosine similar-ity with the candidate properties. For example, for  X  X erson talks like robot X  , the vectors for  X  X alk X  and  X  X obot X  are used to create a composite vector, and the similarity of the resulting vector with a candidate property X  X  context vector is used as the ranking crite-ria. The intuition here is to capture what is common in the context distribution (Mikolov et al., 2013) of  X  X obot X  and  X  X alk X , and the context vector of a suit-able property should have strong similarity with the resulting vector. 3.5 Results for Candidate Re-ranking Table 3 presents MRR results after the initially gen-erated candidates are re-ranked using the influence of the second simile component. For comparison, the MRR results from Table 2 are also presented in the first column ( Orig ).

Influence from the second simile component as-sessed with PMI and EMB 1 improved the MRR scores for some candidate generation methods (e.g., Predicate ADJ), but did not for others (e.g., Mod-ifying ADV). However using the composite word embedding vector (EMB 2 ) to capture the common aspects in the context distributions of the event and vehicle consistently improved MRR for all candi-date generation methods. Consequently, we use the composite word embedding vector as the ranking method for each set of candidate properties. 3.6 Aggregated Ranking Finally, we need to consider all of the properties pro-duced by the various candidate generation methods. As we saw in Table 2, they produce complemen-tary sets of properties and coverage is highest when we use all of them together. To produce an aggre-gated ranking of all candidate properties, we calcu-late the harmonic mean of the rank for each individ-ual candidate generation method. This approach re-wards properties that have a consistently high rank-ing across different methods.

For comparison, we also show results for a voting method where a candidate property is ranked based on how many different methods generated it. To break ties, we used the frequency of the candidate in our Twitter corpus. 3.7 Results for Aggregated Ranking Our final results use two gold standard property sets: (1) Gd (Gold): uses the set of properties from the human annotators, and (2) Gd + WN expands Gold with WordNet synsets (words in the same synset of a gold property are added) and WordNet X  X   X  X imilar to X  relation (words that are connected to a gold prop-erty by the relation are added). The reason for using Gd + WN is to include synonyms of a gold property that would otherwise be considered wrong (e.g., if a human annotator said  X  X eautiful X  and our system said  X  X retty X ).
 The first two columns in Table 4 present MRR results for our final ranking. The results show that with both Gd and Gd + WN , our aggregated ranking using harmonic mean yields much better MRR re-sults than the individual methods and better than the Voted method, yielding our highest MRR: .33 and .41.

The last 4 columns of Table 4 present the percent-age of similes for which an acceptable property was ranked #1 (Top 1) or within the Top 5. Our aggregate ranking scheme ranks an acceptable property in the Top 1 position for 27% of similes based on Gd+WN , and inferred an acceptable property within the Top 5 positions for 58% of all similes.

For the above evaluations, any property given by the annotators is deemed correct, and any consensus that the annotators may have had is not accounted for. To address this, we retained properties with different degrees of consensus, and subdivided the evaluation data set. Each subset of the data kept sim-iles that have properties from a minimum number of annotators, and only those properties are used as the gold standard. WordNet synsets and  X  X imilar to X  re-lations are also used in determining consensus.
Figure 3 shows that for all degrees of consensus, the aggregated ranking is consistently better than the method that uses the explicit property extraction pat-terns, which was the best individual candidate gen-eration method. When properties given by at least 2 annotators are considered as the gold standard, MRR is lower than when properties given by any annota-tor are used. With higher consensus, MRR gradually increases, which is probably because the properties with high consensus have stronger association with the simile components, so are easier to infer. Our gold standard property collection confirmed our intuition that some similes have many plausible in-terpretations while others do not. We hypothesized that this should contribute to the difficulty of implicit property inference. Utsumi and Kuwabara (2005) introduced  X  X nterpretive diversity X  with the hypoth-esis that similes with more diversity in the inferred property tend to be more metaphorical, and the val-ues of salience of the properties are more uniform. They used Shannon X  X  entropy to measure the inter-pretive diversity of a simile.

To explore our hypothesis regarding difficulties associated with property inference, we first clus-ter our gold-standard annotated properties. When a property appears in the WordNet synset of another property, or if two properties are connected by the WordNet  X  X imilar to X  relation, we group the prop-erties to form property clusters. So each property cluster represents a set of words that are synonyms of each other. We aggregate frequency statistics of individual words in a cluster and measure interpre-tive diversity of a simile using Shannon X  X  entropy (here, X is the random variable representing prop-erty clusters of a simile):
Figure 4 shows the entropy curve after the 641 similes are sorted by the entropy values of their property clusters. Based on changes in the slope of the curve, we then divided the data into 3 subsets, similes with high (1 X 100 similes), medium (101 X  500 similes), and low (501 X 641 similes) interpre-tive diversity. Table 5 presents examples of sim-iles in each category. High interpretive diversity is clearly demonstrated by  X  X erson act like mom X  , showing properties with many different characteris-tics attributed to mom. Note that the properties con-tain both positive (e.g., friendly, loving) and negative (scolding, annoying) attributes. On the other side of the spectrum are similes with low interpretive diver-sity, as exemplified by  X  X hroat feel like sandpaper X  where the vocabulary of the property set is more lim-ited.

Table 6 shows that it is much harder to infer the implicit property in similes with high interpretive di-versity, demonstrated by a .19 difference in MRR score from high to low. This trend is also consis-tent when we see the percentage of similes for which the system ranks a plausible property at the topmost position (Top 1) or within the Top 5. It is possible that with low interpretive diversity, when the prop-erty distribution is unimodal or bimodal, statistical associations between a property and simile compo-nents are stronger, and so more easily discovered by our candidate generation and ranking methods. Similes have been studied in linguistics and psy-cholinguistics to understand how humans process similes, comparisons, and metaphors, and the inter-play among different components of these linguistic forms. Glucksberg et al. (1997) presented a property attribution model of metaphor comprehension where the candidate properties are selected from a vehicle and applied to a topic. Chiappe and Kennedy (2000) investigated if the number of properties varies be-tween a metaphor and its simile form. The im-pacts of semantic dimensions of tenor and prop-erty salience have been compared by Gagn  X  e (2002). Fishelov (2007) experimented with affective conno-tation and degrees of difficulty associated with un-derstanding a simile when a simile property is con-ventional or unconventional, or no property is given. Hanks (2005) manually categorized vehicle nouns of similes into semantic categories.

Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using  X  X s ADJ as a/an NOUN X  extrac-tion pattern to acquire knowledge for concept cate-gories. Veale (2012) built a knowledge-base of af-fective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the senti-ment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based tech-niques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affec-tive polarities using supervised classification, with features derived from simile components. Nicu-lae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative.

Computational approaches to work on figurative language also include figurative language identifi-cation using word sense disambiguation (Rentoumi et al., 2009), harvesting metaphors by using noun and verb clustering-based techniques (Shutova et al., 2010), interpreting metaphors by generating literal paraphrases (Shutova, 2010), etc.

Although previous research has extensively used explicit property extraction patterns for various tasks, none has explored the impact of multiple simile components for inferring properties. To our knowledge, we are the first to introduce the task of automatically inferring the implicit properties in open similes, which is fundamental to automatic un-derstanding of similes. In this work, we addressed the problem of infer-ring implicit properties in open similes. We showed that acceptable properties for most similes can be identified by harvesting properties using syntac-tic structures, dictionary definitions, statistical co-occurrence, and word embedding vectors. We then demonstrated that capturing the combined influence of a simile X  X  event and vehicle terms using a com-posite word embedding vector improved our ability to rank candidate properties. Finally, we showed that properties harvested by different methods can be aggregated and effectively ranked using the har-monic mean of rankings from the individual meth-ods. Our method for inferring implicit properties performed best on similes with low interpretive di-versity. In future work, we plan to use the inferred properties to improve affective polarity recognition in similes.
 This material is based upon work supported in part by the National Science Foundation under grants IIS-1450527 and IIS-1302668.
