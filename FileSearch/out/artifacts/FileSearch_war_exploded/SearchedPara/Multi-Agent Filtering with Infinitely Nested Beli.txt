 The existence of nested beliefs is one of the defining characteristics of a multi-agent world. As an agent acts, it often needs to reason about what other agents believe. For instance, a teacher must consider what a student knows to decide how to explain important concepts. A poker agent must think about what cards other players might have  X  and what cards they might think it has  X  in order to bet effectively. In this paper, we assume a cooperative setting where all the agents have predetermined, commonly-known policies expressed as functions of their beliefs; we focus on the problem of efficient belief update, or filtering.
 We consider the nested filtering problem in multi-agent, partially-observable worlds [6, 1, 9]. In this setting, agents receive separate observations and independently execute actions, which jointly change the hidden state of the world. Since each agent does not get to see the others X  observations and actions, there is a natural notion of nested beliefs. Given its observations and actions, an agent can reason not only about the state of the external world, but also about the other agents X  observations and actions. It can also condition on what others might have seen and done to compute their beliefs at the next level of nesting. This pattern can be repeated to arbitrary depth.
 The multi-agent filtering problem is to efficiently represent and update these nested beliefs through time. In general, an agent X  X  beliefs depend on its entire history of actions and observations. One approach to computing these beliefs would be to remember the entire history, and perform inference to compute whatever probabilities are needed at each time step. But the time required for this computation would grow with the history length. Instead, we maintain a belief state that is sufficient for predicting future beliefs and can be approximated to achieve constant-time belief updates. We begin by defining an infinite sequence of nested beliefs about the current state s t , and showing that it is sufficient for predicting future beliefs. We then present a multi-agent filtering algorithm that maintains a compact representation sufficient for generating this sequence. Although in the worst case this representation grows exponentially in the history length, we show that its size remains constant for several interesting problems. We also describe an approximate algorithm that always maintains a constant representation size (and constant-time updates), possibly at the cost of accuracy. In experiments, we demonstrate efficient and accurate filtering in a range of multi-agent domains. In existing research on partially observable stochastic games (POSGs) and Decentralized POMDPs (DEC-POMDPs) [6, 1, 9], policies are represented as direct mappings from observation histories to actions. That approach removes the need for the agents to perform any kind of filtering, but requires the specification of some particular class of policies that return actions for arbitrarily long histories. In contrast, many successful algorithms for single-agent POMDPs represent policies as functions on belief states [7], which abstract over the specifics of particular observation histories. Gmytrasiewicz and Doshi [5] consider filtering in interactive POMDPs . Their approach maintains finitely nested beliefs that are derived from a world model as well as hand-specified models of how each agent reasons about the other agents. In this paper, all of the nested reasoning is derived from a single world model, which eliminates the need for any agent-specific models.
 To the best of our knowledge, our work is the first to focus on filtering of infinitely nested beliefs. There has been significant work on infinitely nested beliefs in game theory, where Brandenburger and Dekel [2] introduced the notion of an infinite sequence of finitely nested beliefs. However, they do not describe any method for computing these beliefs from a world model or updating them over time. Another long-standing line of related work is in the epistemic logic community. Fagin and Halpern [3] define labeled graphs called probabilistic Kripke structures , and show how a graph with finitely many nodes can define an infinite sequence of nested beliefs. Building on this idea, algorithms have been proposed for answering queries on probabilistic Kripke structures [10] and on influence diagrams that define such structures [8]. However, these algorithms have not addressed the fact that as agents interact with the world over time, the set of observation sequences they could have received (and possibly the set of beliefs they could arrive at) grows exponentially. In this section, we describe the world model and define the multi-agent filtering problem. We then present a detailed example where a simple problem leads to a complex pattern of nested reasoning. 3.1 Partially observable worlds with many agents We will perform filtering given a multi-agent, decision-theoretic model for acting in a partially observable world. 1 Agents receive separate observations and independently execute actions, which jointly change the state of the world. There is a finite set of states S , but the current state s  X  S cannot be observed directly by any of the agents. Each agent j has a finite set of observations O j that it can receive and a finite set of actions A j that it can execute. Throughout this paper, we will use superscripts and vector notation to name agents and subscripts to indicate time. For example, a t  X  A j is the action for agent j at time t ; ~a t =  X  a agents; and a j 0: t = ( a j 0 , . . . , a j t ) is a sequence of actions for agent j at time steps 0 . . . t . The state dynamics is defined by a distribution p 0 ( s ) over initial states and a transition distribution state and the previous joint action. Each agent j sees only its own actions and observations. To these distributions define the joint world model: 3.2 The nested filtering problem In this section, we describe how to compute infinitely nested beliefs about the state at time t . We then define a class of policies that are functions of these beliefs. Finally, we show that the current nested belief for an agent i contains all of the information required to compute future beliefs. Throughout the rest of this paper, we use a minus notation to define tuples indexed by all but one agent. For example, h  X  i 0: t and  X   X  i are tuples of histories and policies for all agents k 6 = i . We define infinitely nested beliefs by presenting an infinite sequence of finitely nested beliefs. For beliefs of all the other agents (what the other agents believe about the state of the world). We can compute the tuple of zeroth-level beliefs b  X  i, 0 t for all agents k 6 = i by summing the probabilities of all histories h  X  i 0: t that lead to these beliefs (that is, such that b  X  i, 0 t = B  X  i, 0 ( h  X  i 0: t ) ): The delta function  X  (  X  ,  X  ) returns one when its arguments are equal and zero otherwise. For example, at level 2, the function returns a joint distribution over: the state, what the other agents believe about the state, and what they believe others believe. Again, these beliefs are computed by summing over histories for the other agents that lead to the appropriate level n  X  1 beliefs: beliefs each agent k could hold at time t  X  each arising from one of the possible histories h k 0: t . Define b i,  X  t = B i,  X  ( h i 0: t ) to be the infinite sequence of nested beliefs generated by computing B that can be used directly by a filtering algorithm. We will assume that the policies  X  i are represented at arbitrary parts of the infinite sequence b i,  X  t and returns a distribution over actions. We will see examples of this type of policy in the next section. Under this assumption, b i,  X  t is a sufficient statistic for predicting future beliefs in the following sense: To prove this result, we need to demonstrate a procedure that correctly computes the new belief given only the old belief and the new action and observation. The filtering algorithm we will present in Sec. 4 achieves this goal by representing the nested belief with a finite structure that can be used to generate the infinite sequence, and showing how these structures are updated over time. 3.3 Extended Example: The Tiger Communication World We now describe a simple two-agent  X  X iger world X  where the optimal policies require the agents to coordinate their actions. In this world there are two doors: behind one randomly chosen door is a hungry tiger, and behind the other is a pile of gold. Each agent has unique abilities. Agent l (the tiger listener) can hear the tiger roar, which is a noisy indication of its current location, but cannot open the doors. Agent d (the door opener) can open doors but cannot hear the roars. To facilitate communication, agent l has two actions, signal left and signal right, which each produce a unique observation for agent d . When a door is opened, the world resets and the tiger is placed behind a randomly chosen door. To act optimally, agent l must listen to the tiger X  X  roars until it is confident about the tiger X  X  location and then send the appropriate signal to agent d . Agent d must wait for this signal and then open the appropriate door. Fig. 1 shows a pair of policies that achieve this desired interaction and depend only on each agent X  X  level-zero beliefs about the state of the world. However, as we will see, the agents cannot maintain their level-zero beliefs in isolation. To correctly update these beliefs, each agent must reason about the unseen actions and observations of the other agent. Consider the beliefs that each agent must maintain to execute its policies during a typical scenario. Assume the tiger starts behind the left door. Initially, both agents have uniform beliefs about the location of the tiger. As agent d waits for a signal, it does not gain any information about the tiger X  X  location. However, it maintains a representation of the possible beliefs for agent l and knows that l is receiving observations that correlate with the state of the tiger. In this case, the most likely outcome is that agent l will hear enough roars on the left to do a  X  X ignal left X  action. This action produces an observation for agent d which allows it to gain information about l  X  X  beliefs. Because agent d has maintained the correspondence between the true state and agent l  X  X  beliefs, it can now infer that the tiger is more likely to be on the left (it is unlikely that l could have come to believe the tiger was on the left if that were not true). This inference makes agent d confident enough about the tiger X  X  location to open the right door and reset the world. Agent l must also represent agent d  X  X  beliefs, because it never receives any observations that indicate what actions agent d is taking. It must track agent d  X  X  belief updates to know that d will wait for a signal and then immediately open a door. Without this information, l cannot predict when the world will be reset, and thus when it should disregard past observations about the location of the tiger.
 Even in this simple tiger world, we see a complicated reasoning pattern: the agents must track each others X  beliefs. To update its belief about the external world, each agent must infer what actions the other agent has taken, which requires maintaining that agent X  X  beliefs about the world. Moreover, updating the other agent X  X  beliefs requires maintaining what it believes you believe. Continuing this reasoning to deeper levels leads to the infinitely nested beliefs defined in Sec. 3.2. However, we will never explicitly construct these infinite beliefs. Instead, we maintain a finite structure that is sufficient to recreate them to arbitrary depth, and only expand as necessary to compute action probabilities. on nested beliefs. This algorithm is applicable in the cooperative setting where there are commonly a set of Sparse Distributions over Sequences of past states, actions, and observations.
 Sequence distributions. The SDS filter deals with two kinds of sequences: histories h j 0: t = ( a fore acting at time t ; a trajectory is a trace of the states and joint actions through time t . The filter for agent i maintains the following sequence sets : a set X of trajectories that might have occurred so far, and for each agent j (including i itself), a set H j of possible histories. One of the elements of H i is marked as being the history that i has actually experienced. The SDS filter  X   X  j distributions represent what agent j would believe about the possible sequences of states and other agents X  actions given h j 0: t . The  X  j distributions represent the probability of j receiving the observations in h j 0: t if the trajectory x 0: t had actually happened. The insight behind the SDS filter is that these sequence distributions can be used to compute the nested belief functions B i,n ( h i 0: t ) from Sec. 3.2 to arbitrary depth. The main challenge is that sets of possible histories and trajectories grow exponentially with the time t . To avoid this blow-up, the SDS filter does not maintain the complete set of possible sequences. We will see that some sequences can be discarded without affecting the results of the belief computations. If this pruning is insufficient, the SDS filter can drop low-probability sequences and perform approximate filtering. A second challenge is that if we represent each sequence explicitly, the space required grows linearly with t . However, the belief computations do not require the details of each trajectory and history. To compute beliefs about current and future states, it suffices to maintain the sequence distributions  X  j and  X  j defined above, along with the final state s t in each trajectory. The SDS filter maintains only this information. 3 For clarity, we will continue to use full sequence notation in the paper. In the rest of this section, we first show how the sequence distributions can be used to compute nested beliefs of arbitrary depth. Then, we show how to maintain the sequence distributions. Finally, we present an algorithm that computes these distributions while maintaining small sequence sets. The nested beliefs from Sec. 2.2 can be written in terms of the sequence distributions as follows: At level zero, we sum over the probabilities according to agent j of all trajectories with the correct final state. At level n , we perform the same outer sum, but for each trajectory we sum the proba-bilities of the histories for agents k 6 = j that would lead to the beliefs we are interested in. Thus, the sequence distributions at time t are sufficient for computing any desired element of the infinite belief sequence B j,  X  ( h j 0: t ) for any agent j and history h j 0: t .
 Updating the distributions. The sequence distributions are updated at each time step t as follows. The values of  X  j on length-t histories are computed from existing  X  j values by multiplying in the probability of the most recent observation. To extend  X  j to length-t trajectories, we multiply in the probability of the state transition and the probability of the agents X  actions given the past trajectory: Here, to predict the actions for agent k , we take an expectation over its possible histories h k 0: t  X  1 (according to the  X  k distribution from the previous time step) of the probability of each action a t  X  1 given the beliefs B B functions of the zero-level beliefs. The necessary entries are computed from the the previous  X  and  X  distributions as described in Eqs. 2 and 3. This computation is not prohibitive because, as we will see later, we only consider a small subset of the possible histories.
 Returning to the example tiger world, we can see that maintaining these sequence distributions will allow us to achieve the desired interactions described in Sec. 3.3. For example, when the door opener receives a  X  X ignal left X  observation, it will infer that the tiger is on the left because it has done the reasoning in Eq. 6 and determined that, with high probability, the trajectories that would have led the tiger listener to take this action are the ones where the tiger is actually on the left. Filtering algorithm. We now consider the challenge of maintaining small sequence sets. Fig. 2 provides a detailed description of the SDS filtering algorithm for agent i . The filter is initialized with empty histories for each agent and trajectories with single states that are distributed according to the prior. At each time t , Step 1 extends the sequence sets, computes the sequence distributions, and records agent i  X  X  history. Running a filter with only this step would generate all possible sequences. Step 2 introduces three operations that reduce the size of the sequence sets while guaranteeing that Eqs. 2 and 3 still produce the correct nested beliefs at time t . Step 2(a) removes trajectories and histories when all the agents agree that they are impossible; there is no reason to track them. For example, in the tiger communication world, the policies are such that for the first few time steps each agent will always listen (to the tiger or for signals). During this period all the trajectories where other actions are taken are known to be impossible and can be ignored. Step 2(b) merges histories for an agent j that lead to the same beliefs. This is achieved by arbitrarily selecting one history to be deleted and adding its  X  j probability to the other X  X   X  j . For example, as the tiger listener hears roars, any two observation sequences with the same numbers of roars on the left and right provide the same information about the tiger and can be merged. Step 2(c) resets the filter if the marginal over states at time t has become commonly known to all the agents. For example, when both agents know that a door has been opened, this implies that the world has reset and all previous trajectories and histories can be discarded. This type of agreement is not limited to cases where the state of the world is reset. It occurs with any distribution over states that the agents agree on, for example when they localize and both know the true state, even if they disagree about the trajectory of past states. Together, these three operators can significantly reduce the size of the sequence sets. We will see in the experiments (Sec. 5) that they enable the SDS filter to exactly track the tiger communication world extremely efficiently. However, in general, there is no guarantee that these operators will be enough to maintain small sets of trajectories and histories. Step 3 introduces an approximation by removing low-probability sequences and normalizing the belief distributions. This does guarantee that we will maintain small sequence sets, possibly at the cost of accuracy. In many domains we can ignore unlikely histories and trajectories without significantly changing the current beliefs. In this section, we describe the performance of the SDS algorithm on three nested filtering problems. Tiger Communication World. The tiger communication world was described in detail in Sec. 3.3. Fig. 3(a) shows the average computation time used for filtering at each time step. The full algorithm (SDS) maintains a compact, exact representation without any pruning and takes only a fraction of a second to do each update. The graph also shows the results of disabling different parts of Step 2(a-c) of the algorithm (for example, SDS -a,-b,-c does not do any simplifications from Step 2). Without these steps, the algorithm runs in exponential time. Each simplification allows the algorithm to perform better, but all are required for constant-time performance. Since the SDS filter runs without the pruning in Step 3, we know that it computes the correct beliefs; there is no approximation error. 4 Box Pushing. The DEC-POMDP literature includes several multi-agent domains; we evaluate SDS on the largest of them, known as the box-pushing domain [9]. In this scenario, two agents interact in a 3x4 grid world where they must coordinate their actions to move a large box and then independently push two small boxes. The state encodes the positions and orientations of the robots, as well as the locations of the three boxes. The agents can move forward, rotate left and right, or stay still. These actions fail with probability 0.1, leaving the state unchanged. Each agent receives deterministic observations about what is in the location in front of it (empty space, a robot, etc.). We implemented policies for each agent that consist of a set of 20 rules specifying actions given its zeroth-level beliefs about the world state. While executing their policies, the agents first coordinate to move the large box and then independently move the two small boxes. The policies are such that, with high probability, the agents will always move the boxes. There is uncertainty about when this will happen, since actions can fail. We observed, in practice, that it rarely took more than 20 steps. Fig. 3(b) shows the running time of the SDS filter on this domain, with various pruning parameters ( N = 10 , 50 , 100 ,  X  in Step 3). Without pruning ( N =  X  ), the costs are too high for the filter to move beyond time step five. With pruning, however, the cost remains reasonable. Fig. 3(c) shows the error incurred with various degrees of pruning, in terms of the difference between the estimated zeroth-level beliefs for the agents and the true posterior over physical states given their observations. 5 Note that in order to accurately maintain each agent X  X  beliefs about the physical state X  X hich includes the position of the other robot X  X he filter must assign accurate probabilities to unobserved actions by the other agent , which depend on its beliefs. This is the same reasoning pattern we saw in the tiger world where we are required to maintain infinitely nested beliefs. As expected, we see that more pruning leads to faster running time but decreased accuracy. We also find that the problem is most challenging around time step ten and becomes easier in the limit, as the world moves towards the absorbing state where both agents have finished their tasks. With N = 100 , we get high-quality estimates in an acceptable amount of time.
 Noisy Muddy Children. The muddy children problem is a classic puzzle often discussed by re-searchers in epistemic logic [4]. There are n agents and 2 n possible states. Each agent X  X  forehead can be either muddy or clean, but it does not get any direct observations about this fact. Initially, it is commonly known that at least one agent has a muddy forehead. As time progresses, the agents fol-low a policy of raising their hand if they know that their forehead is muddy; they must come to this conclusion given only observations about the cleanliness of the other agents X  foreheads and who has raised their hands (this yields 2 2 n possible observations for each agent). This puzzle is represented in our framework as follows. The initial knowledge is encoded with a prior that is uniform over all states with in which at least one agent is muddy. The state of the world never changes. Observations about the muddiness of the other agents are only correct with probability  X  , and each agent raises its hand if it assigns probability at least 0.8 to being muddy.
 When there is no noise,  X  = 1 . 0 , the agents behave as follows. With m  X  n muddy agents, everyone waits m time steps and then all of the muddy agents simultaneously raise their hands. 6 The SDS filter exhibits exactly this behavior and runs in reasonable time, using only a few seconds per filtering step, for problem instances with up to 10 agents without pruning. We also ran the filter on instances with noise (  X  = 0 . 9 ) and up to 5 agents. This required pruning histories to cope with the extremely large number of possible but unlikely observation sequences. The observed behavior is similar to the deterministic case: eventually, all of the m muddy agents raise their hands. In expectation, this happens at a time step greater than m , since the agents must receive multiple observations before they are confident about each other X  X  cleanliness. If one agent raises its hand before the others, this provides more information to the uncertain agents, who usually raise their hands soon after. We have considered the problem of efficient belief update in multi-agent scenarios. We introduced the SDS algorithm, which maintains a finite belief representation that can be used to compute an infinite sequence of nested beliefs about the physical world and the beliefs of other agents. We demonstrated that on some problems, SDS can maintain this representation exactly in constant time per filtering step. On more difficult examples, SDS maintains constant-time filtering by pruning low-probability trajectories, yielding acceptable levels of approximation error.
 These results show that efficient filtering is possible in multi-agent scenarios where the agents X  policies are expressed as functions of their beliefs, rather than their entire observation histories. These belief-based policies are independent of the current time step, and have the potential to be more compact than history-based policies. In the single-agent setting, many successful POMDP planning algorithms construct belief-based policies; we plan to investigate how to do similar belief-based planning in the multi-agent case.

