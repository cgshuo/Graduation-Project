 Pinar Donmez pinard@cs.cmu.edu Jaime G. Carbonell jgc@cs.cmu.edu Learning to rank has recently drawn broad attention among machine learning researchers (Joachims, 2002; Freund et al., 2003; Cao et al., 2006). The objec-tive of rank learning is to induce a mapping (ranking function) from a predefined set of instances to a set of partial (or total) orders. For instance, in recom-mendation systems each customer is represented with a set of features ranging from the income level to age and her preference order over a set of products (e.g. movies in Netflix). The ranking task is to learn a map-ping from the feature space to a set of permutations of the products. The applications include document re-trieval, collaborative filtering, product rating, and so on. In this paper, we are interested in IR applications, and focus on document retrieval. A number of queries are provided such that each query is associated with an ordering of documents indicating the relevance of each document to the given query. Like many other ranking applications, this requires a human expert to carefully examine the documents in order to assign relevance-based permutations. It is often unrealistic to spend extensive human effort and money for label-ing in ranking. Thus, it is crucial to design methods that will considerably reduce the labeling effort with-out significantly sacrificing ranking accuracy. The active learning paradigm addresses this type of problem. The central idea is to start with only a small amount of labeled examples and sequentially select new examples to be labeled by an oracle. The selected examples are then added to the training set. It is clear that labeling data in ranking requires a complete (or partial) ordering of data whereas in classification la-beling considers only absolute class assignments. The target domain of a set of permutations is more com-plex than that of absolute classes. Hence, it is even more crucial to select the most informative examples to be labeled in order to learn a ranking model using fewer labeled examples.
 In this paper, we propose a novel active sampling framework for SVM rank learning (Joachims, 2002), or RankSVM in short, and RankBoost (Freund et al., 2003). The proposed method considers the capacity of an unlabeled example to update the current model if rank-labeled and added to the training set. We show that this capacity can be defined as a function that estimates the error of a ranker introduced by the ad-dition of a new example. The capacity function takes different forms in RankSVM and RankBoost due to different formulations of the ranking function. For example in the case of RankSVM, the ranking func-tion is defined via a normal vector which is a weighted sum of the support vectors whereas the ranking func-tion is a weighted sum of weak learners in RankBoost. However, in both cases, the proposed strategy selects the samples which are estimated to produce a faster convergence from the current predicted ranking to the true ranking. Our empirical evaluations on two bench-mark corpora from topic distillation tasks in TREC competitions show a significant advantage favoring our method against the margin-based sampling heuristic of (Brinker, 2004; Yu, 2005) and a random sampling baseline.
 The rest of the paper is organized as follows: Section 2 provides a brief literature review to the related work. Section 3 motivates the choice of the proposed active learning framework and introduces two novel methods for active learning in the RankSVM and RankBoost settings. In Section 4, we report the experimental re-sults and demonstrate the effectiveness of our methods on benchmark datasets. Finally, we offer our conclu-sions and next steps in Section 5. A number of strategies have been proposed for active learning in the classification framework. Some of those center around the version space (Mitchell, 1982) reduc-tion principle (Tong &amp; Koller, 2000): selecting unla-beled instances that limit the volume of the version space the most, or equivalently selecting the ones with the smallest margin. Some of the others adopt the idea of reducing the generalization error (Roy &amp; McCallum, 2001; Xu et al., 2003; Nguyen &amp; Smeulders, 2004; Don-mez et al., 2007): the selection of the unlabeled data that has the highest affect on the test error, i.e. points in the maximally uncertain and highly dense regions of the underlying data distribution (Xu et al., 2003; Nguyen &amp; Smeulders, 2004; Donmez et al., 2007). Unfortunately, it is not straightforward to extend these theoretical principles to ranking problems. The gen-eralization power of ranking functions is measured by different evaluation metrics than the ones used for clas-sification. Moreover, the classical performance metrics for ranking, such as MAP (Mean Average Precision), precision at the k th rank cut-off, NDCG (Normalized Discounted Cumulative Gain), etc., are harder to di-rectly optimize than the classical loss functions for classification, i.e. log loss, 0/1 loss, squared loss, etc. Recently, there have been attempts to address the challenges in active sampling for rank learning. Brinker (2004) uses a notion of the margin as an ap-proximation to reducing the volume of the version space. The margin in the ranking scenario is defined as the minimum difference of scores between two in-stances assuming the ranking solution is a real-valued scoring function. Yu (2005) adopted the same notion of margin for SVM rank learning and proposed a batch mode for instance selection that minimizes the sum of the rank score differences of all data pairs within a set of samples. Yu (2005) proposed an efficient im-plementation which considers only the rank-adjacent pairs and showed that this strategy is optimal in terms of selecting the most ambiguous set of samples with re-spect to the ranking function. The major drawback of this margin-based sampling method of (Brinker, 2004; Yu, 2005) is that a scoring function for ranking may assign very similar scores to instances with the same rank label since the ranking function does not distin-guish between the relative order of two relevant or two non-relevant examples. However, such instances do not carry any additional information for the rank learner to distinguish between the relevant and the non-relevant data.
 Another recent development in active rank learning is the divergence-based sampling method of (Amini et al., 2006). The proposed method selects the sam-ples at which two different ranking functions maxi-mally disagree. One of the two functions is the current ranking function trained on the labeled data, and the other is a randomized function obtained by cross vali-dation. The divergence-based strategy is effective only when provided with a sufficiently large initial labeled set, which is impractical for many real-world ranking applications, such as document retrieval. 3.1. Motivation This section presents a novel method for active learn-ing using RankSVM and RankBoost. Roy and McCal-lum (2001) argue that an optimal active learner is the one that asks for the labels of the examples that, once incorporated into training, would result in the lowest expected error on the test set. The expected error on the test set can be estimated using the posterior dis-tribution  X  P D ( y | x ) of class labels estimated from the training set using some loss function L Their aim is then to select the point x  X  such that when added to the training set with a chosen label y  X  , the classifier trained on the new set { D + ( x  X  , y  X  ) } would have less error than any other candidate x . Since the true label y  X  is unknown, the expectation calculation is carried out by calculating the estimated error for each possible label y  X  Y , and then taking the average weighted by the current learner X  X  posterior  X  P
D ( y | x ). The naive implementation of this method would be quite inefficient and almost intractable on large datasets. Roy and McCallum (2001) address this problem using fast updates for a Naive Bayes classifier. Although efficient re-training procedures are available for some learners such as SVMs (Cauwenberghs &amp; Pog-gio, 2000), it would still be infeasible for ranking tasks, especially considering the interactive nature of rank-ing systems. In this paper, we propose a method to estimate how likely the addition of a new example will result in the lowest expected error on the test set with-out any re-training on the enlarged training set . Our method is based on the likelihood of an example to change the current hypothesis significantly. There are a number of reasons why we believe this is a reasonable indicator for estimating that error:  X  Adding a new data point to the labeled set can  X  The more significant that change, the greater  X  We note that a big change in the current hypoth-In the following sections, we briefly review the RankSVM and the RankBoost algorithms and propose a novel active learning method for each. 3.2. Preliminaries Assume the data is represented as a set of feature vec-tors ~x  X  R d and corresponding labels (ranks) y  X  Y = { r 1 , r 2 , ..., r n } where n denotes the number of ranks. We assume binary relevance in this paper, though our framework can be generalized to multi-level ranking scenarios as long as the rank learner works on pair-wise preference relationships, which is the case for the majority of rank learning algorithms. Features are nu-merical values for attributes in the data. Assume fur-ther that there exists a preference relationship between data vectors such that y i  X  y j denotes ~x i is ranked higher than ~x j . A perfect ranking function f  X  F preserves the order relationships between instances: Suppose we are given a set of instances D = { ( ~x i , y i ( ~x i , y i )  X  X  X  Y } m i =1 . The objective for rank learning is to learn a mapping f : X  X  Y 7 X  R that minimizes a given loss function on the training data. 3.3. SVM Rank Learning Assume f  X  F is a linear function, i.e. f ( ~x ) = h ~w, ~x i , that satisfies The SVM model targeting this problem can be formu-lated as a Quadratic Optimization problem: The above optimization can be equivalently written by re-arranging the constraints and substituting the trade-off parameter C for  X  = 1 2 C as follows: where [ . . . ] + indicates the standard hinge loss. ~x 1 is a pairwise difference vector whose label z is positive, i.e., z = +1 if ~x 1  X  ~x 2 and z =  X  1 otherwise. K is the total number of such pairs in the training set. Fi-nally, a ranked list is obtained by sorting the instances according to the output of the ranking function in de-scending order. 3.4. Active Sampling for RankSVM Let us consider a candidate example ~x  X  U , where U is the set of unlabeled examples. Assume ~x is incorporated into the labeled set with a rank label y  X  Y . We denote the total loss on the instance pairs that include ~x by a function of ~x and ~w , i.e. D ( ~x, ~w ) = P J y j =1 [1  X  z j h ~w, ~x j  X  ~x i ] + where J number of examples in the training set with a differ-ent label than the label y of ~x . For instance, J y is the number of negative(non-relevant) examples in the training set if y is assumed to be positive(relevant), and vice versa. The objective function to be mini-mized by RankSVM then becomes: min Assume ~w  X  is the solution to the optimization in Equa-tion 4, and it is unique. Burges and Crisp (2000) show the necessary and sufficient conditions for the unique-ness of the SVM solution. There are only rare cases where uniqueness does not hold, thus it is a rather safe assumption to make. Since we do not actually re-run the optimization problem on the enlarged data, we restrict ourselves to the current solution(hypothesis) of adding each candidate instance on the training loss using the current solution to tell how much incorpo-rating x into the labeled set is likely to change the current hypothesis. First, let us consider two cases. 1. Assume ~w  X  = argmin ~w D ( ~w, ~x ) 2. Assume ~w  X  6 = argmin ~w D ( ~w, ~x ) We now study the second case in more detail. Let  X  ~w be the weight vector that minimizes D ( ~w, ~x ), i.e.  X  ~w = argmin ~w D ( ~w, ~x ). Then, as the difference k ~w  X   X  increases it becomes less likely that ~w  X  is optimal for Equation 5. In other words, the current solution ~w  X  is in most need of updating in order to compensate for the loss on the new pairs. Let us write  X  ~w in terms of Minimizing D ( ~w, ~x ) requires working with the hinge loss, the direct optimization of which is difficult due to the discontinuity of the derivative. However, it can still be solved using a gradient-descent-type algo-rithm 1 . Recall the objective function to be minimized: The derivative of the above equation with respect to ~w at a single point ~x j ,  X  ~w j , is: Algorithm 1 RankBoost
Input: initial data distribution D 1 over X  X  X for t = 1 to T do end for We substitute ~w in Equation 7 for the current weight vector ~w  X  to estimate how the solution of Equation 6 deviates from it, i.e. k ~w  X   X   X  ~w k = k  X  ~w k . We can now write the magnitude of the total derivative as a function of ~x and the rank label y as follows: g ( ~x, y ) = k  X  ~w k = X g ( ~x, y ) estimates how likely the current hypothesis is to be updated to minimize the loss introduced as a result of the addition of the example ~x with the rank label y . Thus, we use this function to estimate the ability of each unlabeled candidate example to change the current learner if incorporated into training. Since the true labels of the candidate examples are unknown, we use the current learner to estimate the true label probabilities. Then, we can take the expectation of g ( ~x, y ) by taking the weighted sum over the current posterior  X  P ( y | ~x ) for all y  X  Y . Among all the un-labeled examples, we choose the one with the highest value for that expectation: 3.5. RankBoost Learning RankBoost is a boosting algorithm designed for rank-ing problems. Like all algorithms in boosting fam-ily, RankBoost learns a weak learner on each round, and maintains a distribution D t over the ranked pairs, X  X  X , to emphasize the pairs whose relative order is the hardest to learn. An outline of the algorithm is given as Algorithm 1. Z t is a normalization constant, and the final ranking is a weighted sum of the weak rankings H ( ~x ) = P T t =1  X  t h t ( ~x ). For more details and theoretical discussion see (Freund et al., 2003). 3.6. Active Sampling for RankBoost This section introduces a similar method for active sampling for the RankBoost algorithm (Freund et al., 2003). Consider a candidate point ~x  X  U and assume it is merged into the training set with rank label y  X  Y . Unlike RankSVM, RankBoost algorithm does not di-rectly operate with an optimization function. But the ranking loss with respect to the distribution at time t can be written as: where I is defined to be 1 if the predicate holds and 0 otherwise. Hence, this is a sum over misranked pairs, assuming ~x 1  X  ~x 2 . The distribution at time T + 1 can be written as: The initial distribution term D 1 can be dropped with-out loss of generality, assuming it is uniform (which is reasonable given the fact that we do not have prior in-formation about the data). Similarly to RankSVM, we would like to estimate how much the current ranking function would change if the point ~x were in the train-ing set. We estimate this deviation by the difference in the ranking loss after enlarging the current labeled set with each example ~x  X  U . The ranking loss on the enlarged set with respect to the distribution D T +1 is:
X Note that the rank label y of ~x is assumed to be pos-itive (relevant) with ~x  X  ~x j in this case. We have a similar calculation for the case where y is assumed to be negative (non-relevant). We adopt the distribution D
T +1 because 1) it can easily be written in terms of the final ranking function, 2) it contains information about which pairs remain the hardest to determine af-ter the iterative weight updates. Then, the difference in the ranking loss between the current and the aug-mented set simply becomes:  X  L ( ~x, y = 1) = X This difference indicates how much the current rank-ing function needs to be modified to compensate for the loss incurred by including this example. Note that I ( x  X  0)  X  e x for  X  x  X  R (Freund et al., 2003). There-fore, the upper bound on  X  L can be written as:  X  L ( ~x, y =  X  1) can be similarly bounded, e.g. loss difference can be estimated by taking the expec-tation over the possible rank labels of ~x with respect to the current ranker X  X  posterior,  X  P ( y | ~x ): E
P ( X  L ( ~x )) = Note the similarity with Equation 9 in the SVM case. Finally, we select the instance ~x that has the highest expected loss differential, e.g. ~x  X  = argmax ~x E  X  P ( X  L ( ~x )). For notational clarity, we take the maximum over the upper bound in Equation 14 as follows: For simplicity, we leave out the normalization constant Q t Z t since we are interested in the relative expecta-tion rather than the absolute expectation. 3.7. Final Selection The sample selection in both RankSVM and Rank-Boost requires estimating a posterior label distribu-tion. We adopt a sigmoid function to estimate that posterior in the SVM case, as suggested by (Platt, 1999): where f ( ~x ) is the real-valued score of the ranking al-gorithm, and C is a constant for calibrating the esti-mate. C is tuned on a separate corpus not used for evaluation in this paper. The final ranking in Rank-Boost is a sum of weak learners with the corresponding weights. When the weights are too small (or too large), the posterior gets close to the extreme (either 0 or 1) regardless of the example. Hence, we normalize the RankBoost output dividing by the maximum possible rank score without changing the rank order: Note max ~x H ( ~x ) = max ~x P T t =1  X  t h t ( ~x ) = P since the weak learner h t ( ~x ) in RankBoost is a { 0,1 } -valued function defined on the ordering information provided by the corresponding feature (Freund et al., 2003). 4.1. Data and Settings We used two datasets in the experiments: TREC 2003 and 2004 topic distillation tasks in LETOR (Liu et al., 2007). The topic distillation task in TREC is very sim-ilar to web search where a page is considered relevant to a query if it is an entry page of some web site rele-vant to the query. The relevance judgments on the web pages with respect to the queries are binary. There are 44 features, e.g. content and hyperlink features, each of which is extracted from each document-query pair and normalized into [0 , 1]. There are 50 and 75 queries with 1% and 0.6% relevant documents in TREC03 and TREC04, respectively. The total number of docu-ments per query is  X  1000 for both datasets. We used the standard train/test splits over 5 folds in LETOR. For each fold, we randomly picked 16 documents in-cluding exactly one relevant document per query for initial labeling. The remaining training data is consid-ered as the unlabeled set. We compared our method with the margin-based sampling of (Brinker, 2004; Yu, 2005) and random sampling baselines. Each method selects 5 documents per query for labeling at each round, e.g. our method selects the top 5 documents according to the criteria in Equation 9 and 16. Then, the ranking function is re-trained, and evaluated on the test set. This process is repeated for 25 iterations which corresponds to labeling only  X  15% of the entire training data. The reported results are averaged over 5 folds.
 We adopted two standard, widely used performance metrics for evaluation, namely the Mean Average Precision (MAP) and the Normalized Discounted Cumulative Gain (NDCG) (J  X arvelin &amp; Kek  X al  X ainen, 2002). For a single query, average precision is de-fined as the average of the precision as computed at each rank for all relevant documents: AP = is the number of documents retrieved, rel () is a bi-nary function on the relevance of a given rank, and the rank cut-off r . MAP is obtained by averaging the AP values for all queries. NDCG is cumulative and dis-counted since the overall utility of a list is measured by the sum of the gain of each relevant document, but the gain is discounted as a function of rank position. The NDCG value of a rank list at position n is given is the rank of the j th document in the list, and Z n is the normalization constant so that a perfect ranking yields an NDCG score of 1. 4.2. Results Figure 1 and 2 plot the performance of the pro-posed method (denoted by DiffLoss ), and as compara-tive baselines, the margin-based sampling and random sampling strategies on TREC 2003 and 2004 datasets. DiffLoss has a clear advantage over margin-based and random sampling in all cases with respect to differ-ent evaluation metrics. The differences over the en-tire operating range are also statistically significant ( p &lt; 0 . 0001) according to a two-sided paired t-test at 95% confidence level. DiffLoss especially achieves 30% relative improvement over the margin-based sampling for RankSVM on TREC 2003 dataset.
 The horizontal line in each figure indicates the perfor-mance if all the training data was used, which we call the  X  X ptimal X  performance. The performance of Dif-fLoss for RankBoost is comparable to the  X  X ptimal X  on TREC 2003 and 2004 datasets. In case of RankSVM, DiffLoss is close to the  X  X ptimal X  on TREC 2003, and outperforms it on TREC 2004 dataset. More precisely, DiffLoss using RankSVM reaches the optimal perfor-mance (even surpassing it on TREC 2004) after 10 rounds of labeling on average (labeling 5 documents per query at each round). DiffLoss using RankBoost, on the other hand, reaches 95% and 90% of the optimal performance on MAP and NDCG@10, respectively on TREC 2004 dataset after 10 rounds. This suggests that carefully chosen samples might lead to a higher level of accuracy than blindly using large amounts of training data. This is an important development over traditional supervised rank learning since it not only reduces the expensive labeling effort, but also may lead to greater generalization power. As follow-up work, we intend to explore methods that will automatically tell the sampling algorithm when to stop so that maximum gain with minimum cost is obtained, as well as explor-ing the underlying criteria for measuring the quality of actively selected examples.
 We conducted another set of experiments to test the hypothesis that selecting a diverse set of samples might lead to better results. We adopted the maximal marginal relevance principle of (Carbonell &amp; Gold-stein, 1998), originally proposed for text summariza-tion. The idea is to select samples for labeling such that they have both the maximum potential to change the current ranking function and are maximally dis-similar to each other. See (Carbonell &amp; Goldstein, 1998) for more details. However, incorporating this di-versity principle into our selection criteria only slightly improved our results at the very beginning of the learn-ing curve, but the improvement vanished afterwards. Thus, we do not report these results here in this paper. We proposed two novel active sampling methods based on SVM rank learning and RankBoost. Our frame-work relies on the estimated risk of the ranking func-tion on the labeled set after adding a new instance with all possible labels. The samples with the largest ex-pected risk(loss) differential are selected to maximize the degree of learning at the fastest rate. Empirical results on two standard test collections indicate that our method significantly reduces the required number of labeled examples to learn an accurate ranking func-tion. Possible extensions of this work include a study of the risk minimization in terms of direct optimization of ranking performance metrics, such as MAP, NDCG, precision@k, etc. and self-regulating algorithms that can decide when to terminate.
 This material is based in part upon work supported by the Defense Advanced Projects Research Agency (DARPA) under Contract No. FA8750-07-D-0185. Amini, M., Usunier, N., Laviolette, F., Lacasse, A., &amp;
Gallinari, P. (2006). A selective sampling strategy for label ranking. ECML  X 06 (pp. 18 X 29).
 Brinker, K. (2004). Active learning of label ranking functions. ICML  X 04 (pp. 17 X 24).
 Burges, C., &amp; Crisp, D. (2000). Uniqueness of the svm solution. NIPS  X 00 (pp. 223 X 229).
 Cao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y., &amp; Hon,
H.-W. (2006). Adapting ranking svm to document retrieval. Proceedings of the international ACM SI-
GIR Conference on Research and Development in information retrieval (SIGIR X 06) (pp. 186 X 193). Carbonell, J., &amp; Goldstein, J. (1998). The use of mmr, diversity-based reranking for reordering documents and producing summaries. SIGIR  X 98 (pp. 335 X 336). Cauwenberghs, G., &amp; Poggio, T. (2000). Incremental and decremental support vector machine learning. NIPS  X 00 (pp. 409 X 415).
 Donmez, P., Carbonell, J., &amp; Bennett, P. (2007). Dual strategy active learning. Proceedings of the Euro-pean Conference on Machine Learning (pp. 116 X  127).
 Freund, Y., Iyer, R., Schapire, R., &amp; Singer, Y. (2003).
An efficient boosting algorithm for combining pref-erences. Journal of Machine Learning Research , 4 , 933 X 969.
 J  X arvelin, K., &amp; Kek  X al  X ainen, J. (2002). Cumulated gain-based evaluation of ir techniques. ACM Transaction on Information Systems , 20(4) , 422 X 446.
 Joachims, T. (2002). Optimizing search engines using clickthrough data. Proceedings of ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD X 02) .
 Liu, T., Xu, J., Qin, T., Xiong, W., Wang, T., &amp; Li,
H. (2007). Letor: Benchmark dataset for research on learning to rank for information retrieval. SIGIR  X 07 Workshop: Learning to Rank for IR .
 Mitchell, T. (1982). Generalization as search. Journal of Artificial Intelligence , 18 , 203 X 226.
 Nguyen, H., &amp; Smeulders, A. (2004). Active learning with pre-clustering. ICML  X 04 (pp. 623 X 630). Platt, J. (1999). Probabilistic outputs for support vec-tor machines and comparisons to regularized likeli-hood methods. Advances in Large Margin Classi-fiers , 61 X 74.
 Roy, N., &amp; McCallum, A. (2001). Toward optimal active learning through sampling estimation of error reduction. ICML  X 01 (pp. 441 X 448).
 Tong, S., &amp; Koller, D. (2000). Support vector ma-chine active learning with applications to text clas-sification. Proceedings of International Conference on Machine Learning (pp. 999 X 1006).
 Xu, Z., Yu, K., Tresp, V., Xu, X., &amp; Wang, J. (2003).
Representative sampling for text classification using support vector machines. ECIR  X 03 .
 Yu, H. (2005). Svm selective sampling for ranking with application to data retrieval. SIGKDD  X 05 (pp. 354 X 
