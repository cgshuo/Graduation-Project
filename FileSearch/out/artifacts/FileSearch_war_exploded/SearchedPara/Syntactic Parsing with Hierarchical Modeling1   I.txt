 Syntactic parser takes a sentence as input and returns a syntactic parse tree that reflects structural information about the sentence. However, with ambiguity as the central problem, even a relatively s hort sentence can map to a considerable number of grammatical parse trees. There fore, given a sentence, there are two critical issues in syntactic parsing: h ow to represent and score a parse tree.
In the literature, several approaches have been proposed in parsing by repre-senting a parse tree as a sequence of decisi ons with different motivations. Among them, (lexicalized) PCFG-based parsers usually represent a parse tree as a se-quence of explicit context-free produc tions (grammatical rules) and multiply their probabilities as its score (Charni ak 1997; Collins 1999). Alternatively, some other parsers represent a parse tree as a se quence of implicit st ructural decisions instead of explicit grammatical rules. (Magerman et al. 1995) maps a parse tree into a unique sequence of actions and a pplies decision trees to predict next ac-tion according to existing actions. (Ratnaparkhi 1999) further applies maximum entropy models to better predict next ac tion according to existing actions.
In this paper, we explore the above two issues with a hierarchical parsing strategy by constructing a parse tree level by level. This can be done as follows: given a forest of trees, we recursively rec ognize simple constituents first and then form a new forest with a less number of trees until there is only one tree in the newly produced forest. Similar to (Ratnaparkhi 1999), our parser is divided into three consequent mod-ules: POS tagging, chunking and structural parsing. One major reason is that previous modules can d ecrease the search space signifi cantly by providing n-best results only. Another reason is that POS tagging and chunking have been well solved in the literature and we can concen trate on structural parsing by incor-porating the start-of-the-art POS taggers and chunkers. In the following, we will concentrate on structural parsing only.

Let X  X  first look into more details at structural parsing in (Ratnaparkhi 1999). It introduces two procedures ( BUILD and CHECK ) for structural parsing, where BUILD decides whether a tree starts a new constituent or joins the incomplete constituent immediately to its left and CHECK finds the most recently proposed constituent and decides if it is complet e, and alternates between them. In order to achieve the correct parse tree in Fig.1, the first two decisions on NP(IBM) must be B-S and NO . However, as the other children of S have not constructed yet at that moment, there lacks reliable contextual information on the right of NP(IBM) to make correct decision. One solu tion to this problem is to delay the B-S decision on NP(IBM) until its right brother VP(bought Lotus for $ 200 million) has already constructed.

Motivated by above observation, this paper proposes a hierarchical parsing strategy by constructing a parse tree level by level. The idea behind the hier-archical parsing strategy is to parse easy constituents first and then leave those complex ones until more information is ready.

Table 1 shows various tags in the hierarchical parsing strategy. In each pass, starting from left, the parser assigns each tree in a forest with a tag. Consequent trees with tags B-X, I-X, .., E-X from left to right would be merged into a new constituent X . Especially, S-X indicates to form a constituent X alone. The newly formed forest usually has less number of trees and the process will repeat until there is only one tree in the new forest. Moreover, maximum entropy models are used for predicting probability distribution and Table 2 shows the contextual information employed in our model.

The decoding algorithm attempts to find the best parse tree T* with high-est score. The breadth-first search (BFS) algorithm introduced in (Ratnaparkhi 1999) with a compuation complexity of O ( n ) is revised to seek possible sequences of tags for a forest. In addition, heaps are used to store intermediate forests in the evolvement. The BFS-based hierarchical parsing algorithm has a computa-tional complexity of O ( n 2 N 2 M ), where n is the number of words, N is the size of a heap and M is the number of actions. In order to test the performance of this hierarchical model proposed in this paper, we conduct experiments both on Penn WSJ Treebank (PTB) and Penn Chinese Treebank (CTB). 3.1 Parsing Penn WSJ Treebank In this section, all the evaluations are done on English WSJ Penn Treebank. Here, Sections 02-21 are used as the training data for POS tagging and chunking while Section 02-05 are used as the training data for structural parsing. Meanwhile, Section 23 (2,416 sentences) i s held-out as the test data. All the experiments are evaluated using measures of LR(Labeled recall), LP( Labeled p recision) and F1. And POS tags are not included in the evaluation.

Table 3 compares the effect of different window sizes. It shows that, while the window size of 5 is normally used in the literature, extending the window size to 7 (from -2 to 4) can largely improve the performance.
One advantage of hierarchical parsing is its flexibility in parsing a fragment with higher priority. That X  X  to say, it is practicable to parse easy (or special) parts of a sentence in advance, and the n the remaining of the sentence. The problem is how to determine those parts with high priority, such as appositive and relative clauses. Here, we define some simple rules (such as finding (LRB, RRB) pairs or  X  X  X  symbols in a sentence) to figure out the fragments with high priority. As a result, 163 sentences with appositive structure are found with the above rules. The experiment shows that it can improve the F1 by 1.53 (from 77.42 to 78.59) on those sentences, which results in performance improvement from 85.89 to 86.02 in F1 on the whole Section 23. 3.2 Parsing Penn Chinese Treebank The Chinese Penn Treebank (5.1) consists of 890 data files, including about 18K sentences with 825K words. We put files 301-325 into the development sets, 271-300 into the test set and reserve the other files for training. All the following experiments are based on gold standard segmentation but untagged. The evaluation results are listed in Table 4. The accuracy of automatic POS is 94.19% and POS tags are not included in the evaluation.
 Impact of Automatic POS. As shown in Table 4, the performance gap posed by automatic POS is up to 7.22 in F1, which is much wider than that of English parsing performance. The second column in Table 5 shows top 5 POS tagging errors on the test set. Mistaggings between verbs (VV) and common nouns (NN) occur frequently and make up 28% of all POS tagging errors.

In order to verify the effect of those POS tagging errors on the whole perfor-mance, for each error, we obtain the F1 on the test set and the corresponding decline rate (the last two columns in Table 5) by supposing other POS tags are all correct. In particular, both POS tagging errors between verbs and nouns, such as VV  X  NN and NN  X  VV ,and de5 tagging errors ( DEC  X  DEG , DEG  X  DEC ) significantly deteriorate the performance. This is not surprising because: 1) All nouns are immediately merged into NP, and all verbs into VP; 2) de5 has dif-ferent structural preferences if tagged as DEC or DEG .

In order to lower the side effect caused by POS tagging errors, the top K POS results are served as the input of chunking model. Here K is defined as following, where  X  (0  X   X   X  1) is the factor for deciding the number of automatic POS tagged results. The third row in Table 6 shows the performance when  X  =0 . 20. Compare with Other CTB Parsers. (Bikel &amp; Chang 2000) implemented two parsers: one based on the modified BBN model and the other based on TIG. (Chiang &amp; Bikel 2002) used the EM algorithm on the same TIG-parser to detect head constituents by mining latent information. (Levy &amp; Manning 2003) employed a factored model and improved the performance by error analy-sis. Likewise, (Xiong et al. 2005) integrated the head-driven model with several re-annotations into a model with external semantic knowledge from two Chi-nese electronic semantic dictionaries. Table 6 compares above systems. For fair comparisons, we also train our three models (POStagging, chunking and pars-ing) and test the performance with the same training/test sets as theirs. Table 6 shows that our system only performs slightly worse than the best reported system. This may be due to our low performance in chunking. With further analysis on the parsing results, our chunking model only achieves 80.82 in F1 on basic constituents, which make up 40.9% of all constituents. Therefore, there is still much room for performance improvement by employing a better chunking model. This paper represents an attempt at applying hierarchical parsing with machine learning techniques. In the parsing process, we always try to detect constituents from simple to complex. Evaluation on the Penn WSJ Treebank shows that our method can achieve a good performance with more flexibility for future improvement. Moreover, our experime nts on Penn Chinese Treebank suggest that there is still much room for perform ance improvement by employing a better chunking model.
 This research is supported by Proj ect 60673041 under the National Natural Sci-ence Foundation of China and Project 2006AA01Z147 under the  X 863 X  National High-Tech Research and Development of China.

