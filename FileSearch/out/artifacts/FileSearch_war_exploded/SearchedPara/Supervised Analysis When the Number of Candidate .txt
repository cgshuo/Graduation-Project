 New genomic and proteomic tech nologies provide measurements of thousands of features for each case. This provi des a context for enhanced discovery and false discovery. Most statistical and machine learning procedures we re not developed for the p&gt;&gt;n setting and the literature of DNA microarray studies contains many examples of mis-use of an alytic and computational methods such a cross-validation. This pape r highlights some of key aspects of p&gt;&gt;n problems for identifyi ng informative features and developing accurate classifiers. Prediction, classification, cross-validation New technologies for the analysis of biological samples provide information on a genom ic scale. For example, DNA microarrays can provide an estimate of the level of abundance of messenger RNA transcripts for all genes of the organism. Technologies for estimating the abundance of thousa nds of proteins and the copy number of all genes are becomi ng available. Biologists are utilizing these new technologies for a wide variety of objectives. Contrary to popular belief, effective utilization of these technologies depends on having clear objectives, effective experimental designs and appropr iate analysis plans. With genomic or proteomic technologies , the objectives are not gene or protein specific mechanistic hypotheses, as is often the case in other biological investigations. Ne vertheless, clear objectives are important and effective invest igations rarely represent unstructured searching for interesting patterns in a data archive. Three frequently occurring kinds of objectives in DNA microarray investigations have been called class comparison , class prediction and class discovery [18]. Class comparison involves identifying differenti ally expressed in cells from different types of tissue, differen t kinds of patients, or in cells exposed to different experimental conditions. The characteristic feature of class comparison is that the classes to be compared are defined independently of the expression data. Class prediction is similar to class comparison in that the classes are defined independently of the expression data. The emphasis in class prediction problems, however, is in developing a multi-gene classifier that can be applied to expression profiles of samples whose class is unknown to predict the class of the new samples. For example, a class comparison problem may involve identifying the genes that are differentially expressed between patients who respond to a specified treatment and those who don X  X  respond. Developing a classification function that can be used to predict whether a new patient will respond to that therapy based on the gene expression profile of his or her tumor, is class prediction. Class prediction is particularly useful in medical problems of therapy selection or diagnostic classifi cation or prognostic prediction. Class discovery is quite differen t from class comparison or class prediction. In class discovery th ere is no classification defined independently of the expression profiles. The objective is to discover subsets (clusters) of the cases revealed by gene expression profiles and to identify the genes that distinguish the clusters. For example, Bittner et al. [4] examined expression profiles of patients with adva nced malignant melanoma. The focus of the study was on attempting to identify a new taxonomy of advanced melanoma based on gene expression. No useful clinical classification existed. Class discovery also includes studies whose objective is to discover classes of co-regulated genes. Although class comparison and cla ss discovery problems are not unique to the genomics setting, most methods used for such problems were not developed for the context where the number of candidate features (p) is orders of magnitude greater than the number of cases (n). This is th e case for genomic and proteomic studies. The number of features can number in the tens of thousands but the number of cases rarely exceeds a few hundred and often is less than one hundred. In this article I shall review some analytic methods that have been found useful for p&gt;&gt;n class prediction problems, and will to uch on some problems that frequently occur when the p&gt;&gt;n issues are not adequately addressed. Four main components to developing a class predictor are: (i) Feature selection; (ii) Selecting a prediction model; (iii) Fitting the prediction model to training data; and (iv) Estimating the prediction error that can be expected in future use of the model with independent data. Feature selection is often key to developing an accurate class predictor. Often in microarray studies, as long as feature selection is perform ed r easonabl y, accurat e pred ition is achieved with eve n the simplest of p redictive mode ls. It is well known from the theor y of linear regression th at in cluding too man y  X  X oise variab les X  in the predic tor r educe s the accura cy of predi ction . A noise var iabl e is a var iabl e that is not r elated to th e th ing b eing predic ted. Feature s elect ion is par ticularly important in microarr ay studies because the number of no ise var iables may be ord ers of magnitude gr eater th an the numb er of relevan t variables. The influen ce o f th e genes th at ac tua lly distingu ish th e c lasses m ay b e lost among th e n oise of the more numerous noise genes unless we selec t th e inform ativ e gen es to be util ized b y the class predi ctor . The m ost com monl y us ed appro ach to f eatur e s elect ion is to identif y the g enes that ar e dif ferentially expressed among th e clas ses when co nsidered individ ually. For exam ple, if ther e are two classes, one can com pute a t-test or a Ma nn-Whi tney test for each gen e. The log-ratios or log-signals ar e g ener ally used as the bas is of th e s tatistical signif ican ce tests. The gene s that ar e differen tia lly expres sed at a specified signifi can ce level ar e selec ted for in clusion in the class predic tor. Th e stringenc y of the significance level con trols th e nu mber of gen es th at are included in th e model. If one wants a class predictor b ased on a small number of gen es, th e threshold significance level is made v ery sm all. Several autho rs have d eveloped me thods to id entif y optimal sets of genes which together provi de good discrimination of the clas ses [5] , [13] , [6] , [12] . Th ese algorithm s ar e g enera lly v ery com putation ally intensive . Unfor tunate ly, it is not cle ar whe ther the incre ased computational ef fort of thes e m etho ds is warran ted. In s ome cas es, the c laim s m ade do not appear to be based on properly cross-valid ated calcu lations ; al l of the da ta b eing used to select th e g enes and cross-valida tion used on ly for fitting the parameters of th e model. T horou gh studies comparing the performance of s uch methods to the simpler univariate methods are n eed ed.
 Some investigators have used linear com binat ion s of gen e expression values as predictors [2 4] [11] . Principa l componen ts are the orthogon al linear combinations of th e gen es showing the greatest v ariab ility among th e cas es. Th e pr incipal components are s ometim es referred to as singular v alues [1] . Us ing princip al components as p redictive featur es provides a vast reduction in the dimension of th e expression data , but h as two se rious lim itations. One is th at the p rincip al components ar e no t n ecessarily good predictors. The second prob lem is that measuring the pr incip al components req uires measuring expression of all the gen es. The method of g ene shaving attempts to prov ide lin ear combinations with proper ties similar to th e pr incipal componen ts that does no t require measurin g all of the gen es [10] . . Many algorithms have been used effectiv ely with DNA microarray data for class predicti on. Dudoit et al. [7] compared several algorith ms using publicly ava ilab le dat a sets. Th e algorithms comp ared includ ed n earest n eighbor classification and several var iants of lin ear discrim inant ana lysis an d cl assifica tion trees. A linear discriminant is a f unction where x i deno tes the log-r atio or log-signal for th e i X  X h gen e, w the weight given to that gen e, and the sum mation is over th e se t F of fea tures (g ene s) selec ted for inclus ion in th e cla ss predictor . For a two-clas s problem , th ere is a thres hold valu e d, and a sample with exp ression profile d efined by a v ector x of values is predicted to b e in class 1 or class 2 dep ending on whether () lx as computed from equation (1) is less than or grea ter than d res pect ivel y. Several kinds of class pred icto rs used in the liter ature h ave the form shown in ( 2). Th ey differ with reg ard to h ow the w eights are determined . The oldest form of linear d iscriminan t is Fisher X  X  line ar dis criminant [16] . Th e we ights are select ed so tha t the m ean value of () lx in class 1 is maximally differen t from th e mean value of () lx in clas s 2. The squ ared differen ce in means divid ed by the pooled estimate of th e with in-class variance of () lx was the s pecific m easure us ed b y Fisher. To com pute thes e weigh ts, one m ust estim ate th e correl ation between all p airs of gen es th at were selected in the f eatu re s election step . The stud y by Dudoit et al. indi cated tha t Fisher l inear dis criminant an aly sis did no t perform well unless the number of se lec ted ge nes wa s sma ll rela tive to the nu mber of sam ples ; oth erwise there are too m any correlations to estimate and the me thod tends to be un-stab le and over-fit the data. Diagonal lin ear discriminant an aly sis is a special case of Fisher linear discr iminant an aly sis in wh ich the correlatio n among gen es is ignored [7] . By ignoring such correlations, one avoids h aving to estim ate m any param eters , and o btains a m ethod which perfo rm s better when the number of sample s is sma ll. Golub X  X  we ighte d voting method [9] and the comp ound covar iate p redictor of Radm acher et al.[14] are s imilar to diagon al lin ear dis crim inan t anal ysis and ten d to perform ver y well when the number of sam ples is sm all. The y com pute the weigh ts base d on th e univariate pred iction str ength of individual gen es and ignor e correlations among the gen es. Support vector machines are po pular in th e machine learn ing liter atur e. Lin ear kernel support vector m achin es u se a p redi ctor of the form of equation (2). Th e weights ar e d eterm ined b y optim izing an er ror rat e crit erion , however , ins tead of a leas t-squares criter ion as in linear di scriminant analy sis [15] . Although there ar e more complex forms of support vector machines, they appear to be infe rior to lin ear ker nel SVM X  X  for c lass predic tion with large numbers of g enes [3] . Khan et al.[11] reported accura te class prediction among small, round blue cell tumors of childh ood using an ar tificial n eural network. The inputs to the ANN were the first ten principal components of the gen es; that is, the l0 orthogonal lin ear combinations of the g enes that accounted for most of th e variab ility in gene expr essi on among samples. Their neur al network us ed a linear trans fer function with no hidden lay er and hence it was a linear percep tron classifier of the f orm of equation (2). Most true ar tific ial neu ral networks have a h idden lay er of nodes, use a non -linear tr ansfer functions and ind ividual featur es as inputs. Such a  X  X eal X  n eural network may not p erform as well as the prin cipal component per ceptron model o f Khan et al. becaus e of the number of par ameters to b e estim ated would b e to o large for the available number of samples. In the stud y of Dudoit et al. [7] , the simplest meth ods, diagon al linear discr iminant an aly sis and nearest neighbor classification, performed as w ell or better th an the more complex methods. Nearest n eighbo r classification is ba sed on a featu re set F of g enes selec ted to b e inf orm ative for dis crim inat ing th e classes and a distance fun ctio n (, ) dx y which m eas ure s the d istance b etween the expression profiles x and y of tw o sa mple s. The dista nce function utilizes only th e g enes in the select ed s et of fea tures F. To classif y a sample with expression profile y , co mpute (, ) dx y for each sample x in th e tr ain ing set. The predicted class of y is the class of th e s ample in th e tr aining set which is closest to y with reg ard to th e dis tance func tion. A v arian t of neares t neighbor classification is k -near est neighbor classification . For example with 3-nearest neighbor classification , you find the three sam ples in th e training set whi ch are closest to the sam ple y . The class which is m ost represen ted among these th ree samples is the predic ted cl ass for y . Dudoit et al. also studied some more complex m ethods such as clas sificat ion t rees and aggreg ated classifica tion trees . Thes e methods did not appear to perfo rm better th an diagonal linear discriminant an aly sis or neares t n eighbor classification . Ben -Dor et al. [3] also compared sever al methods on several pub lic datasets and found that n earest neighbor classification g enerally performed as well or b etter than more complex methods. Most kinds of pr edictors hav e pa rameters th at must be assigned values b efore th e pred icto r is ful ly specified . The se param eters are in m any wa ys equival ent to t he r egres sion coef fic ients of lin ear and non-linear r egression models. After selecting the kind of class predictor to be u sed, th e pr edicto r is fitt ed to a se t of dat a. Th e nu mbe r of pa rame ters t hat must be specified is often proportion al to the number of genes selected for inclusion in the model. For some kinds of predictors there is a cut-point th at must b e specified for translating a quan titative predictive ind ex into a pr edicted class label (eg 0 or 1) for binar y class pred iction problems. Comple tely spe cifying the pred ictor means specif ying al l of thes e as pects of th e pr edi ctor, th e type of predictor, the genes included and the val ues of a ll pa rame ters. It is im portant to estim ate the accurac y of class pr edic tion for future samples for which the clas s is unknown ? Knowing that there ar e high ly statis tical ly significan t gen es th at are differen tially ex pressed betw een the classes is no t enough . We want to know ho w accu rat ely we can pr edi ct whi ch cl ass a n ew sam ple is in . For a fu ture sam ple, we wil l appl y a full y spe cified predictor d evelo ped using the data av ailable tod ay. If we are to emulate th e futu re pred ictive setting in dev elopin g our estimate of predic tive accur acy, we m ust se t aside some of our sa mple s a nd ma ke them compl etely inaccessibl e unt il we ha ve a fully spec ified predictor th at has been d evelope d from scratch w ithout u tilizing thos e s et as ide s amples . To proper ly esti mate th e accurac y of a pred ictor f or future sam ples, th e cu rrent se t of sam ples m ust be par titioned in to a tra ining se t and a se parate test set. The tes t set em ulat es the set of future sam ples for which class la bels ar e to b e pr edic ted. Consequently the test samples cannot be used in any way for the development of the pred iction mo del. This m eans that the test samples cannot be used for est imating th e p arameters of the model and they cannot be used f or selecting th e genes to be used in th e m odel . Th is la ter poi nt is o ften ov erlook ed. The m ost straigh tforward m ethod of es timating th e a ccur acy of of samples into a training s et and a test set as described in th e previous par agraph. Rosenwald et al. [17] used this approach successfully in their intern ational stud y of progn ostic pr ediction for larg e cell lymphoma. They used two th irds of their samples as a tr aining set . M ultipl e kinds of p redic tors were studied on th e training set. Wh en th e collaborators of that stud y agreed on a single full y spe cified predic tive m odel , th ey accessed th e test se t for the first tim e. On the test se t there w as no adju stm ent of th e model or f itting of parameters. Th ey me rely use d the sa mpl es in the test set to ev alua te the predi ctions of the model that was completely specified using only the tr aining data. Cross-validation is an altern ative to the split sam ple m ethod of estim ating predi ction ac cura cy. There ar e s everal form s of cros s-valid ation . One commonly used varian t, lea ve-on e-out cr oss-validation ( LOOCV) , star ts lik e split-sam ple cross valid ation in form ing a train ing set of sam ples and a te st se t. W ith LOOCV, however, the test set consists of only a si ngl e sa mpl e; the re st of the sam ples are plac ed in th e trai ning set . The sa mple in the test set is p laced asid e and not uti lized at all in th e d evelopm ent of th e class pred iction model. Using only th e tr ain ing se t, the inform ative gen es are sele cted an d the par ameters of th e m odel are fit to the da ta. Le t us ca ll M 1 the model develo ped with sample 1 in the test set . When this m ode l is fu lly d evelop ed, it is used to predic t th e class of sam ple 1 . Thi s predic tion is m ade using th e expression profile of s ample 1 , but obviously without using knowledge of th e tru e class of sample 1. S ymbolically , if x denotes the complete expr ession profile of sample 1, th en we apply model M 1 to 1 x to obtain a pr edicted class . Th is predic ted class is com pared to th e tru e clas s label c If the y disagr ee, then the predi ction is in error . Th en a new training set  X  tes t set par titio n is crea ted . Th is t ime sam ple 2 is placed in th e test set and all of the oth er samples , including sam ple 1, ar e pl aced in the tra inin g set. A new m odel is constructed from scratch using th e sa mples in th e new tra ining set. Call this m odel M 2 . Model M 2 will g enera lly not con tain th e same genes as model M 1 . Althou gh the same algo rithm for g ene selec tion and parameter estim ation is used , since model M constructed from scratch on th e n ew tr aining set, it will in gen eral not con tain ex actly th e s ame gen e set as M 1 . After creating M is applied to the expression profile 2 x of th e sam ple in th e n ew tes t set to obt ain a pred icted class . If th is pred icted class does not agr ee with th e tru e class l abel c the pr edic tion is in error . The pro cess des cribed in the previous paragr aph is repeated n times where n is the number of bi ologically indep endent samples. Each tim e it is applied , a diff ere nt sam ple is use d to form the single -sample test se t. During the n steps, n diff erent models are crea ted and ea ch one is used to pr edic t th e class of the om itted sam ple. The p rediction errors ar e to taled and tha t is the leave-on e-out cross-validated estimate of the pred iction erro r. W ith two clas ses, on e can use a s imilar app roach to ob tain cross-validat ed estim ates of th e sensitivit y, sp ecificit y, and ROC curve [21] . If we use all of the da ta to se lect genes and constr uct a model, there is no indep endent data left to va lidly estim ate pred iction error. A com monl y us ed inval id estim ate is cal led the re-substitution estimate [19] . You use all th e samples to d evelop a model M. Then you predi ct the class of e ach sam ple i using it X  X  expression profile i x ; ()  X  i cM x = i . Th e pr edic ted cl ass lab els are com pared to th e true clas s labe ls and th e errors ar e to taled. Simon et a l. [19] perform ed a s imulation to ex am ine th e b ias in estim ated error rates for class pre diction . In a sim ulated da ta set , twenty expressio n profiles of 600 0 genes w ere ran domly generated from the same d istribu tion. Ten profiles were arbitr arily assigned to  X  X lass 1 X  and the oth er ten to  X  X lass 2 X , cre ating an artif icial separation of th e prof iles into two classes. Since no true underly ing diff erence ex ists betw een the two class es class prediction will p erform no better than a r andom g uess for futur e biologically ind ependent samples. Hen ce, th e estimated er ror r ates for simulated data sets should be centered around 0.5 (i.e, ten misclassifica tion s out of twent y). Figure 1 shows the observed nu mber of misclass ifications resulting from each lev el of cros s-validation for 2000 sim ulated data sets. It is w ell-known that th e re-substitutio n es timate of err or is biased for sm all d ata sets and the sim ulation confirm s this, wi th 98.2 % of th e si mulated d ata set s resulting in zer o misclassification s even though no true under lying differen ce exists between the two grou ps. Moreover, the maximum number of misclassified profiles using the re-substitution method was only one. Two ty pes o f leave-one-out cross -valid ation were studied. In on e approach th e f eatures to be us ed in the class pred ictor wer e selec ted using all of the dat a before star ting the cr oss-validation process. This is parti al cross-vali dation . W ith pro per cross-valid ation , the g ene selection is re-done fo r each leave-on e-out training set. Figure 1 shows that p artial cross-valid ation is abo ut as b ad as no cross-valida tion. Cross-valida tin g the pred iction rule aft er selection of diff erentially expr esse d ge nes from the full da ta se t does li ttle to corr ect the bi as of th e re-substi tution estim ator: 90.2% of sim ulated d ata sets st ill result in zero m isclassific ations. It is no t un til gene se lection is also subject ed to cross-validat ion that we observ e results in lin e wi th our expe cta tio n: th e m edian number of misclassified prof iles jumps to eleven , although th e range is large (0 to 20). The simulation results underscor e th e importance of cross-valid ating all st eps of predi ctor construction in estim ating the Veer et a l. [23] predic ted clinic al out com e of p atients with axill ary nod e-ne gativ e bre ast can cer (m etastatic diseas e wi thin 5 years versus disease-free at 5 years) from gene ex pression profiles, first using the re-subst itution method an d then using a full y cros s-valid ated appro ach . The inv estiga tors controll ed the num ber of m iscl assified r ecurr ent cases (i .e., the sensitivi ty of th e test) in both situ ations, so her e we focus attent ion on th e differen ce in es timated er ror r ates for th e d iseas e-f ree cas es. Th e improperly cr oss-valid ated metho d and the proper ly cross-valid ation result in est imated erro r rates of 27 % (1 2 out of 44) and 41% (18 out of 44), r espectively . The improperly cross-validated method results in a seriously biased under-estimate of th e error rate . W hile van  X  t Veer et a l. rep ort both estimates of the erro r rate , the p roperl y cros s-valid ated es tim ate was reported on ly in the supplemental results section o n the website and the inv alid estim ate re ceiv ed m ore a tten tion. Another example of this occurred in a study where classificat ion tr ees we re built from gen e expression data to classif y spec imens as normal colon or co lon cancer [25] . The authors used a p rocedure th at on ly cross-valid ated steps that o ccurr ed afte r selection of genes for in clus ion in th e pr edic tor f rom the fu ll d ata set. As our sim ulation shows, not subje cting gene se lection to cross-valida tion can r esult in a large bias . Oth er examples are described b y Ambroise and McLach lan [2] . Another common bias in r eported cross-valid ated error rates arises from th e considerat ion of multiple predi ction m odels. Often, the pred ictive algor ithm has one o r m ore tuning parame ters associat ed with it. F or exam ple , the P AM algori thm has a param eter th at controls th e d egree of shri nka ge of t he cl ass specific centro ids [22] . Suppose one computes a proper cross-valid ated es tim ate of the error ra te () e  X  for a rang e of values of the tuning param eter  X  . Although () e  X  may be an unbiased estimator of th e true value ( ) e  X  , () { } mi n e  X  regard to  X  ) is no t an unb iased estimator of ( ) { } Consequently , optimization of tuning par ameters should be perform ed with in ea ch s tep of th e cros s-val idatio n. Radmacher et al. [14] discuss a p aradigm for prop er cross-valid ation of class predictors . Th ey propose that the statistical significance of the cross-validate d error rate b e reported. Th e usual methods f or determining s tatis tical sign ificance of an error leav e-one-out tr aining sets ar e not indep endent (th ey ar e almost completely ov erlapping), th e numbe r of cross-validated errors does not h ave a binomial d istrib ution. Radmach er et al. [14] provide an algor ithm for estimating the statistical significance of the cros s-valid ated erro r es timate. Th ey cons ider all pos sible (o r a large number of random) permutati ons of th e clas s labels th at pres erve the nu mbers of s amples in e ach class. For ea ch perm utation of the c lass l abels , the en tire cross-v alid ation procedure is r epeated. They thus gener ate th e p ermutation distribution of the cross-validated prediction error . The proportion of the permutations that give as small a cross-validated er ror r ate as tha t obt ain ed for the tru e data is tak en as the statisti cal significance level for the cross-validated error r ate. There is considerable confusion about the proper use of cross-valid ation . You cannot cross-valid ate a model. Th e model to b e data. The cross valid ation proced ure does not u tilize that model. It utili zes the m odel bu ilding algor ithm , in cluding the fe atur e-Class prediction problems in p &gt;&gt;n settings ar e in creas ingl y im portant in m edica l app licat ions of genomics, b ut provid e serious cha llenges to the stat istic al and com putati onal sc ient ist. Conventional wisdom and routin e practices for p &lt;n pred iction problems tend to give poor r esults in th e p &gt;&gt;n set ting. Guidelines commonly used to guid e modelin g, such as use of VP dimension com plexit y of th e select ed mode l, not of the space from which that m odel was s elected . selec tion algor ithm , to att empt to provide an unb iased estimate of the error r ate of the com plete dat a m odel. This po int seem s to be sometimes misu nderstood b y computer scien tists and statisticians . Cross-validation is a limited for m of valid ation. Leav e-one-ou t cross valid ation is known to p rovide an estimate o f the error rate having large var iance and other f orms of cross-validation and bootstrap r e-sampling prov ide smal ler varian ce estimates [8] . Cross-validation does not provid e as stringen t a test of a model as would testing a model on a tru ly indep endent set of data from a algorithms for p&gt;&gt;n prob lems. A lthough th e simp ler models may not be sufficiently rich to descr ibe the true relatio nship between predic tors and outcom e, ther e is not sufficient data to fit models that are m ore f lexible . W ith ord ers of m agnitud e more candid ate featur es th an cas es, huge tr aining sets ar e n eed ed to eff ectively utili ze com plex models. Some critic isms of c ross-va lida tion, howev er, seem invalid . Cross valid ation does provide an essen tial ly unb iased estim ate of th e error r ate of classifica tion that would be obtained for samples from the same distribution as in the tr aining set, in spite of some assertions to th e contrar y [20] . Some individu als p oint out th at with tens of thou sands of features , th ere will almost surel y be a featur e s et th at perfec tly predi cts both th e tr ain ing set and ever y split sam ple t est set. Al though thi s is true, it is not a v alid criti cism of cross-valida tion. A proper cross valid ation must apply the f eatu re s election and m odel b uilding algor ith m to e ach leave one out tr aining set and to select a single model f or classif ying the left out s ample. The f act th at the re is a m odel th at pred icts perfectly , does not imply that so me well defin ed algorithm wi ll find it for each leave one out tr aining set. Algor ithms that overfit the d ata wil l general ly not hav e a low cross-va lid ated error estim ate if the cr oss-validation is perform ed prope rly. Sample re-use methods such as cr oss-validation an d bootstrap ar e frequently used improperly for p &gt;&gt;n prob lems, n ot only by experim ental scientis ts, but als o by statistical and com putat ional scientists . Such methods, when u sed properly , are ver y valuable in p&gt;&gt;n prob lems. [1] Alter, O., Brown, P. O. and Botstein, D. Singular [2] Ambroise, C. and McLachlan, G. J. Selection bias in [3] Ben-Dor, A., Bruhn, L., Friedman, N., et al. Tissue [4] Bittner, M., Meltzer, P., Chen, Y., et al. Molecular [5] Bo, T. H. and Jonassen, I. New feature subset [6] Deutsch, J. M. Evoluti onary algorithms for finding [7] Dudoit, S., Fridlyand, J. and Speed, T. P. Comparison [8] Efron, B. and Tibshira ni, R. Improvements on cross-[9] Golub, T. R., Slonim, D. K., Tamayo, P., et al. [10] Hastie, R., Tibshirani, R., Eisen, M., et al. Gene [11] Khan, J., Wei, J. S., Ri ngner, M., et al. Classification [12] Kim, S., Dougherty, E. R., Barrera, J., et al. Strong [13] Ooi, C. H. and Tan, P. Genetic algorithms applied to [14] Radmacher, M. D., McShan e, L. M. and Simon, R. A [15] Ramaswamy, S., Tamayo, P., Rifkin, R., et al. [16] Ripley, B. D., Pattern recognition and neural [18] Simon, R., Korn, E. L., McShane, L. M., et al., Design [19] Simon, R., Radmacher, M. D., Dobbin, K., et al. [20] Somorjai, R. L., Dolenko, B. and Baumgartner, R. [21] Swets, J. Measuring the accuracy of diagnostic [22] Tibshirani, R., Hastie, T. and al., e. Diagnosis of [23] van't-Veer, L. J., Dai, H., Vijver, M. J. v. d., et al. [24] West, M., Blanchette, C. and Dressman, H. Predicting [25] Zhang, H., Yu, C. Y., Singer, B., et al. Recursive 
