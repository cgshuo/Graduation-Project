 The proliferation of Web 2.0 has enabled ready ac-cess to large amounts of community created content, such as status messages, blogs, wikis, and reviews. These form an important source of knowledge in our day to day decision making, such as deciding which restaurant to try, or which movie to watch, or which city to visit etc. Unfortunately, such con-tent typically focuses on one real world entity at a time, whereas, a user deciding between alterna-tives is most interested in a comparative analysis of strengths and weaknesses of each.
There have been some recent attempts to create comparisons using expert knowledge, but generat-ing such comparisons manually does not scale  X  even pairwise comparisons are quadratic in the num-ber of entities. Few automated comparisons for spe-cific products with pre-defined attributes (e.g., lap-tops, cameras) exist; they are typically powered by existing structured knowledge bases. To the best of our knowledge, prior work on automatically gener-ating comparisons for arbitrary domains from un-structured text, does not exist.

We define a novel task of generating entity com-parisons from textual corpora in which each docu-ment describes one entity at a time. For broad appli-cability, we do not restrict ourselves to a pre-defined ontology; instead, we use textual phrases that de-scribe entities as our unit of information. We call these descriptive phrases  X  they encompass general attribute-value phrases, opinion phrases, and other descriptions of the facets of an entity. We gener-ate entity comparisons in a tabular form where the phrases are organized semantically, thus, allowing for direct comparisons. Figure 1 shows a sample city comparison generated by our system for tourism.
Our comparison generation algorithm extracts de-scriptive phrases per entity and clusters them into semantic groups. We perform clustering via a topic model, where phrases from an entity are combined into one document. The topics identify prominent facets of the entities. Unfortunately, since the num-ber of entities being compared is usually small, just statistical co-occurrence of words and phrases is not sufficient to identify good topics. In response, we use vector embeddings of descriptive phrases and employ a Gaussian extension of probabilistic latent semantic analysis (pLSA) over these vectors.
We also modify Gaussian pLSA to additionally incorporate an entity-balance term, preferring topics in which phrases from the entities are represented in a proportionate measure. The balance term trades off the discovery of unique facets for each entity with that of common facets. This enables direct compari-son between entities leading to an overall improved comparison table. Since the balance term is only a preference (not a constraint), it still allows the al-gorithm to exhibit clusters which may be sparsely represented (or not represented at all) in one of the entities.

We demonstrate the usefulness of our ideas on two domains  X  tourism and movies. Based on user experiments, we find that the entity-balanced model outputs much better comparisons as compared to an entity-oblivious model such as GMM. In summary, our paper makes the following contributions:  X  We define a novel task of generating entity  X  We present the first system to output such  X  Human subject evaluations using Amazon Recently, the internet has seen a growth in websites offering comparisons for different entities. Prod-uct websites such as eBay maintain comparisons for products. Google also outputs pre-built compar-isons between common entities when queried with the word  X  X s. X  between them. Both of these output purely structured attribute-value information and are unable to compare along more qualitative and de-scriptive dimensions such as ease of living or qual-ity of nightlife when comparing cities, for exam-contributed comparisons that have been categorized based on the nature of the entities being compared. These are manually curated and therefore do not scale to the quadratic number of entity pairs.
Perhaps the most closely related work to ours is the field of contrastive opinion mining and summa-rization (Kim et al., 2011; Liu and Zhang, 2012). Examples include extraction of contrastive senti-ments on a product (Lerman and McDonald, 2009) and summarization of opinionated political articles (Paul et al., 2010). Contrastive opinion mining ex-tracts contrasting view points about a single entity or event instead of comparing multiple ones. A re-cent preliminary study extends this for comparing reviews of two products (Sipos and Joachims, 2013). It uses a supervised method for learning sentence alignments per product-type, and does not organize various opinions for an entity via clustering.
Other related work includes comparative text mining tasks where document collections are ana-lyzed to extract shared topics or themes (Zhai et al., 2004). Since such methods only identify latent top-ics for the full document collection, they can X  X  be directly used for a specific comparison task.
Since our system is a combination of IE and clus-tering, we briefly describe related approaches for these subtasks.
 Information Extraction: Our work is related to the vast literature in information extraction, in par-ticular Open IE (Banko et al., 2007). Our use of POS patterns for extracting domain-specific descrip-tive phrases is similar in spirit to ReVerb X  X  pat-terns for relation extraction (Etzioni et al., 2011) and adjective-noun bigrams for fine grained attribute ex-traction (Huang et al., 2012; Yatani et al., 2011). Adapting the literature on entity set expansion (Pan-tel et al., 2009; Voorhees, 1994; Natsev et al., 2007), our system expands seed nouns for broader cover-age. We use Wordnet and distributional similarity-based approaches for this (Curran, 2003; Voorhees, 1994).
 Clustering: Our entity-balanced clustering algo-rithm is related but different from previous work on balanced clustering. Prior work (Banerjee and Ghosh, 2006; Yuepeng et al., 2011) has focused on generating different clusters to be equi-sized. Other work (Zhu et al., 2010; Ganganath et al., 2014) en-forces size constraints on clusters. Our idea of bal-ance, on the other hand, is targeted towards a better comparison and prefers that entities are well repre-sented (balanced) in each cluster. Our motivation is to concisely compare two or more entities to aid a user X  X  decision making. We make several choices in our task definition to help with this goal. First, we decide to output comparisons using a succinct tabular representation (see Figure 1). It has higher information density compared to, say, writing a natural language comparison summary.

Second, our unit of information is a descriptive phrase . We define it as any short phrase that de-scribes an entity  X  these include attribute-value pairs (e.g.,  X  X reek art X ), opinion phrases (e.g.,  X  X pectacu-lar views X ), as well as other descriptions (e.g.,  X  X ld-est church of Europe X ).

Third, for better readability, our table must or-ganize the information coherently along various as-pects relevant for a comparison. We achieve this by grouping related descriptive phrases. The choice of aspects should be dependent on the specific entities being compared, e.g., the facet of  X  X eaches X  may split into  X  X ater activities X  and  X  X each types X  for Jamaica v.s. Hawaii, but not for San Francisco v.s. Bombay.

Moreover, comparisons are meant to highlight both the similarities and the differences between en-tities. We therefore need to trade-off the discovery of unique facets of an entity with those which are common to the entities being compared. Thus, while clusters that balance the entities are preferable, it is also acceptable to have clusters where one of the en-tities is sparsely represented (or not represented at all). This would happen in situations where that en-tity does not express a particular aspect and other entities do. Comparisons must trade off semantic co-herence of facets with entity-balance in each facet.
Last, but not the least, since the comparisons are targeted to aiding user X  X  decision making, under-standing her intent is important. As an example, the user may be interested in city-comparison for the purpose of tourism, or for choosing a city to live in. Descriptive phrases for the former could be related to sightseeing, shopping, etc., but for the latter they may cover aspects such as living expenses, trans-portation, and pollution. We accommodate this ne-cessity by allowing minimal human supervision for specifying user intent. This supervision can come in forms such as an intent-relevant seed noun list, or topic-level annotation following unsupervised topic modeling, etc. This supervision further guides de-scriptive phrase extraction.
 System Architecture: Our system consists of a pipeline of information extraction, clustering, clus-ter labeling and phrase ordering. IE extracts descrip-tive phrases relevant to user-intent and we develop a new clustering algorithm that produces better com-parisons by balancing the entities in each cluster. We identify cluster labels based on the most frequent words in a cluster. We order phrases within a clus-ter based on the distance from the centroid. We now describe our IE and clustering techniques in detail. 3.1 Information Extraction Our IE pipeline works in two steps. We first extract descriptive phrases via POS patterns and then fil-ter out the non-topical phrases. For filtering, first we create a seed list of relevant nouns via minimal hu-man supervision, which are then expanded by item-set expansion. Descriptive phrases with a noun in the expanded list are retained, and rest are filtered.
Preliminary analysis on a devset revealed that a large fraction of descriptive phrases are noun phrases (NPs). We first extract all NP chunks from the collection and, additionally, using POS tags, ex-tract any adjective-noun bigrams that are part of a bigger NP chunk, or missed due to chunking errors. This forms the initial set of descriptive phrases. Filtering for User Intent:
These descriptive phrases include those that are not relevant for user intent such as  X  X xcellent schools X  for tourism. We filter these phrases by matching them to a list of intent-specific nouns. This list is created by first curating a seed list and then ex-panding it using item-set expansion. We employ two methods to obtain a seed list for specifying user in-tent: (1) a list of user-specified seed nouns, and (2) a labeling of LDA topics based on top words in each topic.

In the first approach we get the seed nouns di-rectly from the domain expert. Our system sup-ports the process by identifying frequent nouns and showing those to the annotator to annotate. For our tourism system, an author spent about three hours to produce a list of 100 seed nouns.

Since this process requires significant effort per user intent, we also investigate a semi-automatic ap-proach in which we run Latent Dirichlet Allocation (LDA) (Blei et al., 2003) on the whole phrase list. We then show the top 20 words in each topic and ask the annotator to provide only topic-level annota-tions. We treat the top 15 words from each positively labeled topic to be in the seed set. Since the number of topics is usually not that large, this significantly reduces the time required for annotation. E.g., we ran LDA with 20 topics and it took about 10 mins. for an author to annotate them. However, the seed nouns are noisier due to noise in LDA.
 Seed List Expansion: Finally, we use ideas from item-set expansion to expand the seed list for im-proved coverage. We implement two approaches for this step. In the first method (WN) we use Wordnet (Miller, 1995) to include words that are a direct hop away from the seed nouns. In the second approach (WV), we use word-vector embeddings (Collobert et al., 2011) and include top 10 neighbors of each seed in our expanded list. The expansions capture near-synonyms and topically related words.
 IE Experiments: We now present comparisons of various IE methods on a small development set. We on one city) and manually annotated an exhaustive set of descriptive phrases. This forms our devset for IE comparisons.

We chose various parameters in our IE systems so that our precision never drops below 0.70. For exam-ple, we used K=15 for choosing the top words from LDA into seed list. We use this target precision, be-cause we believe that for any human-facing system the precision needs to be high for it to be considered acceptable by people.
 Table 1 compares the performance of the various IE methods. Not surprisingly, we find that manual seed lists obtain a much higher recall as compared to LDA seeds, at approximately the same level of pre-cision. Both Wordnet and word-vector improve the recall substantially, though vectors are more effec-tive. The recall of all nouns is only 0.67 because a large number of descriptive phrases were larger n-grams (not just adjective-noun bigrams) and were missed due to chunking errors. 3.2 Building Clusters for Comparison Our next task is to construct meaningful compar-isons using these phrases. A useful comparison of entities should organize the available information in a way that is easy to comprehend by the user. To-wards this goal, we group the related descriptive phrases across a number of clusters. But simply hav-ing a good clustering of descriptive phrases may not be enough. We would like to have a clustering that explicitly captures the individual characteristics of each of the entities as well as makes the relative strengths and weaknesses of each entity apparent. For example, Figure 2 (Right) shows three different clusterings of phrases from two cities; phrases from each city are in a different color. Here, the third clus-tering is most appropriate for comparison, because not only is it a good clustering of descriptive phrases from each city considered separately, but the clusters produced also have entity-balance , i.e., the clusters produced have a good balance of both cities; both of these are key elements of comparison.

We first observe that a topic model such as Prob-abilistic Latent Semantic Analysis (pLSA) is a good fit to our clustering problem. In pLSA documents are characterized as mixtures of topics and topics as distributions over words. For our problem, we could combine all phrases for an entity into one document, and run pLSA to identify a coherent set of topics, which can then be used as clusters. Such a model will allow different entities to express topics in dif-ferent proportions.

We note that LDA, which is a strict generaliza-task. LDA typically uses a sparse Dirichlet prior on document-topic distribution, which would not be ap-propriate since for comparison we would like to rep-resent each entity in as many topics as possible.
Unfortunately, a direct application of pLSA may not yield good results. This is because typically the number of entities being compared (i.e., the number of documents in pLSA) is very small (often 2), there-fore, there isn X  X  enough statistical regularity to find good coherent topics. The alternative proposition of learning topics on the whole corpus isn X  X  very ap-pealing either, since that will learn global topics and not the topics particularly meaningful for the current comparison at hand.
In response, we exploit the availability of pre-trained word vectors as a source of background se-mantic knowledge for every phrase, and generalize the pLSA model to Gaussian pLSA (G-pLSA). We construct a vector representation for each descriptive phrase by averaging the word-vectors of individual model is pLSA with each topic-word distribution represented as a Gaussian distribution over descrip-tive phrases in the embedding space. This model is also similar to the recently introduced Gaussian LDA model (Das et al., 2015), but without LDA X  X  Dirichlet priors as discussed above.

Gaussian pLSA has several advantages for our task. First, it can meaningfully learn topics only for the entities being compared, instead of needing to learn a global topic model over the whole corpus. Second, due to additional context from word vec-tors, the topics are expected to be much more coher-ent compared to traditional topic models for cases when the underlying corpus is small, as in our case. Finally, in our model the vectors are generated from a Gaussian distribution and that helps capture the theme of the cluster directly by enabling a centroid computation in the embedding space. This is espe-cially useful for identifying and ranking important descriptive phrases per cluster while generating the comparison table.
 and the corresponding cluster (topic) id, respec-tively, for the j th entity e j . Then, the log-likelihood L ( X ) of the observed data can be written as: Here, | X j | and | Z | are the total number of phrases and clusters 5 respectively, for a given entity e j and, | E | is the total number of entities being considered for comparison.  X  denotes the vector of all the pa-rameters. We optimize the expression L ( X ) using EM and estimate the parameters of the model. As can be seen, the clusters are shared across entities, and the phrases generated are independent of the en-tity given, a cluster and the entities themselves are free to exhibit clusters in different proportions.
We also note just as pLSA can be seen as a nat-ural extension of mixture of unigrams (Blei et al., 2003), Gaussian pLSA is an extension from the Gaussian Mixture Model (GMM) which is entity-oblivious. GMM generates each phrase independent of the entity it came from and hence, distributes en-tity phrases arbitrarily across clusters. We use GMM as a baseline for our experiments. Figure 2 (left) il-lustrates the two models in plate notation.
 Entity-Balanced Gaussian pLSA: Vanilla Gaus-sian pLSA may not always lead to a good clustering for comparison since the expression above does not involve any term to balance the entity-information in clusters, as motivated earlier. Thus, we incorporate a regularizer term to have a good balance (proportion) of entities in each cluster (see Figure 2 (right) (c)) resulting in our final model for comparison called Entity-Balanced Gaussian pLSA (EB G-pLSA) . The plate notation for EB G-pLSA is identical to G-pLSA.

Our regularizer is a function of the KL-divergence between multinomial distributions for every pair of entities. KL-divergence KL ( P || Q ) between two discrete distributions P ( x ) and Q ( x ) is defined as P of similarity and is equal to 0 when the two dis-tributions are identical (and greater than 0 oth-erwise). Symmetric KL-divergence is defined as Sym -KL ( P,Q ) = KL ( P || Q ) + KL ( Q || P ) .
Let P  X  nomial distributions for generating the cluster id z given the entities e j and e k , respectively. Here,  X  j and  X  k denote the respective multinomial parame-ters. We add a regularizer term to the log-likelihood minimizing the sum of symmetric KL-divergence between the distributions P  X  for every pair of entities e j and e k . Adding this reg-ularizer requires the multinomial distributions to be similar to each other, thereby preferring balanced clusters over unbalanced ones. Our regularized av-erage log-likelihood can be written as: L ( X ) is the total log-likelihood as defined in the pre-vious equation. M = tal number of entities being compared.  X  is a con-stant controlling the weight of the regularizer. Note that we add the regularizer term to the average log-likelihood (instead of the total log-likelihood) in or-der to have the same regularizer value for compar-isons having varying number of data points (descrip-tive phrases). This is important to obtain a single value of  X  which would work well across different entity comparisons. In our experiments,  X  was tuned using held-out data and was found to be robust to small perturbations.

We use standard EM to optimize the regularized log-likelihood. Since the regularizer does not have any hidden variables, E -step is identical to the one for the unregularized case. During M -step, the val-ues maximizing the mean parameters  X  z and the  X  parameter can be obtained analytically. There is no closed form solution for the parameters  X  j , X  k . We perform gradient descent to optimize these parame-ters during the M -step. In our experiments, we did not estimate the co-variance matrices  X  z and kept them fixed as a diagonal matrix with the diagonal entry (variance) being 0 . 1 . We did not learn the co-variance matrices as that would have increased the number of parameters substantially, and thus, had the danger of over fitting. The small value of the variance chosen was to ensure less overlap between different clusters.
 Clustering Experiments We conducted preliminary experiments to compare the performance of GMM (vanilla Gaussian mixture modeling using word vec-tors) with G-pLSA and EB G-pLSA on a develop-ment set consisting of 5 random city pairs. The de-scriptive phrases were constructed using the auto-mated seed list as described in IE Section. We manu-ally created the gold standard clusterings. The num-ber of clusters was set to the number in the gold set for each of the city pairs.

We used f-measure and pairwise accuracy to eval-uate the deviation from the gold standard for the clusterings produced by each of the algorithms. Ta-ble 2 shows the results. EB G-pLSA performs better than the other two algorithms on both the metrics, and especially on pairwise accuracy. Performance of G-pLSA is very similar to GMM. In order to evaluate the usefulness of our system we conducted extensive experiments on Amazon Me-chanical Turk (AMT). Our experiments answer the following questions. (1) Are comparisons generated using our clustering methods G-pLSA and EB G-pLSA preferred by users against the entity oblivi-ous baseline of GMM? (2) Are our system-generated comparison tables helpful to people for the task of entity comparison? Datasets &amp; System Settings: We experiment 6 on two datasets  X  tourism and movies. For tourism, we downloaded a collection of 16,785 travel arti-cles from WikiTravel. The website contains arti-cles that have been collaboratively written by Web users. Each article describes a city or a larger geo-graphic area that is of interest to tourists. In addition, pects of a city from a tourism point of view (e.g., places to see, transportation, shopping and eating). For our proof of concept, we performed IE only on the  X  X laces to see X  sections.

For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014). It has over 7.9 million reviews for 250,000 movies. We combined all the reviews for a movie, thus, generating a large review document per movie. This dataset is much noisier compared to WikiTravel due to presence of slang, incorrect grammar, sarcasm, etc. In addition, users also tend to compare and contrast while re-viewing movies so there are even references to other movies. As a result, the descriptive phrases extracted were much more noisy.

For the time consuming manual seed list setting of our IE system, we only use the tourism dataset. For movies, we generate seeds using annotation over LDA topics only. For all systems we use word-vectors to expand the seed list.

For each table, we generated k clusters where k 1980), and we displayed at most 30 phrases per clus-ter. We did not display any cluster that had less than 4 phrases. 4.1 Evaluation of Clustering Algorithms In order to examine whether clustering using EB G-pLSA indeed produces best comparison tables, we conducted a human evaluation task on Amazon Mechanical Turk (AMT) where users of our sys-tem were asked to indicate their preference between two comparison tables. Since we have three systems we performed this pairwise study thrice. In each study, two comparison tables were generated from different systems. For each entity-pair we asked four workers each to select which comparison table they preferred. The order of the tables was randomized to remove any biasing effect. We paid $0.3 for each ta-ble comparison. Table 3 reports the results for both domains where descriptive phrases were generated using LDA+WV.

On 30 city-pairs in the Tourism domain, work-ers preferred the comparison tables generated using EB G-pLSA 53% of the time and GMM was pre-ferred only 13% (the rest were ties). It is worth-while to note that whereas in 20% of the compar-isons, EB G-pLSA had a clear 4-0 margin, there was no such comparison where all the workers preferred the GMM model. We also requested users to provide the reasons for their preferences. While most users specified a non-informative reason such as  X  X ike it better X , some users gave specific reasons such as  X  X ubdivides the parts I find useful into more specific categories X  and  X  X asy to understand and more spe-cific points of comparison X . Our results also show that G-pLSA is a distinct improvement over GMM (44% vs. 16%). EB G-pLSA had a marginal edge over G-pLSA (43% vs. 30%).

On movies domain, we report results on 20 movie-pairs and we again found an overwhelming preference for the system using EB G-pLSA for clustering. 55% of the time, the output of EB G-pLSA was preferred over GMM X  X  10%. Other com-parisons between G-pLSA and GMM, and between our G-pLSA and EB G-pLSA systems also fol-low trends similar to tourism domain. The perfor-mance of EB G-pLSA is statistically significantly better than GMM for both the tourism and the movie datasets, with p values being less than 0.00004 and 0.002, respectively, using a one-sided students t-test. This strong preference suggests that the clustering induced by incorporating entity balance in the clus-ters produces much better comparison tables. 4.2 Value of Comparison Tables The goal of our experiments in this section was to assess whether our comparison tables add value to some realistic task and to understand the overall usefulness of our system. To our knowledge there are no other automated systems comparing cities for tourism (or movies), hence we could not eval-uate our system against existing approaches. There-fore, we decided to evaluate the benefit of the out-put generated by our system (i.e., comparison tables) against reading the original WikiTravel articles. For fairness we only use the  X  X laces to see X  sections from WikiTravel, since that was the raw text used in gen-erating comparison tables in the first place.
Since the comparisons are generated automati-cally, people may not find them understandable, or there may be missing valuable information. We test this in a human subject evaluation. We adapt the evaluation methodology developed recently for con-trasting multiple ways of presenting information and testing the overall learning of the subjects (Shahaf et al., 2012; Christensen et al., 2014). The evalua-tion is divided into two parts. In the first part the workers are given a limited time to read the informa-tion provided (articles or comparison tables) for an entity-pair. They are then asked to write a short 150-300 word summary contrasting different aspects of the two entities. Each user writes two summaries, one based on articles and the other based on our ta-ble. Our study pairs two users such that if user1 read the articles for city pair 1 and the table for city pair 2, their partner user will see the reverse. The work-ers were additionally asked which knowledge source they preferred and why.

Making a worker create summaries using both in-formation sources helps reduce the effect of worker comprehension and skill in the evaluation of our task, as each worker contributes to summaries cre-ated using our system as well as the baseline. In or-der to reduce the effects of any sequence bias, half the mechanical turk workers were first shown the output of our system followed by the articles and the other half (partners) were shown content the other way around.

In the second part of this experiment we directly compare the knowledge acquisition of these work-ers. In particular, we ask a different set of work-ers to evaluate the summaries created by the part-nered workers. In each task, a worker has to compare two summaries for the same entity-pair, one created using tables by one worker and other created us-ing articles by their partner. Each summary pair was shown to four different users and each of them was asked to select the summary they preferred for com-paring and contrasting the entities. Since we perform this experiment on Tourism data, the MTurk task de-scriptions explained that the intent of the compari-son is tourism and their summaries or preferences must be from that perspective. 4.2.1 Results
We performed this evaluation on twenty city pairs using both our information extraction methods i.e. Manual+WV expansion (referred as TABLE-M) and LDA+WV expansion (referred as TABLE-LDA) along with the EB G-pLSA method for clus-tering. The city pairs were chosen such that the cities are related but not too similar, and the workers would likely not have thought of the specific com-parisons before.

We found that in the first part where workers were given 10 minutes to create the summaries, they on average asked for 30% more time to create the sum-maries when information was presented as article. This supports our belief that our system-generated tables successfully reduce information overload. It also suggests that the structure added by the system (clusters) was useful for the comparison task and re-duced workers X  cognitive load.

We now present the results for the second part of the study in which workers evaluated the com-parison summaries written by the workers in the first part. Within 20 city-pairs, summaries for 5 city pairs (25%) generated based on TABLE-M were preferred and 5 (25%) generated based on original articles were chosen. The workers were indifferent in 10 of the city pairs (both summaries got two votes each). This shows that despite having a very high compression ratio, workers still managed to create summaries that were comparable in quality to those created by reading original documents. We repeated the same study using TABLE-LDA and found that summaries for 8 city pairs (40%) generated based on TABLE-LDA were preferred and 5 (25%) gen-erated based on original articles were chosen. The workers were indifferent in 6 of the city pairs (both summaries got two votes each).
 We did not repeat this experiment using the Movies data set as the source articles were concate-nated reviews with no structure and it would not be surprising that users prefer our system. In summary, we find that both our systems convey adequate and useful information in the comparisons and the sum-maries generated by users using our systems were found to be as good as the ones created by users reading the full articles. We define a novel task of automatically generating tabular entity comparisons from unstructured text. We also implement the first system for this task that first extracts descriptive phrases from text and then clusters them to generate comparison tables. Our clustering algorithm is a Gaussian extension of p-LSA, where the descriptive phrases are represented using embeddings in the word vector space. In or-der to have a better comparison between entities, we incorporate a balance term which prefers clus-ters where entities are proportionately represented.
We perform extensive human-subject evaluations for our systems over Amazon Mechanical Turk (AMT) on two datasets  X  tourism and movies. We find that AMT workers overwhelmingly prefer EB G-pLSA based comparisons over GMM-based. We also assess the value of our generated comparisons over reading the original articles. We find that while both sets of workers learned as much, the workers viewing tables asked for less additional time to nar-rate a comparison in words. Overall, we believe that comparison tables add value for users deciding be-tween multiple entities. In the future we wish to per-form joint extraction and clustering instead of our current pipelined approach.
 We would like to thank the users of our system: Bhadra Mani, Dinesh Khandelwal, Eshita Sharma, Kuntal Dey, Leela Muthana, Noira Khan, Samuel Kumar, Seher Contractor and Prachi Jain, and the anonymous Amazon mechnical turk workers for their evaluation and insights. We would also like to thank Ankit Anand, Happy Mittal and anonymous reviewers for their suggestions on improving the paper. The work was supported by IBM Research, Google language understanding and knowledge dis-covery focused research grants to Mausam, a KISTI grant and a Bloomberg grant also to Mausam. We would also like to acknowledge the IBM Research India PhD program that enables the first author to pursue the PhD at IIT Delhi.
