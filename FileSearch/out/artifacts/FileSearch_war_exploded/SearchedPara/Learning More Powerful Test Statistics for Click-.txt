 Interleaving experiments are an attractive methodology for evalu-ating retrieval functions through implicit feedback. Designed as a blind and unbiased test for eliciting a preference between two retrieval functions, an interleaved ranking of the results of two re-trieval functions is presented to the users. It is then observed whether the users click more on results from one retrieval function or the other. While it was shown that such interleaving experiments re-liably identify the better of the two retrieval functions, the naive approach of counting all clicks equally leads to a suboptimal test. We present new methods for learning how to score different types of clicks so that the resulting test statistic optimizes the statistical power of the experiment. This can lead to substantial savings in the amount of data required for reaching a target confidence level. Our methods are evaluated on an operational search engine over a collection of scientific articles.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval.
 General Terms: Measurement, Human Factors, Experimen-tation.
 Keywords: Implicit feedback, retrieval evaluation, click-
Given the rapidly growing breadth and quantity of information needs and retrieval tasks, the need to develop scalable and reliable evaluation methodologies has likewise been gaining in importance. Towards this end, evaluating retrieval performance based on im-plicit feedback (e.g., clicks, reformulations and dwell time) is an attractive option for several reasons. First, the evaluation is done on the actual population of users in their natural usage contexts, which is difficult to replicate when using editorial judgments. Sec-ond, recording implicit feedback is inexpensive and much faster than obtaining editorial judgments. And, finally, implicit feedback is available even for collections where it would not be economically feasible to hire relevance judges.

One approach for deriving reliable judgments from implicit feed-back is to focus on collecting relative as opposed to absolute feed-back. For example, while it is difficult to interpret clicks on an absolute scale (e.g., clicked results are relevant, non-clicked results are not relevant), there is clear evidence that clicks provide reli-able relative feedback (e.g., clicked results are better than skipped results) [2, 14, 18]. This property is exploited in Interleaving Ex-periments [13, 18] to compare the relative quality of two ranked retrieval functions h and h . For every incoming query, the rank-ings of the two retrieval functions are presented to the user as a sin-gle interleaved ranking, and the user X  X  clicks are observed. If the user clicks more on results from h than from h in the interleaved ranking, it was shown that one can reliably conclude that h is pre-ferred over h [18, 17]. From an experiment design perspective, interleaving provides a blind paired test where presentation bias is eliminated through randomization under reasonable assumptions.
In this paper, we aim to make interleaving experiments more ef-ficient  X  or scalable  X  by developing a more powerful test statistic. Our motivation comes from the intuition that not every click in the interleaved ranking is equally informative. For example, a click on the result at rank 1 in a query session immediately followed by a  X  X ack X  (i.e., a quick return to the search results page) is probably less informative than the last click in the session (which satisfies the information need). As such, having more flexible weighting schemes on clicks can reduce the variance of the test statistic improved experiment design will allow us to confidently tease apart the quality of two competing retrieval functions using substantially less data.

We present three learning methods for optimizing test statistics by using training data from pairs of retrieval functions of known relative retrieval quality (e.g., by gathering enough data so that the conventional test statistic is significant). The learned test statis-tic can then be used to more quickly identify the superior retrieval function in future interleaving experiments. Learning test statistics
This is also known as the credit assignment problem [17]. Algorithm 1 Team-Draft Interleaving
Input : Rankings A =( a 1 , a 2 ,... ) and B =( b 1 , b 2
Init : I  X  () ; TeamA  X  / 0 ; TeamB  X  / 0 ; while (  X  i : A [ i ]  X  I )  X  (  X  j : B [ j ]  X  I ) do end while
Output : Interleaved ranking I , TeamA , TeamB can be thought of as solving the inverse problem of conventional hypothesis testing, and we present an empirical evaluation on real data from an operational search engine for research papers.
The traditional Cranfield methodology [20] relies solely on edi-torial judgments for evaluation, where human judges assign explicit relevance ratings to results. Though effective at small scales, this approach quickly becomes infeasible as the evaluation tasks grow in size and number. While methods exist that reduce the labeling requirements to some extent (e.g., [6, 3, 4]), or that reduce the cost of collecting explicit feedback [5, 19], leveraging usage logs has been steadily gaining in popularity due to their inexpensive avail-ability.

Our work is closely related to the topic of learning user behavior models, since implicit feedback is essentially a reflection of human behavior. Fox et al. [11] learned an association between implicit feedback gathered by an instrumented browser and explicit judg-ments of satisfaction. Other existing approaches typically learn user behavior or document relevance models from passively col-lected usage logs (e.g., [2, 7, 10, 21, 9], often with the goal of aiding the retrieval function in providing more relvant results (e.g., [1, 8]).

Our work is distinguished from the aforementioned work on user modeling in at least two aspects. First, it is set within the frame-work of a well-controlled paired experiment design. We will be able to exploit the properties of this experiment design when de-riving and theoretically justifying the learning methods, as well as when interpreting the results. Second, prior work on user model-ing for this purpose focused largely on evaluation at the result (i.e., document) level (e.g., [1, 14, 22]). In contrast, we are interested in performing more holistic evaluations for ranking functions.
Radlinski &amp; Craswell [17] recently conducted a study comparing conventional measures (e.g., NDCG, MAP) to interleaving metrics based on clickthrough data. They found that manually increasing the weight of clicks lower in the ranking improved the statistical power of the interleaving metric. We will provide automatic learn-ing methods for optimizing the power of the test statistic, making use of many attributes beyond the rank of the clicks.
In analogy to experiment designs from sensory analysis (see e.g. [15]), interleaving experiments [13, 18] provide paired preference tests between two retrieval (i.e., ranking) functions. Such paired Figure 1: An example showing how the Team-Draft method interleaves input rankings A and B for different random coin flip outcomes. Superscripts of the interleavings indicates team membership. experiments are particularly suitable in situations where it is diffi-cult or meaningless to assign an absolute rating (e.g., rate this taste on a scale from 1 to 10), but a relative comparison is easy to make (e.g., do you like taste A better than taste B). To elicit such pairwise preferences, both alternatives have to be presented side-by-side and without presentation bias. For example, the order in which a sub-ject tastes two products must be ra ndomized, and the identity of the products must be  X  X lind X  to the user.

For the case of comparing pairs of retrieval functions, interleav-ing experiments are designed to provide such a blind and unbiased side-by-side comparison of two retrieval functions h and h .When a user issues a query q , the rankings A = h ( q ) and B = computed but kept hidden from the user. Instead, the user is shown a single interleaved ranking I computed from A and B , so that clicks on I provide feedback on the users preference between A and B un-der reasonable assumptions.

In this paper, we focus on the Team-Draft Interleaving method [18] that is summarized in Algorithm 1. Team-Draft Interleaving creates a fair (i.e. unbiased) interleaved ranking following the anal-ogy of selecting teams for a friendly team-sports match. One com-mon approach is to first select two team captains, who then take turns selecting players in their team. Team-Draft Interleaving uses an adapted version of this approach for creating interleaved rank-ings. Suppose each document is a player, and rankings A and B are the preference orders of the two team captains. In each round, cap-tains pick the next player by selecting their most preferred player that is still available, add the player to their team and append the player to the interleaved ranking I . We randomize which captain gets to pick first in each round. An illustrative example from [18] is given in Figure 1.

To infer whether the user prefers ranking A or ranking B , one counts the number of clicks on documents from each team. If team A gets more clicks, A wins the side-by-side comparison and vice versa. Denoting the sets of clicks on the respective teams with C and C for query q , the mean or median value of the test statistic over the distribution P ( q ) of queries reveals whether one of h and h is consistently preferred over the other. Section 3.1 discusses three possible tests that detect whether the mean or median of  X  ( is significantly different from zero.

Note that the presentation is unbiased in the sense that A and B have equal probability of occupying each rank in I . This means that any user that clicks randomly will not generate a significant preference in either direction.

In this paper, we address one shortcoming of the test statistic in (1): the test statistic scores all clicks equally, which is likely to be suboptimal in practice. For example, a user clicking back immediately after a click is probably an indicator that the page was not good after all. The goal of this work is to learn a more refined function score ( q , c ) that scores different types of clicks according to their actual information content. This scoring function can then be used in the following rule Note that this reduces to (1) if score ( q , c ) is always 1.
In the following, we will use a linear model score ( q , c to score clicks, where w is a vector of parameters to be learned and  X  ( q , c ) returns a feature vector describing each click c in the con-text of the entire query session q . We can now rewrite  X  ( as where Feature vectors  X  ( q , c ) will contain features that describe the click in relation to position in the interleaved ranking, order, and presen-tation. In Section 5.2, we will decribe the feature construction used in our empirical evaluation.
To decide whether an interleaving experiment between h and h shows a preference in either dir ection, one needs to test whether some measure of centrality (e.g. median, mean) of the i.i.d. ran-dom variables  X  i  X   X  ( q , C , C ) is significantly different from zero. For conciseness, let (  X  1 ,...,  X  n ) denote the values of random sample. We consider the following three tests, which will also serve as the baseline methods in our empirical evaluation.
The simplest test, and the one previously used in [12, 13, 18], is the Binomial Sign Test (see e.g. [16]). It counts how often the sign of  X  i is positive, i.e. S =  X  n random variable, and the null hypothesis is that the underlying i.i.d. Bernoulli random variables [  X  i &gt; 0 ] have p = 0 . 5.
Unlike the Binomial Sign Test, the z-Test (see e.g. [16]) uses the magnitudes of the  X  i and tests whether their sum is zero in ex-pectation. The z-Test assumes that S = 1 n  X  n i = 1  X  i is normal, which is approximately satisfied for large n . The ratio of the observed value s = 1 n  X  n i = 1  X  i and standard deviation st d ( score z = s / st d ( S ) , monotonically relates to the p-value of the z-test. While st d ( S ) has to be known, an approximate z-test results t-test accounts for the additional v ariability from the estimate of the standard deviation, but for large samples z-test and t-test are virtu-ally identical.

Finally, we consider the Wilcoxon Signed Rank Test (see e.g. [16]) as a non-parametric test for the median of the  X  i being 0. To compute the test statistic, the observations are ranked by Let the resulting rank of  X  i be r i . The test statistic is then computed as W =  X  sign (  X  i ) r i ,and W is tested for mean 0 using a z-test.
The idea behind learning is to find a scoring function that results in the most sensitive hypothesis t est. To illustrate this goal, con-sider the following hypothetical scenario where the scoring func-session from other clicks within the same session. The correspond-ing feature vector  X  ( q , c ) would then have two binary features Assume for simplicity that every query session has 3 clicks, with  X  X ot last clicks X  being completely random while  X  X ast clicks X  fa-voring the better retrieval function with 60% probability. Using the weight vector w T =( 1 , 1 ) (i.e., the conventional scoring func-tion), one will eventually identify that the better retrieval function gets more clicks (typically after  X  280 queries using a t-test with p = 0 . 95). However, the optimal weight vector w T =( 1 , 0 identify the better retrieval function much faster (typically after  X  150 queries), since it eliminates noise from the non-informative clicks.

The learning problem can be thought of as an  X  X nverse X  hypoth-esis test: given data for pairs ( h , h ) of retrieval functions where we know h h , find the weights w that maximizes the power of the test statistic on new pairs. More concretely, we assume that we are given a set of ranking function pairings { ( h 1 , h 1 ) ,..., ( which we know w.l.o.g. that h i is better than h i ,i.e. h preference may be known by construction (e.g., h i is a degraded version of h i ), by running interleaving until the conventional test statistic that scores each click uniformly becomes significant, or through some expensive annotation process (e.g., user interviews, manual assessments). For each pair ( h i , h i ) , we assume access to usage logs from Team-Draft Interleaving [18] for n i queries. For each query q j , the clicks C j and C j for each  X  X eam X  are recorded training sample Note that we are essentially treating all interleaving pairs as a single combined example 2 . After training, the learned w and the resulting test statistic  X  w ( q , C , C ) will be applied to new pairs of retrieval functions ( h test , h test ) of yet unkown relative retrieval quality.
We now propose three learning methods, with each correspond-ing to opimizing a specific inverse hypothesis test.
In the simplest case, we can optimize the parameters w of score to maximize the mean difference of scores between the better and the worse retrieval functions, To abstract from different scalings of w and to make the problem well posed, we impose a normalization constraint || w || = to the following optimization problem: which can be written more compactly using  X  j =  X  ( q j ,
A better approach may be to explicitly treat each interleaving pair as a separate example. This has the the following closed-form solution that can be derived via Lagrange multipliers: While maximizing the mean difference is intuitively appealing, one key shortcoming is that variance is ignored. In fact, one can think of this method as an inverse z-Test, where we assume equal variance for all w . Since the assumption of equal variance will clearly not be true in practice, we now consider the following more refined methods.
The following learning method removes the assumption of equal variance and optimizes the statistical power of a z-Test in the gen-eral case (with the null hypothesis that the mean is zero). Finding the w that maximizes the z-score (and therefore the p-value) on the training set corresponds to the following optimization problem: w  X  = argmax While (3) has two symmetric solutions, we are interested only in as For any w solving this optimization problem, cw with c &gt; a solution. We can thus rewrite the problem as Using the Lagrangian and solving for zero derivative w.r.t. w and  X  , one arrives at a closed form solution. Denoting  X  =  X  j  X  j and  X  =  X  j  X  j  X  T j can be written as While not used in the experiments for this paper, a regularized version  X  reg of the covariance matrix  X  can be used to prevent overfitting. One straightforward approach is to add a ridge term  X  reg =  X  +  X  I ,where I is the identity matrix and  X  is the regulariza-tion parameter.
Last but not least, we consider a learning method that relates to inverting the Wilcoxon Rank Sign test. A good scoring function  X  ( q , C , C ) for the Wilcoxon test should optimize the Wilcoxon statistic, which can be computed as follows. Assuming h h w.l.o.g., we denote a prediction as  X  X orrect X  if  X  w ( q , otherwise, we denote it as incorrect. Ranking all observations by |  X  w ( q , C , C ) | (assuming no ties), the Wilcoxon statistic is isomor-phic to the number of observation pairs where an incorrect observa-tion is ranked above a correct observation. One strategy for mini-mizing the number of such swapped pairs, and therefore optimizing the p-value of the Wilcoxon test, is to choose where Pr ( h h | q , C , C ) is the estimated probability that h is better than h given the clicks observed for query q .

We estimate Pr ( h h | q , C , C ) from the training data S using a standard logistic regression model Using again the convention that h h for the training data and abbreviating  X  j =  X  ( q j , C j , C j ) , the parameters w are chosen via maximum likelihood, w  X  denotes the logistic regression solution on the training data. We used the LR-TRIRLS package 3 to solve this optimization problem. The final ranking function can be simplified to the linear function  X  ( q , C , C )= w T  X  ( q , C , C ) , since it produces the same rankings and signs as (4).
We evaluated our methods empirically using data collected from the Physics E-Print ArXiv 4 . In particular, we used two datasets of click logs collected while running Team-Draft Interleaving ex-periments. For both datasets, we recorded information for each query (e.g., the entire session) and click (e.g., rank, timestamp, re-sult information, source ranking function, etc). This information is used to generate features for learning (see Section 5.2 below). One could also collect user-specific information (e.g., user history), but we have not done so in the following experiments.  X  X old standard X  . Our first dataset is taken from the Team-Draft experiments described in [18]. In these experiments, the incumbent retrieval function was corrupted in multiple ways to provide pairs of retrieval functions with known relative quality. This provides cheap access to a  X  X old standard X  dataset, since one knows by con-struction which retrieval function is superior within each pair. A total of six pairs was evaluated, with each yielding slightly over 1000 query sessions.

New interleaving experiments . Our second dataset was gener-ated via interleaving pairs of retrieval functions without necessar-ily having knowledge of which retrieval function is superior within each pair. For example, one retrieval function that we consid-ered modifies the incumbant retrieval function by giving additional weight to query/title similarity. It is a priori unclear whether this would result in improved retrieval quality. Ideally (and intuitively), learning a test statistic on the gold standard dataset should help us more quickly determine the superior retrieval function within these interleaving pairs. We examine this hypothesis further in Section 6.4. A total of six different retrieval functions are considered in this setting. We collected click data from interleaving every possible pairing of the six, resulting in fifteen interleaving pairs with each http://komarix.org/ac/lr/ http://arxiv.org yielding between 400 and 650 query sessions. We then removed three of the fifteen interleaving pairs from our analysis, since all methods (including the baselines) showed poor performance (p-value greater than 0 . 4), making them uninteresting for comparison purposes.
The features that are used in the following experiments describe a diverse set of properties related to clicking behavior, including the rank and order of clicks, and whether search result clicks led to a PDF download in ArXiv 5 .Let C own and C ot her denote the clicks from the own team and the other team, respectively, for a single query session. Recall from (2) that our feature function  X  ( q , C own , C ot her ) decomposes as We will construct  X  ( q , c ) for c  X  C own in the following way: 1. 1 always 2. 1 if c ledtoadownload 4. If | C own | == | C ot her | : 5. If it is a single-click query: 6. If it is a multi-click query: Analogously, we construct  X  ( q , c ) for c  X  C ot her by swapping C and C ot her in the preceding feature definitions.

Note that some features are more naturally expressed at the query level. For example, feature 3 can be equivalently expressed directly as feature of  X  ( q , C own , C ot her ) as For clarity, we focus our formulation on click-level features, since most features we used are more naturally understood at the click level.
All search results correspond to research papers that are available for download.
 Figure 2: Comparing the sample size required versus target t-test p-value in the synthetic experimental setting. Measure-ments taken from 1000 bootstrapped subsamples for each sub-sampling size.
For ease of presentation, we will only show comparisons against the t-test baseline; our empirical results also hold when comparing against the other baselines. In general, we find the inverse z-test to be the best performing method, with the inverse rank test often being competitive as well.
We first conducted a synthetic experiment where all six gold standard interleaving pairs in the training set are mixed together to form a single (virtual) interleaving pair. From this, 70% of the data was used for training and the remaining 30% for testing. In-tuitively, this setup satisfies the assumption that the click distribu-tion we train on is the same as the click distribution we test on  X  a core assumption often made when analyzing machine learning approaches.

Figure 2 shows how the required sample size grows with de-creasing target t-test p-value. This plot (and all similar plots) was generated by subsampling the test set (with replacement) at varying subset sizes and computing the p-value. Subset sizes increase in in-crements of 25 and each subset size was sampled 1000 times. Our goal is to reduce the required sample size, so lower curves indicate superior performance.

We observe in Figure 2 that our methods consistently outperform the baseline. For example, for a target p-value of 0.01, the inverse z-test requires only about 800 samples whereas the baseline t-test requires about 1200  X  a 50% improvement. In all of our subse-quent experiments, we find that the max mean difference method consistently performs worse than the inverse z-test. As such, we will focus on the inverse rank test and the inverse z-test in the re-maining empirical evaluations.
To give some insight into the scoring function  X  w ( q , C w
T  X  ( q , C , C ) learned by our methods, Table 1 shows the weights w generated by the inverse rank test on the full gold standard train-ing set. Since the features are highly correlated, it is difficult to gain insight merely through inspection of the weights. As such, we now provide a selection of prototypical example queries for which we will compute the feature vector  X  =  X  ( q , C , C ) and the value of  X  w ( q , C , C ) . 1. Single click on result from h at rank 1 : Feature vector Table 1: Weights learned by the inverse rank test on the full gold standard training set. See Section 5.2 for a full description of the features.

ID Feature Description (w.r.t.  X  ( q , c ) ) Weight 1 Click 0.056693 2 Download 0.020917 3 More clicks &amp; downloads than other team 0.052410 4a 1 [# Clicks equal]  X  Title bold frac 0.083463 4b 1 [# Clicks equal]  X  Abstract bold frac 0.118568 5a Single click query AND Rank &gt; 1 0.149682 5b Single click query AND Rank  X  10 0.004950 6a Multi-clicks AND First click 0.063423 6b Multi-clicks AND Last click 0.000303 6c Multi-clicks AND First click AND Rank &gt; 1 0.015217 6d Multi-clicks AND Click at rank = 1 0.018800 6e Multi-clicks AND Click at ranks  X  3 -0.00419 6f Multi-clicks AND Click at ranks  X  10 0.067362 6g Multi-clicks AND Regr ession click 0.033067 2. Single click on result from h at rank 3 : Feature vector 3. Single click on result from h at rank 3 followed by download : 4. One click on result from h at rank 1, followed by another
In this setting, we trained our models on five of the gold standard interleaving pairs and tested on the remaining one, repeating this process for all six pairs. This provides a controlled way of eval-uating generalization performance. Figure 3 shows how required sample size changes as the target p-value decreases. Again, lower curves indicate superior performance. We observe the inverse z-test performing at least as well as the baseline on all except training pair 3. Note, however, that training pair 3 is an exceptionally easy case where one can achieve confident p-values with very little data. We observe the inverse rank test to also be competitve, but with some-what worse performance.
To further evaluate the methods in a typical application scenario, we trained our models on all six of the gold standard interleaving pairs, and then tested their predictions on new interleaving pairs. It should be noted that we did not examine the new interleaving dataset when developing the features described in Section 5.2. As such, this evaluation very closely matches how such methods would be used in practice.

Figure 4 shows, for all twelve test cases, how required sample size changes as the target t-test p-value decreases. We observe both learning methods consistently performing at least as well as, and often much better than, the baseline t-test (with the exception of Exp 1). We also verified that all methods and baselines agree on the direction of the preference in all cases (since we are using a two-tailed test).

Table 2 provides numerical comparisons for several standard sig-nificance thresholds. For half of the twelve test cases, the inverse z-test reduces the required sample size by at least 10% for a target significance of p = 0 . 1. For a quarter of the cases, the inverse z-test achieves a significance of p = 0 . 05 using the available data whereas methods outperform baseline in most cases. the baseline t-test fails to do so. These results imply that substantial savings can be gained from employing optimized test statistics.
In this section, we discuss and summarize the core assumptions and limitations of our approach.

While the learned test statistics generally improved the power of the experiments on new retrieval function pairs ( h , h likely a limit to how different the new pair may be from the training pairs. If the retrieval functions to be evaluated move far from the training data (e.g. after several iterations of improving the ranking function), it might be necessary to add appropriate training data and re-optimize the test statistic. Furthermore, we do not believe that test statistics learned on one search engine would necessarily generalize to a different collection or user population.
A key issue in generalizing to new retrieval function pairs lies in the appropriate choice of features  X  ( q , C , C ) if the chosen features allow the learning algorithm to models spe-cific idiosyncracies of the training pairs, this will likely result in poor generalization on new pairs. Furthermore, different systems may record different types of usage behavior (such as maintaining user IDs for personalization purposes). This dictates the types of features that are available to the learning methods.

Pooling the training examples from multiple training pairs into one joint training set might lead to unwanted results, since the learning methods optimize an  X  average X  statistic over multi-ple pairs. In particular, the methods might ignore difficult to dis-criminate pairs in return for increased discriminative power on easy pairs. It would be more robust to minimize the maximum p-value uniformly over all training pairs.

Finally, the empirical results need to be verified in other retrieval domains. Particularly interesting are domains that include spam. It would be interesting to see whether one can learn scoring functions that recognize (and discount) clicks that were attracted by spam.
We have presented learning methods for optimizing the statisti-cal power of interleaving experiments for retrieval evaluation. Given clickthrough data from interleaving pairs of retrieval functions of known relative retrieval quality, our proposed methods learn an op-timized test statistic. We showed that these learned test statistics generalize to new retrieval functions, often substantially reducing the number of queries needed for evaluation. &gt; 625 415 59 70 352 174 &gt; 400 &gt; 625 &gt; 475 95 128 &gt; 500 328 &gt; 400 &gt; 625 &gt; 475 142 174 &gt; 500 &gt; 425 &gt; 400
Inv. rank test p=0.2 373 149 180 90 64 575 157 &lt; 50 71 353 141 &gt; 625 296 74 129 &gt; 500 260 &gt; 400 &gt; 625 423 99 184 &gt; 500 365 &gt; 400 &gt; 625 199 144 138 &gt; 500 339 &gt; 400
The idea of evaluating and learning via pairwise comparisons is attractive due to its simplicity in interpretation. In such cases, it is generally quite intuitive to design meaningful hypothesis tests. As such, the general techniques described in this paper for opti-mizing these statistical tests (e.g., inverse z-test) can also be ap-plied to other domains beyond traditi onal information retrieval us-ing domain-appropriate feature representations.
 The work is funded by NSF Awards IIS-0812091 and IIS-0905467. The first author is also supported in part by a Microsoft Research Graduate Fellowship and a Yahoo! Key Scientific Challenges Award. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web search [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning [3] J. A. Aslam, V. Pavlu, and E. Yilmaz. A sampling technique [4] C. Buckley and E. M. Vorhees. Retrieval evaluation with [5] C. Callison-Burch. Fast, cheap, and creative: Evaluating [6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test [7] B. Carterette and R. Jones. Evaluating search engines by [8] O. Chapelle and Y. Zhang. A dynamic bayesian network [9] G. Dupret and C. Liao. Cumulating relevance: A model to [10] G. Dupret and B. Piwowarski. A user browsing model to [11] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. [12] T. Joachims. Optimizing search engines using clickthrough [13] T. Joachims. Evaluating retrieval performance using [14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, [15] D. Laming. Sensory Analysis . Academic Press, 1986. [16] A. Mood, F. Graybill, and D. Boes. Introduction to the [17] F. Radlinski and N. Craswell. Comparing the sensitivity of [18] F. Radlinski, M. Kurup, and T. Joachims. How does [19] R. Snow, B. O X  X onnor, D. Jurafsky, and A. Y. Ng. Cheap and [20] E. M. Vorhees and D. K. Harman, editors. TREC: Experiment [21] K. Wang, T. Walker, and Z. Zheng. Pskip: Estimating [22] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias:
