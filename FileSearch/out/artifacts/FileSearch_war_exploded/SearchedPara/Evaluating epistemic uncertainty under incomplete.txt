 1. Introduction
In this study we revisit the implications on system comparisons that arise from incomplete relevance assess-ments (i.e. incompleteness ), and in particular the assumption that unassessed documents are not relevant.
Instead of assuming unassessed documents are not relevant ( Salton, 1992 ), or more recently, ignoring docu-ments that are not judged when estimating performance ( Buckley &amp; Voorhees, 2004 ), we propose an alterna-tive method: to quantify the proportion of unassessed documents in a systems ranked list. This alternative method leads to a complementary evaluation methodology, providing a new set of measures that attempt to quantify the epistemic uncertainty during system comparison. for both researchers who re-use collections, and also for designers of new test collections. Adopting such an approach is important as researchers can detect potential uncertainty during evaluation, and also identify uation methodology, as well as highlighting the implications of using particular performance metrics, related to the depth of measurement, under incomplete assessments.

Before introducing this new methodology we first provide some background context by reviewing the run-ning debate on incompleteness and the subsequent implications on system comparison (Section 1.1 ). Next we discuss the typical evaluation metrics used within the context of incompleteness (Section 2 ). We then introduce this new methodology which augments the current evaluation protocol (Section 3 ). Next, we provide an empir-ical analysis of this approach across a range of existing test collections (Section 4 ). Finally, we discuss the implications of this study (Section 5 ), before concluding the paper (Section 6 ). 1.1. Background
The beginnings of laboratory based evaluation of Information Retrieval (IR) systems involved studies such as the comparison of ASITIA and Documentation Inc. indexing systems, the Cranfield I and II experiments,
SMART evaluation, and the in-depth evaluation and failure analysis of the Medlars search service [see were relatively small compared to modern collections  X  the Cranfield collection consisted of 1400 documents.
Studies involving larger collections followed, notably the STAIRS study for IBM where over 35,000 pages of text were used as a collection ( Blair &amp; Maron, 1985 ). The experience gained from these studies has been incor-porated into the creation of modern collections where collection size has grown considerably. The most widely used examples being the Text Retrieval Evaluation Conference (TREC) initiative ( Voorhees &amp; Harman, 2005 ), the Cross Language Evaluation Forum (CLEF), 2 and NTCIR. 3
Most modern test collections adhere to a well defined model based on the findings of these previous studies referred to as the Cranfield paradigm ( Voorhees, 2002 ). A corpus that follows the Cranfield model will consist of a collection of documents, statements of information need (named topics), and a set of relevance judge-ments listing the relevant documents that should be returned for each topic. To ensure fair comparison between systems, a number of key assumptions are made, including: The topics are independent.
 All documents are judged for relevance (i.e. completeness).
 These judgements are representative of the target user population.
 Each document is equally important in satisfying the users information need.
 The gathering of relevance assessments is independent of any evaluation that will use these assessments.
These assumptions are made to ensure fair comparison of system performance, although to develop an  X  X  X deal X  X  collection where these assumptions hold is unrealistic  X  factors such as the collection size and available (human) resources often dictate to what degree these assumptions do hold. As a consequence these assump-tions are often relaxed while compensating for any potential uncertainty that could be introduced with any relaxation, such as a bias favouring one system over another. For example, under the original paradigm rel-evance judgements were assumed to be complete i.e. all documents were judged in the document collection per topic statement. Initially completeness was viable but as document collection size increased, assessing all doc-uments for all topics became intractable without exhaustive resources. It has been estimated that it would take more than nine months to judge an average size TREC collection for a single topic ( Voorhees, 2002 ). Not only is this expensive both in terms of time and resources, but over a protracted time period the criteria an assessor will use to judge a document (for relevance) could deviate significantly, resulting in inconsistencies in the relevance assessments. 4
Nonetheless, incompleteness is problematic to laboratory based system evaluations. For example, the computation of recall based performance measures require the complete set of relevant documents (in the col-lection) to be known. To address this limitation, techniques such as system pooling have been proposed ( Spa  X  rck-Jones &amp; Van Rijsbergen, 1975 )  X  pooling is a process of combining a number of varying search approaches to sample the collection for relevant documents providing an unbiased estimate of recall. From this estimate, the relative system performances can be compared.

When using pooling to estimate recall it is difficult to ascertain whether the majority of relevant documents have been discovered. There have been a number of empirical studies that have attempted to quantify how many relevant documents remain undiscovered. For example, Zobel (1998) defined a method to extrapolate the potential numbers of unassessed relevant documents in TREC collections. He approximated that a large percentage of relevant documents were still to be discovered, especially across topics where a large number of relevant documents were already found through pooling, concluding that the assumption of unassessed doc-uments as not relevant was unfounded.

For this very reason the effect that pooling, in the context of relevant document recall, may have on system comparisons has been investigated. Such studies have focused upon several different areas of the completeness assumption and system pooling including; the effect on system comparison and the subsequent uncertainty when using incomplete relevance judgements ( Buckley &amp; Voorhees, 2004; Voorhees &amp; Harman, 2005; Zobel,
Sanderson &amp; Joho, 2004; Zobel, 1998 ), automatically generated relevance assessments ( Aslam &amp; Savell, 2003; Soboroff, Nicholas, &amp; Cahan, 2001 ), and the importance of significance testing during system compar-now standard assessment procedure of pooling, and the resultant evaluation measures adopted, does indeed impact upon the fair and unbiased comparison of retrieval systems, and to what extent.

A recent investigation of the TREC Robust-HARD 2005 collection identified a bias in the collection which was a result of both a shallow pool depth and similar runs forming the system pool ( Buckley, Dimmick,
Soboroff, &amp; Voorhees, 2006 ). The outcome was a bias favouring systems that retrieved documents containing the  X  title  X  keywords of the TREC topic. Although this does not necessarily indicate a failing of system pooling it motivates the need for a stronger evaluation framework, which considers aspects such as pooling, the status of unassessed documents, and measurement depth within the evaluation.

We now critically review this debate in further detail. We first highlight the potential implications of incom-pleteness within the Cranfield paradigm and any uncertainty that may be introduced as a result. We then investigate these implications within a newly proposed framework designed to quantify the uncertainty between system comparisons. 1.2. System evaluation under incomplete relevance judgements
Swanson (1988) stated as one of his postulates of impotence, a set of truisms for Information Retrieval, that it is never possible to verify if all relevant documents have been discovered for a topic, as one can never exam-ine all documents without unlimited resources while using a strict and static set of criteria for judging rele-vance. This truism is related to the universal rule that empirical evidence can always be refuted but never verified. In other words, a claim of completeness can always be refuted by discovering a previously unassessed relevant document within a collection. This postulate is especially resonant given the size of modern test col-lections such as those created as part of the TREC initiative ( Voorhees &amp; Harman, 2005 ). For this very reason relative system performances are compared instead of absolute values of effectiveness, as measures of retrieval performance often require knowledge of the total number of relevant documents in a collection with respect to each topic. It is assumed that relative system performances can be compared as long as robust strategies to estimate the proportion of documents relevant to a topic are used, ensuring fair experimental conditions for the systems under comparison. One such technique for recall estimation is system pooling. 1.2.1. System pooling System pooling was proposed to address the intractability of the completeness assumption ( Spa  X  rck-Jones &amp; Van Rijsbergen, 1975 ). Pooling is a focused sampling of the document collection that attempts to discover all potentially relevant documents with respect to a search topic e.g. approximate the actual number of relevant documents for a given topic. To do so, a number of (diverse) retrieval strategies are combined to probe the document collection for relevant documents. 5 Each system will rank the collection for a given topic, then the top k documents from the subsequent ranked lists are collated, removing duplicates, to form a pool of unique documents. 6 All documents in this pool are then judged for relevance by an assessor(s) using specified when determining which documents are (topically) relevant or not relevant. 1.2.2. Status of unassessed documents
The remaining unassessed documents are assumed not to be relevant. This assumption follows the argument put forward by Salton, that by using a range of different IR systems the pooled method will discover  X  X  X he vast majority of relevant items X  X  ( Salton, 1992 ). This argument is based on the assumption of diminishing returns i.e. because many runs contribute to the system pool, it is highly likely that the majority of relevant docu-ments, or those documents representative of relevant documents, are returned through pooling. If this assumption holds then there is little need to assess those documents not included during pooling.
There has been a running debate about the validity of the assumption that unassessed documents are not relevant. Initially this assumption was made on smaller collections such as Cranfield and CACM, however, as
Blair posited, the percentage of unassessed documents could be up to 99% for a given topic with respect to a modern collection, leaving a large proportion of relevant documents undiscovered ( Blair, 2002b ):
This argument can be explained as follows: 1. The assumption that the intellectual content of a document can be accessed by using some form of query term matching alone. Even by submitting variations of query terms, adjusted through trial and error, as in a typical search session, the likelihood of a searcher finding a substantial proportion of relevant documents has been discovered to be low across a number of various studies ( Blair, 1996 ). An explanation for this lim-itation is that the intellectual content of a document is difficult to represent automatically and relates to
Swanson X  X  5th and 6th postulates of impotence ( Swanson, 1988 ). A document can be about a topic without ever mentioning key terms or phrases that a user may expect to appear. Also query terms chosen by the user may not discriminate between relevant and non-relevant documents, especially as the collection size grows ( Blair, 2002a ). A user searching for documents on a new subject may not select terms representative of that topic, which will discriminate relevant from non-relevant documents that share similar vocabulary. Conse-quently, not all potentially relevant documents will be retrieved through keyword matching techniques alone. For example, the study by Buckley et al. (2006) illustrates how a shallow pool depth may result in a bias favouring the retrieval of relevant documents which contain title keywords in the TREC topic (i.e. short queries) over relevant documents that do not. 2. The belief that pooling is accurate because relevant documents were discovered. Locating a proportion of relevant documents is not a sole indicator of good retrieval performance, as the proportion of relevant doc-uments missed is not known unless it is quantified through other means. Swanson refers to this as the  X  X  X al-lacy of abundance X  X   X  by discovering a (substantial) number of documents about a topic creates a illusion that little remains hidden ( Swanson, 1960 ). The searcher, or in this case the pooling process, cannot be used to determine how much still remains hidden. Such an assumption comes from the mistaken belief that sci-ence  X  X  X eeks confirmations of a hypothesis rather than rejections X  X . Confirmations of a hypothesis are easy to show but do not  X  X  X rovide deep insights unless there is some degree of risk in the predictions X  X  ( Blair, 1996 ).
This philosophy is motivated by Karl Popper ( Popper, 2000 ), who outlined a number of considerations concerning the verification of scientific theories including that  X  X  X onfirming evidence should not count except when it is the result of a genuine test of the theory; and this means that it can be presented as a seri-ous but unsuccessful attempt to falsify the theory X  X . In other words, system pooling alone cannot be used to confirm the accuracy of system pooling. 3. A pooling of systems may not always search in all the best places. This is a reflection of the diversity of the systems contributing to the pool. Does the selection of systems included in the pool share enough diversity as to cover all means of searching, thereby locating all potentially relevant documents that could be found through searching? If the retrieval mechanisms in the pool are (theoretically) similar then we could argue no. If they exhaustively cover many varying approaches (both manual and automatic the converse. But paradoxically we will never be confident of which case is true until we can quantify with a high degree of confidence how accurate each retrieval system is at discovering all relevant documents, as indicated in (2) , therefore researchers should always err towards the side of caution when reporting results.
However, in a rejoinder to the comments of Blair, Voorhees and Harman (2003) highlight a key point that as pooling is a union of many different ranking approaches and because only relative system performance is measured, if the number of systems contributing to a pool is sufficiently large and these systems are diverse, bias towards one or a set of systems should be minimised, even though not all relevant documents are found.
Absolute system performance may not be accurately estimated using incomplete relevance assessments, but the relative performances of systems can be fairly compared. This is related to the argument put forward by Salton (1992) , where as long as the conditions remain even for all systems, then the relative differences between systems can be compared fairly and with a high degree of certainty.

However, we hypothesise that uncertainty remains when comparing the relative performance of systems due to the status of unassessed documents (i.e. those documents not in the pool of documents to be assessed for relevance). When using pooling to estimate recall it is difficult to ascertain whether the majority of relevant documents have been discovered ( Zobel, 1998 ). It is not clear what impact the potential proportion of relevant unassessed documents may have on system comparisons. A number of studies have investigated this issue and whether system pooling is a robust solution to the completeness problem as a result of this ( Buckley &amp; Voo-rhees, 2004; Voorhees, 2002; Wallis &amp; Thom, 1996; Zobel, 1998 ). We now summarise these studies. 1.2.3. Potential implications of incompleteness on evaluation
Zobel (1998) investigated if potential bias, introduced during pooling, may effect system evaluation. Two forms of potential bias were investigated: (1) system reinforcement and (2) system omission bias. The first form of bias assumed that (theoretically) related systems involved in the pooling process, or approaches that combine a number of different retrieval strategies, could reinforce each other at the expense of more diverse systems (to these techniques), with the performance of these novel systems being underestimated as a consequence. The pre-mise was that systems that rank similar document sets will have a higher likelihood of documents from this set being included in the judgement pool than more diverse, unrelated, techniques. These related systems as a result could share a larger representation of assessed documents per topic. Under the same assumption, techniques that combine different retrieval approaches represented in the system pool may also have performance overes-timated. The assumption is that combination approaches (may) inadvertently mimic the pooling procedure thus maximising the number of judged documents in the final ranked. As a side-effect of pooling, both scenarios increase system performance artificially as the probability of an unassessed document being relevant is zero while the probability of an assessed document is significantly higher than zero.

If both scenarios do occur then this may be compounded by current evaluation practice. It is common to use metrics estimated from a measurement depth d larger than the pool depth k for system comparison. For example, the measurement depth for TREC is at d = 1000 documents per topic, while the pool depth is no more than k = 100 per submitted run  X  although this threshold may vary depending on task, collection and available resources. The rationale is that good systems will retrieve relevant documents at ranks greater than the pool depth. These documents will, however, be included in the set of assessed documents as they will be retrieved by the remaining systems that form the system pool. Thereby measuring beyond the pool depth will provide better discrimination between systems of varying performance. Despite improved discrimination, uncertainty in the results is also increased. Such practice in turn creates potential uncertainty where similar systems will have a larger proportion of judged documents in the final ranked list to depth d .
Zobel concluded that the artificial effect of system reinforcement would be insignificant if the judging pool is sufficiently deep, although measurement depth may be prone to this effect. The current practice of using a measurement depth larger than the pool depth caused reservations. Increasing measurement depth improves discrimination between systems i.e. determining whether system A is significantly better than B. But a change in measurement depth was noted to affect system ranking in terms of performance when comparing multiple systems. Sometimes the ordering of systems in terms of performance changed by as many as 6 places in the ranking as measurement depth increased. A number of systems were also found to be returning a larger pro-portion of unassessed documents. Applying a measurement depth of 1000 documents could underestimate those systems that are not as well represented as others in terms of the proportion of documents in the judging pool as a consequence.

The second reservation (system omission) highlighted that the evaluation of novel systems that did not con-tribute to the judging pool may have their performance understated. A novel system, especially one that diverges from the pooled retrieval strategies significantly enough that it retrieves a large proportion of docu-ments outwith the pool, could be underestimated. When analysing the ten topics with the highest proportion of relevant documents only, average performance of a novel system was underestimated by up to 19% for the
TREC-3 collection. 10 This underestimation was stated to be dependent on the pool size. The effect of system omission was thought to minimise as the document pool size increased per topic. These fluctuations were con-sidered to be identifiable in practice through the analysis of documents in a ranked list that are not contained in the judgement pool. New novel systems in particular were warned to be aware of potential bias towards their system and underestimation in performance.

This viewpoint was further supported with incompleteness believed to have only a negligible impact on the evaluation of new systems that were not represented in the original system pool ( Voorhees, 2002 ). System omission was believed to be a  X  X  X ed herring X  X , however the impact of system reinforcement was not formally evaluated. In conclusion, the current evaluation practice was found to be robust to the violation of the com-pleteness assumption as the effects of system reinforcement and omission would produce only slight variations in system performance.

Incompleteness was further analysed in a following study ( Buckley &amp; Voorhees, 2004 ). The motivation of this investigation was to measure empirically whether incompleteness was problematic during current IR lab-oratory evaluation, in particular the stability of standard IR measures under incompleteness. To measure the effects of incompleteness, the ranking of systems in terms of performance across both  X  X  X omplete X  X  and incom-plete assessments were correlated. A number of TREC collections were used for the evaluation, with a set of incomplete relevance assessments artificially simulated through the random sampling of the original assess-ments, which was assumed to be  X  X  X omplete X  X . However, as the TREC collections use system pooling to com-pile relevance judgements, these original assessments are invariably incomplete as well.
The findings of the study identified that the practice of system pooling was robust to system omission across a wide range of topics and varying levels of incompleteness. However, the standard evaluation measures were not stable under substantial levels of incompleteness, with only bpref found to be invariant (most of the time).
Therefore, new novel systems not represented in the pool could be fairly compared with those systems repre-sented when using bpref . This result was significantly important as the bpref measure was designed specifically to address the problem of incomplete relevance assessments by removing unassessed documents, which are normally treated as not relevant, from the ranked document list. The bpref metric was found to be more stable across incomplete assessments, although the measure tended to be coarse when there was a small proportion of relevant documents belonging to a topic. It was concluded overall that adopting system pooling for IR sys-tem evaluation was robust to incompleteness.
 The effects on evaluation when using shallow pooling depth has also been studied. The investigation of the
TREC Robust-HARD 2005 collection identified a bias in this collection which was a result of both a shallow pool depth, and (potentially) similar runs forming the system pool ( Buckley et al., 2006 ). The outcome was a bias in the collection favouring relevant documents that contained title terms from the TREC topic compared to other relevant documents. Although this does not necessarily indicate a failing of system pooling it high-lights the need for further investigation into the evaluation framework, especially aspects such as pooling, the status of unassessed documents and measurement depth. 1.3. Focus of this study
Based on an analysis of these studies, we posit that uncertainty remains when comparing the relative per-formance of systems as a result of the status of unassessed documents (being not relevant). One of the cited limitations with laboratory studies is the large amount of subjectivity or uncertainty in such evaluations. The nature of the scientific method demands as much objectivity and certainty as possible. After analysing the his-tory of retrieval evaluation we believe that the status of unassessed documents and the resulting suitability of comparing systems with varying levels of assessed documents is still an open issue. We are especially motivated by the recommendations of Zobel (1998) , who warned researchers when evaluating new systems across existing test collections for cases where performance could be underestimated. However, a standard protocol for detecting such cases has not been proposed as of yet. We therefore propose a new methodology for quantifying uncertainty during system comparisons that may exist because of incomplete relevance assess-ments. By doing so, we can determine when it is possible to fairly compare two systems using current mea-sures, especially those systems that do not contribute to the pool. Instead of compensating for or ignoring potential uncertainty during system comparisons due to incompleteness, we believe that the proportion of unassessed documents should be captured and reported. Reporting this information can be a useful source of information to help quantify the confidence, accuracy and/or reliability of system performance. By captur-ing such information, we can determine whether two systems are compared under similar conditions, and flag those cases when one system may have an advantage over another due to a particular condition found in a test collection. 2. System comparisons
Under the Cranfield model system effectiveness is typically estimated using Precision (i.e. the proportion of retrieved documents that relevant) and Recall (i.e. the proportion of (known) relevant documents retrieved overall). 11 We now define these metrics. Let N be the total number of documents in a collection, R be the set of relevant documents, and X be the set of retrieved documents. Under the current evaluation protocol, the status of unassessed documents is assumed to be not relevant (see Fig. 1 ). Using the contingency Table 1 , Precision can then be defined as: where j . j is a counting measure ( Van Rijsbergen, 1979 ).
 Recall can be defined as:
Typically Precision and Recall measurements are calculated at a particular depth of a systems document ranking (measurement depth d ). This is the overall rank position from which systems are normally compared.
Recall is calculated at rank d rather than for all documents in the collection. Precision is taken at various ranks up until d . Using a fixed measurement depth is motivated by both the problem of incompleteness, and also to allow for averaging over a set of topics. As previously discussed the measurement depth often exceeds the pool depth to provide better discrimination between systems.

Due to incompleteness the absolute system performances are not compared over a set of topics. Instead the relative system performances are compared. If the same inconsistencies such as completeness are common for all systems, where no bias exists favouring one system over another, then the relative performances can be compared with a high degree of confidence ( Salton, 1992 ). Due to the inconsistencies affecting all systems equally, the relative ranking of systems will remain consistent ( Voorhees, 2000 ). Therefore it is common prac-tice to use the relative performance for comparing systems, displaying the error bars over the set of topics, and where possible it is recommended that some form of significance test is applied i.e. Paired sample T -test, Wil-coxon signed ranked test or one-way ANOVA ( Hull, 1993; Sanderson &amp; Zobel, 2005; Savoy, 1997; Zobel, 1998 ).

For significance testing it is often desirable to provide a single point estimate of the performance of a sys-tem. Often precision at a predefined rank d is used ( P @ d ), although the most popular metric adopted is Mean Average Precision (MAP). Average Precision (AP) is defined as: where D k is a sequence of documents ranked by a system, w and d is the predefined measurement depth. AP measures both the Precision over the ranked list but also includes a Recall aspect in the measurement by taking Precision at the rank of each relevant document found.
Therefore a system is rewarded both for finding relevant documents, and also ranking these documents to-wards the top of the ranked list. Over a set of topics, the mean of AP is often taken hence the name Mean
Average Precision. 2.1. The binary preference measure (bpref)
In the previous section, we highlighted that the metrics were defined under the assumption that unassessed documents are considered not relevant. Recently, this assumption was re-addressed by Buckley and Voorhees (2004) , who proposed a new measure called binary preference ( bpref ). Due to incompleteness, the new bpref measure was designed to estimate performance on the assessed documents in a system ranked list only. bpref is the mean number of times R assessed non-relevant documents rank above the R relevant documents, where R is the total number of relevant documents. All unassessed documents not belonging to the judgement pool are ignored, which is intended to limit any potential uncertainty that is a result of incompleteness. Consequently, the status of unassessed documents was reconsidered ( Fig. 2 illustrates this change). Under this new assump-tion, we now have a new set of documents A that represent the assessed set of documents.

Using the updated contingency table (see Table 2 ), bpref can be formally defined as where D k is a set of documents, w k is the absolute rank position of a document in D of preference pairs of assessed relevant and non-relevant documents i.e. w the difference in ranks of each pair.
 We can also redefine measures such as Precision, Recall and AP under this new assumption: 2.2. Which measures to use?
The advent of bpref has lead to a fundamentally different assumption with respect to the status of unas-sessed documents. Typical Recall and Precision based metrics address the problem of incomplete assessments differently to that of bpref . The former suite of metrics assume unassessed documents are not relevant while bpref removes the unassessed documents from the performance estimation process. In other words unassessed documents are ignored. Implicitly bpref assumes that unassessed documents will follow a similar ranking of non-relevant and relevant documents to what has been observed in the assessed documents.

This strategy of bpref  X  to base the performance only on what has been assessed and extrapolating that value to the unassessed results  X  provides a more conservative estimate of a systems performance, particularly as less information is available to estimate the value. Invariably bpref will lower the difference in scores between the systems. Conversely, when using Precision and Recall metrics, the problem of incompleteness is ignored. As a result, the estimate of system performance is more liberal and therefore more prone to increased levels of incompleteness and hence stability ( Buckley &amp; Voorhees, 2004 ).

Given this limitation we still propose to utilise existing measures instead of bpref for this study. This choice is influenced by a number of factors such as (i) bpref strongly correlates with measures such as MAP when lished performance measure such as MAP or P@10, and (iii) what aspect of retrieval bpref is measuring is not as intuitive as Recall and Precision.

The correlation between bpref and MAP weakens when collections have a  X  X  X ubstantial level X  X  of incom-pleteness ( Buckley &amp; Voorhees, 2004 ), with standard Precision and Recall metrics becoming unstable. In this study we are interested in detecting when such situations exist particularly when using standard test collec-tions. For example, when the conditions between two systems are not even for a specific collection, thereby using a standard performance metric would not be suitable during system comparison. Instead of removing information to provide a more stable estimate, as in the case of bpref , we want to detect those scenarios informing the researcher when an alternative means is required such as identifying new collections, using shal-lower measurement depths, generating new relevance assessments, or utilising bpref . 3. Capturing the (un)certainty of system performance
We now propose a methodology to address the epistemic uncertainty associated with system comparisons under incompleteness. Instead of compensating for or ignoring potential uncertainty during system compar-isons, due to incompleteness, we believe that the proportion of unassessed documents should be captured and reported. Reporting this information can be a useful source of information to help quantify the confidence, accuracy and/or reliability of system performance. By capturing such information, we can determine whether two systems are compared under similar conditions, and flag those cases when one system may have an uneven advantage over another due to the a particular condition found in a test collection.

We hypothesise that the (un)certainty associated with estimating a measurement of a systems performance at a depth d is proportional to the number of documents that have been assessed at that depth. Conversely, the uncertainty is proportional to the number of documents that have not been assessed. The more assessed doc-uments contained in a ranked list, the more confident we are in using the estimate of a systems performance at the corresponding depth. For example, when comparing the performance of two systems, if all documents have been assessed in the ranked list of both systems then we have the ideal situation of completeness i.e. the performance estimates for both systems were made with full information. If the ranked lists of both sys-tems are incomplete, but contain similar proportions of assessed documents, then confidence in the relative comparison of these two systems would also be high. However, if we are measuring performance where one system has a substantially larger proportion of assessed documents than another, then the performance estimate of one system is based on limited information relative to the other. It is these cases that we wish to detect, where the conditions for both systems are not even, thus there is a higher degree of uncertainty in the comparison. 3.1. Measure of assessment We propose to capture uncertainty by calculating the proportion of assessed documents in a ranked list. First, we now introduce two measures using the same notation defined in Section 2 . Assessment precision
A is defined as the proportion of assessed documents in a ranked list: The Recall of Assessment is the proportion of assessed documents retrieved overall: where j X \ A j is the number of documents in the set defined by the intersection of X and A ,and j X j is the number of documents in X . Assessment Precision relates to the confidence we place, or the certainty of a per-formance estimate, given a ranked result list. Note that uncertainty associated with the estimate is the com-plement, 1 A p . We now refer to uncertainty and certainty through this measure, where a high
Assessment Precision value relates to high certainty and low uncertainty. This measure is exactly the definition for standard Precision except with respect to assessed as opposed to relevant documents. every Precision and Recall measure there is a corresponding Assessment measure. It should also be noted that the Assessment Precision metrics are functionally related to the corresponding
Precision metrics. This relationship is because A is the union of the set of assessed relevant documents and assessed non-relevant documents. Therefore a system retrieving more assessed documents is likely to have a higher Precision, because assessed documents are more likely to be relevant. Also, when systems have low lev-els of A p there is increased uncertainty in the Precision score, and any subsequent comparison, because of the high proportion of unassessed documents. It is important to consider this context during the evaluation of systems. Assessment Precision provides this context explicitly by capturing the proportion of assessed docu-ments used to estimate the retrieval performance. In this paper, however, we concentrate on applying A fairly compare systems, and leave these other issues regarding A 3.2. System evaluation decision matrix
We now illustrate how Assessment Precision can be integrated into the current evaluation protocol. We motivate the introduction of the System Evaluation Decision matrix in the form of an example system com-parison. We wish to test the performance P (), which denotes the Precision at a given measurement depth d (i.e.
P@10, MAP, etc.), of two systems s 1 and s 2 over a test collection with incomplete relevance assessments. We have the following research hypothesis: H 0 : P ( s 1 )= P ( s 2 ) H 1 : P ( s 1 ) 6  X  P ( s 2 )
To determine the level of confidence we can place on this test, we test the supplementary hypothesis using a corresponding Assessment Precision metric A(), which denotes the A d (i.e. A p @10, A p @1000, etc.): H 0 : A ( s 1 )= A ( s 2 ) H 1 : A ( s 1 ) 6  X  A ( s 2 )
This forms a contingency table of four possible outcomes of interest displayed in Fig. 3 . Significance is denoted as either no difference (==) or the significant differences ( , ) i.e. s s . We assume that statistical significance is determined using an appropriate test such as Wilcoxon sign rank test, paired T -test or ANOVA ( Sanderson &amp; Zobel, 2005 ).

For Case 1, the null hypothesis that P ( s 1 )== P ( s 2 ) and A ( s  X  X  X trong X  X  case because the level of assessment for both s tion used to estimate performance was comparable.

For Case 2, the null hypothesis that P ( s 1 )== P ( s 2 ) cannot be rejected as well, however, the proportion of information used to estimate the performance of both systems was not comparable. In other words, the result list of one system was comprised of a significantly larger proportion of assessed documents than the other, caus-ing a degree of uncertainty in this comparison. It is unknown from this test whether, under comparable con-
For Case 3, also a  X  X  X trong X  X  case, the null hypothesis that P ( s degree of confidence in this outcome as we have either a scenario where both systems share similar proportions of assessed documents, or in special scenarios the system with significantly higher performance has signifi-cantly fewer documents assessed than the other system. In other words, even with further information about this system it could not match (or better) the opposing system.

Finally, for Case 4, another  X  X  X eak X  X  case, the null hypothesis that P ( s cannot place a high degree of confidence in this outcome, as the system with significantly higher performance also reported a significantly larger proportion of assessed documents. This does not indicate that the system with a smaller proportion of assessed documents would share similar performance under equal conditions, but instead flags a potential problem with this comparison.
 Fig. 4 displays an example of a pair-wise system comparison from each case using runs from an official TREC track (Robust 2005). The left-hand column is a plot of system performance (i.e. MAP) across a range of measurement depths. The right-hand column is a plot of the corresponding Assessment precision metric for each run. Of the weaker outcomes Case 2 is particularly interesting as both systems have similar performance, but this performance is based on different proportions of assessed documents. What is interesting is that the system with significantly less assessed documents could potentially be retrieving a wider diversity of docu-ments, with respect to the pool, and some of these documents may be relevant ( Zobel, 1998 ). A subsequent research question would be to investigate why the systems perform as well as each other. As both systems have equal system performance but unequal levels of assessment, this system may potentially improve performance when compared under even conditions. Further investigation may provide stronger supporting evidence.
At this stage a number of steps could be taken. If the goal of the comparison is precision orientated then system comparison could be made at a shallower measurement depth to ensure the likelihood of parity. By doing so we are assuming that at shallower depths systems will have relatively equal proportions of documents assessed. If both systems have contributed to the pooling process then this assumption would hold up until pooling depth has been reached, however, if a system has not contributed to the pool this may not be the case.
The previous step may lead to the creation of test collections with an emphasis on shallow measurement depth ( Sanderson &amp; Zobel, 2005 ). If the goal is to compare a minimal number of systems using shallow measure-ments, where re-usability of the test collection is not important, such a strategy could also be adopted by research groups. For example, provided with enough resources, new relevance assessments could be generated for the collection adopting an efficient strategy such as that outlined by Carterette et al. (2006) . Alternatively, comparisons could be made across different test collections where conditions remain even. This step assumes such collection(s) exist, although collections can be evaluated for such properties using the suite of Assessment measures. Finally, this reinforces the need when building test collections to include novel systems in the pool-ing process. 4. Experimental analysis
To demonstrate the application of the Assessment Precision measure within the evaluation process we con-ducted an empirical analysis to evaluate both its utility, and to provide further justification for its introduc-tion. Our first objective was to examine the officially submitted runs to TREC over a number of collections, spanning a range of years. 15 By using the official runs we can investigate the level of uncertainty during performance comparisons of runs included in the system pool across these collections. Our second objective was to evaluate the implications of measurement depth with respect to the level of assessment between systems at various points. Using the assessment metrics, we were investigating what effect using a measurement depth deeper than the pool depth may have on system comparisons. This is related to the argu-ment that relative system performance can be compared if the conditions remain even for both systems. We then focus our attention on runs from particular collections, such as the Robust-HARD 2005, been identified as potentially problematic to use ( Buckley et al., 2006 ). The aim is to better understand the problems cited with this collection. Finally, we investigate the utility of adopting the assessment metrics for explaining other phenomena in Information Retrieval such as automatic relevance assessments ( Soboroff et al., 2001 ). 4.1. Collections
We used the official submitted runs to TREC covering a number of collections. The collections vary in size and year including the ad hoc retrieval collections TREC-3 and TREC-4 that were originally examined by Zobel (1998) . We also use collections that have also been investigated in other related studies such as
TREC-6, TREC-8 and Robust-HARD 2005 ( Buckley &amp; Voorhees, 2004; Buckley et al., 2006; Soboroff et al., 2001; Voorhees, 2001 ). To complement these collections, we also examined the Robust 2003 collection that contained 250 topics, and the large Terabyte 2004 collection. For each collection we consider all official runs submitted to TREC, including the runs that formed the system pools as well as the remaining official runs submitted by each group. These remaining runs were not included in the system pools but were however eval-uated. As a result, a number of these runs will be incomplete even below the pooling depth. 4.2. Decision matrix experiment
For each collection we first analysed each possible pair-wise system comparison of the officially submitted runs using the decision matrix (see Tables 3 and 4 ). To test for significance across all systems we used the
ANOVA test. If significant differences in terms of performance and assessment across the systems of a collec-tion were found, we performed a followup Bonferroni multiple comparisons to identify which systems differed significantly both in terms of performance and assessment. We repeated this experiment across numerous Per-formance and Assessment Precision metrics, spanning a range of measurement depths; including the Perfor-mance metrics P@ d and MAP@ d (Mean Average Precision at rank d ), as well as the corresponding Assessment Precision metric at the same depth A p @ d .

The reason for examining two types of performance metric is that P@ d metrics are commonly used at shal-low measurement depths, however, using metrics at larger depths can be misleading. Precision is essentially the proportion of relevant documents in the ranked list, it does not account for the placement of relevant docu-ments in this list. For example, two systems could share a similar score of 0.5 for P@100. The first system ranks 50 relevant documents at positions 1 X 50. The other system, however, ranks the 50 relevant documents at 51 X 100. Hence, there is an implicit assumption that the searcher will examine all documents down to the measurement depth, although how realistic this assumption is at large measurement depths is questionable.
Also, as the measurement depth becomes larger than the total number of known relevant documents for a topic, then the maximum possible precision score decreases.

Therefore, MAP is more informative at a deeper measurement depth as it accounts for the accuracy of where the corresponding relevant documents are positioned in the result list, providing more discrimination between systems, as systems are rewarded for ranking relevant documents highly. MAP also has another desir-able property related to recall, where it accounts for the number of discovered relevant documents not included in a runs ranked list in the estimation. Therefore, we provide a summary of these comparisons at measurement depth d = {10, 30, 100, 500, 1000}.
 For each metric, we counted the number of comparisons that fell into each outcome i.e. Cases 1 X 4 (see
Fig. 3 for an outline of each case). Tables 3 and 4 present the proportion of overall system comparisons that fall into each case for Precision and MAP, respectively. Rows indicate different test collections while columns represent different measures, increasing by measurement depth. Each entry represents the proportion of sys-tem comparisons that fall into that case e.g. for the TREC 3@10 entry, 69% of pair-wise system comparisons fall in Case 1, 7% in Case 2, 17% in Case 3 and 7% in Case 4, where there were 40 runs included in as part of the pair-wise comparisons overall. 4.2.1. Results
The first thing we investigated was whether there was a common trend in the proportion of significant pair-wise differences, in terms of system performance, as the measurement depth increased across the various test collections. The reasoning behind the practice of increasing measurement depth is that the discrimination between systems will be greater. The intuition being that good systems will continue to retrieve relevant doc-uments beyond the pooling depth that will have been discovered back by other runs included in the system pool.

A noticeable trend from the results of using P @ d as a metric in Table 3 was the proportion of significant differences which fell into the  X  X  X trong X  X  Case 3 either decreased or remained approximately constant as the measurement depth increased. For example, across the TREC-3 collection, the proportion of significant dif-ferences dropped from 0.17 using P@10 to 0.11 when using P@30. At P@100, 0.05 comparisons feel into this case. When using MAP as a metric, Table 4 , the proportion of comparisons that fell into Case 3 remained approximately constant, between 0.04 and 0.07. This trend was consistent across other collections such as
TREC-8, Robust 03, Robust 05, and HARD 05. for the Terabyte 04 track, the proportion of comparisons that fell into Case 3 increased as the measurement depth tended towards the pooling depth at approximately k = 100, however, after the pooling depth the proportion of significant differences belonging to this case decreased.

However, from these results it would appear that discrimination between the set of systems at best remains constant, and at worse lessens, as the measurement depth increases. From some collections such as Trec-3, 6, 8 and the Terabyte 2004 collections this becomes more stated as measurement depth is increased beyond pool depth.

A similar trend is also followed for system comparisons where the null hypothesis that both systems have equal performance cannot be rejected. As measurement depth increases, the proportion of  X  X  X trong X  X  cases decreases, resulting in a larger proportion of cases where one system has a significantly larger number of judged documents than another.

We then examined the proportion of significant differences between systems that fall into either the  X  X  X trong X  X  or  X  X  X eak X  X  case. Overall, a common trend across collections was that, as measurement depth increased, the proportion of  X  X  X trong X  X  comparisons decreased while the proportion of  X  X  X eak X  X  cases increased.
To illustrate, consider first P@10 for the TREC-3 collection in Table 3 . We find a smaller proportion of com-parisons falling into the  X  X trong X  X  case in contrast to P@30 (0.17 X 0.11). Conversely there is an increase in  X  X  X eak X  X  cases from 0.07 to 0.087. This trend remains as we continue increasing measurement depth towards
P@1000. As the measurement depth increased beyond the pooling depth for many collections, a common pat-tern was for a comparison to change from a  X  X  X trong X  X  to a  X  X  X eak X  X  case. Increasing measurement depth results in a higher degree of uncertainty when comparing systems. 4.2.2. Why the trends?
We then investigated further the effect of increasing the measurement depth had on the proportion of sig-nificant differences falling into Cases 3 and 4. To attempt to answer why this trend of decreasing significance between runs and increased uncertainty occurs, we first examined performance across increasing measurement depth across all collections. We present an summarised illustration of this experiment in Figs. 5 X 10 , which dis-play system performance of all official runs submitted to the TREC-3, Robust 2003 and Robust 2005 collec-tions. We first ranked all runs from high to low performance with respect to MAP@10. To easily identify the change in a system performance across various metrics, this rank remains constant across the remaining plots.
We also plotted system scores using the Assessment metrics A mance and standard error across the topic set, adjusted for multiple comparisons, for measurement depth d = {10, 30, 100, 500, 1000}. Significance can be found where there are two disjoint intervals.
For the TREC 3 collection (see Figs. 5 and 6 ), as measurement depth is increased the difference between system runs becomes less emphasised for the majority of systems. In particular those runs ranked towards the middle begin to report similar performance, converging on each other. The plot of MAP@1000 illustrates that there are approximately three groups, two small groups at the extreme and a large section of average per-forming systems. We tend to find discrimination between systems from both these small groups. It is notice-able that the rank order of runs from MAP@10 to MAP@1000 deviates slightly, with a small number of swaps, which is in accordance to the results found in Voorhees (2000) .

When examining the range of A () assessment metrics, plotted on the right-hand column, we find that the coverage of assessed documents for each systems begins to vary as measurement depth increases. Initially, as measurement depth is above the pooling depth, the majority of system runs have similar levels of assess-ment. As the depth increases, the variance in assessment also increases, which would explain the trends we found in Tables 3 and 4 . As the measurement depth goes beyond the pool depth, the number of significant differences between systems increases, resulting in higher discrimination (between runs). This would explain why there is a large increase in system comparison in the TREC-3 collection that swap from  X  X  X trong X  X  to  X  X  X eak X  X  while measurement depth increases.

In general, runs with relatively poor performance also have a lower level of assessed documents. Systems with middle ranking performance tend to have the largest coverage of assessed documents. The best two per-forming systems retrieving an average proportion of assessed documents. This would indicate that the best systems retrieve a larger proportion of assessed and relevant documents ( R \ X \ A ) compared to those with systems with equal levels of assessed documents but poorer performance, who tend to have a larger proportion of assessed non-relevant documents ( : R \ X \ A ).

For the Robust 2003 collection (see Figs. 7 and 8 ), only a small proportion of significant differences between runs was found overall. Most of these comparisons were the result of a small group of systems with poor per-formance across all metrics. However, there is a pack of systems whose performance deteriorates as measure-ment depth increases. Examining the Assessment metrics explains why this occurs. This small group of runs only returned a partial set of documents ranging from 10 to 50 documents, instead of the 1000 documents allowed per topic in TREC. Therefore as the measurement depth increased the performance of these systems becomes frozen. As a consequence, the number of systems that significantly improved over this group of runs increased as the measurement depth increased. This analysis explains why for the Robust 2003 collection, an increase in significant differences was observed. It is important to note, however, that this increase in signif-icant differences between the  X  X  X rozen X  X  systems and other runs fell into the category of  X  X  X eak X  X  comparison (i.e. Case 4).

We also highlight the results from the Robust 2005 collection, which has reported problems with  X  X  X itle bias X  X  ( Figs. 9 and 10 ). Again, as the measurement depth increases the discrimination of systems at the extremes remain but at the expense of discrimination between systems towards the middle. This is a similar trend to the other collections, where performance between systems levels out as measurement depth increases with the majority of significant differences occurring between these systems at the extreme ends of perfor-mance. However, for this collection it is important to indicate that the system with the best performance at depths 10 and 30 may have underestimated performance at greater depths (i.e. run 1). The reasoning behind this is that the level of assessment at A p @100 onwards for this system is one of the poorest in comparison to the other systems. Other examples include System 9, which becomes the best performing system as measure-ment depth increases, however, the corresponding Assessment score is also higher than average for this system  X  significantly more so than Systems 1 X 5.

Overall, these trends appeared to be consistent across the remaining collections. 4.2.3. What is causing the swaps in the grid?
We then examined in closer detail what conditions would result in a swap from a  X  X  X trong X  X  to  X  X  X eak X  X  com-parison and vice versa when increasing measurement depth, focusing in particular on pair-wise comparisons of individual runs. As a case study we present a comparison of runs from the potentially problematic Robust 2005 collection. In Fig. 11 , we illustrate two examples of system comparisons where there is a swap from  X  X  X trong X  X  to  X  X  X eak X  X  comparisons as measurement depth increases. In both plots we display both the
MAP@ (left) and A p @ (right) metrics at various measurement depths for both systems. Error bars are dis-played to show variation across the set of topics and significance between systems. Runs are referred to by their official TREC run-tag.

An analysis by Buckley et al. (2006) highlighted a potential bias towards documents containing the topic title keywords in this collection, although relevant documents exist that do not contain these words. For example, the SAB05ROR1 run ( Buckley, 2005 ), using existing relevance assessments from another collection to generate an optimal query, reported a large change in performance when removing unique documents pooled by the system from the relevance assessments. This was a result of the ber of unique relevant documents during the pooling process.
 In Fig. 11 (top), we compare this system against another run, 2005 ), which used an external corpora of TREC newswire documents to expand an original query based on the title and description fields of the topic. Up to a measurement depth of 100, MAP score than PIRCRB05TD3 . Beyond this depth, there was a swap in performance where a higher MAP. Examining the corresponding Assessment score, we find comparable levels of assessment until a depth of a 100, where the pooling depth was set at 55 for this collection. Beyond this depth, significantly larger proportion of documents assessed than comparison at depths 10 and 30, that then moves towards a Case 2 comparison after the pooling depth (see Fig. 3 ).

This reflects the findings by Buckley et al. (2006) , where performance is underestimated after a larger mea-surement depth is used. From the study of Zobel (1998) , who investigated the rate of discovering new relevant documents beyond the pool depth, it is uncertain if both systems shared similar levels of assessed documents that performance would have swapped or not. However, as raised by Buckley et al. (2006) ,if not included as part of the pooling process, it would be highly likely that its performance would have been underestimated even more so.
 In Fig. 11 (bottom), we illustrate another similar example comparison from this collection. The run
INDRI05DMMT used a combination of a term dependence model and a mixture of relevance models trained using a superset of TREC newswire articles ( Metzler, Diaz, Strohman, &amp; Croft, 2005 ), while original title query with using an online semantic lexicon Wordnet ( Liu &amp; Yu, 2005 ). The performance of
UIC0501 was on average higher than INDRI05DMMT until a depth of 100 was reached, then again there was a swap between the two runs. From the plots of assessment, this swap in performance also coincided with a significant drop in the proportion of assessed documents for the UIC0501 4.2.4. Automatic relevance assessments We then examined another use of assessment to explain a phenomena reported by Soboroff et al. (2001) .
The motivation of this study was to attempt to produce automatically generated relevance assessments based on the pooling process. This would be advantageous for producing inexpensive assessments for testing systems over very large corpora such as the Web. By randomly sampling the document pool, automatic relevance assessments were generated. Documents in this sample (of the pool) were considered relevant. Using these pseudo relevance assessments it was then possible to accurately rank a large proportion of the systems. This ranking, using the automatically generated pseudo relevance judgements, correlated strongly to that of the actual judgements defined by human assessors. The only runs that were not ranked accurately tended to be found at the extremes (i.e. very good or poor systems).

In a follow up study by Aslam and Savell (2003) it was identified that the problem of ranking those systems towards the extremes could be a problem of  X  X  X opularity X  X . In other words, the pseudo relevance assessments were ranking how similar systems were to each other, where the better systems were retrieving slightly different documents to this middle section, hence the difficulty in accurately ranking them. Popularity was a measure of the set of retrieved documents from each system i.e. comparing the ranked lists of the set of systems S ={ s 1 , ... , s S } using the corresponding similarity of the retrieved set of documents from each system
We believe this result can be explained further by analysing the assessment level of each run. In other words, the latent variable that is being used to rank the  X  X  X opular X  X  systems accurately is the set of assessed documents in X for a system i.e. for system s 1 the proportion of assessed documents in the ranked list X
To test this assumption we ranked all runs for the TREC-3, TREC-6, TREC-7 and TREC-8 collections with respect to MAP performance in Fig. 12 , from high MAP to low. For each collection, we also plotted each systems corresponding A p @1000 value.

From the plots, we found that it was those systems at the extremes that do not contribute to the assessment pools in comparison to the middle section of runs. Also, the better systems appear to be good at discovering relevant assessed documents ( R \ X \ A ) while minimising the proportion of non-relevant assessed docu-ments in the ranked list ( : R \ X \ A ). While the poorer systems retrieved more non-relevant assessed and unassessed documents ( X \: A ).

We also calculated the Kendal tau correlation in system rankings between MAP and A@1000. This mea-sures the concordance in the ranking of all systems in terms of Performance and Assessment metrics. A high correlation close to one would indicate high agreement in the system rankings, while a score of zero would indicate no relationship. The reported correlation for TREC-3 was 0.394, TREC-6 was 0.402, TREC-7 was 0.402, and TREC-8 was 0.462, corresponding to those of the ranked lists of pseudo relevance judgements found by Soboroff et al. (2001) , and the ranking by popularity of Aslam and Savell (2003) .

Therefore, those systems that were ranked accurately by the pseudo relevance assessments contribute most to the pool of assessed documents. In other words they are  X  X  X opular X  X  due to both the theoretical similarity of the systems (i.e. generic systems) but also they are popular due to the contribution to the set of assessed doc-uments A . Hence, the reasoning why those systems were not accurately ranked was due to the lower contri-bution of assessed documents A . In effect, the pseudo judgements were generated using an IR system (the random sample of the pool A ). Therefore, those systems with the highest coverage of assessed documents (the average or  X  X  X opular X  X  systems) correlated highly to this new IR system, while the systems at the extremes did not correlate as strongly, and thus, were not ranked as accurately. 5. Discussion
By proposing the System Evaluation Decision matrix we are essentially asking the question  X  are the con-ditions for both systems even? This relates to the initial defence of the current IR evaluation protocol, where the relative performances of systems can be fairly compared if the experimental conditions remain even for all ( Salton, 1992 ). Using the level of assessment metrics, A (), we can determine whether there is a significant dif-ference in the proportion of documents assessed between two competing systems. In other words, the level of information used to estimate performance for both systems. If there is a significant difference (between sys-tems), then this introduces some degree of epistemic uncertainty into the comparison. With such a scenario, the performance of one system may be underestimated compared to the other. At this stage, it would be beneficial to perform further evaluation of the system at different measurements depths, on different collec-tions, or even generating a new set topics and/or relevance assessments. This is the advantage of using the Sys-tem Evaluation Decision matrix, where such scenarios can be detected, informing the researcher of potential uncertainty when comparing systems using incomplete assessments so that they can then act accordingly.
From the results of this study, it would appear that the use of a measurement depth larger than the pooling depth is questionable, and re-iterates the concerns raised by Zobel (1998) . As the measurement depth increases beyond the pooling depth, uncertainty across many system comparisons also increases. Interestingly, the dis-crimination between systems weakens. This supports the work by Sanderson and Zobel (2005) , where a mea-sure such as P@10 can be used to discriminate between systems, placing more emphasis on a larger set of test topics.

A potential explanation why there is a decrease in discrimination at lower measurement depth is because performance estimates vary more across topics, there is also less information to estimate performance (i.e.
Assessment), resulting in wider confidence intervals. Also the majority of systems, what Aslam and Savell (2003) refer to as the  X  X  X opular X  X  systems, appear to discover a similar proportion of relevant documents once the measurement depth increases. The best systems, even though the relative performance is high, and they rank highly compared to other systems, performance is still underestimated because these systems return more unique documents than the  X  X  X opular X  X  set of systems. Potentially, these better systems discover more relevant unassessed documents ( R \ X \: A ) that are not accounted for because of the necessity of fixing the pool depth. If this is the case, then it would warrant further investigation into alternative pooling strategies ( Cor-mack et al., 1998; Zobel, 1998; Robertson &amp; Soboroff, 2003; Sanderson &amp; Joho, 2004 ).

Prior to the pooled documents being judged for relevance by assessors, the suite of assessment metrics could be deployed for assessing any irregularities such as the title bias found in the Robust-Hard 2005 collection.
For example, official runs could be evaluated with respect to the level of Assessment at various ranks. Any noticeable drop off after the pooling depth could be identified and correct accordingly before documents were finally assessed, ensuring parity in conditions across all systems. It could also be envisaged that different the-oretical and cognitive models, not included in the pool, be supplemented to ensure the re-usability of the col-diverse pool of systems should reflect the different algorithmic and cognitive interpretations of the documents stored within a collection.

Finally, the relative rankings of systems will remain stable across measurement depths and varying levels of incompleteness in general. However, we believe it is the cases where systems jump in ranking because of the effects of incompleteness, in particular relating back to Case 2 in the decision matrix, where interesting cases are. It is those systems that are novel or diverse that require more consideration during Laboratory IR evaluation, because it is these systems that retrieve the larger number of unique, and potentially relevant documents. For instance, a recent study has shown the utility of defining ranking algorithms designed to provide more topical variation in a results list ( Chen &amp; Karger, 2006 ). This may result in the performance of such algorithms being underestimated. This is one area where graded relevance assessments and the cor-2002 ), when used alongside the System Evaluation Decision matrix and the corresponding Assessment metrics. 6. Conclusions
In this paper we argued that uncertainty when using incomplete relevance assessments should be identified during the evaluation process. Consequently, we proposed a new set of metrics based on of the level of assess-ment that provide an indication of uncertainty during system comparisons. If the level of assessment between systems is similar, we believe that a fair comparison can be made, otherwise uncertainty has been introduced into the evaluation process. By using the System Evaluation Decision matrix we can make stronger claims of significance (or not), and guide subsequent research to decide when further testing is required. Finally, as part of an empirical study that adopted this methodology, we provided supporting evidence which questions the practice of using a measurement depth that exceed the pooling depth. Future work will investigate this impli-cation in greater detail, along with the relationship between Assessment and Precision.
 Acknowledgements
We thank Fabio Crestani, Kalervo Ja  X  rvelin, David Harper, and the anonymous reviewers for their invalu-able comments.
 References
