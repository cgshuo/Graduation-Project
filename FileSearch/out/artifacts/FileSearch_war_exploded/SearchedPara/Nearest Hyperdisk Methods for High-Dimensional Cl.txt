 Eskisehir Osmangazi University, Eskisehir, Turkey Bill Triggs B ILL .T RIGGS @ IMAG . FR Laboratoire Jean Kuntzmann, Grenoble, France Rowan University, Glassboro, NJ USA Nearest neighbours (NN)  X  assigning the query to the class with the nearest training sample(s) under some suitable dis-tance metric  X  is one of the simplest methods for multi-class classification. Asymptotically it makes at most twice as many errors as the optimal Bayes rule classifier, but this result assumes dense sampling which requires train-ing sets that are exponentially large in the dimensionality of the underlying feature space class distributions. In high-dimensional problems such as text, gene or visual object classification, tractable training sets are necessarily much smaller than this, and the performance of NN can often be poor. The main problem is the sparse and irregular distribu-tion of the training samples, which often leaves  X  X oles X  in the input space  X  regions that have few or no nearby train-ing samples from the relevant class. Equivalently, local density estimates in high dimensions are intrinsically noisy because any region with a radius significantly smaller than that of the class has such a small volume relative to that of the class that it typically contains few or no samples. These effects make the inter-class decision boundaries of high di-mensional NN and local kernel based methods erratic, thus leading to classification errors.
 One way to circumvent this problem is to approxi-mate each class with a point set that  X  X ills in the holes X  between the examples. In particular, any convex set containing the examples has this prop-erty. Several approximations of this kind have already been studied including the affine hulls, convex hulls, bounding hyperspheres and bounding hyperellipsoids of the examples (Gulmezoglu et al., 2001, Laaksonen, 1997, Nalbantov et al., 2007, Vincent &amp; Bengio, 2001). Despite the simplicity of their geometry, such approximations are useful in high dimensions because in any case fine local de-tails can not be resolved with practical numbers of samples. Queries are classified to the class whose convex approxi-mation is closest to the query point  X  a convex nearest-point problem that can be solved reasonably efficiently with stan-dard methods. This is equivalent to NN in which additional points are fantasized to fill in the set of each class. Affine hulls ( i.e . spanning linear subspaces that have been shifted to pass through the centroid of the class) were first used for global classifiers of isolated words and hand-written digits in (Gulmezoglu et al., 2001, Laaksonen, 1997), giving good classification performance. Similarly, (Nalbantov et al., 2007) used convex hulls for global classifiers on some of the UCI and SlatLog prob-lems, comparing these to Support Vector Machines (SVMs) both theoretically and empirically. Such global convex approximations may fail to capture the decision bound-aries of classes with nonlinear boundaries and one can also build more local approximations, or even build a separate approximation for each query sample based on convex approximations of its k nearest neighbours from each class. Again the query is classified to the (lo-cally) nearest hull. Although this is not immune to the hole problem, (Vincent &amp; Bengio, 2001) reported signifi-cant improvements over traditional NN for affine and con-vex hull methods of this kind in handwritten digit clas-sification. Another way to handle complex boundaries is via nonlinear mapping to a high-dimensional feature space ( e.g . via a kernel) followed by a global convex set approxi-mation of the kind described below.
 Besides classification, approximations based on affine or convex hulls have also been used for dimensionality reduc-tion. Mixtures of Principal Component Analyzers can be used to approximate nonlinear data manifolds under local linearity assumptions (Hinton et al., 1997). Locally Lin-ear Embedding (Roweis &amp; Saul, 2000) approximates the nonlinear structure of high-dimensional data by exploit-ing local affine/convex reconstructions. (Verbeek, 2006) combined several locally valid linear manifolds to obtain a global nonlinear mapping between the high-dimensional sample space and a low-dimensional manifold. In (Cevikalp et al., 2008), we proposed a margin based dis-criminative dimensionality reduction method based on con-vex models of classes.
 The current paper presents a new convex approximation based classifier that models each class with its bounding hyperdisk  X  the intersection of the affine hull and the min-imal bounding hypersphere of its training examples. Hy-perdisks are attractive primitives because they maintain the stability of the affine hull and hypersphere methods while providing better localization of the training samples and hence potentially better discrimination. Convex hull ap-proximations tend to be unrealistically tight (for practical training set sizes, classes typically extend considerably be-yond the convex hull of the training samples) while affine hull and hypersphere ones tend to be too loose in comple-mentary senses (one too  X  X road X , the other too  X  X eep X ). The hyperdisk approach to some extent captures the best aspects of each method. It can be applied both globally and locally and it is simple enough to be expressible in terms of dot products and hence to allow kernelization.
 The paper is organized as follows. In section 2 we recall the affine and convex hull based methods. Section 3 introduces the hyperdisk method. Section 4 describes our experiments and data sets. Finally, section 5 presents conclusions and future directions. 2.1. Nearest Affine Hull (NAH) Classification Let the training samples be x ci  X  IR d , where c = 1 ,...,C indexes the C classes and i = 1 ,...,N c indexes the N c samples of class c . We suppose that the affine hull of the samples from each class is a proper subset of IR d of dimen-sion less than d (which certainly holds when N c d ). The affine hull is the affine span of the training samples, i.e . the smallest affine subspace containing them The affine hull gives a rather loose approximation to the class region because it does not constrain the position of the training points within the affine subspace. The distance from a query point x q to an affine hull H aff c is the norm of the displacement from x q to the closest point on the hull, which can be expressed as the orthogonal projection of x q normal to the subspace (see, e.g ., (Cevikalp et al., 2007) for derivations): d ( x q ,H aff c ) = k ( I  X  P c )( x q  X   X  c ) k = k P  X  Here: I is the identity matrix, P c is the orthogonal projec-tion onto the spanning subspace (the range of the covari-ance matrix) of the class-c training samples, and P I  X  P c is the orthogonal projection onto the null space of the covariance  X  i.e . the orthogonal complement of the spanning subspace, called the indifference subspace in (Gulmezoglu et al., 2001, Cevikalp et al., 2005).  X  c can be any reference point in H aff c  X  e.g . one of the samples x or their mean  X  and  X   X  c = P  X  c  X  c , the residual of  X  der the projection, encodes the orthogonal displacement of H c from the origin.
 As its name suggests, the NAH classifier assigns the query to the class whose affine hull is the closest: Equivalently, NAH chooses the class that provides the best (smallest k error k ) reconstruction of the query using an affine combination of training samples. The decision boundaries of NAH are piecewise quadratic. Numerically, point projections can be computed on the fly without ex-plicitly evaluating and storing the d  X  d projection matrices P c and P  X  c by using P c = Q c Q &gt; c where Q c is the U ma-trix of the thin SVD (or equivalently the Q matrix of the thin QR decomposition) of the matrix of centred class-c training examples [ x c 1  X   X  c ,..., x cN c  X   X  c ] . In practice the training data is often somewhat noisy. This can harm the classification performance owing to the inclu-sion of spurious  X  X oise X  dimensions in the affine hulls. To reduce this we suppress dimensions of the SVD (and hence of Q c ) that correspond to overly small singular values. For nonlinear classes that lie on smooth manifolds, NAH can also be applied locally by finding the k -nearest samples to the query from each class, building local affine hulls us-ing these nearest neighbors, and assigning the query to the class with the closest hull (Vincent &amp; Bengio, 2001). This can reproduce complex nonlinear decision boundaries. 2.2. Nearest Convex Hull (NCH) Classification The affine hull gives a rather loose approximation to the class region. Alternatively, we can take a maximally tight bound by approximating the class with the convex hull of its training samples. For this, we include non-negativity constraints  X  i  X  0 , i = 1 ,...,N c in (1) and replace all of the affine hull distance computations with convex hull ones. The distance from a query x q to the convex hull of class c is the norm of the displacement from x q to the closest point on the hull. This reduces to solving the following quadratic programming problem where X c is a matrix whose columns are the class-c train-ing samples. Given the optimal  X   X  ci coefficients, the dis-tance from x q to the convex hull of the class c is || x q X c  X   X  c || . This is repeated for each class and the query is assigned to the class with the closest convex hull. Finding the maximum margin between two classes is equivalent to finding the closest points on their convex hulls (Bennett &amp; Bredensteiner, 2000) so convex distances can also be computed by using a classical hard-margin SVM algorithm to find the margin (convex distance) sep-arating each class from the given query point.
 NAH and NCH are  X  X ne class X  methods in the sense that we do not explicitly calculate the decision boundaries dur-ing the training phase. Instead they remain implicit and the decisions are made on-line for each test sample. However both approaches can be viewed as large margin classifiers closely related to hard-margin linear SVM X  X . In particular, the piecewise linear/quadratic decision boundary of NCH contains the SVM boundary as one facet, and generalizes it to use distance to the convex hull rather than linear separa-tion as the decision criterion. In high-dimensional spaces, classes often extend well be-yond the convex hulls of their training samples. For ex-ample, any individual simplex spanned by points sampled from a high-dimensional hypersphere can include only a negligible fraction of the volume of the sphere even if the vertices themselves are well spaced and close to the sur-face of the sphere. Conversely, affine hulls often give a rather loose approximation to the class as they do not con-strain the positions of the training points within the affine subspace. This is problematic if the classes have similar or intersecting affine hulls but very different distributions of samples within their hulls. In such cases the classification performance will be poor if the affine projections of the queries onto the affine hulls are too far from training sam-ples ( e.g . as indicated by large values of the  X  i coefficients for the constructed affine projections). The  X  X oft margin X  approach to handling this is to allow negative weights in (4) but to penalize over-large values by including upper and lower bounds in the quadratic program. However this dete-riorates the run-time efficiency of NAH because the affine hull parameters of classes can no longer be computed in advance.
 Instead, we can keep both a simpler geometric interpre-tation and good run-time efficiency by approximating the class samples with their bounding hyperdisk, i.e . the in-tersection of their affine hull and their minimal bounding hypersphere. 3.1. Global Nearest Hyperdisk Method We will only describe the basic global Nearest Hyper-disk (NHD) classifier, but local application is also pos-sible in the same way as for NAH and NCH. NHD ap-proximates each class with the smallest bounding hyper-disk of its training samples  X  the set formed by inter-secting their affine hull and their smallest bounding hy-persphere. Such hyperdisks can be computed economi-cally and they support rapid nearest point computations. There are already a number of methods based on affine hulls or bounding hyperspheres  X  for example hyperspheres have been used for outlier detection (Tax &amp; Duin, 2004, Shawe-Taylor &amp; Cristianini, 2004) and binary classifica-tion (Wang et al., 2005)  X  but we are not aware of any pre-vious machine learning method based on hyperdisks. The bounding hypersphere of class c is characterized by its cen-ter s c and radius r c . These can be found by solving the following quadratic program or its dual Here  X  i are Lagrange multipliers and  X   X  [0 , 1] is a ceiling parameter that can be set to a finite value to eliminate over-distant points as outliers. Given the solution, the center of the hypersphere is s c = P N c i =1  X  i x ci and the radius is r = || x ci  X  s c || for any x ci with 0 &lt;  X  i &lt;  X  . To compute the distance from a query to the hyperdisk of a class, we find the affine projection of the query onto the affine hull by x aff q = P c ( x q  X   X  c ) +  X  c = P c x q If the projection lies outside the bounding hypersphere we move it along the line joining it to the center of the sphere until it touches the sphere. The distance from the query to the disk is the distance from it to the (possibly moved) projection  X  see fig. 2. Formally, the distance is d ( x q ,H disk c ) = q max( k x aff q  X  s c k X  r c , 0) 2 + k x 3.2. Kernelization of the Hyperdisk Method We now show that the hyperdisk method can be kernelized, allowing it to be used in implicit high dimensional feature spaces induced by Mercer kernels. This brings all of the usual advantages and disadvantages of kernelization, no-tably scope for a richer choice of distance functions and highly nonlinear decision boundaries that can aid data sep-arability in return for the need to work with an implicit model defined by a large set of training samples.
 The kernel trick can be used to map the data into an implicit feature space as in Kernel PCA (Sch  X  olkopf et al., 1998). Let  X  (  X  ) be the implicit feature space embedding and k ( x , y ) =  X  &gt; ( x )  X  ( y ) be the corresponding kernel func-tion. Suppose that we want to project a sample x onto the affine hull of a given set of samples { x i | i = 1 ,...,m } . Let  X  = [  X  ( x 1 ) ,...,  X  ( x m )] be their feature space em-bedding matrix, K =  X  &gt;  X  = [ k ( x i , x j )] be their m  X  m kernel matrix and k x =  X  &gt;  X  ( x ) = [ k ( x i , x )] be the m  X  1 kernel vector of x against the samples. The feature space mean of the samples is  X  = 1 m  X  1 m where 1 m is an m -vector of 1  X  X . The explicit approach detailed below (3) is sample features [  X  ( x 1 )  X   X  ,...,  X  ( x m )  X   X  ] =  X   X  , where  X  = I  X  1 m 1 m 1 &gt; m is an orthogonal projection in sample space that implements subtraction of the mean on  X  . Given this, the projection of x onto the affine hull of the mapped samples is then U U &gt; (  X  ( x )  X   X  ) +  X  , and the squared residual of this projection is k  X  ( x )  X   X  k and linear basis that we choose for computations within the affine hull so long as we do so consistently. In particular, if we choose the orthogonal basis given by U centred at  X   X  = ( I  X  UU &gt; )  X  , the projection of x onto the affine hull is represented simply by U &gt;  X  ( x ) .
 Noting that the D matrices of thin SVDs ( i.e . taking only the significantly non-zero singular values) are invertible, In the kernelized case we can not evaluate the SVD of  X   X  explicitly because this would require numerical com-putations in feature space, but we can work implicitly in of the centred kernel matrix e K = (  X   X  ) &gt; (  X   X  ) =  X  K  X  . Here, V is the same matrix as in the SVD of  X   X  and  X  = D 2 so that A =  X   X  1 / 2 V &gt;  X  .
 Putting all of these pieces together and noting that k  X  ( x ) k 2 = k ( x , x ) , we find that the squared residual error of the projection of x onto the affine hull of the examples is and the sample-space representative of the feature-space projection of x into the affine hull of the samples is sim-ply A k x . We can use the representation vectors A k x for any affine computation within the feature space affine hull, including calculations of hyperspheres and convex hulls, projections of new samples onto these, and within-hull dis-tance computations. To calculate the overall squared dis-tance from the example to the desired convex set within the hull, the squared residual error of the projection onto the hull (7) needs to be added to the squared within-hull dis-tance.
 In retrospect the obvious way to perform the above com-putations would be to use a separate feature subspace (  X  , K , k x , A , etc .) for each class, but in the experiments be-low we actually worked in a global feature subspace based on the combined training samples of all classes. This sub-space contains the affine hulls of all of the classes so the projections of test samples onto classes can be done in two stages, first projecting the sample onto the global affine hull, then projecting the result onto the class hull within the global one. The first projection is class-independent so it simply adds a sample-dependent constant residual to all of the sample-class distances. For decisions based on rela-tive sample-class distances, these constants can be ignored. As a result, it suffices to perform all computations with the global A k x vectors as though they were the original affine input points. In particular, the kernelized versions of NAH, NHD and NCH simply apply the corresponding lin-ear method to the A k x vectors of the global feature sub-space. This process is illustrated in fig. 3. It only provides relative distances, so k ( x , x ) is never needed. We compared the proposed hyperdisk method (NHD) to Nearest Neighbour (NN), Nearest Affine Hull (NAH), Nearest Convex Hull (NCH) and Nearest Sphere Center 1 (NSC) classifiers in two regimes: high-dimensional prob-lems where the dimensionality of the input space is much larger than the size of the training sets and the native (un-kernelized) classifier is used; and low-dimensional prob-lems where the training sets are larger than the dimension-ality of the input space and a kernelized classifier is needed. We tested the methods on three tasks from multi-class vi-sual recognition in the high-dimensional regime, and on five tasks from the UCI collection in the low-dimensional one. In each case, we optimized the algorithm parameters using global coarse-to-fine search, with random partitions of the training data into training and validation sets. 4.1. Experiments on Synthetic Data Before starting, we illustrate some properties of the meth-ods on a simple synthetic data set with four classes. This was produced by creating four unit-radius spheres (one for each class) in 300 dimensions with centres (  X  0 . 2 ,  X  0 . 2 , 0 ,..., 0) , sampling test and training points uniformly within each sphere, then compressing 200 of the dimensions including the second one by a factor of 10  X  see fig. 4 (top). This produces a high dimensional data set with 100D-disk like classes and many irrelevant variables. The classes are fairly well separable but the data has somewhat suboptimal scaling. For the NAH and NHD methods we estimated the affine dimension using an eigenvalue gap de-tector that reliably gave the correct result (100) for all runs with more than about 120 training points.
 Fig. 4 (bottom) shows the resulting recognition rates for NAH, NHD and NCH with varying numbers of training samples. The hyperdisk method predominates, particu-larly for larger numbers of training samples. The exam-ple is somewhat idealized  X  the data is quite clean and the classes have a form that is well adapted to the hyperdisk model  X  but it illustrates several advantages of the hyper-disk method. Firstly, NCH performs poorly. It separates classes { A,B } from { C,D } almost perfectly, but it is not much better than random (around 55-60% correct) at sepa-rating A from B and C from D . This happens because the interclass spacing is small and the convex hulls of the train-ing samples fill so little of the volume of the 100-D class disks that test samples are almost as likely to lie close to the hull of the wrong class as to that of the right one  X  i.e . even though the hulls  X  X ill in the gaps X  between the train-ing samples, they are still very poor estimates of the actual class boundaries. NCH is also much slower than NAH and NHD at run time because it needs to solve a quadratic pro-gram for each test sample to find the nearest point on the hull. Both problems are endemic to the convex hull formu-lation.
 Secondly, NAH does surprisingly well, especially when one considers that it has an asymptotic error rate of 50%: for exact estimates of the 100-D affine hulls of the classes, A and C (and similarly, B and D ) are indistinguishable be-cause they have identical affine hulls. Empirically NAH does much better than this because the estimates of the affine hulls are noisy: being estimated from examples of class A , the hull for class A always passes close to the centre of class A , but its random tilt typically makes it pass somewhat further from the centre of class C , and vice versa. Hence, empirical NAH estimates indirectly incor-porate some information about the relative positions of the classes within their affine hyperplanes. This may explain why the performances of NAH and NHD are often simi-lar in the below experiments on real data. However, as the above results suggest, it is often advisable to incorporate the position information explicitly by using NHD. 4.2. Experiments on Image Datasets ORL Face Dataset. 2 The Olivetti-Oracle Research Lab face dataset contains 10 upright 92  X  112 frontal face im-ages per person of C = 40 individuals, taken at different times with slightly different lighting conditions, image po-sitions, facial expressions and facial details. For this ex-periment we used the raw image pixels as input features without applying any visual preprocessing. For training we randomly selected N = 3 , 5 , 7 images of each individual, keeping the remaining 10  X  N for testing. The results are summarized in table 4.1 (top left). The NHD and NAH classifiers were equal best among the methods tested, fol-lowed by NCH, then NN, with NSC coming last.

Coil100 Objects Dataset. 3 The Coil100 dataset includes 72 views each of 100 different objects taken on a turntable at orientations spaced at 5 degree intervals. We chose 40 objects randomly for the experiments. We used the raw grayscale pixels of the 128  X  128 images as input features, without applying any further visual preprocessing. For training we randomly selected N = 18 , 36 , 54 images of each object, keeping the remaining 72  X  N for testing. The results are given in table 4.1 (top right). NHD and NAH again give very similar results with NHD having a slight edge. NHD achieves the best accuracy for N = 18 , 54 while for N = 36 NCH is preferred to NHD and NAH.
 NSC again produced the worst results.

Birds Dataset. This contains six categories, each with 100 images (Lazebnik et al., 2005). It is a challenging visual object recognition task with the birds appearing against highly cluttered backgrounds and the images having large intra-class, scale, and viewpoint variability. Some exam-ple images are shown in fig. 5. We use a  X  X ag of fea-tures X  representation for the images as they are too di-verse to allow simple geometric alignment of their objects. In this method, patches are sampled from the image at many different positions and scales, either densely, ran-domly or based on the output of some kind of salient re-gion detector. Here we used a dense grid of patches. Each patch was described using the robust visual descriptor SIFT (Lowe, 2004) and vector quantized using nearest neighbor assignment against a 2000 word visual dictionary learned from the complete set of training patches. For training we randomly selected N = 25 , 50 , 75 images of each class, keeping the remaining 100  X  N for testing.
 The results are given in table 4.1 (bottom left). For N = 50 , 75 , NCH achieves the best recognition rates whereas COIL N = 18 N = 36 N = 54 NHD 97.38  X  0 . 3 99 . 35  X  0 . 4 99.93  X  0 . 1 NAH 97 . 33  X  0 . 3 99 . 32  X  0 . 5 99.93  X  0 . 1 NCH 97 . 35  X  0 . 3 99.41  X  0 . 3 99 . 41  X  0 . 4 NSC 82 . 81  X  3 . 3 82 . 94  X  1 . 2 83 . 98  X  1 . 2 NN 96 . 56  X  0 . 5 98 . 84  X  0 . 4 99 . 72  X  0 . 3 UCI Iris IS MF Wine WDBC NHD 96.7 96.0 98.4 96.7 96.3 NAH 96.7 95.7 98.4 96.7 95.3 NCH 96.0 95.7 98.2 97.8 97.7 NSC 96.0 93.5 97.9 96.1 95.1 NN 96.0 96.3 97.6 94.5 96.0 NHD and NAH are equal best for N = 25 . All of the con-vex approximation based methods significantly outperform Nearest Neighbours. 4.3. Experiments with UCI Datasets In the second group of experiments we tested the kernelized versions of the methods on five lower-dimensional datasets from the UCI repository: Iris, Image Segmentation (IS), Multiple Features (MF) -pixel averages, Wine, and Wis-consin Diagnostic Breast Cancer (WDBC). The key param-eters of the datasets are summarized in table 2 and the re-sults are presented in table 4.1 (bottom right).
 In each case the dimensionality of the input space is smaller than the number of samples in each class. It follows that the native NAH classifier cannot be used directly because the affine hull of each class typically spans the entire input space. However kernelized versions of all of the classi-fiers can still be applied. The NCH and NSC formulations directly support kernelization while for NAH and NHD we used the Kernel PCA projection method described in section 3.2. We used Gaussian kernels and 5-fold cross-validation for all experiments.
 NHD and NAH were the equal best classifiers for the Iris and MF databases while NCH came first for Wine and WDBC, and NN for IS. In all of the cases tested the proposed NHD classifier either matches or outperforms the NAH classifier. The convex approximation based ap-proaches typically outperformed NN, but the difference was not as high as in the Birds database. 4.4. Discussion The NHD and NAH classifiers often had almost identical performance but when there were differences NHD usually dominated. This suggests that NHD X  X  tighter bounds on the classes are sometimes useful, but that they are often inactive, either because the affine hull projections of most queries already lie within the class hyperspheres or because the additional projections onto the hyperspheres do not add useful new discriminant information.
 NHD and NAH often outperformed NCH in both the high-dimensional native experiments and the low-dimensional kernelized ones. As mentioned above, in high dimensions the convex hulls of the training samples typically signifi-cantly underestimate the extents of the classes unless the number of samples is exponential in the dimension of the class. Thus, despite the simplicity of their underlying ap-proximations, the affine hulls and hyperdisks may often turn out to be better guides to the region spanned by the class than the convex hulls.
 In the low-dimensional problems, NN (and related kernel methods) often perform relatively well, perhaps because hole artifacts are not so prevalent in low dimensions. Sim-ilarly, as the dimension decreases, NCH progressively im-proves relative to NAH because it provides tighter bounds on the class regions. NHD seems to offer a useful compro-mise here.
 In terms of run-time efficiency NAH and NHD are to be preferred as the affine hull and bounding hyperdisk param-eters can be computed off-line. When there are large num-bers of training samples, NCH often becomes prohibitively slow at run-time because it needs to solve a quadratic pro-gram for each sample-hull distance computation. We have introduced a new method for high-dimensional classification based on approximating each class with the minimal bounding hyperdisk of its training samples  X  the intersection of their affine hull and their bounding hyper-sphere  X  and assigning test samples to the class with the nearest hyperdisk. For robustness, the algorithm uses PCA to suppress over-small  X  X oise X  dimensions in the affine hull and it removes outliers from the hypersphere calculation by bounding their Lagrange multipliers. In practice the hy-perdisk approximation offers a useful middle ground be-tween the loose approximation provided by the affine hull of the samples and the over-tight one given by their con-vex hull. It can also be kernelized to allow it to be used in lower-dimensional problems that require complex decision boundaries.
 Future work. We are currently working on large-margin classifiers that calculate explicit decision boundaries during the training phase by maximizing the separation between the affine hull or hyperdisk approximations of the classes. These may be useful alternatives to SVMs, which maxi-mize the separation between the convex hulls of the classes. Given that the affine hull and hyperdisk methods were often more accurate than the convex hull ones in the experiments, the new methods may yield more efficient classifiers than SVM in terms of both accuracy and computational com-plexity.

