 Text search engines are a fundamental tool nowadays. Their efficiency relies on a popular and simple data structure: the inverted indexes . Currently, inverted indexes can be repre-sented very efficiently using index compression schemes. Re-cent investigations also study how an optimized document ordering can be used to assign document identifiers (docIDs) to the document database. This yields important improve-ments in index compression and query processing time. In this paper we follow this line of research, yet from a differ-ent perspective. We propose a docID reassignment method that allows one to focus on a given subset of inverted lists to improve their performance. We then use run-length en-coding to compress these lists (as many consecutive 1s are generated). We show that by using this approach, not only the performance of the particular subset of inverted lists is improved, but also that of the whole inverted index. Our experimental results indicate a reduction of about 10% in the space usage of the whole index (just regarding docIDs), and up to 30% if we regard only the particular subset of list on which the docID reassignment was focused. Also, decompression speed is up to 1.22 times faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed. Finally, we also improve the Document-at-a-Time query processing time of AND queries (by up to 12%), WAND queries (by up to 23%) and full (non-ranked) OR queries (by up to 86%).
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.2.4 [ Systems ]: Textual databases Algorithms, Experimentation, Performance Inverted index compression, document reordering, query pro-cessing.
Inverted indexes are the de facto data structure to sup-port the high-efficiency requirements of a text search engine [29, 4, 10, 18, 31]. This includes, for instance, providing fast response to thousands of queries per second and using as less space as possible, among others. Given a document collection D with vocabulary  X  = { w 1 , . . . , w V } of V differ-ent words (or terms), an inverted index for D stores a set of inverted lists I w 1 [1 ..n 1 ] , . . . , I w V [1 ..n V ]. Every list I stores a posting for each of the n i documents that contain the term w i  X   X . Typically, a posting stores the document identifier (docID) of the document that contains the term, the number of occurrences of the term in this document (the term frequency) and, in some cases, the positions of the oc-currences of the term within the document. The inverted index also stores a vocabulary table , which allows us to ac-cess the respective inverted lists.

The space required by the inverted lists is dominant in the space of the index. Therefore, lists are kept compressed, not only to reduce their space usage but also to reduce the trans-ference time from disk X  X hich can be up to 4 X 8 times slower if the disk-resident lists are not compressed [10, see Table 6.9 X  X age 213]. To answer a query, the involved lists must be decompressed X  X ully or partially. Hence, fast decompression is a key issue to support quick answers.

Inverted index compression has been studied in depth in the literature [29, 18, 4, 10]. Usually, docIDs, frequencies and positions are stored separately, hence they can be com-pressed independently. Even though it is important to com-press each of these components, this paper is devoted to compressing just the docIDs. Frequencies and term posi-tions can be usually compressed using similar techniques to that used for docIDs. However, the improvements obtained in this paper will be tailored to docIDs. According to [30, 3], docIDs correspond to about 65% of a docIDs+frequencies i ndex. If we also consider positional information, docIDs correspond to about 20% of the overall space [3].
The docIDs of list I w i are compressed sorting them by increasing docID, to then represent the list using gap en-coding: the first docID of the list is represented as it is, whereas the remaining docIDs are represented as the dif-ference with the previous docID. We call DGap this dif-ference. For instance, given the inverted list h 10 , 30 , 65 , 66 , generates a distribution with smaller numbers, in particular for long lists. As we shall see through this paper, many of these DGaps are actually 1s, which correspond to terms that appear in documents whose docIDs are consecutive. To sup-port searching, inverted lists are logically divided into blocks of, say, 128 DGaps each. This allows us skipping blocks at search time, decompressing just the blocks that are relevant for a query, not necessarily the whole list. Among the exist-ing compression schemes for inverted lists, we have classical encodings like Elias [13] and Golomb/Rice [14], as well as the more recent ones VByte [28], Simple 9 [1], Interpolative [19] and PForDelta [32] encodings. All these methods benefit from sequences of small integers.

The assignment of docIDs to a given document database is not trivial. This task X  X sually known as document reorder-ing  X  X onsists in ordering the documents in D , to then assign the docIDs following this order. For instance, a simple and effective method consists in ordering the documents accord-ing to their urls [23]. These are called ordered document collections and will be the focus of this paper. Document reordering is not always feasible, as discussed in [30]. How-ever, there are many applications where this can be used. The advantage of assigning docIDs in an optimized way is that it yields smaller DGaps in the inverted lists, hence bet-ter compression can be achieved [5, 6, 21, 23, 24] and in general a much better inverted index performance [30, 26]. Reductions of up to 50% both in space usage and Document at a Time (DAAT) query processing have been reported [30, 26]. The main reason of this improvement in the query pro-cessing time is that the docIDs that are relevant to a query tend to be clustered within the same inverted lists blocks, hence less blocks need to be decompressed.

A remarkable feature of ordered document collections is that the number of 1 DGaps is increased. For instance, after reordering the TREC GOV2 document collection, the dis-tribution of DGaps has almost 60% of 1s, whereas a random ordering yields just 11% of 1s. Actually, these 1s tend to form long runs in the inverted lists. Having runs of equal symbols in a sequence allows us to use run-length encoding [14]: we simply encode a run writing its length. There are, nowadays, five main ways to take advantage of these runs in the lists, namely: 1. To use compression approaches that have been partic-2. To use compression approaches like Interpolative En-3. To use compression approaches like Interval Encoding 4. To use compression approaches like the one proposed 5. To use compression approaches like VSEncoding [25], Approach (1) above compresses every 1 in a run explicitly . If one uses the OptPFD approach [30] and a given run of 1s is large enough, we can encode each 1 using one bit. Approach (2), on the other hand, is able to represent the runs implic-itly , which is a desirable feature: when the integer encoding process gets into a run, it will encode the extreme values of the run and then each 1 within the run is not represented. That is, interpolative encoding deals with the runs in a nat-ural way. The only detail is that, depending on where a run lies within the list, it could be split into several consecutive runs by the encoding process. This may be not optimal, yet efficient enough in most cases. The main drawback of ap-proach (2) is that both the decoding and query-processing performance are not competitive [10, 30]. Approach (3) also encodes the intervals of consecutive docIDs (i.e., the runs of 1s in the DGap-encoded lists), storing the initial docID in the run, followed by the length of the run. All runs in a list are stored in a separate storage. The remaining docIDs (i.e., those that are not in any run) are called residuals . These are encoded using DGaps, as usual. Even though approach (3) is effective for web graph compression, it has two poten-tial drawbacks in our information retrieval setting. First, the separate storage of the docIDs (i.e., intervals and resid-uals) is not adequate for DAAT query processing. Second, after removing the intervals from the list, bigger DGaps are generated, which is similar to the effect seen in [17]. Unfor-tunately, this worsens the compression ratio achieved. Ap-proach (4) suggests that their method can compress runs of 1s in inverted lists. Yet, it is not studied in depth [2]. Fi-nally, approach (5) could be used along with the techniques proposed in this paper to implicitly compress runs of 1s.
In this paper we study in depth how the inverted index compression and DAAT query processing are affected by the runs of 1s that are generated in ordered document collec-tions. We conclude that properly managing these runs can lead to improvements in the space usage and query process-ing time. See Section 3 for a summary of our contributions.
We study here the basic compression schemes for inverted lists, which shall be used throughout this paper.
VByte encodes an integer using an integral number of bytes [28]. As the encoding is byte-aligned, it can be de-coded faster. To obtain this encoding, the binary represen-tation of the integer to be encoded is split into 7-bit chunks, adding an extra continuation bit (or flag) to every chunk. This flag indicates whether the current chunk is the last one in the representation of the integer or not. The compression ratio of VByte is not efficient when the inverted lists have small DGaps like 1, 2 or 3 (which are relatively frequent in large inverted lists, particularly for ordered collections).
The Simple 9 encoding (S9 for short) [1] aims at a fast decompression, yet achieving a competitive compression ra-tio. Assuming a machine word of 32 bits, we divide it into as many chunks of equal size as we can, storing a DGap in each c hunk. We use a 4-bit header to indicate the decoder how many DGaps have been stored in that word. Hence, we have 28 bits left to store the DGaps. There are 9 ways X  X ence its name X  X f dividing the remaining 28 bits into equal-size chunks: 1 chunk of 28 bits; 2 chunks of 14 bits; 3 chunks of 9 bits, 1 unused bit; 4 chunks of 7 bits; 5 chunks of 5 bits, 3 unused bits; 7 chunks of 4 bits; 9 chunks of 3 bits, 1 unused bit; 14 chunks of 2 bits; and 28 chunks of 1 bit.
To decompress an S9 word, its header is used to determine the case using a switch statement in C, where the 9 cases are hard-coded to improve decompression speed. In practice, the decompression speed is slightly better than VByte , and it has a better compression ratio in general.
The PForDelta encoding [32] divides an inverted list into blocks of, usually, 128 DGaps each. To encode the DGaps within a given block, it gets rid of a given percentage X  usually 10% X  X f the largest DGaps within the block, and stores them in a separate memory space. These are the ex-ceptions of the block. Next, the method finds the largest remaining DGap in the block, let us say x , and represents each DGap in the block in binary using b =  X  log x  X  bits. Though the exceptions are stored in a separate space, we still maintain the slots for them in their corresponding posi-tions. This facilitates the decoding process. For each block we maintain a header that indicates information about the compression used in the block, e.g., the value of b .
To retrieve the positions of the exceptions, we store the position of the first exception in the header. In the slot of each exception, we store the offset to the next exception in the block. This forms a linked list with these slots. In case that b is too small and cannot accommodate the offset to the next exception, the algorithm forces to add extra exceptions between two original exceptions. This increases the space usage when the lists contain many small numbers.

To decompress a block, we first take b from the header, and invoke a specialized function that obtains the b -bit DGaps. Each b has its own extracting function, so they can be hard-coded for high efficiency. Once we decode the DGaps, we traverse the list of exceptions of the block, storing the orig-inal DGaps in their corresponding positions. This step can be slower, yet it is carried out just for 10% of the block. In typical implementations of PForDelta , the header is imple-mented in 32 bits, since we only need to store the values of b (in 6 bits, since 1  X  b  X  32) and the position of the first exception (in 7 bits, since the block has 128 positions). PFor-Delta has shown to be among the most efficient compression schemes [30], achieving a high decompression speed.
The document reordering problem (or, equivalently, the document identifier assignment problem) consists in assign-ing similar docIDs X  X deally, consecutive docIDs X  X o docu-ments that contain similar terms [5, 6, 21, 23, 24]. This generates smaller DGaps in the inverted lists. As discussed in [30], document reordering is not always feasible. In partic-ular, when global measures of page ranking (e.g., PageRank [8] and Hits [16]) are used for early termination. However, some applications benefit from document reordering.
Previous work [30] studies the performance of different compression schemes for ordered document collections. One of the main conclusions from [30] is that some compression schemes are not suitable for these cases, e.g., PForDelta , since most DGaps are small numbers, like 1 and 2. Hence, small values of b should be used in each block (e.g., b = 1 or b = 2). However, recall that PForDelta forces to add ex-tra exceptions when b is not enough to represent an offset, which makes it unsuitable in these cases.

Two new alternative are introduced in [30]. The first one is called New PForDelta ( NewPFD from now on), which sup-ports using any value of b  X  1 without adding any extra ex-ception. The trick is to use the b -bit slot to store just b bits of the offset, while the remaining bits of the offset are stored in a separate space of memory. At decompression time, the original offsets are reconstructed using bit masks. The sec-ond alternative, called Optimized PForDelta ( OptPFD from now on), allows us to use a variable number of exceptions in each block. Hence, one can choose either to minimize the space usage or to maximize the decompression speed, yielding also different trade-offs. One of the main results from [30] is that, given a target decompression speed, New-PFD and OptPFD have a better space usage than PForDelta .
In this paper we show that by properly representing the runs of 1s that appear in inverted lists when document re-ordering is used, relevant improvements in space usage and query time can be achieved. In particular, we introduce a method for focusing the generation of runs of 1s to a par-ticular set of lists and then show how to compress these runs using methods that support efficient query processing. Overall, our results are: 1. We reduce the space usage of the most efficient alter-2. The decompression speed is up to 1.22 faster if the runs 3. We improve the DAAT query processing time of AND 4. We show that by using our approaches, the number of 5. We show that our approaches are more suitable to com-
Inverted list compressors benefit from distributions with small DGaps. Document reordering (Section 2.2) yields small DGaps in the resulting inverted lists. E.g., in the inverted index of the TREC GOV2 collection under random document order, just 10.75% of DGaps have value 1. If, on the other hand, we sort the documents by urls [23], we ob-tain 60.30% of DGaps with value 1. Thus, the ordered case generates many 1 DGaps [12]. Actually, it can be observed that many of these 1s are grouped into long runs. Instead of regarding these 1s separately (as in previous work [30]) we deal with the runs. We aim at improving the space usage and query processing time of current alternatives. Runs can be e ncoded more efficiently, e.g., using run-length compression [14]: a run of repeated symbols can be encoded indicating the symbol and then writing the length of the run. This approach has been successfully used in many areas [14, 20].
The existing document reordering methods are mainly based on the document collection to carry out the order-ing. This yields smaller indexes with enhanced query time [30, 26] since, after reordering, the documents that are rele-vant to a query tend to group into a few inverted-list blocks. Thus, less blocks need to be decompressed at query time. However, typically in practice some inverted lists are more important than others. Hence, one would want to focus on improving their particular performance, as opposed to im-proving the whole index. A particular such application is that of static inverted-list caches: the aim is to maintain in main memory the most-frequently-queried inverted lists, as well as other big inverted lists that are eventually queried and whose big size makes prohibitively expensive to transfer them from secondary storage. This effect cannot be achieved if we use methods that first assign docIDs and then construct the index following this assignment.

We develop next a heuristic to assign docIDs such that it improves the compression of a particular set of inverted lists. Our method uses the inter-list dependencies (i.e., the list intersections) to assign docIDs. As a result, these in-tersections become runs when encoded as DGaps. This will improve the compression ratio of these lists, avoiding the space-usage problems of [17] in cases where document re-ordering is allowed. Also, in Section 6 we will show that the resulting run (i.e., the intersection) will be stored inside a single block, reducing the number of blocks that are de-compressed at query time. In general, finding the docID as-signment that optimizes the amount of runs in the lists is known to be an NP-hard problem [15]. Thus, our technique is a heuristic that attempts just to improve the distribution and quality of runs, though it may not be the optimal one. Let I be the inverted index for a given document collection D , where docIDs have been assigned arbitrarily. Let L = h I 1 , I i 2 , . . . , I i m i be an ordered subset of the inverted lists of I , such that I i j is the j th list in L . The document ordering process will be based on this ordered set of lists. Let F : Z 7 X  Z be a function such that ( i, j )  X  F iff docID i has been already reenumerated as docID j .

Initially, we set F  X   X  , and start the process from I i 1 which stores docIDs d 1 , d 2 , . . . , d l . A simple approach could be to rename d 1  X  1, d 2  X  2, . . . , d l  X  l . For each such d we set F  X  F  X  X  ( d i , i ) } . Notice how this transforms I a single run when representing the list as DGaps. Now, we go on to reenumerate I i 2 , with the restriction that every docID i such that ( i, j )  X  F (for a given j ) cannot be reenumerated anymore during the process. These are called fixed docIDs. Hence, we assign consecutive docIDs (starting from l +1) to any docID i in I i 2 such that ( i, j ) 6 X  F , for any j , and add the corresponding pair to F . For instance, let us consider the following example: I i 1 = h 10 , 30 , 65 , 66 , 67 , 70 , 98 i and I 2 = h 20 , 30 , 66 , 70 , 99 , 101 i . The first step reenumerates I 1 assigning 10  X  1, 30  X  2, 65  X  3, 66  X  4, 67  X  5, 70  X  6, 98  X  7. Now we reenumerate I i 2 , having into account that docIDs 30, 66 and 70 are now fixed. Hence, we assign 20  X  8, 99  X  9, 101  X  10. After the process, the result is I i 1 = h 1 , 2 , 3 , 4 , 5 , 6 , 7 i and I i 2
Notice from the previous example that after the first step, all docIDs in the intersection I i 1  X  I i 2 are fixed docIDs, hence they cannot be changed when processing I i 2 . Notice also how the inter-list dependencies complicate the run genera-tion in I i 2 , as the documents in I i 1  X  I i 2 could have been reenumerated in any order when processing I i 1 .
We can fix this problem as follows. We instead start by reenumerating the docIDs in I i 1  X  I i 2 , renaming them from 1 to | I i 1  X  I i 2 | and add them as fixed docIDs in F . Next, and since the remaining of both lists do not intersect each other, we can reenumerate them to generate a run in each list. In this way, one of the lists will become a single run when DGap encoded, whereas the other will contain two runs. For the previous example, we start reenumerating the intersection I i 1  X  I i 2 as 30  X  1, 66  X  2, 70  X  3. Then, we reenumerate the remaining elements in I i 1 as 10  X  4, 65  X  5, 67  X  6, 98  X  7. Finally, we reenumerate the remaining elements in I i 2 as 20  X  8, 99  X  9, 101  X  10. The resulting lists are I i 1 = h 1 , 2 , 3 , 4 , 5 , 6 , 7 i and I i 2 This generates more runs than the previous approach.
In general, we start from I i 1 and compute I i 1  X  I i 2 I | X  M , for a given threshold M , we compute I i 1  X  I i 2 and repeat the process until | I i 1  X  X  X  I i j +1 | &lt; M . At this point, we take the d documents in I i 1  X  X  X  I i j and assign them consecutive docIDs (e.g., from 1 to d ). Next, we add them as fixed docIDs in F . This will generate a run of length d when computing the DGap encoding of I i j . Then, we go back and assign consecutive docIDs to the d  X  documents in I 1  X  X  X  I i j  X  1 . Notice that d of these docIDs are already fixed from the previous step, hence we only assign docIDs from d +1 to d  X  to the remaining ones. This generates a run of length d  X  in I i j I , where all documents in I i 1  X  I i 2 are fixed and will form a run. Next, the inverted lists I i 1 , I i 2 , . . . , I from L , and their non-yet-fixed docIDs are re-inserted into L as independent lists, following some well-defined order. This process is repeated until L =  X  .

We call intersection-based docID assignment (IBDA) this process. It is important to note that the intersections: are transformed into respective runs in each of these lists after the first step of IBDA. As we shall learn through this paper, this gives us a way to precompute intersections as runs. We just need to be careful in setting a suitable initial order for the lists in L : if, for some reason, we want to transform the intersection between two given inverted lists into a run, these lists must be consecutive in L . This can have many advantages at query time (see Section 6).
To summarize, notice the flexibility of our method, be-cause of the following: This allows us to adapt the method to different situations. Instead of using Interpolative Encoding [19] or Interval Encoding [7] to deal with the runs, we adapt some of the most effective compression schemes to run-length encode runs of 1s in inverted lists. We aim at efficient query pro-cessing time and improved compression ratios.
To run-length encode the runs of 1s with VByte , we need to set a special mark that indicates, at decompression time, that we are in the presence of a run. Since we are encoding DGaps &gt; 0, we use the byte 0 0000000 a s the special mark The idea is to replace x  X  3 consecutive 1s in an inverted list by the 0 0000000 m ark, followed by the VByte encoding of x . We call RLE VByte this scheme. RLE VByte is able to detect and handle very small runs (of length  X  3). We restrict the minimum length to be 3, because this yields the same space as for runs of minimum length 2 (in both cases the minimum run uses 2 bytes), yet the former has better decompression times. Indeed, this will be the only method in this paper able to deal with small runs of length 3  X   X   X  27.
We introduce now an alternative implementation of the original VByte scheme, on which will be based the RLE VByte implementation. The idea is to take advantage, at de-compression time, of several consecutive small DGaps. As-suming that 0 is used as terminator flag in VByte , we take 4 consecutive bytes from the encoding (i.e., a 32-bit word) and carry out a bitwise AND with 0X80808080 . This allows one to determine at once the 4 terminator flags of these bytes. In particular, if the bitwise AND results in a 0, it means that the terminator flags are all 0 , hence we have 4 DGaps en-coded in 4 bytes. We can then hard-code all cases, which can make a difference if most DGaps use 1 byte.

A potential drawback of RLE VByte is that, at decompres-sion time, we need to make an extra comparison per element in the list, to know whether it is the mark of a run or not. However, to decode a run, the original VByte has to make one comparison for every 1. RLE VByte , on the other hand, is able to decode the run more efficiently, as we shall see in our experiments.
S9 can be easily adapted to run-length encode the runs of 1s, as it is suggested for a variant of this scheme [2]. We only need to add a new S9 case, which will be used to encode runs X  X ecall that we use just 9 out of 16 cases available for the 4-bit headers. The remaining bits of the S9 word are used to represent the length of the run. It is rather common in practice to find S9 words that encode 28 ones, followed by a different S9 case (indeed, this is one of the most frequent cases for GOV2 inverted lists).

We define RLE S9 as follows, in order to compress runs of 1s: (C1) 1 chunk of 28 bits; (C2) 2 chunks of 14 bits each; (C3) 3 chunks of 9 bits each, 1 unused bit; (C4) 4 chunks of 7 bits each; (C5) 7 chunks of 4 bits each; (C6) 9 chunks of 3 bits each, 1 unused bit; (C7) 14 chunks of 2 bits each; (C8) 28 chunks of 1 bit each followed by C1; (C9) 28 chunks of 1 bit each followed by C2; (C10) 28 chunks of 1 bit each followed by C3; (C11) 28 chunks of 1 bit each followed by C4; (C12) 28 chunks of 1 bit each followed by C5; (C13) 28 chunks of 1 bit each followed by C6; (C14) 28 chunks of 1 bit each followed by C7; (C15) 28 chunks of 1 bit each followed by 5 chunks of 5 bits each; (C16) there are two cases still missing. The first one consists of a word storing 5 chunks of 5 bits each. The second case corresponds to runs of 1s longer than 28. Since the case of 5 chunks of 5 bits each has 3 bits unused, we use one of them to make the difference between these cases. If the bit is a 0 , then we have 5 chunks of 5 bits each. If the bit is 1 , then we use the remaining 27 bits to represent the length of the run.
Cases 1 to 15 are identified with 4-bit headers from 0000 to 1110 . The remaining two cases use 5-bit headers 11110 and 11111 . These variable-length headers can be uniquely de-coded very efficiently using a switch statement in C, check-ing just 4-bit headers as usual. If the 4-bit header is 1111 , we check the following bit to determine the 5-bit header. The compression process is carried out in two steps: ( 1 ) We first compress the list using the original S9 algorithm; ( 2 ) we traverse the S9 words generated in the previous step, and replace the S9 words that store 28 ones by one of the new cases defined above: if there are multiple consecutive such S9 words, we replace them by case 11111 ; if instead we have one such word followed by a different case, we replace it by the corresponding case 8 to 15.
We define now RLE PFD , which adds run-length-encoding capabilities to PForDelta , with minor changes to the original scheme. We define two kind of blocks: ( 1 ) normal blocks , containing 128 DGaps (as usual), and prefixed by a 32-bit block header; ( 2 ) run blocks , which encode a run storing its length within the 32-bit header. We need an extra bit in the header to indicate whether it corresponds to a normal block or encodes a run length. Fortunately, the original P-ForDelta leaves a unused bit in the header. At decompression time, the flag is checked to see whether one must decompress a normal block or a run.
 This scheme is unable to encode relatively short runs. First, we use 32 bits (the header) to represent the run length. Hence, we define run blocks as encoding runs of length  X  32 (hence, we use at most 1 bit to encode each 1 in the run). Second, it is very likely that some of the 1s lying at the beginning of a run will be used to fill the preceding nor-mal block. Just the remaining 1s could be used to form a run X  X rovided they are 32 or more. Hence, runs of moderate length (  X  100 X 200) X  X hich are rather frequent in practice X  are easy to detect. We can have, for instance, that the first docID in an inverted list is i 6 = 1, and next it has a run of 158 1-DGaps. RLE PFD will not be able to encode the run as such: the first block will be a normal block storing i and then 127 1-DGaps. The next will be also a normal block storing the 31 remaining 1-DGaps (which cannot be regarded as a run, as we already said).
For our experiments we use an HP ProLiant DL380 G7 (589152-001) server, with a Quadcore Intel(R) Xeon(R) CPU E5620 @ 2.40GHz processor, with 128KB of L1 cache, 1MB of L2 cache, 2MB of L2 cache, and a 96GB RAM, running v ersion 2.6.34.8-68.fc13.i686.PAE of Linux kernel. We index the 25.2 million documents from the TREC GOV2 collection. We implement our RLE schemes using C++. We compile our codes with g++ 4.4.5 , with opti-mization flag -O5 . For PForDelta , NewPFD , and OptPFD we use the highly-efficient implementations from [30]. We base our implementation of RLE PFD on OptPFD . VByte and S9 implementations are from ourselves. For VByte , S9, PForDelta , NewPFD and OptPFD , we subtract 1 to each DGap in the index, so they start from zero.
In our experiments, we use the following document orders to assign the docIDs to the TREC GOV2 collection: (1) the original order given to the documents in the collection ( X  X nsorted X  in our tables); (2) the URL sorting ( X  X RL X  in our tables); (3) the IBDA ( X  X BDA X  in our tables) method introduced in Section 4.1.

In our experiments, we found out that IBDA works better on inverted indexes for already-sorted document collections. Thus, we apply IBDA to the inverted index constructed for the URL-sorted document collection. In our particular im-plementation of IBDA, we sort the inverted lists of the index according to the following procedure (recall from Section 4.1 that we need an ordered set of list L ). We take the 10,000 first queries from the TREC 2006 query log and compute the pairs of terms that are most frequently queried. We then sort the inverted lists such that if the most-frequently-queried pair of terms is w i and w j , hence the inverted lists I i and I w j are the first lists in L . We then take the second most-frequent pair of terms and add their inverted lists to L (just in case they have not being added before). This pro-cedure is repeated until all frequent pairs have been added. Notice how in this way we force the inverted lists of the most-frequently-queried pairs of terms to be in consecutive positions of L . The result is that the intersection of these pairs is transformed into a run by IBDA. This precomputes these intersections to speed-up query processing. All lists that do not appear among the most frequent pairs are sorted by length, from longest to shortest. In each step of the algo-rithm, the non-processed list tails are re-inserted according to their lengths.
Table 1 shows the experimental space usage of the do-cID data of the whole inverted index, for different compres-sion schemes and the various docID assignment schemes we tested.

The main conclusions are the following: Table 1: Overall space usage (in MB) of the TREC GOV2 inverted index, for different compression schemes and docID assignment methods. The space includes just the docIDs.

To test decompression speed, we use the TREC 2006 query log and decompress the inverted lists corresponding to the query terms. Table 2 shows the average decompression speed (in millions of DGaps per second) for the different compres-sion schemes and docID assignment methods we have stud-ied. All RLE methods implicitly decompress the runs. Table 2: Average decompression speed (in mil-lions of DGaps per sec.) for different compres-sion schemes and docID assignment methods, on the TREC GOV2 inverted index.
 Compression Reorder Method Scheme Unsorted URL IBDA Interpolative Encoding 43.94 43.05 64.44 V Byte 818.81 827.98 917.47 S9 749.28 983.84 1199.61 OptPFD 927.02 857.48 852.65 RLE VByte 4 46.58 1,313.03 1,988.86 RLE S9 779.52 1,812.22 2,691.09 RLE PFD 989.74 2,026.54 3,931.73
The main conclusions are the following: The results in Table 1 and Table 2 indicate that RLE S9 on IBDA offers the best space vs. decompression-time trade-off. We can conclude that IBDA by itself is able to improve decompression speed by up to 22% on URL. However, using the RLE encodings on IBDA data is the key to obtain the remarkable speedups that we mention above. Document-at-a-Time (DAAT) and Term-at-a-Time (TA-AT) are the usual ways to support efficient query processing in inverted indexes [10]. DAAT is generally faster and uses less memory than TAAT. Next we adapt DAAT query pro-cessing to efficiently handle the runs of 1s in the lists.
DAAT processes simultaneously X  X nd from left to right X  the inverted lists I w i process is carried out with function nextGEQ ( I w j , d ), which yields the smallest docID d  X  in the inverted list I w j such that d  X  d  X  . Since nextGEQ will be invoked with increasing values of d , every inverted list maintains a cursor curr with the current position in that list. Thus, function nextGEQ moves the cursor forward and each invocation to nextGEQ starts from the position where the previous invocation stopped.
To support DAAT, inverted lists are compressed using a block layout: each list is divided into blocks of a given amount of DGaps X  X ypically, 128 DGaps per block. For each block, a block header is maintained. Each such header stores, among other things, the largest absolute docID of the block. This is used for skipping at query time. Assume that we must search for docID d in inverted list I w j . Hence, we invoke nextGEQ ( I w j , d ). This function starts from posi-tion curr and moves through the block headers comparing d with the last docID in each block, skipping all non-relevant blocks. Once we arrive at the block that potentially stores d , we fully decompress it into an auxiliary Buffer . (This is not completely true if the block has been compressed using VByte , where a DGap per DGap decompression and check-ing is more efficient.) Next, we move curr through Buffer from left to right, looking for d .
As in Section 5.4.3, we do implicit decompression of runs, this time to support DAAT query processing. That is, if when decompressing a block we detect that a run has been compressed, we do not write every such 1 in Buffer . In-stead, we just write the special mark 0 in the corresponding position of Buffer , followed by the length of the run. In other words, the time needed to decompress the whole run will be that needed to decompress just its length (i.e., a single integer) from our RLE schemes.

Assume that, at query time, we invoke nextGEQ ( I w j , d ) and that the block that potentially stores d has been decom-pressed into Buffer . Suppose that, looking for d , we reach position j of Buffer and it holds that Buffer [ j ] = 0. That is, we have reached a run of 1s of length  X  = Buffer [ j + 1]. Let d  X  denote the docID stored at position j  X  1 in Buffer . Notice that the docIDs represented in the run lie in the in-skip the run and go on from position k + 2 in Buffer . Oth-erwise, if d  X  +  X   X  d holds, the sought docID lies within the run, hence we cannot skip it. Notice that p = d  X  d  X  is the position within the run corresponding to docID d . In this case we set d  X  = d (indicating to the subsequent nextGEQ invocation that we have processed up to docID d ) and re-turn to the caller. We maintain the values of p and d  X  for the next search on the list. Assume now that we invoke the same block as d , we check whether d  X  +  X   X  p &lt; d  X  X  holds. If that is the case, we can skip the run and go on form posi-tion k + 2 in Buffer . Otherwise, d  X  X  also lies within the run, hence we act as before, updating d  X  and p .

This approach can be seen as an abstract optimization of inverted lists, since we only need to change the way in which function nextGEQ is implemented. Hence, this approach can in principle be used by any DAAT-like algorithm [9, 22], obtaining the gains in space usage of Table 1.
We need to redefine the block structure of the lists, since they store long segments that have been implicitly repre-sented X  X he runs. If, as usual, we force the blocks to store a fixed number of DGaps, then we would need to break some long runs in order to accommodate them within a block. This would dilute the benefits of having long runs. Lists are divided into blocks of fixed size because whole blocks must be decompressed to search inside then (as we saw before). Hence, a block cannot store too many DGaps. However, we do not need to decompress runs explicitly, thus the cost of decompressing a run is actually the same as decompressing a single integer. Also, handling a run in Buffer does not incur in considerable extra costs, just checking whether we can skip the run or not. Thus, we define blocks of fixed size, say 128 DGaps, and regard every run as if it were a single DGap. For RLE S9 , if it happens that the 128th DGap lies in the middle of an S9 word, then we include the whole S9 word within the block. Hence, S9 blocks can have slightly more than 128 DGaps. In the case of RLE PFD , as we already saw in Section 5.3, it handles runs of length  X  32.

Notice that in this way the number of blocks in the lists could be reduced, except for RLE PFD . As a result, we would need to store less block headers, thus reducing the space us-age. Also, we will see in the experiments below that the number of blocks decompressed at query time is reduced when compared to the results in [30], improving the decom-pression effectiveness.
W e test now the efficiency of our approaches at query time. We use the original TREC 2006 query log for the GOV2 collection.
Table 3 shows the space usage of the TREC GOV2 in-verted index. This time we include the space required by the block headers. Disregarding interpolative encoding, the smallest space is achieved by RLE S9 on docIDs assigned by IBDA. The space is about 11.08% smaller than S9 on URL and uses 21.70% extra space on that of interpolative encoding.
 Table 3: Space usage (in MB) of the TREC GOV2 inverted index, for different compression schemes and docID assignment methods. The space includes the docIDs and the block headers.

T able 4 shows the experimental query time (in millisec-onds per query) for different query processing algorithms (in particular, AND, WAND and OR). We compare differ-ent compression schemes and reordering methods.

For AND queries, S9 on URL sorting (the best existing trade-off from [30]) achieves 5.53 msec/q (according to our experiments). Using our IBDA technique to order the docu-ment collection and then our RLE PFD approach to compress the resulting docID inverted lists, we obtain 4.84 msec/q (an improvement of 12.11%). If, alternatively, we use RLE S9 on IBDA, we obtain 5.14 msec/q (an improvement of 7.05% over S9 on URL). Notice also the following: if we con-sider S9 and RLE S9 , both on URL assignment, the query times changes just slightly: from 5.53 to 5.52, respectively. If, on the other hand, we consider using IBDA instead of URL to assign docIDs, the reduction of query time is big-ger. This shows again the effectiveness of IBDA, as it is able to improve the already highly-efficient query times of AND queries [30] (recall, from Section 5.4.1, that we have used a query log to carry out IBDA).
We also test with the WAND query-processing algorithm [9], which is rather popular in current search engines. We use tf-idf ranking and look for top-10 results. As it can be seen in Table 4, we are able to speed-up the query processing by using IBDA and the RLE compression methods. For instance, by using RLE S9 on IBDA docIDs we can enhance the query time from about 60 msec/q to about 47 msec/q (an improvement of about 20.84%). For RLE PFD on IBDA, the improvement is of about 23.44%. Table 5 shows the average Table 4: Experimental query time for the different query processing algorithms we tested.
 Query Compression Reorder Method Algorithm Scheme Unsorted URL IBDA AND V Byte 42.85 22.88 28.88
WAND V Byte 175.71 76.93 75.27 (top-10) S9 145.99 59.50 50.34 OR V Byte 330.39 292.07 288.38 number of docIDs (in millions) that are decoded per query ( runs are regarded as single integers) and average number of blocks (in thousands) that are decompressed per query. Our Table 5: Millions of docIDs decoded on average per query ( X  X nts. X ) and thousands of blocks decom-pressed on average per query ( X  X lks. X ) for WAND.
 The processing is carried out for top-10 results. Comp. Reorder Method Scheme Unsorted URL IBDA VByte 10.07 63.29 5.34 24.04 6.23 18.24 S9 10.98 82.42 4.70 34.53 4.50 33.00 OptPFD 10.91 85.30 4.62 36.11 4.42 34.51 RLE VByte 6.93 60.53 1.71 15.51 1.30 11.58 RLE S9 10.89 81.85 3.21 24.21 2.09 15.87
RLE PFD 10.91 85.44 3.60 31.90 2.22 18.24 results indicate that the number of docIDs decoded (as well a s the number of decompressed blocks) is reduced in two ways: first, by using an RLE compression scheme; second, by using IBDA rather than URL sorting. For instance, if we use RLE S9 on URL sorting instead of S9 on URL, we obtain a reduction of about 30% in decoded docIDs (the same figure can be observed for decompressed blocks). If we now use RLE S9 on IBDA sorting, we obtain a further reduction of about 34% over RLE S9 on URL. This shows that we are able to improve the decompression effectiveness, as we can process the same queries decompressing less blocks and docIDs. This is not only effective to reduce the query time (as we can see in Table 4) but also in cases where accessing blocks is expensive (for instance, when blocks need to be transferred from secondary storage).
Finally, we think that this query-time reduction can be a lso achieved by the Block-Max WAND algorithm from [22].
We use now the run-length encoding of inverted lists to improve the performance of OR queries. In particular, full OR queries, where the full result (rather than a ranking) must be obtained. The main application is the offline merg-ing of inverted lists.

Let I 1 and I 2 be the involved inverted lists. The search algorithm proceeds as usual for DAAT OR queries. How-ever, if while processing I 1 we arrive at a run whose docIDs a run [ d  X  X  , d  X  X  +  X   X  X  + 1] in I 2 , then we switch to I look for d  X  X  +  X   X  X  + 1 in it. We keep repeating this process until the current docID d  X  we are looking for is not within a run. In this case, we can report the run-length encoding of the interval [ d  X  , d  X   X  1]. This allows us to compute the union of these intervals without decompressing them. Notice also that runs are written in the output already encoded. We call RLE OR this process.

Table 4 shows the experimental results. For VByte , S9 and PForDelta we use the traditional DAAT OR processing. As it can be seen, RLE OR introduces relevant improve-ments. In particular, for RLE VByte on IBDA, where we obtain a reduction of 85.75% compared to VByte on URL. The rationale behind this exceptional performance of RLE VByte is that it is the only RLE scheme able to catch short runs. These are rather frequent in practice, which explains the good performance.
We study now how the RLE compression methods and the IBDA docID assignment behave when compressing static inverted-list caches [17]. We show that the RLE compression schemes on docIDs assigned with IBDA are more suitable for compressing a small set of inverted lists, rather than using the classic compression schemes on a global docID assign-ment method, such as URL sorting.
 In our experiments, we use as a building block the TREC GOV2 inverted index with docIDs assigned by URL sorting. We use the TREC 2006 query log to compute the frequency of the query terms. We then sort the inverted lists according to this frequency (from most frequent to least frequent). Next, we consider the top-Q lists in this sorting, for Q = 1 thousand, 2 thousands, . . . , 10 thousands. For each such Q , we apply IBDA on the resulting set of lists, using the order we already mentioned.

Figure 1 shows the space usage as a function of the num-ber of lists considered in each case. We also show a table with the cache hit ratio achieved for every Q . We include just S9 compression, since similar results are obtained for the remaining methods (yet, S9 yields the smallest space usage among all other schemes). As it can be seen, RLE S9 compression yields a considerable reduction of space us-age, either on URL and IBDA sorting. For instance, we can conclude that 10 thousand lists compressed with RLE S9 on IBDA sorting use about the same amount of space than 2 thousand lists compressed with S9 on URL. In other words, within the same space used by S9 on URL to achieve a 24% hit ratio, RLE S9 on IBDA is able to achieve a 51% hit ratio.
An important feature that must be noted is that the re-Figure 1: (Above) Cache space usage as a function o f the number of inverted lists compressed, using variants of S9. (Below) Cache hit ratio achieved for the different number of lists. duction of space usage degrades as we compress more lists. In our experiments, for 1,000 lists and using RLE S9 on IBDA the reduction is of about 31%. For 10,000 lists, on the other hand, the reduction is of about 23% (recall that for the whole index we obtain a reduction of about 10%, recall Tables 1 and 3). We have shown that using our docID assignment scheme IBDA (for focusing the generation of run on a given set of inverted list) and then run-length encoding these lists yields better general performance. This reinforces and improves the results obtained in previous related work [30]. We obtain an improvement of about 10% in space usage compared with the (already very efficient) results from [30]. If we compare our decompression speed with that of [30], we are up to 1.22 times faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed. DAAT query processing can be also improved using our approaches: AND queries can be improved by up to 12%, WAND queries by up to 23% and full OR queries by up to 86%. Finally, we have shown that our approaches are useful for the efficient caching of inverted lists, achieving a reduction in space usage of up to 31%.

As future work, we plan to test our approaches to improve the performance of the Block-Max WAND algorithm of [22]. It would be also interesting to use the approach of [25] along with our approaches. Also, it would be interesting to adapt our schemes to graph compression [7]. That is, adapting our IBDA scheme to generate runs in the adjacency lists to then run-length encode these lists.
The authors want to thank the thorough comments from the anonymous referees, which definitely helped to improve the quality of this paper. [ 1] V. N. Anh and A. Moffat. Inverted index compression [2] V. N. Anh and A. Moffat. Improved word-aligned [3] D. Arroyuelo, S. Gonz  X alez, M. Marin, M. Oyarz  X un, [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern [5] R. Blanco and A. Barreiro. Document identifier [6] D. Blandford and G. Blelloch. Index compression [7] P. Boldi and S. Vigna. The webgraph framework I: [8] S. Brin and L. Page. Reprint of: The anatomy of a [9] A. Broder, D. Carmel, M. Herscovici, A. Soffer, and [10] S. B  X  uttcher, C. Clarke, and G. Cormack. Information [11] J. Dean. Challenges in building large-scale information [12] S. Ding, J. Attenberg, and T. Suel. Scalable techniques [13] P. Elias. Universal codeword sets and representations [14] S. Golomb. Run-length encoding. IEEE Transactions [15] D. Johnson, S. Krishnan, J. Chhugani, S. Kumar, and [16] J. Kleinberg. Authoritative sources in a hyperlinked [17] H. T. Lam, R. Perego, N. T. M. Quan, and [18] C. Manning, P. Raghavan, and H. Sch  X  utze. [19] A. Moffat and L. Stuiver. Binary interpolative coding [20] D. Salomon. Data compression -The Complete [21] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. [22] D. Shuai and T. Suel. Faster top-k document retrieval [23] F. Silvestri. Sorting out the document identifier [24] F. Silvestri, S. Orlando, and R. Perego. Assigning [25] F. Silvestri and R. Venturini. Vsencoding: efficient [26] N. Tonellotto, C. Macdonald, and I. Ounis. Effect of [27] A. Turpin, Y. Tsegay, D. Hawking, and H. Williams. [28] H. Williams and J. Zobel. Compressing integers for [29] I. Witten, A. Moffat, and T. Bell. Managing [30] H. Yan, S. Ding, and T. Suel. Inverted index [31] J. Zobel and A. Moffat. Inverted files for text search [32] M. Zukowski, S. H  X eman, N. Nes, and P. Boncz.
