 Graph clustering (also called graph partitioning)  X  clus-tering the nodes of a graph  X  is an important problem in diverse data mining applications. Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association; spectral methods are widely used for these objectives ,but they require eigen-vector computation which can be slow. Recently ,graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k -means objective function. In this paper ,we ex-ploit this equivalence to develop a very fast multilevel al-gorithm for graph clustering. Multilevel approaches involve coarsening ,initial partitioning and refinement phases ,all of which may be specialized to different graph clustering ob-jectives. Unlike existing multilevel clustering approaches, such as METIS ,our algorithm does not constrain the clus-ter sizes to be nearly equal. Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration. Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm: on a series of benchmark test graphs with up to thirty thousand nodes and one million edges ,our algorithm achieves lower normal-ized cut values in 67% of our experiments and higher ra-tio association values in 100% of our experiments. Further-more ,on large graphs ,our algorithm is significantly faster than spectral methods. Finally ,our algorithm requires far less memory than spectral methods; we cluster a 1.2 mil-lion node movie network into 5000 clusters ,which due to memory requirements cannot be done directly with spectral methods.
 G.1.8.1 [ Numerical Analysis ]: Spectral Methods; H.3.3.a [ Information Search and Retrieval ]: Clustering; I.5.3.a [ Pattern Recognition ]: Clustering Algorithms Algorithms ,Experimentation Graph Clustering ,Kernel Methods ,Multilevel Methods ,Spec-tral Clustering
Graph clustering (also called graph partitioning) is an im-portant problem in many domains. Circuit partitioning, VLSI design ,task scheduling ,bioinformatics ,social network analysis and a host of other problems all rely on efficient and effective graph clustering algorithms. In many data mining applications ,pairwise similarities between data objects can be modeled as a graph ,and the subsequent problem of data clustering can be viewed as a graph clustering problem.
The problem of graph clustering has been studied for decades ,and a number of different approaches have been proposed. Spectral methods have been widely used for graph clustering [2 ,6 ,9]. These algorithms use the eigenvectors of a graph affinity matrix ,or a matrix derived from the affin-ity matrix ,to partition the graph. Various spectral algo-rithms have been developed for a number of different objec-tive functions ,such as normalized cut [9] and ratio cut [2]. Furthermore ,extensive research has been done on spectral postprocessing: going from the eigenvector matrix to a dis-crete clustering [4 ,10].

It was recently shown that a wide class of graph clus-tering objectives ,including ratio cut ,ratio association ,the Kernighan-Lin objective and normalized cut ,can all be viewed as special cases of the weighted kernel k -means objective function [3 ,4]. In addition to unifying several different graph clustering objectives ,including a number of spectral clus-tering objectives ,this result implies that a simple weighted kernel k -means algorithm can be used to optimize graph clustering objectives. The basic kernel k -means algorithm, however ,relies heavily on effective cluster initialization to achieve good results ,and can often yield poor results.
In this paper ,we develop a multilevel algorithm for graph clustering that uses weighted kernel k -means as the refine-ment algorithm. Multilevel methods have been used exten-sively for graph clustering with the Kernighan-Lin objec-tive ,which attempts to minimize the cut in the graph while maintaining equal-sized clusters [1 ,7 ,8]. In multilevel al-gorithms ,the graph is repeatedly coarsened level by level until only a small number of nodes are left. Then ,an ini-Table 1: Examples of graph clustering objectives. Note that Normalized Association = k  X  Normalized Cut. tial clustering on this small graph is performed. Finally ,the graph is uncoarsened level by level ,and at each level ,the clustering from the previous level is refined using the refine-ment algorithm. This multilevel strategy results in a very fast and effective graph clustering algorithm. METIS [8] is perhaps the most popular multilevel algorithm ,and is sig-nificantly faster than spectral methods on very large graphs when equally sized clusters are desired.

However ,all previous multilevel algorithms ,such as METIS , suffer from the serious drawback of restricting clusters to be of equal size. The algorithm presented in this paper removes this restriction: instead of a Kernighan-Lin algorithm for the refinement phase ,our algorithm employs weighted kernel k means.

We present experimental results on a number of test graphs to illustrate the advantage of our approach. We show that our algorithm is significantly faster than eigenvector com-putation ,a necessary step for spectral clustering methods , on large graphs. We also demonstrate that our algorithm outperforms a state-of-the-art spectral clustering algorithm in terms of quality ,as measured by the graph objective func-tion values. On our benchmark test graphs of varying sizes, the final objective function values of the multilevel algorithm are better than the spectral algorithm in 100% of runs for ratio association ,and 67% of runs for normalized cut. We also present results on data arising from real-life data min-ing applications. We cluster the 1.2 million node IMDB movie network into 5000 clusters. Running a spectral algo-rithm directly on this data set would require storage of 5000 eigenvectors ,which is prohibitive.
We first introduce a number different graph clustering ob-jectives. As we will see later ,each of the following objectives may be expressed as special cases of the weighted kernel k means objective function ,which will motivate our use of weighted kernel k -means as the algorithm used in the re-finement phase.

For data given as a graph ,a number of different objectives have been proposed for the graph clustering problem. We are given a graph G =( V , E ,A ) ,consisting of a set of vertices V and a set of edges E such that an edge between two vertices represents their similarity. The affinity matrix A is |V| X |V| whose entries represent the weights of the edges (an entry of A is 0 if there is no edge between the corresponding vertices). We denote links( A , B )= of the weights of edges from one cluster to the other. Let degree( A ) = links( A , V ) ,the links to all vertices from nodes in A .
 A list of some of the most common graph clustering objec-Figure 1: Overview of the multilevel algorithm (for k =2 ). tives is presented in Table 1. The Kernighan-Lin objective, another common objective function ,is the same as ratio cut except that clusters are additionally constrained to be of equal size.

For ratio association ,ratio cut and normalized cut ,the most common approach for optimization has been to use a spectral algorithm. Such an algorithm is derived by relaxing the above objectives so that a global solution to the relaxed objective may be found by computing eigenvectors of an ap-propriate matrix. Once these eigenvectors are computed, a postprocessing step derives a discrete clustering from the eigenvectors. See the references [2 ,9 ,10] for further details on the spectral algorithms associated with each of these ob-jectives.

For the Kernighan-Lin objective ,the most successful ap-proach for optimizing the objective has been to use a mul-tilevel algorithm.
In this section ,we develop a new ,general algorithm for graph clustering based on multilevel methods. The multi-level approach has been made popular by METIS [8] ,though a number of multilevel clustering algorithms have been stud-ied ,dating back to [1 ,7]. Our algorithm will differ from pre-vious approaches in that it works for a wide class of graph clustering objectives ,and that all three phases of the algo-rithm can be specialized for each graph clustering objective. Below ,we describe each phase of the algorithm.

We assume that we are given an input graph ,denoted by G 0 =( V 0 , E 0 ,A 0 ) ,as well as the number of partitions requested, k . See Figure 1 for a graphical overview of the multilevel approach.
Given the initial graph G 0 ,the graph is repeatedly trans-formed into smaller and smaller graphs G 1 , G 2 , ...,G m that |V 0 | &gt; |V 1 | &gt;...&gt; |V m | . To coarsen a graph from G G i +1 ,a number of different techniques may be used. In gen-eral ,sets of nodes in G i are combined to form supernodes in G i +1 . When combining a set of nodes into supernodes, the edge weights between two supernodes in G i +1 are taken to be the sum of the edge weights between the nodes in G i comprising the supernodes.
Our coarsening approach works as follows: given a graph, start with all nodes unmarked. Visit each vertex in a random order. For each unmarked vertex x ,if all neighbors of x have been marked ,mark x and proceed to the next unmarked vertex. On the other hand ,if x has an unmarked neighbor, then merge x with the unmarked vertex y that maximizes where e ( x,y ) corresponds to the edge weight between ver-tices x and y and w ( x )istheweightofvertex x .Thenmark both x and y . Once all vertices are marked ,the coarsening for this level is completed.

As we will see when discussing the connection between graph clustering and weighted kernel k -means ,different graph clustering objectives induce different vertex weights. For ex-ample ,in normalized cut ,the weight of a vertex is its degree , and (1) reduces to the normalized cut between x and y .For ratio association ,(1) simplifies to the heaviest edge criterion of METIS ,since all vertices have unit weight.
Eventually ,the graph is coarsened to the point where very few nodes remain in the graph. We specify a parameter in-dicating how small we want the coarsest graph to be; in our experiments ,we stop coarsening when the graph has less than 20 k nodes ,where k is the number of desired clusters. At this point ,we perform an initial partitioning by cluster-ing the coarsest graph.

One way to obtain an initial partitioning is to use the region growing algorithm of METIS [8]. However ,this ap-proach is inadequate for our purposes since it generates clus-ters of equal size. We have found the best method for ini-tialization to be a spectral algorithm. We use the spec-tral algorithm of Yu and Shi [10] ,which we generalize to work with arbitrary weights [4]. Thus our initial cluster-ing is  X  X ustomized X  for different graph clustering objectives. Since the coarsest graph is significantly smaller than the in-put graph ,spectral methods are adequate in terms of speed. We find the spectral methods to yield the best quality of re-sults ,though they are somewhat less efficient than the region growing algorithm. Hence ,when efficiency is a concern ,the region growing algorithm may be used instead. The final phase of the algorithm is the refinement phase. Given a graph G i ,we form the graph G i  X  1 ( G i  X  1 same graph used in level i  X  1 in the coarsening phase). We extend the clustering from G i to G i  X  1 as follows: if a supernode is in a cluster c ,then all nodes in G i formed from that supernode are in cluster c . This yields an initial clustering for the graph ,which is then improved using a refinement algorithm. Note that we also run the refinement algorithm on the coarsest graph.

The algorithm terminates after the refinement algorithm is run on the original graph G 0 . Since we have a good ini-tial clustering at each level ,the refinement algorithm often converges very quickly. Thus ,this approach is extremely efficient.

At each refinement step ,our algorithm uses an approach inspired by the recently-shown theoretical connection be-tween kernel k -means and spectral clustering [4]. In [4] ,we proved that each of the graph clustering objectives given Objective Node Weights Kernel Matrix Table 2: Popular graph clustering objectives with corresponding weights and kernels given the affinity matrix A earlier may be expressed as special cases of the weighted kernel k -means objective with the correct choice of weights and kernel matrix. Hence ,the weighted kernel k -means al-gorithm can be directly used to locally optimize these graph clustering objectives. Each iteration of this algorithm costs O ( nz ) time ,where nz is the number of nonzero entries in the kernel matrix.

Algorithm 1 describes the weighted kernel k -means algo-rithm. The input is the kernel matrix K ,the number of clusters k ,weights for the points w ,an optional maximum number of iterations ,and an optional initial clustering ,de-
For the Kernighan-Lin (K-L) objective ,which requires equally sized clusters ,an incremental kernel k -means algo-rithm (in which points are swapped to improve the objective function value) can be used to locally optimize the objective.
In Table 2 ,we display each of the earlier graph cluster-ing objectives ,along with the choice of weights and kernel required for the weighted kernel k -means algorithm to mono-tonically optimize the given graph clustering objective. The matrix D is a diagonal matrix whose entries correspond to the sum of the rows of A ,the graph affinity matrix. Fi-nally,  X  is a real number chosen to be large enough that K is positive definite. As long as K is positive definite ,we Graph name #nodes #edges Description
DATA 2851 15093 finite element mesh 3ELT 4720 13722 finite element mesh UK 4824 6837 finite element mesh ADD32 4960 9462 32-bit adder WHITAKER3 9800 28989 finite element mesh CRACK 10240 30380 finite element mesh FE 4ELT2 11143 32818 finite element mesh MEMPLUS 17758 54196 memory circuit BCSSTK30 28294 1007284 stiffness matrix can theoretically guarantee that the algorithm monotoni-cally optimizes the corresponding graph clustering objective. Initialization can be done randomly ,or by using another clustering algorithm ,such as METIS or spectral clustering. In the case of our multilevel algorithm ,initialization is ob-tained from the clustering from the previous level ,or the initial clustering in the case of the coarsest graph.
An extension to the basic batch algorithm presented in [4] uses local search to avoid local optima. To avoid this prob-lem ,it has been shown [5] that local search can significantly improve results by allowing the algorithm to escape such poor optima. The local search algorithm considers the effect on the objective function of moving a point from one clus-ter to another. It chooses a chain of moves that causes the greatest improvement in the objective function value. We incorporate this local search procedure to improve the qual-ity of our weighted kernel k -means batch refinement scheme.
In this section ,we present a number of experiments to show that our multilevel weighted kernel k -means algorithm outperforms spectral methods in terms of quality (normal-ized cut and ratio association values) as well as computation time. We also present results of clustering the 1.2 million node IMDB movie data set graph. In our experiments ,we use the spectral algorithm from [10] ,generalized to work for both normalized cut and ratio association. This algo-rithm is used as the benchmark spectral algorithm in our experiments ,as well as the method for clustering the coars-est graph in our multilevel implementation. To speed up computation ,we considered only  X  X oundary points X  when running weighted kernel k -means; further discussion is omit-ted due to lack of space. All the experiments are performed on a Linux machine with 2.4 GHz Pentium IV processor and 1 GB main memory.

Additionally ,we use the following acronyms: mlkkm(0) stands for our multilevel kernel k -means algorithm with no local search ,mlkkm(20) is our multilevel algorithm with a local search chain length of 20 ,kkm stands for kernel k -means with random initialization ,NCV is short for normal-ized cut value ,and RAV is short for ratio association value.
Table 3 lists 9 test graphs 1 from various sources and dif-ferent application domains. Some of them are benchmark
Downloaded from http://staffweb.cms.gre.ac.uk/  X c.walshaw/ partition/ Table 4: Normalized cut values (NCV) and ratio association values (RAV) for obtaining 5000 clusters of the IMDB movie data set using our multilevel algorithm (mlkkm(0)), kernel k -means (kkm) and METIS matrices used to test METIS. On each of the benchmark graphs ,we compared our multilevel algorithms ,mlkkm(0) and mlkkm(20) ,with the spectral algorithm in terms of ob-jective function values and computation time. We report results for both ratio association and normalized cut ,and when 64 clusters are computed. We do not report compar-isons between our multilevel algorithm and METIS; in [4], we showed that the spectral algorithm is superior to METIS in terms of better ratio association and normalized cut val-ues.

Figure 2 shows the relative performance of the spectral algorithm compared with our multilevel algorithm in terms of RAV and computation time. In the left panel ,for each graph we plot the ratio of the RAV of all methods to that of mlkkm(20)  X  ratios less than 1 indicate that mlkkm(20) produces higher RAVs (and thus performs better). We can see that 100% of the RAVs produced using mlkkm(20) are higher than those obtained by the spectral algorithm. Also, we see that mlkkm(20) performs consistently better than mlkkm(0) ,which implies that local search improves the RAV in all cases (maximally by 3%).

In the right panel of Figure 2 ,we plot the computation time for the three methods. It is clear that our multilevel algorithm is much faster than the spectral algorithm. For example ,in the case of ADD32 ,the spectral algorithm is 48 times slower than mlkkm(20) and 55 times slower than mlkkm(0).

Results for the normalized cut objective appear in Fig-ure 3. For each graph ,we plot the ratio of NCV of other methods to that of mlkkm(20). As opposed to ratio as-sociation ,values that are larger than 1 in Figure 3 indi-cate that mlkkm(20) produces lower NCVs (and thus per-forms better). We see that 67% of the NCVs produced using mlkkm(20) are lower than those produced using the spectral algorithm. We also see that mlkkm(20) ,which utilizes the local search technique ,performs much better than mlkkm(0) in many cases.

In terms of computation time ,our multilevel methods again outperform the spectral algorithm in all cases. For ADD32 ,spectral is 58 times slower than mlkkm(20) and 60 times slower than mlkkm(0).
The IMDB data set 2 contains information about movies, music bands ,actors ,movie festivals and events. By connect-ing actors and movies or events in which they participate, we form a sparse undirected graph with approximately 1.2 million nodes and 7.6 million edges.

It is impractical to run a spectral algorithm directly on this data set to produce 5000 clusters not only because com-
Downloaded from http://www.imdb.com/interfaces than mlkkm(0). Movies Actors Harry Potter und die Kammer des Schreckens: Jamie Yeates ,Chris Rankin Figure 4: Graphical Plots of Cluster 633 of IMDB (Harry Potter). The right plot shows a closer view of the cluster circled in the left plot. puting 5000 eigenvectors of a graph of this size would be extremely slow but also because storing 5000 eigenvectors in main memory requires 24 GB ,which is prohibitive. Thus our multilevel algorithm is also considerably more efficient in terms of memory usage in addition to computation time. Since we cannot run spectral methods on this data set ,we compare our multilevel algorithm (mlkkm(0)) to METIS and weighted kernel k -means with random initialization (kkm). It takes our multilevel algorithm approximately twenty-five minutes to cluster this graph into 5000 clusters ,and the cluster sizes range from 13 to 7616 (for the normalized cut objective). Table 4 shows the normalized cut values and ratio association values for 5000 clusters generated using these three methods. We see that our multilevel algorithm is markedly superior to kkm: its NCV is twice as high as the NCV of mlkkm(0) and the RAV of kkm is less than one tenth of the RAV of mlkkm(0). Comparing METIS and mlkkm(0) ,the latter is superior and achieves a RAV that is 50% higher and a NCV that is 10% lower.

To demonstrate the quality of results ,we discuss a sam-pling of the produced clusters. After clustering ,we rear-range the rows and columns such that rows (columns) in the same cluster are adjacent to one another. Cluster 633, circled in the right plot of Figure 4 ,is of size 121 and mainly contains  X  X arry Potter X  movies and the actors in these movies. The left column of Table 5 lists movies in the cluster ,where we see 3 Harry Potter movies that were re-leased in the past and 1 to be released in November ,2005. There are also 2 other documentary TV programs. The right column of Table 5 lists a selection of some of the ac-tors in the cluster ,where we see the major cast members of the Harry Potter movies ,such as Daniel Radcliffe ,Rupert Grint ,Emma Watson ,etc.

Cluster 3537 contains the short series  X  X estival n  X  umero X  (No. 1-12) ,shot in year 1965; cluster 4400 contains the Ko-rean movie  X  X hunhyang X  and 19 of its cast members who acted only in this movie in the database. Other small clus-ters follow this pattern ,i.e. ,most of them are about one movie or movie series and contain cast members that acted only in this movie or movie series. Popular actors ,direc-tors or well-known movie festivals are associated with more people ,so generally they belong to much larger clusters.
We have presented a new multilevel algorithm for graph clustering. This algorithm has key benefits over existing graph clustering algorithms. Unlike previous multilevel al-gorithms such as METIS ,our algorithm is general and cap-tures a number of graph clustering objectives. This frees us from the restriction of equal-size clusters. It has advan-tages over spectral methods in that the multilevel approach is considerably faster and gives better final objective func-tion values.

Using a number of benchmark test graphs ,we demon-strated that ,in general ,our algorithm produces better ra-tio association values and normalized cut values when using spectral initialization at the coarsest level. As the coarsest graph is significantly smaller than the input graph ,spectral methods are feasible in this case ,and the running time of the algorithm is still much faster than spectral clustering on the input graph. Spectral methods have attracted intense study over the last several years as a powerful method for graph clustering; our results indicate that this may not be the most powerful technique for partitioning graphs. Fur-thermore ,given an input pairwise similarity matrix between data vectors ,our approach can be viewed as a fast multilevel algorithm for optimizing the kernel k -means objective.
Our approach also consumes far less memory than spectral methods. Spectral clustering of a 1.2 million node movie network into 5000 clusters would require several gigabytes of storage. On the other hand ,we were able to cluster this network in approximately twenty-five minutes without any significant memory overhead.
 NSF grant CCF-0431257 ,NSF Career Award ACI-0093404 , and NSF-ITR award IIS-0325116.
