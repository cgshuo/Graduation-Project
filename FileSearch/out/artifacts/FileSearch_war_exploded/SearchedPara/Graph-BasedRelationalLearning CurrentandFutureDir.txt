 Graph-based relational learning (GBRL) di X ers from logic-based relational learning, as addressed by inductiv e logic programming techniques, and di X ers from frequen t subgraph disco very, as addressed by many graph-based data mining techniques. Learning from graphs, rather than logic, presen ts represen tational issues both in input data preparation and output pattern language. While a form of graph-based data mining, GBRL focuses on identifying novel, not necessarily most frequen t, patterns in a graph-theoretic represen tation of data. This approac h to graph-based data mining provides both simpli X cations and challenges over frequency-based ap-proac hes. In this paper we discuss these issues and future directions of graph-based relational learning.
 Graph-based relational learning Graph, relational, structural, learning, disco very Graph-based relational learning (GBRL) is the task of  X nd-ing novel, useful, and understandable graph-theoretic pat-terns in a graph represen tation of data. While data mining approac hes in general address the same task, most graph-based data mining approac hes focus only on the frequency of the pattern. Therefore, we view GBRL as a sub X eld of graph-based data mining (GBDM), because the novelty of a pattern typically involves more than just the frequency of the pattern in the data. This distinguishes GBRL from the many GBDM approac hes focused on  X nding frequen t subgraphs [7; 10; 17], i.e., all subgraphs in the data whose number of instances above some minim um supp ort. We also distinguish GBRL from inductiv e logic program-ming (ILP) approac hes to relation learning. Obviously , the underlying represen tations (graphs vs. logic) are the pri-mary distinction. While graphs are extremely  X  X xible in terms of the data they can encode, the seman tics are not well de X ned. Conceptual graphs (CGs) [16] represen t a body of work aimed at de X ning a graph seman tics similar to that of  X rst-order logic. CGs attac h a seman tics to the graph by distinguishing between relations and entities or attributes. This seman tics allow conversion between CGs and restricted forms of  X rst-order logic. For this reason CG equiv alents of logic provide a reasonably unbiased mechanism for compar-ing graph-based and logic-based relational learners. These equiv alencies between graphs and logic raise the ques-tion of whether GBRL and ILP are performing basically the same type of relational learning, i.e., searc hing equiv-alent spaces. This is not true for two reasons. First, and we view this as an advantage for GBRL, ILP approac hes rely on the prior identi X cation of the predicate or predicates to be de X ned by the learned pattern. GBRL approac hes are more data-driv en, identifying any portion of the graph that is able to distinguish between classes. Second, and we view this as an advantage for ILP, the logic-based rep-resen tation allows the expression of more complicated pat-terns involving, e.g., recursion, variables, and constrain ts among variables. Graphically speaking, variables and vari-able constrain ts would imply that a portion of a graphical pattern matc hes any arbitrary subgraph or that two parts of the graphical pattern must be identical without specifying the parts' structure. These represen tational limitations of graphs can be overcome, but at a computational cost. Only a few GBRL approac hes have been developed to date. Two speci X c approac hes, Subdue [2] and GBI [18], take a greedy approac h to  X nding subgraphs maximizing an infor-mation theoretic measure. Subdue searc hes the space of sub-graphs by extending candidate subgraphs by one edge. Each candidate is evaluated using a minim um description length metric [15], which measures how well the subgraph com-presses the input graph if each instance of the subgraph were replaced by a single vertex. GBI continually compresses the input graph by identifying frequen t triples of vertices, some of which may represen t previously-compressed portions of the input graph. Candidate triples are evaluated using a measure similar to information gain. Kernel-based metho ds have also been used for supervised GBRL [9].
 We describ e two extensions to the basic GBRL approac h that take particular advantage of an information theoretic evaluation measure combined with an iterativ e application. These extensions are supervise d learning and graph grammar induction . We describ e these extensions in the context of the Subdue GBRL system, but they can be implemen ted fairly easily with similar GBRL approac hes. Figure 1: Graph-based supervised learning example with (a) four positiv e and four negativ e examples, (b) one possible graph concept, and (c) another possible graph concept. Extending a GBRL approac h to perform supervised learning involves, of course, the need to handle negativ e examples (focusing on the two-class scenario). In the case of a graph the negativ e information can come in three forms. First, the data may be in the form of numerous small graphs, or graph transactions, each labeled either positiv e or negativ e. Second, data may be comp osed of two large graphs: one positiv e and one negativ e. Third, the data may be one large graph in which the positiv e and negativ e labeling occurs throughout. We will consider the third scenario in section 3. The  X rst scenario is closest to the standard supervised learn-ing problem in that we have a set of clearly de X ned examples. Figure 1a depicts a simple set of positiv e and negativ e exam-ples. Let G + represen t the set of positiv e graphs, and G represen t the set of negativ e graphs. Then, one approac h to supervised learning is to  X nd a subgraph that appears often in the positiv e graphs, but not in the negativ e graphs. This amoun ts to replacing the information-theoretic mea-sure with simply an error-based measure. For example, we would  X nd a subgraph S that minimizes where S  X  g means S is isomorphic to a subgraph of g . The  X rst term of the numerator is the number of false negativ es, and the second term is the number of false positiv es. This approac h will lead the searc h toward a small subgraph that discriminates well, e.g., the subgraph in Figure 1b. However, such a subgraph does not necessarily compress well, nor represen t a characteristic description of the tar-get concept. We can bias the searc h toward a more charac-teristic description by using the information-theoretic mea-sure to look for a subgraph that compresses the positiv e examples, but not the negativ e examples. If I ( G ) repre-sents the description length (in bits) of the graph G , and I ( G j S ) represen ts the description length of graph G com-pressed by subgraph S , then we can look for an S that minimizes I ( G + j S ) + I ( S ) + I ( G  X  )  X  I ( G  X  j S ), where the last two terms represen t the portion of the negativ e graph incorrectly compressed by the subgraph. This approac h will lead the searc h toward a larger subgraph that characterizes the positiv e examples, but not the negativ e examples, e.g., the subgraph in Figure 1c.
 Finally , this process can be iterated in a set-co vering ap-proac h to learn a disjunctiv e hypothesis. If using the error measure, then any positiv e example containing the learned subgraph would be remo ved from subsequen t iterations. If using the information-theoretic measure, then instances of the learned subgraph in both the positiv e and negativ e ex-amples (even multiple instances per example) are compressed to a single vertex. We should note that the compression is a lossy one, i.e, we do not keep enough information in the compressed graph to know how the instance was connected to the rest of the graph. This approac h is consisten t with our goal of learning general patterns, rather than mere com-pression. For more information on graph-based supervised learning, see [6]. As mentioned earlier, two of the advantages of an ILP ap-proac h to relational learning are the abilit y to learn recur-sive hypotheses and constrain ts among variables. Graph grammars o X er the abilit y to represen t recursiv e graphi-cal hypotheses [5]. Graph grammars are similar to string grammars except that terminals can be arbitrary graphs rather than symbols from an alphab et. Graph grammars can be divided into two types: node-replacemen t grammars and hyperedge-replacemen t grammars. Node-replacemen t grammars allow non-terminals on vertices, and hyperedge-replacemen t grammars allow non-terminals on edges. Fig-ure 2b shows an example of a context-free, node-replacemen t graph grammar. Recen t researc h has begun to develop tech-niques for learning graph grammars [8; 4].
 A varian t of graph grammars called stochastic graph gram-mars [11] has been developed to represen t uncertain ty. Each production has an associated probabilit y such that all the productions involving a particular non-terminal on the left-hand side sum to one. This induces a distribution over the graphs in the language accepted by the graph grammar. A related ILP formalism is stochastic logic programs (SLPs) [13], although SLPs achieve Turing equiv alence, while stochas-tic graph grammars are limited to context-free languages. A GBRL approac h can be extended to consider graph gram-mar productions by analyzing the instances of a subgraph to see how they relate to each other. If two or more instances are connected to each other by an edge, then a recursiv e production rule generating an in X nite sequence of such con-nected subgraphs can be constructed. A slight modi X cation to the information-theoretic measure taking into accoun t the extra information needed to describ e the recursiv e comp o-nent of the production is all that is needed to allow such a hypothesis to comp ete along side simple subgraphs (i.e., ter-minal productions) for maximizing compression. The above constrain t that the subgraphs be connected by a single edge limits the grammar to be context free. More than one con-nection between subgraph instances can be considered, and would allow learning context-sensitiv e grammars, but the algorithm is exponen tial in the number of connections. Figure 2b shows an example of a recursiv e, node-replacemen t graph grammar production rule learned from the graph in Figure 2a. These productions can be disjunctiv e, as in Figure 2: Graph grammar learning example with (a) the input graph, (b) the  X rst grammar rule learned, and (c) the second and third grammar rules learned.
 Figure 2c, which represen ts the  X nal production learned from Figure 2a using this approac h. The disjunctiv e rule is learned by looking for similar, but not identical, extensions to the instances of a subgraph. A new rule is constructed that captures the variabilit y of the extensions, and is in-cluded in the pool of production rules comp eting based on their abilit y to compress the input graph. With a prop er encoding of this disjunction information, the MDL criterion will tradeo X  the complexit y of the rule with the amoun t of compression it a X ords in the input graph.
 An alternativ e to de X ning these disjunctiv e non-terminals is to construct a variable whose range consists of the di X eren t values of the production. In this way we can introduce con-strain ts among variables contained in a subgraph by adding a constrain t edge to the subgraph. For example, if the four instances of the triangle structure in Figure 2a each had an-other edge to a c , d , e and f vertex respectiv ely, then we could prop ose a new subgraph, where these two vertices are repre-sented by variables, and an equalit y constrain t is introduced between them. If the range of the variable is numeric, then we can also consider inequalit y constrain ts between variables and other vertices or variables in the subgraph pattern. A form of relational grammar induction takes place in ILP approac hes, in that the learned theory can be viewed as a grammar for generating positiv e examples. In fact, the Duce system [12] used six transformation operators to searc h the space of prop ositional logic grammars guided by a simplic-ity measure. However, the transformations were not data driven as in the above graph grammar induction, and recur-sive productions were not learned. As we mentioned earlier, the starting symbol of an ILP theory is  X xed by the input predicate to be learned. The graph grammar approac h is not restricted to include a particular relation, but essen-tially invents relations that compress the relational data. Some earlier ILP systems did achieve this abilit y to invent relations in the context of learning a particular predicate theory . CIGOL [14] invented predicates using inverse reso-lution. These predicates were also evaluated based on their abilit y to compress the learned theory . The need for practical GBRL algorithms is growing fast. Therefore, we need to address several challenging scalabil-ity issues, including incremen tal learning in dynamic graphs. Another issue regarding practical applications involves the blurring of positiv e and negativ e examples in a supervised learning task, that is, the graph has many positiv e and neg-ative parts, not easily separated, and with varying degrees of class mem bership. We discuss these issues below. Scaling GBRL approac hes to very large graphs, graphs too big to  X t in main memory , is an ever-gro wing challenge. We have investigated two approac hes to address this chal-lenge. One approac h involves partitioning the graph into smaller graphs that can be processed in parallel [3]. A sec-ond approac h involves implemen ting GBRL within a rela-tional database managemen t system, taking advantage of user-de X ned functions and the optimized storage capabili-ties of the RDBMS. These approac hes have shown promise in allowing GBRL systems such as Subdue to process arbi-trarily large graphs.
 A newer issue regarding scalabilit y is what we call dynamic graphs . With the advent of real-time streaming data, many data mining systems must mine incremen tally, rather than o X -line from scratc h. The same is true for GBRL systems. Many of the domains we wish to mine in graph form are dynamic domains, e.g., spatio-temp oral NASA remote sens-ing data. We do not have the time to periodically rebuild graphs of all the data to date and run a GBRL system from scratc h. We must develop metho ds to incremen tally update the graph and the patterns curren tly prevalent in the graph. The approac h we are curren tly investigating is similar to the graph partitioning approac h for distributed processing. New data can be stored in an increasing number of partitions. Information within partitions can be exchanged, or a repar-titioning can be performed if the information loss exceeds some threshold. GBRL can be used to searc h the new par-titions, suggesting new subgraph patterns as they evaluate highly in new and old partitions.
 Similar approac hes have been developed to scale ILP sys-tems. For example, the learning from interpr etations ap-proac h [1] takes advantage of the typical disconnected nature of examples, even in relational domains, to make tractable the relational learning and accompan ying coverage testing. Comparison of these GBRL and ILP techniques for scalabil-ity is a fruitful area of future work. Most graph-based data mining approac hes (and data mining approac hes in general) assume the input data is in the form of transactions, i.e., a set of small, disconnected graphs. At the other extreme is the assumption that the input is one large interconnected graph. The transactional represen ta-tion allows reduced matc hing complexit y and more straigh t-forward assignmen t of graphs to classes for supervised learn-ing. However, some data is more naturally represen ted as one large graph, where the class assignmen ts are made through-out the graph and possibly to varying degrees. We call such a graph a supervise d graph , in that the graph as a whole contains class information, but is not easily divided into individual classi X ed comp onen ts. For example, consider a social network in which we seek to  X nd relational patterns distinguishing various income levels. Individuals of a partic-ular income level can appear anywhere in the graph, and we cannot easily partition the graph into transactions without potentially severing the target relationships. Also, some en-tities in the graph may have mem berships in multiple classes. Such a scenario presen ts a di X cult challenge for future work in graph-based relational learning.
 We are investigating two approac hes to this task. The  X rst involves modifying the MDL encoding to take into accoun t the amoun t of information necessary to describ e the class mem bership of compressed portions of the graph. The sec-ond approac h involves treating the class mem bership of a vertex or edge as a cost, which can vary from -1 for clearly negativ e mem bers to +1 for clearly positiv e mem bers. The information-theoretic value of the subgraph patterns can be weighted by the costs of the instances of the pattern. The abilit y to learn from supervised graphs will also allow the user more  X  X xibilit y in indicating class mem bership where known, and to varying degrees, without having to clearly separate the graph into disjoin t examples. Graph-based relational learning is a fast-gro wing  X eld of data mining due to the increasing interest in mining the rela-tional aspects of graph-orien ted data. GBRL is distinct from frequen t subgraph mining approac hes, because it attempts to identify a small number of subgraph patterns that max-imize an information-theoretic metric, rather than  X nding all subgraphs appearing in a certain percen tage of the input graph. GBRL metho ds di X er from ILP metho ds by lever-aging prop erties unique to graphs versus logic. With over ten years of developmen t, our Subdue approac h has become an e X ectiv e metho d for learning from graphs. Recen t ad-vances in supervised learning and graph-grammar induction have given Subdue capabilities seen in ILP and other GBRL approac hes.
 However, much work in GBRL remains to be done. Be-cause many of the graph-theoretic operations inheren t in GBRL are NP-complete or de X nitely not in P, scalabilit y is a constan t challenge. With the increased need for mining streaming data, the developmen t of new metho ds for incre-mental learning from dynamic graphs is importan t. Also, the blurring of example boundaries in a supervised learning scenario gives rise to a supervised graph, where the class mem bership of even nearb y vertices and edges can vary con-siderably . We need to develop better metho ds for learning in these scenarios.
 As more and more domains realize the increased predictiv e power of patterns involving relationships between entities, rather than just attributes of entities, graph-based relational learning and data mining will become foundational to our abilit y to better understand the ever-increasing amoun t of data in our world. [1] H. Blockeel, L. D. Raedt, N. Jacobs, and B. Demo en. [2] D. Cook and L. Holder. Graph-based data mining. [3] D. Cook, L. Holder, G. Galal, and R. Maglothin. [4] S. Doshi, F. Huang, and T. Oates. Inferring the struc-[5] H. Ehrig, G. Engels, H. Kreo wski, and G. Rozen berg, [6] J. Gonzalez, L. Holder, and D. Cook. Graph-based re-[7] A. Inokuc hi, T. Washio, and H. Moto da. An apriori-[8] I. Jonyer, L. Holder, and D. Cook. Concept forma-[9] H. Kashima and A. Inokuc hi. Kernels for graph classi-[10] M. Kuramo chi and G. Karypis. Frequen t subgraph dis-[11] M. Mosbah. Prop erties of random graphs generated [12] S. Muggleton. Duce: An oracle based approac h to con-[13] S. Muggleton. Stochastic logic programs. In [14] S. Muggleton and W. Buntine. Machine invention of [15] J. Rissanen. Stochastic Complexity in Statistic al In-[16] J. Sowa. Conceptual Structur es: Information in Mind [17] X. Yan and J. Han. gSpan: Graph-based substruc-[18] K. Yoshida, H. Moto da, and N. Indurkh ya. Graph-
