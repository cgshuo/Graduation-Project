 We developed and tested a heuristic technique for extracting the main article from news site Web pages. We construct the DOM tree of the page and score every node based on the amount of text, the number of links it contains and addi-tional heuristics. The method is site-independent and does not use any language-based features. We tested our algo-rithm on a set of 1120 news article pages from 27 domains. Our algorithm achieved over 97% precision and 98% recall, and an average processing speed of under 15ms per page. Categories and Subject Descriptors: I.7.5 [Document Capture]: Document analysis,H.3.1 [Content Analysis and Indexing] General Terms: Algorithms, Experimentation, Performance.
Automatic core content extraction from Web pages is a challenging yet significant problem in the fields of Informa-tion Retrieval and Data Mining. The problem arises par-ticularly on the World-Wide Web, because Web pages are often decorated with side bars, branding banners, and ad-vertisements.
 Figure 1: The shaded area is the text extracted by the algorithm
Large variations in page structure across domains make automatic extraction of core content difficult. Previous ap-proaches have therefore constructed site-specific wrappers [2] , deployed machine learning [1] or used Natural Language Processing (NLP) techniques. These approaches, however, tend to require frequent updating or retraining to handle changes in structure, or employ computationally expensive steps.

We developed CoreEx, a simple heuristic technique that extracts the main article from online news Web pages. CoreEx uses a Document Object Model (DOM) tree representation of each page, where every node in the tree represents an HTML node in the page. We analyze the amount of text and number of links in every node, and use a heuristic mea-sure to determine the node (or a set of nodes) most likely to contain the main content.
CoreEx is motivated by the observation that the main content in a news article page is contained in a DOM node, or a set of DOM nodes, with significantly more text than links. For each node in the DOM tree, we pick a subset of its children which have a high text-to-link ratio. We then calculate the score of the node as a function of the total text and total number of links contained in this subset. The algorithm selects the node with the highest score.
For every node in the DOM tree, we maintain a set of nodes S which stores a subset of its children, and calculate four counts: textCnt , linkCnt , setT extCnt and setLinkCnt . Each textCnt holds the number of words contained in one node. For interior nodes this count includes the sum of the words in the subtree to which the node is the root. Sim-ilarly, linkCnt holds the number of links in or below the node. textCnt of a node is thus recursively defined as the sum of the textCnt of its children. linkCnt follows a similar definition. To keep the score normalized between 0 and 1, a link is counted as one word of text.

We iterate through the node X  X  children and add those whose text-to-link ratio is above a certain threshold to the previously fixed set S . setT extCnt and setLinkCnt are the sum of the textCnt and linkCnt of the nodes added to S.
More formally, once the DOM tree of a page is obtained, we walk up the tree, taking the following steps.
Once the counts are known for each node, the nodes are scored based on their setT extCnt and setLinkCnt . The DOM node with the highest score is picked as the Main Content Node , and the corresponding set S as the set of nodes that contains the content. If two nodes reach identical scores, the node higher in the DOM tree is selected. If two siblings are tied, a random choice is made among them.
The question then is how to compute a score that leads to good choices for core content.
Our scoring function has two components. The first mea-sures the ratio of the amount of text contained in the node to the number of links in the node. The second captures the fraction of the total text in the page that is contained in the node being scored. This component biases the score in favor of nodes which contain more text.

If page text is the total text the webpage contains, the score of node N is weight ratio and weight text are weights assigned to both the components of the scoring function. Both the weights lie between 0 and 1, and have to sum to 1 (to keep the score between 0 and 1). weight text is much lower than weight ratio since we only require a gentle push towards the node that captures more text.

For an empirical determination of the weight constants and the threshold we ran a set of experiments with dif-ferent values of weight ratio , weight text and threshold on 500 news articles that were chosen at random from our test data (described in Section 3.1). The best performance was found to be weight ratio = 0 . 99 , weight text = 0 . 01 and threshold = 0 . 9.
 We perform two preprocessing steps before feeding the HTML into the algorithm. First, we ignore the following tags: select, form, input, textarea and option . This step is motivated by the observation that some pages contain text within these tags that is mostly not content. Secondly, we ignore all script nodes.
Our test data consisted of 1620 manually labeled news article pages from 27 different sources, obtained from the MITRE corporation as per [1]. After using 500 pages for the one-time determination of our weight and threshold con-stants, we were left with 1120 pages for experimentation.
The tests were run on a workstation with four AMD Opteron 1.8 GHz dual core processors and 32 GB main memory . The CoreEx implementation was written in Java 1.5, and the system is single threaded. Our gold set was from [1]
We use a text-based performance measure, which works as follows.

Let the text in node n i have a word count of w i . We weight each node by w i , so that missing a content node or including a non-content node in the result incurs a penalty of w i . If CorrectlyExtracted is the set of all nodes that the algorithm labeled correctly, Extracted is the set of nodes extracted by our algorithm, and Answer the set of nodes marked as the correct answer in the gold set, then the precision is while the recall is
The final system performs remarkably well, with 97% pre-cision and 98% recall, yielding an F1 measure of 0.98. The simple node-based precision and recall, which does not take into account the amount of text in a node, are 93.0% and 92.5% resp. The average processing time per page was 13ms.
We found the performance to be consistent across the 27 domains, with 3 exceptions. The errors were mostly due to unusual HTML structures. Other caveats include image captions, which CoreEx labels as content but the gold set ignores. Finally, in certain pages when the article header appears far away from the body, CoreEx tends to ignore it.
We presented CoreEx, a simple, heuristic approach that extracts the main content from online news article pages.
CoreEx requires no training. It works fast and does not use any computationally expensive steps. For details see [3]. [1] J. Gibson, B. Wellner, and S. Lubar. Adaptive [2] D.-K. Kang and J. Choi. Metanews: An information [3] J. Prasad and A. Paepcke. CoreEx: Content extraction
