 NAOAKI OKAZAKI The University of Tokyo YUTAKA MATSUO
Cyber Assist Research Center, AIST Tokyo Waterfront and MITSURU ISHIZUKA The University of Tokyo 1. INTRODUCTION
Numerous computerized documents are accessible on-line. With the help of search engines, we can obtain relevant documents that fit our interests.
Notwithstanding, we are often disappointed with the quantity of retrieved doc-uments despite having narrowed the range of documents to be read through the search phase. It is necessary to establish technologies that facilitate in-formation utilization because many documents are gathered dynamically by a user. Automatic text summarization [Mani 2001] is a challenge to the infor-mation overload problem. Text summarization provides a condensed text for a given document. Multi-Document Summarization (MDS) [Radev and McKeown 1998], which is an extension of summarization to related documents, has at-tracted much attention in recent years.

Figure 1 illustrates an example of a typical MDS system. Given a number of documents, an MDS system yields a summary by gathering information from original documents. When a user wishes to gain an understanding of retrieved documents, the summary should include as much important information as possible. Therefore, to produce an effective summary, an MDS system should identify information in source documents to determine which information is important for inclusion and which information is unimportant or redundant.
Most existing summarization systems make use of such extraction techniques to find relevant textual segments in source documents. Numerous studies have examined extraction since the early stage of natural language processing [Luhn 1958] because the quality of extraction greatly affects overall performance of MDS systems.

However, to maintain the readability of extracts, we need to ensure that sentences in the extracts are in proper order. Barzilay et al. [2002] conducted experiments to examine the impact of sentence ordering on summary read-ability. That study showed that sentence ordering markedly affects readers X  text comprehension. Sentence position in the original document, which yields a good clue to sentence arrangement for single-document summarization, is in-sufficient for multi-document summarization because we must simultaneously consider interdocument order. For this reason, it is necessary to establish a good ordering strategy for MDS.

From among components in our MDS system for TSC-3, this paper specifi-cally examines a method to arrange sentences that are extracted by important sentence extraction. This paper is organized as follows. The following section (Section 2) reviews the sentence ordering problem in MDS and previous at-tempts to tackle the problem. Section 3 describes the issue of chronological ordering and presents our approach to generate an acceptable ordering by re-solving antecedent information. The subsequent section (Section 4) addresses our experiments and evaluation metrics to test the effectiveness of the pro-posed method. After reporting an outline/evaluation of other components in our MDS system, such as important sentence extraction and redundant clause elimination in Section 5, we conclude this paper. 2. SENTENCE ORDERING PROBLEM
Our goal is to either determine a most probable permutation of sentences or to reconstruct a discourse structure of sentences gathered from multiple sources. When asked to arrange sentences, a human may perform such a task without difficulty just as we put our thought in writing. However, we must consider what accomplishes this task, because computers are, by their nature, unaware of ordering. Discourse coherence, typified by rhetorical relation [Mann and Thompson 1988] and coherence relation [Hobbs 1990], are helpful to re-solve this question. Hume [1748] claimed that qualities from which association arises and by which the mind is conveyed from one idea to another are three: resemblance , contiguity in time or place , and cause and effect . That is to say, we should organize a text from fragmented information on the basis of topical relevancy, chronological and spatial orders, and a cause X  X ffect relation. The fact is especially true for sentence ordering of newspaper articles, because we must typically arrange a large number of time-series events that are related to several topics.

The strategy for sentence ordering that most MDS systems use is chronolog-ical ordering [McKeown et al. 1999; Lin and Hovy 2001], which arranges sen-tences in the order of their publication dates. Barzilay et al. [2002] addressed the problem of sentence ordering in the context of multi-document summariza-tion. They first demonstrated the remarkable impact of sentence ordering on summary readability through their experiment with human subjects. They also used human experiments to identify orderings of patterns that can improve two naive sentence-ordering techniques, such as majority ordering (examines order-ing according to relative frequency in the original documents) and chronological ordering. Based on those experiments, they proposed a strategy that combines constraints from chronological order of events and topical relatedness. Evalua-tion in which they asked human judges to grade summaries showed remarkably improved quality of orderings from the chronological ordering to the proposed method.

Lapata [2003] proposed an approach to information ordering. She introduced three assumptions for learning constraints on sentence order from a corpus of domain specific texts: the probability of any sentence is dependent on its previ-ously arranged sentences; the probability of any given sentence is determined only by its previous sentence; and transition probability from a sentence to its subsequent sentence is estimated by the Cartesian product defined over the features expressing the sentences. For describing sentences by their features, she used verbs (precedent relationships of verbs in the corpus), nouns (entity-based coherence by keeping track of the nouns), and dependencies (structure of sentences). Lapata also proposed the use of Kendall X  X  rank coefficient for an automatic evaluation that quantifies the difference between orderings pro-duced by the proposed method and a human. Although she did not describe performance comparison of the proposed method with chronological ordering, her approach is applicable to documents without publication dates.

Barzilay and Lee [2004] investigated the utility of domain-specific content structure for representing topics and topic shifts. They applied content models to sentence ordering and extractive summarization. Content models are Hidden
Markov Models (HMMs) wherein states correspond to types of information char-acteristics to the domain of interest (e.g., earthquake magnitude or previous earthquake occurrences) and state transitions capture possible information-presentation orderings within the domain. They employed an EM-like Viterbi reestimation procedure that repeats: creating topical clusters of text spans and computing models of word distributions and topic changes from the clusters . Cre-ating initial topical clusters by complete-link clustering via sentence similarity (cosine coefficient of word bigrams), they constructed a content model: a state represents a topical cluster; the state X  X  sentence-emission probabilities are es-timated as the product of word-bigram probabilities; and state-transition prob-ability are estimated by how sentences from the same article are distributed across clusters. Barzilay and colleagues conducted an experiment of ordering sentences that were unseen in test texts and arranged in the actual text. They proposed the use of an original-source-order (OSO) prediction rate , which mea-sures the percentage of test cases in which the model under consideration yields the highest probability to the OSO from among all possible permutations, along with Kendall X  X  metric. The evaluation result showed that their method outper-formed Lapata X  X  method [2003] by a wide margin. They did not address per-formance comparison with chronological ordering because they did not apply their approach to sentence ordering for MDS.

These previous attempts could be classified into two groups: use of chronolog-ical information [McKeown et al. 1999; Lin and Hovy 2001; Barzilay et al. 2002]; and learning natural ordering of sentences from large corpora [Lapata 2003;
Barzilay and Lee 2004]. Advantages of the former group are that such methods are fast, easy-to-implement, and amenable to processing of newspaper articles.
Methods of the latter group are applicable to various source documents, includ-ing newspaper articles. Against the background of these studies, we propose the use of antecedent sentences for coherent arrangement of sentences, which integrates ideas of the above two approaches. We take Barzilay X  X  chronological ordering with topical segmentation as a starting point for newspaper articles.
We consider its practical refinement using in-document preceding sentences for an evaluation criterion to arrange a sentence. 3. IMPROVING CHRONOLOGICAL ORDERING 3.1 Overview of the Proposed Method
Let us consider the example shown in Figure 2. There are three sentences, a , b , and c , that are extracted from different documents and refer to the clone sheep Dolly. Suppose that we infer an order [a-b-c] by chronological ordering.
When we read these sentences in this order, we find that sentence b is posi-tioned incorrectly because sentence b is written on the presupposition that the reader may know that Dolly had a child. An interpretation of this situation is that there were some precedent sentences prior to sentence b in the original document, but sentence extraction did not choose such sentences as summary candidates. Lack of presupposition obscures what a sentence is intended to con-vey, thereby confusing readers. Although we may hit upon a possible solution by which we include such preceding sentences into summary candidates as an exceptional case, the solution is not appropriate in terms of stability (i.e., pre-ceding sentences are not always required) and redundancy (i.e., including sen-tences may generate redundant summaries). Hence, we exclude that approach to expand the output of the sentence extraction, which is presumed to be tuned independently.

We observe the example in Figure 2 again. When reading sentence c ,we note that it can include presuppositional information of sentence b . In addition, sentence c also requires no presupposition other than Dolly X  X  existence, which was already mentioned in sentence a . Based on the analyses, we can refine the chronological order and revise the order to [a-c-b] , putting sentence c before sentence b . This revision enables us to assume sentence b to be an elaboration of sentence c ; thereby, we improve summary readability.

The rest of this section addresses improvement of chronological ordering using in-document preceding sentences followed by a detailed description of chronological ordering itself. Then we describe the entire algorithm along with topical segmentation.
 3.2 Chronological Ordering
It is difficult for computers to find a resemblance or cause X  X ffect relation be-tween two phenomena: numerous possible relations must be classified in detail; moreover, we do not have conclusive evidence whether a pair of sentences that we arbitrarily gather from multiple documents have some relation. A news-paper usually disseminates descriptions of novel events that have occurred since the last publication. Hence, the publication date (time) of each article turns out to be a good estimator of the resemblance relation (i.e., we observe a trend or series of relevant events in a time period), contiguity in time, and a cause X  X ffect relation (i.e., an event occurs as a result of previous events). [Lin and Hovy 2001, 2002] constructed a multi-document summarization sys-tem (NeATS) and arranged sentences in chronological order. They also resolved relative time expressions 1 using rules for actual date estimation. Although re-solving temporal expressions in sentences [Mani and Wilson 2000; Mani et al. 2003] may allow more precise estimation of sentence relations, it is not an easy task. For this reason, we first order sentences in chronological order, assigning a time stamp for each sentence by its publication date (i.e., the date when the article appeared in the paper).

For sentences having the same time stamp, we generate the order based on the sentence position and connectivity. We restore an original ordering if two sentences have the same time stamp and belong to the same article. If sentences have the same time stamp and are not from the same article, we insert a sentence that is more similar to previously ordered sentences to assure sentence connectivity. 3.3 Improving Chronological Ordering
After we obtain a chronological order of sentences, we make an effort to improve the ordering with the help of antecedent sentences. Figure 3 shows the back-ground idea of ordering refinement using a precedence relation. Just as in the example shown in Figure 2, we have three sentences a , b , and c in chronological order. First, we select sentence a out of the sentences and check its antecedent sentences. Seeing that there are no sentences prior to sentence a in article #1, we deem it acceptable to put sentence a here. Then we select sentence b from the remaining sentences and check its antecedent sentences. This time, we find several sentences before sentence b in article #2. Grasping what the an-tecedent sentences are saying by means of cosine similarity of sentence vectors, we confirm, first, whether their subject content is mentioned in previously ar-ranged sentences (i.e., sentence a ). If it is mentioned, we put sentence b here and extend the ordering to [a-b] . Otherwise, we search for a substitution for what the precedent sentences are saying from the remaining sentences (i.e., sen-tence c in this example). In the Figure 3 example, we find that sentence a is not referring to what sentence c X  is saying, but that sentence c is approximately referring to that content. Putting sentence c before b , we finally achieve the refined ordering [a-c-b] .

As the criterion for selecting the sentence to be inserted, we introduce dis-tance to put a sentence after previously arranged sentences. We define the distance as dissimilarity derived from cosine similarity between a vector of the arranging sentence and a vector of its preceding sentences. When a sentence has preceding sentences and their content is not mentioned by previously ar-ranged sentences, this distance will be high. When a sentence has no precedent sentences, we define the distance to be 0.

Figure 4 illustrates how our algorithm refines a given chronological order-ing [a-b-c-d-e-f] . In the Figure 4 example, we do not change the position of sentences a and b because they do not have precedent sentences in their original article (i.e., they are lead sentences 2 ). On the other hand, sentence c has some preceding sentences in its original document. This fact presents us with a choice: we should check whether it is safe to put sentence c just after sentences a and b ; or we should arrange some sentences before sentence c as a substitute for the precedent sentences. Preparing a term vector of the prece-dent sentences, we seek a sentence or a set of sentences that is the closest to the precedent content in sentences {a,b} , d , e , and f by the distance measure defined above. In other words, we assume sentence ordering to be [a-b-X-c] and find appropriate sentence(s) X , if any. Supposing that sentence e in
Figure 4 describes similar content as the precedent sentences for sentence c , we substitute X with Y-e . We then check whether we should put some sentences before sentence e or not. Given that sentence e is a lead sentence, we leave Y as empty (i.e., distance is 0) and fix the resultant ordering to [a-b-e-c] .
Then we consider sentence d , which, again, is not a lead sentence. Preparing a term vector of the precedent sentences of sentence d , we search for a sentence {a,b,e,c} , f . Supposing that either sentence a , b , e ,or c refers to the precedent content closer than sentence f , we make a decision to put sentence d here. In this way, we get the final ordering: [a-b-e-c-d-f] . 3.4 Compatibility with Multi-Document Summarization
We describe briefly how our ordering algorithm functions jointly with MDS. Let us reconsider the example shown in Figure 3. In this example, sentence extrac-tion does not select sentence c X  ; sentence c is very similar to sentence c X  . This may appear to be a rare case for explanation, but it could happen, as we optimize a sentence-extraction method for MDS. A method for MDS (e.g., the method de-scribed in Section 2, MMR-MD [Carbonell and Goldstein 1998]) makes an effort to acquire information coverage under the condition that a number of sentences exist as summary candidates. This is to say that an extraction method should be capable of refusing redundant information.

When we collect articles that describe a series of events, we may find that lead sentences convey similar information throughout the articles, because the major task of lead sentences is to give a subject. Therefore, it is quite natural that: lead sentences c and c X  refer to similar content; an extraction method for
MDS does not choose both sentence c X  and c in terms of redundancy; and the method also prefers either sentence c or c X  in terms of information coverage. 3.5 Implementation
Figure 5 depicts a block diagram of the sentence ordering algorithm. Given nine sentences denoted by (a b...i) , the algorithm eventually produces an ordering: [a-b-f-c-i-g-d-h-e] .

We categorize sentences by their topics in the first phase. The aim of this phase is to group topically related sentences together. It was applied to sentence ordering by Barzilay et al. [2002]. We use the vector space model [Salton et al. 1975] for sentence representation and apply the nearest-neighbor method [Cover and Hart 1967] to obtain topical clusters. Because sentences in newspaper articles are not always long enough to represent their contents in sentence vectors, we assume that a newspaper article is written for one topic and thereby classify document vectors. Given l articles and m kinds of terms in the articles, we define a document-term matrix D ( l  X  represents the frequency of term j in document i ,
Letting D i denote a term vector ( i -component row vector) of document i ,we measure the distance or dissimilarity between two articles x and y using a cosine coefficient:
We apply the nearest-neighbor method [Cover and Hart 1967] to merge a pair of articles when their minimum distance is lower than a given parameter (determined empirically). In this manner, we classify sentences according to topical clusters of articles. We determine an order of clusters based on the chronological order of the first publication date of articles in each cluster.
The rest phases of the algorithm, chronological ordering and improving chronological ordering that we described before, treat the partitioned sen-tences independently. We arrange sentences within respective topical clusters.
In Figure 5 example, we obtain two topical clusters, ( abcfgi ) and ( de h ), as the output from the topical clustering. The second phase orders sen-tences in each topical group by the chronological order and sends two orderings, [a-b-c-i-g-f] and [h-e-d] , to the third phase. The third phase refines each chronological ordering by the proposed method and outputs the final ordering: [a-b-f-c-i-g-d-h-e] .
 4. EVALUATION 4.1 Experiment
We conducted an experiment of sentence ordering through multi-document summarization to test the effectiveness of the proposed method. Extracting sentences up to a specified number (ca. 10% summarization rate), we created a set of candidate summary sentences for each task. We order the sentences by six methods: human-made ordering (HO) as the highest anchor; random ordering (RO) as the lowest anchor; chronological ordering (CO) as a conven-tional method; chronological ordering with topical segmentation (COT) [similar to Barzilay X  X  et al. method 2002]; the proposed method without topical segmen-tation (PO) ; and the proposed method with topical segmentation (POT) . Using 28 topics (summarization assignments) 3 in the TSC-3 test collection, we asked three human judges to evaluate these sentence orderings (i.e., each sentence ordering was assigned with three independent judgments). For each summa-rization topic, we presented six summaries generated by different methods in random order to prevent the bias during the experiment. We describe three tasks to measure the quality of orderings below.

The first evaluation task is a subjective grading by which a human judge marks an ordering of summary sentences on a scale of 1 to 4 (Figure 6(a)).
We give clear criteria of scoring to the judges as follows. A perfect (score 4) summary is a text that we cannot improve any further by reordering. An acceptable (score = 3) summary is one that makes sense and is unnecessary to revise even though there is some room for improvement in terms of readability.
A poor summary (score = 2) is one that loses a thread of the story at some places and requires minor amendment to bring it up to an acceptable level.
An unacceptable summary (score = 1) is one that leaves much to be improved and requires overall restructuring rather than partial revision. We inform the judges that summaries were made of the same set of extracted sentences and only sentence ordering made differences between the summaries to improve consistency in rating.

In addition to the rating, it is useful that we examine how close an ordering is to an acceptable one when the ordering is regarded as poor . We designed a second task that measures closeness of an ordering to a human-made one (Figure 6(b)). However, this task may be so simplistic that it cannot accept several sentence-ordering patterns for a given summary. We infer that it is valuable to measure the degree of correction because the task virtually requires a human corrector to mentally prepare a correct answer for each ordering. For this reason, we introduce another task in which a human judge is presumed to illustrate how to improve an ordering of a summary when he or she marks the summary as poor in the rating task. We restrict applicable operations of corrections to move operations to maintain minimum correction of the ordering.
We define a move operation here as removing a sentence and inserting the sentence into an appropriate place (Figure 6(c)). 4.2 Evaluation Metrics
The remainder of the evaluation design entails the comparison of an order-ing with its reference ordering. Figure 7 shows an ordering of nine sentences (denoted by a, b,...,i ) and its reference (correct) ordering. Supposing a sen-tence ordering to be a rank, we can convert a sentence ordering into a permu-tation, which represents the rank of each sentence. Let  X  an ordering to be evaluated and  X  be its reference ordering. Expressing sen-tences a in 1, b in 2, ..., i in 9 respectively, we obtain permutations
Figure 7:
The above formulation transforms closeness measurement of two orderings into calculation of rank correlation of two permutations  X  rank correlation  X  s (  X  ,  X  ) and Kendall X  X  rank correlation rank correlation metrics:
Therein: n represents the number of sentences; and sgn( x )  X  1 otherwise. These metrics range from  X  1 (an inverse rank) to 1 (an identical rank) via 0 (a noncorrelated rank). For Formula 3, we obtain and  X  k (  X  ,  X  ) = 0 . 11 (i.e., the two ranks are approximately noncorrelated). Spear-man X  X  rank correlation considers the absolute relation of ranking (i.e., absolute position of sentences), and Kendall X  X  rank correlation considers the relative relation of ranking (i.e., relative position of pairs of sentences). Lapata [2003] and Barzilay and Lee [2004] adopted Kendall X  X  rank correlation for their evalua-tions, considering that it can be interpreted as the minimum number of adjacent transpositions needed to bring an order to the reference order.

Let us carefully examine the orderings in Figure 7. Spearman X  X  rank cor-relation and Kendall X  X  rank correlation indicate that they are noncorrelated ranks. However, we notice that the reference ordering can be generated from the ordering by moving a group of sentences c , d , e , f to the position just after sentence h . Although a reader may find the group of sentences c , d , e , f to be in-correctly positioned, he or she does not lose the thread of the summary because
Sentences in a document are aligned one dimensionally: a reader brings together continuous sentences in a text into his or her mind and interprets their meaning. In other words, when reading a text, a reader prefers local cohesion or sentence continuity as a relative relation of discontiguous sentences. Kendall X  X  rank correlation equally penalizes inverse ranks of sentence pairs that are mutually distant in rank (e.g., sentences c and a , c and b ). Therefore, we propose another metric to assess the degree of sentence continuity in reading. We define sentence continuity as the number of continuous sentence pairs divided by the number of sentences: sentence continuity =
Therein, c represents the number of continuous sentence pairs. Although there is no sentence prior to the first sentences, we want to measure the appropri-ateness of the first sentence as a leading sentence. 4 Hence, we define sentence continuity of the first sentence as an agreement of the first sentences between an ordering and its reference. This metric ranges from 0 (no continuity) to 1 (identical). The summary in Figure 7 may interrupt a human X  X  reading after sentences i , f as the human searches for the next sentence to read. We observe six continuities and an agreement of the first sentences and calculate sentence continuity: 7 / 9 = 0 . 78.
Sentence continuity can be expressed through permutations:
Therein,  X  (0) =  X  (0) = 0; equals( x , y ) = 1 when x equals y and 0 other-wise.  X   X  1 ( i ) represents a sentence (or index number) of the i th order in the evaluated of the sentence arranged in the i th order in the reference. Hence, equals(  X  X   X  1 ( i ),  X  X   X  1 ( i  X  1) + 1) = 1 when sentences of ( i in the reference are also continuous in an ordering. 4.3 Results
Figure 8 shows distribution of rating scores of each method as a percentage of 84 (28  X  3) summaries. Judges marked about 75% of human-made orderings (HOs) as either perfect or acceptable; they rejected as many as 95% of random order-ings (ROs). Chronological ordering (CO) did not yield satisfactory results, losing a thread of 63% summaries, although CO performed much better than RO. Topi-cal segmentation did not contribute to ordering improvement of CO either: COT was slightly worse than CO. After taking an in-depth look at the failure order-ings, we found that topical clustering did not perform well during this test. We infer that topical clustering did not prove its merits with this test collection because the collection comprises relevant articles that were retrieved by some query and polished well by a human: they exclude articles that are unrelated to a topic. On the other hand, the proposed method (PO) improved chronological ordering much better than topical segmentation: the sum of the perfect and ac-ceptable ratio jumped from 36 (CO) to 55% (PO). This fact shows that ordering refinement by precedence relation improves chronological ordering by pushing poor ordering to an acceptable level. Kendall X  X  coefficient of concordance ( W ), which assesses the interjudge agreement of overall ratings, indicated that the three judges graded similar score with a high value ( W =
Table I shows the resemblance of orderings to those made by humans. Al-though we found that RO is clearly the worst, as in other results, we found no significant differences among CO, PO, and HO. This result revealed the diffi-culty of automatic evaluation by preparing a correct ordering.

Table II reports the resemblance of orderings to the corrected ones with average scores (AVG) and standard deviations (SD) of the three metrics  X  , and  X  c . Apparently, average figures have a similar tendency to the rating task with three measures: HO is the best; PO is better than CO; and RO is definitely the worst. We applied one-way analysis of variance (ANOVA) to test the effect of these four different methods (RO, CO, PO, and HO). ANOVA verified the effects of the different methods ( p &lt; 0 . 01) for the three metrics. We also applied the Tukey test to compare the differences among these methods. The Tukey test revealed that RO was definitely the worst with all metrics. However,
Spearman X  X  rank correlation  X  S and Kendall X  X  rank correlation significant differences among CO, PO, and HO. Only sentence continuity demonstrated that PO is superior to CO; and that HO is better than CO ( 0 . 05). The Tukey test suggested that sentence continuity has better conformity to the rating results and higher discrimination to make a comparison. As just described, the proposed method shows a significant improvement.
However, evaluation by rating (Figure 8) and comparison with corrected order-ing (Table II) also present a great difference between PO and HO. The main reason they made such a difference is the way of arranging lead sentences. The proposed method is intended to preserve chronological order of lead sentences as long as the refinement algorithm does not choose them as a substitution of preceding information for an arranging sentence. Although a human can devise a presentation order of lead sentences based on common sense, it is difficult for computers to grasp preceding information of each lead sentence.

In addition, several cases were found in which the proposed method inserted an unnecessary or inappropriate sentence as presuppositional information of a sentence. Because we do not apply a deep analysis of discourse structure and instead use precedent relation, a sentence does not always require all or any preceding sentences as presuppositional information. If the proposed method employs unnecessary preceding sentences as presuppositional information, it may choose a sentence that has little relation to the arranging sentence. The proposed method roughly estimates presuppositional information in this man-ner, but shows practical improvement for most summaries. 5. OUTLINE OF OUR MULTI-DOCUMENT SUMMARIZATION SYSTEM
Sentence ordering is a component of our MDS system for TSC-3. This section summarizes our MDS system and its evaluation in TSC-3. For more detailed description of this system, refer to the TSC-3 conference paper [Okazaki et al. 2004].

Figure 9 shows the architecture of our summarization system. In the first step, all documents are passed to CaboCha [Kudo and Matsumoto 2002] acquire dependency structures of sentences and extract named entities. We perform two kinds of tasks on the summarization source: important sentence extraction and analyses for generating a summary of good readability . Important sentence extraction for MDS [e.g., Carbonell and Goldstein 1998;
Radev et al. 2000] should identify information in source documents to determine which information is important for inclusion and which information is unim-portant or redundant in a summary. Assuming that a human reader breaks a sentence into several informational phrases to which the sentence is referring, we express each sentence in attributes and their weights [e.g., Salton et al. 1975;
Nagao and Hasida 1998; Mani and Bloedorn 1999; Wada et al. 2002; Okazaki et al. 2002]. We convert each source sentence into a set of information fragments that consist of dependency relations of two terms and their weights calculated by statistical analysis. Figure 10 demonstrates the procedure for converting a sentence into information-fragment representation. We then determine a set of sentences containing as many important information fragments, formulat-ing the important sentence extraction as a combinational optimization problem under the constraint of the summarization ratio. It is also important to improve summary readability, because MDS gathers information over different docu-ments. From among such components, redundant clause elimination deletes redundant or repeated expressions within sentences [Ishizako et al. 2000]. Such processing cannot be achieved by important sentence extraction. We focus at-tention of repeated expressions peculiar to newspaper articles such as  X  X  no-jiken-de (on the event of  X ) X  and  X  X  mondai-ni-tsuite (as for the problem that  X ). X 
The component extracts clauses, which modify a noun phrase and measures similarity of all pairs of the clauses by dynamic programing (DP) matching. It then deletes clauses which are similar to previously arranged clauses.
Figure 11 shows the evaluation result of content coverage by human sub-jects. Results demonstrate the quality of important sentence extraction. When a summary includes all necessary information, the evaluation value will be 1.
Although we did not use any question information for summarization, our sys-tem (denoted by MOGS) performed well (3rd place; far better than a baseline method; and above average) for both Short and Long summaries. Figure 12 shows the number of redundant or unnecessary sentences in one summary.
It indicates the quality of both important sentence extraction and redundant expression elimination. The more a summary includes redundant information, the higher the evaluation value. Our system (MOGS) hardly includes redun-dant sentences (0 . 067 redundant sentences for a Short summary and 0 sentences for a Long summary on average). 6. CONCLUSION
We described our Multi-Document Summarization (MDS) system for TSC-3, specifically outlining an approach to coherent sentence ordering for the MDS system. We addressed a drawback of chronological ordering, which is widely used by conventional summarization systems: it arranges sentences without considering presupposed information of each sentence. Proposing a method to improve chronological ordering by resolving precedent information of arrang-ing sentences, we conducted an experiment of sentence ordering through MDS.
We also proposed an evaluation metric that measures sentence continuity and an amendment-based evaluation task. The proposed method, which utilizes the precedence relations of sentences, achieved good results, raising poor chrono-logical orderings to an acceptable level by 20%. Amendment-based evaluation outperformed an evaluation that compares an ordering with an answer made by a human. The sentence continuity metric, when applied to the amendment-based task, showed good agreement with the rating result.

Future avenues of this study will point toward further improvement of sen-tence ordering. Assigning the highest priority to preserve chronological or-dering, we can remedy situations in which chronological ordering might fail, based on presuppositional information of respective sentence arrangements.
Although it was practical that the proposed method estimates presuppositional information by preceding sentences, there is room for improving that estima-tion. Arranging lead sentences in original documents also requires further in-vestigation.

Another direction of this ongoing study will be toward establishment of an evaluation methodology for sentence ordering. This study uncovered the diffi-culty of automated evaluation of sentence ordering. We adopted amendment-based evaluation for sentence ordering and showed its accuracy and usefulness.
Nevertheless, it requires a great deal of time and effort that would be difficult to repeat in a regular basis. We recognize the necessity of automatic evalua-tion that will probably feature multiple correct orderings for a summary with extended metrics of Kendall X  X  rank correlation or sentence continuity.
Taking a global view of MDS, it may be an interesting approach that incor-porates important sentence extraction and sentence ordering. Knowing what kinds of events tend to occur after an event, the extraction scheme can pro-mote peripheral information fragments after inclusion of the fragments. This knowledge of a natural course of events benefits sentence ordering. They are not isolated problems: we plan to pursue their integration for their mutual benefits and improvement of overall quality of MDS.
 We used Mainichi Shinbun and Yomiuri Shinbun newspaper articles and the
TSC-3 test collection for evaluation. We also wish to thank TSC organizers for organizing their valuable workshops and community. We thank the reviewers for their very useful comments.

