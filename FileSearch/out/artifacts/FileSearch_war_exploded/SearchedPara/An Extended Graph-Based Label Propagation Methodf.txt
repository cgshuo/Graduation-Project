 Readability assessment is to evaluate the reading difficulty of documents, which can be quantified as reading levels [1]. Measuring automatically the reading difficulty of doc-uments is helpful in areas such as foreign l anguage education [16]. Recently, with the huge volume of documents mounting up on the web, its importance has been boosted in relevant applications, such as information retrieval [3,8].

Researches on readability assessment have a relatively long history from the early 20th century [17]. Many techniques have been used to handle it, such as readability formulae [12], logistic regression [7], SVMs [6], and many others. Text features have been developed to improve the performance of these techniques [6,10]. However, the researches on readability assessment are far from enough, and the traditional readability formulae are still the most used techniques in real world applications.

In this paper, we propose an extended gra ph-based label propagation method for readability assessment. In our method, documents are used as nodes, and the semantic similarities between document pairs as edges to build a graph. Label propagation is used on the graph to propagate class labels from the labeled documents (whose reading levels are known) to the unlabeled ones (i.e. the target documents). The main contributions of the paper are listed as follows:  X  An extended graph-based label propagation method is proposed for the task of read- X  Experiments are conducted systematically on datasets of both English and Chinese.
The rest of the paper is organized as follo ws. Section 2 briefly introduces the back-ground of readability assessmen t, plus concepts of the graph-based label propagation. Section 3 describes the details of our proposed method. Section 4 presents the experi-mental studies. Finally, the conclusion is made and future work is described. In this section, we briefly introduce the background of our work, including the concept of readability assessment and the gra ph-based label propagation methods. 2.1 Readability Assessment Readability assessment is to evaluate the reading difficulty of documents, which is usu-ally quantified as reading levels [1]. The ta sk can be handled by regression, classifica-tion, or ranking [7,13]. At the early stage, researchers typically treated it as a regression problem, and developed many well-known readability formulae [12]. Text features were defined for these formulae to measure both lexical and grammatical complexities of the documents. More useful text features were developed recently by adopting the NLP (Natural Language Processing) techniques [6].

Recent researchers handled readability a ssessment by classification, and used the machine learning techniques. Collins-Thompson and Callan [4] developed statistical language models to compute the reading level of a document. Feng et al. [6] built SVMs (Support Vector Machines) to classify the reading difficulty of documents. Franc  X ois and Fairon [7] compared the performance of the logistic regression model and the ordinal classification model for readability assessment. Additionally, ranking techniques are also used, due to the ordinal nature of r eading levels. Ma et al. [13] assigned read-ing levels by ranking the reading difficulty of documents. Collins-Thompson et al. [3] ranked the reading difficulty of documents fo r information retrieval. Jameel et al. [8] employed the Latent Semantic Indexing (LSI) for ranking the reading difficulty. 2.2 Graph-Based Label Propagation Graph-based label propagation is a method that can be used for classification. A graph is built by entities as nodes, the inter-relations of which are represented by edges. Label propagation is used to propagate class labels from labeled entities to unlabeled ones [11]. Graph-based label propagation has been successfully applied in various applica-tions, such as dictionary construction [11], word segmentation and tagging [19] and image parsing [18]. To our knowledge, it has not yet been applied for readability as-sessment.

Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation [19]. Duri ng graph construction, the similarity func-tion is required to build edges and compute weights for the edges between pairs of the nodes [5]. Some form of edge pruning (called graph sparsification) is required to get a suitable graph for label propagation [9]. Many effective algorithms have been developed for label propagation [15,11]. To determine the reading level of a document, we design an extended version of the graph-based label propagation method [15]. Firstly, we describe how to construct a graph for readability assessment. Secondly, we present the strategies to reinforce the graph by incorporating the pre-classification results. Finally, we use the label propa-gation algorithm to predict the reading levels. Figure 1 shows the framework of our method.
 3.1 Graph Construction for Readability Assessment As stated before, readability assessment can be viewed as a classification problem. Sup-pose there are l labeled documents ( d 1 ,c 1 ) ,  X  X  X  , ( d l ,c l ) and u unlabeled documents d +1 ,  X  X  X  ,d l + u .Let n = l + u be the total number of documents, and C be a set of labels C = { 1 , 2 ,  X  X  X  ,m } , corresponding to the m reading levels ( m = | C | ). Then, agraph G =( V,E,W ) for readability assessment can be built. In G , V is the set of nodes corresponding to the documents ( | V | = n ), E is the set of edges representing relations between pairs of documents, and W is a n  X  n weight matrix weighing the relations. From the set of documents V , the subset V l = { d 1 ,d 2 ,  X  X  X  ,d l } contains the labeled documents, and subset V u = { d l +1 ,d l +2 ,  X  X  X  ,d l + u } contains the unlabeled documents. Based on the graph G , the objective is to propagate (assign) labels to the nodes (documents) in V u .
 To construct an appropriate graph G for readability assessment, we have two steps. The first is building edges by similarity, the second is graph sparsification. Following gives the details.
 Building Edges by Similarity. To construct edges and weights in the graph, we mea-sure similarities between all pairs of documents, and establish an edge between each pair of node, where the computed similarity is assigned as the edge weight. By doing so, we get a complete graph. For similarity computation, we use three types of VSMs (Vector Space Models) to represent a docum ent as a vector, and calculate the similar-ity (cosine similarity or Kullback-Leibler divergence). The VSMs are TF-IDF (Term Frequency-Inverse Document Frequency), LSI (Latent Semantic Indexing), and LDA (Latent Dirichlet Allocation) , which are described below.  X  TF-IDF: TF-IDF is a numerical statistic that is intended to measure how important  X  X SI: LSI defines a latent semantic space where all the words and documents in a  X  X DA: LDA can represent a document as a mi xture of topics, where each topic is Graph Sparsification. For graph sparsification, we filter out edges from the complete graph based on the edge weights. We use three sparsification techniques: directed k NN (d-k nn), undirected k NN (u-k nn), and b -matching.  X  Directed k NN: Each node keeps only k directed edge to its top-k nearest neighbors.  X  Undirected k NN: The procedure is the same as above, save that the edges left are  X  b -matching: Nearly the same as the undirected k NN, where each edge is undi-3.2 Graph Reinforcement by Pre-Classification For readability assessment, we add an extra step between the graph construction and the label propagation. This step focuses on reinforcing the graph to capture the ordi-nal relationship among the reading levels. Pre-classification is required for the graph reinforcement.

We build the feature-based classification models for pre-classification, which have been proved useful [6] in predicting the reading level for a document. The results of pre-classification may provide extra information to prune the edges and amplify the weights. To build the classifier, we extract simple text features [6] from documents. The features used here are listed in Table 1.

By training the classifier using the labeled doc uments, pre-labels (i.e. reading levels) can be predicted for the unlabeled documents, denoted as {  X  c l +1 ,  X  X  X  ,  X  c l + u } .Thus,we ( i  X  X  1 ,  X  X  X  ,l } ) for labeled documents and z unlabeled documents.
 Based on the priori label, we propose four strategies to reinforce the graph built in Section 3.1.  X  X 1: For each edge e u,v  X  E ,if z u = z v , amplify its weight w u,v  X  W to w u,v by  X  X 2: For each edge e u,v  X  E , modify its weight w u,v  X  W to w u,v by the following  X  X 3: For each node v  X  V , for each node u in its neighbor set, keep the edge e v,u if  X  X 4: The process is the same as S3, except that each weight w u,v is amplified as in
Among these four strategies, S 1 amplifies the weights of edges linking nodes with identical priori labels. S 2 amplifies the weight of every edge by an exponent scaled by the label difference of its two ends, which a ssumes an ordinal relation among the read-ing levels. S 3 filters out edges linking nodes with dissimilar levels (the label difference bigger than 1). S 4 integrates the characteristic of S 1 to S 3 . 3.3 Label Propagation Given a graph G =( V,E,W ) defined in Section 3.1, the goal of label propagation is to propagate class labels from the labeled nodes to the entire graph. To handle it, we use the label propagation method presented in [15], which is simple but effective [11]. The method iteratively updates the label distribution using Eq.1. a node u at the t -th iteration. On the right side,  X  u is the normalizing constant,  X  and  X  are hyper-parameters, and r u ( c ) is the initial probability of c on the node u if u is initially labeled (i.e. belonging to the labeled set V l ). The function  X  ( x ) returns 1 if x is true and 0 otherwise. N ( u ) denotes the set of neighbors of u . U ( c ) is the probability of c based on the uniform distribution. The iteration stops when the change in q ( t ) u ( c ) is small enough (e.g. less than e  X  3 ), or t exceeds a predefined number (e.g. greater than 30 ). In this section, we conduct experiments based on two datasets of different languages, to investigate the following three research questions: RQ1: During graph construction, among the three types of VSMs (Vector Space Models), which one is the best for measuring similarities between pairs of documents for readability assessment?
RQ2: During graph reinforcement, among the four strategies, which one is the most suitable for readability assessment, by using different graph sparsification techniques?
RQ3: What are the effects of varying the proportion of labeled documents on the performance of the extended graph-based label propagation method? 4.1 Corpus The experiments are built on datasets of two languages: Chinese and English. We select Chinese because recent researches on readab ility assessment for Chinese are relatively few. We also select English to demonstrate t hat our method can be effectively applied to different languages.

For Chinese, we collect the Chinese primary school language textbooks as the dataset, which contains 6 reading levels corresponding to the 6 grades. To assure the represen-tativeness of the Chinese dataset, we combine two widely used editions of textbooks, which cover 90% of Chinese primary schools. For English, we collect the New Concept English textbooks as the dataset, which contains 4 reading levels corresponding to the 4 volumes. The details of the two datasets are shown in Table 2. For Chinese, we use characters as elemental units, while for E nglish, we use words as elemental units. 4.2 Experiment Settings To answer the research questions, we randomly split each dataset into two sets: one is the labeled set, the other assumed unlabeled set. Firstly, a graph is built following the steps of graph construction described in Section 3.1, using one of the three VSMs and a graph sparsification technique. Secondly, pre-classification is made for the unlabeled set of documents, using logistic regression trained on the labeled set. Thirdly, the graph is reinforced using one of the four strategies described in Section 3.2. After that, the label propagation algorithm is applied to propagate the labels from labeled documents to unlabeled documents. For each document in t he unlabeled set, its predicted label is the one with the maximum probability.

We use prediction accuracy, i.e. the ratio of documents correctly labeled in the un-labeled set, to evaluate the performances of the method with different configurations. To account for randomness, we compute the mean accuracy across the results of 100 validations for each configuration. 4.3 Comparison among the Three VSMs For RQ1, we investigate which one of the three VSMs is the best for building edges and weights of the graph for readability assessment. We assign weights on edges us-ing one VSM (i.e. TF-IDF, LSI or LDA) each time, along with one of the three graph sparsification techniques (i.e. d-k nn, u-k nn and b -matching). Hence, we have 9 different graphs constructed for labe l propagation, each denoted as a combination of the two (e.g. TF-IDF d , LSI u and LDA b ). For each graph, we directly apply the label propagation algorithm to propagate the labels without reinforcing the graph. To make a comprehen-sive comparison, we conduct experiments with variant proportions of the labeled set u-k nn, b for b -matching) of the graphs are set to result in well enough prediction ac-curacies, which will be further discussed in l ater sections. For the Chinese dataset, the threshold is 30 , while for the English dataset, the threshold is 10 .
 Figure 2 shows the measured prediction accuracies for the 9 graphs in line charts. For Chinese, the performances of the graphs using LSI (i.e. LSI d , LSI u and LSI b ) consistently outperform the other graphs. The performances of the TF-IDF sand LDA s are close to each other. An exception is LDA d , which performs better than the other two LDA s, and all the three TF-IDF s. As for the proportion of the labeled set, normally the performance ought to increase as the proportion increases, since more knowledge are obtained. However, for Chinese, this only holds in case of the LSI sand LDA d , which suggests that the others are no better than random predictions.
 For English, the performances of the three LSI s are again better than the other graphs. This time the three LDA s have visible better performances than the three TF-IDF s. Again the graph sparsification techniques have little effects on the prediction accu-racy. The TF-IDF u performs the worst, and no better than random predictions. A phenomenon should be noted that the performances of the LSI s decrease when the pro-portion of the labeled set increases from 0 . 8 to 0 . 9 . This suggests that the graphs built require further improvement.

In summary, among the three VSMs, LSI is the best in building graphs for readabil-ity assessment. The three graph sparsification techniques make little difference to the prediction performance. By the way, the graph requires careful construction, otherwise the results are no better than random predictions. 4.4 Comparison among the Four Strategies For RQ2, we compare the performances of the four strategies of graph reinforcement on the graphs constructed by different gr aph sparsification techniques. Based on the findings in RQ1, we use LSI to compute edge weights. Firstly, we construct graphs with LSI, and using one of the three graph sparsification techniques (i.e. d-k nn, u-k nn and b -matching). Secondly, we reinforce each gr aph using one of the four strategies (i.e. S1  X  S4 ). The graphs without reinforcement (denoted as NS ) are also left for comparison. Finally, we apply the label propagation algor ithm on the graphs to make the predictions. To make a comprehensive comparison, we conduct experiments o n variant settings of the sparse threshold k (or b in b -matching). For the Chinese dataset, k is varied from 4 to 70 step by 3 , and for the English dataset, k is varied from 4 to 40 step by 2 .The maximum k is set according to the size of the dataset. In all the graphs, the proportion of the labeled set is 70% , the parameter a of the three strategies are uniformly set 10 . Figure 3 shows the measured prediction accur acies for the graphs in line charts, where the accuracies of the pre-classificatio n (by logistic regression, denoted as CLF )arealso depicted as baselines.

On the Chinese dataset (the upper three charts in Figure 3, each corresponding to one sparsification technique), the accuracy of CLF is 0 . 4340 . The accuracy of NS shows a rising trend with k , starting from 0 . 3427 ( k =4 ) and becoming stable at about 0 . 4 ( k&gt; 25 ). With b -matching, the accuracy of NS is slightly better than that with d-k nn and u-k nn. This means that without graph reinforcement, CLF is better than the graph-based label propagation for readability assessment. After applying the graph reinforce-ment strategies, our method can outperform CLF with suitable k values. As shown in the upper three figures, S1 outperforms CLF when k&gt; 7 , while it performs worse when k&gt; 25 , and with b -matching, it falls below CLF again. S2 and S4 always outperform CLF , where the optimal k is around 10 . S3 performs well with d-k nn and u-k nn. At starting its accuracy falls below CLF ; but the accuracy rises quickly by k , and becomes the best (about 0 . 475 ) and remains stable from k =25 . Among the three graph sparsi-fication techniques, little difference can be found between Figure 3(a) and Figure 3(b), which demonstrates that the two k NNs perform similarly. In Figure 3(c) however, both S1 and S3 exhibit great differences. S1 does not always outperform CLF now, and S3 drops below CLF at k =16 , and keeps going down when k increases. The reason may be that compared with k NNs, b -matching makes the nodes less different from each other, which can be alleviated by appropriate setting of a .

On the English dataset (the lower three charts in Figure 3), CLF still outperforms NS , that has a relatively stable accuracy with variant k values. This time S3 performs poor, only slightly better than NS in Figure 3(d) and Figure 3(e). This suggests that S3 is not suitable for small number of classes (The English dataset has 4 levels, while the Chinese has 6). The other three strategies have similar trends in 3(d) and Figure 3(e), which rise first, then drop, at last keep stable at larger k values, and remain better than CLF . S2 and S4 perform slightly better than S1 .Again, b -matching makes great differences from the k NNs, where all the strategies exhibit poor performances.

As discussed earlier, the parameter a may have evident effects on the performances of the strategies (i.e. S1 , S2 and S4 ). We make further investigation on a .For S1 and S2 , we repeat the experiments in Figure 3(c) with varying a values, while for S4 ,we repeat the experiments in Figure 3(a). b -matching is repeated twice, since it performs awkward in earlier experiments, while d-k nn is selected for comparison, since S2 and S4 perform similarly. The value of a is set from 2 to 2 11 in logarithmic scale.
As shown in Figure 4(a) and Figure 4(b), S1 and S2 can achieve relatively good and stable performances when a&gt; 16 (although not good at a =10 in Figure 3). The strategies can perform well on b -matching, when the a mplification parameter a is properly set. As shown in Figure 4(c), S4 performs good on d-k nn when a is small, but worse when a&gt; 16 . The above suggests that the strategies are not limited to the k NNs, as long as the parameter a is carefully set.

In summary, all the strategies can improve the performances of both the graphs con-structed for readability assessment, and the pre-classification, when the sparse threshold is properly set for the sparsification technique used. Among the strategies, S1 can be re-placed by S2 or S4, while the latter two perform similarly, and require the amplification parameter a properly set. S3 may not be a good choice when the number of reading levels is small. We cannot find the best strategy that can significantly outperform the others for readability assessment. 4.5 Effects of the Proportion of Labeled Set For RQ3, we study the effects of varying the proportion of the labeled set on the perfor-mance of our proposed method. We conduct expe riments on both English and Chinese datasets, and vary the proportion of the labeled set across 0 . 1 to 0 . 9 step by 0 . 1 .For the graph based method, we implement all the s trategies with suitable settings. All the edges (weights) of the graphs are computed by LSI. The sparsification techniques are selected to suit the strategy, and the sparsification threshold k and amplification param-eter a are set suitable for specific dataset (referring to Figure 3).

For comparison, we implement the support vector machine model (denoted as SVM ), logistic regression model (denoted as LR ) and the Flesch-Kincaid formula (denoted as FK ) respectively, which are commonly used for readability assessment [7,12]. The text features used for training the former two classification models are listed in Table 1. Besides, the indices calculated by LSI are als o included as features for fair comparison with the graph based methods. To develop the Flesch-Kincaid formula for Chinese, we use the number of Chinese characters and strokes instead of English words and syllables. Since the original FK formula is not built specifically on these datasets, and the resulted FK score does not match the reading levels here, we rebuild the formula on both datasets using linear regression. Figure 5 depicts the prediction accuracies of the implemented methods in line charts.

For Chinese, except FK , the prediction accuracies of all the other methods are im-proved when the proportion of the labeled set increases. FK always performs the worst. The reason is that the text features used in the formula may not be suitable to calcu-late directly the reading levels. The graph based method without graph reinforcement performs worse than the two classification models LR and SVM . After applying the graph reinforcement strategies, the graph based method (e.g. S2 k10 a10 ) can result in better performances than SVM and LR , when the proportion of the labeled set is great enough. Here S4 k30 a2 always performs the best. However, as discussed in RQ2, S4 can be outperformed by other strategies. This suggests that the extended graph-based label propagation method has great potential for readability assessment.

For English, again FK performs the worst, and may require enhancement. All the re-inforcement str ategies except S3 k10 can outperform SVM and LR when the proportion of the labeled set is great. However, when the proportion is less than 0 . 3 , SVM performs the best among all. This demonstrates that our method still requires improvement, es-pecially when the proportion of the labeled set is small.

In Summary, for readability assessment, the extended graph-based label propagation method performs well when the proportion of the labeled set is great. However, when the proportion is small, the method is still in need of improvement. In this paper, we propose an extended graph-based label propagation method for read-ability assessment. Three vector space models (VSMs) are employed to compute edges and weights for the graphs, along with three graph sparsification techniques. By in-corporating the pre-classification results, we develop four strategies to reinforce the graphs before label propagation. Based o n the datasets of both Chinese and English, we conduct experiments to investigate the effectiveness of the method. According to the experimental results, we have several findings. Firstly, among the three VSMs, the latent semantic indexing (LSI) is most suitable in building graphs for readability as-sessment. Secondly, all the four strategies can improve the performances of both the graphs constructed and the pre-classification, by proper setting of the graph sparsifica-tion technique and the amplification parameter. Thirdly, the method is still in need of enhancements, when the proportion of labeled documents is small.

During our future work, we plan to work over other options of the method, such as different pre-classification methods, new reinforcement strategies, and suitable graph sparsification techniques. Additionally, other high quality datasets are required to ap-prove the effectiveness of the method.
 Acknowledgments. This work is supported by the National NSFC projects under Grant Nos. 61373012, 61321491, and 91218302.
