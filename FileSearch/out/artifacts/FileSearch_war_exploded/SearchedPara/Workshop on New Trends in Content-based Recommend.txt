 While content-based recommendation has been applied success-fully in many different domains, it has not seen the same level of attention as collaborative filtering techniques have. However, there are many recommendation domains and applications where content and metadata play a key role, either in addition to or instead of rat-ings and implicit usage data. For some domains, such as movies, the relationship between content and usage data has seen thorough investigation already, but for many other domains, such as books, news, scientific articles, and Web pages we still do not know if and how these data sources should be combined to provided the best recommendation performance. The CBRecSys 2014 workshop aims to address this by providing a dedicated venue for papers ded-icated to all aspects of content-based recommendation.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information Filtering ; D.2.8 [ Software Engineer-ing ]: Metrics X  performance measures Algorithms, Experimentation, Human Factors, Theory recommender systems, content-based recommendation, text reviews, user-generated content, implicit feedback, semantics, context
While content-based recommendation has been applied success-fully in many different domains [2], it has not seen the same level of attention as collaborative filtering techniques have. In recent years, competitions like the Netflix Prize 1 , CAMRA 2 , and the Ya-http://www.netflixprize.com/ http://www.dai-labor.de/camra2010/ hoo! Music KDD Cup 2011 [1] have spurred on advances in collab-orative filtering and how to utilize ratings and usage data. However, there are many recommendation domains and applications where content and metadata play a key role, either in addition to or in-stead of ratings and implicit usage data. For some domains, such as movies, the relationship between content and usage data has seen thorough investigation already (e.g. [3]), but for many other do-mains, such as books, news, scientific articles, and Web pages we still do not know if and how these data sources should be combined to provided the best recommendation performance.

The CBRecSys 2014 workshop aims to address this by provid-ing a venue for papers dedicated to all aspects and new trends of content-based recommendation. This would include both rec-ommendation in domains where textual content is abundant (e.g. books, news, scientific articles, jobs, educational resources, and Web pages) as well as dedicated comparisons and combinations of content-based techniques with collaborative filtering approaches.
Relevant topics of the workshop include:
In particular, papers submitted to the the workshop have focused on the following topics. Several papers present hybrid systems combining collaborative filtering and content-based recommenda-tion, finding them complementary, with content-based recommen-dation components especially suitable for tackling the cold-start problem. Other papers investigate how different content features can be used for similarity measures and explore ways to identify which features are the most relevant for a given context. Some papers present approaches to mine user reviews for inferring user preferences on specific attributes of items, essentially deriving more structured feature information from unstructured text. Finally, sev-eral papers look at semantic frameworks and Linked Open Data to measure item similarity across different domains.
To facilitate exploration of the above mentioned topics, the work-shop features an in-workshop challenge on book recommendation. This challenge focuses on recommending new, interesting books to LibraryThing users based on usage data (which books they have added to their collection) and content-based information about the books available in LibraryThing. The rich textual nature of the task makes the challenge an excellent venue to revisit questions about the benefits of content-based filtering vs. collaborative filter-ing, and metadata versus ratings information. At the workshop the evaluation results of the challenge were presented.
For this challenge, a large dataset containing user profiles with book ratings and tags, and 2.8 million book descriptions with li-brary metadata, user ratings, tags, and reviews from Amazon and LibraryThing was be made available.

The dataset for the book recommendation challenge is comprised of two parts: usage data and book metadata. The first part of the dataset for book recommendation is a log of usage data: who added which books to their collection at what point in time. In addi-tion to this, ratings and tags assigned to books are also included in the usage dataset (where available). The user profiles in this data set contain a total number of 1,830,958 unique books added by 78,633 different LibraryThing users, anonymized through dif-ferent privacy-preserving measures. This usage data serves as the main data source for evaluating our challenge, which is described in more detail in Section 3.2.

The second part of our challenge dataset for book recommenda-tion is a collection of metadata records for 2.8 million books cat-aloged on LibraryThing. This collection was crawled from Ama-zon and LibraryThing by the University of Duisburg-Essen in early 2009. From Amazon, there is formal metadata like book title, au-thor, publisher, publication year, library classification codes, Ama-zon categories, and similar product information, as well as user-generated content in the form of user ratings and reviews. From LibraryThing, there are user tags and user-provided metadata on awards, book characters and locations, and blurbs. This part of the challenge data has been used successfully for more retrieval-oriented challenges at the INEX 2011 X 2014 Social Book Search tracks 3 .
The evaluation of the book recommendation challenge follows the familiar backtesting paradigm, where a small number of ran-domly selected books is withheld for each user with the remaining data to be used as training material. If a user?s withheld items are predicted at the top of the ranked result list, i.e., if the algorithm is able to correctly predict the user?s interest in those withheld items, then the algorithm has performed well. The main evaluation metric used in the challenge is the ranking-based metric NDCG@10, as ratings information is sparse in the usage data.

To make evaluation easier for challenge participants and to re-duce the possibility of over-fitting, we divided the usage data into a training and a validation set. The training material contains 90% of the user profiles and is meant for training the participants X  rec-ommendation algorithms. To aid in the learning and parameter op-timization phase on the training material, we performed 10-fold cross-validation on the training set. This resulted in 10 training and test sets, one for each of the folds. We encouraged all participants to use these 10 folds to train their system, so the results of the train-ing phase are directly comparable among participants.

The validation set contains the remaining 10% of the user pro-files (or 7863 users) and was released at a later date its aim was to produce the final comparison of the different submitted approaches. This validation set contains a set of users that are not included in the original training material to avoid over-fitting. To allow par-ticipants to train their systems on these new users as well, we also released a small amount of extra training material corresponding to the users near the end of the challenge.
The workshop material (list of accepted papers, invited talk, and the workshop schedule) ca be found on the CBRecSys 2014 work-shop website at http://ir.ii.uam.es/cbrecsys2014/ .
 The proceedings are published as a CEUR Workshop Proceedings volume. [1] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. The [2] P. Lops, M. de Gemmis, and G. Semeraro. Content-based [3] I. Pil X szy and D. Tikk. Recommending New Movies: Even a https://inex.mmci.uni-saarland.de/tracks/ books/
