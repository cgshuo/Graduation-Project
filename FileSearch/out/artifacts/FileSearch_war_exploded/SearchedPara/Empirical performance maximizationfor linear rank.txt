 In the context of ranking, several performance measures may be considered. Even in the simplest terion, but many possible options. The ROC curve provides a complete description of performance but its functional nature renders direct optimization strategies rather complex. Empirical risk mini-mization strategies are thus based on summaries of the ROC curve, which take the form of empirical risk functionals where the averages involved are no longer taken over i.i.d. sequences. The most popular choice is the so-called AUC criterion (see [AGH + 05] or [CLV08] for instance), but when top-ranked instances are more important, various choices can be considered: the Discounted Cumu-lative Gain or DCG [CZ06], the p -norm push (see [Rud06]), or the local AUC (refer to [CV07]). The present paper starts from the simple observation that all these summary criteria have a common properties in hypothesis testing, see [HS67]. Now, in the statistical learning view, with the impor-tance of excess risk bounds, the theory of rank tests needs to be revisited and new problems come up. The arguments required to deal with risk functionals based on linear rank statistics have been sketched in [CV07] in a special case. The empirical AUC , known as the Wilcoxon-Mann-Whitney statistic, is also a U -statistic and this particular dependence structure was extensively exploited in [CLV08]. In the present paper, we describe the generic structure of linear rank statistics as an or-thogonal decomposition after projection onto the space of sums of i.i.d. random variables (Section mance measures relevant for the ranking problem by showing that the target of ranking algorithms correspond to optimal ordering rules in that sense (Section 3). Eventually, we provide some pre-liminary results in Section 4 for empirical maximizers of performance criteria based on linear rank statistics with smooth score-generating functions. Along the paper, we shall consider the standard binary classification model. Take a random pair ( X,Y )  X  X  X { X  1 , +1 } , where X is an observation vector in a high dimensional space X  X  R d and Y is a binary label, and denote by P the distribution of ( X,Y ) . The dependence structure between X and Y can be described by conditional distributions. We can consider two descriptions: either P = P { Y = 1 | X = x } for all x  X  R d , or else P = ( p,G,H ) with p = P { Y = 1 } being the proportion of positive instances, G = L ( X | Y = +1) the conditional distribution of positive instances and H = L ( X | Y =  X  1) the conditional distribution of negative instances. A sample of size n of i.i.d. L ( X  X  i ) = H , and k + m = n . In this setup, the integers k and m are random, drawn as binomials of size n and respective parameters p and 1  X  p . 2.1 Motivation Most of the statistical learning theory has been developed for empirical risk minimizers (ERM) of sums of i.i.d. random variables. Mathematical results were elaborated with the use of empirical processes techniques and particularly concentration inequalities for such processes (see [BBL05] for an overview). This was made possible by the standard assumption that, in a batch setup, for classifier g : X  X  X  X  1 , +1 } , which is hardly considered in practice because the two populations are rarely symmetric in terms of proportions or costs. For prediction tasks such as ranking or scoring, more involved statistics need to be considered, such as the Area Under the ROC curve ( AUC ), the local AUC , the Discounted Cumulative Gain ( DCG ), the p -norm push, etc . For instance, the AUC , a very popular performance measure in various scoring applications, such as medical diagnosis or credit-risk screening, can be seen as a probability of an  X  X vent of order two X , i.e. depending on sums of i.i.d. random variables by artificially reducing the information available (see [AGH + 05], [Rud06]) or adopt a plug-in strategy ([CZ06]). Our approach is to i) avoid plug-in in order to under-stand the intimate nature of the learning problem, ii) keep all the information available and provide the analysis of the full statistic. We shall see that this approach requires the development of new tools for handling the concentration properties of rank processes , namely collections of rank statis-tics indexed by classes of functions, which have never been studied before. 2.2 Empirical performance of scoring rules The learning task on which we focus here is known as the bipartite ranking problem . The goal of ranking is to order the instances X i by means of a real-valued scoring function s : X  X  R , given the scoring rule s would assign higher ranks to the positive instances (those for which Y i = +1 ) than to the negative ones. The rank of the observation X i induced by the scoring function s is expressed as particular class of simple (conditional) linear rank statistics inspired from the Wilcoxon statistic. Definition. 1 Let  X  : [0 , 1]  X  [0 , 1] be a nondecreasing function. We define the  X  X mpirical W-ranking performance measure X  as the empirical risk functional The function  X  is called the  X  X core-generating function X  of the  X  X ank process X  { c W n ( s ) } s  X  X  . We refer to the book by Serfling [Ser80] for properties and asymptotic theory of rank statistics. We point out that our definition does not match exactly with the standard definition of linear rank statistics. Indeed, in our case, coefficients of the ranks in the sum are random because they involve the variables Y i . We will call statistics c W n ( s ) conditional linear rank statistics . It is a very natural idea to consider ranking criteria based on ranks. Observe indeed that the perfor-mance of a given scoring function s is invariant by increasing transforms of the latter, when evaluated through the empirical W -ranking performance measure. For specific choices of the score-generating function  X  , we recover the main examples mentioned in the introduction and many relevant criteria can be accurately approximated by statistics of this form: 2.3 Uniform approximation of linear rank statistics This subsection describes the main result of the present analysis, which shall serve as the essential distribution function of s ( X ) given Y = +1 , respectively Y =  X  1 . With these notations, the uncon-underlying statistical structure can be revealed by orthogonal projections onto the space of sums of i.i.d. random variables in many situations. This projection argument was the key for the study of em-pirical AUC maximization, which involved U -processes, see [CLV08]. In the case of U -statistics, this orthogonal decomposition is known as the Hoeffding decomposition and the remainder may be decomposition can be considered. We refer to [Haj68] for a systematic use of the projection method for investigating the asymptotic properties of general statistics.
 Lemma. 2 ([Haj68]) Let Z 1 ,...,Z n be independent r.v. X  X  and T = T ( Z 1 ,...,Z n ) be a square T . It satisfies From the perspective of ERM in statistical learning theory, through the projection method , well-known concentration results for standard empirical processes may carry over to more complex col-lections of r.v. such as rank processes , as shown by the next approximation result.
 Proposition. 3 Consider a score-generating function  X  which is twice continuously differentiable VC major class of functions. Then, we have:  X  s  X  X  0 , where b V n ( s ) = P n i =1 I { Y The notation O P (1) means bounded in probability and the integrals are represented in the sense of the Lebesgue-Stieltjes integral. Details of the proof can be found in the Appendix.
 Remark 1 (O N THE COMPLEXITY ASSUMPTION .) On the terminology of major sets and major classes, we refer to [Dud99]. In the Proposition 3 X  X  proof, we need to control the complexity of subsets of the form { x  X  X : s ( x )  X  t } . The stipulated complexity assumption garantees that this collection of sets indexed by ( s,t )  X  X  0  X  R forms a VC class.
 Remark 2 (O N THE SMOOTHNESS ASSUMPTION .) We point out that it is also possible to deal with discontinuous score-generating functions as seen in [CV07]. In this case, the lack of smoothness of  X  has to be compensated by smoothness assumptions on the underlying conditional distributions. An-other approach would consist of approximating c W n ( s ) by the empirical W-ranking criterion where the score-generating function  X  would be a smooth approximation of  X  . Owing to space limitations, here we only handle the smooth case.
 it as a function of the sampling cdf. Denoting by b F s ( x ) = n  X  1 P n i =1 I { s ( X counterpart of F s ( x ) , we have: which may easily shown to converge to E [  X  ( F s ( s ( X )) | Y = +1] as n  X  X  X  , see [CS58]. Definition. 4 For a given score-generating function  X  , we will call the functional a  X  X -ranking performance measure X .
 The following result is a consequence of Proposition 3 and its proof can be found in the Appendix. Proposition. 5 Let S 0  X  S be a VC major class of functions with VC dimension V and  X  be a score-generating function of class C 1 . Then, as n  X  X  X  , we have with probability one: We introduce the class S  X  of scoring functions obtained as strictly increasing transformations of the regression function  X  : The class S  X  contains the optimal scoring rules for the bipartite ranking problem. The next para-graphs motivate the use of W -ranking performance measures as optimization criteria for this prob-lem. 3.1 ROC curves A classical tool for measuring the performance of a scoring rule s is the so-called ROC curve achieved as (  X , ROC( s, X  )) for some scoring function s is called the ROC space.
 It is a well-known fact that the regression function provides an optimal scoring function for the ROC curve. This fact relies on a simple application of Neyman-Pearson X  X  lemma. We refer to [CLV08] for the details. Using the fact that, for a given scoring function, the ROC curve is invariant by increasing transformations of the scoring function, we get the following result: Lemma. 6 For any scoring function s and any  X   X  [0 , 1] , we have: The next result states that the set of optimal scoring functions coincides with the set of maximizers of the W  X  -ranking performance, provided that the score-generating function  X  is strictly increasing. Proposition. 7 Assume that the score-generating function  X  is strictly increasing. Then, we have: Moreover W  X   X  . = W  X  (  X  ) = W  X  ( s  X  ) for any s  X   X  X   X  .
 Remark 3 ( O N PLUG -IN RANKING RULES ) Theoretically, a possible approach to ranking is the plug-in method ([DGL96]), which consists of using an estimate  X   X  of the regression function as a derivative, when  X   X  is close to  X  in the L 1 -sense, it leads to a nearly optimal ordering in terms of W-ranking criterion: However, one faces difficulties with the plug-in approach when dealing with high-dimensional data, see [GKKW02]), which provides the motivation for exploring algorithms based on W-ranking per-formance maximization. 3.2 Connection to hypothesis testing From the angle embraced in this paper, the ranking problem is tightly related to hypothesis testing. Denote by X + and X  X  two r.v. distributed as G and H respectively. As a first go, we can reformu-It is easy to see that the latter statement means that the ROC curve of s dominates the first diagonal of the ROC space. We point out the fact that the first diagonal corresponds to nondiscriminating scoring functions s 0 such that H s 0 = G s 0 . However, searching for a scoring function s fulfilling to be as far as possible from the case where  X  G s = H s  X . This requires to specify a certain measure of dissimilarity between distributions. In this respect, various criteria may be considered such as the L -Mallows metric (see the next remark). Indeed, assuming temporarily that s is fixed and consid-ering the problem of testing similarity vs. dissimilarity between two distributions H s and G s based nonparametric tests based on linear rank statistics have optimality properties. We refer to Chapter 9 in [Ser80] for an overview of rank procedures for testing homogeneity, which may yield relevant criteria in the ranking context.
 and ( X 0 ,Y 0 ) denote independent copies. Furthermore, it may be easily shown that where the cdf F may be taken as any linear convex combination of H s and G s . Provided that H s is possible, even if it means replacing s by F  X  s , which leaves the ordering untouched), the second term may be identified as the L 1 -Mallows distance between H s and G s , a well-known probability metric widely considered in the statistical literature (also known as the L 1 -Wasserstein metric). We now provide a bound on the generalization ability of scoring rules based on empirical maximiza-tion of W-ranking performance criteria.
 Theorem. 8 Set the empirical W -ranking performance maximizer  X  s n = arg max s  X  X  c W n ( s ) . Un-der the same assumptions as in Proposition 3 and assuming in addition that the class of functions  X  1  X   X  : for some positive constants c 1 , c 2 .
 The proof is a straightforward consequence from Proposition 3 and it can be found in the Appendix. In this paper, we considered a general class of performance measures for ranking/scoring which can be described as conditional linear rank statistics. Our overall setup encompasses in particular known criteria used in medical diagnosis and information retrieval. We have described the statistical nature and provided a preliminary generalization bound with a provided the very results on a class of linear rank processes. Further work is needed to identify a variance control assumption in order to derive fast rates of convergence and to obtain consistency under weaker complexity assumptions. Moreover, it is not clear how to formulate convex surrogates for such functionals yet.
 Proof of Proposition 5 By virtue of the finite increment theorem, we have: and the desired result immediately follows from the application of the VC inequality, see Remark 1. Proof of Proposition 3 Since  X  is of class C 2 , a Taylor expansion at the second order immediately yields: with This projection may be splitted into two terms: ( I ) = ( II ) = O
P ( n term may be rewritten as ( II ) = that, uniformly over s  X  X  0 : As F s is bounded by 1 , it suffices to observe that for all i : E  X   X   X  one finally gets that  X  R n ( s ) is of order O P (1) uniformly over s  X  X  0 .
 { P
 X  ( b B n ( s ))  X  ( n  X  1) E [ b B n ( s )] } . Write, for all s  X  X  0 , Hence, we have n which actually corresponds to the degenerate part of the Hoeffding decomposition of the U -statistic b U combined with the basic symmetrization device of the kernel q s , that which concludes the proof.
 Proof of Proposition 7 Using the decomposition F s = pG s + (1  X  p ) H s , we are led to the following expression: Then, using a change of variable: It is now easy to conclude since  X  is increasing (by assumption) and because of the optimality of elements of S  X  in the sense of Lemma 6.
 Proof of Theorem 8 Observe that, by virtue of Proposition 3, and the desired bound derives from the VC inequality applied to the sup term, noticing that it follows from our assumptions that { ( x,y ) 7 X  I { y =+1 }  X  s ( x ) } s  X  X  0 is a VC class of functions. [AGH + 05] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds [BBL05] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classification: A Survey of [CLV08] S. Cl  X  emenc  X on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of [CS58] J. Chernoff and Savage. Asymptotic normality and efficiency of certain non parametric [CV07] S. Cl  X  emenc  X on and N. Vayatis. Ranking the best instances. Journal of Machine Learn-[CZ06] D. Cossock and T. Zhang. Subset ranking using regression. In H.U. Simon and G. Lu-[DGL96] L. Devroye, L. Gy  X  orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition . [Dud99] R.M. Dudley. Uniform Central Limit Theorems . Cambridge University Press, 1999. [GKKW02] L. Gy  X  orfi, M. K  X  ohler, A. Krzyzak, and H. Walk. A Distribution-Free Theory of Non-[Haj68] J. Hajek. Asymptotic normality of simple linear rank statistics under alternatives. Ann. [Hoe48] W. Hoeffding. A class of statistics with asymptotically normal distribution. Ann. Math. [HS67] J. H  X  ajek and Z. Sid  X  ak. Theory of Rank Tests . Academic Press, 1967. [Rud06] C. Rudin. Ranking with a P-Norm Push. In H.U. Simon and G. Lugosi, editors, [Ser80] R.J. Serfling. Approximation theorems of mathematical statistics . John Wiley &amp; Sons,
