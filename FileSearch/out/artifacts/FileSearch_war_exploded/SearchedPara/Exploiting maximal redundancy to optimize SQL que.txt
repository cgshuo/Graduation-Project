 Bin Cao  X  Antonio Badia Abstract Detecting and dealing with redundancy is an ubiquitous problem in query opti-mization, which manifests itself in many areas of research such as materialized views, multi-query optimization, and query-containment algorithms. In this paper, we focus on the issue of intra-query redundancy, redundancy present within a query. We present a method to detect the maximal redundancy present between a main (outer) query block and a subquery block. We then use the method for query optimization, introducing query plans and a new operator that take full advantage of the redundancy discovered. Our approach can deal with redundancy in a wider spectrum of queries than existing techniques. We show experimental evidence that our approach works under certain conditions, and compares favorably to existing optimization techniques when applicable.
 Keywords Intra-query redundancy  X  Nested queries  X  Query optimization 1 Introduction Optimization has always been, and still is, a central topic in database research. The last 30 years have seen a tremendous amount of work in the topic, and that work still continues strong today. This is especially true in the context of Decision Support, where the amount of data keeps growing, queries keep on getting more and more complex, and answers are still expected quickly. SQL, the de facto standard for relational query languages, supports more operations now than they did just a few years ago (cubes, windows, etc.), and this also adds to the complexity. Hence, query optimization continues to be a relevant research topic [ 29 ].
Being a declarative language, SQL relies on optimization techniques to provide responses in a timely manner. However, it is known that complex queries (with subqueries, grouping, and large number of joins) generate so many choices for execution (i.e., possible query plans), that most optimization algorithms cannot guarantee that an optimal plan is chosen. It has been noted that the structure of SQL is itself to blame for the complexity of some queries [ 16 ]. In particular, queries that involve aggregate functions, very common in Decision Support environments, must first compute the aggregate in a subquery before they can use its value in a comparison. For instance, in table ITEM(itemid,supid,price) , where item itemid is supplied by supplier supid at price price , selecting the cheapest item calls for a query with an embedded subquery: select itemid from item where price = (select min(price) The subquery will calculate the price of the cheapest item; the main query will use this information to select the items with such price. Note that both the main query and the subquery will run over the same table, ITEM . This is quite a common situation, since the aggregate is usually computed over data closely related to the data which will use the value of the aggregate for selection. We say that such queries contain intra-query redundancy ,that is, redundancy between a main query block and a subquery block, where redundancy means overlap in the FROM clause and possibly in the WHERE clause. Obviously, queries containing intra-query redundancy are not infrequent. A similar argument is made by Zhu et al. [ 29 ], where the overlapping parts are called  X  X ommon subparts X .

There are several workarounds for this problem: the simplest one is to write the query as a sequence of SQL queries, each one creating a partial solution in the form of a table or view which is later used to compute the final result. However, it is difficult to optimize sequences of SQL queries [ 20 , 23 ]. Extending the SQL language to deal with some of the problems involving aggregation has been proposed [ 2 , 3 ]. The SQL standard group, mindful of the problem, has added the ability to define queries in a WITH clause or inside the FROM subclause, as well as the ability to use a CASE statement in the SELECT clause to define results conditionally. This alleviates, but does not solve, the problem.
 Detecting and reusing repeated effort is related to the general trade-off in Computer Science between time and space: the effort of materializing and storing part of a process that is repeated several times is weighted against the gain of not having to redo the computation. One of the lessons of past research is that simply being aware of (and locating) redundancy is not enough in query optimization. It is well known that even when redundancy is present in SQL queries, the optimal plan may not be the one that gets rid of the redundancy, but one that actually repeats some work. The experiments done by Rao and Ross [ 21 ] are an example of such behavior. Thus, when confronted with redundancy, the challenge for the optimizer is those that avoid it, or somewhere in between are all candidates. Fortunately, in a cost-based framework the problem can be solved by generating all plans and choosing the one with the least cost. However, the presence of redundancy increases the number of possible plans: to the traditional approaches (which do not deal with redundancy) now we have to add plans that are aware of the redundancy and try to suppress it X  X r part of it. We call the traditional plans share nothing , since they repeat any work that is present in both query and subquery. On the other extreme, there are share everything plans, which attempt to do each operation in the query only once. Share everything approaches include the WinMagic technique [ 30 ] and the SegmentApply operator of SQL Server [ 6 ]. In between, there is a number of plans that may share some parts while duplicating others, in an attempt to find the most efficient combination for a given query. In order to analyze the query space efficiently, a cost-based optimizer can proceed as follows: (a) produce the most efficient share nothing plan, using standard optimization techniques; (b) produce the most efficient share everything plan, using some technique like WinMagic or SegmentApply; (c) keep the one with the least cost; and (d) using this least cost as a baseline, use a greedy algorithm to add some sharing (if the best plan was share nothing) or to relax and separate shared parts (if the best plan was share work, one has to make sure that the baseline is truly the least effort that can be accomplished by share nothing or share everything approaches. As stated, the share nothing is the standard approach, which is well understood and developed. However, the share everything approach is more recent and is not really established whether known techniques always provide the best share everything plan. In this paper, we present a new optimization technique in the share everything approach, which uses a new operator (called the for-loop operator) and a slightly different view of shared parts from other techniques. Our experiments have shown that our approach can, in some circumstances, be more efficient than existing techniques when applicable. Thus, our approach can be considered as a complement to existing ones: a cost-based optimizer could use it X  X ogether with other share everything techniques X  X o gather as much evidence as possible that the plan generated in a share everything manner is optimal. This would increase the guarantees that the above strategy would come up with an overall optimal plan. 1
The rest of the paper is organized as follows. In Sect. 2 , we briefly discuss related work in the area of intra-query redundancy optimization and give the motivation for our approach. In Sect. 3 ,wepresenta redundancy detection technique to detect a maximal redundancy in a query. Based on this redundant part, in Sect. 4 , we propose the maximal common block method , which uses all the redundant part and a new operator, the for-loop, which allows efficient computation of several aggregate functions and the predicate that connects a main query and a subquery with one pass over the redundant part. To verify the efficiency of the maximal common block method, we implemented it on top of a leading commercial DBMS, and performed experiments on different kinds of queries derived from the TPC-H benchmark [ 4 ]. In Sect. 5 , we discuss our experiments and detailed performance analysis. Finally, we conclude the paper in Sect. 6 . 2 Related work and motivation One of the most powerful features of SQL is nested queries. A nested query is a query that has another query embedded within it. An embedded query may appear in the FROM clause (called a derived table ), or in the WHERE or HAVING clause (called a subquery ). The query that contains a subquery is called a main query or an outer query (we use the term  X  X he main query X  and  X  X he outer query X  interchangeably in this paper). The subquery can be either aggregate or non-aggregate. If the subquery has an aggregate function( MIN , MAX , SUM , COUNT , AVG ) in its SELECT clause, we say it is an aggregate subquery ; otherwise, we say it is a non-aggregate subquery . The subquery can be either independent of the outer query or correlated to the outer query. A correlated subquery contains a predicate (called a correlated predicate ) that references a table in the outer query; thus, the subquery result depends on each tuple in the outer query. A query is a multi-level query if it has multiple subqueries. A query is a one-level query if all of its subqueries are flat queries which have no subqueries; a query is a two-level queries if at least one of its subqueries is a one-level query; and so on.

Optimization of nested queries has received significant attention since the 1980s. It was originated from the observation thatexecuting nested queries with correlated subqueries using the traditional nested iteration method can be very inefficient [ 13 ]. To improve performance and proposed (e.g., [ 5 , 7 , 13 , 18 , 19 , 24 ]).

Nested queries with aggregate subqueries are widely used in SQL. In some cases, the aggregation in a subquery is computed over the same data (or closely related data) as the data in the outer query. As a result, some SQL queries present a large degree of redundancy in FROM and WHERE clauses between the outer query and the subquery. It is important to point out that redundancy is present because of the structure of SQL, which necessitates a subquery in order to declaratively state the aggregation to be computed. With the addition of user-defined methods to SQL, detecting and dealing with redundancy is even more important, as many time such methods are expensive to compute and it is hard for the optimizer to decide whether to push them down or not [ 12 ]. The following query from the TPC-H benchmark [ 4 ] gives some intuition about this problem.
 Query 1 A query with complete redundancy . This query lists the suppliers who can supply the parts with the desired size and type in a given region at the minimum cost. select s_acctbal, s_name, n_name, p_partkey, from part, supplier, partsupp, nation, region where p_partkey = ps_partkey and
The most noticeable feature of Query 1 is intra-query redundancy because the tables and conditions of the subquery are totally included in those of the outer query. We say it has complete redundancy. As a nested query with a correlated, aggregate subquery, Query 1 However, without redundancy recognized, the common parts (tables and conditions) between the outer query and the subquery have to be computed repeatedly.

Detecting and reusing redundancy underlies many research efforts in the database area: that is common to several queries and therefore may be worth producing once, in advance, and storing it for further use. Work in multi-query optimization [ 22 , 23 ] focuses on finding subexpressions common to several queries and materializing them. In distributed systems, information integration systems, answers to queries are usually constructed by using views, which are closely related to query-containment algorithms for different types of queries [ 17 , 25 ]. For web data extraction and document similarity search, accurately measuring document having intra-query redundancy, that is, redundancy between a main query and a subquery; thus, only the most relevant works are discussed in this section.

The redundancy problem has been studied by several researchers [ 6 , 21 , 22 , 28 , 30 ]. All of them aim to avoid redundancy by computing the common part once and then reusing this part for later processing. Rao and Ross [ 21 ] propose the invariant technique which implements the nested iteration method while considering invariants in the subquery. The basic idea is to recognize the invariant part of the subquery and cache the result of that part after its first execution. Later, the cached result is reused in subsequent executions and combined with the rest of the subquery that is variant for each iteration. However, the invariant technique helps when query rewriting is not applicable. Roy et al. [ 22 ] address redundancy detection and cost-based selection of what to share in the context of multi-query optimization; however, intra-query optimization is slightly different. For the case of subqueries, they only consider reusing nested subquery invariants as in the work of [ 21 ]. Galindo-Legaria and Joshi [ 6 ] solve the redundancy problem in Microsoft SQL Server by generating a common subexpression between the outer query and the subquery first and then using the SegmentApply operator to partition it into segments. The aggregate function is computed on the segment, and then joined with all rows in that segment. This is done one segment at a time. They also consider pushing a join below a SegmentApply operator for more efficient performance. Zuzarte et al. [ 30 ] introduce the WinMagic technique to evaluate queries having redundancy by making use of extended window aggregation capabilities. The WinMagic technique works in a close manner to the technique used in Microsoft SQL Server, but using the window aggregate function instead of the SegmentApply operator. Zhou et al. [ 28 ] propose to exploit similar subexpressions to optimize query batches, nested queries, and maintenance of materialized views. They introduce a mechanism to detect potential sharing opportunities among expressions. Then, for each set of potentially sharable expressions, they construct candidate covering subexpressions (CSE) and use heuristics to prune out less promising candidates. Finally, they resume query optimization to determine which, if any, such subexpressions to include in the final query plan. While these techniques perform better than traditional unnesting approaches due to their consideration of redundancy, they only consider queries with some  X  X estricted X  redundancy. As a more complex example, consider the following query: Query 2 A query with partial redundancy . This query computes the average extended price for the parts with the desired size and type, in which line items were ordered and shipped on a given date before the committed date, and the line item quantity was less than 20% of the average quantity of the line items with a specified available quantity that were shipped, committed, and received in a given order. select sum(l_extendedprice)/7.0 from lineitem, part, orders where p_partkey = l_partkey and
Unlike Query 1 ,Query 2 does not have complete redundancy: the outer query and the subquery still share common tables and common conditions, but extra tables and conditions are present in both the outer query and the subquery. We say such a query has partial redundancy. Furthermore, the join operation between partsupp and lineitem in the subquery is not a lossless join . However, lossless join between common tables and extra tables of the subquery is a requirement of the WinMagic technique. Redundancy may also appear in subqueries in the HAVING clause, in non-aggregate subqueries, or in queries having multiple subqueries. These problems are not covered in existing techniques and need to be carefully addressed.
 In this paper, we deal with complex queries in Decision Support Systems, on top of a Data Warehouses. In this context, queries having intra-query redundancy are still a theoret-ically and practically important problem because some disk-resident data may be accessed more than once, adding slow disk I/O to the query cost -and those accesses could have been avoided, since the data could be reused! Recognizing the importance of this issue, techniques that deal with redundancy have been incorporated into widely used commercial DBMSs such as Microsoft SQL server, IBM DB2, and Oracle. Past research has shown that detecting and  X  X uppressing X  redundancy is worthwhile. Experiments have shown that efficient handling intra-query redundancy can give an approximate 50% performance improvement [ 30 ]. How-ever, existing techniques can only handle queries with  X  X imited X  redundancy. A more general approach that can deal with intra-query redundancy in a wider spectrum of queries than exist-ing techniques is definitely worth studying. Our goal is to establish a more general approach which works on a wider spectrum of queries. Our approach is divided into two parts: first, we propose the redundancy detection technique to detect as much redundancy as possible; second, we introduce the maximal common block method to generate query plans that take into account all the redundancy (like other past approaches), but provides an operator, the for-loop , that is designed to make the most of this situation. We base our discussion on queries containing aggregate subqueries. Later on, we will extend our approach to queries with other types of subqueries. Compared to existing techniques, our approach can be applied to more types of queries having redundancy: queries having complete redundancy, queries having partial redundancy, intra-query redundancy in queries with non-correlated or corre-lated subqueries in the HAVING clause, intra-query redundancy in queries with non-aggregate subqueries, and intra-query redundancy in multi-level queries. As stated in the previous sec-tion, our approach is only one of several possible in the share everything approach, but we will provide experimental evidence that, in some cases, it yields better performance than existing approaches.
In this paper, our example queries are directly or derived from the TPC-H benchmark [ 4 ], which illustrates decision support systems that examine large volumes of data, execute queries with a high degree of complexity, and give answers to critical business questions. The queries are intended to model those commonly encountered in practice in Decision Supportenvironments.Webelievethatourexamplequerieshavebothpracticalandtheoretical meaning to support our proposed solutions. 3 Redundancy detection technique To avoid repeated efforts in evaluating queries having redundancy, the first and the most important thing we should do is to recognize the common part between the outer query and the subquery. We point out that it may be very difficult to determine what goes into a common part. Just because some tables appear in the FROM clause of outer query and subquery, it does not automatically mean that they are in the common part. Also, correlation introduces some nuances. Consider, for instance, the following simplified example over a synthetic database: select * from R, S where R.A = S.B and An example can be built easily to show that, since tables R and S join on different conditions tables only once and allows us to compute the example. In other words, this query has no common part . Even when a common part exists, some difficult problems remain: extra tables (and joins) in query and subquery may alter the common part by introducing duplication or loss of tuples, in which case extra care is needed in order to respect the semantics of the original SQL query. In order to deal with such cases, we introduce the redundancy detection technique that is able to determine what constitutes a legitimate common part, and how to add to it other components of a query. How to detect common subqueries and exploit them in complex query optimization has been studied by Zhu et al. [ 29 ]. They introduce the query graph as a logical representation of a given query and the ring network representation (the ring network, candidate node lists and bitmaps) as an implementation representation of the query graph. Based on the ring network representation, they propose an algorithm to identify common subqueries and generate an efficient execution plan for a given complex query. Our query structure detection technique is based on the work done by Zhu et al. [ 29 ], but for our purpose, their work needs some extension. We use a query pattern to represent the structure of a given query. The query pattern can be populated later based on a query graph , the logical expression of a given query. 3.1 Query patterns A query pattern is a schematic representation of a query using keywords SELECT , FROM , WHERE (and optional GROUP BY , HAVING ), and variables over tables, aggregate functions and conditions. Note that a query pattern represents a class of queries which have similar structures. For instance, a typical one-level nested query with an aggregate subquery can be expressed by the following query pattern (we assume that there are only SELECT , FROM , and WHERE clauses):
Where F ( attr ) denotes an aggregate function F over an attribute attr ; attr list denotes Both attr list and F ( attr ) may appear simultaneously in the SELECT clause if the query has a GROUP BY clause. T 1 and T 2 denote a list of table instances 2 in the FROM clauses 3 of the outer query and the subquery respectively. C 1 and C 2 denote conditions in the WHERE clauses 4 of the outer query and the subquery respectively. We introduce the term linking pred-icate to refer to the predicate that connects an outer query and its subquery. For a query with the linking operator . If a subquery appears in the HAVING clause, the left operand of the linking operator might be an aggregation over the linking attribute; we call such an aggregation linking aggregation ( F linking ). The linking predicate may take the form of F
Note that the above query pattern is just an example query pattern for a nested query with an aggregate subquery. If the query has GROUP BY clause and/or ORDER BY clause, the query pattern should be extended using the above definition.
 Example 1 Obviously, Query 2 matches the above query pattern, in which: F ( attr ) : sum(l_extendedprice)
T 1 :{ lineitem , part , orders }
T 2 :{ lineitem , partsupp }
C 1 :{ p_partkey=l_partkey ,
C 2 :{ l_partkey=p_partkey ,
Note that the linking attribute l_quantity is from lineitem in the outer query, and the linked attribute l_quantity is from lineitem in the subquery.

For queries having redundancy, there must be at least one common table (table instances T can be classified into two parts: common tables ( T common ), which are tables common to the outer query and the subquery; and outer tables ( T out er ), which are tables in the outer query except common tables. Accordingly, C 1 falls into three parts: common outer condi-tions ( C common  X  out er ), which are conditions involving common tables in the outer query; outer conditions ( C out er ), which are conditions involving outer tables; and common outer link ( L common  X  out er ), which are conditions linking common tables and outer tables. Simi-larly, T 2 can also be classified into two parts: common tables ( T common ), which are tables subquery except common tables. Correspondingly, C 2 falls into three parts: common inner conditions ( C common  X  inner ), which are conditions involving common tables in the subquery; inner conditions ( C inner ), which are conditions involving inner tables; and common inner link ( L common  X  inner ), which are conditions linking common tables and inner tables. How to further classify T 1 , C 1 , T 2 ,and C 2 will be discussed in next two subsections.
Based on the above notations, a typical one-level nested query having redundancy with an aggregate subquery in the WHERE clause and in the HAVING clause can be expressed by the query patterns shown in Fig. 1 a and b, respectively. For simplicity, we based our discussion which we will extend our technique to cover more complex queries; accordingly, the query pattern should also be extended. 3.2 Query graphs The logical structure of a given query can be represented by a query graph . In a query graph, block at that level. For instance, for a query with only one subquery, the outer query is at the second subquery is at level l 22 . We create one node for each table instance in the FROM label each node with the table name. If there are join conditions between two table instances, attributes of the primary key of a table, the edge should be directed from the node without the primary key involved to the node with the primary key involved; if join conditions involve two primary keys of the two tables, the edge should be bidirectional. If there are selection conditions on a table instance, we add a loop edge around the node (denoted by s n ij ,where n ij is a node) and label the edge with the selection conditions. Directed edges together with the loop edges at the subquery level is used to determine whether a join is a lossless join or not. If an edge is directed and no loop edge is around the node which is directed to, the join is a lossless join.

A linking predicate is denoted by P linking . To represent the linking predicate, we add a dashed line between the tables contain the linking attribute and the linked attribute (denoted with the linking predicate.

A correlated predicate is represented by an edge across different levels (denoted by e which should be directed in the presence of the primary key. The table of the outer query in the subquery is called the correlated attribute ( attr corr elated ).
 Example 2 The query graph of Query 2 is shown in Fig. 2 . l 1 and l 2 denotes the first level and the second level respectively. Since p_partkey is the primary key of part , the edge between part and lineitem at l 1 and the edge across l 1 and l 2 are directed edges from lineitem to part . Similarly, the edge between orders and lineitem is a directed edge from lineitem to orders due to o_orderkey , the primary key of orders .But the edge is not bidirectional because l_orderkey is only one attribute of the primary key of lineitem (the other attribute is l_linenumber ). Note that the edge between lineitem and partsupp at l 2 is not directed because l_suppkey is one attribute of the foreign key and ps_suppkey is one attribute of the primary key (the other attribute is ps_partkey ). Clearly, the join between lineitem and partsupp is not a loss-less join. Since both the linking attribute and the linked attribute l_quantity are from lineitem ,tomakeitclear,weuse li1 to denote lineitem in the outer query, and li2 to denote lineitem in the subquery. Thus, the linking predicate is li1.l_quantity &lt; 0.2*avg(li2.l_quantity) . 3.3 Populating query patterns We use a query pattern to represent the structure of a given query. The advantage of using a query pattern is that it can represent a class of queries that have similar structure. Once we have an approach to processing the query pattern, all queries that match the pattern can be processed in the same way. The next question is how a query matches a query pattern. In other words, for a given query, how can we populate its query pattern? To answer this question, we can use the query graph and follow three steps below (see Algorithm 1 ):
First, we check if at least one node (table instance) is common between two adjacent levels, edges between the common node and the nodes involved in P corr elated (correlation nodes) in the outer query. If the answer is yes, we put correlation nodes and the common node into T common ; the loop edges around correlation nodes and the common node in the outer query, and P corr elated into C common  X  out er ; and the loop edge around the common node in that means the subquery is not correlated to the outer query. We only need to find common nodes.

Second, to find other common nodes, we start from the known common node and check the nodes linked to it. If there are two identical nodes between two adjacent levels and each of them is linked to the common node with the same edge, we put that node into T common ;the We keep this process until we find all common nodes.
 Third, once we obtain common nodes, we can simply identify outer nodes and inner nodes. For the outer query level, we put the nodes except common nodes and correlation nodes into T and the edges linking common nodes and outer nodes into L common  X  out er . Similarly, for the subquery level, we put the nodes except common nodes into T inner ; the edges linking inner nodes and the loop edges around each inner node into C inner ; and the edges linking common nodes and inner nodes in L common  X  inner .
Algorithm 1 : Algorithm for Populating Query Patterns
It is more complicated for queries with multiple subqueries at the same level. There are two possibilities. First, if the outer query and each subquery share common tables and conditions. For instance, the outer query has some things in common with the first subquery, and has another things in common with the second subquery; but these two subqueries do not share common things. In this case, we can follow the Algorithm 1 to compute the common part between the outer query and each subquery respectively. Second, if the outer query and all subqueries share common tables and conditions, then we need to find common part among subqueries first and then follow the Algorithm 1 to compute the common part between the outer query and the resulting common part from several subqueries. Note that the common part is shrinking as more query blocks are considered, which may reduce the effectiveness of optimization utilizing redundancy. We need to balance the number of query blocks considered and the size of the common part. Under cost-based optimization, the system enumerates different query plans based on different common parts and the optimizer can choose the best one with the least cost. Example 3 Query 2 matches the query pattern shown in Fig. 1 a. Based on its query graph shown in Fig. 2 ,wehave:
Note that P corr elated is { l_partkey=p_partkey }. Also note that p_size=15 and p_type like  X %BRASS X  are the selection conditions on correlation table, which will be used later to get the exactly common part between the outer query and the subquery (denoted 3.4 Summary DBMS receives SQL queries either from end users inputting through a DBMS interface, or from applications where SQL queries are embedded. The database system treats both types of queries equally: the query processor compiles, translates, and optimizes each query through several phases into an executable query plan, executes the plan, and outputs the result. From this point of view, the redundancy detection technique should be inside the query processor: it should be executed immediately after the compiling stage. If redundancy is found between the outer query and the subquery, the query pattern can be populated, and a new query plan (using the for-loop operator (to be discussed later), or the window aggregation) will be generated.
Our redundancy detection technique is based on the work done by Zhu et al. [ 29 ], and they are very similar (both of them use a similar query graph to represent the logical structure of a given query). However, we need to modify their approach simply because of a difference in purposes: the purpose of their work is to find any common subqueries, for instance, the subquery in the FROM clause, while we only need to find the common subexpressions between the query graph by: 1. Adding level to each node, which denotes the query block a table belongs to. 2. Since we always deal with queries with (correlated) aggregate subqueries (non-aggregate 3. We also add directed edges to represent join conditions that involve all attributes of the
Since we have extended the query graph for our purposes, the ring network [ 29 ]also needs to be extended. First, we need to include a query block level for each node. Second, a directed edge can be represented by a Boolean variable for the node that the edge is directed we need to add the linking attribute and the linking operator to the node that represents the table in the outer query a linking predicate involves. Similarly, we need to add the linked aggregation and the linked attribute to the node that represents the table in the subquery a linking predicate involves. We also need to add pointers between these two nodes.
As for cost, clearly there has to be appropriate data structures and algorithms to support the redundancy detection technique. Since our work is based on the work done by Zhu et al. just add more details but not more complexity. According to their cost analysis, the total time complexity to construct a ring network representation for a given query in the worst case is O ( m + n 2 ) ,where n is the number of table instances referenced in the query, and m is the number of join predicates in the query. The time complexity of the implementation algorithm is at most O ( n 4 ) . The time complexity is hence polynomial and acceptable -note that the parameters tend to be relatively small. 4 Maximal common block method The basic idea of the maximal common block method is straightforward: the conditions and tables in the outer query and the subquery are divided into three parts: one that is common in both the outer query and the subquery, one that belongs only to the outer query, and one that belongs only to the subquery. Based on these three parts, a query can be evaluated as follows: first, we create a base relation based on common tables and common conditions; second, starting from the base relation, we compute the aggregation in the subquery based on the tables and conditions belonging only to the subquery; finally, we generate the desired result based on the tables and conditions belonging only to the outer query and the subquery result. 4.1 Generating base relations Even though we have C common  X  out er and C common  X  inner obtained by the redundancy detec-tion technique, what conditions exactly common to the outer query and the subquery still ent conditions. We define the base relation , denoted by BR , as the exactly common part (tables and conditions) between the outer query and the subquery. The common table can be obtained from T common , and the common conditions can be obtained by the following formula: edges around correlation nodes. Therefore, by performing intersection of C common  X  out er and C common  X  inner , C base includes the common conditions between C common  X  out er and C common  X  inner . Except the common conditions, there are other two kinds of conditions: the and the conditions belonging only to the common tables in the subquery (denoted by C base  X  inner , which can be computed by the following formulas: aggregation, and C base  X  out er is used to compute the final result.

Once we have the common tables and common conditions between the outer query and the subquery, the base relation can be simply obtained by performing selection C base on T WHERE C base
Note that: (1) A C or A L denotes the attributes of T common in C or L .(2)If attr linking and attr linked are the same, only one of them appears. (3) For correlated queries, if the linked aggregation is COUNT , the join operation should be replaced by the left outer join operation to avoid the zero count bug [ 7 ]. (4) For correlated queries, if the join of common tables and COUNT , AVG and SUM ), the primary key of the outer query should be retrieved in the base relation. (5) The desired attributes for later processing are listed in the SELECT clause.
If T inner is not empty, which means there are extra tables in the subquery, we can extend the base relation to the extended base relation , denoted by EBR , which includes inner tables to the base relation. Generally, to obtain the extended base relation, we perform a left outer can be pushed down.

Note that: (1) # denotes the primary key. (2) [ attr linked ] denotes an optional attribute. (3) If L common  X  inner is a lossless join, we only need to perform a join between T common and T inner , which is the strategy of the WinMagic technique to handle the extra tables in the subquery. The lossless join can be determined by the query graph where an edge pointed to the node is directed and there is no loop edge around the node.
 Performing an outer join of BR and T inner might cause tuples in BR duplicate if L common  X  inner is not a lossless join. Such duplicates are required to compute the linked aggregation, but will cause an error when computing the final result. The solution to this problem is to use EBR to compute the linked aggregation, and then reduce EBR to BR to compute the outer query. There are two solutions to reduce EBR to BR depending on where before computing the linked aggregation by adding a GROUP BY clause and an aggregate function COUNT( T inner . #) in the SELECT clause. Note that the computation of the linked aggregation is affected by COUNT( T inner . #). On the other hand, if the linked attribute is from T inner , the linked aggregation should be computed first over EBR ,andthen EBR can be reduced by adding a GROUP BY clause and an aggregate function COUNT( T inner . #) in the SELECT clause. With respect to the SQL syntax, the grouping attributes might include the primary key of BR as well as other attributes in the SELECT clause except the attributes Example 4 The base relation BR for Query 2 is obtained by the following query: select p_partkey, l_quantity, l_extendedprice, l_shipdate, l_commitdate, l_receiptdate, l_suppkey,l_orderkey, l_linenumber from lineitem, part where l_partkey = p_partkey and p_size = 15 and p_type like  X %BRASS X  and l_shipdate&lt;l_commitdate
Since T inner is not empty, the extended base relation EBR for Query 2 is obtained by the following query: select p_partkey, l_quantity, l_extendedprice, l_shipdate, l_commitdate, l_receiptdate, l_suppkey,l_orderkey, l_linenumber, count(ps_suppkey) from br left outer join (select ps_suppkey group by p_partkey, l_quantity, l_extendedprice, l_shipdate, l_commitdate, l_receiptdate, l_suppkey,l_orderkey, l_linenumber Note that EBR is not only used to compute the linked aggregation (affected by count(ps_suppkey) ), but also used to compute the final result. 4.2 The for-loop operator Once we obtain the (extended) base relation, the subsequent computations of the linked aggre-gation and the linking predicate can be done efficiently by only one pass over the (extended) base relation. In order to do so, we define a new operator, for-loop , which combines several defined in the literature on query optimization, which introduce special-purpose relational frequently together and they could be more efficiently implemented as a whole. In our par-ticular case, we show later that there is an efficient implementation of the for-loop operator which allows it, in some cases, to compute several basic operators with one pass over the data, thus saving considerable disk I/O. In the following, GB is used to indicate a group-by operation, and AGG F ( A ) ( R ) indicates the aggregation F computed over all values of the attribute A of the relation R . Definition 1 Let R be a relation, sch ( R ) the schema of R , L  X  sch ( R ) , A  X  sch ( R ) , F is defined in two variants:
The main use of a for-loop operator is to compute the linked aggregation and the linking predicate on the fly, possibly with additional selections. In a word, for each group of L ,the for-loop operator computes F ( A ) based on  X  , then filters out tuples that satisfy  X  . Example 5 According to the above definition, the for-loop operator used for Query 1 is FL L , F ( A ), X , X  ( BR ) ,where L is p_partkey , F ( A ) is min(ps_supplycost) ,  X  is empty,  X  is ps_supplycost=min(ps_supplycost) .

As a more complete example, the for-loop operator used for Query 2 is FL L , F ( A ), X , X  (
EBR ) ,where L is p_partkey , F ( A ) is avg(l_quantity) ,  X  is l_commitdate &lt; l_receiptdate and count(ps_suppkey) &lt;&gt; 0 ,  X  is l_shipdate &gt; =  X 1994-01-01 X  and o_orderkey is not null and l_quantity &lt; 0.2*avg (l_quantity) . 4.2.1 Implementation of the for-loop operator The main reason to define the for-loop operator is to compute several results at once with a single pass over the data. To achieve this objective, the operator can be implemented as an iterator that loops over the input implementing a simple program (hence the name). The naive algorithm for implementing the for-loop operator FL L , F ( A ), X , X  ( BR ) is: we bring a list of tuples with the same values of L into memory, then we scan these tuples once to compute F ( A ) based on  X  , and then go back to the first tuple to compute  X  until all these tuples have been computed. Fortunately, the implementation of the for-loop operator can be more efficient by taking use of special features of some aggregates. The basic idea is twofold: first, selections and groupings (either grouping alone or together with aggregate calculations) can be effectively implemented in one algorithm, even if it is algebraically an aggregation and using the aggregate result in a selection can be done at the same time. This is due to the behavior of some aggregates and the semantics of the conditions involved. Assume, for instance, that we have a comparison of the type attr = min(attr2) ,where both attr and attr2 are attributes of some table R . In this case, as we go on computing the minimum for a series of values, we can actually decide, as we iterate over R ,whether some tuples will make the condition true or not ever . This is due to the fact that min is monotonically non-increasing , i.e., as we iterate over R and we carry a current minimum ,this value will always stay the same or decrease, never increase. Since equality imposes a very strict constraint, we can take a decision on the current tuple t based on the values of t.attr and the current minimum, as follows: if t.attr is greater than the current minimum, we can now, in a temporary result temp1 ;if t.attr is less than the current minimum, we should keep it, in case our current minimum changes, in a temporary result temp2 . Whenever the current minimum changes, we know that temp1 should be deleted, i.e., tuples there cannot be part of a solution. On the other hand, temp2 should be filtered : some tuples there may be thrown away, some may be moved to a new temp1 ,somemayremainin temp2 .At the end of the iteration, the set temp1 gives us the correct solution. Of course, as we go over the tuples in R we may keep some tuples that we need to get rid of later on; but the important point is that we never have to go back and recover a tuple that we dismissed, thanks to the monotonic behavior of min . This will allow us to implement the operation efficiently. This behavior does generalize to max, sum, count , since they are all monotonically non-decreasing (for sum , it is assumed that all values in the domain are positive numbers); however, average is not monotonic (either in an increasing or decreasing manner), but can be dealt with by computing sum and count .

Of course, a different operator dictates a different behavior, but the overall situation does not change: we can successfully take decisions on the fly without having to recover discarded attr &lt; min(attr2) ) , then the procedure would be different: if t.attr is greater than or equal to the current minimum, we can safely ignore t ;andif t.attr is smaller than the current minimum, we should keep it in a temporal result temp1 . Whenever a new (lower) minimum is discovered, we need to filter out elements of temp1 , discarding some values. Furthermore, when a subquery is correlated, we use the grouped for-loop and the aggregate computation is done per group. That means that results (and temporary results) are only accumulated by group, and therefore all temporary information needed is likely to fit in memory and performance may be good even for cases where the condition is hard to compute. 4.2.2 Comparison with other operators Thefor-loopoperatorissimilartotheSegmentApplyoperator[ 6 ]andthewindowaggregation [ 30 ] in that all of these operators are used to do computation based on the common part between the outer query and the subquery. In the work of [ 6 ], the common part between an outer query and its subquery is computed once. Then the result is partitioned into segments, and each segment is input to the SegmentApply operator, where the aggregation in the aggregation computes the linked aggregation over the linked attribute for each group of the correlating attribute, and attaches the corresponding result to each tuple of the common part. Later, the linking predicate is evaluated by comparing the linking attribute to the attached value. Both the for-loop operator and the SegmentApply operator are extended relational operators which have to be implemented inside the DBMS. The window aggregation is one of the analytic features defined as part of the ANSI SQL 1999 standard and has been implemented in some commercial DBMS. Thus, the query can be rewritten in SQL with the redundancy considered by using the window aggregation and the WITH clause which declares the common part ( BR or EBR generated by the maximal common block method can be used to compose the WITH clause). The for-loop is also related to the extended grouping operator [ 2 ], in that such operator is able to compute several aggregates, each one depending on a different condition, at once, as far as the grouping attributes are the same; the for-loop has the same ability.

From a performance point of view, the for-loop operator may perform very efficiently due only one pass over the (extended) base relation, especially for aggregations MAX and MIN when the linking operator is  X  =  X  . Furthermore, it can be easily extended to handle different types of queries, as will be shown later. 4.3 Evaluation plan Based on the (extended) base relation and the for-loop operator, the execution plan for queries having redundancy can be created by the following three steps: 1. Wecreatethe(extended)baserelation [ E ] BR (wherethesquarebracketdenotesoptional). 2. For queries with correlated subqueries, we apply a grouped for-loop operator, Example 6 To generate a query plan for Query 2 using the maximal common block method, we follow three steps: 1. Create EBR , which is shown in Example 4 . 2. Apply the grouped for-loop operator, which is shown in Example 5 . 3. Perform join of EBR and orders and compute the aggregation The query tree is shown in Fig. 3 .Wedenoteprojectionby  X  , selection by  X  ,joinby ,left outer join by , group-by by GB , and the for-loop operator by FL . 4.4 Cost analysis The cost of a query plan is defined as the sum of the cost of every individual operator appearing in the plan. Since we deal with complex queries in data warehouse and decision support systems that have very large volumes of data stored on disk, we only consider disk I/O cost (in terms of the number of page I/Os). Clearly, the cost of the plan generated by the maximal common block method is equivalent to the sum of each step X  X  cost.

The cost of the first step is equivalent to the cost of generating the base relation or the is equivalent to the cost of joining all these tables. The actual cost depends on which join of (outer) join of BR and T inner to generate the extended base relation, and plus the cost of generating the extended base relation, which also depends on which join algorithms are used. A group-by can be implemented either by hashing or sorting. Here we assume that sorting is costs 2 Mlog B  X  1 M if a ( B  X  1)-way merge sort is used [ 13 ]. Assume that the size of the extended base relation is M EBR , the cost of a group-by is 2 M EBR log B  X  1 M EBR . Thus, the cost of the first step is: Where the square bracket denotes optional.

The cost of the second step is equivalent to the cost of the for-loop operator. Assume the sizeofthebaserelationis M BR ,thecostoftheflatfor-loopoperatoris M BR ,andthecostofthe if the base relation is not sorted.

The cost of the third step depends on whether there are extra tables in the outer query or not. If there are no extra tables, the final result can be obtained by pipelining to the second of BR and T out er to produce the final result.
 In summary, the cost of the maximal common block method is: 4.5 Extensions In this section, we consider three main extensions of the maximal common block method: subqueries in the HAVING clause, non-aggregate subqueries, and multi-level queries. Clearly, the for-loop operator can be easily extended to deal with several aggregations simultaneously, each one with its own conditions, as far as the grouping attributes are the same for all aggre-where the square bracket denotes optional. This definition is very similar to the multidimen-sional join (MD) operator [ 1 ], which can compute several aggregations based on their dif-ferent conditions at the same time. Unlike that work, though, the for-loop operator is aware of redundancy and works on the (extended) base relation, while the MD operator does not consider redundancy and have to access common tables more than once. 4.5.1 Subqueries in the HAVING clause If a query has a subquery in the HAVING clause, and both query blocks have overlap, it can be handled by the maximal common block method. The for-loop operator is different depending on whether the subquery is correlated or not.

When the subquery is not correlated, any aggregates are computed over a whole base relation. The essential observation is that we can compute any aggregates even if we group the tables by the GROUP BY attributes. The only thing that is needed is to continue the computations, without resetting, across groups (any aggregates in the outer query can be computed per group, as indicated earlier). This way, we can create the groups needed for the final result and compute any needed aggregates at the same time. (assuming there are common tables in main query and subquery, of course). After each group is computed, we can compare the result of the global aggregate so far to the new group, and proceed as in the regular case. That is, we only need to compare once per group. The for-loop operator refers to P linking .

When the subquery is correlated, it can only correlate to the GROUP BY attributes. The same, and others have the same meaning as above.
 Example 7 Consider the following query: Query 3 A query with a non-correlated subquery in the HAVING clause . This query lists the part number and the value of the parts supplied by a given nation, which represent a significant percentage of the total value of all available parts. select ps_partkey, from partsupp, supplier, nation where ps_suppkey = s_suppkey and group by ps_partkey having sum(ps_supplycost*ps_availqty) &gt; (select sum(ps_supplycost*ps_availqty)*[F] Query 3 is the Query 11 in the TPC-H benchmark, which is a query with a non-correlated aggregate subquery in the HAVING clause. The FROM clause and the WHERE clause are the same in the subquery and outer query. In the HAVING clause, both the linking aggre-gation over the linking attribute and the linked aggregation over the linked attribute are sum(ps_supplycost*ps_availqty) . The former is based on each group of ps_partkey , while the latter is based on the entire base relation. Thus, the linked aggre-gation can be computed on the fly with the computation of the linking aggregation, and only the linking aggregation of each group has to be cached for further comparison to solve the linking predicate. The base relation is: select ps_partkey, ps_supplycost, ps_availqty from partsupp, supplier, nation where ps_suppkey = s_suppkey and s_nationkey = n_nationkey and n_name =  X  X ERMANY X  both F 1 ( A 1 ) and F 2 ( A 2 ) are sum(ps_supplycost*ps_availqty) ,  X  1 and  X  2 are empty,  X  is F 1 ( A 1 )&gt; F 2 ( A 2 )  X  X  F ] .
 Example 8 Consider the following query: Query 4 A query with a correlated subquery in the HAVING clause . This query lists the part number and the value of the parts with the desired size and supplied by a given nation, which represent a significant percentage of the total value of the qualifying parts with a specified available quantity. select p_partkey, from part, partsupp, supplier, nation where p_partkey = ps_partkey and group by p_partkey having sum(ps_supplycost*ps_availqty) &gt; Query 4 is derived from Query 3 by adding part into the FROM clause of the outer query, and the correlated predicate p_partkey=ps_partkey into the subquery. Note that both the correlating attribute and the GROUP BY attribute are p_partkey ; thus Query 4 can be evaluated using the maximal common block method. Different from Query 3 , both the linking aggregation and the linked aggregation of Query 4 are based on each group of p_partkey . Thus, the for-loop operator computes both aggregations simultaneously and computes the linking predicate immediately after knowing the results of these aggregations. The base relation is: select p_partkey, ps_supplycost, ps_availqty from part, partsupp, supplier, nation where p_partkey = ps_partkey and p_size &gt;=10 and p_size &lt; 20 and ps_suppkey = s_suppkey and s_nationkey = n_nationkey and n_name =  X  X ERMANY X  p_partkey , F 1 ( A 1 ) and F 2 ( A 2 ) are sum(ps_supplycost*ps_availqty) ,  X  1 is empty,  X  2 is ps_availqty&gt;5000 ,and  X  is F 1 ( A 1 )&gt; F 2 ( A 2 ) . 4.5.2 Non-aggregate subqueries A non-aggregate subquery is linked to an outer query by one of the following operators: EXISTS , NOT EXISTS , IN , NOT IN ,  X  SOME/ANY ,and  X  ALL ,where  X   X  X  &lt;,  X  ,&gt;,  X  , = , =} ; the result is a set of values (maybe empty). The maximal common block method does not apply directly to queries with non-aggregate subqueries. Fortunately, such queries can be rewritten as queries with aggregate subqueries, in particular with the COUNT aggregate (since COUNT is monotone, it adapts well to our approach). Such rewrites must be carefully specified. For EXISTS , NOT EXISTS , we rewrite the subquery into an aggregate subquery with COUNT , which is compared to zero. For SOME/ANY ( IN is equivalent to  X = SOME  X ), we rewrite the subquery into an aggregate subquery with COUNT , where the subquery is exactly as before but with the linking predicate added to its WHERE clause. For queries with a quantified comparison using ALL ( NOT IN equals  X  &lt;&gt; ALL  X ), let the SQL query condition be attr1  X  ALL (select attr2...) . Then the query can be rewritten as (select count(attr2)...)=(select (count(attr2)...) , where the first subquery is exactly as it was in the original, and the second one is also the same as the original but has the predicate attr1  X  attr2 added to the WHERE clause (this approach is basically equivalent to that of [ 1 ]). Note that, as rewritten, this query may not be very efficient to compute. In our approach, though, this transformation is followed by an analysis of overlap and a for-loop operator. In particular, we are introducing two subqueries which are guaranteed to contain considerable overlap, that our approach will detect and take care of. Our final plan as such it delivers excellent performance in universally quantified queries.
 Example 9 Consider the following query: Query 5 A query with an ALL subquery . This query lists the orders ordered on a given date, in which line items were shipped, committed, and received in a given order and the total price of the order was greater than all of the extended price of the qualifying line items received on a given date. select o_orderkey, o_orderpriority, l_quantity from orders, lineitem where l_orderkey = o_orderkey and Query 5 is a query that has a non-aggregate and correlated subquery with the linking operator ALL . Before using the maximal common block method, the non-aggregate subquery should be transformed to two aggregate subqueries using COUNT based on different conditions with an equality condition between them. The transformed query is: select o_orderkey, o_orderpriority, l_quantity from orders, lineitem where l_orderkey = o_orderkey and
When generating the base relation, we use a left outer join of the common table and the correlation table to avoid the count bug . The base relation is: select o_orderkey, o_orderpriority, o_totalprice, from (select o_orderkey, o_orderpriority, are o_orderkey , F 1 ( A 1 ) and F 2 ( A 2 ) are count(l_orderkey) ,  X  1 is l_receiptdate&gt;= X 1993-01-01 X  and l_receiptdate&lt;  X 1994-01-01 X  ,  X  2 is l_receiptdate&gt;=  X 1993-01-01 X  and l_receiptdate&lt; X 1994-01-01 X  and o_totalprice&gt; l_extendedprice ,  X  is F 1 ( A 1 ) = F 2 ( A 2 ) . 4.5.3 Multi-level queries The maximal common block method can also be extended to queries of arbitrary depth. When considering such queries, we distinguish between linear queries (where there is at most one subquery in any given level) and tree queries (where in some level there are two or more subqueries). The maximal common block method can be extended for linear queries by applied from the bottom up, starting at the innermost subquery. Once this step is done, one can proceed up the query until the outermost block. This folding process is similar to the one used in regular unnesting.

Tree queries can also be taken care of, but require some additional care. Assume, for simplicity, that query block B has two subqueries, B 1 and B 2 (the argument that follows can be extended to several subqueries without problem). Assuming that some overlap exists (otherwise our method cannot be applied), we consider two cases:  X  If B i has some overlap with B ,and B j does not ( i , j = 1 , 2 ; i = j ), the B j can be  X  If both B 1 and B 2 have overlap with B , we further distinguish two cases: if B 1 and B 2 Example 10 Consider the following query: Query 6 A query with two subqueries at the same level . This query computes the sum of lineitem quantity from the orders ordered on a given date, in which line items were shipped, committed, and received in a given order, and the extended price was less than the maximum was returned. select sum(l_quantity) from orders, lineitem where l_orderkey = o_orderkey and By transforming the NOT EXISTS subquery into an aggregate subquery using COUNT ,the maximal common block method requires only one access to lineitem . The transformed query is: select sum(l_quantity) from orders, lineitem where l_orderkey = o_orderkey and
To avoid the count bug, a left outer join of orders and lineitem is used to compute the base relation. For each group of o_orderkey , MAX and COUNT can be computed simultaneously. The base relation is: select o_orderkey, l_quantity, l_receiptdate, from (select o_orderkey o_orderkey , F 1 ( A 1 ) is max(l_extendedprice) ,  X  1 is l_receiptdate&gt;=  X 1993-01-01 X  and l_receiptdate&lt; X 1994-01-01 X  , F 2 ( A 2 ) is count (l_orderkey) ,  X  2 is l_returnflag= X  X  X  ,  X  is l_extendedprice &lt; F 1 ( A 1 ) and F ( A 2 ) = 0. 5 Experiments and performance analysis As stated above, an optimizer has several options with respect to redundancy: it can ignore it or use it. When we want to take full use of the redundancy, there are several possibilities: we can use the SegmentApply operator [ 6 ], the WITH clause with window aggregations (i.e., WinMagic) [ 30 ], or the for-loop operator. However, no method is always superior to all on different kinds of queries having redundancy using the maximal common block method which uses the for-loop operator, and other existing techniques.
 We implemented the maximal common block method on top of a leading commercial DBMS, which we call  X  X ystem A X . The maximal common block method is implemented in two stages: first, an SQL query is used to obtain the (extended) base relation. Second, we use stored procedures in procedural SQL to implement the for-loop operator which processes the data fetched from the first stage. In order to simulate the grouped for-loop operator in an effective manner, we make the database sort the (extended) base relation by adding the ORDER BY clause to the query (this is equivalent to implementing the GROUP BY operator by sorting), which we believe is a realistic possibility. The reasons we use stored procedures to implement the for-loop operator are: (1) they run inside the database so that the communication overhead can be reduced significantly compared to an external processing; (2) they can be called by other applications, which makes the maximal common block method more suited for practical use. Note that there exists communication overhead when the stored procedure fetches data from the SQL engine (as observed by [ 1 ], this is one considerable disadvantage that all experimental settings similar to ours must bear).

In our experiments, we created TPC-H databases at scale factors 1 and 10 (size of 1 and 10 GB, respectively) in System A, hosted on a server with an Intel Pentium 4 2.80 GHz processor, two 36 GB SCSI disks, and 1GB memory, running Red Hat Enterprise Linux WS release 3. We configured a buffer cache of size 32 MB, and installed all data and indexes in a single disk. B+ tree indexes on the primary key of each base table were automatically built by System A, and indexes on foreign keys were manually created. The performance metric is the elapsed time of running each query. The elapsed time is measured by taking the average time of three independent runs for each query, which does not include the time for the first query run. Before each running, the buffer cache of System A was flushed. Unless specified otherwise, the results shown in the following figures are obtained running on the buffer cache of 32 MB. We run our experiments on both 1 and 10 GB TPC-H databases. Since the difference among methods is not significant at scale factor 1, we only report and analyze the experiments on scale factor 10.

We performed experiments on all example queries in this paper. We choose the techniques that the maximal common block method compares to depending on the type of subqueries. In fact, we made an attempt to compare the maximal common block method with the best existing technique. For each query, we compare the maximal common block method to the technique which can handle redundancy, as well as the technique which does not take into account redundancy. For the former, we use WinMagic [ 30 ] because it can be implemented directly using SQL. 5 For the latter, we use magic decorrelation [ 24 ]onQuery 1 and Query 2 , because both queries have a correlated, aggregate subquery and magic decorrelation can pro-vide more efficient performance than other techniques in most cases, and the native approach of System A on Query 3 ,Query 4 ,Query 5 ,andQuery 6 , because magic decorrelation deals with correlated, aggregate subqueries in the WHERE clause and cannot be directly used to evaluate other subqueries. For Query 3 , which has a non-correlated subquery in the HAVING clause, the basic approach is to compute the subquery once and then the result is used to compute the outer query and no further optimization is needed. Query 4 is not a typical query on which magic decorrelation can have better performance because it has a corre-lated, aggregate subquery in the HAVING clause. Query 5 and Query 6 cannot be evaluated by magic decorrelation unless their non-aggregate subqueries have been transformed into aggregate subqueries. However, careful attention must be taken to transform NOT IN or ALL subqueries when NULL values are present. The transformed query may be more com-plicated than the original query and performance of magic decorrelation on the transformed query may not be good at all. Hence, if magic decorrelation is not used for some queries it is because it is deemed not particularly efficient for those queries, and we use the native approach instead.

Our first experiment was performed on Query 1 , which has a correlated, aggregate sub-query and complete redundancy. We compare the maximal common block method to magic decorrelation and WinMagic, which were performed manually and then submitted to System A. It is worth noting that the native approach of System A automatically uses the WinMagic maximal common block method and WinMagic perform much better than magic decorrela-tion by avoiding redundant computations. Furthermore, the maximal common block method performs 45% better than WinMagic due to its sequential pass over the base relation. To examine the effect of buffer cache sizes on test methods, we also run Query 1 on a buffer cache of size 128MB. The results are shown in the right side of Fig. 4 . Comparing the left side and the right side of Fig. 4 , we can see that all test methods running on 128 MB perform slightly better than those on 32 MB. On any buffer cache size we used, the maximal common block method performs better than WinMagic, although their difference is less significant on larger sizes.

Our second experiment was done on Query 2 , which has a correlated, aggregate subquery and partial redundancy. Similar to the first experiment, we still compare the performance among the maximal common block method, magic decorrelation, and WinMagic. Note that the original WinMagic has to be extended to correctly compose the WITH clause by using our redundancy detection technique. The performance results are shown in Fig. 5 . The maximal common block method performs 45% better than magic decorrelation. However, WinMagic performs worse than the maximal common block method and performs comparably to magic decorrelation. One reason is that with respect to SQL X  X  syntax, we have to include more GROUP BY clauses. Since GROUP BY is implemented by sorting in System A and sorting is a time-consuming operation, the performance may decrease. One notable point is that the native approach of System A uses the traditional nested iteration method, that is, for each and can not give the answer in 24 h.

Our third experiment was performed on Query 3 , which has a non-correlated, aggregate subquery in the HAVING clause, and Query 4 , which has a correlated, aggregate subquery in the HAVING clause. We compare the maximal common block method to the native approach of System A and WinMagic (needs to be extended). The native approach evaluates Query 3 by accessing the tables in the subquery once to compute the linked aggregation, and then accessing the tables in the outer query to compute the linking aggregation and filter out the final result. Obviously, common parts are accessed twice by the native approach. WinMagic uses two window aggregations to compute the linking aggregation and linked aggregation based on different partitions. The execution results of Query 3 are shown in Fig. 6 a. The performance gain of the maximal common block method is about 50% compared to the native approach. Also, the maximal common block method performs slightly better than WinMagic. One more solution for Query 3 is directly using the WITH clause which computes the outer query; then the HAVING clause is computed by a nested query with a non-correlated subquery which computes the linked aggregation. However, this strategy is restricted to the queries having a complete redundancy. Fortunately, this limit can be relaxed when using the maximal common block method and WinMagic. The execution results of Query 4 are shown in Fig. 6 b. The native approach uses the traditional nested iteration method. We can see that the maximal common block method performs 46% better than the native approach, and 16% better than WinMagic.

Our fourth experiment was done on Query 5 , which has a correlated, non-aggregate sub-query and a linking operator ALL . We compare the performance of the maximal common block method, the native approach, and WinMagic. It is worth noting that antijoin can not be used for the ALL or NOT IN linking predicate without elaborate transformation or some con-straints when null values are present. Figure 7 corresponds to the execution of Query 5 .The native approach performs nested iteration over the subquery. Although the native approach takes advantage of using an index scan of the table in the subquery without accessing the underlying table, the maximal common block method still has about 55% performance improvement. Furthermore, the maximal common block method performs slightly better than WinMagic due to its simultaneous computation of several aggregations.

Our final experiment evaluated Query 6 , a tree query with an aggregate subquery and a non-aggregate subquery at the same level, using the native approach, WinMagic and the maximal common block method. One notable point is that both subqueries are unnested by the native approach; for the NOT EXISTS subquery, antijoin is used. This is the most efficient plan for existing approaches. The native approach evaluates Query 6 in three steps: first, performing an antijoin of orders and lineitem in the NOT EXISTS subquery; second, the resulting tuples are used to unnest the aggregate subquery; finally, the resulting tuples join to lineitem in the outer query to give the final result. Totally, lineitem has to be accessed three times by the native approach. By transforming the NOT EXISTS subquery into an aggregate subquery, the maximal common block method and WinMagic require only one access to lineitem .Afterthe NOT EXISTS subquery is transformed into aggregate subquery, there two aggregates to be computed. The for-loop operator can compute these two aggregates simultaneously, but WinMagic has to do it one by one. Thus, the maximal common block method performs slightly better than WinMagic. The experiment results shown in Fig. 8 demonstrate a significant improvement of the maximal common block method and WinMagic with the performance gain of about 70% compared to the native approach.

Although our reported experiments have shown that our approach performs always better than other approaches, we also noticed that in some cases at scale factor 1, it has similar performance to others. Since we have not covered all possible cases, we conclude that our approach is not always superior, but it is indeed better then existing techniques for some cases. In summary, the maximal common block method performs significantly better than the technique without considering redundancy (e.g., magic decorrelation or the native approach), and performs comparably to redundancy handling techniques (e.g., WinMagic) on our test queries. In our experiments, the maximal common block method always performs slightly better than WinMagic. There are two reasons: first, with respect to SQL X  X  syntax, WinMagic has to use more GROUP BY clauses and therefore performance may decrease; and second, the for-loop operator used by the maximal common block method can compute several aggregate functions simultaneously by only one pass over the data, while one window aggregation used by WinMagic can compute only one aggregate function and the number of aggregate function affects the performance. Considering thatthe maximalcommon block method is implemented outside the SQL engine, which requires a significant communication overhead, we believe the results indicate that the maximal common block method should be considered alongside other methods by a query optimizer. 6Conclusion In this paper, we introduce the redundancy detection technique to detect redundancy for queries having redundancy. Then we propose the maximal common block method to exploit the maximal redundancy detected with the introduction of the for-loop operator. The results of our experiments show that our approach, when applicable, offers performance improvement. Although sharing the largest common part does not always yield the most efficient plan for a given query, it enlarges the optimization search space and offers the possibility for the optimizer to choose the plan with the least estimated cost. We plan to examine the impact of using different amounts of redundancy in the future.
 References Author Biographies
