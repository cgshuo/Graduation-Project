 The Information Bottleneck (IB) approach and independent component analysis (ICA) have both attracted substantial interest as general principles for unsupervised learning [1, 2]. A hope has been, that they might also help us to understand strategies for unsupervised learning in biological systems. However it has turned out to be quite difficult to establish links between known learning algorithms that have been derived from these general principles, and learning rules that could possibly be im-plemented by synaptic plasticity of a spiking neuron. Fortunately, in a simpler context a direct link between an abstract information theoretic optimization goal and a rule for synaptic plasticity has recently been established [3]. The resulting rule for the change of synaptic weights in [3] maxi-mizes the mutual information between pre-and postsynaptic spike trains, under the constraint that the postsynaptic firing rate stays close to some target firing rate. We show in this article, that this approach can be extended to situations where simultaneously the mutual information between the postsynaptic spike train of the neuron and other signals (such as for example the spike trains of other neurons) has to be minimized (Figure 1). This opens the door to the exploration of learning rules for information bottleneck analysis and independent component extraction with spiking neurons that would be optimal from a theoretical perspective.
 We review in section 2 the neuron model and learning rule from [3]. We show in section 3 how this learning rule can be extended so that it not only maximizes mutual information with some given spike trains and keeps the output firing rate within a desired range, but simultaneously minimizes mutual information with other spike trains, or other time-varying signals. Applications to infor-A B Figure 1: Different learning situations analyzed in this article. A In an information bottleneck task and the activity of one or several target neurons Y K 2 , Y K 3 , . . . (which can be functions of the inputs the inputs X K and the output Y K 1 as low as possible (and its firing rate within a desired range). Thus the neuron should learn to extract from its high-dimensional input those aspects that are related to these target signals. This setup is discussed in sections 3 and 4. B Two neurons receiving the same inputs X K from a common set of presynaptic neurons both learn to maximize information transmission, and simultaneously to keep their outputs Y K 1 and Y K 2 statistically independent. Such extraction of independent components from the input is described in section 5. mation bottleneck tasks are discussed in section 4. In section 5 we show that a modification of this learning rule allows a spiking neuron to extract information from its input spike trains that is independent from the component extracted by another neuron. We use the model from [3], which is a stochastically spiking neuron model with refractoriness, where the probability of firing in each time step depends on the current membrane potential and the time since the last output spike. It is convenient to formulate the model in discrete time with step size  X  t . The total membrane potential of a neuron i in time step t k = k  X  t is given by where u r =  X  70mV is the resting potential and w ij is the weight of synapse j ( j = 1 , . . . , N ). An input spike train at synapse j up to the k -th time step is described by a sequence X k j = evokes a postsynaptic potential (PSP) with exponentially decaying time course  X  ( t  X  t n ) with time constant  X  m = 10ms . The probability  X  k i of firing of neuron i in each time step t k is given by where g ( u ) = r 0 log { 1 + exp[( u  X  u 0 ) /  X  u ] } is a smooth increasing function of the membrane potential u ( u 0 =  X  65mV ,  X  u = 2mV , r 0 = 11Hz ). The approximation is valid for sufficiently values in [0 , 1] and depends on the last firing time  X  t i of neuron i (absolute refractory period  X  abs = 3ms , relative refractory time  X  refr = 10ms ). The Heaviside step function  X  takes a value of 1 for non-negative arguments and 0 otherwise.
 This model from [3] is a special case of the spike-response model, and with a refractory variable R ( t ) that depends only on the time since the last postsynaptic event it has renewal properties [4]. The output of neuron i at the k -th time step is denoted by a variable y k i that assumes the value 1 if a postsynaptic spike occurred and 0 otherwise. A specific spike train up to the k -th time step is written The information transmission between an ensemble of input spike trains X K and the output spike D vergence [5], imposing the additional constraint that the firing statistics P ( Y i ) of the neuron should constant target firing rate  X  g accounting for homeostatic processes. An online learning-rule perform-ing gradient ascent on this quantity was derived for the weights w ij of neuron i , with  X  w k ij denoting the weight change during the k -th time step: measures coincidences between postsynaptic spikes at neuron i and PSPs generated by presynaptic action potentials arriving at synapse j , in an exponential time window with time constant  X  C = 1s and g 0 ( u i ( t k )) denoting the derivative of g with respect to u . The term depends on the optimization parameter  X  . We extend the learning rule presented in the previous section to a more complex scenario, where the mutual information between the output spike train Y K 1 of the learning neuron (neuron 1) and some target spike trains Y K l ( l &gt; 1 ) has to be maximized, while simultaneously minimizing the mutual information between the inputs X K and the output Y K 1 . Obviously, this is the generic IB scenario applied to spiking neurons (see Figure 1A). A learning rule for extracting independent components with spiking neurons (see section 5) can be derived in a similar manner.
 derive an update rule for the synaptic weights w 1 j of neuron 1. The quantity to maximize is therefore where  X  and  X  are optimization constants. To maximize this objective function, we derive the weight change  X  w k 1 j during the k -th time step by gradient ascent on (7), assuming that the weights w 1 j can change between some bounds 0  X  w 1 j  X  w max (we assume w max = 1 throughout this paper). Note that all three terms of (7) implicitly depend on w 1 j because the output distribution P ( Y K 1 ) changes if we modify the weights w 1 j . Since the first and the last term of (7) have already been considered (up to the sign) in [3], we will concentrate here on the middle term L 12 :=  X I ( Y K 1 ; Y K 2 ) and denote the contribution of the gradient of L 12 to the total weight change  X  w k 1 j in the k -th time In order to get an expression for the weight change in a specific time step t k we write the probabilities Q of information theory [5]. Consequently, we rewrite L 12 as a sum over the contributions of the individual time bins, L 12 = The weight change  X   X  w k 1 j is then proportional to the gradient of this expression with respect to the gradient yields  X   X  w k 1 j =  X  quantities are implemented online as running exponential averages with a time constant of 10s . Under the assumption of a small learning rate  X  we can approximate the expectation  X  X  X  X k , Y k by averaging over a single long trial. Considering now all three terms in (7) we finally arrive at an online rule for maximizing (7) presynaptic input at synapse j ( X  X orrelation term X ) and terms B k 1 and B k 12 that characterize the postsynaptic state of the neuron ( X  X ostsynaptic terms X ). Note that the argument of B k 1 is different from (4) because some of the terms of the objective function (7) have a different sign. In order to compensate the effect of a small  X  t , the constant  X  has to be large enough for the term B k 12 to have an influence on the weight change.
 contains an extra term B k 12 = F k 12 / ( X  t ) 2 that is sensitive to the statistical dependence between the output spike train of the neuron and the target. It is given by This term basically compares the average product of firing rates  X  g 12 (which corresponds to the joint probability of spiking) with the product of average firing rates  X  g 1  X  g 2 (representing the probability of independent spiking). In this way, it measures the momentary mutual information between the output of the neuron and the target spike train.
 For a simplified neuron model without refractoriness ( R ( t ) = 1 ), the update rule (4) resembles the BCM-rule [6] as shown in [3]. With the objective function (7) to maximize, we expect an  X  X nti-2 . Since there is no refractoriness, the postsynaptic rate  X  1 ( t k ) is given directly by the current value of g ( u ( t k )) , and the update rule (10) reduces to the rate model 3 where the presynaptic rate at synapse j at time t k is denoted by  X  pre,k j = a current membrane potential. The first term in the curly brackets accounts for the homeostatic process and Y K 2 . Note that this term is zero if the rates of the two neurons are independent.
 It is interesting to note that if we rewrite the simplified rate-based learning rule (12) in the following way, we can view it as an extension of the classical Bienenstock-Cooper-Munro (BCM) rule [6] with a two-dimensional synaptic modification function  X (  X  k 1 ,  X  k 2 ) . Here, values of  X  &gt; 0 produce LTD whereas values of  X  &lt; 0 produce LTP. These regimes are separated by a sliding threshold, however, in contrast to the original BCM rule this threshold does not only depend on the running average of a learning neuron conveys about two target signals Y K 2 and Y K 3 . If the target signals are statistically independent from each other we can optimize the mutual information to each target signal separately. This leads to an update rule where B k 12 and B k 13 are the postsynaptic terms (11) sensitive to the statistical dependence between the output and target signals 1 and 2, respectively. We choose  X  g = 30Hz for the target firing rate, and we use discrete time with  X  t = 1ms .
 In this experiment we demonstrate that it is possible to consider two very different kinds of target signals: one target spike train has has a similar rate modulation as one part of the input, while the other target spike train has a high spike-spike correlation with another part of the input. The learning neuron receives input at 100 synapses, which are divided into 4 groups of 25 inputs each. The first remaining groups 3 and 4 are correlated with a coefficient of 0.5 within each group, however, spike trains from different groups are uncorrelated. Correlated spike trains are generated by the procedure described in [7].
 The first target signal is chosen to have the same rate modulation as the inputs from group 1, except that Gaussian random noise is superimposed with a standard deviation of 2Hz . The second target spike train is correlated with inputs from group 3 (with a coefficient of 0.5), but uncorrelated to inputs from group 4. Furthermore, both target signals are silent during random intervals: at each A D Figure 2: Performance of the spike-based learning rule (10) for the IB task. A Modulation of input rates to input groups 1 and 2. B Evolution of weights during 60 minutes of learning (bright: strong synapses, w ij  X  1 , dark: depressed synapses, w ij  X  0 .) Weights are initialized randomly between 0.10 and 0.12,  X  = 10  X  4 ,  X  = 2  X  10 3 ,  X  = 50 . C Output rate and rate of target signal 1 during 5 seconds after learning. D Evolution of the average mutual information per time bin (solid line, left scale) between input and output and the Kullback-Leibler divergence per time bin (dashed line, right scale) as a function of time. Averages are calculated over segments of 1 minute. E Evolution of the average mutual information per time bin between output and both target spike trains as a function of time. F Trace of the correlation between output rate and rate of target signal 1 (solid line) and the spike-spike correlation (dashed line) between the output and target spike train 2 during learning. Correlation coefficients are calculated every 10 seconds. silent for a duration chosen from a Gaussian distribution with mean 5s and SD 1s (minimum duration is 1s ). Hence this experiment tests whether learning works even if the target signals are not available all of the time.
 Figure 2 shows that strong weights evolve for the first and third group of synapses, whereas the efficacies for the remaining inputs are depressed. Both groups with growing weights are correlated with one of the target signals, therefore the mutual information between output and target spike trains increases. Since spike-spike correlations convey more information than rate modulations synaptic efficacies develop more strongly to group 3 (the group with spike-spike correlations). This results in an initial decrease in correlation with the rate-modulated target to the benefit of higher correlation with the second target. However, after about 30 minutes when the weights become stable, the correlations as well as the mutual information quantities stay roughly constant.
 An application of the simplified rule (12) to the same task is shown in Figure 3 where it can be seen that strong weights close to w max are developed for the rate-modulated input. To some extent weights grow also for the inputs with spike-spike correlations in order to reach the constant target firing rate  X  g . In contrast to the spike-based rule the simplified rule is not able to detect spike-spike correlations between output and target spike trains. A Figure 3: Performance of the simplified update rule (12) for the IB task. A Evolution of weights during 30 minutes of learning (bright: strong synapses, w ij  X  1 , dark: depressed synapses, w ij  X  0 .) Weights are initialized randomly between 0.10 and 0.12,  X  = 10  X  3 ,  X  = 10 4 ,  X  = 10 . B Evolution of the average mutual information per time bin (solid line, left scale) between input and output and the Kullback-Leibler divergence per time bin (dashed line, right scale) as a function of time. Averages are calculated over segments of 1 minute. C Trace of the correlation between output rate and target rate during learning. Correlation coefficients are calculated every 10 seconds. With a slight modification in the objective function (7) the learning rule allows us to extract statis-tically independent components from an ensemble of input spike trains. We consider two neurons receiving the same input at their synapses (see Figure 1B). For both neurons i = 1 , 2 we maximize information transmission under the constraint that their outputs stay as statistically independent from each other as possible. That is, we maximize Since the same terms (up to the sign) are optimized in (7) and (15) we can derive a gradient ascent rule for the weights of neuron i , w ij , analogously to section 3: Figure 4 shows the results of an experiment where two neurons receive the same Poisson input with a rate of 20Hz at their 100 synapses. The input is divided into two groups of 40 spike trains each, such that synapses 1 to 40 and 41 to 80 receive correlated input with a correlation coefficient of 0.5 within each group, however, any spike trains belonging to different input groups are uncorrelated. The remaining 20 synapses receive uncorrelated Poisson input. Weights close to the maximal efficacy w max = 1 are developed for one of the groups of synapses that receives correlated input (group 2 in this case) whereas those for the other correlated group (group 1) as well as those for the uncorrelated group (group 3) stay low. Neuron 2 develops strong weights to the other correlated group of synapses (group 1) whereas the efficacies of the second correlated group (group 2) remain depressed, thereby trying to produce a statistically independent output. For both neurons the mutual information is maximized and the target output distribution of a constant firing rate of 30Hz is approached well. After an initial increase in the mutual information and in the correlation between the outputs, when the weights of both neurons start to grow simultaneously, the amounts of information and correlation drop as both neurons develop strong efficacies to different parts of the input. Information Bottleneck (IB) and Independent Component Analysis (ICA) have been proposed as general principles for unsupervised learning in lower cortical areas, however, learning rules that can implement these principles with spiking neurons have been missing. In this article we have derived from information theoretic principles learning rules which enable a stochastically spiking neuron to solve these tasks. These learning rules are optimal from the perspective of information theory, but they are not local in the sense that they use only information that is available at a single A D Figure 4: Extracting independent components. A , B Evolution of weights during 30 minutes of learning for both postsynaptic neurons (red: strong synapses, w ij  X  1 , blue: depressed synapses, w ij  X  0 .) Weights are initialized randomly between 0.10 and 0.12,  X  = 10  X  3 ,  X  = 100 ,  X  = 10 . C Evolution of the average mutual information per time bin between both output spike trains as a function of time. D , E Evolution of the average mutual information per time bin (solid line, left scale) between input and output and the Kullback-Leibler divergence per time bin for both neurons (dashed line, right scale) as a function of time. Averages are calculated over segments of 1 minute. F Trace of the correlation between both output spike trains during learning. Correlation coefficients are calculated every 10 seconds. synapse without an auxiliary network of interneurons or other biological processes. Rather, they tell us what type of information would have to be ideally provided by such auxiliary network, and how the synapse should change its efficacy in order to approximate a theoretically optimal learning rule. Acknowledgments We would like to thank Wulfram Gerstner and Jean-Pascal Pfister for helpful discussions. This paper was written under partial support by the Austrian Science Fund FWF, # S9102-N13 and # P17229-N04, and was also supported by PASCAL, project # IST2002-506778, and FACETS, project # 15879, of the European Union.
 References
