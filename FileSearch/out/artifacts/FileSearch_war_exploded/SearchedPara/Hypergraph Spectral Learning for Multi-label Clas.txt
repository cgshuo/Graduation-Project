 A hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set. It has been applied successfully to capture high-order relations in various domains. In this paper, we propose a hypergraph spectral learning formulation for multi-label classification, where a hypergraph is constructed to exploit the correlation information among different labels. We show that the proposed formulation leads to an eigenvalue prob-lem, which may be computationally expensive especially for large-scale problems. To reduce the computational cost, we propose an approximate formulation, which is shown to be equivalent to a least squares problem under a mild condi-tion. Based on the approximate formulation, efficient al-gorithms for solving least squares problems can be applied to scale the formulation to very large data sets. In addi-tion, existing regularization techniques for least squares can be incorporated into the model for improved generalization performance. We have conducted experiments using large-scale benchmark data sets, and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the high-order relations in multi-label problems. Results also indicate that the approximate formulation is much more efficient than the original one, while keeping competitive classification performance. H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithm Multi-label classification, hypergraph, spectral learning, least squares, canonical correlation analysis, efficiency, regulariza-tion
Multi-label learning studies the problem where each in-stance is associated with a set of labels. Such type of prob-lems occurs in many important applications, such as pro-tein function classification [9], text categorization [25], and semantic scene classification [4]. In contrast to traditional classification tasks where the classes are mutually exclusive, the classes in multi-label learning are overlapped and cor-related, which makes it challenging to predict all relevant labels for a given instance. A straightforward method to perform multi-label learning is to divide it into a number of one-against-all binary classification problems. A potential limitation of this approach is that each label is treated inde-pendently, and the correlation among different class labels is ignored. Various approaches have been proposed in the past to exploit the correlation information contained in mul-tiple labels [15, 16, 20, 21, 22, 26, 27]. In [9], a multi-label SVM model, called RankSVM, was proposed based on novel definitions of loss and margin for multi-label problems.
In this paper, we propose to employ hypergraph [1] to capture the correlation information among different labels for improved classification performance. A hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set. It has been applied for domains where higher-order relations such as co-authorship exist [1, 28]. We propose to employ hyper-graphs to exploit the higher-order relations among multi-ple instances sharing the same label in multi-label learning. Specifically, we propose to construct a hyperedge for each label, and include all instances annotated with a common label into one hyperedge, thus capturing their joint similar-ity. Following the spectral graph embedding theory [7], we propose to compute the low-dimensional embedding through a linear transformation, which preserves the instance-label relations captured by the hy pergraph. The projection is guided by the label information encoded in the hypergraph. In addition, we show the close relationship between the pro-posed formulation and the well-known dimensionality reduc-tion algorithm, Canonical Correlation Analysis (CCA) [14].
The resulting hypergraph learning formulation amounts to solving an eigenvalue problem, which may be computa-tionally expensive for large-scale problems. Motivated from our key observation obtained from CCA, we propose an ap-proximate hypergraph spectral learning formulation, which can be reformulated as a least squares problem. Following the least squares formulation, different regularization tech-niques [12] can be employed to improve the generalization performance of the model. Moreover, efficient algorithms for solving least squares can be applied to scale the algorithm to very large data sets [18]. We have conducted experiments us-ing large-scale benchmark data sets, and our results demon-strate the effectiveness of the proposed hypergraph spectral learning formulation for multi-label classification. Results also show that the scalability of the approximate formula-tion is far superior to that of the original one, while keeping competitive classification performance.

The rest of this paper is organized as follows. We review the basics of hypergraph Laplacian and the least squares in Section 2. We present our multi-label learning formulation based on hypergraph in Section 3. The approximate hyper-graph spectral learning formulation based on least squares is presented in Section 4. We report experimental results in Section 5 and the paper concludes in Section 6.
 Notations n , d ,and k denote the number of training sam-ples, the data dimensionality, and the number of labels, re-spectively. x i  X  R d denotes the i th instance, y i  X  R k tains the label information for x i . X =[ x 1 ,x 2 ,  X  X  X  R d  X  n represents the data matrix, and Y =[ y 1 ,y 2 ,  X  X  X  R k  X  n is the matrix representation for label information. ized Laplacian matrix for the constructed hypergraph, and S = I  X  X  is the similarity matrix, where I is the identity matrix. e is a vector of all ones with an appropriate length.
We review the basics of hypergraph and the least squares technique in this section.
A hypergraph [1] is a generalization of the traditional graph in which the edges, called hyperedges, are arbitrary non-empty subsets of the vertex set. In a hypergraph G = ( V, E ), V is the vertex set and E istheedgesetwhereeach e  X  E is a subset of V . The degree of a hyperedge e , denoted as  X  ( e ), is the number of vertices in e . The degree of every edge in a traditional graph is 2, and it is therefore called a  X 2-graph X . The degree of a vertex v  X  V , d ( v ), is defined as where w ( e ) is the weight associated with the hyperedge e E . The diagonal matrix forms for  X  ( e ), d ( v ), w ( e )arede-noted as D e , D v , W H , respectively. The vertex-edge inci-dence matrix J  X  R | V | X | E | is defined as We can use the higher-order relations encoded in a hyper-graph for multi-label learning. The Laplacian matrix from a traditional graph is widely used for learning from graphs [6]. Regarding the hypergraph, one can either define hypergraph Laplacian directly or expand it into a 2-graph. It has been shown that the Laplacians defined in both ways are similar [1]. In addition, the eigenvectors of these Laplacians have been shown to be useful for learning higher-order relations, and that there is a close relationship between their hyper-graph Laplacians and the structure of the hypergraph. In this section, we briefly review two commonly used expan-sions as well as one hypergraph Laplacian.
In clique expansion, each hyperedge is expanded into a clique. Denote by G c =( V c ,E c ) the 2-graph expanded from hypergraph G =( V, E ) using the clique expansion. We have V c = V and E c = { ( u, v ): u  X  e, v  X  e, e  X  E } .Theedge weight w c ( u, v )of G c is given by In the expanded 2-graph, the vertex degree denoted by d c can be obtained readily, and the corresponding diagonal ma-trix form is denoted as D c . Then the normalized Laplacian of G c is given by L c = I  X  X  c ,where S c is defined as Intuitively in clique expansion, the similarity between two vertices is proportional to the weights of common labels, thus capturing the instance-label relations.
In star expansion, a new vertex is introduced for each hy-peredge and this new vertex is connected to each vertex in this hyperedge. Specifically, for a hypergraph G =( V, E ), the vertex and edge sets of the star-expanded 2-graph, de-noted as V  X  and E  X  , are defined as V  X  = V  X  E and E  X  = { ( u, e ): u  X  e, e  X  E } , respectively. Thus each hyperedge in G is expanded into a star in G  X  , which is a bipartite graph.
The weight w  X  ( u, e )ofanedge( u, e )in G  X  is given by Since V  X  = V  X  E , we can assume that in V  X  ,all v  X  V are ordered before e  X  E .Let M  X  R | V | X | E | denote the weight between the vertices constructed from V and the vertices from E in G  X  . The adjacency matrix for G  X  can be obtained readily from M . Based on the adjacency matrix, the degree denote the diagonal matrices of vertex degrees for vertices in V and E in the expanded graph G  X  =( V  X  ,E  X  ), respectively. Suppose x T =[ x T v ,x T e ] is an eigenvector of Laplacian with corresponding eigenvalue  X  ,where L  X  is the normalized Laplacian for the expanded graph, it can be verified that eigenpair of the Laplacian L  X  is essential for learning. Recall that only x v corresponds to the real vertices in the original hypergraph while x e corresponds to the artificial vertices, thus x v and its corresponding eigenvalue are more important for learning. It follows from Eq. (5) that we can obtain the eigenpair ( x v , X  )from BB T and simplify the computation. We denote S  X  = BB T in the following discussion, and the equivalent matrix form is Intuitively in star expansion, the artificial vertices associated with the labels connect to the vertices from the correspond-ing label, thus capturing the interaction between the data points and the labels. The relationship among different la-bels can then be captured by their interactions with the data points. More details can be found in [1, 29].
Several other methods define the  X  X ypergraph Laplacian X  using analogies from the graph Laplacian [19, 28]. Follow-ing the random walk model, Zhou et al. [28] proposed the following normalized hypergraph Laplacian L z : where In the random walk model, given the current position u  X  V , the walker first chooses a hyperedge e over all hyperedges incident with u with the probability proportional to w ( e ), andthenchoosesavertex v  X  e uniformly at random. Note that each label corresponds to a hyperedge in our frame-work. The transition probability between the nodes associ-ated with two labels captures their similarity. More details can be found in [28].
Least squares is a classical technique for both regression and classification [3]. Given a set of observations x (1 i n ) and their corresponding targets t i  X  R k (1 i n ), the goal is to fit a linear model to the data, where b  X  R k is the bias vector, and W  X  R d  X  k is the weight matrix. Assuming that both the observations { x i } n i =1 and the targets { t i } n i =1 are centered, the bias term becomes zero and can be ignored. The optimal W can be computed by minimizing the following sum-of-squares error function: where T =[ t 1 ,  X  X  X  ,t n ]  X  R k  X  n . The optimal matrix W is where  X  denotes the pseudo-inverse. When least squares is applied for classification, T is known as the class indicator matrix. In binary-class problem we can define t i  X  X  X  1 , 1 For multi-class or multi-label problems, a similar definition can be applied for each label separately [12, 24].
Least squares problems can be solved efficiently using ex-isting techniques, which is very attractive for large-scale data mining. Many iterative algorithms, including the conjugate gradient method, have been proposed in the literature [10, 18]. Compared with the direct methods, these iterative al-gorithms converge very fast for large and sparse problems.
In this section, we present our multi-label learning formu-lation based on hypergraph. We also show the close rela-tionship between the proposed formulation and Canonical Correlation Analysis (CCA).
We propose to learn from multi-label data by exploiting the spectral property of hypergraph that encodes the cor-relation information among labels. To capture the correla-tion among labels, we create a hyperedge for each label and include all data points relevant to this label into the hy-peredge. Based on the Laplacian of the constructed hyper-graph, we propose the hypergraph spectral learning frame-work for learning a low-dimensional embedding through a linear transformation W which solves the following opti-mization problem: where L is the normalized Laplacian of the hypergraph, and W  X  R d  X  k is the projection matrix. In this formulation, the objective function in Eq. (10) attempts to preserve the inherent relationship among data points captured by the Laplacian [2]. It follows from the traditional spectral graph embedding theory [7]. Intuitively, data points that share many common labels tend to be close to each other in the embedded space.
 Define the matrix S as It follows from the property of the normalized Laplacian L that S X  R n  X  n reflects the normalized similarities between different instances (or vertices). Hence, the original opti-mization problem in Eq. (10) can be reformulated equiva-lently into the following form: It can be verified that the optimal W  X  to Eq. (12) consists of the eigenvectors corresponding to the largest eigenvalues for the following eigenvalue problem: where  X   X  R denotes the eigenvalue.

To avoid the singularity of XX T , a regularization term is commonly added. This leads to the following regularized hypergraph spectral learning problem: Recall from Section 2 that different Laplacians can be con-structed from the hypergraph that encodes the label infor-mation, resulting in different spectral learning algorithms.
Canonical Correlation Analysis (CCA) [14] is a well-known technique for finding the correlation between two sets of vari-ables. In CCA, two different views of the same set of objects are given, and a projection is computed for each view such that their correlation is maximized in the dimensionality-reduced space. CCA is commonly used for supervised di-mensionality reduction in which one of the view is derived from the data, and another view is derived from labels. In this setting, the data can be projected into a lower-dimensional space directed by the label information.
We consider the case in which the dimensionality reduc-tion for X  X  R d  X  n is directed by the label information en-coded in Y  X  R k  X  n ,and y i  X  R k contains the label in-formation for x i  X  R n .When YY T is nonsingular, it can be verified that the multiple projection vectors for X under certain orthogonality constraints can be computed simulta-neously by solving the following optimization problem [11]: where W  X  R d  X  is the projection matrix for X and is the number of projection vectors. It is clear that CCA is a special case of the formulation in Eq. (12) in which the similarity matrix S is given by
The spectral learning formulation in Eq. (12) involves an eigenvalue problem in Eq. (13), which may be computation-ally expensive for large-scale problems. In this section, we propose an approximate hypergraph spectral learning for-mulation for improved efficiency. Our experimental results show that this approximate formulation is much more effi-cient and scalable than the original one, while keeping com-petitive classification performance.

One first key observation is that when we use star expan-sion, clique expansion, or Zhou X  X  formulation to derive the Laplacian matrix, the similarity matrices in Eq. (11), de-noted as S c , S  X  ,and S z , respectively, are all positive semi-definite. It is clear that the similarity matrix derived from CCA, denoted as S cca is also positive semi-definite. Math-ematically, they all can be decomposed into the following form:
Note that we assume that the training data are centered, i.e., X is of zero mean. Thus, we have XP = X ,where P is centering matrix defined as P = I n  X  1 n ee T .Itcanbe verified that P is symmetric and PP = P . It follows that That is, we can simply assume that S is centered in terms of both rows and columns. Hence, we have and H T 0 e = 0. These properties can be easily verified in CCA where H 0 = Y T ( YY T )  X  1 / 2 . Moreover, in CCA, the matrix H 0 has orthonormal columns, i.e., H T 0 H 0 = I k
Following CCA, we propose to approximate the hyper-graph Laplacian by requiring the corresponding H 0 to have orthonormal columns. We show that an H 0 with orthonor-mal columns can simplify the computation considerably. In particular, we show that under this approximation, the op-timization problem in Eq. (12) can be reduced to a least problem under a mild condition. We first define the follow-ing assumption: assumption 1. S can be decomposed into the form: where H  X  R n  X  k satisfies the following properties:
Based on Assumption 1, we show that the hypergraph spectral learning formulation can be solved efficiently. De-fine two matrices as follows: It follows that the eigenvalue problem in Eq. (13) can be expressed as Let be the Singular Value Decomposition (SVD) of X ,where r =rank( X ), U and V are orthogonal matrices,  X   X  R d  X  n is diagonal, and U 1  X  r V T 1 is the compact SVD of X , U 1 and let A = P  X  A Q T be the SVD of A ,where P  X  R r  X  r and Q  X  R k  X  k are orthogonal, and  X  A  X  R r  X  k is diagonal. Then the eigendecomposition of C  X  X C H canbeexpressedas Thus, under Assumption 1, the solution to the hypergraph spectral learning formulation is given by the top eigenvec-tors of matrix C  X  X C H ,whichcanbeexpressedas where P contains the first columns of P .
Consider the least squares formulation in Eq. (8) with the class indicator matrix  X  T given by It follows from Eq. (9) that the optimal solution is given by We next show that all diagonal elements of  X  A are ones if rank( X )= n  X  1, which is equivalent to the linearly inde-pendence condition on the data X before centering.
Theorem 4.1. Assume that rank ( X )= n  X  1 and Xe =0 (data centered). Then all diagonal elements of  X  A are ones. Proof. It can be verified that [ V 1 , 1  X  n e ]  X  R n  X  n is an orthogonal matrix. Thus, we have It follows from Eq. (22) that Thus all nonzero singular values of A in  X  A are ones.
Since rank( X  A )= k ,thereare k nonzero eigenvalues. If we choose = k ,then Hence, the only difference between W LS and W HS lies in the orthogonal matrix Q T in W LS . Since orthogonal projec-tions preserve the pairwise Euclidean distance between data points, W HS and W LS are equivalent when classifiers such as the K-Nearest-Neighbor (KNN) based on the Euclidean distance and linear Support Vector Machines (SVM) are ap-plied in the projected space.
The above discussions show that under Assumption 1, the proposed hypergraph spectral learning formulation can be formulated as a least squares problem under a mild con-dition. Regularization can then be applied to control the model complexity and improve the classification performance. Linear regression using the 2-norm regularization, called ridge regression [13], minimizes the penalized sum-of-squares er-ror function. By using the target matrix  X  T as defined in Eq. (24), we obtain the following 2-norm regularized least squares formulation: where  X &gt; 0 is the regulariza tion parameter.
Recall that in CCA, the matrix S satisfies Assumption 1. Thus, the CCA formulation can be transformed equiva-lently into least squares problem. However, this assumption does not hold for hypergraphs in general. We propose to approximate S = H 0 H T 0 in Eq. (16) by HH T subject to the constraint that H satisfies the properties in Assumption 1. This leads to the following optimization problem:
The solution to this problem is summarized in the follow-ing theorem.
 Theorem 4.2. Let S , H ,and H 0 be defined as above. Then the optimal H  X  that solves the optimization problem in Eq. (29) is given by the top k left singular vectors of H Proof. It follows from basic matrix properties that where the last equality follows since H T H = I k .Hence,the minimization problem in Eq. (29) is equivalent to the max-imization of trace( H T S H ), which can be considered as a special case of the more general optimization problem in [8]. The optimal H is given by the eigenvectors of S correspond-ingtothetop k eigenvalues. From Eq. (16), S = H 0 H T 0 , thus the optimal H  X  is given by the top k left singular vectors of H 0 .Alsonotethat H T 0 e =0,whichleadsto H Recall that our proposed spectral learning formulation in Eq. (10) involves an eigenvalue problem in Eq. (13), which has a time complexity of O ( n 2 d ) assuming that n&lt;d .Using efficient algorithms for solving sparse least squares problem, we show that the approximate formulation has a much lower time complexity. In particular, the approximate formulation involves the following two steps for computing the projection matrix W : 1. Compute the top k left singular vectors of H 0 for H . 2. Solve a least square problem using H T as the class The complexity of the first step is O ( nk 2 ). In the second step, we solve k least squares problems. In our implementa-tion, we use the LSQR algorithm proposed in [18], which is an implementation of a conjugate gradient type method for solving sparse least squares problems [5]. Note that the orig-inal matrix X  X  R d  X  n may be sparse in many applications such as text document processing. However, after centering, X is not sparse any more. In ord er to keep the sparsity of X ,thevector x i is augmented by an additional component as  X  x T i =[1 ,x T i ]. This new component acts as the bias for least squares. The extended X is denoted as  X  X  X  R ( d +1)  X  n and the revised least squares problem is formulated as where  X  W  X  R ( d +1)  X  k .Foranewdatapoint x  X  R d ,its projection is given by  X  W T [1; x ].

The computational cost of each iteration of LSQR is O (3 n + 5 d +2 dn ) [18]. Since the least squares problems are solved k times, the overall cost of LSQR is O ( Nk (3 n +5 d +2 dn )), where N is the total number of iterations.
 When the matrix  X  X is sparse, the cost is notably reduced. Suppose the number of nonzero elements in  X  X is z .The overall cost of LSQR is reduced to O ( Nk (3 n +5 d +2 z )). In summary, the total time complexity for solving the approx-imate formulation via LSQR is We observe in our experiments that the iterative algorithm converges quickly, and a total of N = 100 iterations is enough in most cases.
In this section, we empirically evaluate the effectiveness and efficiency of the proposed formulations.
We use both low-dimensional and high-dimensional data sets in the experiments, including the scene data set [4], the Table 1: Summary of statistics of the data sets. n is number of data points, d is the dimensionality, and k is the number of labels.
 yeast data set [9], and two high-dimensional document data sets [16, 21]. The scene and yeast data sets are available at http://www.csie.ntu.edu.tw/  X  cjlin/libsvmtools/datasets, a-nd the two document data sets from Yahoo! are available at http://www.kecl.ntt.co.jp/as/members/ueda/yahoo.tar.gz. For all data sets, the labels that contain less than 50 in-stances are removed. We follow the feature selection meth-ods studied in [23] for text documents and extract different numbers of terms to investigate the performance of algo-rithms. The statistics of these data sets are summarized in Table 1.

For each data set, a transformation matrix W is learned from the training set, and it is then used to project the test data onto a lower-dimensional space in which the linear Support Vector Machine (SVM) is applied for each label separately. The Receiver Operating Characteristic (ROC) score is employed to evaluate the classification performance. All experiments are performed on a PC with Intel Core 2 Duo T7200 2.0G CPU and 2G RAM and all algorithms in our experiments are implemented in Matlab.

We denote the hypergraph spectral learning formulation in Eq. (13) and its regularized version in Eq. (14) as HG and HG  X  , respectively. The approximate (least squares) hy-pergraph spectral learning formulation and its regularized version are denoted as lsHG and lsHG  X  , respectively. The subscripts c ,  X  , z ,and cca are used to denote the formula-tions derived from clique expansion, star expansion, Zhou X  X  formulation, and CCA, respectively.
We compare the performance of the hypergraph spectral learning formulation and its approximate least squares for-mulation on the scene and yeast data sets. For each data set, we increase the size of the training set gradually from 100 to 900 with a step size around 100, and generate ten random training/test partitions in each case. Figures 1 and 2 plot the averaged performance of the hypergraph spectral learning formulation and the least squares formulation on the scene and yeast data sets for all definitions of Laplacian. We can observe that the performance of both methods for all definitions of Laplacian is very close. This shows that the proposed approximate least squares formulation is close to the original one.

We also compare the regularized hypergraph spectral learn-ing formulation with its approximate counterpart. In this experiment, the size of training set is fixed at 900, and dif-ferent values of the regularization parameter  X  are used. We report the results on the scene and document data sets in Figures 3 and 4. It can be observed that the perfor-mance of the regularized algorithms perform much better than their counterparts without regularization. Results also show that the performance of the regularized hypergraph Table 2: Summary of mean ROC scores over all la-bels for all compared algorithms. All the features of the scene and yeast data sets are used while the most frequent 2000 terms of the Arts and Comput-ers data sets are used.
 spectral learning formulation and that of its approximate counterpart are similar in all cases. This again justifies the use of the approximate formulations proposed in this paper.
To evaluate the relative performance of all variants of the hypergraph spectral learning formulation and those of the approximate formulation comprehensively, we report the mean ROC scores over all labels and all splittings for each algorithm in Table 2. The performance of RankSVM [9] is also reported. 3-fold cross validation is used to estimate the optimal regularization parameters. We also apply SVM to each label independently and our results show that it is less effective than RankSVM and is thus omitted in the table. We can observe from Table 2 that the performance of the hypergraph spectral learning formulations is close to that of the approximate formulations. Moreover, regularization al-gorithms always outperform those without regularization, which justifies the use of regularization. All of the pro-posed methods with regularization perform better than the RankSVM algorithm. Results also show that three different similarity matrices S lead to the similar performance, which is consistent with the theoretical analysis in [1].
In this subsection, we study the scalability of regularized hypergraph spectral learning formulation and its approxi-mate counterpart as the sample size and data dimensional-ity increase. The approximate formulation is solved by the LSQR algorithm [18]. Figure 5 depicts the computation time of the two formulations on the Yahoo \ Arts&amp;Humanities data set as the data dimensionality increases with the training size fixed at 2000. It can be observed that the computa-tion time for both algorithms increases steadily as the data dimensionality increases. The computation time of the ap-proximate formulation is substantially less than that of the hypergraph spectral learning formulation. In fact, the com-putation time of the approximate formulation is less than 0.5 second for all tested data dimensionality. We also evaluate the scalability of the formulations in terms of the training sample size. Figure 6 plots the computation time of the reg-ularized hypergraph spectral learning and the regularized approximate formulations on the Yahoo \ Arts&amp;Humanities data set when the training sample size increases gradually with the data dimensionality fixed at 2000. We can ob-serve that the approximate formulation is much more scal-able than the original formulation when the sample size in-creases. We have not further increased the training size in this experiment, due to the high computational cost of the original formulation.

To investigate the scalability of the approximate formula-tions further, we use the entire Yahoo \ Computers&amp;Interne data set which contains 6270 samples with 34096 dimen-sions. We increase the data dimensionality gradually with the training size fixed at 6270 and the computation time of all four approximate formulations is plotted in Figure 7. We can observe that the proposed approximate formulations take less than 9 seconds in all cases. Our experimental re-sults show that the proposed approximate formulations scale to large data sets, while keeping competitive classification performance as shown in Table 2.
We present a hypergraph spectral learning formulation for multi-label classification in this paper. In this formu-lation, a hypergraph is constructed to capture the correla-tion information contained in different labels. We show that the proposed formulation is reduced to an eigenvalue prob-lem which may be computationally expensive for large-scale problems. We further propose an approximate formulation which amounts to solving a least squares problem. Efficient algorithms for solving least squares problem such as LSQR can thus be employed. Experimental results show that the proposed formulation is effective in capturing the correlation information for multi-label classification. Results also show that the approximate formulation is much more efficient, while keeping competitive classification performance.
Experimental results show that the approximate formu-lation is very close to the original formulation in terms of classification performance. We plan to perform a theoreti-cal analysis on the approximate formulation in terms of the error bounds of the approximation formulation. The main focus of this paper is on linear models. We plan to extend the hypergraph spectral learning formulation to the kernel-induced feature space, and apply the kernel formulation to multi-label applications such as protein function prediction [17] where only kernels are available.
 This research is sponsored in part by the Arizona State Uni-versity and by the National Science Foundation Grant IIS-0612069. Figure 3: Comparison of four methods ( HG , lsHG , HG , lsHG  X  ) in terms of mean ROC score on the scene set are generated, and the averaged performance is reported. Figure 4: Comparison of four methods ( HG , lsHG , HG  X  , lsHG  X  ) in terms of mean ROC score on the formulations is much smaller (close to the zero line) in all cases. formulations is much smaller (close to the zero line) in all cases.
