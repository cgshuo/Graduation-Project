 Rocchio X  X  relevance feedback model is a classic query expan-sion method and it has been shown to be effective in boosting information retrieval performance. The selection of expan-sion terms in this method, however, does not take into ac-count the relationship between the candidate terms and the query terms (e.g., term proximity). Intuitively, the proxim-ity between candidate expansion terms and query terms can be exploited in the process of query expansion, since terms closer to query terms are more likely to be related to the query topic.

In this paper, we study how to incorporate proximity in-formation into the Rocchio X  X  model, and propose a proximity-based Rocchio X  X  model, called PRoc ,withthreevariants.In our PRoc models, a new concept (proximity-based term fre-quency, ptf ) is introduced to model the proximity informa-tion in the pseudo relevant documents, which is then used in three kinds of proximity measures. Experimental results on TREC collections show that our proposed PRoc models are effective and generally superior to the state-of-the-art relevance feedback models with optimal parameters. A di-rect comparison with positional relevance model (PRM) on the GOV2 collection also indicates our proposed model is at least competitive to the most recent progress.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models, Relevance feedback Algorithms, Performance, Experimentation Pseudo Relevance Feedback, Rocchio X  X  Model, Proximity-based Term Frequency, Query Expansion
Pseudo relevance feedback (PRF) via query expansion (QE) is an effective technique for boosting the overall per-formance in Information Retrieval (IR). It assumes that top-ranked documents in the first-pass retrieval are relevant, and then used as feedback documents in order to refine the rep-resentation of original queries by adding potentially related terms. Although PRF has been shown to be effective in im-proving IR performance [4, 6, 9, 13, 23, 26, 28, 30, 36, 37, 40, 42] in a number of IR tasks, traditional PRF can also fail in some cases. For example, when some of the feedback documents have several incoherent topics, terms in the ir-relevant contents are likely to misguide the feedback models by importing noisy terms into the queries. This could in-fluence the retrieval performance in a negative way. It is partially because the query itself was ignored in the process of expansion term selection. To be more specific, the term associations between candidate terms and the query terms have been ignored in traditional PRF models.

Term proximity is an effective measure for term associ-ations, which has been studied extensively in the past few years. Most of these studies focus on the term proximity within the original query and adapt this in ranking docu-ments [3, 5, 8, 10, 15, 24, 31, 33]. Various methods of in-tegrating proximity information into a retrieval process are introduced in these papers, and it has proven to be useful in discriminating between the relevant and non-relevant docu-ments.

In the field of PRF, based on the assumption that terms closer to the query terms are more likely to be relevant to the query topic, there are several studies which investigated how to give more weight to these terms in the process of pseudo relevance feedback. For example, to this end, Vechtomova et al. [34] imported a distance factor which combined with Mu-tual Information (MI) for selecting query expansion terms. Lv et al. [16] proposed a positional relevance model (PRM) by using position and proximity information to solve this problem and obtain significant performance.

However, as far as we are aware, there is little work done on incorporating proximity information into the traditional Rocchio X  X  feedback model. Although the Rocchio X  X  model has been introduced in the information retrieval field for many years, it is still effective in obtaining relevant docu-ments. According to [41],  X  X M25 [27] term weighting cou-pled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling ap-proach for many tasks. X  This observation is also supported in our preliminary experiments of this paper. Therefore, it is promising to make an extension of the Rocchio X  X  model to take into account the proximity information.

In addition, it is unknown how to tackle the challenge of modeling the traditional statistics of expansion terms and the relationship between expansion terms and the query terms in a unified framework. In this paper, we propose a proximity-based feedback model based on the traditional Rocchio X  X  model, called PRoc . Unlike the PRM model, we focus on the proximity of terms rather than the positional information. In our method, we estimate the weights of can-didate expansion terms by taking their distance from query terms into account. Specifically, if a term is far away from the query terms in the feedback documents, it is proposed to be punished by discounting its weight because it is likely to be irrelevant to the query topic.

The main contribution of this paper is as follows. First, we study how to adapt the traditional Rocchio X  X  model [28] for proximity information, and propose a proximity-based feed-back model, called PRoc , in which the traditional statistics of expansion terms and the proximity relationship between expansion terms and the query terms are taken into account. Second, we propose to use three proximity measures. Un-like previous methods, the importance of query terms has been taken into account to measure the proximity informa-tion by proposing a new concept, namely proximity-based term frequency. Finally, extensive experiments on standard TREC collections have been conducted to evaluate our pro-posed feedback model from different aspects. We compare our proposed feedback model with strong feedback baselines. Our model can achieve significantly better performance over RM3 and the classic Rocchios X  model.

The remainder of this paper is organized as follows: in Sec-tion 2 we review the related work. In Section 3, three meth-ods for measuring the proximity and our proposed model, PRoc, are presented in details. In Section 4, we introduce the settings of the experiments. In Section 5, the experimen-tal results are presented and discussed. A direct comparison is made to compare PRoc with the most recent work PRM. Finally, we conclude our work with a brief conclusion and future research directions in Section 6.
In information retrieval, PRF via query expansion is re-ferred to as the techniques, algorithms or methodologies that reformulate the original query by adding new terms into the query, in order to achieve a better retrieval performance. There are a large number of studies on the topic of PRF. Here we mainly review the work about PRF which is the most related to our research.
 A classical relevance feedback technique was proposed by Rocchio in 1971 for the Smart retrieval system [28]. It is a framework for implementing (pseudo) relevance feedback via improving the query representation, in which a set of documents are utilized as the feedback information. Unique terms in this set are ranked in descending order of their TFIDF weights. In the following decades, many other rel-evance feedback techniques and algorithms were developed, mostly derived under Rocchio X  X  framework. For example, a popular and successful automatic PRF algorithm was pro-posed by [26] in the Okapi system; Amati et al. [2] pro-posed a query expansion algorithm in his divergence from randomness (DFR) retrieval framework. In addition, with the development of language model [21] in IR, a number of techniques (e.g. [13, 32, 42]) have been developed to fit in the language modeling framework. In addition, Robertson [25] proposed a theoretical feedback model that supports the as-sumption by using of the difference of term distributions to select and re-weight the expansion terms.

For PRF in the language modeling framework, we al-ways exploit feedback information (e.g., the top-ranked doc-uments set, F = D 1 ,D 2 ,...,D | F | ), in order to re-estimate a more accurate query language model. For example, the model based feedback approach [42] is not only theoretically sound, but also performs well empirically. The essence of model based feedback is to update the probability of a term in the query language model by making use of the feedback information. Much like model-based feedback, relevance models [13] also estimate an improved query model. The dif-ference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant doc-ument. Instead, they model a more generalized notion of relevance [18]. Lv et al. [14] have conducted a compara-ble study of five representative state-of-the-art methods for estimating improved query language models in ad hoc infor-mation retrieval, including RM3 (a variant of the relevance language model), RM4, DMM, SMM (a variant of model-based feedback approach), and RMM [13, 32, 42]. They found that SMM and RM3 are the most effective in their experiments, and RM3 is more robust to the setting of feed-back parameters.

However, most of these PRF approaches estimate the im-portance (or probability) of the candidate expansion terms based on their statistics, while the proximity information is always ignored.
Term proximity is the co-occurrences of terms within a specified distance. Particularly, the distance is the number of intermediate terms in a document. Plenty of work has been done to integrate term proximity into both probabilis-tic and language models. Keen [10, 11] firstly attempted to import term proximity in the Boolean retrieval model by introducing a  X  X EAR X  operator. Buttcher et al. [3] pro-posed an integration of term proximity scoring into Okapi BM25 and obtain improvements on several collections. Ra-solofo et al. [24] added additional weight to the top docu-ments which contain query term pairs appearing in a window through a two-phase process, but the improvement is some-what marginal. Song et al. [31] presented a new perspective on term proximity. Query terms are grouped into phrases, and the contribution of a term is determined by how many query terms appear in the context phrases. In order to make it clear that how we could model proximity and incorporate a proximity measure, Tao et al. [33] systemically studied five proximity measures and investigated how they perform in the KL-divergence retrieval model and the Okapi BM25 retrieval model. Under the language modeling framework, Zhao et al. [45] used a query term X  X  proximate centrality as a hyper parameter in the Dirichlet language model. Lv et al. [15] integrated the positional and proximity information into the language model by a different way. They defined a positional language model at each position in documents by create virtual documents based on term probation.
All the above work focuses on how to utilize the proxim-ity information of query terms to avoid documents which contain scattered query terms. This kind of documents should be punished because they are very likely to be irrel-evant. For example, a document contains both  X  X apan X  and  X  X arthquake X  X s possible to be irrelevant to the topic X  X arth-quake in Japan X  if these two terms are not close in the con-text. It could be biased to only  X  X arthquake X  and mention some technologies in  X  X apan X  about  X  X arthquake X . Term proximity is effective to discriminate against these types of documents. Although there have been plenty of efforts in integrating proximity heuristic into existing retrieval mod-els, research work about how to utilize this information for pseudo relevance feedback is still limited. Vechtomova et al. [34] combined several distance factors with Mutual In-formation for selecting query expansion terms from windows or passages surrounding query term occurrences. However, marginal improvements were observed in the experiments. Lv et al. [16] presented two methods to estimate the joint probability of a term w with the query Q at every posi-tion in each feedback document. This is an extension of the state-of-the-art relevance model [13], and significant im-provements were obtained on two collections. Besides the work presented [16, 34], it is difficult to find other systemat-ical work about formally modeling term proximity heuristic in the context of pseudo feedback, especially in the classic Rocchio X  X  model.

In this paper, we propose PRoc models which integrates the proximity information of terms into the traditional Roc-chio X  X  framework. Three kinds of proximity measures are introduced to estimate relevance and importance of the can-didate terms. In order to integrate term proximity into the Rocchio X  X  model, we re-interpret the definition of term fre-quency and introduce a new concept, proximity term fre-quency. Unlike in [34], we conduct our study on a mature feedback model which has proven to be effective. Instead, the work of Vechtomova et al. was based on the Mutual In-formation which failed to perform as well as the traditional feedback model. In contrast to the work in [16], we try to employ proximity heuristic in a formalistic framework which extensively differs from the language modeling framework. Indeed, we do not concern ourselves with the position of each candidate term as in [16]. Meanwhile, to confirm the effectiveness of our model, we compare the performance of PRoc with that of PRM in Section 5. The experimental re-sults show that our proposed PRoc is at least competitive to the most recent work, PRM. In this section, we present the proposed proximity-based Rocchio X  X  model, called PRoc. Specifically, we first briefly introduce the traditional Rocchio X  X  model, and present the adaption of Rocchio X  X  model for proximity information by proposing a new concept, namely proximity-based term fre-quency ( ptf ). Then we describe in details about how to adopt ptf in three investigated proximity measures.
Rocchio X  X  model [28] is a classic framework for implement-ing (pseudo) relevance feedback via improving the query representation. It models a way of incorporating (pseudo) relevance feedback information into the vector space model (VSM) in IR. In case of pseudo relevance feedback, the Roc-chio X  X  model without considering negative feedback docu-ments has the following steps: 1. All documents are ranked for the given query using a 2. Each document in the feedback set R is represented 3. The representation of the query is finally refined by
Many other relevance feedback techniques and algorithms [2, 4, 26] are also derived under the Rocchio X  X  framework. For example, Carpineto et al. proposed to compute the weight of candidate expansion terms based on the divergence between the probability distributions of terms in the top ranked doc-uments and the whole collection. In this paper, we also take advantage of this distributional view. But we re-interpret the definition of term frequency in the KLD formula 2 in-stead of the distribution estimated from a set of top docu-ments. We use the following function to rank the candidate terms: where P ( w | d ) is the probability of candidate expansion term w in feedback document d , P ( w | C ) is the probability in the retrieval collection C .

Traditionally, candidate terms are ranked by their weights in the feedback documents, and the weights are affected by term frequencies in these documents extensively. However, the normal term frequency cannot capture the characteris-tic that whether a candidate term occurs near or far away from the query, such that the candidate term may not be relevant to the query topic. In other words, if the occur-rence of a term is far away from the query terms, it should notbecountedintheeffectivetermfrequencybecausethis term is very likely to be irrelevant to the query topic. Thus, we propose a new concept, proximity term frequency ( ptf ), which models the frequency of a term as well as the seman-tics to the query in terms of proximity. In order to adapt the proximity information, we re-interpret the definition of term frequency by proposing three kinds of proximity measures: window-based method, kernel-based method and the hyper-space analogue to language method. The main research chal-lenge now we are facing is how to evaluate ptf .Inthefol-lowing subsections, we introduce three measures to compute the ptf . Meanwhile, the importance of query terms is also taken into account. A very frequent query term is likely to be close to many candidate terms, which makes it difficult to distinguish the related feedback terms. Inverse document frequency ( idf ) of query terms is integrated to calculate ptf.
The first method adopts a simple window-based n-gram frequency counting method, which has been popular in pre-vious studies on using term proximity for IR (e.g. [17, 20]).
The basic idea of the window-based n-gram counting method is to segment the document into a list of sliding windows, with each window having a fixed window size wSize .Ifa document has a length of l , and the window size is set to wSize , the document is then segmented into l-wSize sliding windows, where each window contains wSize consecutive to-kens. For example, if a document has four tokens A, B, C, and D, and the window size is 3, there are two windows in this document, namely A, B, C and B, C, D. The n-gram frequency is then defined as the number of windows in which all n-gram terms co-occur. There could be two variants of the n-gram models, namely the ordered and unordered n-gram models. The ordered n-gram model takes the order of occurrences of the n-gram terms into account. For the same n-gram terms, the n-grams in which the composing n-gram terms appear in different orders are considered as different n-grams. In contrast, the unordered model ignores the order of occurrences of the n-gram terms. Actually, only a rough distance between terms is considered in this measure. If two terms are in the window, they are strongly related and the co-occurrence is counted in ptf . where q i is a query term, C(t, q i ) is the number of windows in which the candidate term and the query terms co-occur, |
Q | is the number of query terms, and IDF ( q i )equalsto log ( N  X  N t +0 . 5) / ( N t +0 . 5). N is the number of documents in the collection, and N t is the number of documents that contain q i .

The n-gram counting method has the advantage of be-ing straight-forward, and can be easily deployed in practice. It does not take the actual distance between query terms into account directly, and any n-gram terms appear together within a window is counted as one occurrence of the n-gram. If a term is very close to a query term, its co-occurrence count with the query term will be more than that of a term far away from this query term in the sliding windows. This variant of PRoc is denoted by PRoc1 in the rest of this paper.
Following to previous studies [16, 44], an alternative method we use is a kernel-based method to count the term frequency in a document. There are a number of kernel functions (e.g. Gaussian, Triangle, Cosine, and Circle [44]) which were used for measuring the proximity. Gaussian kernel has been shown to be effective in most cases. In this paper, we also use the Gaussian kernel to measure the proximity between a candidate expansion term t and a query term q . where p t and p q are respectively the positions of candidate term t and query term q in a document,  X  is a tuning pa-rameter which controls the scale of Gaussian distribution. In other words,  X  has a similar effect as the parameter wSize in window-based method. In order to keep the consistency with other proximity measures, we also use wSize to denote  X  . Different from the window-based method, the kernel-based method is a soft proximity measure. In particular, even if the appearance of a candidate term and a query term is not in a window of wSize , its weight can still be slightly boosted.
In this method, beside the average proximity to the query, we also take into account the importance of different query terms. Therefore, we build a representational vector for the query, in which each dimension is the weight of a query term by the inverse document frequency formula below, and then the proximity-based term frequency ptf in the Kernel-based method is computed as follows: where q i is a query term, | Q | is the number of query terms, and IDF ( q i ) is the same as in PRoc1. N is the number of documents in the collection, and N t is the number of documents that contain q i . The second variant of PRoc is denoted by PRoc2 in the rest of this paper.
The Hyperspace Analogue to Language (HAL) [12] is a computational modeling of psychological theory of word mean-ing by considering context only as the words that immedi-ately surround the given word. The basic motivation is that when a human encounters a new concept, its meaning is de-rived via other concepts occurred within the same context.
An shown in [39], the HAL Space is automatically built from a corpus of text, defined as follows: for each term in a specified vocabulary V ,a | V | X | V | matrix is built by mov-ing a sliding windows of length wSize across the corpus, where | V | is the number of terms in vocabulary V .Allwords within the window are considered as co-occurring with each other with strengths inversely proportional to the distance between them. The weightings of each co-occurred terms are accumulated over the corpus. Then, a term can be rep-resented by a semantic vector, in which each dimension is the weight for this term and other terms as follows: where k is the distance from term t to t , n(t, k, t X ) is the co-occurrence frequency within the sliding windows when the distance equals k ,and w ( k )= wSize  X  k + 1 denotes the strength.

In this paper, we adapt the the original HAL model simi-larly as in [12]. In particular, in order to measure the prox-imity between a candidate expansion term and the original query, we restrict the context to the query terms, not all the co-occurred terms in the feedback documents. With this adaption, the resulting vector for each candidate term de-notes a proximity relationship with the entire query. Like the Kernel-based method, we also take into account the im-portance factor of query terms in the same way. Then, the HAL based ptf is as follows: IDF ( q i )istheasinPRoc1
In the adaption of proximity information in PRF, ptf re-places the traditional term frequency in our approach.
The weighted HAL model includes the information of term distances and co-occurrence frequencies completely. It is the first time that this linguistic model is adopted to measure the proximity. The third variant of PRoc is denoted by PRoc3 in the rest of this paper.
In this section, we describe four representative test collec-tions used in our experiments: Disk4&amp;5, WT2G, WT10G, and GOV2, which are different in size and genre. The Disk4&amp;5 collection contains newswire articles from various sources, such as Association Press (AP), Wall Street Journal (WSJ), Financial Times (FT), etc., which are usually considered as high-quality text data with little noise. The WT2G col-lection is a general Web crawl of Web documents, which has 2 Gigabytes of uncompressed data. This collection was used in the TREC 8 Web track. The WT10G collection is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Gigabytes of uncompressed data.

The GOV2 collection, which has 426 Gigabytes of uncom-pressed data, is crawled from the .gov domain. This collec-tion has been employed in the TREC 2004, 2005 and 2006 Terabyte tracks. GOV2 is a very large crawl of the .gov do-main, which has more than 25 million documents with an uncompressed size of 423 Gigabytes. There are 150 ad-hoc query topics, from TREC 2004 -2006 Terabyte tracks, as-sociated to GOV2. In our experiments, we use 100 topics in TREC 2005 -2006. The TREC tasks and topic numbers associated with each collection are presented in Table 1. As we can see from this table, we evaluate the proposed ap-proach with a relative large number of queries. In all the Table 1: The TREC tasks and topic numbers asso-ciated with each collection.
 experiments, we only use the title field of the TREC queries for retrieval. It is closer to the actual queries used in the real application and feedback is expected to be the most useful for short queries [42].

In the process of indexing and querying, each term is stemmed using Porter X  X  English stemmer [22], and stopwords from InQuery X  X  standard stoplist [1] with 418 stopwords are removed. The MAP (Mean Average Precision) performance measure for the top 1000 documents is used as evaluation metric, as is commonly done in TREC evaluations. The MAP metric reflects the overall accuracy and the detailed descriptions for MAP can be found in [35]. We take this metric as the primary single summary performance for the experiments, which is also the main official metric in the corresponding TREC evaluations.
In our experiments, we compare our PRoc models with the traditional combination of BM25 and Rocchio X  X  feed-back model. In addition, we also compare the proposed models with the state-of-the-art feedback models in the KL-divergence language modeling (LM) retrieval framework. In particular, for the basic language model, we use a Dirich-let prior (with a hyperparameter of  X  )forsmoothingthe document language model as shown in Equation 8, which can achieve good performance generally [43]. Besides, this is also utilized as the basic model in [16]. where c ( w d ) is the frequency of query term w in document d , p ( w | C ) is the probability of term w in collection C and is the length of document d .Wetraintheparameterinthe document language model in all the experiments in order to make fair comparisons, and focus on evaluating different ways of approaching the query-related topic for PRF.
For PRF in language modeling framework, we first com-pare our proposed model with the relevance language model [13, 14], which is a representative and state-of-the-art approach for re-estimating query language models for PRF [14]. Rel-evance language models do not explicitly model the relevant or pseudo-relevant document. Instead, they model a more generalized notion of relevance R . The formula of RM1 is: The relevance model p ( w | R ) is often used to estimate the feedback language model  X  F , and then interpolated with the original query model  X  Q in order to improve its estimation as follows: This interpolated version of relevance model is called RM3 Lv et al. [14] systematically compared five state-of-the-art approaches for estimating query language models in ad-hoc retrieval, in which RM3 not only yields impressive retrieval performance in both precision and recall metric, but also performs steadily. In particular, we apply Dirichlet prior for smoothing document language models [42].
As we can see from all the PRF retrieval models in our ex-periments, there are several controlling parameters to tune. In order to find the optimal parameter setting for fair com-parisons, we use the training method presented in [7] for both the baselines and our proposed models, which is popu-lar in the IR domain for building strong baselines. In partic-ular, first, for the smoothing parameter  X  in LM with Dirich-let prior, we sweep over values from 500 to 2000 with an in-terval of 100. Meanwhile, we sweep the values of b for BM25 from 0 to 1.0 with an interval of 0.1. Second, for parame-ters in PRF models, we empirically set the number of top documents to 20 in baseline PRF approaches and our PRoc models, the number of expansion terms ( k  X  10 , 20 , 30 , 50), and the interpolation parameter (  X   X  0 . 0 , 0 . 1 ,..., 1 . 0). The window size for PRoc models are from 10 to 1500 with an interval 10. To evaluate the baselines and our proposed ap-proach, we use 2-fold cross-validation, in which the TREC queries are partitioned into two sets by the parity of their numbers on each collection. Then, the parameters learned on the training set are applied to the test set for evaluation purpose as in [19]. Table 2: BM25 vs LM on the four TREC collections
As we mentioned in the previous section, the results of both models are obtained by 2-fold cross-validation. There-fore, it is fair to compare them on these four collections. BM25 slightly outperforms LM with Dirichlet prior on the WT2G collection. The results of these two models are al-most the same over the Disk4&amp;5, WT10G and GOV2 col-lections. This comparison indicates that the classic BM25 model is generally comparative to LM, and it is reasonable to use them as the basic models of the PRF baselines and our proposed model.
From Table 2 to Table 6, we can clearly see that the av-erage performance of PRF models is superior to the basic models in most cases. The classic Rocchio X  X  model achieves improvements of 13.31%, -0.41%, 2.34% and 4.22% over BM25 on the Disk4&amp;5, WT2G, WT10G and GOV2 col-lections, while RM3 obtains significant improvements over LM (4.05%, 7.98%, 3.64% and 3.32%) on all the four collec-tions 1 . The effectiveness of pseudo relevance feedback is re-confirmed in this set of experiments. The classic Rocchio X  X  model, fails to obtain improvement on the WT2G collection. This indicates that the Rocchio X  X  model is not so robust as RM3 in this case. However, the Rocchio X  X  model outper-forms RM3 on the Disk4&amp;5 collection significantly while RM3 performs better than the classic Rocchio X  X  model on the WT2G collection. On the WT10G and GOV2 collec-tions, their results are very close. Therefore, the Rocchio X  X  model is generally comparable to RM3 so that it is still com-petitive to be a strong baseline.

In general, the performance of our proposed PRoc mod-els is close on all the four collections, and all of them obtain more improvements over the basic models than the Rocchio X  X  model and RM3. Specifically, from Table 3 to Table 6, we observe that all the three proximity-based Rocchio X  X  mod-els outperform the classic Rocchio X  X  model (2.00% -11.54%) and state-of-the-art RM3 (4.78% -12.75%) significantly on all the four collections, which demonstrates the effectiveness of the three PRoc models. Although the measures in our PRoc models are different, all of them can successfully model the proximity information to some extent. Furthermore, the PRoc models perform more robustly than the classic Roc-chio X  X  model. It indicates that proximity plays an important role in discriminating relevant expansion terms from irrele-vant ones.

In addition, from Table 4 we observe that PRoc3 outper-forms the other two on the WT2G collection. On the other three collections, the performance of all the three PRoc mod-els is very close. Generally, PRoc3 is slightly more effective than the other two PRoc models.
The computation of these percentages is based on the aver-age performance of the Rocchio X  X  model and RM3 in Table 3  X  6 and MAPs in Table 2.
In our proposed PRoc models, there are two important parameters: (1)  X  in the feedback models controlling how much we rely on the original query and the feedback infor-mation and (2) window size parameter wSize in the calcu-lation of the proximity-based frequency. In our preliminary experiments, we observed that the influence of  X  is similar to that in [38], which investigated this parameter thoroughly. Since we mainly focus on the study of proximity evidence, detailed discussion about  X  will not be made in this paper. wSize is a key parameter for most proximity measures because it determines the distance in which terms are con-sidered to be related. Thus, how to find an appropriate win-dow size is very important for adapting the proximity mea-sures. In this section, we attempt to discover some useful evidence for obtaining optimal wSize values. Particularly,  X  in PRoc2 is also interpreted as the window size. From Figure 1 to 4, we show how the performance of our PRoc models changes with wSize on different collections. We investigate a large range of wSize from 10 to 1500, and the numbers of expansion terms are 10, 20, 30 and 50. Gen-erally, the values of wSize affect the performance of all the PRoc models extensively. In the second subfigure of Figure 4, the best MAP is 0.3205 when number of expansion terms is 50, and it falls to 0.2286 when wSize is 1500. Almost 30% of performance is lost in this case.

For PRoc1, PRoc2 and PRoc3, although they are based on different measures, their curves fluctuate similarly on the same collection with different numbers of expansion terms. In contrast, the curves of each PRoc model are various exten-sively on different collections. This demonstrates that the influence of wSize is collection-based. However, the best wSize values for PRoc models are not the same, not even close to each other. For example, on the disk4&amp;5 collection, optimal wSize values for PRoc1 are 80, 50, 100 and 80 over 10, 20, 30 and 50 expansion terms, and the corresponding values for PRoc1 on WT10G is 100, 30, 10 and 10. Thus, the optimal values of wSize depend on the proximity measures and the collections.

Another phenomenon is that the more the expansion terms are selected, the more the performance is affected by wSize . Normally, the performance of PRoc models drops when wSize takes a relatively large value. However, to what extent the performance is affected is determined by the number of ex-pansion terms. Specifically, in Figure 2, while wSize is rela-tively small, the performance of PRoc models with 50 expan-sion terms is the best. However, when wSize is larger than 200, the curves for 50 expansion terms are constantly below all the others. As an additional example, when the number of expansion terms is 30, the performance of PRoc models is the second worst in most cases when wSize is 1500.
This is reasonable because the accumulated influence of proximity information for 50 expansion terms is larger than that of the small numbers. When wSize increases, it is very likely that more noise is adopted in the expansion term selection. The more expansion term are there, the more noisy information is involved. Thus, the wSize must be set very carefully when the number of expansion is larger than 30 in our case.

Additionally, the influence of wSize on PRoc1 is more significant than on the other two over WT10G. Meanwhile, wSize affects PRoc2 more significantly over GOV2 than PRoc1 and PRoc3. Overall, the PRoc3 model is less sensi-tive than the other two PRoc models according to our ex-periments.

In summary, a big challenge is to find an optimal value since the value space is very large without any constraints. It is very time-consuming to try every possible values in the rel-evance feedback process. In order to narrow the value space of wSize , we attempt to find a rule to direct the searching of optimal values. Based on our experimental results, we conjecture that there are two factors affecting the choice of wSize : the average document length (ADL) and the size of a collection. Intuitively, if the average document length is large, it is more likely to have more than one topic in a docu-ment which leads to involve more irrelevant terms. Besides, as the increase of the size of collection, it is more likely to bring irrelevant documents into the feedback process.
In order to minimize the negative influence of noise, the values of wSize should be relatively small especially when the ADL or collection size is large. Only the closest terms will be considered to avoid the selection of irrelevant terms. In our experiments, this rule is supported by some evidence. The ADL of Disk4&amp;5 is 334 and there are only 528,155 doc-uments in this collection. The performance of all the PRoc models is not affected by wSize so extensively as that on the other collections. When there are only 10 expansion terms, the optimal value of wSize can be as large as 1500. On WT10G, which has 1,692,096 documents and an aver-age document length of 426, the optimal wSize values are larger than 50 but smaller than 200. For WT2G, even it has less documents (247,491) than other collections, the optimal wSize values are in a range of (30, 50) because of its long ADL (722). GOV2 is the largest collection with 25,178,548 documents in our experiments, and its ADL(679) is only smaller than that of WT2G. As a result, the optimal values of wSize for GOV2 is the smallest one. It is always 10 in our case. In summary, we can use this rule to narrow the search space of optimal wSize values. If a collection has plenty of documents or its ADL is large (e.g., more than 700), it is always good for us to start from 20 or smaller. Otherwise, we can try a larger starting value like 50 or more.
We also compare our proposed model with the recently developed position relevance model (PRM) [16], which is an extension of the relevance model. In particular, PRM takes into account term positions and proximity with the intuition that words closer to query words are more likely to Table 7: PRoc compares with the classic Rocchio X  X  model, RM3, PRM1 and PRM2 on Tera06 dataset.
 The bold phase style means that it is the best result. MAP 0.3283 0.3156 0.3131 0.3322 0.3319 P@10 0.5800 0.5800 0.5043 0.5306 0.5490 P@30 0.5260 0.5167 0.4660 0.4884 0.4871
P@100 0.3756 0.3664 0.3576 0.3671 0.3741 be related to the query topic, and assigns more weights to candidate expansion terms closer to the query. To make the comparison fair, we train our parameters on the Terabyte05 topics and use Terabyte06 2 topics on the GOV2 collection for testing as Lv. et al. did in [16]. Since we do not give results for the Million Query Track so far, we do not compare our method with PRM on the ClueWeb collection with the topics of this track. In [16], parameter  X  in the Dirichlet smoothing is set to an optimal value of 1500, and we set b in our basic model, BM25, empirically to 0.3 [44]. As we mentioned previously, the performance of BM25 and LM with Dirichlet smoothing does not differ significantly on the GOV2 collection. Therefore, this setting will not affect the comparison. Since PRoc3 is the most robust and performs the best generally, it is selected to make this comparison. There are two versions of PRM, PRM1 and PRM2. The results of RM3, PRM1 and PRM2 are directly from [16].
In Table 7, PRoc3 outperforms the classic Rocchio X  X  model and RM3 significantly in terms of the MAP metric, and it is only slightly inferior to PRM1 and PRM2 by 1.19% and 1.1% respectively. On the P@10, P@30 and P@100 metrics, PRoc3 obtains the best results over all the other four models and outperforms RM3, PRM1 and PRM2 significantly. All these significant tests are based on the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. This shows that the retrieval accuracy of our proposed model is better than that of the PRF models in the language modeling framework in this case. Since the results of PRM1 and PRM2 are opti-mized, it is reasonable to state that our propose model is at least comparable to the most recent progress.
In this paper, a novel feedback model PRoc is proposed by incorporating proximity information into the classic Roc-chio X  X  model. Specifically, we model the statistics of ex-pansion terms and their proximity relationship with query terms by introducing a new concept ptf . Three proxim-ity measures, namely window-based method, kernel-based method and the HAL method, are then proposed for eval-uating the relationship between expansion terms and query terms. The corresponding PRoc models based on these mea-sures, PRoc1 , PRoc2 and PRoc3 , are evaluated extensively on four standard TREC collections. In general, PRoc is very effective and outperforms the state-of-the-art feedback mod-els in different frameworks. Comparing the three variants of PRoc, PRoc3 is more effective and robust than PRoc1 and PRoc2. Meanwhile, our proposed PRoc is at least compet-itive to the most recent work, PRM. Additionally, we care-fully analyze the influence of the parameter of wSize ,and an empirical rule to narrow the value space of the window size is suggested. http://trec.nist.gov/data/terabyte.html
In the future, we will try to discover more about how to effectively adapt proximity into the probabilistic retrieval models. Another possible research direction is to find the exact relationship between the window size factor and the information of collections, e.g., the length distribution of documents. It is also interesting to apply our work to other retrieval frameworks, like DFR or the language modeling framework. This research is supported by the research grant from the Natural Sciences &amp; Engineering Research Council (NSERC) of Canada and the Early Researcher Award/ Premier X  X  Re-search Excellence Award. We thank four anonymous review-ers for their thorough review comments on this paper. [1] James Allan, Margaret E. Connell, W. Bruce Croft, [2] G. Amati. Probabilistic models for information [3] Stefan B  X  uttcher, Charles L. A. Clarke, and Brad [4] G. Romano C. Carpineto, R. de Mori and B. Bigi. An [5] Charles L.A. Clarke, Gordon V. Cormack, and [6] Kevyn Collins-Thompson. Reducing the risk of query [7] Fernando Diaz and Donald Metzler. Improving the [8] Ben He, Jimmy Xiangji Huang, and Xiaofeng Zhou. [9] Xiangji Huang, Yan Rui Huang, Miao Wen, Aijun An, [10] E. Michael Keen. The use of term position devices in [11] E. Michael Keen. Some aspects of proximity searching [12] Ruth Ann Atchley Kevin Lund, Curt Burgess.
 [13] Victor Lavrenko and W. Bruce Croft. Relevance based [14] Yuanhua Lv and ChengXiang Zhai. A comparative [15] Yuanhua Lv and ChengXiang Zhai. Positional [16] Yuanhua Lv and ChengXiang Zhai. Positional [17] Donald Metzler and W. Bruce Croft. A markov [18] Donald Metzler and W. Bruce Croft. Latent concept [19] Donald Metzler, Jasmine Novak, Hang Cui, and [20] Vassilis Plachouras and Iadh Ounis. Multinomial [21] Jay M. Ponte and W. Bruce Croft. A language [22] M. Porter. An algorithm for suffix stripping. Program , [23] Karthik Raman, Raghavendra Udupa, Pushpak [24] Yves Rasolofo and Jacques Savoy. Term proximity [25] Stephen E. Robertson. On term selection for query [26] Stephen E. Robertson, Steve Walker, Micheline [27] Stephen E. Robertson, Steve Walker, Susan Jones, [28] J. J. Rocchio. Relevance feedback in information [29] Gerald Salton. The SMART Retrieval System . [30] Gerard Salton and Chris Buckley. Improving retrieval [31] Ruihua Song, Michael Taylor, Ji-Rong Wen, [32] Tao Tao and ChengXiang Zhai. Regularized estimation [33] Tao Tao and ChengXiang Zhai. An exploration of [34] Olga Vechtomova and Ying Wang. A study of the [35] Ellen M. Voorhees and Donna Harman. Overview of [36] Ryen W. White and Gary Marchionini. Examining the [37] Jinxi Xu and W. Bruce Croft. Improving the [38] Zheng Ye, Ben He, Xiangji Huang, and Hongfei Lin. [39] Zheng Ye, Xiangji Huang, and Hongfei Lin. A [40] Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin. [41] ChengXiang Zhai. Statistical language models for [42] Chengxiang Zhai and John Lafferty. Model-based [43] Chengxiang Zhai and John Lafferty. A study of [44] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. [45] Jinglei Zhao and Yeogirl Yun. A proximity language
