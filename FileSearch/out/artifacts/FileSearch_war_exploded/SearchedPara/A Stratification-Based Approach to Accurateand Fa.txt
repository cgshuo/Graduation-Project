 Image annotation is an important research problem in content-based image re-trieval (CBIR) and computer vision with broad applications. For a given image, such as a picture, we want to extract the semantics of the image in the form of a list of semantic concepts (or called semantic keywords). For example, the image at the top of Figure 1 can be annotated using three semantic concepts: tiger , stone ,and grass .
 training set of images that are annotated by human experts is provided to train and test an annotation system. After training, the annotation system is to an-notate images that are not in the training set. Therefore, the critical problem becomes how to efficiently build an accu rate model from the training data set and apply the model in the annotation.
 els for accurate image annotation. Please see Section 4 for a brief review. While the existing methods differ from each other in one way or the other, most of them treat an image as a set of image blobs and analyze the semantic concept of each image blob ( that is in the image region level ) to build a vocabulary of blob word to represent the whole image.
 corresponds to multiple semantic concepts. Treating an image as a whole may not help to identify the features that are highly informative for some specific semantic concepts. For example, in th e image shown at the top of Figure 1, the area of yellow/white stripes may strongly suggest the semantic concept of tiger , and the area of green with texture may i ndicate the existence of the semantic concept grass . If we can divide the image properly, then we may be able to make a good annotation of the image.
 mantic concepts is still a challenging problem due to the semantic gap. In fact, it is basically another image annotation. Therefore, instead of using blob words to describe the semantic of the image, we believe that it is more effective to rep-resent semantic concepts of images by some low level visual feature descriptors (called  X  visual words  X ) just analogous to the roles of keywords in text docu-ments 1 .Moreover, visual words can be learned more accurately at the image blob level than from the whole image. 1.1 General Ideas In this paper, we develop a novel stratification-based approach to effective and fast image annotation. The framework is shown in Figure 1.
 gion likely corresponds to a single object in the image. This stratification ap-proach is an application of the divide-and-conquer strategy. The intuition is that fewer objects an area corresponds to, more probably the semantic concepts can be correctly annotated. In order to segment the image properly, we apply the normalized-cuts method [13], which partitions an image into regions such that each region is consistent in visual features.
 for annotation. Na  X   X vely, one may want to extract a semantic concept (i.e., a blob word ) for each region, and simply take the set of the semantic concepts as the annotation of the whole image. However, such a na  X   X ve method may not be effective. The segmentation of image s into regions may not be very reliable. Regions are related in an image. That is, directly extracting semantic concepts from a region may not be accurate. Moreover, regions should not be treated equally. For example, a region with a substantially larger area should carry a heavier weight in the determination of the semantics of an image.
 We extract the visual features from each region, such as color and texture. Then a learning method is employed to discret ize the corresponding visual features of these regions to form a visual word vocabulary , analogous to the keyword vocabulary in the text document processing domain. With these visual words, we can describe the image mo re accurately than using blob words .
 topic-concept distribution and contributions of regions are also considered. In the training phase, a na  X   X ve Bayesian model is built to capture the correlation between semantic concepts and visual words. Moreover, in many data sets, images are divided according to topics. Two images are in the same topic fold if they share similar semantic concepts. In the annotation phase, we also consider the topic-concept distribution, i.e., the topic information is used in the annotation. We also consider the size of regions  X  the corresponding visual words are weighted in the annotation phase. 1.2 Our Contributions and the Organization of the Paper We develop a novel stratification-based approach for image annotation. While the general ideas are described in Sectio n 1.1, the concrete technical approach is developed in the rest of the paper. We conduct an extensive performance study using real data sets and compare with the state-of-the-art methods. The exper-imental results clearly show that our approach outperforms many traditional image annotation methods, and has an accuracy comparable to the state-of-the-art Continuous-space Relevance Model, but our approach is dramatically more efficient  X  it is over 200 times faster in our experiments.
 to stratify images and how to extract visual words from regions. Annotation of images using visual words and corresponding algorithms are addressed in Section 3. The factors of topic information and weighted visual words are also considered. We review related work in S ection 4. An extensive performance study is reported in Section 5. The paper concludes in Section 6. 2.1 Stratification Images can be segmented into several sub-regions with particular semantic mean-ings, analogous to partitioning a text document to paragraphs or sentences. For each region, more lower-level descriptors can be derived such as the visual fea-tures, which are analogous to the words in a text document. We can describe the structural organization of images in a hierarchy of three levels, namely images, regions, and visual descriptors, as shown in Figure 1.
 image in the following steps: 1. Image segmentation.We segment each image into different regions using the 2. Visual feature extraction for each region, such as color, texture, etc. 3. Image feature discretization and visual word vocabulary generation using a represented by a set of segmented regions: I = { r 1 ,...,r m } .Eachregion r i is represented by a fixed number of visual words: r i = { f i 1 ,...,f iM } ,f ij  X  V .We define an indicator function g on regions and visual words: By summing up visual words from all regions in an image, we represent an image as a vector of visual words: I =( s 1 ,...,s k ), where s i is the number of regions containing visual word v i : s i = m j =1 g ( r j ,v i ).
 of blob words, our stratified image model has two major advantages.
 tion. For example, in the Corel dataset used by [4,6,7,11], an image only has 1-10 blobs. Nevertheless, an image can have 36-360 visual words in the same dataset, which help our model behave more accurately in probability estimation. word after region clustering. All low-level visual feature information are ignored. There is no remedy if a region is clustered i ncorrectly. Although a similar problem exists in feature discretization, by keep ing all low-level visual feature elements we greatly reduce the impact of an incorrect classification. 2.2 Extraction of Visual Words To generate a visual word vocabulary, we need to convert feature vectors of regions into discrete-valued vectors. Ma ny methods [5,3,9,6] can be used to dis-cretize real-valued data . We employ a supervised method presented by Fayyad and Irani [5] to discrete r eal-valued visual features. The method is based on a minimal-entropy heuristic. As shown in [3], this method often achieves the best performance among supervised discretization methods.
 Minimal-Entropy Based Discretization. Given dataset S = { s 1 ,...,s n } and class label set C ,let v i be the continuous data value of s i ,and c i be the class label of s i . The entropy of dataset S is: where S ( c ) denotes all data points with class label c in S . A boundary T par-titions S into two sets S 1 = { s j | v j &lt;T } and S 2 = { s k | v k &gt;T } .Thebest boundary T minimizes the entropy after partition, which is: Then the boundary selection is repeated on S 1 and S 2 . The partitioning process is applied recursively until some certain stop criteria is satisfied. Information Gain is defined as the entropy reduction after a partition.
 In Fayyad and Irani X  X  approach [5], the recursive partitioning process stops iff: where k i denotes the number of class labels represented in S i . Since partitions are evaluated independently using this criteria during the recursive procedure, the continuous space is not evenly partitioned. Areas having relative high entropy are likely to be partitioned more finely.
 Discretization of Visual Features. We describe each region using a 36-dimensional visual feature vector, which includes the average rgb color, the av-erage lab color, area, the mean oriented energy and the area, etc. We assume that each image region inherits all keywords from its parent image. The keywords associated with an image region serve as the class labels for visual features in discretization. Given all data values on one dimension of visual features along with associated class labels, the minimal -entropy based discretization(MED) is applied to this dimension. Since MED can only handle data with a single label, data with multiple labels needs to be decomposed into multiple data entries each with a single label. For example, we should decompose a data entry valued at 0.35 with class labels  X  X iger X ,  X  X rass X ,  X  X ky X  into 3 data entries all valued at 0.35 but with class labels  X  X iger X ,  X  X rass X , and  X  X ky X , respectively. discrete bins on all dimensions form the visual word vocabulary. The size of the visual word vocabulary is one of the key aspects influencing the model perfor-mance. To control the granularity of discretization, we add a parameter  X  (the value of  X  is 30 in our experiment) to tight the stop criteria: We also use a minimum partition size to constrain the discretization not to partition too finely in areas which have relative high entropy. After applying this modified MED on every dimension of visual features, we build a vocabulary having 424 visual words. 3.1 A Na  X   X ve Bayesian Model Given an un-annotated image I , we first employ a segmentation method [13] to split it into regions r 1 ,...,r n . Then a feature-extraction algorithm [4] is em-ployed to derive a feature vector from a region. Let f i denote the set of visual words extracted from region r i ,and f i = { a i 1 ,a i 2 ,...,a im } . For each keyword w in pre-defined keyword vocabulary W , we use the logarithmic probability log P ( w | f 1 ,...,f n ) to estimate the relationship between the image and word w . Without loss of generality, we can assume that p ( w ) is a constant, and P ( f 1 ,...,f n ) is independent of word w , which can be neglected in the com-puting of the annotation. Thus, we have: The set of visual words from different r egions are assumed to be independent. For each f i =( a i 1 ,...,a im ), we again treat every visual word in the set statis-tically independent. From (9), (10), and (11), the posterior probability distribution of words for a given un-annotated image I is derived as follows: The denominator of (13) is a normalizing factor. According to our stratification model, an image is represented by a visual word vector ( s 1 ,...,s k ). Therefore, we can simplify (13): where { v 1 ,v 2 ,...,v k } denotes the visual word vocabulary. P ( v i | w )canbeesti-mated by counting the occurence of v i in all images annotated with w . 3.2 Annotation with Weighted Visual Words The commonsense tells that larger the regions, often more decisive to the theme of the image . Thus, the larger regions should be assigned heavier weights. In this subsection, we discuss an extension to our basic method. In this extended method, the size of regions is considered.
 image. The weight of region r is defined according to x r : where n denotes the number of regions in image I and  X  determines the degree of x r affecting the weight of visual words in region r . x r would have more significant influence on region weighting as  X  increases. We optimize the performance on a small test set to pick the best  X  value.
 section 3.2: In a weighted stratified image model, we represent an image as a vector of visual words: I =( s 1 ,...,s k ), where s i is the total weight of regions containing the visual word v i : s i = n j =1 h ( r j ,v i ).
 3.3 Annotation with Topic Information Most datasets, which can be used as training sets like Corel dataset [4,7,11,6], are categorized into folds. Images within the same fold may have different manual annotation but they all share a similar topic. Can we use the topic information to improve the annotation? annotations of the images in this fold. We estimate the probability of word occurrence by counting the manually annotated words of images in the fold. The heuristics here, which incorporates the topic distribution in the annotation, is that the keywords with a higher probability in the fold are  X  X opular X , and thus may have a higher probability to be used to label new images exhibiting the similar topic .
 un-annotated image is divided into two phases. First, we estimate the prob-ability distribution of the keywords in the un-annotated image by employing equation 13.
 which has the most similar word distribution to the un-annotated image. We employ the negative Kullback-Liebler divergence to model the similarity of two un-annotated image I ,and TS denote the set of all topic models in the training set. The topic T ( I ) of image I is defined as follows: Then the keyword distribution of I and T ( I ) are incorporated to determine the final annotation of the image I .Let G ( . | I ) denote the above mixture distribution, then for each keyword w ,wehave: where  X  determines the degree of interpolation between P ( . | I )and P ( . | T ( I )). P ( w | T ( I )) can be estimated by counting the occurence of w in all images in Topic T ( I ). All the keywords are ranked according to G ( . | I ), and the top-k keywords with the largest probability are picked out as the annotation of the image. To label an image, most previous methods employ segmentation to divide the images into blobs [4,7,11,8], or even grids [12,6], and then the joint probability is estimated on the image regions and the keywords of a pre-defined vocabulary. posed by Mori, et al. [12] assigns image annotations to every region of the image. In this model, auto annotation is based on the frequency of the co-occurrence of a word and an image region. Li and Wang [10] proposed a multi-resolution 2D hidden markov model to view the gen eration of images as a random process following an underlying probability distribution. The model represents an image by feature vectors under several resolutions. This hierarchical structure helps to catch the spatial context of an image.
 model, image regions are clustered into a vocabulary of blobs. Each image is rep-resented by a number of blobs from the vocabulary. To annotate images, they em-ploy a classical translation machine model to translate a vocabulary of blobs to a vocabulary of words. Jeon, et al. [7] proposed a cross-media relevance model. In-stead of finding the corresponding word for each blob, this model learns the joint probability of all blobs and words within an image. Their experiments showed that the method outperforms the co-occurrence model and translation model significantly on a 5000-image corel dataset. Lavrenko, et al. [11] proposed the continuous-space relevance model which can be viewed as a continuous version of CMRM. They employ a non-parametric kernel-based density estimate to learn the probability of continuous feature vector occurrence. It has a much better per-formance than the cross-media relevance model but suffers from low efficiency. to relate words with images. Feng, Manmatha and Lavrenko [6] proposed the multiple Bernoulli relevance model which uses a Bernoulli process to generate words instead of assuming a multinomial distribution over words. Jin, Cai and Si [8] proposed a coherent language model for image annotation that takes into account the word-word correlation. 5.1 Dataset In our evaluation, we use a 5000-image corel dataset provided by Duygulu [4]. The data set is available at http://www.cs.arizona.edu/people/kobus/research/ data/eccv2002. All images are already segme nted into regions using normalized-cut algorithm [13]. Each image contains 1-10 image regions. A 36 dimensional feature vector is extracted from each reg ion. There are 371 different concepts used in the manual annotation. Each image is associated with 1-5 words to describe the essential semantics of the image. This dataset was first used by Duygulu to evaluate the translation model [4]. It was also used by Jeon and Lavrenko to evaluate the cross-media relevance model [7] and the continuous-space relevance model [11].
 4500 images as the training set. 5.2 Performance Comparison We compare the annotation performance of our model with other four different models, including the co-occurrence model [12], the translation model [4], the cross-media relevance model [7] and the continuous-space relevance model [11]. After auto annotation, we use each word from the vocabulary to perform single-word retrieval in the test set. We judge whether an image is correctly retrieved by looking at its manual annotation. We define F measure: we denote the Bayesian approach without incorporating region weighting and topic model as BA, the Bayesian approach only incorporating topic model as BA-T, the Bayesian approach with region weighting and topic model as BA-TW.
 is significantly better than the co-occurre nce model, the translation model and the cross-media relevance model. BA-T W has a noticeable improvement over BA and BA-T. The continuous-space relevance model and BA-TW have similar per-formance in F-measure. Please note that the continuous-space relevance model uses real-valued feature vectors. This is often more effective than the discretize values, but is also much more costly in terms of runtime, as shown in the next subsection. 5.3 Efficiency Comparison We compare the annotation efficiency of our model with the cross-media rel-evance model [7] and the continuous-space relevance model [11]. We focus on the efficiency of the three models in annotating new images, so the time cost in training and the time cost in segmentation of un-annotated images are not taken into consideration. We record the time for the three models to annotate 500 images. All experiments is done on a laptop PC which has one P4 1.8Ghz CPU and 384 Megabytes main memory.
 CRM and about 3 times faster than CMRM. To understand the experimental results, we analyze the computational complexity of annotating one image in the three models as follows.
 annotate one image is O ( | W | X | V | ), where W is the text word vocabulary and V is the visual word vocabulary. A large-scale training set is very useful for models to learn the relationships between images and words. Since the computational cost of annotation in CMRM and CRM depends on the size of training set, our model has a better scalability than these two models on large-scale training sets. In this paper, we proposed a stratified image description and a Bayesian model for image annotation. We showed that this model has a good balance between performance and efficiency, as well as a good scalability. Our model employs the Minimal-Entropy based Method for feature discretization. We use the region weighting technique and the topic model-based enhancement to improve the annotation performance.
 our model. We believe that a better understanding in the relationship between regions is very useful for learning the semantics of an image. We will also consider using some multi-dimensional discretizati on methods to discretize feature vectors in order to catch the dependency of features.
 This work was supported in part by the NSF of China under grant number 60403018, NSF of Shanghai o f China under grant nu mber 04ZR14011, the NSF Grant IIS-0308001, and the NSERC Discovery Grant 312194-05. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

