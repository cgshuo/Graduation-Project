 Since the rise of collaborative tagging systems on the web, the tag recommendation task  X  suggesting suitable tags to users of such systems while they add resources to their col-lection  X  has been tackled. However, the (offline) evalua-tion of tag recommendation algorithms usually suffers from difficulties like the sparseness of the data or the cold start problem for new resources or users. Previous studies there-fore often used so-called post-cores (specific subsets of the original datasets) for their experiments. In this paper, we conduct a large-scale experiment in which we analyze dif-ferent tag recommendation algorithms on different cores of three real-world datasets. We show, that a recommender X  X  performance depends on the particular core and explore cor-relations between performances on different cores.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Recommender; Core; Graph; Evaluation
Recommender systems often have to deal with sparse data in which only little or nothing is known about many users or items. Alongside works that specifically tackle this task, it is common to focus on a dense subset of the data [12, 5] that contains enough information to produce helpful recommen-dations. For graph data, a commonly used technique are generalized cores [1] which comprise a dense sub-graph in which every vertex fulfills a specific constraint, e.g., the de-gree of each node is above a certain threshold. The influence of these cores on the performance of different recommenda-tion algorithms has not been analyzed so far.

In this paper, we investigate cores that have been used in the evaluation of tag recommender systems . Tag recom-Table 1: The number of users, tags, resources, tag assignments (tas), and posts of the datasets and the levels l chosen for the experiments. ommendation algorithm based on latent dirichlet allocation on a post-core at level 100 of a dataset from Delicious.
In the following, we describe the setup of our experiments to test different evaluation procedures with different cores, levels, and metrics for tag recommender algorithms.
We use three publicly available datasets from two collab-orative tagging systems (cf. Table 1): The BibSonomy 1 dataset (from 2012-01-01) is based on the regular dumps of the publicly available data. 2 The generation of the dataset is described in [6], including a more in-depth description of the data. BibSonomy supports tagging of both bookmarks and publication metadata, hence we split the data into two parts: book and publ . From Delicious 3 we use a dataset ( deli ) we obtained from July 27 to 30, 2005 [4] which is a subset of the Tagora dataset. 4 As Lipczak et al. [9] pointed out, tags from automatically imported posts are problematic for train-ing and evaluating tag recommenders, since their provenance is unknown. The ability of a recommender to (not) predict such tags does not allow us to draw any conclusion about its performance on predicting user-generated tags. Hence, we applied a similar cleansing strategy as described in [9]: we removed sets of posts that were posted at exactly the same time by the same user. In addition, we cleaned all tags as described in [6], i.e., we ignored tag assignments with the tags imported , public , system:imported , nn , system:unfiled , converted all tags to lower case, and removed all characters which were neither numbers nor letters.
The dimensions of our experiments are the three datasets, the two different core types, the chosen levels (see Table 1), the recommendation algorithms, and the evaluation metrics.
The construction of the graph-cores and post-cores is de-scribed in [7]. The data of a tagging system is modelled as a tripartite hypergraph, which has users, resources and tags as nodes and a hyperedge between a user u , a resource r and a tag t , if the user u assigned the tag t to the resource r . The graph-core at level l is the largest possible subgraph of that graph, such that each node (i.e., user, tag, resource) occurs in at least l tag assignments, i.e., has a node degree of at least l . In contrast, the post-core at level l is the largest subgraph, such that each node occurs in at least l posts. Since one post can consist of several tag assignments (all http://www.bibsonomy.org/ http://www.kde.cs.uni-kassel.de/bibsonomy/dumps/ http://www.delicious.com/ http://www.tagora-project.eu/data/#delicious
The most prominent observation is that the performance at different core levels depends both on the dataset and on the algorithm. In Figure 1, we see exemplarily the pre@5 scores for the five algorithms over different levels for the graph-core of the datasets publ and deli . Although most of-ten the scores rise with increasing level, there are exceptions like most popular tags by user on publ and adapted PageRank on deli with (sometimes) sinking scores. The same plots for book (omitted due to space limitations) are similar to publ . The results for rec@5 are similar to those of pre@5.
Further, we leveraged the property that the graph-core always contains the post-core at the same level: Next to the scores on the graph-cores (  X  ) we plotted the scores of the same experiments with only a slight modification of Leave-PostOut X  X  post selection process. Where we usually choose one post per user at random, we now choose posts randomly such that they are also contained in the post-core (+). Com-paring the scores on arbitrarily chosen posts to those from the post-core, we see that in particular for low levels it is eas-ier to predict tags for posts from the post-core than for ar-bitrarily chosen posts. There are however some exceptions, in particular FolkRank and most popular tags by user . We conclude that focusing on posts from the the dense part of the data often overestimates the performance of recommen-dation algorithms.
Evaluating recommender systems usually has the goal to determine one algorithm that performs best on one or sev-eral datasets and therefore several algorithms are ranked according to their performance. Since several setups for ex-periments are possible  X  several core types, levels and met-rics  X  the question arises, whether the ranking of recom-menders varies depending on the chosen setup. To inves-tigate this question we determine the algorithm rankings, where the algorithms are ranked according to their recom-mendation quality. A ranking can be computed on the raw datasets, and on each of the two core types at all chosen levels. 5 Between two rankings (on two different setups) we can determine Pearson X  X  correlation coefficient r , as a mea-sure of how likely the score rankings of the recommenders are (linearly) correlated. The coefficient ranges from  X  1 (anti-correlation) through 0 (no correlation) to 1 (perfect correlation). As Pearson X  X  r takes the particular score val-ues (the value describing one recommender X  X  performance on one setup) of the algorithms into account, we addition-ally use another metric that only considers the order of the algorithms in a ranking: the number of discordant pairs d . 6 Given two rankings, the algorithms A and B are discordant, when in one ranking A performs better than B while in the other ranking B is better than A . Thus in our case of five algorithms, d is between 0 (the rankings agree completely) and 10 (one ranking is the reverse of the other).

Table 2 shows the mean pairwise (averaged over any pair of two different setups) values of r and d together with the standard deviations exemplarily for the metrics pre@5, rec@5, and MAP. We can observe that on no dataset we get
That is, 11 different setups (see Table 1).
This value is closely related to the ranking correlation mea-sure Kendall X  X   X  . Table 2: The mean pairwise Pearson X  X  r , the number of discordant pairs d in the algorithm rankings on different cores, and their standard deviation  X  . F igure 2: The mean pairwise Pearson X  X  correlation r and number of discordant pairs d over the cut-level k for the metrics rec@ k and pre@ k . order. On the two BibSonomy datasets, the values are sim-ilar: here, on average, in two rankings one or two pairs of recommenders have different order. Thus it is evident, that different setups lead to different conclusions about the rank-ing of algorithms. We computed which of the cores yield the ranking that is most consistent with the raw data. For all datasets these are the graph-cores at levels 2 and 3, i.e., the two largest cores.

The consistency of the rankings also depends on the par-ticular metric that is employed. In Figure 2, we see the mean pairwise values of r and d for rec@ k and pre@ k with k run-ning from 1 through 10. We see that for deli the consistency is quite stable for both metrics precision and recall. How-ever, for the other datasets the values vary and the highest consistency is achieved for k = 10.
We have analyzed the use of cores for the evaluation of tag recommendations. In the experiments, we have shown that using cores for offline evaluation has its pitfalls as rec-ommenders perform differently in different core setups of the same dataset. Focusing on one particular core can pro-duce non-stable results, evaluating the performance of rec-ommenders on another core type or at another core level might yield a different ranking of the algorithms. Thus, we have established, that next to the choice of the dataset(s), the evaluation procedures and measures, the choice of the core is of significance to the outcome of experiments. To minimize this influence, any evaluation should be performed either directly on the raw data or on several core types and levels. On the other hand, we could observe that even cores
