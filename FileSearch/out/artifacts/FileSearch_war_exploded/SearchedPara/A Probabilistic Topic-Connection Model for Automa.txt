
Dept. of ECE at University of Missouri in Columbia, MO, USA, 
Dept. of ECE at Drexel Univer sity in Philadelphia, PA, USA, The explosive increase of image data on Internet has made it an important, yet very challenging task to index and automatically annotate image data. To achieve that end, sophisticated algorithms and models have been proposed to study the correlation between image content and corresponding text description. Despite the success of previous works, however, researchers are still facing two major difficulties that may undermine their effort of providing reliable and accurate annotations for images. The fi rst difficulty is lacking of comprehensive benchmark image dataset with high quality text descriptions. The second difficulty is lacking of effective way to represent the image content and make it associate with the text descriptions. In our paper, we ai m to deal with both problems. To deal with the first problem, we utilize Wikipedia as external knowledge source and enrich the ont ology structure of ImageNet database with comprehensive and highly-reliable text descriptions from Wikipedia articles. To a ddress the second problem, we develop a Probabilistic Topic-Connection (PTC) model to represent the connection between latent semantic topic in text description and latent patterns from image feature space. We compare the performance of our m odel with the currently popular Correspondence LDA (Corr-LDA) model under the same automatic image annotation scen ario using cross-validation. Experimental results demonstrate that our model is able to well represent the connection between late nt semantic topics and latent patterns in image feature space, thus facilitates knowledge organization and understanding of both image and text descriptions. H.2.8 [ DATABASE MANAGEMENT] : Database applications  X  Data mining; Image databases; I.2.6 [ARTIFICIAL INTELLI-GENCE] : Learning General Terms : Algorithms, Experimentation, Theory. Probabilistic models, topic learning, Gibbs sampling, image feature extraction, automatic image annotation. The prevalence of digital imaging device, such as digital camera and digital video camera, has brought an increasingly large amount of stored multimedia data, especially digital images. With nearly a million new images being added in one day, the Flickr.com now hosts over 3 billion shots of user-uploaded images. In comparison, Facebook. com, another famous online image sharing platform, has already hit 4.1 billion images on its site. Manually annotating such a huge amount of image dataset is time-consuming, laborious and prohibitively expensive. Therefore, it is very important to achieve some extend of automation in image annotation, which is currently a very difficult, yet long-term cost-efficient way to face the challenge of enormous explosion of dig ital images. Breakthroughs in automatic image annotation will help to organize the massive amount of digital images, prom ote developing and studying of image storage and retrieval syst ems, and serve for many other applications such as product searching, online studying, online image-sharing, etc. Automatic image annotation is clos ely related to computer vision, image processing and content-based image retrieval [1, 2] the last decade, we have seen great advance in developing automatic image annotation syst ems, related works involve considering image annotation as a clustering/categorization problem [3, 4] , considering image annotati on as an image searching problem [5] , and considering image annotation as statistical modeling problem [6-9] . Despite the success of previous works, however, researchers are still f acing two major difficulties that could undermine their effort of providing reliable and accurate annotation for images. The firs t difficulty is lacking of comprehensive benchmark image dataset with high quality text descriptions. The second difficulty is lacking of effective ways to represent the image content and associate it with the text descriptions. High quality text descriptions of images play a critical role as training and benchmarking data in developing and evaluating an automatic image annotation system . Labeled image datasets such as Caltech 101/256 Categories [10, 11] , PASCAL [12] have been popular with the co mputer vision community as benchmark training datasets; thes e datasets provide high quality image captions, yet lack of dive rsity, as only a limited number of object categories are covered. Images and captions from online data source like Flickr.com and Facebook.com have high diversity, however, due to their social networking purpose, image captions are characterized by fr ee-form text and user-sensitive descriptions, thus too noisy to be directly used as benchmark data. The recently established ImageNet dataset [14] provides large scale ontology of image that is built upon the WordNet Structure Organized by an ontology structure, images in the ImageNet dataset are grouped into sets of cognitive synonyms (synsets), each expressing a distinct semantic concept. Due to its completeness and accuracy, the ImageNet dataset may potentially serves as benchmarking data for image annotation algorithms. One problem with ImageNet data set is that it still lacks of comprehensive text descriptions fo r image data. Therefore, in our research, we utilize Wikipedia as external knowledge source and enrich the ontology structure of ImageNet database with comprehensive and highly-relia ble text descriptions from Wikipedia articles. In automatic image annotation, how to bridge over the  X  X emantic gap X  [1] between user and image features is another major challenge. Specifically, the first step of this problem is to identify a set of image features that well preserve the semantic consistency of image content. In recent years, affine invariant local image detectors and descriptors [23-25] have exhibited very good performance in image categorization and semantic imag e retrieval across several well-known databases such as the Caltech 101 , the TRECVID and the Visual Object Classes (VOC) datasets [3,4, 10-12] . The major goal of developing a local descriptor is to make it invariant under image variations (such as affine tr ansform and illumination change), while maintaining high repeatability and discriminative power. Usually, it starts with a detection step which detects key points as local appearances of an image, then follows with a description step, in which local image patches containing these key-points are quantized into feature vectors in an affine invariant way. Comprehensive study conducted by Mikolajczyk and Schmid indicates that the SIFT descriptor [23] outperforms other affine invariant local descriptors due to its high robustness to image variations [26] . State-of-the-art image-representation methods further cluster and quantify local SIFT descriptors into visual code-words [24] , and then apply text-like indexing schemes on  X  X ag-of-visual words X  re presentation of images [9, 20-22] One problem with SIFT descriptors is that since it is derived from key-points, it may become le ss discriminative due to the quantification and increase of image number [20] consideration, it is suggested th at researcher may combine both point features (such as SIFT descriptor) and region features (which are derived from local hom ogeneous parts in objects) to improve the understanding of image content [27, 28] approach, we extract both SIFT features and Maximally Stable Extremal Region (MSER) [25] , a widely used image region feature, to provide an efficient and robus t representation of local image appearance. After representing image content as combination of SIFT and MSER features, the second step of the problem is to uncover latent semantic topics from the co-occurrence patterns of image content and corresponding text desc riptions. In the data mining and information retrieval commun ity, there has been a long time focus on using probabilistic models to study the correlation between image and text descriptions. Specifically, the Correspondence LDA (CorrLDA) model [6] , which is initially proposed by Blei et al. for automatic image annotation, provides a natural way to learn latent topics from text word and other entities (such as image features). As repr esented in Fig. 1, this model enforces great degree of corres pondence between word and entity topics. It first generates latent topic for each text word, resulting in a document-level mixture of word topics (Fig. 1b). This document-level topic mixture then replicates itself as the composition of entity topics, whic h is used to supervise the generation of associated entities, resulting in a direct connection between word and entity topics . Most recent extensions of CorrLDA model, including sophisti cated correspond topic models that extend to different kinds of entities (such as protein entities visual words, and ontology-based biomedical concepts follow the same generative process as the prototype CorrLDA model. Fig. 1 Graphical illustration of generating latent topics for Despite its great success in many data mining applications, the CorrLDA model may encounter so me problems when dealing with both images and text descri ptions. Since the text words and extracted image features have totally different characteristics, it is very possible that a word topic is connected to multiple entity topics each stands for a specific image feature pattern, instead of connected to only one entity topic. To better explain this problem, let X  X  consider a simple image-text modeling problem in Fig. 1, for simplicity, we name the entity topic of image feature as  X  X isual topics X . Assuming that we have a vocabulary of 6 words (branch, tree, leaf, species, animal and ground) and a total of 5 word topics each have a unique distribution of generating words (F ig. 1a). Take word topic 3 for example, it has high probability generating  X  X ranch X ,  X  X ree X  and  X  X eaf X  while low probability genera ting  X  X pecies X ,  X  X nimal X  and  X  X round X , so it may be related to the concept of forest. As a comparison, topic 5, which ha s high probability of generating  X  X ranch X ,  X  X pecies X  and  X  X nimal X  , may represents concept of branch splitting during animal species evolution. Now supposing that we have an image about needle-leaf forest and a piece of text description that explain the needle-leaf forest, and that we choose to represent image content by visu al code-words that are derived from SIFT descriptors. As we can see in Fig. 1, in the sense of single-word features, this example is almost  X  X niform topic X , which is mainly composed of t opic 3 (Fig. 1b). However, in the sense of image feature representation, this example is not really a  X  X niform topic X  case. Although th e image purely depicts the scene of needle-leaf forest, however, it still have multiple visual topics corresponding to different image re gions such as trunks, leaves, path, grass, etc (Fig. 1c). For ex ample, the visual topic  X  X runks X  may favor some visual code-words that occur more frequently in trunks (e.g. vertical lines); simila rly, visual topic  X  X eaves X  may in turn privilege other visual code-w ords such as blob-like structures. Since each region takes up similar portion of area in the image, there is no evidence that any of these visual topics be dominant in the entire image. Therefore, the shifting from word topic to visual topics is not as transparent as assumed in CorrLDA model. In our research, we argue that each word topic is related to multiple visual topics, with different connection strength respectively. Based on this assu mption, we propose a probabilistic Topic-Connection (PTC) model fo r automatic image annotation. The model is estimated via colla psed Gibbs sampling algorithm, while the parameter selection is achieved by studying the likelihood and perplexity. We compare the performance of our model with the Corr-LDA model unde r the same automatic image annotation scenario using cross-validation. The remainder of this paper is or ganized as follows. In Section 2, we describe the procedure of inde xing image and text description. In Section 3, we present the generative process of CorrLDA model and our probabilistic Topic-Connection model. Section 4 provides the collapse Gibbs sampling algorithms for inference and learning proposed probabilistic m odels. Section 5 reports the experimental results of the pr oposed method and compares our approach to the CorrLDA model. We conclude the paper in Section 6. ImageNet dataset [14] provides large scale ontology of image that is built upon the WordNet Structure. According to its latest release, ImageNet hosts a total of 15589 synsets of WordNet, with an average of 50-500 images under each synset. In our approach, we enrich image data from ImageNet dataset with high quality text descriptions from Wiki pedia articles to provide benchmarking data set for au tomatic image annotation. Wikipedia is one of the most comprehensive and well-formed electronic knowledge repositories on the web with millions of articles contributed collaboratively by volunteers. Because of its reliability, accuracy and neutral point of view, Wikipedia has been exploited as external knowledge source in many application of text mining [16-18] . Although Wikipedia is different from standard WordNet ontology, which is backed up by structured thesaurus, however, each article in Wikipedia only describes one single concept under a hierarchical categorization system. Therefore, the title of each article (which is a succinct phrase) still resembles an ontology term. This feature makes it possible to map a WordNet synset to a Wikipedia article (Fig. 2), which in turn provides text descriptions for images under this synset. In our research, we found matched Wikipedia articles for nearly 75 percent of the synsets we studied. In learning unambiguous semantic topics from text descriptions, polysemies and synonyms are the major barrier. In [9], the author suggests using both ontology-based biomedical concepts and single-word features to overcome the polysemy and synonym problems in biomedical literatures. In public domain, where ontology-based concept is not av ailable and domain knowledge is rare, we propose to use multiwor d phrases in conjunction with unigram features. The multiword phrases usually have unchanged meanings, thus reduce the ambiguity in unigram  X  X ag-of-word X  document model. Therefore, the indexing of text desc riptions involves two parts, i.e. the term indexing and the phrase indexing. The term indexing is simply achieved by calculating the term frequency of each word after lemmatizing and stop-word re moval. In our approach, the Van Rijsbergen's stop-word list [30] is used to remove non-content-bearing terms. We utilize the statistical extraction tool Xtract to identify frequent multiword phrases from the text description. The estimated precision is a bout 80%. Xtract uses four parameters, strength (k0), spread (U0), peak z-score (k1), and percentage frequency (T), to control the quantity and quality of the extracted phrases. In general, the bigger those parameters the higher quality but less quantity phrases will be produced. In our experiment, we set those four para meters to (1, 1, 4, 0.75) after extensive tuning and testing. In our approach, we choose to represent the image content by both point-level SIFT features (Fig. 3b) and region-level Maximally Stable Extremal Regi on (MSER) features (Fig. 3c). (a) Original image (b) SIFT features (c) MSER features Following the framework in [9], we extract and index SIFT features as follows. Firstly, we employ the Difference-of-Gaussian (DoG) salient point detector [23] to detect salient points from images. The detection is achieved by locating scale-space extreme points in the difference-of-Gaussian images and the main orientations of salient points ar e determined by image gradient. Then, image patches containing the salient points are rotated to a canonical orientation and divided into 4 X 4 cells. In each cell, the gradient magnitudes at 8 different orientations are calculated. Consequently, each salient point is described by a 128-dimensional SIFT descriptor. In this way, each image in the training dataset is represented as a set of SIFT descriptors. After that, K-mean clustering is performed to quantize all the extracted SIFT descriptors and produce a finite dictionary of appearance patterns called  X  X ode-book of visual words X , with each cluster center as a unique  X  X isual word X . In our approach, SIFT descriptors extracted from training images are grouped into 2000 clusters, thus the vocabulary size of visual words is 2000. Finally, the indexing of SIFT features is accomplished by computing the term frequency of visual words with respect to each image document. The Maximally Stable Extremal Region(MSER) is a widely used image feature to represent regions. Unlike the SIFT descriptors, which is derived from key-points, the detected MSER regions are local homogeneous parts in object s (Fig. 3c). Although the MSER detector output relatively smaller number of MSER features than SIFT descriptors, their distinct ness is higher. Specifically, MSER detection begins with segmenti ng a set of image regions whose inner intensity value is less than certain thresholds while all intensities around the region boundary is greater than the same threshold. After that, a maximally stable extremal region is obtained when the area of the segments changes the least with respect to the threshold [25] . Extensive study reveal that the set of MSER regions is closed under c ontinuous geometric transforms, thus providing an efficient affine invariant region detector for local image appearance [29] . We also extend MSER detector to multiple scales by constructing Gaussian pyramid and applying MSER detection separately in each resolution level. After MSER detection, each detected elliptical region is normalized to circular patch of cons tant radius. In order to further improve its scale and affine invariant capability, we rectify each patch to canonical orientation fo llowing the coordinate transform in [29]. We set the size of final normalized patches as 21 pixels by 21 pixels. Fig. 4 PCA component number settings for the quantification An effective descriptor for imag e patch representation should be compact while remaining highly di stinctive. Thus we chose to perform principle component analysis (PCA) on MSER normalized patches to provide a robust and compact representation of image regions. More specifically, we construct covariance matrix for a total of 140,000 MSER normalized patches extracted from training dataset, each of which is a 21 X 21 dimensional vector of image in tensity. Then, we perform eigen-decomposition on the covariance matrix to obtain the eigenvectors. We then obtain the projection matrix which is composed of first k principal components, i.e. eigenvectors corresponding to k largest eigen-valu es. In this way, we build the eigenspace for all the MSER normalized patches in our training dataset. To represent a new MSER patch, we simply multiply its 21 X 21 dimensional vector with the projection matrix to obtain its k dimensional projection. After extensive testing, we set k=50 (which means that all the MESR features are represented as 50-dimensional vectors). Reconstructi on result in Fig. 4 shows that, when using the first 50 principle components, we are able to achieve significant dimension reduction of the MSER features without losing important details. In this section, we introduce the proposed Probabilistic Topic-Connection (PTC) model that addresses the problem in Corr-LDA model which we mentioned in Section 1. We begin this section with the introduction of extended CorrLDA model, which is the state-of-t he-art in modeling image and associated text description [8, 9] . By presenting the generative process of CorrLDA model, we make explicit its problem of topic replicating from text to image. Then, in a series of steps, we show how we address the problem in CorrLDA model by introducing new latent semantic variables and relations to the generative process. In Figure 5, we provide graphical representations of both extended CorrLDA model and our m odel, in which we highlight the innovation part of proposed model by red dash line. Following the convention in depic ting graphical representation of topic models, we use round nodes to represent random variables, in which the light blue nodes sta nd for latent random variables, while the yellow nodes denote observed ones during the model training. The rounded boxes are used to represent fixed hyper-parameters of the model, while the edges illustrate the conditional dependency underlying the generative process. z w  X 
T1 For clarity, we name each image-text pair as one document . Some notations to be used in the two topic models are listed as follows: D is the number of documents, T is the anticipated number of latent topics, N w d is the total number of text words in document d , N d denotes the total number of extracted multiple-word phrases extracted visual words and MSER regions in document d , respectively. In the model, parameters ,, hyper-parameters for the Dirichlet distributions. In our approach, we assume symmetric priors, with ,, parameters. Detailed explanations of notations used in Figure 5 and following discussions are summarized in Table 1. The CorrLDA model is a generative model for correlated multiple type entity data. Similar to other generative models, the CorrLDA model assume that the observed data is generated by some parameterized random variable known as  X  X atent topics X . Specifically, a  X  X ord topic X  (denotes by  X  X  X  in Fig. 5a) is used to derive the generation of the text words from a topic-specific word distribution (e.g. for a word topic that is related to the concept of forest, the corresponding word distribution will have high probability generating words like  X  X ranch X ,  X  X ree X  and  X  X eaf X ). In a text document, the word topics (which usually relate to some semantic concepts) play an intermediate role between basic elements (words) and high level semantic meanings. counterpart of the word topic, each had a unique distribution over image features. Specifically, each visual topic is formalized as cluster of features that represent similar image appearance or fit the same distribution in the image feature space. For example, the visual topic  X  X runks X  may favor some image patterns that occur more frequently in trunks (e.g. vertical lines), while visual topic  X  X eaves X  may privilege some blob-like image patterns. After identifying latent topics and assigning topic labels to each entity in a document, each document may in turn be represented by a document-level mixture of la tent topics. The document-level topic mixture is defined as a probability distribution of latent topics with respect to each document, specifying which word topics are most likely to be generated from observed text description, or which kind of visu al topics are most likely to be generated from observed image features. When the instances from one entity type (say, the text words) serves as description of other entity types (such as tags, image features), the CorrLDA model directly use the latent topics of the former entities to generate the later entities. Consequently, both types of entities will share the same document-level topic composition, resulting in str ong correspondence between them. The generative process for the extend CorrLDA model (Fig.5a) is: In the step 1 of the generative process, a T -dimensional topic-prior vector  X  d is sampled for each document d , with the k dimension of the vector represen ts the prior probability of the k topic in d . For each document d , the generative process of the N words is achieved by sampling topics from the document-topic multinomial distribution (with prior  X  d ) and sampling words from the topic-word multinomial distribution (with prior  X  k ). After that, instead of being sampled from their own topics, all the other 1. For the d th ( d =1... D ) document, sample ~() 2. For the k th ( k =1... T ) topic, sample ~() 3. For each of the N w d words w i in document d : 4. For each of the N p d phrases p i in document d : 5. For each of the N v d visual words v i in document d : 6. For each of the N r d MSER region feature r i in document d : entities (phrases, visual words, etc) are sampled from the same topic as words. A closer look into the generative process of extended CorrLDA model reveals that, the document-level topic composition is only decided by the single-word feature, even though other entities description. What X  X  more, in the extended CorrLDA model, the image features are generated from the word topics. However, as we discussed in Section 1, each word topic may be related to multiple visual topics, enforcing word topics to image features may ignore such a relation and make topic modeling results inconsistent with the underlying image patterns. With this consideration, in our new model, we allow each word topic to connect to multiple visual topics, with different prior probabilities. This model also allo w for different number of word topics and visual topics. The generative process for the Probabilistic Topic-Connection (PTC) Model (Fig.5b) is: As represented in the generative pr ocess, new latent variables had been introduced to allow for more flexible sampling of word topics and visual topics. Specifically, latent variable s and s X  play the role as word topic indicators, while latent variable  X  the prior probabilities of word topic k connecting to any visual topics. For a given image feature, the model firstly sample a word topic indicator, then sample the visual topic according to the priori distribution of corresponding word topic connecting to different visual topics. In both models, we deal with four types of entities: single-word, multiple word phrases, visual words and MSER region features. The topic-specific single-word distribution is modeled as a multinomial distribution over W di fferent words, denoted by Multi(  X  ), in which  X  is a W-dimensional prior vector. Similarly, we model the topic-specific multiple word phrases distribution as multinomial distribution. The visual words have similar statistical properties with text words, thus assumed to follow multinomial distributions, too. As mentioned in Section 2.2, each 50-dimensional MSER feature vector is obtained from the projection to the first 50 principle components of image patch eigenspace. Therefore, each dimension of MSER feature is real-valued and thus follows a Gaussian distri bution, whose mean and variance are topic-specific. For all multinomial distributions, the Dirichlet distribution is used as prior, which yields posterior probability also in the form of Dirichlet distribution. For Gaussian distribution, the standard way is to draw its parameters from the our approach, for the algebraic convenience in calculation, we place a non informative prior over the parameters  X  and  X  which the actual mean and variance are approximated by sample mean and variance.
  X  A D  X  T matrix that indicates the document-topic  X   X  ,  X  In recent years, several methods have been developed for estimating the latent variable in topic model, such as the 1. For the d th ( d =1... D ) document, sample ~() 2. For the k th ( k =1... T 1 ) text topic, sample ~() 3. For the j th ( j =1... T 2 ) visual topic, sample ~() 4. For each of the N w d words w i in document d : 5. For each of the N p d phrases p i in document d : 6. For each of the N v d visual words v i in document d : 7. For each of the N r d MSER region feature r i in document d : variational expectation maximiza tion, expectation propagation, and Collapse Gibbs sampling [31] . Compared to the other two methods, Gibbs sampling is less co mputationally intensive, and often yields relatively simple algorithms for approximate inference [31] . With this consideration, we perform the Collapse Gibbs Sampling procedure for m odel estimation. In the Gibbs Sampling process, a Markov chain is constructed and converges to the posterior distribution on latent topics. The transition between successive states in the Markov chain is modeled by repeatedly drawing a topic for each observed entity from the conditional probability. Due to the space limit, we only introduce our implementation of the Gibbs Sampling for proposed PTC model. For the extended CorrLDA model, our implementation is similar with that outlined in [8] and [9].
 Given the generative process in Section 3.2, our objective is to compute the entity-topic posterior probability and sample topic for each entity from posterior probability. Thus, we derive the posterior sampling equations as follows, in which we follow the standard notations detailed in Table 1. Sampling a word topic ( The above posterior probability is obtained by integrating out (collapsing) all the latent variables Sampling a word topic ( ' Sampling a visual topic ( Sampling a visual topic ( ' which (1)()(50) ( ,..., ,..., ) nT In equation 4, the term in the form of ( ) () 2 student-t density with mean u , variance 2 / s n and n-1 degree of freedom. As we place a non-informative prior over the Gaussians, the mean and variance of each Gaussian are purely determined by their sufficient statistics (i.e. the sample mean u and sample variance 2 s , respectively). As a result, the student-t density function in equation 4 provides the confidence of drawing the value of () n r from a topic-specific Gaussian distribution (please refer to [32] Ch. 3.2 for detailed derivation of this conclusion). During the Gibbs Sampling proce sses based on above posterior distributions calculations, we may also update single latent variables in the following manner: In this section, we apply the proposed PTC model to topic learning and compare the performan ce of our model with that of the extended Correspondence LDA (Corr-LDA) model under the same image annotation scenario using cross-validation. The performance of automatic image annotation is evaluated by perplexity and annotation accuracy. The image dataset used in our study is downloaded from the ImageNet database (http://www.im age-net.org/) under the granted access permission, following the term of access. The ImageNet is built on the hierarchical ontology structure provided by WordNet, in which each node involves a group of images that depict a particular concept named as a synonym set, or  X  X ynset X . Specifically, we download a total of 508 synsets under the  X  X lower X  subtree, 1473 synset s under the mammal subtree and 1118 synsets under the tree subtree. Following the term mapping schema in Section 2.1, we map each synset to a Wikipedia article that describes the same concept. Then, we parse the structured content of Wikipedia articles a nd apply a rule-based method to identify the explanative sections . Unrelated sections such as  X  X xternal_links X  and  X  X eferences X  are removed from the articles. After that, to ensure the quality of text description, we filter out articles with insufficient words (&lt;200 words). The qualified articles then serve as text desc ription for corresponding ImageNet synsets. In total, we obtain comp rehensive text descriptions for 1452 synsets (330, 562 and 560 synsets for subtrees  X  X lower X ,  X  X ammal X  and  X  X ree X , respectively). For each of the 1452 synsets, we randomly select 5 images from the corresponding image group and adjust them to normalized size (640 X 480 pixels). After that, we replicate the text descriptions to each of the 5 images, resulting in 5 image-text pairs. As introduced in Section 2, we make index for single-words and multiple word phrases in the text descriptions, and extract visual-word features as well as MESR region features from images. In total, we indexed 5,699,505 word tokens which belong to 35,744 different words, 624,205 multiple word phrases from a total number of 13078 unique phrases, 7,945,075 visual words (an average of 1095 visual words per image) from a vocabulary size of 2000, and a total of 924,924 MSER region features (an average of 127 MSER regions per image). Th e original dataset is divided into 5 subsets with equal size. Of the 5 subsets, one subset (20%) is retained as the validation data for testing the model, and the remaining 4 subsets (80%) are us ed as training data. For image annotation evaluation, the cross-va lidation process repeats 5 times, with each of the 5 subsets used once as the validation data. After that, we take the average results for evaluation. The estimation of the proposed probabilistic topic model is achieved by performing Gibbs Sampling over training dataset until convergence (generally, the model takes less than 100 iterations to converge). Once the topic model is estimated from the training dataset, we will be able to evaluate it by log-likelihood and visualize the uncovered latent topics. Log-likelihood is one of the standard criteria for generative model evaluation. It provides a quantitative measurement of how well a topic model fits the training data. The score of log-likelihood (which is a negative number) is th e higher the better. In practice, the log-likelihood of elements given latent topics can be calculated by integrating out all the latent variables. In our study, we are interested in which topic model is more suitable to study the latent patterns of image features. Thus, instead of calculating word-likelihood, we choose to evaluate the log-likelihood of visual words for both models. In the proposed probabilistic topic-connection (PTC) model, The marginal likelihood of visual words v given all the visual topics y is p( v | y ), which can be calculated by integrating out latent variables  X  : The final log-likelihood of visual words is obtained by taking the logarithm of eq. (6) and averaging the resulting summation by V . For the extended Corr-LDA model, the log-likelihood can be calculated in a similar way, the only difference is, instead of using their own latent topics, the visual words in Corr-LDA model directly use latent topics generated from text words. In Fig. 7a, we plot the l og-likelihood for both models under different topic number (to make this comparison fair, the number of word topic and visual topics ar e made equal). It shows that our model has higher log-likelihood than Corr-LDA model, which means that our model fits training data better. It also shows that the log-likelihood of both models increase as the number of topic increase, which suggests that a relatively greater topic number may potentially fit the training data better. However, it should be noted that there is a trade-off between topic numbers and convergence time of model estimation, and the unbounded increase of topic number may results in an over-fitting problem. In order to better interpret the uncovered latent topics, we visualize the word topics by providing the top-ranked words, top-ranked phrases and most related imag es. As represented in Fig. 6, the words and phrases are sorted by their probability of being generated from a word topic, while images are sorted by the probability of containing that word topic (by counting the topic indicator variables of image features). Topic84 Topic116 Fig. 6 Illustration of uncovered latent topics by PTC model In Fig. 6, we present two exam ples of uncovered latent word topics. The former one is a topic re lated to the concept of  X  X rchid X , while the later one is a topic related to the concept of  X  X eopard X . By providing a combination of words, multiple word phrases and images, it becomes much easier to interpret the domain knowledge captured by each topic. As we can see, the uncovered latent topics show high consis tency to semantic concepts. Since we are targeting modeling the connection between image and text, we are interested in its ability of predicting missing text descriptions from image features. With this consideration, we conduct an automatic image annota tion experiment, in which we compare our model with the Corr-LDA model. During the experiment, the text description of testing data is considered as unknown (missing). The predictive ability of both models is evaluated by both perplexity and annotation accuracy. The perplexity is a standard criterion for topic models that evaluates how well the model predicts the new data. Specifically, the perplexity of a set of testing documents the exponential of the negative normalized per-word predictive log-likelihood using parameters fro m the trained topic model. The score of perplexity is the lower the better. With uncovered latent topics from training image-text pairs, the problem of estimating topic prio rs in testing images can be approximated by performing Gibbs sampling over observations of image features, while keeping all the topic-entity conditional probability fixed. It should be noted that we need to know the posterior probability of word topic indicators given visual topics: p( s | y ) when estimating the new docum ent-level word topic prior. In our study, this probability is approximated by counting the number of evidences across the training dataset. Upon the convergence of the Gibbs sampling process over testing data, the word perplexity of testing image-text pairs is: One advantage of our model is that it assigns different topic numbers to different types of da ta, which makes this model more suitable to deal with image and associated text. Fig.7b represents the perplexity comparison between our model and the Corr-LDA model as the increase of word t opic number, in which the number of visual topics in our model is fixed to 1000. It shows that the perplexity of our model is consistently lower than Corr-LDA model, which suggests that our m odel is 'less surprised' by the testing data, thus demonstrates better performance. Also, it shows that the predictive ability of our model may benefit from greater visual topic number, as it tend to have lower perplexity as the visual topic number increases (Fig. 7c) Upon the convergence of the Gibbs sampling process over testing data, the probability of annotating the  X  X issing X  words and phrases of an image can be calculated via the production of document-level word topics prior probability and the topic-word/phrase conditional probability. Words and phrases with highest probability are then used as the annotation. After that, the image annotations are compared with the ground truth, in which the cross-validation process repeats 5 times, and the results are averaged to produce the final annotation accuracy. In our study, the annotation accuracy of our model and Corr-LDA model are compared under their best performance (i.e. 1000 visual topics and 300 word topics for PTC m odel, and 500 word topics for Corr-LDA model). The experiment result (Fig. 8) shows that our model consistently outperforms Corr-LDA in both word and phrase annotations. In this paper, a probabilistic t opic-connection model is proposed to deal with the problem of mode ling images and associated text description. Specifically, new latent variables have been introduced to allow for more flexible sampling of word topics and visual topics, in which one word topic may connect to multiple visual topics. The proposed mode l provides better representation of the connection between latent se mantic topics and latent image patterns, thus achieves better performance in the task of automatic image annotation compared to the traditional Corr-LDA model. This research work is supported in part from the NSF Career grant IIS 0448023, NSF CCF 0905291, NSF IIP 0934197, NSFC 90920005  X  X hinese Language Seman tic Knowledge Acquisition and Semantic Computational Mode l Study, X  and the Program of Introducing Talents of Discipline to Universities B07042 (China). [1] A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and [2] Lew, M. S., et al. Content-based multimedia information [3] Jia Li and James Z. Wang, ``Real-time Computerized [4] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman, [5] Changhu Wang, Lei Zhang, Hong-Jiang Zhang. Learning to [6] David M. Blei, Michael I. Jordan: Modeling annotated data. [7] Gustavo Carneiro, Antoni B. Chan, Pedro J. Moreno, Nuno [8] Amr Ahmed, Eric P. Xing, William W. Cohen, Robert F. [9] X. Chen, C. Lu, Y. An, and P. Achananuparp. Probabilistic [10] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of [11] G. Griffin, A. Holub, and P. Perona. Caltech-256 object [12] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, [13] B. Russell, A. Torralba, K. Murphy, and W. Freeman. [14]J. Deng, W. Dong, R. Socher , L.  X  X . Li and L. Fei-Fei, [15] Christiane Fellbaum (1998, ed.) WordNet: An Electronic [16] Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, [17] Hu, J., Fang, L., Cao, Y., et al. Enhancing Text Clustering by [18] Wang, P. and Domeniconi , C. 2008. Building Semantic [19] F. Smadja, Retrieving collections from text: Xtract. [20] J. Yang, Y. G. Jiang, A. G. Hauptmann, C. W. Ngo, [21] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid, Local [22] Yu-Gang Jiang, Chong-Wa h Ngo, Jun Yang: Towards [23] Lowe, D. Distinctive Image Features from Scale-Invariant [24] Sivic, J., Zisserman, A.: Video Google: A Text Retrieval [25] J. Matas, O. Chum, U. M ., T. Pajdla. Robust wide baseline [26] K. Mikolajczyk and C. Schmid. A performance evaluation of [27] L.-J. Li, R. Socher and L. Fei-Fei. Towards Total Scene [28] Zhong Wu, Qifa Ke, M. Isard, Jian Sun,Bundling features for [29] Per-Erik Forss X n and David G. Lowe, "Shape descriptors for [30] Van Rijsbergen, C.J., Inform ation Retrieval, Butterworths, [31] T. L. Griffiths, M. Steyvers. Finding scientific topics. [32] A. Gelman, J. Carlin, H. Ster n, and D. Rubin. Bayesian Data 
