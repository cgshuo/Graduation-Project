 ORIGINAL PAPER Itay Bar-Yosef  X  Isaac Beckman  X  Klara Kedem  X  Itshak Dinstein Abstract We present our work on the paleographic analysis and recognition system intended for process-ing of historical Hebrew calligraphy documents. The main goal is to analyze documents of different writ-ing styles in order to identify the locations, dates, and writers of test documents. Using interactive software tools, a data base of extracted characters has been estab-lished. It now contains about 20,000 characters of 34 different writers, and will be distinctly expanded in the near future. Preliminary results of automatic extrac-tion of pre-specified letters using the erosion operator are presented. We further propose and test topological features for handwriting style classification based on a selected subset of the Hebrew alphabet. A writer identification experiment using 34 writers yielded 100% correct classification.
 Keywords Binarization  X  Character extraction  X  Writer identification  X  Document analysis  X  Historical documents 1 Introduction Paleography is the study of ancient handwritten manu-scripts. Among other things, it deals with dating and localizing ancient and medieval scripts, and with study-ing the development of the letter shapes. The work reported in this paper is part of a project to develop algorithms and tools for computerizing paleographic analysis of old Hebrew calligraphy scripts. (see Fig. 1 for example image.)
The Hebrew ancient handwriting used in our research, is influenced both by time and place X  X ifferent regions over different periods of time use different styles of the same alphabet. In this sense, paleographic research is related to script and writer identification (see Sect. 5). One of the important aspects of paleographic research is visual analysis of character shapes. The main problem in quantitative shape analysis of handwritten characters is the development of features that can deal with the variations of the character X  X  forms. The vari-ations between styles, and the changing morphology of the letters are sometimes represented by tiny detail differences.

The first published work regarding the use of image processing for paleographic research (as far as we know) was published in 1971 [1]. Colette Sirat [2] is the author of another early publication reporting the use of com-puter image processing methods for paleographic research. Features based on run-length histograms were used in [3] for style identification of ancient Hebrew handwriting. An expert system using document analysis strategies for authentication of Hebrew manuscripts is reported in [4].

We present an overall system for paleographic analysis and recognition of old Hebrew calligraphic documents. First, we present our novel approach for document binarization. Since our writer identification approach is based on style analysis of selected letters, we propose a segmentation-free approach for extract-ing the letters automatically. Finally, we describe the writer identification approach based on style analysis of the selected letters (note that the origin and date of the documents can be derived from the writer X  X  identity). Our paper is organized as follows: Sect. 2 describes our binarization method, Sect. 3 presents the database and ground truth generation. In Sect. 4 we lay out our novel character extraction method based on the erosion oper-ator. Section 5 describes our method for writer iden-tification, and Sect. 7 summarizes our work presented here. 2 Binarization method In general, historical document images are of poor qual-ity because the documents have degraded over time due to storage conditions, and to the quality of the written parchment. As a result, the foreground and background are difficult to separate. The problem is particularly diffi-cult because many documents have varying contrast, smudges, variable background intensity and presence of seeping ink from the other side of the document. We use the multi-stage algorithm presented in [5]. In the first stage, an initial binary image B is obtained by applying a global threshold. This suffices for noise free characters. Then, an evaluation procedure determines which of the connected components in B are well separated from the background, and which components need to be refined as we describe below.
 Note 1 Throughout this paper the term character refers to the graphic representation of a letter. The term letter refers to the alphabet, i.e., the letter Aleph, the letter Bet, etc. 2.1 Quality evaluation For each connected component in B , we create a seed image , which contains the low intensity pixels of the character. For each CC i ,i={1, ... ,N}, where N is the number of connected components, let m i be the mean gray scale value (in the original image) of pixels belong-ing to CC i . Let SD i be the seed image of CC i using m i a local threshold: SD i = In good characters the transition between the seed and the background forms a narrow band of pixels. In noisy characters this transitions are wider and irregular (see Fig. 2 for example). Denote by CC ={ CC 1 , CC 2 , ... , CC N } the set of connected components in B . We com-pute SD i , the seed image of CC i ,i= { 1, ... , N } . Denote by T i the pixels belonging to CC i but not to SD i , T i CC i  X  SD i . The pixels of T i belong to the transition between the seed and the background. For each pixel in T , we compute its distance to the closest pixel in SD i as follows. Within the bounding box of CC i , we treat SD i as the foreground and the rest as background ( CC i  X  SD i ). Then we apply a distance transform algorithm [6], which calculates for each background pixel (pixels in the set CC i  X  SD i ) the distance to the closest foreground pixel (pixels in the set SD i ). Denote by DT i the set of dis-tances calculated for T i .Weusethe variance of DT i as a measure to discriminate between good characters and noisy characters. Figure 2 shows two characters ( CC i ), their corresponding seed image (SD i ) and the difference between the character and its seed image ( T i ).
Notice Fig. 2a X  X , in which a well segmented charac-ter is presented. As can be seen, the set T i is composed of a narrow band of pixels, uniformly scattered around SD i . The mean value of the set DT i is  X  = 4.01 and the variance is  X  = 2.121. Fig. 2d X  X  depict a noisy character. The set T i for this character is in part narrow as that of a well segmented character, and in part wider and irregular where patches of noise are present. The mean value of the set DT i for this character is  X  = 6.02 and the corresponding variance is  X  = 30.31. As illustrated in these figures, the variance  X  i has a much higher value for noisy characters than for well segmented ones. Let  X  be the variance of the set DT i , and let  X  mean be the average of  X  i , i={1,...,N}.

If  X  mean  X   X  ths , where  X  ths is an empirically based threshold, we assume that the document is entirely degraded, and all of its components, CC i , are classified as noisy. Otherwise, the document is composed of both good and noisy characters. In this case, every component CC i ,with  X  i  X   X  mean is classified as a well segmented character. The rest of the components are processed using the local method described in the next section. Notice that the penalty on classifying a good character as a noisy one, is the extra computation time involving the calculation of the local method. 2.2 The local method Even when a character is noisy or faded, the seed body of the character is easy to detect. This fact led us to adopt a region growing scheme in which we first detect the seed image of the characters, and then apply a grow-ing process that expands the character to its final form (see Fig. 3).

For each CC i classified as a noisy character, we define the seed image as in Sect. 2.1. The Growing process is an iterative process in which during each iteration a set of candidate pixels is observed. Each pixel from this set is tested whether it can join the foreground or not. The process is terminated when no pixel is added to the foreground. The Algorithm goes as follows. Starting from the seed images SD i , repeat until the foreground set does not change: 1. Find all candidate pixels The candidate pixels are 2. Assign candidate pixels For each candidate pixel p ,
Figure 4a shows an original manuscript. Its binary image produced by this algorithm is shown in Fig. 4b.
The results of the binarization algorithm were evalu-ated subjectively, where in each document the percent-age of correctly segmented characters was counted. Our approach was evaluated on the 34 historical documents described in the next Section. These documents con-tained approximatley 20, 000 characters, were the per-centage of correctly segmented characters was 94%, where in a substantial set of documents the average percentage reached up to 98%. Most of the problems occured in documents where the character strokes had almost disapeared, and the accuracy of the adaptive local method was too sensitive to handle the rapid inten-sity changes. In few cases, the presence of bleedthrough caused small fragments due to the seed creation proce-dure, as can be seen in the middle of line 3 in Fig. 4b. For more details on the evaluation of the binarization method see [5]. 3 Calligraphic letter database Our data consist of 34 calligraphic manuscripts from the archive of the School of Library, Archive and Information Studies, the Hebrew university, Jerusalem, Israel . The manuscripts are from the fourteenth to six-teenth century, written in different parts of the world. The Hebrew alphabet, consists of 22 letters, five of them have special forms when appearing at the end of a word. 3.1 Ground truth generation In the document image analysis (DIA) research area, the term  X  X round truth X  refers to various attributes asso-ciated with the document. Parameters like the respec-tive letter ASCII code, the bounding box coordinates of characters, the size of each character, etc., are associated with characters. Information like the document X  X  writer identity, or any other global attributes, characterize the whole document. We have developed a visualization tool that enables interactive generation of the ground truth.

Consider the ground truth generation for a given gray scale manuscript. The user first enters the global information regarding the document. This includes the writer X  X  identity, the date and place of the document writing, and any other global relevant information. Then we apply the binarization algorithm presented previ-ously, and a standard connected component labelling operation. The labelled components are displayed, and the user can tag each connected component with the respective letter ASCII code. In cases where the user identifies a set of connected components that belong to the same character, the user can link the compo-nents and assign the tag to set of the connected com-ponents. The program computes the bounding box for the connected components of each character, their cen-ter of gravity, and their sizes. The sub-image within the bounding box is extracted and entered with the respec-tive information into the database. The visualization tool was developed using MATLAB , and we use Access as the data base.

Since historical documents often suffer from ink loss and smear, this causes a large number of broken and merged characters. A good OCR system for such doc-uments must handle well these phenomena. Following this observation, for each character, along with its ASCII code, bounding box coordinates and size, we specify whether it is broken, merged or degraded (see Fig. 5 for example). 4 Extraction of selected letters In Sect. 5 we describe our approach for writer identifi-cation of ancient manuscripts. Our method is based on style analysis of several selected letters. Since there is no transcription of the manuscripts, in order to automate the identification process we have developed a segmen-tation-free approach for extracting these letters. In this section, we describe our approach.

There are several papers dealing with retrieval of complicated characters or extraction of pre-defined symbols. A system for retrieval of Chinese calligraphic characters is reported in [7], where characters are rep-resented by an approximated point context. In Saykol et al. [8], features based on angular and distance span of shapes are used for symbol extraction. The symbols are maintained in a codebook for the purpose of content-based image retrieval of Ottoman documents. A seg-mentation-free approach for recognition of arabic text is presented in [10]. Text primitives are extracted using mathematical morphology in order to recognize words. They report promising results for symbol extraction and word recognition.

We propose a segmentation-free approach for extrac-tion of pre-specified letters based on the well-known erosion operator (for another use of the erosion oper-ator, see Haralick et al. [11]). The extraction process is composed of several stages: structuring element gen-eration, character extraction, character validation and structuring element adaptation. 4.1 Object-based erosion features Let I be a set of the foreground pixels in a binary image. Each object in I is represented by one or more con-nected components. See, for example Fig. 6, in which sets of connected components represent some Hebrew Calligraphic characters. Denote by ={  X  1 , ... ,  X  N } the set of object classes, each class representing a letter, where N is the number of letters. For each object class  X  , i ={ 1, ... , N } , we generate a structuring element S such that the number of translations in which S i is con-tained in an object class  X  i is maximal, and the number of translations in which S j is contained in an object class  X  when j = i is minimal.
 The erosion operation , causes objects to shrink. The amount and the way that they shrink depends upon the choice of the structuring element. We show that when a suitable structuring element is used, the con-nected components of substantial area in the binary image D n = I S n , are most likely associated with objects belonging to class  X  n . Consider the following definition of the erosion operation: D For each foreground pixel ( r , c ) in D n , the structur-ing element S n translated by ( r , c ) is contained in I . Denote the set of all foreground connected compo-nents of I by C ={ C 1 , C 2 , ... , C M } , and the set of all foreground connected components in image D n by CD n ={ CD n 1 , CD n 2 , ... , CD n L } (see Fig. 10b).
The following two claims shed light on relevant prop-erties of the relations between elements of CD n and C , based on the number of connected components in S n . Claim 1 If the structuring element S n is connected, then for each connected component CD n i in the eroded image D n , there exists a connected component C k such that CD n i = C k S n .
 Proof For each pair of connected (neighboring) pixels ( p , q )  X  S n and ( r , s )  X  S n ,theset ( S n ) ( p , q also connected.
 Claim 2 If S n is a union of K connected components, S CD n i , there are at most K connected components in C , such that where C n i , j is the j th component among the set of con-nected components representing the i th object of class  X  n .
 Proof S n = X  K k = 1 S n , k , where S n , k is a connected com-ponent. According to Claim 1 , there is one connected component, say C n i , k , such that CD n i , k = C n i , k is true for k ={ 1, 2, ... , K } . CD n i = n k = 1 C n i , k nected component containing all translations for which (
S be inter-connected, C n i contains at most K connected components.

Figure 7 illustrates a letter  X  X leph X  represented by two connected components (in gray color). Superim-posed on the letter (in white) is the structuring element dilated by the respective CD n i component. Notice that erosion followed by dilation is the well known open operator (for more details see Haralick et al. [9]). The object based erosion features are defined as E to the i th connected component in the binary image D n . It is the number of possible translations of S n such that S n is included in C n i . A high value of E n i indicated that the component C n i may represent an element belonging to w n . 4.2 Generating the structuring element The structuring element S n for class  X  n is generated in the following manner. Let C n i , i ={ 1, ... , T n } be T of connected components, representing a training set of T n elements of class ized to a standard height, and their widths are stretched by the same factors as their heights. Then, we calculate the maximum intersection (under translation) of these sets, and denote it by CS n . CS n = max ( T n i = 1 C n i set of pixels belonging to CS n is contained in each one of the training set elements. The structuring element S n is a pseudo medial axis of CS n (i.e., strokes are replaced by thin lines, and small blobs are replaced by few pixels at and around the center of mass). Figure 8 illustrates the process of generating a structuring element for the letter Aleph. 4.3 The letter extraction process The extraction process is composed of several stages. In the first stage, we use the erosion operator to extract the candidate letters. As can be seen in Fig. 9, there are some cases where the structuring element S n is contained in a combination of several characters. Therefore, in the sec-ond stage a validation procedure is invoked in order to decide for each extracted character, whether it belongs to class  X  n or not. In order to make the extraction pro-cess robust and insensitive to different writing styles, an adaptation process of the structuring element S n is applied in the last stage.

Letter extraction The extraction process of objects belonging to class  X  n ,isasfollows.Givenagray scale image, we first binarize it using the algorithm pre-sented in Sect. 2. This results in a binary image I .Fol-lowing the binarization, we normalize I  X  X  height such that lines height in I will be equal on all documents. This is done inorder to make the structuring elements S n , n ={ 1, ... , N } insensitive to different object sizes. Then we apply the erosion operator on I , using the structur-ing element S n of the training set. The eroded image D n contains a set of connected components, CD n i , each representing a match between S n and the corresponding component in I , C n i (see Fig. 10b).

The validation process For each C n i , i ={ 1, ... , N x where N x is the number of letters extracted with S n ,we compute a measure V n i for validation as follows. V where CS n is the maximum intersection (under transla-tion) between the training objects of class  X  n (0  X  V n i C THS = 0.9.

Adaptation of the structuring element When deal-ing with writing styles different from the style of the training set, the extraction process often yields poor results. In order to adapt to the new style, we generate a new structuring element based on the extracted let-ters. After extraction and validation of C n i ,weusethe extracted characters as a training set for generating a new structuring element S n as described in Sect. 4.2.
We summarize the overall letter extraction algorithm as follows: 1. Structuring element generation 2. Character extraction 3. Validation measure 4. Structuring element adaptation
Section 6.1 summarizes the experimental results. 96% correct extraction rate of the letter Aleph is achieved with twenty six manuscripts written by different writers. 5 Writer identification The writer identification problem is treated by compar-ing questioned handwriting with samples of handwrit-ing obtained from known sources for the purposes of determining the identity of the writer. In other words, it is the examination of the design, shape and struc-ture of handwriting to determine authorship of given handwriting samples. There are two main approaches to the off-line method, namely, text-dependent and text-independent .The text-independent approach uses fea-ture sets whose components describe global statistical features extracted from the entire image of a text, or extracted from a region of interest, therefore may be called texture analysis approaches. The text-dependent approach uses features extracted from one or a lim-ited group of characters. In Said [13], a text-dependent approach is presented, based on texture analysis of randomly extracted text blocks with Gabor filters. Schomaker et al. [14], evaluate the performance of edge-based directional features in comparison to several non-angular methods. Srihari et al. [15,16] performed a study on the individuality and discrimination power of charac-ters and words by combining micro and macro features. They showed that different handwritten characters, have different discrimination power.

We propose a text-dependent approach based on a selected set of letters X  X he letters Aleph, Ain and Lamed (see Fig. 11). For each letter, a feature vector is extracted, and using dimension reduction techniques, the most discriminative features are selected for writer identification. 5.1 Feature extraction Given a binary image representing a character, we decompose its shape into regions, using the convex hull of the character. Decomposing the character X  X  shape, reduces it X  X  complexity, and simplifies the description process. The convex hull H S of an arbitrary set S is the smallest convex polygon containing S . The set differ-ence H S -S is called the convex deficiency D of the set S . We use the set D to extract shape information from the character (Fig. 12).

Let B be the binary image of a letter in our docu-ment. Denote by S the set of all pixels where B ( i , j ) = and denote by D the convex deficiency of S .Theset D consists of a number of disjoint connected components included in H S . These connected components represent the character concavities. In each letter, some of the components, depending on the character shape, are of substantial size. The rest of the components are due to noise and are insignificant. We will refer to the large connected components as dominant background sets .In the letter Aleph, there are four dominant background sets, while in each of the letters Lamed and Ain, there are two substantial components. The number of dom-inant background sets is independent of character size and orientation.

Denote the sets of pixels belonging to the dominant background sets by D i ,i={1, ... , n}, where n is the number of dominant sets, according to the following:
Let D 1 be the component for which the y coordi-nate of its center of mass is maximal, and number D i , i={2, ... ,n} in a clockwise order. We use geometrical and topological parameters of the character B and of the sets { D style. We use the following notations: Let S be a set of pixels. Denote by | S |  X  X he number of pixels in the set S and denote by Major ( S ) , Minor ( S diameters of the major/minor axis of the ellipse having the same second moment as S respectively.

For each set D i , i ={ 1, ... , n } , the following features are extracted: nant background set and the convex hull.
 ellipse.

F 3i  X  X oncavity features. The character X  X  internal boundary, within D i , is divided into two segments by the highest curvature point in that segment (see Fig. 13c). The length ratio of these two segments is used as a rough estimation of D i  X  X  curvature.
 F 4i  X  X ompactness = 4  X   X  area ( D i )/ perimeter ( D i ) The compactness is defined as the ratio of the area of an object to the area of a circle with the same perimeter.
F 5i  X  X oment features. The 2D moment of order ( p + q ) of an image B ( x , y ) is defined as m pq =
Central moments are defined as  X  pq = x y ( x  X   X  x ) p ( y  X   X  y ) q B ( x , y ) , where  X  x = m 10 moments are translation invariant. In order to obtain scale invariance, we use the normalized central moment,  X  set of features are extracted from the character image B (as they were defined for D i in F 1i , F 2i above): F F F In total, the number of extracted features, depending on the chosen letter is 13* n +3, where n is the number of dominant background set. 5.2 Dimensionality reduction Several papers use feature selection and extraction tech-niques to find the most discriminative features. Wang et al. [17], uses principal component analysis (PCA) fol-lowed by linear discriminant analysis (LDA) to lower the feature dimension and to find the most discrimi-native features for writer identification. In [18], a script identification approach is reported based on feature sub-set selection from a large set of features.

Each of the letters used in our experiment has its own set of features that maximizes the discrimination between writers. In order to find the best set of fea-tures, we implement two well established techniques for dimensionality reduction. The first is feature selection algorithm which searches for the most discriminative features. The second is the Fisher linear discriminant analysis , which finds a linear transformation that maxi-mizes the classes separability. 5.2.1 Feature selection The problem of feature selection is defined as follows: given a set Y of d features, select a subset X of size m that leads to the smallest classification error according to a criterion function J . A natural choice for the criterion function is J = 1  X  P e , where P e denotes the classifica-tion error. A survey on feature selection algorithms can be found at [19 X 21]. Feature selection methods fall into categories of filter methods, which use feature selection as a preprocessing step to classification, and wrapper methods, which use classification internally as a means of selecting features. The simplest wrapper method is forward selection (FS). It starts with the empty set and greedily adds attributes one at a time. At each step FS adds the attribute that, when added to the current set, yields the best result. Once an attribute is added, FS can-not remove it later. Backward selection (BS) starts with all attributes and greedily removes them one at a time in a same manner. A powerful and widely used selection algorithm is the Sequential forward floating selection (SFFS) [22]. It is characterized by the changing number of features included or eliminated at different stages of the algorithm. We use the SFFS algorithm in the feature selection process. 5.2.2 Linear discriminant analysis LDA is a popular method for dimensionality reduction that searches for a linear transformation which maps a q -dimension vector x onto an r -dimensional ( r  X  q  X  1) vector Y = W T X , while retaining a maximum amount of discrimination information. For the c -class problem, let m i i = 1, ... , c be the i th class mean vector, with n samples. The within-class scatter matrix is defined as S between-class scatter matrix are defined as S total mean vector, P i is the a priori class probabilities of the i th class.

According to Fisher X  X  criterion, the transformation matrix W can be obtained by maximizing the ratio det ( W T S w W ) . It has been shown that the optimal transfor-mation matrix can be found by solving the generalized eigenvalue problem ( S b  X   X  i S w W ) = 0. The eigenvectors corresponding to the r largest eigenvalues then make up the rows of W . 5.3 Classification The classification we have employed is based on one let-ter at a time. We have used the letters Aleph, Ain and Lamed. Given an unknown document (writer), we ex-tract the characters Aleph, Ain and Lamed. The selected features are computed in a manner discussed in Sect. 5.2, and a classifier is applied. We used in our experiments both the K-nearest neighbors (KNN) classifier with K ={ 1, 5 } , and the Linear Bayes clas-sifier, assuming normal distributions [23].

Denote by N i the number of characters identified as writer i by the classification procedure, then the writer of themanuscript is identifiedas writer k ,if k = arg max ( N
Thirty four documents were used in our experiments, each written by different writer. A correct classification rate of 100% was achieved. The experimental results are reported in Sect. 6.2. 6 Experimental results 6.1 Letter extraction experiments The letter extraction process was evaluated on a set of twenty two documents. Eight documents were chosen randomly, from which the training set T was generated. After the generation of S n from T , we applied the letter extraction process on each of the twenty two documents. The results of the experiment are summarized in Table 1.
The 5.35% classification error is illustrated in the following figures. Part of the errors occur due to deg-radation of characters, as can be seen in Fig. 14a. The structuring element does not fit inside the character due to missing parts or holes. A reconstruction procedure which identifies and reconstructs the degraded charac-ters can solve this problem. Another kind of error is shown in Fig. 14b, depicting a stretched character at the end of a line. This is a common left justification in Hebrew calligraphy. These Alephs should be detected with a special structuring element.

The false detection errors mostly occur when the structuring element S n is contained in more then one character, as depicted in Fig. 15. Although the valida-tion procedure should handle these situations, there are some cases where more information is needed in order to correctly classify the character (i.e., word level information). 6.2 Writer identification experiments Thirty four documents were used in our experiments. Twenty Aleph, Ain and Lamed characters were extracted from each document. Two sets of experiments were conducted: The first experiment conducted for each letter (Aleph, Ain and Lamed) in a  X  X eave one out X  manner. For example, the 34  X  20 = 680 Aleph charac-ters were divided into a training set of 646 characters and a test set of 34 characters, one from each document. This classification was repeated 20 times, thus each character was classified once.

In the second experiment, each class was divided into two sets: a training set of fifteen characters, and a test set containing the remaining five characters (for each letter).

In both experiments, we compare the effectiveness of the dimension reduction techniques X  X DA verses SFFS.

Experiments show that the Linear Bayes classifier outperformed the KNN classifier in all categories. Table 2 shows the results of the first experiment. Using the SFFS selection algorithm to select the best set of features, followed by the LDA transform to further lower the feature dimensionality, performed slightly bet-ter then direct use of LDA.

Table 3 shows the results of the second experiment according to the decision in Sect. 5.3. At the first experi-ment, combining the SFFS selection algorithm and LDA performed the best. The best results were obtained by using the letter Aleph , which has a more intricate shape. 7 Summary In this paper we have presented our work on Paleo-graphic analysis of old Hebrew calligraphic scripts. We have developed an adaptive binarization method, which shows very good results. In Sect. 4 we presented a character extraction algorithm based on the erosion operator. We intend in the near future to extend the structuring element generation into a learning process in which for each writing style, the optimal structuring element is generated. In addition, we intend to auto-mate the training set generation based on maximiz-ing the intersection of the training elements. In Sect. 5 we presented a writer identification method based on extraction of a geometric parameters from several let-ters, followed by selection of the most discriminative fea-tures. Experimental results on 34 writers, yielded 100% correct identification. However, the result of our method should be examined on a much larger database. We plan to expand the database by at least an order of magnitude in the near future.
 References
