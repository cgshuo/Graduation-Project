 Feature-based collaborative filtering models, such as state-of-the-art factorization machines and regression-based la-tent factor models, rarely consider features X  structural infor-mation, ignoring the heterogeneity of inter-type and intra-type relationships. Na  X   X vely treating all feature pairs equally would potentially deteriorate the overall recommendation performance. In addition, human prior knowledge and other hierarchical or graphical structures are often available for some features, e.g., the country-state-city hierarchy for ge-ographic features and the topical taxonomy for article fea-tures. It is a challenge to utilize the prior knowledge to further boost performance of state-of-the-art models. In this paper we employ rich features from both user and item sides to enhance latent factors learnt from interaction data, uncovering hidden structures from features X  relation-ships and learning sparse pairwise and tree structural con-nections among features. Our framework borrows the mod-eling strengh from both structural sparsity modeling and la-tent factor models. Experiments on a real-world large-scale recommendation data set demonstrated that the proposed model outperforms several strong state-of-the-art baselines. H.3.5 [ Information Storage and Retrieval ]: Online In-formation Services Structured Sparse Regression, Structured Sparse Feature Graph Learning, Hierarchical Sparse Coding, Structured Sparse Coding, Feature-based Collaborative Filtering
Feature based latent factor models have received increas-ing attention in recent years due to its capability to effec-tively solve the cold-start problem. There have been many feature based collaborative filtering ( CF ) models proposed re-cently, which can be grouped into two categories. The first type of models includes all variants of latent factor mod-els ( LFM ) which have been proven as an effective approach to personalization and recommender systems. The core of LFM is to learn user-specific and item-specific features from user-item interactions and utilize these features for future predictions/recommendations. State-of-the-art LFM exploits low-rank latent spaces of users and features and treats latent factors that are learnt from user-item historical data as fea-tures. This type of models has gained significant successes in a number of applications, including the Netflix competi-tion. The second category is the factorization machine ( FM which explicitly learns the mapping function from features to rating score circumventing the dependency on user/item latent factors as in the latent factor models, resulting in an effective model for the cold start problem [ 9].

Although these feature-based CF models have been shown to be effective, they do not utilize the feature structure in-formation. For example, conventional latent factor models (e.g., matrix factorization or tensor factorization models) like RLFM [1, 2] learn mapping functions from user/item fea-tures to user/item latent vectors assuming the features have a flat first-order structure. Later, [ 10] showed that this kind of mapping can be extended to any non-linear models. Al-though the formalism is flexible, it leaves too much room for practitioners to choose which non-linear model to use for a particular application. Also, it is hard to incorporate human prior knowledge on the feature structure into the framework, unless through careful feature engineering, and the proposed inference algorithm is difficult to use in large-scale settings. Similar to RLFM , Gantner et al. [4] proposed a model to explicitly learn the mapping function from fea-tures to latent factors, resulting in an effective model for the cold start problem. But it still makes the flat first-order feature structure assumption. In the other line of work, Rendle et al. [9] proposed a more compact model called fac-torization machine FM . Basically FM is a second-order regres-sion model which directly maps the user-item-event concate-nated features to rating score by learning the implicit map-ping functions from features to latent factors, resulting in an effective model for the cold start problem. However, the issue of encoding structural human prior information still boils down to sophisticated feature engineering and it X  X  not clear how to incorporate the heterogenous feature structures into model training to enhance the rating prediction perfor-mance. Though FM considers the second-order feature struc-ture, it simply uses all the feature pairs for prediction. Quite a lot of work in sparse coding area have shown that many signals tend to have a sparse representation from basic com-ponents in nature, and a sparse model often outperforms a dense model and also has the variable selection effect. In-spired by this, a sparse model that uses an appropriate sub-set of feature pairs might have a better performance.
In practices, human prior knowledge or explicit structure information about these features is also sometimes available. For example, the topical categories on news articles may nat-urally be organized into hierarchies or graphs. Another good example would be demographical information about users, especially their geo-locations that are aligned with countries, states and cities, defined in real-world geo-political settings. These prior knowledge and structures are invaluable infor-mation for better user understanding and profiling and even-tually better recommendation results. However, it is not straightforward to encode this kind of structural prior knowl-edge into state-of-the-art recommendation models like RLFM and FM . One approach might be to construct features captur-ing these structures and embed them into regression models. But the interplay between an optimal way to construct such features and train a better regression model based on these features to map to latent features becomes non-trivial in this case. Some previous work has been proposed to impose structural information on latent variable models, which are not necessarily directed graphical models. For instance, He et al. [5] proposed a general learning framework which in-duces sparsity on the undirected graphical model imposed on the vector of latent factors. Although the paper shares a similar idea with our framework, their work cannot handle heterogeneous types of features and complex dependencies. Also, it is hard to link their work to state-of-the-art LFM used in CF settings. Along the line of undirected graphical models, Min et al. [ 8] proposed sparse high-order Boltzmann machines, aiming to capture dependencies between latent variables. Again, it is not obvious to plug the model into state-of-the-art CF approaches.

In this paper, we propose a structured sparse second-order regression model with structural prior knowledge in a prin-cipled way. The notion of types of features is introduced such that different types of features would have different structures (e.g., topical categories versus geographical loca-tions). We consider two kinds of structures. For inter-typed features (which are of different kinds), the model is able to learn sparse relationships between different types of features (e.g., age and gender). For intra-typed features (which are in the same kind) that have a hierarchy (tree), e.g., we have the country-state-city hierarchical tree for the geo-location fea-ture terms, the model learns a sparse hierarchical structure on the tree such that if a parent feature edge (or interchange-ably a feature pair) is selected, then all its descendant edges should be selected as well, and if a parent edge is removed then all its descendant edges should be removed too.
We model the rating score of an event by a user on an item by a second-order polynomial regression where b is the global bias for all events, w is the first-order weights, and p is the feature size. x is the concatenated fea-Figure 1: An illustration of hierarchy for features of the same type. Each node corresponds to a feature node, and each edge a feature pair. ture vector for the user-item-event triplet as is used in Apparently, we directly use a pair-wise weight factor  X  ij each feature pair (or edge) ( i, j ) rather than the inner prod-uctoflatentfactors v i and v j in FM because it would be eas-ier to learn the structures by imposing structured sparsity-inducing norm regularization, otherwise we would need to minimize the approximation error to push  X  ij  X  v i , v j any activated feature pair ( i, j ). Actually the feature size is relatively small w.r.t. the data size which is usually the case for practical large-scale machine learning problems, thus the complexity of O ( p 2 ) should not be the bottleneck.
As we discussed before, heterogeneous feature types exist for the user-item-event triplet. For users, it is comparatively easy to obtain certain prior structures for some types of fea-tures (e.g., geographical locations), while it is not straight-forward for other types (e.g., age and gender).
 We partition the features into different types (groups). Let T denote the number of feature types for users and G = { g 1 ,g 2 ,...,g T } the non-overlapping grouping of fea-ture indices, i.e., partition of feature indices by feature types. g t is the set of feature indices for feature type t . Denote p t the number of features in feature type t , so that and T s =1 p s = p . If we differentiate different types of fea-tures, the second-order term in Equation 1 can be further expanded as As we partition features into different types, edges of the whole pair-wise feature graph are divided into two parts as well: inter-type edges and intra-type edges. Intra-type edges are initialized from prior knowledge if we have prior hierar-chies while inter-type edges are learned. Figure 1 gives an example of the hierarchy for features of one type.
We impose a sparse prior on all inter-typed feature edges, whereas in order to learn sparse hierarchical structure on certain intra-typed feature edges (e.g., geo features), we cast the problem into a hierarchical sparse coding problem by us-ing structured sparsity-inducing norms as regularization [ 11, 6]. The hierarchical sparsity-inducing regularization has a desirable property that if one edge is included then all its ancestor edges should all be included as well. Equivalently, if one edge is removed from the tree, then all its descendant edges should all be removed too. We can also view the edge hierarchy as pre-ordering of edges, i.e., the parent edge has to be pre-ordered before its child edges. For intra-type edges with hierarchy, we use tree-structured set of groups defined as follows [ 6].

Definition 1. A set of edge groups G { g } to be tree-structured in { 1 ,..., |E|} if g  X  X  g = { 1 ,..., and if for all g,h  X  X  , ( g  X  h =  X  )  X  ( g  X  horh  X  g ), where E is the set of intra hierarchical feature graph edges. For such a set of groups, there exists a (non-unique) total order relation such that: We can also say that the tree-structured set of groups con-sists of all the subtrees. As we can see from the defini-tion, we can expand this tree-structured set of groups by adding inter edge and self-edge groups, where each inter edge and self-edge forms a singleton edge group, without affect-ing the total order relation since they are non-overlapping sets of singleton groups isolated from the tree-structured set of groups. We thus propose to use a mixed singleton and tree-structured sparsity-inducing norm T (  X  )imposed on the whole feature graph edge weight vector  X  which is defined as T (  X  )  X  = posedofelementsof  X  for indices in edge group g .
Adding the standard l 2 -norm regularization on b and w , we want to minimized the regularized risk on all N events where  X  and  X  are positive real number parameters for reg-ularization. b and w i can be updated by alternating least-squares (ALS) optimization by where e n = y n  X   X  y n is the training error of the n th event, and
We apply proximal gradient descent to update  X  | g for each group g in a block coordinate descent strategy where the key step is to compute the proximal mapping w.r.t. l 2 -norm constant for f . To speed up the algorithm, we need to esti-mate L to avoid line search. By applying Gersgorin theorem, it can be proved that L =2 Note that e n should be updated accordingly each time b , w , and  X  | g are updated to guarantee the correctness of the algo-rithm. The final algorithm has an O ( Np 2 ) time complexity and an O ( p 2 + N z ) space complexity where z is the average number of non-zero elements for all event feature vectors.
Dataset : We use Yahoo X  X  homepage stream data as a running example of our experiments. A dataset from a sam-ple of traffic of users from six international sites, including United Kingdom, France, Italy, Germany, Spain and Ireland of Yahoo homepage streams is collected. The dataset con-tains users X  interactions, clicks and views, on articles shown in the stream section from May 1st, 2014 to May 16th, 2014. We group users by time periods and construct sessions of all items a user consumed in a particular time period (e.g., months, day, hour and etc.). Mimicking real settings of on-line systems, events from May 1st to May 9th are used to build the training set, and the remaining events are used to build the test set. The training set has 4 . 75M events based on the interaction between between 37 , 668 users and 30 , 260 news items. The test set has 4 . 46M events based on the in-teraction between between 45 , 422 users and 25 , 569 news items.

Features : The critical part of our proposed framework, with other feature-based CF is to utilize different types of user and item features. Therefore, in addition to the user interaction data, the dataset also includes the user-side fea-tures and item-side features. The user-side feature set con-sists of three components: mensional (e.g., male, female and unknown). age groups. structure, including countries, states and cities. which yields 6744-dimension user-side features. Note that not all features have prior structures. The item-side feature set consists: could fall into multiple categories), a directed acyclic graph (DAG) prior structure. which results in 1347-dimension item-side features. Fi-nally, each event is associated with one item-specific feature denoted as Freshness , which measures the lifetime of the item. Summarizing, the dataset has 8 , 092-dimension ex-plicit features for all events, the average number of non-zero features for each event is 8 . 682.

Evaluation Method and Metrics : In addition to eval-uating how well we can fit the data by measuring Rooted-Mean-Squared-Error ( RMSE ), we also evaluate the proposed methods in terms of ranking metrics. We use Mean-Average-Precision ( MAP ), Mean-Reciprocal-Rank ( MRR )andPreci-sion@10 ( P@10 ) as main evaluation metrics.

Baselines : We compare our model, denoted as STSR with three standard baseline methods: for user/item latent factors. This includes all classic LFM models like [7]. for user/item latent factors. This includes models like [ 1, 3]. Table 1: Average performance with standard devi-ation shown in parentheses. (* represents  X 0.000 X , i.e., *14 = 0.00014).
 Method RMSE MAP MRR P@10 ZeroMean . 757(  X  00) . 122(  X  16) . 149(  X  20) . 038(  X  04) RLFM . 339(  X  07) . 157(  X  74) . 196(  X  99) . 052(  X  31) FM . 329(  X  00) . 167(  X  00) . 207(  X  00) . 053(  X  00) STSR . 329(  X  00) . 175(  X  14) . 218(  X  15) . 058(  X  05) Method RMSE MAP MRR P@10 ZeroMean . 961(  X  13) . 129(  X  03) . 147(  X  03) . 037(  X  02) RLFM . 339(  X  03) . 166(  X  73) . 194(  X  83) . 050(  X  23) FM . 329(  X  00) . 176(  X  00) . 205(  X  00) . 052(  X  00)
STSR . 329(  X  00) . 185(  X  18) . 218(  X  22) . 057(  X  06) second-order regression model with latent feature factoriza-tion [9].

Setting : We do grid search for all parameters of all models and report the best average performance in 5 runs. For ZeroMean and RLFM , we do grid search for  X  2 ,  X  2 u v in 10 ality K in { 5 , 10 , 20 , 50 } .For FM , we do grid search for the regularization parameter  X  in 10  X  4 , 10  X  3 , 10  X  2 , and latent space dimensionality K in { 4 , 8 , 16 , 32 } .For STSR , we search  X  in 10  X  4 , 10  X  3 , 10  X  2 , 10  X  1 and  X  in
We run each model for 5 times from different initializa-tion and average the metrics from multiple runs. The com-parison result is shown in Table 1. From the table we see that STSR outperforms other models in all ranking metrics significantly. As expected, ZeroMean does poorly in all rank-ing metrics, which validates the statement made in previous studies that it does not handle cold-start problems. RLFM does significantly better than ZeroMean , implying that fea-tures are critical to address cold-start problems and bet-ter user/item factors might be learned by utilizing features. FM performs significantly better than RLFM which is consis-tent with the statement in literature. Finally, STSR learns sparse relationships among features and only the links that are truly contributing to the performance will be enabled through structured sparse coding. Thus, the sparse struc-ture learned from STSR turns out making a difference in terms of ranking metrics.

Parameter Analysis :Weinvestigatehowdifferentsets of parameters impacting the performance in Figure 2.The optimal parameters are  X  =0 . 001 , X  =10  X  5 . The figure shows that STSR is not very sensitive to  X  with wide ranges, the metrics only differ in ten-thousandths place. STSR is mildly sensitive to  X  when  X  is relatively small, but larger  X  does badly hurt the ranking performance. In practice, one can use a validation set (with gold standard) to tune the parameters by grid search.
We proposed a structured sparse regression model that is able to learn heterogeneous sparse structure on the user/item features. We developed an efficient learning al-gorithm that scales linearly to the number of events. Exper-Figure 2: Average metrics in 5 runs with different  X  and  X  for STSR iments on a large-scale news recommendation dataset have demonstrated the superior performance of our model com-pared to the state-of-the-art alternatives, especially for the cold start users, for whom the structure of features turns out to be critical information for the recommendation quality.
