 Classification of short text messages is becoming more and more relevant in these years, where billion of users use online social networks to communicate with other people. Under-standing message content can have a huge impact on many data analysis processes, ranging from the study of online social behavior to targeted advertisement, to security and privacy purposes. In this paper, we propose a new unsu-pervised knowledge-based classifier for short text messages, where each category is represented by an ego-network. A short text is classified into a category depending on how far its words are from the ego of that category. We show how this technique can be used both in single label and in multi-label classification, and how it outperforms the state of the art for short text messages classification.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Informa-tion filtering ; I.2.7 [ Artificial Intelligence ]: Natural Lan-guage Processing X  Text analysis Short Text Classification; Knowledge-based classification; Algorithms; Experimentation
Recent years have seen billion of users interacting with social media web sites, exchanging their opinions and their interests [17]. This huge phenomena has big impact, for in-stance, on the many companies that want to adopt targeted advertisment for the online social audience. According to the  X  X he State of Marketing 2014 X  report [13], 60% of mar-keters used social listening strategies in 2013, whereas 24% plan to do so in 2014, but only 31% of them think their social listening is fully effective. Moreover, 57% of mar-keters use social advertising. Facebook, Twitter, LinkedIn and others are daily destination for millions of consumers who spend an average of 37 minutes per day on those chan-nels [1]. But again, only 23% of digital marketers find their social advertising efforts effective. Understanding what a user posts on a social network can have two orthogonal and very relevant objectives: making marketing strategies more effective by, at the same time, providing the users effective tools to protect them against a too much aggressive adver-tising. For example, users could allow the release of their posts about technology in order to receive advertisements on the last smartphone or tablet, but they could deny the release of posts about politics, avoiding to get targeted to political advertisements. To cope with these scenarios, in this paper, we propose a new unsupervised classification al-gorithm that tries to overcome the limitations of the state of art classifiers for short text classification. Unlike super-vised classification algorithms, like SVM and Naive Bayes , unsupervised classifiers do not require a big and represen-tative training set to build their model. As such, they are better suited for a social network scenario, where this type of training set is expensive to be built and it is not currently publicly available.

A further limitation of current approaches is that they are mainly based on the bag of words model for text rep-resentation [9], where each message is represented by a fea-ture vector containing the term frequency ( TF ), or the term frequency-inverse document frequency ( TF-IDF ), of the words it contains. As short texts, like users posts, do not provide sufficient word occurrences, these traditional repre-sentation methods have limitations. To overcome this prob-lem, we propose a new classifier that leverages Semantic Web and linked data technologies [2]. More precisely, our algo-rithm represents each category as an ego-network, automat-ically built using two well-known knowledge bases, Word-Net 1 and Yago 2 . The ego of the network is composed by a manually defined set of words, called centroids, that bet-ter represent the meaning of the considered category. Each node is connected to other words to which it is semantically related in some ways. A short text is classified into a cate-gory depending on the ties between the words of the message and those in the ego-network of the category, and on how far these words are from the ego. In this way, no training set is needed and, unlike the bag of words model, each word of a text is evaluated in term of distance instead of frequency. The smaller the distance between a word and an ego of a category is, the greater is the semantic similarity between the word and that category. This works especially well for short messages, where a distance metric is more robust than frequencies, due to the few words contained in each message. Experimental results on two real short text datasets indicate that the proposed classifier outperforms state-of-art meth-ods for short text classification, including both supervised and semi-supervised learning techniques as well as enriching features methods.

The remainder of this paper is organized as follow. Sec-tions 2 and 3 discuss the related work and the background, respectively. Section 4 describes our proposed model to clas-sify a short text message. In Section 5, we illustrate all the steps involved in the classification process. Section 6 dis-cusses our experimental results. Finally, Section 7 concludes the paper and discusses future research directions.
A lot of techniques for short text classification exist. Since it is difficult to mine significant statistics from short texts, many approaches try to overcome this limitation using semi-supervised learning [20, 21] that improves the learning pro-cess by exploiting the unlabeled data in addition to the la-beled training data, and transfer learning [10], that applies the knowledge learned from one domain to another domain. In particular, [20] is based on the observation that there usu-ally exists ordered subsets in short texts, called  X  X nformation paths X , and that such ordering relations can be exploited to achieve an higher overall accuracy. However, as all the semi-supervised learning techniques, [20] is composed by many training/testing iterations that are time consuming and have an important impact on performance. For this reason, other techniques try to tackle the problem with feature enrich-ment. For instance, [4, 12] exploit an external corpus and use the topics as new features, whereas [3, 14, 16] integrates the search results returned from search engines into the short text. [5] enriches the short text representation by incorpo-rating a vector of topical relevances. The topics are derived from the knowledge embedded in the short text collection of interest by using an hierarchical clustering algorithm. In [7], classification accuracy is improved by doing the topic cor-relation analysis of training and test data. Although these methods improve the accuracy of short text classification to some extent, they all need both a training set and some external auxiliary data obtained through a time-consuming manual labeling and collection. In [19], the proposed method yields similar accuracy compared to the baseline by using the representative query words of the short text to search the categories from the labeled short texts, used as knowledge of the categories. This work shows that some inherent features of short texts can play a key important role for classification but, however, it still requires a labeled dataset. Our method is different from these techniques. It is an unsupervised and knowledge-based text classifier, where no training phase and no labeled data are required, rather a knowledge, in term of proximity of concepts and entities, about the categories of interest is used in order to classify a short message.
Because meaningful sentences are composed of meaningful words, any system that wants to process natural language must have information about words and their meaning. In order to understand the meaning of a text, we start from three observations: (i) different socio-professional groups use different words and terminologies to express and communi-cate contents and topics. For instance, topics related to fi-nance will contain words like  X  X oan X ,  X  X ank X , whereas words like  X  X atch X ,  X  X ace X  are more used in a sport context; (ii) the same concept can be expressed using different words; (iii) knowledge about events/facts and entities (like persons, or-ganizations, cities, etc.) is needed to fully understand some messages. For example, a message like  X  X arack Obama lives in the White House X  can be fully understood only if we know who Barack Obama is and what the White House is. There-fore, we use WordNet and Yago to build categories. Word-Net is a lexical database that groups nouns, verbs, adjec-tives and adverbs into sets of cognitive synonyms, called ( synsets ), each expressing a distinct concept: synsets are interlinked by means of conceptual-semantic and lexical re-lations. However WordNet alone is not enough to cope with point (iii) because it does not contain knowledge about enti-ties, relations and events relating to a concept. For example, WordNet contains the concept  X  X oliticians X  or  X  X ingers X  but it does not contain people (entities) that are politicians or singers. For this reason, we also use Yago, a huge ontol-ogy, hierarchically organized using WordNet, and contain-ing more than 1.7 million entities and more than 15 million relations about these entities [18], called facts .
WordNet is a lexical database of the English language where words are linked via semantic relationships. The fun-damental building block of WordNet is a synset. A synset is a collection of words that are synonymous. All synsets are categorized into four parts of speech (POS): nouns, verbs, adjectives, and adverbs. While a traditional dictionary or thesaurus is focused on the meaning of words, WordNet is focused on the relationships between synsets. The following are the semantic relationships available in WordNet orga-nized by the part of speech they can connect:
All parts of speech. Synonymy is WordNet X  X  basic re-lation, because WordNet uses sets of synonyms (synsets) to represent a concept. In contrast, Derivation connects a con-cept which is a derivationally related form of another con-cept. For example,  X  X olitician X  and  X  X olitical X  are a deriva-tionally related forms of  X  X olitics X .

Nouns only. Hypernymy links more general concepts to a more specific one, e.g., flower-rose. Hyponymy is the inverse of hyperonymy and it links more specific concepts to a more general one, e.g., rose-flower. Meronymy is the part-whole relation. For example, a meronymy links  X  X andle X  to  X  X oor X , because an handle is a part of a door. Holonymy is the inverse of meronymy and it links a concept denoting the whole to another one denoting a part of, or a member of the whole, e.g., door-handle.

Verbs only. Hypernymy links a verb Y to a verb X , if the activity implied by X is a kind of the activity implied by Y (e.g., to perceive is an hypernym of to listen). Troponymy: the verb Y is a troponym of the verb X , if the activity Y is a manner of doing the activity denoted by X (e.g., to lisp is a troponym of to talk). Entailment : the verb Y is entailed by X if, by doing the activity denoted by X you must be doing the activity denoted by Y (e.g., to sleep is entailed by to snore).
Adjectives. Pertainymy connects an adjective to the noun to which is pertaining to (e.g., geopolitical is pertain-ing to geopolitics). Other relations link adjectives to their related nouns, to verbs of which they are a participle, and to other adjectives to which they are similar.

Adverbs. They follow the classification of the adjective from which they derive (e.g., Politically is pertaining to the adjective political).
YAGO is a large ontology with high coverage and precision [18], whose model uses the same knowledge representation as RDFS. All objects (e.g., cities, people, even URLs) are represented as entities in the YAGO model. Two entities can stand in a relation. For example, entity Barack Obama stands in the hasWonPrize relation with the entity Nobel Peace Prize , and this is represented by the triple ( Barack Obama , hasWonPrize , Nobel Peace Prize ). These triples formed by an entity, a relation and another entity, are called facts . The two entities in the triples are called arguments of the fact. Numbers, dates, strings and other literals are represented as entities as well. This means that they can stand in a relation with other entities. For example, Obama was born on 4 th August 1961, is represented by the fact ( Barack Obama , wasBornOnDate ,  X 1961-08-04 X  ) in the model. Similar entities are grouped into classes. For exam-ple, the class politician comprises all politicians, whereas the class singer comprises all singers. Each entity is an instance of at least one class. The YAGO model expresses this by the type relation, as in the fact ( Barack Obama , type , politi-cian ). Each Yago class is derived, and exactly correspond, to one WordNet synset. In this way, given a synset syn , it is easy to see if there are entities related to it in Yago, simply looking for all the facts of type (*, type , syn) and taking the first argument. We assume a set of relevant categories C = { C 1 , C 2 , ..., C } , and a set of short messages TM = { m 1 , m 2 ,..., m n to which we want to assign their corresponding categories. If the relevant category should be one, we speak of single-label classification, whereas in case multiple categories can be as-signed to a short text we speak of multi-label classification. A classifier can be modeled as a function f : TM  X  2 that takes as input a text t  X  TM , and returns the set of categories related to t .

The main idea on which our model is built is that, given a text t , we can infer the categories of t having a knowl-edge of the ties between the words and entities (e.g., people, organizations) appearing in t and those used to represent a category. Ties are not measured in terms of frequencies (bag of words approach) but in a semantic way. In fact, if a text t contains words or entities semantically close to a certain category c , it is likely that t is related to c . For instance, suppose to have only two categories, namely politics and sport , and suppose that we want to classify a text t accord-ing to them. If t contains the words  X  X olitical X  and  X  X arack Obama X , it is likely that t is talking about politics because these words are semantically close to the words used in a politic domain. Perhaps these two words can be found even in a sport context but, however, they would be semantically more distant from the concept of sport. Therefore, having a knowledge base containing the words/entities used in each category of interest and their ties, and a distance function that returns the semantic distance between a word and a category, we can easily compute a semantic score for a short text and assign it the category/ies with the highest score/s. In what follows, we will use the term concept to indicate all single words or sequences of words that model a concept (e.g.,  X  X olitics X ,  X  X olitical Science X ), whereas we will use the term entity to indicate single words or sequences of words that model a pure entity, like people, organizations, cities, etc. (e.g.,  X  X arack Obama X ,  X  X icrosoft X ,  X  X ew York X ).
A category can be thought of as an ego-network. The cen-ter of the network is made of the set of the most representa-tive concepts that describe the category. These concepts are called the centroids of the network. Each node is connected to other concepts/entities to which it is semantically related in some way. Semantic relations include all those in Word-Net and those deriving from Yago, as discussed in Section 3. Therefore the ego-network of a category can be modeled as a directed labeled graph, where nodes are concepts/entities, and an edge label represents the type of the semantic re-lationship between the concepts/entities it connects. More formally:
Definition 1 (Category). Let T be the set of seman-tic relationships that exist between concepts/entities. A cat-egory CA is a tuple CA = &lt; N,C,E, X  &gt; , where: In the next section, we will describe how we can automati-cally build this type of ego networks exploiting YAGO and WordNet.

Example 1. Figure 1 shows a portion of the Politics cat-egory. We assume that the concepts politics , politicians , po-litically and political form the ego of the network, i.e., red nodes inside the dotted circle. Each concept in the network is related to some others by semantic ties. For example, politics is connected to geopolitics with an edge with label hypernym , whereas geopolitics is connected to politics with an edge labelled hyponym . Furthermore, geopolitical is a pertainym of geopolitics , whereas regime is a synonym of government which is, in turn, a synonym of politics .
Having this representation of a category, there is a nat-ural way to model the semantic similarity between a con-cept/entity and a category: the smaller the distance be-tween the concept/entity and the center of the category is, the greater is the similarity. Referring to Figure 1, politics and geopolitics are semantically more similar than politics and geostrategy , because the first pair of concepts is directly connected, whereas the second one is connected with a path of length 2. Similarly, politics is semantically closer to gov-ernment than regime . The entities Obama and Bush , in-stead, are equally semantically similar to the centroid politi-cian , because they are equally distant from it. Distance between two nodes is the shortest path between them, also called geodesic distance. Since the center of a category can be made of more than one centroid, the distance between a concept/entity and a category is the minimum geodesic distance between the concept/entity and a centroid of the category.
 Definition 2 (Category Distance). Given a category CA = &lt; N,C,E, X  &gt; and a concept/entity ce , the distance between ce and CA is defined as follows: where SP c,ce denotes a shortest path between c and ce
According to Definition 2, the distance between a con-cept/entity and a category is zero, if the concept/entity is one of the centroids of the category, it is infinite if the con-cept/entity is not present in the category CA , it is equal to the minimum length of the shortest paths SP between the concept/entity ce and each centroid c  X  C of the category CA , otherwise. Therefore, the category distance falls in the range [0,  X  ]. We are interested in assigning a semantic score to a given concept/entity ce with respect to a category CA . This has to be maximum when the concept/entity ce is a centroid, and it should decrease when the distance between the concept/entity and the center increases. To this pur-pose, we can easily use the inverse function of the distance as semantic score. More formally: Definition 3 (Semantic Score). Let CA = &lt; N, C, E,  X &gt; be a category, and let ce be a concept/entity. The semantic score of ce with respect to CA , denoted as SS ce,CA is defined as follows:
The semantic score between a concept/entity ce and a category CA can be generalized to the semantic score be-tween a text message m and a category CA . In defining such score, we have to take into account that not all the concepts/entities in a message can exist in the considered category nor they all have the same discriminating power. Therefore, we cannot compute the score of a message as the sum or the mean of the scores of its concepts/entities. As an example, consider a message with 10 concepts/entities, where two of them are politician and politics , whereas all the others are not related to the politics category so they do not exist in the category graph. If we compute the score of this message for the politics category as the mean of the scores of each concepts/entitities it contains, we will get a score 0.20 ( politician and politics have score 1) that tends to zero, due to the eight concepts/entities with score zero. If we aggregate the scores with the sum of the scores of each concept/entity, we can get that the sum of low scores ex-ceed the score of a concept/entity very close to the center. These two behaviors are clearly undesirable. In fact, in the first case, concepts/entities not related to a category reduce too much the power of high discriminating concepts/entities, like the two centroids politician and politics . In the second case, instead, a lot of low scores can overcome few very high scores. Furthermore, it is desirable that the semantic score of a message falls in the range [0,1]. In fact, we need to normalize the scores of a message such that a huge category cannot beat a small category depending only on its size.
A possible solution is using a scheme similar to tf-idf , as-suming that the terms are concepts/entities of ego-networks. However, there are some drawbacks in our context, that we have experimentally tested. Indeed, each concept/entity can be present at most once in a category so the tf term is re-duced to be just 1 / | N | and the idf component tends to zero. This because our aim is to expand each ego-network as much as possible in order to compute a distance value for all con-cepts/entities with respect to a given category.

Therefore, we use a sum weighted function over the se-mantic scores of the concepts/entities of the message, with the idea that the smaller the distance of a concept/entity to the center is, the greater should be its weight. In par-ticular, we give weight one to concepts/entities that have the minimum score, whereas for the other ones we assign a weight equal to the ratio of their score with the minimum one. More formally: Definition 4 (Semantic Score of a Message). Let CA = &lt; N,C,E, X  &gt; be a category, and let m  X  TM be a text message. Let CE m be the set of concepts/entities con-tained in m , and minSS be the minimum semantic score greater than zero of a concept/entity ce  X  CE m wrt category CA . The semantic score of m wrt category CA , denoted as SS m,CA , is defined as follows:
Example 2. Consider the message m =  X  X eostrategy is a kind of geopolitics X  . Referring to Figure 1, the shortest path from geostrategy to politics has length 2 (Geostrategy  X  X  X  X  X  X  X  X  X  X  Geopolitics hyponym  X  X  X  X  X  X  X  X  X  X  Politics), whereas geopolitics is directly connected to the centroid politics, so the shortest path has length 1 (Geopolitics hyponym  X  X  X  X  X  X  X  X  X  X  Politics).
Finally, suppose the shortest path between  X  X ind of  X  and the ego is 119, whereas  X  is  X  and  X  a  X  are ignored because they are stop words. Thus, the score of each concept and that of the message is as follows:
In si ngle label classification, we assign with each mes-sage m the category C j , resulting in the maximum semantic score. Formally:
Definition 5. (Single Label Classification Function) Let m  X  TM be a message to which we want to assign the cat-egory that better describes its content. The classification function SLCF ( m ) : TM  X  C is defined as follows:
S LCF ( m ) = c i |  X  j, 0  X  j  X  k  X  j 6 = i, SS m,c i  X  SS
In a multi-label scenario, we have to define a criterion to assign more than one relevant category to a message. In the literature, there are two main methods for tackling this problem [8], namely problem transformation and algorithm adaptation. Problem transformation methods transform the multi-label problem into a set of binary classification prob-lems, which can then be handled using single-level classi-fiers. In contrast, algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem into a simpler one, they try to address the problem in its full form. Our method is an unsupervised score based classifier which returns a score in [0, 1]. This score allows us to estab-lish an ordering among categories. Therefore, we are able, given two or more categories, to assign a message the cat-egory with the highest score. However by using this score, we are not able, given only one category, to decide whether a message belongs to that category or not, as required in transformation methods. In fact, in these methods we have to create a single-label classifier for each category that, given a message m , must respond if m belongs to it or not with yes/no responses. The problem is that there is no threshold that allows to discriminate if a score is stronger enough to consider a category relevant or not, as the following example shows.

Example 3. Consider the message  X  X reat geostrategy X  and the category politics depicted in Figure 1. Suppose the concept Great has a distance 99 from the center so its se-mantic score is 1 / (1 + 99) = 0 . 01 , whereas geostrategy is at distance 2 from the ego, therefore its semantic score is 1 / (1 + 2) = 0 . 33 . The semantic score of the message is Given the score 0.32 alone, a single-label classifier will tend not to classify this message in the politics category. Con-versely, if we know that all the scores of the other categories are less than 0.32 we can easily assign the politics category to this message.

To use our algorithm in a multi-label context, we have therefore defined a simple adaptation method. The main idea is that we can automatically find the most relevant categories of a given message depending by the distribu-tion of the scores of each category. In particular, relevant categories receive higher scores than that of non relevant ones and the differences between scores of relevant categories should be much lower than differences between scores of rel-evant and not relevant categories. Since distance based clus-tering methods, like k-mean, try to group data minimizing distances between data inside a cluster and maximizing dis-tances between clusters, we can easily adopt one of them to cluster our categories into two clusters depending by their scores. In this way we obtain one cluster containing the group of high scores, called C Assigned and one with lower scores, called C NotAssigned . We assign to a message all cat-egories that fall into the first cluster.

Definition 6. (Multi Label Classification function) Let m  X  TM be a message to which we want to assign the cat-egories that better describe its content. Let C Assigned set of categories that fall into the Assigned cluster, whereas C
NotAssigned is the set of categories belonging to the Not As-classification function MLCF ( m ) : TM  X  2 C is defined as follows: In sec tion 6.3.3, we will show our good results for multi-label classification, obtained using a simple k-mean with mini-batch optimization[15] as selection of initial centroids of our two clusters.
The message classification process involves many steps, as depicted in Figure 2. First of all, the classification model is built (step (a) ). This means that the ego-network of each category of intereset is built using WordNet and Yago as knowledge bases. After that, given an input text message msg , it is first parsed (step (b) ) thus creating a feature vector FV msg describing its content. FV msg is then fed into the classifier (step (c) ), that returns the category c (or the set of categories) it predicts for msg . In the following sections, we discuss all those steps in details.
In this section, we explain how we build the ego-network of a category. The process starts from the identification of the centroids forming the center of the network, that we assume manually defined. We suppose that the centroids are defined by domain experts that specify as centroids the keywords that better represent a category. For example, we have used the name of the category as centroids of the category itself in our experiments. For each centroid, we use WordNet and Yago to determine its semantic related concepts. We repeat this step for each encountered concept in order to expand the network as much as possible. The pseudo-code for this simple procedure is showed in Algorithm 1.

Line 1 contains an initialization phase, where each compo-nent of the category C k we want to build is set to be empty. Then, the procedure calls the BuildCategory function (line 2) giving it as input the set of all centroids CS . Build-Category returns the ego-network representing the category (cfr. Definition 1) and a set of pairs, called Distances (line 1), containing, for each concept/entity of the category, its distance from the center. In this way, we pre-compute the Algorithm 1: Building a category.
 distances between all the concepts/entities and the center in order to improve the performance of the classifier.

The BuildCategory function simply iterates over the set of centroids and, for each of them, performs three actions: (i) it adds the centroid to the set of nodes N and to the set of centroids C (lines 4-5), (ii) it calls the CheckYago function (line 6), and (iii) it calls the Build function (line 7) passing as parameters the centroid and its distance from the center, which is zero for the first call.

The CheckYago function (line 33) checks if a concept rep-resents a Yago X  X  class (line 34) and, if this is the case (lines 35-37), it adds to the category all the Yago X  X  entities with an edge of type  X  X nstance of X  connected to the concept (line 36), through the Add function.

The Add function (line 25) adds a concept/entity to the category. In order to do that, it needs four parameters: the concept/entity concEnt we want to add, the concept to which it is semantically related ( origin ), the label of the semantic relation, and the distance between concEnt and the center. With these parameters, it adds the input con-cept/entity to the set of nodes (line 26), it adds the edge ( origin,concEnt ) to the set of edges of the category (lines 27-28), it adds the label of the edge to the set of the seman-tic relations (line 29), and it creates the association between the edge and its label (line 30). Finally, it adds to the set of distances the pair (concEnt, distance) (line 31).

The Build function (line 10) represents the core of the building process. It requires two parameters, a concept and its distance from the category center. This function adds to the category all the semantically related concepts/entities of the concept received as input and tries to expand the net-work recursively calling itself on each of the newly inserted concept. Therefore, by querying WordNet, it retrieves all the semantically related concepts of the concept received as input and stores them in the set src (line 11). Src is a set of pairs ( c,r ), where c is a concept semantically related to the one received as input, and r is the semantic relation that exists between the two. If src is empty, meaning that the concept has no semantically related concepts, the function ends, because it is not possible to further expand the net-work (lines 12-14). In contrast, if src is not empty, function Build performs three actions for each pair ( c,r ) in src (lines 16-18), in case c has not been analyzed yet: (i) it adds the concept c to the category by calling the Add function (line 19), (ii) it calls the CheckYago function in order to see if there are entities related to c to be added to the network, and (iii) it calls recursively itself on c specifying its distance plus one as current distance (line 21).

Algorithm 1 is O ( n log e ) in time, where n is the number of distinct concepts of WordNet and e is the number of en-tities in YAGO. The Build function is first called for each centroid at line 7. Build is recursive, and it calls itself for each concept related to its first parameter, if the concept is not already present in the set of nodes N (cfr. the checks at lines 16-18). A concept is in N if the Build function has already been called for it. The three operations performed by Build are: (i) retrieve all concepts related to the one passed as first parameter (line 11), (ii) call the Add and the (iii) ChekYago functions. It is possible to retrieve the se-mantically related concepts in constant time using a simple hash table, where keys are the concepts and values are sim-ple structures containing the related concepts and the name of their relations. The Add function simply updates the set of nodes, edges and distances and thus can be performed in constant time. Finally, CheckY ago is O (log e ) because we need to find and retrieve all the facts of type (*, type , class) and take the first argument of the fact (cfr. Section 3), and this can be done in logarithmic time traversing a B-tree index created over the third component of the fact.
The goal of this phase is, given a message msg , to re-turn a set of relevant concepts/entities with which we query the set of categories. To achieve this, we perform a se-quence of four operations over the message: (i) tokenization, (ii) part of speech tagging (POS), (iii) name entity recogni-tion (NER), and (iv) bigram generation. We use the Apache OpenNLP 3 library for them, since it comes with a maxi-mum entropy classifier for each of these tasks. In particular, the text message msg is fed into the tokenizer that splits msg into tokens. The array of tokens is then given as input to the POS that, for each token, assigns the corresponding part-of-speech tag. The output of the POS is an array of pairs Out P OS = [( t 1 ,pos 1 ) , ( t 2 ,pos 2 ) ,..., ( t for each token t i , we have its part-of-speech pos i tag (e.g., noun, verb, adjective, adverb). Finally, this array is fed into the NER that returns the entities contained into msg . In particular, the NER aggregates contiguous tokens that to-gether form an entity. The output of the NER is an array of pairs: entities = [( { t x ,t x +1 , ... t x + i } ,type ) , ( { t t component is the set of contiguous tokens that make up an entity, whereas the second component defines the type of the entity. Using Apache OpenNLP, we are able to detect six types of entities: people, organization, location, date, time, and money. The output of NER contains important information about the entities contained into the message. However, even a combination of tokens that are not an en-tity can be useful to classify a message, like phrasal verbs (  X  X ook for X  ,  X  X ome up X  , etc.). A reasonably good trade off between performance and information enrichment is to gen-erate also all the bigrams of two contiguous tokens. Discov-ered bigrams are modeled as an array of pairs: bigrams = [( { t x , t x +1 } , pos x ), ( { t x +1 , t x +2 } , pos pos n  X  1 )] , where we take the part of speech of the first to-ken of the bigram as the part of speech of the bigram itself. Stop words are removed only after the NER and bigrams generation, because they provide useful information to de-tect them. For example, entities like Dolce &amp; Gabbana , and bigrams like take off would not be recognized if we remove stop words before applying NER and generating all the bi-grams. Therefore, given a message msg , step (b) in Figure 2 ends returning to the classifier the set of entities, the set of bigrams, and the set of tokens with their part-of-speech tags.
Let us see how EgoCentric classifies a message given its features returned by the parsing phase, by first focusing on the single label scenario. The procedure is showed in Al-gorithm 2 by function Classify . For each category in the input category set, this function takes a copy of the enti-ties, bigrams and tokens with their part-of-speech (line 4). Then, it starts to find each entity contained into the input message into the graph of each category, by invoking func-tion getSemanticScore (line 6), that computes the semantic score of an entity/concept (see Section 4). If the score is greater than zero (line 7), it means that the entity has been found in the category graph, so the entity and its score are added to the Scores Ci set (line 8). Moreover, in this case we do not need to exploit the tokens composing the entity so we remove them from Tokens , nor the bigrams, that are removed from Bigrams (lines 9 -10). We repeat the same procedure for all remaining bigrams (lines 13-19), and sin-gle tokens (lines 20-27). In particular, we first analyze the input received by the parser with a priority: entities first, then bigrams and, finally, single tokens, due to their decreas-ing information richness. At the end, Scores Ci contains all the scores of meaningful entities, bigrams ,and tokens, so we compute the total score of a message with respect to a cat-egory C i , SS m,C i , calling function getMsgScore (line 26), and add the pair ( C i ,SS m,C i ) to the Scores set. After iter-ating these steps for each category, the Scores set contains the scores of the message for all the categories in the input category set. Classify returns the category with the highest score (line 28).
 Algorithm 2: Classifying a message.

Example 4. Consider the message msg =  X  X bama is look-ing for voters X  and suppose we want to classify it given two categories: Politics and Animals. Algorithm 2 receives from the parser three arrays containing entities, bigrams, and tokens of msg , respectively: (i) entities = { ( {  X  X bama X  } , People) } , (ii) bigrams = { ( {  X  X bama X ,  X  X s X  } , noun), ( {  X  X s X ,  X  X ooking X  } , verb), ( {  X  X ooking X ,  X  X or X  } , verb), ( {  X  X or X ,  X  X ot-ers X  } , preposition) } , and (iii) tokens = { ( X  X bama X , noun), ( X  X ooking X , verb), ( X  X oters X , noun) } . Algorithm 2 starts with category Politics and the elements in entities (line 5). It finds the entity  X  X bama X  (see Figure 1) at distance 1, so SS
Obama,politics = 1 / (1 + 1) = 0 . 5 (line 6). Since one entity has been found, all the bigrams containing  X  X bama X  and the token  X  X bama X  are removed from bigrams and to-kens , respectively (lines 9 -10). Then the algorithm starts to analyze bigrams (line 13). The only meaningful bigram found in politics is ( X  X ooking X , X  X or X ) . Suppose it is at dis-14). Then, the tokens looking and for are removed from tokens (line 17). At line 20, the remaining token, that is,  X  X oters X  is analyzed. Suppose it is at distance 4, so SS voters,politics = 1 / (1 + 5) = 0 . 2 . Then, the score of the message is computed (line 26) through the getMsgScore function: Table 1: Google search snippets as training and test
Suppose, for simplicity, that SS msg,animals = 0 . At the end of the execution of the algorithm (line 28), category pol-itics is returned because it has the highest score.

In a multi-label scenario, this algorithm must be slightly changed. First, we apply a clustering method over the Scores set defined at line 2. In particular, we cluster the scores the strategy illustrated in Section 4. Second, we need to modify the return statement (line 28) in that we return a set containing all the categories with scores in the C Assigned cluster.
In this section, we report performance results of our clas-sifier, both on a single label and multi label scenario, on three short text datasets. Then, we provide the execution time of our classifier.
For our experiments, we used three real-world short text datasets. One is the  X  X earch Snippets dataset X  4 . The other two are  X  X ingle label tweets X  and  X  X ulti label tweets X , which contain thousands of tweets collected by ourselves by ex-tracting them from the Twitter Firehose 5 . The first one was chosen because it had been commonly used in other works on short text classification, whereas the Twitter datasets have been generated to analyze the effectiveness of our method on very short texts for which we were not able to find a standard available dataset.

The Search Snippets dataset has been introduced in [12]. It consists of search snippets retrieved from Google. They used various phrases belonging to 8 different categories to perform the Web search. The dataset has been built by selecting the top 20 (for training data) and the top 30 (for test data) snippets from the retrieval results. The underly-ing idea of the framework presented in [12] is that, for each category, they collected a large-scale external data collec-tion, called  X  X niversal dataset X , and then built a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection using latent semantic analysis (LSA) [6]. They used Wikipedia as a universal dataset. To collect the data, they prepared var-ious seeds , crawling keywords coming from their 8 different categories. For each seed keyword, they ran JWikiDocs 6 to download the corresponding Wikipedia page and crawl rele-vant pages by following outgoing hyperlinks. Details of this dataset are summarized in Tables 1 and 2, where we report the number of snippets for each category and some seed key-words used for crawling the universal dataset. Single la-bel tweets dataset consists of tweets labeled with only one of 9 different categories: alcohol, health, holiday, politics, Table 2: Seed keywords used to build the universal religion, school, sex, sport, and work. We sampled many English tweets from the Twitter Firehose and discarded a few bad tweets (e.g., non-English, containing only an URL) to obtain a sample of 200 tweets per category for a total of 1800 tweets.

Multi label tweets dataset, instead, consists of tweets belonging to more than one category. We used the same set of categories as in the single label scenario. We took a sample of 100 tweets following the same approach as single label tweets. We asked 9 validators to assign to each tweet the categories he/she thought were relevant for it. We con-sidered as categories of a tweet all those assigned by at least four validators. The details of the dataset are illustrated in the Table 3, where we report the number of tweets belonging to each category.
In the experiments, we compare our method with the fol-lowing baseline methods: (i) PH is an enriching method introduced in [12], it extends short texts with single granu-larity topics. (ii) CH is an enriching method proposed in [4], which extends short texts with multi-granularity topics. (iii) DA is a cluster-based enriching method, presented in [5]. (iv) ZH is a semi-supervised learning method introduced in [20], which classifies test data iteratively by detecting infor-mation paths over the data and rebuilding the classifier on different subsets of training and test data in each iteration. (v) SU is a non-parametric approach for short text classi-fication presented in [19]. It first selects the most represen-tative and topical-indicative words from a given short text as query words, and then searches for a small set of labeled short texts best matching the query words. The predicted category label is the majority vote of the search results. All these methods have been compared with EgoCentric on the search snippets dataset. Two enriching methods, namely PH and CH , need external text resources. An external dataset 8 collected using the seed keywords showed in Table 2 is available for the search snippets dataset. Additionally, we compare our method with the state of the art classifiers offered by the python library Scikit-learn 9 [11] on the single label tweets dataset.
We used two different criteria to select centroids of each category for our classifier. For the tweets datasets, we used as centroids the names of our 9 categories and all the verbs, adjective, and adverbs that derive from them using the per-tainymy and derivation relationships provided by WordNet. The search snippets dataset, however, contains more gen-eral categories, like  X  X ulture-Arts-Entertainment X . Indeed, the authors have used seed keywords ranging from  X  X rts X  to  X  X ashion X  and  X  X ook X  to build this category so it mod-els broader concepts than the ones behind the name of the category itself. To cope with this fact, we used the seed keywords for enriching the ego of each category.This is a fair strategy because all the enriching methods use these keywords to create new features, so this does not give an advantage to EgoCentric. In particular, we start from the name of the category as centroid. For example, the  X  X usi-ness X  category has the word business as centroid, whereas  X  X ulture-Arts-Entertainment X  has culture , arts , and enter-tainment as centroids. Then, for each seed keyword showed in Table 2, we compute its distance with respect to the cen-troids of the category, and we add up to the top 50 closest words to the centroids of the category. In particular, we performed 50 experiments considering an increasing number of keywords as centroids at each iteration. At experiment i , with 1  X  i  X  50, each ego contains the top i closest words, and the results shown in Table 4 are the average of the re-sults of all these experiments.
The experimental results on search snippets are illustrated in Table 4.
 Table 4: Accuracy comparison with baseline methods.
 PH uses the maximum entropy classifier offered by JMax-Ent 10 , whereas CH and DA apply SVM as learning algo-rithm adopting SV M light with default parameter settings SU employs Lucene 12 to index all words of each snippets. Logistic Regression 13 is used as the classifier of ZH , with default parameter settings. Moreover, for both PH and CH , two parameters need to be set: (i) the number of top-ics learned from the external dataset, and (ii) the weight of topics. CH has a further parameter, i.e., the number of granularity of topics, which is set to 3, whereas DA depends by the number of topic clusters N and SU depends by the number of words used as query. Table 4 reports the best accuracy obtained by the methods varying all these param-eter settings. PH achieves the best accuracy using a topic http://svmlight.joachims.org/ Table 5: EgoCentric vs State of the Art classifiers on Table 6: EgoCentric performance on multi label tweets weight equal to 50, whereas CH with topic weight 150. Fi-nally, DA best classifies with 50 cluster topics, whereas SU using 5 words as query. We can see that our classifier out-performs all the baseline methods.
On the single label tweets dataset, we compared EgoCen-tric with the state-of-art classifiers offered by the Python machine learning library Scikit-learn[11], namely: Ridge Clas-sifier, Linear Support Vector Classifier (LinearSVC), linear classifier (SVM) with Stochastic Gradient Descent (SGD-Classifier), Perceptron, Passive Aggressive Classifier (PA-Clas), Naive Bayes classifier for both multivariate Bernoulli models (BernNB) and multinomial ones (MultinomialNB), k-Nearest Neighbors classifier (k-NN), and the Nearest cen-troid classifier (Rocchio). We have splitted our dataset into two halves of 900 tweets (100 per category), one for training and the other for testing, in order to train all those classifiers that require a training phase. The results we obtained are showed in Table 5. The rows of the table are the classifiers and, for each of them, we report the overall precision ( P ), recall ( R ), and F 1 score, computed as the mean over all cat-egories. Egocentric has the best overall behavior achieving 0.91 precision, 0.85 recall, and 0.85 F 1 , without requiring any training phase.
We have studied the performance of our classifier in a multi-label scenario, using the simple adaptation method discussed in Section 5, with the k-mean as clustering method and the mini-batch optimization as centroid selection crite-rion. The results are illustrated in Table 6, where we report precision, recall, F 1 score, and the number of tweets, for each category. The last gray row, Overall, contains the over-all behavior of our classifier, computed as the mean over all categories. EgoCentric obtains a 0.86 overall F 1 score. Fi-nally, we have computed the average Hamming loss between the set of labels predicted by our classifiers and the ground truth defined by the validators as index of performance. If  X  y is the predicted value for the j-th label of a given sample, y is the corresponding true value, and n cat is the number of categories, then the Hamming loss L Hamming between two samples is defined as:
Figure 3: Execution time vs Size of input message. where 1( x ) is the indicator function. EgoCentric has an Hamming loss equal to 0.052, meaning that, on average, misclassified tweets are very few. For example, our classifier correctly classifies tweets like  X  X ank holiday? Let the meet-ings begin  X  as work and holiday, and  X  X o love. Only sex, drugs, alchool and party hard, ***. X  as sex and alcohol.
In the last experiment, we studied how the execution time of EgoCentric changes by increasing the size of input mes-sages. The experiment has been conducted on a MacBook Pro with an i7 2.3 Ghz, 16 GB of Ram and 512 GB of SSD. The results are showed in Figure 3. The x axis reports the size of the message, which is measured by the size of a tweet. A size one means that the message is equal to a tweet (140 characters), a size 2 means that a message is twice a tweet (280 characters) and so on. The y axis shows the time taken by Egocentric to classify a given message. The rela-tion among the classification time of a message and its size is linear. In particular, a message with size one, i.e., a tweet, is classified in 4.3 ms , whereas a very long text with size 200 is classified in 1206 ms . In all the cases, about 80% of the classification time is due to the NER, on which we depend from the Apache OpenNLP library. However, a single tweet is classified in 4.3 ms, so about 230 tweets are analyzed per second.

On the other hand, the building of each category has taken always less than a second, with an average of 672 ms. With respect to quantitative analysis, averaging running time (10 experiments) on our two single label datasets is 15,34 sec-onds (5,37 seconds to build the categories and 9,97 seconds to classify all the snippets), and 11,98 seconds (6,07 seconds for building the categories and 5,51 seconds for classifying tweets), respectively. On search snippets, EgoCentric is nine times faster than ZH that achieves the second best overall accuracy. In fact, ZH takes 135.58 seconds to classify all the snippets due to its time consuming re-training phases.
In this paper, we have proposed a simple but effective unsupervised and knowledge-based method for short text classification, representing each category of interest as an ego-network. The method is independent from the category set, the only requirement is to know the category centroids. Empirical results show that our method outperform baseline methods and it can be extended to multi-label classification preserving good performance. As future work, we plan to make EgoCentric a general framework where different se-mantic score metrics and different clustering methods can be applied, studying which metric is better in which domain and investigating the best strategy for automatic centroid selection. Then, we intend to integrate EgoCentric in an Online Social Network (OSN) where each category can be enriched with user profiles. Finally, we plan to use our classi-fier to make easier the specification of OSN privacy settings. [1] E. Adler. Social media engagement: The surprising [2] C. Bizer, T. Heath, and T. Berners-Lee. Linked [3] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring [4] M. Chen, X. Jin, and D. Shen. Short text [5] Z. Dai, A. Sun, and X.-Y. Liu. Crest: Cluster-based [6] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [7] L. Li, X. Jin, and M. Long. Topic correlation analysis [8] G. Madjarov, D. Kocev, D. Gjorgjevikj, and [9] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [10] S. J. Pan and Q. Yang. A survey on transfer learning. [11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, [12] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
 [13] P. Redsicker. 5 social media trends for 2014: New [14] M. Sahami and T. D. Heilman. A web-based kernel [15] D. Sculley. Web-scale k-means clustering. In WWW , [16] D. Shen, R. Pan, J.-T. Sun, J. J. Pan, K. Wu, J. Yin, [17] SocialTimes. The growth of social media. [18] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: [19] A. Sun. Short text classification using very few words. [20] S. Zhang, X. Jin, D. Shen, B. Cao, X. Ding, and [21] X. Zhu. Semi-supervised learning literature survey.
