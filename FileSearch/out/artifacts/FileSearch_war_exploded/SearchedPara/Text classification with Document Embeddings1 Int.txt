 Text classification is a crucial and well-proven method for organizing the collec-tion of large scale documents, which has been widely used in a lot of tasks in natural language processing or information retrieval, for instance, spam filter-ing[1, 3, 5],email routing[11] and sentiment analysis[21].
 on machine learning algorithms[25], such as decision tree[7, 22], Naive Bayes[15], support vector machine (SVM)[10], and so on.
 vector, the documents are often represented with vector space model (VSM) [24] (also called bag-of-words(BOW)). Each dimension corresponds to one word, and the dimensionality of the vector is the size of the vocabulary. However, this kind of representation has two disadvantages: (1) the represented vector is often high dimensional and very sparse, which brings a challenge for traditional machine learning algorithm; (2) it ignores the semantics of the words.
 are still sparse and not optimum. A great correlation and redundant information exist among these features.
 shown excellent performance in various natural language processing tasks [17, 2, 6, 19, 9]. Each word is represented by a dense vector and words with similar meanings will be close to each other in the vector space. Distributed repre-sentations, which are originally designed for words[23], have also been used to represent phrases sentences[18, 16].
 there are two problems when utilizing the word embeddings on text classification. 1. It is still not clear to combine the word embeddings to represent the doc-2. Traditional word embeddings are learned by probabilistic language model in classification task. A document is also represented by a vector, called document embedding . Document embeddings can be calculated by the vector representa-tions of its containing words. Our method can handle all words contained in a document and need not reduce the dimensionality of input.
 embeddings of its containing words. Since the word embedding can represent different meanings of each word, the document embedding can also represent different meanings of each document and documents close to each other in the vector space may be of the same topic. The experimental results also proves this hypothesis.
 related works on text classification. Section 3 describes our architecture and learning algorithms. The experiments will be detailed described in section 4 and finally there will be a conclusion. Recently, deep neural networks are so popular and are widely used in lots of do-mains for the purpose of classification, including text classification. For instance, Restricted Boltzman Machines(RBMs) have been utilized to do the document and image classification[12].
 of input data to model the semantic correlation among words of documents for text classification. However, since he used Restricted Boltzmann Machines (RBM) [8] to obtain the high level abstraction of input data, the dimensionality of the input data need be reduced in advance. Thus, a lot of information may be lost.
 that learns continuous distributed vector representations for pieces of texts. The texts can be of variable-length, ranging from sentences to documents. Although paragraph vector can be applied to variable-length pieces of document, it is learned separately before they are used in text classification.
 vectors in an order given by a parse tree of a sentence, using matrix-vector operations. However, it has been shown to work for only sentences because it relies on parsing. According to our hypothesis, a document can be represented as the accumulation of all words it contains. Each word has an exact and unique meaning, which is represented by the different decimals in each element of the word embedding. Similarly, the document X  X  meaning is also represented by each element of its document embedding . 3.1 Document Embeddings The architecture we propose is described in figure 1. X i means the i th word shows up X i times in the document. We first transform X into x by: so that the following calculations will not be affected by the length of the doc-ument.
 U is a look up table of the dictionary. Each column in U is a word embedding. As the description above, a document S is calculated as: where Z 1 is a document embedding.
 where W is the a weight matrix and f ( z ) is sigmoid activation function: 3.2 Training phases The training problem is to determine the parameters of the network = ( U; W ) from the training data.
 through all the training data iteratively, and update the weight matrices U and W online (after processing every document).
 At each training phase, is updated with the standard backpropagation algo-rithm and the gradient of the error vector is computed using a cross entropy criterion: Where y ( x ) is the gold classification vector, using n-hot encoding and the cross entropy[20] we use here is: where m is the category count.
 follows: We assume Now that through back propagation we get Then using chain derivation rule we get Now we can easily find that the weight matrix W between the document vector d ( x ) and the output layer y ( x ) can be updated as: and the dictionary look up table matrix U can be updated following: where is the learning rate. We evaluate the performance our method by comparing the BOW representation. 4.1 Datasets We setup our experiments on two datasets: LSHTC and Sogou datasets. fication (LSHTC) Challenge 3 . The challenge is based on a large dataset created from Wikipedia and the document set is multi-class, multi-label and hierarchical though we do not utilize any hierarchical information. Documents here is high dimensional(roughly 1,620,000) and very sparse on each category. The format of each document is like: which means the document belongs to category 12370 and 30678 at the same time, and the remaining part is the sparse representation in bag-of-words. that most frequently appear. We only choose the documents which only contain one category in the top 100 categories.
 10 samples for testing. Finally we get 13113 training documents and 800 testing documents, every document is single-labeled. The categories we choose are shown in table 1.
 corpus mainly come from Sohu.com. All the documents X  categories are manually labeled and the category count is 10.
 17910 documents on 9 categories. The detail on each category is shown in table 2 We firstly do word segmentation on the whole set and get about 270,000 words in our dictionary. Then we transfer each document into bag-of-words representation just like the above dataset. Finally we got 17,014 training vectors and 896 testing vectors, all are single-labeled.
 4.2 Experimental settings Firstly, we initialize U and W (described in section 3) with random decimals between -1 and 1. We found that the larger the hidden units count is, the higher the classification accuracy will be, however the memory and time cost will also grow up. So hidden units count is set to 30 , balancing the memory cost and the outcome accuracy. Learning rate is dynamic and initialized to 1.0. When error ( x ) grows up in 10 consecutive training documents, learning rate will decrease by 0.01, vice versa.
 tic Gradient Descent (SGD). We go through all the training data iteratively, and update the weight matrices U and W online (after processing every document) until convergence appears. Here we define convergence as error ( x ) is smaller than 10 5 in 10 consecutive training documents.
 network directly as the classification vector and use the index of the largest element as the category number. Second, we replace each point in bag-of-words vector with the corresponding column in U (which is in fact a kind of word embedding), multiplying the T F of the word. Then we use a linear SVM classifier to train and test the new document vectors.
 computing and accelerate the whole process. 4.3 Results To compare the performance of our representations with BOW, we use the pop-ular LIBSVM[4] as the final classifier. Table 4 and 5 show the results. NN-15 represents that the dimensionality of document embedding is 15, while NN-30 represents that the dimensionality of document embedding is 30.
 results without combining SVM. The evaluations without SVM have a similar accuracy with BOW+SVM when a document is represented by 30 dimensional vector. With combining SVM, our method outperforms BOW+SVM a lot on both the datasets.
 mances on both the datasets. This is mainly because we represent each docu-ment in a more detailed way comparing with pure bag-of-words representations. And the detailed representations are highly related to the topic of documents. Thus making the classification has a higher accuracy.
 quite different. Performance on Sogou is better than that on LSHTC, on all the five methods. That is perhaps due to different data density of the two datasets. We believe that our architecture is more powerful on dense dataset, which has more average documents on each category. Therefore, our method which can generate document embedding to represent a document and do the document classification task is efficient and useful. Liu [14] used deep belief network (DBN) for text classification. However, since he used Restricted Boltzmann Machines (RBM) [8] to obtain the high level abstraction of input data, the dimensionality of the input data needs to be reduced in advance. Thus a lot of information may be lost.
 that learns continuous distributed vector representations for pieces of texts. The texts can be of variable-length, ranging from sentences to documents. Although paragraph vector can be applied to variable-length pieces of document, it is learned separately before they are used in text classification.
 vectors in an order given by a parse tree of a sentence, using matrix-vector operations. However, it has been shown to work for only sentences because it relies on parsing. In this paper, we propose a neural network architecture for text classification. In our architecture, each document is represented by a low dimensional embedding that is similar to word embedding. Experiments show that our embeddings have a higher classification accuracy than BOW vectors.
 task. Besides, we will also investigate whether it can increase the performance by increasing the network layers.
 We would like to thank the anonymous reviewers for their valuable comments. This work was funded by NSFC (No.61003091) and Science and Technology Commission of Shanghai Municipality (14ZR1403200).
 [1] Androutsopoulos, I., Koutsias, J., Chandrinos, K.V., Paliouras, G., Spy-[2] Bengio, Y., Schwenk, H., Sen X cal, J.S., Morin, F., Gauvain, J.L.: Neural [3] Carvalho, V.R., Cohen, W.W.: On the collective classification of email [4] Chang, C.C., Lin, C.J.: Libsvm: a library for support vector machines. ACM [5] Cohen, W.W.: Learning rules that classify e-mail. In: AAAI spring sympo-[6] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, [7] Dumais, S., Platt, J., Heckerman, D., Sahami, M.: Inductive learning algo-[8] Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data [9] Huang, E.H., Socher, R., Manning, C.D., Ng, A.Y.: Improving word repre-[10] Joachims, T.: Text categorization with support vector machines: Learning [11] Khosravi, H., Wilks, Y.: Routing email automatically by purpose not topic. [12] Larochelle, H., Bengio, Y.: Classification using discriminative restricted [13] Le, Q.V., Mikolov, T.: Distributed representations of sentences and docu-[14] Liu, T.: A novel text classification approach based on deep belief network. [15] McCallum, A., Nigam, K., et al.: A comparison of event models for naive [16] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed [17] Mikolov, T., Yih, W.t., Zweig, G.: Linguistic regularities in continuous space [18] Mitchell, J., Lapata, M.: Composition in distributional models of semantics. [19] Mnih, A., Hinton, G.E.: A scalable hierarchical distributed language model. [20] Nasr, G.E., Badr, E., Joun, C.: Cross entropy error function in neural net-[21] Pang, B., Lee, L.: Opinion mining and sentiment analysis. Foundations and [22] Quinlan, J.R.: Induction of decision trees. Machine learning 1(1), 81 X 106 [23] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by [24] Salton, G., Wong, A., Yang, C.: A vector space model for automatic index-[25] Sebastiani, F.: Machine learning in automated text categorization. ACM [26] Socher, R., Lin, C.C., Ng, A.Y., Manning, C.D.: Parsing Natural Scenes [27] Yang, Y., Pedersen, J.: A comparative study on feature selection in text
