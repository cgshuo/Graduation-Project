 This tutorial addressed two trending topics in the field of rec-ommender systems research, namely A/B testing and real-time recommendations of streamed data. Focusing on the news domain, participants learned how to benchmark the performance of stream-based recommendation algorithms in a live recommender system and in a simulated environment. H.4 [ Information Systems Applications ]: Miscellaneous Experimentation Stream Recommendation; A/B testing; Offline Evaluation
The release of the Netflix dataset and the associated chal-lenge was a key event that resulted in offline evaluation [11] being the most commonly used methodology for the evalua-tion of recommender systems. Although it comes with clear advantages such as the ability to compare the performance in predicting user ratings with state-of-the-art techniques, it has been criticized for its inability to incorporate the user in the evaluation process. An alternative evaluation paradigm is online evaluation , also referred to as A/B testing [1]. Online evaluation aims to benchmark varieties of a recom-mender system by a larger group of users. It is increasingly adopted for the evaluation of commercial systems with a large user base as it provides the advantage of observing the efficiency of recommendation algorithms under real condi-tions. However, while online evaluation is the de-facto stan-dard evaluation methodology in Industry, university-based ommendation task to interested researchers via their Open Recommendation Platform (ORP) [2]. As users visit se-lected news publishers, ORP randomly forwards recommen-dation requests to registered participants [8]. The tutorial will detail the idea, technologies, and existing SDK to use ORP. Hence, participants will be able to evaluate their rec-ommendation algorithms in a live system. We argue that this use case represents a unique opportunity for researchers in the area of recommender systems.

The second subtask of NewsREEL focuses on simulating a constant data stream as provided by ORP. In contrast to the first scenario, performing an offline evaluation allows us to issue the same request to different algorithms and subse-quently compare them. Additionally, it allows to measure factors such as time and space complexity. In NewsREEL, the Idomaar framework 2 is employed to simulate this data stream. Idomaar is a novel reference framework that was developed in the context of the European project CrowdRec [9]. It adopts open-source technologies widely known by the research community to allow handling of large-scale streams of data (e.g., Apache Kafka, Apache Spark, etc.). Focusing on the news scenario, participants learned how to use this technology to simulate constant data streams.
This tutorial was designed for researchers and practition-ers in the field of recommender algorithms. Researchers learned the steps required to setup a recommendation server, connect to ORP and evaluate recommendation algorithms. Practitioners learned about the special characteristics of str-eamed data for recommender systems. Further, they got to know recommendation algorithms able to deal with such conditions. Participants learned how to use ORP and the reference framework Idomaar to evaluate their recommen-dation algorithms in an authentic and a simulated setting, respectively.
In recent years, a significant number of papers focused on improving the accuracy when predicting ratings. Typically, rating prediction evaluation starts by partitioning available data into training and test sets. However, commercial rec-ommender systems face dynamics disregarded by this method-ology as they often meet an endless stream of interactions between two fluctuating sets.

This tutorial supported researchers to start focusing on streams as recommender systems X  input. We showed how participants can approximate an authentic way to evaluate their recommendation algorithms. This can help the com-munity to better distribute their attention to problems urg-ing industrial recommender systems.
Having attended the tutorial, participants learned how to tackle the specific challenges of stream-based recommen-dations, in particular news recommendation. The tutorial highlighted differences between living lab scenarios and stream simulation. The following aspects were covered: http://rf.crowdrec.eu/
