 Colorado Reed cr478@cam.ac.uk Zoubin Ghahramani zoubin@eng.cam.ac.uk Engineering Department, Cambridge University, Cambridge UK S.1. Truncated Gaussian Properties In the main text we examined a truncated Gaussian of the form: with N representing a Gaussian distribution. The first two moments of T N (  X   X  kd ,  X   X  2 kd ) are: representing the scaled complementary error function. The entropy is
H ( q ( a kd )) = S.2. Shifted Equivalence Classes Here we discuss the  X  X hifted X  equivalence class of bi-nary matrices first proposed by Ding et al. (2010). For a given N  X  K binary matrix Z , the equivalence class for this binary matrix [ Z ] is obtained by shifting all-zero columns to the right of the non-zero columns while maintaining the non-zero column orderings, see Fig-ure 1. Placing independent Beta(  X  K , 1) priors on the Bernoulli entries of Z and integrating over these pri-ors yields the following probability for Z , see Eq. 27 in Griffiths &amp; Ghahramani (2005): not change in the infinite limit. For the second term we let K 0 = K  X  K + and have K ! in Griffiths &amp; Ghahramani (2005) show that this term becomes 1 as K  X   X  . The infinite limit of the third and fourth terms are determined in the Appendix of Griffiths &amp; Ghahramani (2005). Combining all four terms together yields:
P ([ Z ]) = where H N is the N th harmonic number.
 The probability of the shifted equivalence class is nearly identical to the probability of the left-ordered-form equivalence class: P ([ Z ] lof ) = where K h is the number of columns of Z with binary value h  X  { 1 ,..., 2 N  X  1 } when the first row is taken to be the most significant bit. The only difference be-tween Eq. 9 and Eq. 10 is the denominator of the first fraction. For the left-ordered-form, this term penal-izes Z matrices with identical columns. In the feature assignment view, this term penalizes features that are assigned to the exact same set of observations. The K + ! term in the shifted equivalence class prior does not distinguish between identical and distinct columns of Z , and in turn, does not penalize repeated feature assignments. These two equivalence class probabilities are proportional in the limit of large N as the proba-bility of two columns being identical approaches 0. S.3. Hyperparameter Inference In the main text we assumed the hyperparameters  X  = {  X  X , X  A , X  } were known (i.e. estimated from the data). Placing conjugate gamma hyperpriors on these parameters allows for a straightforward extension in MEIBP inference is carried out exactly as discussed in the main text except all instances of  X  X , X  A , and  X  are replaced with the expectation from their respective variational distribution. Furthermore the variational lower bound also has three additional entropy terms for gamma distributions, one for each hyperparameter. S.4. Evidence as a function of Z n  X  As shown in the main text, we obtain a submodular objective function for each Z n  X  , n  X  X  1 ,...,N } by ex-amining the evidence as a function of Z n  X  while holding constant all n 0  X  X  1 ,...,N }\ n . The evidence is which nearly factorizes over the Z n  X  because the like-lihood component and parts of the prior components naturally fit into a quadratic function of Z n  X  . The ln K + ! and  X  k only couple the rows of Z when K + changes, while the log-factorial term couples the rows of Z through the sums of the columns. Both of these terms only depend on statistics of Z (the m k values and K + ), not the Z matrix itself, e.g. permuting the rows of Z would not affect these terms. Furthermore, ln K + and  X  k have no N dependence and become in-significant as N increases. These observations, in con-junction with the MEIBP performance in the exper-imental section of the main text, indicate that opti-mizing Eq. 24 for Z n  X  is a reasonable surrogate for optimizing Z .
 Here we explicitly decompose Eq. 24 to show its Z n  X  dependency. Decomposing ln straightforward if we first define the function:  X  ( z nk ) = where the  X  \ n  X  subscript indicates the variable with S.5.1. Learning K + An ostensible advantage of using Bayesian nonpara-metric priors is that a user does not need to specify the multiplicity of the prior parameters. Clever sam-pling techniques such as slice sampling and retrospec-tive sampling allow samples to be drawn from these nonparametric priors, c.f. Teh et al. (2007) and Pa-paspiliopoulos &amp; Roberts (2008). However variational methods are not directly amenable to Bayesian non-parametric priors as the variational optimization can-not be performed over an unbounded prior space. In-stead, variational methods must specify a maximum model complexity (parameter multiplicity). Several heuristics have been proposed to address this limita-tion: Wang &amp; Blei (2012) sampled from the variational distribution for the local parameters X  X hich included sampling from the unbounded prior X  and used the empirical distributions of the local samples to update the global parameters, while Ding et al. (2010) simply started with K + = 1 and greedily added features. We did not address these techniques in this work as the MEIBP performed competitively with the unbounded sampling techniques without employing these types of heuristics. Furthermore, here we demonstrate that the MEIBP can robustly infer the true number of latent features when the K + bound is greater than the true number of latent features.
 For this experiment we generated the binary images dataset used in Griffiths &amp; Ghahramani (2005), where the dataset, X , consisted of 2000 6  X  6 images. Each row of X was a 36 dimensional vector of pixel inten-sity values that was generated by using Z to linearly combine a subset of the four binary factors shown in Figure 2. Gaussian white noise, N (0 , X  X ), was then added to each image, yielding X = ZA + E . The feature vectors, Z n  X  were sampled from a distribution in which each factor was present with probability 0 . 5. Figure 3 shows four of these images with different  X  X values.
 We initialized the MEIBP with K = 20,  X  X =1.0,  X 
A = 1 . 0,  X  = 2,  X   X  kd  X  |N (0 , 0 . 05) | (variational fac-tor means),  X   X  kd  X  X N (0 , 0 . 1) | (variational factor stan-dard deviations), z nk  X  Bernoulli( 1 3 ). With this ini-tialization, we tested the MEIBP robustness by per-Attias, H. A variational bayesian framework for graph-ical models. Advances in Neural Information Pro-cessing Systems , 12:209 X 215, 2000.
 Ding, N., Qi, Y.A., Xiang, R., Molloy, I., and Li, N. Nonparametric Bayesian matrix factorization by
Power-EP. In 14th Int X  X  Conf. on AISTATS , vol-ume 9, pp. 169 X 176, 2010.
 Griffiths, T. and Ghahramani, Z. Infinite latent fea-ture models and the Indian buffet process. Technical report, Gatsby Unit, UCL, London, UK, 2005.
 Papaspiliopoulos, O. and Roberts, G. O. Retrospec-tive Markov chain Monte Carlo methods for Dirich-let process hierarchical models. Biometrika , 95(1): 169 X 186, 2008.
 Teh, Y.W., Gorur, D., and Ghahramani, Z. Stick-breaking construction for the indian buffet process. In Int X  X  Conference on AISTATS , volume 11, 2007. Wang, C. and Blei, D. Truncation-free online varia-tional inference for bayesian nonparametric models.
In Advances in Neural Information Processing Sys-
