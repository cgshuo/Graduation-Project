 Wei Lee Woon  X  Kuok-Shoong Daniel Wong of-concept, we had previously presented a framework where the optimal alignments of doc-uments could be used for visualising the relationships within small sets of documents. In is proposed.
 Keywords String matching  X  Text processing  X  Data mining  X  Versioning  X  Information retrieval 1 Introduction we demonstrated the use of string alignment techniques (normally used to study biological modifications.
 tion in document versioning. The system is presented with a number of unlabelled document only inter-document distances. The proposed method serves both as a useful application in mining. 1.1 Motivations Rapid advances in digital database technology motivated the development of data mining increase.
 mining concepts and paradigms by the document mining community. One notable example is the way in which documents are represented; widely known as the  X  X ector space model X  allowing standard data mining tools to be applied.

However, there are also short-comings to this approach. In particular, there are crucial differences between documents and database records which make the VSM inappropriate data base greatly exceeds the number of fields. While this situation may still arise when when smaller sets of documents need to be analysed. The worst-case scenario occurs with a very small collection of very long documents, such as when analysing a set of source code versions. The resulting vector space would be very sparse and probably rank deficient. To overcome this problem, intensive dimensionality reduction is often performed using either feature selection procedures [ 3 ] or dimension-reducing projections [ 26 ].
In addition, document mining research has focused on the data-mining derived operations documents or source code versions provides an example where the documents do not occur in in data mining, the problem of analyzing the version histories of documents has received scant attention. 1.2 Related work clustering of documents, most of which are conducted in the context of the vector space representation of documents.

However, while unusual, previous studies of alternative document representations have Levenshtein X  X  distance between documents [ 12 ]; however, this method uses alignments at study of text entry methods for mobile phones [ 25 ]. Our method operates at the level of documents.

Another similar approach in terms of document representation is the relatively recent where each  X  X imension X  is the weighted number of occurrences of a particular substring (which need not be contiguous) in a document. As the number of such substrings is poten-concerned.
 document mining schemes include the use of contextual features of words to derive addi-examples include sentence level contextual information [ 22 ]downtoconsecutivewords[ 6 ]. words has also been presented in [ 4 ].

One of the most notable examples of an approach that focuses on the exploratory aspect of document mining (as opposed to classification and retrieval) is the WEBSOM technique web search results to help elucidate themes and subject areas. Another similar study uses to automatically organize a collection of web-documents into a coherent hierarchy.
In short, while still relatively uncommon, the study of alternative frameworks for anal-mentioned above do bear certain similarities to the proposed methodology, we believe that the novel combination of the document alignment measure with the document versioning investigation in this area, as well as a useful new procedure in its own right. for future research. 2 Automatic document versioning 2.1 Overview where we proposed a simple system for visualising similarity relationships within sets of documents based on the following two-step procedure: 1. Calculate the differences between documents in terms of the minimum edit distance,
In this paper we extend the above-mentioned framework by demonstrating its performance in automatic document versioning. 2.2 Data construct appropriate data sets of our own. In total three such sets were identified: 2.2.1 Linux kernel source code writing 656 versions in total) X  X aking it suitably challenging X  X nd it is a well known and publicly available resource. 1  X  /linux/drivers/block/loop.c  X  /linux/kernel/fork.c  X  /linux/net/ethernet/eth.c.
 eth.c , respectively. 2.2.2  X  X iki X  pages of webpages, the  X  X ayback Machine X  2 was used to retrieve previous versions of webpages.
Ideally we would like to use webpages which are regularly updated and which have a regularly updated and comparatively lengthy entries.
 page, we were able to obtain 82 versions. 2.2.3 University webpages of general (i.e., non-wiki) webpages, as these were likely to be more challenging. For this test, the homepages of the universities of Cambridge and Oxford were chosen. As before, the Wayback Machine was used to obtain earlier versions of the pages, which for these two sites were 406 and 314, respectively. 2.3 Feature extraction 2.3.1 Edit distance two documents or strings. This is found using a dynamic programming algorithm similar to the Needleman X  X unsch method used to align biological sequences [ 19 ]. Briefly, the algo-in the two strings to be compared.

As shown in the figure, the algorithm starts from the top left corner with a score of 0, column is filled out and so on. Each cell is filled using the following relationship: described here.

Note that the cells on the leftmost column or top row can only be reached by the cell following three operations: 1. A gap inserted into string x . 2. A gap inserted into string y . 3. A substitution of x i with y j (or vice versa).
 The choice amongst the three alternatives is determined by which will provide the high-programming algorithm may be obtained from [ 17 ]. 2.3.2 Vector space representations In addition to the method above, we also use two VSM-based document representations in our experiments. While serving as a benchmark against which the alignment based approach versioning, since that would also be an interesting discovery.

There are a large number of VSM-based document features available [ 10 ]; we choose two common variants: 1. Term Frequency (TF). This is simply the raw count of the number of occurrences of 2. Term Frequency Inverse Document Frequency (TFIDF). For term i , this is defined as:
The two methods above both result in a vector representation, where each document is defined as: representation of document d . 2.4 Inferring document version history Once the inter-document distances for all pairs of documents in the collection have been calculated using one of the methods described above, the problem of inferring the version history can be formulated as the problem of finding the minimum Hamiltonian path .Ingraph graph exactly once (Fig. 2 is an illustration of this).

The document versioning problem maps easily into this context, where each document is regarded as a vertex, with the weight of the edges connecting the documents given by the edit distances between them. As the graph representing a collection of documents would be follow the principle of parsimony and assume that the path with the minimum total weight represents the most likely candidate for the actual version history.
 search algorithm: Algorithm GreedySearch ( { V , E } ) Input: Aset { V , E } of vertices and weighted edges (i.e.: distance) Output: Shortest Hamiltonian path found [ v 1 ,v 2 ,...,v n ] 1. v  X  [] 2. Select random v  X  V and insert into list v 3. repeat 4. V  X  V  X  X  v } 5. Select v  X  V with minimum distance to the vertices on either end of v 6. Append v to this end of v 7. until | V |== 0 ods (for example genetic algorithms), which might have produced slightly better results. However, it was felt that for the present preliminary investigation the method chosen was adequate. 2.5 Scoring of results In order to evaluate the accuracy of the proposed method, a method for numerically scor-this purpose. The procedure is as follows: Algorithm Score ( d 1 , d 2 ,..., d n ) Input: An ordered list of document versions Output: Numerical score reflecting the accuracy of the list 1. Accuracy  X  0 2. for i  X  1 to n 3. for j  X  1 to ( i  X  1) 4. if Earlier ( d j , d i ) then 5. Accuracy  X  Accuracy+1 6. for j  X  ( i + 1) to n 7. if Earlier ( d i , d j ) then 8. Accuracy  X  Accuracy+1 9. return 100  X  Accuracy/ [ ( n  X  1 ) n ] % history of the document.
 intuition, we generalise the above method by allowing the calculation to be restricted to localised regions of the list. This local variant of the algorithm is as follows: Algorithm LocalScore (w , d 1 , d 2 ,..., d n ) 1. Accuracy  X  0 2. for i  X  1 to n 3. TmpAccuracy  X  0 4. for j  X  max { 1 , i  X  w } to ( i  X  1) 5. if Earlier ( d j , d i ) then 6. TmpAccuracy  X  TmpAccuracy+1 7. for j  X  ( i + 1) to min { n , i + w } 8. if Earlier ( d i , d j ) then 9. TmpAccuracy  X  TmpAccuracy+1 11. return 100  X  Accuracy/ n % 2.6 Significance of results To achieve this, surrogate data was generated by randomly permuting the order of the ver-against which the actual score obtained was compared. A standard T distribution was used to model this collection.
 approaching 1.00. 2.7 Optimality of the minimum path approach the minimum path will actually result in the correct version history.

To address this issue, a number of additional observations and experiments have been we would expect that this total score would be the minimum hamiltonian path through all the document versions.
 to the following alternative version orderings: 1. The sequence obtained using the greedy algorithm; this gives us an idea of how much 2.8 Implementation details all experiments were conducted on a Dual Quad-core 2.33 GHz Xeon Processor system with 4 GB of RAM. As a general guide, aligning two 5,000 word documents requires around 10 s, two 8,000 word documents takes 47 s while aligning two 10,000 word documents requires on these figures as the implementation would require significant further optimisation. 3Results 3.1 Notation results are tabulated in Tables 1 , 2 , 3 . The following notation is used: 3.2 Source code versioning task
The scores obtained when the versioning algorithm was applied to the kernel source code are presented in Table 1 . The following observations could be made: 3.3 Wikipedia entries on Perl. Some observations are: 3. In general, using the raw HTML versions of the documents resulted in better accura-3.4 University webpages These results are presented in Table 3 . 3. The DA scores were still marginally higher in the case of the Cambridge University 3.5 Comparison of true and optimal version histories for the kernel source code, Wikipedia and university webpages, respectively. To simplify absolute distance by the traversal distance of the actual version history.
From these results it can be observed that: 2. The ratios for the approximated values are broadly higher for the experiments which 3. For the traversal distances of sequences in the immediate neighbourhood of the true 4 Conclusions and future plans As with any method based on inferring an unseen process, there were some errors in the distance model used, results proved to be less impressive.

An advantage of the described framework is that it is hugely flexible and can be adapted plentiful, including (but not restricted to): 3. One of the main problems which we encountered was the heavy computational require-the proposed method were conducted with three different types of documents, and with at were invariably favourable.
 References Author Biographies
