 There has been considerable work on user browsing mod-els for search engine results, both organic and sponsored. The click-through rate (CTR) of a result is the product of the probability of examination (will the user look at the re-sult) times the perceived relevance of the result (probability of a click given examination). Past papers have assumed that when the CTR of a result varies based on the pattern of clicks in prior positions, this variation is solely due to changes in the probability of examination.

We show that, for sponsored search results, a substantial portion of the change in CTR when conditioned on prior clicks is in fact due to a change in the relevance of results for that query instance, not just due to a change in the prob-ability of examination. We then propose three new user browsing models, which attribute CTR changes solely to changes in relevance, solely to changes in examination (with an enhanced model of user behavior), or to both changes in relevance and examination. The model that attributes all the CTR change to relevance yields substantially better predictors of CTR than models that attribute all the change to examination, and does only slightly worse than the model that attributes CTR change to both relevance and exami-nation. For predicting relevance, the model that attributes all the CTR change to relevance again does better than the model that attributes the change to examination. Surpris-ingly, we also find that one model might do better than an-other in predicting CTR, but worse in predicting relevance. Thus it is essential to evaluate user browsing models with respect to accuracy in predicting relevance, not just CTR. H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval Algorithms, Experimentation, Human Factors KDD X 10 , July 25 X 28, 2010, Washington, DC, USA.

Web search engines have become an essential tool for nav-igating the vast amounts of information on the internet. Im-plicit user feedback, specifically, click-through data, is valu-able for optimizing search engine results [2, 10, 14]. Click-through data plays an equally important role in estimating the quality of sponsored search results [15].

Any attempt at using click-through data for search or sponsored search runs into the following issue: Eye-tracking studies show that people tend to scan the search results in order [3, 11]. However, they are likely to click on a result as soon as they find one that they consider helpful, and if that result provides a sufficiently helpful answer, may not look at other results. This causes position bias : the same result will get a higher click-through rate (CTR) if it is positioned towards the top of the page (versus the bottom).

Thus algorithms that use click-through data have to take position bias into account. Initial work on estimating posi-tion bias modeled CTR as the product of perceived relevance (probability of a click given that the user examined the re-sult) times the probability of examination (probability that the user would examine this specific position) [15]. The ex-amination probability was estimated by looking at the CTR of the same result in different positions.

Subsequently, there have been many papers on better esti-mating the probability of examination by using the pattern of clicks on prior results [5, 7, 9, 12], or using both prior clicks and the relevance of prior results [4, 8, 18]. We discuss this work in detail in Section 2. The key point is that all of these papers assume that if CTR changes when conditioned on the pattern of clicks on prior results, the change in CTR is solely due to changes in the probability of examination.
Consider a query such as  X  X anon S90 X . The user could be planning to buy the camera immediately, in which case the sponsored results are highly relevant. On the other hand, if the user is just starting to learn about the camera, the sponsored results will be much less relevant. Thus a click on the first sponsored result is a signal that the other sponsored results are also relevant.

Say we now partition the query instances corresponding to the query  X  X anon S90 X  into two sets based on whether the first result got a click: the  X  X lick X  and  X  X o-click X  sets. The second result will be highly relevant for the query instances in the  X  X lick X  set, and less relevant for the  X  X o-click X  set, even though the query and result are the same. Thus the second result will have higher CTR in the  X  X lick X  set than in the  X  X o-click X  set. However, current user models assume that the relevance is the same in both the  X  X lick X  and  X  X o-click X  sets, and that all the difference in CTR for the second result is because users in the  X  X lick X  set were more likely to examine the second result than users in the  X  X o-click X  set.
In this paper, we examine the implications of the above insight. In Section 3, we show that, for sponsored search results, an increase in relevance is indeed responsible for a substantial portion of the increase in CTR when conditioned on prior clicks. We then propose three new user browsing models in Section 4, which attribute CTR changes solely to changes in relevance, solely to changes in examination (with an enhanced model of user behavior), or to both changes in relevance and examination. We evaluate the accuracy of these models when predicting CTR in Section 5, and the accuracy when predicting relevance in Section 6. Our results show that, surprisingly, one model might do better than another in predicting CTR but worse in predicting relevance. We conclude with a summary of our results and directions for future work in Section 7.
Prior user browsing models for web search results can be partitioned into three groups based on how they estimate the probability that the user examines a specific position: Some of the models were originally targeted at sponsored search, while others were targeted at organic search results. However, while the parameter values might differ, all of these models are general enough to apply to both organic search and sponsored search.
We use the following notation: The examination hypothesis , originally proposed by Richardson et al. [15] and formalized by Craswell et al. [5], observes that to be clicked, a result must be both examined and relevant: Richardson et al. [15] assume that the probability a result is viewed depends solely on its position, and is independent of other results.

We call the statistical model that derives from the exam-ination hypothesis the baseline model : where Richardson et al. [15] proposed estimating the  X  i eters by presenting users with the same result at different positions and observing the change in CTR.
An implicit assumption in the above formulation is that the probability of examining the result in position i does not depend on click events in other result positions. A plethora of recent papers explore models that incorporate this infor-mation into the examination probabilities.

The cascade hypothesis [5] assumes that users scan each result sequentially without any skips:
The cascade model [5] further constrains that the user continues examining results until she clicks on a result, and does not examine any additional results after the click: This model is quite restrictive since it allows at most one click per query instance.

The dependent click model (DCM) [9] generalizes the cascade model to instances with multiple clicks: The authors suggest estimating the position effects  X  i using maximum likelihood.

The user browsing model (UBM) [7] is also based on the examination hypothesis, but unlike the cascade model and DCM, does not force Pr( E i = 1 | E i  X  1 = 1 ,C i  X  1 to be 1. In other words, it allows users to stop brows-ing the current results and instead reformulate the query (or perhaps give up). UBM assumes that the examination probability is determined by the preceding click position p ( i ) = argmax l&lt;i { C l = 1 } : where  X  i = Pr( E i = 1) is the examination probability of po-sition i without taking other click information into account (just as in the baseline model), and  X  i,p ( i ) denotes the cor-rection factor over  X  i given p ( i ) = argmax l&lt;i { C avoid confusion between the above  X  X ser browsing model X  in Equation 4, and the category of user browsing models, we will refer to this specific model as UBM.

The bayesian browsing model (BBM) [12] uses exactly the same browsing model as UBM. However, BBM adopts a Bayesian paradigm for relevance, i.e., BBM considers rel-evance to be a random variable with a probability distri-bution, rather than a fixed (but unknown) value to be esti-mated. In the context of this paper, where we are focused on the user browsing model, UBM and BBM can be considered equivalent. Note that our notation is slightly different than that in [7]. We consider  X  to be a correction factor on  X  , while they used  X  for the product of our definitions of  X  and  X  .
Next, we summarize models that take into account both clicks on prior results, and the relevance of those results, to predict the probability of examination.
 The click chain model (CCM) [8] is a generalization of DCM obtained by parameterizing  X  i and by allowing the user to abandon examination of more results: Pr( E i = 1 | E i  X  1 = 1 ,C i  X  1 = 0) =  X  1 .
 Thus if a user clicks on the previous result, the probability that they go on to examine more results ranges between  X  2 and  X  3 depending on the relevance of the previous result.
The general click model (GCM) [18] treats all relevance and examination effects in the model as random variables: This allows online inference within the cascade family. These authors show that all previous models are special cases by suitable choice of the random variables A i ,B i , and r  X  ( i )
In our discussion so far, relevance referred to  X  X erceived X  relevance  X  whether the user considers the result relevant before she clicks on the result. Post-click relevance is a mea-sure of whether the user was satisfied with their experience after clicking on the result. Perceived relevance is positively correlated with post-click relevance [16]. However, there are cases where perceived relevance is high and post-click rele-vance is low (e.g., snippet or creative is inaccurate), or vice versa (e.g., only a small fraction of people searching  X  X a-hoo X  want answers.yahoo.com  X  but for those people, it X  X  perfect). Thus both perceived and post-click relevance are equally important for user satisfaction.

The dynamic bayesian model (DBM) [4] uses the  X  X ser satisfaction X  (post-click relevance) of the preceding click to predict whether the user will continue examining additional results: where s  X  ( i  X  1) is the satisfaction of the user in the previous clicked result. They propose an EM-type estimation method to estimate  X  and the user satisfaction variables.

The session utility model (SUM) [6] proposes a user browsing model based on the  X  X ntrinsic X  (post-click) rele-vance of the sequence of clicked results in a user session. However, it does not model examination or pre-click rele-vance.

Our focus in this paper is on correctly estimating exam-ination and perceived relevance. Thus in the rest of the paper, we will use  X  X elevance X  as shorthand for  X  X erceived relevance X , and focus on the models in Sections 2.1 and 2.2. We will briefly revisit the other models when we discuss fu-ture work in Section 7. There is an implicit assumption underlying the models in Sections 2.1, 2.2 and 2.3: More formally, these models assume that the perceived rel-evance, Pr( C i = 1 | E i = 1), of a result is independent of the pattern of clicks on prior results:
The constant relevance assumption may appear quite rea-sonable at first glance: Isn X  X  the relevance of the result sim-ply dependent on the query and the result? However,  X  X ele-vance X  can have two very different meanings: 2 Aggregate relevance depends only on the query and result  X  precisely the intuition behind the constant relevance as-sumption. However, Pr( C i = 1 | E i = 1 ,C 1: i  X  1 ) corresponds to instance relevance (i.e., relevance for the current query instance), not aggregate relevance. It is easy to make a case that C 1: i  X  1 is in fact a predictor of instance relevance, espe-cially for sponsored search results.

The key intuition is that the query string does not fully capture user intent . Consider a query  X  X anon T2i X . Some subset of users who issue this query will be interested in buying a camera at the time they issued the query, and sponsored search results will be highly relevant to them. Other users may be potentially interested in buying a cam-era at some point in the future, but are currently primarily interested in learning more about the camera. The spon-sored results will be much less relevant to these users. Thus for queries with multiple user intents, often only one (or a subset) of these intents is represented by sponsored search results. For such queries, Pr( C i = 1 | E i = 1) for a query instance will be strongly correlated with Pr( C j = 1 | E for the same query instance, where positions i and j both correspond to sponsored results. 3
Assume that for some position, we can get a set of query instances where we know the probability of examination is close to 1, i.e., the user examined the position with high probability. Given such a set, it would be easy to test whether the constant relevance assumption is valid. If CTR is independent of the pattern of clicks on other positions, then the assumption is true. If CTR increases as the number of clicks on other positions increases, then the assumption is false, since the CTR change must be solely due to change in instance relevance.
If one also considers post-click relevance,  X  X elevance X  has four distinct meanings: post-click versus pre-click, and ag-gregate versus instance relevance.
The fact that query reformulations are common suggests that similar correlations may also exist among organic search results. Figure 1: Distribution of the difference in positions between temporally adjacent clicks.

We now show that there is indeed a way get such a subset of query instances. If users scan results linearly from top to bottom, and there is a click at position i , then the user must have examined the results at positions 1 through i  X  1. We show next that users indeed scan linearly from top to bottom.
There is evidence from eye-tracking studies [3, 11] that users scan results linearly from top to bottom. The user browsing models in Sections 2.2 and 2.3 also assume linear scan.

However, there have also been eye-tracking studies show-ing that rather than a simple linear scan, users do page chunking. They partition the page into chunks, select the chunk they want to examine, and then scan items in that chunk in a linear fashion [1]. Hence we first do some due diligence to see whether the data supports the linear scan assumption.

Formally, we would like to assume that a click at position i implies that the user examined all preceding positions with probability close to 1:
Figure 1 shows the distribution of temporally adjacent pairs of clicks as a function of their positional distance, over all sponsored search results. A negative distance corre-sponds to pairs of adjacent clicks where the user clicks are linear scan order (i.e., top to bottom), while a positive dis-tance corresponds to bottom to top order. The gap at 0 simply means that users typically do not have consecutive clicks on the same result.

Only around 5% of temporally adjacent click pairs are both not in linear scan order and have a gap greater than 2. Even in these cases, it X  X  possible that the user scanned linearly and then went back to an earlier result, e.g., when comparison shopping. Thus Equation 5 appears to be a rea-sonable basis for testing the constant relevance assumption. Now that we have a set of queries and positions where Figure 2: Testing the constant relevance assump-tion.
 Pr( E i = 1) is close to 1, we can look at whether relevance changes when conditioned on the pattern of other clicks for that query instance.

We first introduce some notation. Let s ( i ) equal 1 iff there was a click below position i , i.e., the set { j | j &gt; i,C is non-empty. Let p 0 ( i ) equal 1 if there was a click above position i , and 0 otherwise. Equation 5 implies that if the constant relevance assumption is true, we would expect: Pr( C i = 1 | p 0 ( i ) = 1 ,s ( i ) = 1) = Pr( C i = 1 | p This follows from the fact when s ( i ) = 1, Pr( E i = 1)  X  1, and the constant relevance assumption implies that rele-vance is also fixed, so Pr( C i = 1 | p 0 ( i ) = 1 ,E i = 1) = Pr( C i = 1 | E Let Q i ( j ) denote, for some specific query and result, the subset of query instances where s ( i ) = 1 and p 0 ( i ) = j . If the CTR is different when conditioned on p 0 ( i ), i.e., and the difference is statistically significant, we would have disproved the constant relevance assumption.

To increase the power of the statistical test, we extend the above test to multiple queries and results, within a single configuration and position. Let T i ( j ) denote, for a specific configuration, the set of query instances where s ( i ) = 1 and p ( i ) = j . Since there may be small differences in the mix of queries and results between T i (0) and T i (1), we change the denominator to be the expected clicks, rather than impres-sions. Let L i ( j ) be defined as: The numerator is the observed number of clicks, i.e., the sum of the observed relevance. The denominator is the sum of the expected relevance. The constant relevance assumption implies that the ratio of the observed to the expected rele-vance should be independent of p 0 ( i ). Hence if the constant relevance assumption is true, and any errors in relevance es-timates are approximately equal in both sets, L i (1) should be roughly equal to L i (0).

We present results for the 3-8 configuration (3 top results, 8 rhs results) in Figure 2. Here T 2 refers to the second top result, R 3 refers to the third rhs result, etc. The red line shows the observed value of L i (1) /L i (0) for different positions, with a 99% confidence interval (  X  2.58 standard deviations). The confidence intervals were estimated by par-titioning the data into 10 sets and computing variance. The ratio is quite consistent across positions. The dip at R1 is because users typically scan the top sponsored results first, then the organic search results, and then the rhs sponsored results. L i (1) /L i (0) is much greater than 1 for all positions, and the results are statistically significant. This disproves the constant relevance assumption.

The results are consistent across other configurations. The weighted average of L i ( j ) /L i (0) over all configurations and positions is 2.69, with a 99% confidence interval of  X  0.05.
Having shown that changes in CTR when conditioned on other clicks are at least partly due to changes in in-stance relevance, we propose new user browsing models that take advantage of this insight. Our first model, pure rele-vance , is a strawman that assumes that CTR changes are solely due to changes in instance relevance, and not due to changes in examination. (However, as we will see later, this strawman does surprisingly well.) Our second model, max-examination , assumes (like prior work) that CTR changes are solely due to changes in examination  X  but uses addi-tional information to better predict whether the user ex-amined the result. Our third model, JRE generalizes both these models, and allows CTR changes to be caused by both changes in examination and instance relevance.
The pure relevance model assumes that any changes in the probability of a click due to conditioning on prior clicks is caused solely by change in the expected instance relevance of the result. Thus the probability of examination is assumed to be independent of clicks on other results: where C 6 = i is the pattern of clicks in all positions except i , The model then assumes that the changes in instance rele-vance can be estimated using the total number of clicks in other positions: where
The weight for a given configuration and position is Thus r  X  ( i ) is the aggregate relevance, while r  X  ( i ) estimated instance relevance after conditioning on C 6 = i pure relevance model is then defined by
Pr( C i = 1 | C 6 = i ) = Pr( E i = 1) Pr( C i = 1 | C
While the pure relevance and baseline models will yield different CTR estimates for any query instance, they will in fact yield identical relevance estimates r  X  ( i ) . For any result, both pure relevance and baseline have the same probability and examination are the same, the relevance estimate must also be the same.
Recall from Equation 5 that if there is a click on a position below i , then there is a high probability that position i was examined. So, if we include information about clicks below position i while estimating the probability of examination, the model should have significantly more information than models (like UBM/BBM) that only consider clicks above position i .

With the above intuition in mind, we propose the max-examination model. As before, let p ( i ) be the position of the preceding click. Let s ( i ) be 0 if there is no click below position i and 1 if there is a click below i . We define e ( i ): We then replace the p ( i ) used in the UBM model (Equa-tion 4) with e ( i ), and thereby incorporate the case when the click occurred below i . To avoid confusion, we also change lowing equations for the max-examination model:
A natural generalization of the pure relevance and max-examination models is to combine their features, and allow CTR changes to be caused by both changes in examina-tion and changes in instance relevance. We call this the joint relevance examination (JRE) model . We combine the relevance component from the pure relevance model (Equa-tion 6), with the examination component from the max-examination model (Equation 8), to get
Note that an estimate of  X  i,e ( i ) in the JRE model will not be the same as the corresponding value in the max-examination model, since the credit is shared between  X  and  X  . For the same reason, an estimate of  X   X  ( i ) will be different in the JRE model and the pure relevance model.

Conceptually, there is a single  X  X rue X  value of aggregate relevance, r  X  ( i ) . However, different models may yield differ-ent estimates of r  X  ( i )  X  we will explore this issue further in Section 6.
In the pure relevance and JRE models, we implicitly as-sume that the set of results are homogeneous, and therefore a click on one result would likely be a good predictor of clicks on other results. However, it is easy to come up with scenarios where the results are diverse, e.g., for the query  X  X aguar X , one would expect a positive correlation between clicks on results about the car, or between clicks on results about the animal, but a negative correlation between clicks on a car result and clicks on an animal result.

Instance relevance may also be different between top and rhs sponsored search results, even when both sets of results are on the same topic. Users typically scan top sponsored links before organic results, while they scan rhs sponsored links after organic results. Therefore the top results may be relevant, while the rhs results may be less relevant if the top or organic results were sufficient to answer the query.
Fortunately, given a partitioning of the results into (ap-proximately) homogeneous groups, it is trivial to update the pure relevance and JRE models. The only change is that  X  ( i ), rather than being the number of clicks on other results, becomes the number of clicks on other results in the same group.
We showed in Section 3.2.2 that the constant relevance as-sumption was incorrect. We now evaluate which user brows-ing models best predict CTR in offline analysis: models that attribute CTR changes solely to examination, solely to in-stance relevance, or to both examination and instance rele-vance?
We compare the three models proposed in Section 4 to two of the user browsing models from prior work: We describe how we fit the parameters in the models in 5.1, followed by experimental results in Section 5.2.
To evaluate the user browsing models for predicting CTR, we need to combine them with a machine learning system for predicting relevance. We used Google X  X  production system for predicting relevance of sponsored links for both the ma-chine learning and user browsing components of the baseline model. The user browsing model in Google X  X  system does not make use of co-click information, and is thus similar to the baseline model (Equation 2). The machine learning model for predicting relevance uses the query, the position bias for the position in which the result appeared, whether the result was clicked, and various features of the query and the sponsored result to predict relevance.

Our baseline model is sufficiently complex that, if we di-rectly add a new feature (such as a different user browsing model), it X  X  not possible to isolate the accuracy gains from the new feature, versus the new feature shifting the model to a different local optimum. To get a fair comparison, we use the output of the baseline model (  X  i r  X  ( i ) and separately optimize the co-click dependent parameters.
The input data for these models was a 10% sample, over a week, of the logged predicted CTR (  X  i r  X  ( i ) ) from the base-line model, along with co-click information. We used a 50-50 split of the data into training and testing.

For each model, we fit parameters separately for each con-figuration (number of top and rhs sponsored results) and position. To keep the notation simple, we will express the model for position j without reference to the configuration. We next describe how we estimate the parameters for each model.

We estimate the  X  i,p ( i ) parameters in UBM/BBM (Equa-tion 4), where p ( i ) represents the position of the preceding click (if any) above position i , using: The numerator corresponds to the total number of clicks at position i for query instances where p ( i ) = k , while the denominator corresponds to the expected number of clicks (without including  X  i,k ). Thus  X  i,k is set to the value where the number of clicks predicted by UBM ( P p ( i )= k  X  i  X  equals the observed number of clicks.

The methodology for estimating  X  i,e ( i ) is similar, we again equalize the predicted and observed number of clicks:
For the pure relevance model from Section 4.1, we simi-larly estimate  X  i, X i using: As we discussed in Section 4.3, the top and rhs sponsored results are sufficiently diverse that clicks on the top are not necessarily a signal of instance relevance for the rhs (and vice versa). Thus we restrict  X  ( i ) to be the number of clicks at other position in the same slot, i.e., the number of other top clicks for top sponsored results, and the number of other rhs clicks for rhs sponsored results.

For JRE, we need to estimate two parameters,  X  i,e ( i ) and  X   X  and  X  parameters to 1.0. We then repeatedly re-estimate the parameters using: The results are not sensitive to the algorithm for fitting the parameters  X  the relative accuracy of the models was similar when we tried logistic regression (after mapping the models to odds space). 5
Figure 3 shows the results for the 3-8 configuration (3 top sponsored results, 8 rhs sponsored results), for three dif-ferent accuracy metrics: log-likelihood, squared error, and absolute error. The y-axis shows improvements in the met-ric relative to the baseline model for the various positions in this configuration (shown on the x-axis). As before, T 1 represents the first top result, R 3 the third rhs result, etc.
First, note that there is a clear ordering, consistent across all metrics, between the models: JRE  X  pure relevance &gt; max-examination &gt; UBM/BBM &gt; baseline.

Consistent with prior work, UBM/BBM does significant better than baseline, by leveraging co-click information. In-terestingly, max-examination does significantly better than UBM. 6 The difference is highest for the first rhs position R 1. Top clicks are not a strong predictor of examination for the rhs, while a click below R 1 is a strong predictor of examination.

Pure relevance does slightly better than max-examination wrt log-likelihood, and substantially better wrt the other metrics. Surprisingly, JRE does only slightly better than pure relevance. Together, these results suggest that changes in instance relevance are probably responsible the majority of the changes in CTR (when conditioned on other clicks), with the caveat that the examination and relevance features are correlated (not clearly separable).

Figure 4 shows the overall percentage improvement across all configurations and positions, with 99% confidence inter-vals (2.58 standard deviations) computed by partitioning the test data into 10 groups. The relative performance of the models is consistent with Figure 3. The separations are all statistically significant, except between relevance and JRE for the first two metrics.
Many papers on user browsing models for web search eval-uate their models solely based on the accuracy of the model for predicting CTR in offline analysis [4, 7, 8, 9, 12, 18]. Exceptions include [5], who ran live experiments on organic search results, and [6], who used human ratings of relevance.
Intuitively, one might expect that the model that does best at predicting CTR (Pr( C i = 1)) offline will also do best at predicting relevance (Pr( C i = 1 | E i = 1)). How-ever, this is not the case. Consider the results from Sec-
In the initial version of the paper, we had mapped each model into the corresponding model in odds space, and used logistic regression  X  glm() in package:stats in R  X  to fit the parameters. Based on reviewer feedback, we decided to di-rectly fit the parameters. The results in the initial version of the paper were almost identical to those presented below, since CTR for sponsored links is sufficiently less than 1, and with a small enough range, that effectively odds is a linear function of probability.
The only exception is at T3, where max-examination does slightly worse. Whether there was a click on T 1 or T 2 is not purely a predictor of examination, it X  X  also a predictor of instance relevance of T 3. Our guess is that, for T 3, the instance relevance component of this signal is more impor-tant than the increase in probability of examination due to the user clicking on a rhs result. UBM/BBM 1.82  X  0.05 0.44  X  0.03 0.75  X  0.03 Max-Exam. 2.82  X  0.07 0.52  X  0.05 1.11  X  0.03 Relevance 3.22  X  0.08 1.16  X  0.07 1.88  X  0.04 JRE 3.34  X  0.09 1.21  X  0.07 1.98  X  0.04 Figure 4: Percentage improvement over all configu-rations, with 99% confidence intervals. tion 5.2. For predicting CTR, pure relevance dominated max-examination which dominated baseline. However, base-line and pure relevance yield identical relevance estimates, they only differ in the CTR estimates. So if max-examination is worse than baseline and pure relevance at predicting rel-evance, then max-examination and baseline are a pair of models where one is better at CTR and the other at rele-vance. If max-examination is better than baseline and pure relevance in predicting relevance, then max-examination and pure relevance form a similar pair, with one better at CTR and the other at relevance.

To get intuition on how a model might do better at CTR but worse at relevance, consider the following example. As-sume an oracle that outputs 1 if the user clicked, and 0 else. Consider a browsing model that uses the oracle X  X  output as Pr( E i = 1), the probability of examination. For any result, Pr( E i = 1) equals 1 if the user clicked and 0 otherwise, so we get Pr( C i = 1 | E i = 1) = 1. Every result has the same relevance. This model will have perfect accuracy in predict-ing CTR, but terrible accuracy for relevance. We give a more realistic example in Appendix A, that shows treating clicks on other results as predicting examination, when they are (partly) predicting instance relevance, can be similarly problematic.

We now present results using live experiments to deter-mine which of the two models, baseline/pure relevance, and max-examination are better at predicting relevance. Spon-sored results are ranked by expected revenue per impression, i.e, bid  X  relevance; results with higher expected revenue are shown in higher positions where they are more likely to be examined by users. A model that yields more accurate rele-vance estimates should result in a more accurate ranking of results, and therefore higher revenue and CTR.
As in Section 5.1, we used Google X  X  production system for predicting relevance of sponsored results as the baseline model. Since we are now evaluating relevance, the produc-tion system also serves as the pure relevance model (since pure relevance and baseline have identical relevance esti-mates).

For the max-examination model, we estimated  X  i,e ( i ) from the logs (separately for each configuration and position), us-ing the same methodology as in Section 5.1. We can now use the logged probability of examination (from the base-line model) and  X  i,e ( i ) to get the probability of examination with the max-examination model. We used Sawzall [13] to compute, for each sponsored result, the cumulative prob-ability of examination with both the baseline model and the max-examination model, over a week of data (without sampling). Let these values be E b and E m respectively. For each result, we multiplied the relevance scores from the baseline model by E b /E m to approximate the relevance scores we would have obtained had we trained a machine learning model to predict relevance directly using the max-examination browsing model. 7 We selected the 2 million most significant changes, where significance was defined as the number of clicks for that result times the change in rel-evance. This subset covered a substantial majority of the total significance of all the changes.

We applied these 2 million adjustments to the baseline model to get a reasonable proxy to the max-examination model. In particular, the direction of the difference in accu-racy between this model and the baseline should be the same as between a trained-from-scratch max-examination model and the baseline  X  and what we care about (in this paper) is not the exact magnitude of the difference in accuracy, but only about understanding which one is better at predicting relevance, max-examination or the baseline?
We ran a live experiment [17] on a small fraction of the google.com sponsored search traffic, and compared the met-rics of the baseline and max-examination models. We found that baseline/pure relevance had better revenue and CTR than max-examination, with the results being statistically significant. In our system, changes in revenue and CTR could also be partly due to other factors, in particular, the tuning of the function for determining when to show spon-sored results in the top slot. However, the results remained consistent through several retunings, thus we are confident that the results do reflect the accuracy of the models in pre-dicting relevance. 8
Since pure relevance did significantly better than max-examination at predicting CTR, it is not surprising that pure relevance also did better at predicting relevance. However, the results would have been very surprising if we had not presented the pure relevance model, and treated this solely as a comparison between the baseline and max-examination. From that perspective, max-examination does better at pre-dicting CTR by leveraging co-click information, but because it incorrectly assigns credit to examination instead of rele-vance, actually does worse at predicting relevance.
Past work on user browsing models assumed that changes in CTR when conditioned on clicks in prior positions are due to changes in probability of examination. We showed that for sponsored search results, this fundamental assumption is contradicted by the data. We presented a plausible alter-nate conjecture: that relevance of the result for that query instance is strongly correlated with clicks on other results, and is responsible for a substantial portion of the changes in conditioned CTR. We proved this conjecture by finding a subset of query instances where the examination proba-bility for certain positions is known to be close to 1, and showing that clicks on prior results still resulted in dramatic increases in CTR. Recall that r  X  ( i ) = P C  X  ( i ) / P E  X  ( i ) . So we multiply by E b to get Pr( C i = 1) and then divide by E m .
At the time we ran the experiments, we expected max-examination to be more accurate than baseline. Thus we were highly motivated to get max-examination to work. This paper came about from our efforts to understand why we did not succeed!
We came up with new user browsing models that model changes in CTR (when conditioned on clicks in other posi-tions) as caused by changes in instance relevance, or both relevance and examination. We also came up with an en-hanced version of the UBM model, max-examination, that leverages information from both prior clicks as well as clicks below the current position, and predicts CTR better than the UBM model. Our new model, pure relevance, that at-tribute changes solely to instance relevance does significantly better at predicting CTR than the models that attribute CTR change solely to examination. In fact, the pure rel-evance model does only slightly worse than the more gen-eral model that attributes CTR change to both instance relevance and examination. This implies that changes in instance relevance account for a substantial portion of the change in CTR when conditioned on prior clicks.

Finally, we showed that evaluating user browsing mod-els solely using offline analysis of CTR prediction can be problematic. A user browsing model may leverage infor-mation about clicks on other results (or other information about other results) to get superb accuracy when predicting CTR offline, but such an analysis cannot reveal whether the model is also correctly attributing credit between relevance and examination. If the model incorrectly attributes credit, it could end up with estimates of relevance and examina-tion that are not very accurate in isolation, but whose prod-uct (CTR) is indeed accurate. We demonstrate that this is not purely theoretical, but indeed an important practical issue, by comparing the baseline/pure relevance and max-examination models in live experiments. Although the max-examination model does much better at CTR prediction in offline analysis, it does worse than the baseline/pure rele-vance models in predicting relevance, and therefore worse in live experiments. This reinforces our earlier conclusion that relevance is a key driver of changes in CTR when conditioned on other clicks, and also shows that directly evaluating rel-evance (through live experiments or human ratings) is an indispensable part of the evaluation of any user browsing model.
 Our findings open up several directions for future work.
It would be interesting to see whether CTR changes for organic search results (when conditioned on prior clicks) are also substantially due to changes in instance relevance.
Quantitatively assigning credit between instance relevance and examination appears quite difficult. One approach might be to look at the values of the features corresponding to in-stance relevance and examination in the JRE model. How-ever, these features are strongly correlated. Hence we found that with either iterative fitting or logistic regression, the values of the features are sensitive to the details of the algo-rithm and regularization, even though the final predictions are quite insensitive. Quantitative assignment of credit re-mains a challenging open problem.

A natural next step would be to generalize the pure rele-vance and JRE models to incorporate information about the relevance of prior results (Section 2.3), or the satisfaction of the user with the prior clicked results (Section 2.4). In par-ticular, the session utility model [6] is intuitively appealing, but does not model examination or perceived relevance. A model that includes the key insights of both JRE and the session utility model would be very elegant. [1] Enquiro research, search engine results 2010. [2] E. Agichtein, E. Brill, and S. T. Dumais. Improving [3] A. Aula and K. Rodden. Eye-tracking studies: more [4] O. Chapelle and Y. Zhang. A dynamic bayesian [5] N. Craswell, B. Ramsey, M. Taylor, and O. Zoeter. An [6] G. Dupret and C. Liao. A model to estimate intrinsic [7] G. Dupret and B. Piwowarski. A user browsing model [8] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, [9] F. Guo, C. Liu, and Y. Wang. Efficient multiple-click [10] T. Joachims. Optimizing search engines using [11] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [12] C. Liu, F. Guo, and C. Faloutsos. Bbm: Bayesian [13] R. Pike, S. Doward, R. Griesemer, and S. Quinlan. [14] F. Radlinski and T. Joachims. Query chains: Learning [15] M. Richardson, E. Dominowska, and R. Ragno.
 [16] D. Sculley, R. Malkin, S. Basu, and R. J. Bayardo. [17] D. Tang, A. Agarwal, D. O. Brien, and M. Meyer. [18] Z. Zhu, W. Chen, T. Minka, C. Zhu, and Z. Chen. A
We give a concrete example of how UBM (Equation 4) can be more accurate than the baseline model (Equation 2) in predicting CTR, but worse in predicting relevance.
Assume that all changes in CTR when conditioned on other clicks are due to changes in instance relevance, not due to changes in examination. Let there be exactly 2 result positions, with examination probabilities
Let the UBM parameters for the second position be: 9
We now consider a specific pair of results s 1 =  X  (1) and s =  X  (2), with historical CTR of 0.2 and 0.1 respectively. Let there be 100 impressions. Since UBM is identical to baseline for the first position, we focus on s 2 .
 Relevance: Recall that in this scenario all CTR changes are due to changes in instance relevance. Thus the base-line model will correctly estimate Pr( E 2 = 1) = 0 . 5, and correctly estimate relevance of s 2 as Pr( C 2 = 1 | E 2 = 1) = CTR: The baseline model does not make use of prior clicks, and will estimate Pr( C 2 = 1) = 0 . 1. The absolute error (summed over 100 impressions) is: Relevance: Since Pr( C 1 = 1) = 0 . 2 for s 1 , UBM estimates Pr( E 2 = 1) as: and therefore estimates relevance as Pr( C 2 = 1 | E 2 = 1) = 0 . 4 = 0 . 25. Notice that UBM ends up with an inaccurate estimate of relevance.
 CTR: UBM model will estimate: Notice that the overall estimate for Pr( C 2 = 1) = Pr( C 1)  X  0 . 25 + Pr( C 1 = 0)  X  0 . 0625 = 0 . 1 is correct. The estimates of Pr( C 2 = 1 | C 1 = 1) and Pr( C 2 = 1 | C 1
An example setting that yields these parameters is when the average CTR in position 1 is 1/3, and Pr( C 2 =1 | C 1 for any pair of results. Solving Pr( E 2 = 1) = Pr( C also correct. The error in examination probability is exactly canceled out by the error in the relevance estimate. The absolute error (over 100 impressions) is: Error = 5  X | 1  X  Pr( C 2 = 1 | C 1 = 1) | The absolute error is less, though the relevance estimate is worse. It is easy to extend this example such that the posi-tions of s 1 and s 2 exchange in live serving (by appropriately choosing bids for s 1 and s 2 , resulting in lower revenue and CTR in live experiments.

We discuss the repeatability of our analysis and experi-ments, assuming access to logs from a search engine. The key point is that while we may not have provided sufficient details for someone to exactly replicate what we did, our re-sults do not depend on those omitted details, e.g, someone could use a different machine learning algorithm for predict-ing relevance, and we would expect them to get the same results.

In Section 3, recall that the ratio L i (1) /L i (0) only de-pended on any errors in relevance estimation being roughly evenly distributed across the two sets. Thus we expect that one could use any reasonable machine learning algorithm for predicting relevance (e.g., a production system), and still get the same result wrt whether L i (1) /L i (0) is different from 1 with statistical significance.

Similarly, in Section 5, we treat the output of the baseline model as given, and only fit the co-click dependent param-eters. So the results (ordering of the models wrt accuracy) should again be independent of which machine learning algo-rithm is used for the baseline. Thus one can take the output of any production system for predicting relevance as given, and repeat our experiment.

In Section 6, it should be straightforward to compute the adjustments to relevance for max-examination if the produc-tion system uses a browsing model similar to the baseline model. If the production system uses a different browsing model, then one can use our methodology to first estimate the corrections to get relevance estimates for baseline, and then estimate a second correction to get the relevance esti-mates for max-examination. However, it may or may not be easy to run a live experiment with these corrections, based on the available infrastructure. If it is difficult to run live experiments, measuring relevance using human raters would be as effective, and would in fact nicely complement our ex-periments.
