 Streaming user-generated content in the form of blogs, microblogs, forums, and multimedia sharing sites, provides a rich source of data from which invaluable information and insights maybe gleaned. Given the vast volume of such social media data being continually generated, one of the challenges is to automatically tease apart the emerging topics of discussion from the constant background chat-ter. Such emerging topics can be identi fi ed by the appearance of multiple posts on a unique subject matter, which is distinct from previous online discourse. We address the problem of identify-ing emerging topics through the use of dictionary learning. We propose a two stage approach respectively based on detection and clustering of novel user-generated content. We derive a scalable approach by using the alternating directions method to solve the resulting optimization problems. Empirical results show that our proposed approach is more effective than several baselines in de-tecting emerging topics in traditional news story and newsgroup data. We also demonstrate the practical application to social media analysis, based on a study on streaming data from Twitter. H.0 [ Information Systems ]: General Algorithms, Design, Performance The wide-spread popularity of social media, such as blogs and Twitter, has made it the focal point of online discussion and break-ing news. Given the speed at which such user-generated content is produced, news fl ashes often occur on social media before they appear in traditional media outlets. Twitter, in particular, has been at the forefront of updates on disasters, such as earthquakes, on the 2009 post-election protests in Iran, and even on news of celebrity deaths [29]. Identifying such trending topics is of great interest be-yond just reporting news, with applications to marketing, disease control, national security and many more. The business case for marketing and PR is particularly compelling, given that 19% of all tweets [16] and 32% of blog posts [26] talk about products or brands. Businesses need to be aware of what consumers, in gen-eral, are saying about their products; especially since any emerging negative information or opinions are best dealt with promptly.
Motivated by this need, we focus on the task of automatically detecting emerging topics, hot topics, or buzz from streams of doc-uments/posts. For a subject to be considered an emerging topic, it must have support , in that it must appear from multiple sources; anditmustbe novel , in that it should be different from topics that have been, or are already, popular and well-known. Several cur-rent techniques that can be applied to address this problem, do not adequately meet both these criteria.

One possible approach to identifying emerging topics in docu-ment streams is to take all new posts and identify sets of posts that are similar. Extensive research in the area of document cluster-ing and topic modeling with Latent Dirichlet Allocation [5], Prob-abilistic Latent Semantic Analysis [15], and Non-negative Matrix Factorizations [20] can be brought to bear here. Although cluster-ing and topic modeling techniques (including, the dynamic ones) can fi nd sets of posts expressing cohesive patterns of discussion, they are not guaranteed to identify clusters that are also novel or informative compared to previously appearing topics.

An alternative approach is to use techniques that have been de-veloped for First Story Detection (FSD) [2], in the context of Topic Detection and Tracking (TDT) for traditional news streams. While seemingly related, FSD is only focused on detecting when a doc-ument discusses a previously unseen event. While fi rst story de-tection by itself is very valuable for broadcast news, given the low signal-to-noise ratio in social media, it is less effective. In the space of social media, many posts that may be considered as  X  fi rst stories X  solely because they are very different from previous posts, may ac-tually be of little value, e.g., the fact that intlevg on Twitter is eating  X  X oney roasted peanut s for lunch X , may be of little interest to the population at large.

For successful emerging topic detection, we need to identify sev-eral recent posts that are both similar to each other, and are dissim-ilar to previous posts. In this paper, we propose an approach based on sparse coding, in which data vectors are modeled as sparse lin-ear combinations of basis elements. A stream of documents (where each document is modeled a vector x  X  R m ) can be used to learn a dictionary ( A  X  R m  X  k )of k atoms , such that the documents can be approximately represented by a linear combination of a few atoms. If a new document cannot be represented with low error as a sparse linear combination of these atoms, it is a good indicator of novelty of the document. Novel document, thus identi fi ed, are used to learn a new dictionary of novel topics. This new dictionary is then used to cluster similar novel posts together, which we identify as the emerging topic clusters.

We validated our approach on several datasets from broadcast news, news groups, and Twitter. Empirical results show that our approach is more effective at detecting emerging topics in terms of precision and recall, than an approach based on fi rst story detec-tion followed by clustering of the fi rst stories detected, and other variations. This paper makes the following main contributions: (1) We formulate the task of detecting novel signals in streaming datasets as a sparse signal representation problem. A signal is represented with a sparse code over an existing dictionary along with a sparse error term. A novel signal is detected based on the lack of sparsity in such a representation. While our main application is emerging topic detection on streaming text, our methodology applies more broadly to other domains. (2) Our objective function is a combination of the 1 -norms of a sparse error (robust reconstruction) and a sparse code, i.e., e  X  x 1 (for the signal y = A x + e ), which appears well suited for sparse high-dimensional datasets such as those that arise in text applications. Additionally, we have non-negativity constraints on the sparse code ( x ) and dictionary ( A ), to maintain inter-pretability. Such a natural formulation combining sparsity, ro-bustness, and non-negativity appears novel and highly appropri-ateforthetask. (3) Most algorithms for dictionary learning are iterative batch proce-dures, that operate on the entire training set, which is not feasible for very large and dynamic data. We present an online algorithm which computes the sparse coding for each data point only once, instead of an iterative batch update scheme. As a result, our ap-proach is very scalable and well-suited for streaming data, such as from social media. (4) We use a new practical alternating direction method (ADM) to solve various optimization problems appearing in our formula-tion, that may be of independent interest. ADM has recently gathered signi fi cant attention in the Machine Learning commu-nity due to its wide applicability to a range of learning problems with complex objective functions [6]. (5) We introduce a new dictionary learning based text clustering algorithm which, in our setting, outperforms the widely used
Spherical K -Means [12] algorithm for clustering novel docu-ments. The new algorithm is more generally applicable, and inherits the scalability of dic tionary learni ng methods. Notation. We use [ n ] to denote the set { 1 ,...,n } . Vectors are always column vectors and are denoted by boldface letters. For a scalar r  X  R ,letsign ( r ) denote the sign of r and soft sign ( r )  X  max {| r | X  T, 0 } . The operator soft is extended to a matrix by applying it to every entry in the matrix. For arbitrary real matri-ces the standard inner product is de fi ned as A, B = Tr ( A B ) and the (squared) Frobenius matrix norm A 2 fro = A, A is the sum of all squared entries of the matrix. We use s.t. as an abbrevi-ation for subject to.
Sparse error recovery has found success in applications such as image and speech processing [7], computer vision and pattern recognition [23]. One of most successful (and relevant to this pa-per) applications is in robust face recognition [36, 35]. It is known that a well-aligned frontal face image under different lighting and expression lies close to a special low-dimensional linear subspace spanned by the training samples from the same subject [4]. This ob-servation has led to face recognition to be cast as a sparse represen-tation problem where the objective is to recover a high-dimensional sparse signal x  X  R k from a highly compressed linear measure-ment y = A x + e  X  R m (where e is an unknown error vec-tor) [35]. In recent work, Wright et al. [36] proposed to estimate w =[ x , e ] jointly as the sparsest solution to the extended equa-tion: min w w 1 s.t. y =[ A, I ] w , where I is the identity matrix. They show this formulation performs exceedingly well for robust face recognition. For example, they show that this 1 -minimization enables almost perfect recognition even if more than 60% pixels of the query image are severely corrupted. In a later paper, Wright and Ma [35] provided theoretical justi fi cation for this 1 -minimization formulation. We use a similar formulation to construct a sparse representation (see Section 4). One crucial difference between our approach and that advocated by Wright et al. is that our matrix does not contain all the documents, but is a compact representative dictionary of the documents seen before. This makes our approach more scalable.

Dictionary learning falls into a general category of techniques known as matrix factorization . In this paper, we additionally en-force non-negativity constraint on the factor matrices and such fac-torizations have been widely studied as non-negative matrix fac-torization (NMF) [20]. Most NMF formulations deal with squared loss [20, 22] . Several papers have considered 1 -regularized matrix factorizations with applications to topic models in text, e.g., [17, 39] but with an 2 -loss function. Similar to our loss function (6), Ke and Kanade [19] consider an 1 -loss function for non-negative matrix factorization, but do not impose sparsity in the factors. In their paper, Ke and Kanade [19] gave a simple alternative convex programming approach for solving non-negative matrix factoriza-tion with 1 -loss. Another framework with closely related goals is that of Robust PCA [8] where a decomposition of a matrix is sought in terms of a low-rank component with small nuclear norm and sparse errors. However, the repeated need to solve linear/quadratic programs or SVD in these approaches not scalable to large ma-trices. Our use of the alternating directions method allows us to scale-up to large matrices that are typically encountered in the text domain. For a review of ADMs and their suitability to a variety of large scale learning problems, we point the reader to [6]. To the best of our knowledge, our approach is the fi rst to propose a scalable algorithm for 1 -regularized dictionary learning with ro-bust 1 -reconstruction error with an application in temporal topic analysis of streaming documents.
In order to represent documents in our data stream, we will use the conventional vector space model with TF -IDF (Term Frequency-Inverse Document Frequency) term-weighting [24]. Additionally, each document has a timestamp that indicates when the document arrives. The timestamp could be at the granularity of our liking, e.g., it could be the day or the exact time the document arrives. As new documents come in and new terms are identi fi ed, our vocab-ulary set increases, but for simplicity, we are going to work with a global vocabulary space containing m terms that is independent of t . The extension to the case where the vocabulary set increases with t is quite simple, and just requires adjusting the size of the matrices by zero-padding.

Let { P t  X  R m  X  n t ,t =1 , 2 ,... } denote a sequence of stream-ing matrices, where P t represents the sparse term-document matrix whose unit 1 -normalized columns are the documents with times-tamp t and n t is the number of documents with timestamp t ogous to P t ,wealsode fi ne P  X  t which is constructed by using all Figure 1: Schematic of our framework: At each timestep, we solve a sparse coding/dictionary learning problem with 1 re-construction error and 1 regularization. The reconstruction error is used to identify novel documents which are again clus-tered using our framework to report emerging topics. documents whose timestamp is  X  t .Let n  X  t be the number of doc-goal of emerging topic detection is to identify sets of documents in P t that are similar to each other and are dissimilar to documents in
We split the task of emerging topic detection into two sub-tasks: (1) Identifying the novel documents ( Nvl t ) from P t (novel docu-ments are those that belong to any of the emerging topics at time and (2) Clustering the novel documents. We use dictionary learning to solve both these sub-tasks (see Figure 1).
Let A t  X  1  X  R m  X  k represent the dictionary matrix after time t  X  1 ; where dictionary A t  X  1 is a compact summary representation of all the documents in P  X  t  X  1 . Each column of A t  X  1 basis vector or atom . The exact construction of the dictionary is described later, but ideally we want the dictionary to have a set of representative atoms for each of the old topics. With such a repre-sentative dictionary, documents from old topics can be represented as a linear combination of the atoms corresponding to that topic.
Given a new document vector y with timestamp t ,weseeif y could be represented as a sparse linear combination of the columns of
A t  X  1 . The sparsest representation is the solution of where  X  0 is the 0 -norm, counting the non-zero entries of a vec-tor. However, in the general case, solving (1) is NP-hard and also hard to approximate [3]. Recently, a series of papers (see [9, 13] and references therein) have shown that under some favorable con-ditions one could obtain the solution to (1) by solving the following In essence, (2) can be viewed as a convex relaxation of (1). We will work with the 1 -norm based convex relaxation in the sequel.
In most practical situations, (2) is not applicable because it may not be possible to represent y as A t  X  1 x , e.g., if y has new words which are absent (i.e., have no support) in A t  X  1 . In such cases, one could represent y = A t  X  1 x + e where e is an unknown noise vector. Normally, if one considers a speci fi c topic over time, typ-ically a small set of new words get introduced in the discussion, e.g., if the topic is a sporting event like the Olympics, with time the discussion shifts to different athletes or events. The error vector e captures these terms. Since there are few such new words, the vec-tor e is sparse. However, these terms do get used with much higher frequency within few documents than one might expect from the overall statistics of the corpus. So even though e is sparse it has some large, impulsive values. A natural relaxation to (2) for han-dling noise is min x x 1 s.t. y  X  A t  X  1 x 2  X   X , x  X  0 .The above problem (generally without the non-negativity constraint) is commonly known as the constraint basis pursuit denoising prob-lem [10] and its variant is referred to as the unconstrained basis pursuit denoising problem, also known as the Lasso [31]. Here,  X ,  X  &gt; 0 are parameters. The formulation (3) naturally takes into account both the reconstruction error (with the y  X  A t  X  1 x 2 2 term) and the complexity of the sparse decomposition (with the x 1 term).

In the presence of isotopic Gaussian noise the 2 -penalty on e = y  X  A t  X  1 x gives the best approximation of x [36, 37, 38]. However, for text documents (and in most other real scenarios), the noise vector e rarely satis fi es the Gaussian assumption, and some of its coef fi cients contain large, impulsive values. In such scenarios, the 2 -penalty of (3) may give an extremely bad ap-proximation of x [37, 27]. However, in such real-world scenarios recent results [19, 36, 38, 35] have shown that imposing an reconstruction error gives a more robust and better approximation of x . We refer readers to these papers to get a more detailed exposi-tion of the advantages of 1 over 2 -reconstruction error. The basic intuition is that the ability of 1 -penalty to recover the true solution x is independent of the magnitude of the e , and depends only on the signs of e and the relative geometry of the column space of and the unit 1 ball [36].

We use the following 1 -formulation to recover x It is well-known that unlike (3) where squared 2 -norm is used on the error, the 1 -reconstruction error makes (4) an exact penalty method in the sense that it reduces to (2) when  X &gt; 0 is less than some threshold [38]. Additionally, Yang et al. [38] claim that even without impulsive noise the 1 -reconstruction error does not harm the solution quality as long as the data does not contain a large amount of Gaussian noise.
 Given a new document y with timestamp of t and a dictionary A t  X  1 , we solve (4) to determine whether y is novel (with respect to dictionary A t  X  1 ) or not. If the objective value of (4) is  X  X mall, X  then y is well-reconstructed by a linear combination of some ba-sisvectorsin A t  X  1 . We mark such documents as non-novel and discard them. Now, if the objective value is  X  X arge, X  then y has no good reconstruction among the basis vectors of the previous topics, thus suggesting novelty of y . We add such documents to the set Nvl t . Note that all documents are normalized to unit 1 length, and hence the objective values are in the same scale. The performance of this algorithm depends on the  X  X uality X  of the dictionary. We now describe our dictionary learning formulation.
For simplicity of explanation, we assume that the dictionary is of fi xed size R m  X  k (independent of t ). Classic dictionary learn-ing techniques [28, 1, 21] consider a fi nite training set of signals S =[ s 1 ,..., s n ]  X  R m  X  n and optimize the empirical cost func-tion n i =1 l ( s i ,A ) , where A  X  R m  X  k is the dictionary and is a loss function such that l ( s ,A ) should be small if representing the signal s in a sparse fashion. The value k to as the size of the dictionary. A typical choice of the loss function is the Lasso-style objective However, as we mentioned above in the absence of Gaussian noise using 1 -loss on the error improves the estimation, therefore, we use the following loss function l ( s ,A )=min x s  X  A x 1  X  x 1 . Additionally, we want dictionary A and x  X  X  to contain non-negative entries. Therefore, the problem of dictionary learn-ing becomes min
In our setting at time t , we want to update the dictionary so that it forms a compact summary representation of all the documents in P nary for time t by minimizing min Equivalently this could be written as minimizing the following func-tion over ( X,A ) : Sincewearefactorizing P  X  t into two non-negative matrices, Equation (6) (without the  X  X 1 term) is referred to as 1 non-negative factorization problem [20]. The optimization prob-lem (6) is in general non-convex. But if one of the variables, either A or X is known, the objective function with respect to the other variable becomes a convex function (in fact, a linear function) and the global solution to (6) can be found. This iterative alternative minimization is the core idea behind most algorithms for dictionary learning [28, 1, 21]. However, t hese algorithms access the whole of the dataset in each iteration, and therefore, these algorithms cannot ef fi ciently deal with large datasets. To overcome this problem, we use an online version of dictionary learning, where we update only A and use X obtained from previous stages of the algorithm (see Section 5).
We now describe our procedure for going from novel documents to emerging topics. The idea is to again use dictionary learning. Given as input a set of (novel) documents and the number of topics ( k 1 ) to be generated, we use a suitable modi fi cation of (6) to do detect emerging topics. The idea is as follows: If Nvl t represents the set of novel documents, we learn a dictionary with k 1 where each atom corresponds to an emerging topic. In other words, we minimize the following function over ( R, S ) : ation and S  X  R k 1  X | Nvl t | is the matrix of topic-document associ-ation. The distribution of terms in each column of R is the latent representation of the k 1 emerging topics in the data. In order to present these topics in a meaningful way to users, we represent each topic by the most relevant documents to the topic. We do this by assigning each document to the atom in which it has the most dominant representation as given by the matrix S .Thisgivesa clustering of novel documents into emerging topics.
We present in this section our algorithm for detecting emerg-ing topics. Our main procedure is summarized in Algorithm N C
LUST . The algorithm alternates between a  X  X etection stage X  and a  X  X ictionary learning stage. X  The detection stage at time input the dictionary A t  X  1 and P t , and for each document p computes the best representation of p j in terms of A t  X  1 ing (4) (where y is replaced by p j ). It is quite easy to transform (4) into a linear program. However, in our experiments the linear pro-gramming approach turned out to be quite slow (even when using commercial solvers like CPLEX). To speedup our algorithm, we use the alternating directions method (also known in the literature as alternating directions method of multipliers) [11, 38]. We ex-plain this approach in Section 6. We classify a document p novel if the objective value of (4) is above some chosen threshold  X  .Let Nvl t  X  P t be the set of document that are marked as novel at time t . The set of novel documents is then passed as input to Al-gorithm D ICT C LUST which does clustering of these documents as explained in Section 4.3. Since, the size of Nvl t is typically small, we solve it using a simple iterative batch procedure, alternatively fi xing R,S and updating the other using the method of alternating directions.
 We perform the dictionary learning stage in an online fashion. Online dictionary learning was recently introduced by Mairal et al. [22] who showed that it provides a scalable approach for han-dling large dynamic datasets. They consider an 2 -loss function and show that their online algorithm converges to the minimum ob-jective value in the stochastic case. Our online dictionary learning framework has similar structure to that of Mairal et al. , although we focus on 1 -loss. In the online setting, instead of (6), we update the dictionary by minimizing the following function over A where X  X  t =[ x 1 ,..., x n  X  t ] are computed during the previous detection stages. Notice that min A  X  0 f X ( A ) is an upper bound on min A,X  X  0 f ( X, A ) (as we only optimize over A in f X ever, unlike f ( X,A ) (which is not convex), minimizing f convex program (in fact, it can be re-cast as a linear program). For ef fi ciency purposes, we use the method of alternating directions to compute f X ( A ) (see Section 6). The matrix A that minimizes the right hand side of (8) is the new dictionary A t .
 Remark. Our framework allows for several variations. By default, the optimization of A is initialized using A t  X  1 and therefore the number of topics tracked by the system is static. Alternatively, the set of emerging topics as discovered by the procedure described in Section 4.3 may also be explicitly injected as columns (in addition to
A t  X  1 ) in the initialization of A . In this case, the number of top-ics tracked steadily grows. A guided process may also be possible where a user scans the list of emerging topics and selectively in-troduces topics of interest in the dictionary. For simplicity, in this paper we use the default variation.
To speedup the algorithms N VL C LUST and D ICT C LUST ,weuse the method of alternating directions to solve the various optimiza-tion problems. The reader is referred to [38, 6] and references therein for details and historical development of the alternating di-rection method (ADM). We start with a brief review of the gen-eral framework of ADM from [38]. Let p ( x ): R a  X  R and q ( y ): R b  X  R be convex functions, F  X  R c  X  a , G  X  R c  X  b
Input: A t  X  1  X  R m  X  k , p j  X  R m ,  X ,  X ,  X ,  X  &gt; 0
A i =1 , 2 ,..., convergence do a)
 X  b) c) d) the end of the Section 1.

Detection stage:
Dictionary Learning stage:
Input: Matrix of documents Nvl t ,  X   X  R , k 1  X  R z  X  R c . Consider the following optimization problem where the variable vectors x and y are separate in the objective, and coupled only in the constraint. The augmented Lagrangian for the above problem is given by L ( x , y , X  )= p ( x )+ q ( y )+  X  ( z  X  F x  X  G y )+ where  X   X  R c is the Lagrangian multiplier and  X &gt; 0 is a penalty parameter.

ADM utilizes the separability form of (9) and replaces the joint minimization over x and y with two simpler problems. The ADM fi rst minimizes L over x , then over y , and then applies a proximal minimization step with respect to the Lagrange multiplier i th iteration of the ADM procedure, given ( x ( i ) , y ( i ) Here,  X &gt; 0 is a constant. The ADM procedure has been proved to converge to the global optimal solution under quite broad condi-tions [11]. We now apply ADM to the various optimization prob-lems encountered in Algorithms N VL C LUST and D ICT C LUST ADM procedure stops when the relative change in the objective function is less than a given constant. In practice few iterations are needed to reach good results. Due to lack of space all proofs are omitted and can be found in the full version of the paper. ADM for Detection Stage. Let R + be the set of positive real num-bers. In the detection stage of Algorithm N VL C LUST for each doc-ument p j , we solve the following program This can be written equivalently as Then the augmented Lagrangian form of (11) is
L ( x , e , X  )= min +  X  ( p j  X  A t  X  1 x  X  e )+(  X / 2) p j  X  A t  X  1 x  X  e 2 We now apply ADM to the above Lagrangian. Let us assume that follows. First, for a fi xed x ( i ) and  X  ( i ) , we update e by solving min
L EMMA 6.1. The minimum value of the above optimization is attained by setting e = soft ( p j  X  A t  X  1 x ( i ) +  X  ( i ) The above lemma gives the derivation for e ( i +1) .Now,fora fi xed e ( i +1) and  X  ( i ) a simple manipulation shows that we can obtain x that minimizes (12) by solving the following min
The subscript ( i ) denotes the i th iteration of the ADM procedure. However, instead of solving (13) exactly, we approximate it by min where  X &gt; 0 is a proximal parameter and The above approach belongs to the class of proximal gradient meth-ods in optimization [33, 38]. We use it in the context of a composite function with a smooth and a non-smooth part, where the gradient is computed only based on the smooth part [33].

L EMMA 6.2. The minimum value of (14) is attained by setting x =max { x ( i )  X   X  g ( i )  X  (  X  X  ) / X , 0 } .
 The above lemma gives the derivation for x ( i +1) .Nowgiven fi xed x rized in Figure 2. The following theorem shows the convergence of the ADM.

T HEOREM 6.3. Let  X , X  &gt; 0 satisfy  X  X  max ( A )+  X &lt; 2  X  max ( A ) is the maximum singular value of A . For any fi xed starting { ( e (1) , x (1) , X  (1) ) } converges to { (  X  the optimum solution to (11) .
 ADM for Dictionary Learning Stage. In the dictionary learning stage, we solve (8) which can be rewritten as Since X  X  t 1 is a constant term in the objective, we can ignore that term and look at the following optimization problem The augmented Lagrangian form of (15) is L ( A,  X  ,  X ) = min where  X   X  R m  X  n is a multiplier and  X &gt; 0 is a penalty parame-ter. The ADM equations for this stage are a generalization (matrix version) of the ADM equations derived for the detection stage. Fig-ure 2 summarizes these equations. We use A t  X  1 as warm restart for computing A t . Similar to Theorem 6.3 one can also show the con-vergence of these ADM equations.
 ADM for Algorithm D ICT C LUST . We now derive the ADM equa-tions for solving (7). We solve (7) by doing an alternative mini-mization over R and S .
 We use ADM to solve both (17) and (18). The derivation of the ADM equations are quite similar to that done for detection and dictionary learning stages, so we summarize the results in Figure 3. Remark about Parallelization. The optimization problems en-countered in detection stage, dictionary learning stage, and Algo-rithm D ICT C LUST can all be trivially parallelized. For detection stage, we could invoke ADM for each p j  X  P t in parallel. For, the dictionary learning stage, we can rewrite the P  X  t  X  AX  X  t 1 rows in P  X  t and A , respectively. Since the terms in the summa-tion are independent of each other, we can solve them in parallel using a modi fi cation of the ADM equations used for the detection stage. The right hand side of Figure 2 is a matrix version of this parallel optimization over rows of A . A similar idea also works for alternative minimization used in Algorithm D ICT C LUST .
In this section, we fi rst empirically evaluate our approach on publicly-available labeled datasets from news streams and news-groups. We then present a study of our approach applied to Twitter data from a PR campaign.
For the purpose of evaluation, we assume that documents in the corpus have been identi fi ed with a set of topics. For simplicity, we assume that each document is tagged with a single, most dominant topic that it associates with which we refer to as the true topic for that document.

We use variations of standard IR measures like pairwise preci-sion, recall, and F1 score [24]. Given P t , the set of documents ar-riving at time t ,let TNvl t  X  P t be the set of true novel documents in
P t (i.e., TNvl t contains documents belonging to true emerging topic clusters). Let C t be the set of system generated emerging topic clusters at time t ,andlet T t be the true emerging topic clus-ters at time t . Note that clusters in T t are formed over documents in TNvl t , whereas the clusters in C t are formed over documents in Nvl t  X  P t ,and TNvl t may not be equal to Nvl t .

We de fi ne our evaluation metrics over the novel documents. Pair-wise precision ( Prec nvl ) is the number of pairs of documents that are in the same cluster in both T t and C t divided by the number of pairs of documents that are in the same cluster in C t recall ( Rec nvl ) is the number of pairs of documents that are in the same cluster in both T t and C t divided by the number of pairs of documents that are in the same cluster in T t .PairwiseF1( is the harmonic mean of Prec nvl and Rec nvl . The following ex-ample illustrates these de fi nitions. Let P t = { p 1 ,p TNvl t = { p 3 ,p 4 ,p 5 } ,and Nvl t = { p 3 ,p 4 ,p 5 ,p { ( p 3 ,p 4 ,p 5 ) } (i.e., T t contains one cluster made up of and C t = { ( p 3 ,p 4 ,p 6 ) , ( p 5 ,p 7 ) } . Then,
We compare the performance of our algorithm against three al-ternative approaches we created, which are based on combining nearest neighbor (NN) and K -Means algorithms with dictionary learning. We describe these baselines below: NN-K M : To detect novel documents, we use the nearest neigh-bor approach used by the UMass FSD system [2], which is one of the best performing system for this task [29]. As in the UMass system, we use cosine distance as a similarity measure and a IDF weighted document representation. Every document in P whose cosine distance to its nearest neighbor in P  X  t  X  1 some  X  is marked as novel. We build on this algorithm to get a baseline for emerging topic detection, by running a K -Means clus-tering with cosine distance (a.k.a. Spherical K -Means) on the doc-2. for i =1 , 2 ,..., convergence do Table 1: Results for TDT2 and 20 Newsgroups corpora. We set for each phase by varying  X , X  in the interval [0 , 1] .InPhase uments marked novel. We use Spherical K -Means, as it is a well-established approach to clustering high-dimensional text data [12]. D
ICT -K M : The second baseline is a modi fi cation of our dictionary based scheme. We use a dictionary learning approach to detect novel documents (this can be done by invoking Algorithm N C
LUST without Step 3) and then run a Spherical K -Means cluster-ing on these novel documents to create emerging topic clusters. NN-D ICT : The third baseline is also a modi fi cation of our dic-tionary based scheme. We fi rst use the nearest neighbor approach (explained above) to detect novel documents and then run Algo-rithm D ICT C LUST on these novel documents to create emerging topic clusters.

We implemented all the algorithms in Matlab. In all our experi-ments k = 100 and  X  =1 / 100 . We noticed that varying these pa-rameters does affect the evaluation metrics (e.g., increasing the dic-tionary sizes leads to better scores, but comes at a cost of increased running times). However, due to space constraints, we postpone a discussion of these results to the full version of this paper. The pa-rameters of ADM are fi xed as  X  =10 ,  X  =1 / 50 ,and  X  =1 . 618 (these are chosen in consultation with [38] and Theorem 6.3 for faster convergence).
We use two standard labeled datasets to evaluate the performance of our proposed algorithm. We start by describing these datasets and our experimental setup.

Our fi rst dataset is the NIST topic detection and tracking (TDT2) corpus 2 which consists of news stories in the fi rst half of 1998. For our evaluation, we use a set of 9,394 documents represented over 19,528 terms and spread over 27 weeks. These documents are par-titioned into 30 human-labeled topics. We introduce the documents from the 27 weeks in 5 different phases. In the zeroth phase, we introduce all the documents between weeks 1 to 5 and these doc-uments are used for initializing the dictionary A 0 .Wedosoby running Algorithm N VL C LUST (without the Step 3) in an of fl ine fashion (as in we alternate between the detection stage and dic-tionary learning stage for each document). In the fi rst phase, we introduce all the documents between weeks 6 to 7 and run Algo-rithm N VL C LUST on these documents with dictionary A 0 .Inthe second phase, we introduce all the documents between weeks 13 and run Algorithm N VL C LUST on these documents with dictio-nary A 1 (outputted by the fi rst phase). We repeat the same steps for the third phase (between weeks 14 to 17 ) and fourth phase (be-tween weeks 18 to 27 ). The reason for choosing such asymmetric time intervals is to make sure that at least one new topic cluster gets introduced in each phase.

As our second dataset we use the 20 Newsgroups corpus 3 .The corpus contains 18,774 articles distributed among 20 clusters where each cluster is a Usenet group. For our experiments, we use a vo-http://www.itl.nist.gov /iad/mig/te sts/tdt/1998/ http://people.csail.mit.edu/jrennie/20Newsgroups/ cabulary of 10,000 terms selected based on frequency. We do a set of controlled experiments on this corpus. Again, we introduce the documents in phases. Documents within each cluster are tempo-rally ordered, and we use this temporal ordering to introduce the documents. At the end of Phase i  X  1 , we have documents from some (old) clusters, and in Phase i we introduce a mixture of doc-uments, some coming from these old clusters and some belonging to new clusters; and see how well our algorithm performs in detect-ing these new clusters. We begin Phase 0 with documents sampled from 6 randomly chosen clusters. In each subsequent phase, we introduce documents from 2 new clusters. The numbers of docu-ments from added at each phase are presented in Table 1.
For our baselines with K -Means clustering, we run the algorithm 8 times (with random initialization for centroids) and take the best result. For each phase, we set both k 1 (number of topics to be generated by Algorithm D ICT C LUST )and K (number of clusters to be generated by K -Means) to be equal to 10 . We vary the threshold 0  X   X   X  1 to fi nd the threshold where F1 nvl is maximized for our algorithm (similarly, for the D ICT -K M algorithm with threshold and for the NN-K M and NN-D ICT algorithms with threshold
Table 1 presents the maximum F1 nvl for both datasets (obtained by varying  X , X  ). Our algorithm always outperforms all the three baselines. For TDT2, our algorithm gives on average 16 . 9% provement in F1 nvl score over the NN-K M , 6 . 7% improvement over D ICT -K M ,and 4 . 3% improvement over NN-D ICT . For 20 Newsgroups, we notice on average 16 . 0% improvement over NN-K
M , 7 . 0% improvement over D ICT -K M ,and 9 . 0% improvement over NN-D ICT . While the results shows that we get a big im-provement over NN-K M , the improvements over D ICT -K M and NN-D ICT are only moderate. Since, both D ICT -K M and NN-D
ICT use in parts our dictionary learning algorithm, the results suggest that using dictionary learning could lead to an improve-ment in the F1 nvl score over just using nearest neighbor/clustering techniques. Furthermore, our algorithm consistently outperforms D
ICT -K M , and NN-D ICT consistently outperforms NN-K M ,in-dicating that a dictionary-based clustering scheme results in better clustering than Spherical K -Means.

Note that, the F1 nvl scores are lower than what one would en-counter in  X  X ypical clustering X  applications. Intuitively, this hap-pens because we evaluate these metrics only over the novel doc-uments, which are far fewer in number than the documents from the old clusters; and each missed document (from TNvl t \ Nvl affects Rec nvl combinatorially because of the pairwise mistakes that they lead to (similarly, each wrongly added document from Nvl t \ TNvl t severely affects Prec nvl ).

In Figure 4, we present interpolated (pairwise) precision-recall curves [24] for the TDT2 dataset, obtained by varying the threshold  X , X  . We see that the precision-recall curve of our algorithm mostly dominates the curves of the baseline algorithms, which illustrates the superiority of our algorithm over different operating points. The curves for the 20 Newsgroups corpus are similar and are omitted for lack of space.
The performance of our approach relies on fi rst, accurately iden-tifying novel documents, and then appropriately clustering these document. Results in the previous section show that we do well in the overall task of emerging topic detection, but in order to get a better understanding of the strengths of our approach, we inves-tigate performance on the sub-task of novel document detection. Given a set of documents, this task is to classify each document as either novel (positive) or non-novel (negative). True labeling is created by labeling the documents from any of the emerging topic clusters as positive, and the documents from any of the existing true topics as negative. For evaluating this classi fi cation task, we use Area Under Curve (AUC) of the ROC curve [24]. As a baseline, we use the NN approach explained in Section 7.2. As mentioned earlier, this NN approach is one of the best performing systems for FSD, a task that is very similar to novel document detection.
The results, presented in Table 2, show that for this sub-task our dictionary-based approach and the NN approach have almost the same performance. Even though the AUCs are near identical, the sets of documents that the two approaches mark as novel are quite different. This is evidenced by the superior performance of our approach over NN-D ICT , which uses the same approach for the second sub-task of clustering novel documents into emerging topics (see Table 1). In summary, while our dictionary-based approach is picking novel documents with the same accuracy as NN, it is more likely to pick documents from the same emerging topic.

Table 2: Performance, in AUC, on novel document detection.
In Section 7.3, we evaluated our approach on labeled benchmark data sets. Here, we analyze how well our approach performs in practice, when applied to Twitter data. In particular, we look at the Twitter discussion around IB M X  X  Watson DeepQA system. February 2011, the Watson DeepQA system participated on the http://www.ibm.com/watson Jeopardy! game-show, competing against humans; and the event was televised over 3 days: Feb 14, 15, and 16. We used Twit-ter X  X  search API to collect all tweets relevant to the participation of Watson in Jeopardy! from Feb 1  X  16; where tweets were judged relevant by keyword and pattern fi lters. We used a sample set of 8,434 tweets from Feb 1  X  14, to initialize our dictionary. Then, we tested our system on detecting emerging topics on a sample of 5,199 tweets from Feb 16. The entire corpus of tweets had a vo-cabulary of size 1,139, after removing common stop words.
We give as input to Algorithm N VL C LUST the learned dictionary and the tweets of Feb 16. Instead of presenting the entire range of parameter values, here we pick k 1 =15 and  X  =0 . 5 to illus-trate the clusters identi fi ed at a particular setting. From the topic clusters outputted by our algorithm, in Table 3, we present three selected ones. These clusters were manually inspected to identify the dominant topic in them. Note that, these three topics are emerg-ing, as the date of their occurrence is Feb 15 or later, whereas we learn our initial dictionary only on tweets till Feb 14.
The fourth and fi fth columns in Table 3 show the size of the novel clusters detected, and the number of tweets that were judged, by a human annotator, to be relevant to the identi fi ed dominant topic in these clusters. The sixth column shows the top keywords for each topic. These are selected by looking at the atoms corresponding to these topics in the dictionary matrix produced by Algorithm D C
LUST and by picking the top 3 words on which these atoms have the largest mass. The fi rst selected cluster contains tweets on Wat-son X  X  Final Jeopardy mistake on Feb 15th, which subsequently re-ceived much media coverage. The second and third selected clus-ters contain tweets related to two in fl uential news articles on Feb 16. Note that, large fractions of tweets in these clusters are rele-vant to the dominant topic, indicating high coherence of the iden-ti fi ed clusters. Table 4 presents three sample tweets from each of the three topics; which illustrates that our approach can cluster to-gether tweets that are clearly on the same topic, even if syntactically quite different. Being able to automatically comb through tens of thousands, and potentially millions, of documents, to identify such emerging topics of discussion amongst consumers is of signi fi cant value to PR and marketing efforts.
Several existing approaches to identify hot topics or trends in so-cial media are based on the frequency of mentions of terms, such as Twitscoop.com, Trendistic.com, Twopular.com, and trends on Twitter.com. While high frequency of terms may be a good indica-tor of popularity, it does not necessarily identify new or emerg-ing trends. Instead, comparing the relative frequency of occur-rence of terms and phrases in the current time period to the oc-currences in the past, is likely to identify more topical phrases. Tomokiyo and Hurst [32] propose such an approach for extracting key-phrases based on statistical language models, which they ap-ply to 20 Newsgroups data. Similarly, Cataldi et al. [25] present an approach in which they identify emergent keywords, which have been extensively used in a given time period, but not in previous ones. They fi nd words that frequently co-occur with each emer-gent word, and report these together as emerging topics. Glass et al. [14] go a step further, from emerging words to tracking emerg-ing memes  X   X  X istinctive phrases which propagate relatively un-changed. X  Their work focuses on building models to predict which memes will spread widely. Our work differs from the above ap-proaches by going beyond unigrams and n-grams, to identifying novel clusters of similar documents, which provides a richer char-acterization of topics. Furthermore, we distinguish emerging, novel topics from merely popular or well-known ones.
User-generated content is increasingly playing a pivotal role as the source for breaking news and developments, as well as shaping opinions on a variety of matters ranging from products to policies. In this paper, we presented a dictionary learning based framework for detecting emerging topics in social media and related streams. The dictionary learning formulation naturally combines ideas from robustness, sparsity, and non-negative matrix factorization for anal-ysis of streaming text. The overall framework was divided into two stages X  fi rst, determining novel documents in the stream, and sub-sequently identifying cluster structure among the novel documents. The objective functions in each stage were optimized using the al-ternating directions method, which can be easily parallelized for further scalability. Empirical evaluation on a variety of datasets illustrate the effectiveness of the proposed framework.
One can envision several directions for future work based on the proposed framework. While the current work uses fi xed sized dic-tionaries, using adaptive dictionaries whose size changes based on the set of active or emerging topics may be more desirable in certain applications. While learning the dictionary A t , the current work uses the entire historical data P  X  t . A fully online version which only maintains suf fi cient statistics of historical data may be more scalable for real world streams. Further, from an optimization per-spective, one may be able to use accelerated gradient descent and related proximal methods [33] to further speed up the alternating directions method. Finally, while the focus of the current work was on detecting emerging topics in text streams, similar ideas can be developed for other domains (such as healthcare, climate sciences) where detection of novel signal streams is of interest. We would like to thank Richard Lawrence for many fruitful dis-cussions. We would also like to thank Estepan Meliksetian and Phaneendra Divakaruni for collecting the Jeopardy Twitter dataset. This research is continuing through participation in the Anomaly Detection at Multiple Scales (ADAMS) program sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) un-der Agreement Number W911NF-11-C-0200. AB was supported in part by NSF CAREER award IIS-0953274, and NSF grants IIS-1029711, IIS-0916750, and IIS-0812183. [1] M. Aharon, M. Elad, and A. Bruckstein. The K-SVD: An [2] J. Allan. Topic Detection and Tracking: Event-based [3] E. Amaldi and V. Kann. On the Approximability of [4] R. Basri and D. Jacobs. Lambertian Re fl ectance and Linear [5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet Allocation. [6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. [7] A. Bruckstein, D. Donoho, and M. Elad. From Sparse [8] E. Candes, X. Li, Y. Ma, and J. Wright. Robust Principal [9] E. Candes and P. Randall. Highly Robust Error Correction by [10] S. Chen, D. Donoho, and M. Saunders. Atomic [11] P. Combettes and J. Pes quet. Proximal S plitting M ethods in [12] I. S. Dhillon and D. S. Modha . Concept Decompositions for [13] D. Donoho. For most Large Underdetermined Systems of [14] K. Glass and R. Colbaugh. Toward Emerging Topic [15] T. Hofmann. Probabilistic Latent Semantic Analysis. In UAI , [16] B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury. Twitter [17] R. Jennaton, J. Mairal, G. Obozinsky, , and F. Bach. [18] C. Johnston. Creators: Watson has no speed advantage as it [19] Q. Ke and T. Kanade. Robust L1 Norm Factorization in the [20] D. Lee and H. Seung. Learning the Parts of Objects by [21] H. Lee, A. Battle, R. Raina, and A. Ng. Ef fi cient Sparse [22] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online Learning [23] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. [24] C. Manning, P. Raghavan, and H. Sch X tze. Introduction to [25] C. Mario, D. C. Luigi, and S. Claudio. Emerging Topic [26] P. Melville, V. Sindhwani, and R. Lawrence. Social Media [27] K. Min, Z. Zhang, J. Wright, and Y. Ma. Decomposing [28] B. Olshausen and D. Field. Sparse Coding with an [29] S. Petrovi  X  c, M. Osborne, and V. Lavrenko. Streaming First [30] D. Sullivan. Could Google Play Jeopardy Like IBMs [31] R. Tibshirani. Regression Shrinkage and Selection via the [32] T. Tomokiyo and M. Hurst. A Language Model approach to [33] P. Tseng. Aprroximation Accuracy, Gradient Methods, and [34] Wikipedia. Watson (computer). http://bit.ly/mTwstz. [35] J. Wright and Y. Ma. Dense Error Correction Via [36] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust [37] A. Y. Yang, S. S. Sastry, A. Ganesh, and Y. Ma. Fast [38] J. Yang and Y. Zhang. Alternating Direction Algorithms for [39] Y. Zhang, A. d X  X spremont, and L. E. Ghaoui. SparsePCA:
