 Shared Nearest Neighbor Density-based clustering (SNN-DBSCAN) is a robust graph-based clustering algorithm and has wide applications from climate data analysis to network intrusion detection. We propose an incremental extension to this algorithm IncSNN-DBSCAN, capable of finding clusters on a dataset to which frequent inserts are made. For each data point, the algorithm maintains four properties: near-est neighbor list, strengths of shared links, total connection strength and topic property. Algorithm only targets points that undergo change to their properties. We prove that, to obtain the exact clustering it is sufficient to re-compute properties for only the targeted points, followed by possible cluster mergers on newly formed links and cluster splits on the deleted links.
 Experiments on KDD Cup 1999 and Mopsi search engine 2012 datasets respectively demonstrate 75% and 99% reduc-tion in the size of the set of points involved in property re-computations. By avoiding most of the redundant property computations, algorithm generates speedup up to 250 and 1000 times respectively on those datasets, while generating the exact same clustering as the non-incremental algorithm. We experimentally verify our claim for up to 2500 inserts on both datasets. However, speedup comes at the cost of up to 48 times more memory usage.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering ; I.2.6 [ Artificial Intelli-gence ]: Learning X  Knowledge acquisition Algorithms, Experiments  X  Corresponding author Incremental clustering, Graph-based clustering, Shared Near-est Neighbor, Density-based clustering, Dynamic dataset
Many popular clustering algorithms use the stationary na-ture of data to find a globally optimal solution [6]. When changes are made to the dataset, these algorithms have to be run on the entire dataset to update possible changes in clus-tering, involving significant redundant computations. This is unacceptable in scenarios such as network intrusion de-tection and web-scale document clustering where the inserts to the dataset are quite frequent. Incremental clustering algorithms target this fundamental problem of avoiding re-dundant computations.
 We present IncSNN-DBSCAN algorithm which is an incre-mental extension to the SNN-DBSCAN algorithm[2]. We chose SNN-DBSCAN because of its ability to identify clus-ters of varying densities and shapes, even in datasets with high dimensionality. It uses a more reliable similarity mea-sure -Shared Nearest Neighbor (SNN) similarity and then applies DBSCAN [4] for robustness of clusters against noise. IncSNN-DBSCAN specifically targets only those points that undergo changes to their properties after an insert. These properties are: nearest neighbor list (NN-list), strengths of shared links, total connection strength and topic property. Points that undergo changes in these properties are catego-rized into three types I, II &amp; III, depending on severity of change in their properties after insertion of new data point. These changes lead to formation of new SNN connections and deletion of old ones. New SNN connections could merge existing clusters and deletions could split them. Just like In-cDBSCAN [5], our algorithm is order-independent, that is output does not depend on the order in which updates are made to the dataset.
 We experimentally evaluate performance of our algorithm on two real world datasets: KDD-1999 1 and Mopsi 2012 2 . Expensive property re-computations are required for a max-imum of 25% of original data points for KDD-1999 dataset and 1% of the original data points for Mopsi 2012 dataset. This results in speedup up to 250 times and 1000 times re-spectively on these datasets. However, speedup comes at the cost of up to 48 times memory usage.
 Rest of the paper is organized as follows. Preliminaries and http://kdd.ics.uci.edu/databases/kddcup99/ kddcup99.html http://cs.joensuu.fi/sipu/datasets/ definitions are presented Section 2. IncSNN-DBSCAN al-gorithm is discussed in Section 3. Experimental results are analyzed in Section 4. Finally, we conclude in Section 5 along with discussion about future work.
Definition 1. Clustering: Given a data set A and a similarity function sim ( x,y ) , a clustering is defined as a mapping f : A  X  N such that for x,y  X  A and a threshold
Definition 2. Incremental Clustering: Given a data set A alongwith its initial clustering f : A  X  N and an update sequence of n data points. After k  X  n updates let A 0 be the new data set, then an incremental clustering is defined as a mapping g : f,A 0  X  N isomorphic to the one-time clustering f ( A 0 ) by the non-incremental algorithm.
Calculating Shared Nearest Neighbor similarity [1], be-gins with construction of k-nearest neighbor graph. Two points are called shared nearest neighbors (SNN), if they are present in each others nearest neighbor lists (NN-list). If two SNNs p 1 &amp; p 2 , have more common neighbors in their NN-lists then shared link p 1  X  p 2 also strengthens. We chose an approach in [3], to measure shared link strengths, which is, the sum of the product of the ranks assigned by p &amp; p 2 to their common neighbors. If two points belong to different clusters then SNN similarity score between them is low. Even if points of different clusters are located in each others nearest neighbor list, they will not have sufficient common nearest neighbors.
SNN density-based clustering [2] defines a cluster to be a maximal set of SNN reachable points. SNN reachability exists between two points p and q , if we can find a sequence of SNN connected points starting from p up to q (or vice-versa). SNN connection exists between two points p &amp; q , if they have a shared link with strength above a thresh-old called merge threshold (merge t) and at least one of the points is a topic point and none of them is a noise , see Figure 1. A point is a topic, if its total connection strength (TCS) is greater than or equal to a threshold called topic thresh-old (topic t). TCS is the number of strong links incident on a point. Strong links are shared links with strength above another threshold called strong threshold (strong t), where strong threshold  X  merge threshold. A point is considered as noise, if its TCS falls below noise threshold (noise t). TCS rises when a point is surrounded by similar points (with respect to nearest neighbors). Thus, points in regions of high and low density will have high SNN-connectivity, but the ones located in the transition between these regions will have low SNN connectivity.
Due to insertion of a new point new pt , three types of changes can be observed , see Figure 2 ( p  X  q means q is in p 0 s NN-list, p  X  q means p,q have shared link). 1. (Type I) Changes in NN-list, shared link strengths, 2. (Type II) Changes in shared link strengths, total con-3. (Type III) Changes only in cluster membership First, new pt X  X  NN-list needs to be constructed by scanning all other points in the data set. Existing data points com-pete to appear in NN-list of new pt and vice versa. Points that accept new pt into their NN-list fall in Type I. Such points have to delete one of the older members from their NN-list and degrade rank of others who earlier ranked equal to or below, new pt . This directly affects strength of shared links of Type I points, thereby affecting its total connection strength and topic property. But the contribution from a possible new shared link between new pt and Type I point could be sufficient to help Type I point surpass its past total connection strength and make it a topic from earlier being a non-topic. NN-lists of non-Type I shared neighbors of Type I points is not changed by new pt . However, the rank of non-Type I points could be degraded in NN-list of Type I points, they are the Type II points. This might result in weaken-ing of shared link strengths between a Type I point and its Type II neighbors. As a result, total connection strength and topic property of Type II points will also change. Flow diagram of our algorithm is given in Figure 3. The al-gorithm proceeds outwards from the newly inserted new pt , to characterize the layer-wise effect of the insertion. The al-gorithms is divided into five phases: phase 0, phase 1, phase 2, merge phase, and split phase. Interested readers can see our complete implementation at http://goo.gl/6INMjU Non-Type I and non-Type II points that are SNN-reachable from a Type II point are labeled as Type III points. For all Type III points, NN-list and shared links are unchanged. However, a Type II neighbor of a Type III point can change from a topic to a non-topic point. As a result, an earlier SNN connection between them could be lost. This opens up possibility of splitting the existing cluster that contains these points.
 Points that are not SNN-reachable from new pt , Type I or II point after the insertion are unaffected in all of their four properties. Even their cluster membership would not change. The costly computations for updating properties of such points can be safely pruned.

NN-list computed for each data point by IncSNN-DBSCAN algorithm is same as the non-incremental algorithm. Ev-ery other property of data points and the final clustering is based on NN-lists of all the data points. Therefore, output of IncSNN-DBSCAN algorithm is order-independent and matches the non-incremental algorithm.
Our main focus was to verify that the clusterings of our al-gorithm and SNN-DBSCAN match and to demonstrate the speedup and memory requirements. First, SNN-DBSCAN runs on a portion of the data to come-up with parameters that will be fixed for the rest of the in-sertions. Then IncSNN-DBSCAN and SNN-DBSCAN both run on incremental changes using these parameters. Clus-ters generated by IncSNN-DBSCAN pass isomorphism test with clusters generated by SNN-DBSCAN. We measure per-formance of IncSNN-DBSCAN algorithm in terms of the speedup, that is the ratio of running time of SNN-DBSCAN algorithm to running time of our algorithm. Other statis-tics like the memory usage and number of Type I,II &amp; III points were also obtained to explain the reasons behind the speedup.
The Network Intrusion Data from KDD Cup 1999, con-sists of TCP sessions with 41 attributes. We used a subset of 3500 records from this dataset. We made the first 1000 records of the filtered dataset our initial portion to fix pa-rameters and 2500 insertions were made subsequently.
Mopsi Locations 2012, is a 2-dimensional dataset taken from user locations on Mopsi -a location based search en-gine, having 13467 records. An initial portion with 10000 records was used to set the parameters, followed by 2500 inserts. As shown in Figure 4, the speedup showed a steady rise with increase in number of inserts. To give a perspec-tive on a 250 times speedup in KDD-1999 dataset, our im-plementation completed 3000 inserts in 2.5 minutes and the non-incremental version took nearly 15 hours. The basis of this speedup can be explained using Figure 5. Only 25% of the data points required re-computation of properties, eliminating significant portion of redundant computation. On Mopsi dataset our algorithm finished in under 5 min-utes, while SNN-DBSCAN took 53 hours. Our algorithm achieves speedup of up to 1000 times by avoiding property re-computations for up to 99% of data points, as shown in Figure 5. However, the speedup comes at the cost of in-creased memory requirements as shown in Figure 7. Addi-tional main memory is used to store last clustering results, properties for data points and book-keeping data structures to keep track of Type I, II &amp; III changes.
The types of points comprising the affected set for both datasets are shown in Figure 6. On an average, Type I points were found to be more than any other kind. This is because, for a point to be regarded as a Type I point, it is enough for the new pt to find a place in the NN-list of that point, but the same point might not find any place in the NN-list of new pt . A Type II point must have had a shared link with Type I point. A shared link by virtue of being a mutually accepted link are rarer to find than just a one way acceptance required for a Type I point. Type III automatically become rarer because Type II points are rare.
IncSNN-DBSCAN is an order-independent, incremental version of the SNN-DBSCAN algorithm. By updating prop-erties of only a fraction of pre-determined points, a signif-icant portion of redundant computations is avoided. Per-formance of our algorithm is demonstrated on two real-world datasets with speedup of multiple orders of magni-tude. However, significant memory overhead is the bottle-neck of our algorithm. Our algorithm supports only inser-tions to the dataset. However, supporting deletions will be an important future direction. [1] R. Jarvis, and E. Patrick. Clustering Using a Similarity [2] L. Ert  X  oz, M. Steinbach, and V. Kumar. Finding [3] L. Ert  X  oz, M. Steinbach, and V. Kumar. Finding topics [4] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [5] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and [6] R. Xu, D. Wunsch. Survey of Clustering Algorithms.
