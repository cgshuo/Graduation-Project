 Liang Wang, Xiaozhe Wang, Christopher Leckie, and Kotagiri Ramamohanarao There has recently been growing interest i n algorithms that can extract useful information from non-traditional data such as images and videos [1]. Human mo-tion analysis [2] aims to discover and understand patterns of human movements from video sequences, e.g. , determining typical and anomalous motion patterns, classifying motions into known categories ( e.g. , walking or riding), and discover-ing unknown motion patterns by clustering. Human motion analysis has a wide range of applications such as video surveillance ( e.g. , finding suspicious events such as a person wandering around in a parking lot), human-machine interface ( e.g. , gesture-driven control) and video understanding and summarization ( e.g. , interpretation of sport events).

This paper focuses on the analysis of short video clips consisting of individ-ual atomic motions . One of the key challenges in the interpretation of human motions is how to transform semantically agnostic video signals to meaningful feature representations ( i.e. , low-level feature extraction steps) that provide a sufficient encoding of different motion structures. The resulting outputs can be used as inputs to higher-level recognition processes. There are several issues in this context that raise challenges for human motion analysis [3]: 1) Repeated performance of the same motion by the s ame person in differ ent instances can vary. 2) The same motion performed by d ifferent people can vary because dif-ferent people have different physical structures or perform motions in different ways. 3) The same motion may have different temporal durations because of the difference in motion speeds. 4) Different motions may have significantly different temporal durations. We consider these variations in motions (due to different instances, different persons with different body types and motion styles, and dif-ferent motion speeds) as intra-and inter-person variations on both temporal and spatial scales . The objective of this paper is to develop an approach to represent and recognize articulated and deformable human motions while accounting for the above spatio-temporal variations in motion execution.

To this end, this paper proposes a method based on characteristic descriptors for recognizing human motion sequences, as shown in Figure 1. The proposed method consists of the following steps: 1) Extract space-time silhouettes of the moving human from the input sequence. We use normalized raw silhouettes as visual cues, because they are simple but informative, and easy to obtain from original video data. 2) Transform high-dimensional silhouette inputs into a low-dimensional feature space. In particular, we use computationally efficient Tensor Subspace Analysis (TSA) [4] for dimensionality reduction while preserving spatial information of silhouette images. To the best of our knowledge, no previous work has investigated its use in this context. 3) Map each motion sequence into a form of multi-dimensional time series in the learned embedding space, from which we extract structure-based statistical fea tures to construct a v ector-based pattern representation ( i.e., characteristic-based descriptor ), which naturally converts our temporal sequence classification into a static classification problem. 4) Learn a multi-class Support Vector Machine (SVM) classifier [5] using labeled data, and then use it to classify unknown motion sequences into one of a set of known motion categories. Experimental results on two real-world video data sets have validated the proposed method.

The remainder of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 details each st ep of the proposed method. The experi-mental results are presented in Sectio n 4, prior to a summary in Section 5. Motion representation and recognition are central to the interpretation of human motions. Various visual cues have been examined in current studies on human motion analysis, e.g. , optical flow [6], local descriptors [7], motion trajectories from feature tracking [8,3], etc. For example, Schuldt et al . [7] constructed video representations in terms of lo cal space-time features. Efros et al . [6] proposed a spatiotemporal descriptor based on blurred optical flow measurements to recog-nize actions.

Image measurements in terms of optical flow or interest points can be unre-liable in cases of smooth surfaces, motion singularities and low-quality videos. Feature tracking is difficult due to the great variability in the appearance and articulation of the human body. Fortunately, human motions can be regarded as temporal variations of human silhouettes over time. The use of features derived from silhouettes has been explor ed recently. For example, Blank et al . [9] utilized properties of the solution to the Poisson equation to extract features from space-time silhouettes for action recognition and detection. Silhouette extraction from video is relatively easier for current im perfect vision techniques. So the method that we present here uses (probably impe rfect) space-time silhouettes as basic cues to derive effective motio n feature representations.

There are two major categories of approaches to motion recognition [2]. The approaches based on template matching first convert time-varying features cor-responding to a motion sequence into a static pattern, and then compare it to pre-stored motion prototypes during recognition. In contrast, state-space ap-proaches usually use temporal models such as Hidden Markov Models (HMMs), Conditional Random Fields (CRFs) or their variants [10,8,11] to model and clas-sify motions. For example, Nguyen et al . [8] learned and detected activities from movement trajectories using hierarchical HMMs.

Temporal probabilistic models such as HMMs and CRFs usually require very detailed mathematical and statistical modeling, which involves assumptions about the probability distributions of variables of the dynamical model and de-velopment of inference methods and para meter learning algorithms, which have a high computational cost. In contrast, our proposed method converts a sequence of silhouette images associated with a motion video into a form of multivariate time series, from which we extract structural statistical features to summarize motion pattern. This strategy using characteristic-based descriptors reduces our temporal classification problem into a static classification one. Accordingly, any of the existing efficient methods for classification can be applied for learning and predicting motion sequence classes. 3.1 From Motion Image Sequences to Silhouette Sequences Informative features are critical to motion characterization. The features should be simple, intuitive and easy to extract automatically. As stated before, our work prefers to use silhouettes as basic cues. How should we segment the moving human region from the background image? This can usually be accomplished by well-established motion detection techni ques. Various categories of methods for motion detection have been widely studied in the computer vision community [2] ( e.g. , background subtraction and temporal differencing). Motion segmentation is not our focus in this paper. As such, the video data sets to be used in our experiments have already containe d the segmented silhouette masks.

Given a motion video V including T image frames I , i.e. , V =[ I 1 , I 2 ,  X  X  X  , I T ], we can obtain an associated sequ ence of moving silhouettes S =[ S 1 , S 2 ,  X  X  X  , S T ]. The size and position of the foreground human region in silhouette images vary with the distance of the human from the camera, the size of the human and the motion being performed. The silhouette images are thus centered and normal-ized on the basis of keeping the aspect ratio property of the silhouette so that the resulting silhouette images  X  S =[ X 1 , X 2 ,  X  X  X  , X T ] are of equal dimensions and contain as much foreground information as possible without distorting the motion shape. 3.2 From Silhouette Sequences to Multivariate Time Series Human silhouettes through the duratio n of a motion may be generally expected to lie on a low-dimensional manifold embedded in a high-dimensional image space. It is well known that high dimensionality not only slows the algorith-mic processing, but also degrades perfor mance. Therefore we are motivated to represent motions in a more compact s ubspace rather than the ambient space. Traditional dimensionality reduction algorithms such as Principal Component Analysis (PCA) and Linear Discriminative Analysis (LDA) [12] for image pro-cessing usually represent an n 1  X  n 2 image by a vector in high-dimensional space R n ( n = n is intrinsically a matrix (or a second-order tensor ). Tensor-based methods have been recently studied in the image proce ssing community [4,13,14,15]. To repre-sent the relationship between the row and column vectors of the image matrix, i.e. , to preserve the spatial information of silhouette images, we select TSA to perform subspace learning of the articulated motion space [16], in which an im-age is represented as a second-order tensor in R n 1  X  X  n 2 (where R n 1 and R n 2 are two vector spaces and  X  denotes the tensor produc t). TSA has been recently proposed and demonstrated for use in static face recognition [4]. Here we extend its application to dynamic silhouette data with highly-varied motion shapes.
Given a set of m points { X 1 , X 2 ,  X  X  X  , X m } in R n 1  X  X  n 2 ( e.g. , normalized silhouettes here), TSA aims to find two transformation matrices U of size n 1  X  l 1 and V of size n 2 { These m points can build a weighted graph G to model the local geometrical structure of data manifold M .Let W be the weight matrix of G ,and where  X  X lose X  can be defined by the k nearest neighbors, i.e. , X i is among the k nearest neighbors of X j ,or X j is among the k nearest neighbors of X i ,and  X  is a suitable constant. The function e  X  X i  X  X j 2 / X  is the so called heat kernel, and  X  is the Frobenius norm of a matrix. A reasonable transformation representing the graph structure can be obtained by solving the following optimization problem based on the graph Laplacian [4]: It is equivalent to the following simultaneous optimization problem [4]: where D is a diagonal matrix with D ii = j W ij , D V = i D ii X i VV T X T i , W An iterative method is suggested in [4] to address this optimization problem. If U is first fixed, then V can be computed by solving ( D U  X  W U ) v =  X  D U v . Once V is obtained, U can be updated by solving ( D V  X  W V ) u =  X  D V u . Thus, the optimal U and V can be obtained by iteratively computing the above generalized eigenvector problems.

After learning the tensor subspace including the first l 1  X  l 2 principal compo-nents, any silhouette sequence V can be accordingly proj ected into a trajectory P in such a parametric space P = { P 1 , P 2 ,  X  X  X  , P T } , P i  X  X  l 1  X  X  l 2 , while the temporal order across frames is preserved explicitly. Then, we may easily con-vert P into a form of multivariate time series with the number of dimensions l = l 1  X  l 2 . 3.3 From Multivariate Time Series to Characteristic-Based Now motion sequence recognition can be regarded as a time series classification problem. Several alternative paradigms for time series classification have been proposed [17]. Here we wish to extract the most informative features to sum-marize multivariate time series so as to turn time series classification into static vector-based classification.

In this study, we investigated various dat a characteristics from diverse per-spectives related to univariate time series. A univariate time series can be rep-resented as an ordered set of n real-valued variables Z 1 , ..., Z n . We selected the nine most informative, representative and easily measurable characteristics to summarize the time series structure: Trend , Seasonality , Serial Correlation , Non-linearity , Skewness , Kurtosis , Self-similarity , Chaotic ,and Periodicity .It can be seen that this set of cha racteristic metrics to re present univariate time series and their structure-based features not only includes conventional features ( e.g. , trend), but also covers many advanced features ( e.g. , chaos) which are de-rived from research on new phenomena [18] . Based on these identified character-istics, corresponding metrics are calculated for constructing the structure-based feature vectors [19], that form a rich portrait of the nature of a time series.
In time series analysis, decomposition is a critical step for transforming the se-ries into a format for statistical measure ment [20]. Therefore, to obtain a precise and comprehensive calibration, some m easures need to be calculated on both the raw time series data, Z t , (referred to as raw data), as well as the  X  X rend and seasonally adjusted X  time series, Z t , (referred to as tsa data). Four of the nine selected features, i.e. , Serial-correlation, Non-linearity, Skewness, and Kurtosis, are calibrated on both raw and tsa data, each of which contributes two metrics to our family. The remaining five selected features are calibrated only on raw data, leading to a total of thirteen metrics.

For each dimension of a l -dimensional multivariate time series, we may obtain 13 statistical features to construct the feature vector. Thus, the multivariate time series can be summarized by a r -dimensional ( r =13  X  l ) vector f .We refer to such a feature vector f as a Characteristic-based Descriptor . 3.4 From Characteristic-Based Descriptor to Motion Recognition Motion recognition aims to classify an unknown test sequence into one of c known motion classes. Among many available methods for static classification problems, we adopt a multi-class SVM classifier because its performance surpasses other competing classification methods on many benchmark data sets [21].
 y  X  X  = { 1 , 2 ,  X  X  X  ,c } are the known class labels. The multi-class SVM involves a set of discriminant functions g y : F X  X  r  X  X  ,y  X  X  defined as functions centered at support vectors S = { s 1 , s 2 ,  X  X  X  , s v } ,s i  X  R r which are usually a subset of the training data, A =[  X  1 ,  X  X  X  ,  X  c ] is composed of all weight vectors, and b =[ b 1 ,  X  X  X  ,b c ] T is a vector of all biases. The multi-class classifica-tion rule q : F X  X  = { 1 , 2 ,  X  X  X  ,c } is defined as Several methods to train multi-class SVM are compared in [5]. Simple One-Against-All decomposition is adopted here, which transforms a multi-class prob-lem into a series of c binary subtasks that can be trained by binary SVMs. Also, we use the Radial Basis Function (RBF) k ( f a , f b )=exp(  X  0 . 5 f a  X  f b 2 / X  2 ) as the kernel in our experiments.
 4.1 Evaluation Databases There is no standard evaluation database in the domain of human motion analy-sis. We use two state-of-the-art databases in [22] and [9] to evaluate our method. These two databases are appreciably sized (among current databases publicly available), in terms of the number of p ersons, motions and video sequences.
D-I: Data set I consists of 10 different motions performed by one person, each comprising 10 instan ces, and 100 sequences in total [22]. These motions are pick up object (Pick), jog in place (Jog), push , squash , wave , kick , bend to the side (Bend-Side), throw , turn around (Turn), and talk on cell phone (Phone). Examples are shown in Figure 2(a). Different instances of the same motion may consist of varying relative speeds. This data set is used to examine the effect of temporal execution rates (alone) on motion recognition, as well as slightly different intra-person motion styles among different instances.
D-II: Data set II consists of 90 low-resolution videos from 9 different people, each performing 10 different motions [9] 1 .Thesemotionsare bend , jump jack (Jack), jump-forward-on-two-legs (Jump), jump-in-place-on-two-legs (Pjump), run , gallop-sideways (Side), skip , walk , wave-one-hand (Wave1), and wave-two-hands (Wave2). Example images are shown in Figure 2(b). Except for bend , whether the other motions are in essence periodic or not, people are asked to perform those motions multiple times in a continuously repetitive manner. From these 90 videos, we extract 198 motion sequences for our experiments, each of which includes a complete cycle of atomic motion. The number of sequences of each motion is respectively 9, 23, 24, 27, 14, 22, 25, 16, 19, and 19 for bend, jack, jump, pjump, run, side, skip, walk, wave1, and wave2. In addition to temporal execution rates, there are inter-person differences between the same motions since different people have different physi cal sizes and perform motions in differ-ent styles and speeds. Thus this data set is more realistic for testing the method X  X  robustness to motion variations at both temporal and spatial scales. 4.2 Data Processing and Classification We adapted the  X  X eave-one-out X  cross-validation method for the experiments on both data sets. For D-I, we partition the data set into 10 disjoint sets, each containing one instance of every class of motion. Each time we leave one set out for the test, and use the remaining nine sets for training. This process is repeated 10 times for D-I. For D-II, since different motions are performed by different people in a varied number of r epeats, we decided to use the person id to subset the data. That is, we divide the data set into 9 sets, each set including all motions from one person. Each time we leave one set out for the test, and use the remaining sets for training. Thus, if one video in the left-out set is classified correctly, it must show a high similarity to a video from another different person performing the same motion. This process is repeated 9 times for D-II.
For silhouette extraction, we directly us e the silhouette masks obtained from [22,9], even though the quality of these silhouette images is not very satisfactory, consisting of leaks and intrusions due to imperfect segmentation. Then, we center and normalize all silhouette images into the same dimension ( i.e. ,48  X  32 pixels). Figure 3 illustrates the process  X  X rom a motion image sequence to an associated sequence of normalized silhouette images X .

When learning the tensor subspace using a given training set, we use the k -nearest neighbors ( k = 20) to construct the affinity graph. A heat function with  X  = 1000 is adopted for the weight matrix. When computing U and V , the number of iterations is taken to be 15, and U is initially set to the identity matrix. TSA significantly reduces the dimension number of the input features from 48  X  32 to 4  X  4( i.e. , l 1 = l 2 = 4) (thus leading to lower computational cost), while achieving high accuracy. Not e that these parameter settings were found empirically in a series of experimen ts. Each motion sequence is projected into a 16-dimensional time series in the learned embedding space, from which we extract 13 statistical features correspo nding to each univariate time series. Then these features from each univariate time series are joined as one 208-dimensional (16  X  13) vector. Figure 4 gives several examples of motion sequences in the form of multivariate time series after TSA transformation (in which only the first 7 dimensions, each color per dimension, are shown for simplicity and clarity).
There are two free parameters which need to be tuned for the SVM, namely the regularization constant C and the argument  X  of the kernel function. A com-mon method to tune the parameters is to use cross-validation to select the best best parameter ( C  X  , X   X  ) is tuned, a multi-class SVM classifier is trained using all training data available. Then it may be used to predict the class labels of new test sequences. We use the parameter sets of (  X  =3 ,C = 10) for D-I and (  X  =2 . 3 ,C = 10) for D-II in our experiments. In addition, we implement the Nearest-Neighbor (NN) classifier as a baseline for comparison. 4.3 Results and Analysis The results of motion sequence recogn ition are summarized in Tables 1 and 2. Note that the recognition rates reported here are measured in terms of the per-centage of correctly classi fied motion sequences among all test sequences. The results show that: 1) Dynamic silhouettes are indeed informative to encode mo-tion information, and our feature extraction and representation methods are effective; 2) D-I is more easily classified. This is probably because all motion instances are from the same person, thus there are comparatively fewer changes among time-varying silhouette shapes when the same motion is performed; 3) In contrast, D-II is harder to classify because those motions are performed by different people with different body builds and motion styles; and 4) SVM per-forms better than NN. In summary, our method is demonstrated to be effective for recognition of human motion sequences with temporal and spatial variations due to different people.

To examine and analyze which motion sequences are incorrectly classified (and why), we show confusion matrices with respect to the two data sets in Fig-ure 5. The elements of each row in the confusion matrix represent the probability that a certain kind of motion is classified as other kinds of motions. From Fig-ure 5, it can be seen that most motion sequ ences have perfect classification, and only a small number of motions ( e.g. , Kick/Pick in D-I, Skip/Jump/Run, and Wave2/Jack in D-II) are easily confused. In addition to poor silhouette segmen-tation, high similarities among silhouette shapes in these motions (with locally similar moving patterns) may contribute to these confusions. 4.4 Discussion and Future Work Although the experiments have demonstrated that our methodology works well, further evaluation on a larger database, with multi-varied motions, persons and scenarios, needs to be examined. Apart from simple silhouette observations, other visual cues could be available from raw videos. Fusion of multiple cues may be preferable for improving accuracy and reliability. When extracting characteristic-based descriptors, we separately process e ach dimension of the multivariate time series and then simply stack these individual features together. More sophisti-cated methods that can exploit mutual information among different dimensions of multivariate time series can be useful.

The proposed method currently focuses on the analysis of short video clips consisting of a single individual motion. As long-term goals, we wish to extend our work in several ways: 1) Segmentation and localization in long videos, to find whether a specified action exists in the observed video, and where it is in the video; 2) Behavior profiling, to summarize which basic actions exist in the video, from which the behavior event can be summarized; and 3) Discovering nor-mal and abnormal motion patterns, which may be performed by automatically clustering motion events that frequently occur over a period of time as normal actions, whereas rare actio ns in comparison can be inferred as being abnormal. This paper has described an effective met hod for motion sequence recognition. It starts with extracting time-varying silhouettes from image sequences, and then embeds dynamic silhouette sequences into low-dimensional multivariate time series by tensor subspace analysis. Cha racteristic-based s tatistical features are obtained from multivariate time series to characterize motion patterns. A multi-class SVM classifier is finally adopted to learn and predict the categories of motion patterns. Our experimental results on two state-of-the-art data sets have validated the proposed method. As a by-product, the multivariate time series for the two video data sets derived from our method provide two dynamic and high-dimensional time series data sets for researchers working on time series analysis in the data mining community.
