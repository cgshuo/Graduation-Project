 1. Introduction
The advent of microarray technology has enabled the researchers to rapidly measure the levels of thousands of genes ex-pared with the standard histopathological tests, the gene expression profiles measured through microarray technology pro-vide accurate, reliable and objective cancer classification.

The DNA microarray data for cancer classification consists of large number of genes (dimensions) compared to the num-ber of samples or feature vectors. The high dimensionality of the feature space degrades the generalisation performance of the classifier and increases its computational complexity. This problem is popularly in known as the small sample size (SSS) lowed by classification in the lower-dimensional feature space. Different methods used for dimensionality reduction can be grouped into two categories: feature selection methods and feature extraction methods. Feature selection methods retain only a few useful features and discard others. Feature extraction methods construct a few features from the large number of used for cancer classification. A number of papers have been reported in the past for the cancer classification task using the microarray data. We provide a brief description of a small sample of these papers to highlight different techniques used for dimensionality reduction and classification for this task.

Golub et al. [13] adopted gene selection criteria based on correlation of genes prior to the classification. The selected genes were utilized in weighted voting (WV) approach for cancer classification. Furey et al. [11] applied similar technique as of Golub et al. [13] for gene selection and demonstrated the use of support vector machine (SVM) for cancer classifi-cation. Dudoit et al. [8] compared the performance of different discrimination methods for classification of tumours.
These methods included nearest neighbour (NN) classifier, linear discriminant analysis (LDA), diagonal discriminant anal-ysis, quadratic classifiers and classification trees. They considered bagging [5] and boosting [9] approaches to select rel-evant genes, which were used in the classification. Nguyen and Rocke [18] proposed partial least square (PLS) method for human tumor classification. They used PLS and principal component analysis (PCA) for dimension reduction as well as quadratic discriminant analysis (QDA) and logistic discrimination (LD) for classification task. Guyon et al. [14] proposed a gene selection criterion utilizing SVM methods based on recursive feature elimination (RFE). Lee et al. [16] developed a hierarchical Bayesian (HB) model for variable gene selection. Instead of fixing the number of selected genes (dimensions), a prior distribution over it was assigned. Bee and Mallick [2] pointed out that this approach is sensitive toward the choice of some hyper-parameters. Consequently, they considered a multivariate Bayesian regression model and assigned priors that favour sparseness in terms of number of genes used. They introduced the use of different priors to promote different degree of sparseness using a two-level hierarchical Bayesian (2L-HB) model. Zhou et al. [28] proposed a Bayesian ap-proach to gene selection and classification using logistic regression model [1] . They used Gibbs sampling and Markov chain Monte Carlo (MCMC) methods to discover important genes. Geman et al. [12] introduced top scoring pair (TSP), which is based on pairwise comparison between two gene expression levels. This TSP algorithm was extended by Tan et al. [24] to k -TSP, which uses k pairs of genes for classifying gene expression data. They investigated the performance of TSP and k -TSP for three different schemes namely one-vs-others scheme, one-vs-one scheme and hierarchical classifi-cation (HC) scheme. Yeung et al. [26] used Bayesian model averaging (BMA) to address multi-class cancer classification problem. A typical gene selection and classification procedure ignores model uncertainty and uses a single set of relevant genes to predict the class. On the other hand, BMA accounts for the uncertainty by averaging over multiple sets of poten-tially overlapping relevant genes. Tan et al. [25] addressed the small sample size problem with microarray data by pro-posing total principal component regression (TPCR). It can classify human tumors by extracting the lateral variable structure underlying microarray data from the augmented subspace of both independent variables and dependent vari-ables. Zhang et al. [27] developed a type of regularization in SVM to identify important genes for cancer classification.
Leng and M X ller [17] used functional logistic regression tool based on functional principal components for classifying temporal gene expression data.

From this short survey, it is clear that most of the techniques described above employ feature (or gene) selection for dimensionality reduction. Since feature extraction method always give better performance than feature selection method ods, several genes are discarded. Only a few genes are retained based on some criterion function, which tries to rank genes identify the number of selected genes that provides optimal performance for the classification of cancerous tissues, though empirical arguments can be made to justify the number of genes to be selected.

In the present paper, we concentrate on the feature extraction methods for dimensionality reduction. Feature extrac-tion methods can provide better classification performance over feature selection methods since in feature extraction the subspace is a linear combination of original feature space [7] . The two popular feature extraction methods for dimensionality reduction are principal component analysis (PCA) and linear discriminant analysis (LDA). The former method concentrates on the representation of data and is not very powerful in discriminating the cancer classes.
The LDA method, on the other hand, is discriminative in nature and could help in classifying a tissue sample more accurately. In LDA, we attempt to maximize between-class scatter with respect to within-class scatter. This process re-quires solving generalized eigenvalue decomposition problem, which involves the computation of the inverse of within-class scatter matrix. Due to the high dimensionality of microarray data compared to the number of samples available, the scatter matrix becomes singular and its inverse computation is not feasible. Looking at the limitations of LDA, we adapted gradient LDA (GLDA) technique [21] which resolves this type of limitation. The GLDA technique utilizes gradi-ent descent algorithm to do dimensionality reduction. Once the dimension is reduced through the GLDA algorithm, the k -nearest neighbour classifier with Euclidean distance measure is used to classify a tissue sample. Experiments on sev-eral microarray gene expression datasets using the GLDA technique show very encouraging results for cancer classification.

The paper is organized as follows. Section 2 describes the three datasets used in the experimentation, Section 3 illustrates the GLDA technique, Section 4 describes the experiments and results, and Section 5 presents our conclusions. 2. Description of datasets used in the experimentation
Three datasets are utilized in this work to show the effectiveness of the proposed algorithm. The datasets are acute Leu-kemia [13] , SRBCT [15] and Lung Adenocarcinoma [3] . The description of the datasets is given as follows: (1) Acute Leukemia dataset [13] : this dataset consists of DNA microarray gene expression data of human acute leukemias (2) SRBCT dataset [15] : the small round blue-cell tumor dataset consists of 83 samples with each having 2308 genes. This (3) Lung Adenocarcinoma dataset [3] : this dataset consists of 96 samples each having 7129 genes. This is a three class 3. The GLDA technique for cancer classification
The GLDA technique avoids the SSS problem, typically encountered in cancer classification using microarray gene expres-singularity issue due to the SSS problem. We then discuss some methods that have been used in the past to overcome the SSS problem. This is followed by a discussion of the GLDA technique. 3.1. Linear discriminant analysis
Linear discriminant analysis is a popular method for feature extraction and dimensionality reduction. It operates in a supervised mode. In order to describe this method, we define first the notation used. The matrix v denotes a d -dimensional set of n training samples in a c -class problem, X ={ x i sents the i th class label. The set v is partitioned into c subsets v sists of n i number of samples such that:
The samples of set v can be written as Given the sample set v , the within-class scatter matrix ( S defined as space (where 1 6 h 6 c 1) in such a way that the Fisher X  X  criterion [7] is maximised, where jj denotes the determinant. The projection is from d -dimensional space to h -dimensional space of the conventional eigenvalue problem where w i are the column vectors of W . The desired h leading eigenvectors are selected for W that correspond to the largest when S W is non-singular. In the cancer classification problem using DNA microarray gene expression data, the matrix S turns out to be singular due to the SSS problem which restricts the direct application of the conventional LDA technique.
Singularity problem can be avoided in a number of ways. For example, Dudoit et al. [8] have used feature selection pro-cess to reduce the number of features from d -dimensional space to a smaller p -dimensional space. Then LDA technique was applied on these p -dimensional features. But the results obtained by Dudoit et al. [8] were not very encouraging for the could have been discarded during the feature selection process.

The SSS problem has also been addressed in the literature by using PCA technique prior to the application of LDA [23,4,20] . The PCA technique reduces the d -dimensional space to smaller dimensional space to make the within-class scatter matrix non-singular. The PCA technique is not optimized for finding discriminant features and therefore could discard some features crucial for classification purpose.

Some techniques that are based on the modified Fisher X  X  criterion [6,22] have also been developed. These techniques do not optimise the Fisher X  X  criterion function in one stage. As a result, they are not optimal.

On the other hand, the GLDA technique avoids the singularity issue associated with the within-class scatter matrix by optimising the Fisher X  X  criterion. Thereby finding the leading eigenvectors for singular S and rank( S B ) P h for DNA microarray data based cancer classification problem.

The orientation W can be evaluated for singular S W by using gradient descent algorithm based on Fisher X  X  criterion. The W value that gives maximum J ( W ) is the desired orientation. The maximization problem can also be transformed to the min-imization problem by denoting b J  X  W  X  X  1 = J  X  W  X  and finding W for which as [21]
The GLDA algorithm computes W in an iterative manner. The equation (5) updates W in the direction of steepest descent converges. In the algorithm we initialize the orientation using a fixed value for W as follows: 4. Experiments and results
This section demonstrates the performance of the GLDA approach in comparison with many previously reported tech-niques on gene expression datasets. Three sets of microarray gene expression datasets for cancer classification are utilized error. 4.1. Training phase
The training phase involves the following steps: (1) Given the DNA microarray gene expression data v ={ x 1 (2) Find the orientation W using the gradient LDA algorithm. This algorithm requires a proper value for learning rate (3) Transform the data from original d -dimensional space to h -dimensional space using W , i.e., y 4.2. Testing phase
The testing or classification phase is described as follows: (1) Given a test sample x 2 R d . Project the test sample to h -dimensional space using the orientation W ; i.e., y = W (2) Compute the Euclidean distance d j = k y y j k for j =1,2, ... , n . 4.3. An illustration using acute leukemia dataset
This section illustrates the GLDA technique in comparison with PCA technique using acute leukemia dataset. The dimen-nique can reduce the dimensions from 7129 to h , where 1 6 scatter plot of GLDA technique, we used scatter plot derived from PCA technique which is depicted in Fig. 1b . It is evident nique will not be very encouraging. This will produce high misclassification error. The classification accuracy using GLDA technique was evaluated 100% whereas the classification accuracy using PCA technique was calculated to be 64.71%. This simple example gives us an encouragement that GLDA technique may produce better classification performance when com-pared with other standard techniques. 4.4. Results
The experimentation was done using Matlab software. The classification results using GLDA technique on acute leukemia dataset are shown in Fig. 2 . This dataset has samples from two cancer classes (ALL and AML). Thus, the GLDA technique is applied on a two-class problem and reduces the dimensionality from 7129-dimensional space to 1-dimensional subspace.
The projection of ALL and AML samples on 1-dimensional subspace is depicted in Fig. 2 a. The y -axis represents 1-dimen-crease in the number of iterations and after some iterations, it saturates (converges). We say that the GLDA algorithm has converged when the relative increase in the successive iterations is about 1%. For the acute leukemia database the conver-sified samples by the GLDA and other techniques is shown in Table 1 . It is evident that the GLDA technique is giving zero zero misclassification error has been obtained using only 1-dimensional vector.

The classification results using GLDA technique on SRBCT dataset are shown in Fig. 3 . The dataset has four types of tumor samples namely BL, EWS, NB and RMS. The GLDA technique is therefore applied on 4-class problem and reduces the dimen-sionality from 2308 to 3. The projection of samples from BL, EWS, NB and RMS classes on 3-dimensional subspace is depicted 3-dimensional subspace. The test samples are also shown in this figure. They are close to their corresponding train samples. was found to be one. The GLDA technique is compared with other techniques in Table 2 and it can be observed that GLDA is giving zero misclassification error for SRBCT dataset.

The classification results using GLDA technique on lung adenocarcinoma dataset are shown in Fig. 4 . The dataset contains 3 types of samples namely stage 1 tumor, stage 3 tumor and non-neoplastic lung type. The dimensionality of samples is re-duced from 7129-dimensional space to 2-dimensional subspace. The projection of samples on 2-dimensional subspace is de-stage 3 tumor are observed in the vicinity of train samples of stage 1 tumor type which would lead to non-zero misclassi-fication error. Fig. 4 b depicts the learning curve on lung adenocarcinoma dataset for a = 0.1. The convergence was reached classes. The value of k was found to be 7. The number of misclassified samples resulting from the GLDA technique is listed the training and test data used in our study may not be same as that used by Tan et al. [24] , nonetheless, Table 3 gives a general idea of comparative performance of the techniques. The GLDA technique produces only two misclassification error which is lower than all the other techniques listed in Table 3 .

In summary, the GLDA technique exhibits better classification performance on all of the three datasets studied in this pa-per than the other techniques. 5. Conclusions
In this paper, we have investigated the use of the LDA technique to reduce the dimensionality of the feature space for cancer classification using microarray gene expression data. Due to SSS problem, the conventional LDA technique cannot
LDA (GLDA) technique for reducing the dimensionality. The resulting lower-dimensional feature vector is classified using the k -nearest neighbour classifier. The GLDA technique is applied to three different microarray datasets. We have shown that this technique produces quite encouraging results on gene expression datasets.

References
