 Shape-based object recognition is a key problem in machine v ision and content-based image re-trieval (CBIR). Over the last decade, numerous shape matchi ng algorithms have been proposed that points matters) and assume that every point on one boundary h as a counterpart on the boundary it and man made) display part-based variation.
 boundary information and can work with non-boundary points . The methods described in [2, 3, 4] undergo a common transformation. Learning these variation-based parts from scratch is an under-constrained problem. To address this, we incorporate prior knowledge about valid part assignments each case, the initial alignment and the final match are shown . The PBPM algorithm combines three key ideas: Probabilistic point matching (PPM): Probabilistic methods that find a soft correspondence features and clutter (Fig. 1).
 Natural Part Decomposition (NPD): Most shapes have a natural part decomposition (NPD) (Fig. an NPD algorithm prior to matching. Throughout this paper, i t is assumed that we have obtained computed for each database shape/image.
 Variation-based Part Decomposition (VPD): A different notion of parts has been used in computer images. We refer to this type of part decomposition (PD) as a variation-based part decomposition (VPD).
 Given two shapes ( i.e. point sets), PBPM matches them by applying a different trans formation to matching, where the known NPD of the data shape is used to bias the algorithm towards choosing a  X  X erceptually valid X  VPD. This is achieved using the equivalence constraint variation-based part .
 parts are composed of spatially localized points of the gene rating shape.
 the generating shape should be mapped to the correct positio n on the data shape despite the lack this using a single nonlinear transformation [2, 3] perform well on some challenging problems. vent these techniques from selecting the discontinuous transformations associated with part-based movements. PBPM learns an independent linear transformati on for each part and hence, can find the correct global match.
 and the way in which this information is incorporated.
 With respect to the shape-matching literature, PBPM can be s een as a novel correspondence tech-nique for use with established NPD algorithms. Despite the l arge number of NPD algorithms, there are relatively few NPD-based correspondence techniques. S iddiqi and Kimia show that the parts used in their NPD algorithm [6] correspond to specific types o f shocks when shock graph repre-the parts. 3.1 Shape Representation ary and the ordering of the points is irrelevant. Given a generating shape X = ( x R pute the correspondence between X and Y . We assume that an NPD of Y is available, expressed as a partition of Y into subsets (parts): Y = S L 3.2 The Probabilistic Model We assume that a data point y is generated by the mixture model outliers. The distribution of y given a foreground component v is itself a mixture model : with Here, T where  X  using the points x generated by a GMM with V components. However, this GMM is embedded in the larger mode l and maximizing the data likelihood will balance this GMM X  X  d esire for coherent parts against the need for the parts and transformations to explain the actual data (the y 3.3 Parameter Estimation With respect to the model defined in the previous section, C1 states that all y same subset Y between data points in mixture models. The basic idea is to es timate the model parameters using the EM algorithm. However, when taking the expectation (of t he complete log-likelihood) we now only sum over assignments of data points to components which are valid with respect to the con-expectation is given by: Note that eq.(5) involves p ( v | Y the term p ( v | y for p ( y where u is the constant associated with the uniform distribution p ( y estimated are  X  of  X  log-likelihood of data points y a weight, p ( v | Y maximum likelihood problem and derive the EM updates as usua l. The resulting EM algorithm is given below.
 E-step. Compute the responsibilities using the current parameters: M-step. Update the parameters using the responsibilities: where Y parameter, c weighted Procrustes matching problem between two points sets, each of size N  X  M  X  the extent to which x squares problem for the optimal transformation parameters s [8]. The weights associated with the updates in eqs.(10-12) are similar to p ( v | y ence is that p ( v | y is propagated throughout the model.
 initialize  X  parameters.
 As discussed in Secs. 1 and 2, unsupervised matching of shape s with moving parts is a relatively unexplored area  X  particularly for shapes not composed of si ngle closed boundaries. This makes examples which demonstrate the various properties of PBPM a nd then consider more challenging the matches found by PBPM, each point y assigned to v = 0 are removed from the figure. For each y find m the distributions N (  X  parts using the known natural part label of the y Fig. 3 shows an example of matching two human body shapes usin g PBPM with V = 3 . The learnt using different values of V are shown in Fig. 4. Predictably, the match improves as V increases, are grouped together.
 In Fig. 5, there are two genuine variation-based parts and X contains additional features. PBPM is more central.
 uniform density, u , and the way in which points are assigned to parts is also impo rtant. the ability of PBPM to handle suboptimal shape representati ons. The correct correspondence and match.
 prune parts during matching. Alternatively, one could run P BPM for a range of V and use a model next section. whereby a single part is learnt and removed from further cons ideration at each stage. Each new part/component should focus on data points that are current ly explained by the background. This weighted log-likelihood Here,  X  is the responsibility of the background component for the su bset Y  X  the superscript of z indicates the number of components that have already been le arnt. Using for the subsets Y The sequential algorithm terminates when the uniform compo nent is not significantly responsible As discussed in [7], the sequential algorithm is expected to have fewer problems with local min-ima since the objective function will be smoother (a single c omponent competes against a uniform for each part are not shown. The outcome of the sequential alg orithm is highly dependent on the value of the uniform density, u . We are currently investigating how the model can be made mor e robust to this value and also how the used x step.
 Figure 9: Results for PBPM; V and initial parameters were found using the sequential appr oach.
Figure 10: Results for PBPM; V and initial parameters were found using the sequential appr oach. approach described in Sec. 5.

