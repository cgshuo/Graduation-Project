 Existing multimedia recommenders suggest a specific type of multimedia items rather than items of different types per-sonalized for a user based on his/her preference. Assume that a user is interested in a particular family movie, it is ap-pealing if a multimedia recommendation system can suggest other movies, music, books, and paintings closely related to the movie. We propose a comprehensive, personalized mul-timedia recommendation system, denoted MudRecS, which makes recommendations on movies, music, books, and paint-ings similar in content to other movies, music, books, and/or paintings that a MudRecS user is interested in. MudRecS does not rely on users X  access patterns/histories, connection information extracted from social networking sites, collab-orated filtering methods, or user personal attributes (such as gender and age) to perform the recommendation task. It simply considers the users X  ratings, genres, role players (au-thors or artists), and reviews of different multimedia items, which are abundant and easy to find on the Web. MudRecS predicts the ratings of multimedia items that match the in-terests of a user to make recommendations. The perfor-mance of MudRecS has been compared with current state-of-the-art multimedia recommenders using various multimedia datasets, and the experimental results show that MudRecS significantly outperforms other systems in accurately pre-dicting the ratings of multimedia items to be recommended. H.3.3 [ Information Search and Retrieval ]: Information filtering Multimedia recommender, rating, genre, review, popularity
Current multimedia recommenders either integrate user group information extracted from social websites, consider user profiles or attributes (such as gender and age), or ana-lyze past behavior of individual users or overall behavior of a community of users to make recommendations. These infor-mation sources are not always available to be downloaded us-ing APIs over the Internet or social networking sites. More-over, majority of the existing multimedia recommenders sug-gest only a particular type of multimedia items, which are either videos, audios, text, or images. The restricted type of recommended items is a deficient and limitation, since various types of multimedia items offer different sources of information that are appealing to extended groups of users. In this paper, we propose a personalized multimedia recom-mendation system, denoted MudRecS, which recommends four different types of multimedia items X  X ovies (videos), music (audios), books (text), and paintings (images) X  X or its users based on their personal preferences.

MudRecS relies on neither training data, predefined knowl-edge bases, nor ontologies to make recommendations. It does not require any manual intervention from its users nor users X  personal information. Instead, it devises recommen-dations solely based on users X  ratings, genres, reviews, popu-larity of artists/authors, and readability measures on multi-media items Is that are vastly available on the Web. Besides considering the genres of Is , MudRecS extracts features from reviews and determines the polarity and general opinions on the features of Is . MudRecS presents a unique approach for configuring the popularity of role players (authors/artists) of Is that is utilized for devising the user preference on Is . Moreover, whenever applicable, MudRecS analyzes the com-prehension levels of its users so that it enhances its predic-tions on Is that are appealing to the users.

The design of MudRecS, which compares favorably to cur-rent state-of-the-art collaborative filtering, content-based, or hybrid approaches for generating recommendations, is sim-ple and easy to develop. It is a contribution to the area of study in multimedia recommendation, since it introduces novel measures that accurately predict the ratings of mul-timedia items of various domains for each individual user. Predicted ratings of items can be used for ranking the rec-ommended items, and items with higher predicted ratings are expected to be more favorable to the users who have previously rated items of the same or a different domain, which constitute the basis for rating prediction.
We organize our paper as follows. In Section 2, we discuss previous work on multimedia recommenders. In Section 3, we detail the design of MudRecS. In Section 4, we present the experimental results which were used to evaluate the performance of MudRecS. In Section 5, we give a conclusion.
Machine learning, information retrieval, natural language processing, and probabilistic techniques have been adapted in the past to develop systems that recommend movies [9], song/music tracks [3], text documents [7], and images [13], to name a few. Majority of existing recommendation sys-tems are either content-based or collaborative-filtering-based. The content-based (or cognitive) approaches [2] create a pro-file to capture the (items of) interest of a user U using words, phrases, and/or features. Recommenders that are content-based identify the common features of items with favorable ratings given by a user and recommend to the user other items that share the same or similar features. Recom-mendation systems purely based on content similarity mea-sure, however, generally suffer from the problems of limited-content analysis and over-specialization [4]. Limited-content analysis is a result of the relatively small amount of in-formation on users or the content of items, whereas over-specialization restricts the number and diversity of items to be recommended, since the predicted rating of an item I for auser U is high only if I matches in content (such as genres or features) with the ones that U likes.

Collaborative filtering is another well-known recommen-dation method [14] which identifies the group of people who share common interest or similar items with user U and rec-ommends items to U based on the group X  X  interests. Ama-zon X  X  recommender, which applied the collaborative-filtering technique [11], suggested items to a user based on other users X  previous purchase patterns and/or rated items. Such a system is differed from MudRecS, since the latter does not consider users X  access patterns nor histories.

Various hybrid approaches [2] have been introduced which exploit the benefits of using both collaborative-filtering and content-based methods to improve the quality of recommen-dations. Contractually, MudRecS considers semantic infor-mation, such as genres, reviews, popularity, and comprehen-sibility of multimedia items to make recommendations.
A method that translates tags assigned to multimedia con-tents for cross-language retrieval is proposed in [13]. The lexical translation of annotated tags, which handles sense disambiguation of tags, however, is restricted to image re-trieval only and relies on social tagging, which is not required by MudRecS. An agent-based multimedia recommendation system, which is also constrained to process image data, is presented in [10]. The recommender depends on a predefined ontology to capture the users X  preferences and identifies the semantics of hypermedia images to derive their contextual information for making recommendations. MudRecS avoids constructing ontologies and accessing hyperlinks to assess multimedia items to be recommended, which eliminates the tedious setup and avoids imposed overheads.
Multimedia items on the Web are rated using the standard  X  X tar rating system X  in which an item is given a number of stars (a rating level) by web users. To predict the ratings of items previously unrated by a user U for making recom-mendations to U , MudRecS considers different sources of information X  U  X  X  ratings, genres, reviews, role players, and text readability levels X  X n items, which are either movies, songs, books, and/or paintings, previously rated by U in two steps: analysis of pre-rated items and prediction.
Analysis . Given a set of multimedia items previously rated by a user U , MudRecS analyzes the preferences of U based on the genres, features, and role players of the items that U (dis)likes. The genres and role players of multime-dia items can easily be extracted from representative and comprehensive multimedia websites, such as imdb.com for movies, last.fm for songs, iblist.com for books, and flickr.com for images, using a simple HTML parser, whereas features of multimedia items can be retrieved from reviews that are available at well-known websites, such as Epinions(.com), Consumersearch(.com), and Consumerreports(.org). All of the rating information of U are archived in MudRecS for fu-ture references. (See tables in Figures 1, 5, and 6 and Table 2 for sample archived data.) Since the number of multimedia items rated by U is only in the hundreds, or thousands the most, this analysis step is not a time-consuming nor labor-intensive process. Moreover, analysis on U is conducted once after which ratings can be predicted for items unrated by U . Prediction . Using the set of analyzed multimedia items S rated by U , MudRecS predicts the rating of an item I unrated by U . MudRecS considers the genres applied to I , using previous ratings given by U on different genres to determine the genre score , denoted GS (discussedinSec-tion 3.1), of I . MudRecS also considers each feature of I ex-tracted from the reviews of I and computes the review score , denoted RwS ,for I using reviews on items in S (detailed in Section 3.2). According to the popularity of each role player, which is either the author, actor, actress, director, or artist, of I , MudRecS assigns a role player score , denoted RP S ,to I (s ee Section 3.3). Last, but not least, MudRecS calculates the readability score , denoted ReS ,for I ,if I is a book (pre-sented in Section 3.4). MudRecS combines GS , RwS , RP S , and ReS of I to compute the Rating Score of I using the Stanford Certainty Factor ( SCF ) [12] at each one of the N rating levels R i (1  X  i  X  N ), where N is often in the range of5to10(asdefinedinEquation1).The rating level with the highest Rating Score is selected as the predicted rating for I ,whichcanbeusedtodeterminethe ranking order of recommended items unrated by U .
 MAX N i =1 { Rating Score ( R i ,I )=( GS ( R i ,I )+ RwS ( R where L is the readability level of I .If I is not a text document, then ReS ( R i ,I,L ) is excluded from Equation 1 when Rating Score ( R i ,I ) is computed.
Agenreisthe category of an item I . (Table 1 shows sample genres for movies, music, books, and paintings, respectively.) The basic assumption of genres is that if two items have the same genre, then they are likely given similar ratings by the same user [6]. Besides determining the genres of I ,Mu-dRecS also considers genre similarity for rating prediction on I , since (i) two different genres can be highly similar, such as  X  X uspense X  and  X  X orror X  in movies,  X  X ip hop X  and  X  X ap X  in music,  X  X ark X  and  X  X ystery X  in books, and  X  X ointil-lism X  and  X  X ivisionism X  in images, (ii) same items are some-times assigned different genres by different experts/users, and (iii) some items have multiple genres assigned to them. Similarities of various genres can be determined using word-Multimedia Sample Genres Items Movies Action, Adventure, Comedy, Crime, Drama Music Classical, Heavy Metal, Jazz, Rap, Rock Books Crime, Fantasy, Fiction, Mystery, Romance Paintings Art, History, Landscapes, Life, Portraits Table 1: Sample genres for different types of multi-media items considered by MudRecS correlation factors (defined in Section 3.1.1). For each rat-ing level of an item I , MudRecS computes the Genre Score ( GS ) for each possible genre of I (as detailed in Section 3.1.2).
To determine the similarity of two genres, we employ the word-correlation factors in the word -similarity matrix [8], denoted WS -matrix . The similarity, denoted Word Sim , of any two non-stop, stemmed words i and j in WS -matrix is computed using the (i) frequency of co-occurrence and (ii) relative distances of i and j in each document in which they occur. WS -matrix was constructed using the documents in the Wikipedia collection (en.wikipedia.org/wiki/Wikipedia: Databasedownload) with 930,000 documents written by more than 89,000 authors on various topics and writing styles.
Compared with WordNet (wordnet.princeton.edu) in which each pair is not assigned a similarity weight, word-correlation factors offer a more sophisticated measure of word similarity. Using the set of multimedia items S of the same domain D previously rated by a user U andanitem I of D ,which is unrated by U with various genres, MudRecS assigns GS of I at the rating level R i (1  X  i  X  N )as GSG ( R i ,G )= + where G Set is the set of genres of I , G j  X  G Set (1  X  j | G
Set | ), and Word Sim ( G , G j ) is the genre similarity of G and G j .Notethat Word Sim ( G, G j )= Word Sim ( G j ,G ).
Equation 2 assigns a higher GSG toagenre G at R i if G in S has been rated at R i by U more frequently than at any other rating levels in S . The multiplication is used to assign weight to genres such that the less similar agenre G j is to G ,the less it is weighted in the computation of GSG ( R i Figure 1 shows an example of applying Equation 2 to a num-ber of genres in movies, which illustrates how the  X  X orror X  genre receives a higher score at rating level 1 than at rating level 5, since most  X  X orror X  movies, along with genres highly similar to  X  X orror X , i.e.,  X  X hriller X , were rated at level 1. On the other hand,  X  X ction X  and its highly similar genre,  X  X d-venture, X  were rated higher at level 5 than at level 1.
We realize that solely relying on the genres of an item to make recommendations could yield poor rating predictions, since (i) a large amount of data are required to accurately determine a particular user X  X  genre preferences, (ii) a poor Figure 1: Four GSG scores computed at two rating levels for an unrated movie based on movies previ-ously rated by a user U ,and WS stands for Word Sim (good, respectively) rating value for an item I with genre G does not necessarily indicate that a user dislikes (likes, respectively) G , but may have given a poor (good, respec-tively) rating to I due to other factors, (iii) items can be highly similar with respect to their genres and being dissim-ilar with respect to other features not related to genres, such as the  X  X tory X  of a movie. MudRecS considers the genres of I as well as other features addressed in online reviews on I .
In predicting the rating of an item I ,consumerreviews, opinions, and shared experiences on I are valuable sources of information which reflect consumers X  preferences on I . Unlike user ratings that simply indicate their degrees of ap-pealing on items, reviews provide more detailed information. Positive/Negative opinions on different features (i.e., prop-erties) of I are often included in a review of I , such as the (originality of the) story in a movie, the (innovative or mono) tone of a song, or the (surprise or predictable) ending of a book. This information can be used to enhance the accuracy of rating prediction in a recommendation system.

To utilize the valuable information included in the set of reviews RS on items previously rated by user U for pre-dicting the rating level of I , MudRecS (i) retrieves a set of reviews on I from review search engines (as detailed in Section 3.2.1), (ii) relies on a feature-detection approach to identify features of I from retrieved reviews, and (iii) de-termines the polarity of each sentence in reviews, which has been clustered based on a particular feature, as either posi-tive or negative (in Section 3.2.2). Hereafter, each identified feature F of I is assigned the Polarity Feature Score ( PFS ) at each rating level based on the number of positive/negative sentences in RS assigned to F at certain rating levels (in Section 3.2.3).
MudRecS retrieves reviews for I from Epinions, Consumer-search, and Consumerreports. Using a query Q with key-words in the item name of I , MudRecS selects the top-33 reviews extracted from each one of the three review repos-itories 1 retrievedinresponseto Q , which yields the set of reviews of I for identifying the (polarity of) features of I .
MudRecS retrieves 33 reviews from each repository, if they exist, since a collection of a hundred reviews on I is an ideal set for analyzing the features of I [5]. Figure 2: An Epinions.com review for the movie  X  X assions of the Christ X  and its identified features
Since each review R in the set of 99 retrieved reviews on an item I may address various features of I , MudRecS identifies distinct features of I and clusters sentences in R basedonthe detected features. Sentences in a feature cluster is assigned the polarity of the feature. To do that, MudRecS (i) gener-ates a set of (cluster) labels , which are non-stop/-numerical/-sentiment named entities, that capture the features exhib-ited in the set of 99 reviews, (ii) assigns each sentence S in R to a feature cluster C basedonthe similarity between S and the label of C , and (iii) identifies positive/negative opinions on each feature using its clustered sentences.
 Creating (Cluster) Labels for Identified Features
Using each review R in the set of 99 reviews on I ,Mu-dRecS creates concise and accurate labels that reflect the features specified in R using a suffix array algorithm, which has been proved to be efficient and effective in discovering key phrases in large text collections [20]. The suffix array algorithm generates a list of candidate (cluster) labels by simply extracting all the suffixes in the sentences in R .Since the generated list of suffixes may include non-representative labels (i.e., non-features) in describing I in R , MudRecS removes labels that are (i) numerical keywords, which sel-dom capture a feature of I , (ii) incomplete , i.e., included as substrings in other labels, (iii) sentiment keywords, i.e., words that express a positive or negative polarity, which is not considered as a feature of I , and (iv) non-named enti-ties, which can be identified using the Stanford Entity Tag-ger (nlp.stanford.edu/software/CRF-NER.shtml), since fea-tures are expressed as named entities, such as theme and plot . In addition, labels that cross sentence boundaries , which indicate a topical shift, and end in the Saxon geni-tive form are excluded. Moreover, candidate (cluster) labels that reference web addresses are discarded and stopwords at the beginning or at the end of a (cluster) label are removed to enhance the readability of the label. Figure 2 shows a review on a movie extracted from Epinions.com with the features identified by MudRecS bolded in the review. Assigning Sentences to Clusters
To cluster sentences in reviews on I w hich address the same (or similar) features of I , we have developed a simple, effective clustering method. Using the set of identified clus-ter labels CL and each retrieved review R on I , MudRecS computes the degree of similarity between each sentence S in R and label L in CL , denoted LS Sim ( S, L ), as defined in Equation 3, using the word-correlation factors (introduced Figure 3: Some features/sentences extracted from the reviews of the movie  X  X assions of the Christ X  in Section 3.1.1). This process clusters sentences that ad-dress the same or similar feature F of I , which later allows MudRecS to determine whether F receives an overall posi-tive/negative opinion captured in the set of 99 reviews on I by counting the total number of positive/negative sentences (based on sentiments in each sentence) assigned to F . where | S | ( | L | , respectively) is the number of distinct, non-stop, stemmed words in S ( L , respectively), w i ( w j ,respec-tively) is a non-stop, stemmed word in S ( L ,respectively), and Word Sim ( w i ,w j ) is the word-correlation factor of w and w j . MudRecS normalizes LS Sim ( S, L ) by dividing the accumulated word-correlation factors with the number of words in S to avoid favoring long sentences.

MudRecS computes LS Sim ( S, L )foreachsentence S in each review R with respect to each label L in CL and assigns S to cluster C (labeled L C )if LS Sim ( S, L C )isthe highest among all the labels in CL .Figure3showssomefeatures (identified by their cluster labels) that are created using a set of reviews on the movie  X  X assions of the Christ X  extracted from Epinions.com. Also displayed in Figure 3 are some sentences that are assigned by MudRecS to a feature. Identifying Sentiment Opinions on Sentences
To determine the positive or negative polarity of a sen-tence S in a cluster, MudRecS calculates the sentiment score of S by subtracting the sum of the negative sentiment word scores in S (determined using SentiWordNet 2 )fromthe sum of its positive sentiment word scores in S , which reflects the sentiment of S such that if its sentiment score is positive ( negative ,respectively),then S is labeled as positive ( nega-tive ,respectively).
MudRecS computes a score for each feature of a rated item in two steps: Review Analysis , which is conducted once, and Review Score Computation , which is applied to each item considered for rating prediction and recommendation.
Review Analysis .Foreachitem I in a given set S of multimedia items of the same domain previously rated at a rating level R i (1  X  i  X  N )byauser U , MudRecS determines which identified feature F in S receives an over-all positive (negative, respectively) feedback based on the
A SentiWordNet (sentiwordnet.isti.cnr.it/search.php) score of a word is a numerical value that indicates the positive , neutral ,or negative orientation of the word. Rating #of + -+ -+ -+ -
Level Movies Total 75 42 33 31 44 25 50 40 35 Table 2: The ratings of 75 movies provided by a user and a number of features of the movies with positive/negative polarity at each rating level set of 99 reviews retrieved for each I by counting the total number of positive and negative sentences in I clustered un-der the label of F (that represents F ). If F is assigned a larger number of positive ( negative , respectively) than nega-tive ( positive , respectively) sentiment sentences clustered by MudRecS, then the polarity of F is treated as positive ( neg-ative , respectively). Hereafter, MudRecS counts the number of times F in items of S has been assigned a positive (nega-tive, respectively) sentiment at each rating level, which con-stitutes the nominator in Equation 4. (See, in Table 2, the number of polarity of the feature Story at each rating level.)
MudRecS computes a Polarity Feature Score , denoted PFS , for each feature F with polarity P F , which is either positive ( X + X ) or negative ( X - X ), at each rating level R i (1  X  i  X  PFS ( R i ,F,P F )= # of Items in S with F and P F Rated at R i by U
Example 1. Shown in Table 2 are the 75 movies rated at five different levels. The provided ratings on the 75 movies given by a user U were extracted from the Netflix dataset. Regardless of its rating level, MudRecS extracts 99 reviews retrieved for each one of the 75 movies M and determines the polarity of each feature covered in M .Table2showsthe number of movies with an identified feature 3 that has been assigned a positive/negative polarity at each rating level.
The sentiments of each feature shown in Table 2 indicate that U is more interested in the Story and Direct ( ion )ofa movie, and less about Plot and Visuals .

Review Score Computation . Using the items ranked by U in the Review Analysis step, the Review score ( RwS ) of an unrated item I is computed at each rating level R i (1  X  i  X  N ) by averaging the PFS of each feature F in I with polarity P F , which is either Positive or Negative ,at R where Features is the set of features identified in I through the 99 reviews retrieved for I , N is the total number of features (in Features )rankedat R i as computed in the Review Analysis step, and PFS is as defined in Equation 4.
MudRecS has identified 11 features for the set of 75 movies, but only 4 out of 11 are displayed, since they are features of the unrated movie  X  X ar of the World X  to be evaluated. Figure 4: The Review score ( RwS ) at each rating level of the movie  X  X ar of the Worlds X  (WoW) com-puted using PFS s based on the polarities in Table 2
Example 2. The first two lines in Figure 4 show the features and their polarities identified by using the 99 re-views extracted for the unrated movie  X  X ar of the Worlds X  (WoW). The RwS of WoW is the highest at rating level 5, which indicates that the (positive) features, i.e., Story and Direction , of WoW are regarded highly by U ,whicharere-flected at the high levels of ranking (levels 4 and 5) of the two features as shown in Table 2.
While some web users might (not) be interested in an item because of its genre(s) and/or particular features, oth-ers (dis)like the item because of its role player(s). For exam-ple, a user might enjoy movies acted by Will Smith regard-less of the genre or visual effects of anyone of his movies. For this reason, besides analyzing the genres and reviews of multimedia items, MudRecS considers their role players.
A movie recommendation system likely suggests movies to auser U with the same actor/actress/director/producer who is highly rated by U , whereas a book recommender would suggest different books with the same authors (e.g., books published as a sequel). MudRecS uses information of role players in rating prediction by analyzing the popularity of a role player based on the ratings of the role player extracted from different multimedia websites (in Section 3.3.1). Here-after, at each potential rating level of an unrated item I , MudRecS computes a popularity score for the role players of I , denoted RoleP layerScore ( RP S ) (in Section 3.3.2).
Popularity plays an important role in rating prediction because most users are influenced by the opinions expressed by others or the degree of exposition about an author/role player in the market [1]. The popularity of a role player P can be determined using various measures, such as the number of sold items involving P or user reviews, which capture the popularity of P 4 . Each popularity ranking is a numerical value between 0 and 1 such that if P is rated (in the ascending order) n out of m role players, then the ranking of P is n m , such that the lower n m is, the more popular P is. The Web is rich with lists of rated role players for different types of multimedia data.

Movies . MudRecS relies on imdb.com to determine the popularity of role players in movies. Imdb.com creates rat-ings of movie actors, actresses, directors, and producers. The ratings provide a snapshot of who is popular based on the activities involving millions of imdb users on the website, which include the number of times an actor X  X /actress X  page is
The Popularity of a role player P is the by-product of the unique characteristics of P , which appeal to users [1]. viewed and statistical data such as movie ratings, box-office gross, the number of movies a role player involved, salaries of the role players, etc. These ratings are accessible through a service, known as STARmeter, on imdb.com, which reflect the popularity of the role players in movies.
 Music . To determine the popularity of music artists, MudRecS considers the ratings on music artists provided by Billboard.com and Mtvmusicmeter.com. Billboard.com, which is a premier music website and a primary source of in-formation on trends and innovation in music, archives an ex-tensive array of (reviews on) songs and artists. The website rates artists based on users X  reviews and the number of users who favor an artist A (through a clickable  X  X avorite X  button on the website). Billboard.com, however, ignores the statis-tical information on A , such as the sales records of A and the number of played/downloaded streams on his/her songs. For this reason, besides the ratings on and the (un)favorability of music artists provided by Billboard.com, MudRecS also considers Mtvmusicmeter.com to determine the rating of a music artist. Mtvmusicmeter.com is a website that offers access to archived profiles of over one million music artists and allows its users to browse/play their songs. The website establishes an artist meter that rates artists based on their popularity, which is determined by tweets, blog posts, news articles, streams, and sales records. To obtain a unified pop-ularity score for a music artist A , MudRecS averages evenly the ratings of A provided by the two music websites.
Books . MudRecS defines the ratings of book authors using the authors X  popularity ratings archived at iblist.com, which is an online database that boasts entries of over 19,000 authors. Iblist.com employs a rating system for authors sim-ilar to the ratings of actors/actresses/directors/producers on imdb.com. The website is managed by a board of volunteer editors and administrators who ensure the quality and accu-racy of posted information. To avoid spam, irrelevant, and irresponsible remarks, third-party reviews are not accepted by iblist.com. Instead, only reviews submitted by registered, well-established users, who have been approved by the board of editors and administrators, can be posted on the website.
Paintings . Since there is no well-established or well-known website that provides a rated list of popular paint-ing artists, MudRecS defines their ratings using Wikipedia. The website maintains a list of the highest, known prices for paintings, which MudRecS uses for identifying the popular-ity of painters such that the higher the cumulative price of a set of paintings created by a painter P is, the higher P is ranked . This notion is adopted, since the prices of paintings are primarily based on the artists X  popularity and vice versa.
MudRecS computes the Role Player Score ( RP S )foreach role player of a multimedia item in two steps: Role Player Analysis (performed once) and Popularity Computation. Role Player Analysis . Given a set of multimedia items S of the same domain previously rated by a user and the pop-ularity of each role player in S determined in Section 3.3.1, MudRecS computes a score for S , denoted Popularity Score ( PoS ), at each rating level R i (1  X  i  X  N ), which measures the average popularity of all the role players in S at R i Figure 5: The computed RP S score at each rating level for the movie  X  X ar of the Worlds X , where Ac-tor i ( 1  X  i  X  2 ) in different movies can be different
Example 3. Figure 5 shows the ratings provided by a user U on six movies extracted from the Netflix dataset and the rankings of two main actors/actresses in each movie at various rating levels obtained from STARmeter on imdb.com PoS (5 ,S ) = 8/400, the lowest value among all the rating levels, indicates that U favors popular actors/actresses.
Popularity Computation . During the recommendation process, MudRecS sums the popularity score of each role player of an unrated item I . The popularity score of each role player in I is then averaged into one score, denoted Avg Pop , using Equation 7, and MudRecS assigns the RP S to I at each rating level in Equation 8.
 Avg Pop ( I )= A  X  I where NR is the number of role players in I .

Equation 8 computes the difference between the average popularity of role players of I and the average popularity of role players of items rated at R i by U .A larger RP S at R i indicates that the popularity ranking of the role play-ers of I is closer to the popularity ranking of role players of items previously rated at R i , which increases the likelihood of assigning the rating level R i to I . Equation 8 is comple-mented, since RP S is combined with other scores using the SCF (see Section 3), and the higher each score is at a rating level, the more likely the rating level is assigned to I .
Example 4. Figure 5 shows the RP S at each rating level for the movie I ,  X  X ar of the Worlds, X  which is played by popular actors as reflected by the averaged Popularity Score of role players of the movie, i.e., Avg Pop ( I ) = 10/400. The RP S for the movie is the highest at rating level 5, which increases the likelihood of the movie being rated high.
The readability level of a book is a value between 0 and 13, i.e., Kindergarten to College. Reading can be a frustrating experience to users who struggle to understand (are not mo-tivated to read, respectively) a book that is beyond (below, respectively) their readability levels. Analyzing the read-ability levels of books pre-rated by a user U can enhance
An actor X  X /actress X  ranking is the same in different movies. Figure 6: The computed ReS value at each rating level for the book  X  X ar of the Worlds X  the accuracy of rating prediction on books unrated by U . MudRecS, which determines whether a book B unrated by U is appropriate for U based on the readability level of B , relies on the readability levels of books pre-rated by U .
Majority of existing readability assessment tools examine lexical, syntactic, and semantic content of a text document I to determine the readability level of I . These properties, however, are not well-designed and mostly based on obser-vations. MudRecS relies on ReadAid [15], a fully-automated readability analyzer, to access the readability level of I .
Using a set of books S previously rated by a user U with their readability levels computed by ReadAid, MudRecS as-signs a readability score , denoted ReS , at each rating level R (1  X  i  X  N )toabook I unrated by U basedontheread-ability level L (0  X  L  X  13) of I determined by ReadAid. ReS ( R i ,I,L )= Bks @ R i L where Bks @ R i L ( Bks @ R i k , respectively) is the total num-ber of books in S at readability level L ( k , respectively) rated at R i by U and TBks @ R i is the number of books in S rated at R i by U . The constraint  X  k = L  X  X voidsadivision by zero. ReS ( R i ,I,L )=0,if TBks @ R i =0.

Equation 9 assigns a higher ReS toabook I at R i if books in S with the same reading level L as I have been rated at R i more frequently than at any other rating levels. 1 | k  X  L | assigns higher weight to similar, but not identical, grade levels, since users in grade levels L and L +1, or L and L can usually comprehend the same materials, whereas users with grade levels 1 and 12, respectively cannot. The smaller the difference between two grade levels is, the higher it is weighted in a computed ReS , which is captured by 1 | k  X  L |
Example 5. Figure 6 shows the ratings on 10 books given by a BookCrossing user U . The rating on an unrated book I ,  X  X ar of the Worlds, X  is determined by using the readability levels of the 10 books pre-rated by U . The readability and rating levels of the books show that books with high ( low , respectively) readability levels are often ranked at high ( low , respectively) rating levels 4-5 (1-2, respectively) by U .At the readability level of 11, I is most likely to be recom-mended to U , which is reflected by the ReS s, since ReS for I is the highest at rating level 4, indicating that the read-ability level of I is closer to the readability level of books in S rated at level 4 by U than at any other rating levels.
MudRecS can be utilized differently to make recommen-dations. Each of the utilizations, listed below, can easily be implemented on top of MudRecS, which only requires a user interface to be constructed that handles users X  inputs. (i) When its users provide ratings on different multimedia items, MudRecS can automatically predict the ratings for other items of the same or a different domain in its underly-ing database, an approach currently adapted by Netflix.com. (ii) When a user offers an item, which is either a book that the user has read, a song that (s)he has listened, a movie or painting the user has seen, or a rating on a multimedia item, MudRecS instinctively generates a number of recommenda-tions (with the same or higher rating) on other items. (iii) A user can specify a multimedia item I (s)he likes and a collection of multimedia items of different domains of interest. In response, MudRecS retrieves the items in the collection closed related to I with the highest rating. (iv) Based on the multimedia items that a user U has previously accessed, MudRecS can suggest closely related items, which is similar to a shopping recommendation sys-tem, such as EBay.com and A mazon.com, th at recommends items according to previous purchases.

MudRecS can predict the rating of either a movie, song, book, or painting I that is (not) in the same domain of items that a user U has previously rated using only the title of I .
To assess MudRecS and compare its performance with ex-isting state-of-the-art multimedia recommendation systems, we adopted the 5-fold cross-validation scheme, which is widely-used for evaluating recommendation and IR systems. Each dataset D (introduced in Section 4.1), which is created for the evaluation purpose, is randomly split into five equal-sized, disjoint subsets D k (1  X  k  X  5). For each D k ,Mu-dRecS determines the genre, review, role player, and the readability (of a book) scores of each item in 5 l =1 ,l = k computes the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) (as defined in Section 4.2) on D k . The RMSEs and MAEs of the five iterations are averaged to yield the corresponding mean error value, respectively.
We evaluate the rating predictions on unrated items de-termined by MudRecS and other recommendation systems on four distinct types of multimedia data: movies (videos), music (audios), books (text), and paintings (images). We compare MudRecS against four most-recently proposed mul-timedia recommendation systems (presented in Section 4.3), each of which predicts ratings on one (or more, but not all) of the four types of multimedia data. In fact, none of the ex-isting recommendation systems, as we are aware of, predicts ratings on all four different types of multimedia data. Be-sides the four recommendation systems, we also compare the rating prediction accuracy of MudRecS on movies against 20 other movie recommendation systems participated in the Netflix contest (see details in Section 4.4). Even though we have evaluated MudRecS on text, audio, video, and image documents individually and independently, the evaluation of MudRecS can easily be extended to cover cross-domain multimedia items recommended for a user, assuming that the required genres, reviews, popularity of role players, and readability (for text data), are available. The datasets employed to evaluate (compare, respectively) MudRecS (with others, respectively) on rating prediction are well-known and widely-used, with the exception of paintings. Figure 7: A snapshot of images in a Facebook survey conducted for constructing the paintings dataset
Movies . The MovieLens and Netflix datasets were chosen for evaluating the rating prediction accuracy of MudRecS on movies. The MovieLens (cs.umn.edu/Research/GroupsLens) dataset was created during a 7-month period from Septem-ber 19, 1997 until April 22, 1998 by the developers of Movie-Lens, a popular web-based recommendation system on movies. The Netflix dataset, on the other hand, was collected as part of a contest in 2008 for predicting movie ratings.
Music . The Yahoo! Music Services dataset, which was created by the Yahoo! research team, contains ratings for songs collected from two different sources: the ratings pro-vided by Yahoo! users of the Yahoo! music services in 2006, and the ratings on randomly selected songs collected during an online survey c onducted by Yahoo ! Research in 2006.
Books . The popular BookCrossing dataset [21] was man-ually created between August and September of 2004 with data extracted from BookCrossing.com.

Paintings . To the best of our knowledge, there is no benchmark dataset available for evaluating the performance of a recommendation system on paintings. To verify the accuracy of MudRecS in rating prediction on paintings, we randomly extracted 1,000 paintings on different genres (with reviews) using the Yahoo! image search engine. To obtain user ratings on the 1,000 paintings, we relied on Facebook users. We prepared 10 different Facebook surveys, each of which includes 100 distinct paintings. (A set of 100 paint-ings is an ideal collection, since a larger number of paintings in a survey would overwhelm its appraisers.) Each survey required each involved user to browse through the paintings and provide star ratings (on a 1-5 scale with  X 5 X  being the highest) on 10 or more paintings, using the rating meter di-rectly below each painting. (Figure 7 shows a snapshot of the Facebook application that includes a number of paintings in one of the surveys.) The  X  X ot Interested X  X utton below each painting could be clicked by the user to assign a star rating of  X 1 X  (the lowest rating) to the painting. The surveys were sent out on May 4, 2011 to different Facebook users who were asked to forward the surveys to others. By May 11, 2011, we accumulated all the responses for our study.
Table 3 shows a summary of the multimedia datasets used for evaluating and comparing the performance of MudRecS.
Root Mean Square Error (RMSE) and Mean Absolute Er-ror (MAE) are two performance metrics widely-used for evaluating rating predictions on multimedia data. Both Multimedia #of #of #of Rating Dataset Users Items Ratings Scale MovieLens 943 1,682 100,000 1-5 Netflix 480,000 18,000 100,000,000 1-5 Yahoo! Music 15,400 1,000 354,000 1-5 BookCrossing 278,858 271,379 1,149,780 1-10 FB Paintings 312 964 3,312 1-5 Table 3: Datasets used for evaluating MudRecS RMSE and MAE measure the average magnitude of error , i.e., the average prediction error, on incorrectly assigned rat-ings. The error values computed by RMSE are squared be-fore they are summed and averaged, which yield a relatively high weight to errors of large magnitude, whereas MAE is a linear score, i.e., the absolute values of individual differences in incorrect assignments are weighted equally in the average. RMSE = where n is the total number of items with ratings to be evaluated, f ( x i ) is the rating predicted by a system on item x (1  X  i  X  n ), and y i is an expert-assigned rating to x i
In this section, we detail the recommenders to be com-pared with MudRecS. These recommenders were chosen, since they achieve high accuracy in rating predictions on items in their respective multimedia domains.

MF . Yu et al. [19] and Singh et al. [17] predict ratings on books, movies, songs, and paintings 6 based on matrix fac-torization ( MF ), which clusters items and users by genres so that ratings are predicted according to the rating patterns of certain groups of items on certain genres. MF can also char-acterize both items and users by vectors of features, which are the properties of an item. The matrix with informa-tion on users, genres, and features of an item is decomposed by features and genres, which can be combined to provide rating predictions for any user-genre-feature combination.
Yu et al. propose a non-parametric matrix factorization (NPMF) method, which does not require the model dimen-sionality, i.e., users, features, and ratings, to be specified apriori. Rather, the dimensionality is determined from pre-viously rated items instead. Singh et al. introduce a col-lective matrix factorization (CMF) approach based on rela-tional learning, which predicts unknown values of a relation between entities of a certain item using a given database of entities and observed relations among entities.

ML . Besides the matrix factorization methods, proba-bilistic frameworks have been introduced for rating predic-tions. Shi et al. [16] apply a supervised machine learning (ML) approach to automatically construct a ranking model/ function from training data to predict ratings on movies 7 The ML approach applies a rank-oriented strategy, which
The systems were originally designed to predict ratings on books and movies only but were implemented by us for com-paring their predicted ratings on songs and paintings as well.
The system was originally designed to predict ratings on movies but was implemented by us for additional compar-isons on books , songs ,and paintings as well. exploits pairwise preferences between items and users to gen-erate a list of ranked items corresponding to the ratings such that the higher an item is ranked, the higher its rating.
DM . Su et al. [18] propose uMender, a music recommen-dation system, which predicts ratings on songs by mining musical content and context information. uMender first uti-lizes the perceptual patterns of songs, which consist of the acoustical and temporal features of a song and then clusters users who provide similar ratings to similar songs to detect patterns for acoustics and temporal features, as well as to discover new, implicit, more applicable perceptual patterns. uMender assigns a rating to an item I based on the percep-tual patterns of I and the cluster to which I belongs.
Netflix . We further compare MudRecS against the 20 systems that participated in the Netflix contest in 2008. The open competition was held by Netflix, an online DVD-rental service, and the Netflix Prize was awarded to the best recommendation algorithm with the lowest RMSE score in predicting user ratings on films based on previous ratings. On September 21, 2009, the grand prize of one million dol-lars were given. The RMSE scores achieved by each of the twenty systems, as well as detailed discussions on their rat-ing prediction algorithms, can be found on the Netflix web-site (netflixprize.com//leaderboard).
Figures 8(a), 8(b), and 8(c) show the MAE and RMSE scores of MudRecS and other recommendation systems on the MovieLens, Yahoo! Music, and BookCrossing datasets, respectively. As the RMSE scores indicate, MudRecS signif-icantly outperforms other systems on rating predictions of the respective multimedia data. Regarding the MovieLens and Yahoo! Music datasets, MudRecS achieves a RMSE score of 0.72 and 0.80, respectively, which imply that, on the average, an incorrectly assigned rating by MudRecS is less than 1 star away from its correct assignment. On the BookCrossing dataset, MudRecS achieves an RMSE score of 0.45, which indicates that the error in rating assignment is less than half a star away from the user-/expert-assigned, actual rating. The MAE scores of MudRecS are also lower than the other systems on all the corresponding datasets. On BookCrossing, the MAE scores show that an error in rating predictions is on an average of 0.2 away from the correct prediction, which indicates a very high accuracy in rating predictions on books. Among all the four multime-dia domains, the MAE scores of MudRecS are at least 0.16 lower than the other compared recommendation systems.
AsshowninFigure8(d)on Paintings , MudRecS achieves the highest RMSE and MAE values compared with other recommenders. Comparatively, the RMSE and MAE scores of MudRecS on paintings are the highest (i.e., worst) among the respective scores on other multimedia datasets. This is probably due to the smaller number of features of paintings in their reviews compared to other types of multimedia data.
The small difference between the RMSE and MAE scores of MudRecS, as shown in Figure 8, verifies that the error in incorrect ratings predicted by MudRecS is insignificant, since the larger the errors is, the heavier they are penal-ized by the RMSE measure, which increases the difference between RMSE and MAE.

On the Netflix dataset, MudRecS achieves a RMSE score 8
MAE scores were not computed on the Netflix dataset due to their unavailability for the other 20 recommenders. Figure 8: The MAE and RMSE scores for various recommendation systems on different datasets of 0.8571. MudRecS outperforms 18 recommendation sys-tems and is only outperformed by two systems (Belkor and Ensemble), both of which achieve the same score of 0.8567, a small, insignificant fraction (0.8571 -0.8567 = 0.0004) bet-ter than MudRecS. The reason for the slightly better RMSE score achieved by the two systems on the Netflix dataset are twofold. Unlike MudRecS, Belkor and Ensemble were specif-ically designed for movie rating predictions, and the con-struction of their algorithms focus on rating patterns found in movies which may not apply to other domains. Moreover, Belkor and Ensemble account for temporal effects, i.e., the fact that a user X  X  preference changes over time, which may lead to different ratings for the same movie over time. The temporal effect, however, does not apply to all users and requires a larger subset of training data in order to obtain reliable results, which are the constraints. In considering a 95% confidence interval, MudRecS significantly outperforms 17 recommendation systems and is not significantly outper-formed by any of the twenty systems. CineMatch, Netflix X  X  recommender, achieves an RMSE score of 0.9514 on the Net-flix dataset, which is outperformed by MudRecS.

On the average, MudRecS takes approximately two sec-onds to make recommendations for a user query.
It is appealing to a user who is recommended multimedia items that are closely related to items that (s)he is interested in. For example, if a user U likes the movie  X  X he Pursuit of Happyness, X  it is likely that U is also interested in  X  X even Pounds, X  which shares the same genre (i.e., drama), actor (i.e., Will Smith), and similar reviews. Instead of match-ing the same type of multimedia items, the recommended items, which are either in the form of movies, music, books, and/or pictures, provide more sources of information for U . Offering different types of multimedia items enriches the learning and entertaining experience for the multimedia rec-ommender users, which can be achieved by MudRecS, our proposed multimedia recommendation system.

Given the ratings of a set of multimedia items S provided by a user U , MudRecS predicts the rating of an item I un-rated by U based on the genres , role players , reviews ,andthe comprehensive level of I . Offering different types of multime-dia recommendations, which include movies (videos), songs (audios), books (text), and paintings (image) of interest to a user is the uniqueness and merit of MudRecS. MudRecS does not rely on user data extracted from social websites, ontologies, access patterns, nor training data, which is sim-ple and easy to implement, since it depends only on users X  ratings, genres, role players, reviews, and readability levels of multimedia items that are widely available on the Web.
Experimental results show that MudRecS accurately pre-dicts the rating levels of multimedia items for recommen-dations. The empirical study demonstrates that MudRecS significantly outperforms existing multimedia recommenda-tion systems in making recommendations based on ratings. [1] E. Arnould, L. Price, and G. Zinkhan. Consumers . [2] L. Campos, J. Fernandez-Luna, J. Huete, and [3] H. Chen and A. Chen. A Music Recommendation [4] C. Desrosiers and G. Karypis. Handbook on [5] D. Dunlavy, D. O X  X eary, J. Conroy, and J. Schlesinger. [6] Q. Gu, J. Zhou, and C. Ding. Collaborative Filtering: [7] Z. Guan, C. Wang, J. Bu, C. Chen, K. Yang, D. Cai, [8] J. Koberstein and Y.-K. Ng. Using Word Clusters to [9] Y. Koren. Factorization Meets the Neighborhood: A [10] O. Kwon.  X  X  Know What You Need to Buy X : Context-[11] G. Linden, B. Smith, and J. York. Amazon.com [12] G. Luger. Artificial Intelligence: Structures and [13] T. Noh, S. Park, H. Yoon, S. Lee, and S. Park. An [14] Y. Park and K. Chang. Individual and Group [15] R. Qumsiyeh and Y.-K. Ng. ReadAid: A Robust and [16] Y. Shi, M. Larson, and A. Hanjalic. List-wise Learning [17] A. Singh and G. Gordon. Relational Learning via [18] J. Su, H. Yeh, and P. Tseng. Music Recommendation [19] K.Yu,S.Zhu,J.Lafferty,andY.Gong.FastNonpar-[20] D. Zhang and Y. Dong. Semantic, Hierarchical, Online [21] C. Ziegler, S. McNee, J. Konstan, and G. Lausen.
