 Principal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and has been widely applied on datasets in all scientific domains. We consider, both theoretically and empirically, the topic of unsuper-vised feature selection for PCA, by leveraging algorithms for the so-called Column Subset Selection Problem (CSSP). In words, the CSSP seeks the X  X est X  X ubset of exactly k columns from an m  X  n data matrix A , and has been extensively stud-ied in the Numerical Linear Algebra community. We present a novel two-stage algorithm for the CSSP. From a theoretical perspective, for small to moderate values of k , this algorithm significantly improves upon the best previously-existing re-sults [24, 12] for the CSSP. From an empirical perspective, we evaluate this algorithm as an unsupervised feature selec-tion strategy in three application domains of modern sta-tistical data analysis: finance, document-term data, and ge-netics. We pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner. In all three application domains, we are able to identify k landmark features, i.e., columns of the data matrix, that capture nearly the same amount of information as does the subspace that is spanned by the top k  X  X igenfeatures. X  G.1.3 [ Mathematics of Computing ]: Numerical Analy-sis X  Numerical Linear Algebra ; E.m [ Data ]: Miscellaneous Algorithms, Theory, Experimentation PCA, Subset Selection, Random Sampling
Principal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and it has been widely applied on datasets in all scientific domains, from the social sciences and economics, to biology and chemistry. In words, PCA seeks to map or embed data points from a high dimensional space to a low dimensional space while keeping all the relevant linear structure intact. PCA is an unsuper-vised dimensionality reduction technique. The only input parameters are the coordinates of the data points and the number of dimensions that will be retained in the embed-ding. A rigorous mathematical framework underlies PCA and guarantees strong optimality properties for the result-ing low-dimensional embedding.

We consider, both theoretically and empirically, the topic of unsupervised feature selection for PCA. Standard moti-vations for feature selection include facilitating data visu-alization, reducing training times, avoiding overfitting, and facilitating data understanding. The vast majority of exist-ing work on this topic focuses on supervised feature selection methods. However, in the context of an unsupervised dimen-sionality reduction technique such as PCA, it is only natural to consider unsupervised feature selection algorithms.
We start with a brief description of PCA. Assume that we are given a dataset consisting of m objects, described with respect to n features or, equivalently, an m  X  n matrix A . Let k  X  n be the dimensionality of the space that we seek to embed our data in, and assume that the columns (features) of A are mean-centered. Then, PCA returns the top k left singular vectors of A (an m  X  k matrix U k ) and projects the data on the k -dimensional subspace spanned by the columns of U k . Let P U k = U k U T k be the projector matrix on this subspace. It is well-known [23] that the resulting projection is optimal in the sense that the residual is minimized over all possible k -dimensional subspaces. Here  X  = 2 or F denotes the spectral or Frobenius norm.
We seek efficient, i.e., polynomial in m and n , feature se-lection algorithms that identify, in an unsupervised manner, a subset of exactly k (out of the n ) features, such that if PCA is applied only on these k features, then the resulting embedding is  X  X lose X  to the embedding that emerges when PCA is applied on all n features. To formally define our metric of  X  X loseness X , let C be the m  X  k data matrix that includes only those columns of A that correspond to the chosen features. We measure the error of a feature selection strategy for PCA by comparing the residual for  X  = 2 , F to the optimal residual of eqn. (1). Here P CC + denotes the projector matrix onto the k -dimensional space spanned by the columns of C ( C + denotes the pseu-doinverse of the matrix C ). Equipped with this error mea-sure our problem is equivalent to the so-called Column Sub-set Selection Problem (CSSP) [9]:
Definition 1. Given a matrix A  X  R m  X  n and a positive integer k , pick k columns of A forming a matrix C  X  R m  X  k such that the residual k A  X  P C A k  X  , is minimized over all possible In modern statistical data analysis, selecting actual features from high dimensional data points can sometimes be advan-tageous to selecting linear combination of actual features. For example, recent applied work has focused on selecting in a principled manner actual columns (as opposed to, e.g., the more traditional eigencolumns) from an object-by-feature data matrix. Static and dynamic data analysis in large sparse graphs ([40]), classification of hyperspectral medical data ([29]), analysis of gene expression data ([27, 1]), can all be handled by techniques that are based on feature selection instead of feature extraction.
This work draws the connection between unsupervised feature selection for PCA and the CSSP. The extensive lit-erature on the CSSP in the Numerical Analysis community provides provably accurate algorithms for unsupervised fea-ture selection. To the best of our knowledge there are no algorithms with similar guarantees in the unsupervised fea-ture selection literature. From a theoretical perspective , we present a novel two-stage algorithm for the CSSP that improves on existing results. (See Section 3 for a detailed description of Algorithm 1, our main algorithm for approxi-mating the CSSP.) The following theorem (due to space con-siderations the detailed proof may be found in [6]) bounds the accuracy of our algorithm.

Theorem 1. The randomized two-phase Algorithm 1 takes as input an m  X  n matrix A of rank  X  and a positive integer k , runs in O (min { mn 2 , m 2 n } ) time, and returns as output an m  X  k matrix C consisting of exactly k columns of A such that with probability at least 1  X  10  X  20 : k A  X  P C A k 2  X  O k A  X  P C A k F  X  O P
C = CC + denotes a projection onto the column span of the matrix C , and A k = P U k A denotes the best rank-k approxi-mation to the matrix A .
 Our spectral norm bound provides the first asymptotic im-provement of the best known results for the CSSP since the seminal paper of Gu and Eisenstat [24]. In particular, we improve the result of [24] by a factor of O ( n 1 / 4 ) , assuming k is a small constant. Our Frobenius norm bound improves existing results [12] by a factor of ( k !) 1 / 2 . A novel feature of the algorithm of this paper is that it combines in a non-trivial manner recent algorithmic developments in the the-oretical computer science community with more traditional techniques from the numerical linear algebra community in order to obtain improved bounds for the CSSP.

From an empirical perspective , we evaluate our main algorithm as an unsupervised feature selection strategy for PCA in a range of data sets from three different applica-tion domains in modern statistical data analysis: finance, term-document analysis, and genetics. Our first dataset is a matrix consisting of the prices of the stocks of the S&amp;P 500 index over 1153 days from 2003 through 2007 [44]; our second dataset consists of document-term matrices from the Open Directory Project [32]; and our third dataset comes from the HapMap project [41] and consists of genetic Single Nucleotide Polymorphism data for 90 individuals of Chi-nese and Japanese ancestry. First, we thoroughly evaluate the performance of our algorithm from a numerical perspec-tive when compared to six existing algorithms for the CSSP. All six algorithms come with some provable accuracy guar-antees, and are explicitly designed to optimize the objec-tive function of eqn. (2). Thus, they are the appropriate choice for an experimental comparison instead of the unsu-pervised feature selection algorithms of Section 2 that opti-mize (either provably or heuristically) very different objec-tive functions. In all three cases, our algorithm consistently outperforms existing algorithms with respect to accuracy, while being three to five times slower in terms of running time. We note that accuracy is more important than run-ning time, since feature selection is typically an off-line task. Then, we pay particular attention to how our algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner. In all three application domains, we are able to identify a small number of landmark features, i.e., columns of the data ma-trix, that capture nearly the same amount of information as does the subspace that is spanned by the top k  X  X igenfea-tures. X  In cases where the PCA embedding of the original data to the k dimensional space resulted to, e.g., separa-tion of the data in different classes, then the chosen features could be used to reproduce this separation. For example, for the ODP data, which consist of collections of documents on two different topics (classes) we demonstrate that if a low (say k = 3 or 4 ) dimensional projection via PCA suffices to separate documents from the two different classes, then, us-ing our algorithm for the CSSP, we can select three or four terms that very accurately describe the two topics. Notation. Let [ n ] denote the set { 1 , 2 , . . . , n } . For any matrix A  X  R m  X  n , let A ( i ) , i  X  [ m ] denote the i -th row of A as a row vector, and let A ( j ) , j  X  [ n ] denote the j -th column of A as a column vector. In addition, let k A k 2 F P If A  X  R m  X  n , then the Singular Value Decomposition (SVD) of A can be written as In this expression,  X   X  min { m, n } denotes the rank of A , U
A  X  R m  X   X  is an orthonormal matrix,  X  A is a  X   X   X  diagonal
Year &amp; ref. Authors p ( k, n ) 1965 [22] Golub 1986 [18] Foster 1987 [8] Chan 1992 [25] Hong-Pan 1994 [10] Chan-Hansen 1994 [11] Chand.-Ipsen 1996 [24] Gu-Eisenstat 1998 [5] Bischof-Orti O ( 1999 [34] Pan-Tang O ( 2000 [33] Pan O ( Table 1: Accuracy of deterministic algorithms for the CSSP. The error bound for the algorithm of [22] appeared in [24]. The algorithm of [22] runs in O ( mnk ) time, the algorithms of [18, 8, 10, 33] and the second algorithm of [24] run in O ( mn 2 ) time, and the remaining algorithms either run in O ( n k ) time, or the authors do not provide a running time bound. matrix, and V A  X  R n  X   X  is an orthonormal matrix. Also,  X  k denotes the k  X  k diagonal matrix containing the top k singular values of A ,  X   X   X  k denotes the (  X   X  k )  X  (  X   X  k ) matrix containing the bottom  X   X  k singular values of A , V k denotes the n  X  k matrix whose columns are the top k right singular vectors of A , and V  X   X  k denotes the n  X  (  X   X  k ) matrix whose columns are the bottom  X   X  k right singular vectors of A . Finally, A + = V A  X   X  1 A U T A denotes the pseudoinverse of the matrix A .
 Prior work on the CSSP. Solving the CSSP exactly is a hard combinatorial problem, and thus research has his-torically focused on computing approximate solutions to the CSSP. Since k A  X  A k k  X  provides an immediate lower bound for k A  X  P C A k  X  , for  X  = 2 , F and for any choice of C , a large number of approximation algorithms have been proposed to select a subset of k columns of A such that the resulting matrix C satisfies for some function p ( k, n ) . Within the numerical linear alge-bra community, most of the work on the CSSP has focused on spectral norm bounds (  X  = 2 ) and is related to the so-called Rank Revealing QR (RRQR) factorization [22]. It is straightforward to prove that any algorithm that constructs an RRQR factorization of a matrix A with provable guaran-tees also provides provable guarantees for the CSSP. Table 1 summarizes existing results (see also a survey in [19]).
Within the theoretical computer science community, much work has followed that of Frieze, Kannan, and Vempala [20] on selecting a small subset of representative columns of A , forming a matrix C , such that the projection of A on the subspace spanned by the columns of C is as close to A as pos-sible. Several distinctive features of work in this community are worth noting. First, these algorithms are randomized instead of deterministic, and thus they have a failure prob-ability. Second, these theorems typically focus on bounding the Frobenius norm, and not the spectral norm, of the error matrix A  X  P C A . Third, these algorithms provide a strong tradeoff between the number of selected columns (which is always more than k ) and the desired accuracy of approx-imation. See [15] and references therein for details. The strongest such result states that there exists an algorithm running in O (min { mn 2 , m 2 n } ) time such that holds with probability at least 1  X  10  X  20 , where C contains at most O ( k log k/ X  2 ) columns of A [14, 15].
 Prior work on unsupervised feature selection. [31, 13] propose heuristic approaches for unsupervised feature selection for clustering algorithms. Their goal is to select a subset of features such that clustering using the full set of features and clustering using only the selected subset of features returns similar results. This is relevant to our work, but somewhat different. Notice that we seek to reproduce the structure of the data in the low-dimensional subspace that is computed via PCA without any assumptions on the clusterability of the data. The aforementioned papers use different metrics (e.g., entropy) in order to determine which features to select or, alternatively, wrap the feature selection technique around a clustering algorithm in order to filter out the irrelevant features. [16] presents an expectation-maximization based scheme for unsupervised feature selec-tion that comes with some provable guarantees under as-sumptions. Similarly, [7] models the unsupervised feature selection as an optimization problem and provides heuris-tic solutions. [26] addresses the problem of selecting fea-tures that capture the same information as the top principal components, and proposes a two-step heuristic for this task. More recently, [39, 42, 43, 28] present algorithms based on spectral methods, that, to the best of our understanding, do not come with provable guarantees of the type that we seek here. Finally, [2] presents heuristic ideas that are similar in spirit to our approach and applies them to microarray data.
In this section, we present and describe Algorithm 1, our main algorithm for approximating the solution to the CSSP. Our main quality-of-approximation theorem for this algo-rithm is Theorem 1. Its proof is omitted due to space con-siderations and is available at [6].

Algorithm 1 takes as input an m  X  n matrix A and a rank parameter k . After an initial setup, the algorithm has two phases: a randomized phase and a deterministic phase. In the randomized phase , a randomized procedure is run to se-lect O ( k log k ) columns from the k  X  n matrix V T k , i.e., the transpose of the matrix containing the top-k right singular vectors of A . The columns are chosen by randomly sam-pling according to a judiciously-chosen nonuniform proba-bility distribution that depends on information in the top-k right singular subspace of A . Then, in the deterministic phase , a deterministic procedure is employed to select ex-actly k columns from the O ( k log k ) columns of V T k chosen in the randomized phase. The algorithm then outputs ex-actly k columns of A that correspond to those columns cho-sen from V T k . Theorem 1 states that the projection of A on the subspace spanned by these k columns of A is (up to bounded error) close to the best rank k approximation to A . Algorithm 1 Input: m  X  n matrix A , integer k .
 Output: m  X  k matrix C with k columns of A . 1. Initial setup: 2. Randomized Phase: 3. Deterministic Phase: 4. Repeat the randomized phase and the deterministic
We emphasize here that 40 repetitions are chosen because they suffice in order to provably reduce the failure probabil-ity in Theorem 1 below 10  X  20 . In more detail, Algorithm 1 first computes a probability distribution p 1 , p 2 , . . . , p the columns of A , i.e., over the set { 1 , . . . , n } . The proba-bility distribution depends on information in the top-k right singular subspace of A . In particular, for all j = 1 , . . . , n define and note that p j  X  0 , for all j  X  { 1 , . . . , n } , and that P j =1 p j = 1 . Thus, knowledge of V k , i.e., the n  X  k matrix consisting of the top-k right singular vectors of A suffices to compute the p j  X  X . The running time of our algorithm is dom-inated by the computation of these p j  X  X : O (min { mn 2 , m time suffices for our theoretical analysis. In practice, of course, Lanczos/Arnoldi algorithms could be used to speed up the algorithm.

In the randomized phase , Algorithm 1 employs a random-ized column selection algorithm to choose O ( k log k ) columns from V T k to pass to the second phase. Let c = O ( k log k ) be a positive integer. For each j  X  { 1 , . . . , n } , independently, the algorithm keeps the j -th column of V T k with probabil-ity min { 1 , cp j } . Additionally, if the j -th column is kept, then a scaling factor equal to 1 / well. Thus, at the end of this process, we will be left with  X  c columns of V T k and their corresponding scaling factors. Notice that due to random sampling,  X  c will generally be dif-ferent than c ; however, with high probability, it will not be much larger than c . In order to conveniently represent the  X  c selected columns and the associated scaling factors, we will use the following sampling matrix formalism. First, define an n  X   X  c sampling matrix S 1 as follows: S 1 is initially empty; for all j , in turn, if the j -th column of A is selected by the random sampling process, then e j (an n -vector of all-zeros, except for its j -th entry which is set to one) is appended to S . Next, define the  X  c  X   X  c diagonal rescaling matrix D follows: if the j -th column of A is selected, then a diagonal entry of D 1 is set to 1 / the randomized phase as outputting the matrix V T k S 1 D consisting of a small number of rescaled columns of V T k simply as outputting S 1 and D 1 .

Then, in the deterministic phase , Algorithm 1 employs a deterministic column selection algorithm to the output of the first phase in order to choose exactly k columns from the input matrix A . To do so, theoretically, we run the Algorithm 1 of [33] on the k  X   X  c matrix V T k S 1 D 1 , i.e., the column-scaled version of the columns of V T k chosen in the first phase 1 . Thus, a matrix V k S 1 D 1 S 2 is formed, or equiv-alently, in the sampling matrix formalism described previ-ously, a new matrix S 2 is constructed. Its dimensions are  X  c  X  k , since it selects exactly k columns out of the  X  c columns returned after the end of the randomized phase. The al-gorithm then returns the corresponding k columns of the original matrix A , i.e., after the second stage of the algo-rithm is complete, the m  X  k matrix C = AS 1 S 2 is returned as the final output.
We evaluate our algorithm on datasets from three different domains. Our datasets consist of ( i ) a date-by-stock matrix of the S&amp;P 500 index since 2003; ( ii ) document-by-term matrices from the TechTC collection from the Open Direc-tory Project, and ( iii ) a subject-by-SNP matrix (SNP, pro-nounced snip , stands for Single Nucleotide Polymorphism) from the HapMap project. We will seek a subset of stocks that captures the behavior of S&amp;P 500, a subset of terms that accurately describes the content of the documents, and a subset of SNPs that suffices to classify each individual to an appropriate population of origin, respectively.
We evaluate our hybrid algorithm from two different per-spectives. First, we are interested in the numerical error incurred by our algorithm, as compared to the SVD and existing deterministic approaches for the CSSP. Second, we are interested in interpreting the selected stocks, terms, and SNPs. In summary, the numerical results indicate that our hybrid algorithm consistently outperforms the deterministic methods, while providing easily interpretable low rank ap-proximations with a small additional error when compared to the SVD.

We briefly describe our experimental setup. Given the
Most deterministic algorithms for the CSSP operate on ma-trices that are m  X  n with m  X  n . In our case, in the second stage, we need to apply a deterministic column selection algorithm to a matrix with more columns than rows. Even though, to the best of our understanding, theoretical bounds for most of the algorithms reviewed in Section 2 hold even if m &lt; n , we opt to employ Algorithm 1 and the related Lemma 3.5 of [33] which is explicitly designed to work for m &lt; n . data matrix A , we fix k and compute the spectral norm residual error for A k (via the SVD) and for a fixed deter-ministic method. Then, we run our hybrid algorithm, using the same deterministic method in the second step. We ex-periment with a range of values for the number of columns that are selected in the randomized step and plot the error of the hybrid approach as a function of this number.
We briefly describe the deterministic methods that are used in our experiments. We also provide pointers to pub-licly available software implementing these methods (see ta-ble 2). 1. Pivoted QR: We employed Matlab X  X  qr function. This 2. SPQR: The Semi Pivoted QR (SPQR) method described 3. High RRQR: High RRQR was devised by Chan in [8]. 4. Low RRQR: Low RRQR was proposed by Chan and 5. qrxp: qrxp is the algorithm implemented as the LA-6. qryp: qryp is the algorithm implemented as the LA-The platform used was a 2.0 GHz Pentium IV with 1GB RAM. Our code was implemented in MatLab 7.0.1. In this extended abstract we do not report running times for the different approaches, since they are less interesting than the accuracy results. However, all approaches run in comparable running times, with our approach being slower by small con-stant factors (three to five), mainly due to the 40 repetitions in Algorithm 1.
 Yahoo! provides historical stock prices for the S&amp;P 500 Index [44]. We collected historical prices for the 500 stocks of the index from Jan 2, 2003 to August 1, 2007, a total of 1153 days. Thus we constructed an 1153  X  500 date-by-stock matrix. The ( i, j ) -th element of this matrix represents the value of the j -th stock at the i -th day. We discarded 19 stocks that had missing values for a large number of dates, and we were left with a 1153  X  481 matrix. We normalized this matrix by computing z -scores for each column and we report results on this normalized date-by-stock matrix. The resulting matrix is quite low-rank from a numerical perspec-tive. In particular, we experimented with three different choices for k = 10 , 15 , 20 ; notice that they correspond to approximately 2.5%, 3.5%, and 5% of the non-zero singular values of the matrix. However, the top 10, 15, and 20 singu-lar values capture a significant percentage of the Frobenius norm of the matrix: 89% , 92% , and 94% respectively.
In all three cases, the hybrid method consistently outper-forms the corresponding deterministic method, and in many cases significantly so. See Figures 1 and 2 for a comparison between our hybrid approach and deterministic strategies for k = 10 and 20 ; k = 15 returned intermediate results and is not shown here.

A few remarks are necessary. First, the gains of employing the hybrid approach are more pronounced for the smallest choice of k = 10 . As k grows, the performance of our hy-brid algorithm drops; for example, for k = 20 , our algorithm does not outperform the Low RRQR algorithm. This seems to agree with our theoretical bound, which worsens as k grows. For example, notice that if k is a constant fraction of n , our theoretical result does not outperform the existing bounds for the CSSP. Second, the choice of the number of columns that are selected in the randomized step of our ap-proach is important. Our theoretical result necessitates that O ( k log k ) columns of A are picked in the randomized step On the other hand, the performance of the deterministic step drops if the number of columns picked in the random-ized step increases. Thus, there exists an optimal choice for the parameter c in the randomized step. This optimal choice is  X  asymptotically  X  O ( k log k ) . This manifests itself in the plots: at some point, picking more columns in the random-ized step results to diminished performance. In most cases, setting c to be a small constant times k suffices for accurate approximations. Indeed, a practical implementation of our approach should  X  X xplore X  a small number of choices for c , say c = 2 k up to c = 10 k in increments of k . Third, Low
Pinning down the exact constant in the big-Oh notation seems quite hard given state of the art results for approxi-mate matrix multiplication that are used in our proof. Figure 1: Comparison of our algorithm with four deterministic strategies and the SVD for k = 10 . The y -axis is normalized so that the spectral error of the best rank-k approximation corresponds to one.
 RRQR (despite the lack of strong theoretical guarantees) turns out to be a solid deterministic strategy for the column subset selection problem for small and medium values of k . In particular, our hybrid algorithm only marginally outper-forms it for k = 10 and k = 15 , and does not outperform it for k = 20 .

Our unsupervised column selection methodology picks k stocks that essentially have the same spectral norm residual as the top k left singular vectors. However, it is not easy to assign a meaning to the selected stocks, partly because a natural interpretation of the top k principal components in financial terms is not obvious. In particular, stocks in the S&amp;P 500 index are separated in ten sectors, e.g., Industrial, Health Care, Consumer Discretionary, Financial, Informa-tion Technology, Utilities, Materials, Consumer Staples, En-ergy, and Telecommunication Services. However, when we examined the projection of the stock matrix on its top ten (or up to 20) singular vectors, we were not able to, for ex-ample, match the axes corresponding to singular vectors to sectors. Reifying the principal components of this data ma-trix seemed hard, hence assigning a meaning to the selected stocks is not straightforward. Table 3 shows the stocks that achieved the minimal residual error among all six methods and all different numbers of columns kept in the randomized step. This result was observed using the qrxp method with Figure 2: Comparison of our algorithm with four deterministic strategies and the SVD for k = 20 . The y -axis is normalized so that the spectral error of the best rank-k approximation corresponds to one. c set to 160 for the randomized step. Notice that four stocks from the Industrials sector were picked; this seems to agree with the fact that this particular sector is quite diverse, and its behavior is representative of the S&amp;P 500 as a whole.
Our second data application comes from the Open Di-rectory Project (ODP) [32], a multilingual open content di-rectory of WWW links that is constructed and maintained by a community of volunteer editors. ODP uses a hierar-chical ontology scheme for organizing site listings. List-ings on similar topics are grouped into categories, which can then include smaller subcategories. Gabrilovich and Markovitch constructed a benchmark set of term-document matrices from ODP, called TechTC (Technion Repository of Text Categorization Datasets [21]), which they made pub-licly available. Each matrix of the TechTC dataset consists of a total of 150 to 200 documents from two different ODP categories. The category that each document belongs to was also made available. As expected, the TechTC matrices are not numerically low-rank. The top 2.5%, 3.5%, and 5% of the non-zero singular values of these matrices capture (on average) 5.5%, 8%, and 12.5% of the Frobenius norm of the matrices. This is to be contrasted with the same values for the S&amp;P 500 matrix, where 90% or more of the Frobenius Stock symbol Stock Name Sector TE TECO Energy Utilities RDC Rowan Cos. Energy CTXS Citrix Systems Inf. Tech.
 AFL AFLAC Inc. Financials TER Teradyne Inc. Inf. Tech.
 PCAR PACCAR Inc. Industrials TYC Tyco Industrials CHRW C.H. Robinson Industrials CAT Caterpillar Inc. Industrials SWK Stanley Works Consumer Disc Table 3: The ten stocks that minimized the spectral norm residual for k = 10 . (i) florida , evansville , their, consumer, reports (ii) diego , evansville , pianos, which, services (iii) florida , nanaimo , served, expensive, other (iv) eureka , california , cobbler, which, insurance (v) eureka , reliable, coldwell, rosewood, information (vi) dallas , nanaimo , untitled, buffet, included (vii) nanaimo , taiwan , megahome, great, states (viii) agent , topframe, spacer, order, during (ix) dublin , beach , estate, spacer, which (x) canada , stone , mainframe, spacer, other norm of the matrix was captured with the same ratio of sin-gular values. We note here that we preprocessed all matrices by removing all words with at most four letters.

Despite this fact, we noticed that at least a few of the 100 matrices had the following property: the documents clustered well when projected in a low dimensional space spanned by the top few (e.g., four or five) left singular vec-tors. We empirically measured this property by computing low-rank approximations for all 100 matrices, and then ap-plying k -means to the low-dimensional data. After measur-ing the quality of the resulting clustering when compared with the available ground truth, we focused on ten datasets where unsupervised clustering techniques performed well in predicting the two distinct clusters. This separation im-plies that the documents are semantically well-represented by low-rank approximation via the SVD, even though they are not numerically low-rank. Since our goal is to discuss unsupervised feature selection, we focused only on these ma-trices. Figure 3 shows a projection of one of the ten matrices on its top two left singular vectors. Notice that even in a 2D space some separation of the two classes is obvious. This separation typically peaks when four or five singular vectors are chosen. For simplicity and uniformity we always set the parameter k of our algorithm to five.

The numerical results of our experiments are very simi-lar to the ones for the S&amp; P 500 and are omitted. Once more, we observe that our hybrid method outperforms most of the deterministic strategies 3 . Table 5 shows some statis-tics for the ten document-term matrices that we included
We should note that the available implementations of the High and Low RRQR methods did not run in these datasets. The available code is designed for matrices with fewer columns than rows, and ran out of memory for the TechTC matrices. Figure 3: Projection of documents from the 11346-22294 categories of the TechTC dataset onto the top two left singular vectors. in our study. Table 4 shows the terms that minimized the spectral norm residual using our hybrid algorithm. Obvi-ously, the selected terms correlate reasonably well with the content of each dataset. In particular, in most cases we pick words that describe the content of each cluster of documents represented in the data.
It is well-known that genetic markers can be used to infer population structure and individual ancestry, two tasks that remain central challenges in many areas of genetics such as population genetics and the search of susceptibility genes for common disorders. The Singular Value Decomposition has recently regained favor for uncovering population structure, since it can be efficiently used to extract the fundamental structure of a dataset without the need for any modeling of the data; see [36] and references therein for a detailed discus-sion. SVD and PCA were first used in population genetics by Cavalli-Sforza to infer axes of human variation [30].
Selecting the appropriate genetic markers to study is crit-ical. Single Nucleotide Polymorhisms (SNPs) are the most abundant forms of genetic variation. Each individual car-ries two identical or distinct copies (alleles) for a given SNP. Each copy is one of (at most) two alternate nucleotides that may appear at any given SNP. The HapMap Project [41] has genotyped millions of SNPs across the whole genome for cer-tain populations. Identifying a minimal set of markers that could effectively be used for inference of population struc-ture will reduce genotyping costs. Several approaches have been used to this end; see [35] for a detailed discussion. In all cases, knowledge of individual membership to a studied population is a prerequisite. When studying admixed pop-ulations it may be difficult to define or sample the ancestral populations. The origin of the study individuals may also be unknown in studies involving large samples of blood donors.
In matrix language, our data consisted of n surveyed SNPs for m individuals. In the dataset that we will analyze here, n  X  2 , 000 , 000 and m = 90 . Our 90 individuals come from a Chinese population and a Japanese population. The en-(i) 10567 1 11346 2 139  X  15170 (ii) 10567 1 12121 3 138  X  11859 (iii) 11346 2 22294 4 125  X  14392 (iv) 11498 5 14517 6 125  X  15485 (v) 14517 6 186330 7 130  X  18289 (vi) 20186 8 22294 4 130  X  12708 (vii) 22294 4 25575 9 127  X  10012 (viii) 332386 10 61792 11 159  X  15860 (ix) 61792 11 814096 12 159  X  16066 (x) 85489 13 90753 14 154  X  14780 1 US: Indiana: Evansville 2 US: Florida 3 California: San Diego: Business, economy 4 Canada: British Columbia: Nanaimo 5 California: Politics: Candidates, campaigns 6 US: Arkansas 7 US: Illinois 8 US: Texas: Dallas 9 Asia: Taiwan: Business and Economy 10 Shopping: Vehicles 11 US: California 12 Europe: Ireland: Dublin 13 Canada: Business and Economy: Industries 14 Materials and Supplies: Masonry and Stone Table 5: The 10 TechTC matrices of our study. tries in the m  X  n matrix are +1 (if both alleles are equal to the first nucleotide),  X  1 (if both alleles are equal to the sec-ond nucleotide), or 0 (if the two alleles are different). The task at hand is to identify a small set of SNPs (columns) and/or individuals (rows) that capture the structure of the data, e.g., that suffice to accurately assign an individual to a population of origin. We will seek to do this in an unsu-pervised manner, e.g., by selecting the most representative SNPs without any a priori knowledge of individual ancestry. Once more, the given matrix is not numerically low-rank. In particular, the top 2.5%, 3.5%, and 5% of the non-zero singular values of our matrix capture 3%, 7%, and 13% of its Frobenius norm. Figure 4 shows a projection of our data (after mean-centering) in the top left singular vector of the matrix A . Clearly, the Chinese and Japanese individuals are well separated, with the exception of a single Japanese subject who lies in the middle. Our goal is to find a small subset of SNPs that reproduce this structure using our hy-brid approach. In order to reduce the running time to a few minutes, we modified Algorithm 1 to perform only ten repetitions. Given the huge number of columns, none of the existing codes for deterministic methods could run without heavy modifications.

Since in this case k is equal to one, we are essentially seek-ing a single column that minimizes the residual error when the whole matrix A is projected on the chosen column. This objective is clearly mundane: first, since k is equal to one, we could solve the whole problem exhaustively (albeit in O ( mn 2 ) time with n  X  2 , 000 , 000). Most importantly, the goal of a geneticist is to identify a small set of SNPs (not necessarily one) that suffices to determine the ancestry of an individual. Hence, a much more meaningful evaluation Figure 4: Individuals of Chinese and Japanese an-cestry from HapMap projected on the top singular vector. Figure 5: Classification accuracy using selected SNPs. of our feature selection algorithm is to examine the perfor-mance of the selected SNPs in separating individuals of Chi-nese and Japanese ancestry. Figure 5 shows the performance of k -means applied on a very small (e.g., 40 or 50 out of 2,000,000) set of SNPs selected using our hybrid algorithm. Notice that selecting 40 SNPs and using them to cluster the 90 individuals in two clusters results to about 7 misclassifica-tions when 1,000 SNPs are selected in the randomized phase. Similarly, 50 SNPs result to only 4 misclassifications when 1,500 SNPs are selected in the randomized phase. When compared to the best existing supervised method for the same task (the measure of Informativeness for assignment defined by Rosenberg in [37]), as well as randomly chosen SNPs, we can easily see that our unsupervised algorithm is essentially as good as the best existing supervised method.
It would be particularly interesting to design unsupervised feature selection algorithms for other, non-linear, dimen-sionality reduction methods. In recent years, we witnessed an explosion in the design of non-linear dimensionality reduction algorithms, including Laplacian Eigenmaps, Lo-cally Linear Embedding (LLE), IsoMap, SemiDefinite Em-bedding (SDE), diffusion geometries, etc. In words, dimen-sionality reduction techniques seek to compress the data while keeping all the relevant structure intact. An important open problem is the development of efficient, unsupervised, and provably accurate approaches that select a subset of features from the data such that running the dimensional-ity reduction algorithm only on this subset of features, as opposed to the full set of features, achieves essentially the same reconstruction.
