 Co-clustering attracted much attentions during the past decade, where the task is to perform clustering on two types of int er-connected entities (i.e., rows and columns of a data matrix) simultaneously. Usually, each row of the data matrix represents an object, and each column of the data matrix represents a feature. For example, in the document analysis, the rows and columns of the data matrix correspond to the documents and words. Co-clustering on documents and words simultaneously can achieve better quality than clustering on documents alone. However, most existing co-clustering methods [1] [4] [8] are mainly partition-based, which usually assume that each entry in a data matrix can only be as-signed to one cluster. Some cases of such application scenarios are as follows:  X  Row/Object Overlapping: In many clustering applications, each individ- X  Column/Feature Overlapping: For the scientific paper clustering prob-Such overlapping structures can often appear in a variety of clustering appli-cations. The clustering quality can be greatly improved if the real overlapping structures of both rows and columns are captured. However, the overlapping scenarios make the problem very challenging from a number of aspects:  X  Most existing works [5] [7] [14] on overlapping structures discovery focus on  X  Another challenge is on how to effectively define the overlapping criteria?  X  Traditional co-clustering approaches usually require users to specify how In this paper, we will study the problem of overlapping structures discovery in the context of co-clustering. This is done by first finding the blocks, which have either dense or sparse connections, by non-overlapping co-clustering. Then based on the discovered blocks, we propose a density guided strategy to select the features (objects), which ca n discriminate the specified object (feature) clusters from other object (feature) clusters. Fi nally, according to the discriminative features (objects), a novel overlapping strategy (OPS), which can work with any non-overlapped co-cluster ing methods, is developed.

The rest of the paper is organized as follows. In Section 2, we introduce the related work. The strategy of overlapping co-clustering is elaborated in Section 3. Then, in Section 4 we introduce the co -clustering methods based on MDL, followed by the experimental evaluation in section 5. Finally, we conclude in Section 6. Co-clustering focuses on simultaneously clustering both dimensions of a matrix by exploiting the clear duality between rows and columns [13] [6]. Most works in co-clustering attempt to discover non-o verlapping structures. Chakrabarti et al. [1] assumed the process of co-clustering as the problem of how to transfer the matrix with the least bits. By minimizing the total bits used to describe the matrix, the homogeneous blocks, whose densities are either very high or very low, are discovered. The denser blocks are used as co-clusters. Later, Papadimitriou et al. [15] further extended this method to the hierarchical situation. Long [12] proposed a Spectral Relational Clustering (SRC) approach, which iteratively embeds each type of data objects into low dimensional spaces. Since SRC needs to calculate eigenvectors, it is very time-consuming for large data set. Cheng et al.[2] devised the sequential bi-clustering model that finds one co-cluster, which has low mean squared residue scores in expre ssion data at each time. Later, Lazzeroni et al. [10] proposed a plaid model for directly finding the overlapping co-clusters, but still can not identify multiple co-clusters simultaneously. Deodhar et al. [3] proposed a robust co-clustering algorithm called ROCC, which can work with various distance measures and different co-cluster definitions. However, in order to handle noisy or incoherent data, where a large fraction of the data points and features is irrelevant and needs to be dis carded, ROCC focuses more on pruning. But in this paper, our assumption is that all of the objects and features are useful. In addition, approaches in [2] [3] [10] only focus on the overlapping structures between co-clusters but not among the row clusters and column clusters. Hence, their goals are quite different from our problem. Wang et al. [16] proposed a method similar to k-means by making use of the correlations between users and tags in social media. However, this method is only tailored for social media domain and is ineffective for the gener al case of overlapping structures. An un-weighted bipartite graph G is described by a binary matrix D of m  X  n , in which each element e i,j (1  X  i  X  m, 1  X  j  X  n ) indicates whether the i -th object has a link relation with the j -th feature or not. R represents the set of rows and C represents the set of columns in D . A is the set of co-clustering algorithms which aim at co-clustering the set of rows, i.e., R into k row clusters and the set of columns, i.e., C into l column clusters. We use I denoting the set of row clusters, i.e., I = {I i } k i =1 ,and J denoting the set of column clusters, i.e., J = {J j } l j =1 . Since each row r stands for an object and each column c stands for a feature in matrix D ,weuse r to represent both row and object and c to represent both column and feature in this paper.
 Definition 1 (Pattern). Given an object-feature matrix D of size m  X  n ,as-sume matrix D is to be co-clustered into k row clusters and l column clusters. A pattern M i =( Q i X ,Q i Y ) is a mapping of rows and columns of matrix D respectively, where Q i X denotes the mapping of rows and Q i Y denotes the map-{ 1 , 2 ,  X  X  X  ,l } . M denotes the set of the patterns in D , i.e., M i  X  X  . In order to gather the similar objects into the same row clusters and the similar features into the same column clusters, co-clustering approach A i  X  X  searches the appropriate optimal pattern M  X   X  X  for optimizing a specified objective function as shown in [1] [13]. A regular co-clustering process is given from Figure 1(a) to 1(b). All the discussion in this section is under the assumption that we have already computed a co-clustered matrix D . In other words, given an object-feature matrix D , the co-clustering approach A i  X  X  has already co -clustered different objects into different row clusters and different features into different column clusters.

We notice that, any row (column) cluster becomes an independent row (col-umn) cluster because it has some discriminative feature (object) sets. Before we give the detailed description of discriminative feature (object) set, we give the observations of co-cluster ing process in Figures 1(a) and 1(b). It is clear that there are four row clusters and four colu mn clusters in this example. From Fig-ure 1(b), we notice that, for each row cluster, it certainly has some features that distinguish the row cluster itself from other row clusters. Otherwise, this row cluster will be merged into other row clusters. As shown in Figure 1(c), the first row cluster and the second row cluster are separated from each other because they have different features. In details, in the first row cluster, features in block P 3and P 5 are most important features. In addition, features in P 5canbemore discriminative than those features in P 3 since other row clusters have much lower densities for features in P 5. Similarly, features in P 1 are more discrimina-tive than features in P 2 for the second row cluster in terms of separating from other row clusters. Symmetrically, f or column clusters, objects located in P 1 are more important than objects located in P 3 to discriminate the first column cluster from other column cluste rs. Compared to objects located in P 3, objects located in P 4 contribute more for discriminating the second column cluster from other column clusters.
For any row cluster I p  X  X  (1  X  p  X  k ), in order to measure the importance of the features in column cluster J s  X  X  (1  X  s  X  l ) for distinguishing row clus-ter I p from other row clusters, the difference of density between the block D p,s and average density of all blocks in the s -thcolumngroupofmatrix D should be considered. This is referred to as the density guided principle for discriminative features (objects) identification. We give the Discriminative Feature Function w ( p,s ) to evaluate the contribution of column cluster J s for separating row cluster I p from other row clusters as follow: where N ( D p,s ) is the density function which measures the percentage of  X 1 X  X  in block D p,s . Obviously, the larger value of w ( p,s ), the more contribution of features in J s for discriminating row cluster I p from other row clusters. Symmet-rically, we further define the Discriminative Object Function w ( s,p ) to evaluate the contribution of row cluster I p for distinguishing column cluster J s from other column clusters below. Definition 2 (Discriminative Feature Set). Given the row cluster I p  X  X  and the column cluster J s  X  X  , the group of features located in the column cluster J s is the discriminative feature set for the row cluster the distinction of row cluster I p from other row cluster I q  X  X  ( p = q ) , i.e., w ( p,s )  X  0 .
 Definition 3 (Discriminative Object Set). Given the column cluster J s  X  J and the row cluster I is the discriminative object set for the column cluster J s iff I p contributes to the distinction of column cluster J s from other column cluster J t  X  X  ( s = t ) , i.e., w ( s,p )  X  0 .

Given an object r  X  X  q , when we consider its relation with row cluster I p ( p = q ), we examine its features shared with the objects in I p . Concretely, the more discriminative feature sets they shared, the closer relation they are. Moreover, for a specified discriminative feature set in I p , if an object r has a higher feature density in this discriminative feature set, it indicates the closer relation between object r and objects in I p .Consequently,foranyrow r  X  X  q , in order to test whether row r should also be placed into row cluster I p ( p = q ) or not, all the discriminative feature sets in p -th row group are considered by Equation (3). where r f is the set of elements from row r locatedincolumncluster J f ; F is index set of discriminative feature set of column cluster, i.e., F = { f | w ( p,f )  X  0 ,
J the right hand side of Equation (3), the first term in the summation indicates the significance of the discriminative feature set to the p -th row group, while the second term measures the density difference of row r relative to the p -th row group. Intuitively, the larger of their product, the more likely that row r will be related to the p -th row group. We take the sum of the products over all discriminative feature sets of the p -th row group as the measure. If E or ( r, p )  X  0, row r will not only be placed into its original row cluster I q but also to row cluster I . We notice that, it is possible for ( N ( r f )  X  N ( D p,f )) to be negative. If in this case, it means row r has a lower feature density located in column cluster J s than I p . Consequently, the possibility of placing row r into row cluster I p is penalized.

Similarly, we have Equation (4) for evaluating any column c  X  X  t whether should also be placed into column cluster J s ( s = t ).
 where c b is the set of elements from column c located in row cluster I b ; B is the index set of discriminative object set of row cluster, i.e., B = { b | w ( s,b )  X  0 ,
I description of Overlapping Pattern Search (OPS) for overlapping co-clustering is given in Algorithm 1. We note that the order of step 2 and step 3 does not matter, since going through the rows and columns are solely based on the appropriate optimal non-overlapping patterns. In other words, step 2 and step 3 are independent. Besides,  X  =  X  = 0 is used in this paper.
 Algorithm 1. OPS ( A i ,D ) We have presented a general framework for overlapping co-clustering based on non-overlapping co-clustering in the last section. In this section, we give the co-clustering approach used in this paper for generating the non-overlapping row clusters and column clusters. Although the overlapping framework described in Section 3 can work with any non-overlapping co-clustering method, here we further extend FACA [1] to generate the non-overlapping co-clusters because it can be parameter free and generate g ood quality results. Due to the space limit, we first briefly introduce the process of FACA. Then we explain how we extend it further to be more efficient and effective. Finding the optimal co-clustering pattern is NP-hard [1]. In order to find the appropriate optimal pattern M  X  , FACA makes use of Minimum Description Length (MDL) theory to encode matrix without information loss. Assume a matrix D with m  X  n is divided into k row clusters and l column clusters. Compressing matrix D includes two parts which are description complexity T m ( D )and code length T c ( D ). Therefore, the total bits used for condensing matrix D is where N h ( D ij ) denotes the number of  X  X  X  X  (h=0 or 1) in block D ij , m i denotes the number of objects in row cluster I i ,and n j denotes the number of features in column cluster J j . All the logarithms are based 2 in Equation (5). Besides, in Equation (5), the first term to the ninth term represents the description com-plexity , and the tenth term represents the code length . The original algorithms only used the tenth term as the objective function. The detailed description of FACA can be found in [1]. As we will see in the experimental section, by retain-ing all these terms to capture the effect of both description complexity and code length, we can achieve better clustering quality. In this section, we test our method on bot h synthetic and real-world data sets. Each experiment is repeated 10 times and the average is reported. We use two metrics to measure the performance. The first metric used in this paper is Purity and the second one is Normalized Mutual Information (NMI) [9]. 5.1 Data Set Description Synthetic Data Set. We generate the synthetic data based on Classic3 1 .Clas-sic3 data set contains three types of non-overlapping documents, which are MEDLINE (medical journals), CISI (information retrieval) and CRANFIELD (aero-dynamics). The documents and words form a bipartite graph described by a binary matrix of 3891  X  5896. In order to get the overlapping documents, firstly, we randomly select 1000 documents from each type of documents. Secondly, we randomly choose two documents d i and d j , which belong to two different types T , T j ( i = j ), from the total 3000 documents. Thirdly, we merge documents d j and d i together to form a new document d ij , which is tagged with two types T i and T j . The above processes repeat until the total specified overlapping percent-age OV % of new documents are generated.
 Real-World Data Sets. In addition to the synthetic data set, we use two real-world data sets to test the proposed method. The first real data set is Reuter data set 2 , which contains 294 documents. Each document records a story happened from February 2009 to April 2009. Among these stories, 40 stories are tagged more than one type of the total six types, which are business , entertainment , health , politics , sport and technology . The second real data set is BBC data set 3 , which also contains six types of documents as the Reuter data set. BBC data set contains the total number of 352 documents and 40 documents are annotated with more than one type.

We notice that the standard text preprocessing approaches such as stemming, stop words removal have already been applied to all of these data sets. 5.2 Experiment Results We compare our method with four state-of-the-art methods. The first one is FACA [1]. The second compared method is NMF [11], which is a co-clustering method based on non-negative matrix factorization. The third compared method is SRC [12] and the fourth one is DOGSM [16]. We note that FACA, NMF and SAC can detect effective row clusters an d column clusters but cannot discover the overlapping structures. While DOGSM can detect the overlapping structures of row clusters and column clusters, but the number of row clusters and column clusters are limited to be exactly the same because DOGSM is a k-means based method.

We first consider the case that the numb er of row clusters and column clusters are presumably given. In Classic3 data set, the number of row cluster k =3is given and the number of column cluster l is unknown. Hence, different values of l =15 , 20 , 25 are provided to all of these compared methods. In this situation, OPS calls extended FACA for non-overlapping co-clustering. Besides, DOGSM is a clustering method similar to k-means, it has no parameter l .Themetrical scores on Purity and NMI on Classic3 data set are given in Tables 1 and 2 re-spectively. We observe that, generally speaking, FACA, NMF, SRC and OPS achieve comparative scores over two met rics on Classic3 data set. In detail, OPS outperforms all of the compared methods. Especially, OPS takes more advan-tage than the other compared methods a s the number of overlapping percentage OV % increases. This is because, comp ared to FACA, NMF and SRC, OPS can discover the overlapping structures which can further reveal the cluster structure of the data set. Even though DOGSM can also discover the overlapping clusters, its performance is not as good as OPS. This is because it can not distinguish the different number of row clusters and c olumn clusters. Besides, compared to other three methods, DOGSM can not make use of the relations between objects and features when performing clustering. This is critical for the performance of clustering when the data is very spars e and noisy. Therefore, OPS gains much better scores on two metrics than DOGSM at different overlapping levels.
Another observation of Tables 1  X  2 is that the metrical scores of two met-rics on the compared methods decrea se as the percentage of overlapping OV % increasing. The probable reasons for this phenomenon are as follows. Firstly, the higher overlapping percentage of the documents makes the data set more com-plicated and challenging to all of the compared methods. Secondly, the number of hybrid objects, which belong to two clus ters, are increasing as the overlapping percentage getting higher. This makes the number of hybrid objects of the same type, such as hybrid objects of LINE and CISI, enough to form new independent clusters. In other words, the ground truth of the number of the row clusters is moving from 3 to 6 as the number of overlapping documents increasing. How-ever, OPS still performs very well even the overlapping percentage OV % = 20%.
In the real data sets of Reuter and BBC, the number of row clusters k = 6 is given, but the number of column clusters is unknown. In order to test the ability of automatically finding the number of row clusters k and column clusters l , we run OPS without given the number of row clusters and column clusters. Since FACA can also automatic ally detect the number of row clusters and column clusters when searching for the optimal pattern, we use FACA(Auto) to denote this situation. While for the other compared methods, we provide exactly the number of row clusters and different parameters for the number of column clusters.

The results of the different methods on Reuter data set are presented in Fig-ures 2(a)-2(b). Since the results of OPS and FACA(Auto) are not affected by the number of column clusters, their results a re horizontal lines over different values of l . Despite without any information of t he number of row clusters and column clusters, OPS still gains the highest scor es over all of the three metrics. It is ev-ident that OPS has better advantage than other compared methods for finding the most appropriate row clusters and column clusters. Though FACA(Auto) can also detect the number of row clusters and column clusters automatically, its performance is not as good as OPS. That is because of the following reasons. Firstly, OPS can discover the overlapping structures hidden among the clusters. Secondly, OPS uses the total bits used to describe the whole matrix as the objec-tive function, which can get an appropriate balance between model description complexity and code length, and improve the co-clustering quality. We keep in mind that OPS automatically detects the number of row clusters and column clusters. We also notice that FACA(Auto) performs better than FACA in this data set. Besides, SRC outperforms NMF in most of the cases. Though DOGSM can also discover the overlapping structur e, it seems very sensitive to the sparsity and noise of the data set. Hence, the performance of DOGSM is relative poor in our tests.

In Figures 2(c)-2(d), we illustrate the results of the compared methods on BBC data set. Once again, OPS gains the highest scores on two different metrics. We note that FACA(Auto) does much poorly on NMI compared to OPS. Moreover, NMF and SRC do poorly on Purity. We observe that the NMI scores of all of the compared methods are not very high. We carefully analyze this phenomenon and find BBC data set is very unbalanced. For example, the number of document annotated as sports is 44, while the number of documents annotated as business is 102, which is more than two times the number of documents annotated as sports . Besides, compared to the Classic3 data set, the number of documents is relatively small, but the number of document clusters is relatively large in this data set. Both of which make the co-clustering in BBC data set a non-trivial challenge for all of these compared me thods. However, even in this case, OPS still gains a comparative performance. Discovering the overlapping structures of objects and features simultaneously is significant in many real-world applications. However, this problem is neglected by many existing works. In this paper, a novel parameter-free algorithm OPS, which utilize a density guided principle to discover the overlapping structures among row clusters and column clusters simultaneously, is proposed. Experiments in-cluding real-world and synthetic data sets demonstrate our method is effective and efficient. Further works will focus on non-binary matric co-clustering. Acknowledgements. This work is supported in part by National Natural Sci-ence Foundation of China through gr ants 61271252, NSF through grants CNS-1115234, DBI-0960443, and OISE-1129076.

