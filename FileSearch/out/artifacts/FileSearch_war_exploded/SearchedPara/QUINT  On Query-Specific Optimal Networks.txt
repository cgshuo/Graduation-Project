 Measuring node proximity on large scale networks is a funda-mental building block in many application domains, ranging from computer vision, e-commerce, social networks, software engineering, disaster management to biology and epidemi-ology. The state of the art (e.g., random walk based meth-ods) typically assumes the input network is given a priori, with the known network topology and the associated edge weights. A few recent works aim to further infer the optimal edge weights based on the side information.

This paper generalizes the challenge in multiple dimen-sions, aiming to learn optimal networks for node proximity measures. First ( optimization scope ), our proposed formu-lation explores a much larger parameter space, so that it is able to simultaneously infer the optimal network topology and the associated edge weights. This is important as a noisy or missing edge could greatly mislead the network node prox-imity measures. Second ( optimization granularity ), while all the existing works assume one common optimal network, be it given as the input or learned by the algorithms, exists for all queries, our method performs optimization at a much finer granularity, essentially being able to infer an optimal network that is specific to a given query. Third ( optimiza-tion efficiency ), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user pref-erence set. We perform extensive empirical evaluations on a diverse set of 10+ real networks, which show that the pro-posed algorithms (1) consistently outperform the existing methods on all six commonly used metrics; (2) empirically scale sub-linearly to billion-scale networks and (3) respond in a fraction of a second.
 Node proximity; Optimal networks
Measuring node proximity (i.e., similarity/relevance) on large scale networks is a fundamental building block in many application domains, ranging from computer vision [23], e-commerce [10, 9], social networks [28, 29], software engi-neering [27], disaster management [41] to biology [22] and epidemiology [31].

The state of the art has mainly focused on best lever-aging the network topology for measuring node proximity. Among others, a prevalent choice for node proximity is ran-dom walk based methods (e.g., random walk with restart and many of its variants -see Section 5 for a review), largely due to its flexibility in summarizing multiple weighted rela-tionships between nodes. These methods typically assume the input network is given a priori, with the known , whether static or dynamic, network topology and the associated edge weights. A few recent works aim to further infer the opti-mal edge weights based on the side information (e.g., user feedback, node/edge attribute information). Representative works include supervised random walk (SRW) [5] and learn-ing to rank methods [2, 3]. Moreover, an implicit assumption behind these existing works is that one common (optimal) network, be it given a priori or learned by the algorithms, exists for all queries. Despite much progress has been made, several algorithmic questions have largely remained nascent.
Q1 Optimal weights or optimal topology? Both supervised
Q2 One-fits-all, or one-fits-one? Most, if not all, of the
Q3 Offline learning or online learning? Even if we re-
We aim to answer all these questions in this paper. First ( for Q1 -optimizaiton scope ), our proposed formulation ex-plores a much larger parameter space, so that it is able to si-multaneously infer the optimal network topology and the as-sociated edge weights. Second ( for Q2 -optimization granu-larity ), while all the existing works assume there is one com-mon optimal network for all queries, our method performs optimization at a much finer granularity, essentially being able to infer a (different) optimal network that is specific to a given query. Third ( for Q3 -optimization efficiency ), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user preference sets, which is often sub-linear wrt the size of the input network. Our main contributions can be summarized as follows:
The rest of the paper is organized as follows. Section 2 formally defines the query-specific optimal network learn-ing problem. Section 3 introduces the proposed algorithms. Section 4 presents the empirical evaluation results. After reviewing related work in Section 5, we conclude the paper in Section 6.
In this section, we present the notations used through-out the paper (summarized in Table 1), formally define the query-specific optimal network learning problem and then give preliminaries on random walk based methods for node proximity.
We use bold upper-case letters for matrices (e.g., A ), bold lowercase letters for vectors (e.g., v ), and lowercase letters (e.g.,  X  ) for scalars. For matrix indexing, we use a conven-tion similar to Matlab as follows. We use A ( i,j ) to denote the entry at the intersection of the i -th row and j -th column of matrix A , A ( i, :) to denote the i -th row of A and A (: ,j ) to denote the j -th column of A . Besides, we use prime for matrix transpose (e.g., A 0 is the transpose of A ).
In our problem setting, we are given a network which is represented by an n  X  n normalized adjacency matrix, which has m non-zero elements (i.e., edges). As mentioned ear-lier, there is often rich information from user provided feed-back/preference in some applications [32, 3]. For a user s , s/he could explicitly indicate some nodes that s/he wants to connect with, defined as positive nodes , and some other nodes that s/he wants to avoid, defined as negative nodes . We use the positive set P = { x 1 ,x 2 ,...,x p } to denote the set of positive nodes, i.e., s likes nodes x i . Similarly, we use the negative set N = { y 1 ,y 2 ,...,y l } to denote the set of negative nodes, i.e., s dislikes y i . See [5, 32] on how to select the positive and negative sets. Our goal is to learn an optimal network for this specific query node s , so that, when measured on the learned network , the proximities from s to the nodes in P and those in N match his/her preference. With these notations, the problem can be formally defined as follows: Problem 1. Query-specific Optimal Network Learning Given: a network with adjacency matrix A , a query node Learn: an optimal network A s specific to the query s .
Random walk based methods (such as random walk with restart [30] and many of its variants) have been a prevalent choice for proximity measures. Here, we present a brief sum-marization of random walk with restart (RWR), which is the base of our proposed methods. Please refer to Section 5 for detailed review of node proximity measures. For a given net-work G , RWR works as follows. Consider a random surfer that starts from the node s . At each step, the random surfer has two options: (1) transmits to one of its neighbors with probability proportional to the edge weights; or (2) jumps back to the starting node s with a restart probability (1  X  c ). The proximity score from node s to node i is defined as the steady state probability r si that the random surfer will visit node i . Define the ranking vector r s for node s as the vec-tor of proximity scores from node s to all the nodes on the network, RWR recursively computes the ranking vector as follows: where A is the adjacency matrix of the network G and e s is a vector of all zeros except 1 at the s -th position.
The ranking vector r s can also be computed in the fol-lowing closed form: r s = (1  X  c )( I  X  c A )  X  1 e s . Let Q = ( I  X  c A )  X  1 , we can see that r s is the s -th column of Q with some constant scaling. In other words, we can regard the element Q ( i,j ) as the (scaled) proximity score from node j to node i . For the normalized matrix A , it is often chosen as the stochastic matrix of the input network. Nonetheless, from the algorithmic perspective, we can also choose other forms to normalize A , e.g., the so-called normalized graph laplacian [42]. In fact, as long as the leading eigenvalue of A is less than 1 /c , we can show that Eq. (1) always converges to its closed-form solution. Having this in mind, we will re-move this constraint (i.e., A being a stochastic matrix) to simplify the proposed algorithms. We will also discuss how to impose such a constraint if the stochastic matrix is indeed a desired output in some applications.
In this section, we present our algorithm, QUINT , to learn a QU ery-specific optI mal N eT work (i.e. Problem 1). We first introduce the proposed formulations and give optimiza-tion solutions, followed up with scalable algorithms and some variants.
Given the input network with the adjacency matrix A , and a query node s along with its associated positive node set P and negative node set N , we want to learn an optimal network with adjacency matrix A s such that the proxim-ity from s to positive nodes in P and negative nodes in N matches the preference. The key ideas behind our proposed formulations can be summarized as follows. First, we want to avoid that the learned network A s deviates too far from the input network A . Second, on the learned network A s the measured proximity should match the user preference. That is, if we compute the ranking vector r s for s using the learned adjacency matrix A s , the proximity from s to any positive node in P is greater than to any negative node in intuition, we propose the following formulation to learn a query-specific network: where Q = ( I  X  c A s )  X  1 contains the pairwise node prox-imities on the learned network. In Eq. (2), the objective function states that the distance from the learned network adjacency matrix A s to the original network A measured by Frobenuis norm should be minimized; while in the con-straint, we want the proximity from node s to any positive node in P to be greater than to any negative node in N . This is a hard constraint as we do not allow any exception. In practice, the constraint might not be satisfied by all the possible pairs of positive and negative nodes. Instead, we can relax the constraint and introduce some penalization if violations occur. This relaxed soft version can be formulated as follows: arg min where  X  is the trade-off parameter that balances between the adjacency matrices difference and constraint violations. To penalize the constraint violations, we introduce the loss Q ( y,s ) &lt; Q ( x,s ), i.e., the constraint is not violated, then penalize such a violation. In the paper, we consider the Wilcoxon-Mann-Whitney (WMW) loss [35] with width b , which is differentiable and was originally proposed to opti-mize the area under the ROC curve (AUC):
Remarks: The formulation in Eq. (3) bears a high-level resemblance to the supervised random walks (SRW) [5] in terms of the way they encode the user preference. Nonethe-less, there are several subtle differences between them. First, we explore a much larger parameter space in the order of O ( n 2 ), whereas SRW only searches in a d -dimensional vec-tor space, where d is the length of the feature vector. The potential benefit is that we are able to search for both the optimal topology and the associated edge weights. Second, our formulation is tailored for a specific query node, and thus is potentially able to learn an optimal network for that specific query (instead of one universal network for all the queries). Third, in our formulation, we drop the typical con-straint that requires the learned network A s to be a stochas-tic matrix. We find that such a relaxation will greatly ease the optimization algorithm, without a noticeable empirical performance degradation. For readers who are interested in keeping this constraint in our formulation, we will present a variant to do so in Section 3.4.3.
The objective function in Eq. (3) is non-convex. We aim to solve it using gradient descent based methods by first calculating the derivative of L ( A s ) w.r.t. A s and then up-dating the adjacency matrix A s along the negative direction of the derivative.

The derivative of L ( A s ) w.r.t. A s can be written as fol-lows: where we denote d yx = Q ( y,s )  X  Q ( x,s ) and we apply chain rule in the second step. The WMW loss function is differentiable and its derivative is computed as:  X  X  ( d yx g ( d yx )(1  X  g ( d yx )) . To compute the derivative of is more involved, since Q is an inverse of a matrix, which itself is a function of A s .

From the basic identity for derivatives of a matrix in-verse [24], we have that: where 1  X  i  X  n , 1  X  j  X  n , and J i,j is a single-entry matrix with J ( i,j ) = 1 and zeros everywhere else.

Based on the above equation, we have that
The intuition behind the formulation is that the update to A s ( i,j ) is proportional to the product of the proximity from i to x and the proximity from s to j . In other words, if i is a close neighbor of x (i.e., large Q ( x,i )) and j is a close neighbor of s (i.e., large Q ( j,s )), as illustrated in Figure 1, then A s ( i,j ) will have a relatively large update. This makes sense since a larger increase on A s ( i,j ) (i.e., a pair of close neighbors of the query node s and a positive node x , respectively) will increase the chance of reaching x from s by random walks (i.e., increasing Q ( x,s ) measured on the updated adjacency matrix A s ).

Following this, we can compute the derivative of  X  Q ( x,s ) as:
Therefore, the derivative of L ( A s ) w.r.t. A s can be rewrit-ten as follows:
The above optimization solution for the query-specific net-work learning is summarized in Algorithm 1. From the al-gorithm we can see that, we not only update the weights of existing edges, but might also add an unseen edge and/or remove a noisy edge during the update (i.e., changing the original topology).
 Algorithm 1 QUINT  X  Learning a Query-Specific Optimal Network Input: (1) the initial network adjacency matrix A , Output: The adjacency matrix A s of the optimal network 1: Initialize A s = A ; 2: while not converged do 3: for each positive node x from P do 4: for each negative node y from N do 5: Compute Q (: ,s ), Q ( y, :) and Q ( x, :); 7: end for 8: end for 11: end while 12: return the learned network adjacency matrix A s ; We summarize complexities of Algorithm 1 in Theorem 1. Theorem 1. (Time and Space Complexities of Algorithm 1). Algorithm 1 takes O ( T 1 |P| X |N| ( T 2 m + n 2 )) time, where T the number of iterations to convergence, and T 2 is the num-ber of iterations in the power method for computing ranking vector. It takes additional O ( n 2 ) space.

Proof. The inner loop, line 5 and line 6, is executed |P| X |N| times. For line 5, it would be too expensive ( O ( n to first compute Q = ( I  X  c A s )  X  1 . Instead, we can use power method to extract the corresponding column of Q by Eq. (1) and this line would take O ( T 2 m ) time, where T is the number of iterations in the power method and m is the number of edges. Line 6 involves a multiplication of a column vector and a row vector, which would take O ( n 2 ). As a result, the overall time complexity is O ( T 1 |P| X |N| ( T n )), where T 1 is the number of iterations to convergence. The two vectors in the multiplication are dense and their multiplication would result in an n  X  n dense matrix, taking O ( n 2 ) additional space.
To learn an optimal network for one query using Algo-rithm 1, it would take O ( T 1 |P| X |N| ( T 2 m + n 2 )), which is not scalable to large real network data (usually in the order of millions or billions of nodes/edges), not to mention that it practically eliminates the possibility to conduct learning in on-line stage. In this subsection, we introduce an effec-tive on-line algorithm to learn the optimal network for each query. The key idea behind the fast solution is that the op-timal network can be approximated by a rank-one pertur-bation to the original network. If we observe more closely at the derivative of L ( A s ) w.r.t. A s in Eq. (9), the second term (the summation over x and y ) is exactly a matrix of rank one for a specific query.

Following the rank-one perturbation assumption, we can approximate the optimal network as A s = A + fg 0 , where f and g are n -dimensional vectors we want to learn. Having this, we can formulate our optimal network learning with rank-one perturbation as follows: where  X  and  X  are the trade-off parameters. Notice that in this formulation, Q = ( I  X  c A  X  c fg 0 )  X  1 is a function of both f and g .

We apply an alternating strategy to solve the above for-mulation. Let us first fix g and solve for f . The derivative of L ( f ) w.r.t. f can be written as follows: where we denote d yx = Q ( y,s )  X  Q ( x,s ). The question now becomes how to compute the derivative  X  Q ( x,s )  X  f .
Again, according to the basic identity for derivative of a matrix inverse, we have the following:
Following this, we obtain
Now, we can compute the derivative  X  Q ( x,s )  X  f as follows:
Therefore, the derivative of L ( f ) w.r.t. f can be computed as follows:
The computation for the derivative of L ( g ) w.r.t. g is sim-ilar to Eq. (15) with f substituted with g and g substituted with f .

We summarize the above optimization solution for optimal network learning with rank-one perturbation in Algorithm 2, along with its complexity analysis in Theorem 2.
 Theorem 2. (Time and Space Complexities of Algorithm 2). Algorithm 2 takes O ( T 1 |P| X |N| ( T 2 m + n )) time, where T Algorithm 2 QUINT -rankOne  X  Learning a Query-Specific Optimal Network Input: (1) the initial network adjacency matrix A , Output: The rank-one perturbation to the network f and 1: Initialize f and g ; 2: while not converged do 3: for each positive node x from P do 4: for each negative node y from N do 5: Compute Q (: ,s ), Q ( y, :) and Q ( x, :); 7: end for 8: end for 9: Compute the derivative  X  L ( f )  X  f by Eq (15); 13: end while 14: return the learned rank-one perturbations f and g ; the number of iterations to convergence, and T 2 is the num-ber of iterations in the power method for computing ranking vector. It takes additional O ( n ) space.

Proof. The inner loop, line 5 and line 6, is executed |P| X  |N| times. For line 5, we can use power method to extract the corresponding column of Q by Eq. (1) and this line would take O ( T 2 m ) time, where T 2 is the number of iterations in the power method. Line 6 takes O ( n ). As a result, the overall time complexity is O ( T 1 |P| X |N| ( T 2 m + n ), where T is the number of iterations to convergence. The additional space takes O ( n ), i.e., the length of the vectors.
Remarks: The major computational overhead of Algo-rithm 2 comes from line 5 to extract certain columns of Q , which leads to an O ( T 2 m ) complexity. In the next sub-section, we propose additional ways to further speed up the algorithm to scale linearly and even sub-linearly on n .
In this subsection, we first provide several ways to further speed up the proposed algorithm to scale linearly and even sub-linearly; and then point out a way to satisfy stochastic-ity of the output and to learn the decay factor c in RWR. As we mentioned above, the major time complexity of Algorithm 2 is due to the extraction of certain columns of Q using the power method. In fact, we could approximate Q using Taylor approximation as follows: If we use first order Taylor approximation, i.e. k = 1 in Eq. (16), line 5 in Algorithm 2 would only take O ( n ) in-stead of O ( T 2 m ). Therefore, the overall time complexity of Algorithm 2 can be reduced to O ( T 1 |P| X |N| n ).
It is often not necessary to update the global adjacency matrix, instead, we could focus on certain local zones in the network that will play a more important role to the proximities from the query node s .

In particular, we would update the local structure in the neighborhood of the query node as well as the neighbor-hood of the preference sets. Denote the neighborhood of node s (including s ) by N ( s ) = { z | ( z,s )  X  E } X  X  s } and de-note the neighborhood of the positive and negative nodes by N ( P , N ) = { z | z  X  N ( x ) or z  X  N ( y ) ,  X  x  X  P ,  X  y  X  N} . When we update f using Eq. (15), we only need to update f ( i ) ,i  X  N ( s ). Similarly, when we update g , we only need to update g ( i ) ,i  X  N ( P , N ). This will further bring down the time complexity of Algorithm 2 to O ( T 1 |P| X  |N| max( | N ( s ) | , | N ( P , N ) | )) (Theorem 3). Theorem 3. (Time Complexity of QUINT -rankOne with First-order Taylor Approximation and Localized Rank-One Perturbation.) If we use first-order Taylor approximation for Q and localized rank-one perturbation in QUINT -rankOne, the optimal network.

Proof. Omitted for brevity.
As mentioned earlier, we do not require the learned net-work A s to be a stochastic matrix, which largely eases the optimization process. If a stochastic matrix is indeed the de-sired output network, we can naturally modify the proposed algorithms to fulfill it. To be specific, immediately follow-ing the gradient descent step in the proposed algorithms, we introduce a simplex projection operation for each column of A s [33]. In this way, we ensure that matrix A s is always a valid stochastic matrix. However, we do not observe a noticeable empirical improvement by the simplex projection operation, yet it introduces an additional O ( n 2 log n ) (since we need to do so for n columns) into the overall time com-plexity. Therefore, we do not recommend it in practice.
An important parameter in RWR is the decay factor c , which is usually manually set. As a side product, our pro-posed methods naturally provide a way to learn the param-eter c . In this setting, we assume the network structure A is known and fixed, and we have the following optimization formulation:
The key to the above optimization problem is to calculate the derivative of L ( c ) w.r.t. c . We have that
Again, we have the following identity
Following this, we can get
The derivative of L ( c ) w.r.t. c becomes
We omit the detailed algorithm description for learning the parameter c due to the space limit.
In this section, we design and conduct experiments mainly to answer the following questions:
We test our algorithms on a diverse set of real-world net-work datasets, including collaboration networks, social net-works, infrastructure networks, etc. The statistics of all the datasets used are summarized in Table 2.

Collaboration Networks. We use four collaboration networks from arXiv preprint archive 1 . In the networks, the nodes are authors and an edge exists between two authors if they have co-authored the same paper. We consider such collaboration networks from four areas of Physics: Astro-physics ( Astro-Ph ), general relativity and quantum cosmol-ogy ( GR-QC ), high energy physics theory ( Hep-TH ) and high energy physics phenomenology ( Hep-PH ).

Social Networks . Here, nodes are users and edges in-dicate social relationships. Among them, Last.fm provides a music streaming and recommendation service and users can befriend with each other. LiveJournal provides social networking service where users can write a blog, journal or diary. LinkedIn provides social networking service to pro-fessionals and helps people find the right position. Twitter is a popular micro-blogging website where people can follow each other. Email-Enron is a communication network that covers around half million email communications [17]. The nodes are email addresses and an edge exists between two nodes if they have communicated through emails.

Infrastructure Networks . Oregon is a network of routers in Autonomous Systems (AS) inferred from Oregon route-views between March 31, 2001 and May 26, 2001 [17]. Airport network represents one month of internal US air traf-fic links between 2,833 airports 2 .
 Sports Networks . NBA dataset contains NBA and ABA statistics from the year of 1946 to the year of 2009 [18]. The nodes are players and two players have an edge if they played in the same team before.

Biology Networks. Gene is a human gene regula-tory network obtained based on gene expression profiles 3 Protein is a network of proteins obtained by BLAST algo-rithm [4] for comparing the sequence similarity.
We test the effectiveness of our query-specific optimal net-work learning algorithms in the task of link prediction  X  to http://arxiv.org/ 2 http://www.levmuchnik.net/Content/Networks/ NetworkData.html http://www.cise.ufl.edu/research/sparse/matrices/ Belcastro/human gene2.html Co llaboration I nfrastructure predict links in the network that will be created in the fu-ture. We reserve half of the edges as the training set (i.e., the observed network) and the rest as testing set and for choosing the positive and negative node sets. In particular, for each query node s , we choose 5 positive nodes to which it has an edge and 5 negative nodes to which it has no edge. To simulate the noisy edges, we add edges from the query node to its negative nodes with weight 1 and edges from the neighborhood of the query node to the neighborhood of the negative nodes with weight 0.1.

Evaluation metrics: We quantify the performance for link prediction using several metrics. Among them, Mean Aver-age Precision ( MAP ) [36] measures the overall performance based on precision at different recall levels. Mean Percent-age Ranking ( MPR ) [13] computes the average percentile-ranking over all node pairs (e.g., a percentile-ranking 0% means that connected node has the highest ranking score). Half-Life Utility ( HLU ) [8] estimates how likely a node will connect to the nodes in the ranking list, and the likelihood would decay exponentially as the rank increases.

In addition, we also consider the commonly used Area under the ROC curve ( AUC ), Precision@K, and Recall@K. Ideally, we want to achieve lower MPR value and higher values on other metrics.

Repeatability of Experimental Results. All the datasets are publicly available. We will release the code of the proposed algorithms through authors X  website. For all the results re-ported, we set  X  =  X  = 0 . 1 and b = 1. The experiments are performed on a Windows machine with four 3.5GHz Intel Cores and 256GB RAM.
We perform the effectiveness comparisons with the follow-ing methods: 1. Random Walk with Restart (RWR): perform RWR on 2. Common Neighbors: count the number of common 3. Adamic/Adar [1]: similar to Common Neighbors but 4. Supervised Random Walk (SRW) [5]: learn a function 5. wiZAN Dual [36]: incorporate node similarities into 6. ProSIN [32]: refine the network structure in response 7. QUINT -Basic: use the power method to extract columns 8. QUINT -Basic1st: use first order Taylor approximation 9. QUINT -rankOne: use first order Taylor approximation
The effectiveness comparison results across a diverse set of networks on the six evaluation metrics are shown from Fig-ure 2 to Figure 6. We have the following observations: (1) our QUINT family algorithms consistently outperform other comparison methods across all the datasets on all the six evaluation metrics. For example, on the Astro dataset, com-pared with the best competitor ProSIN, QUINT -rankOne is 21.4% higher on MAP, 13.5% higher on HLU. (2) The QUINT -Basic1st and QUINT -rankOne share a similar perfor-mance as QUINT -Basic, indicating that both approximation strategies are effective.
 We also perform a t -test between the MAP results of QUINT -rankOne and the best competitor ProSIN on Astro . The p -value is 3.7 e -44, which suggests the improvement of the proposed methods is indeed statistically significant.
Scalability: We show the running time per query vs. num-ber of nodes ( n ) and edges ( m ) on the LinkedIn , LiveJournal and Twitter networks from Figure 8 to Figure 13. We do not report the running time of QUINT -Basic on Twitter dataset because of  X  X ut of memory X  error ( O ( n 2 ) space complexity). Our QUINT -rankOne algorithm scales sub-linearly w.r.t. to both n and m on LinkedIn and Twitter , while scales linearly w.r.t. to n and m on LiveJournal . This is in accordance with our complexity analysis, since QUINT -rankOne is lin-ear w.r.t. to the size of neighborhood of the query as well as positive and negative nodes, which is at most linear w.r.t. n and m . In addition, it only takes  X  0.34s for QUINT -rankOne to process one query on the largest dataset (i.e. Twitter ) with  X  1.4 billion edges and  X  40 million nodes. In this section, we review the related work.

Node Proximity and RWR . Measuring node proximity in a network is an important task in many real applications, ranging from computer vision [23], e-commerce [10, 9], so-cial networks [28, 29, 11], software engineering [27], disaster management [41] to biology [22] and epidemiology [31]. The state of the art has mostly focused on best exploring the topology information for measuring the proximity. Among others, the random walk based methods have gained preva-lence largely due to its superiority in capturing the multiple weighted relationships between nodes. Despite their suc-cesses, several limitations still exist.

First (Q1), most of the existing works assume a fixed net-work topology along with its associated edge weights, al-though the real networks could be incomplete and noisy. For this reason, Agarwal et al. [3, 2] propose to compute the global rankings of nodes in a network by learning the edge weights; Backstrom and Leskovec [5] propose to learn the optimal edge weights based on the node and edge at-tributes/features. However, these methods still assume the topology of the given network is fixed. Another work pro-poses to use side information to refine the network topology Figure 8: Running time per query vs. number of nodes on LinkedIn dataset (y-axis is in log scale). In-set: scalability of QUINT -rankOne only (y-axis is in linear scale). Figure 10: Running time per query vs. number of nodes on LiveJournal dataset (y-axis is in log scale). In-set: scalability of QUINT -rankOne only (y-axis is in linear scale). Figure 12: Running time per query vs. number of nodes on Twitter dataset (y-axis is in log scale). Inset: scalabil-ity of QUINT -rankOne only (y-axis is in linear scale). [32], so that the random walks can be guided towards/away from certain specific zones in the network.

Second (Q2), an implicit assumption behind most existing works is that one common (optimal) network exists for all queries. In some scenarios, it is desirable to find a query-specific optimal network. Some recent work aim at finding k nearest neighbor nodes without calculating the ranking scores of all nodes for a given query node [7, 34, 39]. How-ever, their focus is more on the computational efficiency.
Third (Q3), scalable computation is a key challenge for node proximity measures, and various solutions have been proposed to alleviate this issue. Lofgren et al. [21] propose to efficiently compute the RWR score between two given nodes under a certain error bound based on a bi-directional search. Yu and Lin [38] incrementally update the RWR scores when the underlying network structure changes without recom-puting from scratch. Shin et al. [26] scale up RWR compu-tation by reordering the adjacency matrix so that it contains a large and easy-to-invert submatrix. After the preprocess-ing, the RWR scores can be efficiently calculated. Zhang et al. [40] design a sampling method to accelerate the proximity computation based on the concept of random path. These efforts are complementary to the proposed algorithms in this paper -after we learn a query-specific optimal network, we can leverage these methods to speedup the subsequent prox-imity score computation.

Overall, none of the existing works simultaneously address all these challenges (i.e., Q1-Q3 summarized in Section 1).
Collaborative Filtering . Considering the problem set-ting, our work is also related to collaborative filtering [16, 14]. For example, we can apply collaborative filtering to learn the hidden relationships between nodes, and use the recovered weights as node proximity [36]; however, collabo-rative filtering still uses a fixed observed network/matrix as input. Ruchansky et al. [25] propose to use side information to complete a matrix/network (i.e., to infer user preference) by actively probing a subset of true underlying matrix; in contrast, our goal is to use the user preference to learn the optimal network for a given query.

Link Prediction . Finally, our work is also related to link prediction [19, 6]. While link prediction aims at pre-dicting the existence of a network edge, our goal is to learn an optimal network where edges can be added, strength-ened or weakened. RWR score computation based on our learned network could be used to improve link prediction. For example, we can directly use the RWR scores to predict links [20], or merge RWR scores with other inputs such as node attributes for link prediction [37, 12].
In this paper, we study the problem of learning query-specific optimal networks for node proximity measures. The proposed QUINT algorithm advances the state of the art in multiple dimensions, with a larger optimization scope , at a finer optimization granularity , and with a much better opti-mization efficiency . The extensive empirical evaluations on a diverse set of real networks show that the proposed algo-rithms (1) consistently outperform all the existing methods on all six commonly used metrics; (2) scale (sub)-linearly to billion-scale networks; (3) respond in a fraction of a second. This work is partially supported by the National Science Foundation under Grant No. IIS1017415, by DTRA under the grant number HDTRA1-16-0017, by Army Research Of-fice under the contract number W911NF-16-1-0168, by Na-tional Institutes of Health under the grant number R01LM011986, Region II University Transportation Center under the project number 49997-33 25 and a Baidu gift. Jie Tang is supported by the National 863 Program (2014AA015103, 2015AA124102) and NSFC (2014CB340506, 2012CB316006). [1] L. A. Adamic and E. Adar. Friends and neighbors on [2] A. Agarwal and S. Chakrabarti. Learning random [3] A. Agarwal, S. Chakrabarti, and S. Aggarwal.
 [4] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and [5] L. Backstrom and J. Leskovec. Supervised random [6] N. Barbieri, F. Bonchi, and G. Manco. Who to follow [7] P. Bogdanov and A. Singh. Accurate and scalable [8] J. S. Breese, D. Heckerman, and C. M. Kadie.
 [9] Y.-C. Chen, Y.-S. Lin, Y.-C. Shen, and S.-D. Lin. A [10] H. Cheng, P.-N. Tan, J. Sticklen, and W. F. Punch. [11] D. F. Gleich and C. Seshadhri. Vertex neighborhoods, [12] N. Z. Gong, A. Talwalkar, L. Mackey, L. Huang, [13] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [14] P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank [15] M. Kim and J. Leskovec. The network completion [16] Y. Koren. Factorization meets the neighborhood: a [17] J. Leskovec and A. Krevl. SNAP Datasets: Stanford [18] L. Li, H. Tong, N. Cao, K. Ehrlich, Y.-R. Lin, and [19] D. Liben-Nowell and J. Kleinberg. The link-prediction [20] R. N. Lichtenwalter, J. T. Lussier, and N. V. Chawla. [21] P. A. Lofgren, S. Banerjee, A. Goel, and C. Seshadhri. [22] J. Ni, H. Tong, W. Fan, and X. Zhang. Inside the [23] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [24] K. B. Petersen and M. S. Pedersen. The matrix [25] N. Ruchansky, M. Crovella, and E. Terzi. Matrix [26] K. Shin, J. Jung, S. Lee, and U. Kang. Bear: Block [27] D. Surian, N. Liu, D. Lo, H. Tong, E.-P. Lim, and [28] H. Tong and C. Faloutsos. Center-piece subgraphs: [29] H. Tong, C. Faloutsos, B. Gallagher, and [30] H. Tong, C. Faloutsos, and J. Pan. Fast random walk [31] H. Tong, B. Prakash, C. Tsourakakis, T. Eliassi-Rad, [32] H. Tong, H. Qu, and H. Jamjoom. Measuring [33] W. Wang and M. A. Carreira-Perpin  X an. Projection [34] Y. Wu, R. Jin, and X. Zhang. Fast and unified local [35] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz. [36] Y. Yao, H. Tong, G. Yan, F. Xu, X. Zhang, B. K. [37] Z. Yin, M. Gupta, T. Weninger, and J. Han. A unified [38] W. Yu and X. Lin. Irwr: Incremental random walk [39] C. Zhang, S. Jiang, Y. Chen, Y. Sun, and J. Han. Fast [40] J. Zhang, J. Tang, C. Ma, H. Tong, Y. Jing, and J. Li. [41] L. Zheng, C. Shen, L. Tang, T. Li, S. Luis, and S.-C. [42] D. Zhou, B. Sch  X  olkopf, and T. Hofmann.

