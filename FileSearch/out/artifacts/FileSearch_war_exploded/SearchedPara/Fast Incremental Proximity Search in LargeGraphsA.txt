 Purnamrita Sarkar psarkar@cs.cmu.edu Andrew W. Moore awm@google.com Amit Prak ash amitprakash@google.com Google Inc. Pittsburgh, PA 15213 Link prediction in social net works, personalized graph searc h techniques, fraud detection and collab orativ e ltering in recommender net works are imp ortan t prac-tical problems that greatly rely on graph theoretic measures of similarit y. Giv en a node in a graph we would like to ask whic h other nodes are most simi-lar to this node. Ideally we would like this similarit y measure to capture the graph structure suc h as hav-ing man y common neigh bors or having sev eral short paths between two nodes. This kind of structural in-formation can be easily quan ti ed using random walks on graphs: di usion of information from one node to another. Most random-w alk based ranking algorithms can be categorized into two broad categories. Probabilit y of reaching a node: This is the ba-sis of measures like personalize d page rank . Person-alized page-rank vectors (PPV) have been used for keyw ord searc h in databases (Balmin et al., 2004) and entity-relation graphs (Chakrabarti, 2007). These approac hes focus on computing appro ximate PPV at query time (details in section 6), and quan tify the per-formance in terms of the deviation of the appro xima-tion from the exact. However, it is not clear if PPV itself has good predictiv e power.
 Expected number of hops to reach a node: This is also called the hitting time (Aldous &amp; Fill, 2001). The symmetric version of this is the com-mute time between two nodes. These metrics have been sho wn to be empirically e ectiv e for ranking in recommender net works (Brand, 2005) and link pre-diction problems (Lib en-No well &amp; Klein berg, 2003). These measures usually require O ( n 3 ) computation. Recen tly Spielman and Sriv asta va (2008) have come up with a novel appro ximation algorithm for ecien tly computing comm ute times by random pro jections. However it is only applicable to undirected graphs. Sark ar and Mo ore (2007) introduced the notion of truncated comm ute times and demonstrated that it had good predictiv e power for link prediction tasks. However their algorithm (GRANCH) required storing poten tial nearest neigh bors of all nodes in the graph in order to answ er nearest neigh bor queries. The key con tribution in this pap er are: 1) we com bine sampling with deterministic pruning to design an algorithm whic h retriev es top k neigh bors of a query in truncated comm ute time incremen tally without cac hing informa-tion about all nodes in the graph. 2) We investigate localit y prop erties of truncated hitting and comm ute times. 3) We sho w that on sev eral link prediction tasks these measures outp erform PPV in terms of predictiv e power, while on others they do comparably . 4) Our algorithm can pro cess queries at around 4 seconds on average on graphs of the order of 600 ; 000 nodes on a single CPU mac hine.
 The rest of the pap er is organized as follo ws: in sec-tion 2 we pro vide relev ant bac kground. In section 3 we introduce our hybrid algorithm, and pro vide sample complexit y results for random sampling. The local-ity prop erties of truncated hitting and comm ute times are investigated in section 4. We presen t empirical re-sults in section 5, and conclude with related work in section 6. A graph G = ( V; E ) is de ned as a set of vertices V edges E . The ij th entry of the adjacency matrix W denotes the weigh t on edge ( i; j ), and is zero if the edge does not exist. P = p ij ; i; j 2 V denotes the transition probabilit y matrix of this Mark ov chain, so that p ij = w Hitting time h ij : The hitting time from node i to node j is de ned as the exp ected num ber of steps in a random walk starting from i before node j is visited for the rst time. Recursiv ely h ij can be written as h ij = 1 + Comm ute time c ij : Comm ute time between a pair of nodes is de ned as c ij = h ij + h ji . 2.1. Truncated Hitting Time The hitting and comm ute times are sensitiv e to long range paths (Lib en-No well &amp; Klein berg, 2003) whic h result in non-lo cal nature. They are also prone to be small if one of the nodes is of large degree (Brand, 2005). This renders them ine ectiv e for personaliza-tion purp oses. In order to overcome these shortcom-ings, Sark ar and Mo ore (2007) de ne a T-trunc ated hitting time , where only paths of length less than T are considered. We shall use h , h T interc hangeably to denote truncated hitting time. h T ij can be de ned recursiv ely as where h T is de ned to be zero if i = j or if T = 0. The above equation expresses h T in a one step look-ahead fashion. The exp ected time to reac h a destina-tion within T timesteps is equiv alen t to one step plus the average over the hitting times of it's neigh bors to the destination in T 1 hops. If there is no path of length smaller than T from i to j , this automatically sets h T ( i; j ) to T . 2.2. GRANCH (Sark ar &amp; Moore, 2007) The truncated hitting time from all nodes to a desti-nation node can be computed in O ( ET ) time using dy-namic programming. However in order to compute the hitting time from a query node to a destination, one has to compute the hitting time of all nodes to the des-tination, thus computing the entire matrix whic h tak es O ( N ET ) time ( N and E are the num ber of nodes and edges resp ectiv ely).
 In order to get around the above problem the graph is decomp osed into N overlapping neigh borho ods for eac h node. Eac h neigh borho od is computed in a way to include poten tial nearest neigh bors and prune away the rest. The authors pro vide bounds on the hitting time from all nodes within the neigh borho od of i to i . The hitting time from any node outside the boundary to the destination is quan ti ed by only two num bers: a lower and an upp er bound. As the neigh borho ods are expanded more the bounds become tigh ter. This way eac h column of the truncated hitting time ( H T ) matrix is lled up partially . After iterating over all nodes it is possible to look at one row and obtain ranking from the bounds on hitting time from a node.
 GRANCH computes all pairs of nearest neigh bors by cac hing information for all nodes in the graph. This does not work when the graph is changing con tinu-ously . We introduce a hybrid algorithm whic h essen-tially com bines the above branc h and bound tric k with sampling techniques to obtain nearest neigh bors of a query node in comm ute time with high probabilit y. We presen t an algorithm to compute appro ximate nearest neigh bors in comm ute times, without iterating over the entire graph. We com bine random sampling with the branc h and bound pruning scheme men tioned before, in order to obtain upp er and lower bounds on commute times from a node. This lets us compute the k nearest neigh bors from a query node on the y . For any query node we compute hitting time from it using sampling. We main tain a bounded neigh borho od for the query node at a given time-step. We compute estimated bounds on the comm ute time from the nodes within the neigh borho od to the query . Comm ute time of nodes outside the neigh borho od to the query are characterized by a single upp er and lower bound. We expand this neigh borho od until this lower bound ex-ceeds 2 T 0 , whic h guaran tees that with high probabilit y we are excluding nodes whic h are more than 2 T 0 com-mute distance away. These bounds are then used for ranking the nodes inside the neigh borho od.
 We will rst describ e a simple sampling scheme to obtain -appro ximate truncated hitting times from a query node with high probabilit y.
 3.1. Sampling Scheme We prop ose a sampling scheme to estimate the trun-cated hitting time from a given query node i in a graph. We run M indep enden t T -length random walks from i . Lets say out of these M runs m random walks hit j for the rst time at f t k these we can estimate the follo wing 1. The probabilit y of hitting any node j for the rst 2. The rst hitting time can be estimated by We pro vide bounds (details in App endix) similar to Fogaras et al. (2004) 1. The num ber of samples M required in order to 2. The num ber of samples M required in order to Theorem 3.1 For a given node i , in order to obtain numb er of samples M should be at least 1 2 2 log( 2 n ) . Theorem 3.2 Let v j ; j = 1 : k be the top k neigh-bors of i in exact T -trunc ated hitting time. Let = h T ( i; v k +1 ) h T ( i; v k ) . Then numb er of samples M should be at least 2 T 2 2 log( nk= ) in order to have P r ( 9 j k; q &gt; k; ^ h T ( i; v j ) &gt; ^ h T ( i; v The details are pro vided in the app endix. The above theorem says nothing about the order of the top k neigh bors, only that if the gap between the hitting times from i to the k th and k + 1 th nearest neigh bors is large, then it is easy to retriev e the top k nearest neigh bors. We could change the statemen t sligh tly to obtain a sample complexit y bound to guaran tee the exact order of the top k neigh bors with high probabil-ity. The main di erence will be that it will dep end on 3.2. Lower and Upper Bounds on c T ij Let us denote the neigh borho od of node j by N BS ( j ). The boundary of this is denoted by ( j ).In Eqn (1) h t ( i; j ) is computed using the hitting time from its direct neigh bors to j , whic h are computed in the t 1 th iteration. Since only the hitting times of nodes within N BS ( j ) are stored, a boundary node would not have access to the hitting time of at least one of its neigh bors. Those values can be upp er and lower bounded as follo ws. The fastest possible way to reac h node j from any node outside N BS ( j ) would be by jumping to the node on the boundary ( j ) whic h has the closest optimistic hitting time to j . This gives us a lower bound on the hitting time of all nodes outside N BS ( j ) to j .
The pessimistic bound is T . Plugging in these bounds in equation (1) whenev er the neigh bors are outside the neigh borho od of the destination gives the expressions for optimistic ( ho T ij ) and pessimistic ( hp T ij ) bounds on hitting times (details in Sark ar and Mo ore (2007)). Now we have the expressions for the lower and upp er bounds for the hitting times of the nodes in N BS ( j ) to j ( ho and hp values). The hitting time from j to nodes within N BS ( j ) can be estimated using the sampling scheme describ ed in section 3.1. Com bining the two leads to the follo wing.
 Theorem 3.3 The trunc ated commute time betwe en nodes i 2 N BS ( j ) and j will be lower and upp er bounde d by co T ij and cp T ij with probability 1 if the numb er of samples for estimating ^ h T ij exceeds the lower bound in theorem 3.1, wher e We would use b co ij = ^ h T ji + ho T ij and similarly b cp denote estimates of these bounds. In order to prune away nodes whic h are not poten tial nearest neigh bors we also need to obtain a lower bound on the comm ute time between j and any node outside N BS ( j ). The incoming lower bound is given by equation 2. Now note that for the outgoing lower bound we need the minim um of h T jk ; 8 k 62 N BS ( j ).
 Lemma 3.4 The numb er of samples M should be at least 1 2 2 log( 2 n ) in order to obtain Thus an estimate of the outgoing lower bound can be computed from the hitting times obtained from sam-pling. Com bining the two we obtain an estimate on the lower bound on 2 T -truncated comm ute time d lb -ct ( j ) from j to any node outside N BS ( j ).
For our implemen tation, we alw ays used estimated, not the exact bounds. This introduces an additiv e error in our results (pro of excluded for lack of space). 3.3. Expanding Neigh borhood Now we need to nd a heuristic to expand the neigh-borho od suc h that both the outgoing and incoming comp onen ts of the lower bound increase quic kly so that the threshold of 2 T 0 is reac hed soon.
 For the incoming lower bound we just nd the x clos-est nodes on the boundary whic h have small optimistic hitting time to the query . We add the neigh bors of these nodes to the neigh borho od of j . For the outgo-ing lower bound, we compute the nodes outside the boundary whic h a random walk is most probable to hit. We do this by main taining a set of paths from j whic h stop at the boundary . These paths are aug-men ted one step at a time. This enables one step look-ahead in order to gure out whic h nodes outside the boundary are the most probable nodes to be hit. We add y of these nodes to the curren t boundary . 3.3.1. Ranking The ranking scheme is similar to GRANCH and is rather intuitiv e. So far we only have lower and upp er bounds for comm ute times from node j to the nodes in N BS ( j ). Lets denote this set as S . The comm ute time from j to any node outside S is guaran teed to be bigger than 2 T 0 . The true k th nearest neigh bor will have comm ute time larger than the k th smallest lower bound i.e. co value. Lets denote the k th smallest co value by X . Now consider the nodes whic h have upp er bounds ( cp values) smaller than X . These are guaran-teed to have comm ute time smaller than the true k th nearest neigh bor. Adding a multiplicativ e slac k of to X allo ws one to return the -appro ximate k nearest neigh bors whic h have comm ute time within 2 T 0 . Note that the fact that no node outside set S has hitting time smaller than 2 T 0 is crucial for ranking, since that guaran tees the true k th nearest neighb or within 2 T 0 comm ute distance to be within S . Since all our bounds are probabilistic, i.e. are true with high probabilit y (because of the sampling), we return -appro ximate k nearest neigh bors with high probabilit y. Also the use of estimated bounds ( b co , b cp ) will introduce an additiv e error of 2 T (ignoring a small factor of T ). 3.4. The Algorithm at a Glance In this section we describ e how to use the results in the last subsections to compute nearest neigh bors in truncated comm ute time from a node. Given T; ; k our goal is to return the top k -appr oximate nearest neighb ors (within 2 T additive error) w.h.p. First we compute the outgoing hitting times from a node using sampling. We initialize the neigh borho od with the direct neigh bors of the query node (W e have set up our graph so that there are links in both di-rections of an edge, only the weigh ts are di eren t). At any stage of the algorithm we main tain a bounded neigh borho od for the query node. For eac h node inside the neigh borho od the hitting times to the query can be bounded using dynamic programming. Com bining these with the sampled hitting times gives us the es-timated b co , and b cp values. We also keep trac k of the lower bound d lb -ct of the comm ute time from any node outside the neigh borho od to the query node. At eac h step we expand the neigh borho od using the heuristic in section 3.3. Similar to GRANCH we recompute the bounds again, and keep expanding until d lb -ct exceeds 2 T 0 . W.h.p this guaran tees that all nodes outside the neigh borho od have comm ute time larger than 2 T 0 T . Then we use the ranking as in section 3.3.1 to obtain k -appro ximate nearest neigh bors (with an additiv e slac k of 2 T ) in comm ute time. We start with a small value of T 0 and increase it until all k neigh bors can be returned. As in Sark ar and Mo ore (2007) it is easy to observ e that the lower bound can only increase, and hence at some point it will exceed 2 T 0 and the algo-rithm will stop. The question is how man y nodes can be within 2 T 0 comm ute distance from the query node. In section 4 we pro ve that this quan tity is not too large for most query nodes. In this section we analyze the localit y prop erties of truncated hitting times. We sho w that most nodes in a graph will have only a smal l num ber of neigh bors within 2 T 0 T -truncated comm ute time. We would do this in three steps. First we sho w that num ber of nodes within hitting time T 0 from a node i is small. Then we would mak e a similar argumen t that the num ber of nodes within T 0 -hitting distance to i is also small. Fi-nally we would mak e an argumen t about the neigh bors of i in comm ute time.
 Theorem 4.1 For any graph G and constants T and T 0 , the numb er of nodes within a trunc ated hitting dis-tanc e of T 0 from any node is at most T 2 = ( T T 0 ) . Let P &lt;T ij denote the probabilit y of hitting node j start-ing at i within T steps and ~ P t ij the probabilit y of hit-ting j in exactly t steps for the rst time from i .
De ne S i as the neigh borho od of i whic h consists of only the nodes within hitting time T 0 from i .
However the left hand side is lower bounded by j S i j T T 0 T using (6). Whic h leads us to the upp er bound j S within T 0 hitting distance. If T and T 0 are constan t w.r.t n , then using the above bound and coun ting ar-gumen ts we can also sho w that there can be at most O ( p ting time to them .
 We have sho wn that not more than O ( have more than O ( than T 0 to them. We already have a bound of T 2 = ( T T 0 ) on the num ber of nodes with hitting time smaller than T 0 from a node. We want to bound the num ber of nodes within comm ute time 2 T 0 .
 In other words we have pro ven that f j j h T ij T 0 g and f j j h T ji T 0 g are small. Now we need to pro ve that f j j h T ij + h T ji 2 T 0 g is also small. Note that the above set consists of 1. S 1 = f j j h T ij T 0 S 1 can have at most T 2 = ( T T 0 ) nodes. Now consider S . S 2 will have size smaller than jf j j h T ji T 0 gj . Using our result from before we can say the follo wing Lemma 4.2 Let T be constant w.r.t n . If T 0 is bounde d away from T by a constant w.r.t n , i.e. T = ( T T 0 ) is constant w.r.t n , then not mor e than O ( p with trunc ated commute time smal ler than 2 T 0 . The impact of lemma 4.2 is that in a sequence of O ( p dom), for eac h node, there would be at most O ( other nodes within 2 T 0 comm ute distance on average. We have examined our algorithm on Entity Relation (ER) datasets extracted from the Citeseer corpus, as in Chakrabarti (2007). This is a graph of authors, pap ers and title-w ords extracted from Citeseer. 5.1. Dataset and Link Structure The link structure is obvious: 1. Bet ween a pap er and a word app earing in its title. 2. From a pap er to the pap er it cites, and one with 3. Bet ween a pap er and eac h of its authors. As observ ed by Chakrabarti (2007), the weigh ts on these links are of crucial imp ortance. Unlik e some other approac hes (Balmin et al., 2004; Chakrabarti, 2007) we also put links from the pap er layer to the word layer. This allo ws ow of information from one pap er to another via the common words they use. The links between an author and a pap er are undirected. The links within the pap er layer are directed. We use the con vention in Chakrabarti (2007) to put a directed edge from the cited pap er to the citing pap er with one-ten th the strength.
 For any pap er we assign a total weigh t of W to the words in its title, a total weigh t of P to the pap ers it cites and A to the authors on it. We use an inverse frequency scheme for the pap er-to-w ord link weigh t, i.e. the weigh t on link from pap er p to word w is W 1 =f w = ( times word w has app eared in the dataset. We set W = 1 ; A = 10 ; P = 10 so that the word layer to pap er layer connection is almost directed. We add a self loop to the leaf nodes, with the same weigh t as its single edge, so that the hitting times from these leaf nodes are not very small.
 We use two subgraphs of Citeseer. The small one has around 75 ; 000 nodes and 260 ; 000 edges: 16 ; 445 words, 28 ; 719 pap ers and 29 ; 713 authors. The big one has around 600 ; 000 nodes with 3 million edges : 81 ; 664 words, 372 ; 495 pap ers and 173 ; 971 authors. 5.2. Prepro cessing We remo ve the stop words and all words whic h app ear in more than 1000 pap ers from both the datasets. The num ber of suc h words was around 30 in the smaller dataset and 360 in the larger one. We will mak e the exact dataset used available on the web. 5.3. Experimen ts The tasks we consider are as follo ws, 1. Paper prediction for words (W ord task): We pick 2. Paper prediction for authors (Author task): Ex-The hybrid algorithm is compared with: 1) Exact truncated hitting time from the query , 2) Sampled truncated hitting time from the query , 3) Exact trun-cated comm ute time from the query , 4) Exact trun-cated hitting time to the query , 5) Personalized Pager-ank Vector and 6) Random predictor. Note that we can compute a high accuracy estimate of the exact hit-ting time from a query node by using a huge num ber of samples. We can also compute the exact hitting time to a node by using dynamic programming by iterating over all nodes. Both of these will be slower than the sampling or the hybrid algorithm as in Table 1. Distance from a set of nodes Hitting and com-mute times are classic measures of pro ximit y from a single node. We extend these de nitions in a very simple fashion in order to nd near-neigh bors of a set of nodes. The necessit y of this is clear, since a query often consists of more than one word. We de ne the hitting time from a set of nodes as an weigh ted av-erage of the hitting times from the single nodes. For hitting time to a set, we can change the stopping con-dition of a random walk to \stop when it hits any of the nodes in the set". We achiev e this via a simple scheme: for any query q , we mer ge the nodes in the query in a new mega node Q as follo ws. For any node v 62 Q P ( Q; v ) = the probabilit y of transitioning to node v from node q in the original graph.
 uniform weighing function, i.e. w ( q ) = 1 = j Q j . The hitting/comm ute time is computed on this mo di ed graph from Q . These mo di cations to the graph are local and can be done at query time, and then we undo the changes for the next query .
 Our average query size is the average num ber of words (authors) per pap er for the word (author) task. These num bers are around 3 (2) for the big subgraph, and 5 (2) for the small subgraph.
 Figure 1 has the performance of all the algorithms for the author task on the (A) smaller, (B) larger dataset and the word task on the (C) smaller and (D) larger dataset, and Table 1 has the average run time. As men-tioned before for any pap er in the testset, we remo ve all the edges between it and the words (authors) and then use the di eren t algorithms to rank all pap ers within 3 hops of the words (5 for authors, since au-thors have smaller degree and we want to have a large enough candidate set) in the new graph and the re-moved pap er. For any algorithm the percen tage of pap ers in the testset whic h get rank ed within the top k neigh bors of the query nodes is plotted on the y axis vs. k on the x axis. We plot the performances for six values of k : 1 ; 3 ; 5 ; 10 ; 20 and 40.
 The results are extremely interesting. Before going into much detail let us examine the performance of the exact algorithms. Note that for the author task the exact hitting time to a node and the exact com-mute time from a node consisten tly beats the exact hitting time from a node , and PPV. However for the word task the outcome of our exp erimen ts are the op-posite . This can be explained in terms of the inheren t directionalit y of a task. The distance from the word-layer to the pap er-la yer gives more information than the distance from the pap er layer to the word layer, whereas both directions are imp ortan t for the author task, whic h is why comm ute times, and hitting time to a node outp erform all other tasks.
 We only compare the predictiv e power of PPV with our measures, not the runtime . Hence we use the exact version of it. We used c = 0 : 1, so that the average path-length for PPV is around 10, since we use T = 10 for all our algorithms (ho wever, much longer paths can be used for the exact version of PPV). PPV and hitting time from a node essen tially relies on the probabilit y of reac hing the destination from the source. Even though hitting time uses information only from a truncated path, in all of our exp erimen ts it performs better than PPV, save one, where it beha ves comparably . Word task: The sampling based hitting time beats PPV consisten tly by a small margin on the big-ger dataset, whereas it performs comparably on the smaller one. Hitting times and PPV beat the hitting time to nodes for the word task. In fact for k = 1 ; 3 ; 5, for the smaller dataset the hitting time to a query node isn't much of an impro vemen t over the random predic-tor (whic h is zero). This emphasizes our claim that the hitting time to the word layer does not pro vide signif-ican t information for the word task. As a result the performance of the exact and hybrid comm ute times deteriorates.
 Author task: The hitting time to the query and the exact comm ute time from a query have the best per-formance by a large margin. The hybrid algorithm has almost similar performance. Hitting time from the query is beaten by these. PPV does worse than all the algorithms except of course the random predictor. Num ber of samples: For the small graph, we use 100 ; 000 samples for computing the high accuracy ap-pro ximation of hitting time from a node; 5000 samples for the word task and 1000 samples for the author task. We use 1 : 5 times eac h for the larger graph. We will like to point out that our deriv ed sample complexit y bounds are interesting for their asymptotic beha vior. In practice we exp ect much few er samples to achiev e low probabilit y of error. In Figure 1 sometimes the exact hitting (comm ute) times does worse than the sampled hitting time (hybrid algorithm). This migh t happ en by chance, with a small probabilit y. In this section we brie y examine algorithms whic h have been dev elop ed using random walks on graphs, and their applications. Brand (2005) uses di eren t random walk based measures to compute the top k rec-ommendations for a particular customer in a customer-movie graph from the movielens dataset. The sub-matrices of the hitting and comm ute times matrices are computed by iterativ e sparse matrix multiplica-tions (details in Sark ar and Mo ore (2007)). However it is only tractable to compute these measures on graphs with a few thousand nodes for most purp oses. Lib en-No well and Klein berg (2003) sho wed that the hitting and comm ute times perform poorly for link prediction tasks, because of their sensitivit y to long paths. The most e ectiv e measure was sho wn to be the Katz measure (Katz, 1953) whic h directly sums over the collection of paths between two nodes with exp onen tially deca ying weigh ts. However, ranking un-der the Katz score would require solving for a row of the matrix ( I A ) 1 I , where I and A are the iden-tity and adjacency matrices of the graph and is the deca y factor. Even if A is sparse the fast linear solv ers will tak e at least O ( E ) time.
 Tong et al. (2007) uses escap e probabilit y from node i to node j to compute direction awar e proximity in graphs. A fast matrix solv er is used to compute this between one pair of nodes , in O ( E ) time. Multiple pairs of pro ximit y require computation and storage of the inverse of the matrix I P , whic h would be intractable for large graphs (10 K nodes). Jeh and Widom (2002b) use the notion of expected f-hitting dis-tanc e , whic h is the hitting time (in a random walk with restart) between a set of nodes in a pro duct graph with N 2 nodes. The quadratic time complexit y is reduced by limiting the computation between source and des-tination pairs within distance of r .
 The main idea of personalize d pager ank is to bias the probabilit y distribution towards a set of webpages par-ticular to a certain user, resulting in a user-sp eci c view of the web. It has been pro ven (Jeh &amp; Widom, 2002a) that the PPV for a set of webpages can be computed by linearly com bining those for eac h indi-vidual webpage. However it is hard to store all pos-sible personalization vectors or compute the person-alized pagerank vector at query time because of the sheer size of the internet graph. There have been man y novel algorithms for ecien tly computing PPV (Jeh &amp; Widom, 2002a; Haveliw ala, 2002; Fogaras et al., 2004). Most of these algorithms compute partial PPVs oine and com bine them at query time.
 The Obje ctR ank algorithm (Balmin et al., 2004) computes keyw ord-sp eci c ranking in a publication database of authors and pap ers, where pap ers are con-nected via citations and co-authors. The personalized pagerank for eac h word is computed and stored oine, and at query time com bined linearly . Chakrabarti (2007) et al. sho w how to compute appro ximate per-sonalized pagerank vectors using clev er neigh borho od expansion schemes whic h would drastically reduce the amoun t of oine storage and computation. Man y graph-based learning algorithms rely on com-puting pro ximit y measures in graphs. These graphs can be very large and undergoing con tinuous change, hence fast incr emental algorithms are needed. In this pap er we have com bined sampling techniques with branc h and bound pruning to compute near neigh bors of a query node with high probabilit y. Our pro ximit y measures have been empirically sho wn to often outp er-form a popular alternativ e, namely personalized pager-ank on link-prediction tasks.
 Proof of Theorem 3.1: We pro vide a bound on the Proof of Theorem 3.2: Consider a sampled path of Proof of lemma 3.4: Let S be a set of nodes. Let q = arg min k 2 S h ( j; k ). Let m = arg min k 2 S ^ h ( j; k ). We kno w that h ( j; q ) h ( j; m ), since q is the true minim um, and ^ h ( j; m ) ^ h ( j; q ), since m is the node whic h has the minim um estimated h-v alue. Using the sample complexit y bounds from theorem 3.1, we have ^ h ( j; m ) ^ h ( j; q ) w:h:p h ( j; q )+ T . For the other part of the inequalit y we have ^ h ( j; m ) w:h:p h ( j; m ) T h ( j; q ) T . Using both sides we get h ( j; q ) T w:h:p ^ h ( j; m ) w:h:p h ( j; q ) + T . Using S to be the set of nodes outside the neigh borho od of j yields lemma 3.4.
