 Named entity clustering is a classic task in NLP, and one for which both supervised and semi-supervised systems have excellent performance (Mikheev et al., 1998; Chinchor, 1998). In this paper, we describe a fully unsupervised system (using no  X  X eed rules X  or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In ad-dition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of orga-nization. Finally, the model defines a notion of con-sistency between different references to the same en-tity; this component of the model yields a significant increase in performance.

The main motivation for our system is the re-cent success of unsupervised generative models for coreference resolution. The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. They report a named entity score of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems.
We suspect that better models for named entities could aid in the coreference task. The easiest way to incorporate a better model is simply to run a super-vised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsu-pervised generative model for named entities. As far as we know, this work is the best such model.
Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: ( X  X ord Motor Co. X ,  X  X ord X ), while erroneously merging others: ( X  X ord Motor Co. X ,  X  X ockheed Martin Co. X ). Ng (2008) showed that better features for match-ing named entities X  exact string match and an  X  X lias detector X  looking for acronyms, abbreviations and name variants X  improve the model X  X  performance substantially. Yet building an alias detector is non-trivial (Uryupina, 2004). English speakers know that  X  X resident Clinton X  is the same person as  X  X ill Clin-ton X  , not  X  X resident Bush X . But this cannot be im-plemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meul-der, 2003). For a survey of the state of the art, see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised ap-proaches such as Black et al. (1998) have used clus-tering to group together different nominals referring to the same entity in ways similar to the  X  X onsis-tency X  approach outlined below in section 3.2.
Semi-supervised approaches have also achieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clus-tering model (which they call an EM model), initial-ized with the same seed rules, performs much more poorly at 83%.

Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of  X  X ities such as X X  and simi-lar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality se-mantic features elsewhere.

Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a  X  X epresentative X  and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, in-cluding misspellings and initials. In addition, they confirm our intuition that Gibbs sampling for infer-ence has insufficient mobility; rather than using a heuristic algorithm as we do (see section 3.5), they use a data-driven block sampler. Charniak (2001) uses a Markov chain to generate 6 different com-ponents of people X  X  names, again assuming that the class of personal names can be pre-distinguished us-ing a name list. He infers coreference relationships between similar names appearing in the same docu-ment, using the same notion of consistency between names as our model. As with our model, the clusters found are relatively good, although with some mis-takes even on frequent items (for example,  X  X ohn X  is sometimes treated as a descriptor like  X  X ecretary X ). Like Collins and Singer (1999), we assume that the named entities have already been correctly extracted from the text, and our task is merely to label them. We assume that all entities fit into one of the three MUC-7 categories, LOC (locations), ORG (organi-zations), and PER (people). This is an oversimplifi-cation; Collins and Singer (1999) show that about 12% of examples do not fit into these categories. However, while using the MUC-7 data, we have no way to evaluate on such examples.

As a framework for our models, we adopt adap-tor grammars (Johnson et al., 2007), a framework for non-parametric Bayesian inference over context-free grammars. Although our system does not re-quire the full expressive power of PCFGs, the adap-tor grammar framework allows for easy develop-ment of structured priors, and supplies a flexible generic inference algorithm. An adaptor grammar is a hierarchical Pitman-Yor process (Pitman and Yor, 1997). The grammar has two parts: a base PCFG and a set of adapted nonterminals. Each adapted nonterminal is a Pitman-Yor process which expands either to a previously used subtree or to a sample from the base PCFG. The end result is a posterior distribution over PCFGs and over parse trees for each example in our dataset.

Each of our models is an adaptor grammar based on a particular base PCFG where the top nonter-minal of each parse tree represents a named entity class. 3.1 Core NP Model We begin our analysis by reducing each named-entity reference to the contiguous substring of proper nouns which surrounds its head, which we call the core (Figure 1). To analyze the core, we use a grammar with three main symbols ( NE x ), one for each named entity class x . Each class has an asso-ciated set of lexical symbols, which occur in a strict think of the NE i as the semantic parts of a proper expand to any string of words; the ability to gen-erate multiple words from a single symbol is use-ful both because it can learn to group collocations like  X  X ew York X  and because it allows the system to handle entities longer than four words. However, we set the prior on multi-word expansions very low, to avoid degenerate solutions where most phrases are analyzed with a single symbol. The system learns a separate probability for each ordered subset of the NE i (for instance the rule NE so that it can represent constraints on possible refer-ences; for instance, a last name can occur on its own, but not a title. 3.2 Consistency Model This system captures some of our intuitions about core phrases, but not all: our representation for  X  X ill Clinton X  does not share any information with  X  X resi-dent Bill Clinton X  except the named-entity class. To remedy this, we introduce a set of  X  X ntity X  nonter-minals E k , which enforce a weak notion of consis-tency. We follow Charniak (2001) in assuming that two names are consistent (can be references to the same entity) if they do not have different expansions for any lexical symbol. In other words, a particu- X  X resident X , a first name E 1 These are generated from the class-specific distribu-tions, for instance E 0 we intend to be a distribution over titles in general.
The resulting grammar is shown in Figure 2; the prior parameters for the entity-specific symbols E i are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities E k with a standard adap-tor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the E k from a Chinese Restaurant process prior. (Gen-eral grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase ( X  X aury Cooper, a vice president  X ) or a non-proper prenominal ( X  spokesman John Smith X ) 1 . If the en-tity is the complement of a preposition, we extract the preposition and the head of the governing NP ( X  X  federally funded sewage plant in Georgia X ). These are added to the grammar at the named-entity class level (separated from the core by a special punctua-tion symbol).

Finally, we add information about pronouns and wh-complementizers (Figure 3). Our pronoun infor-mation is derived from an unsupervised coreference algorithm which does not use named entity informa-
Pronouns
Pronouns
Pronoun tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels X  for the models without pronoun information, this matching is arbitrary and must be inferred dur-ing the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak and Elsner (2009). For the evaluation set, we use the named entity data from MUC-7. Here, we extract all strings in &lt; ne &gt; tags and determine their cores, plus any relevant modifiers, governing prepositions and pronouns, by examining the parse trees. In addi-tion, we supply the system with additional data from the North American News Corpus (NANC). Here we extract all NPs headed by proper nouns.

We then process our data by merging all exam-ples with the same core; some merged examples from our dataset are shown in Figure 4. When two examples are merged, we concatenate their lists of attack airlift airlift rescu # wing # of-commander of-command with-run # # # air-india # # # # abels # # it # # gaudreau # # they he # # priddy # # he # spokesman bird bird bird director bird ford clin-ton director bird # johnson # before-hearing to-happened of-cartoon on-pressure under-medicare to-according to-allied with-stuck of-government of-photographs of-daughter of-photo for-embarrassing under-instituted about-allegations for-worked before-hearing to-secretary than-proposition of-typical # he he his he my himself his he he he he i he his his i i i he his # modifiers, prepositions and pronouns (capping the length of each list at 20 to keep inference tractable). For instance,  X  X ir-india X  has no features outside the core, while  X  X ing X  has some nominals ( X  X ttack X  &amp;c.) and some prepositions ( X  X ommander-of X  &amp;c.). This merging is useful because it allows us to do in-ference based on types rather than tokens (Goldwa-ter et al., 2006). It is well known that, to interpo-late between types and tokens, Hierarchical Dirich-let Processes (including adaptor grammars) require a deeper hierarchy, which slows down inference and reduces the mobility of sampling schemes. By merg-ing examples, we avoid using this more complicated model. Each merged example also represents many examples from the training data, so we can summa-rize features (such as modifiers) observed through-out a large input corpus while keeping the size of our input file small.

To create an input file, we first add all the MUC-7 examples. We then draw additional examples from NANC, ranking them by how many features they have, until we reach a specified number (larger datasets take longer, but without enough data, results tend to be poor). 3.5 Inference Our implementation of adaptor grammars is a mod-ified version of the Pitman-Yor adaptor grammar sampler 2 , altered to deal with the infinite number of entities. It carries out inference using a Metropolis-within-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree.

To do Gibbs sampling for our consistency-enforcing model, we would need to sample a parse for an example from the posterior over every pos-sible entity. However, since there are thousands of entities (the number grows roughly linearly with the number of merged examples in the data file), this is not tractable. Instead, we perform a restricted Gibbs sampling search, where we enumerate the posterior only for entities which share a word in their core with the example in question. In fact, if the shared word is very common (occuring in more than .001 of examples), we compute the posterior for that entity only .05 of the time 3 . These restrictions mean that we do not compute the exact posterior. In particular, the actual model allows entities to contain examples with no words in common, but our search procedure does not explore these solutions.

For our model, inference with the Gibbs algo-rithm seems to lack mobility, sometimes falling into very poor local minima from which it does not seem to escape. This is because, if there are several ref-erences to the same named entity with slightly dif-ferent core phrases, once they are all assigned to the wrong class, it requires a low-probability se-ries of individual Gibbs moves to pull them out. Similarly, the consistency-enforcing model gener-ally does not fully cluster references to common en-tities; there are usually several  X  X ill Clinton X  clus-ters which it would be best to combine, but the se-quence of moves that does so is too improbable. The data-merging process described above is one attempt to improve mobility by reducing the number of du-plicate examples. In addition, we found that it was a better use of CPU time to run multiple samplers with different initialization than to perform many itera-tions. In the experiments below, we use 20 chains, initializing with 50 iterations without using consis-tency, then 50 more using the consistency model, and evaluate the last sample from each. We discard the 10 samples with worst log-likelihood and report the average score for the other 10. 3.6 Parameters In addition to the base PCFG itself, the system re-quires a few hyperparameter settings: Dirichlet pri-ors for the rule weights of rules in the base PCFG. Pitman-Yor parameters for the adapted nonterminals are sampled from vague priors using a slice sam-pler (Neal, 2003). The prior over core words was set to the uniform distribution (Dirichlet 1.0) and the prior for all modifiers, prepositions and pronouns to a sparse value of .01. Beyond setting these param-eters to a priori reasonable values, we did not opti-mize them. To encourage the system to learn that some lexical symbols were more common than oth-ers, we set a sparse prior over expansions to sym-bols 4 . There are two really important hyperparame-ters: an extremely biased prior on class-to-pronoun-type probabilities (1000 for the desired class, .0001 for everything else), and a prior of .0001 for the Word  X  Word Words rule to discourage symbols expanding to multiword strings. We performed experiments on the named entity dataset from MUC-7 (Chinchor, 1998), using the training set as development data and the formal test set as test data. The development set has 4936 named entities, of which 1575 (31.9%) are locations, 2096 (42.5%) are organizations and 1265 (25.6%) people. The test set has 4069 named entities, 1321 (32.5%) locations, 1862 (45.8%) organizations and 876 (21.5%) people 5 . We use a baseline which gives all named entities the same label; this label is mapped to  X  X rganization X .

In most of our experiments, we use an input file of 40000 lines. For dev experiments, the labeled data contributes 1585 merged examples; for test experi-ments, only 1320. The remaining lines are derived Model Accuracy Baseline (All Org) 42.5 Core NPs (no consistency) 45.5 Core NPs (consistency) 48.5 Context Features 83.3 Pronouns 87.1 Model Accuracy Baseline (All Org) 45.8 Pronouns 86.0 using the process described in section 3.4 from 5 million words of NANC.

To evaluate our results, we map our three induced labels to their corresponding gold label, then count the overlap; as stated, this mapping is predictably encoded in the prior when we use the pronoun fea-tures. Our experimental results are shown in Table 1. All models perform above baseline, and all fea-tures contribute significantly to the final result. Test results for our final model are shown in Table 2.
A confusion matrix for our highest-likelihood test solution is shown as Figure 5. The highest confusion class is  X  X rganization X , which is confused most often with  X  X ocation X  but also with  X  X erson X .  X  X ocation X  is likewise confused with  X  X rganization X .  X  X erson X  is the easiest class to identify X  we believe this explains the slight decline in performance from dev to test, since dev has proportionally more people.

Our mapping from grammar symbols to words ap-pears in Table 3; the learned prepositional and mod-ifier information is in Table 4. Overall the results are good, but not perfect; for instance, the P ers states are mostly interpretable as a sequence of ti-tle -first name -middle name or initial -last name -LOC 1187 97 37 ORG 223 1517 122 PER 36 20 820 last name or post-title (similar to (Charniak, 2001)). The organization symbols tend to put nationalities and other modifiers first, and end with institutional types like  X  X nc. X  or  X  X enter X , although there is a sim-ilar (but smaller) cluster of types at Org 2 , suggest-ing the system has incorrectly found two analyses for these names. Location symbols seem to put en-tities with a single, non-analyzable name into Loc 2 , and use symbols 0, 1 and 3 for compound names. our NANC dataset includes many of these, but we failed to account for them in the model. Since they appear in a single class here, we are optimistic that they could be clustered separately if another class and some appropriate features were added to the prior. Some errors do appear ( X  X upreme court X  and  X  X ouse X  as locations,  X  X inister X  and  X  X hairman X  as middle names,  X  X ewt gingrich X  as a multiword phrase). The table also reveals an unforeseen issue with the parser: it tends to analyze the dateline be-ginning a news story along with the following NP ( X  X ASHINGTON Bill Clinton said... X ). Thus com-mon datelines ( X  X ashington X ,  X  X ew york X  and  X  X os angeles X ) appear in state 0 for each class. As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. To our knowledge, the closest existing system to such a model is the EM mix-ture model used as a baseline in Collins and Singer (1999). Our system improves on this EM system in several ways. While they initialize with minimal supervision in the form of 7 seed heuristics, ours is fully unsupervised. Their results cover only exam-ples which have a prepositional or modifier feature; we adopt these features from their work, but label all entities in the predefined test set, including those that appear without these features. Finally, as dis-cussed, we find the  X  X erson X  category to be the eas-iest to label. 33% of the test items in Collins and Singer (1999) were people, as opposed to 21% of ours. However, even without the pronoun features, that is, using the same feature set, our system scores equivalently to the EM model, at 83% (this score is
P ers 0 P ers 1 P ers 2 P ers 3 P ers 4 rep. john (767) minister brown jr. sen. (256) robert (495) j. smith (97) a washington david john (242) b smith (111) dr. michael l. johnson iii los angeles james chairman newt gingrich williams senate president e. king wilson house richard m. miller brown new york william (317) william (173) kennedy clinton president sen. (236) robert (155) martin simpson republican george r. davis b
Org 0 Org 1 Org 2 Org 3 Org 4 american (137) national university research association washington american (182) inc. (166) medical center washington the new york corp. (156) news inc. (257) national international (136) college health corp. (252) first public institute (87) services co. los angeles united group communications committee new house hospital development institute royal federal museum policy council british home press affairs fund california world international (61) defense act
Loc 0 Loc 1 Loc 2 Loc 3 Loc 4 washington (92) the texas county monday los angeles st. new york city thursday south new washington (22) beach river (57) north national (69) united states valley tuesday old east (65) baltimore island wednesday grand mount california river (71) hotel black fort capitol park friday west (22) west (56) christmas bay hall east (21) lake bosnia house center haiti great san juan supreme court building
Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-mod according-to (1044) director president-of $ university-of calif. played-by spokesman chairman-of giant city-of newspap[er] directed-by leader director-of opposit[e] from-to state led-by presid[ent] according-to (786) group town-of downtown meeting-with attorney professor-at pp state-of n.y. from-to candid[ate] head-of compan[y] center-in warrant met-with lawyer department-of journal out-of va. letter-to chairman member-of firm is-in fla. secretary-of counsel members-of state house-of p.m. known-as actor spokesman-for agenc[y] known-as itself on dev, 25% people). When the pronoun features are added, our system X  X  performance increases to 86%, significantly better than the EM system.

One motivation for our use of a structured model which defined a notion of consistency between en-tities was that it might allow the construction of an unsupervised alias detector. According to the model, two entities are consistent if they are in the same class, and do not have conflicting assignments of words to lexical symbols. Results here are at best equivocal. The model is reasonable at pass-ing basic tests X   X  X r. Seuss X  is not consistent with  X  X r. Strangelove X ,  X  X r. Quinn X  etc, despite their shared title, because the model identifies the sec-ond element of each as a last name. Also correctly,  X  X r. William F. Gibson X  is judged consistent with  X  X r. Gibson X  and  X  X ibson X  despite the missing el-ements. But mistakes are commonplace. In the  X  X ibson X  case, the string  X  X illiam F. X  is misana-lyzed as a multiword string, making the name in-consistent with  X  X illiam Gibson X ; this is probably the result of a search error, which, as we explained, Gibbs sampling is unlikely to correct. In other cases, the system clusters a family group together under a single  X  X ntity X  nonterminal by forcing their first names into inappropriate states, for instance assign-ing P ers 1 Bruce, P ers 2 Ellen, P ers 3 Jarvis, where first name of a different individual. To improve this aspect of our system, we might incorporate name-specific features into the prior, such as abbreviations and the concept of a family name. The most critical improvement, however, would be integration with a generative coreference system, since the document context probably provides hints about which entities are and are not coreferent.
 The other key issue with our system is inference. Currently we are extremely vulnerable to falling into local minima, since the complex structure of the model can easily lock a small group of examples into a poor configuration. (The  X  X illiam F. Gibson X  case above seems to be one of these.) In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split-merge samplers (Jain and Neal, 2000) and the per-mutation sampler (Liang et al., 2007a). One inter-esting question is how well these samplers perform when faced with thousands of clusters (entities).
Despite these issues, we clearly show that it is possible to build a good model of named entity class while retaining compatibility with generative sys-tems and without supervision. In addition, we do a reasonable job learning the latent structure of names in each named entity class. Our system improves over the latent named-entity tagging in Haghighi and Klein (2007), from 61% to 87%. This sug-gests that it should indeed be possible to improve on their coreference results without using a super-vised named-entity model. How much improvement is possible in practice, and whether joint inference can also improve named-entity performance, remain interesting questions for future work.
 We thank three reviewers for their comments, and NSF for support via grants 0544127 and 0631667.
