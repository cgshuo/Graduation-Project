 In many real-world applications, such as image processing, computational biol-ogy and natural language processing, the dimensionality of data is usually very high. Due to the complexity and noise of high-dimensional data, the effectiveness of regression or classification is limited. This can be improved via dimensionality reduction which finds a compact representation of the data for classification.
A more popular technique for dimensionality reduction is discriminant anal-ysis. To handle nonlinear problems, the kernel discriminant analysis (KDA) is proposed in [1], which computes the discriminative projection from the data set that is mapped nonlinearly into the reproducing kernel Hilbert space (RKHS). We observe that the kernel is chosen before learning in the KDA method. How-ever, the kernel-based learning methods are desirable when integrating the tuning of kernel into the learning space.

In addition, the discriminant multiple kernel learning methods require a plenty of labeled samples to discriminate the unlabeled data from each class. In real-world applications, it is usually costly or even impossible to get such a huge number of labeled samples from the same distribution. When this situation oc-curs, the performance of discriminant kernel learning methods is poor. Then one expects to carry out discriminant analysis with the help of other related labeled data from other domains. This brings out the cross-domain problem since the ex-isting discriminant kernel learning makes the assumption that the training data and the test data are independent and identical. To resolve this problem, cross transfer learning is proposed, whose aim is to improve learning in the in-domain by porting the labeled sample from out-of-domain to that from in-domain to carry out dimensionality reduction. Sev eral works have been done by combining unsupervised dimensionality with cluster ing, such as transferred dimensionality analysis(TDA) [2], which intends to select the most discriminative subspace and clustering at the same time. Maximum m ean discrepancy embedding(MMDE)[3] tries to find a subspace where training and test samples distribute similarly to solve the sample selection bias problem in an unsupervised way. S.Si et al.[4] proposed using evolutionary cross-domain discriminative Hessian eigenmaps by minimizing the quadratic distance between the distribution of the training set and that of the test set. However, it could not solve non-linear problems.
In this paper, we develop a new dimensionality reduction method, called do-main transfer discriminant kernel learning method (DTDKL), which transfers the knowledge from labeled data in out-of-domain to the in-domain by explicitly carrying out kernel discriminant learning and transfer leaning in a coherent way. More specifically, DTDKL tries to find a projection to maximize the Fisher dis-criminant ratio in the optimal feature space and minimize the maximum mean discrepancy (MMD) of the different distributions simultaneously. In fact, DT-DKL provides a method to learn an optimal kernel function and discriminant projection at the same time.
 The key contributions of the paper can be highlighted as follows:  X  To the best of our knowledge, DTDKL is the first semi-supervised  X  By comparing the state-of-the-art dimensionality reduction methods, DT-The rest of this paper is organized as fo llows: Section 2 presents the related works and preliminaries of DTDKL; Section 3 proposes DTDKL method by embedding maximum mean discrepancy ( MMD) into discriminant analysis to tackle the cross-domain problem; Sectio n 4 presents our exper imental results to demonstrate its applications. Finally, we conclude the study in Section 5. 2.1 Discriminant Multiple Kernel Learning Dimensionality reduction has always attracted amount of attention. Various meth-ods have been proposed in a recent survey [ 5] to solve this problem. The canoni-cal dimensionality reduction algorithm is linear discriminant analysis (LDA) [6], which is finding the most discriminative subspace for different classes in the origi-nal space. And with the development of kernel-based methods, kernel discriminant analysis has received a lot of interest for nonlinear problems. The KDA algorithm finds the direction in a feature space, defined by a kernel function, onto which the projections of different classes are well separated [1,7]. Note that the kernel function plays a crucial role in kernel methods, and Lanckriet et al. [8] pioneered the work of multiple kernel learning (MKL) in which the optimal kernel is ob-tained as a linear combination of pre-dete rmined kernel matrices. Based on ideas of MKL, the kernel-based learning method for discriminant analysis was reformu-lated as semi-definite programming (SDP) in Kim et al. [9]. Ye et al. [10] improved the efficiency of the problem and extended naturally to the multi-class setting by casting the SDP formulation in quadratically constrained quadratic programming (QCQP) and semi-infinite linear programming (SILP). 2.2 Transfer Learning and Maximum Mean Discrepancy Semi-supervised learning aims to mak e use of unlabeled data in the process of supervised learning and it has also been widely used in many areas related to transfer learning. One of the typical branches is to find criteria to estimate the distance between different distributions. A well-known example is Kullback-Leibler (K-L) divergence. Many criteria are parametric for the reason that an intermediate density estimate is usually required. To avoid parametric estima-tion, some nonparametric methods are proposed to evaluate the distance between the different distributions of data sets. Maximum Mean Discrepancy (MMD) is a effective nonparametric criterion for comparing distributions based on RKHS [11]. Suppose F be a class of functions f : X  X  R ,and X =( x 1 ,...,x n 1 ), Y =( y 1 ,...,y n 2 ) be random variable sets drawn from distributions P and Q , respectively. The maximum mean discrepancy and its empirical estimate is as follows: The function space F could be replaced by H which is a universal RKHS. By the fact that in RKHS, f ( x ) can be expressed as an inner product via f ( x )=  X  ( x ) ,f H ,where  X  ( x ): X  X  H , then one may rewrite MMD as follows: In terms of the MMD theory [11], the distance between distributions of two samples is equivalent to the distance between the means of the two samples mapped into a RKHS. Let X  X  IR d and Y = { X  1 , +1 } denote the input space and the output space, out-of-domain data samples with n out = | D out | ,and D in = D in l  X  D in u be the Typically, n in l n in u .Let P and Q be the marginal distribution of D in and D out , respectively. We assume that the n out out-of-domain samples and the n in in-domain samples are drawn independently and identically from a fixed but unknown underlying probability distribution P and Q , respectively. Our task is to predict the labels y in n in the in-domain data set. 3.1 Standard Discriminant Kernel Learning Analysis The standard kernel discriminant analysis learns the kernel and the direction Let K : X  X  X  X  IR be a kernel function. Then, Mercer X  X  Theorem [12] tells us the kernel function implicitly maps the input space X to a high-dimensional (possibly infinite) Hilbert space H equipped with the inner product  X  ,  X  H through a map  X  : K  X  H : This space is called the feature space, and the mapping is called the feature mapping. They depend on the kernel function K and will be denoted as  X  K and H
Let x + and x  X  denote the collection of data points from positive and negative classes, respectively. Then the total nu mber of data points in the training set D l is n l = n + + n  X  . The standard kernel discriminant analysis [9] learns the kernel K and direction w  X  H k via the optimization problem where K 1 ,...,K p be the given p based kernels,  X  0 means its elements  X  i are nonnegative,  X &gt; 0 is a regularization parameter, I is the identity operator in H
K ,  X  + K and  X  and  X  + K and  X   X  K are the sample covariances Note that (2) is a supervised learning model which neglects the knowledge of the unlabeled data, and hence can not yield the favorable classification. In addition, this model requires all samples to come from the identical distribution, which means that it can not deal with the cross-domain problem. Motivated by this, we propose a domain transfer kernel learning method in the next subsection. 3.2 Domain Transfer Kernel Learning for Discriminant Analysis Note that if the distributions P ( x )and Q ( x ) are completely independent, then the out-of-domain data D out is useless; if P ( x )and Q ( x ) are identical, then the cross-domain problem becomes the standard classification problem. However, in most cases P ( x )and Q ( x ) are neither independent nor identical, for which we may use the cross-domain projection vector that is learned from out-of-domain data set D out and the in-domain data set D in with MMD formulation. Then, the optimization problem of DTDKL can be formulated as: where  X   X  0 is a parameter to balance the difference of data distributions of two domains and the Fisher discriminant ratio of KLDA for labeled samples. This optimization problem involves two classes of variables. One is the kernel matrix K which represents the adaptive feature space, and the other is the projection direction w for the dimensionality reduction.

By specializing KLDA K,w ( D l ) and MMD 2 K ( D out , D in )as F ( w,K )and  X  in  X   X  out , respectively, (3) becomes where means and covariances, respectively. Co mparing with the model (2), we see that anewterm  X   X  in  X   X  out 2 is introduced into the objective of (4). This term is a concave function that will bring a concavification effect on the original non-concave objective of (2). So, the globally optimal solution of the maximization problem (4) can be easier found than that of the problem (2) proposed in [9] .
Note that the last term in F  X , X  ( w,K ) is independent of w . Hence, the maxi-mization problem (4) can be rewritten as Using the same arguments as in [9], we know that the globally optimal solution of the inner maximization problem Substituting this into the objective of (8), we obtain that where On the other hand, from the Representer Th eory [12], the optimal discriminative projection in DTDKL is the span of the images of the training points in the feature space. Note that in this method the training set includes both labeled data and unlabeled data due to the MMD formulation. Hence, there exists a vector  X   X  IR n such that where
In fact, we can find a closed-form expression of  X  : where a is an n-dimensional vector given by and the matrix G is defined as Because the unlabel data is introduced in this model, the representation of the can be written as where b =( b 1 ,...,b n ) with Then, the optimization problem (8) can be reformulated as By the Schur Complement Theorem, we know that The last two equations show that (13) is equivalent to where
Algorithm 1. Kernel Discriminant Learning in Cross-domain Problem Thus, we convert the nonconvex optimization problem (4) into a convex semidef-inite programming problem. Similar to the one obtained by [9], it can be solved by interior-point method softwares such as SeDuMi or SDPT3.

The cost of constructing t he basic kernel matrices is O ( n 2 d ), the combining the p basic kernel matrices costs O ( n 2 p ), and computing the gradient and Hessian of the objective is O ( n 3 ). so the total cost per Newton step of interior-point the total cost grows like O ( n 3 ), which is the same as that of SVMs. The SDP guarantees the convergence of the algorithm. In this work, we carried out experiments on three real-world data collections from two different domains to evaluate the des cribed algorithms. The performance is compared with MKDL-DA [9], and Semi-sup ervised kernel discriminant analysis SKDA [13] as well as other transferred dimensionality reduction method, TKDR [2] and MMDE [3]. 4.1 Data Sets and Experiment Setup As shown in Table 1, the data collections consist of Reuters-21578 [14], 20-Newsgroups [15] and SyskillWebert [ 14]. Amoung them, Reuters-21578 and 20 Newsgroups is the standard used to test web page ratings. The important statis-tics and pre-processing procedures of th ese collections are presented below. Data Sets Description. With a hierarchical structure, SyskillWebert database consists of the HTML source of web pages plus the ratings of a user on those web pages. Four separate subjects are contained in the web pages. Associated with each web page are the HTML source and a user X  X  rating in terms of  X  X ot X ,  X  X edium X  or  X  X old X  [16]. As demonstrated in Table 1, all of the four subjects are involved in our study.  X  X oat X  is reserved as the set of in-domain and the other are used as the out-of-domain data. Compared to the  X  X old X  pages, the total number of pages rated as  X  X edium X  or  X  X ot X  is fewer. Hence, we combine the  X  X edium X  and  X  X ot X  pages together, and change the labels of those pages as  X  X on-cold X  to form a binary classification problem. The learning task is to pre-dict the user X  X  preferences for the given web pages. the Rueters-21578 is another text repository which consists of Reuters news wire articles organized into five top categories, and each category contains various sub-categories. Three cate-gories,  X  X rgs X ,  X  X eople X  and  X  X laces X , we remove all the documents of  X  X SA X  in order to make the size of these three categories nearly even [16]. For each category, all of the sub-categories are then organized into two parts, and each part has different distribution and approx imately equal size. Th erefore, one part can be used for the in-domain and the other is treated as the out-of-domain purpose. According to the method described in [17], three cross-domain learning tasks are generated as listed in Table 1, and the learning objective aims to clas-sify articles into top categories. Similar to Reuters-21578 data, 20-Newsgroups corpus contains 7 top categories and these top categories contain 20 subcate-gories which have approximately 20,000 newsgroup documents. We select four top categories  X  X om X ,  X  X  ec X ,  X  X alk X  and  X  X ci X  in thi s experiment. Thus, three other cross-domain tasks are formed as listed in Table 1.
 Experiment Setup. On one hand, for each in-domain data set employed in the experiment, we further split it into two parts: in-domain data with labels( D l )and the in-domain data without labels( D u ). We randomly select 50% data from out-of-domain and in-domain, respectively. The ratio between | D l | and | D u | is 1:9. All of the in-domain data without labels( D u ) are used as the test sets while the training sets consist of the data points with labels from both the in-domain D l and out-of-domain ( D out ). On the other hand, the kerne l is a convex combination of 10 Gaussian kernels [10]: where  X  i are the weights of the kernels to be determined. The values of  X  i were chosen uniformly over the interval [10  X  1 , 10 2 ] on the logarithmic scale. The regularization parameter  X  in DTDKL and the MMD parameter  X  was fixed to 10  X  6 and 1, respectively. As a matter of f act, the algorithm is not sensitive to the parameter  X  for a wide range.

Any ordinary classifier, such as Na  X   X ve Bayes, K-nearest, can be used in the di-mensionality reduction method. In our experiments, we simply choose the nearest centroid method. 4.2 Experimental Results For performance evaluation, we use a ccuracy, which has been widely used as a evaluation metric, we systematically compare the proposed algorithms to some classifiers, including discriminant MKL-DA [9], SKDA [13], as well as TKDR [2], MMDE[3]. All of the results reported below are mean of that running 10 times.
In this section, we use accuracy as the e valuation metric, and compare the proposed algorithms to MKL-DA, SKDA and TKDR. The results show clearly that DTDKL is able to alleviate the influence of different distributions. Table 2 summarizes the accuracies of MKL-DA, SKDA, TKDR, MMDE, and DTDKL on the three databases with the best results highlighted in bold font. It can be seen that the MAP of the DTDKL methods is consistently higher than the other methods on all of the data sets. Moreover, it is general trend that those problems with higher precision, g enerally, have a smaller error.
 Overall Performance Our proposed method DTDKL outperforms all the other algorithms in terms of accuracy, demonstrating that DTDKL learns a robust target classifier.And it is also easy to notice that the MMDE and DTDKL performs much better than the other three methods, even TKDR. Moreover, the standard deviation of DTDKL is much smaller, means that it is more stable. For the SyskillWebert collection, compared to DTDKL X  X  rivals, on average it achieves at least 1.23%, 0.24% and 1.05% higher accuracy on  X  X oatVsBands X ,  X  X oatsVsBiomedical X  and  X  X oatVsSheep X , respectively. The better p erformance can be ascribed to trans-ferring the in-domain and out-domain data to a features whose the discriminant distance of the data is maximum and the maximum mean discrepancy comes out to be minimum. For the Reuters-21578 data set, the accuracy of DTDKL on average achieve at least 1.6%, 1.7% and 0.7% higher that other approaches on  X  X rgsVsPeople X ,  X  X rgsVsPlaces X  and  X  X eopleVsPlaces X  respectively.The simi-lar performance explanation provided to DTDKL method on Reuters-21578 can also applied here. On the 20 News-group data set, the DTDKL methods per-form best among the total tasks. Compare DTDKL and MKL-DA, we can see that the MAP of DTDKL is at least 6.6% higher, even nearly 10% higher than MKL-DA, which confirm the positive effect of MMD.
 Sensitivity This study evaluates the sensitivity of varied sizes of labeled in-domain data and conducted on the three collection. The results are demonstrated in Fig.1. It is evident that, as the size of the labeled in-domain data increases, DTDKL performs better than or as equal as its competitors at most case. For example, as shown in Figure 1(c), DTDKL achieves at least 5% higher accuracy than other methods on each size of labeled in-domain data. As a general trend, the accuracy of DTDKL steadily improves wh en the number of labeled in-domain data increase from 1% to 10%. Consequently, we infer that, better performances can be obtained if more labeled in-domain data are provided. We have proposed a unified dimensionality reduction in cross-domain problems to simultaneously learn a kernel function as well as Fisher discriminant direction by maximizing the Fisher discriminant d istance and minimizing the distance of out-of-domain and in-domain. Moreover, we assume that the kernel function in optimal kernel discriminant analysis is a linear combination of multiple base kernels; Thus, it can be efficiently solve by SDP. Experimental result show that DTDKL method outperforms existing dimensionality reduction in cross-domain in three text data sets.

