 AMAC  X HERDA  X  GDELEN and MARCO BARONI, CIMeC, University of Trento Everyday knowledge, otherwise known as commonsense , is a vast network of generic facts which everyone assumes that almost everyone else knows and thus enables peo-ple to function in the diverse conditions of daily life and coordinate their behaviors. Such knowledge looks na  X   X ve and superficial at first look (bedrooms have floors, a grocery item has a price), but essentially it is what sets human beings apart from state-of-the-art artificial intelligence systems [Minsky 2000]. The importance of rep-resenting everyday knowledge in a computational system in order to attain human-level intelligence has been acknowldged since the early days of artificial intelligence research [McCarthy 1959], and the area continues to be a hot topic for recent AI research.

The attacks at the commonsense knowledge problem have ranged from manual cre-ation of commonsense knowledge repositories, such as the Cyc database, to knowledge-poor induction of such knowledge from the text available on the Web [Banko et al. 2007; Lenat 1995]. However, the manual method is laborious and expensive, while the text mining methods are prone to noise. Banko et al. estimate that about 20% of the mil-lions of generic facts they extracted from the Web with a state-of-the-art large-scale information extraction system are wrong [Banko et al. 2007].

A more human-computation-centered approach to collecting commonsense knowl-edge is to recruit laypeople from the Web and have them contribute to a knowledge base. The Open Mind Common Sense project [Speer 2007] relies on the good will of Web surfing volunteers, and has been quite successful at collecting tens of thousands of generic facts from ordinary people. An alternative to volunteer work is that of games with a purpose [Von Ahn 2006], inducing Web surfers to contribute various kinds of useful knowledge while playing and having fun. Recently, social networking sites like Facebook have been used to deploy such games for easier access to a large user base [Rafelsberger and Scharl 2009].

In this study, we present a combination of a text mining algorithm and a human computation framework that allows us to attain high throughput (in terms of an-notated assertions) while keeping the precision at acceptable levels in harvesting commonsense knowledge.The first part of the proposed architecture is BagPack (Bag-of-words representation of Paired concept knowledge), an algorithm based on a vector-space model for representing commonsense assertions (or, more generally, statements about the semantic relation linking two concepts). For a given relation (e.g., LocationOf, MotivatedByGoal, ...), it evaluates a set of assertions and outputs a list of candidates ranked according to their likelihood of being true. The output of BagPack is the input of the second part of our proposed architecture, Concept Game, which is a Facebook slot-machine-like game that lets players validate the candidates. The players are shown many  X  X andom X  assertions and are asked to identify those that make sense in order to gain points.

The abundance of the candidate assertions mined by BagPack allows us to free the players from the burden of producing the commonsense facts. All they have to do is to express their assent (or lack of it). We leverage this opportunity by implementing a fast-paced game that does not place a high cognitive load on the player. The can-didate assertions extracted from corpora also allows us to employ a more data-driven approach to extend the current knowledge bases; unlike in other human-computation approaches, we can tap the corpora to collect assertions that volunteers/players do not typically provide. In addition, the slot-machine game provides a convenient excuse for the noise in the candidates displayed. The Concept Game is a game of chance and a player is expected to see many meaningless assertions before  X  X itting the jackpot X . Thus, the low precision of the text miner module becomes a natural part of the game experience, not a source of frustration. Another advantage is that, besides their inher-ent value, such cleaned-up data can be used to assess the quality of text mining and fed back to the algorithm as labeled training materials.

The main goal of the current article is to introduce our combined architecture, and to show that not only does it allow us to collect significantly better commonsense facts than the state-of-the-art text miner alone, but also that the commonsense dataset it constructs can be used to improve text mining performance in subsequent runs. After reviewing some related work in Section 2, we describe our text mining algorithm in Section 3. We provide preliminary results on both a widely used semantic task where we can compare its performance with the state-of-the-art, and tuned to extract commonsense assertions in Section 4. In Section 5, we describe the design of our game. In Section 6, we describe our combined mining + gaming experimental procedures and report the results. Section 7 concludes with the main achievements and future directions. To the best of our knowledge, our proposal is the first system integrating commonsense harvesting by text mining and a game with a purpose. However, examples of both approaches  X  implemented individually  X  exist, and we will review the text mining and human computation approaches separately. Commonsense consists of assumptions and known facts about daily life shared by a great majority of people. The ontologist Barry Smith stresses the massive amount of common sense:  X  X ommon sense includes a massive storehouse of factual knowledge about colors and sounds, about time and space, about what foods are edible and what animals are dangerous. X  [Smith 1995]. For an average citizen of the modern world, we can easily extend this definition to cover facts like  X  X ne should turn off his cellphone while attending a concert. X  or  X  X eople eat breakfast in the morning. X  [Lieberman 2008]. It looks tricky to appeal to a majority for a definition ( X  X hat exactly is a great majority of people? X ) but this appeal is the very essence of the functional benefits of commonsense. It allows us to communicate with other people without being verbose to the point that communication becomes impossible. For efficient communication, we can rely on the set of beliefs and facts we share with our interlocutor [Wright et al. 1996].

Representation of commonsense knowledge is at the core of AI research, and there have been several attempts to solve this problem. One of the well-known AI research projects that attempts to solve the commonsense problem is the Cyc project initiated by Douglas Lenat in 1984 [Lenat 1995]. Today, Cyc is a commercial knowledge base (owned and developed by Cyccorp Inc. 1 ) that intends to capture a significant portion of commonsense knowledge. It employs a detailed ontology of concepts, actions, and rules and has its own special formal language to enter new facts. The underlying semantics extends beyond a first-order predicate logic to allow an expressive representation. Re-searchCyc is a limited version of Cyc available under a free license and  X  as of 2005  X  it contained approximately a million assertions about more than a hundred thou-sand symbols [Ramachandran et al. 2005]. The fact that Cyc is a formal repository of commonsense knowledge increases the amount of effort to construct, extend, and maintain it because we need to know about the formal semantics and syntax underly-ing the knowledge base.

Open Mind Common Sense (OMCS) is another commonsense database project ini-tiated by MIT X  X  Media Lab in 1999 [Speer 2007]. A key difference between OMCS and Cyc is that the former relies on semi-structured snippets of natural language phrases to represent commonsense knowledge instead of a logical formalism. For example, an instantiation of the commonsense relation of AtLocation between the two concepts ash-tray and bar can be represented as  X  X omething you find at a bar is an ashtray X . Here, the template  X  X omething you find at a {} is an {} . X  corresponds to LocationOf, and the concepts are normalized forms of natural language phrases (e.g.,  X  X shtrays, X   X  X n ashtray, X  and similar phrases are normalized to ashtray by means of lemmatization and stop-word removal.
OMCS X  X  choice of semi-structured phrases reduces the deductive inference power of the system because of the ambiguities introduced by natural language and the imper-fect mapping of phrases to concepts. On the other hand, the less formal representa-tion allows to recruit laypeople to enter facts into OMCS because almost no training is required to formulate facts. In fact, the entire content of OMCS is based on the volunteers X  efforts. Since 1999, more than 16,000 people contributed to OMCS via a Web-based interface, resulting in more than 700,000 English facts [Havasi et al. 2009]. Moreover, the ambiguity and redundancy introduced by the natural language repre-sentation can be an advantage for flexible inference. Interested readers can refer to Liu and Singh [2004] for a detailed discussion of the advantages and disadvantages of the natural language representation.

ConceptNet 2 is a semantic network that is based on the facts contained in OMCS [Liu and Singh 2004]. The vertices are the concepts and the links come from a closed set of commonsense relations such as AtLocation, CapableOf, and HasLastSubEvent. We rely on ConceptNet to seed our text miner (see Section 4.1). Corpus-based techniques proved themselves reliable methods of knowledge extraction by using statistical analysis of frequently occurring patterns in large amounts of text. They are widely adapted in domains ranging from machine translation to biomedical text mining [Buitelaar and Cimiano 2008].

Can we follow a corpus-based path for our own problem and mine commonsense from corpora? At a first look, the answer should be no because, by definition, com-monsense facts go unstated explicitly in discourse. A robber does not start his threat by reminding his victim that  X  X uns shoot bullets X  and  X  X ullets kill people X . Rather, he simply says  X  X ive me your wallet or I will shoot! X  (assuming, for the sake of argument, that we have a corpus of robberies). As a matter of fact, precisely this observation motivates the effort for constructing commonsense knowledge bases: If commonsense was readily found in text (context in general), no one would have to collect it!
Nonetheless, whether it is possible to extract general (and of course true) facts about the daily life from corpora is an empirical question, and the literature provides encour-aging results on this problem. A key observation is that many types of commonsense knowledge are reflected at the surface level in text, albeit somewhat indirectly, as part of statements whose communicative aim is not to state a commonsense truth. Lucy Vanderwende cites the following example [Vanderwende 2005]: Upon reading this sentence, even if we know nothing about what a mammal or bat is, we can deduce that  X  bats can fly;  X  mammals (mostly) do not fly.
 Therefore, algorithms that leverage such implicit clues can be employed to extract commonsense knowledge. Vanderwende herself presents a proof of concept to show how such a system can be implemented.

In another related study, Strohmaier and Kr  X  oll [2009] analyze the search query logs released by commercial search engines and conclude that  X  X earch query logs are a potential source of common human goals X  [Strohmaier and Kr  X  oll 2009]. Their methodology is to compare the verb phrases extracted from query logs to the verb phrases already contained in ConceptNet. A significant amount of overlap between the goals extracted from corpus and the goals contained in ConceptNet motivates their conclusion.

The aforementioned two studies tell us how and why corpus-based approaches can succeed in the task of commonsense knowledge extraction. The following studies are examples of some systems that attempt to carry out this task.

KNEXT (KNowledge EXtraction from Text) is a system proposed for extracting  X  X en-eral world knowledge from miscellaneous texts, including fiction X  [Schubert and Tong 2003]. These authors provide an extensive evaluation of the output of KNEXT on the British National Corpus (a balanced and representative collection of spoken and writ-ten English samples, containing 100 million words) 3 and, according to an evaluation by five human judges, almost 60% of the generated propositions were found to be  X  X eason-able general claims X  by any given judge. However, the agreement between judges on individual facts was not extremely high. Considering only cases where all five judges agreed, the ratio of  X  X easonable general claims X  reduces to one third. Recently, it was also shown by Schubert and collaborators that with significant postprocessing and fil-tering of output more noisy corpora such as texts coming from blogs can be used as another source of commonsense knowledge [Gordon et al. 2010].

Using Web-based text as a source for commonsense knowledge is indeed becoming a popular technique. The abundance of text coming from blogs, Web pages, discussions in newsgroups, and other social media (e.g., Twitter, Facebook) allows us to cover a wide array of domains and extract a great number of facts. The sheer amount of input also helps fight the reported noise in the output of knowledge extraction systems. It is possible to apply very strict filtering conditions and focus on a relatively small subset of the output to obtain high-quality facts with a trade-off in recall (the amount of extracted knowledge relative to the amount that could be extracted in ideal conditions). Below we discuss some examples of this approach.

In 2005, a research group from Cyccorp reported a preliminary experiment for ex-tending the knowledge base of Cyc by issuing template-based queries to a commercial search engine and analyzing the results [Matuszek et al. 2005]. The first step in their approach is to identify missing pieces of knowledge in Cyc, such as the missing infor-mation on the founder of the Palestine Islamic Jihad Organization, represented as a tuple (foundingAgent PalestineIslamicJihad ?WHO). In subsequent steps, they refor-mulate the missing information as a natural language template such as  X  X IJ, founded by *, X  issue a search to Google by using this template as the query, and analyze the resulting snippets. An example snippet returned by the search engine is  X  X IJ founder Bashir Musa Mohammed Nafi is still at large X  which suggests that Bashir Musa Mo-hammed Nafi is a likely candidate for insertion into the initial incomplete tuple. After some postprocessing and filtering, which includes consistency checking with the facts already known in Cyc, the final candidates are presented to a human volunteer or to an expert and those that pass the final evaluation are inserted in Cyc.

The ConceptMiner [Eslick 2006] is another system that populates commonsense assertions by issuing pattern-based queries to Google. The system is based on Con-ceptNet (the semantic network that is based on OMCS), and its aim is to extend Con-ceptNet by finding new pairs of concepts that are related to each other by one of the commonsense relations contained in ConceptNet. The key idea in ConceptMiner is similar to the one discussed in Vanderwende [2005]; a set of known facts contained in ConceptNet serves as a seed, and then ConceptMiner extracts typical surface-level lin-guistic expressions that bind pairs of concepts bound by a given relation. For instance, for a pair of concepts dog and bark which are related to each other by CapableOf, a search operation can result in the following phrases:  X  X hen a dog barks, X   X  X he dog never stopped barking, X  and so on 4 . Then, another search session is used to find fur-ther instantiations of those surface forms to get new candidates of concept pairs for the relation (e.g., the queries  X  X hen a {}{}  X  or  X  X he {} never stopped {}  X  are issued and the results are parsed to extract new pairs of concepts). Statistical analysis and extensive filtering of the candidates result in a substantially smaller but high quality set of assertions that can be inserted in ConceptNet.

Another application which uses ConceptNet as a seed and attempts to extend it by using the Web as a corpus is that of Yu and Chen [2010]. In this approach, for each relation in question, a dataset of assertions is constructed from the facts stored in ConceptNet. In addition to the original ConceptNet assertions (which serve as posi-tive training instances), an equal number of randomly-crafted assertions are added to the datasets to serve as negative instances. Then, each instance (i.e., each pair of con-cepts) in a training set is represented in a vector space that captures the co-occurrence patterns in a corpus (Yu and Chen use Google Web 1T 5-Gram corpus). The results show that a support vector machine (SVM) trained on a subset of the initial dataset is able to discriminate meaningful facts from random facts in the leftout part of the dataset with an accuracy that is significantly higher than random performance. The accuracy of the trained SVMs range from 55% to 85% for different relations and model parameters (where 50% is the random baseline).

Never-Ending Language Learner (NELL) is a system that was recently implemented as part of the Read the Web project to extract structured facts from documents found in the Web [Carlson et al. 2010]. NELL aims to populate and grow a knowledge base that is shared between several learner components (e.g., a wrapper-based learner that extracts lists from pages, a rule learner, a pattern learner). The knowledge base is structured according to an ontology specifying a set of semantic categories (e.g., city , company ) and relations between instances of these categories (e.g., hasOf-ficesIn(organization, location) ). Initially, the knowledge base is populated with a small set of instances of the categories and the relations. NELL analyzes the Web documents and populates even more instances based on the output of its components. Sharing the same knowledge base allows the components to be trained in a coupled setting be-cause they can learn from each other X  X  output. NELL works in an incremental way and adds only a few instances that are likely to be true at each step. Using indepen-dent components may help to avoid semantic drift (i.e., accumulation of errors leading to deteriorating performance in the long run), but still a human intervention is deemed essential in maintaining the overall quality of NELL X  X  output. Quinn and Bederson classify different approaches to harnessing the computation ca-pabilities of humans [Quinn and Bederson 2009]. Among the several dimensions they use for categorization, the following are relevant for our discussion: Motivation of users, techniques for coping with noise, minimum participation time, and cognitive load placed on the player. Below, we discuss the human-computation-based approaches to commonsense knowledge collection that we are aware of along these dimensions.
Verbosity [Von Ahn et al. 2006] is a word-guessing game very much like the com-mercial game  X  X aboo X . In a single round of the game, a describer tries to tell the secret word to a guesser by filling in one or more of the slot-based templates he or she is given. The templates are crafted in order to collect commonsense knowledge about the secret word. Some examples are  X  X t is a type of , X   X  X bout the same size as  X . If the guesser is able to find out the secret word, the clues that the describer provided are considered to be true and are stored. Common Consensus [Lieberman et al. 2007] is another game with a purpose based on the popular TV show  X  X amily Feud X  where the player is asked to guess the popular answers for a given question like  X  X omething that is likely to be found in a kitchen is  X . Both games X  output is used to populate facts in OMCS, and consequently also in ConceptNet [Speer et al. 2010].

Both Verbosity and Common Consensus are games with a purpose, where the play-ers X  primary motivation is having fun, and commonsense knowledge collection is a side effect of the game playing. Thus, these games differ from the volunteer-based knowledge collection effort of OMCS. In order to cope with noisy input, both games can employ redundancy and accept a fact as true only after it is asserted by more than one player. However, the players are free to introduce the facts they like, and that means many valid facts may be asserted only once X  X esulting in a sparse dataset. The games can only guide the players to provide information about a given domain or a tar-get concept, but they lack the ability to validate a given commonsense fact. Coupled with the tendency of players to abuse the system, especially for Verbosity where the players try to do their  X  X est X  to convey the secret word, noise reduction and validation call for nontrivial and usually heuristics-based approaches [Speer et al. 2010]. For both games, the overhead of playing is quite low and players can almost immediately start to contribute. However, the burden of producing commonsense facts are placed on the players. In Verbosity, it is up to the describer to think and find good clues, and in Common Consensus, the player has to come up with plausible candidates for the popular answers. Thus, the games are cognitively engaging and the cognitive load on the player is quite high.

Another application that is related to Cyc is the game called FACTory 5 . In FACtory, the player is shown a set of commonsense statements and is asked to express his opinion about the statement. The FACTory X  X  commonsense statements are generated from the CYC repository, and players must tell whether they think the statements are true or false. Extra points are awarded when a player agrees with the majority answer on a fact and a certain consensus threshold has been reached. Although the system is presented as a game X  X nd thus the motivation for answering the questions is expected to be that of having fun X  X n its current stage, there is no clear fun aspect in FACTory, which looks like an interactive and marginally less boring mechanism to collect human opinions about a given set of commonsense statements. An important difference between FACTory and the previous two games is that FACTory relies on Cyc as a source of commonsense statements. Therefore, all it has to do is to ask for the opinion of the player. This has the benefit of reducing the cognitive load on the player (it is easier to say yes to the question  X  X an salmon be found in a fridge? X  rather than to try to come up with  X  X almon X  as an answer to the question  X  X hat can be found in a fridge? X 
There are other attempts to crowdsource commonsense data evaluation/collection, by utilizing services like Amazon X  X  Mechanical Turk 6 , for example, Gordon et al. [2010]. We consider these payment-based crowdsourcing methods as complementing the games-with-a-purpose approach rather than competing with it. For the evaluation of small datasets, crowdsourcing may be a convenient alternative, but as the amount of data to be annotated increases, so does the cost of annotation. In contrast, the op-erational costs of games are usually almost constant (e.g., a small monthly reward to motivate players) and enlarging the user base would not incur any additional costs. Current estimates are that the unit cost per response for our Concept Game in its initial phase has been comparable to, if not less than, the cost reported for Mechanical Turk services (around 300 to 500 responses per USD spent) [Gordon et al. 2010]. As (read if) the game becomes more popular among Facebook users, the unit cost will get much lower. The approach to the extraction of semantic information taken by our text miner of choice, namely BagPack (Bag-of-words representation of Paired concept knowledge), is to construct a vector-based representation of a pair of terms in such a way that the vector represents both the contexts where the two terms co-occur (which should be very informative about their relation) and the contexts where the single terms occur on their own (possibly less directly informative, but also less sparse). As far as we know, BagPack is the first semantic mining algorithm that exploits together informa-tion about the contexts in which paired terms co-occur, as well as contexts in which the pair components occur separately.

For a given pair of concepts, BagPack constructs three different subvectors, one for the first term (recording frequency of sentence-internal co-occurrence of the first term with context items which may be unigrams, that is, single words or n-grams, that is, word sequences, depending on implementation), one for the second (with the same kind of information), and one for the co-occurring pair (keeping track of the items that occur in sentences where both terms occur). The concatenation of these three subvectors is the final vector that represents the pair. In Herda  X  gdelen and Baroni [2009], where we first introduced it, the vector space that BagPack constructs was employed exclusively in a supervised setting: The vectors constructed for labeled pairs (including positive and negative examples of the target relation) were fed to a support vector machine that was then used to classify or to rank unlabeled pairs according to their confidence scores. In this study we also employ BagPack in an unsupervised setting where we use the cosine similarities between the vectors directly to assess semantic similarity.
Which specific text mining algorithm is employed before the human computation step is not a crucial aspect of our combined architecture for text mining and human computation. We would like to think of the text miner as a  X  X lug-in, X  and any rea-sonable model which produces a list of candidate assertions ranked according to their likelihood of being meaningful would do the job. That said, we use BagPack because it is a model that was readily available to us, which proved a robust and reliable repre-sentation of concept pairs. A detailed description of BagPack is beyond the scope of this article; therefore, we limit our discussion of BagPack in this study to its comparison with other algorithms on two semantic benchmarks: The first experiment is based on the SAT analogy recognition task. The second requires the classification of meaningful and meaningless assertions expressing three ConceptNet relations.

For both tasks, the same BagPack implementation and source corpora are used, as described below.

We carried out our tests on the Web-derived English Wikipedia and ukWaC corpora, about 2.8 billion tokens in total (we use the preprocessed versions from http://wacky.sslmit.unibo.it ). We did not carry out a search for  X  X ood X  parame-ter values. Instead, the model parameters are generally picked at convenience to ease memory requirements and computational efficiency. Once we construct the vectors for a set of word pairs, we get a co-occurrence matrix with pairs on the rows and fea-tures (words that co-occur with the pairs) on the columns (including pair-and single-occurrence features). PMI feature weighting is applied to the co-occurrence matrix [Church and Hanks 1990].

Please refer to the more thorough description and evaluation of BagPack we report in Herda  X  gdelen and Baroni [2009], which also shows how BagPack is a highly adaptive algorithm, reaching (near) state-of-the-art performance on a variety of tasks. The SAT analogy questions task was introduced by Turney et al. [2003]. In this task, there are 374 multiple choice questions with a pair of related words as the stem (e.g., wallet-money ) and 5 other pairs as the choices (e.g., safe-lock , suitcase-clothing , camera-film , setting-jewel , car-engine ). The correct answer is the choice pair which has the relationship most similar to that in the stem pair ( suitcase-clothing in this exam-ple). We chose to evaluate BagPack on this task because it is a standard benchmark for corpus-based semantic models, and thus allows us to assess the relative performance of BagPack with respect to many state-of-the-art models. Moreover, the sort of ana-logical reasoning required by the task (you use a wallet to store money like you would use a suitcase to store clothing) is a classic example of reasoning with commonsense knowledge, and we might hope that good performance of a model on the SAT cues good performance on commonsense tasks in general. 3.1.1. Task-Specific Setup. We adopt an unsupervised approach to answer the SAT questions. The cosine similarities between the choices and the stem pair are computed for each question and the choice that is most similar to the stem is picked as the predicted answer. 3.1.2. Results. Table I compares the performance of BagPack to that of recent studies in which various state-of-the-art text miners have been tested on the SAT challenge. 7 Overall, the performance of BagPack is not quite at the top of state-of-the-art, but com-parable to that of several recent systems, and in particular to those implemented by Baroni and Lenci [2010], that used a similar, slightly larger corpus (ukWaC, Wikipedia plus the 100M-word British National Corpus). We perform a more direct comparative evaluation of BagPack as a commonsense miner on the task of deciding whether an assertion from the ConceptNet knowledge base instantiates a certain commonsense relation or not. We compare BagPack with two recent text miners that are readily available to us. Latent Relational Analysis (LRA) was originally proposed by Turney [2006b], and was shown by its author to reach state-of-the-art performance at a number of different tasks in the original paper and subsequent work.
LRA is similar to BagPack in that it harvests lexical patterns that connect two concepts in context, but, unlike BagPack, it does not rely on contextual information relating to the two concepts separately. It tackles data sparseness by also harvesting patterns for pairs of concepts similar to the target ones (e.g., using automobile-wheel as a proxy for car-wheel ), and by applying singular value decomposition to the co-occurrence matrix representing the target pairs. Type-based Distributional Memory (TypeDM) was among the best models across a range of semantic tasks in the recent extensive evaluation by Baroni and Lenci [2010]. TypeDM exploits lexical patterns as well as syntactic dependency relations to attain a richer representation of the contexts in which pairs co-occur. Moreover, it tries to measure the variety of contexts in which the pairs co-occur, rather than simple frequency of co-occurrence. It uses a form of smoothing-by-similar-pairs analogous to the one for LRA.

For TypeDM, we exploited the precompiled version available from http://clic.cimec.unitn.it/dm . For LRA, we adopt the implementation described in Baroni and Lenci [2010], accepting all their parameter specifications. For both methods, the parameter choices we are inheriting have been shown to be effective across a number of tasks by Baroni and Lenci [2010]. The algorithms are trained on the same corpus used for BagPack, enriched with the 100M tokens of the British National Corpus. Both algorithms, like BagPack, produce a co-occurrence matrix representing pairs of words in terms of contextual information, that we then feed as feature vectors to the same supervised classifier, for full comparability. 3.2.1. Task-Specific Setup. Data . We focus on three of the five ConceptNet relations we will work with below when we test our combined text miner + game architecture, namely IsA, AtLocation, and HasProperty (see Section 4.1 for examples and discus-sion). For the remaining two relations (MotivatedByGoal and SymbolOf), the TypeDM model has very low coverage, and we do not want differences in coverage to trivially impact comparative performance. Also because of coverage issues, we restricted the choice of assertions from ConceptNet to those that do not involve concepts expressed by more than one word (TypeDM only contains single-word concepts). We remark that, independently of performance, the possibility of harvesting assertions containing mul-tiword concepts is a big advantage of BagPack over many related algorithms. Approx-imately 65% of the assertions represented in the five ConceptNet relations discussed in this article contain at least one multiword expression.

For each relation, we randomly picked from ConceptNet 4 250 assertions that exem-plify the relation and 250 assertions that, somehow, were introduced to ConceptNet by a user as instantiating the relation, but later demoted by other users as being mean-ingless. (for HasProperty we could only extract 244 assertions of this kind). The latter were used as challenging negative examples, since they are not totally random bind-ings of a relation and two concepts, but at one point they were conceived as a likely candidate of being true by at least one person.

Tuning the Models. As far as BagPack is concerned, for each pair represented in the dataset, a vector was constructed in the same way as for the SAT task. The TypeDM vectors were extracted from the precompiled TypeDM resource, and LRA vectors for the pairs were constructed following the Baroni and Lenci procedure [Baroni and Lenci 2010]. The following steps are identical for all models. For each relation we trained a SVM and tested its performance on a test set in a 10-fold cross-validation setting which was independently repeated 10 times itself. The results are based on the averages over all folds. Following the suggestion of Hsu and Chang [2003], before SVM training, values, from upper and lower bounds (the symbols  X   X  t and  X   X  t denote the average and standard deviation of the feature values, respectively). We use the C-SVM classifier as implemented in the Matlab toolbox of Canu et al. [2005] with a linear kernel and the cost parameter C set to 1. 3.2.2. Results. As a performance measure of the models, we use the area under the ROC curve (AUC). This measure can be interpreted as the probability that a classifier will rank a random positive instance higher than a random negative instance [Fawcett 2006]. An AUC value of 0.5 means chance performance. Traditional measures such as precision and recall are not suitable in our task for several reasons, including the following: (1) Both precision and recall need a threshold value applied on the posterior probabilities of the models, and deciding on a threshold is not a trivial task. For a fair and meaningful comparison of different models, we would need to carry out ex-tensive validation experiments, and that is beyond the scope of this article. (2) AUC is invariant to relative class distributions [Airola et al. 2011]. Considering that we do not have a reliable estimate on the prior class probabilities in general (i.e., the ratio of meaningful assertions among all possible candidate assertions that can be constructed by using a corpus), AUC allows us to focus on the discriminative power of a model without having to pick a threshold and without worrying about the class distributions of a particular dataset.

The average AUC values for the three relations are given in Table III. For two of the three relations, IsA and HasProperty, BagPack performs significantly better than DM and LRA. For AtLocation, we did not observe any significant difference between the performances of the models. Having evaluated BagPack on a generic semantic mining task (SAT) and compared its performance in recognizing commonsense assertions represented in ConceptNet, we now turn our attention to its application to commonsense mining, with the first of a series of experiments in which we attempt to extract new commonsense assertions of the sort that are stored in ConceptNet from the Web (note the difference with the classification task in Section 3.2, where the test set is also derived from CN, and thus no new assertions are mined from free Web text). A summary of the datasets that we use in this study is presented in Table II.

The training examples fed to BagPack come from the ConceptNet database, whereas evaluation is carried on a candidate set of assertions mined from Wikipedia. In this task, we use a support vector machine (SVM) that is trained on labeled examples of the training set (i.e., ConceptNet-based assertions) and the confidence scores of the SVM on the test set (i.e., Wikipedia-based candidate assertions) are used to rank and evaluate the performance.
 The initial training datasets are based on the assertions contained in ConceptNet 4, which is a freely available semantic network associating pairs of concepts with more than 25 semantic relations. In our study, we focus on five relations that represent rather different ways in which concepts are connected and correspond to more (IsA) or less (SymbolOf) traditional ontological relations. The relations tend, moreover, to link words/phrases from different syntactic classes: IsA ( cake, dessert ); AtLocation ( cake, oven ); HasProperty ( dessert, sweet ); MotivatedByGoal ( bake cake, eat ); SymbolOf ( Sacher Torte, Vienna ). These five relations altogether constitute approximately half of the assertions represented in ConceptNet 4.

The training datasets of each relation consist of approximately 500 assertions. To-gether, we call them the 5-relation ConceptNet dataset in Table II, above. Half of the assertions (SymbolOf is instantiated by 151 assertions only, and we used all of them) were randomly sampled from ConceptNet and the remaining assertions were constructed as bogus assertions by randomly picking an original assertion from the first half (e.g., Sacher Torte SymbolOf Vienna ) and changing (i) either one of its asso-ciated concepts with a random concept from ConceptNet (e.g., Sacher Torte SymbolOf win election ); or (ii) the original relation with another of the five relations we work with (e.g., Sacher Torte IsA Vienna ).

For annotation of the training dataset, we recruited a total of 22 expert raters, all advanced students or researchers in artificial intelligence, semantics or related fields. The raters were given precise instructions on the purpose of the procedure and had to annotate assertions as meaningful or meaningless . For each rater, we computed the probability of agreement with the majority vote on a random assertion and, as a precaution to ensure high-quality data, we discarded the responses of five raters with a probability lower than 0.70. Only the 2,051 assertions that received at least two meaningful or two meaningless responses were considered for further analysis. The final label of an assertion was decided by the majority vote, and the ties were broken in favor of meaninglessness. Table IV summarizes the annotation results for each relation. Note that some of the original assertions coming from ConceptNet were rated as meaningless (for example: bread IsA put butter ; praise IsA good job ; read newspaper MotivatedByGoal study bridge ). These assertions should serve as high-quality negative instances, given that they made their way into ConceptNet at one time as plausible assertions. Unlike in the SAT and ConceptNet classification tasks reported above in Section 3, where we are given a list of concept pairs in advance to extract commonsense asser-tions from free text, we need a way to harvest candidate assertions, which can then be ranked by the algorithm.

The candidate assertions are mined from the syntactically parsed Wikipedia corpus made available by the WaCky project (see link above). The top 10,000 most frequent verbs, nouns, and adjectives were considered as potential concept heads, and we ex-tracted potential concept phrases with a simple grammar aimed at spotting (the con-tent words of) noun, verb, and adjective phrases (for example, the grammar accepts structures like Adj Noun , Verb Adj Noun ,and Adv Adj ). In this phase, we were not interested in the semantic association between the concept pairs, but simply tried to generate lots of pairs to feed to the trained BagPack models.
 The pair extraction algorithm applied to Wikipedia produced 116,382 concept pairs. Then, we randomly sampled 5,000 pairs (containing 5,385 unique concept phrases) from this set, and generated 10,000 directed pairs by ordering them in both directions. Approximately 68% of the concepts in the sampled pairs were single words, 30% were 2-word phrases, 2% contained 3 or more words. Some example concept phrases that were mined are wing , sport team , fairy tale , receive bachelor degree , father X  X  death ,and score goal national team .

Finally, we associated the sample pairs with each of the five relations we study, ob-taining a set of assertions that contain the same concept pairs, but linked by different relations. This step resulted in 10,000 candidate assertions for each relation. In order to provide a gold standard for further analysis, two expert raters annotated approximately 400 assertions for each relation (Wikipedia candidates in Table II). Note that, unlike the annotated 5-relation ConceptNet dataset of Section 4.1, the gold standard is based on actual new candidate assertions mined from Wikipedia. The sample was picked post hoc (i.e., after carrying out the ranking procedure described in the next section), consisting of the candidate assertions that were ranked top by the BagPack models among the initial 10,000. The raters X  overall Cohen X  X  kappa was 0.37. The raters agreed on 183 meaningful assertions (8.8%) and 1508 meaningless assertions (72.6%). Any assertion that was annotated as meaningful by at least one rater was assumed to be meaningful for purposes of assessing performance, since the rate of meaningless assertions is so high that we reasoned it would be sufficient for an assertion to be at least potentially meaningful to be worth further inspection. As a side note, the observed Cohen X  X  kappa is very low compared to other tasks reported in the literature. However, in commonsense data annotation it is very hard to achieve higher agreement ratios. For example, in Schubert and Tong [2003], the reported mean kappa between pairs of raters on a 3-way decision task (involving categories  X  X rue, X   X  X alse, X  and  X  X ndecidable X ) is 0.375. Experiments with other raters and more detailed guide-lines lead us to kappas comparable to the one we are reporting, suggesting, together with Schubert and Tong X  X  results, that we might be hitting an upper bound on human agreement about corpus-derived commonsense assertions.
 For each of the five datasets coming from the previous step, we trained a separate BagPack model. We extracted the co-occurrence vectors from the Web-derived English Wikipedia and ukWaC corpora as we did for the SAT analogy and ConceptNet classifi-cation task.

Since ConceptNet concepts are often expressed by multiple words ( Sacher Torte , eat too much , ...), we empl oyed a shallow search for the concept phrases. Basically, for a single phrase, we looked for the occurrence of the constituents with possible inter-mittent extra elements. We say a concept occurs in a sentence if all its constituents occur in the same order in the sentence with possible intermittent extra elements, and if the concept phrase spans no more than twice its original length (e.g., if the concept is a 4-word phrase, it can span at most an 8-word range). Two concepts are said to be co-occurring if both concepts occur in the same sentence; that they do not overlap (i.e., the last word of the first concept comes before the first word of the second concept); and the range that both concepts span together is not longer than 20 words (i.e., at most 18 elements can occur between the first word of the first concept and the last word of the last concept). For efficiency reasons, a maximum of 1,000 sentences were used to extract co-occurrence statistics for a given pair. The features in the co-occurrence matrix were weighted by PMI [Church and Hanks 1990]. The features were restricted to the most frequent 5,000 lemmas in ukWaC, resulting in a 15,000-dimensional vector for each pair.

We use the same settings for SVM implementation and training that we used in 3.2 (e.g., feature scaling, linear kernel, etc.). Similar to the ConceptNet classification task we used in Section 3.2, we report the area under the ROC curve (AUC); see examples of ROC curves for AtLocation in Figure 3. Table V reports the AUC obtained by the BagPack models trained on the ConceptNet-based training set (5-relation ConceptNet) and evaluated on the gold stan-dard candidate assertions (Wikipedia candidates). For all relations, the performance of BagPack was significantly above the random baseline. However, AUC for Motivat-edByGoal was barely above chance level, and even the best AUC performance of 0.66 that was obtained on AtLocation was quite low, suggesting that BagPack alone cannot be used to extract reliable commonsense assertions from corpora despite its good com-parative performance in classifying ConceptNet-extracted relations that we reported in Section 3.2.

Please note that compared to the AUC values given in Section 3.2, current values reported are very low. In the previous ConceptNet learning task we used a  X  X losed X  dataset that contains only original ConceptNet assertions whose labels are based on ConceptNet. Presumably, this is an easier dataset compared to the Wikipedia-based test set, which contains assertions that are mined from Wikipedia.
 The Concept Game 8 (CG) is a game whose aim is to collect commonsense from laypeo-ple. It is based on the idea that production of verbal information is a significant burden on the players and, that it is possible to design enjoyable games that do not require the players to produce assertions. Therefore, the game aims to achieve its purpose not by having the players produce commonsense assertions, but having them verify already collected candidate assertions. This approach allows us to design fast-paced games where the interaction between the user and the game is limited to an expres-sion of assent. CG is presented in the context of a slot machine that produces random assertions. A meaningful assertion is a winning configuration. The trick is that the winning configurations do not dispense rewards automatically, but that they first have to be recognized by the player in order to  X  X laim their money X . In this way, players tell us which assertions they found meaningful.

The game consists of independent play sessions, each of which starts with an al-location of 40 seconds. First, the player sees three slots with images of rolling reels, which correspond to the left concept, relation, and the right concept of an assertion. Then, the contents of the slots are fixed one by one, with some values picked from the database and, as a result, an assertion is displayed. At that point, the player has to press one of two buttons labeled  X  X eaningless X  or  X  X eaningful X . If the player presses the Meaningful button, it can result in two different outcomes: either the displayed assertion is indeed meaningful and (s)he is rewarded with two points and two bonus seconds (i.e., true positives are rewarded), or the assertion is in fact meaningless, and the player loses three points and three seconds (i.e., false positives are penalized). However, pressing the meaningless button does not change the score or the remain-ing time (i.e., false negatives are not penalized nor are true negatives rewarded). The feedback is conveyed to the player visually and acoustically (e.g., in case of a reward, a green color flashes; in case of a penalty, a red color flashes). The reels roll again, and the process repeats. This continues until the end of the allocated time, which can get longer or shorter depending on rewards and penalties. A typical screenshot of the game is given in Figure 1.

In the previous description, we pretended that the game already knows which la-bels are meaningful, and rewards or penalizes the user accordingly. This is not the case here. In CG, we employ a validation policy similar to the honor-based, proof-of-payment method employed in many public transportation systems. In such a system, instead of checking every user, periodic controls are carried out to make sure the abuse of the system is effectively discouraged. In the current implementation, the probabili-ties of showing a known meaningless, a known meaningful, and a candidate assertion are 0.4, 0.3, and 0.3, respectively. In other words, 30% of the collected responses are for the new candidate assertions proposed by BagPack. For the candidate assertions, whatever the user responds is accepted as the correct answer, and this is the actual knowledge we want to harvest. The meaningless assertions are used to verify that the user is not abusing the game. The meaningful assertions are used to make sure the players score points and do not get frustrated. Note that increasing the precision of the candidate generation would help us to display more unknown assertions, without a significant impact on the game experience (i.e., players would still be able to score points without the support of the meaningful assertions we display). To allow the players to warm up, we implicitly trained them and did not show any unknown asser-tions until they completed three sessions with positive scores. Note that the produc-tion of assertions that are known to be meaningless is relatively cheap. We can use the candidate assertions that are designated to be meaningless by the players (this is what we do currently) or randomly combine concepts and relations to automatically gener-ate assertions that are likely to be meaningless.

Technically, the game is almost equivalent to asking a group of raters to tick those assertions from a list which they think make sense. This is a dull task, especially if there are few meaningful assertions compared to meaningless ones. In the context of a slot machine, however, the experience of seeing many meaningless assertions becomes part of the game, which creates an expectation in the player that (hopefully) resolves with a  X  X inning X  configuration. The relatively short session timing, combined with the need to be accurate because wrong claims are penalized, should keep the attention level of the players up, and consequently add to the fun. We made sure that players are aware of their achievements (they see total and session scores they have collected) and have an incentive to keep playing (we also display a top score list that shows the users who scored highest in a single session, and we implemented a ladder system where the players are represented by cute avatars). Taking advantage of the integration with Facebook, we asked the players X  permission to post their activity in our game to their public walls and give them the opportunity to invite their friends in order to go up in the ladder.

As of September 2011, 1145 Facebook users had tried the application, from whom 278 passed the implicit training session and actually contributed to our dataset. After the first visit to the application page, 67% of the contributers returned and played the game at least on one other day, while 36% returned at least on two other days. In total, we collected over 210,000 responses, approximately 65,000 of them for unknown, potentially meaningful, assertions.
 In the following two experiments, we implement the mining+gaming pipeline and eval-uate the quality of the assertions annotated by the players. In the first experiment, we kick-start the system by using the 5-relation ConceptNet dataset as the training data and compare the performance of BagPack alone and in combination with the Con-cept Game in the commonsense harvesting task. As a result, a new annotated dataset of commonsense knowledge is produced. In the second experiment, we bootstrap the combined systems and use the output of the first experiment as the training data for a second round of BagPack training. Then, we compare the performance of the original BagPack models and the bootstrapped BagPack models on a new test dataset (i.e., the evaluation dataset in Table II). 6.1.1. Experimental Setup. The experimental procedure is summarized in Figure 2. The seed assertions based on ConceptNet and manually annotated, as described in Section 4.1, serve as the BagPack training data (5-relation ConceptNet in Table II), and the set of assertions mined from Wikipedia serve as candidates to be ranked by the Concept Game. As already explained in Section 4.5, a separate BagPack model was trained for each relation, and the candidates were ranked according to the confidence scores of these models. For each relation, we kept the top 400 ranking assertions as the set to be fed into the game (Wikipedia candidates in Table II). We compare the performance of the labeling obtained from the game to the results we obtained by the  X  X ure BagPack X  approach in Section 4.5. Note that the gold standard for the Wikipedia candidates is obtained by the two expert raters X  annotation described in Section 4.3.
Once we ranked and filtered the candidate assertions by BagPack, we were ready to recruit players for their annotation via the Concept Game. For this purpose, 18 people were contacted by e-mail and invited to play the game, mostly college students and staff that the authors knew personally. Unlike the raters used in the previous steps, players were not experts. The game was open to this  X  X emi-public X  for approximately 10 days. We used the negative training assertions for control purposes to penalize players X  wrong decisions. 6.1.2. Results. In total, 25 players (7 presumably invited by those we contacted) re-sponded and provided a total of 5,154 responses for the candidate assertions. The ratio of players who scored an assertion as meaningful was the CG score of the assertion. In addition to CG scores, we already had the BagPack confidence scores of the assertions.
In our analysis, we considered the 1,838 assertions (Wikipedia candidates) that re-ceived at least two meaningful or two meaningless responses, distributed across rela-tions as shown in Table VI. The assertions that were labeled as meaningful consisted of 547 unique concepts: 86% of them were not attested in the 5-relation ConceptNet dataset (23% were not attested in the entire ConceptNet knowledge base); see Table II for a summary of the datasets.

Using the expert raters X  judgments as the gold standard for the candidate asser-tions, we computed relation-specific ROC curves for the CG. The areas under the ROC curves are given in Table VII; we repeat the BagPack X  X  AUC values from the experi-ment in Section 4.5 for easier comparison. As an illustration, the ROC curves obtained for AtLocation are given in Figure 3.

We observe that when the top BagPack candidate assertions are ranked by using the answers of the players, the performance considerably increases for all relations. This proves two points: First, BagPack alone is not sufficient to evaluate candidate assertions mined from Wikipedia reliably; and second, ConceptGame is able to improve performance. As an ad hoc evaluation, we computed Cohen X  X  kappa between the gold standard of the raters and the output of ConceptGame. The kappa value was 0.39 (comparable to the kappa value of 0.37 between the two raters themselves). Coupled with the high AUC values reported for the CG scores, we conclude that the annotation of the game players is comparable to manual annotation by experts. In the previous experiment, we saw that combining BagPack and Concept Game can lead to a high-quality dataset of commonsense knowledge. So far, our criterion for quality has been the judgments of two expert raters on a Wikipedia candidates dataset (see Table II). Now, we want to show that the output of the combined architectures can actually improve the performance on a more realistic task. Bootstrapping the entire system with its own output provides us with such a task. By bootstrapping, we mean training a new BagPack model only on the assertions mined from Wikipedia, with their labels decided by the Concept Game players. This experiment can be thought of as a continuation of the previous experiment, where we used the labeled candidate assertions (the output of kick-starting) as the training seed of a new round of BagPack training, like adding a  X  X rain arrow X  from the box at the bottom of Figure 2 to the Bag-Pack box. We compare the performance of the bootstrapped BagPack and the original (i.e., ConceptNet-based) BagPack on a new evaluation set. This will allow us to see if the output of the Concept Game is of sufficient quality to allow such a bootstrapping. 6.2.1. Experimental Setup. In this experiment, we employ two different BagPack train-ing seed assertion sets: The 5-relation ConceptNet dataset and the candidate asser-tions annotated by the Concept Game in the previous experiment (i.e., the Bootstrap dataset summarized in Table II). For obvious reasons, the latter is called the bootstrap dataset from now on. In addition, we combined the two into a third dataset, the com-bined dataset. Descriptive statistics for the ConceptNet-based and bootstrap training sets were already given in Tables IV and VI, respectively  X  the former as a result of expert annotation and the latter as a result of game playing.

To construct a final evaluation dataset (which will be called the evaluation dataset from now on), we randomly sampled approximately 1000 assertions for each relation by using the pairs mined from Wikipedia. We did not pick a set of candidates ranked by BagPack, as we did in the previous experiments, but used a random sample of assertions because we wanted to compare the performance of different BagPack models on the same evaluation set  X  ranking and filtering the assertions by any of the BagPack models would create a bias. We made sure, moreover, that, for each relation, the three corresponding BagPack seed datasets are disjoint with the evaluation set (i.e., they do not have any common assertions). The evaluation assertions were rated by the players of CG during a two-month period between April and May 2010, and they received at least two responses from different players. We already gave some details on this game-playing session at the end of Section 5. For the annotation of the evaluation dataset, the majority vote was used with ties broken in favor of being meaningless. The details of the evaluation dataset are given in Table VIII. 6.2.2. Results. In Figure 4, we report the area under the curve (AUC) values of the three BagPack models for each relation. The error bars represent the confidence inter-vals at the 95% significance level obtained by 5000 resamples with replacement.
For AtLocation, HasProperty, and SymbolOf relations, bootstrapping obtains even better results compared to using original ConceptNet-based assertions. For IsA and MotivatedByGoal, bootstrapping is almost as good as using the original training dataset. The combination of two datasets brings additional improvements in IsA and MotivatedByGoal, but the differences are not significant.

The main result of this experiment is that the data constructed with our method and no manual intervention, can be used as training data for a supervised algorithm and that it is as good, if not better, than expensive data obtained via expert annotation. Note that in this experiment, the gold standard for the evaluation dataset is provided by Concept Game, not by the experts, as the first experiment already showed that the output of Concept Game is of sufficient quality (see Bootstrap and Evaluation rows in Table II). The fact that both the bootstrap and evaluation datasets are annotated by the same process (i.e., Concept Game) may create an advantage for bootstrapping. Since the actual goal in this task is to find candidate assertions which are likely to be found meaningful by the players, we see the effect of a shared annotation mechanism as an opportunity to be seized, not a nuisance variable to be controlled. Nevertheless, to see the extent of the effect, one of the authors manually annotated the entire evalu-ation dataset, and we replicated the experiment with his ratings as the gold standard. The performance of the bootstrapped BagPack is not significantly worse than the orig-inal BagPack  X  its AUC values for the AtLocation, IsA, and HasProperty relations are higher than the original BagPack, although the differences are much less pronounced.
Another possible confounding factor that we considered is the amount of overlap between the datasets in terms of concepts. The bootstrap dataset and the evaluation dataset come from the same population of assertions that are mined from Wikipedia. Therefore, even though they are disjoint in terms of assertions, they have a signifi-cant number of common concepts, and that may be one of the reasons why we obtain better results by bootstrapping. On the average, 21% of the unique concepts in the bootstrap dataset also occur in the evaluation dataset. In comparison, approximately 16% of the unique concepts in the ConceptNet-based dataset occur in the evaluation dataset. In order to control for this shared-concepts effect, we marked all concepts that are part of positive instances in the evaluation dataset and removed all assertions that contain these concepts from both training datasets. As a result, the total number of assertions in the bootstrap dataset reduced from 1838 to 1646, and the number of assertions in the ConceptNet-based dataset reduced from 2051 to 1892. Predictably, almost all computed AUC values are lower compared to the previous experiments with the untouched datasets. However, even though the decrease in AUC for the bootstrap dataset is visibly higher, bootstrapping results are almost as good as the original Bag-Pack results. The fact that the gain from bootstrapping diminishes when we remove the common concepts from the training and evaluation sets is not discouraging, since in real-world settings the bootstrap datasets will come from the same population as the future evaluation sets (as it does in our case), therefore a certain amount of overlap is to be expected.

Due to space limitations we do not report the figures related to these two control experiments.
 Summarizing the second experiment, the bootstrap dataset  X  which was mined from Wikipedia, filtered by BagPack, and annotated by the 25 Concept Game players  X  is at least as successful as the 5-relation ConceptNet dataset, which depends on Concept-Net and was annotated by 22 experts, in seeding BagPack to extract commonsense knowledge from corpora. We conclude that, in our task of training BagPack to extract commonsense knowledge, the output of the Concept Game can be used instead of a dataset that is created by expert raters. This in turn bids well for application of Bag-Pack and Concept Game in other AI tasks requiring quality commonsense data. We showed how two different approaches to knowledge extraction can work in tandem and produce a new dataset of high precision commonsense knowledge. The combined mining+gaming architecture was able to produce significantly better commonsense facts than our quality text miner alone. Furthermore, we report that bootstrapping the system with its own output improves the performance. In our case, bootstrapping amounts to reducing the need for expensive expert-annotated data to a first seeding of the system.

In addition, we do not have to rely on the human contributors for the creation of new knowledge, but use the corpus (via the text miner) as the source. Presumably, this allows us to collect assertions about concepts which humans do not tend to state explicitly. This advantage was reflected in the fact that 23% of the concepts that we mined were not attested in the entire ConceptNet knowledge base.

Concept Game is a fully functional and public Facebook application. In the short term, we are looking for ways to make the game more attractive to a wider nonspecial-ized audience. We would like to convert the lemma sequences produced by BagPack into natural-sounding sentences. We have recently started to offer small gifts to top players as an incentive to start and keep on playing.
 A recent (and raw) snapshot of the data we collected is downloadable from the Web. 9 Once we gain a reasonably wide player-base and construct a larger dataset of common-sense assertions, we plan to share the dataset in a more structured form.

In future analyses, we would also like to look for cultural differences in assertions that receive contrasting ratings from players from different continents. Using Face-book as our platform allows us to access the demographics of players for statistical analysis.

While these and many other avenues of development and analysis should be pur-sued, we believe that our current results make a strong case for the feasibility of an approach that mixes text mining and social intelligence to harvest commonsense knowledge on a large scale.

