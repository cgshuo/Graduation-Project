 1. Introduction whereas fuzzy clustering can be used to handle the problem of vague boundaries of clusters. In fuzzy clustering, the requirement of crisp partition of the data is replaced by a weaker requirement of fuzzy partition, where the associations among data are repre-sented by fuzzy relations. Fuzzy clustering can be applied to a wide variety of applications like image segmentation, pattern recognition, object recognition, and customer segmentation etc. The Fuzzy C means (FCM) ( Bezdek, 1981 ) algorithm, proposed by
Bezdek (1981) , is the first and most widely used algorithm for clustering because it has robust characteristics for ambiguity and can retain much more information than hard segmentation meth-ods. FCM is the extension of the fuzzy ISODATA algorithm proposed by Dunn (1974 ). FCM has been successfully applied to feature omy, geology, medical imaging, target recognition, and image segmentation. In case the data is noisy, FCM technique wrongly classifies noisy objects because of its abnormal feature data.
Various approaches are proposed by researchers to compensate this drawback of FCM. Another similar technique, PCM, proposed by Krishnapuram and Keller (1993 ) interprets clustering as a in one or two clusters. To overcome the problem of identical clusters Pal et al. (2005) introduced PFCM that generates both membership and typicality values when clustering unlabeled data.
PFCM failed to give desired results if the data-set consists of unequal size clusters with noise. Dave ( Dave and Krishnapuram, Noise Clustering, which identifies outliers in separate cluster.
Problem with NC is that it does not identify outliers which are located in between the clusters and is not independent of number
Kaur and Gosain, 2010 ). Krishna K. Chintalapudi proposed Cred-ibility Fuzzy c means (CFCM) ( Chintalapudi and Kam, 1998 ), which on cluster centroids. Although CFCM is sound in reducing the more than one cluster ( Kaur and Gosain, 2011 ; Prabhjot Kaur and Gosain, 2010 ).

Recently, a new concept of partition, the credal partition, developed in the framework of belief function theory, has been introduced ( Denoeux and Masson, 2004 ; Denoeux and Smets, 2006 ; Masson and Denoeux, 2008 , 2009 ; Antoine et al., 2009 ). This concept generalizes existing concepts of hard, fuzzy (prob-ing (EVCLUS) ( Denoeux and Masson, 2004 ), Evidential c -means (ECM) ( Masson and Denoeux, 2008 ), Relational Evidential c means (RECM) ( Masson and Denoeux, 2009 ) and constrained evidentrial c means (CECM) ( Antoine et al., 2009 ) have been proposed in order to derive such credal partitions from data and to increase the robustness against outliers. All these algorithms are based upon Dampster X  X hafer theory of belief functions. They assign a basic belief assignment to each object in such a way that the degree of conflict between the masses given to any two objects functions into the cluster analysis domain. It was designed to handle relational data. It is applicable to both metric and non-metric dissimilarity data and does not use any explicit geome-trical model of the data. It only postulates that more similar two objects, the more plausible it is that they belong to the same cluster. ECM is the direct extension of FCM and Dave X  X  noise represented by the prototype and the similarity between an object and a cluster and is measured using Euclidean metric. ECM is computationally more efficient than EVCLUS when applied to object data. RECM is the relational version of ECM in the framework of belief functions. RECM provides similar results with EVCLUS but is computationally much more efficient than EVCLUS. Recently, CECM is proposed which is another variant of ECM taking into account the pairwise constraints. These constraints are translated into the belief function framework and integrated in the cost function.

All the above techniques, except NC approach and credal based clustering methods, do not result into efficient clusters because noiseless clusters.

The clustering output depends upon various parameters like the clustering method highly relies on the choice of distance metric. FCM used Euclidean distance as a distance measure thus can only able to detect hyper spherical clusters. Researchers have proposed various other distance measures like Mahalanobis distance measure, Kernel based distance measure in the data space and in high dimensional feature space so that non-hyper spherical/non-linear clusters can be detected ( Zhang and Chen, 2003 , 2004 ). Tsai and Lin (2011 ) proposed extension of FCM; FCM-s and KFCM-s by incorporating new distance metric into FCM and its kernelized version. They incorporate the distance variation of each individual data group into the FCM method to regularize the distance between a data point and the cluster centroid, and to able to detect non-hyperspherical clusters with uneven densities for linear as well as for non-linear separation. But these methods suffer with the same problem as conventional presence of noise.
 Earlier we proposed a technique called Density Oriented Fuzzy C -Means (DOFCM) ( Kaur and Gosain, 2011 ; Prabhjot Kaur and the data-set before creating clusters. DOFCM results into  X  n  X  1 X  outliers. DOFCM is based on the concept that if outliers are not required in clustering, their memberships should not be involved during clustering. We tried to nullify the affect of outliers by assigning them zero membership value during clustering. In this paper, we are further trying to improve the performance of
DOFCM and presenting Density oriented kernelized approach to fuzzy c -means using new distance metric (DKFCM-new) algo-rithm by incorporating the distance measure proposed by Tsai and Lin. It is the improvement over DOFCM by incorporating Kernel function and new distance metric.

Visualizing the clustering results can help to quickly assimilate this information and provide insights that support and comple-ment textual descriptions or statistical summaries. A standard way to display the result of clustering is the use of dendrograms, where bottom shows the initial element and each next row shows how clusters are combined. But still with this technique we are lacking the insight into the distribution of the elements. So we need a representation with the help of which a user can view any details he wants. One such representation is called calendar. But presentation. The combination of cluster analysis with a calendar representation provides good opportunities for interaction ( van Wijk and Edword, 1999 ).

The organization of the paper is as follows: Section 2 , briefly reviews Fuzzy C -Means (FCM) and its variants, Kernel based FCM (KFCM), Density Oriented FCM (DOFCM), FCM-s and KFCM-s . Section 3 describes the proposed algorithm, Density oriented
Kernel zed approach to fuzzy c -means using new distance metric (DKFCM-new). Section 4 evaluates the performance of the pro-posed algorithms using synthetic, standard and real life data-sets followed by concluding remarks in Section 5 . 2. Background information 2.1. The fuzzy c-means algorithm and its variants
This section briefly discusses the Fuzzy C -Means (FCM) and its variants, Kernel based FCM, Density Oriented FCM and FCM-s and of clusters are denoted by v i and d ik is the distance between x and v 2.1.1. The fuzzy c-means
FCM is the most popular fuzzy clustering algorithm. It assumes that number of clusters  X  c  X  is known in priori and minimizes the objective function ( J FCM )as J  X  where d ik  X  : x k v i : , and u ik is the membership of pixel  X  x cluster  X  i  X , which satisfies the following relationship: u ik  X  1 , i  X  1 , 2 , ... , n  X  2  X 
Here  X  m  X  is a constant, known as the fuzzifier (or fuzziness index), which controls the fuzziness of the resulting partition. m  X  2 is used in this paper. Any norm : * : can be used for iteration scheme known as the alternating optimization techni-que. The conditions for local extreme for (1) and (2) are derived using Lagrangian multipliers: u  X  where 1 r i r c ; 1 r k r n and v i  X  continuous update of U and V , until 9 U ( l  X  1) U (l) 9 o the number of iterations. FCM works fine for the images which are not corrupted with noise but if the image is noisy or distorted then it wrongly classifies noisy pixels because of its abnormal feature data which is pixel intensity in the case of images, and results in an incorrect membership and improper segmentation. 2.1.2. Possibilistic C-Means clustering (PCM) and Keller (1993 ) relaxed the column sum constraint and proposed a possibilistic approach to clustering by minimizing objective function as J PCM U , V  X  X  X  where Z k are suitable positive numbers. The first term tries to reduce the distance from data points to the centroids as low as that in FCM but the membership matrix of PCM is updated as or identical clusters. 2.1.3. Possibilistic Fuzzy C-Means clustering (PFCM) possibilistic approach and hence, it has two types of member-ships, viz. a possibilistic ( t ki ) membership that measures the absolute degree of typicality of a point in any particular cluster sharing of point among the clusters. PFCM minimizes the objec-tive function as J PFCM U , V , T  X  X  X  subject to the constraint that define the relative importance of fuzzy membership and typi-cality values in the objective function. The minimization of objective function gives the following conditions: and and v  X 
Though PFCM is found to perform better than FCM and PCM but when two highly unequal sized clusters with outliers are given, it fails to give desired results. 2.1.4. Noise clustering (NC)
Noise clustering is introduced by Dave to overcome the major given by d  X  d , 8 i  X  13  X 
The NC algorithm considers noise as a separate class. The membership u * i of x i in a noise cluster is defined as u  X  1 NC reformulates FCM objective function as J for k  X  n  X  c  X  1. d  X  and membership equation is u  X  Noise clustering is a better approach than FCM, PCM, and
PFCM. It identifies outliers in a separate cluster but does not result into efficient clusters because it fails to identify those outliers which are located in between the regular clusters (refer
Section 4.2 ). Its main objective is to reduce the influence of sets usually contain cluster structures that differ from our assumptions. So a clustering technique should be independent of the number of clusters for the same data-set. In NC, noise distance is given as (16).

Here, noise distance depends upon distance measure, number of assumed clusters, and l , which is the value of multiplier used is interpreted that if the number of clusters is increased, d assumes high values. NC assigns only those points to noise cluster whose distance from regular clusters is less than the distance the same data-set, NC does not detect outliers, because in that scenario, the average distance between points and regular clus-ters decreases and the noise distance remains almost constant or
NC degrades when the number of outliers are increased for the same data-set (refer Section 4.2 ). 2.1.5. Credibilistic Fuzzy C Means (CFCM)
Krishna K. Chintalapudi proposed Credibilistic Fuzzy C -means (CFCM) to reduce the effect of outliers by introducing a new variable, credibility. Credibility is a function that assumes low value for outliers and high for non-outliers. CFCM defines cred-ibility as c  X  1 a k is the distance of vector x k from its nearest centroid. The The parameter y controls the minimum value of c k so that the Setting y  X  1 reduces the scheme to FCM while y  X  0 assigns zero memberships to the noisiest vector. CFCM partitions data set by minimizing: J Subject to the constraint
Since for outliers, credibility is very small so the memberships generated by CFCM for outliers are smaller than those generated with FCM. Although, it is superior to FCM, PCM, and PFCM but we observed that most of the time it assigns some outlier points to more than one cluster (refer Section 4.2 ). Another problem identified by Chen and Wang (1999 ) is that credibility is esti-mated by unstable prototypes that are undergoing the conver-gence process and result affects with the change in initial prototypes. Moreover, it does not separate outliers so noiseless clusters are not obtained. Its main emphasis is only to reduce the effect of outliers on regular clusters. 2.2. Kernel based approach
The kernel function can be applied to any algorithm that solely depends on the dot product between two vectors. Wherever a dot product is used, it is replaced by a kernel function. When done, linear algorithms are transformed into non-linear algorithms. Those non-linear algorithms are equivalent to their linear origi-nals operating in the range space of a feature space j . However, because kernels are used, the j function does not need to be ever explicitly computed. This is highly desirable, as sometimes our higher-dimensional feature space could even be infinite-dimen-sional and thus infeasible to compute.

A kernel function is a generalization of the distance metric that measures the distance between two data points as the data points are mapped into a high dimensional space in which they are more clearly separable. By employing a mapping function F  X  x  X  , which separable data structure existing in the original data space can possibly be mapped into a linearly separable case in the higher dimensional feature space.

Given an unlabeled data set X  X  { x 1 , x 2 , y , x n } in the p -dimensional space R P , let F be a non-linear mapping function from this input space to a high dimensional feature space H : F : R p -H , x -F  X  x  X 
The key notion in kernel based learning is that mapping function F need not be explicitly specified. The dot product in the high dimensional feature space can be calculated through the kernel function K  X  x i , x j  X  in the input space R P . Kx i , x j  X  F x i  X  X  F x j  X  21  X 
Consider the following example. For p  X  2 and the mapping function F
F : R 2 -H  X  R 3 x i 1 , x i 2  X  X  - X  x 2 i 1 , x 2 i 2 , ffiffiffi Then the dot product in the feature space H is calculated as
F x  X  X  U F x j  X  x 2 i 1 , x 2 i 2 , where K -function is the square of the dot product in the input space. We saw from this example that use of the kernel function makes it possible to calculate the value of dot product in the feature space H without explicitly calculating the mapping func-tion F . Some examples of kernel function are:
Example 1: Polynomial Kernel: K  X  x i , x j  X  X  X  x i x j  X  c  X  c
Z 0 , d A N
Example 2: Gaussian Kernel: K  X  x i , x j  X  X  exp  X  X  : x i where s 4 0
Example 3: Radial basis Kernel: K  X  x i , x j  X  X  exp  X  P x a function.

Example 4: Hyper Tangent Kernel: K  X  x i , x j  X  X  1 tanh  X  X  x = s 2  X  X  , where s 4 0 2.3. Kernel version of Fuzzy C-means (KFCM)
KFCM represents the kernel version of FCM by exploiting a cluster centers i.e. mapping the data points from the input space to a high dimensional space.
 KFCM modify the objective function of FCM with the mapping F as J  X 
F v  X  X  : The distance in the feature space is calculated through the kernel in the input space as follows:
F  X  : F x k  X  X  F  X  v i  X  : 2  X  F x k  X  X  F  X  v i  X   X  X  X  F x k  X  F x k  X  X  F x k  X  X  2 F x k  X  X  F v i  X  X  X  F v i  X  X  F v i  X  X   X  Kx k , x k  X  X  2 Kx k , v i  X  X  X  Kv i , v i  X  X  X  24  X 
If kernel width is a positive number then Kx , x  X  X  X  1. Thus (23) can be written as J  X 
Minimizing (25) under the constraint of U , we get u  X  v  X  2.4. Density Oriented Fuzzy C Means (DOFCM)
DOFCM separates noise into different cluster. It modifies the of density of points in the data. Neighborhood membership of a point  X  i X  in the data-set  X  X  X  X sdefinedas point i; Z max is the maximum number of points in the neighborhood of any point in the data-set.
 calculated as per (28) and from the complete range of neighbor-hood membership values, depending on the density of points in considered as an outlier if its neighborhood membership is less than  X  a  X . Let  X  i  X  be a point in the data-set  X  X  X  , then if to zero.
 where d ki  X  : x k v i : .
 cluster. Hereafter, it will use i  X  c  X  1.
 on fuzzy membership is extended to 0 r instead of the following which was used in conventional FCM algorithm 2.5. Fuzzy c-means with new distance metric (FCM-s ) new distance measure into the conventional FCM. New distance metric is defined as ^
Membership and modified cluster center equations are u  X  where 1 r i r c ; 1 r k r n and v  X  2.6. Kernel Fuzzy c-means with new distance metric (KFCM-s )
Conventional FCM and FCM-s can only deal with linearly separable data points in observation space. The observed data-set can be transformed to higher dimensional feature space by applying a non-linear mapping function to achieve non-linear separation. KFCM-s incorporates a new distance measure into the mapped feature space. New distance metric is defined as: ^
F where F s i is the weighted mean distance of cluster i in the mapped feature space and is given by
F KFCM-s minimizes objective function: J satisfies the following relationship: and : F x k  X  X  F  X  v i  X  : 2 is the square of distance between F x
F v k  X  X  : As per (24), minimizing (41) w.r.t. U , we get u  X  v  X 
The KFCM-s algorithm allows the clustering of non-hyper-spherically shaped data with uneven densities in the mapped feature space and achieved nonlinear separation of the data in the observation space.

Although, the results of FCM-s and KFCM-s are efficient for noiseless data, they do not perform well in the presence of noise. 3. The proposed technique 3.1. Density oriented Kernelized approach to Fuzzy C-means with new Distance metric (DKFCM-new)
After explaining KFCM, KFCM-s and DOFCM, we are now in a position to construct the density oriented kernel version of FCM using new distance measure.

DKFCM-new is designed with the motive to remove the affect of noise on the cluster centroid locations and on the output of resulting clusters for data as well as high dimensional feature space. Like NC technique, DKFCM-new results in  X  n  X  1 X  clusters with  X  n  X  good clusters and one noise cluster. Proposed algorithm first identify outliers and then apply clustering algorithm to contruct noiseless clusters. 3.2. Identification of outliers
DKFCM-new identifies outliers on the basis of density of points neighborhood. DKFCM-new defines density factor, called neigh-borhood membership, which measures density of an object in relation to its neighborhood. As per the technique, the neighbor-least a minimum number of other points to become a good point (non-outlier). Shape of the neighborhood is determined by the choice of a distance function used for two points x 1 and x 2D space, the neighborhood shape is a rectangle and by using Euclidean distance it is spherical Neighborhood membership of a point  X  i  X  in the data-set  X  X  X . is defined as M point i , Z max is the maximum number of points in the neighbor-hood of any point in the data-set.
 q E X 9 dist  X  i , q  X  r r neighborhood the distance between points  X  i  X  and  X  q  X .

Neighborhood membership of each point in the data-set  X  X  X  X s calculated as per (45) and from the complete range of neighbor-hood membership values, depending on the density of points in considered as an outlier if its neighborhood membership is less a point in the data-set  X  X  X  whose neighborhood membership is  X  X  X  , then if: M to zero. 3.2.1. Selection of the threshold value  X  a  X 
Ideally, a point will be outlier only if no other point is present in its neighborhood i.e. when neighborhood membership is zero or threshold value  X  a  X   X  0. However, in the proposed scheme, a point is considered as an outlier when its neighborhood member-outlier. Its value depends upon the nature of data-set i.e. how sets. Various steps to select threshold value are:
Step 1 : A neighborhood radius is calculated i.e. r neighborhood
Calculation of neighborhood radius is done as per Ester et al. (1996 ).

Step 2 : Considering the circular region using r neighborhood number of points in the local neighborhood of every point in i.e. Z max (this is done to normalize the local membership values).
 normalize the local membership values between 0 and 1).
Step 5 : By visually observing the data-set, we select a parti-values and its corresponding M neighborhood value is selected as threshold value ( a ).

This concept can be best realized through example. Let x 1 x , k x 1 has two points in its neighborhood ( k 1 and k 2 ), x 2 points ( p 1 , p 2 ,and p 3 ), x 3 has 15 points, and x 4 neighborhoods. As x 1 , x 2 , k 1 , k 2 , p 1 , p 2 ,and p be considered as an outlier if no other point is present in its with real life data-sets. To tackle this problem, the proposed observing the data-set carefully.

With this proposed condition, from Fig. 1 , after visual observa-tions, we have selected that a point will be considered as an outlier if the number of points in their neighborhood is less than example we are assuming the Z max value. So, the threshold value  X  a  X  is calculated as For considering a point to be an outlier, we have selected Threshold value  X  a  X  X 
Hence, all the points whose neighborhood membership is less the above points are:
Point x 1 : M neighborhood  X  2/15  X  0.133, which is less than 0.266. Hence, x 1 is an outlier.

Point x 2 : M neighborhood  X  3/15  X  0.20, which is less than 0.266. Hence, x 2 is an outlier.

Point x 3 : M neighborhood  X  15/15  X  1.0, which is more than 0.266. Hence, x 3 is not an outlier.

Point x 4 : M neighborhood  X  9/15  X  0.60, which is more than 0.266. Hence, x 4 is not an outlier.
 visually verified. Let us observe it with synthetic data-sets, D45 shows identification of outliers by changing the threshold value  X  a  X  with data-set D45. We observed from Fig. 2 and Table 1 that large values of  X  a  X  lead to more compact clusters with more number of outliers. As  X  a  X  -0, DKFCM-new behaves as KFCM-s . Proper selection of  X  a  X  would provide better results. done as follows. 3.3. Clustering process 3.3.1. Formulation distance metric and modify its objective function with the mapping F as follows: where u ki is the membership of pi x el x i in cluster k and ^
F From (49), DKFCM-new objective function can be re-written as J where F s i is the weighted mean distance of cluster i in the mapped feature space and is given by
F and : F x k  X  X  F  X  v i  X  : 2 is the square of distance between F x
F v i  X  X  : The distance in the feature space is calculated through the kernel in the input space as follows:
F
As we are adopting radial basis kernel in the propose techni-que so Kx , y  X  X  X  exp where h is defined as kernel width and it is a positive number, then Kx , x  X  X  X  1 : Thus (50) can be written as J J
Given a set of points X , we minimize J DKFCM -new in order to determine u ki and v i . We adopt an alternating optimization approach to minimize J DKFCM -new and need the following theorem:
Theorem 1. The necessary conditions for minimizing J DKFCM new under the constraint of U, we get:
Minimizing (54) under the constraint of U , we get u  X 
But as per the definition of DKFCM-new, we are assigning zero membership to the points which are identified as outliers. So the modified membership equation for DKFCM-new is u  X 
Here the constraint on fuzzy membership is extended to 0 r
And cluster centers are calculated as v  X  Proof. We differentiate J DKFCM -new with respect to u ki are given in Appendix A.
 Various steps of the proposed algorithm are:
Density oriented kernel ZED approach to fuzzy c -means by Output : Cluster centroids matrix, Outlier vector, and Identification of outliers Step 1 : for i  X  1,2,3, y , n ; do: (a) Calculate the number of points in the neighborhood of (b) Select Z max the data-set from the whole range of neighborhood membership values.
 Clustering process Step 4 : Determine initial centroids v k .
 Step 5 : Initialize the membership u ki and update the memberships of all the outliers to zero Step 6 : for n  X  1, 2, 3, y max_iter;do: (a) Update all centroids v n i using (59) (b) Update all membership values u ki using (57) (c) Compute objective function ( O n ) using (55) (d) Compute E n  X  max 9 O n O n 1 9 ,if E n ( , stop; 4. Results and simulations
In the first subsection, we have compared proposed method with the six methods: FCM, KFCM, DOFCM, FCM-s , and KFCM-s . In the second subsection, we compared DKFCM-new with the other well known noise resistant methods like PCM, PFCM, CFCM and NC. In the last subsection, proposed method is compared with credal partition based evidential clustering methods. Experiments are implemented and simulated using MATLAB Version 7.0 on Core-2 Due processor, 2.0 GHz with 4 GB RAM.

We considered following common parameters: m  X  2, which is a common choice for fuzzy clustering, e  X  0.0001, maximum iterations  X  100. 4.1. Comparison with FCM, KFCM, DOFCM, FCM-s and KFCM-s Example 1. Data-set: Diamond data-set (D11), D12 (referred from Pal et al. (2005 )). D11 is a noiseless data-set of points x fg 11 i  X  1 . D12 is the union of D11 and an outlier 12.
Algorithms: FCM, KFCM, DOFCM, FCM-s ,KFCM-s and DKFCM-new Number of Clusters: 2 (Identical data with Noise) Fig. 3 shows clustering results of FCM, KFCM, FCM-s ,KFCM-s , DOFCM and DKFCM-new.  X  *  X  symbol shows centroids. Table 2 lists the centroids generated with the algorithms. It is observed from the figure that FCM could not detect the clusters and its perfor-mance is badly affected with the presence of even single outlier.
FCM-s although detected the clusters but the centroids are diverted towards the outlier. Centroids generated with KFCM and KFCM-s are not much affected with the presence of noise but the figure and the final clusters are including the noise point because they are not identifying outliers. DOFCM and DKFCM-new without noise. It is observed from the Fig. 3 and Table 2 that
DKFCM-new has detected almost original centroid locations and its The ideal (true) centroids of data-set D11 are:
V  X 
To show the effectiveness of the proposed algorithm, we also KFCM-s /DOFCM/DKFCM-new. Table 3 lists the error percentage.
Example 2. Data-set: Dunn, 2-dimentional Square data ( Dunn, 1974 ) (142 points)
Algorithm: FCM, KFCM, DOFCM, FCM-s , KFCM-s and DKFCM-new Number of Clusters: 2
DUNN Square data-set with noise is used in this example. It
FCM methods have improved the per formance in comparison to FCM are not producing original clusters. DOFCM and DKFCM-new have identified the outliers but the cl uster locations generated with DKFCM-new are almost close to ideal centroids.
 The ideal (true) centroids of DUNN data-set are
V  X  Table 3 lists the error percentage.

Example 3. Data-set: Bensaid ( Bensaid et al., 1996 ) 2-dimen-tional data (213 points) Algorithm: FCM, KFCM, DOFCM, FCM-s ,KFCM-s and DKFCM-new
Number of Clusters: 3 clusters with 2 small and 1 big size clusters
Bensaid X  X  two-dimensional data-set consisting of one big and structure of this set but have increased count of core points and added uniform noise to it, which is distributed over the region [0,120] [10,80]. Fig. 5 shows the clustering results of all the methods. The clustering results of FCM method, whether in the observed space or in the feature space or including the new distance, in all the cases is highly affected with the presence of noise
FCM-s and KFCM-s are not able to detect clusters properly as in both the cases. Although, both DOFCM and DKFCM-new have identified noise and original clusters. But DKFCM-new outper-formed its earlier version.
 4.1.1.1. High dimensional data-set rithm on well known real data-sets namely Wisconsin breast cancer
Huang X  X  accuracy measure ( r )( Huang and Ng, 1999 ). r  X  where n i is the number of data occurring in both the i th cluster value r  X  1.

Example 4. Data-set: Wisconsin Breast cancer data set, 30-dimentional data (569 points) Algorithm: FCM, KFCM, DOFCM, FCM-s ,KFCM-s and DKFCM-new Number of Clusters: 2 clusters Size of Clusters: 357,212 distinguish between benign and malignant cancers based on the available 30 attributes. The original database contains 569 instances. The class distribution is 357 benign and 212 malignant, respectively. Table 4 shows misclassification and the accuracy is given by DKFCM-new, in which out of 569 points only 08 points are misclustered with the accuracy value of 0.9929. KFCM-s is close to DKFCM-new with 14 misclassifications. KFCM and FCM-s have almost same performance with 92 and 94 misclassifications with the accuracy values of r  X  0.9490 and 0.9173 respectively. Size of clusters: 50, 50, 50
It is a four-dimensional data set containing 50 samples each of well separated from the other two, while classes 2 and 3 have some overlap. We made several runs of these algorithms and the proposed method. In the case of Iris data-set, DKFCM-new has detected the original clusters with the accuracy value of 1. As indicated in Table 5 , FCM has 20 misclassifications when com-tions made by KFCM and KFCM-s are 02 and 04 respectively. 4.2. Comparison of DKFCM-new with other noise resistant methods like PCM, PFCM, CFCM and NC
We also compared the proposed method with other well known comparison of DKFCM-new with PCM, PFCM, CFCM and NC with D12 clusters. The centroids of NC and CFCM are not much affected with NC and DKFCM-new separate the outlier but as compare to NC,
DKFCM-new generated almost original centroids. Table 6 lists the centroid locations and error percentage for the above methods.
Noise resistant methods should be independent of the location of outliers i.e. the method should be able to identify outliers locating anywhere in the data-set. Problem with NC is that it is not independent of the location of outliers. It does not identify outlier which lies in between the two clusters. To prove, we are approximately equal size with noise and the data points are distributed over two dimensional spaces. Figs. 7 (a) and (b) shows original data-set and its noisy version. From Fig. 7 , it can be one which is lying in between the clusters. Consider a labeled point  X  X  X  as shown in Fig. 7 (b) (which is an outlier). As per NC approach, this point cannot be considered as an outlier because its membership degree to the noise cluster can never be less than the distance from its regular clusters as it is located in between the regular clusters ( Dave and Krishnapuram, 1997 ; Dave, 1991 , 1993 ). This scenario can never be justified with NC approach.
DKFCM-new has detected it as an outlier because the local that DKFCM-new has detected original clusters.
 of clusters for the same data-set ( Rehm et al., 2007 ). NC is not independent of the number of clusters i.e. if the number of clusters is increased for the same data-set, it does not identify outliers whereas DKFCM-new is independent of number of clusters for the same data-set. To test this, same data-set is divided into four clusters. Fig. 8 shows that NC could not detect whereas DKFCM-new detected all the outliers.
 of outliers i.e. performance of the algorithm should not change on increasing the number of outliers in the same data-set. Fig. 9 shows the performance comparison of NC and DKFCM-new when we increased the number of outliers from one to four in the D12 data-set.

It is visually verified that the performance of NC is degraded as we increased the number of outliers whereas the cluster centroids did not change in case of the proposed method. 4.3. Comparison of DKFCM-new with credal partition based evidential clustering methods
Credal partition based method are developed within the framework of belief functions and based upon Dempster X  X hafer theory of evidence ( Denoeux and Masson, 2004 ; Denoeux and
Smets, 2006 ; Masson and Denoeux, 2008 , 2009 ; Antoine et al., 2009 ). A credal partition extends the existing concepts of hard, fuzzy (probabilistic) and possibilistic partition by allocating a mass of belief for each object. It represent partial knowledge regarding the class membership of an object i by a bba ( basic belief assignment) m i on the set O  X  { o 1, y , o c }. Evidential C -means (ECM) is inspired from FCM and NC algorithm.
A credal partition ( Masson and Denoeux, 2008 ) is defined as of partitioning:
If a credal partition from object data is derived then we have to determine, for each object i , the quantities m ij ; between an object and any nonempty subset of O has thus to be defined like NC. Like fuzzy clustering assumes that each class is represented by the center v k ; it associated barycenter v subset A j of O .

Barycenter v j is associated to A j by v  X  where c j  X  9 A j 9 denotes the cardinality of A j . s  X 
The distance d ij is defined by d 9 : x i v j : 2  X  63  X 
In credal framework, empty set is equivalent to the noise cluster. The objective function of ECM is J ECM  X  M , V  X  9 Subject to
The criteria of ECM is similar to J NC except that an additional method. The fundamental difference between ECM and NC is that a credal partition has more degrees of freedom than a fuzzy one.
The performance of NC and credal clustering based methods (ECM, RECM, CECM) is similar. They identified the outliers and separate then in the empty set. But, as there is a need to set a distance in case of NC; these algorithms will not be independent data-set.

DKFCM-new is the extension and kernelized version of FCM with a new distance metric which also consider the variation of clusters whereas in credal based methods, Euclidean distance is used which can only detect hyper spherical clusters.

Thus, it is evident that DKFCM-new performed better as compared 5. Conclusion
In this paper, we proposed a robust kernel approach to the earlier proposed method. Proposed method used a new distance variation of data-points within each cluster to DOFCM in the feature space. The DKFCM-new with the new distance metric significantly shows improvement over DOFCM and other compe-titive methods. To test the performance of proposed method it is applied to synthetic data-sets, Standard data-sets like DUNN and
Bensaid data-set, and to real life data-sets like Wisconsin Breast cancer and Iris data-set and compared with various kernel based methods. The performance of proposed method is also compared with other robust clustering methods like PCM, PFCM, CFCM, NC and credal based clustering methods like ECM, RECM, CECM. It has been found that proposed algorithm significantly outperformed its earlier version and was found highly robust to noise and outliers. Appendix A. Kernel version of Density oriented Fuzzy C -means with new distance metric (DKFCM-new) with new distance metric
In this appendix, we give the proof of DKFCM-new which is the kernelized version of density oriented fuzzy c -means clustering algorithm by incorporating new distance metric. The problem of minimization of objective function J DKFCM -new {given in (48)} subjected to the constraint specified by (2) is solved by minimiz-ing a constraint free objective function defined as J where l i  X  i  X  1 , 2 , 3 , ... , c  X  are Langrangian multipliers. u ki and v k , yields the solution for the problem: ^ Kx , y  X  X  X  exp
Kx , y  X  X  X  exp J x k v i J where h is defined as kernel width and it is a positive number, then Kx , x  X  X  X  1 : Hence, (67) will become : F x k  X  X  F  X  v i  X  : 2  X  X  1 Kx k , v i  X  X  X 
A.1.1. Partial derivative of J DKFCM -new with respect to v @ J @ v i  X  @ J @ v i  X  0 v i  X 
A.1.2. Partial derivative of J DKFCM -new with respect to u @ u ik  X  mu @ u ik  X  0 ) mu ) u ik  X 
In view of (72), Eq. (73) can be written as ) u ik  X 
By using equation, : F d 2 be re-written as u  X  References
