 The technique of k -nearest neighbor (kNN) is one of the most popular classifica tion algorithms. Several reasons account for the widespread use of this metho d: it is straightforward to implement, it generally leads to good recognition performance thanks t o the non-linearity of its decision bound-aries, and its complexity is independent of the number of cla sses. In addition, unlike most alterna-tives, kNN can be applied even in scenarios where not all cate gories are given at the time of training, such as, for example, in face verification applications wher e the subjects to be recognized are not known in advance.
 The distance metric defining the neighbors of a query point pl ays a fundamental role in the accuracy of kNN classification. In most cases Euclidean distance is us ed as a similarity measure. This choice fair to assume that all features are equally scaled and equal ly relevant. However, in most cases the data is distributed in a way so that distance analysis along s ome specific directions of the features space can be more informative than along others. In such case s and when training data is available in advance, distance metric learning [5, 10, 4, 1, 9] has been shown to yield significant improvement in kNN classification. The key idea of these methods is to appl y transformations to the data in order to emphasize the most discriminative directions. Euclidea n distance computation in the transformed space is then equivalent to a non-uniform metric analysis in the original input space. In this paper we are interested in cases where the data to be us ed for classification is very high-dimensional. An example is classification of imagery data, w hich often involves input spaces of thousands of dimensions, corresponding to the number of pix els. Metric learning in such high-dimensional spaces cannot be carried out due to overfitting a nd high computational complexity. In these scenarios, even kNN classification is prohibitively e xpensive in terms of storage and com-putational costs. The traditional solution is to apply dime nsionality reduction methods to the data and then learn a suitable metric in the resulting low-dimens ional subspace. For example, Princi-pal Component Analysis (PCA) can be used to compute a linear m apping that reduces the data to tractable dimensions. However, dimensionality reduction methods generally optimize objectives un-related to classification and, as a consequence, might gener ate representations that are significantly less discriminative than the original data. Thus, metric le arning within the subspace might lead to suboptimal similarity measures. In this paper we show that b etter performance can be achieved by directly solving for a low-dimensional embedding that opti mizes a measure of kNN classification performance.
 Our approach is inspired by the solution proposed by Weinber ger et al. [9]. Their technique learns a metric that attempts to shrink distances of neighboring si milarly-labeled points and to separate points in different classes by a large margin. Our contribut ion over previous work is twofold: In this section we briefly review the algorithm presented in [ 9] for metric learning in the context dimensional projections of the inputs via a novel direct opt imization.
 A fundamental characteristic of kNN is that its performance does not depend on linear separability of classes in input space: in order to achieve accurate kNN cl assification it is sufficient that the majority of the k -nearest points of each test example have correct label. The work of Weinberger et al. [9] exploits this property by learning a linear transf ormation of the input space that aims at creating consistently labeled k -nearest neighborhoods, i.e. clusters where each training example and its k -nearest points have same label and where points differentl y labeled are distanced by an additional safety margin. Specifically, given n input examples x class labels y the following objective function:  X  ( L ) = X where  X  of first aims at pulling closer together points sharing the same label and that were neighbors in the original space. The second term encourages distancing each example x points by an amount equal to 1 plus the distance from x points. This term corresponds to a margin condition similar to that of SVMs and it is used to improve generalization. The constant c controls the relative importance of these two competing ter ms and it can be chosen via cross validation.
 Upon optimization of  X  ( L ) , test example x projection x  X  interpreted as kNN classification in the original input spac e under the Mahalanobis distance metric induced by matrix M = L T L . Although Equation 1 is non-convex in L , it can be rewritten as a semidefinite program  X  ( M ) in terms of the metric M [9]. Thus, optimizing the objective in M guarantees convergence to the global minimum, regardless o f initialization.
 When data is very high-dimensional, minimization of  X  ( M ) using semidefinite programming meth-ods is impractical because of slow convergence and overfitti ng problems. In such cases [9] propose applying dimensionality reduction methods, such as PCA, fo llowed by metric learning within the resulting low-dimensional subspace. As outlined above, th is procedure leads to suboptimal metric learning. In this paper we propose an alternative approach t hat solves jointly for dimensionality reduction and metric learning. The key idea is to choose the t ransformation L in Equation 1 to be a nonsquare matrix of size d  X  D , with d &lt;&lt; D . Thus L defines a mapping from the high-dimensional input space to a low-dimensional embedding. Euclidean dist ance in this low-dimensional embed-ding is equivalent to Mahalanobis distance in the original i nput space under the rank-deficient metric M = L T L ( M has now rank at most d ).
 Unfortunately, optimization of  X  ( M ) subject to rank-constraints on M leads to a minimization prob-lem that is no longer convex [8] and that is awkward to solve. H ere we propose an approach for minimizing the objective that differs from the one used in [9 ]. The idea is to optimize Equation 1 directly with respect to the nonsquare matrix L . We argue that minimizing the objective with respect to
L rather than with respect to the rank-deficient D  X  D matrix M , offers several advantages. First, our optimization involves only d D rather than D 2 unknowns, which considerably reduces the risk of overfitting. Second, the optimal rectangular matrix L computed with our method automatically sat-isfies the rank constraints on M without requiring the solution of difficult constrained min imization problems. Although the objective optimized by our method is also not convex, we experimentally demonstrate that our solution converges consistently to be tter metrics than those computed via the application of PCA followed by subspace distance learning ( see Section 4).
 We minimize  X  ( L ) using gradient-based optimizers, such as conjugate gradie nt methods. Differen-tiating  X  ( L ) with respect to the transformation matrix L gives the following gradient for the update rule: We handle the non-differentiability of h ( s ) at s = 0 , by adopting a smooth hinge function as in [8]. In the previous section we have described an algorithm that j ointly solves for linear dimensionality reduction and metric learning. We now describe how to  X  X erne lize X  this method in order to compute non-linear features of the inputs that optimize our distanc e learning objective. Our approach learns a low-rank Mahalanobis distance metric in a high dimensiona l feature space F , related to the inputs by a nonlinear map  X  :  X  D  X  F . We restrict our analysis to nonlinear maps  X  for which there exist kernel functions k that can be used to compute the feature inner products withou t carrying out the map, i.e. such that k ( x We modify our objective  X  ( L ) by substituting inputs x a transformation from the space F into a low-dimensional space  X  d . We seek the transformation L minimizing the modified objective function  X  ( L ) .
 The gradient in feature space can now be written as: where s Let  X  = [  X  matrix allowing us to write L as a linear combination of the feature points. This form of no nlinear map is analogous to that used in kernel-PCA and it allows us to parameterize the transformation L in terms of only d n parameters, the entries of the matrix  X  . We now introduce the following Lemma which we will later use to derive an iterative update rule for L .
 features  X  Proof Defining k puted as L  X  where E v other columns. Setting proves the Lemma.
 This result allows us to implicitly solve for the transforma tion without ever computing the features in the high-dimensional space F : the key idea is to iteratively update  X  rather than L . For example, using gradient descent as optimization we derive update rul e: where  X  is the learning rate. We carry out this optimization by itera ting the update  X   X  ( X   X   X   X ) until convergence. For classification, we project points on to the learned low-dimensional space by exploiting the kernel trick: L  X  We compared our methods to the metric learning algorithm of W einberger et al. [9], which we will refer to as LMNN (Large Margin Nearest Neighbor). We use KLMCA (kernel-LMCA) to denote the nonlinear version of our algorithm. In all of the e xperiments reported here, LMCA was initialized using PCA, while KLMCA used the transformation computed by kernel-PCA as initial guess. The objectives of LMCA and KLMCA were optimized using the steepest descent algorithm. We experimented with more sophisticated minimization tech niques, including the conjugate gradient method and the Broyden-Fletcher-Goldfarb-Shanno quasi-N ewton algorithm [6], but no substantial improvement in performance or speed of convergence was achi eved. The KLMCA algorithm was implemented using a Gaussian RBF kernel. The number of neare st neighbors, the weight c in Equation 1, and the variance of the RBF kernel, were all autom atically tuned using cross-validation. The first part of our experimental evaluation focuses on clas sification results on datasets with high-dimensionality, Isolet, AT&amp;T Faces, and StarPlus fMRI: Except for Isolet, for which a separate testing set is specifi ed, we computed all of the experimental results by averaging over 100 runs of random splitting of the examples into training and testing sets. For the fMRI experiment we used at each iteration 70% of the da ta for training and 30% for testing. For AT&amp;T Faces, training sets were selected by sampling 7 ima ges at random for each person. The remaining 3 images of each individual were used for testing.
 Unlike LMCA and KLMCA, which directly solve for low-dimensi onal embeddings of the input data, LMNN cannot be run on datasets of dimensionalities suc h as those considered here and must be trained on lower-dimensional representations of the inp uts. As in [9], we applied the LMNN algorithm on linear projections of the data computed using P CA. Figure 1 summarizes the training and testing performances of kNN classification using the met rics learned by the three algorithms for different subspace dimensions. LMCA and KLMCA give conside rably better classification accu-racy than LMNN on all datasets, with the kernelized version o f our algorithm always outperforming the linear version. The difference in accuracy between our a lgorithms and LMNN is particularly dramatic when a small number of projection dimensions is use d. In such cases, LMNN is unable to find good metrics in the low-dimensional subspace compute d by PCA. By contrast, LMCA and KLMCA solve for the low-dimensional subspace that optimize s the classification-related objective Figure 2: Image reconstruction from PCA and LMCA features. ( a) Input images. (b) Reconstruc-tions using PCA (left) and LMCA (right). (c) Absolute differ ence between original images and reconstructions from features for PCA (left) and LMCA (righ t). Red denotes large differences, blue indicates similar grayvalues. LMCA learns invariance to ef fects that are irrelevant for classification: non-uniform illumination, facial expressions, and glasse s (training data contains images with and without glasses for same individuals). of Equation 1, and therefore achieve good performance even w hen projecting to very low dimen-sions. In our experiments we found that all three classificat ion algorithms (LMNN, LMCA+kNN, and KLMCA+kNN) performed considerably better than kNN usin g the Euclidean metric in the PCA and KPCA subspaces. For example, using d = 10 in the AT&amp;T dataset, kNN gives a 10.9% testing error rate when used on the PCA features, and a 9.7% testing er ror rate when applied to the nonlinear features computed by KPCA.
 While LMNN is applied to features in a low-dimensional space, LMCA and KLMCA learn a low-rank metric directly from the high-dimensional inputs. Con sequently the computational complexity of our algorithms is higher than that of LMNN. However, we hav e found that LMCA and KLMCA converge to a minimum quite rapidly, typically within 20 ite rations, and thus the complexity of these algorithms has not been a limiting factor even when applied t o very high-dimensional datasets. As a reference, using d = 10 and K = 3 on the AT&amp;T dataset, LMNN learns a metric in about 5 seconds, while LMCA and KLMCA converge to a minimum in 21 and 24 seconds , respectively.
 It is instructive to look at the preimages of LMCA data embedd ings. Figure 2 shows comparative re-constructions of images obtained from PCA and LMCA features by inverting their linear mappings. The PCA and LMCA subspaces in this experiment were computed f rom cropped face images of size 50  X  50 pixels, taken from a set of consumer photographs. The datase t contains 2459 face images corresponding to 152 distinct individuals. A total of d = 125 components were used. The subjects shown in Figure 2 were not included in the training set. For a g iven target dimensionality, PCA has the property of computing the linear transformation minimi zing the reconstruction error under the L2 norm. Unsurprisingly, the PCA face reconstructions are e xtremely faithful reproductions of the original images. However, PCA accurately reconstructs als o visual effects, such as lighting varia-might potentially hamper recognition. By contrast, LMCA se eks a subspace where neighboring ex-amples belong to the same class and points differently label ed are separated by a large margin. As a result, LMCA does not encode effects that are found to be insi gnificant for classification or that vary largely among examples of the same class. For the case of face verification, LMCA de-emphasizes changes in illumination, presence or absence of glasses and smiling expressions (Figure 2). When the input data does not require dimensionality reductio n, LMNN and LMCA solve the same convergence to the global minimum of the objective. However , even in such cases, KLMCA can be used in lieu of LMNN in order to extract nonlinear features fr om the inputs. We have evaluated this use of KLMCA on the following low-dimensional datasets from the UCI repository: Bal, Wine, Iris, and Ionosphere. All of these datasets, except Ionosphere, h ave been previously used in [9] to assess the performance of LMNN. The dimensionality of the data in th ese sets ranges from 4 to 34. In order (a) (b) Figure 3: kNN classification accuracy on low-dimensional da tasets: Bal, Wine, Iris, and Ionosphere. (a) Training error. (b) Testing error. Algorithms are kNN us ing Euclidean distance, LMNN [9], kNN in the nonlinear feature space computed by our KLMCA algorit hm, and multiclass SVM. to compare LMNN with KLMCA under identical conditions, KLMC A was restricted to compute a number of features equal to the input dimensionality, altho ugh in our experience using additional nonlinear features often results in better classification p erformance. Figure 3 summarizes the results of this comparison. Again, we averaged the errors over 100 ru ns with different 70/30 splits of the data for training and testing. On all datasets except on Wine , for which the mapping to the high-dimensional space seems to hurt performance (note also the h igh error rate of SVM), KLMCA gives better classification accuracy than LMNN. Note also that the error rates of KLMCA are consistently lower than those reported in [9] for SVM under identical trai ning and testing conditions. Our method is most similar to the work of Weinberger et al. [9] . Our approach is different in focus as it specifically addresses the problem of kNN classificatio n of very high-dimensional data. The novelty of our method lies in an optimization that solves for data reduction and metric learning simultaneously. Additionally, while [9] is limited to lear ning a global linear transformation of the inputs, we describe a kernelized version of our method that e xtracts non-linear features of the inputs. We demonstrate that this representation leads to significan t improvements in kNN classification both on high-dimensional as well as on low-dimensional data. Our approach bears similarities with Lin-ear Discriminant Analysis (LDA) [2], as both techniques sol ve for a low-rank Mahalanobis distance metric. However, LDA relies on the assumption that the class distributions are Gaussian and have identical covariance. These conditions are almost always v iolated in practice. Like our method, the Neighborhood Component Analysis (NCA) algorithm by Gol dberger et al. [4] learns a low-dimensional embedding of the data for kNN classification usi ng a direct gradient-based approach. NCA and our method differ in the definition of the objective fu nction. Moreover, unlike our method, NCA provides purely linear embeddings of the data. A contras tive loss function analogous to the one used in this paper is adopted in [1] for training a similar ity metric. A siamese architecture con-sisting of identical convolutional networks is used to para meterize and train the metric. In our work the metric is parameterized by arbitrary nonlinear maps for which kernel functions exist. Recent work by Globerson and Roweis [3] also proposes a technique fo r learning low-rank Mahalanobis metrics. Their method includes an extension for computing l ow-dimensional non-linear features us-ing the kernel trick. However, this approach computes dimen sionality reductions through a two-step solution which involves first solving for a possibly full-ra nk metric and then estimating the low-rank approximation via spectral decomposition. Besides being s uboptimal, this approach is impractical for classification problems with high-dimensional data, as it requires solving for a number of un-knowns that is quadratic in the number of input dimensions. F urthermore, the metric is trained with and not strictly necessary for good kNN classification perfo rmance. The Support Vector Decompo-sition Machine (SVDM) [7] is also similar in spirit to our app roach. SVDM optimizes an objective that is a combination of dimensionality reduction and class ification. Specifically, a linear mapping from input to feature space and a linear classifier applied to feature space, are trained simultane-ously. As in our work, results in their paper demonstrate tha t this joint optimization yields better accuracy than that achieved by learning a low-dimensional r epresentation and a classifier separately. Unlike our method, which can be applied without any modificat ion to classification problems with more than two classes, SVDM is formulated for binary classifi cation only. We have presented a novel algorithm that simultaneously opt imizes the objectives of dimensionality reduction and metric learning. Our algorithm seeks, among a ll possible low-dimensional projec-tions, the one that best satisfies a large margin metric objec tive. Our approach contrasts techniques that are unable to learn metrics in high-dimensions and that must rely on dimensionality reduction methods to be first applied to the data. Although our optimiza tion is not convex, we have exper-imentally demonstrated that the metrics learned by our solu tion are consistently superior to those computed by globally-optimal methods forced to search in a l ow-dimensional subspace.
 The nonlinear version of our technique requires us to comput e the kernel distance of a query point to all training examples. Future research will focus on render ing this algorithm  X  X parse X . In addition, we will investigate methods to further reduce overfitting wh en learning dimensionality reduction from very high dimensions.
 Acknowledgments We are grateful to Drago Anguelov and Burak Gokturk for discu ssion. We thank Aaron Hertzmann and the anonymous reviewers for their comments.

