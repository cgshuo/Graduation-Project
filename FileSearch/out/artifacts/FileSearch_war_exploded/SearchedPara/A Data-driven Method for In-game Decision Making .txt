 Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven. In this paper we show how machine learning can be applied to generate a model that could lead to better on-field decisions by managers of pro-fessional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predic-tive models that can be used to help decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suit-able for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters.

For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model could have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time. For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.
 I.2 [ Artificial Intelligence ]: Applications and Expert Sys-tems; G.3 [ Probability and Statistics ]: Multivariate statis-tics, Time series analysis Machine Learning Applications MLB, Predictive Modeling
Perhaps the most important in-game decision a baseball manager makes is when to relieve a starting pitcher [2]. To-day, managers rely on various heuristics (e.g., pitch count) to decide when a starting pitcher should be relieved [2, 17]. In this paper, we derive from data a model that can be used to assist in making these decisions in a principled way.
We pose the problem as classification problem of predict-ing whether a starting pitcher would give up at least one run if allowed to start the next inning. (This formulation seems appropriate late in close games, but is probably not the right formulation early in a game or during a blowout.) Our method uses information about the current at-bat, game situation, and historical data.

First, we reformulate our problem into a regression prob-lem using Pitcher X  X  Total Bases (PTB) [9] instead of runs allowed as the dependent variable because PTB is generally accepted as a better indicator of pitcher effectiveness. We solve the regression problem using regularized least squares to produce a pitcher-specific predictor for the expected PTB in the next inning. The main technical interest of this work lies in our approach to constructing the variables used in the regularized regression.
We evaluate our method on the MLB 2006-2010 data set from STATS Inc. 1 , which contains a record of each pitch thrown in MLB games in both the regular and post seasons. We train our model using the first 80% of the games of each season and test it on the last 20%.

We evaluate our model relative to what actually occurred in the games in the test set. First, we learned a model that closely reflects the actual decisions made by MLB managers. http://www.stats.com/baseball.asp It is well known that managers rely heavily on pitch count and the opposing team X  X  scoring to determine when to re-lieve the starting pitcher [2, 17, 12]. This manager model learns pitcher-specific parameters that fit these two variables against the manager X  X  decisions on the training data.
For the 76 pitchers who pitched at least 500 pitches in each year, our manager model accurately models the manager X  X  decision in 95%(  X  1 . 2%) of the innings, i.e., it is a reasonably accurate model of what major league managers actually do. When the manager model is used to predict whether or not a run will be scored, it correctly predicts 75%(  X  4 . 6%) of the innings. In contrast, our model makes a correct prediction for 81%(  X  4 . 9%) of the innings. Our model also outperforms the manager model in F-score (0 . 41 vs 0 . 26) and odds ratio (3 . 2vs1 . 2).

Second, we demonstrate that applying our model would have led to a different decision 48% of the time after the fourth inning in close games. For those close game situa-tions in which a manager actually left a pitcher in that the model would have removed, 60% of the time the pitcher sur-rendered a run. Unfortunately, it is impossible to say what would have happened in those situations where a manager removed a pitcher that the model would have allowed to continue, since we don X  X  know whether or not the removed pitcher would have given up a run had he not been removed.
In Section 2 we present some background on baseball and related work. In Section 3 we describe our method. In Sec-tion 4 we discuss the measures of performance used to eval-uate our method, and present the results of a series of tests in which comparisons are made using several performance measures. Section 5 discusses the real world applications of our method. In Section 6 we discuss the limitations and outline future work. Section 7 provides a conclusion.
Baseball was one of the first sports to attract statistical analysis. As a sport, it has all the necessary characteris-tics for performing a quantitative analysis: a rich dataset with detailed records for several decades, ordered game pro-gression that allows activity to be nicely segmented, and a reasonable amount of independence between events. This has led to the development of many statistical methods for assessing individual baseball players over the years [14, 1, 11, 3].

Bill James used statistical data in the 1980s to analyze why teams win and lose. He termed his approach sabermet-rics. In an effort to quantify the contribution of players to wins and losses he invented many statistical measures such as runs created, Component ERA, and similarity scores[16].
Baseball Prospectus, published by Gary Huckabay from 1996, uses a system called Vladimir to predict a player X  X  performance in the next season using the context of the player X  X  statistics such as the venue in which the game is played. It projects how the performance evolves as a player ages [8, 15].

In 2003, Nate Silver published a state-of-the-art sabermet-ric system for forecasting MLB player performance, termed PECOTA. It is a nearest neighbor based similarity metric that searches through thousands of players to find the ones with similar profiles. It computes the odds of a draft player having a successful future[14]. It extended the method to compute the statistical similarity between any two major-league players published in Baseball Abstract by Bill James in 1986 [10]. Since then there have been many commercial systems developed to model the progression of players[1, 11, 3, 13, 15].

Other than [6], which described a method for predicting the next type of pitch, almost all of the existing baseball sta-tistical methods address high level problems that span multi-ple games: projecting a player X  X  performance over the years, evaluating a player X  X  contributions to the wins and losses, and optimizing a team budget. In contrast, we present a machine learning method to assist decision making during a game. In particular we present a method for producing predictive models that can be used to help decide whether a pitcher should be replaced at various points in the game.
Pitchers are divided into two categories, starters (the first pitcher for each team) and relievers (all subsequent pitch-ers). Starters rarely pitch the complete game, and deciding when to replace the starter is often the most important in-game decision that a manager has to make. Early in the game the decision is based upon many factors, including the effectiveness of the starter and the managers desire to con-serve the bullpen. Later in the game, the decision is based primarily on the manager X  X  estimate of the relative expected effectiveness of the starter and the pitcher who would replace him. Our work is designed to provide useful information in the second half of close games. We first build a pitcher-specific predictor for the expected PTB in the next inning. We then use this prediction to build a classifier that predcits whether a run will be given in the next inning.
The independent variable x of our model incorporates the following groups of information available to a manager at the time a decision has to be made: Table 1 lists our feature vector.
By construction, our feature vector contains only contin-uous variables. However, many variables such as batter, batting team, and home team are naturally categorical. We use prior statistics to transform these variables into con-tinuous variables. For example, the batting team is rep-resented by prior statistics of how many times the pitcher faced the team, the runs given, and number of hits. We compile pitcher-home team, pitcher-batting team, pitcher-inning, and pitcher-defense configuration priors. For each of the next three batters in the line-up, we compile the aver-ages of pitcher-batter priors and batter priors (independent of pitcher). These priors are represented by how many times they faced each other, Component ERA (ERC) for the pair, slugging percentage (SLG) for the pair, and number of hits and runs between them.

Our predictors incorporate at-bat information, game statis-tics, and the context of the game statistics. For instance, the pitcher-venue prior implicitly captures information about the dimensions of the stadium where the game is played. Such information can provide critical predictive information for some pitchers and batters. For example, in 2012, the BostonRedSoxpitcherFelixDoubront X  X ERAwasalmost a full run higher in his home stadium (which has a short left field wall) than elsewhere.

These baseball statistics attempt to quantify certain char-acteristics and skills of the players that are not captured by runs alone. SLG measures the power of a hitter, ERA dis-counts luck by excluding non-earned runs, and component ERA includes partial runs, i.e., hits and walks.
Sometimes, these priors may be unreliable because of small support. A particular pitcher might not have faced a partic-ular batter, or thrown only a few pitches to that batter. In such cases, the prior statistics may not be meaningful. We improve the utility of values with low supports by shrinking them towards the global average [4]. For example, global av-erage of pitcher-batter prior (e.g., SLG) is the pitcher prior against all the batters.

Suppose a random variable s  X  N (  X ,  X  2 )isobservedvia  X  s | s  X  N ( s,  X  2 s ). Under the Bayesian interpretation, posterior mean s for observation  X  s is given by For the pitcher-variable prior  X  s , global average p , support n , and some constant  X  , by making the assumption that  X  2 s proportional to 1 /n , we derive the shrunk pitcher-variable prior s
When choosing the predictors, we try to minimize the in-fluence of luck, and focus on variables that are less erratic and capture the root causes. For instance, we slugging per-centage to capture the skill of batsman better than RBI. Similarly, strikeouts, walks, and steals yield predictive in-formation about the skills of the pitching team. Also, when using balls and walks count, we discount intentional balls and intentional walks.

The actual number of runs fails to take into account luck and near misses. In baseball near term outcomes are often dominated by randomness. In order to take that into ac-count, we use Pitcher X  X  Total Bases (PTB) (Equation 3) in place of runs as the dependent variable. PTB is a factor in Component ERA, a baseball statistic invented by Bill James [3,9].
 Here, H is hit, HR is home run, BB is walk, HBP is hit by pitch, and IBB is intentional walk.
 3-5 Outs, Inning number and pitch count 6-7 Strikes, balls (intentional balls discounted) 8-10 Bases advanced, home runs, hits in play 12-13 Steals, strike outs 14-21 Pitch Velocity &amp; Pitch Zone (binned) 22-26 Pitcher-Inning (Count, SLG, ERC, Runs, Hits) 27-31 Pitcher-Batting team (Count, SLG, ERC, Runs, Hits) 32-36 Pitcher-Venue (Count, SLG, ERC, Runs, Hits) 37-41 Pitcher-Defense (Count, SLG, ERC, Runs, Hits) 42-56 Pitcher-Batter (1-3) (Count, SLG, ERC, Runs, Hits) 57-68 Batter (1-3) (SLG, ERC, Runs, Hits)
First, we formulate our problem as a regression prob-lem, and solve it using regularized least squares [7]. For pitcher j , our training data is a set of n j points of the form { feature vector (Table 1).
We next move to the binary problem of predicting whether there will be a run given in the next inning. We do this by binarizing the outputs of the regression r j i , and finding a pitcher specific cut-off  X  b j on the estimated PTB that maxi-mizes the prediction accuracy on the validation set. Prediction outputs are given by sign ( x j i .  X  w j T  X   X 
Our model is pitcher-specific, and we learn feature weights independently for each pitcher. However, we want to take advantage of the intuition that feature weights should have some relationship across pitchers (e.g., Fenway Park presents challenges for most left handed pitchers). Therefore, we rewrite Equation 4 with a multi-task learning formulation [5] to share information across pitchers j =1 , 2 , ..., J . min where w j = w 0 + v j .

Regularization parameters  X  1 , X  2 and shrinkage coefficients  X  are found by maximizing the area under the ROC curve for true class labels y j i (whether a run is given in the next inning) and the prediction scores r j j (estimated PTB) on the validation set.
We evaluate our method on the MLB data for years 2006-2010 from STATS Inc., which contains a record of each pitch thrown in both the regular and post seasons [6]. We train our model using the first 80% of the games of each season, and test it on the last 20%. We choose the regularization pa-rameter and the threshold cut-off to binarize the regression outputs using cross validation on the training set. Figure 1: Histogram of the pitch counts when the starting pitcher is relieved
We first evaluate our model (Ours) relative to a model designed to mimic the way managers made decisions (Man-ager). We learn a manager model based on actual decisions of MLB managers. Managers often use pitch count (e.g., pitch count &gt; 75) (Figure 1) and opposing team score to decide when to relieve the starting pitcher [2, 17, 12]. Our manager model learns pitcher-specific parameters that fit these two variables against the manager X  X  decisions on the training data. For the 76 pitchers who pitched at least 500 pitches in each year, this heuristic model accurately models the manager X  X  decision in 95%(  X  1 . 2%) of the innings (F-score 0 . 87  X  0 . 02), i.e., it is a reasonably accurate model of major league managers.

The manager model predicts whether or not a run will be scored in 75%(  X  4 . 6%) of the innings. In contrast, our model makes a correct prediction for 81%(  X  4 . 9%) of the innings. Figure 2 plots the histogram of accuracies for both methods. Notice that the methods are far more accurate for some pitchers than for others.
The weights of a linear regression can be used to identify the most useful predictors. Table 2 lists the top 5 predictors based on the mean of the weights across pitchers ( | w 0 | Table 2: Top predictor weights across pitchers ( | w 0 | )
Surprisingly, pitch count does not appear in Table 2. This does not mean that pitch count is irrelevant in predicting scoring. The inning (in the form of pitcher-inning prior based on SLG), a variable that correlates well with pitch count, turned out to be the most important predictor.
Table 3 lists predictors with the highest variance across pitchers based on the standard deviation of the weights across pitchers ( this table also appear in Table 2. This emphasizes the im-portance of the role played by having both w 0 and v j in Equation 6.
 Table 3: Top predictor weights with high variance across pitchers (
Although features such as previous inning statistics (num-ber of strike outs, ball and strike counts, etc.) do not appear in the top 5 list, they offer critical information in certain sit-uations, as we will see in Section 5.
Next, we test the predictabilities of starting pitchers in various situations. We use our method X  X  accuracy to quan-tify the predictability. Figure 3 plots the average accuracy against the inning number across all the pitchers for our method and the manager model. The sizes of the circles denote the number of games.

At the beginning of the game, both methods have similar predictability. The difference increases drastically in later innings. As the game progresses, especially after the 4th inning (i.e., in the second half of a game), by exploiting the information in previous innings to track the progression of the game, our method becomes more accurate.
Since runs are scored in only about 10% of the innings pitched, accuracy is not the best measure of performance. A model that always predicts no-run innings would be accurate 90% of the time, but a prediction that always leads to leaving the pitcher in would be of no use. Therefore, we use the following set of measures for evaluation. The relationships  X  0 . 1) 0.26 (  X  0 . 15) 3.2(  X  1 . 2) 1.2 (  X  1 . 1)
Figure 3: Accuracy of the models for each inning among the variables used in these evaluation measures are depicted in Figure 4.

Table 4 presents the results for three well known pitchers and a summary across the pitchers. Our method outper-forms the manager model in all categories. The difference in the average odds ratios is particularly striking. For our model, in those the innings where a pitcher is predicted to al-low a run, the likelihood of him giving a run is 220% greater than his likelihood on all the innings. This improvement is only 20% for the manager model. The relatively poor per-formance of the manager model does not seem to be related to managers removing starting pitchers earlier to  X  X ave their arms, X  since on average our model removes pitchers sooner than does the manager model.
In this section, we discuss the potential impact of using our method to decide when to remove a starting pitcher. We consider only the fifth inning on, since in the early parts of the game many factors other than expected effectiveness figure into the decision of whether to remove the starting pitcher. Out of 21 , 538 innings, our model disagreed with the manager X  X  actual decision (i.e., not what our manager model would have done) a surprisingly high 66% of the time.
Our model has a  X  X uicker hook X  than most MLB man-agers, i.e., it tends to remove starters earlier than is typical. Given the relatively poor performance of pitchers when they are left in despite what our model would suggest, there is reason to believe that applying our model will lead to fewer runs being surrendered.
In the July 19, 2010 Brewers at Pirates game, the Pitts-burgh Pirates changed their pitchers 4 times in the game (twice in the 7th inning). Table 5 lists the runs scored in each inning. Innings where a pitching change happened are highlighted. Jeff Karstens, the starting pitcher for the Pi-rates in that game, was allowed to pitch through the 6th inning and was relieved only after the damage was done. It is possible that if the pitcher change had happened earlier, the result of the game may have been different. We inves-tigate whether our model would have recommended such a change. Figure 5: Runs given by Jeff Karstens in each inning Figure 5 shows the runs and PTB given the starting pitcher. At first glance, it seems that we couldn X  X  have predicted the sixth inning, because neither runs nor PTB of the previous innings (1-5) are indicative of what happened next. Figure 6: Runs predicted by our model for Jeff Karstens in each inning
Figure 6 shows the estimated PTB X  X  produced by three different components of our method. Using only the pitcher-inning prior statistics (orange), i.e., pitcher X  X  likelihood of giving up a run in an inning, we estimate the expected PTB to be 5.7 for the 6th inning. This is well below Jeff Karstens X  pitcher specific cut-off  X  b j (PTB = 8.1). When other fac-tors (e.g., pitcher-batter, pitcher-venue priors) are consid-ered (light blue), estimated PTB rises to 7.6. However, this model wrongly predicts a run for the 3rd inning. This is because of a high prior for this pitcher for the 3rd inning. Our final model (dark blue), which also incorporates previ-ous innings X  results, avoids the mistake in the 3rd inning, and correctly estimates a high PTB for the 6th inning. Inning Bases Events
Table 6 lists the factors that played an important role in our prediction. Since our model is also using the results from previous innings, our model gradually decreases the estimated PTB for the pitcher until the 5th inning, because he appears to be effective (zero bases advanced in the first 4 innings). The four strike outs in the previous two innings, which is indicative of the pitcher X  X  skill, result in a drastic drop in the estimated PTB for the 5th inning. However, a home run in the 5th inning coupled with his high prior statistics for the 6th inning results in a reversion to a high estimated PTB for the 6th inning, which turns out to be correct in this case.
There are several technical limitations in evaluating the real world application of our method. First, it is impossible to say what would have happened in those situations where a manager removed a pitcher that the model would have kept (i.e, we don X  X  have gold standard), since we don X  X  know whether or not the removed pitcher would have given up a run had he not been removed. Hence, our evaluations are one sided.

We consider only whether a run will be given in the next inning to decide whether to let the starting pitcher start the next inning. In practice, many other considerations come into play, particularly in early innings. Also, the starting pitcher is often removed in the middle of the inning. Our current method doesn X  X  address this scenario. We intend to do pitch by pitch prediction in our future work.

We make no claim for the optimality of our choice of fea-tures. In fact, we expect that further study will lead to feature vectors that yield better performance.

We expect that our method can be applied to other sim-ilar sports. For instance, in cricket, we can use the same approach to predict the best bowler to bowl the next over.
Using information about the current at bat, game situa-tion, and historical data, we estimate the number of runs given in the next inning. Using our method, MLB team managers can decide when a starting pitcher should be re-lieved. The results suggest that using our model might have led to better decisions than those made by major league managers.

For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up sur-rendering a run 60% of the time in the next inning, despite the fact that runs are scored in only about 10% of innings.
We would like to thank STATS LLC. for providing us with the data. This work was supported by Quanta Computers Inc.
 [1] J. Albert. Pitching statistics, talent and luck, and the [2] G. Baseball. The pitching rotation and the [3] B. Baumer and S. Ben. Why on-base percentage is [4] R. M. Bell and Y. Koren. Scalable collaborative fil-[5] T. Evgeniou and M. Pontil. Regularized multi X  X ask [6] G. Ganeshapillai and J. Guttag. Predicting the next [7] A. E. Hoerl and R. W. Kennard. Ridge regression: Bi-[8] G. Huckabay. 6-4-3: Reasonable person stan-[9] Imaginesports. Glossary@ONLINE. http: [10] B. James. Whatever happened to the Hall of Fame . Free [11] J. Keri and B. Prospectus. Baseball Between the Num-[12] B. Prospectus. Baseball Prospectus 2004 . Wiley, 2004. [13] B. Prospectus. Baseball Prospectus 2011 . Wiley, 2011. [14] N. Silver. Introducing pecota. Baseball Prospectus , [15] N. Silver. The Signal and the Noise: Why So Many [16] S. Sullivan. State of the art: The actuarial game of [17] F. Zimniuch and L. Smith. Fireman: The Evolution of
