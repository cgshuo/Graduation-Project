 With billions of handsets in use worldwide, the quantity of mobility data is gigantic. When aggregated they can help understand complex processes, such as the spread viruses, and built better transportation systems, prevent traffic con-gestion. While the benefits provided by these datasets are indisputable, they unfortunately pose a considerable threat to location privacy.

In this paper, we present a new anonymization scheme to release the spatio-temporal density of Paris, in France, i.e., the number of individuals in 989 different areas of the city released every hour over a whole week. The density is computed from a call-data-record (CDR) dataset, pro-vided by the French Telecom operator Orange, containing the CDR of roughly 2 million users over one week. Our scheme is differential private, and hence, provides provable privacy guarantee to each individual in the dataset. Our main goal with this case study is to show that, even with large dimensional sensitive data, differential privacy can pro-vide practical utility with meaningful privacy guarantee, if the anonymization scheme is carefully designed. This work is part of the national project XData ( http://xdata.fr ) that aims at combining large (anonymized) datasets provided by different service providers (telecom, electricity, water man-agement, postal service, etc.).
Mobile phone datasets have become widely available in recent years and have opened the possibility to improve our understanding of large-scale social networks by investigat-ing how people exchange information, interact, and develop social interactions. While the benefits provided by these datasets are indisputable, they unfortunately pose a consid-erable threat to location privacy. Not only this can impact people lives negatively, this also affects research. Because privacy is so important to people, companies and researchers are reluctant to publish mobile phone datasets by fear of being held responsible for potential privacy breaches. It is therefore urgent to develop practical tools for private releases and analysis of mobility datasets.

This paper focuses on applications that only need location counts, i.e., the repartition of visitors on a map at a given period of time. This information can typically be published by dividing a map into cells, and then releasing the count (i.e., number of users) associated to each of the cells. How-ever, it is important to ensure that this publication does not leak any information about the mobility patterns of indi-vidual users, which might be trivial when, for example, the count values are small.

Why Differential Privacy? Privacy has different def-initions and different models have been proposed. In this paper, we use differential privacy [10] which has emerged as a compelling privacy model. The advantage of this model, compared to the many others proposed in the literature, is two-fold. First, it provides a formal and measurable privacy guarantee regardless what other background information or sophisticated inference technique the adversary uses even in the future. Second, it is closed with respect to sequential and parallel composition, i.e., the result of the sequential or parallel combination of two differential private algorithms is also differential private.

This has particular importance in practice, since it does not only simplify the design of anonymization solutions, but also allows to measure how much privacy remains when a given dataset, or a set of correlated datasets, is anonymized (and released) several times, possibly by different entities.
Differential-private schemes often requires adding noise to the published data (e.g., to the published location counts) so that it leaks almost no information about any partici-pating individual, but still reveals aggregated information about the population as a whole. The variance of the noise is calibrated to both the sensitivity of the counts (i.e., the maximal change of the counts due to the inclusion/exclusion of a single record in a dataset) and a desired privacy level  X  . For large-dimensional data, such as temporal density, the sensitivity is usually so large that the added noise is much larger than the actual density count values for stringent pri-vacy requirement (i.e.  X  &lt; 1). Consequently, the resultant anonymized data is often meaningless.

Contributions. In this paper, we show that, for a given privacy level (i.e., a given  X  ), the magnitude of noise can be substantially reduced by using several optimizations and by customizing the anonymization mechanisms to the pub-lic characteristics of datasets and applications. We observe that the temporal density, of each cell, can be characterized by a periodic time series. This is explained by the fact that aggregated mobility patterns are quite periodic. Moreover these time series follow very similar trends, as most people in nearby cells have similar calling patterns. As a conse-quence, time series can be compressed by sampling, cluster-ing and low-pass filtering. Sampling and clustering reduce data sensitivity, which results in lower added noise and bet-ter performance. We further attenuate the distortion result-ing from the compression and perturbation phases via novel optimization and post-processing algorithms. We show ex-perimentally that the achieved performance is quite high and that differential privacy can be practical in real-world applications. However, we believe that there are probably no differential private anonymization techniques that fit all applications, and that anonymization algorithms have to be customized to each application and dataset.

The XData project. This work was done in the context of the French XData project. XData is a national funded project under the  X  X ig Data program X , whose goal is to study the benefit of combining and cross-processing different types of datasets provided by various service providers (such as Orange, Electricit  X e de France, La Poste, etc.). However, according to the European Data Protection laws (Directive 95/46/EC), all datasets have to be anonymized, prior cross-processing, such that data subjects are no longer identifi-able. The law does not dictate any specific privacy model, but stipulates that  X  to determine whether a person is iden-tifiable, account should be taken of all the means likely rea-sonably to be used either by the controller or by any other person to identify the said person  X . We believe that the best existing model to achieve this goal is probably differential privacy. This paper shows how to anonymize the mobil-ity data, provided by Orange, under the differential privacy model. In particular, we show how to release density infor-mation. Geographical density is useful in many of applica-tions envisioned by the XData projet, such as identifying areas where to install new businesses or build new infras-tructures.
Several recent studies have demonstrated the privacy risks of releasing location data by re-identifying individuals from geospatial data sets [8, 28, 13]. As a result, a plethora of privacy-preserving techniques have been introduced, how-ever, most of them do not provide any formal privacy guar-antee (see [6] and the references therein).

Differential privacy (DP) was first rigorously presented in [10] with the Laplace mechanism (LPA) as a first generic tool to guarantee DP. There exist two relaxations of  X  -DP; (  X  ,  X  )-probabilistic DP [20] and (  X , X  )-indistinguishability [9]. The former guarantees  X  -DP with high probability (  X  1  X   X  ), while the latter relaxes the bound of  X  -DP.

Location privacy has also been addressed in the context of DP. [5] applies DP to location, and more generally, se-quential data release. However, this scheme does not release time information apart from the sequentiality of locations. Another recent work [2] formalizes location privacy as pro-tecting the users X  location within a radius r with a level of privacy that depends on r . This corresponds to a generalized version of DP. They target LBS applications and add noise directly to users X  GPS coordinates. Commuting patterns in U.S. were anonymized in the probabilistic DP model in [20]. This scheme has also been deployed in practice within the project called OnTheMap by the U.S. Census Bureau. Other authors [7, 19] apply different spatial decomposition tech-niques to partition the two dimensional domain into cells, and then obtain noisy counts for each cell. However, these techniques are not concerned with releasing multiple counts over time.

Several DP techniques have been proposed to release time series. In [12], the authors propose a framework to release real-time aggregate statistics under DP based on filtering and adaptive sampling. Some other proposals [11, 4] provide a weaker guarantee on continuous data streams; they pro-vide event-level privacy to protect an event (i.e., one user X  X  presence at a particular time point), rather than the pres-ence of that user. As all these works address the real-time (interactive) release of aggregates, they are usually less ac-curate than off-line (non-interactive) approaches, which can access the whole time series and build more accurate models for perturbation.

The most related work to ours is [24, 17, 23]. All of them address the off-line release of time series under DP. Ras-togi and Nath [24] proposed a Discrete Fourier Transform (DFT) based algorithm which guarantees DP by perturbing the discrete Fourier coefficients. This technique was further improved for DP histogram release in [1]. We reuse the im-proved private DFT algorithm from [1], and further improve its accuracy at the cost of some privacy. In [17], the time se-ries are pre-processed by pre-sampling and smoothing them via averaging. These techniques diminish the global sensi-tivity of the data, and thereby allows to lower the injected Laplace noise. We also use a similar sampling technique to [17] in order to compress time series. However, [17] is a more general solution and targets even non-periodic time series where averaging may be a better perturbation model than DFT. As aggregated location counts are typically pe-riodic, DFT is a more accurate perturbation model for our application. Finally, DP-WHERE [23] adds noise to the set of empirical probability distributions which is derived from the original CDR datasets, and samples from these distribu-tions to generate synthetic CDRs. Instead of releasing the whole CDR dataset, we only aim at releasing the temporal density of IRIS cells which is a more specific problem. This mapping between CDRs and IRIS cells influences utility and is not considered in [23].
Intuitively, differential privacy [10] (DP) requires that the outcome of any computation be insensitive to the change of a single record in the dataset. Consequently, for a record owner, it means that any privacy breach will not be due to participating in the database.
 Definition 1 (Differential Privacy) A privacy mecha-nism A gives  X  -differential privacy if for any database D and D 2 differing on at most one record, and for any possible output O  X  Range ( A ) , e  X   X   X  Pr [ A ( D 2 ) = O ]  X  Pr [ A ( D 1 ) = O ]  X  e  X   X  Pr [ A ( D where the probability is taken over the randomness of A .
A relaxation of DP is probabilistic-DP [20], where privacy breaches may occur with very small probability. Definition 2 (Probabilistic Differential Privacy [20]) A privacy mechanism A gives (  X , X  ) -probabilistic differential privacy if for any database D 1 and D 2 differing on at most one record, and for any possible output O  X  Range ( A ) , we can partition the output space  X  into  X  1 and  X  2 such that (1) for all O  X   X  1 , e  X   X   X  Pr [ A ( D 2 ) = O ]  X  Pr [ A ( D 1 ) = O ]  X  e  X   X  Pr [ A ( D and (2) for any database D , Pr [ A ( D )  X   X  2 ]  X   X  where the probability is taken over the randomness of A .
 The latter definition guarantees that algorithm A achieves DP with high probability (  X  1  X   X  ), and the set  X  2 contains all outputs that are privacy breaches according to Definition 1. The probability of these outputs are bounded by  X  . Notice that with  X  = 0 probabilistic DP boils down to Definition 1. Although probabilistic DP has weaker privacy guarantee than Definition 1, it provides higher utility in practice.
The definition of differential privacy enjoys the property of sequential composition, which specifies the privacy guar-antee in a sequence of computation.
 Theorem 1 ([21]) Let A i each provide  X  i -differential pri-vacy. A sequence of A i ( D ) over the dataset D provides P i  X  i -differential privacy. Three principal techniques for achieving (probabilistic) DP are Laplace mechanism [10] (LPA), Gaussian mecha-nism [15] (GPA), and Exponential mechanism [22]. A fun-damental concept of all these techniques is the global sensi-tivity of a function [10]: Definition 3 (Global Sensitivity) For any function f : D  X  R d , the sensitivity of f is  X  f = max D 1 , D 2 || f ( D f ( D 2 ) || 1 for all D 1 , D 2 differing in at most one record. The global sensitivity is also called as L 1 -sensitivity due to the L 1 -norm used in its definition and is denoted by  X  1 Similarly, the L 2 -sensitivity  X  2 f of a function f , which is used later in this paper, is defined by the L 2 -norm || X || Laplace mechanism (LPA). A standard mechanism to achieve differential privacy is to add Laplace noise to the true output of a function. The noise is generated according to a Laplace distribution with the probability density function (PDF) p ( x |  X  ) = 1 2  X  e  X  X  x | / X  , where  X  is calibrated as follows. Theorem 2 ([10]) For any function f : D  X  R d , the mechanism A gives  X  -differential privacy, if L i (  X  ) are i.i.d Laplace vari-ables with scale parameter  X  =  X  f/ X  .
 Gaussian mechanism (GPA). An alternative technique to achieve probabilistic DP is to add Gaussian noise to the true output of a function. The noise is generated accord-ing to the Gaussian distribution with the PDF p ( x |  X  ) = Theorem 3 ([15]) For any function f : D  X  R d , the mechanism A gives (  X , X  ) -probabilistic differential privacy for any  X   X  1 and  X  2  X  2( X  2 f ) 2 ln(4 / X  ) / X  2 , where G i (  X  ) are i.i.d Gaus-sian variables with variance  X  2 .

Assuming identical values of  X  , a Gaussian random vari-able is more concentrated around 0 than a Laplace random variable thereby ensuring better utility for GPA. However, this larger accuracy also entails weaker privacy, since there is a small probability  X  that  X  -DP will not hold.
 Exponential mechanism. The exponential mechanism [22] captures all DP mechanisms with a measurable output space. In particular, it assigns exponentially greater proba-bilities of being selected to outputs of higher utility so that the final output would be close to the optimum yet still dif-ferential private.
 Theorem 4 ([22]) Given a utility function u : ( D X R )  X  R for a database D , the mechanism A ,
A ( D ,u ) = return r with probability  X  exp  X u ( D ,r ) gives  X  -differential privacy, where  X  u = max  X  r, D | u ( D 1 ,r )  X  u ( D 2 ,r ) | .
The error between the private and original time series is measured by the following metrics. Consider counts X = { X 0 ,...,X n  X  1 } . We denote the original time series by X , the sanitized one by  X  X .
 Mean Relative Error (MRE): MRE( X ,  X  X ) = igates the effect of very small counts. Following the convention [27], we adjust  X  to 0 . 1% of P n  X  1 i =0 X i Pearson Correlation (PC): PC( X ,  X  X ) = linear correlation between the noisy and the original time series (i.e., whether they have similar trends), and it always falls between -1 and 1.
Our goal is to release the spatio-temporal density of 989 non-overlapping areas in Paris, called IRIS cells. Each cell is defined by INSEE 1 and covers about 2000 inhabitants. The set of all IRIS cells is denoted by L henceforth, and are depicted in Figure 1 based on their contours 2 .

We aim to release the number of all individuals who vis-ited a specific IRIS cell in each hour over a whole week. Since human mobility trajectories exhibit a high degree of tempo-ral and spatial regularity [14], one week long period should be sufficient for most practical applications. Therefore, we are interested in the time series X L =  X  X L 0 ,X L 1 ,...,X of any IRIS cell L  X  L , where X L t denotes the number of individuals at L in the ( t + 1)th hour of the week, such that any single individual can visit a tower only once in an hour.
National Institute of Statistics and Economics: http://www.insee.fr/fr/methodes/default.asp?page= zonages/iris.htm
Available on IGN X  X  website (National Geographic Insti-tute): http://professionnels.ign.fr/contoursiris We will omit t and L in the sequel, if they are unambiguous in the given context. X L denotes the set of time series of all IRIS cells in the sequel. To compute these time series, we use a CDR (Call Data Record) dataset provided by the French telecom company Orange 3 , where T represents the set of cell towers of the operator, and a cell tower T  X  T is visited by an individ-ual at time t , if the operator has a recorded event at time t at tower T related to the individual. An event can be an incoming/outgoing call or message to/from the individual. This dataset contains the events of N = 1 , 992 , 846 users at | T | = 1303 towers within the administrative region of Paris (i.e., the union of all IRIS cells) over a single week (10/09/2007 -17/09/2007). Within this interval, the aver-age number of events per user is 13.55 with a standard de-viation of 18.33 (assuming that an individual can visit any tower cell only once in an hour) and with a maximum at 732. The set of all events related to an individual constitute his/her record (trajectory) in the dataset. Similarly to IRIS cells, we can create another set of time series X T , where X denotes the number of visits of tower T in the ( t + 1)th hour of the week.
We map the counts in X T to X L as follows. First, we compute the Voronoi tesselation of the towers cells T which is shown in Figure 1. Then, we calculate the count of each IRIS cell in each hour from the counts of its overlap-ping tower cells; each tower cell contributes with a count which is proportional to the size of the overlapping area. More specifically, if an IRIS cell L overlaps with tower cells { T 1 ,T 2 ,...,T c } , then at time t .

The rationale behind this mapping is that users are usu-ally registered at the geographically closest tower at any time. We acknowledge that this mapping algorithm might sometimes be incorrect, since the real association of users and towers depends on several other factors such as signal strength or load-balancing. Nevertheless, without more de-tails of the cellular network beyond the towers X  GPS posi-tion, we are not aware of any better mapping technique. to a sanitized version  X  X L such that  X  X L satisfies Definition 1. That is, the distribution of  X  X L will be insensitive (up to  X  ) to all the visits of any single user during the whole week, meanwhile the error between  X  X L and X L is small.
 Our sanitization algorithm is sketched in Algorithm 1. First, the input dataset is pre-sampled such that only ` visits are retained per user (Line 1). This ensures that the global sensitivity of all the time series (i.e., X L ) is no more than ` . Then, the pre-sampled time series of each IRIS cell is com-puted from that of the tower cells using Voronoi-tesselation http://www.orange.com F igure 1: IRIS cells of Paris (left) and Voronoi-tesselation of tower cells (right) A lgorithm 1 Our sanitization scheme 2: Compute the IRIS time series X L f rom X T (Se ction 4.2) 3: Compute the minimum cover C  X  T  X  L and the corresponding a nd Formula (1) (Line 2). After the largest cells, which cover the whole city and also have large counts, from T  X  L are iden-tified (Line 3), their time series are perturbed to guarantee privacy (Line 4). In order to mitigate the distortion of the previous steps, we apply smoothing on the perturbed time series as a post-processing step (Line 5). Finally, the time series of all IRIS cells are computed from the post-processed time series in C (Line 6).
To perturb the time series of all IRIS cells, we first have to compute their sensitivity, i.e.,  X  1 ( X L ). To this end, we first need to calculate the sensitivity of the time series of all tower cells, i.e.,  X  1 ( X T ). Indeed, Formula (1) does not change the L 1 -sensitivity of tower counts, and hence,  X  1  X  ( X L ).  X  1 ( X T ) is given by the maximum total number of (tower) visits of a single user in any input dataset. This upper bound must universally hold for all possible input datasets, and is usually on the order of few hundreds; recall that the maximum number of visits per user is 732 in our dataset. This would require excessive noise to be added in the per-turbation phase. We instead follow a different approach and divide the whole sanitization process into two main steps. We first perturb a pre-sample of our dataset which better withstands perturbation, and then mitigate the distortion effect of sampling in a post-processing step described later in Section 5.4.

In particular, we truncate each record of any input dataset by considering at most one visit per hour for each user, and then select at most ` of such visits per user uniformly at random over the whole week. This implies that a user can contribute with at most ` to all the counts in total regard-less of the input dataset, and hence, the L 1 -sensitivity of the dataset always becomes ` . The pre-sampled dataset is denoted by X , and  X  1 ( X T ) =  X  1 ( X L ) = ` . To sanitize X L , there are two basic (naive) approaches. First, we can directly perturb the IRIS counts X L plying the Laplace mechanism:  X  X L t = X L t + L ( `/ X  ) for all L  X  L . Alternatively, we can first perturb the tower counts X
T to obtain  X  X T , then compute the noisy IRIS counts  X  X L from  X  X T by applying Formula (1). Although both tech-niques guarantee  X  -DP according to Theorem 2, in terms of utility, they are suboptimal.

Cells with small counts have larger relative error, whereas larger counts better resist noise. This is due to the fact that the injected noise is independent of the magnitude of the original counts but only depends on their sensitivity. Therefore, the best approach is to first select cells having the largest counts (which can be either tower or IRIS cells) such that they cover whole Paris, perturb the counts of these cells, and then recompute the noisy counts of the smaller IRIS cells, which were not selected in the cover, from the larger (noisy) tower counts.

However, selecting the optimal cover of Paris (i.e., the set of cells having the largest counts) must also be differ-ential private. Fortunately, a simple heuristic helps us to accurately approximate the optimal cover without using the true counts of the cells (that would require to introduce more noise): cells with large size tend to have large counts 4 is also confirmed by Figure 2a and 2b. Hence, we re-state the problem as follows.
 How can we select the minimum cardinality subset of cells C  X  T  X  L such that C is a complete cover (i.e., covers whole Paris)?
More formally, let G ( V,E ) denote a graph, where each vertex corresponds to a cell in T  X  L , and ( v,v 0 )  X  E iff cells v and v 0 overlap. In this setting, our problem translates to the classical minimum vertex cover problem [3]. Indeed, as T and L are also complete covers of Paris, each vertex in V has at least one edge (a tower cell is always overlapped by at least one IRIS cell, and vice-verse), and we want to com-pute the minimum cardinality subset of cells which cover all the overlapping areas between cells. Although the minimum vertex cover problem is NP-hard in general, our covering problem belongs to the special cases which can be efficiently solved.
 Theorem 5 (Minimum cardinality cover) Selecting the minimum cardinality subset of cells C  X  T  X  L such that C is a complete cover can be solved in O ( | T || L | p | T
Proof. G ( V,E ) is bipartite, since the IRIS cells as well as the tower cells are partitionings of Paris, i.e., there are no overlapping cells in any of the two sets. Hence, for all ( v,v 0 )  X  E , either v  X  T and v 0  X  L , or v 0  X  T and v  X  For bipartite graphs, the minimum vertex cover problem is equivalent to the maximum matching problem based on K  X  onig X  X  theorem [3], which can be solved in polynomial time, e.g., with the Hopcroft-Karp algorithm [16] in O ( | E | p where | V | = | T | + | L | and | E | X | T || L | .
Tower cells are represented by their Voronoi polygons as it is depicted in Figure 1 Figure 2c shows the largest IRIS and tower cells covering Paris. We computed the mean count of each cell over the whole week which are illustrated by the cell colors. Appar-ently, the minimum cover contains cells with larger counts; the mean counts are 91 on average for the IRIS cells (Figure 2b) and 120 on average for the minimum cover (Figure 2c).
However, care must be taken before computing the cell counts in the minimum cover C . Since C can contain both IRIS and tower cells which may overlap, the L 1 -sensitivity of all the counts in C can be larger than ` . Indeed, if C contains a tower cell T and one of its overlapping IRIS cell L , then adding/removing a user who visited T at time t will change X t with 1, and also X not optimal) solution is to modify the counts of all towers in
C which have overlapping IRIS cells. For example, if T overlaps with IRIS cells { L 1 ,L 2 ,...,L c } , then X
After identifying the largest covering cells, their time se-ries (i.e., X C ) can be perturbed by adding L ( `/ X  ) to each count in all time series (see Theorem 2). Unfortunately, this naive method provides very poor results which is also illustrated in Figure 3a. Indeed, individual cells have much smaller counts than the magnitude of the injected noise; the standard deviation of the Laplacian noise is 141 with  X  = 0 . 3, which is even larger than the mean count in the minimum cover.

A better approach exploits (1) the similarity of geograph-ically close time series, as well as (2) their periodic nature. In particular, we first cluster nearby less populated cells un-til their aggregated counts become sufficiently large to resist noise. The key observation is that the time series of close cells follow very similar trends, but their counts usually have different magnitudes. Hence, if we simply aggregate (i.e., sum up) all time series within such a cluster, the aggregated series will have a trend close to its individual components yet large enough counts to tolerate perturbation. To this end, we first accurately approximate the time series of individual cells by normalizing their aggregated time series (i.e., divide the aggregated count of each hour with the total number of visits inside the cluster), and scale back with the (noisy) total number of visits of individual cells.

In order to guarantee DP, we also need to perturb the ag-gregated time series before normalization. To do so, we ex-ploit their periodic nature and apply a Fourier-based pertur-bation scheme [24, 1]: we add noise to the Fourier coefficients of the aggregated time series, and remove all high-frequency components that would be suppressed by the noise. As only low-frequency components are retained and perturbed, this method preserves the trends of the original data more faith-fully than LPA.

The whole perturbation process is summarized in Algo-rithm 2. First, the noisy total number of visits of each cell in the minimum cover C is computed by adding noise L (2 `/ X  ) to P 167 t =0 X i t for cell i (Line 1). These noisy total counts are used to cluster similar cells in Line 2 by invoking Algorithm 3. When the clusters are created, their aggregated time se-ries (i.e., the sum of all cells X  time series within the cluster) is perturbed with a Fourier-based perturbation scheme in Line 5 (Algorithm 4). Finally, the perturbed time series of each cell i in cover C is computed in Line 7 by scaling back | = 989 Algorithm 2 Perturbation 2: E := Cluster( C , X ,  X  S ) //see Algorithm 3 3: for each cluster E  X  E do 5:  X  X E := EFPAG( X E , X / 2 , X  ) //see Algorithm 4 6: for each cell i  X  E do 8: end for 9: end for the normalized aggregated time series with the noisy total count cell i (i.e., with  X  S i ). Since Line 1 guarantees  X / 2-DP to the total counts ( X  1 ( X C ) = ` ), it follows from Theorem 1 that Algorithm 2 is (  X  ,  X  )-DP if EFPAG is (  X / 2 , X  )-DP.
Algorithm 3 is a simple iterative process that, in each iter-ation, merges the least visited cluster with its geographically closest neighboring cluster until all clusters in the resultant configuration have a total count larger than a predefined threshold  X  . Initially, each cluster is a singleton composed of an individual cell in the cover. Then, in Line 4, we select the cluster which has the smallest (noisy) total count, and merge with its closest neighboring cluster in Line 5-8. The distance between two clusters are measured with the physi-cal distance between their cluster centers 5 . In each step, the noisy total count of each cluster is computed as the sum of all (noisy) total counts of each cell within the cluster (Line 7). Since the total counts of cells are noisy, Algorithm 3 preserves DP. To perturb aggregated time series, we build on the Fourier Perturbation Algorithm (FPA) [24]:
The cluster center is the centroid of its consitituent cell polygons.
 Algorithm 3 Cluster cells 9: end while 1. Compute the Fourier coefficients F = 2. Remove the last n  X  k coefficients from F , which cor-3. Generate the noisy version of F k , denoted by  X  F k , by 4. Pad  X  F k to be a n -dimensional vector by appending
FPA provably guarantees  X  -DP [24]. Enhanced FPA [1] improves basic FPA by selecting the coefficients to be removed more effectively. Specifically, in Step 2, EFPA chooses k probabilistically using the exponential mechanism such that the values of k which minimize the root-sum-squared error E || X  X   X  X || 2  X  q P n i = k +1 | F i  X  1 (RSSE) have exponentially larger probability to be selected. In this paper, we improve the accuracy of EFPA in two ways. First, instead of the Discrete Fourier Transform (DFT), we Algorithm 4 EFPAG Figure 3: Noisy time series of an IRIS cell (  X  = 0 . 3, ` = 30) apply the orthonormal version of the Discrete Cosine Trans-form (DCT), which tend to provide smaller high frequency components due to its different boundary conditions [26]. This can result in smaller RSSE when these components are removed in Step 2. Moreover, since we use orthonormal DCT, the resultant scheme also preserves  X  -DP [1].

Second, instead of adding Laplace noise, we add prop-erly calibrated Gaussian noise to the first k Fourier coeffi-cients of X thereby providing larger accuracy at the cost of weaker privacy. More specifically, when  X  F k is generated in Step 3, we employ the Gaussian mechanism instead of LPA and add i.i.d Gaussian noise G (  X  ) to each coefficient in F k , where  X  = probabilistic DP based on Theorem 3. In addition, we se-lect k with probability  X  exp  X   X   X  u G ( X ,k ) 4 in Step 2, where u follows from Theorem 5 in [1] and Theorem 3. When we use GPA in Step 3, the new scheme (with DCT) is denoted by EFPAG in the rest.
 Since Gaussian noise has smaller variance than Laplacian, EFPAG provides better accuracy than EFPA. Specifically, the variance of the Gaussian noise added to the Fourier coefficients is 8 X  2 ( X ) 2 ln(4 / X  ) / X  2 , which is independent of the number of retained coefficients (i.e., k ). By contrast, the variance of the Laplace noise added to the coefficients in EFPA is 8 X  2 ( X ) 2 k/ X  2 which linearly increases with the number of retained coefficients. However, this improvement also leads to some privacy degradation which is measured by  X  .

EFPAG is summarized in Algorithm 4, where the total budget  X  is uniformly divided between GPA (Line 4) and exponential mechanism (Line 2), therefore, EFPAG is (  X , X  )-probabilistically DP due to Theorem 1.
 Figure 4: Our scheme before improvements (  X  = 0 . 3, ` = 30).
 Figure 5: Our scheme after improvements (  X  = 0 . 3 ,` = 30)
Finally, in order to employ EFPA(G), we need to compute the L 2 -sensitivity of the counts in the cover C , i.e.,  X  Indeed, since E is a partitioning of C ,  X  2 ( X E ) =  X  2 Recall that, as a result of pre-sampling, at most a single visit of a user is retained in any slot, and at most ` visits per user over the whole week. This means that, for any t , there is only a single tower whose count can change (by at most 1) by modifying a single user X  X  data. From Formula (1), it follows that the total change of all IRIS cell counts is at most 1 at any t , and hence  X  2 ( X L )  X   X  2 ( X T ) = on the definition of L 2 -norm. Since C  X  T  X  L ,  X  2 ( X  X  ` . Figure 3 illustrates the improvement of our approach (Clustering + EFPAG) over simple LPA.
Although our approach is clearly superior to LPA, Figure 4 still suggests a large error on average. This difference be-tween  X  X and X is the result of two errors: the sampling error (between X and X ) is attributed to pre-sampling, whereas the perturbation error (between  X  X and X ) is due to our perturbation scheme.

As illustrated by Figure 4a, sampling error mainly dis-torts large counts: although the noisy counts are close to the counts of the truncated (pre-sampled) time series be-tween 9:00 AM and 11:00 PM, it is still far from the original count values. This significantly increases MRE.

In addition, as Figure 4b also shows, noisy counts also deviate from pre-sampled as well as from original counts around the local minimas (close to 4:00 AM every day), which further deteriorates MRE. This perturbation error is caused by the higher frequency components that are retained and perturbed by EFPA(G).
To alleviate these errors, we propose two further improve-ments: first, we improve the perturbation of total cell counts (Line 1 in Algorithm 2), which is used in cell clustering (Al-gorithm 3) and scaling (Line 6 in Algorithm 2). Then, as a post-processing step, we smooth out small counts (i.e., be-tween 0:00 and 6:00 AM) through non-linear least-square fitting to diminish perturbation error.
Recall that we scale back the normalized aggregated time series with  X  S i in Line 6 (Algorithm 2), where P of cell i ,  X  X i (Line 6) will be a scaled down version of the original time series X i due to the fact that the ` visits per individual are sampled uniformly at random. Also, as we have discussed in Section 5.1, adding Laplace noise directly the sensitivity of P 167 t =0 X i t is  X  1 ( X ) which is too large.
We rather perturb the original total count P 167 t =0 X i a different approach: we first approximate the relative fre-quencies of each tower by another constraint sampling, and scale back these frequencies to count values with the (noisy) total number of visits in the dataset. The main idea is that sampling requires only a small amount of noise to guarantee privacy, while the total number of all visits is so large that it tolerates a large noise magnitude.
 In particular, we estimate the histogram H where a bin H j represents the frequency of visits at tower j , i.e., H P visits in the dataset ( K = P 167 t =0 P T  X  T X T t ). To do so, we sample a single visit per user uniformly at random, and create a new histogram  X  H from the sampled visits (with size N ). Using this approximative histogram  X  H , the to-tal number of visits P 167 t =0 X j t of a tower j is computed as (  X 
H j + L (2 / X  0 ))  X   X  K , where  X  K = K + L (2 X  1 ( X ) / X   X  ( X ) is universally fixed for all input dataset 6 having the noisy P 167 t =0 X j t for each tower cell j , we can also compute the noisy P 167 t =0 X L t for any IRIS cell L (using For-mula (1)) and calculate  X  S i for all cell i in C . This technique is 2  X  (  X  0 / 2) =  X  0 differential private based on Theorem 1.
Figure 6a compares the accuracy of our sampling approach to perturb the relative frequencies of each tower (i.e., sam-pling is followed by adding L (2 / X  0 ) to each H j naive Laplace approach (i.e., L (2 X  1 ( X ) / X  0 ) is added to each H j without sampling). Sampling clearly boosts the accuracy of histogram perturbation (Figure 6a) especially for smaller values of  X  0 , and eventually yields significantly more accurate estimation of P 167 t =0 X T t for all towers T (Figure 6b).
The effect of scaling is illustrated in Figure 5a. Recall that the full privacy budget  X  is divided equally between EFPA(G) and scaling (see Algorithm 1). Hence, our sani-tization scheme is  X  -DP (or (  X , X  )-prob. DP with EFPAG) based on Theorem 1.
In order to smooth out low (noisy) counts around the local minimas (around 4:00 AM each day), we fit an exponential curve to the noisy counts between 0:00 AM and 4:00 AM  X  1 ( X ) is fixed to 732 in this paper. Notice that although  X  ( X ) is large so is K : K = 137 , 255 , 052 in our dataset, and | K  X   X  K | /K is less than 10  X  5 on average (a) Perturbing H with sampling Figure 6: Perturbing the total visits P 167 t =0 X T t of each tower cell T . where the counts are exponentially decreasing, and another exponential curve between 4:00 AM and 6:00 AM, where the counts are exponentially increasing. In particular, we fit function g ( x,a,b ) = a  X  exp( b  X  x ) to the noisy counts, i.e., com-pute parameters a,b such that the error P i (  X  X i  X  g ( x is minimized where x i runs over the hours of the given time intervals for each day, and then replace the noisy counts with the values of the fitted function. This is a standard non-linear least square fitting problem which can be ap-proximated with any numerical minimization method (e.g., Levenberg-Marquardt algorithm [18]). Since this operation is performed on the noisy time series, it is already private. The effect of smoothing in illustrated in Figure 5b.
We evaluate the utility of our scheme depending on the guaranteed privacy (i.e.,  X  ) with EFPA and EFPAG, where  X  = 2  X  10  X  6 &lt; 1 /N . The minimum total count  X  used in Algorithm 3 is adjusted such that the expected RSSE is less than 1% of the total count when all coefficients are retained in EFPA(G). That is,  X  = variance of noise added to each coefficient. We compare our approaches to the naive Laplace mechanism (LPA) that adds L ( `/ X  ) noise to each count of each time series in cover C . We use the CDR dataset described in Section 4.

First, we analyze the utility depending on the pre-sampling size ` . Then, we show how pre-sampling com-bined with the improved scaling and smoothing boost ac-curacy, and also report the error distribution among indi-vidual IRIS cells. Finally, we measure the Earth Mover X  X  Distance (EMD) which captures the error between spatial distributions in terms of geographical distances.
Recall that the number of visits retained per user (i.e., ` ) determines the injected noise in the perturbation phase. In general, larger values of ` imply larger noise, which degrades utility. On the other hand, smaller values of ` preserve more information about individuals which results in a more accu-rate representation of the original dataset. The goal is to select a value of ` which yields the best trade-off. Neverthe-less, we experimentally show next that our scheme exhibits stable performance with quite different values of ` . In Figure 7, we report the utility of our scheme with EF-PAG for three values of ` : 10, 30 and 168. In each case, perturbation is followed by the improvements described in
Figure 7: Utility depending on the pre-sampling size ` . Figure 8: Utility of our scheme with different perturbation techniques.
 Section 5.4. Specifically, we computed the average of MRE and Pearson correlation over all cells. We repeated the whole process 20 times and plotted the mean and standard devia-tion of the average MRE and PC over all executions.

LPA has significantly larger error for all values of ` , and provides especially poor results for smaller values of  X  . By contrast, our scheme does not only provide practical utility even for stringent privacy guarantee, but also has stable per-formance for different ` . For instance, for  X  = 0 . 3, MRE is less than 20%, while PC is larger than 0.95. Therefore, the output of our scheme has almost perfect linear correlation with the original data thanks to the combination of cluster-ing and the Fourier perturbation approach. Moreover, the variations of these values are very moderate: 0 . 2  X  0 . 03 and 0 . 95  X  0 . 03, respectively, for different values of ` . In the rest of the paper, we fix ` to 30.

EFPAG and EFPA 7 are compared in Figure 8. EFPAG outperforms EFPA especially for smaller  X  : MRE is reduced by 0.03 and PC is increased by 0.02 on average.
The distortion effect of pre-sampling is mitigated by the improved scaling step detailed in Section 5.4.1. Our aim now is to show that scaling and smoothing indeed results in better utility. Figure 8 depicts a variation of our scheme, denoted by  X  X FPA  X   X  when the improvements described in Section 5.4 are not employed after perturbation. The results show that MRE is reduced by 0.07 on average when the pre-sample size is diminished to ` = 30 and improvements are employed. By contrast, PC is increased only by about 0.01 for smaller values of  X  ; the change is not so significant due to the fact that scaling does not influence linear correlation,
Recall that EFPAG adds Gaussian noise whereas EFPA adds Laplacian noise to the retained Fourier coefficients. Figure 9: Error depending on time with EFPAG (  X  = 0 . 3 ,` = 30) Figure 10: Error and Pearson correlation on IRIS cells with EFPAG, ` = 30. The box extends from the lower to up-per quartile values of the cell errors, with a red line at the median. and smoothing modifies relatively small number of counts in general.

In Figure 9, we also plotted the average relative error de-pending on the time for our scheme with and without im-provements. In particular, we computed the relative error and took the average over all cells in each hour. Figure 9a confirms that scaling significantly diminishes the relative er-ror in daylight when counts are larger. The improvement can be almost a factor of 4. This has particular importance in practice, as location counts in daylight are usually more important than at night.
Figure 10 shows through box plots how MRE and Pearson correlation change among IRIS cells; we compute these met-rics for each IRIS cell, and compute the corresponding box plot over the metric values of all cells. Although medians do not change significantly for different values of  X  , MRE has larger variation for smaller  X  , i.e., there are more cells which have larger error.

The MRE and PC of individual IRIS cells are also illus-trated by color maps in Figure 11. This figure shows that our scheme can provide practical utility for most cells with strong privacy guarantee. Specifically, the average MRE over all cells is only 0.17 with  X  = 0 . 3.
In order to compare the sanitized spatial probability dis-tribution with the original one at a given time, we use Earth Mover X  X  Distance (EMD) [25]. EMD measures the  X  X mount of energy X  (or cost) needed to transform one distribution to another, and is a metric for probability distributions (i.e., location counts have to be normalized). Formally, for any Figure 11: MRE and PC of each IRIS cell ( ` = 30,  X  = 0 . 3) P denotes the set of all possible flows (each f ij represents the amount of probability mass transported from IRIS cell i to j ), and d ij is the geographical distance between the centers of cells i and j , resp. Intuitively, EMD measures the meters of error between two spatial density maps. Figure 9b reports the EMD depending on the time. The mean EMD over the whole week is 258 meters for EFPA, 188 meters for EFPAG, and 341 meters for LPA.
The goal of this work is to demonstrate through a real-world application that differential privacy can be a practi-cal model for data anonymization, even if the input dataset has large dimension and/or is highly sensitive. We showed that, in order to achieve meaningful accuracy, the sanitiza-tion process has to be carefully customized to the applica-tion and public characteristics of the dataset. We strongly believe that there are no  X  X niversal X  sanitization solutions that fit all applications, i.e., provide good accuracy in all scenarios. In particular, achieving the best performance re-quires to find the most faithful and concise representation of the data, such that it withstands perturbation. In our ap-plication (i.e., spatio-temporal density), clustering and sam-pling with Fourier-based perturbation are seemingly the best choices due to the periodic nature and large sensitivity of lo-cation counts. We experimentally showed that our scheme can provide practical utility and strong privacy guarantee.
