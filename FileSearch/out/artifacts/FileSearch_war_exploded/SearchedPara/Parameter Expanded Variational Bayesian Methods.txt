 (Jordan et al., 1998; Beal, 2003; Bishop &amp; Tipping, 2000)). parameters to speed up the deterministic approximation of the target distribution. the component updates are given by It is easy to derive the updates in this case: variance 1 + D .
 of The expanded model reduces to the original one when  X  equals the null value  X  the reduction rule Correspondingly, we change the measures of q ( w ) and q ( z ) : them point directly to the convergence point. In the general PX-VB formulation, we over-parameterize the model p (  X  x ,D ) to get p the original model is recovered for some default values of the auxiliary parameters  X  =  X  algorithm consists of the typical VB updates relative to p approximation. Put another way, we push the change in p permits us to return the model into its original form and set  X  =  X  Specifically, we first expand p (  X  x ,D ) to obtain p often achieves solution similar to what VB achieves, with faster convergence. A simple strategy to implement PX-VB is to use a mapping S ables  X  x . After sequentially optimizing over the components { q ( x over  X  . Then, we reduce p M optimized over each q (  X  x need not be optimal) for the mapping S is that the reduction step in PX-VB changes the lower-bounding distributions { q ( x reduction step via M applied to speed up VB convergence.
 and sparse learner. 3.1 Bayesian Probit regression models.
 The data likelihood for Probit regression is where X = [ x rewrite the likelihood in an equivalent form Given a Gaussian prior over the parameter, p ( w ) = N ( w | 0 ,v imate the posterior distribution p ( w , z | X , t ) by q ( w , z ) = q ( w ) Q KL ( q ( w ) Q n q ( z n ) k p ( w , z , t | X )) , we obtain the following VB updates: where TN ( z expand the orginal model p (  X  w ,  X  z , t | X ) to p such that Setting c = c Then, we minimize KL q ( z ) q ( w ) k p where M is the dimension of w . In the degenerate case where v The transformation back to p Accordingly, we change q ( w ) to obtain a new posterior approximation q We do not actually need to compute q By changing variables w to posteriors remains the same. After obtaining new approximations q c = 1 for the next iteration.
 VB updates are geared towards providing an approximate posterior distribution. for probit regression. We set v VB and PXVB is illustrated in figure 1. difference between two consecutive estimates of the posterior mean of the parameter w . VB converges immediately while VB updates are slow to converge. Both PX-VB and VB trained estimation for probit model. 3.2 Automatic Relevance Determination proposed by Bishop and Tipping (2000) for sparse Bayesian regression and classification. The likelihood for ARD regression is where  X  exp(  X  X  x i  X  x j k / (2  X  2 ) , where  X  is the kernel width.
 In ARD, we assign a Gaussian prior on the model parameters w : p ( w |  X  ) = Q M where the inverse variance diag(  X  ) follows a factorized Gamma distribution: to address this issue.
 First, we expand the original model p ( Setting r = r at the same time. This gives where f =  X   X   X  P the regular update over q ( w ) achieves a smaller KL divergence, we reset r = 1 . Given r and q ( w ) , we use respondingly, we change q ( w ) = N ( w |  X  N ( time update q (  X  ) . Then we change q (  X  ) back to q the space limitation, we skip the details here.
 Specifically, we introduce the variable c : Figure 2: Convergence comparison between VB and PX-VB for ARD regression on synthetic data Setting c = c in figure 2(a) and (b).
 We compare the convergence speed of VB and PX-VB for the ARD model on both synthetic data genes. For this task, we use 3 order polynomial kernels.
 In this section, we analyze convergence of VB and PX-VB, and their convergence rates. Define an objective function as the unnormalized KL divergence: gence.
 Q ( q ) results in at least linear convergence to an element q ? in the solution set. Define the mapping q ( t +1) = M [ q model. 4.1 Convergence rate of VB and PX-VB The matrix rate of convergence DM ( q ) : Define the global rate of convergence for q : r = lim the faster the algorithm converges.
 Define the constraint set g Theorem 4.1 The matrix convergence rate for VB is: where P Proof : Define  X  as the current approximation q . Let G function Q ( q ) under the constraint g Let M Then by construction of VB, we have q ( t + s/S ) = M DM S ( q ? ) . At the stationary points, q ? = DM s ( q ? ) for all s . We differentiate both sides of equation (23) and evaluate them at q = q ? : It follows that DM ( q ? ) = Q S To calculate DG at  X  = q ? , such that Similarly, we differentiate the Lagrange equation DQ both sides at  X  = q ? . This yields Combining (25) and (26) yields In the s update we fix q removed B Denote C where \ S, \ S means without row S and column S .
 Inserting (28) into (27) yields where I P The above results help us understand the convergence speed of VB. For example, we have For q Clearly, if we view D 2 Q the faster the convergence. In the extreme case, if there is no correlation between q q rate same as the global rate. Therefore, the instant convergence of q convergence rate.
 gence by reducing the correlation among { q VB implictly defines a mapping between q to q DM x ( q ? ) = DG 1 ( q ? )  X  X  X  DG  X  ( q ? )  X  X  X  DG S ( q ? ) where E and F are two matrices. Thus, as long as the largest eigenvalue of M eigenvalue of this mapping. The smaller the largest eigenvalue of M fast convergence rate. the auxiliary method is substantial.
 Acknowledgments
