 Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Re-cent research has shown that the Latent Dirichlet Alloca-tion (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking ap-proaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user X  X  information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets.
 Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning H.3.1 [Information Storage and Re-trieval]: Content Analysis and Indexing General Terms: Algorithms, Experimentation Keywords: Topic Model, Topic and Keyword Re-ranking
Latent topic models are effective methods for extract-ing latent semantic information from text corpora [4, 7, 3]. Among these models, Latent Dirichlet Allocation (LDA) [3] appears to be the most effective one. LDA models a doc-ument as a mixture of latent topics and each topic can be further represented by a set of keywords. As a result, it allows a document to belong to multiple topics. However, LDA is a general model without considering different users X  information needs. For example, the LDA topics are ran-domly ordered, and users must manually navigate the topic list to find the ones that they are interested in. This task becomes more difficult, if there is a large number of top-ics. To better help users consume the LDA-derived topics in an interactive visual text analytic process [9], we focus on enhancing the LDA topic modeling results.
 In this paper, we present a set of re-ranking techniques to enhance the topic modeling results. The work closest to ours is an approach described in [2], which uses a TFIDF-like term score to re-rank keywords. Compared to this work, ours also re-ranks the LDA-derived topics in addition to re-ranking the topic keywords. As a result, our work can select the most salient topics that meet different user interests.
We denote a text corpus as D = { d 1 , d 2 , . . . , d M } where d m is a document and M is the number of documents in the corpus. Each document consists of a sequence of words W m = { w m, 1 , w m, 2 , . . . , w m,N m } , where N m is the num-ber of words in document d m . We also define a dictio-nary V = { v 1 , v 2 , . . . , v V } , where V is the size of the dic-tionary. Moreover, z is a latent variable representing the latent topic associated with each observed word. We denote Z ated with the word sequence W m . The generative procedure of LDA can be formally defined as: 1. For all the topics k  X  1 , . . . , K : 2. For all the documents d m where m  X  1 , . . . , M : We denote  X  k = (  X  k, 1 ,  X  k, 2 , . . . ,  X  k,V ) T  X  R p ( w = v i | z = k ). Thus, the parameters for the topic-word distribution can be represented as  X  = (  X  1 ,  X  2 , . . . ,  X   X  R K  X  V , where K is the topic number. Moreover, we denote  X  k | d m ). Then the parameters for document-topic distribu-tion is  X  = (  X  1 ,  X  2 , . . . ,  X  M ) T  X  R M  X  K .
Given a set of training documents, inferencing a topic model involves estimating the document-topic distribution  X  and the topic-word distribution  X  [3, 5]. In our experi-ments, we use Gibbs sampling [5].
In this section, we introduce our topic and keyword re-ranking methods to enhance the LDA output for more ef-fective human consumption.
We first describe how we re-rank LDA-derived topic key-words since the order of these keywords directly affects the semantics and thus the importance of a topic. The native or-der of topic keywords produced by LDA may not be ideal for users to understand the semantics of a topic. For example, when LDA is applied to a financial news corpus, common words such as Dow, Jones, Wall, Street etc., are normally ranked high in many topics because they are relevant to all of the topics. These words however are not useful in helping users identify interesting topics since all of them are about finance. To better help people identify salient information, we thus re-rank the LDA-derived topic keywords to refine the topic definitions.

Inspired by the term re-weighting techniques used in in-formation retrieval (IR) [13, 8], we have experimented with two LDA-versions of TFIDF-like scores: and where the native weight  X   X  k,i generated by LDA corresponds to the term frequency. The topic proportion sum and topic proportion product are used respectively in KR 1 and KR 2 to simulate the inverse document frequency to re-weight the native weights. In fact, KR 2 is the same as the re-weighting technique used in [2].
The LDA-derived topics are randomly ordered which may not be equally important to a user. It is thus useful to order the topics so that the most important ones can be shown first. In general, the definition of importance may vary from one user to another. For example, a user may prefer to see the most talked topics, i.e. topics that cover more documents. In this case, the rank of a topic would be higher, if it covers more document content in the corpus. In contrast, a user may be interested in a set of distinct topics that have the least amount of content overlap with one another. In this case, we will rank topics based on their content uniqueness. Next, we describe a few application-independent topic re-ranking methods that computes the topic ranks based on different ranking criteria.
The first topic re-ranking method assumes that topics that cover a significant portion of the corpus content are more important than those covering little content. However, we consider topics that appear in all the documents (e.g. a topic on disclaims derived from a legal collection) to be too generic to be interesting, although they have significant con-tent coverage. Thus we rank such topics lower. As a result, our first topic ranking metric is a combination of both con-tent coverage and topic variance. More precisely, we define: and where the weight N m is the document length.
 Then the rank of a topic is defined as: where  X  1 and  X  2 are the control parameters. Specifically, if  X  1 = 1 and  X  2 = 0, the ranking is determined purely by topic coverage [14]. In contrast, if  X  1 = 0 and  X  2 = 1, the rank is simply determined by topic variance, a criterion that is similar to principle component analysis [1].
While topic variance used in the first method reflects a topic X  X  representative power, the Laplacian score of a topic represents its power in discriminating documents from dif-ferent classes [6]. Our second method on topic re-ranking is motivated by the observation that two similar documents are probably related to the same topic while documents that are dissimilar probably belong to different topics. Since the Laplacian score of a topic reflects its power in discriminating documents from different classes while preserving the local structure of a document collection, we develop a Laplacian score-based topic ranking method so that it assigns high ranks to those topics with high discriminating power. It consists of five main steps: 1. Represent each document d m as a node in a graph. Its features are represented by  X   X  m . 2. Construct the T -nearest neighbor graph based on a similarity matrix S where S ij = exp  X   X  d 2 ij / 2  X  2  X  . Here, d can be either Euclidian distance or Hellinger distance [2]. 3. Compute graph Laplacian L = D  X  S where D is a diagonal matrix and D ii = P M j =1 S ij is the degree of the ith vertex. 4. For each topic t k = (  X   X  1 ,k ,  X   X  2 ,k , . . . ,  X   X  t 5. Compute the Laplacian score of the k -th topic:
Remark 1 : To find the T -nearest neighbors of a topic, we keep a T -size heap. For each topic, we compute its distances to all the other topics and then check whether to insert it to the heap. Thus, the main time complexity is in graph Laplacian construction which is O ( M 2 K + M 2 log T ).
We have also developed a topic re-ranking approach based on the pairwise mutual information of two topics. This met-ric computes the information that two topics share. It also measures how much knowing one of the topics reduces our uncertainty about the other. Using this metric, we can rank topic by measuring the amount of the pairwise mutual infor-mation between two topics. Specifically, we use the following procedure [12] to determine the rank of each topic. 1. For  X  i, j , first compute M I ( t i , t j ) based on the doc-topic distributions of t i and t j . Then construct a complete graph G where the weight of an edge e t i , t j is M I ( t 2. Build the maximal spanning tree MST of the complete graph G : ( V , E ), where V and E are vertex and edge sets. 3. Define the relevant topic set T rel = { t 1 , t 2 , ..., t the corresponding edges in MST. 4. While |T rel | &gt; 0, to the others in T rel , remove this topic t v ( T rel  X  X  5. Rank the topics according to the order in which they were removed. Rank the last removed topic the highest.
Remark 2 : We use the Prime X  X  algorithm to construct the MST. Thus, to compute the pairwise mutual information for topic re-ranking needs O ( K 2 M ). By using a heap to con-struct a priority queue, we can build an MST in O ( |E| log |V| ) = O ( K 2 log K ) time.
The last similarity-based re-ranking method is developed to maximize topic diversity and minimize redundancy. While all the above methods use topic-document relationships, here we employ topic-word distributions to compute topic simi-larity. We have slightly modified the algorithm proposed in [11] to derive a topic rank: 1. For  X  i, j , compute the similarity s ij for  X  i and  X  based on maximal information compression index [11]. 2. Sort the similarities for each topic. 3. Define the reduced topic set T red = {  X  1 ,  X  2 , ...,  X  4. While |T red | &gt; 0, remove  X  j in T red which satisfies j = arg max i max j s ij . 5. The rank of a topic is determined by the topic removal order. The last removed topic should be ranked the highest.
Remark 3 : In this algorithm, constructing the simi-larity scores needs O ( K 2 M ) and sorting the scores needs O ( K 2 log K ).
We have tested our topic and keyword re-ranking tech-niques using two different data sets in a series of experi-ments.
 The first data set is a personal email collection dated from February to December 2008 with 8326 email messages. Each email is associated with a set of meta data such as sender, receiver, time, subject, body and reply counts. Only the subject and the body of each email were used to train the topic model. We pre-processed each email to remove ir-relevant information such as email signature and also did stop word removal. After pre-processing, the email collec-tion contained 958,069 word tokens in total.

The second data set is an online document collection that contained text retrieved by a search engine. These docu-ments came from various news, blog and forum web sites. The search engine used X  X IG insurance X  X s the query and re-trieved 34,690 documents from January 2008 to April 2009. After pre-processing, the final AIG collection contained 11,491,246 word tokens in total.

To test our methods, we ran LDA five times and we show both the average and standard derivation for the five runs. We adopted an LDA algorithm that was trained with opti-mized hyper-parameters. For each run of LDA, we set the maximum iteration to 1000. The initial model parameters were set to the default values in the Mallet LDA toolkit [10]. We empirically set the topic number for the email and AIG data sets to 18 and 20 respectively.
For each data set, we asked an expert to annotate the topics and the corresponding topic keywords learned by our methods. The annotation was repeated for each of the five LDA runs on each data set.
For the email data set, the person who owned the email collection helped us annotate the results. She was asked to label each topic as either  X  X ery important X ,  X  X omewhat important X  or  X  X nimportant X . In addition, for each topic, she was also asked to label each of the top 50 keywords as either  X  X elevant X  or  X  irrelevant X . When asked about how she ranked these topics, the email owner summarized her criteria as: (1) A  X  X ery important X  topic clearly describes a major project that the email owner heavily involved. (2) A  X  X omewhat important X  X opic focuses on a specific event, such as writing a paper. (3) An  X  X nimportant X  topic either lacks a clear focus or is about very general work-related activities.
For the AIG news data set, we asked a person who was familiar with the recent AIG-related events to help us anno-tate the topics and keywords. This person determined the importance of a topic as follows: (1) A  X  X ery important X  topic clearly describes an event directly related to AIG, e.g. the AIG bonus controversy. (2) A  X  X omewhat important X  topic focuses on some background events such as the 2009 presidential election or the financial market crisis. (3) An  X  X nimportant X  topic is defined as either confusing or irrele-vant, e.g. a topic about various advertisements.

Given the annotated topics and keywords, we compared the automatic topic and keyword re-ranking results with the human-provided results using the F 1 -measure, a criterion commonly used in information retrieval (IR). Following the IR tradition, in our analysis we categorized our topics an-notated by our experts into both  X  X elevant X  and  X  X rrelevant X . The  X  X elevant X  topics are those that are either  X  X ery impor-tant X  or  X  X omewhat important X  while  X  X rrelevant X  topics are those that are  X  X nimportant X . Similarly, based on our topic or keyword re-ranking methods, each topic or keyword can be categorized as either  X  X etrieved X  or  X  X ot retrieved X  de-pending on the assigned ranks and the cut-off threshold used in each evaluation metric ( X  X op 5 X  means only the top five keywords are retrieved).
The keyword re-ranking results are shown in Tables 2 and 4. In these Tables, KR 0 is the baseline that uses the LDA estimated parameters  X   X  k,i directly; KR 1 and KR 2 are de-fined in section 3.2. We can see that KR 1 performed better than KR 0 and KR 2 . It shows that for the two selected data sets, topic proportion sum is better than topic proportion production in weighing the proportion.

The topic re-ranking results for the email data set are shown in Table 3. In the Table,  X  X .V. X  represents Weighted Topic Coverage and Variation ,  X  X .S. X  represents Laplacian Score ,  X  X .I. X  represents Pairwise Mutual Information , and  X  X .S. X  represents Topic Similarity . Our results show that the Laplacian score-based method outperformed the other methods. In particular, all the top five retrieved topics were labeled by the email owner as relevant.

The topic re-ranking results for the AIG data set are shown in Table 5. The Laplacian score-based method also outperformed all the other methods using this data set. Overall, the Laplacian score-based re-ranking method seems to capture the essence of an important topic the best. Ta-ble 1 shows the details of the Laplacian score-based topic re-ranking results for the AIG data set. It contains the top 10 keywords of all the 20 topics. government; int X  X  means international).
In this paper, we have presented several topic and keyword re-ranking methods. Our experiments show that among the topic keywords re-ranking methods that we investigated, KR 1 outperformed others on both of our test data sets. Moreover, among all the topic re-ranking methods that we tested, the Laplacian score-based method performed the best.
We are also working on several areas to further improve the current re-ranking methods. One area is to allow users to interactively define their topic or keyword ranking cri-teria. We are also interested in exploring the use of vari-ous application-specific features to build more accurate topic summarization systems. We would like to thank David Mimno, Tom Griffiths, and Gregor Heinrich for their help and advices on LDA and its implementation. Table 4: News (AIG) Keyword Re-ranking Results.
 Table 5: News (AIG) Topic Re-ranking Results.

