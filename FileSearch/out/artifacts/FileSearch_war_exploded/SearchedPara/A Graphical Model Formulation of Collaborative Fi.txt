 Aaron J. Defazio aaron.defazio@anu.edu.au Tib  X erio S. Caetano tiberio.caetano@nicta.com.au NICTA and Australian National University Recommendation systems have presented new and in-teresting challenges to the machine learning commu-nity. The large scale and variability of data has meant that traditional approaches have not been applicable, particularly those that have quadratic or cubic running time.
 This has led to the majority of research taking two tracks: (1) latent-factor models, and (2) neighbour-hood models. Latent factor models embed both users and items into a low dimensional space, from which predictions can be computed using linear operations. These include continuous approaches, such as low-rank approximate matrix factorization (Funk, 2006) and binary variable approaches, such as restricted Boltz-mann machines (Salakhutdinov et al., 2007).
 The second track, neighbourhood models, is the one explored in this work. Neighbourhood methods form a graph structure over either items or users, where edges connect items/users that are deemed similar. Rating predictions are performed under the assumption that users rate similar items similarly (for an item graph) or that similar users have similar preferences (for a user graph) using some form of weighted average (Sarwar et al., 2001).
 In this paper we propose a neighbourhood model that treats the item graph as a undirected probabilistic graphical model. This allows us to compute distribu-tions over ratings instead of the point estimates pro-vided by alternative neighbourhood methods. Predic-tions for a user are performed by simply conditioning the model on her previous ratings, giving a distribu-tion over the set of items she has yet to rate. The pre-dictive rule takes into account non-local information in the item graph, allowing for smaller neighbourhood sizes than are used in other approaches.
 We also present an efficient learning algorithm for our model, based on the Bethe entropy approximation. It exploits a decomposition of the variable matrix into diagonal and off-diagonal parts, where gradient ascent need only be performed for the diagonal elements. As our method loops over a set of edges instead of the full set of training data, training is orders of magni-tude faster than stochastic gradient descent (SGD) ap-proaches such as Koren (2010). For example, the item graph for the data-set we consider (see Section 6) has approximately 40 thousand edges, which is small com-pared to the 1 million data-points that are considered in each SGD iteration.
 We begin by introducing the foundations of our model. We are given a set of users and items, along with a set of real valued ratings of items by users. Classical item neighbourhood methods (Sarwar et al., 2001) learn a graph structure over items i = 0 ...N  X  1, along with a set of edge weights s ij  X  R , so that if a query user u is presented, along with his ratings for the neighbours of item i ( r uj , j  X  ne ( i )), the predicted rating of item i is where  X  i  X  R represent average ratings for that item over all users. See Figure 1 for an example of an actual neighbourhood for a movie recommendation system. In order to use the above method, some principle or learning algorithm is needed to choose the neighbour weights. The earliest methods use the Pearson correla-tion between the items as the weights. In our notation, the Pearson correlation between two items is defined as The set of neighbours of each item is chosen as the k most similar, under the same similarity measure used for prediction. More sophisticated methods were developed for the NetFlix competition, including the work of Bell &amp; Koren (2007), which identified the fol-lowing problems with the above:  X  Bounded similarity scores can not handle deter- X  Interactions among neighbours are not accounted  X  The weights s ij cause over-fitting when none of Learning the weights s ij under an appropriate model can alleviate all of these problems, and provide supe-rior predictions (Bell &amp; Koren, 2007), with the only disadvantage being the computational time required for training the model. Learning the neighbourhood structure for such a model is not straightforward due to the potentially quadratic number of edges. In this paper we take the approach used by other neighbour-hood methods, and assume that the neighbourhood structure is chosen by connecting each item to it X  X  k most similar neighbours, using Pearson correlation as the similarity measure. We denote this undirected edge set E . Structure learning is in principle possi-ble in our model, using variants of recently proposed methods for covariance selection (Duchi et al., 2008). Unfortunately such methods become unusable when the number of items considered exceeds a few thou-sand.
 In the next section, we show that the edge weights can be interpreted as parameters of a distribution defined by a particular graphical model. This interpretation leads to fundamentally different rating and training methods, which we explore in Sections 4 and 5 respec-tively. Undirected graphical models are a general class of probability distributions defined by a set of feature functions over subsets of variables, where the func-tions return a local measure of compatibility. In our case the set of variables is simply the set of items, whose values we treat as continuous variables in the range 1 to 5. For any particular user, given the set of their items ratings ( R K ), we will predict their rat-ings on the remaining items ( R U ) by conditioning on this distribution, namely computing expectations over P ( R U | R K ).
 The most common feature domains used are simple tuples of variables ( i,j ), which can be equated with edges in the graphical model, in our case the item graph. We will additionally restrict ourselves to the class of log-linear models, which allows us to write the general form of the distribution as where  X  is the set of features weights, and Z is the par-tition function, whose value depends on the parame-ters  X , and whose purpose is to ensure the distribution is correctly normalized. The choice of feature function for log-linear models is problem dependent, however in our case two choices stand out. We want functions that encourage smoothness, so that a discrepancy be-tween the ratings of similar items is penalized. We propose the use of squared difference features These features, besides being intuitive, have the ad-vantage that for any choice of parameters  X  we can form a Gaussian log-linear model that defines that same distribution.
 In a Gaussian log-linear model, pairwise features are defined for each edge as f ij ( r i ,r j ) = ( r i  X   X  i )( r and unary features as f i ( r i ) = 1 2 ( r i  X   X  i ) 2 . The pair-wise feature weights  X  ij correspond precisely with the off diagonal entries of the precision matrix (that is, the inverse of the covariance matrix). The unary feature weights  X  i correspond then to the diagonal elements. Thus we can map the squared difference features to a constrained Gaussian, where the diagonal elements are constrained so that for all i , We will denote the sparse symmetric matrix of weights for both the Gaussian and squared difference forms as  X , with the diagonal constrained in this way. This allows us to freely convert between the two forms. We will impose an additional constraint on the allow-able parameters, that each off-diagonal element is non-positive; this constraint ensures that only similarity (as a opposed to dissimilarly) is modelled, and pre-vents numerical issues in the optimization discussed in Section 5. The feature functions defined in the previous section appear somewhat arbitrary at first. We will now show that they are additionally motivated by a simple link to existing neighbourhood methods. Consider the case of predicting a rating r ui , where for user u all ratings r uj , j  X  ne( i ) are known. These neighbours form the Markov blanket of node i . In this case the conditional distribution under the item field model is: This is a univariate Gaussian distribution, whose mean is given by a weighted sum of the same form as for traditional neighbourhood methods. In practice we rarely have ratings information for each item X  X  com-plete neighbourhood, so this special case is just for illustrating the link with existing approaches. In the general case, conditioning on a set of items K with known ratings r K , with the remaining items de-noted U , we have: Thus computing the expected ratings requires nothing more than a few fast sparse matrix operations, includ-ing one sparse solve. If the prediction variances are required, both the variances and the expected ratings can be computed using belief propagation, which often requires fewer iterations than the sparse solve opera-tion (Shental et al., 2008).
 The linear solve in this prediction rule has the effect of diffusing the known ratings information over the graph structure, in a transitive manner. Figure 2 shows this effect which is sometimes known as spreading activa-tion. Such transitive diffusion has been explored previ-ously for collaborative filtering, in a more ad-hoc fash-ion (Huang et al., 2004).
 Note that this prediction rule is a bulk method, in that for a particular user, it predicts their ratings for all unrated items at once. This is the most common case in real world systems, as all item ratings are required in order to form user interface elements such as top 100 recommendation lists. The traditional way to train an undirected graphical model is using maximum likelihood. When exact in-ference is used, this is equivalent to maximum entropy training (Koller &amp; Friedman, 2009), as one is the con-cave dual of the other. However, in practice variational approximations such as the Bethe approximation are used; for example when inference is performed using Belief propagation. In which case the approximate maximum entropy problem is not usually concave, and the solutions of the two are no longer necessarily equiv-alent.
 In discrete models, the (constrained) approximate maximum entropy approach has been shown to learn superior models in some cases, at the expense of train-ing time (Granapathi et al., 2008). In the case of the item field model we will establish that approximate maximum entropy learning is significantly faster, with comparable accuracy.
 We first consider the case of a Gaussian model, with the variance-style features described in Section 2, which our constrained Gaussian model builds upon. Let  X  denote the empirical rating covariance matrix, which we cap to non-zero only at locations ( i,j )  X  E . The full covariance matrix is dense, however we only need to access the entries at these locations, and it is notationally convenient to treat the rest as zero. The optimization procedure is over the parameters of the beliefs B = { b i ,b ij } . The approximate maximum en-tropy objective is subject to the constraints: Constraints 3 and 4 are the marginal consistency con-straints. When applied to Gaussian beliefs they simply assert that the covariance entries of beliefs with over-lapping domains should be equal on that overlap. Con-straints 5 and 6 are the normalizability constraints. For Gaussian beliefs they just enforce that the covari-ance matrices are all positive definite.
 In general graphical models, the beliefs are separately parametrized distributions over the same domains as the factors, in our case they take the form of 2D and 1D mean zero Gaussian distributions. We will make use of a representation of the beliefs in a compact form of a single (sparse) symmetric matrix of the same struc-ture as the covariance matrix, which we will denote C . This representation simply maps C ij = E b ij [ x i x evaluated at any of the beliefs. This is well defined as the value will be the same for any belief, as noted above. This representation makes the consistency con-straints implicit, using what is essentially variable sub-stitution.
 The Bethe entropy approximation of the beliefs in our notation is: Notice that log terms are undefined whenever the be-lief covariances are not positive definite. Thus the nor-malizability constraints are also extraneous. So for a purely Gaussian model, the approximate maximum entropy problem simplifies to: Stated this way, the solution is trivial as the con-straints directly specify C . However, we are interested in learning the weights  X , which are the Lagrange mul-tipliers of the equality constraints. The Lagrangian is where  X  X  ,  X  X  is the standard inner product on matrices. The Lagrangian has derivatives: Algorithm 1 Diagonal ascent algorithm for approxi-mate maximum entropy learning input: covariance  X , size N , step size  X 
C =  X  and k = 1 repeat until sufficient convergence return  X  Equating the gradients to zero, gives the following equation for  X : which gives the closed form solution The same solution for  X  can also be obtained us-ing the pseudo-moment matching technique, as  X  is a fixed point of the GaBP update equations for this parametrization. If we were applying a vanilla Gaus-sian model, we could use this result directly. However, for the item field model we have constraints on the diagonal. Using variable substitution on the diagonal, we get the following Lagrangian: which we denote L 0  X  ( C,  X ). It has gradients: In order to optimize the diagonally constrained objec-tive, we can take advantage of the closed form solution for the simpler unconstrained Gaussian. The proce-dure we use is given in Algorithm 1. It gives a quality solution in a small number of iterations (see Figure 3). The core idea is that if we fix the diagonal of C , the rest of the elements are determined. The gradient of the diagonal elements can be computed directly from  X , so we recalculate it at each iteration, then take a gradient step. The dual variables are used essentially as notation for the entropy gradients, not in any deeper sense. 5.1. Missing Data &amp; Kernel Functions The training methods proposed take as input a sparse subset of a covariance matrix  X , which contains the sufficient statistics required for training. It should be emphasized that we do not assume that the covariance matrix is sparse, rather our training procedure only needs to query the entries at the subset of locations where the precision matrix is assumed to be non-zero. As our samples are incomplete (we do not know all item ratings for all users), the true covariance ma-trix is unknown. For our purposes, we form a covari-ance matrix by assuming the unrated items are rated at their item mean. More sophisticated methods of imputation are possible; we explored an Expectation-Maximization (EM) approach, which did not result in a significant improvement in the predictions made. It did however give better prediction covariances. In general a kernel matrix can be used in place of the covariance matrix, which would allow the introduction of item meta-data through the kernel function. We left this avenue for future work. 5.2. Conditional Random Field Variants Much recent work in Collaborative filtering has con-cerned the handling of additional user meta-data, such as age and gender information usually collected by on-line systems (Stern et al., 2009). These attributes are naturally discrete, and so integrating them as part of the MRF model results in mixed discrete/continuous model. Approximate inference in such as model is no longer a simple linear algebra problem, and conver-gence becomes an issue. User attributes are better handled in a conditional random field (CRF) model, where the conditional distributions involve the contin-uous item variables only.
 Unfortunately the optimization technique described above does not extend readily to CRF mod-els. Approximate maximum entropy training using Difference-of-convex methods has been applied to CRF training successfully (Granapathi et al., 2008), al-though such methods are slower than maximum like-lihood. We explored CRF extensions using maximum likelihood learning, and while they did give better rat-ings predictions, training was slow due to the large number of belief propagation calls. While practical if the computation is distributed, the training time was still several hundred times slower than any of the other methods we tested. 5.3. Maximum Likelihood Learning with Belief An objective proportional to the negative log-likelihood for a Gaussian distribution under the Bethe approximation can be derived from the approximate entropy Lagrangian (Equation 7) using duality theory. First note that the Lagrangian can be split as follows: The dual is then formed by maximizing in terms of C : The term inside of the minimization on the right is the Bethe free energy (Yedidia et al., 2000). By equating with the non-approximate likelihood, it can be seen that the log partition function is being approximated as: The value of log Z ( X ) and a (locally) minimizing C can be found efficiently using belief propagation (Cseke &amp; Heskes, 2011). For diagonally dominant  X  belief propagation can be shown to always converge (Weiss &amp; Freeman, 2001). The diagonal constraints as well as the non-positivity constraints on the off diagonal elements of  X  ensure diagonal dominance in this case. Maximum likelihood objectives for undirected graphi-cal models are typically optimized using quasi-newton methods, and that is the approach we took here. The diagonal constraints are easily handled by variable substitution, and the non-positivity constraints are simple box constraints. We used the L-BFGS-B algo-rithm (Zhu et al., 1997)  X  a quasi-newton method that supports such constraints. The log-partition function log Z ( X ) is convex if we are able to exactly solve the inner minimization over C , which is not the case in general. For our comparison we tested on 2 representative datasets. The 1M ratings MovieLens dataset (Grou-pLens Research) consists of 3952 items and 6040 users. As there is no standard test/training data split for this dataset, we took the approach from Stern et al. (2009), where all data for 90% of the users is used for training, and the remaining users have their ratings split into a 75% training set and 25% test set. The 100K ratings MovieLens dataset involves 1682 items and 943 users. This dataset is distributed with five test/train parti-tions for cross validation purposes which we made use of.
 All reported errors use the mean absolute error (MAE) measure ( 1 N P N i |  X  i  X  r i | ). All 2 datasets consist of rat-ings on a discrete 1 to 5 star scale. Each method we tested produced real valued predictions, and so some scheme was needed to reduce the predictions to real values in the interval 1 to 5. For our tests the val-ues were simply clamped. Methods that learn a user dependent mapping from the real numbers into this in-terval have been explored in the literature (Stern et al., 2009).
 Table 1 shows the results for the two Movielens datasets. Comparisons are against our own imple-mentation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochas-tic gradient descent for all factors) and the neighbour-hood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective. All implementations were in python (using cython compilation), so timings are not comparable to fast implementations. We also show results for vari-ous methods of training the item field model besides the maximum entropy approach, including exact max-imum likelihood training.
 The item field model outperforms the other neighbour-hood methods when sparse (10 neighbour) models are used. Increasing the neighbourhood size past roughly 10 actually starts to degrade the performance of the item field model: at 50 neighbours using maximum en-tropy training on the 1M dataset the MAE is 0.6866 vs 0.6772 at 10 neighbours. We found this occurred with the other training methods also. This may be caused by an over-fitting effect as restricting the number of neighbours is a form of regularization.
 The latent factor model and the least squares neigh-bourhood model both use stochastic gradient descent for training. They required looping over the full set of training data each iteration. The maximum entropy method only loops over a sparse item graph each it-eration which is why it is roughly two thousand times faster to train. Note that the dataset still has to be processed once to extract the neighbourhood structure and covariance values, the timing of which is indicated in the precomputation column. This is essentially the same for all the neighbourhood methods we compared. In an on-line recommendation system the covariance values can be updated as new ratings stream in, so the precomputation time is amortized. Training time is more crucial as multiple runs from a cold-start with varying regularization are needed to get the best per-formance (due to local minima). There has been previous work that applies undi-rected graphical models in recommendation systems. Salakhutdinov et al. (2007) used a bipartite graphi-cal model, with binary hidden variables forming one part. This is essentially a latent factor model, and due to the hidden variables, requires different and less efficient training methods than those we apply in the present paper. They apply a fully connected bipartite graph, in contrast to the sparse, non-bipartite model we use. Multi-scale conditional random fields mod-els have also been applied to the more general social recommendation task with some success (Xin et al., 2009). Directed graphical models are commonly used as a modelling tool, such as in Salakhutdinov &amp; Mnih (2008). While undirected models can be used in a sim-ilar way, the graph structures we apply in this work are far less rigidly structured.
 Several papers propose methods of learning weights of a neighbourhood graph (Koren, 2010) (Bell &amp; Koren, 2007), however our model is the first neighbourhood method we are aware of which gives distributions over its predictions. Our model uses non-local, transitive information in the item graph for prediction. Non-local neighbourhood methods have been explored us-ing the concept of spreading activation, typically on the user graph (Griffith et al., 2006) or on a bipartite user-item graph (Lie &amp; Wang, 2009). We have presented an undirected graphical model for collaborative filtering that naturally generalizes the prediction rule of previous neighbourhood methods by providing distributions over predictions rather than point estimates. We detailed an efficient training al-gorithm based on the approximate maximum entropy principle, which after preprocessing takes less than a second to train, and is two orders of magnitude faster than a maximum likelihood approach. Our model has fewer parameters than other comparable models, which is an advantage for interpretability and training. NICTA is funded by the Australian Government as represented by the Department of Broadband, Com-munications and the Digital Economy and the Aus-tralian Research Council through the ICT Centre of Excellence program.

