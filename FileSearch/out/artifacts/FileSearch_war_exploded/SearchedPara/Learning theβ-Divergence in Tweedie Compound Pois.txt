 Umut S  X im  X sekli umut.simsekli@boun.edu.tr Ali Taylan Cemgil taylan.cemgil@boun.edu.tr Yusuf Kenan Y X lmaz kenan@sibnet.com.tr Sibnet Computers Ltd, 34742 Kad X k  X oy, Istanbul, Turkey Non-negative Matrix Factorization (NMF) is a widely used algorithm for data analysis. The goal is calcula-tion a factorization of the form: where X is the given data matrix,  X  X is an approxi-mation to X , and Z 1 , and Z 2 are non-negative fac-tor matrices. This model has been applied to various fields including signal processing, finance, bioinformat-ics, and natural language processing (Cichocki et al., 2009). One of the most popular approaches for com-puting factorizations is based on minimization of a di-vergence D : In practice, a separable divergence D ( X ||  X  X ) = P i,j d ( X ( i,j ) || gence (i.e., cost) functions are special cases of the  X  -divergence, defined as p = 2  X   X  : where p is an index parameter. By taking appropriate limits it is easy to verify that d p is the Euclidean dis-tance square, information divergence or Itakura-Saito divergence (F  X evotte et al., 2009) for p = 0 , 1 and 2 respectively.
 The key idea of the current paper is to exploit the close connection between  X  -divergences and a partic-ular exponential family, the so-called Tweedie models (Y X lmaz &amp; Cemgil, 2012). It turns out that Tweedie densities, to be described in more detail in the follow-ing section, can be written in the following moment form where  X  x is the mean,  X  is the dispersion and p is the index parameter of the  X  -divergence defined in (3). An important property is that the normalization constant Z does not depend on  X  x ; hence it is easy to see that for fixed p and  X  , solving a maximum likelihood problem for  X  x is indeed equivalent to minimization of the  X  -divergence.
 Note that for the familiar Gaussian case, we have d ( x ;  X  x ) = ( x  X   X  x ) 2 / 2
P ( x ;  X  x, X ,p = 0) = the dispersion is simply the variance. As for all ad-missible p we have a similar form, Tweedie models generalize the established theory of least squares lin-ear regression to more general noise models (restricted to identity link functions).
 Matrix factorization (MF) is often viewed as a diver-gence minimization problem, and various algorithms for solving the optimization problem in (2) have been proposed. Often, multiplicative updates are used in practice for their simplicity, yet many extensions and variations have been proposed (Y X lmaz et al., 2011). However, the divergence minimization perspec-tive does not provide a complete picture of MF models. One key question is the choice of the divergence. In practice, several divergence functions are tried on the problem and models are evaluated according to an ap-plication specific success criterion. Another problem arises in collective factorization, for example when we wish to decompose several matrices collectively as in the following block matrix model This can be viewed as a coupled factorization of X and X 2 where the factor Z 1 is being shared. If the data matrices are representing different modalities, it is natural that we might want to choose a cost function that puts more emphasis on one matrix using weights as We will refer to such cost functions as mixed  X  -divergences. The probabilistic perspective provides here a natural, data driven formulation in choosing the relative weights by maximization of a joint likelihood with respect to the dispersion parameters  X   X  and pos-sibly the individual divergences D p  X  via determination of p  X  for  X  = 1 , 2. The Tweedie family is a particular exponential dis-persion model (EDM) (J X rgensen, 1997). EDM X  X  are a well-studied family of distributions and have found place in various fields. It has an important role at statistical data analysis as the response distribution of the generalized linear models (McCulloch &amp; Nelder, 1989).
 An exponential dispersion model (in canonical form) can be defined by a two parameter density as follows (J X rgensen, 1997): where  X  is the canonical (natural) parameter,  X  is the dispersion parameter and  X  is the cumulant (log-partition) function ensuring normalization. Here, h ( x, X  ) is the base measure and is independent of the canonical parameter. For EDM, it is easy to verify that the mean  X  x (also called expectation parameter) and the variance Var { x } are obtained directly from the first and second derivatives of  X  (  X  ) with respect to the canonical parameter Here v ( X  x ), the second derivative, is also known as the variance function (Tweedie, 1984; Bar-Lev &amp; Enis, 1986; J X rgensen, 1997).
 As a special case of EDMs, Tweedie distributions TW ( x ;  X  x, X ,p ) specify the variance function as The variance function is related to the p  X  X h power of the mean, therefore it is called a power variance func-tion. Note that this choice directly dictates the form of  X  (  X  ) that can be solved as Here, different choices for p yield well-known impor-tant distributions such as the Gaussian ( p = 0), Pois-son ( p = 1), compound Poisson (1 &lt; p &lt; 2), Gamma ( p = 2) and inverse Gaussian ( p = 3) distributions. Excluding the interval 0 &lt; p &lt; 1 for which no EDM exists, for all other values of p not mentioned above, one obtains stable distributions (J X rgensen, 1997). In this study, we focus on the inference in the ma-trix/tensor factorization models with p  X  (1 , 2) and p is unknown. Tweedie distribution with p  X  (1 , 2) is equivalent to the compound Poisson distribution and has a support for continuous positive data and a discrete probability mass at zero. The presence of the discrete mass at zero makes this distribution suit-able for many applications where observations are of-ten zero but sometimes are positive. Handling this using a single family has been illustrated to be useful in many applications, including actuarial science (no claim/claim amount), rainfall modeling (no rain/rain amount), fishery prediction (no catch/some catch) (Dunn &amp; Smyth, 2005).
 Maximum likelihood estimation of the compound Pois-son distribution is relatively simple only if the index parameter p is known beforehand. If p is not known, it is a quite challenging task to make inference on the compound Poisson models. Related to this problem, in (Zhang, 2012) the authors present likelihood-based inferential methods and a Monte Carlo EM algorithm for making inference in compound Poisson models. In another recent study (Lu et al., 2012), the authors present a score matching method for finding the best p for the simpler case where they assumed unitary dis-persion. In this study, we present three methods for making inference in matrix/tensor factorization mod-els with compound Poisson observation models. In the first and the second methods, we follow a vari-ational approach, where in the third method we in-tegrate out the dispersion parameter. We evaluate the proposed methods on two applications. Firstly, we evaluate our methods on modeling symbolic repre-sentations for polyphonic music. Secondly, we define a novel coupled tensor factorization model and evaluate our methods on prediction of the lyrics of a song from its audio features. The goal in this section is to give a compact charac-terization of the compound Poisson distribution as a Tweedie model (J X rgensen, 1997). We will show that the Tweedie density with p  X  (1 , 2) coincides with the compound Poisson density. A random variable x that is the sum of n independent and identically distributed Gamma random variables is compound Poisson dis-tributed, when n is Poisson distributed. The genera-tive model is (J X rgensen, 1997): where n and g i are Here, PO and G denote the Poisson and Gamma den-sities, respectively. The marginal density P ( x ) is com-pound Poisson. More compactly, we can also write x | n  X  X  ( x ; an,b ).
 To show the equivalence to the Tweedie, we first note that the cumulant generating function (CGF) K u ( s ) of a random variable u with density P ( u ) is defined as K u ( s ) = log G u ( e s ) where G u ( z ) =  X  z u  X  generating function. From basic probability theory, we know that the generating function of the sum of a random number of iid variables is obtained by nesting as G x ( z ) = G n ( G g ( z )), where
G n ( z ) = exp(  X  ( z  X  1)) G g ( z ) = (1  X  log( z ) /b ) are generating functions for the Poisson and Gamma densities. By substitution we obtain the CGF of x as Now, we will show that we obtain the same CGF start-ing from the power variance assumption. We can easily verify that CGF for EDM in (7) is given by (J X rgensen, 1997; Dunn &amp; Smyth, 2005) If we substitute the expression for  X  (  X  ) in (11) and then express the result as a function of the expectation parameter  X  x by noting that (as d X /d  X  x = v ( X  x )  X  1 =  X  x  X  p ), we obtain K x ( s ;  X , X  ) = that has the same form as (14). By matching term by term, we see that the Tweedie distribution for 1 &lt; p &lt; 2 is the compound Poisson distribution with the following parameter mapping: By using this mapping, the joint distribution can be written as follows: P ( x,n |  X  x, X ,p ) = P ( x | n,  X  x, X ,p ) P ( n |  X  x, X ,p ) It turns out that P n p ( x,n | X  ) does not have a closed form. Here, Dunn and Smyth provide numerical meth-ods for approximate computation (Dunn &amp; Smyth, 2005), but we propose here two simpler algorithms. An example pdf of a compound Poisson distribution is given in Figure 1. An interesting property of the joint distribution in (18) is that  X  x and n are conditionally independent given the index parameter p and the dispersion  X  , as the joint factorizes such that there are no cross terms that contain both  X  x and n . Besides, the terms that depend on  X  x are specified by the  X  -divergence. Therefore, any standard algorithm that minimizes the beta divergence can be used here.
 When dealing with factorization models (i.e  X  x is de-composed into some latent factors), we seek the best factorization whose form can vary depending on the application. If we consider the model that is defined in (6), maximum likelihood estimation of the factors under mixed cost functions can be achieved by iter-atively applying the multiplicative update rules given in (Y X lmaz et al., 2011). The update rule for the factor Z 1 can be written as follows: where p  X  are the index parameters,  X   X  are the disper-sion parameters, A  X  B and A B denotes element-wise product and division of two matrices A and B , respec-tively. Here,  X   X  (  X  ) are functions that are defined as follows: where &gt; denotes the matrix transpose. Besides, M  X  is a binary matrix of size X  X  that have values of 1 (0) where X  X  is observed (missing).
 When p  X  and  X   X  are not known beforehand, the infer-ence problem gets complicated. In this study, we focus on estimating p  X  and  X   X  when p  X   X  (1 , 2). Since p  X  and  X   X  are conditionally independent from the factors, given the mean parameter, our methods can be used in any matrix and tensor factorization model. There-fore, we stick to our vector notation where we define x  X  vec ( X  X  ),  X  x  X  vec (  X  X  X  ), m  X  vec ( M  X  ), and  X  denotes the observed matrix/tensor index for the case when we have multiple (most likely multimodal) ob-served matrices/tensors. Here, vec (  X  ) is the vectoriza-tion operator (i.e. the colon operator in Matlab). In the next subsections, we present three novel infer-ence methods for estimating the index parameter in Tweedie compound Poisson models. In the first and the second methods we follow a variational approach, where in the third method we integrate out the dis-persion parameter and make inference on the marginal distribution. 4.1. Variational Approach In this section, we present two variational methods, namely the Iterative Conditional Modes (ICM) and the Expectation-Maximization (EM) algorithms.
 The ICM algorithm iteratively maximizes over the pa-rameters n ,  X  , and p given x and  X  x . Even though the maximization over n is intractable, we can find the mode n  X  by approximating the log  X (  X  ) functions in (18) by using Stirling X  X  approximation, as proposed in (Dunn &amp; Smyth, 2005). The mode has the following analytical form: Maximizing the dispersion parameter  X  is straightfor-ward, however, since the index parameter p and  X  are closely related to the variance and may affect each other, it can be necessary to regularize  X  in order to have a better estimate of p . It is easy to verify that the conjugate prior of the dispersion parameter is the in-verse Gamma distribution. Therefore, here we assume an inverse Gamma prior on  X  :  X   X  X G (  X  ;  X   X  , X   X  ). The optimal dispersion, given the other parameters is as follows: Surprisingly, none of the references we are aware of used this conjugate prior. In the next section we will use this property to analytically integrate out the dis-persion parameter.
 The last step of the ICM algorithm is to compute the maximization over p . Since the optimal p does not have an analytical solution, we consult numerical methods. As the domain of p is limited to (1 , 2), we run a simple line search procedure in order to estimate the index parameter p .
 To sum up, at each iteration of the estimation algo-rithm, we first estimate the factors and compute the mean parameter  X  x . Then, we compute the parameters n  X  and  X   X  that are described above, and finally we compute the optimal index parameter p . This proce-dure is run until convergence.
 The EM algorithm is quite similar to the ICM algo-rithm in algorithmic sense, where we merely replace n  X  with the expectation  X  n  X  in (23). Unfortunately, computing this expectation is also intractable. There-fore, we use a numerical method that is similar to the one proposed in (Dunn &amp; Smyth, 2005). By using the fact that the conditional distribution of n is unimodal, we approximate the expectation by numerically com-puting it around the mode which is defined in (22). The rest of the EM algorithm is the same as the ICM algorithm. 4.2. Integrating out the Dispersion Parameter The dispersion parameter plays a key role when there are more than one observed tensor (see (19)). How-ever, when we have only one observed tensor, the dis-persion parameter does not contribute to the estima-tion of the factors in a factorization model as it cancels out in the multiplicative update rules.
 In this section we integrate out the dispersion param-eter  X  and n and make inference on the marginal dis-tribution. When assumed an inverse Gamma prior on  X  , we obtain the following marginal distribution: P ( x,n ) = In order to estimate the index parameter p , we also marginalize out n by using numerical methods. Fi-nally, the optimal p is found by a line search algorithm, similar to ICM and EM. In order to evaluate our methods, we conduct experi-ments on both synthetic and real data. Due to space limitations, in this paper we only present the experi-ments that we conduct on real data. The other exper-iments can be found in http://www.cmpe.boun.edu. tr/ ~ umut/icml2013 . 5.1. Polyphonic Music Modeling Along with the rapid development of computa-tional power and statistical modeling techniques, factorization-based music modeling has become pop-ular. This paradigm has been shown to be successful in many applications including polyphonic pitch tran-scription, source separation and audio restoration. Recent studies suggest that, when designed properly, polyphonic pitch transcription methods with higher level musical models yield better transcription per-formance (Boulanger-Lewandowski et al., 2012). In this section, we present a tensor factorization model for symbolic musical data modeling. This model can be used as a side model for factorization-based audio models.
 Symbolic music representation is similar to the sheet representation of music, where symbolic data contain high level musical information, such as note onset times, note durations, and the pitch of the notes that occur in a musical piece. Musical Instrument Digital Interface (MIDI) is one of the standards of symbolic music representation.
 One disadvantage of the symbolic representation is that it does not reflect the temporally varying charac-teristics of the musical notes. We have the information of the velocities at the note onsets, however we cannot obtain the damping structure that the notes naturally have. Therefore, in order to have a better representa-tion, we quantize the time into time-frames and encode the musical information into a matrix X  X  { X ( n,t ) } where n is the note index and t is the time frame in-dex. Here X ( n,t ) simulates the time-varying velocity (volume) of note n during time frame t . For instance, if the note n is active at both the time-frame t and t + 1, then the velocities have the following relation: X ( n,t + 1) =  X X ( n,t ) where 0 &lt;  X  &lt; 1. This repre-sentation mimics the structure of an excitation matrix of the Nonnegative Matrix Factorization model for au-dio signals (Smaragdis &amp; Brown, 2003).
 By construction, only a couple of notes will be active at a given time frame t , therefore X will consist of mostly zeros and some positive values. We can ob-serve that assuming a compound Poisson observation model is quite reasonable as the compound Poisson distribution has a nonnegative probability mass at 0 and a continuous density on positive values.
 In this study, we use Nonnegative Matrix Factor De-convolution (NMFD) model (Smaragdis, 2004) in or-der to model the modified symbolic musical data. Apart from using the benefits of the NMF model, this model is also capable of modeling the temporal in-formation of the music. We can define the model as follows:
X ( n,t )  X   X  X ( n,t ) = X where D is the dictionary tensor and E encapsulates the corresponding excitations.
 Since we have only one observed tensor in this model, we can use all three of the inference methods that have been described. In order to evaluate our methods on modeling the symbolic data, we firstly erase some columns (time frames) of the data, then reconstruct the missing parts by using the NMFD model. This reconstruction problem is not trivial as entire time frames (columns of X ) can be missing.
 In our experiments we use the MIDI Aligned Piano Sounds (MAPS) database (Emiya et al., 2010). We use 10 excerpts from 5 different classical music pieces. Af-ter generating the X matrices from the symbolic data, we randomly erase some columns of the data which are going to be reconstructed later on. In order to obtain the reconstructed symbolic data, we simply combine the observed parts of X and the estimated parts of  X  X : M  X  X + (1  X  M )  X   X  X , where M is the binary mask that is introduced in Section 4. We evaluate and com-pare the performances of our methods by measuring the signal-to-noise ratio (SNR) between the corrupted and the reconstructed symbolic musical data.
 In our experiment settings, the duration of the ex-cerpts is 10 seconds, where we use time frames of 93 milliseconds. We select  X   X  = 5 and  X   X  = 3, | k | = 50, and |  X  | = 5 for all methods, where | X | denotes cardi-nality. The results are shown in Figure 2.
 The results suggest that, the methods always improve the quality of the corrupted symbolic data. The ICM and the EM algorithm give similar results, where the Bayesian method seems to be more sensitive to the missing data than the variational methods. The esti-mated index parameter p differs for each piece that is reconstructed. Besides, each algorithm finds different p values: the average values for the index parameter are 1 . 01 (ICM), 1 . 19 (EM), and 1 . 26 (Bayesian). For all methods, we get about 4 dB SNR improvement where 50% of the data is missing; gracefully degrading from 10% to 90% missing data. Figure 3 visualizes an example reconstruction. It can be observed that the compound Poisson model yields a better reconstruc-tion, where the Gaussian model introduces spurious notes.
 As the results are encouraging even when quite long portions of the data are missing, we can say that mod-eling the polyphonic music with this approach seems reasonable and might produce good results when used in more complicated models. 5.2. Coupled Audio and Lyrics Modeling In this section, we illustrate how our approaches can be used with multimodal data. Coupled factorization models have been shown to be useful at fusing infor-mation from multimodal data (S  X im  X sekli et al., 2012). Here, we illustrate how the index parameter p and the corresponding dispersion  X  will be estimated under coupled models with mixed observation models where at least one of the observation model is the compound Poisson model.
 We present a novel coupled matrix factorization model which combines audio features and the lyrics of songs. The aim of this application is to predict the bag-of-words representation of the lyrics of a song given its audio features. This is an interesting application which tries to estimate the keywords that should exist in the lyrics of a song by making use of its audio features and the information from other songs.
 Suppose we observe the matrices X 1  X { X 1 ( f,s ) } and X 2  X  { X 2 ( w,s ) } , where X 1 contains the song-level audio features and X 2 contains the bag-of-words rep-resentation of the lyrics of the songs in their columns. Here, f denotes the audio feature index, s is the song index, w is the word index. We decompose these ma-trices by using the NMF model as follows: where D 1 and D 2 are the dictionary matrices and E 1 and E 2 are the corresponding excitation matrices. By also assuming a low rank model over the excitation matrices, we hierarchically factorize the excitations by using another NMF model as follows: where B 1 and B 2 are the dictionaries for the excita-tions. With a final assumption that a particular song would use the same columns of the dictionaries B 1 and B 2 , we can say that it would have the same excitations. By this approach, we can relate the audio features to the lyrics. We define the ultimate coupled model as follows: Figure 4 visualizes this model. Note that, an NMF-based approach is proposed for modeling lyrics in (Dik-men &amp; F  X evotte, 2012) and the authors report success-ful results.
 One can come up with many different applications by using this model; in this study, we focus on the pre-diction of the lyrics of a song in a bag-of-words rep-resentation. It is fairly easy to predict the lyrics of a particular song by using this model: we mark the re-lated parts of the binary mask M 2 (see Section 4) as unobserved, then make predictions by using  X  X 2 . In our experiments we use the Million Song Dataset (MSD) and the MusiXmatch dataset (Bertin-Mahieux et al., 2011). The MSD is a free collection of audio features and metadata that are gathered from a large number of music tracks. These features include the key, tempo, time signature, duration, genre tags, year, loudness, and the chroma features of the songs. We use the song level features of random 500 pop songs where we use 2827 features for each song, yielding an audio feature matrix X 1 of size 2827  X  500.
 The MusiXmatch dataset contains the lyrics of the songs in a bag-of-words representation. This dataset contains more than 230 thousand songs, all being matched with the ones of MSD. Here, we use the num-ber of occurrences of the most common 5000 words of each song, where these 5000 words cover over 92% of all the words in the dataset. We use the same songs that are selected while constructing X 1 . Therefore, we have the lyrics matrix X 2 of size 5000  X  500, where each column of X 2 holds a bag-of-words lyrics of a song. In our experiment settings, we select p 1 = 1 with uni-tary dispersion, which corresponds to the Poisson ob-servation model. Note that, we could also optimize the dispersion  X  1 , but this is out of the scope of this study. We set | k | = | n | = 25 and | r | = 10. In order to esti-mate the factors, we use the method that is presented in (Y X lmaz et al., 2011). At each run, we estimate the factors, the index parameter p 2 , and the dispersion  X  2 We predict the lyrics of random 10 songs at once and we repeat this process 5 times.
 In order to assess the quality of the predictions, we measure the word detection performance. We estimate the predictions  X  X 2 and then consider the words as de-tected if the corresponding entries in  X  X 2 are above some threshold. We compute the true positive and the false positive rates as the performance metrics. Figure 5 visualizes the results. It can be observed that both algorithms yield very similar results. We get more than 80% of true positive rate while keeping the false positive rate less than 20%. Besides, the ICM algorithm seems more advantageous since its compu-tational requirements are much lower than the EM al-gorithm. These results are encouraging since the lyrics are predicted by solely using the song level audio fea-tures. The compound Poisson distribution is a useful distri-bution for sparse data as it has a discrete probability mass at zero and a support for continuous positive data. In this study, we presented inference methods for estimating the index and the dispersion parame-ter of the Tweedie compound Poisson models. In the first two methods, we followed a variational approach, where in the third method we estimated the index pa-rameter by using its marginal distribution. One of the contributions of this study is to make use the conju-gate prior on the dispersion parameter, which has not been investigated in the literature yet.
 We evaluated and compared our methods on real data. Firstly, we evaluated our methods on modeling sym-bolic representations for polyphonic music. Secondly, we defined a novel coupled tensor factorization model and evaluated our methods on prediction of the lyrics of a song from its audio features. Our conclusion is that the compound poisson based factorization mod-els can be useful for sparse positive data.
 Funded by T  X  UB  X  ITAK grant number 110E292, project Bayesian matrix and tensor factorizations (BAYTEN). U. S  X . is also supported by a Ph.D. scholarship from T  X  UB  X  ITAK.
 Bar-Lev, S. K. and Enis, P. Reproducibility and nat-ural exponential families with power variance func-tions. Annals of Stat. , 14, 1986.
 Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whit-man, Brian, and Lamere, Paul. The million song dataset. In Proceedings of the 12th International
Conference on Music Information Retrieval (ISMIR 2011) , 2011.
 Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal depen-dencies in high-dimensional sequences: Application to polyphonic music generation and transcription.
In International Conference on Machine Learning (ICML) , 2012.
 Cichocki, A., Zdunek, R., Phan, A. H., and Amari, S. Nonnegative Matrix and Tensor Factorization . Wiley, 2009.
 S  X im  X sekli, U., Y X lmaz, Y. K., and Cemgil, A. T. Score guided audio restoration via generalised coupled tensor factorization. In IEEE International Confer-ence on Acoustics, Speech, and Signal Processing , 2012.
 Dikmen, Onur and F  X evotte, C  X edric. Maximum marginal likelihood estimation for nonnegative dic-tionarylearning in the gamma-poisson models. IEEE
Transactions on Signal Processing , 60(10):5163 X  5175, 2012.
 Dunn, P. K. and Smyth, G. S. Series evaluation of tweedie exponential dispersion model densities. Stats. &amp; Comp. , 15:267 X 280, 2005.
 Emiya, V., Badeau, R, and David, B. Multipitch es-timation of piano sounds using a new probabilistic spectral smoothness principle. IEEE TASLP , 18(6): 1643 X 1654, 2010.
 F  X evotte, C., Bertin, N., and Durrieu, J. L. Nonnega-tive matrix factorization with the Itakura-Saito di-vergence. with application to music analysis. Neural Computation , 21:793 X 830, 2009.
 J X rgensen, B. The Theory of Dispersion Models . Chap-man &amp; Hall/CRC Monographs on Statistics &amp; Ap-plied Probability, 1997.
 Lu, Zhiyun, Yang, Zhirong, and Oja, Erkki. Selecting  X  -divergence for nonnegative matrix factorization by score matching. In Proceedings of 22nd Inter-national Conference on Artificial Neural Networks (ICANN 2012) , volume 7553 of Lecture Notes in
Computer Science , pp. 419 X 426, Lausanne, Switzer-land, 2012. Springer.
 McCulloch, C. E. and Nelder, J. A. Generalized Linear Models . Chapman and Hall, 2nd edition, 1989. Smaragdis, P. Non-negative matrix factor deconvo-lution; extraction of multiple sound sources from monophonic inputs. In ICA , pp. 494 X 499, 2004. Smaragdis, P. and Brown, J. C. Non-negative matrix factorization for polyphonic music transcription. In
IEEE Workshop on Applications of Signal Process-ing to Audio and Acoustics , pp. 177 X 180, 2003. Tweedie, M. C. An index which distinguishes between some important exponential families. Statistics: ap-plications and new directions, Indian Statist. Inst., Calcutta , pp. 579 X 604, 1984.
 Y X lmaz, Y. K. and Cemgil, A. T. Alpha/beta di-vergences and tweedie models. arXiv:1209.4280 v1 , 2012.
 Y X lmaz, Y. K., Cemgil, A. T., and S  X im  X sekli, U. Gener-alised coupled tensor factorisation. In NIPS , 2011. Zhang, Yanwei. Likelihood-based and bayesian meth-ods for tweedie compound poisson linear mixed
