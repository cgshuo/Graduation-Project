 they are not analyzed in real-time. chain of words of the same speaker. Such granular analysis enables detecting change in sentiment [21], or multiple emotional states during a call. However, making mean-adhere to a coherent syntactic structure. 
We suggest five types of factors by processing information from multiple layers of forms X  X hich are related to the structure and prosody of a conversation X  X ome factor types exploit the textual content, while others overlook it. ences from our work. while it is clear from the original text that the dependency is delete:files . 
The work of [19] designed a supervised learning scheme, to detect customer satis-domain specific features (for example, using emoticons and hashtags), or not conver-the current study, we do not have access to acoustic data. ed. We then employ a supervised machine learning approach, with the goal of discri-from imbalanced datasets, and high dimensionality of the vector space. 3.1 Factors look the textual content. Following, the five types of factors are described. features were computed:  X  sues.  X  for the agent and customer respectively.  X  emotional state of the speaker.  X  divided by the agent's total speaking time.  X  DUR is the duration of the utterance. overlooking the actual content. The following three features were computed:  X   X   X  NU is the number of utterances in the call. mon and informative terms. The following lexical features were computed:  X  ance, respectively. We calculate this feature by creating a tri-gram representation names. Since customers do not typically call to praise a product, it can imply re-lated problems.  X 
NWC is the number of words in the call. An emotional call may be longer, in terms  X 
NWC A , NWC C are the accumulated number of words in the agent's utterances in the  X   X   X 
AVGWR is the average word ratio, i.e., the number of words spoken by the custom- X   X 
NR is the number of repetitions, i.e., how many words repeat more than once in the  X  Bag-of-words (BOW): Bag-of-words are models where single words or word stems unigram model which was more effective than incorporating bi-grams or three-grams. uni-gram. based on their origin:  X  detected key terms: their proximity to other key terms in the utterance, the num-absence of key terms, analyzing the length and frequency of text sequences with-out key terms.  X  age number of context terms per key term, the number of context terms shared by several key terms etc. the attempt to identify similar information as explained before. 3.2 Learning Algorithms We explored machine learning algorithms while placing emphasis on facing our spe-which is common in text processing and language-model bases techniques. The learn-our problem along with the motivation for their use.
 ing to classifier D j . The aggregated confidence for class w determined by the following averaging equation: 
Generally, ensemble techniques were found more resistant than base-algorithms in linear transformation, which increases the diversity among ensemble classifiers. dimensionality, which can lead to overfitting. It is well known that SVMs work well be resistant to noisy text classification tasks [26]. this case. The BOW model was used as a baseline, since it is considered as a strong baseline for sentiment analysis [12, 18]. 4.1 Datasets The speech recognition system is able to segment utterance boundaries and to distin-call center of telecom companies and correspond with users making queries regarding Tele1 was the larger dataset comprised of 377 calls and 20,338 utterances compared Tele2 comprise 15.39% of the dataset, while 20.31% of the Tele1 dataset. 4.2 Gold Standard We developed a designated tagging tool to enable the annotators to view a complete cally identify the speaker X  X amely, whether an utterance is spoken by the customer, disagreement between speakers is observed. Since the agreement rate was high (near-utterances that both judges agreed upon their sentiment. 4.3 Evaluation Measures curve measure, since it is not overwhelmed by majority instances of unbalanced data-to our experiments. 4.4 Results (which has an AUC of 0.5). The best performance was achieved when using all fac-significance 0.1). All othe r thod which is hardly indic "all factors" system includ e board  X ,  X  okay well then I s ances classified as negativ e venty fifty did out until to m September the  X ,  X  Im im ve ry 4.5 Cross Corpus Exp e In the previous evaluation problem. In these experim e used as a test set. The cr o expensive to annotate eac h evaluates to what extent e a out similarly to the five f o taken from the other corpu s
Table 3 summarizes th e indicates that Tele1 was u s uses all factors achieves th e at all. Fig. 3 presents the i n textual factor type and usin g From Fig. 2 and Fig. 3 it c a The benefits of this type a r tual factors do not assign t higher values, based on th e ers context in the vicinity o intuitive example is negat i Composing bi-grams and t more likely to contain noi which overlook the textual minor extent. In Table 2, w 0.55 -0.6, which is slightly tural information carries a c we generated a simple set o sation could be a promisin g mation overlooks the actu a they achieve comparable r e This important finding ma y and may contribute greatl y carries meaningful signal, a dently it is far less effectiv explained by the fact that s p therefore lexicons and kno "soothingly" appears in th e conversation. Apart from t h due to the irregularity trai argument, however may al s poor voice signals, and rep e
The combined model a c with the contextual model, ficance. We found the Rot a sionality and due to its en s Table 2 demonstrates that t learning is further supported by the BOW model. Which it uses SVM, it is less effec-Table 2 and Table 3), we can see that the RF model maintains robustness which indi-over-fitted. a specific utterance. 
