 Many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances. How-ever, what if there are multiple labeling sources ( X  X racles  X  or  X  X xperts X ) with different but unknown reliabilities? Wit h the recent advent of inexpensive and scalable online annota -tion tools, such as Amazon X  X  Mechanical Turk, the labeling process has become more vulnerable to noise -and with-out prior knowledge of the accuracy of each individual la-beler. This paper addresses exactly such a challenge: how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling effort. More specifically, we present IEThresh (Interval Estimate Threshold) as a strat-egy to intelligently select the expert(s) with the highest e s-timated labeling accuracy. IEThresh estimates a confidence interval for the reliability of each expert and filters out th e one(s) whose estimated upper-bound confidence interval is below a threshold -which jointly optimizes expected accu-racy (mean) and need to better estimate the expert X  X  accu-racy (variance). Our framework is flexible enough to work with a wide range of different noise levels and outperforms baselines such as asking all available experts and random expert selection. In particular, IEThresh achieves a given level of accuracy with less than half the queries issued by al l-experts labeling and less than a third the queries required b y random expert selection on datasets such as the UCI mush-room one. The results show that our method naturally bal-ances exploration and exploitation as it gains knowledge of which experts to rely upon, and selects them with increasing frequency.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation ; H.2.8 [ Database Applica-tions ]: Data mining Algorithms, Design, Experimentation, Performance, Mea-surement noisy labelers, estimation, active learning, labeler sele ction
In many data mining applications, obtaining labels for the training data is an error-prone process. Brodley [2] notes that class noise can occur for several reasons including sub -jectivity, data-dependent error, inappropriate feature i nfor-mation used for labeling, etc. Inductive learning algorith ms aim at maximizing the classification accuracy based on a set of training instances. The maximum accuracy achieved de-pends strongly on the quality of the labeling process. In the case of multiple labelers (experts), class noise is typical ly a common problem since the quality of annotations may not be controlled. For instance, multiple experts might not agr ee on the medical diagnosis of a clinical case, or on the primary topic of a document, even if we hypothesize the existence of a ground (or consensus) truth. In remote-sensing applicatio ns, image analysis is often a manual process with subjective la-beling by multiple labelers. In on-line labeling marketpla ces such as the Mechanical Turk (http://www.mturk.com), we do not know a priori the reliability of individual labelers, though a distribution is expected.

Sheng et al. [15] addresses multiple noisy labelers and proposes repeated labeling (obtaining multiple labels fro m multiple labelers for some or all data points) to improve la-bel and model quality. Their work mainly focuses on tasks where it is relatively cheap to obtain labels compared to the cost of data gathering and preprocessing. They have shown that repeated labeling can be preferable to single la-beling in the presence of label noise, especially when the cost of data preprocessing is non-negligible. Despite thei r insightful analysis, the practical value of repeated label ing varies greatly with different cost models and with different labeler accuracies. For instance, they make the strong and often unrealistic simplifying assumption that all labeler s are identical, with the same probability of making a labeling mistake. As briefly noted by the authors [15], unknown dif-fering qualities require more sophisticated strategies to deal with noisy labelers in general. In real life, we face the prob -lem of inferring individual labeler quality in the absence o f the gold standard labels. Estimating each labeler X  X  qualit y can prove crucial to improve the overall labeling and help mitigate considerably the effect of labeling noise.
In this paper, we address directly the challenge of active labeling with multiple noisy labelers, each of which has un-known labeling accuracy. The goal is to estimate each la-beler X  X  accuracy and use the estimates to select the highest quality labeler(s) for additional label acquisition. Labe ler accuracy estimation requires a degree of exploration as wel l as exploitation in the form of single-labeling with the best labeler(s). Hence, a sophisticated learner should acquire la-beler and label knowledge through repeated trials, balanci ng the exploration vs. exploitation tradeoff, by first favoring the former and moving gradually to increasing exploitation . We report on the first multi-labeler active learner with such properties. We use the words  X  X racle X ,  X  X xpert X  and externa l  X  X abeler X  interchangably.

We adopt the Interval Estimation (IE) learning [5, 10] as a building block for our framework. IE attempts to esti-mate the confidence interval on the expected response of an action and then selects the action with the highest upper confidence interval. In our problem, taking an action cor-responds to selecting an oracle to query for labeling and an instance to label. We use the responses of the oracles them-selves to evaluate the performance of each oracle. Therefor e, inclusion of inferior oracles can dramatically slow conver -gence. To overcome this issue, we propose a thesholding mechanism (IEThresh) -to filter out inferior oracles early in the process. This helps to narrow down the set of po-tentially good oracles and improves the estimation accurac y with many fewer exploratory trials.

In order to apply IEThresh to our problem, we need to define an appropriate reward function. The reward of each labeler is directly related to whether the labeler makes a la -beling mistake or not. Unfortunately, an exact calculation of the reward function is impossible since the true label is unknown. A natural way to estimate the true label is to take the majority vote among the predicted labels from multiple labelers. In this paper, we assume an individual labeler ac-curacy is better than random guess, i.e. &gt; 0 . 5 in the binary case. Under this assumption, it is unlikely that all labeler s make a labeling mistake at the same time; hence, the ma-jority label is a close approximation to the true label. The method is robust to occasional errors by the majority vote method as demonstrated on several benchmark datasets. We report results on six (mostly UCI) datasets to which we add random noise to simulate a set of labelers, and we also re-port results on two datasets annotated with real labelers [19]. For the simulated-error cases, we varied the labeler a c-curacies to show that our method IEThresh can detect the most accurate ones even among a uniform or skewed mix of good and bad labelers. We compared our method IEThresh to two baselines: asking all labelers (as in [15]) and select -ing a random labeler for each instance. Section 4 details our thorough comparison.

The rest of the paper is organised as follows: The next section summarizes the relevant work in the literature. Sec -tion 3 describes the Interval Estimate Threshold method. The experimental evaluation is detailed in Section 4. Fi-nally, we offer our conclusions and potential future directi ons in Section 5.
Traditional active learning focuses on selecting data to be labeled to improve the model performance. Thus far, tradi-tional active learning assumed that there is a single oracle (expert) that answers every query with the correct label; hence, the label acquisition is a noise-free process. These assumptions lack realism as also noted in our earlier work [3]. We addressed fallible experts together with reluctant and variable-cost ones in a setting called proactive learni ng as an alternative to traditional active learning. That work assumes two experts with differing costs: e.g. one is the perfectly reliable expert whereas the other is a noisy exper t whose reliability is conditioned on the instances. They fur -ther assume that the fallible expert provides a confidence score together with the label. The confidence score is used to assess the quality of the expert. The instance-condition al reliability of the fallible expert is estimated via an explo -ration phase where the most representative instances are queried and the confidence is propagated through the neigh-bors. The paper provides a decision-theoretic framework to make the optimal instance-expert selection. This paper ad-dresses limitations in that work, including generalizing f rom two to multiple experts, eliminating the need that one ex-pert be a perfect oracle, and eliminating the need for explic it and reliable self-reporting of labeling confidence levels.
Furthermore, considering the cost factor in data mining applications has recently become increasingly popular. Ut il-ity based data mining [12] introduces a general economic setting to formulate a strategy of maximum expected utility data acquisition. Budgeted learning, active feature acqui si-tion, etc. address the notion of costly data acquisition. In budget-constrained learning, the total cost of data elicit a-tion is bounded, and label queries may have non-uniform cost. The goal of the learner is to produce the most ac-curate model under the budget constraints. Active feature acquisition (AFA) considers data with missing feature val-ues. AFA tries to optimize the improvement in model accu-racy at minimum cost via selective feature acquisition [9, 1 4] Cost-sensitive learning, on the other hand, generally deal s with the cost of misclassification but not the cost of label-ing. This line of work is complementary to the methods and results presented in this paper, and interesting future wor k would entail a combined model.

Repeated labeling on the same data point has been con-sidered by [15, 17, 18] because the labels may not be reliable , without estimating reliability of specific points or region s of the instance space or labelers. The focus of [17, 18] is to lea rn from probabilistic labels in the absence of ground truth in a n image processing application. In their task, the domain ex-perts examine an image and provide subjective class labels. They provide a probabilistic framework to model the subjec-tive labeling process and use EM to estimate the model pa-rameters as maximizers of a likelihood function [18]. Sheng et al. [15] relies on an active learning framework that uses repeated labeling and provides conditions where repeated labeling can be effective for improving data quality. Their results point out that repeated labeling can give additiona l benefit especially when the labeling quality is low. However , their work assumes the same level of accuracy for each la-beler. The method in this paper goes beyond their results by relaxing the assumptions of identical labeling error rat es among experts and a priori knowledge of said error rates. Moreover, our method is adaptive as it transitions grad-ually from exploration-heavy to exploitation-heavy phase s, as knowledge of individual labeler accuracy accrues.
In this section, we describe our multi-expert active sam-pling method, which we call IEThresh. It builds upon In-terval Estimation (IE) learning [5, 10] which is useful for addressing the exploration vs. exploitation tradeoff. IE ha s been used extensively in reinforcement learning for action se-lection and in stochastic optimization problems. We first ex -plain IE and then discuss how we extend it to learn the best oracle(s) to query, favoring exploration in the early phase s and exploitation (least error-prone oracle selection) wit h in-creasing frequency.
The goal of IE is to find the action a  X  yielding the highest expected reward with as few samples as possible; i.e. a  X  arg max and must be estimated from observed samples. Before each selection, IE estimates a standard upper confidence interva l for the mean reward of each action using the sample mean and standard deviation of rewards received so far using that action: where m ( a ) is the sample mean for a , s ( a ) is the sample standard deviation for a , n is the number of samples ob-dent X  X  t-distribution with n  X  1 degrees of freedom at the  X / 2 confidence level.

IE then selects the action with the highest upper confi-dence interval. The reason is that such an action has a high expected reward and/or a large amount of uncertainty in the reward. If an action has large uncertainty, it indicates tha t the action has not been taken with sufficient frequency to yield reliable estimates. Selecting this action performs e x-ploration which will increase IE X  X  confidence in its estimat e and has the potential of identifying a high reward action. Se -lecting an action with a high expected reward performs ex-ploitation. Initially, the intervals are large due to the un cer-tainty of the reward estimates and action choices tend to be explorative. Over time, the intervals shrink and the choice s become more exploitative. IE automatically trades off these two.  X  is a parameter that weights exploration more strongly when it is small and exploitation more strongly when it is large.  X  = 0 . 05 is a common reasonable choice.
The IE algorithm described above can be adapted to work with multiple noisy oracles. Taking an action corresponds to selecting an oracle to ask for a label in our active learnin g framework, assuming we have already selected an instance to label. Our framework is flexible to work with any instance selection strategy and any supervised learning method. For simplicity, we select the instance to label via uncertainty sampling [7] and adopt a logistic regression classifier to ob -tain posterior class probabilities P ( y | x ). The most uncer-tain instance is selected for labeling: One also needs to estimate a reward function for each ora-cle based on the labels received. The reward of each oracle should be related to the true label for the queried instance, which is not known. Hence, we need a mechanism to esti-mate the true label. We use a majority vote among multiple, possibly noisy labelers to infer the true label  X  which will b e correct often, but not always. We propose the following re-ward function  X  r : K  X  X  0 , 1 } as a mapping from the set of labelers K to a binary value. It is 1 if the labeler agrees with the majority label  X  y , and 0 otherwise. This reward estimate requires sampling some or all oracles to take the majority vote. Its accuracy depends on how well the majority vote represents the true label. When the individual labeler quality is high, the majority vote is a cl ose estimate of the true label since it is unlikely that a majorit y of the oracles make a mistake on the same instance. We propose to adopt a threshold on the upper interval to 1) filter out the less reliable oracles from the majority voting , 2) reduce the labeling cost and 3) compute the reliability estimates more efficiently. Given k oracles, we select each oracle a that has an upper bound UI ( a ) (Equation 1) larger than some fraction of the maximum bound at time t : where S t is the set of selected oracles to be queried for label-ing. 0 &lt;  X  &lt; 1 is a parameter tuned on a separate dataset that is not used in the experiments. 1 We note that this may result in choosing different number of oracles each time, bi-asing towards the more reliable ones as the reward estimates become more accurate. We smooth the confidence interval estimates by initially giving each oracle a reward of 1 and 0. At the first iteration, they have the same upper bound and all oracles are selected. As the bounds tighten, under-performing oracles are filtered out and the reliable ones are selected for labeling. The upper bound can be high because there is either little information about the oracle (high va ri-ance) or the entire interval is high and the oracle is good (high mean). It is possible that a previously filtered-out oracle will be selected again if the upper bounds of the re-maining oracles lower sufficiently. We give below an outline of how IEThresh works: 1. Initialize samples for each oracle with rewards 1 and 0 2. Fit a logistic regression classifier to training data T 3. Pick the most uncertain unlabeled instance x  X  for la-4. Compute the upper confidence interval for each oracle 5. Choose all oracles S t within  X  of the maximum upper 6. Compute the majority vote  X  y of the selected oracles S 7. Update training data T = T  X  X  x  X  ,  X  y } 8. Add calculated rewards (Eqn. 3) to the samples for S t 9. Repeat 2-8
We note that a more sophisticated tuning could further improve the results, but our experimental results indicate that a reasonable threshold works quite effectively. Our empirical evaluation indicates that IEThresh is very effective in filtering out the less reliable oracles early in the process and continues to sample the more reliable ones. Next, we describe our experimental results in detail.
We conducted a thorough analysis on eight benchmark datasets from [1, 13, 19]. Six of these datasets are classi-fication problems with characteristics given in Table 1. If the dataset was not originally binary, we converted it using random partitioning into two classes as described in [13]. We partition each of these datasets into 70% / 30% train/test splits. For each dataset, the initial labeled set includes o ne true positive and one true negative instance so that each method has the same initial performance before active learn -ing. The rest of the training set is used as the unlabeled pool . We compared IEThresh with two baselines: asking all the oracles (repeated labeling) as presented in [15] (we refer i t as Repeated ), and asking a randomly chosen oracle (which is referred as Random ). Each time an unlabeled instance is selected by the active learner, a label is generated accordi ng to the true accuracy q of the selected oracles(s), i.e. the true label y  X  X  1 , 0 } is assigned with probability q and 1  X  y is assigned with probability 1  X  q . If more than one oracle is chosen, then the majority vote is assigned as the label for that instance (ties are broken randomly). We set the total number of oracles to k = 10. After labeling, the instance is added to the training set and the classifier is re-trained on the enlarged set. The classifier is tested on the separate tes t set every time a new instance is added, and the classification error is reported. The results are averaged over 100 runs.
The remaining two datasets in our experiments are from the natural language understanding tasks introduced in [19 ]. This collection was created using Amazon X  X  Mechanical Turk (AMT) for data annotation. AMT is an online tool where remote workers are paid to complete small labeling and an-notation tasks. We selected two binary tasks from this col-lection: the textual entailment recognition (RTE) and tem-poral event recognition (TEMP) tasks. In the former task, the annotator is presented with two sentences for each ques-tion. He needs to decide whether the second sentence can be inferred from the first. The original dataset contains 800 se n-tence pairs with a total of 165 annotators who contributed to the labeling effort. The latter task involves recognizing th e temporal relation in verb-event pairs. The annotator decid es whether the event described by the first verb occurs before or after the second. The original dataset contains 462 pairs with a total of 76 annotators. For both datasets, the quality (accuracy) of annotators are measured by comparing their annotations with the gold standard labels. Unfortunately, most of the annotators completed only a handful of tasks. Therefore, we selected a subset of these annotators for each dataset such that each annotator has completed at least 100 tasks. They have differing accuracies ranging from as low as 0.44 to over 0.9. We note that this violates our assumption of better-than-random labelers. This is a real-life datase t that is not generated based on our assumptions; hence, it is useful to test the robustness of our approach to these. Due to the lack of a large amount of data, we selected only the instances for which all annotators provided an answer, to en -able our method to select one, several or all the annotators, Figure 3: Average classification error vs. total num-ber of oracle queries on ringnorm dataset. For the top figure, accuracy  X  [ . 8 , 1] for k good = 5 labelers and ac-curacy  X  [ . 5 , . 7] for the remaining k bad = 5 labelers. k good decreases down to 1 and k bad increases up to 9 from top to bottom. by IEThresh during first 150 iterations.
 Table 1: Properties of six datasets used in the ex-periments. All are binary classification tasks with varying sizes.
 and to have consistent baselines. The annotator accuracies and the size of each dataset is reported in Table 2. We compared our method IEThresh against Repeated and Random baselines on these two datasets. In contrast to the UCI data experiment, there is no training of classifiers for this experiment. Instead, the test set predictions are made directly by AMT labelers. Hence, we randomly selected 50 instances from each dataset to be used by IEThresh to infer estimates for the annotator accuracies. The remaining in-stances are held out as the test set. The annotator with the best estimated accuracy is evaluated on the test set. The total number of queries are then calculated as a sum of the number of queries issued during inference and the number of queries issued to the chosen annotator during testing. Re-peated and Random baselines do not need an inference phase since they do not change their annotator selection mecha-nism via learning. Hence, they are directly evaluated on the test set. The total number of queries is assigned comparably for IEThresh and Repeated; however, it is equal to the num-ber of test instances for the Random baseline since it querie s a single labeler for each instance; thus, there can only be as many queries as the number of test instances.
Figure 1 compares three methods on six datasets with simulated oracles. The true accuracy of each oracle in Fig-ure 1 is drawn uniformly at random from within the range Table 2: The size and the annotator accuracies for each AMT dataset.
 Table 3: Performance Comparison on RTE data.
 The last column indicates the total number of queries issued to labelers by each method. IEThresh performs accurately with comparable labeling effort to Repeated.
 [ . 5 , 1]. The figure reports the average classification error with respect to the total number of oracle queries issued by each method. IEThresh is the best performer in all six datasets. In ringnorm and spambase datasets, IEThresh ini-tially performs slightly worse than the other methods, indi -cating that oracle reliability requires more sampling in th ese two datasets. But, after the estimates are settled (which happens in  X  200 queries), it outperforms the others, with especially large margins in spambase dataset. The results reported are statistically significant based on a two-sided paired t-test, where each pair of points on the averaged re-sults is compared.
 We also analyzed the effect of filtering less reliable oracles . An ideal filtering mechanism excludes the less accurate or-acles early in the process and samples more from the more accurate ones. In Figure 2, we report the number of times each oracle is queried on image and phoneme datasets. The x-axis shows the true accuracy of each oracle. We consider the first 150 iterations of IEThresh and count the number of times each oracle is selected. Each color corresponds to a different time frame; i.e. blue, green and red correspond to Figure 4: Average classification error vs. total num-ber of oracle queries on UCI mushroom dataset. For the top figure, accuracy  X  [ . 8 , 1] for k good = 5 labelers and accuracy  X  [ . 5 , . 7] for the remaining k bad = 5 la-belers. k good decreases down to 1 and k bad increases up to 9 from top to bottom.
 Table 4: Performance Comparison on TEMP data.
 The last column indicates the total number of queries issued to labelers by each method. Repeated needs 840 queries in total to reach 0 . 95 accuracy to be comparable with IEThresh.
 0 th  X  10 th , 10 th  X  50 th and 50 th  X  150 th iterations, respec-tively. At first, each oracle is chosen almost equally since t he algorithm explores every possibility to improve its estima tes. Gradually, we see that less accurate oracles are sampled wit h decreasing frequency, as reliance shifts to the more accura te ones. The method continues to update its oracle estimates until the estimates converge and become stable.

We further varied distribution of oracle accuracies to chal -lenge IEThresh. Figures 3 and 4 show the resulting perfor-mance of each method on ringnorm and mushroom datasets. The top figure on each graph indicates the case with 5 highly fallible oracles with accuracy level within [ . 5 , . 7], and 5 re-liable ones with accuracies within [ . 8 , 1] range. From the top figure to the bottom, the set of oracles becomes more skewed towards the fallible oracles. The results point out that IEThresh generalizes to work with a wide range of or-acle reliability distributions. Even in the challenging ca se where there are only one or two reliable oracles, the al-gorithm is able to detect the good ones. Figures 5 and 6 report a similar set of results from a different perspective. The graphs show the total number of queries required to achieve a target classification accuracy. IEThresh require s the least number of queries for a given accuracy level for most cases. Especially when the accuracy targets are high, giving time for IEThresh to stabilize its oracle accuracy es -timates, it can improve classification accuracy without the intensive labeling effort required by the baselines. To test the effectiveness of using upper confidence interval in IE learning, we compare with a variant of IEThresh, which we call IEMid, in Figure 6. IEMid considers only the sample mean reward ( m ( a ) in Eqn. 1) of each oracle and selects the oracles whose average reward is larger than a threshold. The results indicate that considering the variance in rewar d estimates emphasizes better exploration, which is crucial es-pecially when there are only a few good labelers available.
Lastly, we report the results on the RTE and TEMP datasets that have real annotations from multiple less-tha n-perfect labelers. Table 3 reports the accuracy of each metho d on the test set for RTE data with the corresponding num-ber of oracle queries issued. The accuracy of IEThresh is the same as the accuracy of the single best labeler in this datase t (See Table 2), indicating that IEThresh managed to detect the best labeler during the inference phase. The Repeated and Random baselines perform poorly in this dataset due to the majority of highly unreliable labelers. Table 4 reports the results on the test set for TEMP data. IEThresh is the best performer in this dataset with a moderate labeling ef-fort. The Repeated labeling baseline needs 840 queries in total to reach 0 . 95 accuracy. Random baseline stops at 140 queries since this is the size of the test set and it queries a single labeler per instance.
In this paper, we explored an algorithm for estimating the accuracy of multiple noisy labelers and selecting the best one(s) for active learning. Specifically, we proposed IEThresh as an effective solution that naturally incorporat es the exploration vs. exploitation tradeoff. Filtering out th e less reliable labelers as early as possible boosts performa nce. Our experimental evaluation indicates that estimating ora cle accuracy, and utilizing these estimates in the active learn ing process is more effective than the naive counterparts such as asking all labelers or asking a random labeler, which were reported in the recent literature. Even under challenging conditions where the number of reliable labelers is low or some oracles are worse than random (perhaps the AMT la-belers misunderstood the instructions), IEThresh is capab le of estimating the best labeler(s) through selective sampli ng and updating oracle accuracy estimates.

There are several directions for expanding the research re-ported here. One major direction is to track variable oracle performance over time since it could change depending on numerous reasons, e.g. oracle fatigue. For some problems, the labeler quality might go down with extensive labeling due to exhaustion and in some others it might increase with learning. Hence, it is crucial to design methods that goes be -yond consistent labeler quality. Another major direction i s to condition the probability of making a labeling mistake on the data instance, or at least the region of the instance spac e which contains the instance. Then, it is crucial to estimate this probability for a representative subset of the input sp ace and generalize to the entire space. Another direction is to relax the assumption that the noise generation is uncorre-lated. It is possible that the labelers make correlated erro rs as noted by [15]. This is a more challenging task since the correlation parameters need to be estimated together with the noise probabilities. Lastly, we assumed that the cost of labeling is the same for each labeler in this paper. However, it is likely that more accurate labelers cost more than the le ss accurate ones. Furthermore, the cost of each instance might differ according to the difficulty of labeling that instance. I n such cases a decision-theoretic utility model would be cen-tral. These are interesting and challenging problems that we began investigating under simpler scenarios and plan to investigate in this challenging setting with multiple orac les with a priori unknown labeling accuracies. [1] Blake and C. J. Merz. UCI repository of machine [2] C. E. Brodley and M. A. Friedl. Identifying and [3] P. Donmez and J. G. Carbonell. Proactive learning: [4] Y. Freund and R. E. Schapire. A decision-theoretic Figure 5: Total number of oracle queries required to reach a target accuracy is plotted on UCI spam-base dataset. For the top figure, accuracy  X  [ . 8 , 1] for k good = 5 labelers and accuracy  X  [ . 5 , . 7] for the re-maining k bad = 5 labelers. k good decreases down to 1 and k bad increases up to 9 from top to bottom. Figure 6: Total number of oracle queries required to reach a target accuracy is plotted on UCI im-age dataset. For the top figure, accuracy  X  [ . 8 , 1] for k good = 5 labelers and accuracy  X  [ . 5 , . 7] for the re-maining k bad = 5 labelers. k good decreases down to 1 and k bad increases up to 9 from top to bottom. [5] L. P. Kaelbling. Learning in Embedded Systems . PhD [6] A. Kappor and R. Greiner. Learning and classifying [7] D. Lewis and W. Gale. A sequential algorithm for [8] G. Lugosi. Learning with an unreliable teacher. [9] P. Melville, M. Saar-Tsechansky, F. Provost, and [10] A. Moore and J. Schneider. Memory-based stochastic [11] C. T. Morrison and P. R. Cohen. Noisy information [12] F. Provost. Toward economic machine learning and [13] G. R  X  atsch, T. Onoda, and K. R. Muller. Soft margins [14] M. Saar-Tsechansky, P. Melville, and F. Provost. [15] V. Sheng, F. Provost, and P. G. Ipeirotis. Get another [16] B. W. Silverman. Some asymptotic properties of the [17] P. Smyth, U. Fayyad, M. Burl, P. Perona, and [18] P. Smyth, U. Fayyad, M. Burl, P. Perona, and [19] R. Snow, O X  X onnor, D. Jurafsky, and A. Ng. Cheap [20] Z. Zheng and B. Padmanabhan. Selectively acquiring
