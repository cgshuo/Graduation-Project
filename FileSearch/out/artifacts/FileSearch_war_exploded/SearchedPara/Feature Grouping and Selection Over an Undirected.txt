 High-dimensional regression/classification continues to be an important and challenging problem, especially when fea-tures are highly correlated. Feature selection, combined with additional structure information on the features has been considered to be promising in promoting regression/classifi-cation performance. Graph-guided fused lasso (GFlasso) has recently been proposed to facilitate feature selection and graph structure exploitation, when features exhibit certain graph structures. However, the formulation in GFlasso relies on pairwise sample correlations to perform feature grouping, which could introduce additional estimation bias. In this paper, we propose three new feature grouping and selection methods to resolve this issue. The first method employs a convex function to penalize the pairwise l  X  norm of con-nected regression/classification coefficients, achieving simul-taneous feature grouping and selection. The second method improves the first one by utilizing a non-convex function to reduce the estimation bias. The third one is the extension of the second method using a truncated l 1 regularization to further reduce the estimation bias. The proposed methods combine feature grouping and feature selection to enhance estimation accuracy. We employ the alternating direction method of multipliers (ADMM) and difference of convex functions (DC) programming to solve the proposed formu-lations. Our experimental results on synthetic data and two real datasets demonstrate the effectiveness of the proposed methods.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Feature grouping, feature selection, undirected graph, l 1 ularization, regression, classification
High-dimensional regression/classification is challenging due to the curse of dimensionality . Lasso [18] and its var-ious extensions, which can simultaneously perform feature selection and regression/classification, have received increas-ing attention in this situation. However, in the presence of highly correlated features lasso tends to only select one of those features resulting in suboptimal performance [27]. Several methods have been proposed to address this issue in the literature. Shen and Ye [15] introduce an adaptive model selection procedure that corrects the estimation bias through a data-driven penalty based on generalized degrees of freedom. The Elastic Net [27] uses an additional l 2 reg-ularizer to encourage highly correlated features to stay to-gether. However, these methods do not incorporate prior knowledge into the regression/classification process, which is critical in many applications. As an example, many bi-ological studies have suggested that genes tend to work in groups according to their biological functions, and there are some regulatory relationships between genes [10]. This bio-logical knowledge can be represented as a graph, where the nodes represent the genes, and the edges imply the regu-latory relationships between genes. Therefore, we want to study how estimation accuracy can be improved using de-pendency information encoded as a graph.

Given feature grouping information, the group lasso [1, 7, 22, 11] yields a solution with grouped sparsity using l 1 penalty. The orignal group lasso does not consider the over-laps between groups. Zhao et al. [24] extend the group lasso to the case of overlapping groups. Jacob et al. [7] intro-duce a new penalty function leading to a grouped sparse solution with overlapping groups. Yuan et al. [21] propose an efficient method to solve the overlapping group lasso. Other extensions of group lasso with tree structured regu-larization include [11, 8]. Prior works have demonstrated the benefit of using feature grouping information for high-dimensional regression/classification. However, these meth-ods need the feature groups to be pre-specified. In other words, they only utilize the grouping information to obtain solutions with grouped sparsity, but lack the capability of identifying groups.

There are also a number of existing methods for feature grouping. Fused lasso [19] introduces an l 1 regularization method for estimating subgroups in a certain serial order, but pre-ordering features is required before using fused lasso. A study about parameter estimation of the fused lasso can be found in [12]; Shen et al. [13] propose a non-convex method to select all possible homogenous subgroups, but it fails to obtain sparse solutions. OSCAR [2] employs an l 1 regular-izer and a pairwise l  X  regularizer to perform feature selec-tion and automatic feature grouping. Li and Li [10] suggest a grouping penalty using a Laplacian matrix to force the co-efficients to be similar, which can be considered as a graph version of the Elastic Net. When the Laplacian matrix is an identity matrix, Laplacian lasso [10, 5] is identical to the Elastic Net. GFlasso employs an l 1 regularization over a graph, which penalizes the difference |  X  i  X  sign( r ij )  X  encourage the coefficients  X  i , X  j for features i, j connected by an edge in the graph to be similar when r ij &gt; 0, but dissimilar when r ij &lt; 0, where r ij is the sample correlation between two features [9]. Although these grouping penalties can improve the performance, they would introduce addi-tional estimation bias due to strict convexity of the penalties or due to possible graph misspecification. For example, ad-ditional bias may occur when the signs of coefficients for two features connected by an edge in the graph are different in Laplacian lasso [10, 5], or when the sign of r ij is inaccurate in GFlasso [9].

In this paper, we focus on simultaneous estimation of grouping and sparseness structures over a given undirected graph. Features tend to be grouped when they are con-nected by an edge in a graph. When features are connected by an edge in a graph, the absolute values of the model coefficients for these two features should be similar or iden-tical. We propose one convex and two non-convex penalties to encourage both sparsity and equality of absolute values of coefficients for connected features. The convex penalty includes a pairwise l  X  regularizer over a graph. The first non-convex penalty improves the convex penalty by penal-izing the difference of absolute values of coefficients for con-nected features. The other one is the extension of the first non-convex penalty using a truncated l 1 regularization to further reduce the estimation bias. These penalties are de-signed to resolve the aforementioned issues of Laplacian lasso and GFlasso. The non-convex penalties shrink only small differences in absolute values so that estimation bias can be reduced; several recent works analyze their theoretical prop-erties [26, 14]. Through ADMM and DC programming, we develop computational methods to solve the proposed for-mulations. The proposed methods can combine the benefit of feature selection and that of feature grouping to improve regression/classification performance. Due to the equality of absolute values of coefficients, the model complexity of the learned model can be reduced. We have performed experi-ments on synthetic data and two real datasets. The results demonstrate the effectiveness of the proposed methods.
The rest of the paper is organized as follows. We intro-duce the proposed convex method in Section 2, and the two proposed non-convex methods in Section 3. Experimental results are given in Section 4. We conclude the paper in Section 5.
Consider a linear model in which response y i depends on avectorof p features: where  X   X  R p is a vector of coefficients, X  X  R n  X  p is the data matrix, and  X  is random noise. Given an undirected graph, we try to build a prediction model (regression or classification) incorporating the graph structure information to estimate the nonzero coefficients of  X  and to identify the feature groups when the number of features p is larger than the sample size n .Let( N,E ) be the given undirected graph, where N = { 1 , 2 ,...,p } is a set of nodes, and E is the set of edges. Node i corresponds to feature x i .Ifnodes i and j are connected by an edge in E , then features x i and x j tend to be grouped. The formulation of graph OSCAR (GOSCAR) is given by min where  X  1 , X  2 are regularization parameters. We use a pair-wise l  X  regularizer to encourage the coefficients to be equal [2], but we only put grouping constraints over the nodes con-nected over the given graph. The l 1 regularizer encourages sparseness. The pairwise l  X  regularizer puts more penalty on the larger coefficients. Note that max {|  X  i | , |  X  j decomposed as ( |  X  i +  X  j | + |  X  i  X   X  j | ) can be represented by where u , v are sparse vectors, each with only two non-zero entries u i = u j = 1 2 ,v i =  X  v j = 1 2 .ThusEq.(2)canbe rewritten in a matrix form as where T is a sparse matrix constructed from the edge set E . The proposed formulation is closely related to OSCAR [2]. The penalty of OSCAR is  X  1  X  1 +  X  2 i&lt;j max {|  X  i | The l 1 regularizer leads to a sparse solution, and the l ularizer encourages the coefficients to be equal. OSCAR can be efficiently solved by accelerated gradient methods, whose key projection can be solved by a simple iterative group merging algorithm [25]. However, OSCAR assumes each node is connected to all the other nodes, which is not suffi-cient for many applications. Note that OSCAR is a special case of GOSCAR when the graph is complete. GOSCAR, incorporating an arbitrary undirected graph, is much more challenging to solve.
We propose to solve GOSCAR using the alternating di-rection method of multipliers (ADMM) [3]. ADMM decom-poses a large global problem into a series of smaller local subproblems and coordinates the local solutions to identify the globally optimal solution. ADMM attempts to com-bine the benefits of dual decomposition and augmented La-grangian methods for constrained optimization [3]. The problem solved by ADMM takes the form of
ADMM uses a variant of the augmented Lagrangian method and reformulates the problem as follows: L ( x , z ,  X  )= f ( x )+ g ( z )+  X  T ( Ax + Bz  X  c )+  X  with  X  being the augmented Lagrangian multiplier, and  X  being the non-negative dual update step length. ADMM solves this problem by iteratively minimizing L  X  ( x , z , x , z ,and  X  . The update rule for ADMM is given by
Consider the unconstrained optimization problem in Eq. (3), which is equivalent to the following constrained optimization problem: where q , p are slack variables. Eq. (4) can then be solved by ADMM. The augmented Lagrangian is
L  X  (  X  , q , p ,  X  ,  X  )= 1 2 y  X  X  X  2 +  X  1 q 1 +  X  2 p 1 where  X  ,  X  are augmented Lagrangian multipliers.
Update  X  :Inthe( k + 1)-th iteration,  X  k +1 can be up-dated by minimizing L  X  with q , p ,  X  ,  X  fixed: The above optimization problem is quadratic. The optimal solution is given by  X  k +1 = F  X  1 b k ,where The computation of  X  k +1 involves solving a linear system, which is the most time-consuming part in the whole algo-rithm. To compute  X  k +1 efficiently, we compute the Cholesky factorization of F at the beginning of the algorithm: Note that F is a constant and positive definite matrix. Using the Cholesky factorization we only need to solve the follow-ing two linear systems at each iteration: Since R is an upper triangular matrix, solving these two linear systems is very efficient.

Update q : q k +1 can be obtained by solving which is equivalent to the following problem: Eq. (7) has a closed-form solution, known as soft-thresholding : where the soft-thresholding operator is defined as:
Update p : Similar to updating q , p k +1 can also be ob-tained by soft-thresholding :
Update  X  ,  X  : A summary of GOSCAR is shown in Algorithm 1.
 Algorithm 1: The GOSCAR algorithm Input : X , y ,E, X  1 , X  2 , X  Output :  X  Initialization: p 0  X  0 , q 0  X  0 ,  X  0  X  0 ,  X  0  X  0 ;
Compute the Cholesky factorization of F ; do
Until Convergence ; return  X  ;
In Algorithm 1, the Cholesky factorization only needs to be computed once, and each iteration involves solving one linear system and two soft-thresholding operations. The time complexity of the soft-thresholding operation in Eq. (8) is O ( p ). The other one in Eq. (9) involves a matrix-vector multiplication. Due to the sparsity of T , its time complex-ity is O ( n e ), where n e is the number of edges. Solving the linear system involves computing b k and solving Eq. (6), whose total time complexity is O ( p ( p + n )+ n e ). Thus the time complexity of each iteration is O ( p ( p + n )+ n e
The grouping penalty of GOSCAR overcomes the limita-tion of Laplacian lasso that the different signs of coefficients can introduce additional penalty. However, under the l  X  penalty on this pair may still be large due to the property of the max operator, resulting in the coefficient  X  i or  X  ing over penalized. The additional penalty would result in biased estimation, especially for large coefficient, as in the lasso case [18]. Another related grouping penalty is GFlasso, |  X  i  X  sign( r ij )  X  j | ,where r ij is the pairwise sample correla-tion. GFlasso relies on the pairwise sample correlation to decide whether  X  i and  X  j are enforced to be close or not. When the pairwise sample correlation wrongly estimates the sign between  X  i and  X  j , an additional penalty on  X  i and  X  would occur, introducing estimation bias. This motivates our non-convex grouping penalty, ||  X  i | X  X   X  j || ,thatshrinks only small differences in absolutes values. As a result, esti-mation bias is reduced as compared to these convex grouping penalties. The proposed non-convex methods perform well even when the graph is wrongly specified, unlike GFlasso. Note that the proposed non-convex grouping penalty does not assume the sign of an edge is given; it only relies on the graph structure.
The proposed non-convex formulation (ncFGS) solves the following optimization problem: min  X  f ( magnitudes of differences of coefficients ignoring their signs over the graph. Through the l 1 regularizer and grouping penalty, simultaneous feature grouping and selection are per-formed, where only large coefficients as well as pairwise dif-ferences are shrunk.

A computational method for the non-convex optimization in Eq. (11) is through DC programming. We will first give a brief review of DC programming.

A particular DC program on R p takes the form of with f 1 (  X  )and f 2 (  X  ) being convex on R p . Algorithms to solve DC programming based on the duality and local op-timality conditions have been introduced in [17]. Due to their local characteristic and the non-convexity of DC pro-gramming, these algorithms cannot guarantee the computed solution to be globally optimal. In general, these DC algo-rithms converge to a local solution, but some researchers observed that they converge quite often to a global one [16].
To apply DC programming to our problem we need to decompose the objective function into the difference of two convex functions. We propose to use: The above DC decomposition is based on the following iden-tity: ||  X  i | X  X   X  j || = |  X  i +  X  j | + |  X  i  X   X  j | X  ( that both f 1 (  X  )and f 2 (  X  ) are convex functions. minorization of f 2 (  X  ), where  X  ,  X  is the inner product. Then DC programming solves Eq. (11) by iteratively solving a sub-problem as follows: Since  X  k , X  X  2 (  X  k ) is constant, Eq. (12) can be rewritten as Let c k =  X  X  2 (  X  k ). Note that where d i is the degree of node i ,and I (  X  ) is the indicator function. Hence, the formulation in Eq. (13) is which is convex. Note that the only differences between the problems in Eq. (2) and Eq. (15) are the linear term ( c k ) T  X  and the second regularization parameter. Similar to GOSCAR, we can solve Eq. (15) using ADMM, which is equivalent to the following optimization problem: There is an additional linear term ( c k ) T  X  in updating compared to Algorithm 1. Hence, we can use Algorithm 1 to solve Eq. (15) with a small change in updating  X  : where s represents the iteration number in Algorithm 1. The key steps of ncFGS are shown in Algorithm 2.
 Algorithm 2: The ncFGS algorithm Input : X , y ,E, X  1 , X  2 , Output :  X 
Initialization:  X  0  X  0 ; while f (  X  k )  X  f (  X  k +1 ) &gt; do end return  X  ;
It is known that the bias of lasso is due to the looseness of convex relaxation of l 0 regularization. The truncated l regularizer, a non-convex regularizer close to the l 0 regular-izer, has been proposed to resolve the bias issue [23]. The truncated l 1 regularizer can recover the exact set of nonzero coefficients under a weaker condition, and has a smaller up-per error bound than lasso [23]. Therefore, we propose a truncated grouping penalty to further reduce the estima-tion bias. The proposed formulation based on the truncated grouping penalty is where and J  X  ( x )=min( x  X  , 1) is the truncated l 1 regularizer, a surrogate of the l 0 function;  X  is a non-negative tuning pa-rameter. Figure 1 shows the difference between l 0 norm, l Figure 1: Example for l 0 norm (left), l 1 norm (mid-dle), and J  X  ( | x | ) with  X  = 1 8 (right). norm and J  X  ( | x | ). When  X   X  0, J  X  ( | x | )isequivalenttothe l norm given by the number of nonzero entries of a vector. When  X   X | x | ,  X J  X  ( | x | )isequivalenttothe l 1 norm of x . Note that J  X  ( ||  X  i | X  X   X  j || ) can be decomposed as and a DC decomposition of J  X  ( |  X  i | )is Hence, the DC decomposition of f T (  X  ) can be written as where f f th iteration. We have Then the subproblem of ncTFGS is which can be solved using Algorithm 1 as in ncFGS. The key steps of ncTFGS are summarized in Algorithm 3. Algorithm 3: The ncTFGS algorithm Input : X , y ,E, X  1 , X  2 , X , Output :  X 
Initialization:  X  0  X  0 ; while f (  X  k )  X  f (  X  k +1 ) &gt; do end return  X  ; ncTFGS is an extension of ncFGS. When  X   X |  X  i | ,  X  i ,ncT-FGS with regularization parameters  X  X  1 and  X  X  2 is identical to ncFGS (see Figure 3). ncFGS and ncTFGS have the same time complexity. The subproblems of ncFGS and ncTFGS are solved by Algorithm 1. In our experiments, we observed ncFGS and ncTFGS usually converge in less than 10 itera-tions.
We examine the performance of the proposed methods and compare them against lasso, GFlasso, and OSCAR on synthetic datasets and two real datasets: FDG-PET images 1 and Breast Cancer 2 . The experiments are performed on a PC with dual-core Intel 3.0GHz CPU and 4GB memory. The code is written in MATLAB. The algorithms and their associated penalties are: http://adni.loni.ucla.edu/ http://cbio.ensmp.fr/  X  jvert/publi/
To evaluate the efficiency of the proposed methods, we conduct experiments on a synthetic dataset with a sample size of 100 and dimensions varying from 100 to 3000. The re-gression model is y = X  X  +  X  ,where X  X  X  (0 , I p  X  p ) , X  N (0 , 1), and  X  i  X  X  (0 , 0 . 01 2 ). The graph is randomly gen-erated. The number of edges n e varies from 100 to 3000. The regularization parameters are set as  X  1 =  X  2 =0 . 8max {| with n e fixed. Since the graph size affects the penalty,  X  and  X  2 are scaled by 1 n e to avoid trivial solutions with di-mension p fixed. The average computational time based on 30 repetitions is reported in Figure 2. As can be seen in Figure 2, GOSCAR can achieve 1e-4 precision in less than 10s when the dimension and the number of edges are 1000. The computational time of ncTFGS is about 7 times higher than that of GOSCAR in this experiment. The computa-tional time of ncFGS is the same as that of ncTFGS when  X  = 100, and very close to that of ncTFGS when  X  =0 . 15. We can also observe that the proposed methods scale very well to the number of edges. The computational time of the proposed method increases less than 4 times when the num-ber of edges increases from 100 to 3000. It is not surprising because the time complexity of each iteration in Algorithm 1 is linear with respect to n e , and the sparsity of T makes the algorithm much more efficient. The increase of dimension is more costly than that of the number of edges, as the com-plexity of each iteration is quadratic with respect to p . Figure 2: Comparison of GOSCAR, ncFGS, ncT-FGS (  X  =0 . 15 ), and ncTFGS (  X  = 100 )intermsof computation time with different dimensions, preci-sions and the numbers of edges (in seconds and in logarithmic scale).
We use five synthetic problems that have been commonly used in the sparse learning literature [2, 10] to compare the performance of different methods. The data is generated from the regression model y = X  X  +  X  , X  i  X  X  (0 , X  2 ). The five problems are given by: 1. n = 100 ,p = 40, and  X  =2 , 5 , 10. The true parameter 2. n =50 ,p = 40,  X  =(3 ,..., 3 3. Consider a regulatory gene network [10], where an en-4. Same as 3 except that 5. Same as 3 except that We assume that the features in the same group are connected in a graph, and those in different groups are not connected. We use MSE to measure the performance of estimation of  X  , which is defined as
For feature grouping and selection, we introduce two sep-arate metrics to measure the accuracy of feature grouping and selection. Denote I i ,i =0 , 1 , 2 , ..., K as the index of different groups, where I 0 is the index of zero coefficients. Then the metric for feature selection is defined as and the metric for feature grouping is defined as where s s measures the grouping accuracy of group i under the as-sumption that the absolute values of entries in the same group should be the same, but different from those in differ-ent groups. s 0 measures the accuracy of feature selection. It is clear that 0  X  s 0 ,s i ,s  X  1.

For each dataset, we generate n samples for training, as well as n samples for testing. To make the synthetic datasets more challenging, we first randomly select n/ 2 coefficients, and change their signs, as well as those of the corresponding features. Denote  X   X  and  X  X as the coefficients and features after changing signs. Then  X   X  i =  X   X  i ,  X x i =  X  x i th coefficient is selected; otherwise,  X   X  i =  X  i ,  X x that  X  X  X   X  = X  X  . We apply different approaches on  X  covariance matrix of X is used in GFlasso to simulate the graph misspecification. The results of  X  converted from  X   X  are reported.

Figure 3 shows that ncFGS obtains the same results as ncTFGS on dataset 1 with  X  =2when  X  is larger than |  X  | . The regularization parameters are  X  X  1 and  X  X  2 for ncTFGS, and  X  1 and  X  2 for ncFGS. Figure 4 shows the av-erage nonzero coefficients obtained on dataset 1 with  X  =2. As can be seen in Figure 4, GOSCAR, ncFGS, and ncTFGS are able to utilize the graph information, and achieve good parameter estimation. Although GFlasso can use the graph information, it performs worse than GOSCAR, ncFGS, and ncTFGS due to the graph misspecification. Figure 3: MSEs (left), s 0 (middle), and s (right) of ncFGS and ncTFGS on dataset 1 for fixed  X  1 and  X  . The regularization parameters for ncTFGS are  X  X  1 and  X  X  2 .  X  ranges from 0.04 to 4.

The performance in terms of MSEs averaged over 30 sim-ulations is shown in Table 1. As indicated in Table 1, among existing methods (Lasso, GFlasso, OSCAR), GFlasso is the best, except in the two cases where OSCAR is bet-ter. GOSCAR is better than the best existing method in all cases except for two, and ncFGS and ncTFGS outperform all the other methods.

Table 2 shows the results in terms of accuracy of feature grouping and selection. Since Lasso does not perform fea-ture grouping, we only report the results of the other five methods: OSCAR, GFlasso, GOSCAR, ncFGS, and ncT-FGS. Table 2 shows that ncFGS and ncTFGS achieve higher accuracy than other methods.

Table 3 shows the comparison of feature selection alone (  X  2 = 0), feature grouping alone (  X  1 = 0), and simulta-neous feature grouping and selection using ncTFGS. From Datasets Lasso OSCAR GFlasso GOSCAR ncFGS ncTFGS Figure 4: The average nonzero coefficients obtained on dataset 1 with  X  =2 : (a) Lasso; (b) GFlasso; (c) OSCAR; (d) GOSCAR; (e); ncFGS; (f) ncTGS Table 3, we can observe that simultaneous feature grouping and selection outperforms either feature grouping or feature selection, demonstrating the benefit of joint feature grouping and selection in the proposed non-convex method.
We conduct experiments on two real datasets: FDG-PET and Breast Cancer. The metrics to measure the performance of different algorithms include accuracy (acc.), sensitivity (sen.), specificity (spe.), degrees of freedom (dof.), and the number of nonzero coefficients (nonzero coeff.). The dof. of lasso is the number of nonzero coefficients [18]. For the algorithms capable of feature grouping, we use the same definition of dof. in [2], which is the number of estimated groups.
In this experiment, we use FDG-PET 3D images from 74 Alzheimer X  X  disease (AD), 172 mild cognitive impair-ment (MCI), and 81 normal control (NC) subjects down-loaded from the Alzheimer X  X  disease neuroimaging initiative (ADNI) database. The different regions of whole brain vol-ume can be represented by 116 anatomical volumes of in-terest (AVOI), defined by Automated Anatomical Labeling (AAL) [20]. Then we extracted data from each of the 116 AVOIs, and derived average of each AVOI for each subject. In our study, we compare different methods in distinguish-Figure 5: Subgraphs of the graph built by SICE on FDG-PET dataset, which consists of 265 edges. ing AD and NC subjects, which is a two-class classification problem over a dataset with 155 samples and 116 features. The dataset is randomly split into two subset, one training set consisting of 104 samples, and one testing set consisting of the remaining 51 samples. The tuning of the parameter is achieved by 5-fold cross validation. Sparse inverse covari-ance estimation (SICE) has been recognized as an effective tool for identifying the structure of the inverse covariance matrix. We use SICE developed in [6] to model the con-nectivity of brain regions. Figure 5 shows sample subgraphs built by SICE consisting of 115 nodes and 265 edges. The results based on 20 replications are shown in Table 4. From Table 4, we can see that ncTFGS achieves more ac-curate classification while obtaining smaller degrees of free-dom. ncFGS and GOSCAR achieve similar classification, while ncFGS selects more features than GOSCAR.

Figure 6 shows the comparison of accuracy with either  X  1 or  X  2 fixed. The  X  1 and  X  2 values range from 1e-4 to 100. As we can see, the performance of ncTFGS is slightly better than that of the other competitors. Since the regularization parameters of subproblems in ncTFGS are  X  1  X  and 2  X  2  X  solution of ncTFGS is more sparse than those of other com-petitors when  X  1 and  X  2 are large and  X  is small (  X  =0 . 15 in this case).
We conduct experiments on the breast cancer dataset, which consists of gene expression data for 8141 genes in 295 Table 3: Comparison of feature selection alone (FS), feature grouping alone (FG), and simultaneous fea-ture grouping and feature selection (Both). The average results based on 30 replications of three datasets with  X  =5 : Data3 (top), Data4 (middle), and Data5 (bottom) are reported. The numbers in parentheses are the standard deviations.
 Meth. MSE s 0 s FG 2.774(0.967) 0.252(0.156) 0.696(0.006) FS 6.005(1.410) 0.945(0.012) 0.773(0.037) Both 0.348(0.283) 0.996(0.014) 0.978(0.028) FG 9.4930(1.810) 0.613(0.115) 0.770(0.038) FS 6.437(1.803) 0.947(0.016) 0.782(0.046) Both 4.944(0.764) 0.951(0.166) 0.890(0.074) FG 10.830(2.161) 0.434(0.043) 0.847(0.014) FS 10.276(1.438) 0.891(0.018) 0.768(0.026)
Both 7.601(1.038) 0.894(0.132) 0.919(0.057) breast cancer tumors (78 metastatic and 217 non-metastatic). The network described in [4] is used as the input graph in this experiment. Figure 7 shows a subgraph consisting of 80 nodes of the used graph. We restrict our analysis to the 566 genes most correlated to the output, but also connected in the graph. 2/3 data is randomly chosen as training data, and the remaining 1/3 data is used as testing data. The tuning parameter is estimated by 5-fold cross validation. Table 5 shows the results averaged over 30 replications. As indicated in Table 5, GOSCAR, ncFGS and ncTFGS outperform the Figure 6: Comparison of accuracies for various methods with  X  1 fixed (left) and  X  2 fixed (right) on FDT-PET dataset. other three methods, and ncTFGS achieves the best perfor-mance.
In this paper, we consider simultaneous feature group-ing and selection over a given undirected graph. We pro-pose one convex and two non-convex penalties to encourage both sparsity and equality of absolute values of coefficients for features connected in the graph. We employ ADMM and DC programming to solve the proposed formulations. Numerical experiments on synthetic and real data demon-strate the effectiveness of the proposed methods. Our results also demonstrate the benefit of simultaneous feature group-ing and feature selection through the proposed non-convex Figure 7: A subgraph of the network in Breast Can-cer dataset [4]. The subgraph consists of 80 nodes. methods. In this paper, we focus on undirected graphs. A possible future direction is to extend the formulations to directed graphs. In addition, we plan to study the general-ization performance of the proposed formulations. This work was supported in part by NSF (IIS-0953662, MCB-1026710, CCF-1025177, DMS-0906616) and NIH (R01LM010 730, 2R01GM081535-01, R01HL105397).
