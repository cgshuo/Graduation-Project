 1. Introduction the members of the target community ( Fox, Akscyn, Furuta, &amp; Leggett, 1995; Gon X alves et al., 2004 ). conform to one or more metadata standards that specify, among others, a standardized set of metadata fields and their semantics for the description of digital objects. The Dublin Core, for the representation and storage of information about scientific publications and Web pages.  X  and 7).
 &amp; Beheshti, 1997 ), as well as in the database realm.
 although this is not the focus of this work.

This article presents an approach to identifying duplicated bibliographic metadata records. We assume that a mapping main contributions of this work are: cially designed for the digital library domain; development of new approaches for automatic bibliographic metadata deduplication.
The quality of the proposed deduplication functions is evaluated in experiments with two real datasets and compared process.
 directions. 2. Related work proach can be used to deduplicate objects with complex structures such as XML documents. complex values (MCV). MAV are dependent on the application domain while MCV are defined according to the child nodes X  set.
 Other works have proposed strategies based on machine learning techniques to estimate record similarities. The Active distance and another based on Support Vector Machines (SVMs) ( Boser, Guyon, &amp; Vapnik, 1992 ). computationally less demanding than those generated by other approaches since they potentially use less evidence. The identification.
 of each function. 3. The metadata deduplication approach we are aware of the problem of schema matching but this is out of the scope of this work main metadata fields that describe the digital objects, as we shall see. 3.1. IniSim implementation of this function through a linear algorithm.
 Let C be a set of proper nouns and N S be the set of natural numbers f 0 ; 1 g ; IniSim : f C C g! N IniSim returns s 2 N S j s  X  1. Otherwise, s = 0 is returned. IniSim is defined by Eq. 1 as where a , b are strings formed by the initials of the author names, a
Initials(Lennon, John)) = IniSim (JWL, LJ)=1 ( a 1 = b 2 ^ a (JW, LJ) = 0 .
 are usually the most reliable evidence for misspelling cases. 3.2. NameMatch author names associated with two distinct digital objects.
 Let C be a set of proper names, R S be the set of real numbers in the interval [0,1] and N {0,1}. NameMatch : f C C R S g! N S is defined by the algorithm 3 for i 1 to m 4 do for j 1 to n 5 do if INISIM ( K i , L j )=1 6 then counter counter +1; 7 L j null ; 8 if counter /Max( m , n ) P t N 9 then return 1; 10 else return 0; where K , L are lists composed of the initials of the author names of the digital objects being compared, K ment of the K list, i.e., the initials of i th author name of the first object, L j th author name of the second object and t N 2 R S is a similarity threshold. In addition, length of a list, counter indicates the number of matches found by function IniSim and of the largest list of words.
 ilarity score s 2 N S j s  X  1, otherwise it returns s =0.
 list of author names of a publication is not complete. In such cases, the threshold t lows two metadata records with different numbers of authors to be possible candidates for a match. Errors are also possible in the initials of author names. However, the threshold t functions IniSim and NameMatch . 3.3. MetadataMatch Let M be the metadata record of a digital object, R S be the set of real numbers in the interval [0,1] and N natural numbers {0,1}. The function MetadataMatch : f M M N R 1 if j YEAR ( a ) YEAR ( b ) j 6 t Y 2 then for all AUTHOR ( a ) 4 for all AUTHOR ( b ) 6 if NAMEMATCH ( iniList a , iniList b , t N )=1 8 then return 1; 9 return 0; where a and b are metadata records of two digital objects, t t 2 R S is the minimum matching threshold between author names, t
YEAR returns the publication year of a digital object, AUTHOR initials of an author name, ADD inserts the initials of an author name into a list, fined in Section 3.2 , TITLE returns the title of a digital object and the titles of two objects by using Levenshtein X  X  method ( Levenshtein, 1966 ).
 year. Then, the initials of the author names are extracted (lines 2 X 5) and added to the lists iniList compared by the function Levenshtein .
 returns 1 (line 8). At any time, if a condition is evaluated as false then the algorithm returns 0 (line 9).
The proposed function NameMatch aims at producing a maximum recall. The majority of the relevant matches will be precision scores in the results of our experiments confirm this hypothesis.
 The deduplication process may also achieve high levels of precision due to the comparison of titles by the function of performed comparisons. In our experiments, we also quantified the reduction in the number of comparisons for each tested condition and applied function.
 the function NameMatch may be replaced by another one that implements a specific strategy for matching author names actly what we did for the comparison with the baselines in the experiments we describe in the next section. 4. Experimental evaluation experiments are divided into two groups: plication from 2% to 62% in the digital library dataset and from 7% to 188% in the article citation dataset. experimental results show that the proposed functions produced statistically equivalent results without the burden and cost of any training process.
 Neto, 1999; Manning et al., 2008 ) and Wilcoxon signed-rank test ( Siegel, 1956; Wilcoxon, 1945 ). The experiments were performed in a personal computer with 1.86 GHz dual core processor and 2.0 GB of DRR2 memory.
Our implementation has required only 60 MB of memory. 4.1. Datasets The first dataset used in our experiments was created with metadata records extracted from the digital libraries BDB-
Comp and DBLP. When these metadata records were collected (March 1st, 2007), there was about four thousand references to scientific papers published in Brazil in BDBComp. The metadata records were harvested by means of the OAI-PMH pro-countries. The metadata records were collected from the digital library website in a specific XML format. the total of selected digital objects in the Libraries dataset.
 of duplicated records in this first dataset.
 2008 ). Table 4 shows an example of duplicated records in this collection.
 used in our experiments. 4.2. Results of the first group of experiments Fragments e MCV Set . In this group of experiments we varied the following parameters of MetadataMatch : NameFunc  X  name similarity function, which can assume Guth , Acronyms , Fragments or the proposed function IniSim ;
MatchAlg  X  author name matching algorithm, which can assume MCV Set or the proposed NameMatch algorithm; t
Y  X  publication year difference; t
L  X  threshold applied to the function Levenshtein ; t
F  X  threshold applied to the function Fragments ; t
N  X  threshold applied to the function NameMatch . 4.2.1. Libraries experiments
For the experiments with the Libraries dataset, the thresholds assume the following values: t
Y = 0 (same year) or t Y = 1 (1 year before/after); 0.5 6 t L 6 0.7; 2 6 t F 6 4; 0.75 6 t N 6 1.0.
 sure for each combination of parameters. We show the worst and best F -measures for each combination between the name metadata records identified by the specialist. 4.2.2. Cora experiments
For the experiments involving the Cora dataset, the thresholds assume the following values: t
Y = 0 (same year) or t Y = 1 (1 year before/after); 0.5 6 t L 6 0.7; 1 6 t F 6 4; 0.6 6 t N 6 1.0.
 measures were determined based on the 305 distinct bibliographic citations identified by means of the class field. 4.2.3. Analysis of the results and IniSim showed recall values between 63.67% and 81.33% (lines 5 X 8; 13 X 16).
 with all combinations of existing functions, we obtained improvements up to 62% (Libraries dataset) and 188% (Cora dataset).

To better understand the sensitivity of the functions with respect to the number of authors to be matched, we ran a the Cora dataset, the experiments produced better results than using t liographic metadata records because it does not need to match the exact number of authors. It is worth noticing, tion of the collection.
 Regarding the effect of the t Y parameter, for the Libraries dataset changes from t t for these experiments are ommited due to space constraints.
 1956; Wilcoxon, 1945 ) comparing the proposed functions IniSim and NameMatch with the functions Fragments and MCV calculated are lower than the statistical significance threshold a = 0.01. 4.2.4. Analysis of the cases of failure Set .( Table 5 , line 6; Table 6 , line 6).
 identified.
 The problems presented by the function combinations fall in one of the following cases: removal.
 ure for the known functions. In case of the Library dataset, one additional pair was not identified. set, affecting both combination of functions.
 Different number of authors (line 4 of Table 7 )  X  when NameMatch uses the threshold t data records differ widely in the number of authors (more than 25%). This problem occurred in five pairs of non iden-combination. Given the inflexibility of the combination  X  X  X /S X  X , which requires the match of the exact number of authors, this problem caused errors in additional pairs achieving nine cases in the Library dataset and 666 cases in total for Cora.
 for the Cora dataset.
 occurred in only one pair of non identified duplicate metadata records for the Libraries dataset. the position of the names, did not suffer from this problem.
 rect value but these were missing in 3974 pairs of non identified duplicate metadata records for the Cora dataset. lem occurred in 357 pairs of non identified duplicate metadata records for the Cora dataset.
To summarize the discussion, when compared to the best combination of functions found in the literature, our proposed 4.3. Results of the second group of experiments
This section describes the second group of experiments: the comparison of our approach with a state-of-the-art super-vised genetic programming method for record deduplication ( Carvalho et al., 2008 ). the name similarity and the author name matching functions: we used the proposed functions IniSim and NameMatch . The similarity thresholds were also the same.
 mean of the results of all runs. This experimental setup is similar to that used in ( Carvalho et al., 2008 ).
The Cora and Libraries experimental results are summarized in Table 8 which presents the mean balanced F -measure and function MetadataMatch : t Y =1, t N = 0.6, t L = 0.6 when using the Cora dataset and t imental configuration is also different.
 data which, in some cases, for instance, large repositories, may be very expensive or even impossible to obtain.
We also quantified the reduction in the number of comparisons for each condition and function internally performed by the function MetadataMatch . Analyzing the results for the Libraries dataset, the average number of comparisons made between the publication years was 1,504,245. NameMatch was applied to 363,637 out of the 1,504,245 pairs since the tween author names. The average number of comparisons made by the function Levenshtein was only 383 out of the reduced to 0.025% of the total possible number.

Analyzing the results for the Cora dataset the average number of comparisons made between publication years was pairs (1.68% of total possible number). 5. Conclusions any type of training, which sometimes is very expensive to carry out.

The experimental results show that the performance of the proposed functions IniSim and NameMatch are statistically most expensive of the functions.
 valuable for the development of new approaches because they show the difficulties of matching some metadata elements metadata records in other domains where the deduplication of proper names is essential. is essential. 6. Acknowledgments
CNPq Universal project ApproxMatch (Grant number 481055/2007-0), MCT/CNPq/CT-INFO projects Gest X o de Grandes Vol-umes de Documentos Textuais (Grant number 550891/2007-2) and InfoWeb (Grant number 550874/2007-0), and by the authors scholarships and individual research grants from CAPES, CNPq and FAPEMIG.
 References
