 system [6, 7].
 characteristics of GPs in order to accommodate nonstationa ry data. robot arm. vector x query point x the existence of a spatially localized weighting kernel w every { x goal is to find a Bayesian formulation of determining b and h simultaneously. 2.1 Model For the locally linear model at the query point x dom variables z [12] and modify the linear model y , where z Normal (0 , X  both additive noise terms. Note that x [ x is the m th coefficient of x coefficient of b and b The z variables allow us to derive compu-tationally efficient O ( d ) EM-like updates, as we will see later. The prediction at the query point x sume the following prior distributions for our model, shown graphically in Fig. 1: where 1 is a vector of 1 s, z covariance matrix of b the Scaled-inverse- X  2 distribution ( n the scale parameter). The Scaled-Inverse- X  2 distribution was used for  X  prior for the variance parameter of a Gaussian distribution . indicator-like weight, w the local model if w where w weighting kernel K as some explicit function, we model the weights w random variables, i.e., p ( w parameter q Our choice of function for q updates. We place a Gamma prior over the bandwidth h where a kernel width. 2.2 Inference Q i =1 p ( y i , z i , b ,w i , h , X  weight w fashion to mixture models, as: L = log Expanding the log p ( w ( x posterior of h term so that  X  log(1 + x problematic terms in log p ( w hidden variables Q ( b , X  mation of the true posterior, e.g., Q ( b , X  p ( w im = 1 | y i , z i , x i ,  X  ,w i,k 6 = m ) , is inferred using Bayes X  rule: where  X  = { b , X  m this) and list only the posterior updates for h where I Q vision by zero,  X  w x of the expression for  X  b a data sample i with lower weight w influenced by the residual error at each data point (see poste rior update for  X  w shaping is able to ignore outliers that a classical GP fits. A few remarks should be made regarding the initialization of priors used in the posterior EM updates.  X  reflect a large uncertainty associated with the prior distri bution of b . The initial noise variance,  X  on the noise variance. To adjust the strength of this prior, n be set to the number of samples one believes to have seen with noise variance  X  should be set so that the kernel is broad and wide. We use value s of a that some sort of initial belief about the noise level is need ed to distinguish between noise and structure in the training dat a. Aside from the initial prior on  X  sets in our evaluations. 2.3 Computational Complexity arises from the introduction of the hidden random variables z requires O ( NdI more appealing to real-time applications. the corresponding inputs, i.e., k ( x training data is a function of the distances | x nonstationary spatial GP into a latent space [19].
 v v centered at x values of { h shaping algorithm and then optimize the hyperparameters v sample x x unlike previously proposed nonstationary GP methods [8, 9] . 4.1 Synthetic Data dimensional data) would be wrong.
 some of the more nonlinear and noisier regions after 30msec. 4.2 Robot Data properties X  X ypical characteristics of most control learni ng problems. q equation is  X  q = J # ( q )  X  x  X   X  ( I  X  J # J )  X  X  k chooses a solution which produces a  X  y that pushes the y coordinate toward y weight from the forward model and additionally weighted by t he reward. implementation of the local kernel shaping algorithm.

