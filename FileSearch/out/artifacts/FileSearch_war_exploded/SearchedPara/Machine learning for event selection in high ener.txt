 1. Introduction
The field of high energy physics is devoted to the study of the elementary constituents of matter. By investigating the structure of matter and the laws that govern its interactions, this field strives to discover the fundamental properties of the physical universe. In experimental high energy physics, the goal is to test predictions made by current theories such as the Standard Model (Weinberg,1967; Glashow,1961; Yao et al., 2006 ), which describes the behavior of three of the four fundamental forces.
The primary tools of experimental high energy physicists are modern accelerators, which collide protons and/or anti-protons to create exotic particles that occur only at extremely high energy densities. Such particles have not existed naturally since the first moments after the Big Bang, when the energy density of the universe was much higher. Observing these particles and measuring their properties may yield critical insights about the very nature of mass.
 Two particles of particular interest are the top quark and the Higgs boson . The top quark, first observed in 1995 ( Abe et al.,1995; Abott et al.,1995 ), is nearly as massive as a gold nucleus, making it by far the most massive subatomic particle ever observed. The top quark is important because precise measurements of its mass can stringently test theories about the origins of particle mass (Hashimoto et al., 2001; Heinemeyer, 2003; The LEP Collabora-tion, 2004; Miransky et al., 1989 ). Only the world X  X  most powerful accelerator, the Fermilab Tevatron in Batavia, Illinois, has sufficient energy to produce top quarks. By contrast, the Higgs boson ( Higgs, 1966 ) has never been observed. In fact, it is the only remaining particle predicted by the standard model whose existence has not been experimentally verified ( Yao et al., 2006 ). Since the Higgs boson is theorized to give mass to other particles through its interactions ( Kado and Tully, 2002 ), it is central to current theories about particle mass. Hence, observing the Higgs boson is a paramount goal in high energy physics.

Producing and observing such particles require extraordinary resources. The Tevatron accelerator and its particle detectors cost billions of dollars to construct and approximately a million dollars per day to operate. As a result, extracting maximal information from the resulting data is essential. In this article, we study the use of machine learning methods to aid this process. In particular, we investigate their efficacy for event selection for top quark mass measurement and Higgs boson search.

In an accelerator event, protons and/or anti-protons are accelerated and annihilated. The resulting energy causes new particles to form, which can be observed via detectors that surround the point of collision. However, the vast majority of events do not produce particles of interest, such as the top quark or Higgs boson. For example, though the Tevatron produces approximately 10 10 events per hour, approximately one results in a top quark, on average. Therefore, good data analysis depends on effective event selection, in which events producing particles of interest ( signal ) are separated from those producing other particles ( background ). Event selection is difficult because several types of background can mimic the signal X  X  characteristic signature. Hence, event selection in high energy physics is an exciting challenge for machine learning. In this article, we compare two different approaches to this problem.

The first approach is based on supervised learning methods, which are used to train classifiers that distinguish signal from background. Such methods have already proven successful in similar event selection problems by training neural networks (Abazov et al., 2001; Acosta et al., 2005 ) or support vector machines (Whiteson and Naumann, 2003 ) to classify events as signal or background. This supervised approach is most effective in the narrow class of problems in which the classification accuracy of the event selector is closely correlated with the quality of the resulting data analysis and systematic uncertainties are minimal. However, top quark mass measurement and Higgs boson search exemplify a broader class of problems where higher classification accuracy does not necessarily result in superior analysis performance. Instead, the top quark mass measurement is more sensitive to the presence of some background events than others, in ways that are difficult to predict a priori . The Higgs boson search is limited by systematic uncertainties on the background events. Therefore, selectors that maximize classification accuracy may perform worse than those that (1) increase the quantity of signal by tolerating harmless background, (2) reduce the quantity of signal to eliminate disruptive background, or (3) minimize the impact of systematic uncertainties.
To find such selectors, we introduce a second, novel approach that uses stochastic optimization techniques. Rather than maxi-mizing classification accuracy, this approach directly optimizes selectors for their true purpose: maximizing either the precision of top quark mass measurements or the sensitivity to the presence of the Higgs boson. Using NeuroEvolution of Augmenting
Topologies (NEAT) ( Stanley and Miikkulainen, 2002 ), an evolu-tionary method for training neural networks, we optimize event selectors that operate either in conjunction with supervised classifiers or in lieu of them.

This article presents experiments that compare the perfor-mance of manually designed heuristic selectors to neural network selectors trained with backpropagation ( Rumelhart et al., 1986 )or
NEAT. In both top quark mass measurement and Higgs boson search, the learning methods perform significantly better than the heuristic approach, confirming that machine learning can greatly benefit event selection in high energy physics. Furthermore, the
NEAT selectors yield by far the best analyses, demonstrating the advantage of the stochastic optimization approach in an applica-tion area previously assumed the province of supervised methods.
Finally, this article describes a detailed case study in which the best performing selector is applied to real data gathered by the CDF II detector at the Tevatron. The result is a substantial reduction in uncertainty in the top quark mass measurement, yielding by far the most precise measurement of this type to date.
Obtaining a similar reduction in uncertainty would otherwise require producing many more collisions at great expense. Hence, this new approach to event selection has already contributed substantially to our knowledge of the top quark X  X  mass and our understanding of the larger questions upon which it sheds light.
The approaches we propose also offer potential benefits beyond the analysis of data gathered at the Tevatron. The future of high energy physics lies with the large hadron collider (LHC), a new accelerator currently under construction. Once in operation, the LHC will produce collisions at much higher frequency and energy than the Tevratron. The resulting torrent of data will require highly effective event selection, which can potentially be aided by the methods presented in this article.

The remainder of this paper is organized as follows. Section 2 overviews the process of producing, detecting, selecting, and analyzing events in modern high energy accelerators. Sections 3 and 4 describe methods for performing event selection with the aid of supervised learning and stochastic optimization, respec-tively. Sections 5 and 6 compare the performance of these methods on the problems of top quark mass measurement and
Higgs boson search, respectively. Section 7 describes the use of our optimized selector to produce a new top quark mass measurement with data from the Tevatron. Section 8 discusses the the implications of these results, Section 9 outlines our plans for future work, and Section 10 concludes. 2. Events in high energy physics
This section provides an overview of the process of producing, detecting, selecting, and analyzing events at modern high energy accelerators, with particular focus on measurement of the top quark X  X  mass and the search for the Higgs boson. 2.1. Producing events
To provide enough energy to produce massive exotic particles such as the top quark or the Higgs boson, one must accelerate and annihilate lighter particles and their anti-particles. Fig. 1 shows the Fermilab Tevatron accelerator complex in Batavia, Illinois, which includes a series of smaller accelerators that seed the final 6.5-km Tevatron ring. The Tevatron accelerates protons and anti-protons to a center-of-mass energy of 1.96tera-electron-volts (TeV), the highest controlled energy events ever achieved. 2.2. Detecting events
Top quark and Higgs boson events are extremely rare. Though the Tevatron produces approximately 10 10 events per hour, only approximately 1 per hour yields a top quark and approximately 0.01 per hour produces a Higgs boson. Even when produced, detecting these particles is a great challenge. They cannot be directly observed since they are smaller than the wavelength of visible light. In addition, they decay into other particles almost immediately, in less than 10 17 s.

Though the massive top quark and Higgs boson cannot be directly observed, the lighter stable particles to which they decay, called decay products , can be observed. Multiple layers of detectors (Abulencia et al., 2005 ) surround the point of collision for this purpose. As the decay products pass through these detectors, they interact with them in a way that allows their direction and energy to be observed. Fig. 2 depicts CDF II, one of the two detectors at the Tevatron. 2.3. Selecting events
Since the vast majority of events do not produce particles of interest, culling signal from background is crucial to obtaining high quality analyses. This process occurs in two phases: (1) pre-selection , in which simple rules are applied to eliminate events that are trivially known to be background and (2) final selection ,in which more subtle properties of the remaining events are examined to determine whether they contain particles of interest.
Final selection can be performed using heuristics developed by physics experts ( Abulencia et al., 2004 ) or, preferably, using machine learning, as will be detailed in Sections 3 and 4. The remainder of this section describes the pre-selection process for top quark mass measurement and Higgs search. 2.3.1. Pre-selection for top quark mass measurement
In top quark mass measurement, the pre-selection phase discards all events that do not display the top quark X  X  character-istic signature. This signature emerges from the direction and energy of the top quark X  X  decay products, shown in Fig. 3 . The decay products consist of two leptons; two bottom quarks, which are seen in the detector as  X  X  X ets X  X  of lower energy particles; and an energy imbalance caused by missing neutrinos, which escape undetected. Pre-selection is safe because it has nearly 100% recall, i.e., signal events are almost never discarded.

Unfortunately, pre-selection does not have high precision because the characteristic signature is not specific to top quark decay ( Abulencia et al., 2004 ). In fact, although pre-selection discards more than 99% of the events, 83% of those that remain consist of background that mimics the top quark X  X  signature. In particular, only five types of events may survive pre-selection. Their particular nature is not important for the process of final selection, but their relative frequencies are. These events are, in order of diminishing importance: (i) two gluons and a Z boson decaying to a pair of stable leptons (ii) the simultaneous measurement of three gluons and a W (iii) two gluons and a Z boson decaying to a pair of unstable (tau (iv) the simultaneous measurement of two gluons and two W (v) the simultaneous measurement of a Z boson and a W boson
While these backgrounds mimic the top quark X  X  basic signature, they differ from top quark events in more subtle ways, e.g. the distribution of energy in the leptons or jets. Final selections, described in Sections 3 and 4, can further prune the data by exploiting these differences. 2.3.2. Pre-selection for Higgs boson search
As in top quark mass measurement, pre-selection in Higgs boson search discards all events without the appropriate signature. In this case, the signature results from the Higgs boson X  X  decay products, shown in Fig. 4 , which consist of two jets from bottom quarks and two jets from other quarks. As with the top quark, pre-selection for the Higgs boson has nearly perfect recall but poor precision, as this signature is not unique to the Higgs boson. Despite discarding more than 99% of the events, pre-selection leaves a pool of events still overwhelmingly dominated by background: only 1 in approximately 3500 events involves a Higgs boson. Though these backgrounds produce a similar signature, the energy of their jets will differ slightly, making it possible to suppress their influence during the final selection. 2.4. Analyzing selected events
The events that survive the final selection are used for data analysis, either to measure the mass of the top quark or to determine whether the Higgs boson was observed.

In the former case, the top quark X  X  mass can be measured by inferring the likely mass of the observed decay products in each event ( Abulencia et al., 2005 ). In particular, probability density functions in top quark mass are obtained for every event.
Combining these into a single joint probability density function and computing its expected value yields the final estimate of the top quark X  X  mass (see Fig. 5 ). Additionally, prob-abilities that the observed decay products are due to one of the major background processes are calculated and used to suppress the influence of background events in the mass measurement. Minimizing the uncertainty of this measurement is the primary goal of event selection for top quark mass measurement.

In the latter case, the data are used to determine whether the presence of the Higgs boson can be confirmed. Since the Higgs boson occurs so rarely at the Tevatron, confirmation can occur only with powerful event selection and large numbers of events.
Confirmation is further hindered by a significant theoretical uncertainty in the nature of background events to the Higgs boson. However, even if there are not enough data to confirm the
Higgs boson X  X  presence, the sensitivity of the selector can still be quantified. This is done using the scale factor , which measures how much more frequently than predicted by theory the Higgs boson must occur for it to be observed. The theoretical uncertainty in the background events yields a systematic uncertainty in the rate of
Higgs boson events, and significantly increases the necessary scale factor. Minimizing the scale factor is the primary goal of event selection for Higgs boson search. 3. Supervised learning for event selection
This section describes how final event selection can be performed with the aid of supervised learning methods which maximize classification accuracy. In a narrow class of problems, these are equivalent to a likelihood ratio test, which the
Neyman X  X earson Lemma ( Neyman and Pearson, 1933 ) suggests that are optimal. This approach is standard in the physics community and serves as a point of comparison for the optimization approach that will be described in Section 4.
Supervised methods can be used to train classifiers that separate signal from background. However, such classifiers cannot be trained on real accelerator data because the true labels for such events are unknown. Instead, they are trained with data drawn from simulators of the accelerator ( Sjostrand et al., 2001 ) and detector ( Agostinelli et al., 2003 ), which generate events and model the interaction of their decay products with the detector. -ln (P) 0 5 10 15 20
These simulators are not  X  X  X oys X  X  but instead sophisticated models that capture our best current understanding of the underlying physical processes and have been extensively verified using data from previous accelerators ( Agostinelli et al., 2003 ).
Since the true mass of the top quark is not known, it is important that selectors for top quark mass measurement be robust across a range of likely masses. Hence, simulated events are generated using three likely mass values: 165, 175, and 185giga-electron-volts per speed of light squared  X  GeV = c 2  X  . Each event is described using the following six features: (1) the mass of the system of two leptons, (2) the number of identified bottom quarks, (3) the imbalance of transverse momentum, indicating the presence of undetected neutrinos, (4) the total transverse energy of all decay products, (5) the minimum angle between a jet and the unbalanced transverse momentum, and (6) the minimum angle between a jet and a lepton. Fig. 6 shows the distribution of values in the dataset of simulated events, after pre-selection, for these six features.

The signal for the Higgs boson is more subtle than that of the top quark, and the backgrounds are both more numerous and less well understood theoretically. As a result, when searching for the
Higgs boson, different features are used to describe each event. In particular, these features summarize the information contained in the decay products into three higher level physics quantities: (1) C , which indicates whether the decay products are consistent with a Wh event, (2) C Z , which similarly indicates consistency with a Zh event, and (3) C B , consistency with a background event.
Fig. 7 shows the distribution for simulated events after pre-selection for these three features.

Using these features to describe each simulated event, we can construct a training set by labeling each collision as signal or background. The remainder of this section describes two super-vised learning approaches for training selectors with such data. 3.1. Training binary classifiers
The simplest supervised learning approach to event selection involves training binary classifiers. This approach has proven successful on related event selection problems, using neural networks ( Abazov et al., 2001; Acosta et al., 2005 ) or support vector machines ( Whiteson and Naumann, 2003 ). In this article, we train feed-forward neural networks with backpropagation (Rumelhart et al., 1986 ).

For top quark mass measurement, we use networks with 6 inputs, 14 hidden nodes, and 1 output. These networks are fully connected, i.e., there is a link between every input and every hidden node and one between every hidden node and every output. In training, each event is labeled 1 if it is signal and 0 otherwise. In testing, an event is classified as signal if the network X  X  output is greater than a threshold t 2 X  0 ; 1 . Since we cannot quantify a priori the trade-off between precision and recall, we set t to the value that maximizes classification accuracy on the training set. To find this value, we sample the range  X  0 ; 1 at regular intervals of 0.025, computing the classification accuracy at each point.

For Higgs boson search, we use fully connected networks with three inputs, two layers of hidden nodes with four and three nodes, respectively, and one output. In training, each signal event
Fraction of Events
Fraction of Events is labeled 1 and each background event 0. However, for testing, we do not apply a specific threshold to the output. Instead, the entire distribution of output values for all pre-selected events is used in measuring sensitivity to the presence of the Higgs boson. This process is described in more detail in Section 4.2. 3.2. Training multi-class classifiers
A potential disadvantage of the binary classification approach is that it gives all backgrounds the same label. Learning may be easier if the problem is cast as a multi-class classification task, where each type of background is treated as a separate class.
To test this approach, we train a set of one-against-all (Hsu and Lin, 2002 ) classifiers, each of which strives to distinguish a given class from all the others.

In the case of top quark mass measurement, there are six classes: one signal class and five background classes, as described in Section 2.3.1. We train six binary classifiers, each of which uses the same network topology described above. When training the k th classifier, each event is labeled 1 if it is in class k and 0 otherwise. Note that one of these classifiers, which distinguishes signal from all five background classes, is identical to the binary classifier described in Section 3.1. In testing, an event X  X  classification corresponds to the network with the highest output. Note that the multi-class approach is not applicable to Higgs boson search, which has a single dominant background. 4. Optimization methods for event selection
The supervised approach described above is most effective in the narrow class of problems in which the classification accuracy of the event selector is closely correlated with the quality of the resulting data analysis. In this case, the Neyman X  X earson Lemma argues that such techniques are optimal.

However, top quark mass measurement and Higgs boson search exemplify a broader class of problems where higher classification accuracy does not necessarily result in better analysis. Instead, the analysis is more sensitive to the presence of some background events than others, in ways that are difficult to predict a priori . Therefore, selectors that maximize classifica-tion accuracy may perform worse than those that (1) increase the quantity of signal by tolerating harmless background, (2) reduce the quantity of signal to eliminate disruptive background, or (3) minimize the impact of systematic uncertainties.

Since the costs of misclassification are not always the same, one way to address this challenge would be to treat event selection as a cost-sensitive supervised learning problem ( Elkan, 2001 ). In such problems, the cost of misclassification can vary depending on either the true class, the incorrectly predicted class, or both. As a result, the goal is not to minimize classification error but instead to minimize the total cost of misclassification.
Specialized learning methods exist to tackle such problems. For example, the MetaCost algorithm ( Domingos, 1999 ) relabels training data such that a classifier trained to minimize classifica-tion error on the relabeled data will minimize the costs of misprediction. However, MetaCost and other methods like it are not applicable to the event selection problem because they require i when the true class is j . In event selection, this cost matrix is not known a priori . Furthermore, even if it were known, it is unlikely that cost-sensitive methods would substantially improve perfor-mance, since the disruption caused by any particular background event can depend on subtle features that are poorly correlated with that event X  X  true type.

While cost-sensitive methods are not applicable to event selection, more general-purpose optimization methods are. Such methods search the space of possible selectors for one that maximizes a given fitness function. Previous studies ( Cranmer and
Bowman, 2005 ) have shown that optimization techniques can compete with supervised methods in similar event selection problems by optimizing criteria related to classification accuracy.
However, those studies used manually defined criteria that the designers hoped would yield effective selectors. In this article, we present a new approach that directly optimizes selectors for their true purpose: maximizing either the precision of the top quark mass measurement or the sensitivity to the presence of the Higgs boson.

In the remainder of this section, we first define fitness functions for each event selection problem and then describe several optimization approaches that make use of these functions. 4.1. Fitness function for top quark mass measurement
The goal of event selection for top quark mass measurement is to obtain a measurement with maximal precision. Hence, the fitness of any given event selector is the precision of the mass measurement obtained using the subset of simulated events it selects. This choice of fitness function naturally accounts for the disruptive effects of the background events that a selector allows,
Fraction of Events 0.05 0.1 C and therefore gives optimization methods the possibility to realize significant performance gains.

All background events that survive pre-selection are roughly consistent with top quark events. Disruptive background events are those that are very consistent with top quark events of a specific top quark mass, and inconsistent with events at other masses. Though they contain no information about the true top quark mass, they sway the measured mass due to their apparent precision.

The most precise measurements are those with the smallest statistical uncertainty, which we measure by calculating the standard deviation of the mass estimates the selector produces on a series of 1000 independent trials at each of the three likely top quark masses. In each trial, we randomly select events, with replacement, from the pre-selected training set and feed them to the selector. The number of events in each trial was chosen to approximately equal the number of events accumulated by the
CDF II detector. The events that survive selection are used to estimate the top quark X  X  mass, as described in Section 2.4. The standard deviation of these estimates reflects the statistical uncertainty of mass measurements produced by that selector.
Note that it is not necessary to optimize event selectors for accuracy because the mass measurement is calibrated for accuracy using simulated events, a process known as bias correction (Abulencia et al., 2005 ). As shown in Fig. 8 , we can correct for any discrepancy between the measured mass M m and the true mass M using simple linear regression, with the actual mass of the simulated events used as labels for training. Hence, background events that do not lower precision are harmless even if they introduce bias. The best selector is that which produces the most precise mass measurements, regardless of the resulting bias. The fitness function for the top quark mass measurement F M t standard deviation of bias-corrected mass estimates: F where M i b is the i th mass measurement after bias correction and M is the average of the N bias-corrected mass measurements. 4.2. Fitness function for Higgs boson search
The goal of event selection for Higgs boson search is to find a selector with maximal sensitivity to the presence of the Higgs boson. As mentioned in Section 2.4, this can be quantified by computing the selector X  X  scale factor, which measures how much the Higgs boson X  X  rate of production would need to increase for it to be observed or statistically excluded.

More specifically, the scale factor is the median 95% expected confidence level upper limit on the Higgs boson production rate, relative to the theoretically predicted rate. To compute confidence level limits, we conduct 4000 independent trials; each trial uses randomly selected simulated events, as with the fitness function for top quark mass measurement.

For each trial, we compare the distribution of the selector X  X  outputs for the simulated events to distributions we would expect for this selector given two hypotheses: (1) that the data contain only background events and (2) that the data contain both background and Higgs boson events. The greater the difference between the distributions for the two hypotheses, the more confident the resulting statistical limit will be (Yao et al., 2006 ).
Increasing the scale factor, and therefore the rate of Higgs boson events, results in a larger difference between the two hypotheses and a more confident limit on the Higgs boson rate. We increase the scale factor of the Higgs boson production rate until the two distributions are different enough to allow us to set a 95% confidence level upper limit on the Higgs boson production rate, averaged over the 4000 trials. Fig. 9 illustrates this process.
This scale factor is the final physics result of a Higgs search, and includes such effects as theoretical uncertainty in the simulation of the background events that can weaken the search. Choice of the scale factor as the fitness function allows optimiza-tion methods the opportunity to select events whose uncertainty will have minimal impact. 4.3. Optimizing binary classifiers
The simplest way to use these fitness functions to improve event selection is in optimizing the threshold t of the binary classifier. As before,wesampletherange  X  0 ; 1 at regular intervals of 0.025. However, at each point we compute the value of the fitness
Events
True Mass M [GeV/c ] function, instead of the classification accuracy. Note that this approach is applicable only to top quark mass measurement, since the binary classifiers for Higgs boson search do not use a threshold. 4.4. Optimizing multi-class classifiers
Optimizing t could improve performance by effectively balancing the trade-off between precision and recall. However, it is still suboptimal because it treats all background types equally. A selector that optimizes the output of the multi-class classifier (made up of multiple binary classifiers) could perform much better: by distinguishing between different background types, it could favor harmless events and discard disruptive ones. This approach is also applicable only to top quark mass measurement, since Higgs boson search has a single dominant background.
The one-against-all approach to multi-class classification does not have thresholds to tune. Nonetheless, its performance can be improved using stochastic optimization techniques. Instead of directly using the classifiers for selection, we use their classifica-tions as input to a selector trained to maximize one of our fitness functions. This selector is also a neural network but its internal structure and weights are determined not by backpropagation but by a stochastic optimization method called NeuroEvolution of
Augmenting Topologies ( Stanley and Miikkulainen, 2002 ). While many other optimization methods could be used in its place, we chose NEAT for event selection because of its previous empirical success on difficult optimization tasks ( Stanley and Miikkulainen, 2002; Whiteson and Stone, 2006 ). Here we provide a brief overview of the NEAT method; a complete description is provided by Stanley and Miikkulainen (2002).

Like other neuroevolutionary methods, NEAT uses evolutionary computation to train neural networks. In a typical neuroevolu-tionary system ( Yao, 1999), the weights of a neural network are strung together to form an individual genome. A population of such genomes is then evolved by evaluating each one and selectively reproducing the fittest individuals through crossover and mutation.
Most neuroevolutionary systems require the designer to manually determine the network X  X  topology (i.e., how many hidden nodes are there and how they are connected). By contrast, NEAT automatically evolves the topology to fit the given problem.

NEAT begins with a uniform population of simple networks with no hidden nodes and inputs connected directly to outputs. In addition to standard weight mutations, two special mutation operators incrementally introduce new structure to the popula-tion. Fig. 10 depicts these operators, which add hidden nodes and links to the network. Only those structural mutations that improve performance tend to survive; in this way, NEAT searches through a minimal number of weight dimensions and finds the appropriate level of complexity for the problem. 4.5. Optimizing selectors without supervised learning
A more radical departure from the traditional approach to training event selectors is to entirely eliminate the use of
Conf. Level for Upper Limit [%] 60 80 100
Events 10 2 10 3 supervised methods. In this approach, the inputs to the NEAT selector are not the outputs of the one-against-all classifiers but instead the original features that served as inputs to those classifiers. As a result, training classifiers is no longer necessary.
Instead, we treat event selection purely as an optimization problem and rely on NEAT to find a selector that maximizes the appropriate fitness function. Unlike the optimization approaches described above, this approach is applicable to both top quark mass measurement and Higgs boson search.

The networks have one input for each event feature (six for top quark mass measurement and three for Higgs boson search) and one output. These networks use the threshold t  X  0 : 5inboth training and testing. It is no longer necessary to tune t since NEAT evolves networks that are optimized for a fixed value of t .
Finding a good selection in this manner is challenging in part because of the size of the search space. The set of possible selections is the power set of the events. Hence, given n events, there are 2 n possible selections. Nonetheless, directly searching for selectors that minimize mass measurement uncertainty yields much better performance than maximizing classification accu-racy, as the results in the following sections confirm. 5. Comparative results for top quark mass measurement
To assess the efficacy of the methods presented in Sections 3 and 4, we evaluated each one on the top quark mass measurement problem, averaging performance over 10 independent runs, using 10,000 simulated events. These runs were conducted using 10-fold cross validation: in each run, 75% of the events are selected at random for training and the remaining 25% reserved for testing. 5.1. Supervised learning results
Fig. 11 shows the classification accuracy on training data for networks trained with backpropagation on simulated pre-selected events, averaged over 10 independent runs. As described in Section 3.2, each network is trained to identify one class of events. Binary classification uses only the network trained to identify top quark events, while multi-class classification uses all six networks. The fully connected, feed-forward networks have 6 inputs, 14 hidden nodes, 1 output, and are trained with a learning rate of 0.001 and a momentum rate of 0.5. Accuracy during training is measured using a threshold t  X  0 : 5.

On the data reserved for testing, the binary classifier had an average classification accuracy of 93 1%. The multi-class classifier identified the correct class with an accuracy of 83 1%. If the multi-class classifier is not penalized for labeling backgrounds with the wrong background class, its accuracy improves to 91 1%. Table 1 shows the precision and recall in training and testing for both the binary and the multi-class classifiers. The similarity in performance between training and testing indicates that the classifiers are not overfit to the training data. Binary and multi-classifiers give mass measurements with an average uncertainty 5.2. Optimization results
If the threshold of the binary classifier is selected to minimize mass measurement uncertainty instead of maximizing classifica-tion accuracy, as described in Section 4.3, the resulting selectors allow for mass measurements with substantially lower average uncertainty, 0 : 82 0 : 04 relative to the heuristic.
Using NEAT to perform stochastic optimization yields even more precise measurements. Fig. 12 shows mass uncertainty on the training set for the best network in each generation trained with NEAT. It compares the performance of NEAT optimizing multi-class classifiers to its performance optimizing directly on the features, without the help of supervised methods. The results are averaged over 10 runs for each method. In testing, the average mass uncertainty of the final generation champions relative to the
Classification Accuracy [%]
Relative Uncertainty heuristic was 0 : 66 0 : 02 and 0 : 64 0 : 02 for the two approaches, respectively.

Table 1 also shows precision and recall in training and testing for the NEAT with features approach. The high recall and low precision suggest that the optimization approach improves the precision of the mass measurement by keeping more top quark events (yielding higher recall) but allowing more harmless background events (yielding lower precision).

Fig.13 shows the expected top quark mass uncertainty for each network evaluated during a typical NEAT run, as well as the fraction of selected events that are signal. Though NEAT finds networks with a signal fraction higher than 0.8, the networks with the lowest top quark mass uncertainty have a starkly lower signal fraction, less than 0.4. This result confirms our hypothesis that signal fraction is not well correlated with uncertainty in the resulting measurement, as the best performing selectors tolerate substantial harmless background.

Fig. 14 summarizes the performance on testing data of all the machine learning methods we employed and compares it to the performance of a heuristic selector designed manually by physicists. Student X  X  t-tests confirm with greater than 98% confidence the statistical significance of the differences between (1) the heuristic selector and each learning method, (2) each supervised method and each optimization method, and (3) the optimized binary classifier and each NEAT method. 6. Comparative results for Higgs boson search
We also evaluated the relative performance of the supervised learning and optimization strategies on the problem of Higgs boson search. As before, each run was conducted using 10-fold cross validation with a 75%/25% split between training and testing. 6.1. Supervised learning results
Recall that the multi-class classification approach is not applicable to Higgs boson search since there is only a single dominant background. Thus, we tested only the binary classifica-tion approach. Fig.15 shows the classification accuracy on training data for networks trained with backpropagation on simulated pre-selected events, averaged over 10 independent runs. We use a fully connected, feed-forward neural network with three inputs, two layers of hidden nodes with four and three nodes, respectively, and one output node. The network is trained using a learning rate of 0.001 and a momentum rate of 0.5.

The classification accuracy does not improve dramatically during training due to the nature of the features, which themselves encapsulate a significant fraction of the classification information through their description of the consistency of each
Relative Uncertainty 0.6 0.8 1 1.2 Network
Relative Uncertainty 0.2 0.4 0.6 0.8
Classification Accuracy [%] 67 68 69 70 71 72 event with one of the two signals or the one background hypothesis. Table 2 shows precision and recall in training and testing. Before classification, the background dominates the pre-selected events; signal to background ratio is approximately 1:3500. Therefore, even relatively high classification accuracy and corresponding impressive reduction in background rate gives a fairly low precision, yielding a signal to background ratio of approximately 1:5. The average resulting scale factor was 0 : 92 0 : 04, relative to the heuristic. 6.2. Optimization results
Since optimizing classifiers is not applicable in Higgs boson search, we tested only the pure optimization approach, where NEAT trains a selector that receives each event X  X  features directly.
Fig. 16 shows the scale factor on the training set for the best networks discovered by NEAT. For reasons of convenience, these experiments use a slightly different version of NEAT called rtNEAT (Stanley et al., 2005 ). rtNEAT is a steady-state evolutionary method, which means it does not have explicit generations but instead one population that changes gradually over time.
Thus, the results in this figure are plotted against total fitness evaluations rather than generations. As before, the results are averaged over 10 runs for each method. In testing, the average scale factor of the final champions relative to the heuristic was 0 : 67 0 : 03.

We do not present precision and recall results for this case because, when optimization methods are applied to Higgs boson search, such metrics are not meaningful and do not have a clear interpretation. The reason, as explained in Section 3.1, is that the output of the network is never interpreted as a class label and thus no threshold is applied to it; instead we merely seek the network that maps events to [0, 1] in a way that minimizes the scale factor.

Fig. 17 summarizes the performance on testing data of both learning methods that were applied to event selection for Higgs boson search and compares it to the performance of a heuristic selector. Students t-tests confirm with greater than 99% confidence the statistical significance of the differences between the NEAT method and both the heuristic and the backpropagation methods. 7. Application to Tevatron events The top quark was first observed in 1995 at the Fermilab Tevatron, which remains the sole accelerator with enough energy to produce it for direct study. The top quark X  X  mass is extraordinary on the scale of fundamental particles, nearly 40 times larger than the second most massive quark and comparable to the mass of a gold atom. Its enormous mass remains a puzzle from a theoretical standpoint, so physicists have focused great effort on probing the precious few top quarks collected in the five years since the CDF II detector became operational.

Consequently, precise measurements of the top quark X  X  mass are a chief scientific priority for the Fermilab program. Stringent efforts are made by the accelerator and detector teams to maximize the size of the accumulated data sample in order to reduce statistical errors, and improvement in techniques for extracting maximal precision from the collected data is the subject of much active research ( Estrada, ; Brubaker, ; Varnes, ; Kovalev, ; Jayatilaka, 2002 ). Typically, the measurements are made and updated yearly, as the dataset grows. In this section we describe how our work has contributed to this effort. In particular, we detail the successful application of a NEAT selector to a re-analysis of the data collected by the CDF II detector.
Prior to this work, event selection for top quark mass measurement was done using the heuristic selector ( Abulencia et al., 2004 ) described in this article. In analyzing the selected events, the measurement was dominated by statistical uncertain-ties due to the sparsity of observed top quarks. Our results demonstrating the potential of stochastic optimization methods for event selection ( Whiteson and Whiteson, 2007 ) convinced physicists that this approach could significantly increase the quantity of signal in the final selection and thus improve the precision of the resulting measurement.

Relative Scale Factor
Relative Scale Factor The data sample used consists of events collected between
March 2002 and 2007. From more than five years of accumulated collisions, 642 events satisfy the pre-selection, of which we expect 131.6 on average to be signal. A neural network trained with NEAT in the manner described in Section 4.5 was used to produce a final selection containing 344 events, of which we expect 121.8 on average to be signal. Since the selector was trained on simulated data, it is critical that the real data are well described by the simulated data. Fig. 18 , which shows neural network output for the pre-selected data as well as for the simulated events, confirms that this is the case.

Using this final selection, a new mass measurement was obtained ( Aaltonen et al., 2009 ):
M  X  171 : 2 2 : 7  X  statistical  X  2 : 9  X  systematic  X 
This is the world X  X  most precise top quark mass measurement using this characteristic signature, by nearly a factor of two (Aaltonen et al., 2008 ) (see Fig. 19 ). It is the first measurement of this type not dominated by statistical uncertainties. Obtaining a similar reduction in uncertainty would otherwise require producing many more collisions at great expense.

Following this measurement, competing measurements of the top quark mass revisited the design of the heuristic selection to more closely follow the selection derived with our optimization.
Thus, this new methodology has changed the way physicists devise heuristic selectors. 8. Discussion
The results presented in Sections 5 and 6 confirm the conclusion of earlier work ( Abazov et al., 2001; Acosta et al., 2005; Whiteson and Naumann, 2003 ) that machine learning methods can substantially outperform heuristic event selectors.
However, previous results demonstrated only that learned selectors had higher classification accuracy, while these results directly verify that they can improve the quality of the resulting analysis. More importantly, these results confirm the advantage of treating event selection as an optimization rather than a supervised learning problem.

In top quark mass measurement, the chief difficulty lies in determining which backgrounds will be harmful to the precision of the resulting measurement. The heuristic selector ( Abulencia et al., 2004 ) strives to eliminate as much background as possible, without regard to how disruptive any particular event may be. It explicitly removes events which have decay products consistent with the largest background, Z boson with gluons. The heuristic performs well in this regard, as the final selection has a signal fraction of approximately 2/3. However, it also removes a lot of signal. By contrast, the selector evolved by NEAT allows approxi-mately 25% more signal into the final sample than the heuristic. It is able to do so by tolerating background that is minimally disruptive to the measurement. As Fig. 13 makes clear, the best performing selectors are those that tolerate substantial quantities of harmless background.

The lack of correlation between the signal fraction and the quality of the resulting analysis also explains why optimization methods outperform supervised methods. By simply maximizing classification accuracy, neither the binary nor the multi-class classification approaches consider which background is worth tolerating. Even the most na X   X v e optimization approach, which merely tunes the threshold t of a binary classifier, outperforms the best purely supervised approach. Furthermore, the performance of NEAT when trained directly on the features suggests that supervised methods are not necessary for this task.

In Higgs boson search, event selection is made difficult by event-by-event differences in the theoretical uncertainties in the description of the physical process that causes background events.
In this case, the heuristic selector makes the reasonable approximation that the three input features can be treated as probabilities that the event is Higgs boson signal ( Wh or Zh )ora background event. The heuristic forms a simple likelihood ratio of the signal and background features, which tends to give large values to signal events and smaller values to background events.
Events 10 10 2
This heuristic is suboptimal because it does not suppress events whose uncertainties weaken the final limit.

Supervised methods can marginally improve classification accuracy over these raw features, which results in a small but significant reduction in the resulting scale factor. However, as with top quark mass measurement, directly optimizing the selector yields by far the greatest performance.

In both tasks, the performance improvement is not only significant but also substantial. In top quark mass measurement,
NEAT produces 29% lower uncertainty than the supervised approach. In Higgs boson search, NEAT produces a 37% lower scale factor than the supervised approach. Obtaining these improvements would otherwise require accumulating 66% or 88% more events, respectively, costing literally tens of millions of dollars and hundreds of person-years.

These savings are not hypothetical, since this new methodol-ogy for training event selectors has been applied, not only to simulated data, but to real data collected on the Fermilab Tevatron accelerator. As described in Section 7, using the best selector trained by NEAT results in the most precise top quark mass measurement of this type to date, by nearly a factor of two.
Consequently, this novel approach to event selection has directly aided the progress of high energy physics, by contributing substantially to our knowledge of the top quark X  X  mass and our understanding of the larger questions upon which it sheds light. 9. Future work
We believe that the intersection of machine learning and high energy physics is a highly promising but under-explored research area. Thus, we hope that the results presented in this article will mark only the beginning of a long and fruitful effort to bridge the gap between these two disciplines. Such research can be a boon to both fields, by providing machine learning researchers with a challenging, realistic proving ground for their methods and arming high energy physicists with the tools to make the most of their data. In this section, we outline some of our plans for future work. 9.1. Methods
Many other machine learning techniques besides those tested here could potentially aid event selection for top quark mass measurement and Higgs boson search. In the future, we aim to further broaden our search for the methods most suitable to these tasks. While many other traditional supervised methods, such as decision trees ( Quinlan, 1986 ), Bayesian classifiers ( Domingos and
Pazzani, 1997 ), nearest neighbor methods ( Shakhnarovich et al., 2005 ), or support vector machines ( Cortes and Vapnik, 1995 ), could be used in place of backpropagation, there is not much reason to believe they would significantly improve performance, as previous studies have shown little performance difference between such methods on related event selection problems (Whiteson and Naumann, 2003 ).

However, it is entirely plausible that other optimization methods could fare better than NEAT. The full representation-learning power of NEAT may not be necessary on these tasks, in which case simpler methods such as hill climbing or simulated annealing ( Kirkpatrick et al., 1983 ) could do just as well. Other neuroevolutionary methods such as CoSyNE ( Gomez et al., 2006 ) or the recently developed estimation of distribution algorithms (Pedro and Lozano, 2002 ) are also potential contenders.
Another possibility for improving performance lies in the area of feature selection , wherein a learning algorithm seeks the most useful subset of features describing each example. Since too few features doom the learner to sub-optimal performance and too many features can lead to intractable dimensionality, feature selection can be a critical issue. Since event selection methods in high energy physics have so far relied on a manually selected set of features, the potential exists for performance improvement via automatic feature selection. However, successfully applying it in this domain is not a trivial task. In fact, our preliminary experiments have shown that making additional features avail-able to the learner does not improve performance, suggesting that the current features selected by expert physicists are already quite good. Nonetheless, a careful study of feature selection in this application area is an important subject for future work. There are many feature selection methods for supervised learning ( Blum and Langley, 1997 ), but only a few exist for optimization techniques. However, FS-NEAT ( Whiteson et al., 2005 ), an extension to NEAT that automates the process of feature selection, could be particularly applicable.

Whereas feature selection aims to pick the right features from a given set, feature extraction or feature construction (Fawcett, 1993; Utgoff, 2001 ) aims to build new, more useful features from those provided. This strategy could also yield significant perfor-mance dividends. However, it may prove just as challenging as feature selection. In fact, the optimization results presented in Section 5.2 (see Fig. 12 ) show that NEAT does not perform better using features extracted by supervised learning rather than using the original features.

Finally, we plan to explore the use of structured prediction methods ( Taskar, 2004; Tsochantaridis et al., 2005 ) for event selection. These methods, which in recent years have proven effective in a wide range of tasks from natural language processing to computational biology ( Getoor and Taskar, 2007 ), represent a new hope for supervised methods in event selection. Unlike traditional supervised techniques, structured prediction methods can minimize arbitrary cost functions that consider dependencies between examples. Hence, such methods could directly maximize the precision of top quark mass measurements or sensitivity to the Higgs boson, while still fully exploiting the availability of labeled training data. 9.2. Applications
As described in Section 7, the methodology introduced in this article has already been applied to real data from the Tevatron to obtain a new top quark mass measurement. In the future, we hope to similarly employ the best selectors trained for Higgs boson search to aid final determinations of whether the Higgs boson was observed at the Tevatron.
 However, the future of high energy physics lies not with the Tevatron but with the large hadron collider, shown in Fig. 20 , which is currently under construction near Geneva, Switzerland (Cho, 2006 ). First collisions at the LHC will be in 2009 at a center-of-mass energy of 14TeV, nearly an order of magnitude greater than the Tevatron. In addition, the rate of collisions will be two orders of magnitude higher. The resulting torrent of data will pose a plethora of new machine learning challenges. For example, event selection for top quark mass measurement at the LHC will be very different than at the Tevatron, since the LHC X  X  higher energy levels are expected to produce top quark collisions much more frequently. In addition, collisions at the LHC will have enough energy to directly observe the Higgs boson, though doing so will require highly effective event selection.

More generally, the scale of high energy physics demands fast, intelligent, and automatic processing of enormous quantities of data. The potential for connections between machine learning and high energy physics stretches beyond the problem of event selection, to reconstruction and classification of the decay products inside each event. In the future, we hope to explore such topics as well. 10. Conclusion
This article describes the use of machine learning methods to aid the process of selecting events at high energy accelerators for the purpose of studying the fundamental nature of matter and its interactions. First, we apply supervised learning methods, which have succeeded previously in similar tasks. Second, we present a new approach that uses stochastic optimization techniques to directly search for selectors that maximize either the precision of top quark mass measurements or the sensitivity to the presence of the Higgs boson. Empirical results confirm that stochastically optimized selectors result in substantially better analyses and therefore more powerful physical insight. This article also describes a case study in which the best selector is applied to real accelerator events from the Fermilab Tevatron, resulting in the most precise top quark mass measurement of this type to date and hence contributing significantly to the progress of high energy physics.
 Acknowledgments The authors thank Peter Stone, Risto Miikkulainen, Ken Stanley,
Razvan Bunescu, Misha Bilenko, and Gwenn Englebienne for their comments and suggestions on this research.
 References
