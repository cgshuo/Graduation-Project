 Discriminative sentence modeling aims to capture sentence meanings, and classify sentences accord-ing to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering X  X or example, n -gram fea-tures (Cui et al., 2006), dependency subtree fea-tures (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011) X  X an play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Re-ichartz et al. (2010) by specifying a certain mea-sure of similarity between sentences, without ex-plicit feature representation.
Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose un-supervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) ex-tend such approaches to learn sentences X  and para-graphs X  representations. Compared with human engineering, neural networks serve as a way of au-tomatic feature learning (Bengio et al., 2013).
Two widely used neural sentence models are convolutional neural networks (CNNs) and recur-sive neural networks (RNNs). CNNs can extract words X  neighboring features effectively with short propagation paths, but they do not capture inher-ent sentence structures (e.g., parse trees). RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree. However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009). (CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)
A curious question is whether we can com-bine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).

In this paper, we propose a novel neural ar-chitecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Net-work (TBCNN). 1 Our models can leverage differ-ent sentence parse trees, e.g., constituency trees and dependency trees. The model variants are de-noted as c-TBCNN and d-TBCNN, respectively. The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation.
One potential problem of RNNs is that the long propagation paths X  X hrough which leaf nodes are connected to the output layer X  X ay lead to infor-mation loss. Thus, RNNs bury illuminating in-formation under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for model-ing time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this prob-lem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).

Recurrent networks . A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree.
First, a sentence is converted to a parse tree, ei-ther a constituency or dependency tree. The corre-sponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tree is repre-sented as a distributed, real-valued vector.
Then, we design a set of fixed-depth subtree fea-ture detectors, called the tree-based convolution window . The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in Figure 1c. For-mally, let us assume we have t nodes in the con-volution window, x 1 ,  X  X  X  , x t , each represented as an n e -dimensional vector. Let n c be the number of feature detectors. The output of the tree-based convolution window, evaluated at the current sub-tree, is given by the following generic equation. where W i  X  R n c  X  n e is the weight parameter asso-ciated with node x i ; b  X  R n c is the bias term.
Extracted features are thereafter packed into one or more fixed-size vectors by max pooling, that is, the maximum value in each dimension is taken. Finally, we add a fully connected hidden layer, and a softmax output layer.

From the designed architecture (Figure 1c), we see that our TBCNN models allow short propaga-tion paths between the output layer and any posi-tion in the tree. Therefore structural feature learn-ing becomes effective.

Several main technical points in tree-based con-volution include: (1) How can we represent hid-den nodes as vectors in constituency trees? (2) How can we determine weights, W i , for depen-dency trees, where nodes may have different num-bers of children? (3) How can we pool varying sized and shaped features to fixed-size vectors?
In the rest of this section, we explain model variants in detail. Particularly, Subsections 3.1 and 3.2 address the first and second problems; Sub-section 3.3 deals with the third problem by intro-ducing several pooling heuristics. Subsection 3.4 presents our training objective. 3.1 c-TBCNN Figure 2a illustrates an example of the con-stituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammat-ical constituent, e.g., a noun phrase. Sentences stituency trees are binarized for simplicity.
One problem of constituency trees is that non-leaf nodes do not have such vector representations as word embeddings. Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b). After pretraining, vector representations of nodes are fixed.

We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children c l and c r , their vec-tor representations denoted as p , c l , and c r . The convolution equation, specific for c-TBCNN, is ciated with the parent and its child nodes. Su-perscript ( c ) indicates that the weights are for c-TBCNN. For leaf nodes, which do not have chil-dren, we set c l and c r to be 0 . this problem. We propose several heuristics for pooling along a tree structure. Our generic de-sign criteria for pooling include: (1) Nodes that are pooled to one slot should be  X  X eighboring X  from some viewpoint. (2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it. Thus, (approximately) equal amount of information is aggregated along different parts of the tree. Following the above intuition, we pro-pose pooling heuristics as follows.  X  Global pooling. All features are pooled to  X  3 -slot pooling for c-TBCNN. To preserve  X  k -slot pooling for d-TBCNN. Different from
We assess the efficacy of pooling quantitatively in Section 4.3.1. As we shall see by the exper-imental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large. TBCNNs are not very sensitive to pooling methods. 3.4 Training Objective After pooling, information is packed into one or more fixed-size vectors (slots). We add a hidden layer, and then a softmax layer to predict the prob-ability of each target label in a classification task. The error function of a sample is the standard cross entropy loss, i.e., J =  X  the ground truth (one-hot represented), y the out-put by softmax , and c the number of classes. To regularize our model, we apply both ` 2 penalty and dropout (Srivastava et al., 2014). Training details are further presented in Section 4.1 and 4.2. In this section, we evaluate our models with two tasks, sentiment analysis and question classifica-tion. We also conduct quantitative and qualitative model analysis in Subsection 4.3. 4.1 Sentiment Analysis 4.1.1 The Task and Dataset Sentiment analysis is a widely studied task for discriminative sentence modeling. The Stanford movie reviews. Two settings are considered for sentiment prediction: (1) fine-grained classifi-cation with 5 labels ( strongly positive , positive , neutral , negative , and strongly negative ), and (2) coarse-gained polarity classification with 2 labels ( positive versus negative ). Some examples are shown in Table 1. We use the standard split for training, val-idating, and testing, containing 8544/1101/2210 sentences for 5-class prediction. Binary classifi-cation does not contain the neutral class.

In the dataset, phrases (sub-sentences) are also tagged with sentiment labels. RNNs deal with them naturally during the recursive process. We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014). The training set therefore has more than 150,000 entries in total. For validating and testing, only whole sentences (root labels) are considered in our experiments.

Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing. 4.1.2 Training Details This subsection describes training details for d-TBCNN, where hyperparameters are chosen by validation. c-TBCNN is mostly tuned syn-chronously (e.g., optimization algorithm, activa-tion function) with some changes in hyperparam-eters. c-TBCNN X  X  settings can be found on our website.

In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hid-den layer. Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus. 2-slot pooling is applied for d-TBCNN. (c-TBCNN uses 3-slot pooling.)
To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200. We use ReLU (Nair and Hinton, 2010) as the activation function .
For regularization, we add ` 2 penalty for vastava et al., 2014) is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. 4.1.3 Performance Table 2 compares our models to state-of-the-art results in the task of sentiment analysis. For 5-class prediction, d-TBCNN yields 51.4% accu-racy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015). c-TBCNN is slightly worse. It achieves 50.4% accuracy, rank-ing third in the state-of-the-art list (including our d-TBCNN model).

Regarding 2-class prediction, we adopted a sim-5-class network is  X  X ransferred X  directly for binary classification, with estimated target probabilities (by 5-way softmax ) reinterpreted for 2 classes. (The neutral class is discarded as in other stud-ies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list.

In a more controlled comparison X  X ith shal-low architectures and the basic interaction (lin-early transformed and non-linearly squashed) X  TBCNNs, of both variants, consistently outper-form RNNs (Socher et al., 2011b) to a large ex-tent (50.4 X 51.4% versus 43.2%); they also con-sistently outperform  X  X lat X  CNNs by more than 10%. Such results show that structures are im-portant when modeling sentences; tree-based con-volution can capture these structural information more effectively than RNNs.

We also observe d-TBCNN achieves higher per-formance than c-TBCNN. This suggests that com-pact tree expressiveness is more important than in-tegrating global information in this task. 4.2 Question Classification We further evaluate TBCNN models on a ques-5452 annotated sentences plus 500 test sam-ples in TREC 10. We also use the stan-dard split, like Silva et al. (2011). Target la-bels contain 6 classes, namely abbreviation , entity , description , human , location , and numeric . Some examples are also shown in Table 1.

We chose this task to evaluate our models be-cause the number of training samples is rather small, so that we can know TBCNNs X  perfor-mance when applied to datasets of different sizes. To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively. We do not back-propagate gradient to embeddings in this that the network is transferred directly from that of 5-class. Table 3: Accuracy of 6-way question classification. task. Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%.

Table 3 compares our models to various other methods. The first entry presents the previous state-of-the-art result, achieved by traditional fea-ture/rule engineering (Silva et al., 2011). Their method utilizes more than 10k features and 60 hand-coded rules. On the contrary, our TBCNN models do not use a single human-engineered fea-ture or rule. Despite this, c-TBCNN achieves similar accuracy compared with feature engineer-ing; d-TBCNN pushes the state-of-the-art result to 96%. To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task. 4.3 Model Analysis In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs. 4.3.1 The Effect of Pooling The extracted features by tree-based convolution have topologies varying in size and shape. We pro-pose in Section 3.3 several heuristics for pooling. This subsection aims to provide a fair comparison among these pooling methods.

One reasonable protocol for comparison is to tune all hyperparameters for each setting and com-pare the highest accuracy. This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning. An alternative is to predefine a set of sensible hy-perparameters and report the accuracy under the same setting. In this experiment, we chose the latter protocol, where hidden layers are all 300-dimensional; no ` 2 penalty is added. Each config-uration was run five times with different random initializations. We summarize the mean and stan-dard deviation in Table 4.

As the results imply, complicated pooling is bet-ter than global pooling to some degree for both model variants. But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data. In our experiments, we ap-ply 3-slot pooling for c-TBCNN and 2-slot pool-ing for d-TBCNN. Table 4: Accuracies of different pooling methods, averaged over 5 random initializations. We chose sensible hyperparameters manually in advance to make a fair comparison. This leads to performance degradation (1 X 2%) vis-a-vis Table 2. Figure 4: Accuracies versus sentence lengths.
Comparing with other studies in the literature, we also notice that pooling is very effective and ef-ficient in information gathering. Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sen-timent classification. Our TBCNNs are typically trained within 25 epochs. 4.3.2 The Effect of Sentence Lengths We analyze how sentence lengths affect our mod-els. Sentences are split into 7 groups by length, with granularity 5. A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457. Figure 4 presents accuracies versus lengths in TBCNNs. For comparison, we also reimple-mented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b). Thus, we think our reimplementation is fair and that the comparison is sensible.
We observe that c-TBCNN and d-TBCNN yield very similar behaviors. They consistently outper-form the RNN in all scenarios. We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words. This re-sult confirms our theoretical analysis in Section 2 X  X or long sentences, the propagation paths in RNNs are deep, causing RNNs X  difficulty in in-formation processing. By contrast, our models ex-plore structural information more effectively with tree-based convolution. As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences. 4.3.3 Visualization Visualization is important to understanding the mechanism of neural networks. For TBCNNs, we would like to see how the extracted features (af-ter convolution) are further processed by the max pooling layer, and ultimately related to the super-vised task.

To show this, we trace back where the max pooling layer X  X  features come from. For each di-mension, the pooling layer chooses the maximum value from the nodes that are pooled to it. Thus, we can count the fraction in which a node X  X  fea-tures are gathered by pooling. Intuitively, if a node X  X  features are more related to the task, the fraction tends to be larger, and vice versa. Figure 5 illustrates an example processed by d-we applied global pooling because information tracing is more sensible with one pooling slot. As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest. The 2-layer windows correspond-ing to  X  visual will impress viewers , X   X  the stunning dreamlike visual , X  say, are discriminative to the sentence X  X  sentiment. Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling. On the other hand, words like the , will , even are known as stop words (Fox, 1989). They are mostly noninformative for sen-timent; hence, no (or minimal) features are gath-ered. Such results are consistent with human intu-ition.

We further observe that tree-based convolution does integrate information of different words in the window. For example, the word stunning ap-pears in two windows: ( a ) the window  X  stunning  X  itself, and ( b ) the window of  X  the stunning dream-like visual , X  with root node visual , stunning acting as a child. We see that Window b is more rel-evant to the ultimate sentiment than Window a , with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment. In fact, Window a has a larger fraction than the sum of its children X  X  (the windows of  X  the , X   X  stunning , X  and  X  dreamlike  X ). In this paper, we proposed a novel neural discrim-inative sentence model based on sentence parsing structures. Our model can be built upon either constituency trees (denoted as c-TBCNN) or de-pendency trees (d-TBCNN).

Both variants have achieved high performance in sentiment analysis and question classification. d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous state-of-the-art results in both tasks. The results show that tree-based convolution can capture sentences X  structural information effectively, which is useful for sentence modeling.
 This research is supported by the National Basic Research Program of China (the 973 Program) un-der Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301.

