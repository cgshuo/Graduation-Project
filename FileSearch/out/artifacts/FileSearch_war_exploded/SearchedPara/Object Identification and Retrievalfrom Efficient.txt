 The primary difference between text and non-text IR is that text IR attempts to retrieve relevant documents based on  X  X emantic X  content whereas traditional non-text IR (e.g., content based image retrieval (CBIR)) attempts to retrieve images based on  X  X yntactic X  (i.e. low-level) features. If we considered, for the sake of illustration, that an image is analogous to a printed page of a document, on such an analogy, traditional CBIR which is feature based, would roughly be similar to retrieving text documents based on their font size, their layout, the colour of the ink, etc. (i.e., physical characteristics of the document) rather than on their meaningful content.
 image to retrieve images, using non-image (usually text) to retrieve images, or using image to retrieve non-images (information in general). Traditional CBIR is of the first variety, annotated image banks and Google are of the second, and there are only a few efforts in the third. In this paper we explore the possibility of non-image retrieval based on the content of images. Such an approach roughly requires an object identification phase to generate 1 the semantic content followed by whatever suitable actions based on those semantics.
 are particularly interested in two of them. The first is for homeland security or image monitoring in general. In our discussions with intelligence agencies, they tell us that they need a way to filter images in the same way that text is filtered. Traditional CBIR does not work for their scenario which needs to work at the semantic object level and not at the syntactic feature level. Another strong reason is that it is easy to give a codeword to replace a name or definite description. So terrorists may use FOO in email and chat to refer to, say, the Subic Bay Naval Base thus defeating keyword spotting algorithms, but if they were to exchange an image, it would be very difficult to code it. Note that they cannot just encrypt their conversations or images since encrypted data is a red flag in monitoring scenarios.
 increasing prevalence of cameras on mobile phones. During an industry panel at the Consumer Electronics Show in Jan 2005 2 it was estimated that 700 million mobile handsets will be sold in 2005 and 2/3 of them will have cameras. A significant number of pictures taken on such cameras are likely to be  X  X  X hrow-away X  X  images, i.e., pictures taken to serve a function and which has no value once that function is served. Scenarios mooted include taking a picture of a dress to get an opinion from a friend, or as an illustration to a message. But the scenario which we are interested in, is taking a picture to find out more information. So a tourist takes a picture of an unknown landmark, sends it to a server, and gets back useful information. Or a health-conscious consumer takes a picture of his dinner, sends it to a server and gets back nutritional information. work including the task, the indexing and retrieval paradigm, a unique image dataset, and the requirement for fast query processing. Then we describe im-portant applications related to the object identification task followed by related approaches. Section 4 is devoted to the description of the current prototype on mobile image-based tourist directory. Our experimental evaluation on our unique STOIC dataset is given in Section 5. There are four key aspects in our work. First, we look at object identification as an important genre of image search. Second, images are used as intermediate means to retrieve information about an object or location. Third, we introduce a new type of image dataset with associated queries and relevance judgments. Con-ventional image datasets are not designed or evaluated at the semantic level. Last but not least, we show that simple image feature matching is sufficient for good object identification if we provide a sufficient image set for object description. Such efficient techniques are necessary for very large scale critical applications such as homeland security monitoring or the limited processing capacity of the ubiquitous camera phones. 2.1 Object Identification The object identification task may be described as follows: given an image, de-termine the referent 3 for the most salient object in the image. For example, all three images in Figure 1 are of the same object, albeit from different perspec-tives, scales, and colour. The referent is the Merlion statue in One Fullerton in Singapore. The most salient object in the image may be the image in its entirety, e.g., the skyline image in Figure 2 but which also includes the Merlion in it. based on the referent of the object can be activated. For example, you may be a tourist looking at the Merlion but not knowing what it is. You snap a picture using your mobile phone and send to a Tourist Information server (via multimedia messaging). The system identifies the object as the Merlion and sends back information about it. The link to the information is through what the object is and not what the object looks like . We describe such a system prototype known as Snap2Tell below.
 detection in images. This is a more constrained task as there is an implicit normalization of the face. However there are also clear limitations, e.g., a picture of the back of a person X  X  heard is not a candidate for face recognition. More generally for an object, however, we should be able to identify it from any angle. This paper will not be considering face recognition but landmarks recognition. 2.2 Image Set as Index In our image-to-information task, images that capture varying appearances of an object are used as an intermediate means in indexing and matching for retriev-ing the final information for a given query image. That is, in our mobile tourist information application, we index text and audio description of landmarks by a set of images of this landmark. We know that content access using image is still a difficult task for a computer, because low level features are often not enough to describe the intrinsic semantics depicted in an image content. Some meaningful image annotation can be automatically obtained like in [1], but they are only useful for categorizing a set of images by a few image content description (sand, sea, sky) and not very useful for the selection of one particular building, or sculp-ture. To overcome this problem, we propose to describe one item image using a set of representative images that are taken with different viewing perspective, distances, and lighting conditions.
 information like synonyms or ontology or linguistic features. (Ex: in answering definitional question [9], one can use prototypal linguistic expressions). Long texts are better retrieved because of augmented probability of world concur-rency related to the query. In our case, we think that increasing the amount of related relevant images to an object in the database will improve the chance of identifying this object. 2.3 The STOIC Dataset Traditional CBIR efforts have often started from fairly arbitrary collection of im-ages, i.e., those which are easily available to the researchers, rather than designed specifically for research. The Singapore Tourist Object Identification Collection (STOIC) dataset is designed to explore new possibilities in image search, specif-ically in the genre of object identification from images.
 or of tourist spots in Singapore. There are 120 different spots 5 .Figure3shows the current distribution of number of spots for a given set size of index images. We can see that a majority of scenes (43) have an image index set between 5 and 9, the rest have various amount of images up to 190. Images are taken with eight different imaging devices, with different resolutions, from different perspectives, at different times, under different weather conditions, and by different people. Imaging devices include low and high end digital cameras, PDA cameras and phone cameras. In particular, the images taken by the PDA and phones are of much lower resolution, often colour shifted, with poor contrast and uneven saturation. Each image is tagged with metadata to identify the referent and to provide context information for experimentation. Context information includes location (GPS and phone cell id), author, date, and device.
 the query set are images of the tourist spots or objects in the tourist spots. Each image in the query set has an unambiguous referent.
 success of a retrieval or a match of the query image. In particular, the idea of a ranked list (with traditional precision and recall measures) does not help at all (see 5.1). Instead, we have to use the following evaluations: strict for a given query image, if the retrieved first image has the same referent, loose for a given query image and a ranked list of retrieved images, what is Assessments of relevance, i.e., that an image has the same referent as a query image, were done by several assessors. The operational criteria for saliency was according to the intuitions of assessors. Thus if a Merlion image was the query, it would match the image in Figure 1 but not Figure 2 even if few pixels represent it in Figure 2. The complete test collection, comprising the STOIC dataset, queries and relevance judgments, are freely available at http://ipal.imag.fr/snap2tell/. 2.4 Efficient Image Processing We are motivated in this research by the applications mentioned above. They have the same requirement for efficient image processing. There is still a limita-tion on the carrier bandwidth to transfer images taken on mobile phones. Ideally, the image processing should take place on the phone and only a proxy (e.g., im-age feature vectors) be sent to the server. Given the low compute capability on the phone, this requires that only simple image processing techniques can be applied if we are to maintain a realistic response time. Thus a program which can do scene/object identification very efficiently is needed.
 more computationally efficient. We compensate for the simplicity by loading the server side (the matching engine) with many more examples of the objects to be recognized. Our hypothesis is that this compensation works sufficiently well for the object identification task. The touring machine [2] is an example of augmented reality in urban environ-ment. This is an early version of digital help to orientation and access to infor-mation with mobile devices. The system Infoscope [3] is a good example of what can be Multimedia Mobile Information Retrieval. This is mainly an information augmentation system for foreign travelers, which superimposes new information like text translation into an actual picture of a scene. The system is composed by a PDA client associated with GPS for location, liked with a phone for commu-nication. The communication with a server is required due to processing power limitations of small devices like PDA. The second application they proposed, In-formation Augmentation in the City , add information to an actual picture such as details about a flat for rental house hunting.
 is already possible with the GPS devices or the GSM cellular network infra-structure. However, knowing the location of a mobile phone user is not sufficient to determine what he or she is interested in (or looking at). The location-based information certainly helps to refine the user X  X  context, but fails to capture his or her intention. Hence, image-based query is complementary to the context localization information.
 device used is a PDA system connected to internet through WLAN. It supposes that this wireless access point is installed in the area in which the system is going to work. The system includes an iPAQ 3870, a NexiCam PDA camera, an orientation sensor, and a GPS receiver. The position detection is ensured by a GPS attached to the PDA. However, the direction and tilt sensor is connected to the PDA via a laptop computer due to technical difficulty. The camera is integrated into the communication device and localization is provided by the telecommunication operator.
 gether with GPS and orientation data are sent to a server. The server then runs the 3DMax program to generate a reference image from the same position and angle in a 3D model built in advance based on the GPS and orientation data. The matching is performed using detected line features. Only one building model has been constructed and tested in the paper though color segmentation has been explored for future experimentation.
 PDA. In Snap2Tell, we have chosen a camera mobile phone which is a lighter and more ubiquitous device. The camera is integrated into the communication device and localization is provided by the telecommunication operator. Moreover, our approach of scene recognition is different. Instead of unnatural matching between a real image and a synthesized image from 3D model, our server will match the query image with different images of a scen e, taken using different angles and positions. We think that 3D model construction is costly and not applicable to all kinds of scenes. The PDA system [4] requires a GPS device, orientation sensor, and WLAN connection. We think this solution is not realistic. MMS as the communication infrastructure. However the image database was constructed from 12 , 000 web-crawled images where the qualities are difficult to control. The 50 test query images were centered around only 3 selected locations. The evaluation was still based on conventional image retrieval paradigm using the percentage of attempts their test subjects found at least one similar image among the first 16 retrieved images. In Snap2Tell, we consider a more com-prehensive set of locations for both database and queries. The evaluation was carried out using object identification paradigm with the use of contextual cues such as location priming and with investigation into the effect of poor quality query images produced by mobile devices. Imagine you are at a tourist spot looking at a beautiful lake or interesting monu-ment. Instead of searching through your travel guide books to learn more about the scene, you snap a picture of the scene using your camera phone and send it to a service provider. Short time after, you receive an audio clip or a text message that provides you more information about the scene. You can continue to enjoy the scene while your fingers carry out this information retrieval task. As the saying goes  X  X  picture is worth thousand words X , a tourist can forget about the hassle of looking up scene description in a travel guide that distracts him/her from enjoying the scene or recalling the right name for the scene (as-suming he/she knows what the scene is) to access a text-driven information directory. Moreover the charging of the on-demand service is more fine-grained and hence can tailor to the need of each tourist. Service providers can charge a fee for using this fun, easy-to-use and convenient picture-driven information directory, independent of the MMS charges. 4.1 System Architecture The Snap2Tell framework is realized as a typical three-tier client/server archi-tecture. The client is a mobile phone with built-in camera that supports MMS, and GPRS such as the Nokia 7650 model used in our development and test. With the camera phone, a user can launch the Snap2Tell application to send a request to the application server. The request is a picture of a real scene or object that information is sought.
 formation from the mobile network operator. With the location identified, the Snap2Tell server sends a SQL query to the database to retrieve the image meta-data for the scenes related to the location. The image meta-data of the query image is extracted and compared with image meta-data of the scenes by image matching algorithm. If the best matching score is above a certain threshold, scene descriptions of this best matched image is extracted from the scene database. Otherwise, a no match situation has occurred.
 (powered by Symbian OS v6.1), and is one of the earliest all-in-one device that combines mobile phone, digital camera and PDA functions. Figure 4 displays a sequence of screen shots for a running Snap2Tell client which is written in C++ programming language. Following a top-down, left-to-right order, the first three screen shots shows the invocation of the Snap2Tell application on Nokia 7650 phone. After the Snap2Tell application is active, the user can start the camera to take a picture or open an existing image stored on the phone to used as the query as shown in the fourth screen shot. I n this illustration, the user has chosen to select a stored image  X  X upremeCourt-16.jpg X  as query (fifth screen shot on the second row) which is displayed in the sixth screen shot. Note that if the user has decided to take a picture instead, the video camera will be turned on to allow the user see what the camera is focused at.
 the  X  X et Description X  option to initiate a query. As described above, the query will be sent as a MMS to the Snap2Tell application server. Once a MMS reply is received from the Snap2Tell application server, the user can play the MMS. As illustrated in the last screen shot in Figure 4, the description is shown as text or/and audio. The Snap2Tell application server is the functional core of the system and is developed in Java. 4.2 Scene Database Using Singapore in our tests, we have set up an original data set of image and descriptions which is a subset of the STOIC dataset. We have divided the map into zones. A zone includes several locations, each of which may contain a num-ber of scenes. A scene is characterized by images taken from different viewpoints, distances, and possibly lighting conditi ons. Besides a location ID and image ex-amples, a scene is associated with a text description, an audio description which is send to the user as answer to his query. Figure 5 shows relationships among zone, location, scene, and category with examples. For Location 11: Chinatown in Zone 4, two scenes  X  X hinatown X  and  X  X hian Hock Keng Temple X  are shown. Three scenes labeled as  X  X ndian National Monument X ,  X  X upreme Court X , and  X  X ir Raffles Statue X  are located in Location 14 of Zone 5. As the STOIC Dataset is an ongoing task, we have set up a first experimentation with only 535 images, 2 devices (cameras) and 103 locations. This first version of STIOC gives us the opportunity to test the usefulness of location based context and impact of simple image structure using blocks. For this initial study, we have adopted color histograms [7] to characterize and index the images. They are known to be invariant to translation and rotation about the viewing axis and change only slowly under change of angle of view, change of scale, and occlusion. a trade-off between content symmetry and spatial specificity. If we want images of similar semantics with different spatial arrangement (e.g. mirror images) to be treated as similar, we can have histograms of larger blocks (i.e. the extreme case will be a single block that covers the entire image, similar to the effect of a global histogram). However, spatial locations are sometimes important for discriminating more localized objects. Then local histograms will provide good sensitivity to spatial specificity. Furthermore, we can attach different weights to the blocks to emphasize the focus of attention: in our case we have emphasized the center. That is, the similarity  X  between a query q (with m local blocks Z j ) and an image x (with m local blocks X j ) is defined as: where  X  j are weights, and  X  ( Z j ,X j ) is the similarity between two image blocks defined as Note that this similarity measure is equivalent to histogram intersection [7] be-tween histograms H i ( Z j )and H i ( X j ).
 space as it is found to be perceptually more uniform than the standard RGB color space [5]. The number of bins of a color histogram is b 3 where b is the number of equal intervals in each of the H, S, and V dimensions. We have tested from 2 to 14 bins. We have also partitioned an image in identical blocks in both X-Y dimensions (i.e. K  X  K grid). When two images are compared, we only compare the two local histograms of the corresponding blocks (Eq. (2)) with equal weights (eq. (1)). We have tested a maximum of K = 8 in each dimension. That is, images are split into 64 blocks, and 64 histograms are computed in this case. Note that K = 1 refers to global color histogram.
 methodology for evaluation. In all tests, each image of the test collection is considered as a query and is removed from the collection. This image is tested for histogram similarity matching against the rest of the collection: we compute histogram similarity between this query image and all other images, and we sort the similarities in descending order. The strict mean precision is the percentage of success in object identification for the whole collection. 5.1 Influence of Bins and Blocks Results of this first experimentation is shown in Figure 6. In this figure we see the percentage of strict identification. A strict identification arises when the most similar image to the query image belongs to the same scene. In that case, the system has recognized the correct scene.
 creases and hence the quality of the results increases too. At some point, more histogram bins may result in mismatch of the bins when slight change in the color distribution can cause shifts of pixel counts in adjacent bins. Without using any location information, the overall best precision is found at 11 bins using 3  X  3 blocks with a mean of 73 . 4% precision. Using block provides some improvement which shows the importance of image structure. We can see that 3  X  3 block seems to be the minimum for a noticeable improvement. More blocks leads to more computation and not much improvement. Also, after 6 bins, the improvement is to very noticeable.
 an image retrieval task like in traditional CBIR. It is in fact the ratio of images in the correct scene, on the total of images retrieved when all images of this scene are retrieved. The distribution of best performance is not the same: figures are much lower. Excluding low bins and low blocks number, we obtain precision between 18% and 25%. The distribution of the results depends on the content of actual images. For some scenes, the image set is very homogeneous, and the whole set is retrieved at the top of the list. As for other scenes, the set is very heterogeneous because the pictures are taken differently with varying distances or view angles, hence this image set is less consistent and reduce the precision at full recall. Clearly in these situations, the color histogram approach is not discriminative enough. But, even if this measure is very common in Information Retrieval, in our case, it is not important at all because we only need that the first image retrieved to be correct to have a correct answer. Moreover, if we want our system to work in a lot of situations, the picture set that describes an object has to cover as much different situations as possible. For example, we can plan to take 4 or 8 pictures in circle around on e landmark, and select also different circle sizes: the more different images we have of the object, the more chances we have to be close to an image query. In that sense, a good indexing image set is expected to have very low recall at full precision because the variety of this data set is related to its quality. We really expect in the image database, to have a lot of different images for the same object in order to maximize the chance to find the correct image and then to recognize the landmark. 5.2 Influence of Context In this empirical study, we also want to investigate the effect of context as location information, or the position of the device, for reducing the search space and performance improvement.
 is, we select the best matching image from the images that share the same context as the query image. Contexts are: zone or location information (see 5.2). This figure shows only the mean precision at 3  X  3 blocks. Clearly, we notice the enhancement when context information is used. We reach 82 . 4%, and this follows intuition. As before, partitioning the images into smaller blocks for matching is useful, and we notice little variation with context information. Using zone the overall best precision it at 79 . 6% with 5  X  5 blocks and 14 bins; using location, the overall best precision is at 82 . 6% with 4  X  4 blocks and 11 bins. 5.3 Influence of Structure The role of the query image is to indicate the information need. Hence we make the hypothesis the center of the image should be more relevant than the edges. Thus we propose to weight blocks according to their position at the matching process.
 to the edge of the pictures, using two weightings: from 1 (edge) to 2 (center) and from 1 to 5. The results shows that for our collection, doubling the center with 3  X  3 blocs with 11 bins, is the best choice (74 . 9%) compared to the table 6. We have the same behavior using contextual information. Hence, we have a small improvement using weighted blocks. 5.4 Influence of Set Size For this test, we enlarge the STOIC collection to reach 1650 images with 68 scenes and 8 cameras. The new set of cameras has been chosen so to reflect diversity of camera quality in term of pixel size, sharpness and color consistency. With this new set we imposed a minimum of 5 images for each scene, so we can have at least 4 images do describe an object, during the leave one out process. We notice a positive change in Figure 9. The curves labeled as  X  X mall X  refers as the small set of image and is a partial copy of the curves in Figure 6. Thus, it tends to prove the variety hypothesis for this sort of dataset: the quality in term of strict retrieval is related to the variety of the set associated to objects. previous value of 18  X  25%. Thus, it is worth to mention that the full recall is reduced when the diversity of the image set associated to the object increases. Of course, this reduction depends on the matching algorithm itself, but even with a very good image matching process, if the side of one landmark is very different for one other side, it could be impossible to link semantically these two images to the same object only on a visual basis. 5.5 Influence of Top Section Wecanviewoursectionofthetopfirstimageasak-NN(nearestneighbour) classifier with k=1, and examine the behaviour of our collection when we increase the k value. Except for the worth situation (no blocs and only 2 bins), we never notice any improvement when increasing this k value. Figure 10 shows the best and worst situation with every k from 1 to 30. Notice that in case of identical value for two scenes, we choose the set of images with the overall closer total added distance.
 decrease. One possible explanation of our results is the too small size of image set per scene. 5.6 Influence of Device Quality With 8 different cameras, we can test the influence of the device itself. The results 6 in table 1 are computed by eliminating from the dataset all images that has been taken by the same device. In that way we test the compatibility of one device among all other devices on the whole image collection.
 imum) in the strict precision. This can be explained by shift of device X  X  color characteristics. The size of the CCD in mega pixel or the year of marketing, does not seem to be very important. When one looks at actual images, we can clearly notice that the Pocket PC and the phone produce very low quality im-ages. Hence we are surprised by the relative good results of the hand phone compared to those of the Pocket PC. It is also surprising that the best results belong to a chip camera (camera 1). We can conclude that we will need either device dependent color calibration or low lever feature extraction less sensitive to camera characteristics. Concerning our Snap2Tell prototype, our approach deals with real situation and real access device in order to measure the feasibility of such a system. It turns out that we have stretched the limit of currently available wearable technology, but we are convinced that ubiquitous computing is going to have rapid development in the very near future. We are also convinced that mobile information access, in particular context-aware image-based information access, will be a hot research and development topic.
 us that simple matching, based on color histograms, combined with localization information, seems powerful enough to solve this object identification problem thought image matching mainly, because of task we have: retrieving among a set of images describing one object, the only one that is closed to image query. It is not a usual IR querying task, and the poor value of the precision at full recall is not that significant in this case. We also know that even if the prototype is functional and that these results are encouraging, results are not good and stable enough to be used for a real commercial product and relying only on color distribution is for sure too weak in many other real situations as it is shown by our test on the 8 different cameras. We are about to investigate this aspect and adding more discriminative image features extractions. The complete test collection is freely available at http://ipal.imag.fr/snap2tell/. This work has been done under a IPAL CNRS France and I2R A-STAR Singapore common funded research action.

