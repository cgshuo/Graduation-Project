 Traditionally, information retrieval systems aim to maximize the number of relevant documents returned to a user within some win-dow of the top. For that goal, the probability ranking principle , which ranks documents in decreasing order of probability of rel-evance, is provably optimal. However, there are many scenarios in which that ranking does not optimize for the user X  X  information need. One example is when the user would be satisfied with some limited number of relevant documents, rather than needing all rel-evant documents. We show that in such a scenario, an attempt to return many relevant documents can actually reduce the chances of finding any relevant documents.

We consider a number of information retrieval metrics from the literature, including the rank of the first relevant result, the %no metric that penalizes a system only for retrieving no relevant re-sults near the top, and the diversity of retrieved results when queries have multiple interpretations. We observe that given a probabilistic model of relevance, it is appropriate to rank so as to directly op-timize these metrics in expectation. While doing so may be com-putationally intractable, we show that a simple greedy optimiza-tion algorithm that approximately optimizes the given objectives produces rankings for TREC queries that outperform the standard approach based on the probability ranking principle.
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval X  Retrieval Mod-els General Terms: Algorithms Keywords: Information Retrieval, Formal Models, Machine Learn-ing, Subtopic Retrieval It is a common rule of thumb in that the Probability Ranking Principle (PRP) is  X  X ptimal. X  Under reasonable assumptions, one can prove that ranking documents in descending order by their probability of relevance yields the maximum expected number of relevant documents, and thus maximizes the expected values of the well known precision and recall metrics [14].
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.

Simply returning as many relevant documents as possible, how-ever, is not the only possible goal. For example, since a single relevant result often provides  X  X he answer X  to the user X  X  query, we might be concerned only with whether our system returns any rel-evant results near the top. This is plausible for question answer-ing, or for finding a homepage. It also captures a notion of  X  X are minimum success X  that can be meaningful for hard queries. The TREC robust track, focusing on such hard queries, defines and uses the %no metric X  X he fraction of test queries on which a system re-turned no relevant results in the top ten [20].

As we shall argue below, the probability ranking principle is not optimal in such a case. For if the system X  X  model of relevance is wrong, it will be wrong over and over again, returning an entire list of irrelevant documents X  X ne might say that the expected num-ber of relevant documents is large, but the variance in the outcome is also high. Similarly, it is common wisdom that some queries such as  X  X rojan horse X  can express multiple, distinct information needs X  X bout a type of malware, a Trojan war artifact, or other mi-nor usages. A PRP-based approach may choose one  X  X ost likely X  interpretation of the query, and provide results that satisfy only that interpretation, leaving users with rarer interpretations unsatisfied.
Given that we have stated a clear metric (success in finding at least one relevant document) we argue that under a probabilistic model of document relevance, there is a particularly natural ap-proach to designing a retrieval algorithm for it X  X amely, to rank documents so as to optimize the expected value of the metric .In particular, we should rank so as to maximize the probability of find-ing a relevant document among the top n . While exactly optimizing this quantity is NP-hard, we derive a greedy heuristic for approx-imately optimizing it. Intriguingly, our greedy algorithm can be seen as a kind of blind negative relevance feedback, in which we fill each position in the ranking by assuming that all previous doc-uments in the ranking are not relevant.

We demonstrate that our approach is effective in practice. We evaluate the performance of our greedy algorithm on queries from various TREC corpora. We show that it retrieves at least one rel-evant document more often than the traditional ranking (with sta-tistical significance). We give special attention to the robust track, where one of the goals is to minimize the chance of returning no relevant results, and show that our algorithm does well.

In addition to the robust track X  X  %no metric, we consider a num-ber of other standard metrics from the literature. For example, we might be interested in how far down the ranked result list we must go to find the first relevant document. The search length (SL) [4] and reciprocal rank (RR) [15] metrics measure this quantity in dif-ferent ways. On the other hand, if we believe that the results of a query may have different  X  X ubtopics X  (facets of the query) or that multiple queriers might have different relevance judgments, we might want to ensure that a result set offers good  X  X overage X  of the different possibilities. The instance recall metric [9, 21] measures the number of different subtopics or queriers who are satisfied by a given result set.

One can apply our approach, of ranking to optimize the expected value of the metric, to all of these metrics. For each metric the ex-act optimization problem is different, and in each case it appears intractable. In a fortunate coincidence, however, our greedy al-gorithm for the %no metric is also a natural greedy algorithm for all of the metrics listed above. We report results for all of these metrics over TREC corpora, and show that our greedy algorithm outperforms the PRP baseline on them. Conversely, our analysis leads us to the possibly surprising conclusion that PRP ranking is not optimal for the heavily used mean average precision metric.
We also explore the goal of perfect precision , where the objec-tive is to not retrieve any irrelevant documents. We show how blind positive relevance feedback arises naturally as a greedy heuristic for achieving this goal. To tie together the disparate goals of nonzero precision (at least one relevant document) and perfect precision, we introduce k -call , a class of metrics that ranges smoothly between the two extremes. We argue that it captures a desire to trade  X  X ual-ity X  for  X  X iversity, X  and discuss the application of our approach to these metrics.

The broad applicability of using probabilistic models to optimize for specific metrics suggests a general principle, which we call the Expected Metric Principle (EMP). The EMP states that, in a proba-bilistic context, one should directly optimize for the expected value of the metric of interest. The PRP is a special case of the EMP for the precision and recall metrics.

One possible criticism of the EMP is that it  X  X eaches to the test X  X  it encourages the algorithm to do well only on the evaluation crite-rion. This has led to much gaming of the SPECFP benchmarks for numerical computation, for example, as companies incorporated special purpose code for solving only the SPECFP instances. But this gaming is a consequence of the evaluation criterion failing to accurately measure what is wanted out of the system. We argue that the metrics that we study are truly the metrics that matter in certain cases, so that algorithms optimized for them are desirable.
Intriguingly, while explicitly aiming only to find one relevant document, we demonstrate the unplanned effect of increasing the diversity of documents at the top. This highlights one way in which seeking one relevant document is different from seeking many .If a query has multiple interpretations (as was the case for  X  X rojan horse X  above), or if there are multiple subtopics, it may be hard to decide which is the proper one. PRP ranking puts all its eggs in one basket X  X t identifies the most likely interpretation, and finds many results for that one. But an algorithm that needs only one rel-evant document can do better by retrieving one document for each case, thus satisfying the goal whichever interpretation or subtopic is desired.

Recent work [3, 21] has developed heuristics for increasing di-versity for this precise purpose, but our approach appears to be the first in which diversity arises automatically as a consequence of the objective function rather than being manually optimized as a proxy for the true objective of interest. As a benefit, there are no new  X  X arameter knobs, X  beyond those already used in probabilis-tic document models, that must be tweaked in order to tune our algorithms.

We give anecdotal evidence that our approach promotes diver-sity by looking at ambiguous queries on the Google search engine. We observe that while the probability ranking principle tends to re-turn documents only relevant to the  X  X ajority vote X  meaning of the query, our approach satisfies that meaning but simultaneously re-turns results relevant to other, rarer meanings of the query. We fol-low with more quantitative evidence based on TREC results with multiple raters, where our approach satisfies more raters than PRP, and TREC results with subtopic annotations, where our approach retrieves more subtopics than PRP.
Our discussion of related work splits into three categories: defi-nitions of and motivations for retrieval metrics, algorithms for op-timizing those metrics, and approaches to diversity in result sets.
The main metric we examine is essentially the %no metric, which is studied by Voorhees [19]. She finds that the metric was less sta-ble than traditional measures. However, this instability does not affect our ability to probabilistically model and optimize for it.
Cooper [4], who introduces the search length metric, argues that trying to retrieve as many documents as possible is not necessar-ily the appropriate objective for meeting user information need. Cooper explicitly divides an information request into a  X  X elevance description X  (i.e., a query) and a quantification that specifies the de-sired number of relevant results. He defines a class of search length metrics, which measure the number of irrelevant documents a user would have to examine before finding a  X  X ufficient X  number of rel-evant documents. Our paper focuses on the case of  X  X ne document sufficiency, X  though we also touch on the  X  k document sufficiency X  case when we define k -call later in the paper.

Shah and Croft [15] explore the problem of high accuracy re-trieval , where the objective is to have high precision in the top doc-ument ranks. They argue that mean reciprocal rank is a useful met-ric for this scenario. As previously mentioned, we also demonstrate the applicability of our heuristics to MRR.
Our approach fits within a general risk minimization framework propounded by Zhai and Lafferty [22]. They observed that one could define an arbitrary numeric loss function over possible re-turned documents rankings, which measures how unhappy the user is with that set. The loss function generally depends on unavailable knowledge about the relevance of particular documents. But given a probabilistic model, one can compute an expected value for the loss function, or expected loss , and return a result that optimizes the expected loss. Much of our paper deals with the loss function that is (say) -1 when the top ten results contain a relevant document (indicating a positive satisfaction) and 0 when it does not.
Like us, Gao et al. [6] follow the approach of letting the met-ric directly drive the retrieval algorithm. However, instead of us-ing a document model from which the optimal algorithm can be determined through analysis, they train a system to weight doc-ument features so as to optimize the metric (average precision in their case) and show that such training leads to an algorithm that achieves good performance on the metric with new queries.
Bookstein [1] proposes a sequential learning retrieval system that bears some similarity to ours. He argues that a retrieval system should sequentially select documents according to the probability of relevance conditioned on the selection and relevance of previ-ous documents (essentially relevance feedback). However, his pro-cedure requires explicit user feedback after each result retrieved, whereas our system proposes an objective function and then uses a sequential document selection algorithm to heuristically optimize that objective without further user input.
Our greedy algorithm for achieving perfect precision seems re-lated to pseudo-relevance feedback , an approach commonly used in the literature to improve overall retrieval performance on stan-dard metrics [2, 5]. Our metric for retrieving at least one relevant document, on the other hand, produces an algorithm that appears to be doing negative pseudo-relevance feedback. In either case, rather than feeding back all of the top documents at once, we progres-sively feed back more and more top relevant documents in selecting latter-ranked documents.
In their subtopic retrieval work, Zhai et al. [21] posit, as we do, that there may be more than one meaningful interpretation of a query. They assume that a query may have different subtopic in-terpretations, and reorder the results so that the top includes some results from each subtopic. Their system involves separate consid-eration of novelty and redundancy in a result set, which are then combined via a cost function. Our approach, in contrast, aims di-rectly at the goal of maximizing the chances that the user will get an answer to  X  X heir X  interpretation of the query. Aiming directly arguably is beneficial in that it reduces the number of system ele-ments, such as novelty and redundancy, whose interactions we have to design and tweak. Conversely, it is possible that by modeling novelty and redundancy richly, the Zhai et al. model can outper-form our simpler one.

The work of Zhai et al. is in turn based on Carbonell and Gold-stein [3] X  X  maximum marginal relevance (MMR) ranking function. They argue for the value of diversity or  X  X elevant novelty X  in the results of a query, and propose MMR as an objective that intro-duces such diversity in a ranked result set. Our greedy heuristic for optimizing the  X  X ne relevant document X  objective simplifies to a computation that bears some relation to the MMR computation. However, MMR is advanced as a heuristic algorithm for reducing redundancy and achieving the hard-to-define notion of diversity , whichinturnisbelievedtobe related to the desired objective. Our ranking algorithm arises naturally from the application of a simple greedy heuristic to the optimization of a clear, natural, formally de-fined objective function. In addition, while the iterative greedy ap-proach is implicit in the definition of MMR, our greedy approach is simply one heuristic applied to optimizing our well-defined objec-tive function; we expect that better optimization algorithms such as local search would yield improved values for our objective, which should translate into improved retrieval performance.

Our goal of retrieving one relevant document, and its inherent diversifying tendency, bears superficial similarity to clustering, in the sense that clustering is also used as an approach to quickly cover a diverse range of query interpretations [10]. Our technique sidesteps the need for clustering interface machinery, utilizing the standard ranked list of documents instead. Furthermore, we aim to directly optimize the probability that a user finds a relevant docu-ment, rather than going through the intermediate notion of separate document clusters. Again, this avoids the need to define and tweak new algorithmic parameters.
We consider several metrics in this paper. Search length [4] (cf. section 2) is the rank of the first relevant document in a result list minus one, and reciprocal rank [15] is one over the rank of the first relevant result. With multiple queries we can take the mean of both quantities, yielding the mean search length (MSL) and mean reciprocal rank (MRR) metrics.

Introduced for the TREC robust track [20], the %no metric mea-sures the percentage of queries for which no relevant documents are retrieved. Put another way, it assigns value to any result set containing at least one relevant document.

In line with Cooper X  X  [4] notion of quantification, we generalize the %no metric with a new class of binary metrics under the name k -call at n . Given a ranked list, k -call at n is one if at least k of the top n documents returned by the retrieval system for the given query are deemed relevant. Otherwise, k -call at rank n is zero. In particular, 1 -call is one if a relevant document is found and zero otherwise. Averaging over multiple queries yields mean 1 -call , which is just one minus the %no metric used in the TREC robust track. On the other hand, n -call at n is a measure of perfect precision : returning only relevant documents. Varying k between n and 1 offers a way to express  X  X isk tolerance X : do we wish to aim for many relevant documents and take the chance of finding none, or will we settle for fewer documents if it improves our chances of finding them?
When we explicitly have a notion of different subtopics of a query, and of different documents covering different subtopics, we can define instance recall [9] at rank n (also called S-recall [21]) as the number of unique subtopics covered by the first n results, divided by the total number of subtopics.
Our work is rooted in standard Bayesian information retrieval techniques [11, 16]. In this approach, we assume that there are two distinct probability distributions that generate the relevant and irrel-evant documents respectively. Let d be a document, and r abinary variable indicating the relevance of that document. The probabil-ity ranking principle suggests that documents in a corpus should be ranked by Pr[ r | d ]  X  X hat is, the likelihood that a document was generated by the relevant distribution. An application of Bayes X  Rule followed by a monotonic transformation gives us a ranking value for documents: Here, Pr[ d | r ] and Pr[ d | X  r ] represent respectively the prob-abilities that the relevant and irrelevant distributions assign to the document.

We thus need to compute Pr[ d | r ] and Pr[ d | X  r ] . In our paper we emphasize the objective function, rather than the mod-eling issues associated with Bayesian retrieval. Therefore we use the familiar and simplistic Na  X   X ve Bayes framework, with multino-mial models as the family of distributions. A document is thus a set of independent draws from a word distribution over the corpus. A multinomial distribution is described by parameters  X  i each term (word) i in the corpus. A document X  X  probability is the product of each of its term X  X  corresponding  X  i , normalized so the distribution sums to one. In our experiments, we used the heuristic of applying a log-transformation to the term frequencies (that is, substituting log(1 + t i ) for t i ), which has been shown in the litera-ture to improve Na  X   X ve Bayes performance for text applications [13]. It remains to determine the parameters  X  i for each distribution. To model the fact that we do not know exactly what terms appear in relevant and irrelevant distributions, we specify a prior probability distribution over the parameters (a distribution over possible docu-ment distributions). The prior reflects our initial beliefs (e.g. that a given  X  i parameter is likely to be small). We proceed to revise our beliefs about those parameters by incorporating observed data. We use a standard Dirichlet prior , centered on the background word distribution over the entire corpus. We then take the user query as  X  X raining data X  X  X  sample from the relevant document distribution that gives evidence about the parameters of that distribution. This evidence leads us to a new posterior estimate of the probability of parameters of the relevant distribution (in particular, one in which the query term X  X  parameters are likely to be large). Given these two distributions, we are able to measure the probabilities that certain sets of documents are relevant or irrelevant. For our baseline PRP model, this is all the training we do. As we will show later, our new EMP-based algorithms will feed back selected corpus documents into the document distributions as additional  X  X raining data. X 
Our use of priors creates a  X  X inkage X  between documents. Al-though each document is assumed to be generated independently from its (relevant or irrelevant) distribution, positing one document to be relevant leads us to believe that the parameters associated with that document X  X  words are stronger in the relevant distribu-tion, which in turn leads us to believe that similar documents are more likely to be relevant.
In many systems, the evaluation metric (e.g., mean reciprocal rank) is different from the objective function used to rank docu-ments (e.g., probability of relevance). The EMP posits that the right objective function is the (expected value of the) evaluation metric itself. Consider optimizing for the k -call at n metric. Since k -call is always 0 or 1, this is equivalent to maximizing the probability that we find k relevant documents among the first n .

Let d i denote the i th document of the ranked result set, and r denote a binary variable indicating that the d i is relevant. Result numbering is 0-based, so the first result is d 0 .

The k =1 version of our objective function is the probability that at least one of the first n relevance variables be true, that is: In general, the objective function for arbitrary k is the probability that at least k documents are relevant, that is: Contrast these objectives with the PRP ranking by Pr[ r | defining a objective in line with the metric, we are explicitly aiming for results that the metric rewards.

Note that our objectives are indifferent to the ordering of docu-ments within the top n . This is to be expected X  X ecause our metric is insensitive to where the relevant results are within the top n ,just that there are enough of them, our objective function will be insen-sitive to the same conditions.

The next two sections discuss the optimization and calculation of the objective function in depth.
Notably, while the PRP objective could be optimized by select-ing each document independently, our new objectives (equations 2 and 3) seem to be more complex, requiring us to consider inter-actions between multiple documents in the result set. It no longer seems possible to judge each document individually. So more com-plex optimization algorithms are needed.

One way to perfectly optimize the k -call at rank n objective func-tion for a corpus of m documents would be to evaluate, for each possible returned sets of n documents, the probability that that set has at least k relevant documents. For any specific set of n doc-uments this evaluation is tractable, but the tremendous number of n distinct subsets make this approach impractical for most rea-sonable values of m and n .

In general, finding the optimum subset is NP-hard. Space lim-itations preclude a full proof, but one can show that by assigning specific weights to the distributions, one can reduce the NP-hard clique problem to optimizing the expected k -call. Since solving our problem would let us solve an NP-hard problem, our problem is NP-hard as well, implying that exactly optimizing our objective is intractable. Therefore we consider a greedy approach that re-duces the search space of result sets.

A greedy algorithm is an algorithm that always selects a locally optimal intermediary to a solution. They tend to be simple ap-proaches that work well in a variety of contexts. A greedy algo-rithm for our problem is to successively select each result of the result set. Consider finding the optimal result set for k -call at rank n . We select the first result by applying the conventional probabil-ity ranking principle. Each result thereafter is selected in sequence. For the i th result, we hold results 1 through i  X  1 to their already selected value, and consider all remaining corpus documents as a possibility for document i . We calculate an expected k -call score for the result set including each such document, and pick the high-est scoring document as the i th result. If i&lt;k , we maximize the i -call score instead as a stepping stone towards maximizing k -call.
Unlike PRP ranking, optimizing our objective function exactly may require knowing both k and n . That is, the set of 10 docu-ments optimizing the odds of getting a relevant document in the top 10 need not contain the 5 documents optimizing the odds of getting a relevant document in the top 5. Thus, it might not be pos-sibly to simultaneously optimize these two quantities. Our greedy heuristic, on the other hand, is not affected by the value of n that we choose; thus, while it may not be returning the best document subset for any particular n , it may arguably be returning a ranking that is reasonably good for all n .

If our goal were to maximize precision and recall, then the nat-ural greedy approach would be to select each successive document to maximize its probability of relevance (independent of the previ-ously selected documents). This is exactly the PRP ranking mech-anism. In this sense, the greedy algorithm we have proposed is a generalization of the greedy algorithm that optimizes for PRP.
In this section we examine how we would use the greedy algo-rithm described previously to actually optimize our objective func-tion. We focus on the k =1 and k = n cases, where the greedy algorithm has a particularly simple instantiation. We also touch on the general case for intermediate values of k .
Consider the case where k =1 . The first result is obtained in the conventional fashion X  X y choosing the document d 0 maximizing Pr[ r 0 | d 0 ] . Having chosen the first document d 0 , we want to select the second document d 1 so as to maximize Pr[ r 0  X  r 1 | This is merely an instantiation of equation 2 with n =2 . We can expand this expression by partitioning the event of interest r into the disjoint events r 0 and r 1  X  X  r 0 :
Pr[ r 0  X  r 1 | d 0 ,d 1 ] where the simplification in the last line follows because r dependent of d 1 . We wish to choose the document d 1 maximizing this quantity. Only one of the three probabilities in the equation depends on d 1 , however, so it is sufficient to maximize that term: A similar analysis shows that we can select the third result by max-imizing Pr[ r 2 | X  r 0 ,  X  r 1 ,d 0 ,d 1 ,d 2 ] . In general, we can select the optimal i th document in the greedy approach by choosing the document d i that maximizes: This expression tells us that for each new result, we should assume that all the past results were irrelevant, and find the document of greatest relevance conditioned on that assumption. This makes in-tuitive sense X  X f a previous document were able to satisfy the user query (i.e., was relevant), then we would not care about what doc-uments were displayed subsequently. Thus we try to select the best new document assuming that all previous documents were irrele-vant. This formula also fits nicely with the Bayesian information retrieval model: the assumption that the previous documents are ir-relevant is incorporated in a straightforward fashion as an update to the probability distribution associated with the irrelevant docu-ments; the relevance probabilities of new documents are then com-puted using that updated irrelevant document distribution.
We can perform a similar greedy-algorithm derivation for the case where k = n . In that case, we find that we should select the i th document according to: Again this is intuitive X  X f we want to maximize the odds of per-fect precision then once we select even one irrelevant document we have failed; thus, we must forge ahead on the assumption that all documents ranked so far are relevant. As in the k =1 case, this leads to a simple update rule for the prior probability distributions.
Because these simplified forms for k =1 and k = n do not involve addition of probabilities, they have the advantage that we can use the ranking value form of Pr[ r | d ] (equation 1) rather than the full Bayesian expansion, just as in PRP. 7.3 1 &lt;k&lt;n
We briefly turn to the more general problem of trying to get an arbitrary k relevant documents among the top n . In this case, our objective be to maximize the probability of having at least k rele-vant documents in the top n . Using the same technique of breaking the objectives into chained conditional probabilities, we can derive a ranking value formulation for each step of the greedy algorithm. For brevity we omit the actual derivation here, and focus the re-mainder of the paper on the k =1 and k = n cases.
We have remarked on two other metrics aimed at  X  X he first rel-evant document X  X  X earch length and reciprocal rank. Using our EMP approach, our goal should be to optimize the expected values of these quantities: minimize expected search length and maximize expected reciprocal rank (note that E [1 /X ] =1 /E [ X ] ,sothese two objective may optimize differently).

Let us consider the expected (over result sets) search length develop a greedy algorithm for it. Suppose that the first i docu-ments in the ranking d 0 ,...,d i  X  1 have been chosen and we wish to greedily select d i . For those events in which there is a relevant document already in the ranking, our choice does not affect the expectation. So, we should choose greedily conditioned on there
Our terminology overloads Cooper X  X  [4] definition of expected search length, which addressed ties in the ranking by randomly or-dering tied results. being no previous document relevant . Subject to this condition, the natural document to choose is the one that has the largest prob-ability of relevance subject to this condition, since this greedily maximizes our chance of  X  X erminating the search X  at document d
In other words, we should choose the document d i that has max-imum probability of relevance subject to no previous document in the list being relevant X  X xactly the same heuristic as we used for optimizing 1-call.

A similar argument shows that our greedy algorithm is also a natural heuristic for optimizing expected reciprocal rank. These observations mean that we can experiment with three metrics for the price of one. Our tables report all three metrics, and demon-strate that our greedy algorithm improves on PRP for all of them.
Now consider the instance recall metric, which measures the number of distinct subtopics retrieved. If a query has t subtopics, then instance recall can be written as ( S 1 + S 2 +  X  X  X  + S S j is an indicator variable that is 1 if a document from the j stance (subtopic) is included in the result set. Our approach calls for maximizing the expectation of this quantity, which (due to lin-earity of expectation) is proportional to greedily optimizes for each S j separately (since S j is simply the 1-call metric for the j th subtopic) and thus for the sum as well.
It is important to note that although we have described a heuristic that is effective for four metrics, they are in fact distinct metrics, and it is conceivable that more sophisticated algorithms could lead to divergent results that optimize one at the expense of the others. On the other hand, it is also conceivable that since these metrics are closely related, there is a ranking that (in expectation) optimizes several or all of them simultaneously.
 We briefly consider one other metric, mean average precision . This is one of the most commonly used metrics for evaluating re-trieval systems. As with the standard precision metric, it is nat-ural to assume that the PRP holds as the (easy) way to optimize it. Note, however, that when there is only one relevant document, average precision simplifies to RR. Our discussion above, which argues that one should  X  X edge X  one X  X  retrieval in order to optimize RR, thus indicates that PRP does not yield the optimum ranking for average precision .
In the introduction, we argued that optimizing 1-call would au-tomatically lead a system to select a more  X  X iverse X  result set. To explore this, we first present results from running our procedures over the top 1000 results returned by Google for two canonically ambiguous queries,  X  X rojan horse X  and  X  X irus. X  We used the titles, summaries, and snippets of Google X  X  results to form a corpus of 1000 documents for each query.

The titles of the top 10 Google results, and the PRP and greedy rerankings, are shown in figure 1. (The titles have been shortened for fit.) Our greedy algorithm was set to optimize for 1-call X  X hat is, the probability of returning one relevant document. Different table cell shadings indicate different broad topic interpretation of a result (e.g., white for computer Trojan horses and various grays for other interpretations). In the  X  X rojan horse X  example, the greedy al-gorithm returns a significantly more diverse set of results in the top 10 (spanning five distinct interpretations) than PRP and the original Google results, which return respectively three and two interpreta-tions. The diversity for  X  X irus X  is also notable; greedy returns the most medical (non-computing) virus results in the top ten, beating PRP. Interestingly, Google does not return any medical virus infor-mation in its top ten, so a user looking for that interpretation would be disappointed by Google.
In this section, we discuss our results with the greedy algorithm on various TREC corpora. We denote the greedy algorithm that op-timizes for (expected) 1-call as 1-greedy. We also look at using the greedy approach to optimize for 10-call (perfect precision), which we denote as 10-greedy. All experiments are done with result sets of size ten. Each corpus was filtered for stop words and stemmed with a Porter stemmer. For both PRP and k -greedy, the query was weighted at one fiftieth of the relevant distribution prior (this was the best weighting for PRP, and we kept it unchanged for 1-greedy). In each case, we ran k -greedy over the top 100 results from PRP. (Generally we found that our algorithms would select from within the top 100 PRP results even when given a choice from the entire corpus.)
Because we did not rank the entire corpus in our results (as doing so would be prohibitively slow), we compute search length only over the first ten results. If there are no relevant results in the top ten positions, we assume a search length of ten. Similarly, we assume a reciprocal rank of zero if a relevant result is not found in the top ten. Therefore our reported MSLs and MRRs are slight underestimates of what their values would be over a full ranking.
 We used the set of ad hoc topics from TREC-1, TREC-2, and TREC-3 to set the weight parameters of our model appropriately. Using the weights we fond, we then ran experiments over the TREC 2004 robust track, TREC-6, 7, 8 interactive tracks, and TREC-4 and TREC-6 ad hoc tracks.
As with any model, our model has a set of tweakable weights that could greatly affect retrieval performance depending on how they are set. For our model, the key weights to consider are the strength of the relevant distribution and irrelevant distribution pri-ors with respect to the strength of the documents that we add to those distributions.
 To tune these weights, we used the corpus from the TREC-1, TREC-2, and TREC-3 ad hoc task, consisting of about 742,000 documents. There were 150 topics for these TRECs (topics 51 through 200).

We find that when the prior weight is well tuned, 1-greedy out-performs PRP on the metrics where it should X  X hat is, 1-call at 10, MRR, and MSL. Similarly, with a well tuned prior weight, 10-greedy outperforms PRP on 10-call. For brevity, we do not report the full weight tuning results in this paper.

Our tuning shows that 1-greedy performs best when the irrele-vant distribution prior is set very low, to less than the weight of one document, whereas 10-greedy performs best when the relevant distribution prior is set at an intermediate value of approximately the weight of 10 documents. The former indicates that feeding back negative documents strongly is important for diverging the results away from PRP, whereas the latter indicates that we may be  X  X rowning out X  the query if our feedback documents are weighted too strongly.

Since TRECs 1, 2, and 3 were used for tuning weights, retrieval results on them were not meaningful. Instead, for evaluation we applied 1-greedy and 10-greedy with the prior weight settings we found in this section, to a different corpus and set of topics.
We turn our attention to the TREC 2004 robust track. The robust track uses a standard ad hoc retrieval framework, but is evaluated with an emphasis on the overall reliability of IR engines X  X hat is, minimizing the number of queries for which the system performs badly. There were 249 topics in total 2 , drawn from the ad hoc task of TREC-6,7,8 (topics 301 to 450), the 2003 robust track (topics 601-650), and the 2004 robust track (topics 651-700). The cor-pus consisted of about 528,000 documents. Note that there is no overlap between this corpus and the TREC-1,2,3 corpus, in either documents or topics.

From the 249 robust track topics, 50 were selected by TREC as being  X  X ifficult X  queries for automatic search systems. We sepa-rately call out the results for these 50 topics.

Table 1 presents the results for the robust track. We show a no-ticeable improvement in 1-call by using 1-greedy instead of PRP. When we restrict our attention to just the 50 difficult queries, the results overall are lower, but 1-greedy is still more likely than PRP to return relevant results. Similarly, 10-greedy X  X  10-call score is higher than the corresponding PRP and 1-greedy scores. The dif-ficult queries live up to their name for 10-call X  X one of our algo-rithms satisfy the strict 10-call criterion over that subset.
We also note that 1-greedy actually has worse precision at 10 than PRP. However, as we argued earlier, precision is not the appro-priate metric for our task, so a lower precision score is not problem-atic. Interestingly, 10-greedy does not affect precision noticeably,
One topic was dropped because the evaluators did not deem any documents relevant for it. likely because precision rewards result sets that have lots of rele-vant documents, and 10-greedy is more likely to have every result be relevant.

Finally, our performance on the other metrics for which we are greedily optimizing, namely MRR and MSL, is better under 1-greedy than with PRP. Because our 1-greedy procedure attempts to diversify the result set after selecting the first result, we would expect that it would be more likely to find a relevant result for the next few positions than PRP (recall that both methods choose the first result identically). In other words, if the first result was not rel-evant, 1-greedy will be more likely to select something different, and thus, something relevant, for the second result. On the other hand, PRP will stick with the same interpretation of the query, so if the first document was of the wrong interpretation (and thus irrele-vant) the second document would more likely continue that trend. To examine the gains we are making in MRR and MSL, consider figure 2, which graphs the location of the first relevant document for the topics. As the figure demonstrates, it is more often the case that 1-greedy chooses a relevant document for the second position, but the effect disappears for higher ranks, as we would expect.
We conducted statistical significance tests on the robust track experiment X  X  results to compare our greedy algorithms against the PRP baseline. For 1-greedy vs. PRP on 1-call, a one-tailed McNe-mar test gives p =0 . 026 , which indicates significance at the 5% level. For 10-greedy vs. PRP on 10-call, p =0 . 0002 , which indi-cates significance at the 1% level. Using a one-tailed Wilcoxon test, we find that for 1-greedy vs. PPR on MRR and MSL, p =0 . 314 , which is not statistically significant. The MRR and MSL gains are interesting but too minor to be significant.
As described in section 8, optimizing for 1-call has the side-effect of seeking diversity in the result set X  X t returns more distinct interpretations of the query in expectation. The TREC-6, 7, and 8 interactive track runs afford us a unique opportunity to test the performance of our system for diversity, because each run X  X  top-ics were annotated with multiple  X  X nstances X  (i.e., subtopics) that described its different facets [9]. The document judgments were also annotated with the instances that they covered. In total, there were 20 topics, with between 7 and 56 aspects each, and a corpus of about 210,000 documents.

Table 2 lists the instance recall at rank ten results, along with instance recall scores computed on result sets from the subtopic retrieval work, corresponding to configurations presented in table 2 of Zhai et al. X  X  paper [21]. In their work, they looked at reranking a mixed pool of relevant and irrelevant documents drawn from the top documents selected by a language model baseline. For parity of comparison, we simulated their experiment conditions by reranking the same starting pool of documents as they did.

We note that 1-greedy outperforms our own PRP baseline, as we would expect. However, 1-greedy underperforms Zhai et al. X  X  sys-tem. Zhai et al. X  X  language model baseline appears to be a much bet-ter model for aspect retrieval than Na  X   X ve Bayes in the first place. If we had a well-tuned baseline, our 1-greedy would presumably per-form better as well. Indeed, Zhai et al. X  X  reranking systems do not improve upon their baseline on instance recall, though this is proba-bly due to their focus on optimizing the more sophisticated metrics of S-precision and WS-precision, and the (W)S-precision/S-recall curves.
Another way of viewing the 1-call goal is from a multi-user perspective. Different users may intend different interpretations, as was evident from the Google examples presented earlier. For TREC-4 and TREC-6, multiple independent annotators were asked to make relevance judgments for the same set of topics, and over the same corpus [18, 7, 17]. In the TREC-4 case, these were topics 202 through 250, over a corpus of about 568,000 documents, and in the TREC-6 case, topics 301 through 350 over a corpus of about 556,000 documents (the TREC-6 topics are a subset of the robust track topics). TREC-4 had three annotators, TREC-6 had two. Method 1-call (1) 1-call (2) 1-call (3) 1-call (total) 1-greedy 0.776 0.633 0.714 2.122 Method 1-call (1) 1-call (2) 1-call (3) 1-call (total) 1-greedy 0.800 0.820 N/A 1.620
The individual 1-call scores for each run and each annotator are presented in table 3. The last column is the sum of the previous columns, and can be considered to be the average number of anno-tators that are  X  X atisfied X  (that is, get at least one result they con-sider relevant in the top ten) by the respective result sets. Over both corpora, 1-greedy on average satisfied more annotators than PRP.
To better understand 1-greedy X  X  improvements, we also looked specifically at instances where 1-greedy returned a relevant result in the top ten (that is, satisfied the 1-call criterion) and PRP did not. The results for topic 100 from the TREC-1,2,3 weight-tuning development set is presented in figure 3 (document titles have been summarized for clarity and space), with the relevant result shaded. The topic description is: It is notable that PRP wastes its time on the wrong interpretation of the title X  X ooking for technologies that control data transfer ,such as hard drive controllers. While 1-call also pulls up that interpre-tation, it moves away quickly enough that it can bring back more results on actual tech transfers, including the relevant Soviet Union-related result. 3
While the probability ranking principle is appropriate in many settings, it is not the universally  X  X ight X  approach for optimizing all objective functions. We have identified a common scenario in which the principle is not optimal, and given an approach X  X he Ex-pected Metric Principle X  X o directly optimizing other desired ob-jectives. We have shown that this approach is algorithmically fea-sible, and that it does yield better results for the given metrics.
Much remains to be done to explore heuristics that optimize our new, or other, objective functions. While the greedy approach per-forms reasonably well, we might expect more sophisticated tech-niques, such as local search algorithms, to perform better. We have focused on the  X  X xtreme points X  k =1 and k = n .
 There is likely to be some value in filling in the middle. For exam-ple setting k =3 says that a user wants several relevant documents but does not need them all to be relevant. As in the 1-call case, this would seem to allow the optimization algorithm to hedge X  X t has room, for example, to include 3 distinct interpretations in the top 10.

Our focus on an objective function means that our approach can theoretically be applied to any probabilistic model in which it is possible to discuss the likelihood of relevance of collections of doc-uments. This includes, for example, the two-Poisson model [8], or the language modeling approach [12]. Those better models would hopefully yield better performance.

In general, our work indicates the potential value of  X  X eaching to the test X  X  X hoosing, as the objective function to be optimized in the probabilistic model, the metric used to evaluate the information retrieval system. Assuming the metric is an accurate reflection of result quality for the given application, our approach argues that optimizing the metric will guide the system towards desired results. As an example, it may be worth using this approach with the well known average precision metric as the objective function.
Result 10, on the PCB tech transfer to the Soviet Union, could possibly be judged relevant as well, but was not in the document judgments at all, indicating that it was never judged.
The authors would like to thank Susan Dumais, Jaime Teevan, and the anonymous reviewers for their comments and feedback, and ChengXiang Zhai, William Cohen, and Ellen Voorhees for their code and data. Harr Chen is supported by a National Defense Science and Engineering Graduate Fellowship (NDSEG).
