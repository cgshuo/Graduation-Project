 Modern molecular biology is rapidly advanced by the increasing use of computa-tional techniques. For tasks such as RNA gene prediction [1], promoter recogni-tion [2], splice site identification [3], and the classification of protein localization sites [4], it is often necessary to addres s the problem of imbalanced class distri-bution because the datasets extracted from those biological systems are likely to contain a large number of negative examples (referred to as majority class) and a small number of positive examples (referred to as minority class). Many pop-ular classification algorithms such as support vector machine (SVM) have been applied to a large variety of bioinformatics problems including those mentioned above (e.g. refs. [1,3,4]). However, most of these algorithms are sensitive to the imbalanced class distribution and may not perform well if being directly applied on the imbalanced data [5,6].

Sampling is a popular approach to addressing the imbalanced class distri-bution [7]. Simple methods such as random under-sampling and random over-sampling are routinely applied in many bioinformatics studies [8]. With random under-sampling, the size of the majority class is reduced to compensate the im-balance, whereas with random over-sampling, the size of the minority class is increased to compensate the imbalance. Although they are straightforward and computationally efficient, these two methods are prone to either increased noise and duplicated samples or informative sample removal [9]. A more sophisticated approach known as SMOTE is to synthesize  X  X ew X  samples using original sam-ples in the dataset [10]. However, many b ioinformatics problems often present several thousands of samples with a highly imbalanced class distribution. Ap-plying SMOTE will introduce a large number of synthetic samples which may increase the data noise substantially. Alt ernatively, a cost-m etric can be speci-fied to force the classifier to pay more attention to the minority class [11]. This requires to choose a correct cost-metric which is often unknown apriori .
Several recent studies found that ensemb le learning could improve the per-formance of a single classifier in imbalanced data classification [6,12]. In this study, we explore along this direction. In particular, we introduce a sample sub-set optimization technique for  X  X ntelligent under-sampling X  in imbalanced data classification. Using this technique, we designed an ensemble of SVMs specifi-cally for learning from imbalanced biologi cal datasets. This system has several advantages over the conventional ones:  X  It creates each base classifier using a roughly balanced training subset with  X  The system embraces an ensemble framework in which multiple roughly bal- X  As opposed to random sampling, the sample subset optimization technique  X  The aforementioned biological probl ems often present several thousands of The rest of the paper discusses the details of the proposed sample subset op-timization technique and the associat ed ensemble learning system. Section 2 presents the ensemble learning syste m. Section 3 describes the main idea of sample subset optimization. The base classifier and fitness function of the en-semble system are described in Section 4 . Comparisons with typical sampling and ensemble methods are given in Section 5. Section 6 concludes the paper. Ensemble learning is an effective approach for improving the prediction accuracy of a single classification algorithm. Such an improvement is commonly achieved by using multiple classifiers (known as the base classifiers) each trained on a subset of samples created by random sampling such as those used in bagging [13], or cost-sensitive sampling such as those used in boosting [14]. The base classifiers are typically combined using an integration function such as averaging [15] or majority voting [16].

We propose an ensemble learning syste m specifically designed for imbalanced biological data classification. The schematic representation of the proposed sys-tem is shown in Figure 1. It has three main components  X  sample subset opti-mization, base classifier, and fitness function. The key of this ensemble system is the application of the sample subset optimization techniques (to be described in Section 3).

Suppose that a highly imbalanced dataset contains n samples from the ma-jority class and m samples from the minority class where n m , the system creates each sample subset by including all m minority samples and selecting a subset of samples from the n majority samples according to an internal opti-mization procedure. This procedure is conducted to generate multiple optimized sample subsets, each being a roughly balanced subset containing m minority samples and n i carefully selected majority samples, where n i n ( i =1 ...L ) and L is the total number of optimized sample subsets. Using those optimized sample subsets, we can obtain a group of base classifiers c i ( i =1 ...L ), each being trained on its corresponding sample subset { m + n i } . The base classifiers are then combined using majority voting to form an ensemble of classifiers.
Algorithm 1 summarizes the procedure. A line starting with  X // X  in the al-gorithm is a comment for its adjacent next line. Algorithm 1. sampleSubsetOptimization The key function in Algorithm 1 is the optimization procedure applied to select a subset of samples from the majority class (Algorithm 1, line 13). The principal idea of the sample subset optimization procedure is to apply a cross validation procedure to form a subset in which eac h sample is selected according to the internal classification accuracy. In this section, we describe its formulation using a particle swarm optimization (PSO) algorithm [17], and analyze its behavior using a synthetic dataset. The base classifier and the fitness function used for optimization are discussed in Section 4. 3.1 Formulation of Sample Subset Optimization We formulate the sample subset optimization using a particle swarm optimiza-tion algorithm. In particular, for each sample from the majority class a dimension in the particle space is assigned. That is, for n majority samples, the particle is an indicator function I x j takes value  X 1 X  when the corresponding j th sample x j is included to train a classifier. Similarly, a  X 0 X  denotes that the correspond-ing sample is excluded from training. By optimizing a population of L particles p i ( i =1 ...L ), the velocity of the i th particle v i,j ( t ) and the position of this particle s i,j ( t )inthe j th dimension of the solution space are updated in each iteration t as follows: where pbest i,j and gbest i,j are the previous best position and the best position found by informants, respectively. c 1 , r 1 , c 2 ,and r 2 are the learning rates and social coefficients. random () is the random number generator with a uniform distribution of [0,1].

Representing this optimization procedure in pseudocode, we obtain Algorithm 2. Note that the PSO algorithm produces multiple optimized sample subsets in parallel. Therefore, by specifying the popSize parameter, we can obtain any number of optimized sample subsets wit h a single execution of the algorithm. Algorithm 2. optimizeMajoritySamples 3.2 Analysis of Behavior We analyze the behavior of sample subset optimization by using an imbalanced synthetic data. Samples are created with each has two features. These two fea-tures are generated from the same distribution. Specifically, 20 samples of the majority class are generated from a normal distribution N (5 , 1) and 10 samples of the minority class are generated from a normal distribution N (7 , 1). In ad-dition, 5  X  X utlier X  samples are introduced to the dataset. They are labeled as majority class, but are generated from the normal distribution of the minority class. The class ratio of the data is 25:10.
Figure 2(a) shows the original dataset and the resulting classification bound-ary of a linear SVM, and Figure 2(b) shows a dataset after applying sample subset optimization and the resulting classification boundary of a linear SVM. Note that this is one of the optimized dataset which is used to train one base classifier. Our ensemble is the aggregation of multiple base classifiers trained on multiple optimized datasets. It is evident that the class ratio is more balanced after optimization (from 25:10 to 15:10). In addition, the 3 out of 5 outlier sam-ples are removed, and 7 redundant majority samples which has limited effect on the decision boundary of the linear SVM classifier are removed to correct the imbalanced class distribution. We select SVM as the base classifier for building the ensemble system. SVM is routinely applied to many challenging bioinformatics problems. The design of the fitness function is another important facet for sample subset optimization. It determines the quality of the base classifiers, and thus the performance of the ensemble. The following subsections desc ribe these two components in details. 4.1 Base Classifier of Support Vector Machine SVM is a popular classification algorithm which has been widely used in many bioinformatics problems. Among different kernel choices, linear SVM with a soft margin is robust for large scale and high-dimensional dataset classification [18]. Let us denote each sample in the dataset as a vector x i ( i =1 ...M )where M is the total number of samples, and y i is the class label of sample x i .Each component in x i is a feature x ij ( j =1 ...N ) interpreted as the j th feature of the i th sample, where N is the dimension of the feature space. In our case, features could be GC-content, dinucleotide valu es, or other biological markers used to characterize each sample.

A linear SVM with a soft margin is trained by optimizing following functions: where w is the weight vector,  X  i are slack variables, and b is the bias. The constant C determines the trade-off between maximizing the margin and minimizing the amount of slack.

In this study, we utilize the implementation proposed by Hsieh et al. [19]. This is an implementation for fast and large scale linear SVM, which is especially suited as base classifier for ensemble learning due to its computational efficiency.
Notice that classifiers are trained both for sample subset optimization and for composing ensemble. However, these two procedures are independent from each other, and therefore, the classifiers trained for sample subset optimization are not the classifiers used for ensemble. The purpose of the classifiers trained in the sample subset optimization procedure are to provide fitness feedbacks of the selected samples, whereas the classifiers used for composing ensemble are trained by using the optimized sample subsets and serve as the base classifiers of the ensemble. To maximize the specificity of the feedbacks, the same classification algorithm, that is, linear SVM, is used for both procedures. 4.2 Fitness Function For building a classifier, a subset of samp les from the majority class is selected according to an indicator function set p i (seeSection3.1),andcombinedwith the samples from the minority class to form a training set D p i train . The goodness of an indicator function set can be assessed by the performance of the classifier trained with the samples specified by i t. For imbalanced data, one effective way to evaluate the performance of the classifier is to use area under the ROC curve function, where D p i train denotes the training set generated using p i and D test de-notes the test data. Function AU C () calculates the AUC value of a classification model h i ( D a , D b ) which is trained on D a and evaluated on D b .
Moreover, the size of the subset is also important because a small training set is likely to result in a poorly trained model with poor generalization. Therefore, the fitness function can be construct ed by combining the two components: where Size () determines the size of a subset (specified by p i ). Coefficients w 1 and w 2 are empirical constants which can be adjusted to alter the relative importance of each fitness component. The default values are w 1 =0 . 8and w 2 =0 . 2asthey work well in a range of datasets. In this section, we first describe four imbalanced biological datasets used in our experiment. They are generated from several important and diverse biological problems and represent different degrees of imbalanced class distribution. Next we present the performance results of our ensemble algorithm compared with six other algorithms using those datasets. 5.1 Datasets We evaluated different algorithms using datasets generated for identification of miRNA, classification of protein localization sites, and prediction of promoter (drosophila and human). Specifically, the miRNA identification dataset contains 691 positive samples and 9248 negative samples, which is described by 21 fea-tures [21]. The protein localization dataset is generated from the study discussed in [22]. We attempted to differentiate membrane proteins (258) from the rests (1226). The human promoter dataset contains 471 promoter sequences and 5131 coding sequences (CDS) and intron se quences. Compared to the human pro-moter dataset, the drosophila promoter dataset has a relatively balanced class distribution with 1936 promoter sequences and 2722 CDS and intron sequences. We calculated the 16 dinucleotide features according to [23].
 The datasets are summarized and organ ized according to class ratio in Table 1.
 5.2 Performance Comparison The performance of the single classifier of SVM was used as the baseline for all datasets. We compared the single classifier approaches including random under-sampling with SVM (RUS-SVM), random over-sampling with SVM (ROS-SVM), SMOTE sampling with SVM (SMOTE-SVM), and the ensemble approaches including boosting with base classifiers of SVM (Boost-SVMs), bagging with base classifiers of SVM (Bag-SVMs), and our sample subset optimization technique with SVM (SSO-SVMs).

For the ensemble methods, we tested the ensemble size from 10 to 100 with a step of 10. A 5-fold cross-validation procedure was applied to partition datasets for training and testing, and each algorithm was tested on the same parti-tion to reduce evaluation variance. Among the six tested algorithms, four of them employed the randomi zation procedure. They are RUS-SVM, ROS-SVM, Bag-SVMs, and SSO-SVMs (note that the Boost-SVMs algorithm uses the reweighting implementation and is deterministic). For those with the randomiza-tion procedure, we repeated the test 10 t imes, each time with a different random seed.

Figure 3 shows the results comparison. It can be seen that in most cases en-semble approaches give higher AUC values than the single classifier approaches. For single classifier approaches, random under-sampling, random over-sampling, and SMOTE sampling do improve the classification results when the analyzed dataset has a highly imbalanced class distribution such as the cases in Figure 3(b)(c)(d). However, the improvements become less significant when the imbal-ance is moderate (drosophila promoter dataset in Figure 3(a)). SMOTE sampling performs better than random under-sampling and over-sampling approaches in the case of protein localization (Figure 3(b)). However, the performance gain is marginal in other three datasets (Figure 3(a)(c)(d)). We do not observe significant difference of the performance between random under-sampling and random over-sampling, except in the case of miRNA identification (Figure 3(d)) where random over-sampling is relatively better than random under-sampling.
For ensemble approaches, Boost-SVMs performs surprisingly worse than the other two approaches in most cases and the performance fluctuates among dif-ferent ensemble sizes. This may be caused by its training process in that the boosting algorithm assigns increasingly more classification weights to those most  X  X ifficult X  samples in each iteration. However, those  X  X ifficult X  samples could be the outliers and cause deleterious effect when the classifiers pay too much at-tention on classifying them while ignoring other more representative samples. In this regard, Bag-SVMs and SSO-SVMs appear to be the better approaches. However, SSO-SVMs almost always performs the best in every case and gener-ates much smaller performance varian ce when different random seeds were used. It is likely that the SSO-SVMs can capture the most representative samples from the training set which gives a better generalization on unseen data classification. We also observe that the improvement is more significant when the datasets has a highly imbalanced class distribution (Figure 3(b)(c)(d)).

Table 2 shows the AUC values of both single classifier and ensemble ap-proaches. For the ensemble approaches, the AUC value is the average of those given by the ensemble sizes from 10 to 100. The proposed SSO-SVMs performs the best in all four tested datasets. Comparing these results with the base-line of a single SVM, they account for 10%-20% improvements. To confirm the improvements are statistically significant, we applied a one-tail student t -test and compared SSO-SVMs with the other six methods. Table 3 shows the p -value of the comparison. In all four datasets, the performance of SSO-SVMs is significantly better than the other six methods, with a p -value smaller than 0.05. Therefore, we confirmed the effectivenes s of the proposed ensemble approach. In this paper we introduced a sample subset optimization technique for sampling optimal sample subsets from training data. We integrated this technique in an ensemble learning framework and create d an ensemble of SVMs specifically for imbalanced biological data classification. The proposed algorithm was applied to several bioinformatics tasks with moderate and highly imbalanced class distribu-tions. According to our experimental results, (1) the approaches based on data sampling for a single SVM are generally less effective compared to the ensemble approaches; (2) the proposed sample subset optimization technique appears to be very effective and the ensemble optimized by this technique produced the best classification results in terms of AUC value for all evaluation datasets.
