 Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are m gram models (Chen and Goodman, 1996), based on explicit storage of m grams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact.

Depending on the order and the training corpus size, a typical m gram LM may contain as many as several hundred billions of m grams (Brants et al., 2007), raising challenges of efficient stor-age and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or re-lated compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). How-ever, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An impor-tant exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20  X  the training corpus).
 compressed suffix trees (C ST s) (Sadakane, 2007) to build compact indices with much more mod-est memory requirements (  X  the size of the cor-pus). We present methods for extracting frequency and unique context count statistics for m gram queries from C ST s, and two algorithms for com-puting Kneser-Ney LM probabilities on the fly us-ing these statistics. The first method uses two C
ST s (over the corpus and the reversed corpus), which allow for efficient computation of the num-ber of unique contexts to the left and right of an m gram, but is inefficient in several ways, most notably when computing the number of unique contexts to both sides. Our second method ad-dresses this problem using a single C ST backed by a wavelet tree based FM-index (Ferragina et al., 2007), which results in better time complexity and considerably faster runtime performance.

Our experiments show that our method is prac-tical for large-scale language modelling, although querying is substantially slower than a S RI L M benchmark. However our technique scales much more gracefully with Markov order m , allowing unbounded  X  X on-Markov X  application, and enables training on large corpora as we demonstrate on the complete Wikipedia dump. Overall this paper il-lustrates the vast potential succinct indexes have for language modelling and other  X  X ig data X  prob-lems in language processing. Suffix Arrays and Suffix Trees Let T be a string of size n drawn from an alphabet  X  of size  X  . Let T [ i..n  X  1] be a suffix of T . The suffix tree (Weiner, 1973) of T is the compact labeled tree of n + 1 leaves where the root to leaf paths correspond to all suffixes of T $, where $ is a ter-minating symbol not in  X  . The path-label of each node v corresponds to the concatenation of edge labels from the root node to v . The node depth of v corresponds to the number of ancestors in the tree, whereas the string depth corresponds to the length of the path-label. Searching for a pattern  X  of size m in T translates to finding the locus node v closest to the root such that  X  is a prefix of the path-label of v in O ( m ) time. We refer to this ap-proach as forward search . Figure 1a shows a suffix tree over a sample text. A suffix tree requires O ( n ) space and can be constructed in O ( n ) time (Ukko-nen, 1995). The children of each node in the suffix tree are lexicographically ordered by their edge la-bels. The i -th smallest suffix in T corresponds to the path-label of the i -th leaf. The starting position of the suffix can be associated its corresponding leaf in the tree as shown in Figure 1a. All occur-rences of  X  in T can be retrieved by visiting all leaves in the subtree of the locus of  X  . For exam-ple, pattern  X  X he night X  occurs at positions 12 and 19 in the sample text. We further refer the number of children of a node v as its degree and the num-ber of leaves in the subtree rooted at v as the size of v .
 The suffix array (Manber and Myers, 1993) of T is an array SA [0 ...n  X  1] such that SA [ i ] corre-sponds to the starting position of the i -th smallest suffix in T or the i -th leaf in the suffix tree of T . The suffix array requires n log n bits of space and can also be constructed in O ( n ) time (K  X  arkk  X  ainen et al., 2006). Using only the suffix array and the text, pattern search can be performed using bi-nary search in O ( m log n ) time. For example, the pattern  X  X he night X  is found by performing binary search using SA and T to determine SA [18 , 19] , the interval in SA corresponding the the suffixes in T prefixed by the pattern. In practice, suffix arrays use 4  X  8 n bytes of space whereas the most ef-ficient suffix tree implementations require at least 20 n bytes of space (Kurtz, 1999) which are both much larger than T and prohibit the use of these structures for all but small data sets.
 Compressed Suffix Structures Reducing the space usage of suffix based index structure has recently become an active area of research. The space usage of a suffix array can be reduced sig-nificantly by utilizing the compressibility of text combined with succinct data structures. A suc-cinct data structure provides the same function-ality as an equivalent uncompressed data struc-ture, but requires only space equivalent to the information-theoretic lower bound of the underly-ing data. For simplicity, we focus on the FM-Index which emulates the functionality of a suffix array over T using nH k ( T ) + o ( n log  X  ) bits of space where H k refers to the k -th order entropy of the text (Ferragina et al., 2007). In practice, the FM-Index of T uses roughly space equivalent to the compressed representation of T using a standard compressor such as bzip2 . For a more compre-hensive overview on succinct text indexes, see the excellent survey of Ferragina et al. (2008).
The FM-Index relies on the duality between the suffix array and the BWT (Burrows and Wheeler, 1994), a permutation of the text such that T bwt [ i ] = T [ SA [ i ]  X  1] (see Figure 1). Search-ing for a pattern using the FM-Index is performed in reverse order by performing RANK ( T bwt ,i,c ) operations O ( m ) times. Here, RANK ( T bwt ,i,c ) counts the number of times symbol c occurs in T bwt [0 ...i  X  1] . This process is usually referred to as backward search . Let SA [ l i ,r i ] be the in-terval corresponding to the suffixes in T match-ing  X  [ i...m  X  1] . By definition of the BWT, T bwt [ l i ,r i ] corresponds to the symbols in T pre-ceding  X  [ i...m  X  1] in T . Due to the lexico-graphical ordering of all suffixes in SA , the interval SA [ l i  X  1 ,r i  X  1 ] corresponding to all occurrences of  X  [ i  X  1 ...m  X  1] can be determined by comput-ing the rank of all occurrences of c =  X  [ i  X  1] in T bwt [ l i ,r i ] . Thus, we compute RANK ( T bwt ,l i ,c ) , the number of times symbol c occurs before l i and RANK ( T bwt ,r i + 1 ,c ) , the number of occurrences of c in T bwt [0 ,r i ] . To determine SA [ l i  X  1 ,r we additionally store the starting positions C s of all suffixes for each symbol s in  X  at a negligi-ble cost of  X  log n bits. Thus, the new interval is computed as l i  X  1 = C c + RANK ( T bwt ,l i ,c ) and r
The time and space complexity of the FM-index thus depends on the cost of storing and pre-o=old=101 , t=the=110 , n=night=1001 and T=town=111 . processing T bwt to answer RANK efficiently. A wavelet tree can be used to answer RANK over T bwt in O (log  X  ) time. The wavelet tree re-duces RANK over an alphabet  X  into multiple RANK operations over a binary alphabet which can be answered in O (1) time and o ( n ) bits extra space by periodically storing absolute and relative RANK counts (Munro, 1996). The al-phabet is reduced by recursively splitting sym-bols based on their code words into subgroups to form a binary tree as shown in Figure 1b for T bwt . To answer RANK ( T bwt ,i,c ) , the tree is traversed based on the code word of c , per-forming binary RANK at each level. For exam-ple, RANK ( T bwt , 17 ,  X  X  X  ) translates to performing RANK ( WT root , 17 , 1) = 12 on the top level of the wavelet tree, as t=the=110 . We recurse to the right subtree of the root node and com-pute RANK ( WT 1 , 12 , 1) as there were 12 ones in the root node and the next bit in the code-word of  X  X he X  is also one. This process contin-ues until the correct leaf node is reached to answer RANK ( T bwt , 17 ,  X  X  X  ) = 5 in O (log  X  ) time. The space usage of a regular wavelet tree is n log  X  + o ( n log  X  ) bits which roughly matches the size of ditional space is needed to access SA [ i ] or the in-verse suffix array SA  X  1 [ SA [ i ]] = i . In the sim-plest scheme, both values are periodically sam-pled using a given sample rate SAS (e.g. 32) such that SA [ i ] mod SAS = 0 . Then, for any SA [ i ] or SA  X  1 [ i ] , at most O ( SAS ) RANK operations on ent sample rates, bitvector implementations and wavelet tree types result in a wide variety of time-space tradeoffs which can be explored in prac-tice (Gog et al., 2014).

In the same way the FM-index emulates the functionality of the suffix array in little space, compressed suffix trees (C ST ) provide the func-tionality of suffix trees while requiring signifi-cantly less space than their uncompressed coun-terparts (Sadakane, 2007). A C ST uses a com-pressed suffix array (C SA ) such as the FM-Index but stores additional information to represent the shape of the suffix tree as well as information about path-labels. Again a variety of different stor-age schemes exist, however for simplicity we fo-cus on the C ST of Ohlebusch et al. (2010) which we use in our experiments. Here, the shape of the tree is stored using a balanced-parenthesis (BP) se-quence which for a tree of p nodes requires  X  2 p bits. Using little extra space and advanced bit-operations, the BP-sequence can be used to per-form operations such as string-depth( v ), parent( v ) or accessing the i -th leaf can be answered in con-stant time. To support more advanced operations such as accessing path-labels, the underlying C SA or a compressed version of the LCP array are re-a C ST requires roughly 4  X  6 n bits in addition to the cost of storing the C SA . For a more extensive overview of C ST s see Russo et al. (2011).
 Kneser Ney Language Modelling Recall our problem of efficient m gram language modeling backed by a corpus encoded in a succinct index. Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM (Kneser and Ney, 1995), specifically the inter-polated variant described in Chen and Goodman (1996), which has been shown to outperform other n gram LMs and has become the de-facto standard.
Interpolated Kneser-Ney describes the condi-tional probability of a word w i conditioned on the context of m  X  1 preceding words, w i  X  1 where lower-order smoothed probabilities are de-fined recursively (for 1 &lt; k &lt; m ) as  X  In the above formula, D k is the k gram-specific discount parameter, and the occurrence count N 1+ (  X  observed word types following the pattern  X  ; the occurrence counts N 1+ ( defined accordingly. The recursion stops at uni-gram level where the unigram probabilities are de-fined as  X  P ( w i ) = N 1+ ( The key requirements for computing probability under a Kneser-Ney language model are two types of counts: raw frequencies of m grams and occur-rence counts, quantifying how many different con-texts the m gram has occurred in. Figure 2 (right) illustrates the requisite counts for calculating the probability of an example 4-gram. In electing to store the corpus directly in a suffix tree, we need to provide mechanisms for computing these counts based on queries into the suffix tree.

The raw frequency counts are the simplest to compute. First we identify the locus node v in the suffix tree for the query m gram; the frequency corresponds to the node X  X  size , an O (1) operation which returns the number of leaves below v . To il-lustrate, consider searching for c ( the night ) in Fig-ure 1a, which matches a node with two leaves (la-belled 19 and 12), and thus c = 2 .

More problematic are the occurrence counts, which come in several flavours: right contexts, N 1+ (  X  to both sides of the pattern, N 1+ ( of these can be handled easily, as where v is the node matching  X  , and label ( v ) de-notes the path-label of v . 5 For example, keep in has two child nodes in Figure 1a, and thus there are two unique contexts in which it can oc-cur, N 1+ ( keep in tially matches an edge in the forward suffix tree in Figure 1a as it can only be followed by in , N 1+ ( the keep ing applies to computing N 1+ ( we also have a second suffix tree representing the reversed corpus , we first identify the reversed pat-tern (e.g., in keep R ) and then use above method to compute the occurrence count (denoted hereafter N1 P ( t,v, X  ) 6 , where t is the C ST .).
The final component of the Kneser-Ney LM computation is N 1+ ( contexts considering symbols on both sides of the pattern. Unfortunately this does not map to a sim-ple suffix tree operation, but instead requires enu-meration, N 1+ ( where F (  X  ) is the set of symbols that can follow  X  . Algorithm 1 shows how this is computed, with lines 7 and 8 enumerating s  X  F (  X  ) using the edge labels of the children of v . For each symbol, line 9 searches for an extended pattern incorporat-ing the new symbol s in the reverse C SA (part of the reverse C ST ), by refining the existing match v
R using a single backward search operation af-ter which we can compute N 1+ ( deals with the special case where the pattern does not match a complete edge, in which case there is only only unique right context and therefore
N1 P and N1 P F RONT B ACK can compute the requisite occurence counts for m gram language modelling, however at considerable cost in terms of space and time. The need for twin reverse and forward C ST s incurs a significant storage over-head, as well as the search time to match the pat-tern in both C ST s. We show in Section 5 how we can avoid the need for the reversed suffix tree, giving rise to lower memory requirements and faster runtime. Beyond the need for twin suf-fix trees, the highest time complexity calls are string-depth , edge and backward-search . Calling string-depth is constant time for internal nodes, but O ( SAS log  X  ) for leaf nodes; fortunately we Algorithm 1 Two-sided occ., N 1+ ( Precondition: v F in forward C ST t F matches  X  Precondition: v R in reverse C ST t R matches  X  1: function N1PF RONT B ACK ( t F ,v F ,t R ,v R , X  ) 2: o  X  0 3: d  X  string-depth ( v F ) 4: if d &gt; |  X  | then 5: o  X  N1 P ( t R ,v R , 6: else 7: for u F  X  children ( v F ) do 8: s  X  edge ( u F ,d + 1) 9: u R  X  back-search ( v R ,s ) 10: o  X  o + N1 P ( t R ,u R , 11: return o can avoid this call for leaves, which by definition extend to the end of the corpus and consequently to edge and backward-search however cannot be avoided. This leads to an overall time complex-ity of O (1) for N1 P and O ( F (  X  )  X  SAS  X  log  X  ) for N1 P F RONT B ACK , where F (  X  ) is the number of following symbols and SAS is the suffix array value sample rate described in Section 2. The methods above for computing the frequency and occurrence counts provide the ingredients necessary for computing m gram language model probabilities. This leaves the algorithmic problem Algorithm 2 KN probability P w k | w k  X  1 of efficiently ordering the search operations in for-ward and reverse C ST structures.

This paper considers an interpolated LM for-mulation, in which probabilities from higher or-der contexts are interpolated with lower order es-timates. This iterative process is apparent in Fig-ure 2 (right) which shows the quantities required for probability scoring for an example m gram. Equivalently, the iteration can be considered in re-verse, starting from unigram estimates and suc-cessively growing to large m grams, in each stage adding a single new symbol to left of the pattern. This suits incremental search in a C ST in which search bounds are iteratively refined, which has a substantially lower time complexity compared to searching over the full index in each step.
 Algorithm 2 presents an outline of the approach. This uses a forward C ST , t F , and a reverse C ST , t , with three C ST nodes (lines 2 X 4) tracking the match progress for the full i gram ( v all ( i  X  1) gram context ( v F ,v R ), i = 1 ...m . The need to maintain three concurrent searches arises from the calls to size , N 1+ ( tively). These calls impose conditions on the di-rection of the suffix tree, e.g., such that the edge labels and node degree can be used to compute Algorithm 3 Precompute KN discounts the number of left or right contexts in which a pattern appears. The matching process is illus-trated in Figure 2 where the three search nodes are shown on the left, considered bottom to top, and their corresponding count operations are shown to the right. The N 1+ ( in the reverse C ST (left-most column, v all the N 1+ (  X  (right-most column, v F , matching the ( i  X  1) gram context). The N 1+ ( forward match while also requiring a match for the ( i  X  1) gram context in the reversed C ST , as tracked by the middle column ( v R ). Because of the mix of forward and reverse C ST s, coupled with search patterns that are revealed right-to-left, incremen-tal search in each of the C ST s needs to be han-dled differently (lines 7 X 11). In the forward C ST , we perform backward search to extend the search pattern to the left, which can be computed very ef-ficiently from the BWT in the C SA . 9 Conversely in the reverse C ST , we must use forward search as we are effectively extending the reversed pattern to the right; this operation is considerably more costly.
 The discounts D on line 12 of Algorithm 2 and puted directly from the C ST s thus avoiding several costly computations at runtime. The precomputa-tion algorithm is provided in Algorithm 3 which operates by traversing the nodes of the reverse C
ST and at each stage computing the number of m grams that occur 1 X 2 times (used for computing D m in eq. 1), or with N 1+ ( for computing D k in eq. 2), for various lengths of m grams. These quantities are used to compute the discount parameters, which are then stored for PUTE D ISCOUNTS algorithm can be slow, although it is significantly faster if we remove the edge calls and simply include in our counts all m grams fin-ishing a sentence or spanning more than one sen-tence. This has a negligible (often beneficial) ef-fect on perplexity. The above dual C ST algorithm provides an el-egant means of computing LM probabilities of arbitrary order and with a limited space com-plexity ( O ( n ) , or roughly n in practice). How-ever the time complexity is problematic, stem-ming from the expensive method for computing N1 P F RONT B ACK and repeated searches over the C
ST , particularly forward-search . Now we out-line a method for speeding up the algorithm by doing away with the reverse C ST . Instead the crit-ical counts, N 1+ ( puted directly from a single forward C ST . This confers the benefit of using only backward search and avoiding redundant searches for the same pat-tern (cf. lines 9 and 11 in Algorithm 2).

The full algorithm for computing LM prob-abilities is given in Algorithm 4, however for space reasons we will not describe this in de-tail. Instead we will focus on the method X  X  most critical component, the algorithm for computing Algorithm 5. The key difference from Algorithm 1 is the loop from lines 6 X 9, which uses the interval-symbols (Schnattinger et al., 2010) method. This method assumes a wavelet tree representation of the SA component of the C ST , an efficient encod-ing of the BWT as describes in section 2. The interval-symbols method uses RANK operations to efficiently identify for a given pattern the set of preceding symbols P (  X  ) and the ranges SA [ l s ,r s ] corresponding to the patterns s X  for all s  X  P (  X  ) Algorithm 4 KN probability P w k | w k  X  1 using a single C ST Algorithm 5 N 1+ ( by visiting all leaves of the wavelet tree of sym-bols occurring in T bwt [ l,r ] (corresponding to  X  ) in O ( | P (  X  ) | log  X  ) time (lines 6-8). These ranges SA [ l 0 ,r 0 ] can be used to find the corresponding suf-fix tree node for each s X  in O (1) time. To illus-trate, consider the pattern  X  =  X  X ight X  in Fig-ure 1a. From T bwt we can see that this is pre-ceeded by s =  X  X ld X  (1 st occurrence in T bwt ) and s =  X  X he X  ( 3 rd and 4 th ); from which we can com-pute the suffix tree nodes, namely [15 , 15] and [16 + (3  X  1) , 16 + (4  X  1)] = [18 , 19] for  X  X ld X 
N1 P B ACK 1 is computed in a similar way, us-ing the interval-symbols method to compute the number of unique preceeding symbols (see Sup-plementary Materials , Algorithm 7). Overall the time complexity of inference for both N1 P B ACK 1 Table 1: Dataset statistics, showing total un-compressed size; and tokens, types and sentence counts for the training partition. For Wikipedia the Word Types, and Tokens are computed based on characters. and N1 P F RONT B ACK 1 is O ( P (  X  ) log  X  ) where P (  X  ) is the number of preceeding symbols of  X  , a considerable improvement over N1 P F RONT B ACK using the forward and reverse C ST s. Overall this leads to considerably faster computation of m gram probabilities compared to the two C ST ap-proach, and although still slower than highly opti-mised LM toolkits like S RILM , it is fast enough to support large scale experiments, and has consider-ably better scaling performance with the Markov order m (even allowing unlimited order), as we will now demonstrate. We used Europarl dataset and the data was num-berized after tokenizing, splitting, and excluding XML markup. The first 10 k sentences were used as the test data, and the last 80% as the train-ing data, giving rise to training corpora of be-tween 8M and 50M tokens and uncompressed size of up to 200 MiB (see Table 1 for detailed cor-pus statistics). We also processed the full 52 GiB uncompressed  X 20150205 X  English Wikipedia ar-ticles dump to create a character level language model consisting of 72 M sentences. We excluded 10 k random sentences from the collection as test data. We use the SDSL library (Gog et al., 2014) to implement all our structures and compare our in-dexes to S RI L M (Stolcke, 2002). We refer to our dual-C ST approach as D -C ST , and the single-C ST as S -C ST .

We evaluated the perplexity across different lan-guages and using m grams of varying order from m = 2 to  X  (unbounded), as shown on Figure 3. Our results matched the perplexity results from Figure 3: Perplexity results on several Europarl languages for different m gram sizes, m = 2 ... 10 , 15 , 20 ,  X  .
 S
RI L M (for smaller values of m in which S RI L M training was feasible, m  X  10 ). Note that perplex-ity drops dramatically from m = 2 ... 5 however the gains thereafter are modest for most languages. Despite this, several large m gram matches were found ranging in size up to a 34 -gram match. We speculate that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order m gram LMs and appear to discount large m grams too aggressively. We leave further exploration of richer discounting techniques such as Modified Kneser-Ney (Chen and Goodman, 1996) or the Sequence Memoizer (Wood et al., 2011) to our future work.

Figure 4 compares space and time of our in-dexes with S RI L M on the German part of Eu-roparl. The construction cost of our indexes in terms of both space and time is comparable to that of a 3 / 4 -gram S RI L M index. The space us-age of D -C ST index is comparable to a compact 3 -gram S RI L M index. Our S -C ST index uses only 177 MiB RAM at query time, which is compara-ble to the size of the collection ( 172 MiB). How-ever, query processing is significantly slower for both our structures. For 2 -grams, D -C ST is 3 times slower than a 2 -gram S RI L M index as the expen-sive N 1+ ( large m grams, our indexes are much slower than S
RI L M . For m &gt; 2 , the D -C ST index is roughly six times slower than S -C ST . Our fastest index, is 10 times slower than the slowest S RI L M 10 -gram index. However, our run-time is independent of m . Thus, as m increases, our index will become more competitive to S RI L M while using a constant amount of space. Figure 4: Time versus space tradeoffs measured on Europarl German (de) dataset, showing memory and time requirements. Figure 5: Runtime breakdown of a single pattern averaged over all patterns for both methods over the Wikipedia collection.

Next we analyze the performance of our in-dex on the large Wikipedia dataset. The S -C ST , character level index for the data set requires 22 GiB RAM at query time whereas the D -C ST re-quires 43 GiB. Figure 5 shows the run-time per-formance of both indexes for different m grams, broken down by the different components of the computation. As discussed above, 2 -gram per-formance is much faster. For both indexes, most time is spent computing N1 P F RONT B ACK (i.e., tree traversal used in S -C ST roughly reduces the running time by a factor of three. The complex-ity of N1 P F RONT B ACK depends on the number of contexts, which is likely small for larger m grams, but can be large for small m grams, which sug-gest partial precomputation could significantly in-crease the query performance of our indexes. Ex-ploring the myraid of different C ST and C SA con-figurations available could also lead to significant improvements in runtime and space usage also re-mains future work. This paper has demonstrated the massive poten-tial that succinct indexes have for language mod-elling, by developing efficient algorithms for on-the-fly computing of m gram counts and language model probabilities. Although we only consid-ered a Kneser-Ney LM, our approach is portable to the many other LM smoothing method formulated around similar count statistics. Our complexity analysis and experimental results show favourable scaling properties with corpus size and Markov or-der, albeit running between 1-2 orders of magni-tude slower than a leading count-based LM. Our ongoing work seeks to close this gap: preliminary experiments suggest that with careful tuning of the succinct index parameters and caching expensive computations, query time can be competitive with state-of-the-art toolkits, while using less memory and allowing the use of unlimited context.
 Ehsan Shareghi and Gholamreza Haffari are grate-ful to National ICT Australia (NICTA) for gen-erous funding, as part of collaborative machine learning research projects. Matthias Petri is the recipient of an Australian Research Councils Dis-covery Project scheme (project DP140103256). Trevor Cohn is the recipient of an Australian Re-search Council Future Fellowship (project number FT130101105).

