 Intelligence analysis can be aided and guided by models of the analysts X  interests and priorities. This paper describes our ap-which analyst models are used to guide the searching behavior of model include concepts and relations, both of which help to cap-concepts and relationships have associated scalar parameters to provide a quantitative measure of the user X  X  level of interest. We have developed algorithms for dynamically adapting the weights algorithms we have built an Analyst Modeling Environment workbench. We have tested our approach on this workbench using traces generated by human analysts, and have demonstrated im-provements over current state of the art search engines. I.2.6 [Learning]: Knowledge Acquisition, H.3.3 [Information Search and Retrieval]: Search Process, Clustering, Information Filtering, . Algorithms, Human Factors, Performance, Experimentation. Adaptive Search, User Modeling, IR, Intelligence Analysis. When searching information through massive data intelligence analysts face a number of obstacles including inadequate exami-nation of evidence [5] and limitations of human cognitive capac-ity [6]. Conventional information retrieval (IR) is limited in their our approach to alleviate analysts X  problems is to dynamically meaning and implications of the facts. (e.g. rating a document). The inter-est levels in the model are adapted using an extended version of our Bubble Up algorithm [1]. The re-vised algorithm works with concept map models and textual data. AME uses the Implicit Ontology-based Model Expansion algorithm to adapt the structure of the model. Concept and relations are automati-cally expanded with WordNet [7] hyponyms and hypernyms. Those tentative additions with interest lev-els exceeding a threshold in subse-quent adaptation are formally admit-ted to the model. AME was implemented in Java as a client-server Web application. The application takes the appearance of a IR system, and is employed to evaluate model adaptation algo-rithms. The evaluation compared the performance of Google search with or without the aid of AME. The data came from ARDA X  X  NIMD program and consisted of analysis activities of volunteer intelli-sequences of Web search events present in the data. A search saga consists a series of chapters . A chapter is composed of following engine, a sequence of link follow-ups. For the evaluation, we extracted 131 usable sagas of Google search activities. The performance metrics are precision and recall. Precision is given chapter. Recall is relevant results found in a given chapter chapter came. We have two experimental conditions:  X  Google  X  alone and  X  AME+Google  X . The calculation of the metrics for each chapter in a saga for Google alone is straightforward because the data are readily attainable from the saga. For AME+Google, a analyst model was initialized with the terms extracted from the tasking description. Whenever a link follow-up occurred, AME assumed the linked document was relevant and adapted the model. In addi-tion, query terms are automatically added to the model. The ana-lyst model is used to rank all documents found in the given saga. The top-ranked documents are returned as the  X  AME result page  X , Google. Performance metrics can thus be computed as usual for every chapter in the saga. AME+Google outperforms Google on precision and recall for AME+Google seems to improve over time (i.e., as chapter number increases). 
