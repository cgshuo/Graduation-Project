 There are three general approaches to improving chunking performance: engineer better features, improve inference, and improve the model.

Manual feature engineering is a common direc-tion. One technique is to take primitive features and manually compound them. This technique is common, and most NLP systems use n -gram based features (Carreras and M ` arquez, 2003; Ando and Zhang, 2005, for example). Another approach is linguistically motivated feature engineering, e.g. Charniak and Johnson (2005).

Other works have looked in the direction of improving inference. Rather than predicting each decision independently, previous decisions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently.

The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and Le-Cun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. Besides Collobert and Weston (2008), previous work on sequence labeling usually use previous decisions in predicting output labels. Here we do not take advantage of the dependency between suc-cessive output labels. Our approach predicts each output label independently of the others. This allows us to ignore inference during training: The model maximizes the conditional likelihood of each output label independent of the output label of other tokens.
We use a sliding window approach. The output label for a particular focus token x i is predicted based upon  X  k tokens before and after x i . The entire window is of size k = 2  X   X  k + 1. Nearly all work on sequence labeling uses a sliding window approach (Kudo and Matsumoto, 2001; Zhang et al., 2002; Carreras and M ` arquez, 2003; Ando and Zhang, 2005, for example). We assume that each token x can be transformed into a real-valued feature vector  X  ( x ) with l entries. The feature function will be described in Section 4.

A standard approach is as follows: We first concatenate the features of k tokens into one vector [  X  ( x i then pass [  X  ( x i over the entire window followed by an output log-linear layer.

Convolutional architectures can help when there is a position-invariant aspect to the input. In machine vision, parameters related to one part of the image are sometimes restricted to be equal to parameters related to another part (LeCun et al., 1998). A convolutional approach to sequence labeling is as follows: At the lowest layer we extract features from individual tokens using a shared feature extractor. These higher-level individual token features are then concatenated, and are passed to a feature extractor over the entire window.

In our baseline approach, we apply one convolu-tional layer of feature extraction to each token (one token layer ), followed by a concatenation, followed by one layer of feature extraction over the entire window (one window layer ), followed by a 23-D output prediction using multiclass logistic regres-sion. We abbreviate this architecture as I-T-W-O (input token window output). See Figure 1 for an illustration of this architecture. The most common feature extractor in the literature is a linear filter h followed by a non-linear squashing (activation) function  X  : In our experiments, we use the softsign squash-ing function  X  ( z ) = z / (1 + | z | ). n -class lo-gistic regression predicts  X  ( h ( x )), where softmax  X  that complex cells in the V1 area of visual cortex are not well explained by Eq. 1, but are instead better explained by a model that includes quadratic interactions between regions of the receptive field. Bergstra et al. (2009) approximate the model of Rust et al. (2005) with a simpler model of the transformation q includes J quadratic filters: f ( x ) =  X  ( q ( x )) , q ( x ) = where b , W , and V 1 ... V J are tunable parameters.
In the vision experiments of Bergstra et al. (2009), using quadratic filters improved the gen-eralization of the trained architecture. We were interested to see if the increased capacity would also be beneficial in language tasks. For our logistic regression (I-O) experiments, the architecture is specifically I X   X  q O, i.e. output O is the softmax  X  applied to the quadratic transform q of the input I. Like Bergstra et al. (2009), in architectures with hidden layers, we apply the quadratic transform q in all layers except the final layer, which uses linear transform h . For example, I-T-W-O is specifically I X   X  q T X   X  q W X   X  h O, as shown in Figure 1. Future work will explore if generalization is improved by using q in the final layer. Here is a detailed description of the types of features we use, with number of dimensions:  X  embeddings . We map each word to a real-valued 50-dimensional embedding. These embeddings were obtained by Collobert and Weston (2008), and were induced based upon a purely unsupervised training strategy over the 631 million words in the English Wikipedia.  X 
POS-tag . Part-of-speech tags were assigned auto-matically, and are part of the CoNLL data. 45 dim.  X  label frequencies . Frequency of each label assigned to this word in the training and validation data. From Ando and Zhang (2005). 23 dim.  X  type(first character) . The type of the first charac-ter of the word. type( x ) =  X  X  X  if x is a capital letter,  X  X  X  if x is a lowercase letter,  X  X  X  if x is a digit, and x otherwise. From Collins (2002). 20 dim.  X  word length . The length of the word. 20 dim.  X  compressed word type . We convert each char-acter of the word into its type. We then remove any repeated consecutive type. For example,  X  X abel-making X   X   X  X a-a X . From Collins (2002). 46 dim. The last three feature types are based upon ortho-graphic information. There is a combined total of 204 features per token. We follow the conditions in the CoNLL-2000 shared task (Sang and Buchholz, 2000). Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for validation. 5.1 Training details The optimization criterion used during training is the maximization of the sum (over word positions) of the per-token log-likelihood of the correct deci-sion. Stochastic gradient descent is performed using a fixed learning rate  X  and early stopping. Gradients are estimated using a minibatch of 8 examples. We found that a learning rate of 0.01, 0.0032, or 0.001 was most e ff ective.

In all our experiments we use a window size of 7 tokens. In preliminary experiments, smaller windows yielded poorer results, and larger ones were no better. Layer sizes of extracted features were chosen to optimize validation F1. 5.2 Results We report chunk F-measure (F1). In some tables we also report Acc, the per-token label accuracy, post -Viterbi decoding.

Figure 2 shows that using quadratic filters reliably improves generalization on all architectures. For the I-T-W-O architecture, quadratic filters increase I-T(150)-W(310)-W(310)-O 4 96.87 94.82 validation F1 by an absolute 0.31. Most surpris-ingly, logistic regression with 16 filters achieves F1 = 93.94, which outperforms the 93.83 of a stan-dard (0 filter) single hidden layer neural network.
With embeddings as the only features, logreg with 0 filters achieves F1 = 85.36. By adding all features, we can raise the F1 to 91.96. Alternately, by adding 16 filters, we can raise the F1 to 91.60. In other words, adding filters is nearly as e ff ective as our manual feature engineering.

Table 1 shows the result of varying the overall architecture. Deeper architectures achieve higher F1 scores. Table 2 compares the model as we lesion o ff di ff erent features. POS tags and the embeddings were the most important features.

We applied our best model overall (I-T-W-W-O in Table 1) to the test data. Results are shown in AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 ---Mc03 93.96 ---AZ05--93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3. We are unable to compare to Collobert and Weston (2008) because they use a di ff erent training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision. Our approach is nonetheless compet-itive with approaches that use this extra information. Many NLP approaches underfit important linguistic phenomena. We experimented with new techniques for increasing chunker model capacity: adding depth (automatically inducing intermediate features through the composition of non-linearities), and including quadratic filters. Higher accuracy was achieved by deeper architectures, i.e. ones with more intermediate layers of automatically tuned fea-ture extractors. Although they are a simplification of a theoretical model of V1 complex cells, quadratic filters reliably improved generalization in all archi-tectures. Most surprisingly, logistic regression with quadratic filters outperformed a single hidden layer neural network without. Also, with logistic regres-sion, adding quadratic filters was almost as e ff ective as manual feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions. Thank you to Ronan Collobert, L  X  eon Bottou, and NEC Labs for access to their word embeddings, and to NSERC and MITACS for financial support.

