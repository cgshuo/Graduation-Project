 This paper presents a topic model that discovers the correla-tion patterns in a given time-stamped document collection and how these patterns evolve over time. Our proposal, the theme chronicle model (TCM) divides traditional top-ics into temporal and stable topics to detect the change of each theme over time; previous topic models ignore these dif-ferences and characterize trends as merely bursts of topics. TCM introduces a theme topic (stable topic), a trend topic (temporal topic), timestamps, and a latent switch variable in each token to realize these differences. Its topic layers al-low TCM to capture not only word co-occurrence patterns in each theme, but also word co-occurrence patterns at any given time in each theme as trends. Experiments on various data sets show that the proposed model is useful as a gener-ative model to discover fine-grained tightly coherent topics, takes advantage of previous models, and then assigns values for new documents.
 G.3 [ PROBABILITY AND STATISTICS ]: Time series analysis Algorithms, Experimentation Topic model, Text analysis, Trend analysis, Bayesian hier-archical model, Graphical models
Our aim is to represent trends by discovering arbitrary topic correlations in a collection of time-stamped documents and to determine how these patterns evolve over time. The ability of identify topics and see their evolution over time helps us comprehend and organize large document collec-tions such as web documents, mails, news, and microblogs. Since these user-generated contents help other users to make better judgments of various metadata, there is an urgent need for a principled method that discovers fine-grained tightly-coherent topics and so permits the tracking of temporal and popular events from these contents.
 Motivated by the above practical need, we propose the Theme Chronicle Model (TCM); it identifies trends over each theme by incorporating both of these characteristics in a unified model. This model is designed to detect word co-occurrence distributions over time in each theme, assum-ing that each trend can be captured after distinguishing not only terminology words from the other words, but also sta-ble topics and temporal topics. TCM uses the latent vari-ables of theme topic and trend topic to detect these differ-ences and describe the generative process of each document. Additionally, we add another latent variable, switch vari-able, to represent a probability distribution over these new topics, a document-specific topic, and a background topic. The switch variable handles variables (background, docu-ment specific, theme and trend) for generating words in each token.

A key advantage of TCM is its ability to discover fine-grained tightly coherent topics by separating traditional top-ics, distinguishing between static and dynamic topics; it makes the following contributions.
 Theoretical contribution : (1)TCM detects each theme in a given corpus using theme topic and then represents trends as the set of temporal words in each theme by defining tem-poral words and topics; previous models capture the trend as mere bursts of topics/words over the corpus. (2)TCM pro-vides a general framework for which several existing models can be viewed as special cases such as Switch Topics Over Time (sTOT) [4] and pachinko allocation model (PAM) [5], as this model extends these models to handle the various status of words, and covers specific tasks of these models and improves their quality as shown in the experiments. Practical contribution : (1)TCM models the joint dis-tribution of words and time without using stop word lists and domain specific dictionaries, both of which are expen-sive to make and maintain. This is very useful in predict-ing timestamps based on the words in a given document or retrieving documents that match both specific theme and time. (2)TCM is applicable to a document collection with any other type of observed variables (e.g., review rating, the number of comments, annotated tags, position information, and the number of followers). Since timestamps is one of the supervised variables, TCM can analyze any documents hav-ing other variables without loss of generality; it separates tra ditional topics into stable topics and temporal topics, which are more influenced by the replacement variable. For example, TCM can discover sentiment words in a given re-view data set by using review ratings instead of timestamps. We demonstrate the efficacy of TCM through experiments and show that this model can describe the structure of a wide variety of trends.
Statistical topic models define a probabilistic procedure to generate documents as mixtures of a low-dimensional set of topics. Although time is intrinsically continuous, most studies modeled the evolution of topics over time by estimat-ing the topic distribution at various epochs. For example, He, et al. [1] investigate the citation-oblivious approaches based on the LDA model for topic evolution. Trend Analy-sis Model (TAM) [4] associates single timestamps with topic co-occurrence patterns rather than word co-occurrence pat-terns. Temporal Semantic Analysis (TSA) [6] attempts to incorporate temporal evidence into models of semantic re-latedness, where each concept is no longer scalar, but is in-stead represented as time series over a corpus of temporally-ordered documents. Hong, et al. [2] apply temporal topic models to the real world task of tracking the volume of terms by combining state space models and the volume of terms in a supervised learning fashion, which enables us to effectively predict the volume in the future. Rather than learning the dynamic word distributions or trends of topics over time, TCM learns these changes simultaneously by distinguishing these differences in not only words, but also topics.
In this subsection, we describe our model, TCM; it simul-taneously achieves dimensionality reduction in representing both theme topics and the trend topics associated with each theme topic. Table 1 shows the notations used in this paper; Figure 1 shows the graphical model of TCM.

Since we aim to detect which kind of themes exist in a given document corpus and how topics evolve over time in each theme simultaneously, TCM divides traditional top-ics into stable and temporal topics to detect the change of each theme over time, while previous topic models ignore these differences and characterize trends as bursts of topics. Hence, we call the former theme topics, and the latter trend topics.

Before introducing our model, let us review the concept of previous models PAM [5] and sTOT [4]. PAM aims to capture arbitrary topic correlations with a directed acyclic graph (DAG), where each interior node is considered to be a topic and the leaves of DAG represents individual words in the vocabulary. As each topic in PAM has not only a distribution over other words, but also a distribution over other topics, PAM can capture inter-topic correlations as well as word correlations. In each document, sub-topic z 0 has a distribution over words, and a super-topic z has a distribution over subtopics, where both super-topic and sub-topics are globally shared among all documents. sTOT focuses on separating trend-specific words and back-ground topic words in describing the generative process of Fig ure 1: Graphical Model of TCM: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indi-cates a conditional dependency between variables and stacked panes indicate a repeated sampling with the iteration number shown. Note that both PAM and TCM are directed acyclic graphs, so each in-ternal latent topic variable at a given layer has a distribution over all variables on the next layer. In TCM, theme topics can be connected to all trend topics and to all words, and trend topics can also be connected to all words and to the timestamp. timestamped data. The background topic is the common topic found in almost all documents regardless of their con-tent and time, and thereby generates non temporal words, while the topic variable generates temporal words. For dis-tinguishing temporal words from background topic words, we define r , it acts as a switch to handle these words in each token, and takes value r =0 if word w is generated via the background topic variable, or r =1 if word w is generated via the topic variable.

Theme Chronicle Model (TCM) extends these previous models to detect the theme evolution over time by divid-ing topic z into theme g and trend c . These variables allow us to realize the advantages of each model by describing the generative process more precisely. Some words appear most frequently in a specific period of time and some words appear constantly over long periods in documents of a spe-cific theme; TCM treats the former as words associated with temporal topics. In TCM, both theme topic and trend topic are globally shared among all documents, like super-topics and sub-topics in PAM. Additionally, TCM provides more flexibility than PAM, by enabling theme topic, which corre-sponds with super-topic, to generate words. For distinguish-ing the differences in word tokens, we extend r as a switch for handling more kinds of words as follows. If r =0, TCM selects the background topic as responsible for generating a word. If r =1, TCM selects the document-specific topic as responsible for generating a word. If r =2, TCM selects the theme topic as responsible for generating a stable termi-nology word associated with each topic constantly over long periods. If r =3, TCM selects the trend topic as responsible for generating a temporal word in short periods. Note that a trend topic yields both words and timestamps, while the theme topic generates both words and trend topics.
TCM differs from previous dynamic models such as sTOT and PAM in creating topics as follows: Since sTOT mod-els time jointly with word co-occurrence patterns via each SY MBOL DESCRIPTION G ( C ) number of theme (trend) topics D ( W ) number of documents (unique words)
N d number of word tokens in document d t d ( v d ) the timestamps (rating score) g i the theme associated with the i th token c i the trend associated with the i th token r i the switch associated with the i th token w i the i th token  X  the multinomial distribution of theme topics  X  dg the document and theme specific multinomial  X  c ,  X  2 c the parameters of trend c  X  d the multinomial distribution of r i associated  X  c ( g,d,b ) the multinomial distribution of words specific  X ,  X ,  X ,  X  the fixed parameters of symmetric to pic (with the exception of background words), sTOT can not easily distinguish dynamic topics from static topics in the same document, and so fails to capture correlations be-tween these topics. This problem applies to other models that ignore the hierarchical topic structure in each docu-ment. Although PAM has a structure to capture correlations between topics, the structure does not contain observed vari-able timestamp t d or rating value v d . Even if we incorporate this observed variable into the super-topic layer of PAM, we can not interpret what each super-topic is as easily as a sub-topic, as the super-topic does not have a distribution over words. TCM aims to learn the topic transition pattern from time-stamped documents by slicing traditional topics; the other models are designed to capture changes in topic-specific word distributions over time. This is why TCM allows theme-topic to have a distribution over words and generate words by the help of the switch variable.
Since TCM is a generalization of sTOT and PAM, we can infer TCM by Gibbs sampling in the same way used for these models without loss of generalization. The first step, defining the generative process of TCM for parameter estimation, is as follows: 1. Draw D multinomials  X  from Dirichlet prior  X  , one for 2. Draw DG multinomials  X  g from Dirichlet prior  X  , one 3. Draw C parameters of beta distributions  X  1 c and  X  2 c 4. Draw D multinomials  X  d from Dirichlet prior  X  , one 5. Draw G + C + D + 1 multinomials  X  g ( c,d,b ) from prior The generative model for TCM can be described as a Bayesian hierarchical model. For this inference, we need to calculate conditional distributions. As shown in the pre-vious subsection, the joint distribution of the entire corpus is, therefore, the following mixture: p ( w , r , t , c , g ,  X ,  X ,  X ,  X  |  X ,  X ,  X ,  X ,  X  ) =  X  p (  X  |  X  ) In this equation (1), multinomials  X  g ( c,d,b ) ,  X  dg ,  X  can be adapted by the conjugate prior and then integrated out analytically. In the Gibbs sampling procedure, we need to calculate the conditional distributions P ( g di , c di g represent the theme topic assignments for all tokens except g , c \ di represents the trend topic assignments for all to-kens except c di , and r \ di represents the topic assignments for all tokens except r di . Details of the derivation of Gibbs sampling in TCM are given below.  X ,  X ,  X ,  X ,  X  ), we can work out the conditional distribution P ( g di = j, c di = k, r di = r | w , g \ di , c \ di , r as P ( j, k, r | X  X  X  )  X   X  whe re n dj \ di represents the number of tokens that have been assigned to j in document d , except di , n djk \ di represents the number of tokens assigned to trend k in the tokens associated sents the number of tokens assigned to switch r = 0;back-ground topic (1:document specific topic, 2:theme, 3:trend ) in document d , except di , and n bv ( dv,jv,kv ) \ di represents the number of tokens assigned to word v in background topic (document specific topic, theme j specific, trend k specific), except di , and B is the beta function. Since each word pat-tern and its timestamps are assumed to have been generated conditional on trend topic c , the resulting beta and multi-nomial parameters will correspond.
In this section, we present examples in which TCM dis-covers fine-grained tightly-coherent topics, and evaluate it against sTOT and PAM in quantitative evaluations that use three measures: test set perplexity, timestamps/rating pre-diction, and precision/recall. We conducted both qualitative and quantitative evaluations on three data sets: (1)Data1: 8 years (2001-2008) of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW 1 . Preprocessing was applied to Data1. This yielded a total set of 3078 documents and 20286 unique words from 2204 authors. The timestamp of each document was taken to be the year of the proceedings in which it was published. (2)Data2: Netflix data 2 ; a set of rating records from Nov 1st, 1999 to Dec 31st, 2005. We first selected only those users who rated at least 20 movies and movies that were rated by at least 100 users. By focusing on the bias of ratings, we converted the ratings that were higher than the average rat-ing of the user to 1 (purchase) and to 0 (no purchase) if not. Moreover, we rounded the day of grade (DD/MM/YYYY) to the month of grade (MM/YYYY). Each movie rating list of each user on the same month and the movies he rated are analogous to a document and the words in the document, respectively. This yielded a total set of 21033 movie rating lists (documents) from 90954 users and 7125 movies. (3)Data3: Amazon review data 3 : Each line of this data consists of member id, item id, date, number of helpful feed-backs, number of feedbacks, rating, title and body. Al-though this data set has two different types of observed variables, date t and rating v , we used only rating. This data consists of 5686344 reviews and 3784413 unique words.
Although original PAM does not have any timestamp vari-able, we inserted this variable in each token of PAM as the observed score t d ( v d ) which is conditioned on sub topic z and can be sampled from the sub topic specific Beta dis-tribution ( t d ( v d )  X   X  1 z ,  X  2 z ), like sTOT and PAM, so that PAM generates the timestamp (rating score). In this evalu-ation, the smoothing parameters are set the same to those of previous works. Additionally, we enforced switch vari-able r of sTOT to generate a document specific word. We ran the experiments on PCs with Dual Core 2.66 GHz Xeon processors. h ttp://dl.acm.org/ http://www.netflixprize.com/
Amazon Product Review Data (Huge): http://www.cs.uic.edu/  X  liub/FBS/sentiment-analysis.html
We used test-set perplexity and L1 error to compare TCM with the previous models by varying Z , G and Z 0 , C .
To measure the ability of the proposed model to act as generative models, we computed test-set perplexity (PPX) under the estimated parameters and compared the result-ing values. Perplexity, which is widely used in the lan-guage modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are bet-ter). The perplexity was computed for all algorithms using 100 samples from 1000 different chains using: PPX = exp(  X  1 where N W test is the number of test words, R is the number of samples (from R different chains).

We computed the perplexity as follows. First, we ran-domly split off 10% of each document to create a test part; the remainder was used as the learning part. For every doc-ument, the test part was held out to compute perplexity. Second, the learning part was used for estimating the param-eters by Gibbs sampling. Finally, a single set of topic counts was saved when a sample was taken; the log probability of test words that had never been seen before was computed in the same way as the perplexity computation of previous works [3]. In this calculation, we removed low frequency words (movies), those that appeared in fewer than 3% of all documents (lists), as stop words. We selected the slice size of time as one year (Data1), and one month (Data2). Table 2 shows the results of the perplexity comparison. The results over all data sets, that TCM offers lower per-plexity than the others, and thereby support our idea that theme and trend topic exist in each document and that uti-lizing them yields a reduction in perplexity. Although the ratio of terminology words (theme topics) varies with the data set, detecting these differences is essential for modeling the topic evolution precisely, and gaining the lowest test-set perplexity.
One interesting common feature of sTOT, PAM and TCM is their ability to predict the timestamp/rating given the words in a document. This functionality also provides an-other opportunity to quantitatively compare TCM against sTOT and PAM. For all data sets, we measured the abil-ity to predict a year given Data1, month given Data2, and rating given Data3, as measured by accuracy, L1 error. In this case, we used 10-fold cross-validation which consists in partitioning, a single subsample is retained as the valida-tion data for testing the models, and the remaining sub-samples are used as training data. The cross-validation pro-cess is then repeated 10 times, with each of the 10 sub-samples used exactly once as the validation data. Given a document, we predict its timestamp/rating by choosing the discretized value that maximizes the posterior, which is calculated by multiplying all rating probabilities of a word token from a topic/trend-wise Beta distribution over score Q | | G | , | C | are marked with  X * X .
 TCM achieves double the accuracy of sTOT, and provides an L1 relative error reduction of 2% on average. From these results, TCM attains lower perplexity and more distinct top-ics than sTOT for identifying low dimensional components.
In this paper, we proposed a topic model that explicitly models time jointly with temporal word co occurrence and topic co-occurrence. TCM divides traditional topics into theme and trend topics within each token to detect the dif-ference between temporal topics and stable topics in each document; other topic models assign just word-only top-ics. Experiments on various data sets showed that the pro-posed model can capture interpretable low dimensional sets of these newly introduced topics, and is useful for discov-ering fine-grained tightly-coherent topics. This model ac-commodates different types of response variables commonly encountered in practice. One of the limitations of TCM is that it represents each document as a bag of words and thus ignores the information implicit in word order and phrases. Future work is to extend this model by including N -gram words and to detect temporal topics and stable phrases on each topic. [1] Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles. [2] L. Hong, D. Yin, J. Guo, and B. D. Davison. Tracking [3] N. Kawamae. Predicting future reviews: Sentiment [4] N. Kawamae. Trend analysis model: Trend consists of [5] W. Li and A. McCallum. Pachinko allocation: [6] K. Radinsky, E. Agichtein, E. Gabrilovich, and
