 1. Introduction track at Text Retrieval Conference (TREC) 3 solely dedicated to the biomedical domain, namely, the Genomics Track (Hersh, methods to this evolving field of research targeting biomedical text.

In the post-genomic era, one of the major activities in molecular biology is to determine the precise functions of individual genes or gene products, which has been producing a large number of publications with the help of high throughput gene analyses. To structure the information related to gene functions scattered over the literature, a great deal of efforts have been made to annotate articles by using the gene ontology sortium, 2000 ) was first developed as a collaborative project among three model organism databases, FlyBase, the Saccha-romyces Genome Database, and the Mouse Genome Database, in order to facilitate uniform queries across the different associated biological process (BP), cellular component (CC), and molecular function (MF). The ontologies are structured X  the structure of GO.

Because of the large number of publications and specialized content, GO annotation requires extensive human efforts and substantial domain knowledge, usually conducted by experts. However, Baumgartner et al. (2007) reported that current manual curation for GO annotation would never complete at the current rate of production through careful analyses based automate GO annotation, which could greatly alleviate the human curation. This was one of the primary objectives pursued at the Genomics Track 2004 ( Hersh et al., 2004 ).

The Genomics Track consisted of two tasks: ad hoc retrieval and categorization tasks. For the former, given 50 topics ob-years X  worth of MEDLINE data. The latter task, which is our focus in this paper, was composed of two sub-tasks; one was rently carried out by human experts at Mouse Genome Informatics (MGI) initiative.
 the primary goal was to correctly assign GO domain codes, i.e., MF, BP, and CC (not the actual GO terms) or not to assign them, i.e., negative, for each of the given genes that appear in the article. partly indicates the difficulty of the task.
 tain the gene in question by gene name expansion and a flexible term matching scheme. The resulting document fragments with a supervised term weighting scheme ( Debole &amp; Sebastiani, 2003 ) which consider word distributions in different categories.

This paper focuses on an application of general text categorization and IR techniques to the domain-specific problem of the properties of the terminology in biomedicine. In the following, Section 2 introduces our proposed framework for
GO domain code annotation in detail, and then Section 3 describes the data and evaluation measures used for our experi-paper with a brief summary of our approach and major findings, and possible future directions. 2. Methods
This section details our proposed framework for automatic GO domain code annotation. Hereafter, we will refer to GO cessed and represented in a vector space, and then Section 2.2 presents a classifier to be used for GO annotation. 2.1. Document representation 2.1.1. Identification of relevant paragraphs
GO annotation needs to be made not for each input article but for each gene for which there is experimental evidence that 2.1.1.1. Gene name expansion. Gene name expansion refers to a process to associate synonyms with a given gene name. Gene names (and their products) are known to have several types of synonyms including aliases, abbreviations, and gene symbols ( Sehgal &amp; Srinivasan, 2006 ). For instance,  X  X  X embrane associated transporter protein X  (GeneID fragments mentioning the gene. To obtain such synonyms, we used two sources of information: the article itself and a gene h KEYWORD i and h GLOSSARY i , in which a gene name and its synonym may be explicitly stated. mation to expand the gene name. Incidentally, we also examined the use of body text because gene name abbreviations often classification in our preliminary experiments and thus was not used thereafter.
 As another source of gene name expansion, a gene name dictionary was automatically compiled from existing databases. symbols.
It is often the case that gene name dictionaries automatically compiled from existing databases, such as ours, are noisy important for general-purpose gene name recognition systems. Fortunately, the quality of a dictionary would not be as important in our application, because even if the dictionary provides wrong gene names as synonyms of a given gene, those associated. ered a paragraph as a unit of analysis. Here, the problem is that, besides synonyms, gene names often have many variants due to arbitrary use of special symbols, white space, and capital and small letters ( Cohen, 2005; Cohen, Acquaah-Mensah, the work by Cohen et al. (2002) . Note that the actual order of applying the rules does not make difference. Replace all special symbols (non-alphanumeric characters) with space (e.g., Insert space between different character types, such as alphabets and numerals (e.g.,
Insert space between Greek alphabets and other words (e.g., Lowercase all characters.
 paragraph.

We have so far obtained gene name synonyms and normalized both gene names and text to facilitate gene name iden-differences. To deal with the problem, we used approximate word matching. To be precise, for each target gene name (de-noted as gene ) and each candidate which mentions any word composing the gene name (denoted as candidate ), a word-over-lap score defined below was computed, where M and U represent the number of matching and unmatching words, respectively; a is a penalty for unmatching words resent the h article, gene i pair after stopword removal and stemming by the PubMed stopword list data set).
 appropriate unit. In Section 4.2 , we will examine other alternatives. 2.1.2. MeSH terms
Along with the article itself, we took advantage of external resources, specifically, Medical Subject Heading (MeSH) biomedical articles and is annotated by human experts at NLM.

For each input article, all the associated MeSH terms were obtained from the MEDLINE database (i.e., a set of paragraphs) representing a pair of the article and any gene coupled with it. Note that a special symbol concatenated to each MeSH term so as to distinguish MeSH from other terms. 2.1.3. Feature selection
Feature selection identifies the features (terms) that are more informative in terms of classification according to some 1997 ). For this work, we adopted the v 2 statistic and compute it for every term composing the  X  X  X ocuments X  obtained through the previous steps. v 2 Statistic of term t in class c is defined as t , v 2 statistic was computed for every class, and the maximum score was taken as the v v 2  X  t  X  X  max i v 2  X  t ; c i  X  . Only the top n terms with higher v chose n  X  3000 based on our preliminary experiments. It should be mentioned that, for real-world applications, a smaller feature set would be preferred from the view point of computer resources and processing time. However, this work pursues highest performance possibly obtained by the proposed framework, where those issues are secondary. 2.1.4. Term weighting conventional TFIDF (term frequency-inverse document frequency) defined as ber of documents in which term t appears. In cases where TF  X  t ; d  X  X  0 ; TFIDF  X  t ; d  X  is defined to be 0.
We also examine another term weighting scheme, so called supervised term weighting , proposed by Debole and Sebastiani statistic. Specifically, we test two variants of the scheme, denoted as TFCHI 2.2. kNN classifiers
The decision rule can be expressed as where n c is the k nearest neighbors having class c 2f BP ; MF ; CC g ; ilarity between the arguments. Threshold t c can be optimized to maximize an arbitrary metric (e.g., F data.
 positive classes.

We slightly modified the standard scoring scheme above to multiply the similarity scores by the number of k neighbors having class c , denoted as j n c j , improved classification (around 2% in F 1 score), presumably because for evaluation we used micro-averaged F emphasizes larger classes (see Section 3.3 ). 3. Experimental setup 3.1. Implementation
We implemented for evaluation the categorization framework described in Section 2 in the R programming language, where data preprocessing including feature selection was done by Perl scripts. All the experiments reported in this paper were carried out on a PC running Linux with two 2.00 GHz Intel Pentium 4 processors and 3.5 GB of RAM. 3.2. Data sets one or more genes and each gene is annotated with one or more classes (BP, MF, and CC) or negative by MGI curators. The negatives) for the training and test data, respectively. Because gene names often contain Greek alphabets, character entities used for representing Greek alphabets (e.g.,  X  X &amp; (e.g., alpha ) in advance to facilitate gene name identification.

The training data were used for tuning parameters including the number of k neighbors and per-class thresholds t (6) and were used as pre-labeled instances in classifying the test data by k NN. 3.3. Measures
Following the TREC Genomics Track, we used the micro-averaged F as to make our results directly comparable with the official evaluation. Micro-averaged F paying equal importance to each instance (or label) and hence more influenced by the performance in larger classes, (NEG), 4. Results and discussion 4.1. Primary results
Our proposed framework for GO annotation was applied to the test data, where the per-class thresholds t other parameters including the number of k neighbors were optimized to maximize F the training data. 13 Table 1 compares our results (TFIDF, TFCHI ( Hersh, 2004 ). Incidentally, the best result in TREC was also obtained by our group using TFCHI tafa, 2005 ). The improvement from 0.561 ( X  X  X est X ) to 0.586 ( X  X  X FCHI tion and classification. It should be also mentioned that the TFCHI codes annotation task at the TREC 2004 as well (see Hersh, 2004; Seki et al., 2004 ).

We can observe that, despite the simplicity of our approach, it performed quite well especially with TFIDF and TFCHI the following sections, we take a closer look at major features or components of our framework and discuss their contribution. 4.2. Additional experiments 4.2.1. Alternative settings
We have made a number of design decisions in developing our framework and system features. To investigate the impact the features or components below had made any impact on GO annotation.

Gene name identification: We identified the paragraphs that were likely to contain the target gene using approximate word matching (see Section 2.1 ). Did it actually improve GO annotation? To answer the question, we used exact word matching to identify gene names.

Gene name dictionary: Assuming gene name identification above worked, did the dictionary for gene name expansion contribute to the performance? We examined our framework without the help of the dictionary. Glossary and keyword fields: Similarly, did the use of SGML tags, h onyms improve GO annotation? We tested our framework without the information.

MeSH terms: Did the inclusion of MeSH terms contribute to classification? We tested our framework without using MeSH terms.
 Unit of extraction: Was a paragraph as a unit of extraction appropriate? We explored other units:
Note that, however, we did not let both G  X  S and P  X  G  X  S go beyond paragraph boundaries. Incidentally, our framework focusing on paragraphs would be placed somewhere between P  X  G  X  S and ART. 4.2.2. Empirical observations
Table 2 summarizes the system performance in F 1 for each of the experimental settings described above, where the train-before. Note that the bottom row  X  X  X efault X  corresponds to TFCHI t for k NN was optimized on the evaluation data themselves for this experiment in order to compare possible maximum gain by different settings.
 posed here is needed in order to exhaustively locate gene name occurrences. A possible drawback of approximate word 0.3 (which was used for our experiments) constantly yielded the best performance. 4.2.2.2. Gene name dictionary. Disabling the gene name dictionary also decreased F the target gene, which would have been ignored otherwise. Although our dictionary is automatically compiled without man-synonyms was not found to be helpful for GO annotation. There was little or no difference between the F and synonyms found in these fields out of 882 articles in the training and test data sets. 4.2.2.4. MeSH terms. Similarly to the case of glossary and keyword fields, the F tation, we inspected precision and recall at different thresholds  X  t itives (which decreases precision), resulting in seemingly little difference in terms of F serve much difference whether or not MeSH terms were used. The reason why they were found irrelevant for GO annotation would probably be that while MeSH is annotated with an article, GO annotation is gene-specific. In other words, GO terms annotation if each MeSH term could be precisely associated with an individual gene. immediately preceding and succeeding sentences) and then decreases when the entire article was used (i.e., ART) compared ence from G , G  X  S ,or P  X  G  X  S is insignificant.

To highlight the effect of extracting gene-bearing paragraphs, Fig. 3 compares recall-precision curves for selected para-graphs (Default) and entire article (ART) on the training and test data.

The top two curves were obtained on the test data and the bottom two were obtained on the training. Overall, it can be only relevant paragraphs suppresses false positives when lowering the threshold. 4.2.3. Contributions of different parts of articles
Our current framework makes use of paragraph boundaries in extracting text fragments containing the target gene. How-tation because different parts of articles may have different importance with respect to GO annotation. For example,
Therefore, we examined how useful the individual sections were for GO annotation by using only one section at a time from which gene-bearing paragraphs were extracted. Specifically, we focused on the following sections: abstract, introduction, procedures, methods, and results. Both discussion and conclusion sections were regarded as result sections since they are tified based on section names annotated by SGML tags.

Fig. 4 shows a histogram for F 1 scores produced using individual sections on the training data, where we include results from the use of only titles and only MeSH terms for comparison. The rightmost bar  X  X  X ll X  used all the sections including MeSH, which corresponds to  X  X  X efault X  in Table 2 .

Surprisingly, the Result section alone yielded almost as good F dures), and so on. On the other hand, the Method sections showed the worst performance X  X orse than only titles X  X or GO annotation. This is, however, consistent with the suggestion given by experts that  X  X  X aterials and Methods X  sections can be used for identifying species and GO evidence codes but should not be used for predicting GO terms ( Camon et al., 2005 ). Although not presented here, our experiments on the test data also showed similar results.
In an attempt to improve GO annotation, we explored a better use of the logical structure of articles in two ways: (a) we it was found in Title and Method sections, respectively, as  X  X  failed to improve system performance and rather decreased F article structure. 4.3. Discussions on alternative feature selection policies 4.3.1. Round-robin feature space allocation
As described in Section 2.1.3 , we selected discriminative features (terms) based on the maximum v how many features took maximum v 2 for each class. As a result, the distribution of features is not necessarily balanced.
In fact, among the 3000 features selected for the preceding experiments, 242 took maximum in class BP, 339 in MF, 463 in CC, and 1956 in NEG.

An alternative feature selection policy would be the round-robin proposed by Forman (2004) , which allocates the feature ones. On 19 different benchmark data sets, Forman demonstrated that the round-robin policy could gain a substantial improvement over max v 2 especially when the feature size was small.

In order to examine if Forman X  X  observation holds for GO annotation, we tested the round-robin policy on our data sets setting comparison. The results are summarized in Table 3 .

In brief, the results are mixed; when adopting round-robin, F to include a reasonable number of features for each class without applying round-robin even though the distribution is the others.
 ducted another set of experiments by looking at more specific GO terms instead of the GO domains. The next section de-scribes the design of the experiments and reports the results. 4.3.2. GO term annotation
Each gene (or its product) appearing in the TREC Genomics Track data set is annotated with one or more GO terms along with GO domains. For example,  X  X  X ascular cell adhesion molecule 1 X  (GeneID: 22329) appearing in a MEDLINE article (Pub-
Med ID: 12021259) is assigned, in addition to a GO domain code BP, a more specific GO term  X  X  X ell adhesion (GO code: barely used ( Ogren, Cohen, &amp; Hunter, 2005 ), meaning less training instances for many classes.
For each depth level from 1 (corresponding to GO domains) to 6, we carried out experiments to assign GO terms at that level. Note that a greater depth level involves more classes (GO terms). Since the annotated GO terms in our data set had different depth levels, we traced back each annotated GO term to the root of the ontology and re-assigned GO term(s) at any given level in-between. For instance, suppose that we are attempting to annotate GO terms at the second depth level. tested again TFCHI 2 and TFIDF with/without adopting round-robin. Fig. 6 shows the plots for the training data (left-hand
As can be seen, we still have mixed results. For the training data, adopting round-robin (shown as dotted lines) deterio-
GO annotation even for a larger number of classes by contraries. However, the next section demonstrates that the feature selection policy does make difference when the features associated with negative instances are properly treated. 4.3.3. Effects of negative features
For GO domain annotation, we have selected predictive features based on the v
CP, and NEG) were considered. In classification, however, the features whose v used. (Remember that those instances which were assigned none of the GO domains were considered as negatives.) Thus, be allocated to the other classes. In other words, including more features not for NEG but for GO domains/terms may boost for short.

We tested this idea in conjunction with round-robin to see how those negative features interact with different feature selection policies and consequently affect GO annotation. Fig. 7 plots F
Now, observe that round-robin used with TFCHI 2 outperforms the others both on the training and test data after the tive features based on the standard max v 2 as the number of classes increases. When excluding negative features, round-robin was able to avoid the pitfall with no or little side effect at lower depth levels.

It should be also noted that TFIDF did not work well as compared with TFCHI spite the fact that they used exactly the same set of features. Although round-robin improved F the second and third depth levels, it does not match TFCHI a good term weighting scheme depending on several factors, including the number of classes considered, feature selection policy, and negative features. We will return to this point later in Section 5 . 5. The triage task 5.1. Overview The framework we described so far targeted GO annotation. However, it can be also applied to another task from the TREC binary text categorization problem to classify input text (article) into two bins, i.e., positive and negative.
In terms of text categorization, a primary difference between GO annotation tackled in the previous sections and the tri-mentioned in a given article, our framework to use only gene-bearing paragraphs may be more appropriate. Thus, we adapted our system to extract paragraphs that were likely to contain any gene name identified by gene name recognizer
YAGI ( Settles, 2004 ). Note that MeSH terms associated with a given article were also included as features as in GO annotation. 5.2. Methods We used the same methods described in Section 2 for document representation and classification except the following:
For document representation, paragraphs that were likely to contain any gene name (determined by YAGI) were used. Fea-ture selection and term weighting were done in the same ways as GO annotation. Note that the issues regarding feature selection policies raised in GO annotation do not exist for the triage task since there are only two classes; v given term takes the same value for both classes, and thus there is no need for round-robin or special consideration for negative features.
 5.3. Data and evaluation measures
We used the TREC data set provided for the triage task, which was composed of 5837 full text articles for training (375 cifically, the number of neighbors k and the value of threshold t the normalized utility measure (explained next) on the training data.

For the evaluation measure, normalized utility measure U norm where identified as positive (false positives), respectively. The coefficient u stances. For the Genomics Track 2004 data set, u r was set to 20.
 5.4. Results and discussion are also presented.

Our framework with the term weighting scheme TFCHI 1 compared favorably with the best performing system developed reported that a simple rule which classified articles annotated with the MeSH term negative could have achieved nearly as good performance as the best reported result ( Dayanik et al., 2004 ).
Unlike TFIDF, TFCHI considered word distributions across different classes and was able to assign higher weights even to the terms that appeared in many documents but almost only within a class, such as where MeSH terms are indicated by capitalization.

As can be seen, high-v 2 words, such as Mice , Animals , class-based term weights such as v 2 statistic is more appropriate for classification.

It may be also possible, however, that our framework with TFCHI value of v 2 associated with the MeSH term Mice (remember that simple heuristics using To examine the possibility, we applied the TFCHI 1 scheme to hypothetical test data where the MeSH term comparable to the second best system ( Fujita, 2004 ), which yielded a U 5.5. Concept drift training data with a decrease ranging from 22.2% to 40.5% in U even though frequent terms between the two data sets overlap more than 90% of times.

To examine whether our method is subject to the same problem concerned with the concept drift, we looked at how well our system performed on the training data. Table 5 compares Cohen et al. X  X  result using voting perceptron (which produced their best) and ours with the three different term weighting schemes, where the system performance is shown in U data demonstrating concept drift. 5.6. Follow-up experiments on the TREC 2005 data set
The triage task continued to be challenged at the following Genomics Track 2005 ( Hersh et al., 2005 ). the parameters including feature size n , number of neighbors k , and thresholds t score using the training data. The results are summarized in Table 6 .

As shown, our approach with TFCHI 1 yielded comparable performance with the best U formance among the TREC participants. 6. Related work it looks at another important workshop, the BioCreative challenge, which in part targeted GO annotation.
For GO domain annotation, Settles and Carven (2004) developed a two-tier classification framework using na X ve Bayes (NB) classifiers and maximum entropy (ME) models with several external resources and specialized features. They exploited low parser. The informative words were word n -grams  X  1 6 &amp; Blaschke, 2003 ) and MEDLINE abstracts with which specific genes and GO codes were associated in existing databases other than MGI. The reported F 1 score was 0.514, which is 12.3% lower than our best score reported here. The difference is presumably due to the fact that their system did not employ gene name expansion and approximate word matching which we found highly important for GO annotation.

For the triage task at the Genomics Track 2004, Dayanik et al. (2004) applied Bayesian logistic regression (BLR) models, which estimate a probability that an input belongs to a specific class. For document representation, they used MeSH terms ration. They used only title, abstract, and MeSH terms for features and applied the conventional TFIDF term weighting scheme, and proposed a two-stage classifier which assigned negative to all articles not indexed with the MeSH term which yielded suboptimal results in our experiments, their approach outperformed other TREC participants thanks to the first-stage filtering.

For the following Genomics Track 2005, Niu et al. (2005) proposed a framework incorporating a domain-specific term based on some form of a statistical analysis on the Genomics Track data set, they compared the word distributions in the near those domain-specific bigrams were used for document representation, where the window size was set to 2. As a clas-reported that SVM had shown the best performance ( U norm bination. Judging from the fact that our approach not using corpus comparison yielded a comparable performance with theirs, the effect of the domain-specific terms may be limited.
 as input but aimed at predicting actual GO terms and providing a passage which supports each GO term prediction. In order supports the relation between the associated protein and GO term. Because this evaluation was done only on the submitted positives for the purpose of computing recall, the highest (and likely overestimated) F 0.216 by Ray and Craven (2005) . Although the lessons and data set obtained through the BioCreative challenge are indeed al., 2005; Camon et al., 2005 ).

Following the BioCreative challenge in which most participants looked for evidence in a given article, Stoica and Hearst fact that orthologous genes X  X imilar genes found in different species but originated from a common ancestor X  X ften have the same functions. Stoica and Hearst used this information as a constraint and considered only the GO codes that have been are cases where some GO terms are not usually co-annotated together to the same gene because annotating them together is
GO code assignment. On the BioCreative data set, their proposed approach achieved an F Ray and Craven X  X . Their approach were also tested on two different data sets from EBI human and MGI databases, producing
F of 0.118 and 0.140, respectively. 7. Conclusions
This paper presented our work primarily on automating GO domain code annotation. We approached this task by treating get gene. To exhaustively locate the gene name occurrences, we took advantage of existing databases to automatically com-pile a gene synonym dictionary and preprocessed both gene names and text to tolerate minor differences between them. In names. The collected words were then fed to feature selection using v supervised term weighting schemes.
We evaluated the proposed framework on the TREC 2004 Genomics Track data sets and showed that, overall, our method the round-robin policy became prominent as the number of classes increases. Furthermore, it was demonstrated that our of 0.651 and 0.578 for the TREC 2004 and 2005 data sets, respectively, which were found comparable to the best reported performance. In addition, the TFIDF term weighting scheme was found suboptimal for this particular task and data sets.
Such information may be incorporated into the current framework by way of term weights or a different representation of challenge.
 Acknowledgments This project is partially supported by KAKENHI #19700147, the Nakajima Foundation, the Artificial Intelligence Research
Promotion Foundation Grant #18AI-255, and the NSF grant ENABLE #0333623. We would like to thank Dr. Fabrizio Sebas-icantly improved the manuscript.
 References
