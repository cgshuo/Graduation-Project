 1. Introduction As data sets become larger and more high-dimensional, it becomes increasingly important to find data represen-tations that allow compact storage and efficient distance computation and retrieval. Among the common methods to achieve this is Locality Sensitive Hashing (LSH) [1]. LSH is a framework for mapping vectors into Hamming space, so that the distances in the Hamming hash space reflect those in the input space: similar vectors map to similar hashes. Many LSH schemes have been proposed, divided between metric-driven [2], [3], [4], [5], with the goal of approximating a given distance metric, and data-driven [6], [7], where the hash functions are learned to optimize performance on a task such as classification. In this work, we mostly follow the metric-driven approach, assuming that a good distance function is given (or already learned).
 tors to a sketch in the Hamming hash space, we are able to considerably reduce the number of bits occupied by each data point while still being able to compute accurate approximations to the distances among the data. The distance computations become faster not only because the hash representations are more compact but also because the Hamming distances could be computed using bit oper-ations which involve the minimal amount of floating-point computation and can benefit from bit-counting instructions available on modern processors. Hash representations ofte n allow sublinear-time retrieval of near neighbors from larg e databases (e.g. [2]). In addition, the compact representa-tions and fast distance computations can be used in kernel learning methods such as SVM or Kernel PCA for efficient kernel approximation.
 Weighted Jaccard and  X  1 . Our main motivation is text and image retrieval, where histograms, either normalized or unnormalized, are often used to represent documents (tf-idf-weighted histograms of terms or n -grams) or image statistics (such as histograms of color or texture). T = ( T k ) is defined as k S  X  T k 1 = P k | S k  X  T k | . This distance is a natural fit for data such as histograms, and is less sensitive to outliers than the Euclidean distance. non-negative entries is defined as 1  X  J ( S , T ) where J is the Jaccard Similarity There is a simple relationship between Jaccard similarity considering the cases S  X  T and S  X  T ), we have In the case where the data is  X  1 -normalized, the norms k S k 1 and k T k 1 are fixed and so the Jaccard similarity is a monotonically decreasing function of the  X  1 distance. If the data is not normalized then knowing J ( S , T ) , k S k 1 k T k 1 allows one to compute k S  X  T k 1  X  so if compact representations of S and T allow one to approximate J ( S , T ) then their  X  1 distance can also be approximated by augmenting the compact representation of every vector with that vector X  X   X  1 norm. 2. Related Work Minhash for unweighted sets or binary vectors was intro-duced by [3]. They showed that the probability of hash collision is given by (1), which for binary data represents the ratio of the size of the intersection to that of the union. Extensions have since been proposed. A method dealing with integer weights is mentioned in [5] and was extended in [8] to the case of S k = W k  X  k where W k is an integer weight and  X  k is an input-independent real-valued multiplier that is fixed for each index k . For instance, W can be the frequency of the term k in a document, while  X  k is its inverse document frequency. The time to compute a hash is O ( P k W k ) .
 gests a rejection sampling approach for handling real-valued weights: a sequence of samples ( k, y ) is sampled from an upper bound on all possible vectors, and the first sample satisfying y  X  S k is output. This method is not suitable for sparse vectors and widely varying feature weights.
 expected constant time per non-zero weight, and can be made quite fast in practice through a number of optimizations. Here, we propose an algorithm that runs in worst-case constant time , is simpler to implement, and has the added advantage of requiring a fixed number of random values (3 random numbers per index k ) which means that if the possible set of indices is known in advance then all the random values can be generated offline.
 have the same  X  1 norm, Weighted Jaccard Similarity is a monotonic function of  X  1 distance, therefore our hashing technique can be used in such cases. One of the best known LSH methods for handling  X  1 distances is based on stable distributions [2]. As we will show, our method has better performance characteristics for retrieval and sketching under some common conditions.
 by using a fixed number of bits per hash. This problem was previously addressed in [11], where each Minhash value is mapped to its lowest b bits. Compared to that work, our approach has two advantages. One is that it is more general and applies to any underlying hashing scheme, not just Minhash. The other is that by mapping hashes to b -bit representations randomly we end up with much simpler estimators of the distances between inputs, given the Hamming distances between their hash vectors. 3. Consistent Weighted Sampling In this paper, we follow [10] who presented an algorithm for consistent weighted sampling from a weighted set and showed that consistent sampling leads to an LSH scheme where the probability of hash collision is equal to the Jaccard similarity.
 that generates, for any vector S = ( S k  X  0) , a sample ( k, y ) : 0  X  y  X  S k which is uniform and consistent . from  X  k { k } X  [0 , S k ] , i.e. the probability of selecting k is proportional to S k , and y is uniformly distributed on [0 , S k ] .
 S k  X  S k ), a sample ( k, y ) is selected for S and satisfies y  X  S  X  k , then ( k, y ) would be selected for S  X  as well. easy to show that the probability of collision is the Jaccard similarity: Pr[sample( S ) = sample( T )] = To demonstrate this, consider a sample ( k, y ) from the union of S and T , R = ( R k = max( S k , T k )) . By consistency, if y  X  min( S k , T k ) then ( k, y ) would be selected for both S and T , resulting in a collision. Otherwise, ( k, y ) would be selected for one of S or T (the former if y  X  S k , the latter if y  X  T k ), but not the other, thus no collision. Now using the uniformity, Pr[collision] = Pr[ y  X  min( S k , T k )] = J ( S , T ) . weighted samples (or invertible functions of such samples) as Weighted Minhash , since they generalize the min-wise permutation hashes of [3] from binary to real weights. In sec. 3.1, we will show how to extend the binary case (standard minwise-hashing) to integer weights and then to real weights. The resulting algorithm is presented in sec. 3.2. We then prove its correctness in sec. 3.3, and describe important data-driven optimizations in sec. 3.4. 3.1. From Integer to Real Weights Restricting our problem to integer weights, one sampling scheme [5] works by drawing independent, identically distributed random numbers (from some fixed distribution) v ( j ) for each ( k, j ) : j  X  { 1 , . . . , S k } , and returning for consistency that v k ( j ) depend only on k and j and not on S . In practice, we achieve this by using ( k, j ) as the seed for the random number generator used to draw the corresponding value. This scheme is uniform because all v k ( j ) are i.i.d. and so have equal chances of being the minimum. It is also consistent: if  X  k S  X  k  X  S k then achieved if and only if the element ( k  X  , j  X  ) achieving the minimum for S on the right-hand side also appears on the left-hand side, i.e. if j  X   X  S  X  k  X  . weights, by quantizing each S k with granularity  X  . For each index k we compute and define a k as the corresponding minimum (achieved for x = y k ). Then the sample returned is ( k  X  , y  X  ) where k  X  = arg min k a k and y  X  = y k  X  . Clearly, as  X   X  0 , the integer weights tend to infinity, which makes it impossible to sample all the v k ( x ) directly. However, it is easy to see that y k must be one of  X  X ctive indices X , where v k ( y ) &lt; min x&lt;y v k ( x ) . Fig. 1 illustrates this. In [10], the active indices are sampled in expected constant time per non-zero weight. Let us study the properties of active indices to find a sampling scheme that is worst-case constant time. let  X   X  0 . It is easy to see that v ( y ) = min x  X  S v ( x ) is achieved at the largest active index y satisfying y  X  S . Let z be the smallest active index greater than S , so that y and z are the active indices bracketing S (as shown in Fig. 1).
 y is uniformly distributed on the interval [0 , S ] . (We often use the closed-and open-interval notation interchangeabl y, since the endpoints have measure zero.) Therefore, y = u S where u 1  X  Uniform(0 , 1) .
 cumulative probability function cdf( z ) . For z  X  S , we can see that cdf( z ) is the probability that the interval [ S, z ] contains a value v ( x ) which falls below the minimum achieved on [0 , S ] . In other words, which is the probability that the smallest v ( x ) over 0  X  x  X  z is achieved for x &gt; S . Since all v ( x ) are i.i.d., we have cdf( z ) = z  X  S z = 1  X  S/z (and so P ( z ) = cdf  X  S/z 2 ). Therefore, we can write z = cdf  X  1 (1  X  u 2 ) = where u 2  X  Uniform(0 , 1) (and so is 1  X  u 2 ). Note that z is independent of y  X  knowing where the minimum v occurred on [0 , S ] tells us nothing about where the first smaller value will be obtained on ( S,  X  ) .
 (ln S + ln u 1 ) =  X  (ln u 1 + ln u 2 ) , where u 1 , u 2 Uniform(0 , 1) . Significantly, r does not depend on S . To compute the distribution of r , we first note that if t 1 =  X  ln u 1 then P ( t 1 ) = P ( u 1 ) du 1 dt 1 = e  X  t similar for t 2 =  X  ln u 2 . Then, r = t 1 + t 2 is the sum of independent variables with the distribution P ( r ) = R Gamma(2 , 1) distribution: defined for r  X  0 . Consider a transformation of vari-ables ( y, z )  X  (ln y, r ) . The distribution P ( y, z ) is de-fined for 0  X  y  X  S  X  z and, from independence, P ( y, z ) = P ( y ) P ( z ) = (1 /S )  X  ( S/z 2 ) = z  X  2 distribution P (ln y, r ) is defined for r  X  0 and for ln y such that ln y  X  ln S and ln z  X  ln y = r for some z  X  S ; in other words, ln y  X  [ln S  X  r, ln S ] . From the transformation properties of probability distri-z P (ln y | r ) = P (ln y,r ) P ( r ) = 1 /r which means that, condi-tioned on r , ln y is uniformly distributed on [ln S  X  r, ln S ] . [ln S  X  r, ln S ] has a critical effect on the consistency of our sampling scheme. For example, one option is to set ln y = ln S  X  rb where b  X  Uniform(0 , 1) does not depend on S . This, however, does not produce consistent samples: even a small change in S will cause y to change  X  while in a consistent scheme we should have a non-zero probability of producing identical samples for similar inputs. Instead , we use a uniform  X   X  Uniform(0 , 1) to specify the offset of a regular grid with spacing r , and select, as the sample ln y , the unique element of the grid that falls inside the interval (ln S  X  r, ln S ] . Specifically, we set The use of the piecewise-constant floor function ensures that y and z are usually unaffected by small changes in S , and give rise to consistent samples (as we will show below).
 dices k to produce the consistent sample. Notice that we are free to choose any distribution for the v k ( x ) . Because the minimum of a set of exponential variables is itself exponential, we will draw each v k ( x ) from the exponential distribution with rate  X  : P ( v ) =  X  e  X   X  v , v  X  0 . It is easy to show that the minimum a of S/  X  such variables is then exponential with rate S , and the product  X  1 = aS is exponential with rate 1 : P (  X  1 ) = e  X   X  1 . next active index z &gt; S , which is where v ( z ) drops below a for the first time. The probability of any v falling below a is Pr[ v &lt; a ] = 1  X  e  X   X  a  X  1  X  (1  X   X  a ) =  X  a , and it follows that, as  X   X  0 , the waiting time z  X  S until encountering a sample below a is exponential, with rate a . Therefore,  X  2 = ( z  X  S ) a is exponential with rate 1. z = c/z where c =  X  1 +  X  2  X  Gamma(2 , 1) : P ( c ) = ce  X  c . This means that a can be sampled conditioned on z and independent of S . Furthermore, if z is not usually affected by small changes in S then neither is a , giving the promise of consistency  X  which we will prove in Sec. 3.3. 3.2. Consistent Weighted Sampling Algorithm From the derivations in Sec. 3.1, we obtain a scheme for sampling from a weighted set S = ( S k  X  0) .  X  For each k :  X  Find k  X  = arg min k a k , and return the sample If we do not care about the actual sample but only the collision of samples, then we output the Weighted Minhash ( k  X  , t k  X  ) instead (since the mapping t k  X  y k does not depend on S ), which avoids the use of floating-point values as hashes. To compute multiple hashes, we simply draw different independent sets of variables r k ,  X  k , c k . tions by working in the log domain. To sample a variable r  X  Gamma(2 , 1) , we can represent it as r =  X  ln( u 1 u where u 1 , u 2  X  Uniform(0 , 1) . Alternatively, we can use the efficient Ziggurat method [12]. We can sample c in the same way, or can instead sample ln c directly, also using Ziggurat. Figure 2 summarizes the algorithm. 3.3. Proof of Correctness Below, we will prove that the algorithm does in fact return uniform and consistent samples. 1) Uniformity We shall prove that our sampling scheme produces a uniform sample from { ( k, y ) : 0  X  y  X  S k } . variable y = exp r ln S r +  X   X   X  , where  X  is uni-form, comes from the same distribution as y = exp(ln S  X  rb ) for uniform b (in both cases, ln y is uniformly dis-tributed on [ln S  X  r, ln S ] ). The distribution P ( y, z, a ) is defined for y  X  S , z  X  S and a &gt; 0 , and can be computed using a transformation of variables: independence of r , b and c , and computing the Jacobian, we get ) from { ( k, y ) : 0  X  y  X  S k } e Integration over z yields P ( y, a ) = which shows that y is uniform on [0 , S ] , a is exponential with rate S , and y and a are independent. Now considering all indices k , it is easy to show that, for a set of exponential variables a k with respective rates S k , the probability that a particular a k  X  is the smallest is proportional to S k  X  Pr[ a k  X  = min k a k ] = S k  X  / ( P k S k ) . By independence of a and y , the corresponding y  X  = y k  X  is uniformly distributed on [0 , S k ] . Therefore, ( k  X  , y  X  ) is uniformly sampled from { ( k, y ) : 0  X  y  X  S k } . 2) Consistency Let us fix all of r k ,  X  k and c k . We will show that for two weighted sets S , S  X  such that  X  k S k  X  S  X  k , if ( k  X  sampled for S and satisfies y  X   X  S  X  k , then ( k  X  , y  X  also be sampled for S  X  .
 assumption, y  X   X  S  X  k  X   X  S k  X  , we have We see that and so Therefore y k  X  = y  X  = y  X  k  X  where y  X  k  X  is the value sampled for index k  X  for input S  X  . For any given k , notice that a k = c k e  X  r k /y k is a monotonically decreasing function of y , which in turn is a non-decreasing function of S k . Thus, for all k , a  X  k  X  a k , while a  X  k  X  = a k  X  (since y  X  It follows that arg min k a  X  k = arg min k a k = k  X  which means that the sample ( k  X  , y  X  = y k  X  ) is output for S well as for S , and so our sampling scheme is consistent. 3.4. Efficient Hash Computation We can significantly speed up the computation, by com-puting simple data statistics. Observe that in each sample ( k  X  , y  X  ) (or, equivalently, hash ( k  X  , t  X  ) ), if we know k then y  X  depends only on k  X  and the corresponding weight S k  X  , but not on any of the other weights. Similarly, if we know that k  X   X  K then we only need to examine a subset of indices { k  X  K : S k &gt; 0 } to generate the hash. In practice, we have observed that we can find a rather small set K for each hash, such that most of the hashes have k  X   X  K . Different hashes may have different candidate index sets K , with some indices belonging to most candidate sets (if the corresponding S k are usually large) and some belonging to none (if their S k are usually zero or very small). We count how frequently each index k is chosen by the hash, over a large data set, and for future hash computations consider only the indices for which this frequency is above a threshold. In our experiments, we were able to reduce the number of considered hash/feature pairs by a factor of 200, while affecting only 0.5% of the hashes.
 of [10] is that for each feature and each hash we sample 3 random variables ( r , c and  X  ), independently of the input weight. This suggests that these random variables may be generated ahead of time (perhaps only for those hash / feature pairs which get selected sufficiently frequently)  X  eliminating the need to sample those for every input, and further reducing the hash computation time. 4. Weighted Minhash for Retrieval and Sketching under  X  1 Metric Below, we show how Weighted Minhash can be used for data in  X  1 spaces. We will discuss the use of these hashes for near neighbor retrieval and for sketching the data, where the  X  1 distance between pairs is approximated based on their hashes. We will demonstrate how the hashes can be mapped to a small number of bits to save space, and how the theoretical properties of Weighted Minhash com-pare with the commonly-used random projection method. It should be pointed out that the discussion in this section is not specific to our method, and applies to any consistent weighted sampling scheme. 4.1. Retrieval under  X  1 Metric Often, we deal with  X  1 normalized data, such as histograms  X  i.e., for all S , S 1 = 1 . In this case, Weighted Jaccard Similarity is a monotonic function of  X  1 distance: This allows Weighted Minhash to be used for efficient retrieval under  X  1 . The constraint on the norm is the only one that we impose  X  the non-negativity constraint in (1) can be relaxed by replacing each weight S k with a pair which preserves  X  1 distances and norms.
 bution method of [2], which uses a quantized dot-product with a vector of Cauchy variates. The crucial quantity affecting the retrieval speed in the ( R, c ) -approximate is the probability of hash collision for points at distance R &gt; 0 and p 2 is the probability for points at distance Rc , with c &gt; 1 the approximation factor. Smaller  X  results in faster retrieval. Both [1] and [2] propose hashin g schemes that achieve  X  = 1 /c . With Weighted Minhash,  X  ( R ) depends on R and is defined on R  X  (0 , 2 /c ) :  X  values of c . We can show that lim R  X  0  X  c ( R ) = 1 /c and lim R  X  2 /c  X  c ( R ) = 0 . Furthermore,  X  c ( R ) decreases on (0 , 2 /c ) for all values of c that we examined. We can see that  X  c ( R ) &lt; 1 /c and so, in the case of  X  1 -normalized data, Weighted Minhash allows for more efficient retrieval than the stable distribution method.
 Minhash can also be faster to compute. Indeed, computing a single stable-distribution hash requires us to consider every non-zero weight S k 6 = 0 in the input vector to compute the dot product. On the other hand, with the preprocessing described in Sec. 3.4, we can consider only a small fraction of the features to compute a given hash  X  which is a win compared to the dot product with a Cauchy vector, even though our algorithm spends more time on each weight that it actually considers. 4.2. Weighted Minhash for Sketching Under  X  1 We often want to represent the data using as few bits as possible, while being able to reconstruct the distances efficiently and accurately. We now present a method for accomplishing this with Weighted Minhash, study the accuracy of the  X  1 distance estimation for a given number of bits, and compare with several existing methods. containing its  X  1 norm and H Weighted Minhash values, where each WMH h is an independent sample from the parameterized family H of hash functions: Recall that Pr WMH  X  X  [WMH( S ) = WMH( T )] = J ( S , T ) , where the probability is with respect to a random selection of the hash function from H . Since all of WMH h are independent random samples from H , the normalized Hamming similarity
HashSim H ( S , T ) = (where  X  is Kronecker X  X  delta) is the average of H i.i.d. Bernoulli variables. Thus the Hamming similarity has the expected value of J ( S , T ) and variance allows us to approximate k S  X  T k 1 . Indeed, P k min( S k , T k ) = P k ( S k + T k  X  | S k  X  T k | ) / 2 , and similar for P k max( S k , T k ) , so Defining N = k S k 1 + k T k 1 and d = k S  X  T k 1 , we have Replacing J with HashSim H ( S , T ) yields an estimate  X  d  X  d .
 imate the relationship between J and d as being linear. Then, 4.3. Generating b -bit hashes In the above discussion we ignored the number of bits required to represent each Weighted Minhash WMH h . One possibility is to use one of the standard compression techniques so that the average number of bits per hash is roughly the entropy of the hash. This approach, however, has several disadvantages. One is that the distance com-putations between hashes become much more expensive, as each hash needs to be decompressed on the fly. More importantly, it may often be advantageous to reduce the number of bits per hash at the cost of some spurious hash collisions, with the benefit of being able to use more hashes. The extra hash collisions, which a compression scheme would work hard to avoid, may not be quite so harmful in reality, since it is unlikely that the spurious collisions of different hashes would be consistent enough to significantly affect the distance estimates. This insight w as has been successfully used in the context of hash learning [6]. Below, we propose a scheme to represent each hash using a given number b of bits  X  where b trades off between the number of hashes and their accuracy. This problem has also been addressed in [11], but the method below applies to any hashing scheme and not just Minhash, and allows simpler analysis and much simpler estimators of distances given the number of hash collisions.
 map it to a b -bit value. First, we initialize a random number generator using ( h, WMH h ( S )) as the seed. Then sample an integer b -bit value WMH ( b ) h uniformly from { 0 . . . 2 b  X  1 } .
 bit hashes are too. For different original hashes, on the other hand, the b -bit hashes collide with probability 2  X  b Therefore, and, denoting by HashSim ( b ) H the Hamming similarity of H b -bit hashes, we can approximate Similar to (4), we obtain that the variance of the estimate  X  d ( b ) of the distance d = k S  X  T k 1 depends on H and b as follows: Denoting by B the total number of bits available, we have H = B/b , and us to reason about the optimal number of bits b to represent each hash, assuming that the pairs of vectors we care about have similar distances d and sums of norms N . As an alternative, we may choose b empirically to obtain the most accurate estimate  X  d for the vector pairs of interest. 4.4. Weighted Minhash or Random Projections? Let us now consider another commonly used Locality Sensitive Hashing method, based on Stable Distributions [2]. Here, each hash is a quantized dot product between the vector S and a vector of independent Cauchy variates. The method has a single parameter r , the quantization granularity. For given S , T and r , let us define u = k S  X  T k 1 . Then, the probability of hash collision is We can estimate p as the fraction of colliding hashes, and recover d = k S  X  T k 1 by inverting p ( u ) . For H hashes, the variance of the estimate is To represent the hash in a fixed number b of bits, we can consider several schemes. One is the scheme described above, where each hash is mapped to a random b -bit sequence. Alternatively, we propose to minimize spurious collisions by making the regions represented with the same b bits as separated as possible, as follows: where c is the vector of Cauchy variates and  X  is the random offset. Numerical minimization with respect to r and b shows that for a given distance d = k S  X  T k 1 and available number of bits B the smallest is achieved by the scheme in (9) with r  X  4 . 5 d and b = 1  X  that is, the random projections are quantized, and the quantization bins are alternately assigned to bits 0 and 1 (for comparison, random assignment of each bin to a single bit yields the estimate variance of 11 . 6 d 2 /B ). Note that the in this analysis we are being optimistic, assuming that for each d we can select the corresponding r value, which of course is not true in practice.
 storing non-quantized Cauchy projections, and estimating the k S  X  T k 1 as the maximum likelihood estimate of the scale of a Cauchy distribution. Such a method was proposed in [13], and yields where k is the number of projections. However, even if we can very aggressively compress the projections to 5 bits with no loss in distance estimation accuracy (so that k = B/ 5 ), the resulting variance exceeds that in (10). Cauchy random projections with quantization and remap-ping to bits, we observe that Var[  X  d ] scales inversely with B and quadratically with d , with the caveat for our method that N/d remains constant. Therefore, to compare the methods as well as different values of b for Weighted Minhash, in Fig. 4 we plot Var[  X  d ] /d 2 against d/N . One is that for a fixed d = k S  X  T k 1 we obtain smaller it is advantageous to make the  X  1 norms of the inputs as small as possible, while preserving distances. We propose to minimize the sum of  X  1 norms of the training data by determining, for each dimension k , the median weight k = median { S k } over the training data. Then, we subtract the medians from the weights and replace each weight with a pair to ensure non-negativity: It is easy to see that this transformation preserves the  X  distances, while minimizing the total  X  1 norm of the data. Minhash achieves better variance of the distance estimate than random projections with remapping to 1 bit, as long as the vectors are sufficiently different, i.e. k S  X  T k 0 . 153 ( k S k 1 + k T k 1 ) . On the other hand, in the near-duplicate scenario, where d/N &lt; 0 . 153 , we are better off using random projections. One explanation for this is that the norms of the inputs k S k 1 which we store alongside the Weighted Minhashes help us obtain better distance estimates when these distances are of the same order of magnitude as the norms. The norms become less useful, however, if the distances are much smaller  X  at that point they only amplify the variances.
 interested in become larger compared to the norms, we benefit from using more bits per hash, and need fewer hashes. 5. Experimental Results In our evaluations, we have concentrated on sketching, and analyzed how well the variances of the distance estimate obtained from sketches agree with the theoretical predictions.
 histograms of color, texture and other image properties, fo r a total of about 5  X  10 5 dimensions. The descriptors are somewhat sparse, with about 50000 non-zero weights per image. Some of the constituent histograms are normalized and some are not. We preprocessed the descriptors by subtracting from each dimension the median value of that dimension, computed over a separate set of images, ensuring non-negativity as in (11).
 the average distance k S  X  T k 1  X  19 . 9 . Looking up these values in Fig. 4, we find that for d/N  X  19 . 9 / (2 14 . 2)  X  0 . 7 , we should use Weighted Minhash, with 3 bits per hash. We computed the hashes for different values of b , with the budget of 3000 bytes per image ( B = 24000 ), and found that b = 3 in fact gives the best agreement between the distances and their estimates. Turning now to (6), we expect that Var[  X  d ]  X  0 . 0556 . In our experiment, we obtained almost exactly this value: averaging over many pairs of images, we got E(  X  d  X  d ) 2 = 0 . 0559 . with about 50000 non-zero weights per input, was approxi-mately 7 seconds. Following Sec. 3.4, we selected for each hash the 200 most-frequently selected input dimensions (as evaluated on a separate set of inputs). Now each hash looks only at the dimensions among the 200 which have non-zero weights; the sets of dimensions considered by different hashes may or may not overlap. With this optimization, the variance Var[  X  d ] increases slightly from 0 . 0559 to 0 . 0572 . However, the running time is reduced from 7 seconds to 47 milliseconds per image, i.e. a speedup factor of almost 150. 6. Conclusion We presented a new consistent sampling scheme, which improves on the best existing such scheme by making it worst-case constant-time per sample per non-zero input weight (compared to expected constant-time). Another advantage of our method is that all the random variates used for hash computation can be computed offline, in-dependently of the inputs. The time required to compute the samples was reduced by a factor of 150 by analyzing which samples are affected by which input dimensions. The consistent samples can be used as generalization of Minhash to weighted inputs, with the collision probability given by the Jaccard similarity. We exploit the relationshi p between Jaccard similarity and  X  1 distance to present a hash-based approximate near-neighbor retrieval scheme with performance characteristics superior to the previous ly existing ones. We discussed the use of Weighted Minhash for sketching that allows  X  1 distance approximation, and studied the accuracy of this approximation for a given sketch size. We showed how to map Weighted Minhash, or any other hash, to a fixed number of bits, and investigated the optimal trade-off between the number of hashes and the number of bits per hash. Our experimental results agree well with the theoretical predictions.
 References
