 Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia 1. Introduction
Subgroup discovery is a data analysis approach that aims at fi nding descriptions of subgroups of data instances with unusual statistical distribution of the pr operty of interest [21,22,37]. Currently prevailing subgroup discovery techniques infer subgroups from class-labeled attribute-value data sets. Prominent approaches in this area, such as EXPLORA [21], SD [7], CN2-SD [24], and APRIORI-SD [20] describe each subgroup through a classi fi cation rule. It X  X  condition part, expressed as conjunction of assertions on values of attributes, de fi nes the input attribute subspace where the data instances have an unusual distribution of the class variable. Rules that identify  X  X ood X  subgroups cover typically a large part of input attribute space where the majority of training data instances are labeled with a single class value. It is desired that predictive accuracy of such rules is high. Similarly to the methods of predictive induction [11], subgroup discovery aims to infer rules where such accuracy is attainable. But while the methods of predictive induction construct a comprehensive classi fi er that preferably performs well on the entire attribute space, methods of subgroup discovery infer rules that cover only a subset of training instances. Instead of maximizing the overall prediction accuracy, they focus on revealing and describing the meaningful data subsets.

Example applications of subgroup discovery include the analysis of clinical data [2,8,9], marketing analytics [4], gene expression data analysis [10], analysis of e-learning [31], and analysis of traf fi c accidents [19]. In each of these applications, the problem was conveniently represented with data in the attribute-value form, and classi fi cation rules were used to describe the subgroups. In general, subgroup discovery should assist us in hypothesizing the relations between a set of input features and observed outcomes. The problems considered by subgroup discovery are often complex, both in terms of representation of input and outcome of the observed system. Representation of the outcome with a single class variable is convenient and fosters the utility of standard data analysis tools. Yet, in many real-life domains, the outcomes are complex and need to be described with a number of features. For instance, in medicine, an outcome of clinical procedure may be represented with a set of measurements [29]. In systems biology, any change in the environment or in the genome may be observed at a systemic level through a set of observed variables that describe the phenotype [5]. Hypotheses in chemical genomics need to relate the data on chemical structures with whole-genome observations [15]. In social sciences, data analysis may bene fi t from methods that can address a set of observed factors that describe human behavior [18].

To address such problems, we can bene fi t from the original de fi nition of the subgroup discovery tasks, but take into consideration more complex data structures. In this article, we present an approach to subgroup discovery where the outcome is described with a set of response variables ,or output attributes . For this purpose, we have developed an algorithm called MR-SD for subgroup discovery from data with multi-dimensional fesponses ,or multiple-response subgroup discovery in short. MR-SD identi fi es a set of subgroups  X  collections of data instances from the training data set  X  where in each subgroup the data instances are similar in terms of the values of output attributes and are separable from the rest of the data in the input space.

Let us illustrate multiple-response subgroup mining through a hypothetical ex ample. Figure 1 shows the sample values of two input (left column) and two output attributes (right column). In Fig. 1a, the black circles mark a subset of data instances that are nicely clustered in both spaces. Clearly, these instances are similar in the output space, and comprise a well-de fi ned neighborhood in the input space thus satisfying our constraint for an interesting subgroup. Similarly, the selected data instances in Fig. 1b are again clustered in the output space, and are linearly separable in the input space. This set could also constitute an interesting subgroup, but should use a dif ferent algorithm to report on separability in the input space. While the nearest neighbors algorithm may well-separate subgroup X  X  instances from Fig. 1a, it would fail to do so for a subgroup from Fig. 1b where a linear classi fi er, like logistic regression, would succeed. In Fig. 1c, the selected instances are clustered well in the output space, but are not separable in the input space and therefore do not constitute an interesting subgroup.

The MR-SD algorithm described in the paper is based on the combination of unsupervised (clustering) and supervised (classi fi cation) techniques, and traverses a hierarchical clustering tree to obtain candidates for subgroups.

In the remainder of the paper, we fi rst describe the related work. Then, we formally introduce the algorithm and a set of accompanying techniques for subgroup scoring and selection. We test the behavior of the proposed algorithm on two synthetic data sets and then describe an case study application in the area of analysis of data from European Social Survey. We conclude the paper with discussion and concluding remarks. 2. Related work
The type of the data analysis, where the inference algorithm aims at fi nding interesting data subsets which share unusual properties rather then producing the comprehensive model of the entire data set, was fi rst proposed in 1996 by Kl  X  osgen [21]. Referred to as subgroup discovery , one of the least complex of the subgroup X  X  properties is related to a distribution o f a single binary response variable. Indeed, initial efforts in subgroup discovery research and the majority of existing techniques deal with binary class-labeled data and for search of the subgroups propose various adaptations of rule learning algorithms. Prominent algorithms, that also vary in the utility of different rule scoring approaches [23] include SD [7], CN2-SD [24] and APRIORI-SD [20].

Rule-based approaches for subgroup discovery have also been adapted for discovery of more complex target concepts, including those that include multi-valued classes [1] (still using a single nominal class variable, but instead of two-valued encoding more than two classes) and a numerical response variable [6, 17]. Further, subgroup discovery approaches were proposed for the analysis of data with several binary response variables [39], and the analysis of inferred statistical models, the so-called exceptional model mining [25]. Instead of relating a set of attributes to a class, subgroup discovery can also relate to other properties in the data, like fi nding the subgroup-speci fi c interactions between two variables [27,32].
Neither of the approaches mentioned above can be directly applied to data sets with multi-dimensional response variables. In principle, one could binarize all the variables and use the cluster grouping approach [39]. In this way, however, additional parameters are required for this procedure, with potential loss of the ordering information for continuous attributes, yielding models that are harder to interpret. Alternatively, any of the existing algorithms for single-class subgroup discovery can be used for each response variable independently. Such approach would be computationally more expensive and would create a set of models instead of a single one thus ha mpering interpretability . Methods performing such analysis would also disregard any possible dependencies among the response variables.

The inappropriateness of splitting the multi-dime nsional response probl em to several single-dimensional problems has already been exposed in early reports of algorithms that simultaneously predict a set of response variables. An excellent ex ample of an algorithm that constructs a multi-target prediction model is the hierarchical clustering trees approach [14]. Clustering trees are a generalization of the decision trees that are able to treat several responses simultaneously. Instead of a single-class based attribute scoring (e.g., entropy [30]), the split of the data at each node to a set of data subsets is scored according to within-subset instance similarity by comparing all corresponding pairs of data instances.

Clustering trees cannot be regarded as a subgroup discovery approach: the method aims to construct a global model of the data. Just like for the standard classi fi cation trees, the partition of the data strongly depends on the set of initial splits close to the root of the tree, and the algorithm may fail to uncover many of the interesting patterns due to the recursive nature of the tree construction algorithm. To remedy these shortcomings,  X  Zenko proposed to use rule-based learning as a core algorithm for predictive clustering [34]. His method of predictive clustering rules (PCR) is an extension of the CN2 rule learning algorithm [3] that can model the distribution of a set of response variables instead of a single class. Just like CN2-SD [24], PCR uses a weighted covering rule-discovery technique, allowing the rules to refer to the similar sets of instances from the training set and thus encouraging the inference of overlapping rules. For rule scoring, PCR combines the estimate of accuracy of the rule for the nominal output variables with the decrease of variance of the continuous output variables.

Recently, a different rule-based approach for subgroup discovery was proposed by Hapfelmeier [13], addressing the data containing medical images described with a set of attributes. Authors fi rst partitioned the data using k -medoids clustering on images, and then inferred rules to assign cluster memberships using a standard subgroup discovery technique, the RSD algoritm [38], an extension of CN2-SD [24] for subgroup discovery in relational data sets. Par titioning clustering was also used by our own early approach to multi-response subgroup discovery [33], where data was clustered both in input and output space, and analysis of contingency tables was used to fi nd potentially related clusters with a substantial number of overlapping instances. The major weaknesses of this approach are computational complexity (the method needed to traverse a set of combinations of parameters that determine the number of clusters in both spaces), implicit discovery of subgroups (intersection of clusters in input and output space), and reliance on external post-processing method for the construction of symbolic description for cluster membership.
 3. Methods An input to the proposed multi-res ponse subgroup discovery algorithm (MR-SD) is a training data set E consisting of a random sample of n data instances e i that are described with m x input attributes and m y output attributes. We will denote the two attribute sets with X (input attributes) and Y (output attributes). and y i an attribute-value m y -tuple.
 A subgroup G is a non-empty subset of the input data set E . The set of all possible subgroups G = P ( E ) \{ X  X  contains 2 n  X  1 subgroups, and is too large to be investigated exhaustively. MR-SD applies a heuristics where the data instances are fi rst hierarchically clustered in the space of output attributes, and then candidate subgroups are obtained by traversal of the inferred clustering tree (see Fig. 2). Each candidate s ubgroup is scored according to the sepa rability of its data instances from all other training data instances using a selected supervised data mining technique. Subgroups that exceed a speci fi ed score threshold are then ranked and reported to the data analyst. The proposed algorithm may infer subgroups that are similar in terms of covered instances, and we further propose a post-processing method to prune the subgroup set and identify best-scored subgroups with only a small mutual overlap. The details of the algorithm and its implementation are described below. 3.1. MR-SD Algorithm
MR-SD algorithm (Fig. 3) starts with a hierarchical clustering of data instances in the space of output attributes Y . Instances in the same node of the clustering tree are therefore similar based on their output attribute values. The algorithm then veri fi es which of these clusters contain data instances that are separablein the input space X . Subgroup candidatesare gathered by traversing the hierarchicalclustering tree, testing if the instances in each of the node form a subgroup that can be reliably identi fi ed in the input space using a given supervised data mining technique. For each clustering tree node, it estimates the accuracy of a pr obabilistic classi fi er f G when separating instances from the node (subgroup G )to their complement G C = E \ G .Wede fi ne the probabilistic classi fi er f G as a mapping f G : E  X  [0 , 1] only on the values of input attributes X . If the estimated accuracy of f G exceeds a prede fi ned threshold, we add the subgroup G to a set of interesting subgroups G .

The algorithm is general in terms the clustering algorithm, dissimilarity measure, the supervised data mining method and its related scoring technique; these are all parameters of the method. In our experiments, we used a standard agglomerative hierarchical clustering with Ward X  X  linkage [36] and Manhattan distance on [0 , 1] -scaled continuous attributes and discrete distance (0 for same-valued and 1 for different-valued) for nominal attributes.

We use cross-validation and estimate the area under receiveroperating characteristics curve (AUC [12]) to score separability of subgroup instances in the input attrib ute space. Given a pr obabilistic classi fi er f
G , AUC is equal to the probability that the classi fi er f G distinguishes between a member of G and a member of G C [12]. While any standard measure for classi fi cation accuracy can be used here, AUC X  X  advantage is that its scale does not depend on the prior distribution of the class variable, in our case the frequencies of G and G C . That is, with AUC, the threshold T Acc will have the same meaning for subgroups of different sizes.

Subgroup discovery aims at identifying descriptions of interesting subgroups. The natural candidates for the supervised data mining technique are machine learning algorithms where the structure of the model can be easily communicated to the participating domain expert. In the case study included in this article, where the interpretation of results was essential, we used the naive Bayesian classi fi er and its nomogram-based model visualization [26]. 3.2. Post-processing and subgroup selection MR-SD X  X  search heuristic identi fi es subgroups by traversing the hierarchical structure of clusters. Some of the discovered subgroups may therefore substantially overlap, that is, may share a large number of data instances. From the end-user X  X  perspective it is desired that the algorithm would infer only a small subset of most representative subgroups. For this reason, and to avoid reporting similar subgroups albeit their high-scores, we de fi ne a subgroup post-processing step. Its aim is to identify a subset of highest-scored subgroups with only a small overlap. In principle, if two subgroups substantially overlap, it is reasonable to report only the one with the higher score.

We measure the overlap of two subgroups G i and G j in terms of the proportion of jointly shared instances ( G i  X  G j ) among all instances covered by the two subgroups ( G i  X  G j ), and accordingly use the Jaccard coef fi cient of similarity [16]: We then create a network where each node represents a subgroup from G . Two nodes in this network Suf fi ciently large thresholds lead to fragmented networks composed of a number of connected network components. From each of the components, we select the highest-scored subgroup and include it in the fi nal set of the subgroups G .

An example of subgroup similarity network is shown in Fig. 4. The example demonstrates how the proposed post-processing step reduced a set of eight subgroups inferred by MR-SD to a subset of four, retaining the diverse subgroups with highest AUC scores. 4. Experiments on Synthetic Data Sets
We have studied the performance of the propos ed multi-response subgroup discovery algorithm on arti fi cially generated synthetic data sets. Synthetic data sets intentionally included a target subgroup and we tested if MR-SD can discover it under the presence of noise. We also used these data sets to compare MR-SD to predictive clustering rules [35,34] and outline the differences between these two approaches. 4.1. Data
We have generated two types of synthetic data sets, one with binary ( D B ) and one with continuous ( D C ) input attributes. The size of all data sets was set to n = 200. The data sets were generated so that to include a target subgroup of 40 (20%) data instances. The data sets included fi ve output attributes ( Y 1 ,...,Y 5 ), for which the values were sampled either from a normal distribution N (5 , 1) for subgroup X  X  instances or from N (0 , 1) for all other instances.

Data sets D B included binary input attributes X 1 ,X 2 ,...,X m had fi rst two attributes equal to 1, e.g. X 1 =1  X  X 2 =1  X  G .Valuesof X 1 and X 2 for all other instances were chosen arbitrarily from discrete uniform distribution, but did not include the combination of X 1 =1  X  X 2 =1 . Values of all other attributes X 3 ,...,X m distribution.

For D C , the input attribute values were sampled from N (0 , 1) . First, the subgroup G has been generated with sampling from the selected distribution but constraining the values of X 1 and X 2 to satisfy the relation X 2 1  X  X 2 2  X  1 0. The same distribution was used in generating the complement, where X 1 and X 2 were constrained to X 2 1  X  X 2 2  X  1 &lt; 0.

To distract otherwise clear separation of target subgroup in the output space we have added noise by choosing a proportion P noise of data instances for which the values of their output attributes were randomly permuted across selected instances. Different noise levels from P noise = 0 (no noise) to up to P noise = 0.3 with step 0 . 01 were examined. The introduction of noise degraded both clustering properties of the data set in the output attribute space, and consequently the relations between input and output attributes (see Fig. 5).
 4.2. Evaluation Procedure The synthetic data sets described above were used to test the MR-SD algorithm, and compare it to the PCR, the predictive clustering rules [34]. MR-SD was run with default parameters, using naive Bayesian classi fi er in case of discrete input attributes ( D B ) and support vector machines with a polynomial kernel in case of continuously-valued input attributes ( D C ). The threshold T Acc was set to 0 . 75 .PCRwasrun with the default parameters (multiplicative weighting scheme and search heuristics, disregarding other rules in the rule set). We tested the ability of the t wo procedures to uncover the target subgroup. The set of the subgroups G obtained by the algorithms was compared to the target subgroup, and the algorithm A was then scored according to the discovered subgroup G i  X  X  = G ( A ) which matched the target subgroup best in terms of the inclusion of the relevant data instances as measured by Jaccard coef fi cient of similarity: The algorithms may fail to fi nd any interesting subgroup, that is, none of examined subgroups is scored above T Acc . The corresponding score ( A ) is in such cases set to 0.

For each data set type and level of noise, the experiments were run 50 times. We report the averaged results and standard deviations. 4.3. Results and discussion
The results of the experimental study on synthetic data sets are summarized in Fig. 6. They show that the MR-SD behaves similarly to PCR if the number of input variables is small, the input attributes are discrete and if the function that maps the instances to a subgroup can be represented with a rule (Fig. 6a). Adding irrelevant input attributes, at least to a degree studied here, does not hamper the performance of MR-SD, but may substantially degrade the performance of PCR (Fig. 6b).

Results in Figs 6c and d con fi rm the applicability of MR-SD in cas e of continuous input attributes and more complex subgroup membership functions. The advantage of MR-SD is that it can use any suitable supervised data mining algorithm to characterize the subgroups in the input attribute space, and can hence accommodate for a wider variety of data types and subgroup description functions.
The scoring of subgroup discovery algorithms was based on the discovered subgroup that best matched the target subgroup G . For each data set, both algorithms proposed several subgroups. For data sets D
B, 2 and D B, 10 , MR-SD found more than ten subgroups, but which were similar to each other and have been represented with a single subgroup after the post-processing step. We have also observed that, regardless of the noise level, the subgroup most similar to the target subgroup was also the highest rated by the AUC-based scoring. While the evaluation scores in Fig. 6 steadily decrease with increasing level of noise, the correct identi fi cation of the target subgroup despite noisy data speaks of the robustness of the proposed method. In contrast, PCR proposed two or three subgroups, of which the one matching the target was in most cases not the best-scored one.

For data sets D C, 2 and D C, 10 , MR-SD discovers from 10 to 20 subgroups. The number of discovered subgroups increases with the increasing noise level. Regardless of noise, post-processing returned only one subgroup that matches the target subgroup for D C, 2 , and returned two subgroups for D C, 10 (a target subgroup and a substantially different subgroup). The number of subgroups identi fi ed by PCR also increased with the level of noise, but failed to identify any subgroup that would substantially cover the target one. As the data sets D C, 2 and D C, 10 include the concept which cannot be represented in a rule-based language, adding noise actually helps PCR to propose subgroups that include few of the instances from the target subgroup.

Synthetic data sets studied in this section were crafted to test the capability of MR-SD to reveal the target subgroup, which was purposely designed to be  X  X iscoverable X  by the selected supervised data mining technique. MR-SD had with this an advantage to PCR. Experiments show that MR-SD could be as successful as PCR at uncovering the subgroups that can be modeled with rules, but may outperform PCR when subgroup membership concepts cannot be modeled with classi fi cation rules.

The performance of MR-SD may depend on the choice of the supervised data mining algorithm, as a particular machine learning technique may either fail or succeed in modeling of data in the input space. To illustrate this point, consider th e performance of MR-SD when varying this particular component of the overall algorithm (Fig. 7). As proposed in this paper, the particular classi fi cation technique is a parameter of the method, and should be speci fi ed according to the user X  X  knowledge of the domain. 5. Case study: Analysis of the data from European Social Survey
The European Social Survey (ESS) [18] is a biennial academically-driven social survey designed to describe attitudes, behavior and beliefs of European citizens. The aim of the case study was to apply MR-SD to a real-life data set and to observe if it can uncover interesting subgroups that would relate socio-demographic variables (i nput attributes) to various sets of output attributes, like attitudes and behaviors. Naive Bayesian classi fi er was used to characterize the subgroups in the input attribute space. Subgroups were requested to cover at least 5% but no more than 40% of instances. 5-fold cross validation was used to estimate AUC of the naive Bayesian classi fi er. The threshold T Acc was set to 0.75, a lower bound for the acceptable AUC score [28]. In the post-processing stage, we set T J = 1 2 . 5.1. Data
We have considered the data of the Slovenian survey from the year 2006. The survey included 1.476 persons and recorded over 300 different variables (see Table 1), including socio-demographic characteristics, the use of media, attributes recording social and public trust, and other.
From the data, we have excluded attributes that report on all the interviewer self-completion questions and test questions ( N = 22). We have also excluded near-constant attributes ( N = 25) having the same value for more than 90% data instances, and attributes having more than 10% of missing values ( N = 118). We have then split the remaining ( N = 165) attributes to the input and output set. The input set comprised 13 socio-demographic attributes including gender, age, marital status, and education level. The remaining attributes ( N = 152) were divided into six non-overlapping blocks that correspond to six different sections from the questionary (Table 1). Our analysis was therefore comprised of six different tasks, each sharing the same set of input attributes, but using a different set of output attributes. 5.2. Results
Table 1 summarizes the results of subgroup discovery on the survey data set. MR-SD found interesting subgroups for fi ve out of six modeling tasks. In all tasks where MR-SD initially proposed more than one subgroup, post-processing could substantially reduce the number of subgroups with minimal loss in the coverage. From the set of subgroups, we have selected two that we present in a detail below. 5.2.1. Example subgroup: Media and social trust
A subgroup of 457 (31%) data instances with AUC = 0.85. The naive Bayesian model for the subgroup membership is shown using the nomogram [26] in Fig. 8. The nomogram depicts the in fl uence of the fi ve most informative input variables (age, level of education, ... ). We can conclude that this is a subgroup of a younger, well-educated individuals. To summarize their characteristics with respect to the media and social trust, Table 2 ranks the output attributes according to the degree of difference between the distribution of their values in the subgroup and entire data set. For testing the differences in the distributions we used the t -test and reported the corresponding p  X  X alue which was further adjusted for the multiple comparisons using Bonferroni correction. According to this analysis, the individuals in the subgroup of younger, well-educated individuals very often use Internet and rarely listen to the radio. 5.2.2. Example subgroup: A ttitudes and timing of life
A subgroup discovered for this task consists of 485 (33%) data instances and is well-characterized in the input attribute space (AUC = 0.98). The characterization model in Fig. 9 includes fi ve most important input variables (legal marital status, age, ... ). The most important properties of this subgroup in the output space are summarized in the Table 3. Analogously, the p -values have been computed using t -test for continuous and  X  2 test for categorical attributes and have been further adjusted with Bonferroni X  X  correction.

According to the nomogram (Fig. 9) this subgroup consists of younger, single individuals. The most subgroup-characteristic output variables (Table 3) turn out to measure similar properties ( X  X ot married X ,  X  X ot being parents X ). Very high AUC score is therefore due to similarity of variables in input and output space. Although this subgroup does not represent any useful concept, the experiment nevertheless con fi rms the solid formal background of the proposed technique. 6. Conclusion
In data rich domains, tools of data analytics often look for interesting data subgroups,rather than require a construction of a comprehensive model that would encompass entire data set. Methods of subgroup discovery have been tailored for this task. While most popular techniques in this fi eld address data with a single response variable, many of today X  X  relevant problems from experimental research, industry, and socioeconomics may be described with the data whose description of the outcome is richer and, for example, includes a set of attributes instead of a single class. In the paper, we introduce a new technique that addresses such problems and is able to in fer subgroups from data sets with multi-dimensional responses.

The task of multi-response subgroup discovery is to fi nd subgroups of data sets similar in the description of the output and whose members can be characteri zed in the input attribute sp ace. Our mu lti-dimensional subgroup discovery algorithm (MR-SD) directly addresses this task: candidate subgroups with data instances similar in outcomes are proposed by agglomerative clustering in the output space, and tested if they can be characterized in input space by means of the supervised data mining.

The major strength of the MR-SD are its reliance on standard,ef fi cient and fast algorithms for clustering and machine learning, and o n utility of a standard techni que for subgroup scoring. Additional advantage is that any machine learning technique can be used for subgroup characterization. The particular choice would most likely depend on the type of the problem and input attributes, analyst X  X  preference and familiarity with the technique and its ability to construct an interpretable model.

MR-SD has several potential weaknesses. Traversal of hierarchical clustering tree may yield a number of in composition very similar subgr oup candidates, each of which is i n the current implementation individually scored for its characterization in the input attribute space. The algorithm could be improved through sampling of the candidates, and could instead initially examine only those that are most different in composition. Identi fi cation of compositionally different subgroups is in the proposed version of the method performed after the scoring, which is computationally more demanding, but  X  at least for the subgroup candidates from hierarchical clustering tree do not present the entire set of possible subgroups. Yet, hierarchical clustering provides means for an ef fi cient heuristic search, aiming at minimizing the number of candidates to be scored in the input attribute space.

Being able to incorporate any classi fi cation algorithm for scoring of subgroups in the input space is an advantage of MR-SD, especially when compared to subgroup discovery methods that were developed around a particular modeling technique. This, however, adds another parameter to the method. In this paper, we did not explore means to automatic identi fi cation of suitable selection of supervised data mining technique, and see this as a potential future improvement of the approach.

Both experiments on synthetic data sets and on a real data set demonstrate that the method can discover relevant and interesting data subsets. Experiments on synthetic data sets also demonstrate the advantage of choosing an appropriate machine learning method for subgroup characterization. With increasing practical demand for multi-response subgroup mining techniques, further work should focus on practical applications and application-based optimizations of the approach.
 References
