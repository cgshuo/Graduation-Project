 Most important tools for mining in biological data such as sequence alignment and phylogenetics generally rely on a substitution matrix which ideally reflects the probability of mutating a character in a sequence to another in other se-quences. Most sequence alignment algorithms attempt to find the optimal match of sequences where matching scores are de rived from a substitution matrix [1,2]. It is well known that using a reliable substitution matrix significantly improves the sensitivity of sequence alignment and database search tools [3,4]. Substitu-tion matrices also provide clues to dating of various evolutionary events and many molecular evolution mechanisms, and thus are often used in phylogenetic analysis [5,6].

Classically, a substitution matrix is empirically selected based on some as-sumptions about the sequences being analysed. For protein analysis, the PAM substitution matrices [7] are calculated by observing the differences in related sequences with a certain ratio of substitution residues. The PAM-n matrix esti-mates what rate of substitution would be expected if n % of the amino acids had changed. On the other hand, the BLOSUMs [3] are derived from segments in a block with a sequence identity above a certain threshold.

While much research has been done on substitution matrices for protein, little attention has been paid to DNA substitution matrices despite the need of reli-able tools for aligning genome size sequ ences of the next generation sequencing technology. Since not all DNA substitutions change the encoded amino acids, looking at the amino acid level only would lose some information. As more than one codons can code for the same amino acid, and different strains can show different preferences for codons that e ncode a given amino acid [8], a generic PAM or BLOSUM like substitution matrix for nucleotides such as RIBOSUM [9] can hardly work well on specific DNA sequences.

Work on DNA substitution matrices [10,11,12] often bases on a substitution model. Such examples of substitution models are the CJ69 model [13] which as-sumes all changes among four nucleotides occurring with equal probability and the K80 model [14] which allows transitions and transversions to occur with dif-ferent rates. These models are rarely precise in practice. Traditional substitution matrix derivation methods also depend on sequence alignment, which in turn is plausible only when a reliable substitution matrix is used.

In this paper, we introduce a novel method to generate DNA substitution ma-trices for genomic comparative study. The method is based on information theory foundation [15] and is in expectation maximisation framework. Our method finds the substitution matrix directly from the data being analysed without having to make any assumptions. It considers the substitution matrix as parameters to align sequences and applies an expectation maximisation approach to estimate the parameters that optimise the alignment score. To the best of our knowledge, this method is the first to be able to compute specific substitution matrices for genome size sequences without any assumptions or a prior alignment of the data. The presented technique could be generalised to other types of data as well. Information theory directly relates entropy to the transmission of a sequence under a statistical model of compression. Suppose a sequence X is to be effi-ciently transmitted over a reliable channel. The sender first compresses X using a compression model and transmits the encoded message to the receiver, who decodes the compressed stream using the s ame model to recover the original mes-sage. The information content I X of X is the amount of information actually transmitted, i.e. the length of the encoded message.

Suppose a sequence Y related to X is available to both parties, the sender needs to transmit only the information of X that is not contained in Y .Sincethe receiver also knows Y , X can be recovered correctly. The amount of information actually transmitted in this case is called conditional information content of X given Y , denoted as I X | Y . The more related the two sequences are, the more information the two sequences share, th e shorter message is transmitted. The mutual information of X and Y is defined as the difference between the infor-mation content and the conditional information content: I X ; Y = I X  X  X  X | Y .
Compression of sequences requires a compression model. To measure the con-ditional information content of one sequence given another, the compression model needs to use a substitution matrix as parameters. In light of the Mini-mum Message Length principle [16,17], we propose an expectation-maximisation (EM) algorithm to find the substitution matrix that can produce the most com-pact encoding of the two sequences. In t he E-step, one sequence is compressed on the background knowledge of the other to measure the conditional informa-tion content. The compression uses a substitution matrix, initialised to some default values, as the parameters to be estimated. We use the expert model [18] as the compression model because of its efficient performance and its ability to produce an local alignment of the two sequences [19]. In the M-step, the substitu-tion matrix is then re-estimated based on the mutations observed from the local alignment. The EM process continues until the conditional information content obtained converges to an optimal value. 2.1 The Expert Model The expert model algorithm [18] compresses a sequence X , symbol by symbol by forming the probability distribution of each symbol and then using a primary compression scheme to encode it. The probability distribution at a position is based on all symbols seen previously. Correspondingly, the decoder, having seen all previous decoded symbols, is able to compute the identical probability dis-tribution and thus can recover the symbol. The information content of symbol x i is computed as the negative log of the probability of the symbol [15]:
The algorithm maintains a set of experts to estimate the probability of a symbol. An expert is any entity that can provide a probability distribution of the symbol. An example is the Markov expert of order k which uses a Markov model learnt from the statistics of X to give the probability of a symbol given k preceding symbols. If a related sequence Y is available, the expert model employs align experts each of which considers the next symbol x i in X to be part of a homologous region and align with a symbol y j in Y . The align experts assume a substitution matrix P the entry P ( x, y ) of which is the probability of substituting symbol y in Y by symbol x in X . The probability of symbol x i predicted by an align expert is Pr ( x i | y j )= P ( x i ,y j ). The expert model uses a hash table to propose align expert candidates. A hash table of hash size h suggests every matching h-mer as an align expert which is then evaluated and is discarded if it does not perform significantly better than the Markov expert. The core part of the expert model is the combination of expert predictions. Suppose a panel of experts E is available to the encoder. Expert  X  k gives the prediction Pr ( x m +1 |  X  k ,x 1 ..m )ofsymbol x m +1 based on its observations of pre-ceding m symbols. Expert predictions are comb inedbasedonBayesianaveraging: The weight w  X  k ,m of expert  X  k for encoding x m +1 is assigned to Pr (  X  k | x 1 ..m ) and can be estimated by Bayes X  X  theorem: where Pr (  X  k ) is the prior probability of expert  X  k before encoding any symbol. As Eq. 2 can be normalised to have Pr ( x m +1 | x 1 ..m ) = 1, we can ignore the common denominator in Eq. 3 and take the negative log of the numerators: the right hand side of Eq. 4 represents the length of encoding subsequence x 1 ..m by expert  X  k . As experts are evaluated on a recent history of w symbols, the mes-sage length of encoding x m  X  w +1 ..m is used to determine the weights of experts. Rewriting Eq. 4 for the weight of expert  X  k at position m +1 gives: Using only the Markov expert can produce the information content of sequence X . The conditional content of X given Y is obtained by combining the Markov expert with align experts. Align exper ts are first combined according to Eq. 5 to become the blended align expert , whose prediction is then combined with the Markov expert X  X  prediction. The expert s X  X eightsspecifiedinEq.5involvesthe prior probability Pr (  X  k ) of each expert. As all align experts are proposed by the same hash table, they have the same prior probability and hence the common factor 2  X  log 2 Pr (  X  k ) can be ignored. However, for co mbination of the blended align expert and the Markov expert, their prior probabilities have to be specified. The prior probability of the blended align expert can be estimated from previous iterations of the EM process.

An align expert might be proposed simply by chance. The algorithm consid-ers an align expert plausible if it performs significantly better than the Markov expert. It must encode the last w symbols better than the Markov expert by a threshold T bits, which is a parameter of the algorithm. When the align ex-pert predicts beyond its homologous region, its performance worsens and it is discarded subsequently. Each align expert suggests an alignment of the region starting at the position it is proposed and ending at the position it is discarded. This region is called the maximum-scoring segment pair (MSP). The set of MSPs forms an local alignment of the two sequences.
 2.2 Alignment Score and Mutual Information Content Consider an align expert that uses a substitution matrix P and aligns x i in X to y j in Y . The alignment score is specified by the logarithm of the odds ratio of model H which assumes homology, and model R assuming random[20]: By Bayes X  X  theorem, the numerator of the right hand side can be expressed as: Therefore,
The alignment score of a MSP is the sum of alignment scores of all symbols in the region. If the MSP is from two regions starting at x m and y n respectively and is k symbols long, its alignment score is The two terms are the lengths of the compressed messages of the region x m,k by the Markov expert and by the align exp ert, respectively. In other words, the alignment score of a MSP is the mutual information content of the two regions. 2.3 Computing the Substitution Matrix Once the local alignment of the two sequences is constructed, the substitution matrix is computed from the substitutions observed from the alignment. Entry P ( x, y ) of the substitution matrix gets the value where C x | y is the number of symbol x in X that are aligned to symbol y in Y , and C y is the number of symbol y in all MSPs.

A statistical hypothesis testing is performed to select the  X  X ood X  MSPs to compute the substitution matrix. From Karlin-Altschul statistics [21], the E-value of occurrences of MSPs with a score S or greater is E = KMN 2  X  S where M and N are the lengths of the two sequences and K is the Karlin-Altschul parameter. The occurrences of MSPs can b e modelled by a Poisson process with characteristic parameter E . At the significance level  X  =0 . 05, the substitution matrix is estimated from mutations in MSPs having E-value  X  or a score: We implemented the algorithm in Java and ran experiments on a PC with Intel Core 2 Duo 2.33Ghz CPU and 8GB of RAM, using Sun Java runtime environ-ment 1.5. In our experiments, we used a hash table with hash key of 20 to propose align experts. The threshold T was set to 0.5 bits. The initial substitution matrix is set to have entries of 0 . 7 on the diagonal and 0 . 1 off the diagonal.
It is hard to verify substitution matrices derived from real data. We therefore performed experiments on a set of synthesised data so that the substitution matrix computed can be compared with the matrix used to generate data. The experiment is described in Subsection 3.1. We then ran experiments on a set of real data, as described in Subsection 3.2. 3.1 Experiment on Synthesised Data Synthesised data was used to ensure the correct derivation of substitution ma-trices. The benefit of using artificial data is that the data can be generated with added noise from a known substitution matrix, and hence the computed matrix can be verified. We generated two  X  X odel genomes X  each of which is one million bases long. About 10% of the first genome is  X  X oding regions X  which are copied to the second genome with substitution rates specified by a matrix P target .The  X  X on-coding regions X  of the two genomes are independent on each other.
The substitution matrix is reconstructed from the data by aligning the second genome against the first one. After the fifth iteration the changes to the matrix between two consecutive iterations were negligible. In other words, the matrix converges after 5 iterations and in less than 10 minutes. Table 1 presents the tar-get matrix P target and the computed matrix P computed whose rows and columns are in ACT G order. Given the noise introduced during the generation of the two sequences, the similarity of the comput ed matrix and the target matrix shows the effectiveness of our algorithm. 3.2 Experiment on Plasmodium Genomes We analysed the genomes of four Plasmodium species, namely P. falciparum, P. knowlesi, P. vivax and P. yoelii which cause malaria in various hosts. The genomes are obtained from PlasmoDB release 5.4 ( http://www.plasmodb. org/common/downloads/release-5.4/ ). The nucleotide compositions in these species X  genomes are very different. The AT content in the genome of P. falci-parum is as high as 80% and in coding regions is 76.22% while the AT content in the P. vivax genome and P. vivax coding regions is 57.71% and 53.70% re-spectively. The characteristics of th ese genomes are presented in table 2.
We applied our method to find the substitution matrix for each pair of these genomes. To compute the substitution matrix P Y  X  X of genome Y to genome X , we compressed the genome X on the background knowledge of genome Y . Generally, about 4 or 5 iterations were required for convergence. The substitution matrices of these genomes are presented in Table 3. We have presented a method for dynamically deriving a substitution matrix for analysis of any two long DNA sequences. The method is based on the sound the-oretical foundation from information theory. We have shown that the method successfully regains the substitution matrix from synthesised data derived from a known matrix with introduced noise. The method has also been applied on real data with differing phylogenetic distances and nucleotide composition which would mislead classical statistical methods. Unlike traditional methods, our al-gorithm does not rely on the pre-alignment of sequences or on a substitution model. It incorporates the alignment of sequences and the substitution matrix computed in a expectation maximisatio n process. Furthermore, it can handle very long sequences in practical running time. The method therefore, would facilitate knowledge discovery in large and statistical biased databases.
