 Fernando Diaz Abstract We adapt the cluster hypothesis for score-based information retrieval by claiming that closely related documents should have similar scores. Given a retrieval from an arbitrary system, we describe an algorithm which directly optimizes this objective by retrieval scores, regardless of their origin, we can apply the technique to arbitrary initial retrieval rankings. Document rankings derived from regularized scores, when compared to rankings derived from un-regularized scores, consistently and significantly result in improved performance given a variety of baseline retrieval algorithms. We also present several proofs demonstrating that regularization generalizes methods such as pseudo-relevance feedback, document expansion, and cluster-based retrieval. Because of these strong empirical and theoretical results, we argue for the adoption of score regularization as general design principle or post-processing step for information retrieval systems. Keywords Regularization Cluster hypothesis Cluster-based retrieval Pseudo-relevance feedback Query expansion Document expansion 1 Introduction In information retrieval, a user presents a query to a computer; the computer then returns documents in a corpus relevant to the user X  X  query. A user familiar with the topic may be able to supply example relevant and non-relevant documents. More often, a user is unfamiliar with the topic and possesses no example documents. In this situation, the user query-based information retrieval .

A set retrieval model assigns a binary prediction of relevance to each document in the collection. The user then scans those documents predicted to be relevant. We can see this as a mapping or function from documents in the collection to a binary value. Mathemat-ically, given a query, q , a set retrieval model provides a function, f q : D!f 0 ; 1 g ; from documents to labels; we refer to f q as the initial score function for a particular query. The argument of this function is the retrieval system X  X  representation of a document. The values of the function provide the system X  X  labeling of the documents. Notice that we index functions by the query. We note this to emphasize the fact that, in information retrieval, the score function over all documents will be different for each query. Although we drop the index for notational convenience, the reader should keep in mind that this is a function for a particular query.

A ranked retrieval model assigns some rank or score to each document in the collection and ranks documents according to the score. The user then scans the documents according to the ranking. The score function for a ranked retrieval model maps documents to real values. Given a query, q , the model provides a function, f q : D!&lt; ; from documents to exist many ranked retrieval models based on geometry (e.g., the vector space model (Salton et al. 1975 )) and probability (e.g., the probabilistic model (Robertson et al. 1981 ), inference networks (Turtle and Croft 1990 ), and language modeling (Croft and Lafferty 2003 )). This paper examines the behavior of score functions for ranked retrieval models with respect to the geometry of the underlying domain, D :
One way to describe a function, regardless of its domain, is by its smoothness . The smoothness of a function might be measured, for example, by its continuity, as in Lipschitz continuity. In many situations, we prefer functions which exhibit smoothness. For example, consider the one-dimensional functions in Fig. 1 . If we assume that local consistency or erable because it is smoother.

If only presented with the function in Fig. 1a , then we can procedurally modify the function in Fig. 1b . Post-processing a function is one way to perform regularization (Chen and Haykin 2002 ). In our work, we regularize initial score functions. Because our analysis and regularization is local to the highest scored documents, we refer to this process as local score regularization .
When our domain was the real line, we wanted the value of the function at two points, f ( x small. We adopt a topic-based distance and consider two documents close if they share the between documents can be measured using, for example, inter-document cosine similarity. We would like two documents which share the same topic to receive similar scores. We depict this graphically in Fig. 2a for documents in a two-dimensional embedding space. When presented with a query, the retrieval system computes scores for each document in this space (Fig. 2b ); this is our initial score function. We regularize a function into order to improve the consistency of scores between neighboring documents. This is depicted graphically in Fig. 2c where the value of the function is smoother in the document space. Of course, realistic collections often cannot be visualized like this two-dimensional example. Nevertheless, the fundamental regularization process remains roughly the same.
There is an interesting connection here to the cluster hypothesis. The cluster hypothesis states: closely related documents tend to be relevant to the same request (Jardine and van present theoretical and empirical arguments for why score regularity should be adopted as a design principal for information retrieval systems. Because we formally define this objective and optimize it directly, we view score regularization as being in the spirit of axiomatic retrieval (Fang and Zhai 2005 ).
 Why might systems produce scores which fail to conform to the cluster hypothesis? Query-based information retrieval systems often score documents independently. The score of document a may be computed by examining query term matches, document (Fang et al. 2004 ). Once computed, a system rarely compares the score of a to the score of a topically-related document b . With some exceptions, the correlation of document scores is largely been ignored, leaving room for improvement through regularization.

Broadly, this paper contributes the following results, 1. An algorithm, local score regularization, designed to adjust retrieval scores to respect 2. A reduction of several well-known retrieval methods to score regularization (Sect. 5 ) 3. Experiments demonstrating strong and consistent performance improvements when This paper can be broken into three parts: a description of score regularization, a discussion describe the score regularization algorithm. Because we use a variety of mathematical conventions, we review these conventions and formally state our problem in Sect. 2 . Document affinity and relatedness are critical to regularization. We present a graph-based context of previous work. Because regularization has an interesting relationship to several classic information retrieval techniques, we devote Sect. 5 to reductions of several well-known techniques to score regularization. In the third part of our paper, we present experimental arguments for score regularization. We describe our experimental setup in Sect. 6 . The results of these experiments are presented in Sect. 7 and discussed in Sect. 8 . We conclude in Sect. 9 . 2 Preliminaries 2.1 Notation We adopt vector and matrix notational convention from previous work (Petersen and Pedersen 2005 ). These conventions are reviewed in Table 1 . 2.2 Definitions A collection is a set of n documents which exist in an m -dimensional vector space where m is the size of the vocabulary and elements of the vectors represent the frequency of the term in the document. We define for each document 1 i n a column vector, d i , where each element of the vector represents the frequency of the term in document i ; we refer to this as the document vector . These document vectors may be normalized by their L 1 or L 2 norm. We will attempt to make norms, if any, clear in context. Transposing and stacking up the n document vectors defines the n  X  m collection matrix C .

We define other symbols in Table 1 . Elaborations of definitions will occur when notation is introduced. 2.3 Problem statement We now formally define the regularization task. The input is a vector of document scores. Although the system usually scores all n documents in the collection, we consider only the top ~ n scores. The ~ n 1 vector, y , represents these scores. This vector may be normalized if desired. For example, we normalize this vector to have zero-mean and unit variance. The output is the vector of regularized scores represented by the ~ n 1 vector f . The objective is to define a regularization process which results in a superior ranking of the documents represented in y , given some evaluation measure. In our work, we use mean average precision (MAP) as the evaluation metric. MAP provides a standard and stable evaluation metric (Buckley and Voorhees 2000 ). 3 Computing inter-document affinity In Fig. 2 , we depicted documents existing in some space where proximity related to topical affinity. Our representations will never be as simple as those in our toy example. We now turn to describing one method for describing the relationship between documents. Our represent documents and edges represent the similarity between document vectors. We will build this graph in two steps: (1) compute the similarity between all pairs of documents neighbors of each document. In Sects. 3.1 and 3.2 , we describe two measures of similarity between document vectors. The similarity between all pairs of ~ n documents can be rep-resented by ~ n ~ n matrix A . In Sect. 3.3 , we construct a nearest neighbor graph using the similarity information. 3.1 Cosine similarity document can be placed on an m -dimensional hypersphere (Salton 1968 ). The inner product between document vectors determines affinity, which is equivalent to the standard cosine similarity measure. The ~ n ~ n affinity matrix is defined by, where each element of the matrix defines the symmetric affinity between two documents. 3.2 Language model similarity The language modeling perspective of information retrieval treats the text occurring in a document as having been generated by an unknown probabilistic model (Croft and Lafferty 2003 ). If we constrain this model to have a certain form, then we can then apply statistical methods for estimating the parameters of the model given the text occurring in a document. Although many different models have been proposed, practitioners often assume that each document is generated by a unique multinomial over terms. The parameters of these n multinomials can be estimated in a number of ways but in this section we will focus on the terms, then the maximum likelihood estimate is defined as Therefore, in this section, we consider d to be an L 1 -normalized vector of term frequencies which is equivalent to the maximum likelihood estimate.

For language models, we can adopt a measure of similarity between multinomials. One popular distributional affinity measure in the information retrieval community is the Kullback X  X eibler divergence. However, this measure is asymmetric and has demonstrated mixed results when made symmetric. Therefore, we use the multinomial diffusion kernel (Lafferty and Lebanon 2005 ). We adopt this measure because it is symmetric (allowing closed form solutions in Sect. 4.3 ) and has been used successfully for text clustering and classification tasks. This affinity measure between two distributions, d i and d j , is motivated by Fisher information metric and defined as, where t is a parameter controlling the decay of the affinity. The selection of a value for t will be clarified in Sect. 6.2.5 . In fact, when two multinomials are very similar, the value of the diffusion kernel approximates that of the Kullback X  X eibler divergence.
 The ~ n ~ n affinity matrix is defined by, Notice that, besides the normalization of the vectors, this is equivalent to applying a soft threshold to Eq. 2 . 3.3 Graph construction For the top ~ n documents, we compute the complete ~ n ~ n affinity matrix, A ; however, there are several reasons to consider a sparse affinity matrix instead. For example, we may be more confident about the affinity between very related documents than distant documents. In this situation, the space is often better approximated by the geodesic distances between documents. Consider the clusters of points in Fig. 3 .

We would like the distances to respect the clustering; documents in the same cluster should have smaller distances to each other than documents in different clusters. The ambient distance (Fig. 3 a) clearly does not satisfy this property. Using the geodesic dis-geodesic distances we desire in here. For example, an ~ n ~ n matrix, W , may only include the affinities to the k -nearest neighbors for each document from the affinity matrix, A , and zero otherwise. Constructing a document affinity graph captures the lower-dimensional document manifold and has demonstrated usefulness for text classification tasks (Belkin and Niyogi 2004 ). We explore the appropriateness of this assumption in our experiments.
A retrieval system theoretically provides scores for all n documents in the collection. To perform global analysis, our method would need to construct a graph including all n documents. Computational constraints prevent building the complete affinity matrix. We query-biased graph-construction procedure is depicted in Fig. 4 . We justify this method-ology by noting that the score function will be flat for the majority of the collection since the majority of the collection is non-relevant. Query-biased graphs focus regularization on the portion of the document graph most likely to contain relevant documents.

By using a graph, we assume the presence of a lower-dimensional manifold underlying the documents in the space; however, we should, at this point, stress a few problems with retrieval lie on a lower-dimensional manifold. We cannot visualize the documents in their ambient space and observe some lower-dimensional structure. Implicitly, though, the success of cluster-based retrieval methods suggests that there probably exists some topical substructure (Liu and Croft 2004 ; Xu and Croft 1999 ). From a theoretical perspective, methods such as manifold regularization normally assume a uniform sampling on the manifold (Belkin and Niyogi 2005 ). We need this assumption in order to, for example, demonstrate the convergence of the graph Laplacian in Sect. 4.1 to the continuous Laplacian. However, we cannot assume that topics are equally represented. Some topics will, in general, be represented by fewer documents than other topics. If we use a (biased) sample from the initial retrieval, this non-uniformity will be exacerbated. Therefore, whenever possible, we have attempted to use methods robust to violations of the sampling assumption (see Sect. 4.1 ). 4 Local score regularization In this section, we will present a regularization method which applies previous results from information retrieval. More thorough derivations can be found in cited publications.
Given the initial scores as a vector, y , we would like to compute a set of regularized scores, f , for these same documents. To accomplish this, we propose two contending objectives: score consistency between related documents and score consistency with the initial retrieval. These two objectives are depicted graphically for a one-dimensional sistency of the scores, f ; if related documents have very inconsistent scores, then the value of this function will be high. Let E X  f ; y  X  be a cost function measuring the consistency with the original scores; if documents have scores very inconsistent with their original scores, then the value of this function will be high. For mathematical simplicity, we use a linear combination of these objectives for our composite objective function, where l is a regularization parameter allowing us to control how much weight to place on inter-document smoothing versus consistency with the original score. 2 4.1 Measuring inter-document consistency Inter-document relatedness is represented by the graph, W , defined in Sect. 3.3 where W ij represents the affinity between documents i and j . We define our graph so that there are no self-loops ( W ii = 0). A set of scores is considered smooth if related documents have similar scores. In order to quantify smoothness, we define the cost function, S X  f  X  , which penalizes inconsistency between related documents, We measure inconsistency using the weighted difference between scores of neighboring documents. 3
In spectral graph theory, Eq. 7 is known as the Dirichlet sum (Chung 1997 ). We can rewrite the Dirichlet sum in matrix notation, where D is the diagonal matrix defined as D ii  X  as the combinatorial Laplacian which we represent by D C . The graph Laplacian can be viewed as the discrete analog of the Laplace X  X eltrami operator. Because the Laplacian can be used to compute the smoothness of a function, we may abstract D C and replace it with alternative formulations of the Laplacian which offer alternative measures of smoothness. For example, the normalized Laplacian is defined as, measures the degree-normalized smoothness as, The approximate Laplace X  X eltrami operator is a variation of the normalized Laplacian which uses a modified affinity matrix (Lafon 2004 ). The approximate Laplace X  X eltrami operator is defined as, where we use the adjusted affinity matrix ^ W  X  D 1 WD 1 with ^ D ii  X  approximate Laplace X  X eltrami operator theoretically addresses violations of the uniform sampling assumption. Because we were concerned with the violation of this assumption at the end of Sect. 3.3 , we adopt the approximate Laplace X  X eltrami operator (Eq. 11 ) in our work. We examine the effect of this choice on the regularization performance in Sect. 7.1 .
The value of the objective, S X  f  X  is small for smooth functions and large for non-smooth function. Unconstrained, however, the function minimizing this objective is the constant function In the next section, we will define a second objective which penalizes regularized scores inordinately inconsistent with the initial retrieval. 4.2 Measuring consistency with initial scores We define a second objective, E X  f ; y  X  , which penalizes inconsistencies between the initial retrieval scores, y , and the regularized scores, f , The regularized scores, f , minimizing this function would be completely consistent with 4.3 Minimizing the objective function In the previous two sections, we defined two constraints, S X  f  X  and E X  f ; y  X  , which can be regularized scores, f , such that, compute the regularized scores f .

Our iterative solution to this optimization interpolates the score of a document with the scores of its neighbors. Metaphorically, this process, at each iteration, diffuses scores on the document graph. This is accomplished mathematically by defining a diffusion operator, S , for each Laplacian.
 Given this operator, the score diffusion process can be formulated as, optimization.

In our work, we use the closed form solution to Eq. 13 . The optimal regularized scores can be formulated as the solution of matrix operations, where a is defined above.
 matrix computed in Step 1 is used for adding elements to W in Step 2 and does not define W itself unless k  X  ~ n . 5 Corpus-aware retrieval methods which reduce to instances of iterative score regularization Several classic retrieval methods can be posed as instances of score regularization. We will regularization (Eq. 14 ). In previous sections, we considered only the top ~ n n documents from some initial retrieval. In this section, we may at times consider every document in the collection (i.e., ~ n  X  n ).

For each of the methods in this section, we will be asking ourselves the following summary of these results in Table 2 . 5.1 Vector space model retrieval In Sect. 3.1 , we represented each document as a L 2 normalized, length-m vector, d .A query can also be represented by a normalized, length-m vector, q . A document X  X  score is the inner product between its vector and the query vector (i.e., y i  X h d i ; q i ).
Pseudo-relevance feedback or query expansion refers to the technique of building a model out of the top r documents retrieved by the original query. The system then performs a second retrieval using combination of this model and the original query. In the vector space model, the classic Rocchio pseudo-relevance feedback algorithm assumes that the relevant set be R and r =| R |. In Rocchio, we linearly combine the vectors of documents in R with the original query vector, q . The modified query, ~ q , is defined as, where a is the weight placed on the pseudo-relevant documents. We can then use this new representation to score documents by their similarity to ~ q .
 Theorem 1 Pseudo-relevance feedback in the vector space model is a form of regularization .
 Proof First, we note that the similarity between a document and the new query can be written as the combination of the original document score and the sum of similarities to the pseudo-relevant set, represents the similarity to the pseudo-relevant documents, Eq. 17 in terms of matrix operators to compute the new scores for all documents in the affinity matrix, where r  X  y  X  : &lt; n !&lt; n is defined as, We compare r  X  y  X  to y in Fig. 7 . The r function maps high-ranked documents to pseudo-demonstrates that Rocchio is an instance of score regularization. (
Whereas query expansion incorporates into a query terms from r pseudo-relevant documents, document expansion incorporates into a document the terms from its k most similar neighbors (Singhal and Pereira 1999 ). The modified document, ~ d , is defined as, where a D is the weight placed on the original document vector. N is the set of k documents most similar to document i . Theorem 2 Document expansion in the vector space model is a form of regularization . Proof Define the binary matrix W so that each row i contains k non-zero entries for each of the indices in N ( i ). We can expand all documents in the collection, Given a query vector, we can score the entire collection, The implication here is that the score of an expanded document ( f i ) is the linear combi-nation of the original score ( y i ) and the scores of its k neighbors  X  1 k demonstrates that document expansion is a form of regularization. (
We now turn to the dimensionality reduction school of cluster-based retrieval algo-rithms. In the previous proof, we expanded the entire collection using the matrix W . Clustering techniques such as Latent Semantic Indexing (LSI) can also be used to expand documents (Deerwester et al. 1990 ). LSI-style techniques use two auxiliary matrices: V is the k  X  n matrix embedding documents in the k -dimensional space and U is m  X  k rep-resentations of the dimensions in the ambient space. Oftentimes, queries are processed by projecting them into the k -dimensional space (i.e., ~ q  X  U T q ). We use an equivalent for-mula where we expand documents by their LSI-based dimensions, We then score a document by its cluster-expanded representation. 4 Theorem 3 Cluster-based retrieval in the vector space model is a form of regularization . Proof Our proof is similar to the proof for document expansion. Essentially, the document scores are interpolated with the scores of the clusters. ( 5.2 Language modeling retrieval Recall that in Sect. 3.2 we used L 1 normalized document vectors to compute similarity. The elements of these vectors are estimates of a term X  X  probability given its frequency in the document and the collection. We refer to the L 1 normalized document vector as the document language model, P  X  w j h d  X  . When treated as a very short document, a query can be also represented as m -dimensional language model, P  X  w j h Q  X  . We can rank documents by the similarity of their models to the query model using a multinomial similarity measure such as cross entropy, where q is our initial query model and the log is applied to elements of C (Ro  X  lleke et al. 2006 ). This is rank equivalent to the likelihood of a document generating the sequence of query terms, P  X  Q j h d  X  .

In the language modeling framework, pseudo-relevance feedback can be defined in several ways. We focus on the  X  X  X elevance model X  X  technique (Lavrenko 2004 ). In this case, the original scores are used to weight each document X  X  contribution to the feedback model, models, P  X  w j h d  X  , weighted by their scores where, as before, R is the set of top r documents, Z X  between the original query model and the expanded model. In terms of Fig. 7 , this means using an L 1 normalized version of y . In matrix notation, We then score documents according to Eq. 24 .
 Theorem 4 Relevance models are a form of regularization .
 Proof Our proof is based on a similar derivation used in the context of efficient pseudo-relevance feedback (Lavrenko and Allan 2006 ). Recall that we use  X  log C  X  ~ q to rank the collection. By rearranging some terms, we can view relevance models from a different perspective, where A is an n  X  n affinity matrix based on inter-document cross-entropy. Since the relevance model scores can be computed as a function of inter-document affinity and the Eq. 26 has been shown to improve performance of relevance models and provides an argument for considering the closed form solution in Eq. 15 (Kurland et al. 2005 ). 5 (
Unfortunately, we cannot reduce document expansion in the language modeling framework to regularization. Document expansion in language modeling refers to adjusting the document language models P  X  w j h d  X  given information about neighboring documents (Tao et al. 2006 ). In this situation, the score function can be written as, Because the logarithm effectively decouples the document expansion from the document scoring, the approach used in the vector space model proof cannot be used here.
The language modeling approach to cluster-based retrieval is conceptually very similar to document expansion (Liu and Croft 2004 ; Wei and Croft 2006 ). The distribution P ( z | D ) representing language models for each of our subtopics. When we interpolate these models with the maximum likelihood document models, we get a score function similar to Eq. 23 , document expansion scores, the logarithm prevents converting cluster-based expansion into a regularization form.

It is worth devoting some time to Kurland and Lee X  X  cluster-based retrieval model (Kurland and Lee 2004 ). The model is used to perform retrieval in three steps. First, each document is scored according to an expanded document model. Second, an n  X  n matrix comparing unexpanded and expanded models is constructed. Finally, each document is scored by the linear interpolation of its original (unexpanded) score and the scores of the nearest expanded documents. To this extent, the model combines regularization and document-expansion retrieval in a language modeling framework. Unfortunately, there do not appear to be experiments demonstrating the effectiveness of each of these steps. Is this model an instance of score regularization? Yes and no. The second interpolation process clearly is an iteration of score regularization. The first score is language model document expansion and therefore not regularization.

Recall that the vector space model allowed fluid mathematical movement from query expansion to regularization to document expansion and finally to cluster-based retrieval. This is not the case for language modeling. Language models have a set of rank-equivalent score functions; we adopt cross entropy in our work. The problem, however, is that measures such as the Kullback X  X eibler divergence, cross entropy, and query likelihood all are non-symmetric and therefore not valid inner products. This disrupts the comparison to the vector space model derivations because a smooth transition from regularization (Eq. 27 ) to document expansion is impossible. 5.3 Laplacian eigenmaps Score regularization can be viewed as nonparametric function approximation. An alter-native method of approximation reconstructs y with smooth basis functions. When put in this perspective, reconstructing the original function, y , using smooth basis functions indirectly introduces the desired inter-document consistency (Belkin and Niyogi 2003 ). When Fourier analysis is generalized to the discrete situation of graphs, the eigenvectors of D provide a set of orthonormal basis functions. We can then construct a smooth approx-imation of y using these basis functions. In this situation, our solution is, where E is a matrix consisting of the k eigenvectors of D associated with the smallest k eigenvalues. These eigenvectors represent the low frequency harmonics on the graph and therefore result in smooth reconstruction.
 Theorem 5 Function approximation using harmonic functions of the document graph is a form of regularization .
 vector space model, Eq. 30 can be rewritten as, where the k  X  m matrix U T represents the basis as linear combinations of document vectors and the n  X  k matrix V T projects documents into the lower dimensional space. In language model retrieval, Eq. 30 can be rewritten as, where the k  X  m matrix U T represents the eigenfunctions as geometric combinations of document vectors.

In both situations, new scores are computed as functions of cluster scores and cluster affinities. Therefore, we claim that basis reconstruction methods are an instance of score regularization. ( 5.4 Link analysis algorithms Graph representations often suggest the use discrete metrics such as PageRank to re-weight initial retrieval scores (Brin and Page 1998 ; Cohn and Hofmann 2000 ; Kleinberg 1998 ; Kurland and Lee 2005 ). These metrics can be thought of as functions from a document to a real value, g W : D!&lt; . The function is indexed by the weight matrix W because these metrics are often dependent only on the graph structure. Let g be the length-~ n vector of values of g for our ~ n documents. We will refer to this vector as the graph structure function . The values in g are often combined with those in y by linear combination (e.g., f  X  y  X  g ) or geometric combination (e.g., f  X  y g ).

Many of these methods are instances of the spectral techniques presented in Sect. 5.3 (Ng et al. 2001 ); specifically, PageRank is the special case where only the top eigenvector is considered (i.e., g  X  E 1 ).

We believe it is very important to ask why the graph represented in W is being used in retrieval. For regularization, the matrix W by design enforces inter-document score con-sistency. For hypertext, the matrix W (by way of g ) provides the stationary distribution of the Markov chain defined by the hypertext graph. This can be a good model of page mation is available, though, the model provided by g is less useful (Richardson et al. 2006 ). When the graph W is derived from content-based similarities, what does g mean? It graphs seem weakly justified. We believe that the incorporation of graph structure through regularization, by contrast, has a more solid theoretical motivation.

Because the structure information is lost when computing g from W , we cannot claim that link analysis algorithms are an instance of regularization. 5.5 Spreading activation activation algorithms (Belew 1989 ; Kwok 1989 ; Salton and Buckley 1988 ; Wilkinson and Hingston 1991 ; Croft et al. 1988 ) and inference network techniques (Turtle and Croft graph. Usually only direct relationships such as authors or sources allow inter-document links. These algorithms operate on functions from nodes to real values, h : fD [ Vg ! &lt; . The domain of the functions includes both documents and terms. The domain of the functions in regularization includes only documents. Clearly spreading activation is not a form of regularization.

However, since regularization is a subset of spreading activation techniques, why appropriate for heterogeneous graphs. Asserting that the scores of a term and a document should be comparable seems tenuous. Second, we believe that our perspective is theo-retically attractive because of its ability to bring together several pseudo-relevance feedback techniques under a single framework. Nevertheless, the formal study of hetero-future work. 5.6 Relevance propagation Hypertext collections have inspired several algorithms for spreading content-based scores over the web graph (Qin et al. 2005 ). These algorithms are equivalent to using a hyperlink-based affinity matrix and iterative regularization. A similar approach for content-based affinity has also been proposed (Savoy 1997 ). The foundation of these algorithms is at times heuristic, though. We believe that our approach places regularization X  X hether based on hyperlinks or content affinity X  X n the context of a mathematical formalism. 5.7 Summary perspective of score regularization. We present a summary of these results in Table 2 .
In the course of our derivations, we have sought to generalize and squint when necessary to show similarities between algorithms. In practice, the implementation of these algorithms differs from what is presented here. We believe these implementation differ-ences explain some performance differences and deserve more detailed analysis. A variety of graph algorithms exist which use links based on content and hyperlinks. These algorithms often are very subtle variations of each other when analyzed. We hope that our discussion will provide a basis for comparing graph-based and corpus structure algorithms for information retrieval.

Finally, we have restricted our discussion of scoring algorithms to two popular approaches: vector space retrieval and retrieval of language models. Certainly other models exist and deserve similar treatment. This section should provide a perspective not on only analyzing query expansion, regularization, and document expansion in other frameworks but also on developing query expansion, regularization, and document expansion for new frameworks. 6 Experiments Having presented a theoretical context for score regularization, we now turn to empirically evaluating the application of regularization to retrieval scores. We conducted two sets of experiments. The first set of experiments studies the behavior of regularization in detail for four retrieval algorithms: one vector space model algorithm (Okapi), two language mod-eling algorithms (query likelihood, relevance models), and one structured query algorithm (dependence models); we will abbreviate these okapi, QL, RM, and DM. We present detailed results demonstrating improvements and parameter stability. We will refer to these automatic runs submitted to the TREC ad hoc retrieval track. These experiments dem-onstrate the generalizability of regularization.

For all experiments, we will be using queries or topics on a fixed collection with pool-based relevance judgments. These judgments come exclusively from previous TREC experiments and allow for reproducibility of results. 6.1 Training Whenever parameters needed tuning, we performed 10-fold cross-validation. We adopt a domly partition the queries for a particular collection. For each partition, i , the algorithm is trained on all but that partition and is evaluated using that partition, i . For example, if the training phase considers the topics and judgments in partitions 1 X 9, then the testing phase partition 10. Using each of the ten possible training sets of size nine, we generate unique evaluation rankings for each of the topics over all partitions. Evaluation and comparison was performed using the union of these ranked lists.
 6.2 Detailed experiments For these detailed experiments, we sought baselines which were strong, in sense of high performance, and realistic, in the sense of not over-fitting. Therefore, we first performed experiments in the subsequent sections. We describe our experimental data in Sect. 6.2.1 algorithms in Table 3 . We also present trained parameter values (or ranges if they were different across partitions). In Sect. 6.2.5 we discuss the free parameters in regularization and our method for selecting parameter values. 6.2.1 Topics We performed experiments on two data sets. The first data set, which we will call  X  X  X rec12 X  X , consists of the 150 TREC Ad Hoc topics 51 X 200. We used only the news collections on Tipster disks 1 and 2 (Harman 1993 ). The second data set, which we will call  X  X  X obust X  X , consists of the 250 TREC 2004 Robust topics (Voorhees 2004 ). We used only the news collections on TREC disks 4 and 5. The robust topics are considered to be difficult and have been constructed to focus on topics which systems usually perform poorly on. For both data sets, we use only the topic title field as the query. The topic title is a short, keyword query associated with each TREC topic. We indexed collections using the Indri retrieval system, the Rainbow stop word list, and Krovetz stemming (Strohman et al. 2004 ; McCallum 1996 ; Krovetz 1993 ). 6.2.2 Vector space model scores We conducted experiments studying the regularization of vector space model scores (Robertson and Walker 1994 ). In this approach, documents are represented using a stan-dard tf.idf formula, where d is a length-m document vector where elements contain the raw term frequency, documents according to the inner product with the query vector, h ~ d ; q i .

When computing the similarity between documents, we use an alternate formulation, where c is the length-m document frequency vector. We use this weighting scheme due to its success for topical link detection in the context of Topic Detection and Tracking (TDT) matrix. 6.2.3 Language model scores Language model systems provide strong baselines. We use query-likelihood retrieval (Croft and Lafferty 2003 ) and relevance models (Lavrenko 2004 ). Both of these algorithms are implemented in Indri (Strohman et al. 2004 ).

In the retrieval phase, we use Dirichlet smoothing of document vectors, and maximum likelihood query vectors, parameter, l .

For pseudo-relevance feedback, we take the top r documents from this initial retrieval and build our relevance model using Eq. 25 . In practice, we only use the top ~ m terms in the relevance model. We optimize the parameters r , ~ m , and k .

When computing the similarity between documents, we use the diffusion kernel and maximum likelihood document models, which we found to be superior to smoothed versions for this task. 6.2.4 Dependence model scores Our final baseline system uses a structured query model which incorporates inter-term dependencies (Metzler and Croft 2005 ). We present this baseline to demonstrate the applicability of regularization to non-vector space methods. We use the Indri query lan-guage to implement full dependence models with fixed parameters of  X  k
T ; k O ; k U  X  X f 0 : 8 ; 0 : 1 ; 0 : 1 g as suggested in the literature. 6.2.5 Regularization parameters We performed grid search to train regularization parameters. Parameter values considered are, where t is only swept for runs using the diffusion kernel
We normalized all scores to zero mean and unit variance for empirical and theoretical reasons (Belkin et al. 2004 ; Montague and Aslam 2001 ). We have found that using alternative score normalization methods performed slightly worse but we do not present those results here. 6.3 Regularizing TREC ad hoc retrieval track scores In addition to our detailed experiments, we were interested in evaluating the generalizability of score regularization to arbitrary initial retrieval algorithms. To this end, we collected the document rankings for all automatic runs submitted to the Ad Hoc Retrieval track for TRECs 3 X 8, Robust 2003 X 2005, Terabyte 2004 X 2005, TRECs 3 X 4 Spanish, and TRECs 5 X 6 Chinese (Voorhees and Harman 2001 ). This constitutes a variety of runs and tasks with varying levels of performance. In all cases, we use the appropriate evaluation corpora, not just the news portions as in the detailed experiments. We also include results for the TREC 14 Enterprise track Entity Retrieval subtask. This subtask deals with the modeling and retrieval of entities participating in TREC include a score in run submissions, we cannot be confident about the accuracy of the scores. Therefore, inconsistent behavior for some runs may be the result of inaccurate scores.

We ran experiments using the cosine similarity described in Sect. 3 . Due to the large number of runs, we fix k = 25 and sweep a between 0.05 and 0.95 with a step size of 0.05. Non-English collections received no linguistic processing: tokens were broken on white-space for Spanish and single characters were used for Chinese. Entity similarity is determined by the co-occurrence of entity names in the corpus. The optimal a is selected using 10-fold cross validation optimizing mean average precision. 7 Results In this section, we describe results for our two sets of experiments. Section 7.1 presents a detailed analysis of regularizing several strong baselines. Section 7.2 presents results demonstrating the generalizability of regularization to scores from arbitrary initial retrieval systems. 7.1 Detailed experiments significant improvements across all four algorithms across all collections. This improve-ment is more pronounced for the techniques which do not use pseudo-relevance feedback (okapi and QL). As noted earlier, our pseudo-relevance feedback run (RM) bears theo-retical similarities to regularization (Sect. 5.2 ) and therefore may not garner rewards seen by other methods. Nevertheless, even this run sees significant gains in mean average precision. Regularizing the dependence model scores produce rankings which out-perform baseline relevance model scores for the robust collection.

Next, we examine the impact of our choice of Laplacian. In Sect. 4.1 , we described likely to be a non-uniform sample across topics, we adopted the approximate Laplace X  Beltrami operator which addresses sampling violations. In order to evaluate this choice of Laplacian, we compared the improvements in performance (i.e., map of the regularized scores minus map of the original scores) for all three Laplacians. Our hypothesis was that the approximate Laplace X  X eltrami operator, because it is designed to be robust to sampling violations, would result in strong improvements in performance. The results of this comparison are presented in Fig. 8 . In all cases the simple combinatorial Laplacian und-erperforms other Laplacians. Recall from Eq. 7 that, although it weights the comparisons in scores between documents using W ij , the combinatorial Laplacian does not normalize this weight by the node degrees (i.e., D ii ). Both the normalized Laplacian (Eq. 10 ) and the approximate Laplace X  X eltrami operator (Eq. 11 ) normalize this weight. However, there do not appear to be significant advantages to using the approximate Laplace X  X eltrami oper-ator over the normalized Laplacian.

Our first set of experiments, described in Table 4 , demonstrated improvements across all four baseline algorithms. The a parameter controls the degree of regularization. In algorithms which did not use pseudo-relevance feedback benefited from more aggressive regularization. The pseudo-relevance feedback baseline peaks when initial and regularized scores are more equally weighted.

One of the core assumptions behind our technique is the presence of an underlying manifold or lower-dimensional structure recovered by the graph. The number of neighbors ( k ) represents how much we trust the ambient affinity measure for this set of documents .If performance improves as we consider more neighbors, manifold methods seem less jus-tified. In order to test this assumption, we evaluate performance as a function of the number degradation in performance as more neighbors are considered. This occurs even in the presence of a soft nearest neighbor measure such as the diffusion kernel.
 7.2 Regularizing TREC ad hoc retrieval track scores Our detailed experiments demonstrated the improvement of performance achieved by regularizing three strong baselines. We were also interested in the performance over a wide variety of initial retrieval algorithms. We present results for regularizing the TREC Ad Hoc submissions in Figs. 11 and 12 using cosine similarity. 6 Although regularization on average produces improvements, there are a handful of runs for which performance is significantly degraded. This reduction in performance may be the result of an unoptimized k parameter. Improvements are consistent across collections and languages. 8 Discussion We proposed score regularization as a generic post-processing procedure for improving the performance of arbitrary score functions. The results in Figs. 11 and 12 provide evidence that existing retrieval algorithms can benefit from regularization.

We see the benefits in Table 4 when considering several different baselines. However, we can also inspect the improvement in performance as a function of the number of documents being regularized  X  ~ n  X  . In Fig. 13 , we notice that performance improves and then plateaus. Though regularization helps both Okapi and QL, the improvement is never comparable to performing pseudo-relevance feedback. This means that despite there being strength in the second retrieval missing in regularization. Nevertheless, our strong single-retrieval algorithm, dependence models, achieves performance comparable to relevance models when regularized.

The results in Figs. 8 and 10 suggest that the construction of the diffusion operator is sometimes important for regularization efficacy. Since there are a variety of methods for constructing affinity and diffusion geometries, we believe that this should inspire a formal study and comparison of various proposals.
 The results in Fig. 10 also allow us to test the manifold properties of the initial retrieval. behaves well for the documents in this retrieval. Poorer-performing algorithms, by definition, have a mix of relevant and non-relevant documents. Including more edges in the graph by increasing the value of k will be more likely to relate relevant and non-relevant documents. From the perspective of graph-based methods, the initial retrieval for poorer-performing algorithms should be aggressively sparsified with low values for k . On the other hand, better performing algorithms may benefit less from a graph-based representation allowing us to let k grow. From a geometric perspective, documents from poorer-performing algorithms are retrieved from regions of the embedding space so disparate that affinity is poorly-approxi-mated by the ambient affinity. Documents from better performing queries all exist in a region of the embedding space where affinity is well-approximated by the ambient affinity.
We have noted that the aggressiveness of regularization ( a ) is related to the performance of the initial retrieval. Figure 9 demonstrates that smaller values for a are more suitable for better-performing algorithms. This indicates that the use of techniques from precision prediction may help to automatically adjust the a parameter (Carmel et al. 2006 ; Cronen-Townsend et al. 2002 ; Yom-Tov et al. 2005 ).

Finally, we should address the question of efficiency. There are two points of computational comparisons. For ~ n  X  1,000, this took approximately 8 s. Although most of our experiments use ~ n  X  1,000, our results in Fig. 13 show that ~ n need not be as large as this to achieve improvements. For example, for ~ n  X  100, this computation takes less than 0.5 s. We should optimal performance. This implies that the storage cost would be O ( nk ). The second point of computational overhead is in the inversion of the matrix in Eq. 15 . We show running time as a function of ~ n in Fig. 14 . Note that our experiments, although very expensive when ~ n  X  1,000, can be computationally improved significantly by reducing ~ n to 500 which, according to Fig. 13 , would still boost baseline performance. We could also address the inversion by using the iterative solution. In related work, using a pre-computed similarity matrix and an iterative solution allowed the use of theoretical results from Sect. 5.2 to conduct real-time pseudo-relevance feedback (Lavrenko and Allan 2006 ). 9 Conclusions We have demonstrated the theoretical as well as the empirical benefits of score regulari-zation. Theoretically, regularization provides a generalization of many classic techniques in information retrieval. By presenting a model-independent vocabulary for these tech-niques, we believe that the disparate areas for information retrieval can be studied method for improving arbitrary retrieval algorithms. Because of the consistent improve-ments and potential extensions, we believe that regularization should be applied whenever topical correlation between document scores is anticipated. Furthermore, we believe that, if possible, regularization should be used as a design principle for retrieval models. References
