 Feature extraction for signal representation is an invariable topic in pattern recogni-tion, which aims to concisely describe the dominant feature of a sample set. Numerous methods have been developed to extract reliable features for various data sets. PCA (principal component analysis) may be the most well-known and widely used method. The features extracted by PCA have many good properties and were successfully used in many problems. However, the world is nonlinear. When the linear model of PCA fails to describe the complex inner structures of nonlinear patterns, many nonlinear gener-alizations of PCA are developed. Among those methods, principal curve methods are most attractive. Principal curve methods, such as HS principal curves (HSPC) [1], T principal curve (TPC)[2], K principal curves (KPC)[3] ,principal curves with bounded turn (PCBT)[4], D principal curves(DPC)[5] and so on, are proposed by generalizing respective properties of the first principal component line. These principal curves have tive models that can describe the data best [6][7].

However, for the tasks of extracting reliable features for signal presentation, exist-embodied in neighbor relationships between samples. So, a basic requirement of feature boring. It is easy to know that PCA does keep the neighbor relationship for neighboring samples. However, when we take the project index functions of principal curves as the feature extractors, existing principal curves do not guarantee the neighbor relationship preserving for neighboring samples. Furthermore, reconstruction error minimization is another requirement of feature extraction f or signal representation, but some kinds of principal curves do not satisfy the requirement.

Recently, we propose a new definition of princ ipal curves: Principal Curve with Fea-ture Continuity(PCFC). PCFC is proposed for extracting one-dimensional features for signal representation. It is focused on both the reconstruction error minimization and the neighbor relationship preserving for neighboring samples. From the viewpoint of feature extraction for signal representation, PCFC is an optimal one-dimensional fea-ture extractor in the sample space.

The rest of the paper is organized as follows: In Section 2, we introduce the frame-work of feature extraction for signal representation using curves and evaluate the ex-isting kinds of principal curves inside this framework. Then the definition of PCFC is introduced and its properties are analyzed in Section 3. Finally, Section 4 concludes with a description of direc tions for future research. In this section, we introduce some basic concepts of curves and the framework of feature extraction using curves. 2.1 Preliminaries and Notation Definition 1. A parameterized curve f is a continuous mapping f :  X   X  R d ,where  X  is a close subset of R .
 We denote by I f the domain of f and by G f its range.
 Definition 2. The length of a parameterized curve f :  X   X  R d over an interval [  X ,  X  ]  X  I f , denoted by l ( f , X , X  ) , is defined by where the supremum is taken over all finite partitions of [  X ,  X  ] with arbitrary subdi-vision points  X  = t 0  X  t 1 &lt;  X  X  X   X  t N =  X  for N  X  1 . The total length of the parameterized curve f is defined as: A parameterized curve f is called rectifiable if l ( f ) &lt;  X  .
 Definition 3. Consider a piecewise-linear curve f with vertices v 0 ...v n .Let a i = v  X  v linear curve is defined by For a general curve f , the turn accumulated over an interval [  X ,  X  ] of its domain is defined as the supremum over all piecewise-linear inscriptions in [  X ,  X  ] , i.e.,  X  = t 0 &lt;t 1 &lt;  X  X  X  &lt;t n  X  1 &lt;t n =  X  . The total turn of the interval [  X ,  X  ]  X  I f and t  X  [  X ,  X  ] , l ( f , X ,t )= t  X   X  .

The projection index of a point x to the parameterized curve f is defined as: We denote the distortion of x due to its projection onto a parameterized curve f as: an ambiguity point to f .

For a random variable X , we denote the expected distortion due to its projection onto f as:
Two different parameterized curves f : I f  X  R d and g : I g  X  R d may define a same path passing the same set of points with the same order . We can regard these two parameterized curves define one and the same curve L . In fact, an equivalence re-lation can be established between the param eterized curves, and an equivalence class the same length, the same total turn and the same distortion from a point and a random variable. From the theory of irregular curves [8], we know that curves with finite turn can be parameterized by its arc length. To eliminate the ambiguity of parameteriza-and unless explicitly mentioned, we always prescribe that the parameterized curves are parameterized by their arc length and satisfy t f ( O )=0 . 2.2 Feature Extractor and Reconstruction Function Given a random variable X and a parameterized curve f ,wedefine the curve. So F f is independent of the parameterization.

Now the one-dimensional feature extraction framework using curves can be estab-PCA, in which f is the first principal component line of the random variable. 2.3 Evaluation Criterion s of Feature Extractors When evaluating a feature extractor for signal representation, there are commonly two criterions:  X  Feature Continuity: the feature extractor should be a continuous function of sam- X  Reconstruction Error Minimization: the distortion between the samples and recon-
In the one-dimensional feature extraction framework using curves, these two criteri-ons becomes: 1) F f should be continuous; 2)  X  ( f ) should be minimized.
According to these two criterions, existing principal curve methods are evaluated as follows from the view point of feature extraction for signal representation. on a circle { ( r,  X  ): r = R } , the HSPC, DPC, TPC, KPC with upper-bound of length larger than 2  X R and PCBT with upper-bound of total turn larger than 2  X  are all the circle. Given any parameterization f of the circle, the F f cannot be continuous. From the instance, we can see that the theories of existing principal curves do not guarantee the continuity of the feature extractor F f .

Because of lacking the theoretic assurance of the continuity of F f , in practice, the on a data set of 100 samples uniformly distributed on a unit square, with comparison to those of first principal component line and a desired curve.

We can see from the results that the learning results of K principal curve and HS principal curve achieves smaller reconstruction errors. However, neighboring samples are projected to the points far from each ot her on the curves, which means that the feature continuity has been violated. The first principal component line shown in Figure first principal component line, it achieves a lower value of  X  ( f ) , which means more effective in feature extraction. It is a d esired curve for feature extraction. Reconstruction Error Minimization. KPC and PCBT minimize  X  ( f ) in respective curve classes, while other principal curv es such as HSPC, TPC and DPC do not neces-sarily do so.

To sum up, existing definitions of principal curves are not competent for the task of feature extraction for signal representation. The main reason is that those definitions either. 3.1 Concept and Definition one that minimizes  X  ( f ) with continuous F f . We call it Principal Curve with Feature Continuity(PCFC). To support the validity of the concept of PCFC, we impose more restrict regularity conditions on the class of curves to be studied.

Let us denote the open ball centered at the origin with radius r as B r . Given a pa-0 | X  s  X  ( t, 0] , f ( s )  X  B f |
B r as the curve g with I g =[ t 1 ,t 2 ] and g ( t )= f ( t ) ,t restriction of f to the ball B r .

To avoid the situation that infimum of  X  ( f ) may not be achieved by any curve, (an example is detailed in [4]), we consider the following class of curves [4]: where  X  ( R ) is continuous and decreasing to zero in R .
In order to get reliable cognition about the properties of the data set through the samples must be under control. So we consider the curves inside the following class: where S ( X ) denote the support of the random variable X .
 Finally, for a random variable X , we consider the class of curves as follows: Now, we give the definition of principal curve with feature continuity: Definition 4. Given a random variable X , a parameterized curve f  X  is called the prin-cipal curve with feature continuity for X with parameter ( T, X , X ,K ) if it minimizes Given the definition, following theorem ensures that the PCFC always exists for data distributions with finite second moments and open support: for X with parameter ( T, X , X ,K ) .
 here. The outline of the proof is shown below. The key to prove the theorem is the following lemma: Lemma 1. For any open set Y and compact set A  X  R d ,thesetofcurves { f | G f  X  A,  X  ( f )  X  T } X   X   X ,K,Y is compact.
 Given this lemma, the other part of the proof roughly follows the way of the proof of the existence of PCBT [4]. Let  X   X  =inf f  X   X  { f curve X  f  X  can be defined as the curve that satisfies when  X  r large enough, Thus, f  X  is the desired principal curve with feature continuity.
 When S ( X ) is not open, we consider to expand the support of the distribution: unite S ( X ) with a set of small open balls centered at its boundary points. We denote the consideration of generalization and in fluences of noises in real world problems. 3.2 Properties of Principal Curves with Feature Continuity The definition of principal curves with feature continuity directly results in following proposition: Proposition 1. If f is a PCFC for random variable X with an open support S ( X ) ,then there is no ambiguity point of f in S ( X ) .
 Now we study the differential properties of principal curves with feature continuity. proposition is valid.
 Proposition 2. If f is the principal curves with feature continuity of X ( parameterized by its arc length), then:  X  the non-differentiable points of f ( t ) are not more than countable. Proposition 3. If f is the principal curve with feature continuity of X ( parameterized by its arc length). If for t 0 ,there  X  x 0 , X  0 &gt; 0 satisfying following conditions:  X  t 0 = t f ( x 0 ) ;  X  O ( x 0 , X  0 )  X  S ( X ) ;  X   X  0 &lt; X &lt; X  0 ,t f ( O ( x 0 , X  )) is open. f ( t ) is first order differentiable at t 0 .
 The facts revealed by proposition 3 are illustrated in Figure 2.
Figure 2 shows a part of a distribution and various possibilities of PCFC. In (a), continuity of PCFC. Thus, the non-smoothness described in (a) is not allowed for PCFC. (b) shows the situation that the conditions of proposition 3 describe: however small the neighborhood is, the set of projection indexes of points in the neighborhood is open. somewhere not differentiable is shown in (c). In this case, when the neighborhood is small enough, the points in the neighborhood all project into one and the same point, thus the condition of proposition 3 is not satisfied. In this paper, we proposed a new definition of p rincipal curves -Principal Curve with Feature Continuity to extract reliable one-dimensional features for signal representa-tion. PCFC focuses on feature continuity and reconstruction error minimization. From the viewpoint of signal representation, it is an optimal one-dimensional feature extrac-tor in the sample space. The existence of the PCFC is guaranteed for a large set of data distributions. PCFC also has good differential properties.
 dimensional features for signal representation, we would like to study the definition future work.
 This work was supported by the National Natural Science Foundation of China under Grant No. 60473039, No. 60503026, No. 60472060 and No. 60632050.

