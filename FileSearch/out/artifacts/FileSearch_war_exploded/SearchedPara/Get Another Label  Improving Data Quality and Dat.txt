 This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improve-ment (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon X  X  Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strate-gies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the tradi-tional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.
 H.2.8 [ Database Applications ]: Data mining; I.5.2 [ Design Methodology ]: Classifier design and evaluation Algorithms, Design, Experimentation, Management, Measure-ment, Performance data selection, data preprocessing
There are various costs associated with the preprocessing stage of the KDD process, including costs of acquiring features, formulating data, cleaning data, obtaining expert labeling of data, and so on [ 31 , 32 ]. For example, in order to build a model to recognize whether two products described on two web pages are the same, one must extract the product information from the pages, formulate features for comparing the two along relevant dimensions, and label product pairs as identical or not; this process involves costly manual intervention at several points. To build a model that recognizes whether an image contains an object of interest, one first needs to take pictures in appropriate contexts, sometimes at substantial cost.
This paper focuses on problems where it is possible to ob-tain certain (noisy) data values ( X  X abels X ) relatively cheaply, from multiple sources ( X  X abelers X ). A main focus of this paper is the use of these values as training labels for supervised mod-eling. 1 For our two examples above, once we have constructed the unlabeled example, for relatively low cost one can obtain non-expert opinions on whether two products are the same or whether an image contains a person or a storefront or a building. These cheap labels may be noisy due to lack of expertise, dedication, interest, or other factors. Our ability to perform non-expert labeling cheaply and easily is facilitated by on-line outsourcing systems such as Rent-A-Coder 2 and Amazon X  X  Mechanical Turk, 3 which match workers with ar-bitrary (well-defined) tasks, as well as by creative labeling solutions like the ESP game. 4
In the face of noisy labeling, as the ratio increases between the cost of preprocessing a data point and the cost of labeling it, it is natural to consider repeated labeling : obtaining multiple labels for some or all data points. This paper explores whether, when, and for which data points one should obtain multiple, noisy training labels, as well as what to do with them once they have been obtained.

Figure 1 shows learning curves under different labeling qual-ities for the mushroom data set (see Section 4.1 ). Specifically, for the different quality levels of the training data, 5 the fig-ure shows learning curves relating the classification accuracy of a Weka J48 model [ 34 ] to the number of training data. This data set is illustrative because with zero-noise labels one can achieve perfect classification after some training, as demonstrated by the q =1 . 0 curve.

Figure 1 illustrates that the performance of a learned model Figure 1: Learning curves under different quality lev-els of training data ( q is the probability of a label being correct). depends both on the quality of the training labels and on the number of training examples. Of course if the training labels are uninformative ( q =0 . 5 ), no amount of training data helps. As expected, under the same labeling quality, more training examples lead to better performance, and the higher the quality of the training data, the better the performance of the learned model. However, the relationship between the two factors is complex: the marginal increase in performance for a given change along each dimension is quite different for different combinations of values for both dimensions. To this, one must overlay the different costs of acquiring only new labels versus whole new examples, as well as the expected improvement in quality when acquiring multiple new labels.
This paper makes several contributions. First, under gradu-ally weakening assumptions, we assess the impact of repeated-labeling on the quality of the resultant labels, as a function of the number and the individual qualities of the labelers. We derive analytically the conditions under which repeated-labeling will be more or less effective in improving resultant label quality. We then consider the effect of repeated-labeling on the accuracy of supervised modeling. As demonstrated in Figure 1 , the relative advantage of increasing the quality of la-beling, as compared to acquiring new data points, depends on the position on the learning curves. We show that even if we ignore the cost of obtaining the unlabeled part of a data point, there are times when repeated-labeling is preferable compared to getting labels for unlabeled examples. Furthermore, when we do consider the cost of obtaining the unlabeled portion, repeated-labeling can give considerable advantage.

We present a comprehensive experimental analysis of the relationships between quality, cost, and technique for repeated-labeling. The results show that even a straightforward, round-robin technique for repeated-labeling can give substantial benefit over single-labeling. We then show that selectively choosing the examples to label repeatedly yields substantial extra benefit. A key question is: How should we select data points for repeated-labeling? We present two techniques based on different types of information, each of which improves over round-robin repeated labeling. Then we show that a technique that combines the two types of information is even better.
Although this paper covers a good deal of ground, there is much left to be done to understand how best to label using multiple, noisy labelers; so, the paper closes with a summary of the key limitations, and some suggestions for future work.
Repeatedly labeling the same data point is practiced in applications where labeling is not perfect (e.g., [ 27 , 28 ]). We are not aware of a systematic assessment of the relationship between the resultant quality of supervised modeling and the number of, quality of, and method of selection of data points for repeated-labeling. To our knowledge, the typi-cal strategy used in practice is what we call  X  X ound-robin X  repeated-labeling, where cases are given a fixed number of labels X  X o we focus considerable attention in the paper to this strategy. A related important problem is how in practice to assess the generalization performance of a learned model with uncertain labels [ 28 ], which we do not consider in this paper. Prior research has addressed important problems necessary for a full labeling solution that uses multiple noisy labelers, such as estimating the quality of labelers [ 6 , 26 , 28 ], and learning with uncertain labels [ 13 , 24 , 25 ]. So we treat these topics quickly when they arise, and lean on the prior work.
Repeated-labeling using multiple noisy labelers is different from multiple label classification [ 3 , 15 ], where one example could have multiple correct class labels. As we discuss in Section 5 , repeated-labeling can apply regardless of the number of true class labels. The key difference is whether the labels are noisy. A closely related problem setting is described by Jin and Ghahramani [ 10 ]. In their variant of the multiple label classification problem, each example presents itself with a set mutually exclusive labels, one of which is correct. The setting for repeated-labeling has important differences: labels are acquired (at a cost); the same label may appear many times, and the true label may not appear at all. Again, the level of error in labeling is a key factor.

The consideration of data acquisition costs has seen in-creasing research attention, both explicitly (e.g., cost-sensitive learning [ 31 ], utility-based data mining [ 19 ]) and implicitly, as in the case of active learning [ 5 ]. Turney [ 31 ] provides a short but comprehensive survey of the different sorts of costs that should be considered, including data acquisition costs and labeling costs. Most previous work on cost-sensitive learning does not consider labeling cost, assuming that a fixed set of labeled training examples is given, and that the learner cannot acquire additional information during learning (e.g., [ 7 , 8 , 30 ]).
Active learning [ 5 ] focuses on the problem of costly label acquisition, although often the cost is not made explicit. Ac-tive learning (cf., optimal experimental design [ 33 ]) uses the existing model to help select additional data for which to acquire labels [ 1 , 14 , 23 ]. The usual problem setting for active learning is in direct contrast to the setting we consider for repeated-labeling. For active learning, the assumption is that the cost of labeling is considerably higher than the cost of obtaining unlabeled examples (essentially zero for  X  X ool-based X  active learning).
 Some previous work studies data acquisition cost explicitly. For example, several authors [ 11 , 12 , 16 , 17 , 22 , 32 , 37 ] study the costly acquisition of feature information, assuming that the labels are known in advance. Saar-Tsechansky et al. [ 22 ] consider acquiring both costly feature and label information.
None of this prior work considers selectively obtaining mul-tiple labels for data points to improve labeling quality, and the relative advantages and disadvantages for improving model performance. An important difference from the setting for traditional active learning is that labeling strategies that use multiple noisy labelers have access to potentially relevant addi-tional information. The multisets of existing labels intuitively should play a role in determining the examples for which to acquire additional labels. For example, presumably one would be less interested in getting another label for an example that already has a dozen identical labels, than for one with just two, conflicting labels.
Figure 1 illustrates that the quality of the labels can have a marked effect on classification accuracy. Intuitively, using repeated-labeling to shift from a lower-q curve to a higher-q curve can, under some settings, improve learning considerably. In order to treat this more formally, we first introduce some terminology and simplifying assumptions.
We consider a problem of supervised induction of a (binary) classification model. The setting is the typical one, with some important exceptions. For each training example y i ,x i procuring the unlabeled  X  X eature X  portion, x i , incurs cost C The action of labeling the training example with a label y incurs cost C L . For simplicity, we assume that each cost is constant across all examples. Each example y i ,x i has a true label y i , but labeling is error-prone. Specifically, each label y ij comes from a labeler j exhibiting an individual labeling quality p j , which is Pr ( y ij = y i ); since we consider the case of binary classification, the label assigned by labeler j will be incorrect with probability 1  X  p j .

In the current paper, we work under a set of assumptions that allow us to focus on a certain set of problems that arise when labeling using multiple noisy labelers. First, we assume that Pr ( y ij = y i | x i )= Pr ( y ij = y i )= p j , that is, individual labeling quality is independent of the specific data point being labeled. We sidestep the issue of knowing p j : the techniques we present do not rely on this knowledge. Inferring p j accurately should lead to improved techniques; Dawid and Skene [ 6 ] and Smyth et al. [ 26 , 28 ] have shown how to use an expectation-maximization framework for estimating the quality of labelers when all labelers label all available examples. It seems likely that this work can be adapted to work in a more general setting, and applied to repeated-labeling. We also assume for simplicity that each labeler j only gives one label, but that is not a restrictive assumption in what follows. We further discuss limitations and directions for future research in Section 5 .
To investigate the relationship between labeler quality, num-ber of labels, and the overall quality of labeling using multiple labelers, we start by considering the case where for induction each repeatedly-labeled example is assigned a single  X  X nte-grated X  label  X  y i , inferred from the individual y ij  X  X  by majority voting. For simplicity, and to avoid having to break ties, we assume that we always obtain an odd number of labels. The quality q i = Pr (  X  y i = y i ) of the integrated label  X  y called the integrated quality . Where no confusion will arise, we will omit the subscript i for brevity and clarity.
We first consider the case where all labelers exhibit the same quality, that is, p j = p for all j (we will relax this assumption later). Using 2 N + 1 labelers with uniform quality p ,the integrated labeling quality q is: which is the sum of the probabilities that we have more correct labels than incorrect (the index i corresponds to the number of incorrect labels).

Not surprisingly, from the formula above, we can infer that the integrated quality q is greater than p only when p&gt; 0 . 5. When p&lt; 0 . 5, we have an adversarial setting where q&lt;p , and, not surprisingly, the quality decreases as we increase the number of labelers.

Figure 2 demonstrates the analytical relationship between Figure 2: The relationship between integrated label-ing quality, individual quality, and the number of la-belers. Figure 3: Improvement in integrated quality com-pared to single-labeling, as a function of the number of labelers, for different labeler qualities. the integrated quality and the number of labelers, for differ-ent individual labeler qualities. As expected, the integrated quality improves with larger numbers of labelers, when the individual labeling quality p&gt; 0 . 5; however, the marginal improvement decreases as the number of labelers increases. Moreover, the benefit of getting more labelers also depends on the underlying value of p . Figure 3 shows how integrated quality q increases compared to the case of single-labeling, for different values of p and for different numbers of labelers. For example, when p =0 . 9, there is little benefit when the number of labelers increase from 3 to 11. However, when p =0 . 7, going just from single labeling to three labelers increases in-tegrated quality by about 0.1, which in Figure 1 would yield a substantial upward shift in the learning curve (from the q =0 . 7 to the q =0 . 8 curve); in short, a small amount of repeated-labeling can have a noticeable effect for moderate levels of noise.

Therefore, for cost-effective labeling using multiple noisy labelers we need to consider: (a) the effect of the integrated quality q on learning, and (b) the number of labelers required to increase q under different levels of labeler quality p ;wewill return to this later, in Section 4 .
If we relax the assumption that p j = p for all j , and allow labelers to have different qualities, a new question arises: what is preferable: using multiple labelers or using the best individual labeler? A full analysis is beyond the scope (and space limit) of this paper, but let us consider the special case that we have a group of three labelers, where the middle labeling quality is p , the lowest one is p  X  d , and the highest one is p + d . In this case, the integrated quality q is: Figure 4: Repeated-labeling improves quality when d is below the curve (see text).

When is this quantity greater than that of the best labeler p + d ? We omit the derivation for brevity, but Figure 4 plots the values of d that satisfy this relationship. If d is below the curve, using multiple labelers improves quality; otherwise, it is preferable to use the single highest-quality labeler.
Majority voting is a simple and straightforward method for integrating the information from multiple labels, but clearly with its simplicity comes a potentially serious drawback: in-formation is lost about label uncertainty. In principle, an alternative is to move to some form of  X  X oft X  labeling , with the multiset of labels resulting in a probabilistic label for an example [ 25 ]. One concern with soft labeling is that even in cases where, in principle, modeling techniques should be able to incorporate soft labeling directly (which would be true for techniques such as naive Bayes, logistic regression, tree induction, and beyond), existing software packages do not accommodate soft labels. Fortunately, we can finesse this.
Consider the following straightforward method for integrat-ing labels. For each unlabeled example x i , the multiplied examples (ME) procedure considers the multiset of existing labels L i = { y ij } . ME creates one replica of x i labeled by each unique label appearing in L i . Then, for each replica, ME assigns a weight 1 / | L i | , where | L i | is the number of oc-currences of this label in L i . These weighted replicas can be used in different ways by different learning algorithms, e.g., in algorithms that take weights directly (such as cost-sensitive tree [ 29 ]), or in techniques like naive Bayes that naturally incor-porate uncertain labels. Moreover, any importance-weighted classification problem can be reduced to a uniform-weighted classification problem [ 35 ], often performing better than hand-crafted weighted-classification algorithms.
The previous section examined when repeated-labeling can improve quality. We now consider when repeated-labeling should be chosen for modeling . What is the relationship to label quality? (Since we see that for p =1 . 0 and p = 0 . 5, repeated-labeling adds no value.) How cheap (relatively speaking) does labeling have to be? For a given cost setting, is repeated-labeling much better or only marginally better? Can selectively choosing data points to label improve performance?
Practically speaking, the answers to these questions rely on the conditional distributions being modeled, and so we shift to an empirical analysis based on experiments with benchmark data sets.

To investigate the questions above, we present experiments on 12 real-world datasets from [ 2 ] and [ 36 ]. These datasets were chosen because they are classification problems with a moderate number of examples, allowing the development of Table 1: The 12 datasets used in the experiments: the numbers of attributes and examples in each, and the split into positive and negative examples. learning curves based on a large numbers of individual experi-ments. The datasets are described in Table 1 . If necessary, we convert the target to binary (for thyroid we keep the nega-tive class and integrate the other three classes into positive; for splice , we integrate classes IE and EI; for waveform ,we integrate class 1 and 2.)
For each dataset, 30% of the examples are held out, in every run, as the test set from which we calculate general-ization performance. The rest is the  X  X ool X  from which we acquire unlabeled and labeled examples. To simulate noisy label acquisition, we first hide the labels of all examples for each dataset. At the point in an experiment when a label is acquired, we generate a label according to the labeler quality p : we assign the example X  X  original label with probability p and the opposite value with probability 1  X  p .

After obtaining the labels, we add them to the training set to induce a classifier. For the results presented, models are induced with J48, the implementation of C4.5 [ 21 ]in WEKA [ 34 ]. The classifier is evaluated on the test set (with the true labels). Each experiment is repeated 10 times with a different random data partition, and average results are reported.
We first study the setting where we have the choice of either: We assume for this section that examples are selected from the unlabeled pool at random and that repeated-labeling selects examples to re-label in a generalized round-robin fashion: specifically, given a set L of to-be-labeled examples (a subset of the entire set of examples) the next label goes to the example in L with the fewest labels, with ties broken according to some rule (in our case, by cycling through a fixed order).
When C U C L , then C U + C L C L and intuitively it may seem that the additional information on the conditional label distribution brought by an additional whole training example, even with a noisy label, would outweigh the cost-equivalent benefit of a single new label. However, Figure 1 suggests otherwise, especially when considered together with the quality improvements illustrated in Figure 3 .

Figure 5 shows the generalization performance of repeated-labeling with majority vote ( MV ) compared to that of single labeling ( SL ), as a function of the number of labels acquired Figure 5: Comparing the increase in accuracy for the mushroom data set as a function of the number of labels acquired, when the cost of an unlabeled exam-ple is negligible, i.e., C U =0 . Repeated-labeling with majority vote ( MV ) starts with an existing set of ex-amples and only acquires additional labels for them, and single labeling ( SL ) acquires additional examples. Other data sets show similar results. for a fixed labeler quality. Both MV and SL start with the same number of single-labeled examples. Then, MV starts acquiring additional labels only for the existing examples, while SL acquires new examples and labels them.

Generally, whether to invest in another whole training exam-ple or another label depends on the gradient of generalization performance as a function of obtaining another label or a new example. We will return to this when we discuss future work, but for illustration, Figure 5 shows scenarios for our example problem, where each strategy is preferable to the other. From Figure 1 we see that for p =0 . 6, and with 100 examples, there is a lot of headroom for repeated-labeling to improve generalization performance by improving the overall labeling quality. Figure 5 (a) indeed shows that for p =0 . 6, repeated-labeling does improve generalization performance (per label) as compared to single-labeling new examples. On the other hand, for high initial quality or steep sections of the learning curve, repeated-labeling may not compete with sin-gle labeling. Figure 5 (b) shows that single labeling performs better than repeated-labeling when we have a fixed set of 50 training examples with labeling quality p =0 . 8. Particularly, repeated-labeling could not further improve its performance after acquiring a certain amount of labels (cf., the q = 1 curve in Figure 1 ).

The results for other datasets are similar to Figure 5 : un-der noisy labels, and with C U C L , round-robin repeated-labeling can perform better than single-labeling when there are enough training examples, i.e., after the learning curves are not so steep (cf., Figure 1 ).
We illustrated above that repeated-labeling is a viable alter-native to single-labeling, even when the cost of acquiring the  X  X eature X  part of an example is negligible compared to the cost of label acquisition. However, as described in the introduction, often the cost of (noisy) label acquisition C L is low compared to the cost C U of acquiring an unlabeled example. In this case, clearly repeated-labeling should be considered: using multiple labels can shift the learning curve up significantly. To compare any two strategies on equal footing, we calcu-late generalization performance  X  X er unit cost X  of acquired data; we then compare the different strategies for combining multiple labels, under different individual labeling qualities. We start by defining the data acquisition cost C D : to be the sum of the cost of acquiring T r unlabeled examples ( C
U  X  T r ), plus the cost of acquiring the associated N L labels ( C
L  X  N L ). For single labeling we have N L = T r , but for repeated-labeling N L &gt;T r .

We extend the setting of Section 4.2.1 slightly: repeated-labeling now acquires and labels new examples; single label-ing SL is unchanged. Repeated-labeling again is generalized round-robin: for each new example acquired, repeated-labeling acquires a fixed number of labels k , and in this case N L (In our experiments, k = 5.) Thus, for round-robin repeated-labeling, in these experiments the cost setting can be de-scribed compactly by the cost ratio  X  = C U C L , and in this case C D =  X   X  C L  X  T r + k  X  C L  X  T r , i.e., We examine two versions of repeated-labeling, repeated-labeling with majority voting ( MV ) and uncertainty-preserving repeated-labeling ( ME ), where we generate multiple examples with dif-ferent weights to preserve the uncertainty of the label multiset as described in Section 3.3 .

Performance of different labeling strategies : Figure 6 plots the generalization accuracy of the models as a function of data acquisition cost. Here  X  = 3, and we see very clearly that for p =0 . 6 both versions of repeated-labeling are preferable to single labeling. MV and ME outperform SL consistently (on all but waveform, where MV ties with SL ) and, interestingly, the comparative performance of repeated-labeling tends to increase as one spends more on labeling.

Figure 7 shows the effect of the cost ratio  X  , plotting the average improvement per unit cost of MV over SL as a function of  X  . Specifically, for each data set the vertical differences between the curves are averaged across all costs, and then these are averaged across all data sets. The figure shows that the general phenomenon illustrated in Figure 6 is not tied closely to the specific choice of  X  =3.

Furthermore, from the results in Figure 6 , we can see that the uncertainty-preserving repeated-labeling ME always per-forms at least as well as MV and in the majority of the cases ME outperforms MV . This is not apparent in all graphs, since Figure 6 only shows the beginning part of the learning curves for MV and ME (because for a given cost, SL uses up training examples quicker than MV and ME ). However, as the number of training examples increases further, then (for p =0 . 6) ME outperforms MV . For example, Figure 8 illustrates for the splice dataset, comparing the two techniques for a larger range of costs.

In other results (not shown) we see that when labeling qual-ity is substantially higher (e.g., p =0 . 8), repeated-labeling still is increasingly preferable to single labeling as  X  increases; how-ever, we no longer see an advantage for ME over MV . These results suggest that when labeler quality is low, inductive modeling often can benefit from the explicit representation Figure 6: Increase in model accuracy as a function of data acquisition cost for the 12 datasets; ( p =0 . 6 ,  X  =3 , k =5 ). SL is single labeling; MV is repeated-labeling with majority voting, and ME is uncertainty-preserving repeated-labeling. of the uncertainty incorporated in the multiset of labels for each example. When labeler quality is relatively higher, this additional information apparently is superfluous, and straight majority voting is sufficient.
The final questions this paper examines are (1) whether se-lective allocation of labeling resources can further improve per-formance, and (2) if so, how should the examples be selected. For example, intuitively it would seem better to augment the label multiset { + ,  X  , + } than to augment { + , + , + , + , +
The example above suggests a straightforward procedure for selective repeated-labeling: acquire additional labels for those examples where the current multiset of labels is impure. Two natural measures of purity are (i) the entropy of the multiset of labels, and (ii) how close the frequency of the majority label is to the decision threshold (here, 0 . 5). These two measures rank the examples the same. Unfortunately, there is a clear problem: under noise these measures do not really measure the uncertainty in the estimation of the class label. Figure 7: The average improvement per unit cost of repeated-labeling with majority voting ( MV )over single labeling ( SL ). Figure 8: The learning curves of MV and ME with p =0 . 6 ,  X  =3 , k =5 , using the splice dataset.
 For example, { + , + , + } is perfectly pure, but the true class is not certain (e.g., with p =0 . 6 one is not 95% confident of the true label). Applying a small-sample shrinkage correction (e.g., Laplace) to the probabilities is not sufficient. Figure 9 demonstrates how labeling quality increases as a function of assigned labels, using the (Laplace-corrected) entropy-based estimation of uncertainty (ENTROPY). For small amounts of repeated-labeling the technique does indeed select useful examples to label, but the fact that the estimates are not true estimates of uncertainty hurts the procedure in the long run X  X eneralized round-robin repeated-labeling (GRR) from Section 4.2 outperforms the entropy-based approach. This happens because most of the labeling resources are wasted, with the procedure labeling a small set of examples very many times. Note that with a high noise level, the long-run label mixture will be quite impure, even though the true class of the example may be quite certain (e.g., consider the case of 600 positive labels and 400 negative labels with p =0 . 6). More-pure, but incorrect, label multisets are never revisited.
For a given multiset of labels, we compute a Bayesian estimate of the uncertainty in the class of the example. Specif-ically, we would like to estimate our uncertainty that the true class y of the example is the majority class y m of the multiset. Consider a Bayesian estimation of the probability that y m is incorrect. Here we do not assume that we know (or have estimated well) the labeler quality, 6 and so we presume the prior distribution over the true label (quality) p ( y ) to be uni-form in the [0 , 1] interval. Thus, after observing L pos labels and L neg negative labels, the posterior probability p ( y ) follows a Beta distribution B ( L pos +1 ,L neg +1) [ 9 ].
We compute the level of uncertainty as the tail probability below the labeling decision threshold. Formally, the uncer-tainty is equal to the CDF at the decision threshold of the Beta distribution, which is given by the regularized incomplete beta In our case, the decision threshold is x =0 . 5, and  X  = Figure 9: What not to do: data quality improvement for an entropy-based selective repeated-labeling strat-egy vs. round-robin repeated-labeling. Figure 10: The data quality improvement of the four strategies ( GRR , LU , MU , and LMU ) for the wave-form dataset.
 L pos +1 , X  = L neg + 1. Thus, we set:
We compare selective repeated-labeling based on S LU to round-robin repeated-labeling (GRR), which we showed to per-form well in Section 4.2 . To compare repeated-labeling strate-gies, we followed the experimental procedure of Section 4.2 , with the following modification. Since we are asking whether label uncertainty can help with the selection of examples for which to obtain additional labels, each training example starts with three initial labels. Then, each repeated-labeling strategy iteratively selects examples for which it acquires additional labels (two at a time in these experiments).
 Comparing selective repeated-labeling using S LU (call that LU )to GRR , we observed similar patterns across all twelve data sets; therefore we only show the results for the wave-form dataset (Figure 10 ; ignore the MU and LMU lines for now, we discuss these techniques in the next section), which are representative. The results indicate that LU performs substantially better than GRR , identifying the examples for which repeated-labeling is more likely to improve quality.
A different perspective on the certainty of an example X  X  label can be borrowed from active learning. If a predictive Table 2: Average accuracies of the four strategies over the 12 datasets, for p =0 . 6 . For each dataset, the best performance is in boldface and the worst in italics. model has high confidence in the label of an example, perhaps we should expend our repeated-labeling resources elsewhere.
Of course, by ignoring the label set, MU has the comple-mentary problem to LU : even if the model is uncertain about a case, should we acquire more labels if the existing label multiset is very certain about the example X  X  class? The invest-ment in these labels would be wasted, since they would have a small effect on either the integrated labels or the learning.
Figure 10 demonstrates the improvement in data quality when using model information. We can observe that the LMU model strongly dominates all other strategies. In high-noise settings ( p =0 . 6) MU also performs well compared to GRR and LU , indicating that when noise is high, using learned models helps to focus the investment in improving quality. In settings with low noise ( p =0 . 8), LMU continues to dominate, but MU no longer outperforms LU and GRR .
So, finally, let us assess whether selective repeated-labeling accelerates learning (i.e., improves model generalization per-formance, in addition to data quality). Again, experiments are conducted as described above, except here we compute Figure 11: Accuracy as a function of the number of la-bels acquired for the four selective repeated-labeling strategies for the 12 datasets ( p =0 . 6 ). generalization accuracy averaged over the held-out test sets (as described in Section 4.1). The results (Figure 11)show that the improvements in data quality indeed do accelerate learning. (We report values for p =0 . 6, a high-noise setting that can occur in real-life training data. 7 ) Table 2 summarizes the results of the experiments, reporting accuracies averaged across the acquisition iterations for each data set, with the maximum accuracy across all the strategies highlighted in bold, the minimum accuracy italicized, and the grand aver-ages reported at the bottom of the columns.

The results are satisfying. The two methods that incorpo-rate label uncertainty ( LU and LMU ) are consistently better than round-robin repeated-labeling, achieving higher accu-racy for every data set. (Recall that in the previous section, round-robin repeated-labeling was shown to be substantially better than the baseline single labeling in this setting.) The performance of model uncertainty alone ( MU ), which can be viewed as the active learning baseline, is more variable: in three cases giving the best accuracy, but in other cases not even reaching the accuracy of round-robin repeated-labeling. Overall, combining label and model uncertainty ( LMU )is the best approach: in these experiments LMU always out-performs round-robin repeated-labeling, and as hypothesized, generally it is better than the strategies based on only one type of uncertainty (in each case, statistically significant by a one-tailed sign test at p&lt; 0 . 1 or better).
Repeated-labeling is a tool that should be considered when-ever labeling might be noisy, but can be repeated. We showed that under a wide range of conditions, it can improve both the quality of the labeled data directly, and the quality of the models learned from the data. In particular, selective repeated-labeling seems to be preferable, taking into account both labeling uncertainty and model uncertainty. Also, when quality is low, preserving the uncertainty in the label multisets for learning [ 25] can give considerable added value.
Our focus in this paper has been on improving data quality for supervised learning; however, the results have implica-tions for data mining generally. We showed that selective repeated-labeling improves the data quality directly and sub-stantially. Presumably, this could be helpful for many data mining applications.

This paper makes important assumptions that should be visited in future work, in order for us to understand practical repeated-labeling and realize its full benefits.
Despite these limitations, we hope that this study provides a solid foundation on which future work can build. Furthermore, we believe that both the analyses and the techniques intro-duced can have immediate, beneficial practical application. Thanks to Carla Brodley, John Langford, and Sanjoy Das-gupta for enlightening discussions and comments. This work was supported in part by the National Science Foundation under Grant No. IIS-0643846 and by an NSERC Postdoctoral Fellowship.

