 B. VENKATA SESHU KUMARI , JNTUH, Hyderabad RAMISETTY RAJESHWARA RAO , JNTU, Kakinada Dependency parsing is the task of uncovering the dependency tree of a sentence that consists of labeled links representing dependency relationships between words. Parsing is useful in major natural language processing applications like machine translation, word sense disambiguation, dialog systems, etc. This has led to the development of rule-based, statistical, and hybrid parsers. Due to the availability of annotated corpora in recent years, statistical parsing has achieved considerable success. The availability of a phrase structure treebank for English has seen the development of many efficient parsers.

Unlike English, many Indian languages (Hindi, Bangla, Telugu, etc.) are free-word-order and also morphologically rich. It has been suggested that free-word-order languages can be handled better using the dependency-based framework than the constituency-based one [Bharati et al. 1995]. Due to the availability of dependency treebanks, there have been several recent attempts at building dependency parsers. Two CoNLL shared tasks [Buchholz and Marsi 2006; Nivre et al. 2007a] were held aim-ing at building state-of-the-art dependency parsers for different languages. Recently, in two ICON Tools Contests [Husain 2009; Husain et al. 2010], rule-based, constraint-based, statistical, and hybrid approaches were explored towards building dependency parsers for three Indian languages, namely Telugu, Hindi, and Bangla. In all these efforts, state-of-the-art accuracies have been obtained by two data-driven parsers, namely the Malt parser [Nivre et al. 2007b] and the MST parser [McDonald 2006].
In this work, we show that Combinatory Categorical Grammar (CCG) [Steedman 2000] supertags help the MST parser in parsing Telugu. We first create a Telugu CCG treebank from the Telugu dependency treebank and build a supertagger. Ex-ploring different features and settings, we train the MST parser with a Telugu depen-dency treebank. We then provide CCG supertags as features to the MST parser. We get an improvement of 1.8% in the unlabelled attachment score and 2.2% in labelled attachment score. Our results show that CCG supertags improve the MST parser, especially on verbal arguments for which it has weak rates of recovery. CCG supertags, as they contain subcategorization information, help MST in better identification of verbal arguments.
 The article is arranged as follows. We give a brief introduction to the related work in Section 2. Section 3 describes combinatory categorial grammar and dependency gram-mar, two grammar formalisms explored in this article. In Sections 4 and 5, we describe the development of the CCG treebank and the supertagger. Section 6 discusses depen-dency parsing experiments and analyses the results. We conclude with possible future directions in Section 7. There have been some recent attempts at developing CCG treebanks automatically from the phrase structure and dependency treebanks. Hockenmaier and Steedman [2007] created the first CCG treebank for English from the phrase structure treebank. Following this work, Hockenmaier [2006] and Tse and Curran [2010] developed CCG treebanks for German and Chinese, respectively, from their phrase structure tree-banks. Cakici [2005] designed an algorithm to extract CCG lexicons from dependency treebanks. An Italian CCG treebank was created by Bos et al. [2009] from the Turin University Treebank (TUT), 1 an Italian dependency treebank. They first converted de-pendency trees into phrase structure trees and then applied an algorithm similar to that of Hockenmaier and Steedman [2007]. Ambati et al. [2013] improved the Malt parser trained on a Hindi dependency treebank by using CCG supertags. They first created a Hindi CCGbank from a Hindi dependency treebank. Following Cakici [2005], they first extracted a Hindi CCG lexicon from the Hindi dependency treebank. Using a CCG lexicon, dependency treebank, and a CCG-based chart parser, they developed a CCG treebank. They then built a CCG supertagger from this CCG treebank and pro-vided supertags as features to Malt. Overall improvements of 0.3% and 0.4% in the Unlabelled Attachment Score (UAS) and Labelled Attachment Score (LAS), respec-tively, were obtained. Combinatory Categorial Grammar (CCG) [Steedman 2000] is an efficiently parseable, yet linguistically expressive grammar formalism. CCG has a transparent interface between surface syntax and underlying semantic representation, including predicate argument structure, quantification, and information structure.
In CCG, each word is associated with syntactic categories that are either simple, such as NP , S , or complex, such as (S \ NP)/NP . Complex categories are functors that take arguments and give a result category once these arguments are resolved. For ex-ample, the CCG category for the transitive verb is (S \ NP)/NP . This category takes an object NP to the right and then a subject NP to the left to form a result category of sen-tence S . In CCG, constituents are combined using combinatory rules like application and composition. To handle special cases, type-raising and type-changing rules are also used. Dependency Grammar (DG) describes the syntactic structure of a sentence through dependency graphs. A dependency graph represents words and their relationship to syntactic modifiers using directed edges. These edges can be labelled with grammatical relations like Subject, Object, etc.

Dependency trees can either be projective or nonprojective. As English is a fixed-word-order language, most English sentences can be analyzed through projective trees. But, in free-word-order languages like Czech, Hindi, Telugu, etc., nonprojective depen-dencies are more frequent. Rich inflection systems reduce the demands on word order, leading to nonprojective dependencies [McDonald 2006].

CCG categories contain subcategorization information, whereas dependency graphs don X  X  have this information explicitly. Unlike CCG where a derivation is first required to extract word-word dependencies, DG captures dependencies between words directly. CCG derivations are constrained by the CCG rules used for combining categories. But, in DG, dependencies can occur between any words within a grammar. CCG captures long-distance dependencies elegantly, which can X  X  be done easily in DG [Kim et al. 2012]. Figure 1 gives a dependency tree and CCG derivation for an example Telugu sentence. In this section we first describe the morphological and syntactic features of the Telugu language. Then we describe the process of creating a Telugu CCG treebank from the dependency treebank. This process is developed on top of Cakici X  X  [2005] and Ambati et al. X  X  [2013] work.
 Telugu is one of the official languages of India and the 13th largest language in the world, with over 74 million speakers. 2 It is a morphologically rich and free-word-order language. It is also an agglutinative language where morphological information is available as a suffix to the word, rather than as a separate lexical item. Figure 2 shows different Telugu sentences describing the free-word-order nature and morphological richness of Telugu. Sentence 1 is a simple past-tense sentence  X  X am ate a fruit X . Suffix  X  X u X  of the verb  X  X innadu X  (ate) is a masculine marker. In the 2nd sentence, as the gen-der of the subject changed to feminine (Sita), the suffix of the verb changed to  X  X hi X , which is a feminine marker. The suffix of a verb can change when the tense changes. For example, in the 3rd sentence as the tense changed to present continuous, the suffix of the verb changed accordingly to  X  X unnaadu X . Similar to verbs where the suffix de-pends on different factors like tense, aspect, modality, and subject X  X  gender, the suffix of the nouns represents case or preposition. For example, in sentence 4,  X  X am gave a fruit to Sita in the school X , suffix  X  X i X  of  X  X ithaki X  (to Sita) is the Dative case marker and suffix  X  X o X  of  X  X atashalalo X  (in School) is the marker for preposition  X  X n X . Sentence 5 gives an example of the free-word-order nature in Telugu language. Though subject-object-verb is the preferred word order, different word orders are possible in Telugu as in sentence 5, with object-subject-verb order. The Telugu dependency treebank released in the ICON 2010 Tools Contest [Husain et al. 2010] is used in our work. The data are annotated using part-of-speech (POS) tag-ging, chunking, and dependency annotation guidelines [Bharati et al. 2006, 2009]. The treebank consists of morphological information (root, coarse pos-tag, gender, number, person, case marking, suffix, and TAM (Tense, Aspect, and Modality marker)), POS, chunk, and dependency information. The dependency annotation follows a scheme that can be traced back to Paninian grammar [Bharati et al. 2009], known well suited to modern Indian languages. The dependency labels are syntactico-semantic in nature [Bharati et al. 1995, 2009]. For example,  X  k1  X  usually corresponds to the subject syn-tactic role and agent semantic role. Similarly,  X  k2  X  corresponds to the syntactic role of the object and the semantic role of patient. For purposes of readability, instead of the original treebank dependency labels, their corresponding English labels are used in this article ( SUBJ, OBJ, DEM for k1 , k2 , nmod adj , respectively).

The treebank is available in SSF [Bharati et al. 2007] and CoNLL with the CoNLL format in this article. In this format, word, root, pos-tag, chunk tag, and morphological features are available in the WORD , LEMMA FEATS columns, respectively. Following Ambati et al. [2013], we first made a list of argument and adjunct depen-dency labels in the treebank. Labels like SUBJ (subject) and OBJ (object) are consid-ered as arguments, whereas labels like DEM (determiners), ADJ (adjectives), TIME (time expression), and PLACE (place expression) are marked as adjuncts. As the an-notation scheme and tagset used for the Telugu dependency treebank are similar to Hindi, we first used Ambati et al. X  X  [2013] lists for arguments and adjuncts. Then, to incorporate Telugu-language-specific details, we made a list of special cases based on the Telugu verb frames [Kesidi et al. 2010]. For example, we made a list of motion verbs in Telugu and added a rule in special cases that says  X  X f verb is a motion verb then treat PLACE as argument X . So, whenever we find a motion verb in the data, we treat PLACE as an argument and create the CCG lexicon accordingly.
 We traverse from sentence root through each node in the dependency tree. A node X  X  CCG category is derived from both the parent and children information. We first get the information from the parent. If the node is an argument of its parent, the node X  X  chunk tag is assigned as its CCG category; otherwise, a category similar to X|X is assigned. Here X is the parent X  X  result category and | is directionality ( is decided by the position of the node with respect to its parent. The result category of a node is the category obtained once its arguments are resolved. If  X  (S the category of a node, then its result category is  X  S  X . Hence, the result category of any node can be obtained from its category. Once we have the parent information, we traverse through its children. If a child is an argument then the child X  X  chunk tag with appropriate directionality is added to the node X  X  category. Algorithm 1 sketches this process. A dependency tree and CCG derivation for a simple sentence extracted using this algorithm are shown in Figure 1.
 The CCG category for adjuncts that modify the sentence root is  X  S/S  X . Adjuncts can modify other adjuncts, leading to complex categories like  X  (((S/S)/(S/S))/((S/S)/(S/S)))  X . Taking advantage of CCG X  X  composition rule, we assigned  X  S/S  X  as CCG category for all the adjacent adjuncts. They can be com-posed to a single  X  S/S  X  at the end, eliminating many gigantic adjunct categories (following Cakici [2005]). Using this approach, we can obtain a CCG lexicon from any dependency treebank, provided we have the list of arguments. All the language-specific features can be handled as special cases. Similar approaches have been applied to Turkish and Hindi, which are syntactically different languages. This shows that minimal effort is required to extract a CCG lexicon from any dependency treebank. We use a CCG-based chart parser 4 to convert the CCG lexicon into a CCG treebank, fol-lowing Ambati et al. [2013]. The parser gives multiple derivations for some sentences that are ranked based on two criteria. The first criterion is recovery of the gold depen-dency tree, and the second criterion is the type of rule applied. We rank rules in the following order: Application &gt; Composition &gt; Crossed composition Substitution. Extracting the best derivation for each sentence, we create a CCG tree-bank. We could extract CCG derivations for 99% of the sentences in the dependency treebank. Following Clark and Curran [2004] and Ambati et al. [2013], we developed a CCG su-pertagger based on the maximum entropy approach. We used OpenNLP MaxEnt our experiments. Different features are explored on the development data and those features that gave best results on development data are used for test data. We explored WORD ( w ), LEMMA ( l ), POS ( p ), CPOS ( c ), and the FEATS mat, along with PREFIX ( pre )and SUFFIX ( suf ) features. Table I shows the impact of different features on supertagger performance. In Experiment 1, only the current word (w i ) and pos are provided, giving an accuracy of 56.3%. Adding lemma, cpos (Experiment 2) and feats (Experiment 3) improved the performance by 1.2% and 1.5%, respectively. In Experiment 4, word and pos features in a five-word window are ex-plored, giving an improvement of 2.0%. Experiments 5 and 6 explore different bi-gram features, leading to a further improvement of 2.1% in accuracy. In Experiment 7, pre-fixes and suffixes up to length three are used. These features boosted the performance by 5.4%. As Telugu is an agglutinative language and as the data is very low, prefix and suffix features helped a lot in handling sparsity issues. Tuning the MaxEnt parameters gave a further 2.3% improvement. Final accuracy of the supertagger is 70.8%. We explore different features and settings of MST and develop the best MST model for Telugu. We treat this as our baseline and then add supertags as features. We compare and analyze the results of both these models. The MST parser is a freely available software with a graph-based parsing system de-scribed in McDonald [2006]. The MST parser uses Eisner X  X  algorithm for parsing pro-jective trees and the Chu-Liu-Edmonds maximum spanning tree algorithm for parsing nonprojective trees. Online large margin learning is used for learning [McDonald et al. 2005]. It provides options for first-order and 2nd-order features and a training-k value for using k-best parsers during training. We also modified MST parser code and pro-vided FEATS column-based features for labelling. A nonprojective parsing algorithm with 1st-order features and training-k=5 gave the best results, giving an Unlabelled Attachment Score (UAS) of 90.0% and Labelled Attachment Score (LAS) of 67.1%. We treat this as our baseline. We have explored different ways of incorporating CCG supertags from the supertagger into dependency parsing. Following C  X  ak X c X  [2009] and Ambati et al. [2013], instead of using supertags for all words, we used supertags that occurred at least K times in the training data, and backed off to coarse POS-tags otherwise. We experimented with dif-ferent values of K and found K=20 gave the best results. We also experimented with removing the directionality by changing \ or / to | , however, this experiment didn X  X  im-prove the performance. We obtained final best results of 91.8% UAS and 69.3% LAS by providing CCG supertags from the supertagger as features. Table II shows the results. CCG supertags gave an improvement of 1.8% in UAS and 2.2% in LAS, and all the improvements are statistically significant (McNemar X  X  test, p Table III provides the F-scores of the top five dependency labels for Telugu, which form 65% of the total dependencies, showing the impact of CCG supertags. The F-score of ROOT (root of the sentence) improved by 2.0% after adding CCG supertags, while the performance of SUBJ (subject) and OBJ (object) improved by 1.8% and 3.6%, respectively. Similarly, for VMOD (verb modifier) and COORD (coordination) labels, there were improvements of 0.8% and 6.1%, respectively. Since the preferred word order in Telugu is subject-object-verb, most common CCG supertags for intransitive and transitive verbs are S \ NP and (S \ NP) \ NP , respectively. Providing this information helped in the correct identification of subject and object dependency relations. The CCG supertag of a conjunction is (X \ X)/X , where a conjunction looks for a child to its right and then a child to its left. In case of noun phrase coordination, X=NP and in case of sentential coordination, X=S . In a Telugu dependency treebank, conjunction is the head of the conjuncts, so in the case of sentential coordination, conjunction is the root of the sentence. Hence, providing a CCG supertag for conjunction not only helped in identifying the correct dependency label for its children (COORD), but also in correct identification of the root of the sentence (ROOT dependency label).
CCG supertags contain subcategorization information that is very useful in depen-dency parsing [Zhang and Nivre 2011; Zhang et al. 2013]. Ambati et al. [2013] showed that the subcategorization information in CCG supertags helped in improving the ac-curacy of Malt on long-distance dependencies, for which it is known to have weak rates of recovery. They observed greater improvements for ROOT, COORD, and RELC, the labels for sentence root, coordination, and relative clause, respectively, that take the major share of long-distance dependencies in Hindi. We observed similar improve-ments for Telugu in our experiments with the MST parser. MST is known to be weak at verbal arguments [Ambati et al. 2010; McDonald and Nivre 2007]. Providing CCG supertags as features improved the performance of MST, especially in the recovery of verbal arguments. We showed that CCG supertags which contain subcategorization information help the MST parser in better recovery of verbal arguments for Telugu. We first extracted the CCG lexicon from the dependency treebank. Using both this lexicon and the depen-dency treebank, we created a CCG treebank using a chart parser. We developed a CCG supertagger and provided the supertags as features to the MST parser, achieving an improvement of 1.8% in the unlabelled attachment score and 2.2% in labelled attach-ment score.

In future work, we would like to explore the usefulness of CCG supertags for other languages, especially Indian languages like Bangla for which dependency treebanks are available. We would also like to develop a CCG parser for Telugu using the CCG treebank. Finally, we would like to explore the chart parser of Clark and Curran [2007] and the shift-reduce parser of Zhang and Clark [2011].

