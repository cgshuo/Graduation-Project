 We address a specific enterprise document search scenario, where the information need is expressed in an elaborate manner. In our scenario, information needs are expressed using a short query (of a few keywords) together with examples of key reference pages. Given this setup, we investigate how the examples can be utilized to improve the end-to-end performance on the document retrieval task. Our approach is based on a language modeling framework, where the query model is modified to resemble the example pages. We compare several methods for sampling expansion terms from the example pages to support query-dependent and query-indepen-dent query expansion; the latter is motivated by the wish to increase  X  X spect recall, X  and attempts to uncover aspects of the information need not captured by the query.

For evaluation purposes we use the CSIRO data set created for the TREC 2007 Enterprise track. The best performance is achieved by query models based on query-independent sampling of expan-sion terms from the example documents.
 H.3 [ Information Storage and Retrieval ]: H.3.1 Content Anal-ysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; H.4.m Miscellaneous Algorithms, Measurement, Performance, Experimentation Enterprise search, query modeling, query expansion, language mod-els
Query modeling has been a topic of active research for many years. One popular way of enriching the user X  X  (usually sparse) query, and thus obtaining a more detailed specification of the un-derlying information need is through query expansion, by selecting terms from documents that are known, believed or assumed to be relevant. In the absence of explicit user feedback, the canonical ap-proach is to treat the top-ranked documents retrieved in response to a query as if they had been marked relevant by the user.
Our work takes place in an enterprise setting, where users are more willing than, say, average web search engine users, to express their information need in a more elaborate form than by means of a few key words. In our scenario users have to create overview pages of the information available within the enterprise on a given topic, and the search engine should help them identify key refer-ences , pages on the intranet of the enterprise that should be linked to by a good overview page. The additional information that our users provide consists of a small number of example key references. We refer to those documents as sample documents .

An important research goal in this paper is to devise a way of us-ing these rich specifications of the user X  X  information need, consist-ing of a query and sample documents, in a theoretically transparent manner. We address this goal while working within the generative language modeling (LM) approach to retrieval. Here, one usually assumes that the relevance of a document is correlated with the like-lihood of the query [9, 16, 17], builds a language model from each document, and ranks documents based on the probability of the document model generating the query. Feedback documents are as-sumed to be relevant, which often entails that the generation prob-abilities are (re-)estimated (using the feedback documents). The implicit nature of relevance within the LM framework has attracted some criticism; see, e.g., [23]. This criticism has been addressed in various proposals, including ones that consider not only docu-ment models, but also a language model based on the request, i.e., a query model [14], relevance models [15], and parsimonious lan-guage models [10].

In this paper we use sample documents to explicitly model rele-vance and an important goal for us is to develop methods for accu-rately estimating sampling probabilities. We assume that the query, sample documents and relevant documents are all coming from an unknown relevance model R . Lavrenko and Croft [15] used two methods to build a relevance model  X  R , where P ( t |  X  R tive frequency with which we expect to see term t during repeated independent random sampling of words from all of the relevant documents (see Section 6.1 below). Both approaches assume con-ditional dependence between the query and the terms t selected for expansion. While this dependence assumption may be appropri-ate in some cases (especially if the query is the only expression of the information need that we have), we want to be able to lift it. The reason for this is as follows.  X  X spect recall X  is an important cause of failure of current IR systems [5], one that tends to be ex-acerbated by today X  X  query expansion approaches: key aspects of the user X  X  information need may be completely missing from the pool of top-ranked documents, as this pool is usually query-biased and (to keep precision reasonable) often small, and, hence, tends to only reflect aspects covered by the original query itself [12]. In a scenario such as ours, where a user provides a query plus sample documents, we expect the sample documents to provide important aspects not covered by the query. Hence, we want to avoid biasing the expansion term selection toward the query and thereby possibly loosing important aspects.

Our main contribution is a theoretically justified model for es-timating a relevance model when training material (in the form of sample documents) is available, a model that is fully general in that we can sample expansion terms either independent of, or de-pendent on, the query. Our model has two main components, one for estimating (expansion) term importance, and one for estimating the importance of the documents from which expansion terms are selected X  X e consider various instantiations of these components, including ones where document importance estimations are done in a query independent manner, based on sample documents.
We use data provided by the TREC 2007 Enterprise track to evaluate our models. We compare them against standard blind rel-evance feedback approaches (where expansion terms are selected from a query-biased set of documents) and against relevance mod-els based on the sample documents. Assuming independence be-tween query and sampling terms leads to expanded queries that outperform a high performing baseline with no query expansion, as well systems that perform standard query expansion. Unbi-ased query expansion improves  X  X spect recall X  by bringing in more  X  X are X  relevant documents, that are not identified by the standard (query-biased) expansion methods that we consider.

The remainder of the paper is organized as follows. In Section 2 we discuss related work. In Section 3 we detail our retrieval ap-proach and describe our take on query modeling. In Section 4 we describe our experimental setup and in Section 5 we establish a baseline, using the sample documents to maximize query log like-lihood. Then, in Section 6 we detail several query models, which we evaluate in Section 7. We follow with an analysis in Section 8 and a conclusion in Section 9.
Query modeling, i.e., transformations of simple keyword queries into more detailed representations of the user X  X  information need (e.g., by assigning (different) weights to terms, expanding the query, or using phrases), is often used to bridge the vocabulary gap be-tween query and document collection. Many expansion techniques have been proposed, and they mostly fall into two categories, i.e., global and local. The idea of global analysis is to expand the query using global collection statistics based, for instance, on a co-occurrence analysis of the entire collection. Thesaurus-and dictionary-based expansion as, e.g., in [18], also provide examples of the global approach.

We focus on local approaches to query expansion, that use the top retrieved documents as examples from which to select terms to improve the retrieval performance [19]. In the setting of language modeling approaches to query expansion, the local analysis idea has been instantiated by estimating query language models [13, 24] or relevance models [15] from a set of feedback documents. Yan and Hauptmann [25] explore query expansion in the setting of mul-timedia retrieval. Our work goes beyond this work by dropping the assumption that query and expansion terms are dependent.  X  X spect recall X  has been identified in [5, 8]. Kurland et al. [12] provide an iterative  X  X seudo-query X  generation technique to un-cover aspects of a query, using cluster-based language models.
At the TREC 2007 Enterprise track, several teams experimented with the use of sample documents for the document search task, using a language modeling setting [4, 11, 21] or using ideas remi-niscent of resource selection [6], or using the document structure in various ways [1, 7]. Some groups experiment with the use of sam-ple documents, but the difference between the best performance with sample documents and the best performance without sample documents was modest [2].
In this section we derive our ranking mechanism. We bring query-likelihood LM approaches and relevance models to a com-mon ground, and show that both lead to the same scoring function, although the theoretical motivation behind them is different.
In case of the query likelihood (also referred as standard LM) ap-proach, documents are ranked according to the likelihood of them being relevant given the query P ( D | Q ) . Instead of calculating this probability directly, we apply Bayes X  rule and rewrite it to The probability of the query P ( Q ) can be ignored for the purpose of ranking documents, which leaves us with Assuming that query terms are independent from each other, we estimate P ( Q | D ) by taking the product across terms in the query. Substituting this into Eq. 2 we obtain Here, n ( t,Q ) is the number of times term t is present in the query Q . This is the multinomial view of the document model, i.e., the query Q is treated as a sequence of independent terms [9, 16, 22].
To prevent numerical underflows, we perform this computation in the log domain (thus compute the log-likelihood of the document being relevant to the query) and rewrite our equation as log P ( D | Q )  X  log P ( D ) + P t  X  Q n ( t,Q )  X  log P ( t | D ) . (4) Next, we generalize n ( t,Q ) so that it can take not only integer but real values. This will allow more flexible weighting of query terms. We replace n ( t,Q ) with P ( t |  X  Q ) , which can be interpreted as the weight of term t in query Q . We will refer to  X  Q as query model . We generalize P ( t | D ) to a document model , P ( t |  X  at our final formula for ranking documents: log P ( D | Q )  X  log P ( D ) + P t  X  Q P ( t |  X  Q )  X  log P ( t |  X  Two important components remain to be defined, the query model and the document model. Before doing so, we point out a relation between our ranking formula in Eq. 5 and relevance models.
For relevance language modeling one assumes that for every in-formation need there exists an underlying relevance model R , and the query and documents are random samples from R , see Fig-ure 1. We view documents and queries as samples from R , how-ever, the two sampling processes do not have to be the same (i.e., P ( t | R ) does not need to be the same as P ( t | Q ) or P ( t | D ) , where D is a relevant document). The query model  X  Q is to be viewed as an approximation of R . Estimating P ( t |  X  Q ) in a typical retrieval setting is problematic because we have no training data. (Below, however, we will use the sample documents for this purpose, see Section 6.2.) Documents and queries are represented by a multi-nomial probability distribution over the vocabulary of terms. Doc-uments are ranked based on their similarity to the query model. Figure 1: The query and relevant documents are random sam-ples from an underlying relevance model R .
 The Kullback-Leibler divergence between the query and document models can then be used to provide a ranking of documents: D (  X  Q ||  X  D ) =  X  P t P ( t |  X  Q )  X  log P ( t |  X  D ) + cons ( Q ) . (6) The document-independent constant cons ( Q ) (the entropy of the query model) can be dropped, because it does not affect the ranking of documents; see [14, 26]. If we assume a uniform prior in Eq. 5, maximizing the query log-likelihood in Eq. 5 provides the same document ranking as minimizing the KL-divergence (Eq. 6). The move from P ( t | D ) to the document model P ( t |  X  and 5 is motivated by sparseness issues. To be able to rank docu-ments using Eq. 4, we need to estimate P ( t | D ) , the probability that t would be observed during repeated random sampling from the document model. The maximum likelihood (ML) estimate of a term provides the simplest method for inferring an empirical doc-ument model: P ( t | D ) = n ( t,D ) / P t 0 n ( t 0 ,D ) . If one or more query terms do not appear in the document, it will be assigned a zero probability (Eq. 3).

Nonetheless, creating a document model  X  D can resolve the zero probability problem, by smoothing the ML estimate such that for every term t , P ( t |  X  D ) &gt; 0 . The document model is built up from a linear combination of the empirical estimate, P ( t | D ) , and the max-imum likelihood estimate of the term, given the collection model P ( t | C ) , using the coefficient  X  to control the influence of each: We discuss the problem of estimating the smoothing parameter  X   X  and exploit sample documents for this purpose X  X n Section 5.
As to the query model, we consider several flavors. Our baseline query model consists of terms from the topic title only, and assigns the probability mass uniformly across these terms: As before, n ( t,Q ) is the frequency of term t in Q .

The baseline query model has two potential issues. Not all query terms are equally important, hence, we may want to reweigh some of the original query terms. Also, P ( t | Q ) is extremely sparse, and, hence, we may want to add new terms (so that P ( t |  X  Q ) amounts to query expansion), and for this purpose we will again use the sample documents; see Sections 6.2 and 6.3.

Much of the paper is devoted to investigating ways of construct-ing the query model  X  Q that approximates the true relevance model R accurately. In [15] two methods are presented that estimate rele-vance models by constructing topic models from the topic title only without training data; in this paper, we examine theoretically justi-fied ways of estimating the relevance model when training data (in the form of sample documents) is available.
We addressed the following research questions: Can sample doc-uments be used to estimate the amount of smoothing applied? How does using sample documents compare to blind relevance feed-back? Expansion terms in the case of standard blind relevance feedback are dependent on the original query. How does lifting this assumption affect retrieval performance? To address our re-search questions we ran experiments using the CSIRO Enterprise Research Collection (CERC), a crawl of *.csiro.au (public) web sites conducted in March 2007. The crawl has 370,715 documents, with a total size 4.2 gigabytes [3].

In the 2007 edition of the TREC Enterprise track, CERC was used as the document collection [2]. CSIRO X  X  science communica-tors played an important role in topic creation. They, the envisaged end-users of systems taking part in the TREC Enterprise track, read and create outward-facing web pages of CSIRO to enhance the or-ganization X  X  public image and promote its expertise. A total of 50 topics were created by the science communicators; systems had to return  X  X ey references X  for these topics, i.e., pages that should be linked to by a good overview page.

Assessment was done by the TREC 2007 Enterprise track par-ticipants. Judgments were made on a three-point scale: 2: highly likely to be a  X  X ey reference; X  1: a candidate key page, or otherwise informative to help build an overview page, but not highly likely; 0: not a  X  X ey reference, X  because, e.g., not relevant, off-topic, not an important page on the topic, on-topic but out-of-date, not the right kind of navigation point, or too informal or too narrow an audience. All non-judged documents are considered as irrelevant. For our experiments we used the official qrels released after TREC 2007, consisting of 50 topics, but with the sample documents re-moved from the runs and from the set of relevant documents.
We scored our retrieval output both using the possibly relevant and the highly relevant levels, using mean average precision and mean reciprocal rank.
In order to establish a baseline, we need a reasonable estimate of  X  (in Eq 7) for those documents that are likely to be relevant to a given query, since they are the ones we are interested in.
When training data (in the form of topics and corresponding rel-evance judgments) is available, we can estimate  X  empirically on a set of training topics, and then apply this value on the test set. This way the same amount of smoothing is applied for all queries. When such a set of training topics is not available, one approach is to use automatic relevance feedback [20]. We perform an initial guess for  X  (e.g.,  X  = 0 . 5 ) and assume that the top M retrieved documents are relevant to the query. These M top-scoring documents then become the set we use to estimate  X  .

Given our setting (with sample documents available), we will view these as documents relevant to a given query, and use them for learning settings for the  X  parameter. However, instead of esti-mating a uniform  X  , we estimate a query-dependent  X  Q . Below, we present two unsupervised methods that can accurately estimate this value, and deliver the same performance as the empirical estimate.
Recall that sample documents (which we use for estimation) are removed from the runs and from the relevance judgments; in partic-ular, they are not part of the test data that we use in the estimation
In our first technique for estimating  X  Q (called MAX_AP ) we view the sample documents as if they were the only relevant docu-ments given the query. The process for each query Q is as follows: 1. For each  X  Q  X  (0 , 1) (with steps  X  ); 2. Run retrieval using the parameter  X  Q ; 3. Calculate the average precision (AP) of the sam-ple documents; 4. Select  X  Q that maximizes AP. Formally:
Our second technique sets for estimating  X  Q (called MAX_QLL ) sets  X  Q to the value that maximizes the log-likelihood of the query Q , given a set of sample documents S :  X 
Q = arg max
In order to evaluate the two approximation methods presented above, we first perform an empirical exploration of a query-in-dependent smoothing parameter  X  . That is, we iterate over pos-sible  X  values in steps of  X  = 0 . 01 and calculate the mean average precision (MAP) on the entire set of topics: We refer to this value as the best empirical estimate ( EMP_BEST ). Figure 2 displays the results, using both possibly and highly rel-evant assessments. There is a broad range of settings where per-formance levels close to the maximum are achieved; the maximum AP scores are reached around  X  = 0 . 6 , with a substantial drop in performance for  X   X  0 . 8 .

Next, we use  X  = 0 . 6 and compare our approximation methods against this baseline; see Table 1. Our estimation methods for  X  are effective in estimating  X  . MAX_QLL performed slightly better than MAX_AP , but the differences are not significant. 2
We have fixed our baseline retrieval approach. We set the smooth-ing parameter using an estimation method that exploits sample doc-uments ( MAP_QLL ). Although this method uses only a handful of sample documents per query (3.6 on average), the performance is as good as that of the empirical best; moreover, it can be computed more efficiently. For the remainder of the paper, this (with smooth-ing determined using MAP_QLL ) serves as our baseline.
We now consider different ways of representing the query. For comparison purposes, we first consider standard blind relevance process in this section.
We use the two-tailed paired T-test for significance testing. We consider differences with p &lt; 0 . 01 significant. Figure 2: Effect of smoothing; MAP plotted against the weight (  X  ) of the collection model; results on two relevance levels. feedback using relevance models as defined in [15]. Next, we use the same methods but instead of selecting expansion terms from the top ranked documents in an initial retrieval run, we select them from the sample documents. These expansion methods both as-sume that expansion terms are dependent on the query; after that, we provide a model according to which we can sample terms from the sample documents both independent of and dependent on the original query. The output of these methods is an expanded query model  X  Q .

Next, we combine the selected query terms with the terms from the original query; this is also done in the original query expansion papers (see, e.g., [19]) and in query modeling methods based on language models (see, e.g., [12]) and prevents the topic to shift (too far) away from the original user information need. We use Eq. 12 to mix the original query with the expanded query.
 where P ( t | Q ) and P ( t |  X  Q ) are the probability of term t given the original query Q (see Eq. 8) and the expanded query  X  Q , respec-tively. The expanded query models  X  Q are evaluated in Section 7.1, and their combinations with the original query (by performing an empirical exploration of  X  ), are presented in Section 7.2.
One way of expanding the original query is by using blind rele-vance feedback: assume the top M documents to be relevant given a query. From these documents we sample terms that are used to form the expanded query model  X  Q . Lavrenko and Croft [15] sug-gest a reasonable way of obtaining  X  Q , by assuming that P ( t | be approximated by the probability of term t given the (original) query Q . We can then estimate P ( t |  X  Q ) using the joint probability of observing t together with the query terms q 1 ,...,q k dividing by the joint probability of the query terms: In order to estimate the joint probability P ( t,q 1 ,...,q and Croft [15] propose two methods; they differ in the indepen-dence assumptions that are being made: RM1 It is assumed that t and q i are sampled independently and RM2 The second method tackles a different sampling strategy, and RM1 can be viewed as sampling of all query terms conditioned on t : a strong mutual independence assumption, compared to the pair-wise independence assumptions made by RM2. Empirical evalua-tions reported in [15] found that RM2 is more robust, and performs slightly better that RM1. Our experiments below confirm this.
Next, we follow the approach of the previous section and apply relevance models to the sample documents. Instead of performing an initial retrieval run to obtain a set of feedback documents, we use the sample documents and observe the co-occurrence of term t with query terms q 1 , . . . , q k in the sample documents. I.e., we set M = S . For RM1, we also need to make an extra assumption, viz. that all sample documents are equally important: P ( D ) = 1 / | S | .
Now we introduce a new model based on sampling from doc-uments that are assumed to be relevant. Unlike with the methods considered above, the sampling can be done both independent of, and dependent on, the original query. Our approach to constructing the expanded query  X  Q is the following. First, we estimate a  X  X am-pling distribution" P ( t | S ) using sample documents D  X  S . Next, the top K terms with highest probability P ( t | S ) are taken and used to formulate the expanded query  X  Q : Calculating the sampling distribution P ( t | S ) can be viewed as the following generative process: 1. Let the set of sample documents S be given; 2. Select a document D from this set S with probabil-ity P ( D | S ) ; and 3. From this document, generate the term t with probability P ( t | D ) . By summing over all sample documents, we obtain P ( t | S ) . Formally, this can be expressed as For estimating the term importance, P ( t | D ) , we consider three nat-ural options: The probability P ( D | S ) expresses the importance of sample doc-ument D given the samples S . I.e., this is a weight that determines how much a term t  X  D will contribute to the sampling distribution P ( t | S ) . We consider three options for estimating P ( D | S ) :
We start by evaluating the relevance models using blind feed-back (Section 6.1). We explore the number of feedback documents that need to be taken into account (note that the number of terms extracted is K = 10 ). In Figure 3 (Left) the performance of query expansion using BFB-RM1 and BFB-RM2 on different numbers of feedback documents ( | M | ) is shown. A smaller number of feed-back documents gives better performance on MAP for both models; best performance is achieved with only 5 feedback documents.
Next, we construct relevance models on the sample documents using relevance models (Section 6.2). EX-RM2 fails on two topics (1 and 11), while topic 45 does not have any sample documents. The influence of the number of selected terms K on retrieval per-formance for EX-RM1 and EX-RM2 is displayed in Figure 3 (Cen-ter). The best performance is achieved when selecting 15 terms for EX-RM1 and 25 for EX-RM2.

Finally, we explore the number of selected terms K for our query models generated from sample documents (Section 6.3). Results are displayed in Figure 3 (Right).
 Table 2 records our baseline performance (which is similar to the median achieved at TREC 2007) and summarizes the results for model K MAP MRR MAP MRR baseline .3576 .7134 .3143 .6326 BFB-RM1 10 .3145 .6326 .2679 .5335 BFB-RM2 10 .3382 .6683 .2845 .5609 EX-RM1 15 .3193 .8794 .2813 .7695 EX-RM2 25 .3454 .8596 .3111 .8169 EX-QM-ML 30 .3280 .8508 .2789 .7093 EX-QM-SM 40 .3163 .8050 .2822 .7133 EX-QM-EXP 5 .2263 .6131 .2062 .5854 the expanded query model  X  Q , together with the number K of feed-back terms used. The query models based on query-dependent sam-pling of expansion terms (BFB and EX) perform closer to the base-line than those based on query-independent sampling (in terms of MAP). EX-QM-ML and EX-QM-SM are able to add more terms without hurting performance than EX-RM1 and EX-RM2, thereby allowing more aspects to be retrieved. Next, we combine the expanded query  X  Q and the original query Q , where the parameter  X  controls the weight of the original query (see Eq. 12). We perform a sweep on  X  to determine the optimal mixture weight of the original query. The results are in Figure 4. model  X  MAP MRR MAP MRR baseline .3576 .7134 .3143 .6326 BFB-RM1 0.6 .3677 .6703 .3171 .5772 BFB-RM2 0.6 .3797 .6905 .3296 .6033 EX-RM1 0.4 .4264* .8808* .3758* .8259* EX-RM2 0.4 .4273* .9029* .3833* .8473* EX-QM-ML 0.5 .4449* .8533* .3951* .7911* EX-QM-SM 0.5 .4406* .8771* .3955* .8035* EX-QM-EXP 0.7 .4016* .8148 .3520 .7603* Table 3: Performance of the baseline run, relevance models on blind feedback documents and sample documents, and query models on sample documents using optimal K and  X  settings for each model. Results marked with * are significantly differ-ent from the baseline.
 The best results together with the optimal  X  values are listed in Table 3. Here we see two of the query models based on query-independent sampling outperforming all other query models (in terms of (possibly) relevant MAP), although the differences be-tween the best relevance model (EX-RM2) and our best query model (EX-QM-ML) are not significant.
Finally, we evaluate the three options we considered for esti-mating the importance of a sample document ( P ( D | S ) ); see Sec-tion 6.3. Table 4 lists the results. Non-uniform document im-portance settings tend to hurt MAP performance, for two of the three flavors of term importance estimations (ML, SM); the query-biased setting has an early precision enhancing effect, boosting MRR scores for all term importance estimations methods. 3
So far, we have looked at results at an aggregate level. Next, we continue the comparison by looking at the topic-level perfor-mance. Figure 5 presents the difference in average precision of the best performing query generation methods (BFB-RM2, EX-RM2, and EX-QM-ML) against the baseline. Most topics gain from the query models, although there are always some topics that are hurt. Clearly, EX-RM2 and EX-QM-ML have bigger gains than BFB-RM2; possibly relevant and highly relevant assessments yield sim-ilar patterns.

Next, we zoom in on two example topics, where these methods display interesting behavior. The first example concerns the topic
Only the difference between the P ( D | Q ) and 1  X  P ( D | Q ) ver-sions of EX-QM-SM is significant. Figure 5: AP differences between baseline and (Top): BFB-RM2, (Middle): EX-RM2, (Bottom): EX-QM-ML, on (Left): possibly, and (Right): highly relevant. machine vision ; Table 5 reports the MAP scores, and Table 6 dis-plays the top 10 terms for the query models constructed for the topic machine vision , with EX-QM-ML and EX-RM2 performing much better than BFB-RM2. EX-QM-ML is mostly on target (with a shift to surveillance and security), while the other two models dis-play a shift to a far broader topical area. The next example, termites , shows a different behavior, with BFB-RM2 beating EX-QM-ML, which in turn beats EX-RM2. Table 7 reports the MAP scores, and Table 8 displays the top 10 terms for query models constructed for this topic. We see topic drift for EX-RM2 and EX-QM-ML, but many on target terms for BFB-RM2.
Interestingly, when we compare two document importance esti-mation methods (query-biased and inverse query-biased) and two P ( t |  X  Q ) t P ( t |  X  Q ) t P ( t |  X  Q ) t 0.4123 vision 0.2707 vision 0.2796 vision 0.3935 machine 0.2641 machine 0.2762 machine 0.0336 csiro 0.0735 csiro 0.0513 csiro 0.0303 image 0.0267 projects 0.0248 image 0.0302 toolbox 0.0256 high 0.0224 vehicles 0.0227 robot 0.0245 research 0.0220 safe 0.0221 information 0.0239 systems 0.0214 cam 0.0204 control 0.0223 development 0.0178 traffic 0.0202 visual 0.0204 computing 0.0176 technology 0.0147 object 0.0191 performance 0.0173 camera Table 6: Query models for topic # 32  X  X achine vision". (Left) BFB-RM2; (Center) EX-RM2; (Right) EX-QM-ML. term selection methods (EX-QM-SM and EX-QM-ML), we see a mostly balanced picture; see Figure 6. For some topics the query-biased document importance works best (promoting aspects cov-ered by the query), while for others inverse query-biased works best (promoting aspects not covered by the query that comes with the topic). On average, though, the query-independent sampling delivers the best performance; see Table 4. Figure 6: AP differences between query-biased ( X  X aseline") and inverse query-biased document sampling methods. (Top): EX-QM-ML, (Bottom): EX-QM-SM, on (Left): possibly, and (Right): highly relevant.

Let us return to the issue of aspect recall. We have seen that us-ing query models leads to better ranking of documents. Looking P ( t |  X  Q ) t P ( t |  X  Q ) t P ( t |  X  Q ) t 0.7405 termites 0.4729 termites 0.5653 termites 0.0401 csiro 0.0452 site 0.0299 site 0.0388 wood 0.0443 information 0.0292 information 0.0316 food 0.0412 legal 0.0281 legal 0.0314 termite 0.0410 notice 0.0281 notice 0.0258 vibrations 0.0404 disclaimer 0.0271 disclaimer 0.0242 blocks 0.0402 privacy 0.0271 privacy 0.0231 species 0.0381 web 0.0252 drywood 0.0228 australian 0.0378 subject 0.0243 statement 0.0217 made 0.0378 drywood 0.0173 subject Table 8: Query models for topic # 36  X  X ermites". (Left) BFB-RM2; (Center) EX-RM2; (Right) EX-QM-ML. at the individual documents returned by each model, we find that using blind relevance feedback, recall either decreases (BFB-RM1; over all queries, BFB-RM1 retrieves 2,564 highly relevant docu-ment vs. 2,763 for the baseline; see Table 9) or only marginally increases (BFB-RM2; 2,816 vs. 2,763). On the other hand, expand-ing the query based on the example documents can help to capture on average 10% more relevant documents than the baseline, on both relevance levels; see Table 9. Importantly, there is a number of doc-relevance baseline RM1 RM2 RM1 RM2 ML SM EXP possibly 5,445 5,238 5,582 5,951 5,882 6,052 5,953 5,671 highly 2,763 2,564 2,816 2,954 2,929 3,047 3,019 2,823 uments that are found only when sampling is done independent of the query (EX-QM-*). Consider topic #32 ( machine vision ) again. First, the number of relevant documents found for this topic are the following: baseline: 53, BFB-RM2: 54, EX-RM2: 54, and EX-QM-ML: 62. The additional documents are identified through the new terms introduced by our query models, as is clearly illus-trated in Table 6: the terms cam and camera are captured only by EX-QM-ML. In sum, then, our sampling method from sample doc-uments does indeed pick up different aspects of the topic, and as such, helps improve  X  X spect recall. X 
We introduced a method for sampling query expansion terms in a query-independent way, based on sample documents that reflect aspects of the user X  X  information need that are not captured by the query. We described various versions of our expansion term se-lection method, based on different term selection and document importance weighting methods, and compared them against more traditional query expansion methods that select expansion terms in a query-biased manner.

Evaluating our methods on the TREC 2007 Enterprise track test set, we found that our expansion method outperforms a high per-forming baseline as well as standard language modeling based query expansion methods. Our analysis revealed that our query-indepen-dent expansion method does help to address the  X  X spect recall X  problem, and helped to identify relevant documents that are not identified by the other query models that we considered.
As to future work, we see a number of other ways of exploit-ing sample documents provided for a topic. One possibility is to look at other features of these example documents, including lay-out, link structure, document structure, etc. and favor documents in the ranking that share the same characteristics. Another possibil-ity is to combine terms extracted from blind feedback documents, together with terms from sample documents.
Balog and De Rijke were supported by the Netherlands Organ-isation for Scientific Research (NWO) under project number 220-80-001. Weerkamp and De Rijke were supported by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH con-tract IST-033104. De Rijke was also supported by NWO under numbers 017.001.190, 640.001.501, 640.002.501, STE-07-012.
