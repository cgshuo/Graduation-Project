 Language modeling approaches have been effectively dealing with the dependency among query terms based on N-gram such as bi-gram or trigram models. However, bigram language models suffer from adjacency-sparseness problem which means that dependent terms are not always adjacent in documents, but can be far from each other, sometimes with distance of a few sentences in a doc-ument. To resolve the adjacency-sparseness problem, this paper proposes a new type of bigram language model by explicitly in-corporating the proximity feature between two adjacent terms in a query. Experimental results on three test collections show that the proposed bigram language model significantly improves previous bigram model as well as Tao X  X  approach, the state-of-art method for proximity-based method.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation Keywords: Language models, proximity, bigram model, term de-pendency
The locality among query terms is a useful feature to boost the retrieval effectiveness since the semantics of some queries can be identified based on multi-term level rather than single-term level. Regarding this, language modeling approaches have been flexibly extended to bi-gram or N-gram language model in order to exploit the sequential dependency among query terms [2]. Researchers have observed that bi-gram model is helpful to improve the retrieval effectiveness [2].

However, the paraphrasing phenomenon makes a serious prob-lem when estimating bi-gram language models. Due to the para-phrasing, when two adjacent terms in a query are given, these terms in highly-relevant documents can be represented by different man-ners such as reversing their positions, or inserting other terms be-tween two terms, thus resulting in adjacency-sparseness problem. Thus, the estimated bi-gram model becomes biased, causing the un-fair preference of only to documents where query terms are adjacently-appeared but not to documents where query terms are locally-appeared.
To deal with this, this paper proposes a new type of language model to explicitly incorporate the proximity feature. Note that the previous bigram language model exploited the probability to gen-erate next query term for a given current query term in the stream of a given document. In this paper, we define a general type of bigram language model by the average of the generative probabil-ities of next query term from the set of local contexts of the cur-rent query term. To reasonably estimate the new bigram language model, we define the local context of a term by minimum-length passages between current query term and next query term. The pro-posed approach has some advantages compared to Tao X  X  approach [3], the state-of-the-art proximity-driven approach which explicitly uses the proximity feature but is independently designed to retrieval models. 1) While Tao X  X  approach is largely heuristic, our approach is model-driven approach without losing the elegancy of language modeling approaches. 2) While Tao X  X  approach should calculate distances among all possible pairs in a query, our approach cal-culate distances among only adjacent query terms in a query, thus providing more efficient manner. 3) Our approach shows better per-formance than Tao X  X  approach in standard TREC test collections. First, let us re-visit the background of bigram language model. Suppose that query Q is given by the stream of q 1 the query likelihood of query Q from document D is calculated as follows [2]: language model of document D , respectively.
The proposed bigram language model indicates the average of probabilities to generate term q i from the set of local contexts of q i  X  1 . Generally, an arbitrary pseudo passage sample containing q i  X  1 can be used as a local context of q i  X  1 . Let LC ( q set of such local contexts of q i  X  1 in document D . Then, the pro-posed model redefines the estimation of P ( q i | q i  X  1 where lc indicates a member of LC ( q i  X  1 , D ) . P ( lc | LC ( q indicates the prior probability of local context lc , which is assumed to be uniformly distributed -1 / | LC ( q i  X  1 , D ) | . P ( q follows: where W is a new parameter which is regarded as the maximally allowed window size where two terms q i  X  1 and q i proximally ap-pear in the local context lc . When len ( lc ) is larger than W , P ( q simply indicates the generative probability of q i from local context lc which is obtained from MLE(maximum likelihood estimation) -c ( q i ; lc ) / len ( lc ) .

LC ( q i  X  1 , D ) plays the most important role to estimate P ( q To define LC ( q i  X  1 , D ) , this paper assumes that minimum cover of q for each occurrence of q i  X  1 is an element of LC ( q i  X  1 mum cover of q i for q ( k ) i  X  1 by s pan ( q ( k ) i  X  1 minimum-length passage among candidate passages which contain both q i  X  1 ( k ) and q i . For example, suppose that D is given by  X 1: t 2: t 2 , 3: t 3 , 4: t 1 , 5: t 2 , 6: t 4 , 7: t 5 , 8: t 2 , 9: t t  X . Let us denote a passage by [ p , q ] where p and q are the start position and the end position of passage, respectively. For the first term occurrence of t ( 1 ) 1 -1: t 1 , the minimum cover -span ( t is [1-2], and for t ( 2 ) 1 -4: t 1 , span ( t ( 2 ) 1 , t Eq. (2) is rewritten by
P ( q i | q i  X  1 , D ) =  X 
Since it is well-known that Dirichlet-prior smoothing is better than Jelinek-Mercer smoothing (for short keyword queries), we fo-cus on how the proposed bigram-model can be applied to Dirichlet-prior smoothing. Dirichlet-prior smoothing for unigram language model is formulated as follows [4]: where  X  is a smoothing parameter, P ( q i |  X  q D ) is MLE for unigram language model of D -c ( q i ; D ) / len ( D ) , and P ( q ground collection language model. Note that len ( D ) plays an im-portant role for obtaining smoothed model P ( q i | q D ) as additional evidences. Unlike unigram language model, the evidence sample for bigram language model is not unique, i.e. it consists of several minimum covers. In this paper, we assume that the length of the evidence sample for smoothing of the bigram language model is simply c ( q i  X  1 ; D ) W . Thus, the bigram model version of Eq. (4) is formulated by Unigram language model is used as back-off for bigram language model, i.e. when c ( q i  X  1 ; D ) is 0 or P ( q i | q i  X  1 (5) of unigram language model instead of Eq. (6).
For evaluation, we used standard TREC test collections for ad-hoc retrieval -WT2G and WT10G (WT2G is used for TREC8, and WT10G is used for TREC9 and TREC10). The standard method was applied to extract index terms. We first separated words based on space characters, eliminated stopwords, and then applied Porter X  X  stemming. The title field is utilized as query type of all test collec-tions. The parameter  X  for Dirichlet-prior smoothing is differently Table 1: Retrieval performances (Mean Average Precision) of four different methods. All methods use Dirichlet-prior smoothing as basic retrieval model. Unigram, Bigram and ProxBigram indicate unigram language model, the previous bigram model, and the proposed bigram model, respectively. Tao X  X  Prox indicates the most recent work of proximity-based approach.
 selected depending on each collection so that it maximizes the re-trieval effectiveness of unigram language model. W is fixed to 5. lections. For comparison with previous methods, we append the results of the previous bigram language model and Tao X  X  approach. We used Tao X  X  MinDist for the proximity of query terms -d ( Q , D ) due to its better performance, and used Dirichlet-prior smoothing using the same  X  as the basic retrieval model. Four methods are ab-breviated by Unigram (the baseline), Bigram (the previous bigram model), Tao X  X  Prox (Tao X  X  approach [3]), and ProxBigram (the pro-posed bigram model) in the table. To check whether or not bigram-based or proximity-based method significantly improves the base-line (Unigram), we performed the Wilcoxon sign ranked test and attached  X  and  X  to the performance number of each cell in the table when the test passes at 95% and 99% confidence level, respectively.
As shown in table 1, the proposed bigram language model signif-icantly improves the unigram language model, and better performs than previous bigram language model and Tao X  X  proximity-based approach. This result consistently shows that the proposed model can effectively resolve the adjacency-sparseness problem.
This paper proposed a new type of bigram language model to explicitly support the proximity feature, in order to resolve the adjacency-sparseness problem. Experimental results indicate that the proposed model is promising by showing a better performance than Tao X  X  proximity-based method, and previous bigram language model. Furthermore, the proposed model is more efficient than Tao X  X  proximity method, since our model allows us to calculate only within-document distances between only adjacent query terms. In addition, the proposed model is much more efficient than the de-pendency language model which makes non-trivial burden since full syntactic parsing should be pre-applied to whole documents [1]. In the future, we will explore the proposed bigram model on various different setting of local context sets.
 This work was supported in part by MKE &amp; IITA through IT Lead-ing R&amp;D Support Project and also in part by the BK 21 Project in 2008.
