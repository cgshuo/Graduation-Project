 1. Introduction
Storytelling is an ancient and universal human method for education and entertainment. Recently much research has classification Shen et al. (2004) and question answering Demner-Fushman and Lin (2006) . extract based summarization. Abstract based summarization employs new words, phrases and sentences to express the rization covers generic summarization and query-oriented summarization. The query-oriented method generates summa-ries of documents according to given queries or topics, and the generic method summarizes the overall sense of the document without any additional information.  X 
Extracting central sentences from a document can be formulated as a sentence ranking problem. In Erkan and Radev Moreover, three granularities were considered in Wei et al. (2008) including document, sentence and term. extracted as the summarization. The contributions of this paper are summarized as follows: (i) an embedded graph based sentence clustering method is proposed for sentence grouping of a document, which is (iii) a document summarization framework considering sentence cluster information is proposed and the framework is influence on system performance of different parameters. We conclude our work in Section 5 . 2. Related work ployed to represent the document topics.
 tors of matrix V T . From semantic point of view, each column of V (the singular vector) denotes a set of word combination. The item V the summary of a document.

The graph-based ranking methods were also proposed for document summarization by Erkan and Radev (2004) . Similar for query-oriented summarization Otterbacher et al. (2005) .
 and Nomoto and Matsumoto (2003) .In Nomoto and Matsumoto (2003) , an extended K-means (KM) clustering algorithm ters of abstract documents which summarized those generated in the previous level. Terms that can distinguish the most frequent terms, similarity to neighboring sentences, etc. 3. Sentence ranking using embedded graph based sentence clustering 3.1. Document modeling
To facilitate the document summarization process, we model a document with a text matrix D  X  X  ~ s m is the number of terms and n is the number of sentences of the document. The column vectors ~ s weighted term-frequency vectors of sentences. To build the vector ~ s first and word stemming is performed. The term T ={ t 1 , t ~ s  X  X  s i 1 ; s i 2 ; ... ; s im T where s ij = 1 if term t j two sentences is greater than 0. The weight w ij of an edge denotes the distance between sentences s similarity between the vectors of two sentences, i.e., where the vectors ~ s i and ~ s j are weighted term-frequency vectors of sentence s and 374 tokens. In this figure, only the distances between Sentence 1 s similarities between Sentence 1 and Sentence 3, 5, 7, 9 are all larger than 0, and w w figure. 3.2. Embedded graph based sentence clustering sentences. However, the accuracy of the clustering will be diminished if all the edges with w the document shown in Fig. 1 a, the weight of the edge between sentence s reason is that sentence s 9 represents totally different semantic meanings with s  X  X  X o X  X  and  X  X  X ake X  X . In this case, s 1 is not related with s similar sentences ~ d j ; j  X  1 ; 2 ; ... ; n i , with where X i is the set of sentences most similar to ~ d i . The dimensions of vectors d cosine similarity in (1) can be written as
The Euclidean distance of two vectors is mating the sentence relationship provided that k ~ d i k X k mine the optimal weight matrix c ij subject to the constraint where c ij = 0 for j R X i , and W is the weight matrix composed by c cost function (5) is rewritten as in Pan, Ge and Al Mamun (2009) and Roweis and Saul (2000) where W i  X  X  c i 1 ; ... ; c in multiplier g to (7) . The error cost becomes
By requiring the partial derivatives with respect to each weight c the solution W i is found by solving the equations
The vectors of embedded sentences with enhanced relationship d function sentences. The cost function (11) can be written in matrix form as et al. (2010) where the first constraint is to assure that coordinates d W as its rows.

Fig. 1 a and b. The high weight relationship such as w sim like w sim ( s 1 , s 9 ) is adjusted to be negative. The similarities between s worthy that w sim ( s 1 , s 5 )&gt; w sim ( s 1 , s 3 )in Fig. 1 a, while w fore embedding is a measure of the relationship between two sentences while the embedding operation keeps the failed embedding.
 tering is performed using the new distances. 3.3. Mutual-reinforcement ranking algorithm nected by the weighted edges in the graph. The weight of the edge connecting term t where D is the sentence X  X erm matrix of the document. The weight of the edge connecting sentence s culated by
The weight of the edge connecting term t l and cluster c j We first cluster the sentences using clustering algorithms and then calculate the rank of each sentence with: where r ( s i ) is the rank of sentence s i , r ( c j ) is the rank of cluster c m , n and k are the number of terms, sentences and clusters respectively. The weights w calculated by (14) X (16) respectively. The parameters a i , b fects to the ranks of all items. In our experiments, the system performance did not increase significantly by form: where ~ r  X  s  X  X  X  r  X  s 1  X  ; ... ; r  X  s n  X  T ; ~ r  X  c  X  X  X  r  X  c
W ( j , l )= w ct ( j , l ). The rank is normalized after each iteration through: where k ~ x k X  use 1 e 4).

After the sentence ranking is obtained, we select sentences to compose the document summary. All the sentences are of the summarization is achieved. 3.4. Convergence of mutual reinforcement methods section. Our method can be written in the following format where W cc , W ss , and W tt are zero matrixes. The vectors ~ r terms. The term W cc is the cluster X  X luster affinity matrix, W if the matrix is stochastic and irreducible, out algorithm will converge at a unique ranking.
W , W ss , and W tt . We revise X to X  X  X  1 = k k 1  X  1 1 k are normalized by columns, and denoted by W sc ; W 0 sc ; W alleviate the effect of matrix X , we set a 1 , b 2 and c It can be proved that is stochastic and irreducible. We use M to replace M , and our method will converge to a unique ranking vector. 4. Experimental evaluation 4.1. Corpora and evaluation criterion
The Mutual-reinforcement Ranking algorithm using Embedded Graph based sentence clustering (MREG) is evaluated by is performed using Porter X  X  stemming algorithm Frakes and Baeza-Yates (1992) . We use the well-known ROUGE system Lin (2004) which includes five methods, i.e., ROUGE-N, ROUGE-L, ROUGE-W, tween the candidate summary and the reference summaries where the sentence set S R denotes the reference summaries and s is a sentence in the set, n denotes n-gram and N numberof n-gramsinthe setofreferencesummaries.The lengthoflongestcommonsubsequence(LCS)betweenthecandidate summaryandreferencesummariesisemployedbyROUGE-L,andROUGE-WusestheweightedLCSasshownin (23)Lin(2004) . where C is the candidate summary which contains n words and r m words. LCS ^ ( r i , C ) is the LCS score of the union longest common subsequence between reference sentence r summaries Lin (2004) .

P and ROUGE-SU4 means that the measure calculates n-grams with maximum skip-distance of 4. We use the ROUGE toolkit uation, both model and peer summaries are stemmed using Porter stemmer and superfluous words are pruned. 4.2. Performance comparison
The performance of MREG method on DUC-2001 is compared with the Latent Semantic Analysis (LSA) Gong and Liu better ROUGE scores than the other methods.
The comparison of the results of MREG method with the best original participants of DUC-2001 is shown in Table 2 . results of systems in DUC-2001 are calculated using the result data of submissions downloaded from DUC website. measures.

The performance of MREG method on DUC-2005 is compared with the Mutual Reinforcement Principle (MRC) Wei et al. highlighted as bold emphases in Tables 1 X 3 . 4.3. Discussion of selective parameters
In our experiment, two kinds of parameters affect the system performance: the number of clusters k and the balance f is an integer. The influences of these parameters are shown in the following figures.
Figs. 3 and 4 demonstrate the ROUGE-1 and ROUGE-2 values obtained by KM and agglomerative (AGG) clustering algo-almost always outperforms the baseline system which only calculates the mutual-reinforcement between sentences and The performance of KM-SC method fluctuates dramatically according to the number of clusters. The KM-SC-EM and KM-
TSC-EM methods are more robust than the KM-SC method with respect to different cluster numbers. By comparing the on embedded graph. Fig. 4 indicates that the AGG-TSC-EM method also outperforms the AGG-SC method by using agglom-restriction to parameters a i , b i and c i , i =1,2in (17) : a a : b : c = 1:1:2 and f = 100.
 5. Conclusion the algorithm improved system performance and was more robust with respect to different cluster numbers. Acknowledgment search Grant R-705-000-017-279.
 References
