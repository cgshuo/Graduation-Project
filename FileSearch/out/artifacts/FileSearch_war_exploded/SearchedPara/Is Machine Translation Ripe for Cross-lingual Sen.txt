 Question 1: If MT gave perfect translations (seman-tically), do we still have a domain adaptation chal-lenge in cross-lingual sentiment classification? Answer: Yes. The reason is that while many trans-lations of a word may be valid, the MT system might have a systematic bias. For example, the word  X  X we-some X  might be prevalent in English reviews, but in translated reviews, the word  X  X xcellent X  is generated instead. From the perspective of MT, this translation is correct and preserves sentiment polarity. But from the perspective of a classifier, there is a domain mis-match due to differences in word distributions. Question 2: Can we apply standard adaptation algo-rithms developed for other (monolingual) adaptation problems to cross-lingual adaptation? Answer: No. It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation. We do not know the reason, but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios.

The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems. We invite the reader to disagree with our conclusion (that the true barrier to good performance is not insufficient MT quality, but inappropriate domain adaptation meth-ods). Here we present a series of experiments that led us to this conclusion. First we describe the ex-periment design (  X  2) and baselines (  X  3), before an-swering Question 1 (  X  4) and Question 2 (  X  5). The cross-lingual setup is this: we have labeled data from source domain S and wish to build a sentiment classifier for target domain T . Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs. Book reviews). Our experiments will involve fixing T to a common testset and varying S . This allows us to experiment with different settings for adaptation.
We use the Amazon review dataset of Pretten-(English [EN], Japanese [JP], French [FR], Ger-man [DE]) and markets (music, DVD, books). Un-like Prettenhofer (2010), we reverse the direction of cross-lingual adaptation and consider English as tar-get. English is not a low-resource language, but this setting allows for more comparisons. Each source dataset has 2000 reviews, equally balanced between positive and negative. The target has 2000 test sam-ples, large unlabeled data (25k, 30k, 50k samples respectively for Music, DVD, and Books), and an additional 2000 labeled data reserved for oracle ex-periments. Texts in JP, FR, and DE are translated We perform three sets of experiments, shown in Table 1. Table 2 lists all the results; we will interpret them in the following sections.
 1 Music-EN Music-JP, Music-FR, Music-DE, 2 DVD-EN DVD-JP, DVD-FR, DVD-DE, 3 Book-EN Book-JP, Book-FR, Book-DE, First, we need to quantify the accuracy degrada-tion under different source data, without consider-ation of domain adaptation methods. So we train rectly apply it on test data. The oracle setting, which has no domain-mismatch (e.g. train on Music-EN, test on Music-EN), achieves an average test accu-racy of (81 . 6 + 80 . 9 + 80 . 0) / 3 = 80.8% 4 . Aver-age cross-lingual accuracies are: 69.4% (JP), 75.6% (FR), 77.0% (DE), so degradations compared to or-
Observation 1: Degradations due to market and language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN). Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP. Does this mean JP-EN is a more difficult language pair for MT? The next section will show that this is not necessarily the case. Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors. 4.1 Theory of Domain Adaptation We analyze domain adaptation by the concepts of labeling and instance mismatch (Jiang and Zhai, 2007). Let p distribution of samples x (e.g. unigram feature vec-tor) and labels y (positive / negative). Let p p ( y | x ) p s ( x ) be the corresponding source distribu-tion. We assume that one (or both) of the following distributions differ between source and target:  X  Instance mismatch: p s ( x ) 6 = p t ( x ) .  X  Labeling mismatch: p s ( y | x ) 6 = p t ( y | x ) .
Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word  X  X xcellent X  often, while the other uses the word  X  X wesome X ). This degrades performance because classifiers trained on  X  X xcellent X  might not know how to classify texts with the word  X  X we-some. X  The solution is to tie together these features (Blitzer et al., 2006) or re-weight the input distribu-tion (Sugiyama et al., 2008). Under some assump-tions (i.e. covariate shift), oracle accuracy can be achieved theoretically (Shimodaira, 2000).

Labeling mismatch implies the same input has different labels in different domains. For exam-ple, the JP word meaning  X  X xcellent X  may be mis-translated as  X  X ad X  in English. Then, positive JP reviews will be associated with the word  X  X ad X : p ( y = +1 | x = bad ) will be high, whereas the true conditional distribution should have high p  X  1 | x = bad ) instead. There are several cases for labeling mismatch, depending on how the polarity changes (Table 3). The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009).
Which mismatch is responsible for accuracy degradations in cross-lingual adaptation?  X  Instance mismatch: Systematic MT bias gener- X  Label mismatch: MT error mis-translates a word
Conclusion from  X  4.2 and  X  4.3: Instance mis-match occurs often; MT error appears minimal.
Mis-translated polarity Effect  X  X  0 Loose a discriminative e.g. ( X  X ood X   X   X  X he X ) feature 0  X  X  Increased overlap in e.g. ( X  X he X   X   X  X ood X ) positive/negative data +  X  X  X  and  X  X  X  + Association with e.g. ( X  X ood X   X   X  X ad X ) opposite label 4.2 Analysis of Instance Mismatch To measure instance mismatch, we compute statis-tics between p thereof: First, we calculate a (normalized) average feature from all samples of source S , which repre-sents the unigram distribution of MT output. Simi-larly, the average feature vector for target T approx-imates the unigram distribution of English reviews p ( x ) . Then we measure:  X  KL Divergence between Avg( S ) and Avg( T ),  X  Set Coverage of Avg( T ) on Avg( S ): how many
Both measures correlate strongly with final accu-racy, as seen in Figure 1. The correlation coefficients are r =  X  0 . 78 for KL Divergence and r = 0 . 71 for Coverage, both statistically significant ( p &lt; 0 . 05 This implies that instance mismatch is an important 4.3 Analysis of Labeling Mismatch We measure labeling mismatch by looking at dif-ferences in the weight vectors of oracle SVM and adapted SVM. Intuitively, if a feature has positive weight in the oracle SVM, but negative weight in the adapted SVM, then it is likely a MT mis-translation is causing the polarity flip. Algorithm 1 (with
We found that the polarity flip rate does not cor-relate well with accuracy at all ( r = 0 . 04 ). Conclu-sion: Labeling mismatch is not a factor in perfor-mance degradation. Nevertheless, we note there is a surprising large number of flips (24% on average). A manual check of the flipped words in BOOK-JP re-vealed few MT mistakes. Only 3.7% of 450 random EN-JP word pairs checked can be judged as blatantly incorrect (without sentence context). The majority of flipped words do not have a clear sentiment ori-entation (e.g.  X  X mazon X ,  X  X uman X ,  X  X oreover X ). One of the breakthroughs in cross-lingual text clas-sification is the realization that it can be cast as do-main adaptation. This makes available a host of pre-existing adaptation algorithms for improving over supervised results. However, we argue that it may be Algorithm 1 Measuring labeling mismatch better to  X  X dapt X  the standard adaptation algorithm to the cross-lingual setting. We arrived at this con-clusion by trying the adapted counterpart of SVMs off-the-shelf. Recently, (Bergamo and Torresani, 2010) showed that Transductive SVMs (TSVM), originally developed for semi-supervised learning, are also strong adaptation methods. The idea is to train on source data like a SVM, but encourage the classification boundary to divide through low den-sity regions in the unlabeled target data.

Table 2 shows that TSVM outperforms SVM in all but one case for cross-market adaptation, but gives mixed results for cross-lingual adaptation. This is a puzzling result considering that both use the same unlabeled data. Why does TSVM exhibit such a large variance on cross-lingual problems, but not on cross-market problems? Is unlabeled target data interacting with source data in some unexpected way?
Certainly there are several successful studies (Wan, 2009; Wei and Pal, 2010; Banea et al., 2008), but we think it is important to consider the possi-bility that cross-lingual adaptation has some fun-damental differences. We conjecture that adapting from artificially-generated text (e.g. MT output) is a different story than adapting from naturally-occurring text (e.g. cross-market). In short, MT is ripe for cross-lingual adaptation; what is not ripe is probably our understanding of the special character-istics of the adaptation problem.
