 Index compression techniques are kno wn to substan tially de-crease the storage requiremen ts of a text retriev al system. As a side-e ect, they may increase its retriev al performance by reducing disk I/O overhead. Despite this adv antage, de-velop ers sometimes choose to store index data in uncom-pressed form, in order to not obstruct random access into eac h index term's postings list.

In this pap er, we sho w that index compression does not harm random access performance. In fact, we demonstrate that, in some cases, random access into a term's postings list may be realized more ecien tly if the list is stored in compressed form instead of uncompressed. This is regardless of whether the index is stored on disk or in main memory , since both types of storage { hard driv es and RAM { do not supp ort ecien t random access in the rst place. H.3.4 [ Systems and Soft ware ]: Performance evaluation (eciency and e ectiv eness) Exp erimen tation, Performance Index Compression, Random Access, Main Memory , RAM
Man y searc h engines emplo y index compression tech-niques, suc h as the byte-aligned vByte metho d [19] to decrease the storage requiremen ts of their index structures. If the ma jorit y of the index data are stored on disk, then this also impro ves the searc h engine's retriev al performance, due to reduced disk I/O overhead [14][19]. This even holds if the index access pattern is completely random, since in that case the smaller index leads to a reduced disk seek latency [22][24].
 Cop yright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
In the con text of in-memory indices, however, where all index data are held in RAM, it is sometimes argued that in-dex compression metho ds are not advisable, esp ecially when it is imp erativ e that the searc h engine can carry out random access operations into the inverted lists stored in the index. Random access into a term's postings list is necessary when computing set intersections in Boolean retriev al [6] and in order to apply the MaxScore [21] heuristic in rank ed re-triev al. It is usually realized through some sort of a binary searc h operation (or variations thereof, suc h as interp ola-tion searc h [15] or galloping searc h [7]). Compressed post-ings lists, in whic h eac h posting is stored as a delta value relativ e to its predecessor, are not directly amenable to bi-nary searc h, but need to be decompressed (at least partially) prior to a random access operation.

Brew er [2], therefore, suggests that index compression should only be used if the index access pattern is largely sequen tial. Heman [11] describ es metho ds for keeping the decompression overhead low so that the performance degra-dation { compared to an uncompressed index { becomes negligible.

In this pap er, we sho w that index compression not only does not harm retriev al performance, but in fact can impro ve performance, especial ly if index access is predominan tly ran-dom. Our argumen t is based on the insigh t that there exists virtually no storage medium that pro vides true, penalt y-free random access to large amoun ts of data. Accessing a random byte on disk, for example, involves a costly disk seek (mo ving the disk's read/write head above the disk trac k con taining the desired information) and the disk's rotational latency (waiting until the desired piece of data has spun under the head). Similarly , despite its name, accessing a random loca-tion in RAM may result in a second-lev el (L2) CPU cac he miss or a TLB (translation lookaside bu er) miss | events that carry a gra ve penalt y and that can easily consume a hundred CPU cycles.

The con tribution of this pap er is two-fold. In a rst step, we prop ose a new data structure, the CPSS-tree (\cac he/page-sensitiv e searc h tree"), whic h pro vides highly ecien t random access lookup operations and is an appro-priate data structure for postings lists in an in-memory inverted index. In a second step, we sho w how traditional index compression techniques can be applied to the data stored in a CPSS-tree. We sho w that by taking into ac-coun t the nature of the underlying storage medium and by applying compression only at the same gran ularit y at whic h the storage medium is able to pro vide true random access (usually a single cac he line), it is possible to increase int outPos = 0, previous = 0; for (int inPos = 0; inPos &lt; n; inPos++) f g int outPos = 0, previous = 0; for (int outPos = 0; outPos &lt; n; outPos++) f for (int shift = 0; ; shift += 7) f g uncompressed[outPos] = previous; g arithmetic for every arra y access. the performance of random index access operations beyond what is possible with an uncompressed index.

In the follo wing section, we pro vide a summary of some asp ects of computer architecture and index data structures that are relev ant in the con text of our pap er. We also presen t an overview of related work in the area of cache-c onscious data structures that explicitly tak e into accoun t the di er-ences between various types of memory access operations. In Section 3, we describ e the hardw are and soft ware con gura-tions under whic h we evaluate the various index layouts ex-amined in this pap er. In Section 4, we discuss cac he-ecien t in-memory index structures for uncompressed inverted lists, pointing out imp ortan t asp ects regarding the hierarc hical memory mo del of today's computer systems and prop os-ing a novel cac he-conscious data structure, the CPSS-tree. Section 5, nally , describ es how inverted list compression can be integrated into the index structures from Section 4 without sacri cing random access performance (and in fact impro ving random access performance in all hardw are con-gurations tested in our exp erimen ts). Inverted Indices The fundamen tal data structure of most text retriev al sys-tems is the inverted index [25]. An inverted index consists of two principal comp onen ts: the dictionary and the inverte d lists (or postings lists ). The dictionary pro vides a mapping from eac h term in the index to the position of its postings list. A term's postings list con tains a sequence of postings , iden tifying all occurrences of the term within the text col-lection referred to by the index.

Postings lists may either con tain exact positional infor-mation about all occurrences of a given term, or just a list of docids , numerical iden ti ers referring to the documen ts con taining the term in question. Within the con text of this pap er, we assume that eac h postings list is simply a sequence of docids. However, the metho ds presen ted here are easily applicable to other inverted index varian ts, suc h as positional indices and frequency indices.
 Index Compression In a docid index, every postings list can be represen ted by an increasing sequence of integers. Index compression tech-niques enco de suc h a sequence by transforming the docids into an equiv alen t sequence of delta values. Let be a postings list. Then The elemen ts of the integer sequence ( L ) tend to be rather small and can be enco ded using standard integer enco ding techniques.

One of the most popular metho ds for enco ding suc h a sequence is the byte-aligned gap enco ding metho d called vByte [19]. In vByte, every elemen t of the enco ded list is represen ted by an integral num ber of bytes, where the most-signi can t bit in eac h byte de nes whether the given byte is the last one in the curren t codew ord or whether there are more to follo w. vByte's enco ding and deco ding routines are extremely simple (as sho wn in Figure 1), whic h at query time translates into low decompression latency and high query pro cessing performance. Using the code fragmen ts sho wn in the gure, an average vByte-compressed postings lists can be deco ded in less than 10 CPU clock cycles per posting for the inverted indices used in our exp erimen ts.

Other metho ds, suc h as LLR UN [8] and interp olativ e coding [13] are kno wn to pro duce better compression results than vByte, but require more complicated de-compression operations, making them less interesting for high-p erformance retriev al operations.

Obviously , a -enco ded postings list does not allo w ran-dom access, as the value of every posting dep ends on its predecessor. However, by adding so-called synchr onization points [14] to eac h compressed list, perhaps one for every 64th posting, it is possible to realize quasi-random list ac-cess, where retrieving an individual posting only requires the prior decompression of a small num ber of postings in its immediate neigh borho od, and not the entire list. In its most basic form, a sync hronization point is a record of the form where posting is the raw value of a posting, and position is its (bit-or byte-) position in the compressed postings list. Sometimes, position can be omitted because it is implicit from the way in whic h postings lists are stored in the index.
The concept of sync hronization points can be extended re-cursiv ely, by main taining second-, third-, fourth-order syn-chronization points, if the underlying storage medium (e.g., hard driv e) does not permit true random access into the list of rst-order sync hronization points. Sources of Index Access Latency: Hard Disks Regardless of whether postings lists are kept in the index in compressed or in uncompressed form, accessing a random elemen t of a given list usually does not come for free. The gra vity of the penalt y asso ciated with suc h random access operations dep ends on the storage medium that con tains the index.

If the index is stored on a hard disk driv e, then the cost of a random access operation stems from two sources: Disk seek latency | If the datum to be accessed is stored Rotational latency | Even when the hard driv e does A detailed overview of di eren t performance asp ects of hard disk driv es is given by Ruemmler and Wilk es [18]. The essen tial insigh t here is that random access does not come for free and that the cost of a random access operation dep ends on the size of the on-disk index (because the disk head, on average, needs to travel farther for a large index than for a small one). This phenomenon is addressed in the exp erimen ts conducted by Williams and Zob el [22], who sho w that a compressed on-disk index leads to better random access performance than an uncompressed one. Sources of Index Access Latency: Main Memory For an inverted index stored in main memory , the source of random access latency is not so obvious. It is caused by the various comp onen ts of the memory hierarc hy found in a typical computer system (sho wn in Figure 2).

Compared to the CPU itself, a computer's RAM is ex-tremely slow, usually requiring somewhere in the order of 30{100 CPU cycles to deliv er a mac hine word to the CPU. Man y programs exhibit a strong temp oral and spatial local-ity. That is, they tend to access memory locations that have been accessed very recen tly (temp oral localit y), and they tend to access memory locations that are close to other, recen tly accessed memory locations (spatial localit y). In man y cases, therefore, the performance gap between CPU and main memory can be closed by emplo ying cac hes | very fast, but rather small memory units that con tain copies of frequen tly accessed memory locations.

In most mo dern computer systems, the CPU cac he is split up into two comp onen ts: a rst-lev el (L1) cac he, and a second-lev el (L2) cac he. The L1 cac he operates at full CPU speed, but only has room for a few kilob ytes of data. The L2 serv es as a fallbac k system for the L1 cac he, being sligh tly bigger, but also requiring sligh tly more time to deliv er data to the CPU. A CPU cac he tak es into accoun t spatial localit y by fetc hing data from main memory in larger chunks, called Figure 2: The memory hierarc hy. Performance characteristics of various comp onen ts of a computer system. cac he lines, usually 32 or 64 bytes in size. Once a cac he line has been fetc hed from RAM, access to more data in the same cac he line can be realized very ecien tly (this is called a cache hit ). Accessing a memory location, however, whose cac he line is not curren tly in the CPU cac he is rather costly , because the entire cac he line rst needs to be loaded into the cac he (this is called a cache miss ).

In the con text of random access operations, cac he misses represen t a ma jor performance bottlenec k, as they force the CPU to almost constan tly wait for data from the compara-tively slow main memory . This is almost indep enden t of the size of the second-lev el cac he, as a cac he line is either in the L1 cac he (because it is curren tly being pro cessed) or not in the cac he at all (because it has not been touc hed in a very long time).

In addition to CPU cac he misses, there is a second source of random access latency that is presen t in virtually every computer system: TLB misses. Most mo dern computer sys-tems emplo y paging techniques to translate virtual memory addresses into physical ones. The information needed to perform this translation is found in the pro cess-sp eci c page table. In order to avoid accessing the page table for every memory access operation (whic h would reduce performance by 50%), most CPUs main tain a page table cac he operat-ing at full CPU speed, called the translation lookaside bu er (TLB), in whic h they keep the necessary translation infor-mation for the most recen tly accessed memory pages.
Lik e in the case of the L1 and L2 CPU cac hes, accessing a random memory location is likely to result in a TLB miss , because the page table entry for the resp ectiv e page is not curren tly in the TLB. Compared to an L2 cac he miss, a TLB miss is even more costly , because it normally entails at least two L2 cac he misses (one for the page table entry, another one for the requested data) and sometimes even involves interaction with the operating system's kernel.

An overview of performance asp ects of a computer's memory hierarc hy is given by Smith [20] and Burger et al. [3].
 Cac he-Conscious Data Structures A signi can t amoun t of e ort has been put into the design of algorithms and data structures that tak e the performance characteristics of a computer's memory hierarc hy into ac-coun t and try to minimize the num ber of cac he misses. Usu-ally , these cache-c onscious (or cache-awar e ) data structures are variations of existing and well-studied cache-unawar e data structures.
Closest to the work presen ted here is the researc h con-ducted by Rao and Ross [16]. In their pap er, they discuss how m -ary searc h trees can be adjusted to exhibit impro ved memory access localit y and thus better cac he beha vior in the presence of random access. Eac h node in their CSS-tree (\cac he-sensitiv e searc h tree") data structure occupies ex-actly one CPU cac he line. Nodes only con tain keys, and no pointers to their children, impro ving space utilization and thus decreasing the num ber of cac he misses even further. Child pointers are eliminated by storing all nodes in a large arra y, similar to the data organization found in the canonical implemen tation of Heapsor t . An example of a CSS-tree is given by Figure 3.

The num ber of cac he misses pro duced by a CSS-tree when searc hing for a random elemen t in a tree con taining n el-emen ts is log m +1 ( n ), a substan tial impro vemen t over the log 2 ( n ) cac he misses caused by a na X ve binary searc h. Here, m + 1 is the branc hing factor of the CSS-tree, i.e., m is the num ber of keys that may be stored in a single cac he line (usually 8 or 16).

In a follo w-up pap er, Rao and Ross [17] sho w how the cac he beha vior of B + -trees can be impro ved by eliminating child pointers and by storing all child nodes of a given inner node in a con tiguous fashion. Compared to CSS-trees, their CSB + -trees (\cac he sensitiv e B + -trees") have the adv antage that they supp ort ecien t update operations (the con ten ts of a CSS-tree are assumed to be static).
 Cui et al. [5] prop ose BD-trees, a hash-based alternativ e to CSS-trees and CSB + -trees. Hankins and Patel [10] explore how the node size of a CSB + -tree a ects its cac he misses, TLB misses, and consequen tly its lookup performance.
Similar techniques are also applied in di eren t areas of high-p erformance computing. Xiao et al. [23], for in-stance, sho w how the performance of Mer gesor t can be impro ved by emplo ying tiling and padding to partition the data in a way that better re ects the structure of the cac he. Kowarsc hik and Wei [12] presen t an overview of of cac he optimization techniques in the con text of numerical algorithms.
In the remainder of the pap er, we investigate various list structures that can be used to store the postings lists found in an in-memory inverted index. These structures are eval-uated through performance measuremen ts on realistic sets of data. The general pro cedure of our exp erimen ts is as follo ws: 1. An inverted index (do cid index) is built from a given 2. At random, a term T and a documen t D are chosen. 3. A searc h on T 's postings list is performed, returning For eac h exp erimen t, in order to obtain reliable performance gures, steps 2 and 3 are rep eated sev eral million times for random pairs ( T , D ). The exp erimen t as a whole is re-peated six times, and the lowest time measured is rep orted, follo wing the assumption that any deviations from this best run is due to bac kground system activit y by other pro cesses. The performance num bers presen ted in this pap er only in-clude the time necessary to nd D 0 in T 's postings list, not the time it tak es to pro duce the random pairs ( T , D ), nor the time required to nd T 's entry in the dictionary data structure (for example through a hash table lookup).
In our exp erimen ts, the index is built from the text collec-tion kno wn as TREC disks 1{5 . The collection has a total size of 4.8 GB and consists of 2.1 million distinct terms in 1.5 million documen ts. For eac h term in the collection, we construct a list of docids | integers specifying the set of documen ts the term app ears in. We remo ve from the index all terms that app ear in less than 2 14 = 16,384 documen ts, since for suc h short postings lists the value of random access exp erimen ts would have been rather limited. The result-ing index con tains 3,408 terms with a total of 244 million postings (median list length: 36,003). The total size of the uncompressed index is appro ximately 1 GB, with small vari-ations dep ending on the speci c index structure chosen.
To sho w the general applicabilit y of the techniques pro-posed in this pap er, all metho ds are evaluated on a variet y of di eren t computer systems, purc hased over the last sev en years. An overview of these systems is given by Table 1.
With the exception of the Pentium III, all systems are endued with sucien tly much main memory to load the entire docid index for the TREC collection into RAM. In the exp erimen ts with the Pentium III system, because it variable and dep ends greatly on the memory access pattern. only con tains 768 MB of RAM, we chose to use a sligh tly smaller index instead, con taining postings only for the rst 1 million documen ts in the collection. This explains why, in our exp erimen ts, the Pentium III exhibits better index per-formance than the Athlon MP , despite the Athlon's higher CPU frequency .

In addition to average lookup latency (measured in nanoseconds per random index access), we also rep ort some auxiliary performance measures, suc h as the num ber of L2 cac he misses and TLB misses caused by a single index access operation (on average). All suc h num bers were ob-tained from the built-in performance coun ters of the AMD Opteron CPU used in our exp erimen ts. To access these performance coun ters, we used the oprofile 1 Lin ux kernel mo dule (version 0.9.2 for kernel 2.6.20) and its event types L2 CACHE MISS and L1 DTLB AND L2 DTLB MISS .
When aiming for ecien t random access into the postings lists stored in an inverted le, the most obvious implemen-tation is to store eac h inverted list in uncompressed form (e.g., every posting as a 32-bit integer) and to emplo y a bi-nary searc h algorithm to nd a given docid in the postings list for a given term T .

We implemen ted this index structure and conducted the exp erimen t describ ed in Section 3, using the 5-GB TREC collection. In total, for eac h exp erimen t, 10 million binary searc h operations were performed. Eac h exp erimen t was rep eated six times.

The best-p erforming run from eac h exp erimen t is rep orted in Table 2. It can be seen that a random list accessed, real-ized through binary searc h, tak es surprisingly long. On the Opteron system, for example, the average latency of a searc h operation is 953 ns. Since the median list in the index com-prises 36,003 postings (cf. Section 3), binary searc h should require no more than 16 comparison operations on average to nd the desired list elemen t (2 16 = 65,536). Hence, ac-cording to the results of our exp erimen ts, eac h comparison tak es appro ximately 60 ns, or 60 2 : 8 = 168 CPU clock cycles.

Of course, this rather low performance is not caused by the CPU actually executing 168 operations per comparison, 9 1 http://oprofile.sourcefor ge.net/ (accessed 2007-05-19) AMD Opteron 953 ns 849 ns Intel Core 1,403 ns 1,325 ns AMD Athlon MP 3,061 ns 2,775 ns Intel Pentium III 2,642 ns 2,557 ns L2 cac he misses 24.0 13.7 TLB misses 11.8 7.3 Total index size 930 MB 930 MB Table 2: Average random index access performance with an uncompressed index. Interp olation searc h fares sligh tly better than binary searc h, but on av-erage still requires almost 1 s (2,800 clock cycles) per random access on the Opteron system. but is caused by the poor cac he beha vior of binary searc h. Supp ose the CPU cac he line size is 64 bytes ( b = 16 postings). Then a binary searc h on a given postings list of length n will access roughly log 2 ( n= 16) 12 di eren t cac he lines and thus cause 12 data cac he misses. Similarly , appro ximately log 2 ( n= 1024) 6 di eren t pages are accessed, resulting in 6 TLB misses (assuming n 2 16 ).

While the actual num ber of L2 misses and TLB misses rep orted by oprofile and sho wn in the table is somewhat higher than exp ected (almost exactly twice as high, sug-gesting that something migh t be wrong with the version of oprofile that we used or with the way in whic h we used it), it con rms the suspicion that the low performance of binary searc h is caused by its extremely low cac he eciency .
Emplo ying a more sophisticated searc h pro cedure, suc h as interp olation searc h [15], whic h requires few er access into the list being searc hed, can impro ve the performance of random index access operations sligh tly, as sho wn in Ta-ble 2. However, the impro vemen t is not very dramatic, less than 15% for the four systems we tested, mainly because the speedup achiev ed by more benign cac he beha vior is partially o set by the greater computational complexit y of eac h step in interp olation searc h.
 CSS-T rees It is clear that, in order to realize low-latency random index access, a di eren t arrangemen t of the in-memory postings lists needs to be emplo yed that allo ws the searc h engine to CSS-tree node size 32 bytes 64 bytes 128 bytes AMD Opteron 524 ns 478 ns 488 ns Intel Core 868 ns 758 ns 735 ns AMD Athlon MP 1,827 ns 1,588 ns 1,584 ns Intel Pentium III 1,652 ns 1,529 ns 1,640 ns L2 cac he misses 6.0 4.1 6.6 TLB misses 3.6 3.0 2.7 Total index size 945 MB 934 MB 931 MB Table 3: Random index access performance with CSS-trees (uncompressed). retriev e a random posting with a smaller num ber of cac he misses. As discussed in Section 2, Rao's CSS-tree [16] is suc h a data structure.

We implemen ted an inverted index data structure in whic h eac h postings list is not stored as a at arra y of integers, but inside a CSS-tree. We then rep eated the exp erimen t from before, picking random pairs ( T; D ) and performing a searc h for D in T 's postings list.

The results we obtained for various CSS-tree node sizes (32 bytes, 64 bytes, 128 bytes) are sho wn in Table 3. The variations in the index size (greater node size results in smaller index) are due to a sligh t deviation of our imple-men tation from Rao's originally prop osed tree structure. To simplify the implemen tation, and to allo w eac h tree to be constructed in a bottom-up fashion, our varian t of the CSS-tree replicates tree nodes in the upp er levels of the tree (but not in the two bottom-most levels). That is, the k -th key in a node at one of the higher levels of the tree is a cop y of the rst key in the node's k -th child node (in Rao's implemen-tation, the k -th key in a node would numerically be between the keys in the k -th child and the ( k + 1)-st child). This de-viation has the e ect of increasing the storage requiremen ts of eac h tree by by up to 1.5% for 32-b yte tree nodes (up to 0.4% for 64-b yte tree nodes). However, because the di er-ence is so small, our results still re ect the true performance of CSS-trees on this kind of searc h operation.

Compared to na X ve binary searc h, our implemen tation of the CSS-tree can pro cess a request for a random posting between 73% (Pentium III) and 99% (Opteron) faster (com-paring Table 3 with Table 2). The performance gain stems from a greatly reduced num ber of cac he misses.

For a node size of 64 bytes, whic h is the same as the cac he line size of the Opteron CPU, the average num ber of L2 cac he misses per index access is reduced from 24 to 4.1. What is interesting, however, is that the best results are not alw ays achiev ed for a node size equal to the cac he line size of the CPU. Both the Intel Core and the AMD Athlon MP have 64-b yte cac he lines, but performance is best for a node size of 128 bytes. Similarly , the Pentium III has 32-b yte cac he lines, but exhibits the lowest latency for 64-b yte tree nodes.

The reason for this discrepancy lies in the num ber of TLB misses caused by the random access operations. On the Opteron system, for example, although the num ber of L2 cac he misses is minimized by choosing a node size of 64 bytes, TLB misses are smaller for a node size of 128 bytes. Because TLB misses are far more costly than simple data cac he misses (cf. Table 1), this a ects the lookup perfor-mance of CSS-trees, whic h were designed with the single ob-jectiv e of minimizing data cac he misses, ignoring the TLB.
In order to achiev e optimal random access performance, it seems reasonable to accoun t for both sources of random access latency: L2 cac he misses and TLB misses.
 CPSS-T rees The exp erience we made with CSS-trees motiv ated the de-sign of a new data structure, the CPSS-tree (\cac he/page-sensitiv e searc h tree"). A CPSS-tree is similar to a CSS-tree. However, instead of just minimizing the num ber of cac he lines accessed in a searc h operation, a CPSS-tree also considers the cost asso ciated with a TLB miss. Its primary objectiv e is to minimize the num ber of di eren t pages ac-cessed by a searc h operation (TLB misses). Its secondary goal is to minimize the num ber of cac he lines accessed within eac h page (data cac he misses).

A single node in a CPSS-tree occupies an entire memory page (typically 4 KB), whereas a CSS-tree node occupies just a single cac he line (32 or 64 bytes). If used to store 32-bit docids, the num ber of TLB misses (= num ber of pages touc hed) caused by a searc h operation on a list of n post-ings is then appro ximately log 1024 ( n ), because eac h page can store up to 1,024 postings. Compared to the asymp-totical num ber of TLB misses caused by a CSS-tree with 64-b yte nodes ( log 17 ( n ), because eac h node holds up to 16 keys and has up to 17 children), this constitutes a ma jor impro vemen t.

In order to minimize the num ber of cac he misses per page access, the postings within eac h CPSS-tree node are ar-ranged inside a local CSS-tree. The heigh t of this tree de-pends on the node size in the CSS-trees stored within eac h node of the CPSS-tree. A node size of 32 bytes, leading to a branc hing factor of 32 = 4 + 1 = 9, results in a tree of heigh t 3 (i.e., at most 4 cac he misses per page accessed). With 64-byte CSS nodes, the CSS-tree inside any given CPSS node has heigh t 2 (up to 3 cac he misses per TLB miss).
The general structure of a CPSS-tree is not very di eren t from that of a cac he-sensitiv e BD-tree [5], a hash-based in-dexing structure devised for similar purp oses. Compared to BD-trees, however, CPSS-trees have the adv antage that they allo w more ecien t traversals and range queries. Moreo ver, because keys are not stored in strictly increasing order in a BD-tree, CPSS-trees are better suited for standard query pro cessing tasks (where postings are pro cessed in sequen tial fashion) and more amenable to index compression, whic h is what we are aiming for.
 An example of a fragmen t of a CPSS-tree is sho wn in Figure 4. In the example, eac h key (= posting) consumes 4 bytes, a cac he line holds 64 bytes (i.e., 16 keys per CSS-node), and eac h memory page consists of 4,096 bytes (i.e., 64 CSS-no des per CPSS-no de). The pointers in the example are all implicit pointers. That is, the address of a child node is obtained by arithmetic operations (possible because all nodes are stored in memory in a con tiguous fashion) instead of follo wing explicit memory references.

For storage eciency reasons, our implemen tation does not require the root node of a CPSS-tree to occup y an en-tire memory page. Whenev er the root node consumed few er than 5 cac he lines, it was stored as a simple CSS-tree, di-rectly in the inverted index's dictionary data structure. Ob-viously , in that case, an explicit pointer was needed to the rst child node of the CPSS root node. line size for optimal performance). All child pointers are implicit. Table 4: Random index access performance with CPSS-trees (uncompressed; node size: 4 KB). The num ber of L2 cac he misses is higher than with CSS-trees, but overall performance is better, thanks to a lower num ber of TLB misses per index operation.

The performance results we obtained with our implemen-tation of the CPSS-tree are sho wn in Table 4. Compared to CSS-trees (cf. Table 3), the num ber of L2 cac he misses is increased, but the num ber of TLB misses is decreased. Because of the great cost of TLB misses compared to data cac he misses, overall performance is increased by 15% (Athlon MP), 19% (Opteron), and 21% (Core and Pentium III), resp ectiv ely. The total size of the CPSS-tree-based inverted index is sligh tly higher than that of the CSS-tree-based one (+2.5% for a node size of 64 bytes), due to internal fragmen tation in the relativ ely large CPSS nodes of eac h postings list.

Best results are obtained if the size of eac h CPSS node is chosen to re ect the system's page size and the size of the CSS nodes within eac h CPSS node is chosen to equal the cac he line size of the CPU cac he. For the Pentium III system, for example, 32-b yte nodes (and not 64-b yte nodes, as in the case of CSS-trees) lead to optimal performance.
Existing index compression techniques can be applied to the postings stored in a CPSS-tree in a straigh tforw ard man-ner. The overall structure of the tree remains the same, but postings within eac h leaf node are stored in compressed form instead of uncompressed.

Using vByte [19] (byte-aligned gap enco ding) to compress the postings sequence found in eac h leaf node, the total size of the index can be reduced by up to 73% compared to an uncompressed index in whic h eac h posting consumes 32 bits. Consequen tly, the CPU can mak e better use of its cac hes. The num ber of L2 misses per searc h operation is reduced by 1.1, from 5.9 to 4.8; the num ber of TLB misses is reduced by 0.2, to 1.2 TLB misses per random index access (cf. Table 5, column \0").
 For two of the systems used in our exp erimen ts, Athlon MP and Pentium III, this impro vemen t is already sucien t to impro ve the index's random access performance, from 1,374 to 1,294 ns in the case of the Athlon, and from 1,265 to 1,145 ns in the case of the Pentium III. For the two other systems, however, random access latency is higher with the compressed index than with the uncompressed index. The reason for this is the increased computational load asso ci-ated with decompressing the postings in eac h leaf node.
According to Table 1, decompressing a single vByte-enco ded posting on average tak es about 2 ns on the Opteron. 4 KB (one page). CSS node size: 64 bytes (one cac he line). Sync pts. per leaf 0 1 2 AMD Opteron 400 ns 362 ns 360 ns Intel Core 612 ns 551 ns 549 ns AMD Athlon MP 1,294 ns 1,206 ns 1,194 ns Intel Pentium III 1,145 ns 1,125 ns 1,157 ns L2 cac he misses 4.8 4.8 4.9 TLB misses 1.2 1.2 1.2 Total index size 262 MB 279 MB 298 MB Table 5: Random index access performance with compressed CPSS-trees (vByte). Node size = cac he line size (32 bytes for the Pentium III; 64 bytes for the other systems). The num ber of sync h. points per compressed leaf node is varied between 0 and 2. A leaf node, on average, con tains around 60 compressed postings. We may exp ect that 50% of these need to be decompressed in order to nd the desired posting, leading to an overhead of about 60 ns per searc h operation and having the e ect that the increased cac he eciency does not immediately lead to better random index access.
Fortunately , reducing the decompression overhead is rel-ativ ely simple. Eac h leaf node in the CPSS-tree is aug-men ted with a sequence of sync hronization points (cf. Sec-tion 2). Eac h sync hronization point consists of an uncom-pressed posting (32-bit integer) and an 8-bit integer spec-ifying the position of the compressed posting immediately follo wing the given sync hronization point. The sequence of sync hronization points is stored at the beginning of eac h leaf node (= CPU cac he line), follo wed by a sequence of com-pressed postings.

By adding a single sync hronization point to eac h leaf node in the searc h tree, the decompression overhead can be re-duced by 50%. By adding a second sync hronization point, a further 33% reduction is possible. Table 5 sho ws the per-formance impro vemen ts measured in our exp erimen ts. For the Opteron PC, random access latency can be reduced by 10%, from 401 ns to 360 ns. For the other systems, a simi-lar speedup can be achiev ed (Core: -10%; Athlon MP: -13%; Pentium III: -11%).

Inserting sync hronization points into the leaf nodes in-creases the size of the index, by 14% in the case of two sync hronization points per node. However, compared to an uncompressed index, the resulting index still is relativ ely compact (-69%, compared to uncompressed 32-bit post-ings). Figure 5 sho ws various space-time trade-o points for the Opteron system with 64-b yte cac he lines. Other Compression Metho ds Obviously , the metho d describ ed above does not only work with vByte, but also with other compression metho ds. How-ever, the num ber of compression metho ds that can e ec-tively be used to decrease the random access latency is fairly limited, as their deco ding throughput needs to be extremely high, even with additional sync hronization points in the leaf nodes of eac h CPSS-tree. We tested two metho ds that meet this requiremen t:
The results we obtained with these two metho ds (and our original implemen tation based on vByte) on the Opteron system are sho wn in Figure 6. Both metho ds, Rice and Simple-9, achiev e far better compression rates than vByte. However, Rice's relativ ely high deco ding overhead ( 5 ns per posting) leads to an average random access latency that is almost twice as high as that of vByte. However, by in-serting 5 sync hronization points into eac h leaf node, Rice's random access performance can be reduced to under 400 ns. Remark ably , even with this man y sync hronization points, the total size of the index is still below what we see when vByte is used (237 MB vs. 262 MB).

With Simple-9, the situation is di eren t. While the metho d does not achiev e the same compression rates as Rice coding, it has the adv antage of extremely low-latency deco ding operations. Without any sync hronization points, Simple-9's average random access latency is 406 ns, only AMD Opteron (4-KB pages, 64-b yte cac he lines). 6 ns more than vByte in the same con guration. With 3 sync hronization points per leaf node, Simple-9 achiev es a random access latency of 351 ns. This is lower than what is attainable by using vByte. The reason for Simple-9's dom-inance over vByte (smaller index and lower index access latency at the same time) is that its more compact index structure results in few er cac he misses | an e ect that out weighs the sligh tly increased deco ding overhead.
Text retriev al systems can be distinguished based on whether they keep their index structures in main memory or on disk. A searc h engine's index access pattern, on the other hand, can be classi ed according to whether postings lists are accessed in a random fashion or largely sequen tially .
It has long been kno wn that index compression can impro ve the performance of on-disk indices, regardless of whether they are accessed in a random fashion or sequen-tially [22]. In this pap er, we have sho wn that the random access performance of in-memory indices can also be im-pro ved by applying index compression techniques. The last remaining question, therefore, is whether index compres-sion may help increase the performance of sequential list operations on in-memory inverted les.

At this point, it is not clear whether this is the case. How-ever, if the performance gap between CPU and main mem-ory keeps widening, it migh t in fact be possible that index compression, because it allo ws the CPU to mak e better use of memory bandwidth, also leads to impro ved performance for sequen tial operations on in-memory indices. Novel in-dex compression metho ds like Zuk owski's PFOR and PFOR-delta [26], taking the sup er-scalar nature of mo dern CPUs into accoun t, seem like the most promising candidates. [1] V. N. Anh and A. Mo at. Inverted Index Compression [2] E. A. Brew er. Com bining Systems and Databases: A [3] D. Burger, J. R. Goodman, and G. S. Sohi. Memory [4] S. B X  uttc her and C. L. A. Clark e. Unaligned Binary [5] B. Cui, B. C. Ooi, J. Su, and K.-L. Tan. Main [6] E. D. Demaine, A. Lopez-Ortiz, and J. I. Munro. [7] E. D. Demaine, A. Lopez-Ortiz, and J. I. Munro. [8] A. S. Fraenk el and S. T. Klein. Novel Compression of [9] S. W. Golom b. Run-Length Enco dings. IEEE [10] R. A. Hankins and J. M. Patel. E ect of Node Size on [11] S. Heman. Sup er-Scalar Database Compression [12] M. Kowarsc hik and C. Wei. An Overview of Cac he [13] A. Mo at and L. Stuiv er. Binary Interp olativ e Coding [14] A. Mo at and J. Zob el. Self-Indexing Inverted Files [15] Y. Perl, A. Itai, and H. Avni. Interp olation Searc h { A [16] J. Rao and K. A. Ross. Cac he Conscious Indexing for [17] J. Rao and K. A. Ross. Making B + -Trees Cac he [18] C. Ruemmler and J. Wilk es. An Introduction to Disk [19] F. Scholer, H. E. Williams, J. Yiannis, and J. Zob el. [20] A. J. Smith. Bibliograph y and Reading on CPU Cac he [21] H. Turtle and J. Flo od. Query Evaluation: Strategies [22] H. E. Williams and J. Zob el. Compressing Integers for [23] L. Xiao, X. Zhang, and S. A. Kubric ht. Impro ving [24] J. Zob el and A. Mo at. Adding Compression to a [25] J. Zob el and A. Mo at. Inverted Files for Text Searc h [26] M. Zuk owski, S. Heman, N. Nes, and P. A. Boncz.
