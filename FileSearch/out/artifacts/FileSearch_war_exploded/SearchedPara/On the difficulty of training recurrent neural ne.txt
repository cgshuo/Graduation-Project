 Razvan Pascanu pascanur@iro.umontreal.ca Tomas Mikolov t.mikolov@gmail.com Speech@FIT, Brno University of Technology, Brno, Czech Republic Yoshua Bengio yoshua.bengio@umontreal.ca A recurrent neural network (RNN), e.g. Fig. 1, is a neural network model proposed in the 80 X  X  (Rumelhart et al. , 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the dis-tinction that we allow connections among hidden units associated with a time delay. Through these connec-tions the model can retain information about the past, enabling it to discover temporal correlations between events that are far away from each other in the data. While in principle the recurrent network is a simple and powerful model, in practice, it is hard to train properly. Among the main reasons why this model is so unwieldy are the vanishing gradient and exploding gradient problems described in Bengio et al. (1994). 1.1. Training recurrent networks A generic recurrent neural network, with input u t and state x t for time step t , is given by: In the theoretical section of this paper we will some-In this case, the parameters of the model are given by the recurrent weight matrix W rec , the bias b and input weight matrix W in , collected in  X  for the gen-eral case. x 0 is provided by the user, set to zero or learned, and  X  is an element-wise function. A cost E = P 1  X  t  X  T E t measures the performance of the net-work on some given task, where E t = L ( x t ). One approach for computing the necessary gradients is backpropagation through time (BPTT), where the re-current model is represented as a multi-layer one (with an unbounded number of layers) and backpropagation is applied on the unrolled model (see Fig. 2). We will diverge from the classical BPTT equations at this point and re-write the gradients in order to better highlight the exploding gradients problem:  X  x t  X  x k These equations were obtained by writing the gradi-ents in a sum-of-products form.  X  + x k  X  X mmediate X  partial derivative 2 of the state x k with respect to  X  , where x k  X  1 is taken as a constant with respect to  X  . Specifically, considering eq. 2, the value of any row i of the matrix (  X  + x k Eq. (5) also provides the form of Jacobian matrix where diag converts a vector into a diagonal matrix, and  X  0 computes element-wise the derivative of  X  . Any gradient component  X  E t (4)), whose terms we refer to as temporal contribu-tions or temporal components. One can see that each such temporal contribution  X  E t  X  at step k affects the cost at step t &gt; k . The fac-step t back to step k . We would further loosely distin-guish between long term and short term contributions, where long term refers to components for which k t and short term to everything else. Introduced in Bengio et al. (1994), the exploding gra-dients problem refers to the large increase in the norm of the gradient during training. Such events are due to the explosion of the long term components, which can grow exponentially more than short term ones. The vanishing gradients problem refers to the opposite be-haviour, when long term components go exponentially fast to norm 0, making it impossible for the model to learn correlation between temporally distant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component, and in particular at the matrix factors  X  x t of a product of t  X  k Jacobian matrices. In the same way a product of t  X  k real numbers can shrink to zero or explode to infinity, so does this product of matrices (along some direction v ).
 In what follows we will try to formalize these intuitions (extending similar derivations done in Bengio et al. (1994) where a single hidden unit case was considered). If we consider a linear version of the model (i.e. set  X  to the identity function in eq. (2)) we can use the power iteration method to formally analyze this product of Jacobian matrices and obtain tight conditions for when the gradients explode or vanish (see the supplementary materials). It is sufficient for  X  &lt; 1, where  X  is the spectral radius of the recurrent weight matrix W rec , for long term components to vanish (as t  X   X  ) and necessary for  X  &gt; 1 for them to explode.
 We generalize this result for nonlinear functions  X  where |  X  0 ( x ) | is bounded, k diag (  X  0 ( x k )) k  X   X   X  R , by relying on singular values.
 We first prove that it is sufficient for  X  1 &lt; 1  X  1 is the largest singular value of W rec , for the vanish-ing gradient problem to occur. Note that we assume the parametrization given by eq. (2). The Jacobian norm of this Jacobian is bounded by the product of the norms of the two matrices (see eq. (6)). Due to our assumption, this implies that it is smaller than 1.  X  k, Let  X   X  R be such that  X  k,  X  x k +1 existence of  X  is given by eq. (6). By induction over i , we can show that As  X  &lt; 1, it follows that, according to eq. (7), long term contributions (for which t  X  k is large) go to 0 exponentially fast with t  X  k .
 By inverting this proof we get the necessary condition for exploding gradients , namely that the largest singu-lar value  X  1 is larger than 1 components would vanish instead of exploding). For tanh we have  X  = 1 while for sigmoid we have  X  = 1 / 4 . 2.2. Drawing similarities with dynamical We can improve our understanding of the exploding gradients and vanishing gradients problems by employ-ing a dynamical systems perspective, as it was done before in Doya (1993); Bengio et al. (1993).
 We recommend reading Strogatz (1994) for a formal and detailed treatment of dynamical systems theory. For any parameter assignment  X  , depending on the ini-tial state x 0 , the state x t of an autonomous dynamical system converges, under the repeated application of the map F , to one of several possible different attrac-tor states. The model could also find itself in a chaotic regime, a case in which some of the following observa-tions may not hold, but that is not treated in depth here. Attractors describe the asymptotic behaviour of the model. The state space is divided into basins of at-traction, one for each attractor. If the model is started in one basin of attraction, the model will converge to the corresponding attractor as t grows.
 Dynamical systems theory says that as  X  changes, the asymptotic behaviour changes smoothly almost every-where except for certain crucial points where drastic changes occur (the new asymptotic behaviour ceases to be topologically equivalent to the old one). These points form bifurcation boundaries and are caused by attractors that appear, disappear or change shape. Doya (1993) hypothesizes that such bifurcation cross-ings could cause the gradients to explode. We would like to extend this observation into a sufficient condi-tion for gradients to explode, by addressing the issue of crossing boundaries between basins of attraction. We argue that bifurcations are global events that can have locally no effect and therefore crossing them is neither sufficient nor necessary for the gradients to ex-plode. For example due to a bifurcation one attractor can disappear, but if the model X  X  state is not found in the basin of attraction of said attractor, learning will not be affected by it. However when either a change in the state or in the position of the boundary between basins of attraction (which can be caused by a change in  X  ) is such that the model falls in a different basin than before, the state will be attracted in a different direction resulting in the explosion of the gradients. This crossing of boundaries between basins of attrac-tion is a local event and it is sufficient for the gradients to explode. By assuming that crossing into an emerg-ing attractor or from a disappearing one qualifies as crossing a boundary between attractors, this term en-capsulates also the observations from Doya (1993). We will re-use the one-hidden unit model (and plot) from Doya (1993) (see Fig. 3) to depict our exten-sion, though , as the original hypothesis, our extension does not depend on the dimensionality of the model. The x-axis covers the parameter b and the y-axis the asymptotic state x  X  . The bold line follows the move-ment of the final point attractor, x  X  , as b changes. At b 1 we have a bifurcation boundary where a new at-tractor emerges (when b decreases from  X  ), while at b 2 we have another that results in the disappearance of one of the two attractors. In the interval ( b we are in a rich regime, where there are two attractors and the change in position of boundary between them, as we change b , is traced out by a dashed line. The gray dashed arrows describe the evolution of the state x if the network is initialized in that region. Cross-ing the boundary between basins of attractions is de-picted with unfilled circles, where a small change in the state at time 0 results in a sudden large change in x . Crossing a bifurcation ((Doya, 1993) original hy-pothesis) is shown with filled circles. Note how in the figure, there are only two values of b with a bifurca-tion, but a whole range of values for which there can be a boundary crossing.
 Another limitation of previous analysis is that it con-siders autonomous systems and assumes the observa-tions hold for input-driven models as well. In (Ben-gio et al. , 1994) input is dealt with by assuming it is bounded noise. The downside of this approach is that it limits how one can reason about the input. In practice, the input is supposed to drive the dynamical system, being able to leave the model in some attrac-tor state, or kick it out of the basin of attraction when certain triggering patterns present themselves. We propose to extend our analysis to input driven models by folding the input into the map. We con-sider the family of maps F , where we apply a different F t at each step. Intuitively, for the gradients to ex-plode we require the same behaviour as before, where (at least in some direction) the maps F 1 ,..,F t agree and change direction. Fig. 4 describes this behaviour. For the specific parametrization in eq. (2) we can take the analogy one step further by decomposing the maps F t into a fixed map F ( x ) = W rec  X  ( x ) + b corresponds to an input-less recurrent network, while U t ( x ) = x + W in u t describes the effect of the input. This is depicted in in Fig. 5. Since U t changes with time, it can not be analyzed us-ing standard dynamical systems tools, but  X  F can. This means that when a boundary between basins of attrac-tions is crossed for  X  F , the state will move towards a different attractor, which for large t could lead (un-less the input maps U t are opposing this) to a large discrepancy in x t . Therefore studying the asymptotic behaviour of  X  F can provide useful information about where such events are likely to happen.
 One interesting observation from the dynamical sys-tems perspective with respect to vanishing gradients is the following. If the factors  X  x t large), it means that x t does not depend on x k (if we change x k by some  X , x t stays the same). This translates into the model at x t being close to conver-gence towards some attractor (which it would reach from anywhere in the neighbourhood of x k ). Therefore avoiding the vanishing gradient means staying close to the boundaries between basins of attractions. 2.3. The geometrical interpretation Let us consider a simple one hidden unit model (eq. (8)) where we provide an initial state x 0 and train the model to have a specific target value after 50 steps. Note that for simplicity we assume no input. Fig. 6 shows the error surface E 50 = (  X  ( x 50 )  X  0 . 7) 2 , where x 0 = . 5 and  X  to be the sigmoid function. We can easily analyze the behavior of the model in the linear case, with b = 0, i.e. x t = x 0 w t . We have that that when the first derivative explodes, so does the second derivative.
 In the general case, when the gradients explode they do so along some directions v . There exists, in such situations, a v such that  X  E t R and  X  &gt; 1 (for e.g., in the linear case, v is the eigenvector corresponding to the largest eigenvalue of W rec ). If this bound is tight, we hypothesize that when gradients explode so does the curvature along v , leading to a wall in the error surface , like the one seen in Fig. 6. Based on this hypothesis we devise a simple solution to the exploding gradient problem (see Fig 6). If both the gradient and the leading eigenvector of the curvature are aligned with the exploding direction v , it follows that the error surface has a steep wall perpen-dicular to v (and consequently to the gradient). This means that when stochastic gradient descent (SGD) reaches the wall and does a gradient descent step, it will be forced to jump across the valley moving perpen-dicular to the steep walls, possibly leaving the valley and disrupting the learning process.
 The dashed arrows in Fig. 6 correspond to ignoring the norm of this large step , ensuring that the model stays close to the wall. One key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction. At the wall, a bounded norm step in the direction of the gradient therefore merely pushes us back inside the smoother low-curvature region besides the wall, whereas a regu-lar gradient step would bring us very far, thus slowing or preventing further training.
 The important assumption in this scenario, compared to the classical high curvature valley, is that we assume that the valley is wide, as we have a large region around the wall where if we land we can rely on first order methods to move towards a local minima. The effec-tiveness of clipping provides indirect evidence, as oth-erwise, even with clipping, SGD would not be able to explore descent directions of low curvature and there-fore further minimize the error.
 Our hypothesis, when it holds, could also provide an-other argument in favor of the Hessian-Free approach compared to other second order methods (among other existing arguments). Hessian-Free, and truncated Newton methods in general, compute a new estimate of the Hessian matrix before each update step and can take into account abrupt changes in curvature (such as the ones suggested by our hypothesis) while other ap-proaches use a smoothness assumption, averaging 2nd order signals over many steps. 3.1. Previous solutions Using an L1 or L2 penalty on the recurrent weights can help with exploding gradients. Assuming weights are initialized to small values, the largest singular value  X  of W rec is probably smaller than 1. The L1/L2 term can ensure that during training  X  1 stays smaller than 1, and in this regime gradients can not explode (see sec. 2.1). This approach limits the model to single point attractor at the origin, where any information inserted in the model dies out exponentially fast. This prevents the model to learn generator networks, nor can it exhibit long term memory traces.
 Doya (1993) proposes to pre-program the model (to initialize the model in the right regime) or to use teacher forcing . The first proposal assumes that if the model exhibits from the beginning the same kind of asymptotic behaviour as the one required by the tar-get, then there is no need to cross a bifurcation bound-ary. The downside is that one can not always know the required asymptotic behaviour, and, even if it is known, it might not be trivial to initialize the model accordingly. Also, such initialization does not prevent crossing the boundary between basins of attraction. Teacher forcing refers to using targets for some or all hidden units. When computing the state at time t , we use the targets at t  X  1 as the value of all the hid-den units in x t  X  1 which have a target defined. It has been shown that in practice it can reduce the chance that gradients explode, and even allow training gen-erator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991). One important downside is that it requires a target to be defined at every time step. Hochreiter and Schmidhuber (1997); Graves et al. (2009) propose the LSTM model to deal with the van-ishing gradients problem. It relies on special type of linear unit with a self connection of value 1. The flow of information into and out of the unit is guarded by learned input and output gates. There are several vari-ations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. (2011) use the Hessian-Free optimizer in conjunction with structural damping . This approach seems able to address the vanishing gradient problem, though more detailed analysis is missing. Presumably this method works because in high dimensional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice, one can not guarantee that this property holds. The method addresses the exploding gradient as well, as it takes curvature into account. Structural damping is an enhancement that forces the Jacobian with the exploding gradients problem. The need for this extra term when solving the pathological prob-lems might suggest that second order derivatives do not always grow at same rate as first order ones. Echo State Networks (Jaeger and Haas, 2004) avoid the exploding and vanishing gradients problem by not learning W rec and W in . They are sampled from hand crafted distributions. Because the spectral radius of W rec is, by construction, smaller than 1, information fed in to the model dies out exponentially fast. An extension to the model is given by leaky integration units (Jaeger et al. , 2007), where x These units can be used to solve the standard bench-mark proposed by Hochreiter and Schmidhuber (1997) for learning long term dependencies (Jaeger, 2012). We would make a final note about the approach pro-posed by Tomas Mikolov in his PhD thesis (Mikolov, 2012)(and implicitly used in the state of the art re-sults on language modelling (Mikolov et al. , 2011)). It involves clipping the gradient X  X  temporal components element-wise (clipping an entry when it exceeds in ab-solute value a fixed threshold). 3.2. Scaling down the gradients As suggested in section 2.3, one mechanism to deal with the exploding gradient problem is to rescale their norm whenever it goes over a threshold: Algorithm 1 Pseudo-code for norm clipping if k  X  g k X  threshold then end if This algorithm is similar to the one proposed by Tomas Mikolov and we only diverged from the original pro-posal in an attempt to provide a better theoretical jus-tification (see section 2.3; we also move in a descent direction for the current mini-batch). We regard this work as an investigation of why clipping works. In practice both variants behave similarly.
 The proposed clipping is simple and computationally efficient, but it does however introduce an additional hyper-parameter, namely the threshold. One good heuristic for setting this threshold is to look at statis-tics on the average norm over a sufficiently large num-ber of updates. In our experience values from half to ten times this average can still yield convergence, though convergence speed can be affected. 3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us-ing a regularization term that represents a preference for parameter values such that back-propagated gra-dients neither increase or decrease in magnitude. Our intuition is that increasing the norm of  X  x t error at time t is more sensitive to all inputs u t ,.., u ( puts will be irrelevant for the prediction at time t and will behave like noise that the network needs to learn to ignore. The network can not learn to ignore these irrelevant inputs unless there is an error signal. These two issues can not be solved in parallel, and it seems natural to expect that we might need to force the net-work to increase k  X  x t (caused by the irrelevant input entries) and then wait for it to learn to ignore these input entries. This sug-gests that moving towards increasing the norm of  X  x t can not be always done while following a descent di-rection of the error E (which is, for e.g., what a second order method would do), and a more natural choice might be a regularization term.
 The regularizer we propose prefers solutions for which the error preserves norm as it travels back in time: In order to be computationally efficient, we only use the  X  X mmediate X  partial derivative of  X  with respect to W with respect to W rec when computing the derivative of  X  k ), as depicted in eq. (10). This can be done effi-ciently because we get the values of  X  E We use Theano to compute these gradients (Bergstra et al. , 2010; Bastien et al. , 2012). Note that our regularization term only forces the Ja-direction of the error  X  E do not enforce that all eigenvalues are close to 1). The second observation is that we are using a soft con-straint, therefore we are not ensured the norm of the error signal is preserved. This means that we still need to deal with the exploding gradient problem, as a sin-gle step induced by it can disrupt learning. From the dynamical systems perspective we can see that pre-venting the vanishing gradient problem implies that we are pushing the model towards the boundary of the current basin of attraction (such that during the N steps it does not have time to converge), making it more probable for the gradients to explode. 4.1. Pathological synthetic problems As done in Martens and Sutskever (2011), we address some of the pathological problems from Hochreiter and Schmidhuber (1997) that require learning long term correlations. We refer the reader to this original pa-per for a detailed description of the tasks and to the supplementary materials for the complete description 4.1.1. The temporal order problem We consider the temporal order problem as the pro-totypical pathological problem, extending our results to the other proposed tasks afterwards. The input is a long stream of discrete symbols. At the beginning and middle of the sequence a symbol within { A,B } is emitted. The task consists in classifying the order (either AA,AB,BA,BB ) at the end of the sequence. Fig. 7 shows the success rate of standard mini-batch stochastic gradient descent MSGD, MSGD-C (MSGD enhanced with out clipping strategy) and MSGD-CR (MSGD with the clipping strategy and (Sutskever, 2012) that initialization plays an impor-tant role for training RNNs. We consider three differ-ent initializations.  X  sigmoid  X  is the most adversarial initialization, where we use a sigmoid unit network where W rec , W in , W out  X  N (0 , 0 . 01).  X  basic tanh  X  uses a tanh unit network where W rec , W in , W out  X  N (0 , 0 . 1).  X  smart tanh  X  also uses tanh units and W in , W out  X  N (0 , 0 . 01). W rec is sparse (each unit has only 15 non-zero incoming connections) with the spectral radius fixed to 0.95. In all cases b = b out = 0. The graph shows the success rate out of 5 runs (with different random seeds) for a 50 hidden unit model, where the x -axis contains the length of the sequence. We use a constant learning rate of 0.01 with no mo-mentum. When clipping the gradients, we used a threshold of 1.,and the regularization weight was fixed to 4. A run is successful if the number of misclassified sequences was under 1% out of 10000 freshly random generated sequences. We allowed a maximum number of 5M updates, and use minibatches of 20 examples. This task provides empirical evidence that exploding gradients are linked with tasks that require long mem-ory traces. As the length of the sequence increases, using clipping becomes more important to achieve a better success rate. More memory implies larger spec-tral radius, which leads to rich regimes where gradients are likely to explode.
 Furthermore, we can train a single model to deal with any sequence of length 50 up to 200 (by providing sequences of different random lengths in this interval for different MSGD steps). We achieve a success rate of 100% over 5 seeds in this regime as well (all runs had 0 misclassified sequences in a set of 10000 ran-domly generated sequences of different lengths). We used a RNN of 50 tanh units initialized in the  X  ba-sic tanh  X  regime. The same trained model can address sequences of length up to 5000 steps, lengths never seen during training . Specifically the same model produced 0 mis-classified sequences (out of 10000 sequences of same length) for lengths of either 50, 100, 150, 200, 250, 500, 1000, 2000, 3000 and 5000. This provides some evidence that the model might use attractors to form some kind of long term memory, rather then relying on transient dynamics (as for example ESN networks probably do in Jaeger (2012)). 4.1.2. Other pathological tasks SGD-CR is able to solve (100% success on the lengths listed below, for all but one task) other pathological problems from Hochreiter and Schmidhuber (1997), namely the addition problem, the multiplication prob-lem, the 3-bit temporal order problem , the random permutation problem and the noiseless memorization problem in two variants (when the pattern needed to be memorized is 5 bits in length and when it contains over 20 bits of information; see Martens and Sutskever (2011)). For every task we used 5 different runs (with different random seeds). For the first 4 problems we used a single model for lengths up to 200, while for the noiseless memorization we used a different model for each sequence length (50, 100, 150 and 200). The hardest problems for which only two runs out of 5 suc-ceeded was the random permutation problem. For the addition and multiplication task we observe successful generalization to sequences up to 500 steps (we notice an increase in error with sequence length, though it stays below 1%). Note that for the addition and mul-tiplication problem a sequence is misclassified with the square error is larger than .04. In most cases, these results outperforms Martens and Sutskever (2011) in terms of success rate, they deal with longer sequences than in Hochreiter and Schmidhuber (1997) and com-pared to (Jaeger, 2012) they generalize to longer se-quences. For details see supplementary materials. 4.2. Natural problems We address the task of polyphonic music prediction, using the datasets Piano-midi.de, Nottingham and MuseData described in Boulanger-Lewandowski et al. (2012) and language modelling at the character level on the Penn Treebank dataset (Mikolov et al. , 2012). We also explore a modified version of the task, where we require to predict the 5th character in the future (instead of the next). We assume that for solving this modified task long term correlations are more impor-tant than short term ones, and hence our regulariza-tion term should be more helpful.
 The training and test scores for all natural problems are reported in Table 1 as well as state of the art for these tasks. See additional material for hyper-parameters and experimental setup. We note that keeping the regularization weight fixed (as for the pre-vious tasks) seems to harm learning. One needs to use a 1 / t decreasing schedule for this term. We hypothe-sis that minimizing the regularization term affects the ability of the model to learn short term correlations which are important for these tasks, though a more careful investigation is lacking.
 These results suggest that clipping solves an optimiza-tion issue and does not act as a regularizer, as both the training and test error improve in general. Results on Penn Treebank reach the state of the art for RNN achieved by Mikolov et al. (2012), who used a differ-ent clipping algorithm similar to ours, thus providing evidence that both behave similarly. A maximum en-tropy model with up to 15-gram features has state of the art on Penn Treebank(character-level). However, RNN models with some clipping strategy have state of the art for word modeling and character modeling on larger datasets (Mikolov et al. , 2011; 2012) For the polyphonic music prediction we have state of the art for RNN models, though RNN-NADE, a prob-abilistic recurrent model, trained with Hessian Free (Bengio et al. , 2012) does better. We provided different perspectives through which one can gain more insight into the exploding and vanishing gradients issue. We put forward a hypothesis stat-ing that when gradients explode we have a cliff-like structure in the error surface and devise a simple so-lution based on this hypothesis, clipping the norm of the exploded gradients. The effectiveness of our pro-posed solutions provides some indirect empirical evi-dence towards the validity of our hypothesis, though further investigations are required. In order to deal with the vanishing gradient problem we use a regular-ization term that forces the error signal not to vanish as it travels back in time. This regularization term forces the Jacobian matrices  X  x i only in relevant directions. In practice we show that these solutions improve performance of RNNs on the pathological synthetic datasets considered, polyphonic music prediction and language modelling.
 We would like to thank Theano development team. We acknowledge RQCHP, Compute Canada, NSERC, FQRNT and CIFAR for the resources they provide. Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., and
Bengio, Y. (2012). Theano: new features and speed improvements. Submited to Deep Learning and Un-supervised Feature Learning NIPS 2012 Workshop. Bengio, Y., Frasconi, P., and Simard, P. (1993). The problem of learning long-term dependencies in re-current networks. pages 1183 X 1195, San Francisco. IEEE Press. (invited paper).
 Bengio, Y., Simard, P., and Frasconi, P. (1994). Learn-ing long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks , 5 (2), 157 X 166.
 Bengio, Y., Boulanger-Lewandowski, N., and Pascanu,
R. (2012). Advances in optimizing recurrent net-works. Technical Report arXiv:1212.0901, U. Mon-treal.
 Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-
Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy) . Oral Presentation.
 Boulanger-Lewandowski, N., Bengio, Y., and Vincent,
P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proceed-ings of the Twenty-nine International Conference on Machine Learning (ICML X 12) . ACM.
 Doya, K. (1993). Bifurcations of recurrent neural net-works in gradient descent learning. IEEE Transac-tions on Neural Networks , 1 , 75 X 80.
 Doya, K. and Yoshizawa, S. (1991). Adaptive synchro-nization of neural and physical oscillators. In J. E. Moody, S. J. Hanson, and R. Lippmann, editors, NIPS , pages 109 X 116. Morgan Kaufmann.
 Elman, J. (1990). Finding structure in time. Cognitive Science , 14 (2), 179 X 211.
 Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A Novel
Connectionist System for Unconstrained Handwrit-ing Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 31 (5), 855 X 868. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation , 9 (8), 1735 X 1780.
 Jaeger, H. (2012). Long short-term memory in echo state networks: Details of a simulation study. Tech-nical report, Jacobs University Bremen.
 Jaeger, H. and Haas, H. (2004). Harnessing nonlinear-ity: predicting chaotic systems and saving energy in wireless telecommunication. Science , 304 (5667), 78 X 80.
 Jaeger, H., Lukosevicius, M., Popovici, D., and Siew-ert, U. (2007). Optimization and applications of echo state networks with leaky-integrator neurons. Neural Networks , 20 (3), 335 X 352.
 Martens, J. and Sutskever, I. (2011). Learning recur-rent neural networks with Hessian-free optimization. In Proc. ICML X 2011 . ACM.
 Mikolov, T. (2012). Statistical Language Models based on Neural Networks . Ph.D. thesis, Brno University of Technology.
 Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011). Empirical evaluation and combination of advanced language modeling tech-niques. In Proc. 12th annual conference of the in-ternational speech communication association (IN-TERSPEECH 2011) .
 Mikolov, T., Sutskever, I., Deoras, A., Le, H.-S.,
Kombrink, S., and Cernocky, J. (2012). Subword language modeling with neural networks. preprint (http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf). Pascanu, R. and Jaeger, H. (2011). A neurodynamical model for working memory. Neural Netw. , 24 , 199 X  207.
 Rumelhart, D. E., Hinton, G. E., and Williams,
R. J. (1986). Learning representations by back-propagating errors. Nature , 323 (6088), 533 X 536. Strogatz, S. (1994). Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry,
And Engineering (Studies in Nonlinearity) . Studies in nonlinearity. Perseus Books Group, 1 edition. Sutskever, I. (2012). Training Recurrent Neural Net-works . Ph.D. thesis, University of Toronto.
 Sutskever, I., Martens, J., and Hinton, G. (2011). Generating text with recurrent neural networks. In
L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11) , ICML  X 11, pages 1017 X 1024, New York, NY, USA. ACM.
 Werbos, P. J. (1988). Generalization of backpropa-gation with application to a recurrent gas market
