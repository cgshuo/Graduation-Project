 Very large-scale classification taxonomies typicall y have hundreds of thousands of categories, deep hierarchies, and s kewed category distribution over documents. However, it is still a n open question whether the state-of-the-art technologies in automa ted text categorization can scale to (and perform well on) s uch large taxonomies. In this paper, we report the first eval uation of Support Vector Machines (SVMs) in web-page classification o ver the full taxonomy of the Yahoo! categories. Our accomplishme nts include: 1) a data analysis on the Yahoo! taxonomy; 2) the d evelopment of a scalable system for large-scale text categorizati on; 3) theoretical analysis and experimental evaluation of SVMs in hie rarchical and non-hierarchical settings for classification; 4) an investigation of threshold tuning algorithms with respect to time co mplexity and their effect on the classification accuracy of SVMs . We found that, in terms of scalability, the hierarchical use of SV Ms is efficient enough for very large-scale classification; however , in terms of effectiveness, the performance of SVMs over the Yah oo! Directory is still far from satisfactory, which ind icates that more substantial investigation is needed.
 F.2 [Analysis of Algorithms and Problem Complexity] : Miscellaneous; H.4.m [Information System Applicatio ns]: Miscellaneous; I.5.4 [Pattern Recognition]: Applica tions  X  Text processing. Technology Assessment, Performance and Scalability Analysis, Empirical Validation. Text Categorization, Very Large Web Taxonomies, Thr eshold Tuning Strategies and Algorithm Complexity Text categorization (TC), which is the problem of a ssigning predefined categories to free-text documents by mea ns of supervised learning, is a key technology of text mi ning and has attracted wide attention from many different resear ch fields. In the literature, many algorithms [5][16][18] have been p roposed, such as Support Vector Machines (SVMs), k -Nearest Neighbor ( k -NN), Na X ve Bayes (NB) and so on. Empirical evaluations o n benchmark datasets such as Reuters 21578 [21] and RCV1 [14] h ave shown that most of these methods are quite effective in t raditional text classification applications. However, in recent yea rs, there has emerged a trend for the scale of text categorizatio n problems to become larger and larger. For example, Web taxonomi es (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/ ) often have hundreds of thousands of categories. This naturally raises the question o f whether the aforementioned TC methods can successfully handle t hese very large-scale classification taxonomies. In this pape r, we will try to answer this question from the views of both scalabi lity and effectiveness. For simplicity, we will only discuss SVMs, although the same methodology could be adopted in the analys is of other classifiers. To the best of our knowledge, there are two main ki nds of implementation for SVMs, namely flat SVMs and hiera rchical SVMs. Flat SVMs refer to those SVM classifiers that do not take advantage of the structure of the taxonomy tree. M any multi-class versions of SVMs can be regarded as flat SVMs, such as one-against-rest SVMs, one-against-one SVMs and SVMs wi th error correcting output coding [2][3][9][12]. Among these , the earliest, yet most popular, are one-against-rest SVMs, in whi ch an SVM model is trained to distinguish each category from all the other categories combined. In this paper, we will use thi s as the representative of flat SVMs. Hierarchical SVMs refe r to those methods that decompose the training tasks according to the structure of the taxonomy [4][5][6][10][17][19]. Th at is, an SVM model is trained to distinguish only among those ca tegories with the same parent node in the taxonomy tree. As far as we know, the maximum number of categories ever tested in an evaluation of SVMs, whether flat or hierarchi cal, is no more than 5000 [4]. This is less than 2% of the number o f categories of the Yahoo! Directory. Such limited empirical studie s are far from sufficient for understanding the performance of SVM s over very large-scale taxonomies. In order to gain this under standing, in this paper, we assess the usefulness of SVMs for very la rge-scale categorization tasks where the categories are hiera rchically arranged. In particular, we theoretically analyze t he scalability and effectiveness of SVMs and two threshold tuning meth ods (score cut and rank cut). We also describe a scalable syst em we have developed using distributed classifiers, which can handle experiments on large-scale real-world datasets. Las tly, we provide the first thorough experimental results on the trad eoff between classification effectiveness and efficiency of SVMs with the Yahoo! taxonomy. The paper is organized as follows. Section 2 gives a brief literature review on SVMs. Section 3 introduces some character istics of the Yahoo! Directory. Theoretical analysis and experime ntal results on scalability and effectiveness of SVMs are discus sed in Section 4 and 5. Concluding remarks are provided in Section 6. According to our categorization of SVMs in the prev ious section, for flat SVMs, each SVM model is trained to disting uish one category from all the other categories. For the tes ting phase, an exhaustive search is used to classify an instance i nto the category with the highest confidence score. It is clear that the complexity of flat SVMs is proportional to the number of categori es. Therefore, when handling hundreds of thousands of categories, the computational load will increase to unacceptable le vels. To tackle this problem, people have utilized the hi erarchical structure of the taxonomy tree to decompose the cla ssification task. In [5] and [6], Dumais used hierarchical SVMs to classify the LookSmart dataset. For the training phase, a cl assifier was trained to distinguish only those categories with t he same parent node in the taxonomy tree. And for testing, a pachinko-machine search was used, where an SVM model is used only if the model of its parent category says YES on the test instanc e. They claimed improved classification performance with a signific ant (i.e. more than 80%) reduction in computation compared to the flat baseline. However, because they only used the top two levels of the LookSmart categories (163 categories in total) in t heir experiments, their conclusions might not easily gen eralize to the case of classifying hundreds of thousands of catego ries. Our previous work, [19], is the first paper to give a theoretical analysis of the scalability of TC algorithms. Using the power law to model the category distributions, we derived the bounds of complexity for both flat and hierarchical SVMs. Exp eriments were conducted on OHSUMED [11] to verify the theoretical analysis: for example, it took 102 hours to train flat SVMs o ver OHSUMED and only took 26.3 minutes to train hierarc hical SVMs. However, these experiments were not conducted over the full domain of OHSUMED (with 14,321 categories in t otal) but projected from 94 categories in the heart-disease s ub domain. Furthermore, the classification performance was not reported, so the trade-off between effectiveness and efficiency was not discussed.
 Besides the aforementioned work, other work has als o been proposed to investigate the problem of SVM classifi cation over hierarchical taxonomies [4][5][6][10][17][19]. Once again, they verified their findings over datasets with only hun dreds, or at most a few thousand categories (such as Reuters 21578, R CV1, the heart-disease sub tree of OHSUMED, and WIPO-alpha [ 4]). So in summary, the question still remains open as to whet her SVMs can scale to hundreds of thousands of categories, and w hat the tradeoff between efficiency and effectiveness will be. In th is regard, it will be very meaningful to conduct a thorough experiment al study on SVM classification over the full domain of a very l arge-scale data corpus, which is the motivation of our paper. In order to investigate the efficiency and effectiv eness of SVMs over very large-scale taxonomies, the full domain o f the Yahoo! Directory was used as our experimental data. In thi s section, we will describe some distinguishing characteristics o f the Yahoo! Directory 1 to show that experiments on it can give new insigh ts over previous work on traditional benchmark dataset s. The first characteristic of the Yahoo! Directory is the deep taxonomy hierarchy. At the time of our crawling (Ju ne, 2004), there were 292,216 categories in the Yahoo! Directo ry organized into a 16-level hierarchy. Since the taxonomy is hi erarchical, some categories are only conceptual nodes and have no la beled documents of their own. Therefore, the crawled 792, 601 documents actually belong to 246,279 categories. Fr om Figure 1, we can see that both the categories and documents h ave spindle distributions over levels. That is, there are more categories and documents in the middle than at the upper and lower levels of the taxonomy hierarchy. Figure 1. Category and document distributions in th e Yahoo! The second characteristic is the skewed category di stribution over documents. If we only consider the documents assign ed by the human editors directly to those 246,279 categories (without counting in the documents assigned to their child c ategories), the number of documents per category follows the power law distribution (see Figure 2). That is, most categori es have very few labeled documents. For instance, over 76% of the Ya hoo! categories have fewer than 5 labeled documents. If we denote such categories as  X  X are categories X , we further find th at the proportion of rare categories increases at deeper hierarchy le vels. As shown in Figure 3, there is no rare category in the first le vel, but about 36% are rare categories at deep levels. 
Figure 2. Power law distribution of the sizes of c ategories in Figure 3. Percentage of rare categories at differen t levels of the The third characteristic is that many documents in the Yahoo! Directory have multiple labels (see Figure 4). On a verage, each document has 2.23 labels, while the largest number of labels for a single document is 31. The existence of multiple la bels indicates the necessity of conducting threshold tuning along with the classifier training [20]. However, none of the pre vious work has analyzed or conducted experiments on the scalabilit y of threshold tuning algorithms. Because of these particular characteristics, experi ments based directly on the full set of Yahoo! Directory catego ries will be much more informative than any previous work. There fore, we partitioned the Yahoo! Directory into a training se t and a testing set with a ratio of 7:3 to conduct our experimental study. Note that for ease of evaluation, we need to guarantee that e ach category has at least one positive training example and one test instance, which means that we have to remove those categories conta ining only one labeled document 2 . As a result, 132,199 categories remained in total with 492,617 documents for training and 27 5,364 documents for testing. To our knowledge, this datas et is the largest dataset that has been ever used for evaluating SVM classification. Figure 4. Distribution of labels per document in th e Yahoo! In this section, we discuss whether SVMs (either fl at or hierarchical) can theoretically scale up with reaso nable classification performance. Specifically, we estima te the time complexity of training and testing over the Yahoo! Directory, and discuss factors that may influence the effectivenes s of SVM classification. First, let us consider the complexity of flat SVMs. With the one-against-rest strategy, when training the SVM model for any individual category, we always use the entire train ing set Considering that the complexity of SVMs grows super -linearly with the number of training documents [13][15], the overall complexity of flat SVMs can be represented by where N is the number of training documents; M is the number of categories in the training set, and O( N c ) denotes the average training time per SVM model. For our case, M =132,199 and N =492,617. For testing, since we need to pass an instance onto all of the M SVM models to find the category with the highest co nfidence score, the time complexity is where O(1) denotes the average test time per docume nt per SVM model. Compared to flat SVMs, the case for hierarchical SV Ms is a little more complicated, because the size of the training set varies for different nodes in the hierarchy. The size of the training set for a category in a hierarchical SVM setup equals the num ber of documents in the sub-tree rooted by its parent cate gory shown in [19], this size approximately obeys a powe r law distribution: where m i is the number of categories defined at the i -th level; j is the size-based rank of the categories; n ij is the number of training documents for the j -th category at the i -th level; n of training documents for the most common category at the i -th level; and  X  i is a level-specific parameter. As discovered in [19], the power law distributions at the different levels in OHSUMED are almost the same. As a result, a global was used for the complexity analysis. However, data statistics on the Yahoo! Directory show that the distributions ar e not similar, especially for the top and bottom levels (see Figur e 5). Therefore, instead of using a global  X  , we use local  X  i  X  X  derived from the slopes of the regression lines in Figure 5 for comp lexity estimation. 
Figure 5. Power law distribution of document freque ncy over category rank in the training set of the Yahoo! Dir ectory.
 Furthermore, m i = b i was used in [19] to approximate the number of categories at the i -th level (where b is a global branching factor). However, our observations of the Yahoo! Directory s how that the branching factors are quite different for different levels, and the number of categories per level follows a spindle di stribution (see Figure 1). This distribution cannot be modeled well by m which is an increasing function of i . Therefore, we use different branching factors b i for different levels, and choose m statistics. To summarize, we use the m i and b i values in Table 1 to conduct our complexity analysis, where level 0 is a virtual level for ease of derivation. In particular, Because Considering that for 0  X  &gt; and 1  X   X  , Replacing the numerator and denominators in (5) wit h the upper and lower bounds from (6) respectively, with appropriate substitutions for  X  , we have Table 1. Statistics of the training set of the Yaho o! Directory For the testing phase of hierarchical SVMs, if no t hreshold tuning is used, only one category with the highest confide nce score will be selected at each level. Supposing we have finish ed the category selection at the i-th level, then on average we will test b models to further classify the instance into one of the child categories at the ( i +1)th level. Accordingly, the complexity is As indicated by the third characteristic of the Yah oo! Directory, threshold tuning is necessary for categorizing this multi-labeled data corpus. In this section, we will investigate t wo popular threshold tuning strategies, rank cut (RCut) and sc ore cut (SCut) [20], to see their effects on the computational cos t. RCut  X  for each test document, one sorts categories by descending confidence scores and assigns YES to each of the t -top categories. Obviously when using an empirical parameter t (i.e. the average number of labels per document in the training set), RCut will not introduce any additional computations to the traini ng process. SCut  X  one scores a validation set of documents for each category and tunes the threshold over the local pool of scor es until the optimal performance of the classifier is obtained f or that category. Practically, k -fold (most frequently 5-fold) cross validation is often adopted in SCut, which randomly partitions th e training set into k sets and circularly uses k -1 sets for training and the other one for validation. Then, after determining the opt imal threshold through cross-validation, one trains a final SVM mo del using the entire training set for that category. In this way, for any category, k additional SVM models need to be trained, each usin g training documents (where n is the size of the original training set size for that category, i.e. n = N for flat SVMs and n = n hierarchical SVMs). This corresponds to 1 ( 1) ( 1) c c the normal training complexity of SVMs. The cross v alidation also requires the testing of n k k n  X  = documents. Thus, overall we have: 1) The training complexity of flat SVMs with SCut is 2) The training complexity of hierarchical SVMs with S Cut is It is easy to derive that if 1 1 log log k k k Accordingly, we come to the conclusion that SCut wi th 5-fold cross validation could dominate the training time o f SVMs, because 1 4 5 5 log log 7.2 . c  X  &gt; 5 On the other hand, SCut will not affect the testing phase of SVMs much. In fact, the testing complexities of flat SVMs with and without SCut are equal to each other. For the testing of hierarchical SVMs, the major cha nge brought by SCut is that more than one category may be selected at each level. To reflect this change, we introduce a new paramete r, the average number of selected categories per document (denoted by each level, to refine (8) to Since  X  i is dependent on both attributes of the data and the method of classification, we cannot determine its value me rely based on the statistics of the corpus (as what we have done for b So as a result, we will further discuss (12) with o ur experimental results in Section 5. In summary, we have given a detailed scalability an alysis of SVM classification over the Yahoo! Directory. This anal ysis gives us some intuitive understanding of the differences bet ween flat and hierarchical SVMs. For example, letting k =5 and c =2, we find that SCut consumes more than 75% of the computations in the training process. And, whether using SCut or not, hierarchic al SVMs save more than 90% of the training computations of flat SVMs. Note also that this analysis can generalize to other ver y large-scale data corpora such as ODP, Look Smart, etc. Compared to scalability analysis, classification ef fectiveness is not as clear and predictable because it may be affected by many other factors, such as the number and quality of the trai ning examples. So in this subsection, we will just provide some ge neral ideas on how the particular characteristics of the Yahoo! Di rectory will affect the effectiveness of SVMs. The literature has shown that SVMs have high traini ng performance and low generalization error [14][16][1 8]. However, some papers have also pointed out the potential pro blems of SVMs when the training set is noisy and imbalanced. For example, [1] found that when working with an imbalanced data set, SVMs will produce a less effective classification bounda ry skewed to the minority class. And for the extreme case, when ther e are too few positive examples, SVMs may totally fail, since the re is insufficient evidence for statistical learning. Unf ortunately, many Yahoo!-like large-scale data corpora are seriously imbalanced. For instance, Figure 2 shows that most of the categorie s in the Yahoo! Directory have very few positive examples, but hund reds of thousands of negative examples. In this case, we ca nnot expect the performance of SVMs to be very good. Furthermore, i f evaluating the classification performance with respect to hier archy depth, we would expect the performance to drop at deeper leve ls in the hierarchy, because the proportion of rare categorie s becomes larger (see Figure 3). This imbalance will also affect the classification effectiveness of the threshold tuning methods. As shown in previous benchmark evaluations [20], SCut outperforms RCut for common categories but performs worse for rare categories. Therefore w e can predict that SCut will perform better than RCut at top leve ls but perform poorer at deeper levels of the Yahoo! Directory. Given the training process of hierarchical SVMs (as in the previous subsection), the imbalance of the training set in hierarchical SVMs is not as serious as in flat SVMs the training error of each individual model in hier archical SVMs will be smaller than in flat SVMs. Moreover, some previous work showed that local feature selection and local param eter tuning in hierarchical SVMs can also improve training accurac y. However, considering that the pachinko-machine search will result in error propagation in the testing phase, we cannot conclud e how hierarchical SVMs will perform relative to flat SVM s in terms of classification accuracy without an empirical study, which is the subject of Section 5. In this section, we present extensive experimental results to verify the analysis in the previous sections. For this pur pose, we developed a distributed classification system as sh own in Figure 6 to handle the large scale of our experiments. In th e figure, the controller is responsible for dispatching classific ation tasks to computation agents. The computation agent then trai ns and saves a model locally with the given examples and parameter settings. When testing, the controller multicasts a test inst ance to the selected agents and collects their classification r esults according to a specified strategy (an exhaustive search for flat SVMs and a pachinko-machine search for hierarchical SVMs). In our current system, 10 machines were used, each with four 3GHz CPUs and 4GB of memory. When logging the time complexity, we ignored network transmission time. For our implementation of SVMs, we used the followi ng settings: 1) We used the SMO [15] algorithm with a linear kernel for the 2) We selected 4000 features for each binary classific ation task 3) For our implementation of SCut, we used 5-fold cros s 4) For  X  X ierarchical SVMs X , we used the same divide-an d-Note that RCut is not compatible with hierarchical SVMs. Since RCut will always say YES to the t top categories and never refuse all, a pachinko-machine search with RCut will always descend to a leaf and will never terminate on any non-leaf nod e. We therefore did not test the combination of hierarchical SVMs w ith RCut, but only investigated the other three combinations: fla t SVMs with SCut, flat SVMs with RCut, and hierarchical SVMs wi th SCut. To show the validity of our implementation of SVM a lgorithms, we verified our code on a well-recognized benchmark corpus, RCV1. We used both micro-averaged and macro-average d F1 scores (denoted by Micro-F1 and Macro-F1 respective ly) as the metrics [18]. Our flat SVMs with SCut had a Micro-F 1 of 0.818 and a Macro-F1 of 0.601, which is as good as that r eported in [14]. Thus we are confident that our experimental results in the following subsections are representative and correc t. To better understand the time complexity of SVMs, w e logged their run-times, which are given in Table 2. Note t hat the testing time in this table is the total time for classifyin g all of the instances in the testing set. So to determine the a verage response time for one single document, one must divide the g iven value by the number of test documents. For example, the aver age response time of hierarchical SVMs is 7 which means that hierarchical SVMs can label about 625 documents per second. Hierarchical SVMs 0.42 2.10 0.12 Based on Table 2, we come to the following conclusi ons: 1) SCut dominates computations (about 80%) in the trai ning 2) Flat SVMs cannot be used in very large-scale real-w orld 3) The complexity of hierarchical SVMs is much lower a nd can Furthermore, with Table 2, we can validate the form ulas theoretically derived in Section 4. For this purpos e, we treat the complexities of flat SVMs as references: 
Q h = (14) 
Q Q e h s = = (15) Q h = (16) From this, we can determine that c equals 2.45 for SMO on the Yahoo! Directory, which is a little bit larger than that reported in [15]. Furthermore, we can estimate the training com plexity of hierarchical SVMs according to (7) and (10) as foll ows. 
Q Q h  X   X  = (17) 
Q Q  X   X   X  In order to estimate the testing complexity of hier archical SVMs, we further logged the average number of selected ca tegories per document at each level in Table 3. With this table and (12), we can compute 
Table 3. Average number of categories selected per document 
Level 1 2 3 4 5 6 7 8 
Level 9 10 11 12 13 14 15 16 It can be seen that (17), (18) and (19) are a littl e smaller than what we observed in the experiments (0.42 h , 2.10 h and 0.0016 s respectively). Our explanation of this is that our logged time complexity included the overhead of disk I/O and th e influences of other processes running on the same machine. Theref ore the observations may be noisy and larger than the real complexity. Classification performance of SVMs with respect to hierarchy depth is shown in Figure 7. In this figure, when pe rformance was calculated for the i -th level, we neglected the existence of the deeper levels in the taxonomy tree and put all docu ments in them into their parent categories at the i -th level. In this regard, this figure shows the performance when the task is to cl assify only the top i levels. It is clear that the data points at the 16 -th level correspond to the classification performance over t he full domain of the Yahoo! taxonomy. Figure 7. Classification performance of SVMs over t he Yahoo! By considering effectiveness and efficiency togethe r, we can obtain Figure 8, the F1 scores in which correspond to the data points at the 16-th level in Figure 7. From these t wo figures, we come to the following conclusions. 1) For flat SVMs, SCut outperforms RCut at upper level s of the 2) The classification performance of hierarchical SVMs is 3) In terms of effectiveness, the performance of SVMs, whether Figure 8. Tradeoff between effectiveness and comple xity of In our opinion, the low classification performance is mostly a result of a data sparseness problem in the rare categories of the Yahoo! Directory. That is, most categories simply c annot provide enough evidence for us to learn accurate SVM models . Figure 9 plots the macro-averaged performance of hierarchica l SVMs vs. the number of training examples in each category: t he curve climbs sharply near the right end, meaning that mor e training examples (more than 100 per category) would improve the accuracy significantly. However, fewer than 0.7% of the Yahoo! categories have more than 100 examples (see Figure 2). This is not only a problem for SVMs though. Other statistical classifiers will face similar challenges. That is, current machine l earning methods still need significant improvement when applied to very large-scale datasets. A possible future research directio n is determining how to automatically expand the training set or how to leverage the unlabeled documents on the Web for better class ifier training. 
Figure 9. Classification performance of hierarchica l SVMs vs. The difficulties in applying text categorization al gorithms to very large problems, especially large-scale Web taxonomi es, have been underestimated or at least not studied thoroughly i n the literature. In order to gain a better understanding, we conduct ed the first evaluation of SVMs with the full Yahoo! web-page ta xonomy, which yielded the following new conclusions: 1) Threshold tuning (SCut in our paper) dominates the time 2) In terms of scalability, while the complexity of fl at SVMs is 3) In terms of effectiveness, neither flat nor hierarc hical SVMs 4) The skewed distribution of the Yahoo! Directory and other We would like to thank Bryan Klimt and Qiankun Zhao for their help on polishing the language of this paper. [1] Akbani R., Kwek S., Japkowicz N. Applying support v ector [2] Bottou, L., Cortes, C., Dcnkcr, J., et al. Comparis on of [3] Bredensteiner, E. J., and Bennett, K. P. Multicateg ory [4] Cai, L. and Hofmann, T. Hierarchical Document [5] Chen, H., and Dumais, S. Bringing order to the web: [6] Dumais, S., Chen, H. Hierarchical classification of Web [7] Dunning, T. E. Accurate methods for the statistics of surprise [8] Forman, G. An extensive experimental study of featu re [9] Ghani, R., Using Error-Correcting Codes for Text [10] Granitzer, M. Hierarchical text classification usin g methods [11] Hersh, W., Buckley, C., Leone, T., and Hickam, D. [12] Hsu, C. W., and Lin, C. J. A comparison of methods for [13] Joachims, T. Making large-scale SVM learning practi cal. [14] Lewis, D. D., Yang, Y., Rose, T. G., Li, F. RCV1: a new [15] Platt, J. Fast training of support vector machines using [16] Sebastiani, F. Machine learning in automated text [17] Sun, A. and Lim, E. Hierarchical Text classificatio n and [18] Yang, Y., and Liu, X. A re-examination of text [19] Yang, Y., Zhang, J., and Kisiel, B. A scalability a nalysis of [20] Yang, Y. A study of thresholding strategies for tex t [21] http://www.daviddlewis.com/resources/testcollection s/reuter
