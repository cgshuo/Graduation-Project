 Universitat Polit ` cnica de Val X ncia For many languages that use non-Roman based indigenous scripts (e.g., Arabic, Greek and Indic languages) one can of-ten find a large amount of user generated transliterated con-tent on the Web in the Roman script. Such content creates a monolingual or multi-lingual space with more than one script which we refer to as the Mixed-Script space . IR in the mixed-script space is challenging because queries written in either the native or the Roman script need to be matched to the documents written in both the scripts. Moreover, transliterated content features extensive spelling variations. In this paper, we formally introduce the concept of Mixed-Script IR , and through analysis of the query logs of Bing search engine, estimate the prevalence and thereby establish the importance of this problem. We also give a principled so-lution to handle the mixed-script term matching and spelling variation where the terms across the scripts are modelled jointly in a deep-learning architecture and can be compared in a low-dimensional abstract space. We present an exten-sive empirical analysis of the proposed method along with the evaluation results in an ad-hoc retrieval setting of mixed-script IR where the proposed method achieves significantly better results (12% increase in MRR and 29% increase in MAP) compared to other state-of-the-art baselines. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Mixed-script information retrieval, transliteration, deep-learning  X  Ex cept this author, all authors are sorted alphabetically.
A large number of languages, including Arabic, Russian, and most of the South and South East Asian languages, are written using indigenous scripts. However, due to various socio-cultural and technological reasons, often the websites and the user generated content in these languages, such as tweets and blogs, are written using Roman script [1]. Such content creates a monolingual or multi-lingual space with more than one scripts which we refer to as the Mixed-Script space . Information retrieval in the mixed-script space, which can be termed as Mixed-Script IR (MSIR), is challenging because queries written in either the native or the Roman scripts need to be matched to the documents written in both the scripts.

The process of phonetically representing the words of a language in a non-native script is called transliteration [19]. Transliteration, especially into Roman script, is used abun-dantly on the Web not only for documents, but also for user queries that intend to search for these documents. Since there are no standard ways of spelling a word in a non-native script, transliteration content almost always features extensive spelling variations; typically a native term can be transliterated into Roman script in very many ways [1, 11]. For example, the word pahala ( X  X irst X  in Hindi and many other Indian languages) can be written in Roman script as pahalaa, pehla, pahila, pehlaa, pehala, pehalaa, pahela, pahlaa and so on.

This phenomenon presents a non-trivial term matching problem for search engines to match the native-script or Roman-transliterated query with the documents in multi-ple scripts taking into account the spelling variations. The problem of MSIR, although prevalent in Web search for users of many languages around the world, has received very lit-tle attention till date. There have been several studies on spelling variation in queries and documents written in a sin-gle (native) script [14, 34, 9] as well as transliteration of named entities (NE) in IR [3, 31, 33]. However, as we shall see in this paper, MSIR presents challenges that the current approaches for solving mono-script spelling variation and NE transliteration in IR are unable to address adequately, especially because most of the transliterated queries (and documents) belong to the long tail and hence do not have enough clickthrough evidence to rely on.
In this paper, for the first time, we formally introduce th e problem of MSIR and related research challenges. In order to estimate the prevalence of transliterated queries, we also analyse a large query log of Bing 1 consisting of 13 billion queries issued from India. As many as 6% of the unique queries have one or more Hindi words translit-erated into Roman scripts, of which only 28% queries are pure NEs (people, location and organization). On the other hand, 27% of the queries belong to the entertainment do-main (names of movies, song titles, parts of lyrics, dialogues, etc.), which provide complex and ideal examples of translit-erated queries. Hindi song music is also one of the most case for MSIR. This motivated us to conduct our MSIR stud-ies on Hindi song lyrics.

We propose a principled solution to handle the mixed-script term matching and spelling variation where the terms across the scripts are modelled jointly. We model the mixed-script features jointly in a deep-learning architecture in such a way that they can be compared in a low-dimensional ab-stract space. The proposed method can find the equivalents of the query term across the scripts; the original query is then expanded using the thus found equivalents. Through rigorous experiments on MSIR for Hindi film lyrics, we fur-ther establish that the proposed method achieves signifi-cantly better results compared to all the competitive base-lines with 12% increase in MRR and 29% increase in MAP over the best performing baseline.

The concrete contributions of this paper are two-fold as listed below: 1. The concept, formal definition and representative anal-2. The mixed-script joint modelling technique using deep
The rest of the paper is organized as follows. In Section 2, we introduce the notion of MSIR formally and outline the possible applications scenarios and research challenges. Sec-tion 3 presents the empirical analysis of Bing search engine X  X  query log, and the prevalence and distribution of transliter-ated Hindi queries. Section 4 presents our deep-learning based joint script modelling approach to MSIR. In Section 5 the experimental setup and results are presented along with extensive empirical analysis. Finally, in Section 6 we present the related work and we make the concluding remarks in Section 7.
In this section, we formalize the notion of Mixed-Script In-formation Retrieval along the lines of Crosslingual IR (CLIR). We also list out a set of research challenges in the context of MSIR.
Let L be a set of (natural) languages { l 1 , l 2 , . . . , l assume that every language is generally written using a par-ticular script, which we will refer to as the native script of the language. Let s i be the native script for language l Thus, the set of scripts S = { s 1 , s 2 , . . . , s n } has a one-to-one mapping to L . ht tp://www.bing.com/
Zeitgeist 2010: India -http://www.google.com/intl/en /press/zeitgeist2010/regions/in.html
Any natural language word (or more generally any text fragment) w has two attributes -the language it belongs to and the script it is written in. We use the notation w  X  h l i , s j i to imply that w is in language l i , written us-ing the script s j . When i = j , we say that w is in native script. Else, we say that w is in transliterated form , where transliteration can be defined as the process of loosely or in-formally representing the sound of a word of one language, l using a non-native script s j .

Note that a particular language might be traditionally written in more than one script. For instance, Kurdish is written using the Roman, Cyrillic and Arabic scripts. How-ever, such cases are rare. On the other hand, it is very common to use a script for writing several languages. For instance, the Roman script (with slight variations or addi-tions of diacritics) is used to write English, French, Ger-man, Italian, Turkish and many other languages around the world. Similarly, the Devanagari script is used for writing Hindi, Sanskrit, Nepali and Marathi languages. Our defini-tion does not preclude such a possibility, but we would like to emphasize that it is useful to treat the same script differ-ently when used for writing different languages because the same sequence of letters might have different pronunciations in different languages. Consequently, transliterating a word of l i (say Hindi) into the scripts s j (say, Roman script as used in English orthography) and s k (say, Roman script as used in French orthography) could yield very different re-sults, even though the two scripts use almost an identical alphabet. Given a query q and a document pool D , the task of an IR engine is to rank the documents in D such that the ones relevant to q appear at the top of the ranked list. Depend-ing on the language in which q and D are presented, one can define two basic kinds of IR settings. Without loss of generality, let us assume that q  X  h l 1 , s 1 i . In monolingual IR , D = { d 1 , d 2 , . . . , d N } consists of only those documents that are in the same language and script, i.e., for all k , d  X  h l 1 , s 1 i . This simple scenario is modified in the context of CLIR, where l , i.e., for all k , d i,k  X  h l i , s i i . Note that all the documents in a typical CLIR setup are assumed to be written in the corresponding native scripts.

Based on this fundamental idea of CLIR, we can define a corresponding Mixed-script IR (MSIR) setup as follows. Let q  X  h l 1 , s j i be a query, where j may or may not be equal to 1. The document pool, language l 1 written in script s k , i.e., for all m , d 1 ,k,m h l , s k i . In other words, in the MSIR setup, the query and the documents are all in the same language, say l 1 , but they are written in more than one different scripts. The task of the IR engine is to search across the scripts.
 In the literature, sometimes CLIR is distinguished from Multilingual IR in the sense that the former refers to a case where n = 2 , whereas the latter is a generalization to any n &gt; 2. Likewise, for monolingual IR, n can be assumed to be 1. One could make a similar distinction between Mono-script, Cross-Script and Mixed-Script IR scenarios, where the query and the documents are in one language, but in 1, 2 or more than 2 scripts respectively. Nevertheless, we will refer to both the latter cases as MSIR. All our experiments involve a single language, namely Hindi, and two scripts  X  Devanagari and the Roman script (English orthography), but the proposed approach can be easily extended to a larger set of scripts.
 One can also further generalize the setup to Mixed-script Multilingual IR , where q as well as D can be in one of the several languages written in one of the several scripts. This is also a useful and practical setup, though we will not dis-cuss it any further in this work.
 It should also be noted that like CLIR it is possible in the MSIR setting that for q  X  { l i , s j } , the information might be available only in a d i,j,k where i 6 = j . In such cases, often the user issuing the query might be able to read and write both s j and s j and hence d i,j,k would have solved users info need. However, without MSIR this would not be possible to achieve.
The definition of MSIR setup assumes that the entire query and the each of the documents are in a single lan-guage and single script. However, in practice, one can find queries or documents that contain text fragments written in more than one language or script or both. Furthermore, depending on whether the parts of a query or document are written in a language using the native or a non-native script, one can have native or transliterated queries/documents.
A practical way to address the issue of mixed documents could be to split them into several sub-documents such that each of the sub-documents are in a single language and single script as discussed in [4] given mixing is not at sub-sentence level which falls under different case of code-mixing and out of the scope of this study. Mixed queries, however, cannot be handled through simple splitting because matching parts of a query to the documents does not make sense in the context of IR. Therefore, we extend our MSIR setup to include mixed queries. We define query q as a string of words w 1 w 2 . . . w to different languages, or scripts or both.
The two primary challenges in MSIR are: (a) how to tackle the extensive spelling variations in the transliterated queries and documents during the term matching phase, and (b) how to identify, process and represent a mixed query (and also, the mixed and transliterated documents). In CLIR, there are broadly two approaches to model the crosslin-gual space  X  either documents and queries are translated to bring all words into the same monolingual space, after which monolingual IR techniques and matching algorithms can be directly applied [33], or the crosslingual space is modelled jointly as an abstract topic or semantic space, and docu-ments and queries in all languages are mapped to this com-mon space [5]. Likewise, in MSIR one can X  X ransliterate X  the text to bring everything into a common space and then apply standard matching techniques in the single-script space, or one can jointly model an abstract orthographic space for rep-resenting the words written in different scripts. In this work, we shall explore the latter, which we believe is a more robust and generic solution to the mixed-script space modelling problem as it can simultaneously handle spelling variations in a single script and across multiple scripts. Nevertheless, we do recognize that the transliteration based approach is worth exploring, and machine transliteration, though a well studied problem [19], could present interesting and challeng-ing research problems when applied in the context of Web scale IR.

Mixed query processing is another interesting research challenge, which includes language identification of the query words, which can be either in native or transliterated scripts, and labeling those with semantic or other tags (e.g., entities, attributes). This is challenging especially because depend-ing on the context of the query, the same word, say  X  X an X , could represent the English word man , or a transliterated Hindi word man meaning  X  X ind X , or another transliterated Hindi word maan meaning  X  X eputation X . However, the same word with similar meanings are also used in many other In-dian languages; and it can also have different connotations in other languages (e.g., in Bengali this could also mean  X  X o get offended X ). Hence, language identification seems to be an extremely challenging problem in the MSIR setting, espe-cially when multiple languages are involved. In this work, we limit our experiments to only two languages, namely English and Hindi, and describe some initial results with language identification for transliterated and mixed queries. Apart from these basic challenges, result presentation in MSIR is also an interesting problem because this requires the information on whether the user can read all the scripts, or prefer some scripts over other. There are no user stud-ies related to MSIR and it is ripe with several such open problems. Although the current Web search engines do not support MSIR, they still have to handle a large traffic of mixed and transliterated queries from linguistic regions that use non-Roman indigenous scripts. We are not aware of any previ-ous study on percentage of transliterated queries seen by the commercial search engines. Consequently, we do not know the distribution of transliterated queries across various top-ics and domains, which could provide deeper understanding of the MSIR space and its users. In this section, we present an analysis of mixed and transliterated queries extracted from a large query log of Bing. Our study relies on auto-matic identification and classification techniques for mixed queries that have been developed specifically for this analy-sis.
We conducted the analysis on 13.78 billion queries sam-pled from the logs of Bing that were issued from India. India provides an interesting socio-linguistic context for studying mixed queries because of abundance of Roman translitera-tion and multiplicity of languages and scripts. This dataset consists of 30 million unique queries with an average length of 4.32 words per query. Almost all the queries (99.998%) are in Roman script. For ease of computation, we randomly sampled 1% (i.e., 300,000) of the unique queries and con-du cted the study on this smaller sample.

We automatically identify the transliterated queries using a language classifier, which has been built as follows. We train a maximum entropy classifier using character n -grams for n = 1 to 5 as features for both Hindi and English words, which is based on a similar word-level language identifica-tion work by King and Abney [18]. The classifier was trained on 5000 frequent transliterated Hindi words from Bollywood song lyrics [11]. This dataset is freely available for research purpose. For English examples, we have taken 5000 frequent words from the Leipzig Corpus 3 . We define a query q to be mixed or transliterated if at least 40% of the words in q are classified as Hindi. We tested the performance of the clas-sifier on a set of 2500 unseen words and the accuracy was found to be 97%. Note that the query log is expected to con-tain transliterated queries in other Indian languages as well. Due to a large shared vocabulary, lot of the Roman translit-erated words in other Indic languages have almost similar spellings as in Hindi, and hence, we observe that our classi-fier is able to identify those as well. Nevertheless, our analy-sis is targetted primarily on Hindi transliterated queries and the actual fraction of transliterated queries, considering all Indic languages, can be expected to be much higher than the numbers revealed by this study.

After observing the transliterated queries pulled out by our method, we identified six broad categories or topics to which most of these queries belong: Named Entities , En-tertainment , Information Source , Culture , Recipe and Re-search . Each of these were further refined into a set of sub-categories; e.g., Named Entities can be of three types people , location and organization . Besides, we also observed a few interesting subcategories, which we put together under a catch-all seventh category  X  Others . Table 1 lists all these categories and sub-categories along with example queries.
In order to automatically classify the queries into these categories, we resort to a rather simple minimally supervised approach. Through manual inspection of the transliterated queries, we selected five representative and reasonably fre-quent examples for each sub-category. We extract all queries from our dataset that have at least one word in common with at least one of the five representative queries, and then we extract the top 100 most frequent words in this set of queries. The standard English stop words are then removed from these 100 words; we shall refer to the remaining words as the cue words for the particular subcategory. In this way, we obtain the cue words for each sub-category with very few overlaps, giving us a total of 180 such words. Some exam-ple cue words for each of the sub-categories are reported in Table 1.

Let c j 1 to c j m sub-category. For each of the transliterated queries q = w w 2 . . . w m that we want to categorize, we remove all the stop words and cue words. For each of the remaining words in the query, say w i , we count the number of queries, f in the log where w i co-occurs with the cue-word c j k . Also, let f i be the number of queries in which w i occurs. We compute the score of q with respect to a sub-category j as (if w i is a stop or cue word then it is not considered for score computation): ht tp://corpora.uni-leipzig.de/ q is assigned to the sub-category j  X  for which this score is maximum.
In our dataset, as much as 6% of the unique queries were identified as transliterated, which means that at least 40% of the words in these queries are Roman transliterations of Hindi words. The average query length for the transliterated queries is 2.86, which is less than the average query length of all queries, 4.32. The frequency of the transliterated queries are in general less than that of the non-transliterated ones. Hence, they only constitute about 0.011% of all the queries in our dataset. However, their frequency distribution fol-lows the same power-law pattern as the regular queries, al-beit spanning mainly the medium and low frequency spectra. This also implies that a large number of transliterated and mixed queries belong to the long tail of distribution and may not have enough clickthrough data to help a search engine process them accurately. They must be processed differently recognizing the fact that they are rare, but together they do form a sizeable mass of the search traffic.

Table 1 reports the percentage of the transliterated queries in each of the identified sub-categories. The numbers do not add to 100% because a small fraction, 18% of unique but only 2% of all, queries could not be mapped to any of the categories. It is not surprising that a large fraction of the queries are NEs. Along with Websites , NEs form 50% of the unique queries, though when query frequencies are taken into account NEs only constitute 6% of all queries. Conse-quently, processing of transliterated NEs has received some attention from the IR researchers [21]. Entertainment is the second largest category (27%), of which movies and songs are the most searched categories. These queries are typically longer and more complex than NE queries, and constitutes more than 32% of the transliterated query traffic. Yet, this category has hardly received any special attention from the researchers [6, 11]. We believe that Entertainment is a rich and practically important domain for MSIR, and hence we conduct our MSIR experiments on Hindi song lyrics dataset obtained from [11].
Having defined the basic MSIR setting, and establishing its prevalence in Web search, we now present an approach for modelling the mixed-script space that in turn allows us to develop a MSIR system.
As discussed in Sec 2, the primary challenge in MSIR is to model and match the words across the scripts in the pres-ence of a large number of spelling variations of the words, especially in the non-native script. We shall refer to the variants of the same word in the native and other scripts as term equivalents . The term matching problem could be addressed by using existing approaches such as approxi-mate string matching [14, 34] and transliteration mining [30, 21, 20]. The former techniques can be used to handle the spelling variation in a single script (especially, variations in non-native script), while the latter can help in matching the Category Topics/sub-categories Cue words Example query % of q ueries
Source magazines/news patrika , times, vasundhara eenadu 3 (14.52) Culture Religion festival, god, lord ah oi ashtami 2011 0.4 (0.02)
Others -meaning vi bhaa meaning 0.01 (0.01) terms across the scripts. However, these methods cannot be di rectly applied to solve the term matching problem for a single as well as across multiple scripts at the same time.
Therefore, we propose a framework in which the terms across the scripts are modelled jointly. We achieve this by learning a low-dimensional representation of terms in a com-mon abstract space where term equivalents are close to each other. The core of our approach lies in learning such abstract representation. The concept of common abstract space for mixed-script modelling is based on the fundamental obser-vation that words are transliterated into a non-native script in such a way that the sound or pronunciation is preserved.
We treat the phonemes as character-level  X  X opics X  in the terms. There are some attempts on developing topic models using undirected graphical models like restricted Boltzmann machines (RBMs) [10, 27, 28]. The topic models are usually based on the assumption that each document is represented as a mixture of topics, where each topic defines a probability distribution over words. Similarly, we consider the terms to be represented as mixture of  X  X opics X , where each topic defines a probability distribution over character n-grams.
Phonemes of the language can be captured by the charac-ter n-grams. Consider the feature set F = { f 1 , . . . , f taining character grams of scripts s i for all i  X  { 1 , ., r } and |F| = K . Let t 1 = S i =1 ...r w 1 ,i be a datum from training data T of language l 1 where w 1 ,i represents word w written in language l 1 and script s i where r is the number of scripts being modelled jointly. The datum can be represented as K -dimensional feature vector x where x k is the count of k feature f k  X  F in datum t 1 . We observe that count data of character grams within terms follow Dirichlet-multinomial distribution. Consider N independent draws from a cate-gorical distribution with K categories . In the present con-text, N = P K i x i and { f 1 , . . . , f K } are K categories, where the number of times a particular feature f k occurs in the datum t 1 is denoted as x k . Then x = ( x 1 , . . . , x K multinomial distribution with parameters N and p , where p = ( p 1 , . . . , p K ) and p k is the probability that k takes value x k . The parameter p in our case is not directly available hence, we give it a conjugate prior distribution. Therefore, it is drawn from a Dirichlet distribution with pa-rameter vector  X  = (  X  1 , . . . ,  X  K ). The hyperprior vector can be seen as pseudocounts ( i.e. , counts of each feature ob-served in reference collection) and  X  k = x k collection. Such formulation can be expressed as follows:
The proposed model is based on the non-linear dimen-sionality reduction methods like deep autoencoder [15]. The RBMs are stacked on top of each other to constitute a deep architecture. The bottom-most RBM of our model, which models the input terms, is character-level variant of the replicated softmax (RSM) model presented in [28] for docu-ments. Despite character n-grams follow Dirichlet-multinomial distribution, we can model them under RSM because dur-ing the inference using methods like Gibbs sampling, Dirich-let prior distributions are often marginalised out. Let v  X  { 0 , 1 ,  X   X   X  , N } K represent visible multinomial units and let h  X  { 0 , 1 } m be stochastic binary hidden latent units. Let v be K -dimensional input vector such as feature vector x for datum t 1 , h be m -dimensional latent feature vector and N = P K i =1 x i . The energy of the state { v , h } is defined as: where, v i is the count data x i , W i j is the weight matrix entry between i th visible node and j th hidden node, while a i and b are bias terms of visible and hidden layers respectively. The conditional distributions are given by softmax and logistic functions as below,
As argued in [27], a single layer of binary features may not be the best way to capture complex structure in the count data, more layers are added to create a deep autoen-co der [15]. The further binary RBM X  X  are stacked on top of each other in such a way that output of the bottom RBM is the input to the above RBM. The conditional distribu-tions of these binary RBMs are given by logistic functions as below,
The topic models for documents are usually trained over a subset of vocabulary ( top-n terms) and hence, they have to deal with the non-trivial problem of marginalising over un-observed terms. On the contrary, the term level topic model, proposed in this work, is immune to this problem because the size of phonemes (captured by the character n-grams) for a language is finite and fairly small (only for languages with finite set of alphabets e.g. English with alphabets a-z ). Hence, enough evidence for all the phonemes is found even in a small to moderate size training data, which increases the suitability of our approach to the problem.

For example, without loss of generality consider the to-tal number of scripts in datum being modelled r = 2 for language Hindi where s 1 be the Devanagari script with 50 letters and s 2 be the Roman script (as used in English or-thography) with 26 letters. Then, the size of the feature set F , considering character uni/bi-gram features, is upper bounded by K = 3252 (=26 + 26 2 + 50 + 50 2 ). The architecture of the autoencoder is shown Fig. 1 (a). The visible layer of the bottom-most RBM is character level replicated softmax layer as described in Section 4.2. The character uni and bi grams of the training datum ( r = 2) constitute the feature space F . The hidden layer of the top-most RBM is linear which represents the terms as low-dimensional embedding in the abstract space. The autoen-coder is trained in two phases: i) greedy layer-wise pre-training and; ii) fine-tuning through backpropagation. Dur-ing pre-training, each RBM is trained using contrastive di-vergence (CD 1 ) learning for 50 epoch where CD 1 refers to CD with 1 step of alternating Gibbs sampling [16]. Once the network is pre-trained, the autoencoder is unrolled as shown in Fig. 1 (b) and the cross-entropy error between the input and its reconstruction (output) is backpropagated to adjust the weights of the entire network.

As shown in Fig. 1 (a), the autoencoder is trained with native form and its transliterated form together. In this way, the model is able to learn character level  X  X opic X  X istribution over the features of both scripts jointly.
Once the model is trained, equivalents discovery involves two steps: i) preparing the index of mining lexicon in ab-stract space (offline) and; ii) finding equivalents for the query term (online). The lexicon of the reference collection (ideally mixed-script) which is used to find term equivalents is referred as mining lexicon with size n . The former step is a one-time offline process in which the m -dimensional abstract representation for each term in mining lexicon is obtained as shown in Fig. 1 (c) ( x 1  X  K  X  h 1  X  m ). These representations are stored in index against each term. This index can be seen as an n  X  m matrix H where h  X  H . While the lat-ter step involves projecting the query term into the abstract space ( x q  X  h q ) and calculating the similarity with all the terms in the index. It can be seen as a matrix multiplica-tion operation H h T q considering the similarity function to be cosine . All the terms with sim ( h , h q ) &gt;  X , h  X  H are considered as equivalents of the query word w q where  X  is similarity threshold.
Now we describe the experimental set up for evaluating the effectiveness of the proposed method for retrieval in Mixed-Script space .
We used the FIRE 2013 shared task collection on Translit-erated Search [26] for experiments and training. The dataset comprises of document collection, queryset ( Q ) and rele-vance judgments. The collection ( D 1 ) contains 62,888 doc-uments containing song title and lyrics in Roman, Devana-gari and mixed scripts. Statistics of the document collection is given in Table 2 (a). The Q contains 25 lyrics search queries for Bollywood songs in Roman script with mean query length of 4.5 words. Table 2 (b) lists a few exam-ples of queries from Q .
 No. of Documents 62,888 Tokens 12,738,191
Vocabulary 135,243
Th e experimental setup is a standard adhoc retrieval set-ting.The document collection is first indexed to create an in-verted index and the index lexicon is used as mining lexicon. Being this a lyrics retrieval set up, the sequential informa-tion among the terms is crucial for effectiveness evaluation, e.g.  X  love me baby  X  and  X  baby love me  X  are completely differ-ent songs. In order to capture the word-ordering we consider word 2-grams as a unit for indexing and retrieval.
The non-trivial part of mixed-script IR is query-enrichment to handle the challenges described in Sec. 2. In order to en-rich the query with equivalents, we find the equivalents of the query terms as described in Section 4.5 and the word 2-gram query is formulated as shown in [13].
We consider a variety of systems to be compared with the proposed method. The query formulation is similar for all the systems including the retrieval settings like inverted index, retrieval model and mining lexicon except the method of finding the equivalents. 1. Na  X  X ve : The original query terms are used for the query 2. Na  X  X ve + Trans : The original query terms and their
Ya hoo ! Transliteration: http://transliteration.yahoo.com/ 3. LSI : In this system linear dimensionality reduction tech-4. Editex : An approximate string matching algorithm 5. CCA : The problem of finding equWe ivalents is formu-
We evaluate the effectiveness of the proposed method, referred as Deep and compare it with all the baseline sys-tems. The retrieval performance is measured in terms of mean average precision (MAP) and mean reciprocal rank (MRR) evaluation measures. For each query we evaluated the ranklist composed of top 10 documents. The ranking model is parameter free divergence from randomness (un-supervised DFR) as described in [2] which is shown to be suitable for short queries. The results averaged over Q are presented in Table 3. For Deep , the dimensionality selection was based on our previous experience as described in [12]. For LSI , we tried different dimensionalities in the range of [50,200] with step size of 50 but did not observe any statis-tical significant difference in performance. For CCA , we used the implementation from the original authors optimised for English and Hindi language pair. The code for Deep is made publicly available 5 .
 Table 3: The results of retrieval performance mea-sured by MAP and MRR.

The results in Table 3 are presented after the parameter tu ning of  X  which is better explained later in this section. High MRR score achieved by Deep describes its ability to fetch the first relevant document at very high ranks, a de-sirable quality for Web search in addition to better overall ranking measured by MAP. Although Editex is devised for English and able to operate only in the Roman script space, it performs comparable to CCA and LSI . In order to make a fair comparison, we report two more configurations: Deep-Mono which considers only Roman script equivalents and Editex+Trans in which automatic transliteration of terms are added to enrich Editex . The results clearly outline the superiority of our method for query enrichment. When compared with linear methods such as PCA and CCA which have linear objective functions, the strong performance of Deep suggests that objective function for modelling terms in mixed-script space is actually non-linear and not convex. Because of space constraints, we have left for future work the detailed analysis and comparison of features captured by autoencoder at their different layers. A statistical com-parison of methods is presented in Table 4. There is no ht tp://www.dsic.upv.es/~pgupta/mixed-script-ir. html y denotes p -value according to paired significance T-Test. significant difference in performance of Na  X  X ve+Trans, LSI, Editex and CCA while Deep significantly outperforms all the baselines, as shown with dark-gray b/g, which clearly shows that term equivalents found by Deep are better than the other methods.
 Figure 2: Number of equivalents found in abstract sp ace at similarity threshold (  X  ) ( c.f. Section 4.5).
We present an analysis on the impact of  X  on number of equivalents, which is directly related to the query latency. Fig. 2 depicts the average number of equivalents for each query term wrt corresponding  X  . As can be noticed in Fig. 2, CCA shows a steep increase in number of equivalents which shows, CCA has very dense population in the abstract space and therefore, has around  X  40 equivalents even at a strict threshold of 0.99. Rather Deep and LSI show a moderate increase in the number of equivalents wrt  X  value. Figure 3: Impact of similarity threshold (  X  ) o n re-trieval performance. CCA  X  follows the ceiling X-axis range [0.999-0.99].

We also show how the  X  affects the retrieval performance in Fig. 3. The parameter sweep for  X  is [0.99-0.90] with step of 0.01. Deep exhibits the best performance throughout the tuning range. For CCA we also considered  X  between [0.999-0.99] with step size of 0.001 to better capture its peak performance as shown in Fig. 3 with CCA  X  .
 Figure 4: Snippet of mining lexicon projected in ab stract space using Deep .

Finally we show the power of Deep for finding equivalents by showing a snippet of 20D abstract space as 2D-view in Fig. 4. It can be noticed that mixed-script equivalents of the terms are very close to each other in small clusters and such clusters are well separated from each other. The 2D repre-sentation is achieved using the t-SNE algorithm 6 . We show equivalents of a few terms found using Deep with  X  =0.96 in Table 5. The category  X  X ot sure X  depicts the cases where the terms are quite close to the desired term but not cor-rect may be due to a typo e.g. ehaas vs. ehsaas where the former is not a valid Hindi word.
 Table 5: Examples of the variants extracted using Deep with similarity threshold 0.96 (words beginning with ! and ? mean  X  X rong X  and  X  X ot sure X  respec-tively).

Term Variants ehsaas ehsas, ehasas, ehsaass, eh sAs , ehasaas, mujhe muhjhe, !mujhme, ?mujhea, m J , !mujheme, bawra bawara, baawra, bavra, ! br vA , bawaraa, pe p , ! p r , pee, ! Up , ? pe
Am ong the two steps involved in finding equivalents listed in Sec. 4.5, the indexing step, being one-time and offline, is not a major concern. But the real time similarity estima-tion during the online step while searching for equivalents is very crucial for timely retrieval. As the similarity estima-tion step is essentially a matrix multiplication operation, it ht tp://homepage.tudelft.nl/19j49/t-SNE.html can be easily parallelised using multi-core CPUs or GPUs. In our case the size of mining lexicon n =135,243 and ab-stract space dimensionality m =20. Using a multi-threading framework for matrix multiplication under normal CPU load it takes on an average 0.238 Seconds 7 for the step (ii) to find equivalents for each query word. The time-taken is directly proportional to the mining lexicon size n , dimensionality m and the number of CPU/GPU cores.
Although MSIR has attained very little attention explic-itly, many tangentially related problems like CLIR and translit-eration for IR do discuss some of the issues of MSIR. While languages like Chinese and Japanese use multiple scripts [24], they may not illustrate the true complexity of the MSIR sce-nario envisaged here because there are standard rules and preferences for script usage and well defined spellings rules. In Roman transliteration of Hindi, for example, there are no standard rules leading to a large number of variations. Fur-thermore, these texts are often mixed with English, which makes detection of transliterated text quite difficult.
CLIR typically involve translating queries from one lan-guage to another. However, it is often a reasonable choice to transliterate certain OOV words, especially the Named En-tities (NEs). While NEs have been worked on extensively in IR and CLIR, transliterated queries where the text, in addi-tion to NE, is represented in the script of another language, typically English, have not received adequate attention. In an analysis of the query logs for Greek web users, Efthimi-adis et al. [8] has shown that 90 percent of the queries are formulated using the Roman alphabet while only 8% use the Greek alphabet, and the reason for this [7] is that 1 in 3 Greek navigational queries fail due to the low level of in-dexing by the search engines of the Greek Web. Want et al. [32] employ a translation based method to classify non-English based queries using an English taxonomy system. Though their method shows some promise, it is heavily de-pendent on the availability of translation systems for the language pairs in question. Ahmed et al. [1] show that the problem of transliteration is compounded by the fact that due to a lack of standardization in the way a local language is mapped to the Roman script, there is a large variation in spellings. In their work on query-suggestion for a Bollywood Song Search system [6] also stress on the presence of valid variations in spelling Hindi words in Roman script. Related work by [11] goes into the details of handling these variations while mining transliterated pairs from Bollywood song lyric data crawled from the Web. Edit-distance based approaches have also been popular for the generation of such pairs ([29] for English-Telugu, [17] for Tamil-English, for example). [23] propose a method for normalization of transliterated text that combines two techniques: a stemmer based method that deletes commonly used suffixes [22] with rules for mapping variants to a single canonical form. A similar method that uses both stemming and grapheme-to-phoneme conversion is used by [25] to develop a proof-of-concept for a multilingual search engine for 10 Indian languages. Thus, though there has been some interest in the past especially with respect to handling variation and normalization of transliterated text, on the whole the challenge of IR in the mixed-script space is largely neglected.
We used Intel Xeon CPU E5520 @ 2.27GHz with 4 cores, 8 processors and 12GiB memory.

For languages like Japanese, Chinese, Arabic and most In-dian languages, the challenge of text input in native script means that there is a proliferation of transliterated docu-ments on the web. While the availability of more sophisti-cated and user-friendly input methods with time has helped resolve this for some of these languages (for example Japanese and Chinese), there is still a large number of languages for which the English keyboard and hence the Roman script remains the main input medium. Further, as a number of relevant documents are available in both the native script and its transliterated form, it also becomes important to deal with not only Crosslingual but Mixed-Script retrieval for such languages. Social media is another domain where the use of transliterated text is widespread. Here text nor-malization is complicated further by the presence of SMS-like contractions and interjections, and Code-mixing or the switching between languages at phrase, word and morpho-logical levels. As IR becomes more pervasive in social media, dealing with the complexities of transliteration will become more significant for a robust search engine. Although a very important and prevalent problem, Mixed-Script IR (MSIR) has attained very little attention. In this study, the problem of MSIR is introduced formally along with the involved research challenges. We also fill the void of a quantitative analysis of how much Web search traffic is actually affected by MSIR through a large-scale empirical study of Bing query logs, and thereby outline the prevalence and impact of the phenomenon.
 A principled solution to address the primary challenge of MSIR, the term variations across the scripts, is proposed. The proposed mixed-script joint model learns abstract rep-resentation of terms across the scripts through deep-learning architecture such that term equivalents are close to each other. The deep autoencoder based approach provides highly discriminative and powerful representation for terms with as low dimension as m =20. An extensive empirical analysis is presented of experiments with a practical and important use-case, adhoc retrieval of songs lyrics. Our experiments sug-gest that the state-of-the-art methods for handling spelling variation and transliteration mining have strong effect on success of IR in Mixed-Script space but the proposed method significantly outperforms them.

We lay the stepping stone to the larger goal of MSIR and in future, one should also deal with the associated research avenues such as code-mixing in queries and documents and more general setup of MSIR such as Mixed-Script Multilin-gual IR (MS-MLIR).
We thank Rishiraj Saha Roy(IIT Kharagpur), Rohan Ra-manath (CMU), Prasenjit Majumder (DA-IICT) and Ko-mal Aggarwal (DA-IICT) for helping in corpus creation. The work of the first and last author is supported by WIQ-EI (No. 269180) and DIANA APPLICATIONS (TIN2012-38603-C02-01) projects. Part of this work was carried during the first author X  X  internship at I 2 R, Singapore.
Spandana Gella  X  X  (Univesity of Melbourne, email: sgella@ student.unimelb.ed ) and Jatin Sharma (Microsoft India R&amp;D Pvt Ltd, email: jatinsha@microsoft.com ). th e work was done during the author X  X  internship at Mi-crosoft Research India [1 ] U. Z. Ahmed, K. Bali, M. Choudhury, and S. VB. [2] G. Amati. Frequentist and bayesian approach to [3] H.-H. Chen, S.-J. Hueng, Y.-W. Ding, and S.-C. Tsai. [4] M. Choudhury, K. Bali, K. Gupta, and N. Datha. [5] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [6] N. Dua, K. Gupta, M. Choudhury, and K. Bali. Query [7] E. N. Efthimiadis. How do greeks search the web?: A [8] E. N. Efthimiadis, N. Malevris, A. Kousaridas, [9] J. C. French, A. L. Powell, and E. Schulman.
 [10] P. V. Gehler, A. D. Holub, and M. Welling. The rate [11] K. Gupta, M. Choudhury, and K. Bali. Mining [12] P. Gupta, R. E. Banchs, and P. Rosso. Squeezing [13] P. Gupta, P. Rosso, and R. E. Banchs. Encoding [14] P. A. V. Hall and G. R. Dowling. Approximate string [15] G. Hinton and R. Salakhutdinov. Reducing the [16] G. E. Hinton. Training products of experts by [17] S. C. Janarthanam, S. Subramaniam, and [18] B. King and S. Abney. Labeling the languages of [19] K. Knight and J. Graehl. Machine transliteration. [20] S. Kumar and R. Udupa. Learning hash functions for [21] A. Kumaran, M. M. Khapra, and H. Li. Report of [22] D. W. Oard, G.-A. Levow, and C. I. Cabezas. Clef [23] D. Pal, P. Majumder, M. Mitra, S. Mitra, and A. Sen. [24] Y. Qu, G. Grefenstette, and D. A. Evans. Automatic [25] A. A. Raj and H. Maganti. Transliteration based [26] R. Saha Roy, M. Choudhury, P. Majumder, and [27] R. Salakhutdinov and G. Hinton. Semantic hashing. [28] R. Salakhutdinov and G. E. Hinton. Replicated [29] V. B. Sowmya and V. Varma. Transliteration based [30] R. Udupa and M. M. Khapra. Improving the [31] R. Udupa and M. M. Khapra. Transliteration [32] X. Wang, A. Broder, E. Gabrilovich, V. Josifovski, [33] D. Zhou, M. Truran, T. Brailsford, V. Wade, and [34] J. Zobel and P. Dart. Phonetic string matching:
