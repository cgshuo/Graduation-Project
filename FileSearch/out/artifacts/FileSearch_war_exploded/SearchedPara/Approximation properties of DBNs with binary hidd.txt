 Oswin Krause Oswin.Krause@diku.dk Asja Fischer Asja.Fischer@ini.rub.de Tobias Glasmachers Tobias.Glasmachers@ini.rub.de Christian Igel igel@diku.dk Restricted Boltzmann machines (RBMs, Smolensky, 1986; Hinton, 2002) and deep belief networks (DBNs, Hinton et al., 2006; Hinton &amp; Salakhutdinov, 2006) are probabilistic models with latent and observable vari-ables, which can be interpreted as stochastic neural networks. Binary RBMs, in which each variable con-ditioned on the others is Bernoulli distributed, are able to approximate arbitrarily well any distribution over the observable variables (Le Roux &amp; Bengio, 2008; Montufar &amp; Ay, 2011). Binary deep belief networks are built by layering binary RBMs, and the repre-sentational power does not decrease by adding layers (Le Roux &amp; Bengio, 2008; Montufar &amp; Ay, 2011). In fact, it can be shown that a binary DBN never needs more variables than a binary RBM to model a distri-bution with a certain accuracy (Le Roux &amp; Bengio, 2010).
 However, arguably the most prominent applications in recent times involving RBMs consider models in which the visible variables are real-valued (e.g., Salakhutdi-2010; Le Roux et al., 2011). Welling et al. (2005) pro-posed a notion of RBMs where the conditional dis-tributions of the observable variables given the latent variables and vice versa are (almost) arbitrarily cho-sen from the exponential family. This includes the important special case of the Gaussian-binary RBM (GB-RBM, also Gaussian-Bernoulli RBM), an RBM with binary hidden and Gaussian visible variables. Despite their frequent use, little is known about the approximation capabilities of RBMs and DBNs mod-eling continuous distributions. Clearly, orchestrating a set of Bernoulli distributions to model a distribu-tion over binary vectors is easy compared to approx-imating distributions over  X   X  R m . Recently, Wang et al. (2012) have emphasized that the distribution of the visible variables represented by a GB-RBM with n hidden units is a mixture of 2 n Gaussian distribu-tions with means lying on the vertices of a projected n -dimensional hyperparallelotope. This limited flexibil-ity makes modeling even a mixture of a finite number of Gaussian distributions with a GB-RBM difficult. This work is a first step towards understanding the representational power of DBNs with binary latent and real-valued visible variables. We will show for a subset of distributions relevant in practice that DBNs with two layers of binary hidden units and a fixed family of conditional distribution for the visible units can model finite mixtures of that family arbitrarily well. As this also holds for infinite mixtures of Gaussians with fixed variance restricted to a compact domain, our results imply universal approximation of strictly positive den-sities over compact sets. This section will recall basic results on approximation properties of mixture distributions and binary RBMs. Furthermore, the considered models will be defined. 2.1. Mixture distributions A mixture distribution p mix ( v ) over  X  is a convex combination of simpler distributions which are mem-bers of some family G of distributions over  X  pa-rameterized by  X   X   X . We define MIX( n,G ) = ily of mixtures of n distributions from G . Further-more, we denote the family of infinite mixtures of dis-R Li &amp; Barron have shown that for some family of distributions G every element from CONV( G ) can be approximated arbitrarily well by finite mixtures with respect to the Kullback-Leibler divergence (KL-divergence): Theorem 1 (Li &amp; Barron, 2000) . Let f  X  CONV ( G ) . There exists a finite mixture p mix  X  MIX ( n,G ) such that where and  X  = 4[log(3 The bound is not necessarily finite. However, it follows from previous results by Zeevi &amp; Meir (1997) that for every f and every &gt; 0 there exists a mixture p mix with n components such that KL( f k p mix )  X  + c n for some constant c if  X   X  R m is a compact set and f is continuous and bounded from below by some  X  &gt; 0 (i.e,  X  x  X   X  : f ( x )  X   X  &gt; 0).
 Furthermore, it follows that for compact  X   X  R m ev-ery continuous density f on  X  can be approximated arbitrarily well by an infinite but countable mixture of Gaussian distributions with fixed variance  X  2 and means restricted to  X , that is, by a mixture of distri-butions from the family
G  X  ( X ) = p ( x ) = for sufficient small  X  . 2.2. Restricted Boltzmann Machines An RBM is an undirected graphical model with a bipartite structure (Smolensky, 1986; Hinton, 2002) consisting of one layer of m visible variables V = ( V 1 ,...,V m )  X   X  and one layer of n hidden variables H = ( H 1 ,...,H n )  X   X . The modeled joint distri-with energy E and normalization constant Z = R are mutually independent given the state of the other layer. 2.2.1. Binary-Binary-RBMs In the standard binary RBMs the state spaces of the variables are  X  = { 0 , 1 } m and  X  = { 0 , 1 } n . The energy is given by E ( v , h ) =  X  v T Wh  X  v T b  X  c T h with weight matrix W and bias vectors b and c .
 Le Roux &amp; Bengio showed that binary RBMs are uni-versal approximators for distributions over binary vec-tors: Theorem 2 (Le Roux &amp; Bengio, 2008) . Any distribu-tion over  X  = { 0 , 1 } m can be approximated arbitrarily well (with respect to the KL-divergence) with an RBM with k + 1 hidden units, where k is the number of input vectors whose probability is not zero.
 The number of hidden neurons required can be reduced to the minimum number of pairs of input vectors differ-ing in only one component with the property that their union contains all observable patterns having positive probability (Montufar &amp; Ay, 2011). 2.2.2. Exponential-Family RBMs Welling et al. (2005) introduced a framework for con-structing generalized RBMs called exponential family harmoniums. In this framework, the conditional distri-belong to the exponential family. Almost all types of RBMs encountered in practice, including binary RBMs, can be interpreted as exponential family har-moniums.
 The exponential family is the class F of probability distributions that can be written in the form where  X  are the parameters of the distribution and Z is the normalization constant. 1 The functions  X  ( r ) and  X  ( r ) , for r = 1 ,...,k , transform the sample space and the distribution parameters, respectively. Let I be the subset of F where the components of x = ( x 1 ,...,x m ) are independent from each other, that is, I = { p  X  F |  X  x : p ( x 1 ,...,x m ) = p ( x 1 ) p ( x 2 )  X  X  X  p ( x elements of I the function  X  ( r ) can be written as  X  set of I is the family of Gaussian distributions with fixed variance  X  2 , G  X  ( X )  X  X  , see equation (1). Following Welling et al., the energy of an RBM with binary hidden units and visible units with p ( v | h )  X  X  is given by
E ( v , h ) =  X  not every possible choice of parameters necessarily leads to a finite normalization constant and thus to a proper distribution. If the joint distribution is properly defined, the condi-tional probability of the visible units given the hidden is p ( v | h ) = where Z h is the corresponding normalization constant. Thus, the marginal distribution of the visible units p ( v ) can be expressed as a mixture of 2 n conditional distributions: 2.3. Deep Belief Networks A DBN is a graphical model with more than two layers built by stacking RBMs (Hinton et al., 2006; Hinton &amp; Salakhutdinov, 2006). A DBN with two layers of hidden variables H and  X  H and a visible layer V is characterized by a probability distribution p ( v , h , that fulfills In this study we are interested in the approximation properties of DBNs with two binary hidden layers and real-valued visible neurons. We will refer to such a DBN as a B-DBN . With B-DBN( G ) we denote the family of all B-DBNs having conditional distributions p ( v | h )  X  G for all h  X  H . This section will present our results on the approxima-tion properties of DBNs with binary hidden units and steps:  X  Lemma 3 gives an upper bound on the KL- X  Corollary 6 specifies the previous theorem for the  X  Finally, Theorem 7 generalizes the results to in-3.1. Finite mixtures We first introduce a construction that will enable us to model mixtures of distributions by DBNs. For some family G an arbitrary mixture of distributions p mix ( v ) = P v  X   X  can be expressed in terms of a joint proba-bility distribution of v and h  X  X  0 , 1 } n by defining the distribution over { 0 , 1 } n , where e i is the i th unit vec-tor. Then we can rewrite p mix ( v ) as p mix ( v ) = P This can be interpreted as expressing p mix ( v ) as an el-ement of MIX(2 n ,G ) with 2 n  X  n mixture components having a probability (or weight) equal to zero. Now we can model p mix ( v ) by the marginal distribution P h p ( v | h ) p ( h ) of a B-DBN p ( v , h , with the following properties: 1. p ( v | e i ) = p mix ( v | i ) for i = 1 ,...,n and 2. p ( h ) = P  X  h p ( h ,  X  h ) approximates q mix ( h ). Following this line of thoughts we can formulate our first result. It provides an upper bound on the KL-divergence of any element from MIX( n,G ) and the marginal distribution of the visible variables of a B-DBN with the properties stated above, where p ( h ) models q mix ( h ) with an approximation error smaller than a given .
 Lemma 3. Let p mix ( v ) = P n i =1 p mix ( v | i ) p mix MIX ( n,G ) be a mixture with n components from a family of distributions G , and q mix ( h ) be defined as divergence between p mix and p is bounded by where B ( G,p mix , ) = with and and p mix ( v ) = P h p ( v | h ) q mix ( h ) we can write where  X  ( v ) is defined as above. Thus, we get for the KL-divergence KL( p k p mix ) =  X  using F (0 , v ) = 0. Because 1 + x  X  (1 + x )(1 + ) for all x,  X  0, we can upper bound  X   X  F ( , v ) by  X  with  X  ( v ) as defined above. By integration we get F ( , v ) = Integration with respect to v completes the proof. The proof does not use the independence properties of p ( v | h ). Thus, it is possible to apply this bound also to mixture distributions which do not have conditionally independent variables. However, in this case one has to show that a generalization of the B-DBN exists which can model the target distribution, as the formalism introduced in formula (3) does not cover distributions which are not in I .
 For a family G  X  I it is possible to construct a B-DBN with the properties required in Lemma 3 under weak technical assumptions. The assumptions hold for families of distributions used in practice, for instance Gaussian and truncated exponential distributions. Lemma 4. Let G  X  I and p mix ( v ) = P i =1 p mix ( v | i ) p mix ( i )  X  MIX ( n,G ) with for i = 1 ,...,n and corresponding parameters  X  by equation (5) . Assume that there exist parameters b ( r ) such that for all c  X  R n the joint distribution p ( v , h ) of v  X  R m and h  X  X  0 , 1 } n with energy is a proper distribution (i.e., the corresponding nor-malization constant is finite), where the i th column of For all &gt; 0 there exists a B-DBN with joint distri-that i) p mix ( v | i ) = p ( v | e i ) for i = 1 ,...,n and ii)  X  h  X  X  0 , 1 } n : | p ( h )  X  q mix ( h ) | &lt; . Proof. Property i) follows from equation (4) by setting h = e i and the i th column of W ( r ) to  X  ( r ) (  X  ( i ) Property ii) follows directly from applying Theorem 2 to p .
 For some families of distributions, such as truncated exponential or Gaussian distributions with uniform variance, choosing b ( r ) = 0 for r = 1 ,...,k is sufficient to yield a proper joint distribution p ( v , h ) and thus a B-DBN with the desired properties. If such a B-DBM exists, one can show, under weak additional assump-tions on G  X  X  , that the bound shown in Lemma 3 is finite. It follows that the bound decreases to zero as does.
 Theorem 5. Let G  X  I be a family of densities with p mix ( v | i ) given by equation (6) . Furthermore, let q mix ( h ) be given by equation (5) and let p ( v , h , B-DBN ( G ) with (i) p mix ( v | i ) = p ( v | e i ) for i = 1 ,...,n (ii)  X  h  X  X  0 , 1 } n : | p ( h )  X  q mix ( h ) | &lt; (iii)  X  h  X  X  0 , 1 } n : R Then B ( G,p mix , ) is finite and thus in O ( ) . Proof. We have to show that under the conditions given above R  X   X  ( v )  X  ( v ) d v is finite. p mix ( v ) = P tion, by defining i  X  = arg min i p mix ( v | i ) and h arg max h p ( v | h ) we get p The conditional distribution p mix ( v | i ) of the mixture can be written as in equation (6) and the conditional distribution p ( v | h ) of the RBM can be written as in formula (4). We define and get  X  exp Note that the last expression is always larger or equal to one. We can further bound this term by defining and arrive at By plugging these results into the formula for  X  ( v ) we obtain In the third step, we used that the second term is leads to Z  X  which is finite by assumption. 3.2. Finite Gaussian mixtures Now we apply Lemma 4 and Theorem 5 to mixtures of Gaussian distributions with uniform variance. The KL-divergence is continuous for strictly positive distributions. Our previous results thus imply that for every mixture p mix of Gaussian distributions with uniform variance and every  X   X  0 we can find a B-DBN p such that KL( p k p mix )  X   X  . The following corollary gives a corresponding bound: Corollary 6. Let  X  = R m and G  X  ( X ) be the family of Gaussian distributions with variance  X  2 . Let &gt; 0 and p mix ( v ) = P mixture of n distributions with means z ( i )  X  R m , i = 1 ,...,n . By we denote the edge length of the smallest hypercube containing all means. Then there exists p ( v , h , B-DBN ( G  X  ( X )) , with  X  h  X  X  0 , 1 } n : | p ( h )  X  q mix and p mix ( v | i ) = p ( v | e i ) , i = 1 ,...,n , such that
KL( p k p mix )  X   X  2 n ( n + 1) log(2) + m Proof. In a first step we apply an affine linear trans-formation to map the hypercube of edge length D to the unit hypercube [0 , 1] m . Note that doing this while transforming the B-DBN-distribution accordingly does not change the KL-divergence, but it does change the standard deviation of the Gaussians from  X  to  X /D . In other words, it suffices to show the above bound for D = 1 and z ( i )  X  [0 , 1] m .
 The energy of the Gaussian-Binary-RBM p ( v , h ) is typically written as
E ( v , h ) = with weight matrix W and bias vectors b and c . This can be brought into the form of formula (3) by setting k = 2,  X  (1) j ( v j ) = v j ,  X  (2) j ( v j ) = v 2 j , W (and thus b (1) = 0 ), it follows from Lemma 4 that a and (ii) from Theorem 5 exists.
 It remains to show that property (iii) holds. Since the conditional probability factorizes, it suffices to show that (iii) holds for every visible variable individually. The conditional probability of the j th visible neuron of the constructed B-DBN is given by where the mean z j ( h ) is the j th element of Wh . Using this, it is easy to see that Z because it is the second moment of the normal distri-bution. For R  X   X  X  X  p ( v j | h ) |  X  (1) ( v j ) | d v j  X  =  X  z j ( h ) + =  X  z j ( h ) +  X  z j ( h ) + In the last step we used that z j ( e i ) = z ( i ) by construction and thus z j ( h ) can be bounded from above by Thus it follows from Theorem 5 that the bound from Lemma 3 holds and is finite. To get the actual bound, we only need to find the constants  X  (1) and  X  (2) to be inserted into (9). The first constant is given by  X  (11) shows. The second constant is given by  X  (2) inequality (9) leads to the bound.
 restricted to a compact subset of R m . This can easily be verified by adapting equation (10) accordingly. Similar results can be obtained for other families of distributions. A prominent example are B-DBMs with truncated exponential distributions. In this case the the binary RBM, but the values of the visible neurons is easy to see that for every choice of parameters the normalization constant as well as the bound are finite. 3.3. Infinite mixtures We will now transfer our results for finite mixtures to the case of infinite mixtures following Li &amp; Barron (2000).
 Theorem 7. Let G be a family of continuous dis-tributions and f  X  CONV ( G ) such that the bound n  X  N . Furthermore, for all p mix-n  X  MIX ( n,G ) , n  X  N , and for all  X  &gt; 0 let there exist a B-DBN KL ( f k p )  X  .
 Proof. From Theorem 1 and the assumption that the corresponding bound is finite it follows that for all &gt; 0 there exists a mixture p mix-n 0  X  MIX( n 0 ,G ) with n  X  2 c 2 f  X / such that KL( f k p mix-n 0 )  X  By assumption there exists a B-DBN  X  B-DBN( G ) such that B ( G,p mix-n 0 ,  X  ) is finite. Thus, one can de-fine a sequence of B-DBNs ( p  X  )  X   X  B-DBN( G ) with  X  decaying to zero (where the B-DBNs only differ in the weights between the hidden layers) for which p KL( f k p mix-n 0 ). Thus, there exists 0 such that these inequalities yields
KL( f k p 0 )  X | KL( f k p 0 )  X  KL( f k p mix-n 0 ) | +KL( f k p mix-n This result applies to infinite mixtures of Gaussians with the same fixed but arbitrary variance  X  2 in all components. In the limit  X   X  0 such mixtures can approximate strictly positive densities over compact sets arbitrarily well (Zeevi &amp; Meir, 1997). We presented a step towards understanding the rep-resentational power of DBNs for modeling real-valued data. When binary latent variables are considered, DBNs with two hidden layers can already achieve good approximation results. Under mild constraints, we showed that for modeling a mixture of n pairwise inde-pendent distributions, a DBN with only 2 n + 1 binary hidden units is sufficient to make the KL-divergence between the mixture p mix and the DBN distribution deep architectures used in practice, for instance DBNs having visible neurons with Gaussian or truncated ex-ponential conditional distributions, and corresponding mixture distributions having components of the same type as the visible units of the DBN. Furthermore, we extended these results to infinite mixtures and showed that these can be approximated arbitrarily well by a DBN with a finite number of neurons. Therefore, Gaussian-binary DBNs inherit the universal approx-imation properties from additive Gaussian mixtures, which can model any strictly positive density over a compact domain with arbitrarily high accuracy. Acknowledgments OK and CI acknowledge support from the Danish National Advanced Technology Foundation through project  X  X ersonalized breast cancer screening X , AF and CI acknowledge support from the German Fed-eral Ministry of Education and Research within the Bernstein Fokus  X  X earning behavioral models: From human experiment to technical assistance X .
 Hinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural Compu-tation , 14(8):1771 X 1800, 2002.
 Hinton, Geoffrey E. and Salakhutdinov, Ruslan. Re-ducing the dimensionality of data with neural net-works. Science , 313(5786):504 X 507, 2006.
 Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief nets. Neural Computation , 18(7):1527 X 1554, 2006.
 Le Roux, Nicolas and Bengio, Yoshua. Representa-tional power of restricted Boltzmann machines and deep belief networks. Neural Computation , 20(6): 1631 X 1649, 2008.
 Le Roux, Nicolas and Bengio, Yoshua. Deep belief net-works are compact universal approximators. Neural Computation , 22(8):2192 X 2207, 2010.
 Le Roux, Nicolas, Heess, Nicolas, Shotton, Jamie, and
Winn, John M. Learning a generative model of im-ages by factoring appearance and shape. Neural Computation , 23(3):593 X 650, 2011.
 Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and
Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th An-nual International Conference on Machine Learning (ICML) , pp. 609 X 616. ACM, 2009.
 Li, Jonathan Q. and Barron, Andrew R. Mixture den-sity estimation. In Solla, Sara A., Leen, Todd K., and M  X uller, Klaus-Robert (eds.), Advances in Neu-ral Information Processing Systems 12 (NIPS 1999) , pp. 279 X 285. MIT Press, 2000.
 Montufar, Guido and Ay, Nihat. Refinements of uni-versal approximation results for deep belief networks and restricted Boltzmann machines. Neural Compu-tation , 23(5):1306 X 1319, 2011.
 Salakhutdinov, Ruslan and Hinton, Geoffrey E. Learn-ing a nonlinear embedding by preserving class neigh-bourhood structure. In Proceedings of the Eleventh
International Conference on Artificial Intelligence and Statistics (AISTATS) , volume 2, pp. 412  X  419, 2007.
 Smolensky, Paul. Information processing in dynamical systems: foundations of harmony theory. In Rumel-hart, David E. and McClelland, James L. (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1 , pp. 194 X 281. MIT Press, 1986.
 Taylor, Graham W., Fergus, Rob, LeCun, Yann, and Bregler, Christoph. Convolutional learning of spatio-temporal features. In Computer Vision  X  ECCV 2010 , volume 6316 of LNCS , pp. 140 X 153. Springer, 2010.
 Wang, Nan, Melchior, Jan, and Wiskott, Laurenz. An analysis of Gaussian-binary restricted Boltzmann machines for natural images. In Verleysen, Michel (ed.), Proceedings of the 20th European Symposium on Artificial Neural Networks, Computational Intel-ligence and Machine Learning (ESANN 2012) , pp. 287 X 292. Evere, Belgium: d-side publications, 2012. Welling, Max, Rosen-Zvi, Michal, and Hinton, Ge-offrey E. Exponential family harmoniums with an application to information retrieval. In Saul, Lawrence K., Weiss, Yair, and Bottou, L  X eon (eds.),
Advances in Neural Information Processing Systems 17 (NIPS 2004) , pp. 1481 X 1488. MIT Press, 2005. Zeevi, Assaf J. and Meir, Ronny. Density estimation through convex combinations of densities: Approx-imation and estimation bounds. Neural Networks ,
