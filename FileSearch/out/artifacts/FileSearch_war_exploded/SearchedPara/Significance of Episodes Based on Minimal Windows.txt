
Discovering episodes, frequent patterns from an event sequence has been a fruitful and active field in pattern mining since their original introduction in [1]. Essentially an episode is a set of events that should occur close to each other with possibly some constraints on the order of the occurrences.

The most common way of defining a quality measure of an episode is the number of windows of fixed length in which the episode can be found. Such a measure is antimonotonic and hence all frequent episodes can be found using AP RIORI approach given in [1]. This quality measure has two significant problems. First, the results will depend greatly on the length of the window. If the window is too small, then some interesting occurrences are ignored. On the other hand, if the window is too large, the behavior of occurrences in a single window is ignored.
 Example 1. Consider a serial episode ( a  X  b ) and two sequences  X  abababababababab  X  and  X  abacbadbaxbagbab  X . If we fix the length of a window to be 6 (or larger), then the number of windows covering the episode will be the same for the both sequences. However, occurrences of the episode in these sequences are different.

The second problem is that this measure has no way of incorporating any background knowledge. For example, assume that we know that event a happens relatively seldom, then we are not surprised by the fact if we observe that an episode containing a also occur seldom. Alternative approaches to deal with either the first problem or the second have been proposed and we discuss them in Section VIII.
In this paper we propose a new quality measure for the episodes. Our approach tackles simultaneously both aforementioned problems. To be more specific, given an episode G , we consider the lengths of minimal windows of G . To include background knowledge we assume that for each symbol we have a probability of its occurrence in the sequence. We then compute the expected length of the minimal window based on a model in which the symbols are independent of each other. We say that the episode is significant if the observed minimal windows have abnormal length, that is, the minimal windows are either too short or too long.
 Example 2. Assume that we have an alphabet of size 3 ,  X  = { a,b,c } . Assume that the probabilities for having a be a serial episode a  X  b . Then s is a minimal window for G if and only if it has a form ac  X  X  X  cb . Hence the probability of a random sequence s of length k to be a minimal window for G is equal to p ( s is a minimal window of G, | s | = k ) = We are interested in a probability of a minimal window having length k . To get this we divide the joint probability by the probability p ( s is a minimal window of G ) = 1 / 6 . Using this normalization we get that the probability of a minimal window having length k is equal to p ( | s | = k | s is a minimal window of G ) = 3 / 4  X  1 / 4 for k  X  2 , and 0 otherwise. In this case the distribution is geometric and the expected length of a minimal window is then 7 / 3  X  2 . 3 .

On the other hand, assume that we have a sequence s = accbabacb . The minimal windows in s are s [1 , 4] , s [5 , 6] , and s [7 , 9] . Hence, the observed average length is (4+2+3) / 3 = 3 .

Computing the probability of the length for a minimal window turns out to be a surprisingly complex problem. We attack this problem in Section IV by introducing a certain graph having episodes as the nodes. Then using this structure we are able to compute the probabilities inductively, starting from simple episodes and moving towards more complex ones.

Our recipe for the mining process is as follows: Given the sequence we first split the sequence in two. The first sequence is used for discovering candidate episodes, in our case episodes that have a large number of minimal windows (see Section VI for more details). Luckily, this condition is antimonotonic and we can mine these episodes using a standard AP RIORI method. We also compute the needed probabilities for the events from the first sequence. Once the candidate episodes are discovered and the model is computed we compare the expected length of a minimal window against the average length of the observed minimal windows from the second sequence using a simple Z -test. This step allows us to prune uninteresting episodes, that is, the episodes that obey the independence model.

The rest of the paper is structured as follows. In Sec-tions II X  X II we introduce the preliminary definitions and notation. In Section IV we lay out our approach for comput-ing the independence model. We introduce our method for evaluating the difference between the observed windows and the independence model in Section V. We discuss mining candidate episodes in Section VI. Our experiments are given in Section VII. We present the related work in Section VIII and we conclude our work with discussion in Section IX.
We begin by presenting preliminary concepts and nota-tions that will be used throughout the rest of the paper. In this section we will introduce the notions of sequence and episodes.

A sequence s = ( s 1 ,...,s L ) is a string of symbols coming from a finite alphabet  X  , that is, we have s Such sequences are generated from random sources, hence we also treat s as a random variable in our analysis. Given a sequence s and two indices i and j , such that i  X  j , we denote by s [ i,j ] = ( s i ,...,s j ) a sub-sequence of s .
An episode G is represented by an acyclic directed graph with labeled nodes, that is G = ( V,E,lab ) , where V = ( v 1 ,...,v K ) is the set of nodes, E is the set of directed edges, and lab is the function lab : V  X   X  , mapping each node v i to its label.

Given a sequence s and an episode G we say that s covers the episode if there is an injective map f mapping each node v to a valid index such that the node and the corresponding sequence element have the same label, s f ( v and that if there is an edge ( v i ,v j )  X  E , then we must have f ( v i ) &lt; f ( v j ) . In other words, the parents of the node v i must occur in s before v i . We define a binary function c ( s ; G ) such that c ( s ; G ) = 1 if and only if s covers G . Traditional episode mining is based on searching episodes that are covered by sufficiently many sub-windows of certain fixed size.

An elementary theorem says that in directed acyclic graph there exists a sink, a node with no outgoing edges. We denote the set of sinks by sinks ( G ) . Given an episode G and a node v , we define G  X  v to be the sub-episode obtained from G by removing v (and the incident edges).

Given a collection of episodes G we say that the collection is downward closed, if for a given G  X  G , each subgraph H of G is included in G . Note that the empty episode is included in G . Throughout the whole paper we will be working with downward closed collections of episodes G .
Episode mining is based on finding episodes that occur often enough in sliding window. We approach the problem from a different angle. Given a sequence and a candidate episode we first discover the set of all minimal windows in which the given episode occurs. Once we have obtained this set we will study the length of these windows. If their distribution is abnormal, either the lengths are too short, or too long, we consider that we have discovered an important episode.

In order to make the preceding discussion more formal, let G be an episode, and let s be a sequence. We say that s is a minimal window for G if G is covered by s but not by any proper sub-window of s . We define a function m ( s ; G ) returning a binary value. The function m ( s ; G ) = 1 if and only if s is a minimal window starting for G .

Let s be a random sequence of length L , and write t = s [1 ,L  X  1] and u = s [2 ,L ] . Note that we can write the probability of s being a minimal window as p ( m ( s ; G )) = p ( c ( s ; G ))  X  p ( c ( t ; G )  X  c ( u ; G ))
Our main focus is the distribution of lengths of minimal windows, that is, we are interested in where s is a random sequence. Note that this distribution is defined for all k . In practice, we compute p ( m ( s ; G )) for k = 1 ,...,K , where K is some suitable predetermined constant. Once these values are computed we normalize them so that p G ( k ) becomes a proper distribution. This is equivalent to saying that we are not interested in minimal windows whose length exceeds K . From now on K will always denote the maximal length of a minimal window.
We should point out that even though we limit ourselves to windows of maximal size K this limitation is not as severe as using windows of fixed size K . While we ignore information of longer windows we still are able to detect any deviation occurring with the length of minimal windows shorter or equal than K . On the other hand, in the fixed window approach the information of the length of minimal window is discarded as long as it is short enough.

We devote this section for computing the distribution of lengths of minimal windows. That is we are given probabilities p ( a ) for each symbol in an alphabet and a set of episodes G and for each G  X  G we wish to compute p ( k ) , the probability of a minimal window of G being of size k , according to the independence model.

Our approach for calculating minimal windows is based on Eq. 1. According to this equation we need to solve the probabilities p ( c ( s ; G )) and building certain finite state machines where the states will correspond to the episodes.
 A. Episode Set as Finite State Machine It will be fruitful to represent the given set of episodes G as a certain finite state machine. To be more pre-cise, we define a finite state machine as a DAG M ( V ( M G ) ,E ( M G )) . The states V ( M G ) are exactly the episodes G . Let X,Y  X  X  be two episodes and let v and w be the corresponding states. An edge e = ( v,w )  X  E ( M G with a label a = lab ( e ) exists if and only if X = Y  X  n , where n is a sink node of Y labeled as a . In other words, there should be an edge between an episode and an episode obtained by removing a sink node with a label a .
 Example 3. We consider a downward set of closed episodes machine M G is given in Figure 1.

Given a state v in M G we say that s covers v if there is a sequence t = { s i 1 ,...,s i N } such that v can be reached when t is given as an input. In that case we set c ( s ; v ) = 1 , and 0 otherwise. Similarly we define m ( s ; v ) , when s covers v but s [2 ,L ] and s [1 ,L  X  1] does not cover v .

Comparing this to the definition of coverage for the episode we see the immediate result.
 Proposition 4. The sequence s covers an episode G  X  G if and only if s covers the corresponding v in M G . Conse-quently, a minimal window of G is equivalent to the minimal window of v .
 Let M be a finite state machine and let v be a state in M We say that v is monotonic if a sequence s covering v also covers any parent state of v . If every state in M is monotonic we say that M is monotonic .

A direct corollary of Proposition 4 states that M G monotonic.
 Lemma 5. The machine M G induced from G is monotonic. Let X be the episode represented by the state v and let Y be the episode represented by the state w . If v covers s , then it must cover X . Since Y is an episode obtained from X by removing one sink, s also covers Y and thus cover w . B. Computing Coverage for States of Simple Machines
In this section we will demonstrate how to compute the probabilities p ( c ( s ; v )) for the state v in M . We will make some simplifying assumption concerning the structure of M , and then in the next section we demonstrate how this limitations can be removed.

We say that a machine M is simple if incoming edges for each state v in M have unique labels. Generally, the episode machine M G is not simple. However, if the episodes G have only nodes with unique labels, then M G will be simple.
Our approach is to compute the coverage of a state v based on the coverage of its parent state.
 Proposition 6. Let M be simple and monotonic. Let v be a state in M , and let s be a random sequence of length L with independent symbols. Let t = s [1 ,L  X  1] be the sub-sequence of s without the last element. Define probability d as p ( c ( s ; v )) = d + p ( c ( t ; v )) . Then p ( c ( t ; v )) , that is, d is the probability of sequence s covering v but t = s [1 ,L  X  1] not covering it. Assume that s is such sequence. This implies that there is an edge e = ( w,v ) with lab ( e ) = s L . Fix s L = lab ( e ) . Since M is simple the only path to reach v must use the unique e . We also must have that t covers w but not v . Note that this probability can be written as where the equality follows from since M is monotonic.
The probability of s L being lab ( e ) is p ( lab ( e )) . The result follows by combining these probabilities.

Let us abuse the notation and write p ( c ( L ; v )) to mean the probability p ( c ( s ; v )) where s is a random sequence of length L . The proposition gives us means to compute p ( c ( L ; v )) in an iterative fashion from p ( c ( L  X  1; v )) and from the coverage of parent state. The algorithm for com-puting the coverage is given in Algorithm 2.
 Algorithm 1 Recursive sub-procedure C OVER S TATE for computing the coverage of state v . 1: for e = ( w,v )  X  E ( M ) do 2: if coverage for w is not computed then 4: end if 5: end for 6: for k = 1 ,...K . do 7: d  X  0 . 8: for e = ( w,v )  X  E ( M ) do 9: x  X  p ( c ( k  X  1; w ))  X  p ( c ( k  X  1; v )) . 10: d  X  d + p ( lab ( e )) x . 11: end for 12: p ( c ( k ; v ))  X  d + p ( c ( k  X  1; v )) . 13: end for Algorithm 2 Algorithm C OVER for computing the coverage of state for a simple and monotonic machine M . 1: s  X  the source state of M . 2: p ( c ( k ; s ))  X  1 , for k = 0 ,...,K . 3: for v sink state in M do 5: end for
To analyze the computational complexity, we first note that computing the coverage of state v requires O ( KL ) steps where L is the number of incoming edges of v . Thus computing the coverage of the complete graph will require O ( | E ( M ) | K ) steps. However, in practice the process is more complex. The computations are not numerically stable due to rounding errors in floating-point numbers. To solve this problem we have to resort to exact rational numbers. Using such numbers implies that simple computations are no longer unit operations making the computation times longer. C. Transforming non-simple Machines
In order to use Algorithm 2 our state machine needs to be simple and monotonic. The machine M G is monotonic but not simple. Luckily, we can define a new simple and monotonic machine from which we can compute the cov-erage. Informally, if we reverse the direction of the edges in M G , then making the machine simple is equal to making the reversed non-deterministic machine deterministic.
In order to make this formal, let us first give some definitions. Let V be a subset of states in M . Let a be a label. We also define sub ( V ; a ) = { w | e = ( w,v )  X  E ( M ) ,lab ( e ) = a,v  X  V } to be the set of parents of each v  X  V connected with an edge having a label a . We define where min( X ) results in minimal states of X with respect to the parenthood in M . This guarantees that par ( V ; a ) contains no state v,w such that v is an ancestor of w . We also need to define to be the set of labels of all incoming edges. We define the closure of V inductively to be the collection of sets of states and cl ( V ) = { V } if in ( V ) is empty.

We are now ready to transform M into a simple machine, which we denote by sm ( M ) . The states of the machine sm ( M ) are Let V be a state in sm ( M ) and let a  X  in ( V ) . Let W = par ( V ; a ) . Note that this state exists in sm ( M ) . We define an edge labeled as a from W to V . Let i be the initial state in M . For M G it is the state corresponding to the empty episode. Then { i } is the initial state for sm ( M ) . From now on { i } will always denote the initial state of sm ( M ) . Example 7. We continue Example 3. Note that M G is not simple since v 1 has two incoming edges with a label a . The transformed machine sm ( M G ) is given in Figure 2. Note that, in addition to the states already existing in M G is has now two extra states, namely { v 2 ,v 3 } and { v 2 ,v note that sm ( M G ) is simple.

It is obvious that sm ( M ) is simple. The following propo-sition reveals the expected relationship between M and sm ( M ) . Proposition 8. Let M be a monotonic machine. Let V = { v 1 ,...,v N } be the state in sm ( M ) . Then a sequence s covers V if and only if s covers at least one v i .
 the proposition holds for all parent states of V .

Assume that s covers V . Let t be a sub-sequence of s that leads sm ( M ) from the source state { i } to V . Let s be the last symbol of s occurring in t . There is a parent state W = { w 1 ,...,w L } which s [1 ,e  X  1] covers. By the induction assumption at least one w k is covered by s [1 ,e  X  1] . If there is v j = w k , then v j is covered by s , otherwise there is v j that has w k as a parent state. The edge connecting v and w k has a label s e . Hence s covers v j also.

To prove the other direction assume that s covers v j . Let t be a sub-sequence that leads M from the source state to v . Let s e be the last symbol occurring in t . Let w be the parent state of v j connected by an edge with a label s Since s e  X  in ( V ) , we must have W as a parent state of V such that either w  X  W or an ancestor state u of w is in W . In the latter case, since M is monotonic, s covers u . In either case, by the induction assumption s [1 ,e  X  1] covers W . Hence s covers V .
 Corollary 9. Sequence s covers v in M if and only if s covers { v } in sm ( M ) .
 Corollary 10. Let M be a monotonic machine, then sm ( M ) is also monotonic.
 Let W be a parent state of V . Let v  X  V such that s covers v . If v  X  W , then s covers W . Otherwise there is an ancestor state w  X  W of v . Since M is monotonic, s covers w and thus W .

The corollaries give us means to compute the coverage of states in M G by solving the coverage of the states sm ( M using Algorithm 2.
 D. Computing Co-coverage
Our last challenge is to compute the term design a special finite state machine, denoted by co ( M ) , in which the coverage of certain states will correspond to the last term in Eq. 1. The construction of this machine is based on the previous machines M = M G and sm ( M ) . To avoid confusion we use v and w for the states in M , V and W for the states in sm ( M ) , and greek letters  X , X ,... for the states in co ( M ) . We proceed by constructing the machine first and then prove that it gives us the desired probabilities.

There are three different kinds of states in co ( M ) . The first group consists of one state, namely  X  = { i } , where { i } state of co ( M ) .

The second group consists of certain pairs of states from sm ( M ) . Let V and W be states in sm ( M ) and write  X  = ( V,W ) . The machine will be constructed in such manner that s will cover  X  in co ( M ) if and only if s covers V and s [2 : L ] covers W in sm ( M ) . In order to achieve this we first define a closure cl (  X  ) = {  X  } X  [ Let v be a state in M that is not the source state. For each label a  X  in ( { v } ) we add the states from cl (( { v } ,par ( { v } ; a ))) into co ( M ) . In addition, we add the states from cl (( { v } , { v } )) . We add an edge with a label a to  X  = ( V 1 ,V 2 ) from  X  = ( W 1 ,W 2 ) if W k = par ( V for k = 1 , 2 with one exception: if par ( V 1 ; a ) = V 2 then instead if connecting  X  to ( { i } , { i } ) we connect  X  to  X  = { i } . We also connect the state ( { i } , { i } ) to  X  with an edge accepting any symbol from the alphabet. See Example 11 for illustration.

We will now define our last group of states. For any state v that is not a source in M we add a state  X  = v . For each a  X  in ( { v } ) , we add an edge with a label a from ( { v } ,par ( { v } ; a )) to  X  . We also add an edge from ( { v } , { v } ) to  X  accepting any symbol outside in ( { v } ) . Example 11. We continue the toy example given in Exam-ple 7. A part of the machine co ( M G ) is given in Figure 3. Namely we show the machine solving the co-coverage for the episodes v 2 = ( b  X  a ) and v 5 = ( b ) .

Now that we have defined our machine we are ready to prove that the coverage of the states of the last group actually corresponds to the last term in Eq 1.
 First, we need to point out a certain property of sm ( M ) . Lemma 12. Let M be a monotonic machine. Let s be a sequence of length L covering a state V in sm ( M ) . Then s [1 ,L  X  1] covers par ( V ; s L ) .
 ing to V does not contain s L . Hence s [1 ,L  X  1] covers V = par ( V ; s L ) . Assume that s L  X  in ( V ) . Let W = par ( V ; s L ) . If s [1 ,L  X  1] covers V , then by the monotonicity of sm ( M ) , s also covers W . If s [1 ,L  X  1] does not cover V , then t must have s L as a last symbol and since sm ( M ) is simple, t must go through W Hence, s [1 ,L  X  1] covers W .

Our second step is to describe the coverage of the inter-mediate states in co ( M ) .
 Proposition 13. Let s be a sequence of length L . Let  X  = ( V 1 ,V 2 ) be a state in co ( M ) , then s covers  X  if and only if s covers V 1 and s [2 ,L ] covers V 2 .
 the first step assume that par ( V 1 ; a ) = par ( V If V 2 = { i } , then V 1 is connected to the state  X  = { i } (in sm ( M ) ) by an edge with a label a . Hence, s covering  X  is equivalent to s covering V 1 . The result follows since the state V 2 = { i } is automatically covered. Assume now that V 2 6 = { i } . In this case s [2 ,L ] covering V 2 implies that s [2 ,L ] (and hence also s ) covers V 1 . Since  X  is connected to ( { i } , { i } ) , s covering  X  is equivalent that s [2 ,L ] has a symbol a . But this is equivalent for s [2 ,L ] covering V
Assume now that the result holds for all parent states of  X  . Assume that s covers  X  . There must be a symbol s e and a parent state  X  = ( W 1 ,W 2 ) linked to  X  by an edge with a label s e such that s [1 ,e  X  1] covers  X  . By the assumption s [1 ,e  X  1] covers W 1 and s [2 ,e  X  1] covers W 2 . Thus, s [1 ,e ] covers V 1 and s [2 ,e ] covers V 2 .

Assume now that s covers V 1 and s [2 ,L ] covers V 2 . Let s e be the last element in s such that s e  X  in ( V 1 ) or s in ( V 2 ) . By Lemma 12 we must have that s [1 ,e  X  1] covers par ( V 1 ; s e ) = W 1 and s [2 ,e  X  1] covers par ( V 2 Hence by the induction assumption s [1 ,e  X  1] covers  X  = ( W 1 ,W 2 ) and consequently, since  X  is a parent state of  X  , s [1 ,e ] covers  X  .

Now we are ready to prove that the coverage of states in the last group is exactly what we wish to have.
 Proposition 14. Let  X  be a state in co ( M ) corresponding to a state v in M . Then a sequence of length L covers  X  if and only if s [1 ,L  X  1] and s [2 ,L ] cover v .
 of s that leads to  X  from the source state. Let s e be a last symbol of t . If s e  X  in ( { v } ) then t travels through  X  = ( { v } ,par ( { v } ; s e )) . By Proposition 13, s [1 ,e  X  1] covers { v } and s [2 ,e  X  1] covers par ( { v } ; s e ) and hence s [2 ,e ] and result follows again from Proposition 13.

To prove the other direction assume now that s [1 ,L  X  1] and s [2 ,L ] cover { v } . Assume that s L is not in in ( { v } ) . This implies that s [2 ,L  X  1] covers { v } . Thus, by Proposition 13, s [1 ,L  X  1] covers ( { v } , { v } ) and consequently s covers  X  . On the other hand, if s L  X  in ( { v } ) , then Lemma 12 implies that s [2 ,L  X  1] covers par ( { v } ; s L ) and hence must cover, by Proposition 13, ( { v } ,par ( { v } ; s L )) . Consequently s covers  X  .
 It is easy to see that co ( M ) is simple and monotonic, if M is monotonic. Hence, we can compute the coverage of co ( M G ) using Algorithm 2.

We can now compute the probability of s being a minimal window using Eq. 1. First we solve the coverage using sm ( M G ) . Secondly, we compute the co-coverage using co ( M G ) . Once these are computed we can use Eq. 1.
Let us finish by discussing the relative sizes of the machines. It can be shown that the number of states in sm ( M G ) can be substantially larger than the number of states M G . Consequently, in the worst case our method is not polynomial. Such an explosion, however, requires a specific episode with many nodes having the same label. Such episodes are unlikely to be candidates if we are dealing with sequences that have a large alphabet distributed more or less evenly. Moreover, we demonstrate later that in our experiments the size of sm ( M G ) is about the same as the size of M G .

In this section we will describe how we test whether the discovered minimal windows obey the independence model.
We say that the episode is significant if the average length of the minimal windows is abnormally small or large. In order to measure the abnormality we will use a Z -test. In order to perform this test we need to show that the average length is asymptotically normal and compute the mean and the variance according to the independence model. This is not trivial since the minimal windows correlate within a single sequence s . For example, assume that we are looking for the parallel episode ( a,a,a ) , and assume that we have found a minimal window of size 3 , that is, the window is aaa . Then the next minimal window will have a higher probability of being short, since we already have two a s. Thus the occurrences of minimal windows in s are not independent even if s obeys the independence model.

Assume that we are given a long random sequence s of length N and write X i to be a boolean random variable such that X i = 1 if there is a minimal window starting at i th symbol. Also let Y i be the length of that minimal window and 0 if there is no window. Note that the estimator of the average length is Let us first show that M is normally distributed. To see that note that ( Y i ,X i ) and ( Y i + K ,X i + K ) , where K is the maximum length of a window, are independent. Hence, ( X i ,Y i ) is a strongly mixing sequence which allows us to use a Central Limit Theorem for dependent variables (given in [2], for example) so that ( cally normal as N approaches infinity. Let us denote by C the covariance matrix of this limit distribution. Also write p = E [ X 1 ] and q = E [ Y 1 ] . Using the same theorem we know that the components of C are
C 11 = E ( Y 1  X  q ) 2 + 2
C 22 = E ( X 1  X  p ) 2 + 2
C 12 = E [( X 1  X  p )( Y 1  X  q )] +
Let us define m = q/p which is the average length of the minimal window. Since f ( y,x ) = y/x is continuous and differentiable function at ( q,p ) , we know from Theorem 3.1 in [3] that with mean m and variance  X  2 , where where  X  f = (1 /p,  X  m/p ) is the gradient of f at ( q,p ) . Thus in order to perform a statistical test for an episode G given a sequence s , let W be the sum of lengths of the discovered minimal windows. We consider the following statistic Based on the above discussion if s truly comes from the independence model, then Z is asymptotically distributed as a standard normal distribution N (0 , 1) .

Our remaining task is to compute m and  X  . Note that since we know the probability of s being a minimal window, we can compute p , q , m , and the first terms of C and C 22 . However the last terms of C cannot be computed easily. We resolve this issue by simulating a sequence of independence model and estimating these terms from that sequence.

So far we have assumed that we already know what episodes we wish to test. In this section we will focus on mining candidate episodes.

In order to have a reliable Z -statistic (see Eq. 2), we need to have a decent number of minimal windows. Hence, a good criterion for a candidate episode is that the number of minimal windows exceeds some given threshold. This is the criterion used in M INEPI (see [1]). However, this condition is not antimonotonic as demonstrated in the next toy example. Example 15. Consider the sequence  X  aba  X . There are 2 minimal windows for the parallel episode ( a,b ) , yet there is only one minimal window for the episode ( b ) .

We remedy this problem by making a stronger require-ment. We search all the episodes whose number of non-overlapping windows exceed some given threshold. It turns out, that this condition is antimonotonic and we can search the episodes in a level-wise fashion.

Since there are several ways of selecting non-overlapping subcollection of minimal windows, we will give a more precise definition. Let W be a sequence W of minimal windows of an episode G in a sequence s . Assume that the minimal windows in W are ordered by their occurrences in s . We select the first window and remove any window that overlaps with the selected window. We repeat this until the W has no more windows. We define nm ( G ; s ) to be the minimal windows discovered in such fashion. We first show that this approach produces the maximal number of samples. Proposition 16. Let V be a collection of non-overlapping minimal windows of an episode G in a sequence s . Then | V | X | nm ( G ; s ) | .
 minimal windows of G in s . We will prove that among any sub-collection of W of non-overlapping windows, the collection nm ( G ; s ) has the maximal size. We will prove this by induction over the size of W .

Let w  X  W be the first window in W . Let X be the set of windows that overlap with w (note that w  X  X ). By the definition, we have w  X  nm ( G ; s ) , and the next window will be the first window outside X .

If V  X  X =  X  , then V  X  W  X  X , and the result follows from the induction assumption. Assume that V  X  X 6 =  X  . Any two windows x,y  X  X must overlap, hence V can contain exactly one member of X , say x . This means that V  X  X  x } X  W  X  X , and the result follows from the induction assumption.
 Corollary 17. The quantity | nm ( G ; s ) | is antimonotonic. window in nm ( G ; s ) also contains a minimal window of H . Let V be a collection of minimal windows of H con-structed by taking one minimal window from each window w  X  nm ( G ; s ) . It is obvious that the windows in V do not overlap and that | V | = nm ( G ; s ) . Proposition 16 implies that | nm ( H ; s ) | X | V | .

In [4] the authors introduce a measure for the episodes to be the maximal number of non-overlapping occurrences of the episode s . Since each occurrence is either a minimal window or contains a minimal window, Proposition 16 tells us that nm ( G ; s ) is exactly this measure.

In this section we present our experiments with the quality measure using synthetic and real-world text sequences. A. Datasets
We conducted our experiments with several synthetic and real-world sequences.

The first synthetic sequence, gen-ind consisted of 200000 digits drawn independently from the uniform model. The purpose of this dataset is to show that our method finds very few significant episodes. The second synthetic sequence, gen-co also consisted of 200000 digits. The sequence was generated as follows. First we choose, by a fair coin flip, whether to generate a digit from 0  X  4 or 5  X  9 . In the former case the digit was selected from a uniform model. In the latter case the probability of selecting the digit i was proportional to 0 . 5 x , where x is the distance between the current location and the last location of the digit i  X  5 . Thus in this sequence, the digits i and i  X  5 tend to be close to each other.

Our third dataset, moby , was the novel Moby Dick by Her-man Melville. 1 Our fourth sequence, abstract consisted of 739 first NSF award abstracts from 1990. 2 Our final dataset, address , consisted of inaugural addresses of the presidents of the United States. 3 To avoid the historic concept drift we entwined the speeches by first taking the odd ones and then even ones. The sequences were processed using the Porter Stemmer and the stop words were removed.
 B. Experimental Setup
Our experimental setup mimics the framework setup in [5] in which the data is divided into two parts, the first part is used for discovering the patterns and the second part for testing whether the discovered patterns were significant. We divided each sequence into two parts of equivalent lengths. We used the first sequence for discovering the candidate episodes and training the independence model. Then the discovered episodes were tested against the model using the second sequence.

As candidate episodes we considered only those episodes whose number of non-overlapping windows exceeded some threshold N . When computing the independence model and discovering minimal windows from the test data we only considered the minimal windows of at most K . We used K = 10 for the text sequences and K = 35 , 40 for the synthetic sequences. These thresholds are given in Table I.
Let G be the set of candidates. Since we compute the samples from the test sequence, it is not guaranteed that an episode G  X  G will have enough minimal windows. Hence we discard any episode whose number of minimal windows in the test sequence does not exceed N . We also remove any episodes having the variance 0 since for these episodes the minimal window will always be of the same known size. This set includes all singletons. Let us denote this set of episodes by H . The sizes of these families along with the sizes of the machines sm ( M H ) and co ( M H ) are given in Table II.

For each episode H  X  H we computed the Z -statistic given in Eq. 2. This value is asymptotically distributed as a standard normal distribution. We considered two different P -values. First, we computed a one-sided P -value to we examine whether Z is abnormally small, thus our test will return small P -values if the minimal windows are significantly smaller than expected. Secondly, we computed a two sided P -value to test whether the average of lengths of minimal windows is significantly smaller or larger. The correlation between the minimal windows (see Section V) was computed by simulating a sequence with 10 6 symbols. The computation of P -values lasted about 5 minutes for the generated sequences and less than a minute for text sequences. The most expensive step was the computation of the correlation terms explained in Section V.
 C. Significant Episodes
In this section we will focus on the episodes discovered by our approach.

From each candidate set we computed the significant episodes based on their P -values. As a significance level we used 0 . 05 . We compared raw P -values and also adjusted P -values. The adjustment was done using the Benjamini Hochberg Procedure in order to control the FDR family-wise error [6]. The results are given in Table III.

Let us first consider gen-ind . Since this sequence cor-respond to the independence model there should be no significant episodes. However, since we are accepting 5% of false significant episodes we should expect about 240 significant episodes. The number of significant episodes discovered is about 10% . The higher number for these tests can be explained by the fact that the model we are using is actually trained from the training data and hence contain some error. Should we use the exact model, then the number of significant episodes will drop to 5% . After adjusting the raw P -values, no significant episode remained. Hence, our method did not find any significant episode from gen-ind , as expected.

Next we will consider the sequence gen-co . Here we expect to find significant patterns, since the sequence does not obey the independence model. We see from Table III that this is the case. Even after the adjustment there is a considerate amount of significant episodes. By studying the results we found out that the significant episodes had either the basic form of i  X  i + 5 or a combination of these. This is an expected result since the sequence had i and i + 5 abnormally close to each other. An important observation here is that the algorithm also discovers complex episodes to be important. Namely, the P -value, our quality measure is fair for simple and more complex episodes.

Our next sequence was moby . Since the alphabet in this sequence is quite large, the number of candidate episodes is rather small and a lot of these candidate episodes are in fact singletons  X  in the end 137 episodes were given a P -value. Out of these episodes about 15% were significant based on raw P -value and about 10% when P -values were adjusted. Some examples among the most significant episodes based on one-sided test were ( white  X  whale ) , ( sperm  X  whale ) , ( old  X  man ) , along with their parallel versions. Such episodes imply that these words occur ab-normally close to each other. On the other hand, candidate episodes such as ( time,whale ) or ( ship,man ) were not considered significant. This means that even though these combinations occur often, the episode can be explained by the fact that their individual words are common. Similarly, some of the significant episodes discovered in abstract were ( research  X  project ) and ( undergraduate,student ) . Episodes discovered from address were ( united  X  states ) , ( united,states ) and ( fellow,citizen ) .

Our approach resembles the approach taken in [7], [8] in which the authors considered episode to be significant if the episode occurs too often or not often enough in a fixed window. As a background model the authors used independence model in [7] and markov-chain model in [8]. The main difference between our approach and theirs is that we are studying the behavior of the minimal windows. As we have discussed in the introduction we believe that using the statistics based on minimal windows has an advantage over the fixed window approach.

In [9], the author proposed a criterion for episodes by requiring that the consecutive symbols in a sequence should only within a specified bound. While this approach attacks the problem of fixed windows, it is still a frequency-based measure. This measure, however, is not antimonotonic as it is pointed out in [10]. It would be useful to see whether we can compute an expected value of this measure so that we can compute a P -value based on some background model.
In a related work [11] the authors considered parallel episodes significant if the smallest window containing each occurrence of a symbol of an episode had a small value. Their approach differ from ours since the smallest window containing a fixed occurrence of a symbol is not necessarily the minimal window. Also, they consider only parallel episodes whereas we consider more general DAG episodes. An interesting approach has been also taken in [12] where the authors define a windowless frequency measure of an itemset within a stream s to be the frequency starting from a certain point. This point is selected so that the frequency is maximal. However, this method is defined for itemsets and it would be fruitful to see whether this idea can be extended into episodes.

Finite state machines have been used in [13], [14] for discovering episodes. However, their goal is different than ours since the actual machine is built upon a sequence and not the episode set and it is used for discovering episodes and not computing the coverage.

In this paper we proposed a new quality measure for the episodes. Our approach tackles simultaneously problems with fixed windows but also allows us to incorporate back-ground knowledge. The measure itself is a deviation of the average length of the minimal windows when compared to the expected length according to the independence model.
Our main technical contribution is the technique for computing the distribution of lengths of minimal window. In order to do that we create an elaborate finite state machine and compute probabilities iteratively starting from simple episodes and moving toward complex ones. Once the distri-bution is computed we are able to perform a statistical test on the discovered minimal windows from the test sequence. Our experiments with the text data suggest that this measure finds significant episodes while ignoring uninteresting ones.
The proposed method requires a parameter K , a limit to the size of a minimal window. In this paper we simply have assumed that this parameter is domain-specific and is provided by the user. Setting this parameter high may allow us to discover more interesting patterns. However, when using large values for K , computing the model may become computationally infeasible as we are forced to use exact rational numbers in order to guarantee numerical stability. This computational problems may be solved by simulating the independence model instead of computing the exact probabilities. In such case, more analysis is needed to determine a proper number of steps in this simulation.
As a future work we also consider more elaborate models such as Markov Chains. This has been done in [8] for windows of fixed size and our goal is to extend this approach for minimal windows.

Our experiments revealed an interesting behavior within certain sets. Certain information tend to repeat in sev-eral forms of episodes. For example, we found that both ( white,whale ) and ( white  X  whale ) were significant. This suggests that there is a need for pattern reduction techniques. Such techniques are well studied in the setting of itemsets but are not that well developed for episodes.
