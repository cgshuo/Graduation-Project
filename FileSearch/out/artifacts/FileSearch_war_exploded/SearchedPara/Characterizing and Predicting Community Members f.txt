 Mining different types of communities from web data have attracted a lot of research effo rts in recent years. However, none of the existing community mining techniques has taken into account both the dynamic as well as heterogeneous na-ture of web data. In this paper, we propose to character-ize and predict community members from the evolution of heterogeneous web data. We first propose a general frame-work for analyzing the evolution of heterogeneous networks. Then, the academic network, which is extracted from 1 mil-lion computer science papers, is used as an example to illus-trate the framework. Finally, two example applications of the academic network are presented. Experimental results with a real and very large heterogeneous academic network show that our proposed framework can produce good re-sults in terms of community member recommendation. Also, novel knowledge and insights can be gained by analyzing the community evolution pattern.
 Categories and Subject Descriptors: I.6.5 [Simulation and Modeling]: Model Development.
 General Terms: Algorithm, Design, Experimentation. Keywords: evolutionary web community, heterogeneous network, member characterization, member prediction.
With the availability of massive amount of data on the web, recently, many web-based communities such as web-based social communities, web page communities, and web user communities, have emerged. As a result, there have been increasing research efforts on extracting communities  X  from the web [1, 6, 8, 10, 12, 13, 17, 21]. The basic idea is to model the web data as a graph/network, where the ver-texes represent objects such as web pages or web sites and the edges represent the relationship between web pages or web sites. Then, the problem of community mining is to extract subgraphs satisfying certain properties such as ob-jects within the same community are more similar/close to each other than objects outside the community. For exam-ple, Flake et al . [6] defined a community on the web as a set of sites that has more links to members of the community than to non-members. Then, they proposed an efficient max-imum flow (minimum cut) approach to identify subgraphs that satisfy the definition. In the literature, there are dif-ferent definitions of web communities and web community extraction has been proved useful in many applications such as focused crawler, search engines, web page categorization, and improved filtering mechanism [6, 8, 13, 23].
Most of the community mining efforts focus on defining web communities and proposing corresponding community identification algorithms. Our investigation revealed that these efforts suffer from some combination of the following limitations.
 Heterogeneous objects and relationships: In existing web community mining approaches, web data are modeled as graphs/networks with the assumption that all the objects are of a single type and the relationship between objects are homogeneous . Consequently, web data is represented as a homogeneous network such as the hyperlink-based web page graph in the hits algorithm [11]. However, in reality, web data and corresponding relationships are heterogeneous in nature. Different types of web objects can be found in a net-work. For example, in a web-based academic network we can find paper , researcher , conference ,and journal objects . At the same time, there are different types of relationships between these objects such as a paper "is in proceeding of" a conference, a researcher "is the author" of a pa-per, and two researchers are "co-author" for some papers. As a result, the homogeneous graph/network representation cannot accurately distinguish the heterogeneous web objects and their corresponding relationships.
 Dynamic nature of the data sources: As web data is dynamic, the corresponding representation may evolve over time as well. For example, with the creation of new hy-perlinked web pages and web sites, the web graph structure may change over time. As a result, the web communities may evolve as well. For instance, a specific community may split into several communities or a collection of communities may be merged into one. Furthermore, members in the com-munities may change from one community to another over time. For example, in the research community, when data mining research emerged in early 1990s, it was considered as a part of the database community. But due to its increasing popularity, data mining has evolved into an individual com-munity. However, in majority of existing web community mining approaches, often web data is considered as snap-shot data. Consequently, these efforts have not considered in the web community extraction process the evolutionary nature of web communities as well as the individual members of the communities. Only very recently, there are growing efforts to study evolution of web communities in the context of social network [2, 3, 5, 12, 14, 16, 17].
 Beyond clustering of individual members: Most of the existing work considers the community mining as a cluster-ingproblem. Theobjectiveistoconstructamodelthatcan categorize an object into a specific community. However as we shall see later, in several real life applications, commu-nity mining is not simply a clustering problem. Specifically, there may exist community-wide constraints while the ex-isting clustering problem only considers individual members in the community. For example, in the academic network, there are communities such as conference program commit-tees that need to satisfy certain community-wide constraints. That is, not only should each member in the program com-mittee satisfy certain properties, the community itself as a whole should satisfy some global constraints . For instance, members of the conference program committee community as a whole should cover all the related topics in a specific conference and the geographical locations where related re-search is active.
In this paper, we propose a novel framework of web community mining by combining the evolutionary and heterogeneous properties of network data . A key goal of this framework is to characterize and predict members of a community. In our approach, we first model web data as a heterogeneous network where the vertexes are different types of objects and the edges represent different types of relation-ships. Such representation allows us to clearly differentiate the types of objects and corresponding relationships. Note that the reason we differentiate them is because, as we shall see later, different types of ob jects and relationships often play different roles in different community mining applica-tions. Next, based on a user-defined time granularity, ob-jects and relationships within the same time interval in the heterogeneous network are merged together. Then, a novel structure called vector-based heterogeneous network is pro-posed to represent the relationships for the sequence of time intervals. Note that the edges in the vector-based heteroge-neous network not only represent the relationships between objects but also the evolution pattern of the relationships.
After we have represented the heterogeneous and evolu-tionary properties of the web data using the above struc-ture, we extract features of the community from the vector-based heterogeneous network. We adapt the PopRank algo-rithm [19] to rank the objects and use the rank values as part of the features. Finally, a set of community models is con-structed based on the set of extracted features. We propose a two level community model that consists of a regression phase and a multi-class classification phase. As there often exist hierarchical relationships between communities in real life, we construct the first level of the hierarchy between com-munities with the regression model and use the multi-class classification model to further distinguish the communities that cannot be separated by regression. In summary, the main contributions of this paper are as follows.
The rest of this paper is organized as follows. Related research is presented in Section 2. Section 3 describes the framework of our community mining technique. In Section 4, a real example of the academic network data is used to illustrate the framework. The experimental results based on academic community network data are presented in Section 5. The last section concludes this paper.
Modeling of massive graphs: There has been several work on developing models for massive graphs such as config-uration model [18], generative model [4], Kleinberg X  X  model for the small-world phenomenon [11], forest-fire graph model [15], and biased preferential attachment model [14] for social networks. In contrast, we focus on modeling heterogeneous and evolutionary community network by using novel vector-based heterogenous network.
 Community extraction from static graphs: In [8], Kleinberg et al. defined a web community as a set of repre-sentative authority web pages linked by important hub pages that share a common topic. The hits has been applied to find such web communities in [8, 11]. In [13], Kumar et al. defined a web community as a dense directed bipartite subgraph which contains a complete bipartite subgraph of certain size. They expanded on hubs and authorities by us-ing co-citation as a way to extract all communities on the web and used graph theory algorithms to identify all in-stances of graph structures that indicate community. In [6], Flake et al. defined a community as a vertex subset in which each member vertex has at least as many edges connecting to member vertices as it does to non-member vertices. They proposed to identify such communities using maximum flow and minimum cut algorithms.

Community mining works have also been done in the bib-liometrics and document citation research [1, 20, 23]. In [20], various types of citation mining and bibliometrics tech-niques were discussed in the context of measuring the im-pact of papers, authors, and journals. In [1], graph clus-tering algorithm has been applied to cluster papers based on the citation relationships. In [23], a frequent itemset-based algorithm is proposed to generate the core sets of the communities and merging them with affiliated objects.
The above studies typically extract communities from a static (aggregated) graph and miss the details on the dy-namic behavior about the communities. In contrast, we an-alyze the dynamic features for community extraction.
Community extraction from dynamic networks: Re-cently, there is a large body of work on community extrac-tion from online dynamic networks [12, 17, 21]. In [12], Ku-mar et al. applied Kleinberg X  X  bursty algorithm to identify communities as bursts of hyperlinks between blogs where the bursts are obtained from the time graph extracted from the blog graph as a result of crawling the blogs. Lin et al. [17] proposed a mutual awareness-based model for blog community formation. Note that these approaches consider only dynamic nature of web data whereas our approach is the first to integrate both the dynamic and heterogeneous properties for web community mining.

Community evolution and dynamics: More recently, evolution of large online communities have been studied in [2, 3, 5, 12, 14, 16, 17]. Leskovec et al. [15, 16] have studied the properties of the time evolution of graphs. The results give insights into the evolution of graph properties (such as average vertex degree, distance between pairs of nodes, conductance, network community profile plot )over time and statements about trends can be made. Kumar et al. [12] studied the evolution of the blogosphere as a graph in terms of the change of characteristics, (such as in-degree, out-degree, strongly connected components), the change of communities, as well as the burstiness in blog community. They [14] also classified the social network graph into three groups: the singletons ,the giant component ,andthe middle region , and studied the evolutionary characteristics of these groups. Backstrom et al. [3] provided insights on the struc-tural features that influence individuals to join communi-ties, which communities will grow rapidly, and evolutionary characteristics of overlapping community pairs. Toyoda et al. [22] studied the evolution of web communities from a se-ries of web archives by defining different types of community changes, such as emerge, dissolve, grow, and shrink, as well as a set of metrics to quantify such changes for community evolution analysis. Similar works have been done for citation network [9]. Asur et al. [2] introduced a family of events on both communities and individuals to characterize evolution of communities. They introduced metrics to measure sta-bility, sociability, influence and po pularity for communities
Figure 1: The framework of community mining. and individuals. Falkowski et al. [5] proposed to observe the temporal changes to social networks at the subgroup level instead of vertex and edge level. Lin et al. [17] developed an interaction space -based representation to quantify com-munity dynamics. They established community evolution by maximizing the interaction correlation between commu-nities across two time slices.

In contrast, our work is complementary in nature. Rather than studying the structural properties of a specific type of network (social network or citation network), our approach aims to study general web community network and inte-grates features extracted from the dynamics of network data to enhance the community mining process. In our approach not only the evolution of community itself is considered, but also evolutions of members within each community are incor-porated to make the community mining results more accu-rate. Furthermore, our investigation also includes predicting potential community members ( e.g. , program committee) as well tracking evolutionary features of members.
In this section, we present the framework of community mining based on the evolution of heterogeneous network. As shown in Figure 1, the framework consists of five major components: the time-dependent relation extraction module, the timestamp-dependent segmentation module, the feature extraction module, the model construction module, and the post-processing module. Here, we present the overview of the framework. We shall elaborate on each module in the subsequent sections in the context of academic network. The input to this framework is a set of data sources, domain knowledge, and the targeted applications. The objective of this system is to extract community models that can be used in specific applications.
Given the data sources, the time-dependent relation ex-traction module extracts various types of objects and re-lationships between them. Different from existing relation extraction approaches such as hyperlink extraction, we ex-tract the types of objects and their relationships along with the corresponding timestamps. For example, for the confer-ence program committee application in the academic net-work, different objects such as author , conference , paper are extracted together with different relationships such as apaper "is published in" a conference, someone "is the author of" a paper, someone "is a co-author of" some-one else, and someone "is in the program committee of" Figure 2: Heterogeneous and vector-based network. a conference. At the same time, the corresponding time pe-riod during which the relationships are valid are recorded as well. For instance, a paper is published in 1999 in a con-ference. Using the extracted information, a heterogeneous network can be constructed. Formally, a heterogeneous net-work is defined as follows.

Definition 1. [Heterogeneous Network] A heteroge-neous network H is a 8-tuple H =( V , A ,V,A,s,t, V , A ), where 1) V is a set of nodes and A is a multiset of edges; 2)
V and A are finite alphabets of the available node and edge labels; 3) s : A  X  X  X  V and t : A  X  X  X  V are two maps indicating the source and target nodes of an edge; 4) V :  X  X  X  V and A : A  X  X  X  A are two maps describing the labels of the nodes and edges.

Note that here each node in the network represents an ob-ject and each edge represents the relationship between the two connected objects. In this case, there may be multi-ple edges between two objects and the labels of the edges are the timestamps, types of relationship, and the weight of the relationship. Here weight of the relationship is measured based on the co-occurrence of the two connected objects. For example, a heterogeneous network is shown in Figure 2(a), where the labels of the edges are shown in the top left; the timestamp and weight of the edge are affiliated to each edge. Here weight of the edge is the number of times the relation-ship occurs. For instance, the weight of the edge between two authors represents the number of times two authors co-author papers. Also, each node has its own label that is listed in the bottom left. Note that there may be more than one edge between any two objects.
As mentioned above, each edge in the heterogeneous net-work has a timestamp. In order to monitor the evolution pattern of the objects and their relationships, we need to differentiate the relationships in the temporal dimension. However, in many real life applications, knowing the ex-act time of the relationships between objects may not be necessary. For example, in the conference program commit-tee application, it is not necessary to know the exact time that someone is in the conference program committee of a conference. Rather, it is sufficient to know the year of the conference. Hence, for different applications, users can de-fine any time granularity that is important to the application (such as day, month, year, etc). Based on the time granular-ity, objects and relationships within the same time interval are merged together. Then, the network data is represented as a new type of heterogeneous network called vector-based heterogeneous network , where each edge is a vector w i =[ e ,  X  X  X  , e k ] such that e i represents the weight of the relation-ship between the connected objects during time interval t Formally, it is defined as follows. Definiti on 2. [Vector-basedHeterog eneousNetwork] A vector-based heterogeneous network N is a heterogeneous network denoted as N =( V , A ,V,A,s,t, V , A ), where the label of each edge is a i =(r, w i ), a i  X  A ,risthe relationship between two vertexes and w i is a vector that represents the weights of the relationships in a sequence of time intervals.

For example, given the yearly-based time granularity in the conference program committee application, vertexes rep-resent objects such as paper , conference ,and author , while the edges represent the weights of the relationships on a yearly basis. For instance, in Figure 2(b), the edge between two authors w i =( 2, 1 ) represents that the two authors have co-authored two papers in the first year and one in the second year. Here we use the vector-based network rep-resentation for two reasons. Firstly, the storage space for the vector-based network is substantially smaller than a se-quence of network graphs, as the network graphs can be very huge in many applications. Secondly, the vector-based representation is more flexible compared to a sequence of network graph. Particularly as we shall see later, in the fea-ture extraction phase, different time windows can be used (Figure 3).
The feature extraction module extracts features from the vector-based heterogeneous network. This is the major step where the evolution and heterogeneous properties of the net-work are taking into account. As for the heterogeneous prop-erty, we adapt the PopRank algorithm [19] to rank the ob-jects and use the rank values as part of the features. The rank values are obtained based on similarity propagations between different types of objects. At the same time, there are features that can be directly extracted from the graph using graph properties such as degree and distance. Basi-cally, to represent the evolution of the heterogeneous net-work, we extract two groups of features: the snapshot-based features and the delta-based features .The snapshot-based features refer to features that are extracted from the vector-based heterogeneous network by taking the elements in the same time window in all the vectors. On the other hand, the delta-based features represent how the snapshot-based features change over time. For example, given the academic network data from 1994 to 2004 , for each year there is a snapshot-based feature for each object in the network; for every two consecutive years, there is a delta-based feature for each object. Note that the snapshot-based features can be defined using a time-window. For example, we can take the data from 1994 to 1997 together to get the snapshot-based feature for year 1997 with a time window of size 4 as shown in Figure 3. Here the delta-based features for each object are actually the percentages of change to the corresponding feature values in two consecutive snapshots.
A set of community models can now be constructed based on the set of features extracted using the above mentioned extraction techniques. In this paper, we propose a two level community model that consists of a regression phase and a multi-class classification phase. The underlying intuition is that in many real life applications, there are hierarchi-cal relationships between communities. For example, for the conference program committee community, we have top conferences, second tier conferences, and other conferences. Moreover, for conferences at the same level, there are com-munities with different characteristics. Some conferences fo-cus more on theory while others focus more on application and engineering, even in the same rank. The basic idea is that we can get the first level of the hierarchy between com-munities with the regression model and use the multi-class classification model to further distinguish the communities that cannot be separated by regression.
There may exist constraints that are application depen-dent. The last component, post-processing ,isproposedto handle such constraints. For example, for the conference program committee community application, there are not only local constraints such as the properties of individual candidate program committee members, but also community-wide constraints to the community as a whole. Here lo-cal constraints refer to individual features such as the re-search expertise of the candidate; while community-wide constraints refer to features of the entire community such as the number of PC members, the area of coverage of all members, and the location of the conference. The local con-straints can be modeled in the community model while the community-wide constraints need to be handled via post-processing.

In summary, we propose a framework to model the com-munities taking into account the dynamic and heterogeneous nature of the network. Given the data sources, relations and timestamps are extracted and modeled as a heteroge-neous network. Based on a user-defined time granularity, the heterogeneous network is transformed into a vector-based network representation. Then, community models are con-structed based on the snapshot-based and delta-based fea-tures that are extracted using the object rank algorithm over the vector-based network.
In this section, the academic network is used as an exam-ple to illustrate the above framework in detail. Note that the techniques and models discussed in this section can be ex-tended to other types of network as well. Firstly, we explain the reasons for choosing the academic network as an exam-ple. Then, characteristics of the academic network data are described. Next, details of feature extraction and commu-nity model construction will be presented. Lastly, two appli-cations of the community models are presented to illustrate the usefulness of the model as well as the importance of post-processing.
The reasons for choosing the academic network as an ex-ample to illustrate the above mentioned framework can be summarized as follows.

Firstly, academic network is one typical example of the evolutionary heterogeneous network. The academic network contains various types of objects such as papers , journals , conferences ,and authors . Moreover, there are many types Figure 4: Academic network objects and relations. of relationships between objects. For example, the relation-ships involving authors can be of multiple types such as "co-author of" , "colleague of" , "co-serving of a confer-ence" ,or "member-chair" relationships. Also, the relation-ships between authors and conferences are definitely differ-ent from the relationships between authors and authors. At the same time, the academic network evolves over time. For instance, every year there will be new papers published that contain new authors and new citations to existing papers.
Secondly, there are massive amount of high quality aca-demic network data available on the web such as the aca-demic publication portals: acm digital library , ieee explorer , dblp , CiteSeer , etc. The timestamps of all papers, confer-ences, journals, and their relationships are available as well. Moreover, there are sets of community data available in the academic network. For example, there are conference pro-gram committees, journal editorial boards, special interest groups such as sigmod , siggraph , sigir , etc. The histori-cal information related to these communities is also available from the web. Such large source of data enriched with tem-poral and heterogeneous features provide ideal platform to build our framework.

Thirdly, there exist rich domain-specific constraints in the academic network besides the local constraints. For in-stance, considering the conference program committee com-munity, beside the constraints on individual candidate com-mittee members, there are community-wide constraints such as diversity and coverage .Here diversity refers to the fact that all the members of the program committee should have limited overlaps in terms of physical locations (affiliations) and expertise. Coverage refers to the constraint that all the members of the program committee as a whole should cover all the topics listed for the conferences as well as all the targeted geographic locations.
In this section, we describe the characteristics of the aca-demic network data that will be used in the rest of this pa-per. The data is extracted from the Libra 1 dataset, which contains more than 1 million research papers in the com-puter science area 1989 to 2004 , together with 650,000 au-thors, 1700 conferences, and 480 journals. The types of relationships between these objects are shown in Figure 4. In this paper, mainly five types of relationships are con-sidered. They are "co-author-of" relation between au-thors, "author-of" relation between authors and papers, "in proceeding of" relation between papers and confer-ences/journals, "citation" relation between papers, and "serve for" relation between researchers/authors and con-ferences/journals. In to tal, there are more than 7 million object relationships in this collection. Table 1: Conference statistics and PC information.
Note that there is no program committee information (the "serve for" relationship) in the cu rrent version of Libra. So we extract such kind of information and add them into our dataset. Table 1 shows information related to list of conferences as extracted from the Libra system. Semantics of each column in Table 1 is shown in Table 2. Observe that these conferences are leading conferences in the database and data mining areas. Note that we fail to collect all the historical conference program committee members from the web as some of the web pages are not available any more.
As time-dependent relation extraction have already been described in [19] in the context of Libra system, we will focus our attention on the feature extraction ,the community model construction ,andthe post-processing modules.
The objective of our framework is to construct the com-munity models from academic data. Specifically, we focus on constructing conference program committee community models. As a result, the objective is to extract a group of researchers (authors in the Libra database) to form a com-munity. In this section, we focus on the feature extraction for authors. Basically, there are two types of features for the authors: snapshot-based features and delta-based features .
As mentioned in the preceding sections, some of the fea-tures (such as distance between objects) can be directly ex-tracted from the vector-based network using graph theory while other features may need propagation among different types of objects and relationships. Here, we first review the PopRank algorithm [19] that will be used to extract the propagation-based features. Then, the list of extracted snapshot-based features will be discussed.

The PopRank algorithm was proposed to rank web objects in the heterogeneous relation network. Basically, the popu-larities of web objects are propagated using different types of relationships, where different propagation factors are as-signed automatically for different types of relationships. For example, to get the popularity of a paper, not only the collec-tion of papers is considered, the relations with other objects such as conferences and authors are also taken into account.
To compute the popularity score of an object, the PopRank model takes into account both the popularity of the object Algorithm 1 Snapshot-based Feature Extraction 1: Description 2: Let A be a set of objects 3: for all a i  X  A do 4: for all q j  X  Q do 5: for all valid timepoint t in the time-window do 8: end for 9: end for 10: for all p j  X  P do 11: for all valid timepoint t in the time-window do 14: end for 15: end for 16: end for 17: Return F s and its relations with other objects. We use the following formula to compute the PopRank scores R X of the objects of type X : where R EX is the popularity of object X , which is the prob-ability that the  X  X a ndom object finder X  find this object using only relationship within this type of objects; while R X is the probability that the  X  X andom object finder X  find this object using all relationships with other types of objects.  X  is the damping factor,  X  YX is the propagation factor of the re-lationship from an object of type Y to an object of type X ,and M T YX is the adjacent matrices. For details of the algorithm, please refer to [19].

Table 3 shows the list of sample snapshot-based features for individual authors, where the first five features can be extracted directly using queries against the database. These features are called query-based features .Thelastthreefea-tures are extracted using PopRank and are called PopRank-based features . Note that these features are extracted from the vector-based network with a timestamp and time win-dow. For example, given a time window of size 4 years, the timestamp of 1999 , the corresponding snapshot-based features are extracted using objects and relationships that exist between 1996 and 1999 asshowninFigure3. Forin-stance, if an author has published 20 papers between 1996 and 1999 , then the values for the NumPaper feature is set to 20 . Note that the values for certain features that are cal-culated using PopRank ,suchas BSConf , AuthorRank ,and ExpertRank , are normalized. As a result, there will be a sequence of values for each author for each snapshot-based feature. The snapshot-based feature extraction algorithm is shown in Algorithm 1.
To reflect the evolution of the heterogeneous network, we propose to use the delta-based features. The intuition be-hind is that, to be a conference program committee mem-ber, often the author should not only have been active in the area before but also be active at that time point. The snapshot-based features can reflect how active the author is at a particular time point, while the delta-based features are expected to reflect how active the author is during a certain time period. The delta-based feature extraction algorithm is shown in Algorithm 2.

Given two sets of most recent snapshot-based features of the same author at years t i and t i +1 , denoted as defined as follows. Example 1 : To extract the delta-based features for an au-thor in 1999 , the two sets of most recent snapshot-based features at 1998 and 1999 are used as shown in Figure 3. With the corresponding values of NumPaper ,thevalueof the feature delta-based NumPaper can be calculated accord-ingly. Similarly, the values for other delta-based features can be extracted. Finally, there will be a sequence of values for the delta-based features for each author.

By looking into the properties of the extracted features, the snapshot-based features and delta-based features can be categorized into three classes: publishing , social ,and ex-perience , as shown in Table 3. Here, publishing features are those features that can reflect the author X  X  ability to publish papers such as NumPaper and AreaPaper . Social features refer to features that represent how active the au-thor is in terms of research collaborations, while the experi-ence features reflect the experiences of the authors in terms of organizing a conference or serving as a program commit-tee member or chair. Note that here BSConf is taken as a combined feature of publishing , social ,and experience .
In this section, we present a two-level community model in the context of conference program committee commu-nity. Basically, the model construction process is a learning process. That is, given a list of historical conferences and corresponding program committee members, the objective is to build a model to characterize the program committees in terms of the features of their members. As a result, given a conference and a specific timestamp, we can recommend a list of program committee members based on the con-structed model. Note that in this model construction pro-cess, we use historical conference program committee mem-bers as positive examples to train the models. The reason we do not use negative examples is that it is often inaccu-rate to treat any author who is not selected as a program committee member as a negative example. This is because Algorithm 2 Delta-based Feature Extraction 1: Description 2: Let A be a set of objects 3: for all a i  X  A do 7: end for 8: end for 9: Return F  X  the program committee community has certain community-wide constraints besides constraints to individual authors as mentioned in Section 3. The two-level community model consists of a regression model and a multi-class classification model . We elaborate on them in turn.
The goal of the regression model is to assign each author a score for a specific year to measure the quality of the au-thor. Based on this score we can decides whether he/she can serve as a conference program committee member and which conferences he/she can serve. The intuition is that the score here represents the PopRank of the best conference the au-thor is qualified to serve as a program committee member, which is denoted as the BSConf feature in Table 3. Note that the historical BSConf value can be extracted as the PopRank of the best conference he has served till then using queries. Then, we can use the historical BSConf values to predict the BSConf for the next year.

To get such score value for each author, we propose to build a regression model based on the historical instances of the BSConf values for the conference program committee members and corresponding features. As in different areas, the conference PopRank values may vary. In this paper, we build a general regression model for them by normalizing the values within areas. For instance, we assign rank values of 1 to the best database and the data mining conferences, respectively. The algorithm we used is the regression version of SVM Light 2 .

The basic idea of the training process is to use the nor-malized BSConf feature as the label of each author. That is, the regression model is to assign each author a BSConf value based on all the other features denoted as F t 1 , F and  X  t 1 for a specific time point t 1 . An example training instance is { F t 1  X  1 , F t 1 ,  X  t 1 , BSConf t 1 } ,wherethelast value is taken as the label . Note that the regression model is time-dependent. If we want to build a regression model for 1999 , then all the conference program committee members before 1999 are used to construct the model.

Once the model is constructed, given any author with fea-tures F t 1 , F t 1 +1 ,and  X  t 1 +1 , we can assign a score to him/her. The score is then compared with the conference PopRank values at the time t 1+1 and necessary decisions can be made.
The regression model may generate more than two confer-ences that match with the author in terms of the assigned score and conference PopRank values. The multi-class clas-sification model is then used to verify which conferences he/she may be able to serve as a program committee mem-ber, as different conferences have their own characteristics in choosing program committee members. In this paper, we use the multi-class SVM light 2 .

Similar to the regression model, each program committee member in the historical conferences is taken as a training instance. The two sets of snapshot features and the delta features F t 1 , F t 1 +1 ,and  X  t 1 +1 are used. However, the label is not the BSConf but the corresponding conference name. Note that the multi-class classification model is built from the list of conference whose PopRank values are very close. That is, models are constructed to distinguish these con-ferences where the program committee members may have very similar BSConf values.

For example, based on the Libra data, we build multi-class classification model for a list of database conferences such as sigmod , vldb ,and icde ,wherethe PopRank are very close to each other. Similarly, another multi-class clas-sification model is built for data mining conferences pkdd pakdd ,and icdm . By doing so, we can successfully distin-guish conferences with very close PopRank values.
Once the classification model is constructed, given an au-thor with all the required feature values and a list of candi-date conferences (obtained using the regression model), we can decide which conferences the author is qualified to serve as a program committee member.
In this section, we present two applications of the confer-ence program committee models. The focus of this section is to illustrate the usefulness of the community model as well as necessary post-processing.
This is a tool designed for experts who are expected to or-ganize academic conferences. It provides the basic function of automatically recommending the list of program commit-tee members and advanced functions for interactive refine-ment of the program committee.

Given the name of a conference, the corresponding area, the level of the conference, the program committee chair, and the number of expected members, the basic recommen-dation function works as follows. Firstly, the regression model is applied to the features of the researchers and only those whose output scores are within the specific range of the conference are selected. From the selected researchers, the multi-class classifier is used to select the set of researchers that best match the specific conference. Lastly, the global constraint-based pruning techniques are applied. Note that, usually, the number of researchers satisfying the above cri-teria is much more than the number of program committee members specified by the chair. Hence, we introduce the fol-Figure 6: Screenshot of PC recommendation result. lowing two objectives and a constraint for further pruning (post-processing).
Based on the above objectives and constraint, the selec-tion process is realized using the multi-objective optimiza-tion genetic algorithm [7]. As we shall see in the experi-mental results, this approach can produce satisfactory rec-ommendation results. Note that the program committee chair(s) can assign anyone he/she thinks is qualified but are not selected by the system. He/she can also remove any of the researchers from the recommended list. After that, the system will generate a list of program committee mem-bers that satisfies all the above constraints. Figures 5 and 6 depict the screenshots of the input and output of our rec-ommendation system, respectively.
This is a tool designed for the academic committee in re-search institutes and universities to evaluate the research performance of researchers and faculty members. For in-stance, it can be used as one of the  X  X ools X  to evaluate whether or not to promote a faculty member. The researcher index monitors the performance of a researcher in terms of his/her publishing ability, social activities, and experience of organizing research conferences. Moreover, the researcher X  X  research interests and expertise areas can be tracked. An-other important function of this tool is that the evolution patterns of the performances of well-known researchers in relevant areas can be used as examples to new and junior re-searchers in order to guide them to be successful researchers in the future.
We now present the experimental results to illustrate the performance of the proposed framework in the above men-tioned applications in the context of academic network. We use the Libra dataset as our test bed. The version of Li-bra data used in this paper contains three major types of objects: papers , authors ,and conferences or journals . Details of the dataset have been described in Section 4.2.
To measure the quality of the conference program com-mittee recommendation application, we use part of the data available as source to construct the community model and use rest of the data to evaluate the recommendation qual-ity. Similar to the traditional classification quality measure,
Table 4: Quality of PC recommendation result. hereafter we use precision and recall as performance metrics: P recision = avg . no . of correctly predicted PC members Note that the above quality measures are based on the aver-age of different prediction results. The reason is that, given a conference and related constraints, by running our algo-rithms repeatedly, we may get a set of different prediction results. In real life scenarios, there may be more than one group of program committee members satisfying all the con-straints as well.
 Recommendation quality: Table 4 shows the precision and recall of the automatic recommendation results. The re-sults are summarized based on the area of the conferences, which is shown in the first column of the table. For instance, the database area includes three conferences vldb, sigmod, and icde . Also, the datasets that are used for training and testing are recorded in this table. In this set of experiments, firstly, previous program committee members are used to build community models. Then, the community model are tested with the immediate subsequent data. For example, inthefirstrowofTable4,weuseallthePCmembersinthe database area from 1994 to 2000 as training data to build the prediction model and predict the list of PC members for year 2001 . Note that the precision and recall are the aver-age values for all the conferences in the specific area. From this table, it can be observed that the proposed community model can produce high quality results.
 Effect of Distance: In Table 5, the distance between the training data and the testing data is varied from 1 year to 5 years. Here, the distance refers to the difference between the timestamps of the testing data and the latest training data. For example, all the experimental results that are shown in Table 4 have a distance of 1 year. If we take the data collection from 1994-1999 as training data and use the constructed model to recommend program committee mem-bers for years 2001 and 2002 , then the distances are 2 and 3 years, respectively. Note that for a specific distance value, the precision and recall in this table are computed by taking the mean of the precision and recall values of all the seven conferences listed in Table 1 for the specified distance. It is obvious that when the distance between the training data and testing data increases, the quality of the recommen-dation may decrease slightly. This observation shows that the conference program communities are evolving over time. That is, if the distance between the training and testing data is large enough, the community model cannot accu-rately reflect the current char acteristics of the community. As a result, the quality of recommendation quality decreases. Recommendation quality of a new conference: To
Table 6: Recommendation with general models. evaluate the recommendation quality of new conferences, general models are used for specific conferences. For in-stance, Table 6 shows the quality of recommendation using four different models to generate the list of program commit-tee members for a specific data mining conference  X  pakdd for 2005 . Note that the pakdd conference program com-mittee information is not used in this process, as we are assuming pakdd as a new conference. The four models are constructed as follows. (a) Model 1: All conferences listed in Table 1 except pakdd ;(b) Model 2: All data mining conferences in Table 1 except pakdd ;(c) Model 3: kdd conference; (d) Model 4: Data mining conferences except kdd and pakdd .

It can be observed that the last three models can pro-duce satisfactory recommendation results, while the general model built from all the conference (except pakdd ) can only provide recommendation with limited quality.
We now present different types of evolution patterns of several researchers over time. Here, for each researcher the BSConf feature value is used as an overall measure that combines publishing , social ,and experience features. Note that due to privacy issue we have not revealed the names of the researchers. Figure 7 shows different types of evolution patterns. For instance, researcher 1 becomes very active from 1996 and his/her performance increases till 2001 .Af-ter that he/she has maintained a stable performance. Re-searcher 2 , on the other hand, has consistently maintained a stable performance since 1994 . Performance of researcher 3 is getting better every year. On contrary, the performance of researcher 4 has gone down since 2001 . To identify the  X  X ising star X , we simply use the percentage of changes. If it is larger than a threshold, then the researcher is a rising star. For instance, researcher 1 is identified as a rising star in 1996 as his BSConf value increase dramatically from 1994 to 1996 . Note that the BSConf values (Figure 7) have been normalized to values between 0 and 10.
In this paper, we proposed a novel framework of web com-munity mining that combines the evolutionary and hetero-geneous properties of web data along with their community-wide constraints. We illustrated the usefulness of our frame-work with a real world example based on the academic net-work. In our approach, we proposed a novel structure called vector-based heterogeneous network to model the hetero-geneity and evolutionary features of web objects and asso-ciated relationships. Then, a set of features of a partic-ular community is extracted from this network using the PopRank algorithm [19]. After that, we proposed a two-level community model. Experiments with real academic network data reveal that the proposed framework can pro-duce high quality results and interesting insights about the evolution pattern of the network.
