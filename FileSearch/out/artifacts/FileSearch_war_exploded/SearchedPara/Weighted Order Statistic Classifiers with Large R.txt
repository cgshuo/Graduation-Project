 Reid Porter rporter@lanl.gov Damian Eads eads@lanl.gov Don Hush dhush@lanl.gov James Theiler jt@lanl.gov We investigate the two-class classification problem when the classifier is a weighted order statistic. In two-classclassificationwefindafunction f ( x ):R D  X  { X  1 , 1 } that minimizes the probability of misclassifi-cation defined on a distribution G over R D  X { X  1 , 1 } : This probability is called generalization error and is usually approximated with a loss function based on a training set. A training set is a sample S = ( x (1) ,y (1)) , ( x (2) ,y (2)) ,..., ( x ( N ) ,y ( where examples are chosen independently and at ran-domfromthedistribution G . Oneapproximationthat is often used is misclassification error on the training set: Recently, theoretical and pr actical results have sug-gested loss functions that consider sample margins sometimes improve approximation to generalization error (Mason et al., 2000). In this case, the loss func-tion also includes the distance of each sample to the decisionboundary, yf ( x ). Anumberoftheoremsfrom the probably approximately correct machine learning frameworkbound(1)intermsofsamplemargins,fora number of function classes. These theorems generally take the following form (Schapire et al., 1998): Theorem: For  X &gt; 0 with probability 1  X   X  over the random choice of the training set S , every function f ( x )inafunctionclass F satisfiesthefollowingbound for all  X &gt; 0:
Pr G [ yf ( x )  X  0]  X  Pr S [ yf ( x )  X   X  ]+ (  X ,N, |F| , X  In this inequality, generalization error Pr G is bound by training error Pr S at margin  X  and a second term that depends, among other things, on a complexity measure for the function class F , |F| .Foranum-ber of function classes, an increase in  X  reduces by decreasing the complexity term |F| . In practice, in-creasing  X  will also increase Pr S and the appropriate trade-off must be chosen by the learning algorithm. Several learning algorithms parameterize the tradeoff with a single parameter e.g. introducing a regulariza-tion term into the loss function (3).
 Theoremsthatcharacterizethepreciserelationship(4) aredevelopedforspecificfunctionclasses F e.g. linear classifiersandotherneuralnetworks. Inthispaper,we suggestasimilarrelationshipexistsforweightedorder statistic,andotherstackfilterderivedfunctionclasses. We present a brief account of weighted order statis-tic literature in Section 2 .InSection3,weshowhow weightedorderstatisticclassifiersreducetobinarydo-main classifiers due to thresholding. This links the weighted order statistic classifier to other binary do-main classifiers that are associated most often with ensemble methods.
 We will propose a new type of margin for these func-tion classes called rank-order margin. Our results ap-ply to a large number of ordered function classes in-cluding all digital mappings { 0 , 1 } D  X  X  0 , 1 } (Lin &amp; Coyle, 1990). In Section 4, we investigate the re-lationship between rank-order margin, regularization and generalization with two synthetic experiments. Wecompareourapproachtoboostingwithreal-world data sets from the UCI machine learning repository. We summarize and discuss future research efforts in Section 5. Generalizedmedianorweightedorderstatistic(WOS) filters are an active research topic in non-linear digi-tal filtering (Astola &amp; Kuosmanen, 1997). Their ro-bustness to impulsive noise has made them a popular alternativein applications where linearfilters perform poorly. The linear classifier, or thresholded linear fil-ter, is a direct generalization of the weighted average and sample mean. In a similar way, the median filter is generalized to weighted median and the WOS fil-ter. A more detailed analogy was presented by Arce (1998). In this paper we consider WOS classifiers, or thresholded WOS filters.
 Our analysis and method of optimization are based on techniques known as threshold decomposition and stacking. These techniques were suggested for analy-sisanddesignofnon-lineardigitalfiltering. Ournovel contributionistoapplythesetechniquestolargemar-gin loss functions to solve classification problems. Our approach was inspired by Preston (1990), who described thresholded rank-order filters for a target detectionproblem. Several otherclassifiersarerelated to WOS classifiers, namely min-max classifiers (Yang &amp;Maragos,1995),andmorphologicalnetworks(Ritter &amp; Sussner, 1996).
 WOSclassifiersalsorelatetobinarydomainclassifiers used in ensemble methods. The median, or majority vote is used by bagging (Breiman, 1996). A weighted order statistic is used to combine weak hypothesis in the adaboost m.1 algorithm (Schapire et al., 1998). Direct optimization of margin for this function class was suggested by Mason et al. (2000) and finding the maximum, minimum margin solution was considered by Grove and Schuurmans (1998). Tumer and Ghosh (1999)investigatedtheperformanceoforderstatistics for combining ensembles. In this paper we consider theweightedorderstatistic,andstackfiltercombiners and suggest a mechanism to control complexity. 2.1. Threshold Decomposition Threshold decomposition is an important tool for the analysis and design of non-linear digital filters (Fitch etal.,1984). Wedescribethresholddecompositionfor D inputs: x =[ x 1 ,x 2 ,...,x D ] where each input takes on values from a finite set of quantization levels: x i  X  { X  q +1 ,...,  X  1 , 0 , 1 ,...,q } . Thresholddecomposition waspresentedforreal-valuedinputsbyYinandNeuvo (1994) and Arce (1998). It is convenient to define a threshold function based on the indicator function: Threshold decomposition applies the threshold func-tion to the inputs at each quantization level. The original inputs can be exactly recovered from this de-composition by where T i is the i  X  X h component of the vector function output (5). 2.2. Stack Filters If we define a binary function f ( u ): { 0 , 1 } D  X  X  0 , and apply it to each level of the threshold decomposi-tion then the function: defines a stack filter when f ( u ) satisfies the stacking constraints. The stacking constraints are: A necessary and sufficient condition for f ( u )tosat-isfy the stacking constraints is that it is a Positive Boolean Function (PBF) (Wendt et al., 1986). PBFs are the subset of Boolean functions that can be ex-pressed without complements of the input variables. Figure 1 illustrates the stack filter architecture, and correspondingPBFfora threeinputmedianfunction. 2.3. Weighted Order Statistics Weighted order statistic filters are a useful sub-class of stack filters. In this case, f ( u ) is restricted to a linearly separable positive boolean function: where w i ,b  X  X  0 } X  R + for all i .
 By choosing weights and threshold b , median, order statistic, weighted median and weighted order statis-tic function classes can be implemented (Yli-Harja et al., 1991). In figure 1, the linear seperable PBF for a 3 input median function has all w i =1and b =1 . 5. WOS functions, like the median, can be implemented directly in the real domain with sort-ing (Arce, 1998). However when targeting specialized hardware, like Field Programmable Gate Arrays, ex-tremely efficient implementations of the threshold de-composition architecture are possible (Chen, 1989). 2.4. Data Dependent Threshold We have described threshold decomposition where in-puts take on quantization values from a finite set. A useful property of stack filters is that the filter output is always equal to one of the filter inputs. This means a moregeneraldecompositioncan be defined thatcan be applied to inputs with arbitrary range and preci-sion. We first define some new notation. For an input vector x =[ x 1 ,x 2 ,...,x D ]where x i  X  R, we define For example, sorting elements of the vector x into as-cending order would produce a vector: Let SI f ( x ) be a stack filter defined as: There is a one-to-one corre spondence between a stack index filter and a stack filter: We threshold the stack filter at 0 to produce a stack filter classifier which outputs class labels y  X  X  0 , 1 } Sincethestackfilteroutputisalwaysoneoftheinputs, if x i &gt; =0forall i thenthestackfiltermustassignthe sampletoclass y =1. This maynotbealimitation in someapplications,howeverinothercases,amoreflex-iblemethodofcombinationisrequirede.g. inputscor-respond to randomly gener ated features. One simple waytoincreasethemodelcomplexityistosupplement the stack filter inputs with mirrored input samples: For the rest of this paper we assume a mirrored in-putspaceisused. This approachwassuggestedinthe context of a mirrored thresho ld decomposition archi-tecturebyParedesandArce(2001). Forclassification, the mirrored input space provides an absolute refer-ence point for the stack filter. That is, the median filter, when applied to the mirrored input space aug-mentedwith0,willalwaysoutput0(theadditional0is not necessary in practice). An a lternative interpreta-tion,suggestedbytheweak-orderingpropertyofstack filtersaroundthemedian(Linetal.,1994),isthatthe hard decision boundary at 0 has been replaced with a relative boundary defined by the median.
 Wereformulatethestackfilterclassifier(14)usingthe stack index filter definition (12). Since zerois guaran-teed to lie between x ( D ) and x ( D +1) we have: and applying the stacking property, Observe in (17) that the stack filter classifier reduces toabinarydomainclassifier,whichisappliedtoinputs thresholded at zero. This is an important observation since it links stack filter classifiers to other techniques in machine learning that build binary domain classi-fiers. It also provides some insight into the complex-ity of stack filter classifierscompared to more popular function classes e.g. the WOS classifier is equivalent to thresholding all inputs at zero and then applying a linear classifier. 3.1. Rank-Order Margin Inthissectionweapplythe conceptofmargintostack filterandWOSclassifiers. Itisconvenienttoreturnto class labels y  X  X  X  1 , 1 } . The stackfilter output isone of the input samples, and therefore sample margins yS f ( x ) take values from a finite, discrete set: yS f ( x )  X  X  x 1 ,x 2 ,...,x D ,  X  x 1 ,  X  x 2 ,...,  X  x The classifier decision boundary is at 0, and we mea-surethe distancetothedecisionboundarybyrankor-der. Figure 2 is an example of the rank-order margin in the context of the threshold decomposition archi-tecture. The mirroredinput sample x belongs to class y = 1. A particular stack filter S f ( x ) produces a cor-rect classification S f ( x ) = 7 with rank-order margin  X  =3. 3.2. Loss Functions We now write a loss function for the stack index filter that includes rank-order margin. Training error (3) follows from (17): The stacking property means training error at rank-order margin  X  can be defined as: where  X   X  [1 , 2 ,...,D ]. The rank-order loss functions are illustrated in Figure 3. 3.3. Optimization The loss function (20) applies to the stack filter func-tion class. Wendt et al. (1986) show that under the mean absolute error criteria, stack filter optimization can be posed as a linear programming problem. The same linear program can be applied to the loss func-tion(20)howeverthenumberofvariablesinthelinear program grows exponentially with the input dimen-sion. For the remainder of this paper we investigate the WOS classifier subclass where the number of vari-ables is linear with input dimension.
 Optimizing (20) for the WOS subclass (9) is a linear classifier design problem, s ubject to the stacking con-straints: w,b  X  0. We approximate misclassification error with the perceptron criteria and then use a lin-ear or quadratic program, depending on the type of regularizationthat we choose.
 Trainingsamplesforthelinearclassifierarebinaryvec-torsthatdependontherank-ordermargin  X  .Foreach pair x ( n ) ,y ( n ) in the training sample (2) we define: Weaugmentandtransformthebinarydomaintraining set and weight vector: The optimization problem is: where ( w )= | w | for L 1 regularization and the solu-tion is found with a linear program, or ( w )= w for L 2 regularization and the solution is found with a quadratic program.
 We suggest using rank-order margin  X  as a free pa-rameter to control the complexity of WOS classifiers. Inexperimentswewillfixthe parameter C at500and then select the optimization problem by varying  X  in (21). We compare this approach to regularization in which case we fix  X  at 0 and then select the optimiza-tion problem by varying C in (24). 3.4. Expansion on Training Samples For someclassificationproblemsthe choiceofregular-ization ( w ) can significantly affect classifier perfor-mance. We will see examples of this in our synthetic experiments. When using rank-order margin as the free parameter the choice of regularization may have little effect since C is fixed at 500. We used two opti-mizationproblemstoinvestigatethisfurther. Thefirst optimization problem was presented in (24) where we solve for w,b and find a classifier of the form: where u = T ( x, 0). We call this the primal problem. We propose a second optimization problem called the expansion problem that explicitly expands w in terms of the training samples, and solves for new variables,  X  and b : f ( u )= I This expansion has two significant properties. First, themappingisexplicitandthereforewhensolving(24) intermsof  X  and b the regularizationisappliedtothe new variables: (  X  ). Second, the expansion (26) sat-isfies the stackingconstraints. Other expansionsoften reduce to hamming distances in the binary domain, which do not satisfy the stacking constraints. 4.1. Comparison to Regularization The first synthetic problem is similar to the thresh-old max (TM) problem presented by Perkins et al. (2003). The problem has 100 dimensions, x  X  R 100 and each dimension is uniformly distributed between  X  1 . 866 and 0 . 134. We use the first 10 dimensions of eachsample todetermine classlabels accordingto the following rule: Therangeoftheuniformdistributionissuchthat(27) will assign approximately half the samples to Class 1. Observethatafterthresholdinginputvectorsat0,the problem is separable with a linear classifier and hence separable with a WOS classifier.
 The second synthetic problem was used by Cannon et al. (2003). The problem has 50 dimensions and samples are drawn from Gaussian class conditional probabilities with equal covariance and class means: The class marginal probabilities are Pr ( y =1)= 2 3 and Pr ( y =  X  1) = 1 3 and the Bayes error for this problem is 0 . 15.
 We investigate four methods to control complexity of the weighted order statisti c classifier. These methods are summarized in Table 1. The first two methods userank-ordermarginasthe freeparameter. Thetwo rank-order methods, M1 and M2, differ in the type of regularizationthat is applied in (24). The R1 and R2 methods threshold the input data at zero, and then solvethe binary input linear classifierdesignproblem. Complexityiscontrolledbyregularizationandthefree parameter is C from (24).
 For each synthetic problem we will solve for both w and b in (25) and  X  and b in (26). We have four problems in total, and four methods for each prob-lem. Training and validation sets of size 30, 60, 120, 240,and480areinvestigated. Thetestsetin allcases is 5000 samples. In each experiment, a training set is used to optimize a classifier at each value of the free parameter in Table 1. We choose the classifier that performs best on the validation set and calculate its error on the test set. Results were averaged over 10 trials and are summarized in Figure 4. 4.2. Comparison to Boosting The second experiment is similar to one used by Ma-son et al. (2000) to investigate DOOM. We use four two-class data sets from the UCI Machine Learning Repository(Blake&amp;Merz,1998). Eachproblemisdi-vided randomly into training, validation and test sets of size specified in Table 2.
 Weusedatacenteredspheres(Cannonet al.,2003)to generate weak learners for the boosting procedure. A new pattern x is mapped to an N dimensional vector k =[ k 1 ,k 2 ,...,k N ], k n  X  R, based on training sam-ples: for n =1 ,...,N and  X  is the Euclidean distance. We use the adaboost m.1 algorithm to incrementally construct base hypotheses. The weak learner selects, through exhaustive search, k n from k and the thresh-old t n to minimize training error. For t n we con-sider N +1 discrete values based on the training data (i.e. the average value of consecutive training points after projection). After 100 iterations we augment the model with mirrored (or negative) input samples. Weights are then reoptimized using the M1 method varying margin  X  from 1 through to 50. We averaged over10trialsandthetrainingandtesterrorsareshown in Figure 5.
 Duringboostingweevaluatetheensembleateachiter-ationusingthevalidationset. Wechoosetheiteration with lowest error and report the test error for the 10 trials. We use the same approach to select the best value for rank-order margin and results are summa-rized in Table 3. 4.3. Discussion of Results Withonly10%ofthefeaturesrelevant,theTM-primal problemissolvedbestbytheM1andR1methodswith L 1 regularization. M2 had similar performance to R2 despitethefactthat C wasfixedat500. Thisindicates the TM-primal problem is very sensitive to the choice of regularization. For the TM-expansion problem we observe a significant decrease in performance for all methods, whichcanbeattributedtothelargenumber ofnoisyorirrelevantfeatures. FortheGaussianprimal problemthe R2 method had the best performance. In this experimenttherewaslittle differencebetweenM1 andM2,andbothhadsimilarperformancetoR1. We see a similar result for the Gaussian-expansion prob-lem, and in this case M1, M2 and R1 have similar performance to the Gaussian-primal R2 solution. The  X  -margin curves in Figure 5 are consistent with the hypothesis that rank-order margin is related to generalization error. The Breast Cancer result illus-trates that rank-order marg in sacrifices substantial training error as margin is increased to improve test error. Boosting and the M1 method had similar per-formance in Table 3. The largest improvement was madeontheIonospheredataset. HoweverinFigure5 weobservethatboosting maynothaveconvergedand thereforewithadditionaliterationstheboostingresult would be expected to improve. Wehaveapresentedanewconceptofmarginforstack filter derived function classes. Rank-order margin has three significant properties: (1) There are a finite set of rank-order margin loss functions that can be di-rectly optimized. (2) Ra nk-order margin and L 1 , L 2 regularizationappeartobedifferent,butrelatedmech-anisms for controlling WOS classifier complexity that obtainsimilarperformance. (3)Rank-ordermarginre-quiresreal-valuedinputsfortraining(21),eventhough the final binary domain classifier is applied to inputs threshold at zero. In contrast, regularization is ap-plied directly to the binary domain classifier and does not consider the real-valued inputs. If the classifica-tion problem has only binary inputs, the rank-order margin mechanism cannot be applied.
 Futureresearchwilldetermineifstackfilterbasedclas-sifiers and rank-order margin have a practical pay-off. In the synthetic experiments we solved linear and quadratic programs that have similar complexity to those used to design linear c lassifiers. However, since WOS classifiersthreshold inputs at zero before apply-ing the linear classifier, approximation error can be high. Mapping to largerfeature spacescan reduce ap-proximation error, but this must be done efficiently (e.g. kernel methods) so that the optimization prob-lem remains computationally reasonable. Construc-tive approaches like boosting are one way to search larger feature spaces efficien tly. However, the features andthresholdsfoundbyboostingmaynotbeidealfor the WOS classifier, and as observed in experiments, the performance gains may not be significant. The approximation problem also manifests itself in nonlinear digital filtering and severalresearchershave suggested hyrbid linear / weighted order statistic fil-terstoaddresstheproblem(Yin&amp;Neuvo,1994). Our future research will explore some of these ideas and aims to combine non-linear digital filtering and ma-chine learning researcheffo rts to benefit both fields of study.
 This work was supported by Los Alamos National Lab X  X  Deployable Adaptive Processing Systems and Real World Machine Learning projects.
 Arce, G. R. (1998). A general weighted median filter structure admitting negative weights. Proceedings of the Eleventh International Joint Conference on Artificial Intelligence , 46 , 3195 X 3205.
 Astola, J., &amp; Kuosmanen, P. (1997). Fundamentals of nonlinear digital filtering . New York, NY: CRC Press LLC.
 Blake, C. L., &amp; Merz, C. J. (1998). Uci repository of machine learning databases. University of Califor-nia at Irvine, Dept. of Information and Computer Sciences .
 Breiman, L. (1996). Bagging predictors. Machine Learning , 24 , 123 X 140.
 Cannon,A.,Howse,J.,Hush,D., &amp; Scovel,C.(2003). Simple classifiers. Manuscript Submitted to: IEEE Transactions on Neural Networks .
 Chen, K. (1989). Bit-serial realizations of a class of nonlinearfiltersbasedonpo sitivebooleanfunctions.
IEEE Transactions on Circuits and Systems Recog-nition , 36 , 785 X 794.
 Fitch, J. P., Coyle, E. J., &amp; Gallagher, N. C. (1984). Median filtering by threshold decomposition. IEEE
Transactions on Acoustics Speech and Signal Pro-cessing , 32 , 1183 X 1188.
 Grove, A. J., &amp; Schuurmans, D. (1998). Boosting in the limit: Maximizing the marginoflearnedensem-bles. AAAI-98: Pro ceedings of the Fifteenth Na-tional Conference on Artifical Intelligence . Lin,J.,&amp;Coyle,E.J.(1990).Minimummeanabsolute error estimation over the cl ass of generalized stack filters. IEEE Transactions on Acoustics, Speech, and Signal Processing , 38 , 663 X 678.
 Lin, L., III, G. B. A., &amp; Coyle, E. J. (1994). Stack filter lattices. Signal Processing , 38 , 277 X 297. Mason, L., Bartlett, P. L., &amp; Baxter, J. (2000). Im-proved generalization through explicit optimization of margins. Machine Learning , 38 , 243 X 255. Paredes,J. L., &amp; Arce, G. R. (2001). Optimization of stack filters based on mirrored threshold decompo-sition. IEEE Transactions on Signal Processing , 49 , 1179 X 1188.
 Perkins, S., Lacker, K., &amp; Theiler, J. (2003). Graft-ing: Fast, incremental feat ure selection by gradient descentinfunctionspac. Journal of Machine Learn-ing Research , 3 , 1333 X 1356.
 Preston, K. (1990). Detection of weak subpixel tar-getsusingmesh-connect edcellularautomata. IEEE
Transactions on Aerospace and Electronic Systems , 26 , 548 X 558.
 Ritter,G.X.,&amp;Sussner,P.(1996).Anintroductionto morphological neural networks. 13th International Conference on Pattern Recognition , 4 , 709 X 717. Schapire,R.E., Freund, Y., Bartlett, P., &amp;Lee, W. S. (1998). Boosting the margin: a new explanation for the effectiveness of voting methods. Annals of Statistics , 26 , 1651 X 1686.
 Tumer, K., &amp; Ghosh, J. (1999). Linear and order statistics combinersfor pattern classification. Com-bining Artificial Neural Nets. , 127 X 162.
 Wendt, P. D., Coyle, E. J., &amp; Gallagher, N. C. (1986). Stack filters. IEEE Transactions on Acous-tics, Speech, and Signal Processing , 34 , 898 X 911. Yang, P., &amp; Maragos, P. (1995). Min-max classifiers:
Learnability,designandapplication. Pattern Recog-nition , 28 , 879 X 899.
 Yin, L., &amp; Neuvo, Y. (1994). Fast adaptation and performance characteristics of fir-wos hybrid filters.
IEEE Transactions on Signal Processing , 42 , 1610 X  1628.
 Yli-Harja, O., Astola, J., &amp; Neuvo, Y. (1991). Analy-sisofthepropertiesofmedianandweightedmedian filtersusingthresholdlogicandstackfilterrepresen-tation. IEEE Transactions on Signal Processing , 39 , 395 X 410.

