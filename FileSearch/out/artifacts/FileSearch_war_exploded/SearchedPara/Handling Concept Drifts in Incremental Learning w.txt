 paper, we propose three ctiteria to evaluate the robustness more training data can reduce risk, but the learning process can get computationally intractable. This issue is becoming more evident with large amounts of data available [5]. Researchers in machine learning and data mining [2, 3, 71 have therefore been trying to scale up classical inductive learning algorithms to handle extremely large data sets. Ideally, it is desirable to consider all the examples together for the best learning performance. However, when the training set is large, not all data can be loaded into the memory. One approach to overcoming this constraint is to train using some incremental learning technique, whereby only a subset of the data is used at any one time. 
The aim of this paper is to : (1) define two broad approaches to incremental learning, (2) propose three criteria for evaluating the robustness of incremental learning algorithms, (3) based on previous observations and claims, suggest that support vector machines (SVM) should handle incremental form of learning, and (4) empirically verify the validity of our suggestion, and evaluate the robustness of incremental training method for SVM, based on our three criteria. 
In the next section we define two incremental learning approaches, and criteria for evaluating the incremental algorithms. Section 3 provides empirical evidence that support vectors (SVs) form a sufficient set for incremen-tal learning. Section 4 is about the succinctness of the SV representation. Finally we conclude by summing up our findings and their implications. In general, incremental learning approaches are needed in the following scenarios : [Sl] the example generation itself is time-dependent, e.g. time series data; [S2] new data is obtained in chunks at intervals, e.g. scientific research data; or [S3] the training data is too large to be loaded into the main memory. Given the various types of learning problems incremental methods can help to solve, there are broadly two approaches to incremental learning, viz, [Al] use one example at a time, lets call it instance by instance learning (1X). [A21 use a suitable-size subset of examples at a time. We can call this block by block learning (BBL). Various methods have been reported in the literature on incremental learning, mostly in the context of first two scenarios [l, 4, 8, 9, lo]. A general criterion to evaluate the quality of incremental learning methods is to consider the prediction accuracy on some benchmark data sets. Schlimmer and Granger [9] also propose three criteria: 1. the number of observations (instances) needed to obtain a  X  X table X  concept description, 2. the cost of updating memory, and 3. the quality of learned concept descriptions to measure the usefulness and effectiveness of an incremental learning method. These measures are useful in evaluating the quality of the online algorithms which are trained using time-based training vectors. 
When incrementally training a classifier from a large database, some new questions have to be answered, such as: [Ql] How much better is a learned model at step n + i than another model obtained after step n? [Q2] Can an incremental algorithm recover in the next incremental step or steps, if it goes drastically off the  X  X ctual X  concept at any stage? These questions are important in quantifying the robustness and reliability of incremental algorithms. When the data is not available beforehand, we do not have any choice but to do incremental training. On the other hand when we have the data available, we have a choice between a batch method (even if its expensive, the choice is there) and the incremental method, and so answers to the above questions can help us make those decisions. 
When an inductive learning algorithm has to be trained incrementally, with each new step, the sample distribution of a training subset may vary. This in turn will create perceived drift in the underlying structure of the data space, or virtual concept drip [12]. The challenge then is to ensure that the concept learned in the current step is incorporated into the concept learned in the previous step, so that actual structure of the data space is realized, preserved, and represented in the trained model. This may also be necessary to get a good generalization on the unseen data. So it appears that a very important measure of the effectiveness of any incremental learning method is its ability to capture the concept drift as the training progresses. 
For effective incremental learning, we propose that any method that attempts to handle concept drift, should satisfy the following criteria : [Cl] Stability -the prediction accuracy on the test set should not vary wildly at every incremental learning step; [C2] Improvement -there should be improvement in this prediction accuracy as the training progresses and the learning algorithm sees more training examples; and [C3] Recoverability -the learning method should be able to recover from its errors, i.e., even if the performance drops at a certain learning step, the algorithm should be capable of recovering and improving back to or better than the previous best .performance. Recently, support vector machines (SVMs) [ll] have been rapidly gaining in,popularity as a learning paradigm. SVMs work by mapping the input vectors into a high dimensional feature space, and constructing the Op-timal Separating Hyper-plane through Structural Risk Minimization Ill]. Only the examples which are suffi-cient to define the Optimal Separating Hyper-plane (so called support vectors) are stored for doing future clas-sification. Given that only a small fraction of training examples end up as SVs [l X  X ], the SVM algorithm can summarize the data space concisely. such that each partition fits into the main memory, and then incrementally train an SVM with the partitions. 
The training would preserve only the SVs at each step, and add them to the training set for the next step. The model obtained by this method should be the same or similar to what would have been obtained using all the data together to train. The reason for this is that the SVM algorithm will preserve the essential class boundary information seen so far in a very compact form as SVs, which would contribute appropriately to deriving the new concept in the next iteration. Further, it is tempting to believe that such a method should also satisfy the three criteria of stability, improvement, and recoverability, since the overall structure of the data space will be preserved, and refined with more examples at each step. 
We train an SVM incrementally, using one partition of data at each step, and then preserve just the support vectors. We study whether incremental SVM training can be robust and reliable based on the three criteria of stabiZity,improvement, and recoverability. Further, to get a more complete picture, it is also desirable that at each incremental learning step, we compare the performance of the incrementally trained model against the model trained with all the data so far in one batch. Specifically, we need to compare two cases: 
Case 1: With only the learned model from the seen data, if new information comes in, how does an SVM do? Does the performance deteriorate as we keep discarding examples at each successive step? 
Case 2: With all the seen data, if we see new information, how does an SVM fare on unknown data? designed as follows. Let us call the training set TR, and the test set TE. In each iteration the training set was further split into 10 parts TRi, where i = 1,2, . ..lO. with each part containing mutually exclusive 10% of the data from TR. The following two types of incremental learning were carried out: 
El. We trained an SVM on TR1, then took the SVs from TR1 as WI and added them to TRz. Again we trained an SVM on SV1 + TR2 and used the trained machine to classify TE. Now the SVs chosen from the training of SV1 + TR2 were taken as SV2, and TR3 was added to them. Again the training and testing were done. This step was repeated until all the TRi were used. 
E2. An SVM was trained using TR1. Then the trained machine was used to classify the examples in TE, and prediction accuracy was recorded. Then TR2 was added to TRI, and TR1 + TR2 was used for training. Again the trained machine was used to classify TE, and results were recorded. Then TR3 was added to TR1 + TR2, and so on, until all the TRi were used. We use 10% of the data for each partition TRi. While incrementally learning from large dataset, we can only use as much data at any step as the buffer allows. So this partition could be a certain percentage of the data that X  X  allowed. The two steps above are repeated 10 times. Each time mutually exclusive 10% of the examples were used as a test set TE, and the rest were used for the training set TR (on the lines of lo-fold cross validation.) The datasets used for carrying out the empirical study are summarized in Table 1. The datasets used are those that are standard among the machine learning community, for benchmarking purposes. All the datasets have been obtained from the UCI-machine learning repository2. We used SVM@Jht3 for our experiment. El gives us a model obtained through incremental training. With E2 we get a model trained with all the examples so far, i.e., TR1 U TR2 U . . . U TRi in one batch. At each step, comparing these two cases gives us an insight as to how the incremental method fares when compared with the batch method, and how well it handles the concept drift. Figure 1 shows the plots of the two cases for the datasets as the learning progresses. The horizontal axis shows incremental steps, and the vertical axis (the height of the bar) denotes the prediction accuracy of the models obtained at the end of the corresponding step. 
As can be seen from the figures, the incremental train-ing satisfies the three criteria of stability, improvement and recoverability : (a) The change in prediction ac-curacy at each step does not vary wildly from the per-formance in the previous step (stability). (b) In gen-eral the accuracy improves as the learning progresses through multiple steps (improvement). (c) In cases where the prediction accuracy drops at any incremental step, it recovers in a later step or steps (graph 4, 6, 11) (recoverability). Overall, we can clearly see that SVMs can handle the concept drift well. Table 2: Final prediction accuracy of batch and incremental methods 
Another interesting and important observation is that the prediction accuracy of the incrementally trained SVM method closely follows the performance of the SVM trained in one batch with all the data. This observation is important for the reason that it provides empirical evidence that the SVs chosen by the SVM algorithm are sufficient to represent the data space, even after successive stages of discarding other examples. Table 2 shows the final prediction accuracy of the incremental and the batch training methods after training is completed through 10 steps. The table clearly shows that there is no drop in prediction accuracy of SVMs when they are incrementally trained. 
The very idea of SVs is that they are sufficient to represent the data space, and they have previously been demonstrated to clearly do so [ll], but all these results have been shown with batch training and testing. The novelty of our result is that it is the first empirical evidence that even after successive steps of discarding the redundant examples (as in the context of incremental learning), the successively collected set of SVs still remains a representative set. These results demonstrate that support vectors form a suficient set of examples for incremental learning. Since our aim is to incrementally train SVM from a large dataset, it is desirable that the selected SV set is as small as possible. To investigate whether the selected set of SVs are really a necessary set, we conducted another experiment. In this experiment our aim is to observe the effect of removing some examples from the SV set on the representativeness of the remaining SVs. We conduct an experiment in two steps: Step 1 The SVM algorithm was run on the whole dataset D, and the SV set SV were obtained. Step 2 SVM was trained on the obtained SV set SV using lo-fold cross validation i.e. for lo-fold cross validation the training set TR and the test set TE were both obtained from SV. The average prediction accuracy of the trained machine on the test set was recorded. Table 3: Results illustrating the succinctness of support vector representation 
The effect of second step was that 10% of the examples from SV were used to form TE and the rest used to form TR. Further over the 10 folds of cross validation, different l/lOth of the SVs were missing from the training set. If all the SVs are necessary then each one of them will be non-redundant. As such, it is reasonable to expect that training an SVM with one part TR of the SV and testing on another part TE should yield poor prediction accuracy. Table 3 provides the prediction accuracy on various datasets for this experiment. The first column shows the percentage of examples chosen as SVs in step 1 of the experiment, the second column shows the average prediction accuracy of the classifier when the 10 fold cross validation was carried out on the whole dataset D. The last column shows the prediction accuracy obtained in step 2 above. It can be seen that the accuracy drops drastically when a training set is created from a subset (90%) of the selected SVs. This means that removing even a small portion of the SVs (10%) from the set SV affects the representation capability of the remaining set. This in turn implies that the SV set chosen by the SVM algorithm is a minimal set and removing any examples from SV would result in the loss of vital information about the class distribution. These results clearly demonstrate that set of chosen SVs forms a compact set that retains the structure of the original data space. These results combined with the results of the previous section provide clear evidence that the SVs form a succinct and sufficient set to ensure that incremental training of SVMs is able to capture concept drift, and hence is a robust incremental learning method. Our method can be considered similar in spirit to decomposition or  X  X hunking X  techniques employed to train SVMs, as in [6]. Our method differs in that un-like other methods our technique looks at the examples only once to determine if they will become SVs. Once discarded, the vectors are not considered again. On the other hand, for example, Osuna X  X  decomposition al-gorithm is an iterative method, which cycles through the training set a few times to select the SVs. Our method can be considered as a kind of lossy approx-imation of the chunking methods. Consequently, our positive results show that such an approximation can be performed, without significantly deteriorating the per-formance of the algorithm. 
We started with the two broad categories of incre-mental learning techniques. Then we defined three new criteria for the robustness of incremental learning meth-ods. These criteria relate to the ability of any incre-mental learning method to capture concept drift. F X  X r-ther, we suggested that SVMs should be adaptable to incremental training, and provide the reasons behind our suggestions. To investigate and demonstrate the validity of our suggestion, we empirically showed that support machines are able to capture the concept drift very well. We also provided empirical evidence that the chosen SV sets form a succinct and suficient set that en-ables the incremental SVM to handle the concept drift and satisfy the three criteria for robustness. PI 
PI 
PI PI PI WI WI PI 
