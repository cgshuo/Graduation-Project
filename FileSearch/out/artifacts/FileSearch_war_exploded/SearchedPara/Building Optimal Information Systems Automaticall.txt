 Software frameworks which support integration and scaling of text analysis algorithms make it possible to build complex, high perfor-mance information systems for information extraction, information retrieval, and question answering; IBM X  X  Watson is a prominent ex-ample. As the complexity and scaling of information systems be-come ever greater, it is much more challenging to effectively and efficiently determine which toolkits, algorithms, knowledge bases or other resources should be integrated into an information system in order to achieve a desired or optimal level of performance on a given task. This paper presents a formal representation of the space of possible system configurations, given a set of information pro-cessing components and their parameters ( configuration space ) and discusses algorithmic approaches to determine the optimal config-uration within a given configuration space ( configuration space ex-ploration or CSE ). We introduce the CSE framework , an extension to the UIMA framework which provides a general distributed so-lution for building and exploring configuration spaces for informa-tion systems. The CSE framework was used to implement biomed-ical information systems in case studies involving over a trillion different configuration combinations of components and parame-ter values operating on question answering tasks from the TREC Genomics. The framework automatically and efficiently evaluated different system configurations, and identified configurations that achieved better results than prior published results.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Sys-tems and Software; J.3 [ Life and Medical Sciences ]: Medical in-formation systems configuration space exploration; biomedical question answering
Software frameworks which support integration and scaling of text analysis algorithms make it possible to build complex, high-performance information systems for information extraction, in-formation retrieval, and question answering; IBM X  X  Watson is a prominent example. As the complexity and scaling of information systems become ever greater, it is much more challenging to effec-tively and efficiently determine which toolkits, algorithms, knowl-edge bases (KBs) or other resources should be integrated into an information system in order to achieve a desired or optimal level of performance on a given task.

The information retrieval community has considered the formal evaluation of different systems with standard benchmarks and met-rics for decades, especially in various evaluation conferences, e.g. TREC 1 , NTCIR 2 , CLEF 3 , etc, which provide researchers oppor-tunities to exchanges ideas and experience on how to select com-ponents and evaluate systems. In some evaluations, both features extracted by participants and their end-to-end system outputs are shared [12, 13, 25]; in other cases intermediate subtask outputs are shared for comparative evaluation of one subsystem [21, 22]. To fa-cilitate information system evaluation based on commonly-shared benchmarks, researchers have recently paid considerable attention to open and public evaluation infrastructures [35, 2, 10]. However, due to the lack of a standard task framework, it is difficult to repro-duce experimental results and/or evaluate different combinations of system sub-modules across participants.

Complex information systems are usually constructed as a solu-tion to a specific information processing task in a particular domain, and often lack a clear separation of framework, component config-uration, and component logic. This in turn makes it more difficult to adapt and tune existing components for new tasks without edit-ing the original component source code, an expense that precludes efficient exploration of a large set of possible configurations. To fully leverage existing components, it must be possible to further automatically explore the space of possible system configurations to determine the optimal combination of tools and parameter set-tings for a new task. We refer to this problem as configuration space exploration , and address three main topics in this paper: Figure 1: Overview of configuration space exploration frame-work architecture
To explore these topics, we present a formal representation of the space of possible information system configurations, given a set of information processing components and their parameters (the configuration space ), and also discuss algorithmic approaches which can be used to determine the optimal configuration within a given configuration space ( configuration space exploration or CSE ). Then, we introduce an open-source platform CSE frame-work 4 , an extension to the UIMA framework 5 , which provides a general distributed solution for building and exploring configura-tion spaces for information systems. An overview of the archi-tecture is shown in Figure 1. To the best of our knowledge, our model is the first to formalize and empirically measure solutions for configuration space exploration in the rapid domain adapta-tion of information systems. We propose a novel formulation and solution to the associated constraint optimization problem, which comprises two sub-problems: component characteristics estimation and stochastic scheduling. We also describe open-source software which is the first embodiment of the CSE concept.

The CSE framework was also used to implement a biomedical information system in a case study. For example, responding to a question in the TREC Genomics Track [11] requires a system to answer biomedical questions, by retrieving relevant passages from a collection of journal articles. A very large number of biomedical knowledge bases, algorithms and toolkits for information retrieval, natural language processing and machine learning were cited by the task participants in their notebook papers and subsequent pub-lications. The CSE framework was used to define and explore a configuration space with over a trillion different configurations of components and parameter values. CSE was able to find an optimal configuration of components for TREC Genomics with a passage MAP score better than prior published results for the task. In ad-dition, the CSE framework makes it possible to automatically dis-cover various features of individual components (e.g., impact, pref-erence, speed, etc.) to support adaptive exploration policies during experimentation.
Retrieval evaluation frameworks. Information retrieval com-munities have held a series of competitions and conferences to cre-ate standard benchmarks and metrics to evaluate retrieval systems, and organizers have attempted to tabulate and analyze the most im-portant features of high-performing systems, based on the informa-tion provided by the system reports, e.g. [25]. Due to lack of a standard task framework, organizers rarely reproduce experiments to try different combinations of modules across participants.
Recently, several shared tasks employ standardized data flows for successive subtasks (e.g. IR4QA and CCLQA tasks in the NT-CIR Advanced Cross-Lingual Information Access task [21, 22]), and the results of the upstream tasks were collected from each sys-tem and shared for processing by all participants in the downstream tasks. This approach was able to discover configurations that com-bined different task systems to achieve a better result than the orig-inally reported systems, which lends further motivation to configu-ration space exploration. However, the evaluation achieved interop-erability through task-specific protocols, and did not provide a gen-eral framework for describing and evaluating configuration spaces that included variation on components and parameter settings.
To facilitate system evaluation based on commonly-shared benchmarks, researchers have recently paid more attention to open and public evaluation infrastructures for information retrieval eval-uation and information extraction [35]. More recently, the Work-shop on Data infrastructurEs for Supporting Information Retrieval Evaluation [2] was held to focus on organizing benchmark collec-tions and tools into coherent and integrated infrastructures to en-sure reproducible and comparable experiments; various retrieval evaluation frameworks were presented [10]. However, systems are usually specific to a single retrieval task, and lack a clear separa-tion of framework, component configuration, and component logic, which makes it hard to fully leverage existing tools through auto-matic configuration space exploration.

Biomedical information retrieval and question answering systems. We focus here on summarizing systems and approaches that have participated in TREC Genomics QA competition or have leveraged the task benchmarks outside the formal competition.
A typical biomedical question answering pipeline consists of three major components: keyterm extraction and query expansion, document (or paragraph) retrieval, and passage extraction [25, 30, 27]. Synonyms, acronyms, and lexical variants are processed in the first phase; a retrieval model is applied in the second phase; and the similarity between the query and each retrieved passage is consid-ered in the last phase. Researchers have tried to apply existing gen-eral retrieval systems [4, 26], relevance feedback to the traditional retrieval model [38, 28, 18], or a fusion or shrinkage of retrieval models [7]. Moreover, linguistic knowledge is also incorporated for the task, e.g., POS tags [19], and SVO structure [30].
Recently, researchers in biomedical information retrieval and question answering continue to leverage the TREC Genomics data set for evaluation [37, 14, 32, 20, 23, 17, 5]. Given the absence of an easy-to-use framework for building baseline systems for new tasks and exploring large parts of the configuration space, itera-tive research typically focuses on perturbations to a single mod-ule while k eeping modules and parameters elsewhere in the system frozen. Stokes et al [29] follow the same assumption to combine components from task participants to find optimal configurations for a biomedical IR task, using an approach specific to the task and difficult to generalize. In this paper, we use a biomedical infor-mation system as a case to theoretically and empirically study the configuration space exploration problem.
In this section, we formally define the concepts used in this pa-per, and provide a formal definition of the CSE problem.
Most information systems consist of a number of processing units or components arranged in series, and each component is de-scribed by its input(s) and output(s). For example, a typical ques-tion answering system has four main component types: question analyzer, document retriever, passage extractor, answer generator [31]. A typical ontology-based information extraction pipeline will integrate several preprocessors and aggregators [35]. These pro-cessing steps can be abstracted as phases in a pipeline. Definition 1 (Phase, component, parameter, configuration) . The processing unit as the t -th step in a process can be conceptualized as a phase t . A component f c t in phase t is an instantiated pro-cessing unit, which is associated with a set of parameters , denoted by
According to the definition of component and configuration, an output produced by a processing unit can be formally described as y = f c t ( x j ! c t ) , where x is an input (typed object) from the previous step, and y is the output. Note that parameters are not restricted to numeric values, but can hold a reference to any typed object.
As an example, consider Question Named Entity Recognition, a phase t in a question answering system where the input x a question sentence, and the output x t is a list of named entities. Component f 1 t could be a rule-based named entity extractor, f could be a CRF-based named entity extractor, and f 3 t could be a named entity extractor based on knowledge base lookup. Configu-ration parameter value ! 1 t could be the set of rules to use, ! be a weight trained for the CRF model, and ! 3 t could refer to the knowledge base to be used by the component.

We introduce notations c ( f c t j ! c t ;x ) and b ( f c two important characteristics of the configured component f the cost of resource required to execute the component on input x and the benefit of executing the configured component to per-formance improvement. Resources used by a component include execution time, storage space, network bandwidth, etc., which can be measured by CPU time, allocated memory size, and data trans-fers respectively; a resource utilization measure can also be a more specific function of component characteristics (e.g., the cost to ex-ecute a configured component on Amazon Web Services 6 is a func-tion of execution time and hardware capacity utilized). The benefit of a single component is relatively difficult to measure without be-ing integrated into a system with other components, where we can leverage commonly-used evaluation metrics for information sys-tems, e.g. F-1 and MAP, etc. We simply assume a configured com-ponent shares the benefit with the component that follows, if no direct measurement is available; the true cost or benefit of a con-figured component is estimated based on the cumulative effect over trillions of pipeline combinations. The scope of benefit is not nec-essarily limited to the effectiveness of the component, but may also incorporate its efficiency, i.e. execution time, storage space, etc.
A typical information processing task can be described as n pro-cessing phases arranged sequentially. A unique series of instanti-ated components is grouped into a single trace.
 Definition 2 (Trace and configuration space) . A trace f ! c is an execution path that involves a single config-ured component for each phase, which is formally defined nents with all configurations comprise the configuration space Fj
 X  = f f c j ! c g c , and a subset F j  X  Fj  X  is referred to as a configuration subspace .

For example, question analyzers, document retrievers, passage extractors, and answer generators comprise the configuration space for a typical four-phase question answering task. One single ex-ecution path would be a unique combination of components (e.g.  X  X uery tokenized by white-space string splitter, document retrieved from Indri repository index with default parameters, sentence ex-tracted based on LingPipe sentence segmenter and Vector Space Model similarity calculator X ) or a trace in the configuration space.
We extend the concepts of cost and benefit for a configured com-ponent to a trace and a configuration subspace: the cost to execute a trace is the sum of costs to execute each configured component, and the performance of a trace corresponds to the final output from last execution. They can be formally defined as follows: where x ( c 1 ;:::;c t 1 ) represents the output from a series of ex-mation gathered from all previous phases, due to the recursive definition that x ( c 1 ;:::;c u ) = f c u u ( x ( c 1 ;:::;c u = 1 ;:::;n . The cost of the entire configuration subspace is de-fined as the sum of unique executions of configured components on all outputs from previous phases. The benefit of the configuration space is defined as the benefit of the best-performing trace. c ( F j  X  ;x ) = b ( F j  X  ;x ) = max
We see that c ( F j  X  ;x )  X  = trace f c j ! c has been executed, and another trace f c  X  same prefix, i.e., f c 1 1 j ! c 1 1 ;:::;f c t t j ! c t then we do not need to repeat the executions for the configured components along the prefix, which is one of the key ideas for the problem solution and implementation. We then formally define the problem of configuration space exploration as follows.
 Definition 3 (Configuration space exploration) . For a particular information processing task, defined by m t configured components for each of n phases: f 1 t j ! 1 t ;f 2 t j ! 2 t ;:::;f ited total resource capacity C and input set S , configuration space exploration (CSE) aims to find the trace f k j ! k within the configu-ration space Fj  X  that achieves the highest expected performance without exceeding C of total cost.

The problem can be formulated as a constrained optimization problem as follows: where c ( F j  X  ;X ) is the total cost to consume X , which is defined as  X  configuration generalizable to any unseen input, we use a random variable x to represent the system input, and the goal is to maxi-mize the expected system performance over the random variable. To satisfy the constraint, the problem also involves selecting a sub-space F j  X  Fj  X  and a subset X S .

In this paper and in our initial case study, we adopt the con-straint that each parameter must have a finite number of values in the configuration space, in order to support an efficient practical implementation and experimentation 7 . Moreover, we note the CSE problem focuses on determining the globally optimal trace general to all inputs, whereas researchers have observed that some com-ponents or configurations might improve the system performance on only some types of inputs while hurt the performance on other types [3]. Accordingly, one can further extend the CSE problem to a variant also incorporating the dependency on the input type
We note the problem is trivial if the total cost to iteratively exe-cute all the components and configurations for all inputs does not exceed available processing capacity. However, this is not the case for most real world problems, since the number of traces and execu-tions grow exponentially as the phase increases. A typical informa-tion processing pipeline consisting of 12 components is shown in Section 6; with up to four parameters per component, and up to six options each parameter, there would be an estimated 6.050 10 executions if all the unique traces were evaluated.

The CSE problem description appears isomorphic to constrained optimization. However, in the CSE case, both cost and benefit of a configured component are unknown to the model until the compo-nent is executed. In Section 4, we propose a solution framework.
In this section, we describe a general solution to tackle the ob-jective function (Equation 5). In an ideal case where (1) the ex-act cost c ( f c t j ! c t ;x ) of each configured component on each input is known, and (2) the performance indicator b ( f c t j ! configured component f c t j ! c t is an i.i.d. random variable, the op-timal solution to this problem can be yielded by adding configured components and inputs in descending order of least cumulative cost (LCC), which is defined as the sum of the component X  X  original cost plus minimal additional cost to execute the components down the pipeline, and formulated as follows:
LCC ( f c t j ! c t ;x ) = c ( f c t j ! c t ;x ) +
When we relax the first assumption used for the simplified case (the cost is revealed until the execution is done), the approaches for deterministic scheduling problems [15] do not provide an ade-quate solution. Instead, we refer to the stochastic scheduling prob-lem [8] or stochastic knapsack problem [6], which makes a differ-ent assumption that job durations are random variables with known probability distributions. These approaches provide a more general formulation suitable for the CSE problem. Figure 2: Generative process of cost and benefit distribution
We also relax the second assumption and assume the perfor-mance of a component is not i.i.d. For example, a weak config-ured component f c t j ! c t (e.g., naive approach, or buggy code) tends to exhibit low performance for every input x ; on the other hand, a complex input x (e.g., a very difficult question posed to a ques-tion answering system) may have a negative effect on the perfor-mance of all configured components. We are inspired to lever-age dependency of characteristics and prior knowledge to dynami-cally (and more wisely) select which trace to evaluate next from the pool. We can utilize the execution history to estimate c ( f and b ( f c t j ! c t ;x ) by establishing a dependency between them. In-tuitively, components that previously showed high benefit (or low cost) tend to achieve again high benefit (or low cost) combined with components from other phases, and components of low benefit (or high cost) tend to be pruned from the pool. In addition, we can also estimate the cost from prior knowledge, e.g., retrieving relevant passages usually requires more resources than tokenizing question texts in a retrieval system.

We develop the solution to the CSE problem based on hierar-chical Bayesian modeling, where both prior knowledge and previ-ous execution history are incorporated into the same framework. The basic idea is that we associate each cost c ( f c t j b ( f c t j ! c t ;x ) with an unknown distribution, with parameters repre-senting our knowledge and past observations. Starting with some known priors, we select the trace to execute based on the greedy algorithm for the stochastic knapsack problem, and then update the priors each time we finish an execution. The process repeats until C is reached. We first discuss how to estimate the cost and benefit of each component to solve the stochastic scheduling problem in Sections 4.1 and 4.2, and show an intuitive example in Section 4.3.
We apply a hierarchical Bayesian model to capture the compo-nent characteristics hierarchically inherited from global level and phase level. Specifically, we let two hyperparameters and de-note the average cost and benefit of a configured component glob-ally across all phases and inputs, and then for each configured com-ponent f c t j ! c t in a phase t , we introduce phase-level parameters and t to capture the characteristics shared by the components in phase t , and component-level parameters c t and c t to model the performance of f c t j ! c t regardless of the specific input ogous to t and t , we introduce parameters x and x to denote the average cost and benefit of all configured components for each input x , which indicates the difficulty of processing each input. Given all the hyperparameters, the random variable c ( f c b ( f c t j ! c t ;x ) is the outcome from a series of generative processes from or , which hence defines a joint probability distribution, shown in Figure 2.
Algorithm 1: Greedy algorithm for CSE problem
Based on prior knowledge of expected cost and benefit, we can assign values for global hyperparameters and , and optionally for some phase-level hyperparameters t and t . Subsequently, un-specified phase-level hyperparameters and configured component level parameters c t and c t can be estimated with maximum a pos-teriori method based on priors and , and all observations (cor-responding to previous execution records). For configured com-ponents that have not been executed, c ( f c t j ! c t ;x ) and b ( f can be predicted from estimated parameters c t , x and c t Bayesian inference method. figured components in the pool, we can rewrite the objective for the deterministic scheduling problem (Equation 5) to a stochastic variant (aka stochastic knapsack problem [8, 6]), which assumes that job durations and/or rewards are random variables with known probability distributions, as follows: where E b and E c represent the expected benefit and cost over the random variables b ( f c t j ! c t ;x ) and c ( f c t j !
Among the solutions (or scheduling policies) for the stochas-tic scheduling problem, adaptive policies have been studied exten-sively in the literature, which dynamically choose which traces to execute next based on previously executed traces. In general, they can achieve a better approximation ratio to the optimal solution [6], and can also be naturally integrated with the CSE problem, since the estimation of all parameters and distributions of b ( f and c ( f c t j ! c t ;x ) are dynamically updated as execution proceeds. A commonly adopted greedy policy for the (stochastic) knapsack problem is to sort the items by decreasing (expected) benefit, in-creasing (expected) cost, decreasing (expected) benefit density, or b ( f c nally, based on the benefit evaluated for each trace f c j x , we select the traces by further measuring their generalizability to new data not yet realized, which can be estimated by various model selection methods, e.g. cross-validation, bootstrap, etc.
We present the solution in Algorithm 1, where  X  P  X  represents the configuration subspace spanned by the component pool P , h ( f c t j ! c t ;x ) is a heuristic function defined on f oritize the components. Examples of h function can be cost rel-evant ( E c [ LCC ( f c t j ! c t ;x )] ), benefit relevant ( E
Table 1: Posterior distributions for each of c t , t and c benefit density ( E b [ b ( f c t j ! c t ;x )] = E c [ LCC ( f ( E plies an adaptive policy to execute the trace (lines 8 X 10), predict the random variables (lines 3, 4), and reestimate parameters (line 11) inside the loop (lines 2 X 11). This framework assumes no prior beliefs, nor probability distributions of component characteristics, which can be customized according to specific tasks. One can also consider to change the strategy function h over time, e.g. promot-ing configured components with greater benefit variance or greater cost once the exploration is stuck in a local minimum. Observing how different strategy functions affect the exploration process is left for future work.
To motivate our solution, we assume that the component charac-teristics follow a simple and intuitive 3-stage linear model [16], where each observation and hyperparameter follows a Gaussian distribution, with mean drawn from a distribution parameterized by the upper level in the hierarchy and variance predefined and fixed. In the simplified 3-stage linear model, we assume each input x to each configured component f c t j ! c t follows uniform distribution over all possible inputs. The generative processes for c ( f and b ( f c t j ! c t ;x ) can therefore be instantiated from the general so-lution framework (Figure 2) and formulated as follows:
We derive the posterior distributions for c t , t and c t can be used to update the hyperparameters (line 11 in Algorithm 1). All the posterior probabilities follow Gaussian distribution of the form N (  X  1 ; X  1 ) , and the values of and  X  are listed in Table 1 for each parameter, where n c t represents the number of times f has been executed thus far, and c c t and b c t are the average cost and benefit of the component in the history. We see that at the beginning of the experiment, when n c t = 0 for all t and c , each parameter takes the same value from the user specified hyperparameter ( or ), and as the experiment proceeds, each parameter is dynamically shifted away from the value of hyperparameter to better model the
Finally, we define function h (line 5 in Algorithm 1) as the profit ( E which makes the CSE framework less expensive to implement and execute. Equivalently, we can dynamically prune the components of either high cost or low benefit. In Section 6, we illustrate how the model was implemented in the framework to tackle a real-world CSE task, using the open-source package presented in Section 5.
To facilitate easy integration with existing components, we re-quire a system that is simple to configure and powerful enough to sift through thousands to trillions of option combinations to deter-mine which represent the best system configuration. In this section, we present the CSE framework implementation, a distributed sys-tem for a parallel experimentation test bed based on UIMA, config-ured using declarative descriptors. We highlight the features of the implementation in this section. Source code, examples, and other resources are publicly available. Details of the architecture design and implementation can be found in the extended version [36].
Distributed architecture. In Section 4, we focus on discussion of single-machine solutions to the CSE problem. We have extended the CSE framework to execute the task set X in parallel on a dis-tributed system based on UIMA-AS 10 , where the configured com-ponents are deployed into the cluster beforehand; the execution, fault tolerance and bookkeeping are managed by a master server.
Declarative descriptors. To leverage the CSE framework, users specify how the components f c t 2 F should be organized into phases in a pipeline, which values need to be specified for each configuration ! c t , what is the input set X , and what measurements should be applied. Our implementation introduces extended con-figuration descriptors (ECD) based on YAML 11 format.

Configurable evaluation. CSE supports the evaluation of com-ponent performance based on user-specified evaluation metrics and gold-standard outputs at each phase; these measurements are also used to estimate the benefit for each component and trace trieval metrics are implemented in the open source software.
CSE also provides the capability to calculate statistical signifi-cance of the performance difference between traces on a given task. This is important when attempting to disprove the null hypothesis that the best baseline trace is as good as alternative traces, and cru-cial to understanding each component X  X  contribution (Section 6.4) and prioritizing the traces when adapting to new tasks (Section 6.6).
Automatic data persistence. As mentioned in Section 3, if two traces share the same prefix, it isn X  X  necessary to repeat executions for the configured components along the prefix if we assume that intermediate results are kept in a repository accessible from any trace. Moreover, intermediate data are also useful for error anal-ysis, trace performance analysis, and reproduction of previous ex-perimental results. Therefore, our implementation includes an au-tomatic persistence strategy for data and experiment configuration.
Global resource caching. Online resources are sometimes tem-porarily unavailable, and are occasionally updated, changing their contents; to support reproducibility of results, we implemented a generic resource caching strategy inside the CSE framework. In addition to ensuring the reproducibility of results when external KBs are queried, global resource caching can also speed up CSE experiments by avoiding replicated service calls across traces.
Configurable configuration selection and pruning. The com-ponent selection strategies can be configured by the user; strate-gies corresponding to the heuristic function described in Section 4.2 are implemented in the open source software. K-fold cross-validation, leave-one-out cross-validation and bootstrap methods are integrated into the open source version to estimate the gener-alization performance of top traces.

Graphical user interface. A Web GUI has been integrated into the open source release featuring three views: experiment , ad hoc , and trend . The experiment view helps users monitor configuration exploration progress and evaluation results, and view current and past persisted intermediate data for error analysis. The ad hoc view enables users to submit an ad-hoc input and inspect the system out-put. The trend view allows users to visualize performance change across experiments over a selected period of time.
In this section, we apply the CSE framework to the problem of building an effective biomedical question answering (BioQA) sys-tem from available components and component options. The goal of this work is to demonstrate the effectiveness of the proposed ap-proach and the current open source implementation. Specifically, we employed the topic set and benchmarks from the question an-swering task of TREC Genomics Track [11], as well as commonly-used tools, resources, and algorithms cited by participants. A set of basic components was selected and adapted to the CSE framework implementation by writing wrapper code where necessary. Then an experiment configuration descriptor was defined for the result-ing set of configured components. This configuration space was explored with the CSE algorithm automatically, yielding an opti-mal and generalizable configuration which outperformed published results of the given components for the same task. Detailed compo-nent descriptions and result analysis can be found in the extended technical report [36].
The TREC Genomics QA task involves retrieval of short pas-sages that specifically address an information need expressed as a question, and the answers are provided along with a reference to the location of the answer in the original source document. A total of 64 genomics-related topics were developed for this task, asking about biological processes or relationships between biomedical en-tities. We employed both the document collection and the topic set from the official evaluation and focused on two task-specific metrics: DocMAP and PsgMAP. Intuitively, DocMAP measures the relevance of the documents retrieved by the system, regardless of the relevance and conciseness of the passages extracted. The PsgMAP metric also considers the relevance of extracted passage spans. Details can be found in the overview paper [11].
To evaluate the generality of the framework in selecting op-timal traces for novel data, and further test the performance of the selected traces, we implemented nested K-fold cross-validation (leave-one-out, 5-fold and 10-fold) and nested bootstrap methods, where two subsets of 28 topics used by TREC Genomics 2006 were held out for trace selection (validation set) and performance eval-uation (test set) respectively. Furthermore, we employed 36 topics from TREC Genomics 2007 to test the adaptability of traces for a similar but slightly different task, with questions asking for lists of specific biomedical entities (in Section 6.6).
We mainly considered four different aspects when collecting re-sources and implementing algorithms to build a biomedical ques-tion answering system: NLP tools , KBs , retrieval tools , and rerank-ing algorithms . We considered popular and successful approaches reported in the TREC Genomics literature to explore with the CSE framework. We summarize these components in Table 2, and briefly describe them in the rest of this subsection.

First, many successful systems employed natural language pro-cessing (NLP) of the questions and/or target texts, using algo-rithms, toolkits, and pre-trained models for sentence segmentation, tokenization, part-of-speech tagging, named entity recognition, etc. To establish a benchmark for NLP in our system, we focused on several rule-based approaches for generating lexical variants, and supervised learning methods provided by LingPipe.

Second, tens of biomedical KBs have been organized and main-tained. Some involve knowledge from all areas of biomedical re-search, while others focus on only a few subareas, e.g., disease, ge-nomics, etc. The implication is that for a particular topic, the qual-ity of the answer generated by the system may vary by the KB(s) that are used. We focused on three resources most popular with TREC Genomics participants: UMLS, EntrezGene, and MeSH.
Next, to retrieve relevant documents from the unstructured biomedical corpus, we need to rely on a widely-used open-source search engine, e.g. Indri 12 . Finding an optimal retrieval configu-ration requires selecting parameter values specific to each search engine, such as the retrieval model and the parameters in the scor-ing function, smoothing method, query formulation strategy, etc.
Finally, a number of participants performed postprocessing on their passage output. This was done to refine the boundaries and ranks of retrieved passages, using techniques such as evidence tar-geting, answer merging and rescoring, etc. The aspect that catego-rizes these approaches is reranking algorithms.

We followed the basic pipeline phases for a question answering system [31] and implemented a domain-independent QA frame-work. We integrated benchmarks, task-specific evaluation meth-ods, as well as 12 components, and specified one to several values for each parameter associated with each component, and plugged them into the framework. To support straightforward reproducibil-ity of results, the frameworks, components, and support materials are available online as part of our open source release.
We designed two experiments to evaluate the CSE framework implementation, given moderate and large-scale configurations re-spectively. In the first experiment, we limited the number of op-tions and consequently derived 32 configured components ( which could produce a maximum number of 2,700 traces and re-quire 190,680 executions to process all the questions. An exper-iment of this complexity was expected to execute within a day on our available test hardware (corresponding to C with resource defined by execution time). The mean and standard deviation for global execution time were initially set as 10 seconds ( and ) and the expected benefit for a single configured component was set as 0.1 in terms of PsgMAP ( and ), and the parameters at phase and component levels were estimated by an empirical Bayesian method. In the second experiment, we planned to test the scalabil-ity of the implementation by aggressively setting up to six values for each parameter, and thus yielded a CSE problem with 2,946 configurations and 1.426 10 12 traces, requiring 6.050 10 cutions in total to evaluate the entire space.
 We compare the settings for the two experiments with the official TREC 2006 Genomics test results for the participating systems in Table 3. Although many more configured components were imple-mented by the original participants, only 92 different traces were evaluated. We estimate the number of components, configurations Table 3: Overview of experimental settings and comparison with TREC Genomics 2006 participants and executions evaluated by the official test, and show the evalua-tion results [12] in Table 3 for reference. The results reported from the two experiment settings were evaluated with the nested leave-one-out cross-validation method. Different strategies of estimating generalizability of traces are further compared in Section 6.6.
From Table 3, we can see that the best system derived automati-cally by the proposed CSE framework outperformed the best partic-ipating system in terms of both DocMAP and PsgMAP, with fewer, more basic components. Similarly constrained exploration of the much larger Scaled CSE configuration failed to yield a system that performs as well in comparable time, highlighting the importance of a priori human selection of appropriate configurations and/or hyperparameter values. Nevertheless, the configuration yielded by exploring the larger space at the same capacity cost still outper-formed most of the participating systems reported in the TREC Genomics track paper. The detailed pipeline configuration of the best system derived by the proposed CSE framework is described in Section 6.4 as the baseline to analyze component contributions.
As the best trace has been discovered, we report the performance of traces with only a single component or configuration different from the best trace, in terms of PsgMAP, to provide a useful com-parison. In this section, we investigate in detail how each com-ponent or configuration contributes to overall system performance. The results are shown in Table 4 for nominal configurations, e.g. different KBs, score transformation methods, and Figure 3 for real-valued configurations, e.g. smoothing parameters, term weights, etc. We also report the significance test (t-test) results, and label the scores with different significance levels, in Table 4. The con-figuration of the best system is also shown in Table 4, which uses most available resources (KBs, reranking algorithms, NLP tools).
We see that the performance of leveraging various sources var-ied for synonym expansion and acronym expansion. Incorpora-tion of MeSH in synonym expansion could make the biggest con-tribution to the performance, while both UMLS and EntrezGene also benefited the system X  X  performance. However, they also con-tained more noisy content than MeSH, which hurt the performance when combining with MeSH. As most acronyms were expanded as synonyms, integrating any combination of KBs hardly affected the performance, though we find that EntrezGene is a useful source for acronym expansion compared with others. Unsurprisingly, we con-firm that synonyms should be considered in extracting important sentences and reranking, and filtering overlapping passages could better help the task than only filtering identical passages. Different transformation strategies hardly affected the overall performance.
For the real-valued parameters, we can see from Figure 3 that al-tering weights for concept terms or verbs could greatly change the performance. In particular, concept terms favored higher weights and verbs favored moderate weights ( 0.4). Jelinek-Mercer out-performed Dirichlet smoothing for this task with the best perfor-mance achieved when the parameters were set as 0.1 and 1000. We also found the performance was improved with the parameters set to lower values than those reported in their original paper for the important sentence extraction and reranking algorithms [30].
In this subsection, we demonstrate how the CSE framework dis-covered the best trace presented in Section 6.3 by analyzing the parameters estimated by the framework at different stages.
We show in Figure 4 the likelihood and posterior distributions of cost estimated by the CSE framework when the experiment com-pleted. First, Figures (a) and (b) represent likelihood estimation of cost for five phases: score combination methods (C), overlapping passage solutions (O), retrieval strategists (R), sentence extraction and reranking (S), and term proximity based reranking (P) respec-tively. We can clearly see that the framework discovered each phase had a different distribution of execution time, and C and O compo-nents usually performed faster than R, S, and P. Figures (c) to (d) represent likelihood estimation of component-level cost for these phases. We see that the configured components in each of C, O, and P had similar cost distributions, while the behavior of different S and R components varied, since the computational complexity of these components tended to be easily affected by different config-urations. Figures (e) to (h) are the posterior estimations of to predict c ( f c t j ! c t ) for unseen components. Since enough sam-ples were provided at the end, the mean of each t or c t was close to that of the likelihood estimation, and since more samples were available to estimate the phase-level costs than the component-level costs, the variance of t tended to be smaller than that of
We further investigate how the likelihood and posterior distri-butions of cost changed through the execution process by showing both distributions estimated when the experiment completed 1% of total execution tasks in Figure 5. Comparing with Figure 4, we see that the means of both distributions were smaller than those estimated at the end, due to the randomness of initially sampled components and input sets; the variances of the likelihood distribu-tions were smaller due to fewer samples. According to Algorithm 1, components of downstream phases (C and O) tended to be sam-pled and tested more frequently than those of upstream phases (R, S, and P), due to smaller LCC scores; variances of the estimated posterior distributions of R, S, P components tended to be greater than those of C and O, which is clearly shown in Figures (e) to (h).
Estimating benefit had a similar process as estimating cost. We use various configurations to illustrate the estimated likelihood and posterior distributions of benefit when the 20% of total execution tasks were completed. Figures 6(a) and (b) show the likelihood estimation of the benefit for different configurations of the rerank-ing algorithms and KBs, which exhibited different effects. Since KBs were integrated at the beginning at the pipeline while rerank-ing algorithms were placed at the end, very few combinations of various KBs had been tested thus far, which correspond to the few  X  X harp X  distributions, and the likelihood distributions correspond-ing to others remain unknown; on the other hand, different rerank-ing methods were tested more equally frequently, as a result, their distributions were more alike. This assumption can also be justified from Figures (c) and (b), which correspond to the posterior estima-tions of both component-level benefit means, where variances of posterior distributions for reranking algorithms were close, while those corresponding to the untested KBs took the global variance.
In this subsection, we first demonstrate the performance of trace selection results by various resampling methods. Besides the leave-one-out cross-validation, we also applied 5-fold cross-validation, 10-fold cross-validation (each repeated 100 times) and bootstrap (repeated 300 times) for model selection and performance evalua-tion. We find that the 10 highest ranked traces produced by all the at component level. (e) to (h) are the posterior estimations of cost mean Figure 7: Generalizability of traces with various resampling methods on (a) holdout topic set from 2006 and (b) unseen topic set from 2007. X-and Y-axis correspond to PsgMAP scores. resampling methods are identical; we further compare the perfor-mance of all traces evaluated on the validation set and test set in Figure 7(a). We can see that the traces produced by the proposed framework with a resampling based trace selection method are gen-eralizable to unseen data for the task, and cross-validation methods are less biased than bootstrap method.

We also tested these automatically selected configurations on 36 topics from TREC Genomics 2007 to test their adaptability for a different but similar task. TREC Genomics 2007 topics are in the form of questions asking for lists of specific entities, e.g.  X  X hat [GENES] are genetically linked to alcoholism? X . For this example question, the expected output would be a ranked list of passages that relate one or more entities of type GENE to alcoholism.
We used the 2007 topic set to test the configurations selected by CSE for the 2006 data set (Table 5). We can see that the best trace slightly outperformed the best reported in [13]. The differ-ence in performance on the 2006 and 2007 datasets is plotted in Figure 7(b), which is clearly larger than the performance differ-ence between different folds drawn from the 2006 data set. The best configuration is also different, and favors different KBs and weights. Nevertheless, the coefficient of correlation between 2006 and 2007 tests (0.272) still confirms that the best configurations from 2006 tend to perform better for 2007. We also see that data points are clearly clustered into groups, and the performance differ-ence between clusters tends to be more significantly different than that within each cluster. This indicates that traces which statisti-cally significantly differ from the top traces should be explored to better diversify the system outputs and enhance the likelihood of achieving optimal performance.
Finally, we used the CSE framework to automatically configure a different type of biomedical question answering system for the QA4MRE (Question Answering for Machine Reading Evaluation) task 13 at CLEF, where 12 UIMA components were first developed, and then 46 configurations were specified for CSE framework to explore 1,040 different combinations with 1,322 executions. The CSE framework identified a better combination, which achieved 59.6% performance gain over the original pipeline. Details can be found in the working note paper [24].
In this paper, we investigate the problem of configuration space exploration (or CSE ). We introduce the CSE framework as a solu-tion for building and exploring configuration spaces for informa-tion systems. The framework was used to implement a biomedical information system for the TREC Genomics QA by automatically and effectively exploring over a trillion system configurations.
In future work we plan to investigate to extend the CSE model for continuous parameter values, and incorporate the dependency on the input type. We also plan to apply CSE to the creation of other information processing pipelines in other task domains.
