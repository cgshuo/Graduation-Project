 FUMIYO FUKUMOTO and YOSHIMI SUZUKI University of Yamanashi 1. INTRODUCTION
With the exponential growth of information on the Internet, it is becoming increasingly difficult to find and organize relevant material. Topic detection and tracking (TDT) is a research area to address this problem and includes five different tasks: story link detection, clustering topic detection, new event detection, story segmentation, and topic tracking. The last task, topic tracking, is the focus of this paper. Topic tracking starts from a few sample stories and finds all subsequent stories that discuss the target topic. Here, a topic in the TDT context is something that happens at a specific place and time associated with some specific action [Allan et al. 1998].

A wide range of statistical and ML techniques have been applied to topic tracking, including k NN classification, decision tree induction [Carbonell et al. 1999], relevance feedback method of information retrieval(IR) [Oard 1999; Papka and Allan 1999], unsupervised and supervised clustering [Franz and McCarley 2001], and a variety of types of language modeling [Schwartz et al. 1997; Jin et al. 1999; Schultz and Liberman 1999; Lowe 1999; Yamron et al. 1999; Allan 2002; Allan et al. 2003; Larkey et al. 2004]. The main task of these techniques is to tune the parameters or the threshold to produce optimal re-sults. However, parameter tuning is a difficult issue for tracking [Yang et al. 2000] because the number of initial positive training stories is very small (from 1 to 4), and topics are localized in space and time. For example,  X  X aipei May-oral Elections X  and  X  X .S. Midterm Elections X  are topics, but  X  X lections X  is not. Therefore, the system must estimate whether the test stories are concerned with the same topic with little information about the topic. Moreover, the train-ing data are skewed , i.e., there are large numbers of stories that are labeled negative as compared to those labeled positive. The system thus needs to bal-ance the numbers of positive and negative training stories so as not to hamper the accuracy of estimation.

In this paper, we propose a method for estimating efficient training stories for topic tracking. For a small number of labeled positive stories, we use bilingual comparable corpora (TDT1, TDT3 English and Japanese newspapers, Mainichi, and Yomiuri Shimbun). Our hypothesis using bilingual corpora is that many of the broadcasting stations from one country report local events more frequently and in more detail than overseas broadcasting stations, even if it is an event that has drawn international attention. Let us examine some topic from the TDT corpora. There are 22 stories concerning the topic,  X  X iang X  X  historic visit to Japan X  from TDT3. In contrast, there are 65 stories from the Mainichi Shimbun newspaper over the same period and 46 stories from the Yomiuri newspaper. In a similar way, a topic,  X  X obe Japan quake X  from TDT1 is an event that at-tracted international attention, and 89 stories were included in TDT1. However, Mainichi and Yomiuri Japanese newspapers had many more stories concerning this topic from the same period (5,029 and 4,883 stories, respectively). These observations indicate that it is crucial to investigate the use of bilingual com-parable corpora based on the NL techniques to collect more information about some specific topics. We extract Japanese stories relevant to the positive English stories using English X  X apanese bilingual corpora, together with the EDR bilin-gual dictionary. The associated story is the result of alignment of a Japanese term association with an English term association.

Large numbers of labeled negative stories are classified into clusters us-ing labeled positive stories. We used a semisupervised clustering technique, which combines labeled and unlabeled stories during clustering. Our goal for semi-supervised clustering is to classify negative stories into clusters where each cluster is meaningful in terms of class distribution provided by one cluster of positive training stories. We introduce k -means clustering that can be viewed as instances of the EM algorithm, and classify negative stories into clusters. In the tracking task, no knowledge of the number of topics in the negative training stories is available. Therefore, we use the Bayesian information criterion (BIC) as the splitting criterion of the k means and select the appropriate number for k . 2. RELATED WORK
Most of the work addressing the small number of positive training stories ap-plies statistical techniques based on word distribution and ML techniques. the threshold strategy to tackle the problem. The basic idea behind their work is that stories closer to each other in the stream are more likely to discuss re-lated topics than those further apart. The method is based on unsupervised learning techniques, except for its incremental nature. When a tracking query is first created from the N t training stories, it is also given a threshold. During the tracking phase, if a story S scores over that threshold, S is regarded as rel-evant, and the query is regenerated as if S were among the N t training stories.
This method was tested using the TDT1 corpus and the adaptive approach was found to be highly successful. However, adding more than four training stories provided little help, although in their approach, 12 training stories were added.
More recently, Allan X  X  group at UMass proposed a method using adaptation, the traditional vector-space, and relevance models [Allan et al. 2003; Connell et al. 2004; Larkey et al. 2004]. Adaptation allows addition of the incoming story to the topic representation and recomputes the topic centroid. Adaptation uses an additional threshold that determines whether a new story is sufficiently similar to the centroid to be added to the topic. A new centroid is computed from the story vectors each time a new story is added to the topic. The method proposed in this paper is similar to these methods, however, our method for collecting relevant stories is based on story pairs extracted from bilingual comparable corpora.
 in the cross-language IR task or MT systems/bilingual lexicons [Dagan and
Church 1997; Collier et al. 2002; Utsuro et al. 2003]. Much of the previous work uses cosine similarity between story term vectors with some weighting techniques [Allan et al. 1998], such as tf  X  idf , or cross-language similarities of terms.
 for improving story-link detection. They used source modalities and language information. Source is a type of text. Chen et al. defined two types, asr (auto-matic speech recognition) and text, and the modality pairs consist of (asr,asr), (asr,text), and (text,text). Source-pair specific information is the combination of three languages, English, Arabic, and Mandarin. They reported that the use of source-pair specific information is a larger factor in improving detection performance. Link detection is an intermediate task that is not an end in itself, but rather is necessary to accomplish other event-based topic analysis tasks, such as topic detection and topic tracking. However, there have been few studies in which the results of link detection have been applied to other tasks. We use the results of story pairs between positive stories and the associated stories to tackle a small number of initial labeled positive stories (at most four stories) in the tracking task.

In the TDT tracking task, classifying negative stories into meaningful groups is also an important issue to track topics, as a large number of labeled negative stories are available in the TDT context. Our method to overcome the problem is based on semisupervised clustering [Blum and Mitchell 1998; Ghahramani and Jordan 1994], i.e., k means with EM (expectation maximization). Basu et al. [2002] proposed a method using k -means clustering with the EM algorithm, where labeled data provide prior information about the conditional distribu-tion of hidden category labels. They reported that the method outperformed standard random seeding and COP X  k means [Wagstaff et al. 2001], an alter-native semisupervised k -means algorithm. Our method shares the basic idea of that reported by Basu et al., except that our method does not require the number of k in advance, as it is determined during clustering. We use the BIC as the splitting criterion and estimate the appropriate number for k . This is an important feature because in the tracking task, no knowledge of the number of topics in the negative training stories is available. 3. SYSTEM DESCRIPTION The system consists of four procedures: extraction of bilingual story pairs, ex-traction of monolingual story pairs, clustering of negative stories, and tracking. 3.1 Extraction of Bilingual Story Pairs We extract story pairs that consist of positive English and associated Japanese stories using the TDT English and Mainichi and Yomiuri Japanese corpora. As shown on the top of Figure 1, the TDT English corpus consists of training and test stories. Training stories are further divided into positive (black boxes) and negative stories(stippled boxes). Arrows refer to edges with similarity between stories. For example, on the top of Figure 1, whether story J 2 discusses the target topic and is related to E 1 or not is determined by not only the similarity value between E 1 and J 2 , but also by the similarities between J 2 and J 4 , E 1 , and J 4 .

The extraction of story pairs can be summarized as follows. Let initial positive training stories E i (1  X  i  X  m ) be the initial node and each Japanese story J j ( i  X  4  X  j  X  i + 4) be a node or terminal node in the graph G . 1 We calculate cosine similarities between E i and J j . In a similar way, we calculate similarities between J k and J l ( i  X  4  X  k , l  X  i + 4). If the value of similarity between nodes is above a certain threshold, we connect them by an edge. We call this the original graph (top of Figure 1). Next, we delete an edge that is not a constituent of the maximal connected subgraph. Here, a graph is a maximal connected subgraph when there are no nodes or edges in the graph that could be added to the subgraph and it remains connected (bottom of Figure 1). We note that we use this subgraph instead of the original graph. In the top of Figure 1, we can see that E 1 is similar to J 2 because they are supported by J 4 , i.e., both stories E 1 and J 2 are similar to J 4 . However, we cannot see whether E 1 is exactly similar to J 6 , because it is only supported by J 6 , i.e., E 1 is similar to
J 6 and J 6 is similar to J 8 , but E 1 is not similar to J 8 . The maximal connected subgraph can remove such vague information to extract stories that are very relevant to each other.

From the results obtained with the maximal connected subgraph, we extract pairs of initial positive English and Japanese stories E i and J j , respectively, as a linked story pair, and add the associated Japanese story J j to the training stories. On the bottom of Figure 1, E 1 , J 2 , and J 4 are extracted. The procedure for calculating cosine similarities between E i and J j consists of two substeps: extracting terms and estimating bilingual term correspondences. 3.1.1 Term Extraction. The first step in calculating the similarity between E i and J j is to align a Japanese term with its associated English term using the bilingual dictionary, EDR. However, this naive method suffers frequent failure because of incompleteness of the bilingual dictionary. Let us examine the Mainichi Japanese newspaper stories. The total number of different terms (words) from Oct. 1, 1998 to Dec. 31, 1998, was 59,350. Of these, 30,818 terms are not included in the EDR bilingual dictionary. For example, the Japanese term  X  % (%S%G%P! &lt;  X  (Endeavour) which is a key term for the topic  X  X huttle Endeav-our mission for space station X  from the TDT3 corpus is not included in the EDR bilingual dictionary. New terms that fail to segment during a morphological analysis are also a problem in calculating similarities between stories in mono-lingual data. For example, a proper noun  X  &lt; sETBg3XE15  X   X  (Tokyo Metropolitan Univ.) is divided into three terms,  X  &lt; sET  X  (Metropolitan),  X  Bg3X  X  (Univ.) and  X  E15  X   X  (Tokyo). In a similar way, a compound noun,  X  2HBpA \ : : X  (house search-ing) is divided into two:  X  2HBp  X  (house) and  X  A \ : : X  (searching). To address these problems, we conducted term extraction from a large collection of English and Japanese corpora. Of the several techniques available for term extraction [Chen and Goodman 1996], we used the n -gram model with Church X  X ale smoothing, as Chen reported that it outperforms all existing methods on bigram models produced from large training data sets. All Japanese stories were tagged by the morphological analysis Chasen [Matsumoto et al. 2000]. English stories were tagged by a part-of-speech tagger [Schmid 1995] and stop word removal. We used noun words as terms. The length of the extracted terms does not have a fixed range and we, therefore, set at most five noun words. We applied the normalization strategy shown in Eq. (1) to each length of the terms to bring the n -gram w 1  X  X  X  w n of probability value P ( w n 1 ) into the range [0,1]. 2 We ex-tracted terms whose probability P ( w n 1 ) new is greater than a certain threshold. Words from the TDT English (Japanese newspaper) corpora are identified, if they match the extracted terms.
 3.1.2 Bilingual Term Correspondences. The second step to calculate simi-larity between E i and J j is to estimate bilingual term correspondences using  X  2 statistics for which there are several approaches [Dagan and Church 1997; Collier et al. 2002]. The method proposed by Utsuro et al. [2003] used  X  2 to ex-tract bilingual term correspondences from WWW news sites. They reported that  X  2 is quite effective in reducing the amount of human intervention necessary for selecting correctly estimated bilingual term correspondences. Similar to
Utsuro X  X  approach, we used  X  2 statistics to estimate bilingual term correspon-dences with a large collection of English and Japanese data. More precisely, let E i be an English story (1  X  i  X  n ), where n is the number of stories in the collection and S i J denote the set of Japanese stories with cosine similarities
Japanese stories of S i J are then concatenated into one story S i J automatically, and a corpus C EJ of English and Japanese stories C EJ = {{ E i , S i J }| S i J = 0 } is constructed. Suppose that there are two criteria, monolingual terms t E and t J in the English and Japanese stories, respectively. We can determine whether a particular term belongs to a particular story. Consequently, terms are divided into four classes, as shown in Table I. Based on the contingency table of cooc-currence frequencies of t E and t J , we estimate bilingual term correspondences according to the statistical measure  X  2 .
We extract term t J as a pair of t E , which satisfies the maximum value of  X  2 , i.e., max t
Japanese term pairs, we conducted semi-automatic acquisition, i.e., we manu-ally selected bilingual term pairs, as our source data set was not a clean parallel corpus, but an artificially generated noisy pseudoparallel corpus and it is diffi-cult to compile bilingual terms fully automatically [Dagan and Church 1997].
This manual selection, however, is not very difficult, and it is only necessary once, i.e., it is not necessary to repeat this step for every topic in the collection. It is more efficient to find more relevant monolingual stories or more easier than manual selection of the most relevant Japanese stories as we checked only term pairs instead of reading every story. 3 Finally, we align a Japanese term with its associated English term using the selected bilingual term correspondences and again calculate cosine similarities between Japanese and English stories. 3.2 Extraction of Monolingual Story Pairs
We noted above that our source data set was not a clean parallel corpus. Thus, the difference in date between bilingual stories is one of the key factors to im-prove the performance of extracting story pairs, i.e., stories closer together in the timeline are more likely to discuss related subjects. Therefore, we applied a method for extracting bilingual story pairs from stories closer in the time-lines. However, this often hampers our basic motivation for using bilingual corpora: bilingual corpora help in collecting more information about the target topic. Therefore, we extracted monolingual (Japanese) story pairs using the results of bilingual story pairs and added them to the training stories. Extract-ing Japanese monolingual story pairs is quite simple: Let J j (1  X  j  X  m )be the extracted Japanese story in the procedure extracting bilingual story pairs. We calculate cosine similarities between J j and J k (1  X  k  X  n ). If the similarity value between them is above a certain threshold  X  mono ,weadd J k to the training stories. 3.3 Clustering Negative Stories Our method for classifying negative stories into clusters is based on that re-ported by Basu et al. [2002], which uses k means with the EM algorithm. K means is a clustering algorithm based on iterative relocation that partitions a dataset into the number of k clusters, locally minimizing the average squared distance between the data points and the cluster centers (centroids). Suppose we classify X ={ x 1 ,  X  X  X  , x N } , x i  X  R d into k clusters: one is the cluster which consists of positive stories; other k  X  1 clusters consist of negative stories. Here, which clusters does each negative story belong to? The EM is a method of finding the maximum-likelihood estimate (MLE) of the parameters of an underlying distribution from a set of observed data that has missing value. k -means is es-sentially an EM on a mixture of k Gaussians under certain assumptions. In the standard k means without any initial supervision, the k -means are chosen ran-domly in the initial M step and the stories are assigned to the nearest means in the subsequent E step. For positive training stories, the initial labels are kept unchanged throughout the algorithm, whereas the conditional distribution for the negative stories are reestimated at every E step. We select the number of k initial stories: one is the cluster center of positive stories and other k  X  1 sto-ries are negative stories, which have the top k  X  1 smallest value between the negative story and the cluster center of positive stories. In Basu et al X  X  method, k is given by a user. However, in the negative training stories, the number of clusters is not given beforehand. We thus developed an algorithm for estimat-ing k . It goes into action after each run of k means, 4 making decisions about which sets of clusters should be chosen in order to better fit the data. There are several functions to select the appropriate number for k . We used BIC, as it was more effective than 0/1 loss or log loss functions. 5 BIC is defined by Eq. (3).
 total number of training stories, and p l is the number of parameters in k = l .We the number of n  X  k centroid coordinates, and the MLE for the variance,  X   X  2 .
Here, n is the number of dimensions.  X   X  2 , under the identical spherical Gaussian assumption, is: where x i denotes i th data point in the input, and  X  ( i ) is the centroid associated by the i th point during an iteration. The point (story) probabilities are: hood of ll ( X ) is log i P ( x i ). It is taken at the maximum-likelihood point(story), and, thus, focusing just on the set X m  X  X which belongs to the centroid m and plugging in the MLE yields:
We choose the number of k whose BIC is the highest value. 3.4 Tracking
Each story is represented as a vector of terms with tf  X  idf weights in an n -dimensional space, where n is the number of terms in the collection [Allan et al. 1998]. Whether each test story is positive is judged using the distance (mea-sured by cosine similarity) between a vector representation of the test story and each centroid g of the clusters. Figure 2 illustrates each cluster and a test story in the tracking procedure. Figure 2 shows that negative training stories are classified into three groups. The centroid g for each cluster is calculated as follows: where x ij (1  X  j  X  n ) is the tf  X  idf weighted value of term j in the story x i . The test story is declared positive if the cosine similarity between the test story and the centroid with positive stories has the smallest value. In Figure 2, the test story is regarded as negative, since the value between them is smallest. This procedure is repeated until the last test story is judged. 4. EXPERIMENTS 4.1 Creating Japanese Corpus We chose the TDT3 English corpora as our gold standard. TDT3 consists of 34,600 stories with 100 manually identified topics(1999 Evaluation Topics and 2000 Evaluation Topics). We then created Japanese corpora (Mainichi and Yomiuri newspapers) to evaluate our method, especially the results of the ex-tracted bilingual story pairs.

We annotated the total number of 66,420 stories from Oct. 1 to Dec. 31, 1998, against the 100 topics. Each story was labeled according to whether it discussed the topic or not. Not all the topics defined by TDT3 were present in the Japanese corpora. Moreover, as noted by several researchers, one difficulty of topic tracking is that the discussion of a topic, i.e. the subject, changes overtime when discussed over a long period. However, in TDT3, there is no topic related to Japan discussed over a long period. Therefore, we collected 1 topic from TDT1, which occurred in Japan, and added it to the experiment to examine whether our method is effective for such a topic. TDT1 was collected from Jul. 1 1994 to Jun. 30, 1995, and the first story concerning the  X  X obe Japan quake X  topic is from Jan. 16th. We annotated 174,384 stories of Japanese corpora from the same period for the topic, i.e. from Jan. 16 to Jun. 30, 1995. Each story was classified as concerning the topic based on the agreement of the two authors as judges. Table II shows 24 topics included in the Japanese corpora.  X  X DT X  refers to the evaluation data, TDT1, or 3.  X  X D X  denotes topic number defined by the TDT.  X  X n-Topic X  refers to the number of stories discussing the topic. Bold font indicates topics that occurred in Japan. The annotation was also evaluated by the two authors and the classification was determined to be correct based on the agreement of the two human judges. 6 4.2 Experiments Setup We extracted terms, bilingual term correspondences, and bilingual and mono-lingual story pairs from a large collection of English and Japanese data. The English data used were taken from Reuters X 96 from Aug. 20, 1996 to Aug. 19, 1997 (806,791 stories), TDT1 from Jul. 1, 1994 to Jun. 30, 1995 (15,863 stories), and TDT3 from Oct. 1, 1998 to Dec. 31, 1998 (34,600 stories). The Japanese data consisted of 1,874,947 stories from Mainichi newspaper over a period of 14 years (from 1991 to 2004, 1,499,936 stories), and Yomiuri newspaper over a period of 3 years (1994, 1995 and 1998, 375,011 stories). We divided both English and
Japanese stories into two sets: a training set to estimate thresholds used in our method, and a test set to evaluate each method using these thresholds. Both
English and Japanese stories from Aug. 20, 1996 to Feb. 19, 1997 were used as a training set, and the remaining stories were used as a test set. All Japanese stories were tagged by the morphological analysis Chasen [Matsumoto et al. 2000]. English stories were tagged by a part-of-speech tagger [Schmid 1995], and stop word removal.

Church X  X ale smoothing to noun words for each training data and selected the probability that maximized the average precision (the ratio of correct terms divided by the total number of terms). The threshold value for both English and Japanese was 0.800. We used these thresholds to extract terms from a collection of English and Japanese test data. As a result, we obtained 338,554
Japanese and 130,397 English terms. We used the EDR bilingual dictionary, and translated Japanese terms into English.
 was no translation in the dictionary. Each story is represented as a vector of terms with tf  X  idf weights [Allan et al. 1998]. We calculated story similar-ities and extracted story pairs between positive and associated stories. The threshold values for bilingual (  X  bi ) and monolingual (  X  mono ) pairs were 0.65 and 0.48, respectively. These threshold values were determined using three sets of training data, i.e., we choose those values that maximized the average precision among them. Moreover, we also estimated term correspondences. We manually selected bilingual terms in the topmost 3000, according to the  X  2 value, as we obtained few bilingual terms when the rank order of bilingual term correspon-dences sorted by  X  2 value was lower than 3000.

In tracking, we used the extracted terms together with all verbs, adjectives, and numbers, and represented each story as a vector of these with tf  X  idf weights.
We set the evaluation measures used in the TDT benchmark evaluations.  X  X iss X  denotes the miss rate, which is the ratio of the stories judged as YES but were not evaluated as such for the run in question.  X  X /A X  shows the false alarm rate, which is the ratio of the stories judged as NO, but were evaluated as YES . The DET curve plots misses and false alarms, and better performance is indicated by curves closer to the lower left of the graph. The detection cost function ( C Det ) is defined by Eq.(8).
 C
Miss , C Fa , and P Target are the costs of a missed detection, false alarm, and priori probability of finding a target, respectively. C Miss , C Fa , and P Target are usually set to 10, 1, and 0.02, respectively. The normalized cost function is defined by Eq. (9), and lower cost scores indicate better performance. 4.3 Basic Results Table III summarizes the tracking results.  X  X ilingual corpora and clustering X  shows the results obtained by our method.  X  X ot clustering X  and  X  X ot bilingual corpora X  show the results to examine the effects of using bilingual corpora and k means with EM for classifying negative training stories, respectively.  X  X ot clustering X  in Table III refers to the results using the extracted bilingual stories and without classifying negative stories. We calculated the centroid using all negative training stories and a test story was judged to be negative or positive by calculating cosine similarities between the test story and each centroid of negative and positive stories.  X  X ot bilingual corpora X  indicates the results using only k means with EM for classifying negative training stories and without bilingual story pairs.

Each result in Table III denotes micro-averaged scores.  X  X ecall X  is the ratio of correct assignments by the system divided by the total number of correct assignments.  X  X recision X  is the ratio of correct assignments by the system, di-vided by the total number of system X  X  assignments. MIN denotes MIN ( C Det ) Norm , which is the value of ( C Det ) Norm at the best possible threshold. N t is the number of initial positive training stories.

The results of  X  X ilingual corpora and clustering X  are always better ( MIN ) than the other two approaches in both TDT1 and TDT3, even for a small number of initial positive training stories.
 and clustering, X   X  X ot clustering X  and  X  X ot bilingual corpora, X  for 23 topics from
TDT3. Figure 4 shows the results for four topics from TDT1 and TDT3 that occurred in Japan. To make some comparison possible, only N t = 4 is given for each. Both figures show that we have an advantage using bilingual corpora and k means with EM, especially; the results using bilingual corpora were slightly better than those using k means with EM. 4.4 Effects of Extracted Bilingual and Monolingual Story Pairs
We used the results of bilingual and monolingual story pairs for tracking. The contribution of the extracted story pairs is best explained by looking at the two results: (i) the tracking results obtained by our method, i.e., bilingual and monolingual story pairs, with only English and Japanese stories with difference in dates between them of  X  4, and without story pairs, i.e., not bilingual corpora, and (ii) the results of story pairs with varying values of N t . Figure 5 shows DET curves for 23 topics, N t = 4.
 the overall performance, especially, the results obtained by our method were better than those with only English and Japanese stories differing in date by  X  4.
 sociated stories by varying values of N t . Results are shown as micro-averaged scores.  X  X ecall X  is the ratio of correct pair of related stories in Japanese and
English assignments by the system divided by the total number of correct as-signments.  X  X recision X  is the ratio of correct pairs of related stories in Japanese and English assignments by the system divided by the total number of system X  X  assignments. As shown in Table IV, the system using our method correctly ex-tracted stories related to the target topic, even for a small number of positive training stories, as the ratio of precision in N t = 1 was 0.82. However, the re-call values in Table IV are low. We used a simple similarity measure, cosine similarity, to obtain story-pairs. However, there are other similarities between stories such as KL divergence and it would be worth evaluating the method us-ing such similarities. Another solution is to use an incremental approach, i.e., by repeating story pair extraction, new story pairs that have not been extracted previously may be extracted, so there is considerable room for improvement.
The effects of story pairs for the tracking task are also dependent on the performance of bilingual term correspondences. We obtained 1368 English and Japanese term pairs in the topmost 3000, according to the  X  2 value, when the period was  X  4. Figure 6 illustrates the results using different number of days (  X  1to  X  10) from the same period of TDT1 and TDT3. For example,  X   X  1 X  shows that the difference of dates between English and Japanese story pairs was less than  X  1. The y axis shows the precision, which is the ratio of correct term pairs by the system divided by the total number of system X  X  assignments (3000 assignments). As shown in Figure 6, the difference in dates between bilingual story pairs affects the overall performance. 4.5 Effects of BIC with EM-Based k Means We have shown that k means with EM is effective for tracking, based on the results without classifying negative stories in Section 4.3. Recall that we used the BIC as the splitting criterion and estimated the appropriate number for k in k means with EM. To examine the effect of using the BIC, we performed a comparison with a predefined k , i.e., k = 10, 50, or 100. Figure 7 shows part of the result for k = 100. As can be seen in Figure 7, the method without classifying negative stories ( k = 0) did not perform as well and resulted in a high miss rate. This result is not surprising, because the size of negative training stories is large compared with that of positive stories and, therefore, the test story is erroneously judged as NO. Furthermore, this indicates that we need to run BIC, as the result was better than those with any predefined k , i.e., k = 10, 50, or 100.
 each of the 24 topics, and the number of clusters k obtained by our method. As shown in Figure 8, there is no correlation between them. The minimum number of clusters k was 44 and the maximum was 100. 4.6 Comparison to Related Work
Recall that we used a subset of the topics defined by the TDT, as the purpose of this study was to investigate whether the bilingual comparable corpora help increase the initial seed stories and, therefore, overall tracking performance.
Thus, we implemented the UMass adaptation method with relevance models [Allan et al. 2003; Connell et al. 2004; Larkey et al. 2004] and compared the results. This method is similar to our method in adaptation and is an enhance-ment based on several years of TDT tracking research, while there is other research work on topic tracking, such as the approaches proposed by CMU and
ICT in 2004 [Connell et al. 2004; Zhang and Callan 2004; Yu et al. 2004]. Topic tracking based on relevance modeling is shown in Eq. (10).

P ( w | GE ) in Eq. (10) denotes the general English probability of term w , com-puted from the entire collection of stories. We used TDT1 (15,863 stories) and
TDT3 (34,600 stories) English stories. The terms we used in their method are all nouns, verbs, adjectives, and numbers. Moreover, we added the extracted terms by our method to these part-of-speech words to make their results com-parable with the results by our method. P ( w | S ) is the language model of the story. P ( w | T ) denotes the relevance model of the topic T and is pruned to 100 terms. T is represented by a centroid, which is an average of the vector representatives of the training stories. For the story model P ( w | S ), we also used smoothed maximum-likelihood estimates rather than relevance models. P ( w | T ), on the other hand, was constructed incrementally as follows: it starts by setting P 0 ( M d ) = 1 N P ( w | T ) is initialized according to Eq. (11).
 C in Eq. (11) is the entire collection of stories. The tracking is as follows: if the similarity Sim track ( T , S ) of some story S exceeds a predefined threshold  X  ad , the topic model is updated. The update involves setting P n one of the original N t training stories, or one of the n ad stories which scored above the adaptation threshold  X  ad . We set  X  ad to 0.5. P n all other stories and P ( w | T ) is recomputed using Eq. (11) with P n number of stories for adaptation n ad is less than 100 [Larkey et al. 2004]. The results are shown in Figures 9 and 10.

Figure 9 shows DET curves for 23 topics, N t = 4 by our method and UMass adaptation method with relevance models; and Figure 10 shows the results for four topics from TDT1 and TDT3 English, which occurred in Japans, N t = 4. It can be seen that the performance of our method was competitive to the adaptation method with relevance model. The DET curve of the adaptation method shows an emphasis on low false-alarm rates. On the other hand, the DET curve produced by our method illustrates the balance of recall and pre-cision, as it selected points much closer to the middle of the graph. The DET curves in Figure 10, in which we used four topics occurring in Japan show that our method retains a low miss rates, as the difference in miss rates between the two methods was greater than 20% when the false alarm rate dropped below 0.05%. 5. CONCLUSIONS AND FUTURE WORK We addressed the issue of the differences in size between positive and neg-ative training stories for the tracking task and proposed a method for topic tracking that uses bilingual comparable corpora and semisupervised cluster-ing. The purpose of this study was to investigate whether (i) using bilingual comparable corpora can help increase the initial seed articles concerning the topic originating in the source language country, and (ii) semisupervised clus-tering is effective for a large number of labeled negative stories. We showed that our basic assumption was correct and works well even for a small number of initial positive training stories. To examine the effects of the method proposed here for other topics, especially those covered poorly by the Japanese media, we performed an experiment using topics unrelated to Japan. The overall results showed that there is more FA probability, while the results of our method are competitive to the related work. Our experiments suggest a number of topics for future work as follows. 1. Extension to general topic tracking 2. Automatic extraction of term correspondences 3. Splitting criterion BIC with EM-based k means
