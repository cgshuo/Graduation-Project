 (KPCA) [19] has repeatedly outperformed PCA in many image mo deling tasks [19, 14]. property to have for algorithms in computer vision. Figure 1: Several types of data corruption and results of our method. a) original image, b) cor-ruption by additive Gaussian noise, c) missing data, d) hand occlusion, e) specular reflection. f) to i) are the results of our method for recover-ing uncorrupted data from b) to e) respectively.
 reconstructed and the input data. 2.1 KPCA and pre-image space where linear PCA is performed.
 Let X denote the input space and H the feature space. The mapping function  X  : X  X  H is H exist; furthermore k ( x , y ) = h  X  ( x ) ,  X  ( y ) i [18]. via the kernel trick. Let D = [ d d i  X  X  X  i = 1 , n ij of K is k ( a eigenvectors in the feature space V can be computed as V =  X A , where  X  = [  X  ( d ensure orthonormality of { v the principal subspace { v i =1 h  X  ( x ) , v i i v i where r ( x ) =  X  T  X  ( x ) , and M = X a al [13] propose to approximate the reconstruction of x by arg min objective functions have been proposed by Kwok &amp; Tsang [10] a nd Bakir et al [2]. 2.2 KPCA-based algorithms for dealing with noise, outliers and missing data weigh the importance of some features over the others.
 &amp; Lawrence [16] do not address the problem of outliers.
 3.1 KPCA reconstruction revisited to  X  ( x ) as possible. In other words, finding the KPCA reconstruction of x is to optimize: arg min z ||  X  ( z )  X  P  X  ( x ) || 2 2 .
 quence, finding z that minimizes ||  X  ( z )  X  P  X  ( x ) || 2 x . Furthermore, it is unclear how to incorporate robustness t o the above formulation. reconstruction of x is taken as: Algorithm 1 RKPCA for missing attribute values in training data Input: training data D , number of iterations m , number of partitions k .
Initialize: missing values by the means of known values. for iter = 1 to m do end for relative importance of these two terms. This approach is dep icted in Fig. 3b. by a robust function E there is no reason why E reconstruction of x can be taken as: By choosing appropriate forms for E sample outliers. We will show that in the following sections . 3.2 Dealing with missing data in testing samples with missing values, a logical function E E 3.3 Dealing with intra-sample outliers in testing samples To handle intra-sample outliers, we could use a robust funct ion for E  X  exp (  X   X  and  X  is a parameter of the function. This function is also used in [ 6] for Robust PCA. 3.4 Dealing with missing data and intra-sample outliers in t raining data similarly.
 exp (  X   X  resulting kernel matrix cannot be guaranteed. 3.5 Optimization exp (  X   X  || x  X  y || 2 ) ) that is the most widely used kernel. If E  X  z E ( z ) =  X  z E 1 ( z ) +  X  z E 2 ( z ) = 0 .
  X  z E 1 ( z ) =  X  2  X  2 where Q is a matrix such that q when the difference between two successive z  X  X  is smaller than a threshold. Note that W matrix. Therefore, W only happens if some elements of the diagonal of W are 0 and 1 T 1 Consider the update rule given in Eq.6: z = W  X  1 Furthermore, each element of x is weighted differently by W case of missing data (some entries in the diagonal of W , and therefore W bution of x also depends on the ratio  X  cluster. The attraction force generated by a training data p oint d z towards a Gaussian cluster that is close to x .
 One can derive a similar update rule for z if E is a diagonal matrix; the i th entry of the diagonal is  X / (( z at every iteration as follows:  X  = 1 . 4826  X  median ( {| z 4.1 RKPCA for intra-sample outliers reconstruction [10]. The experiments are done on the CMU Mul ti-PIE database [8]. as the face mask and the values inside the mask are vectorized .
Figure 4: a) 68 landmarks, b) a shape-normalized face, c) synthetic occlusion.
 10  X  7 ,  X  2 = 10  X  6 , and C = 0 . 1 . The parameters are tuned using validation data. 4.2 RKPCA for incomplete training data missing attributes, with m = 25 , k = 10 ,  X  = 0 . 0375 (same as [16]),  X  exactly the same as the setting by [16]. Oil Flow dataset. Our method outperforms other methods for a ll levels of missing data. outperforms other methods for all levels of missing data. 4.3 RKPCA for denoising of our algorithm is comparable with those of other methods.
 defined in Sec. 3.2 with W being the identity matrix. We use Gaussian kernel with  X  =  X  and C = 1 . These parameters are tuned using validation data. method preserves much more fine details than the others. our algorithm outperforms existing methods.

