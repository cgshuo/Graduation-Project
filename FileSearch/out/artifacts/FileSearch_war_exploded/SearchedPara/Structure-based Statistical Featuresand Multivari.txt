
We propose a new method for clustering multivariate time series. A univariate time series can be represented by a fixed-length vector whos e components are statistical features of the time series, capturing the global structure. These descriptive vectors, one for each component of the multivariate time series, are concatenated, before being clustered using a standard fast clustering algorithm such as k -means or hierarchical clustering. Such statistical feature extraction also serves as a dimension-reduction procedure for multivariate time series. We demonstrate the effective-ness and simplicity of our proposed method by clustering human motion sequences: dynamic and high-dimensional multivariate time series. The proposed method based on univariate time series structure and statistical metrics pro-vides a novel, yet simple and flexible way to cluster multi-variate time series data efficiently with promising accuracy. The success of our method on the case study suggests that clustering may be a valuable addition to the tools available for human motion pattern recognition research.
The clustering of time serie s data has attracted great at-tention in the data mining co mmunity recently . Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes [24]. The clustering or classification of univariate time se-ries has been recognized as an essential tool in process con-trol, intrusion detection and ch aracter recognition, etc. [12]. As in many other real-world applications, the volumes of data collected in the form of time series are growing rapidly. Challenges are raised by the growth of data in three differ-ent directions:  X  The length of time series, or the number of data points  X  The number of objects in the data set could increase  X  The dimension of the objects observed could increase, Multivariate time series datasets have appeared in both prac-tical industry domains (for example, telecommunication and network) and scientific res earch fields (neural imaging and pattern recognition). In a later section, we include a case study in human motion pattern recognition.

Recent research has proposed many approaches for dealing with the considerable lengths and large num-ber of objects now appearing in (univariate) time series datasets. Some popular methods include applying Fourier and wavelet transformations, as well as statistical parame-ter extraction for models such as the AutoRegressive Mov-ing Average. Typically, the transformed series or extracted parameters are clustered using conventional clustering algo-rithms such as k -means clustering [44]. However, there is evidence that some of these methods are inappropriate for massive multivariate time series. They are either undefined and very expensive to compute on high-dimensional data, or restricted to data that satisfies strong linearity assumptions. The challenge of clustering large datasets of multivariate time series remains. Our intention was to produce a sim-ple , flexible and accurate method for clustering multivariate time series.

In this paper, we propose a new method based on extract-ing structure-based statistical features for clustering multi-variate time series. We wish to extract the most informative features or characteristics to rep resent the multivariate time series in our datasets. Such metrics, extracted from univari-ate time series structure, are used to construct new vectors for fast clustering algorithms, like k -means.

The remainder of the paper is organized as follows. Sec-tion 2 presents the related work aligned with our research focus. Section 3 explains our proposed method on cluster-ing with new vectors from feature extraction on multivariate time series data set. Section 4 describes the details of the statistical metrics extracted. The case study on human mo-tion recognition with experimental results are demonstrated in Section 5.
Clustering time series and other sequences of data has become an important topic, motivated by several research challenges including similarity search of medical and astro-nomical sequences, as well as the challenge of developing methods to recognize dynamic changes in time series [40]. However, most of the literature deals with methods and ap-plications on univariate time series data: only a few appli-cations have been reported on clustering multivariate time series data. There are three main tracks in current multi-variate time series clustering.
 Principal Component Analysis (PCA) PCA has been most commonly used in the limited number of applications on clustering multivariate time series data [43, 46]. Mul-tivariate time series data have been clustered according to features found using PCA as the dimension-reduction tool for the feature space. Huang used PCA to split large multi-variate time series clusters into smaller clusters [23]. In gen-eral, the number of principal components should be known as a predetermined parameter, which may be difficult to select. In very recent research, Singhai and Seborg [42] demonstrated clustering multivariate time series by com-bining two similarity factors: one is based on a PCA of the series, the other one is based on Mahalanobis distance between datasets. The final step is the application of the k -means algorithm to cluster multivariate time series based on the similarity factors calculated.
 Hidden Markov Models (HMMs) HMMs have been used to cluster multivariate time series [35] based upon their ability to capture both the dependencies between variables and the serial correlations in the measurements [37]. An assumed probability distribution is required for the HMM representation for multivariate time series data.
 Unfolding Data Wang and McGreavy [49] proposed un-folding each multivariate time series into a long row vector and supplying it to the Autoclass algorithm [7]. When the length of the multivariate time series increases or varies, this method could become computationally infeasible.
Therefore, from the above brief discussion on three re-lated works, we find some drawbacks of these approaches, for instance, lack of flexibility, running time overheads,and computational restrictions. However, these approaches have paved a direction for our research in seeking a more flexible, simple and less complex means to deal with multi-variate time series data for cl ustering or classification.
A number of authors have clustered time series based on structure-based similarity measures. Nanopoulos extracted four basic statistical features from Control Chart Pattern data and used them as input in a multi-layer perception neu-ral network for time series classification [34]. Their experi-mental results showed the robustness of the method against noise and time series length compared to other methods that used every data point. By using two popular feature extraction techniques, the Discrete Wavelet Transform and the Discrete Fourier Transform, M  X  orchen has demonstrated the advantages of feature extraction for time series clus-tering in terms of computational efficiency and clustering quality on a benchmark dataset [33]. In a classification setting, parameters of a AutoRegression Moving Average (ARMA) model can be estimated and used as a limited di-mensional representation for the original time series [11]. However, using ARMA parameters is not a reliable method because different sets of par ameters can be obtained from time series with similar structure that could affect the clus-tering results dramatically. Ge and Smyth [13] used an ap-proach for time series pattern matching based on segmental semi-Markov models: this proved to be flexible and accu-rate on real datasets. The time series is modeled as k dis-tinct segments X  X ith constraints on how the segments are linked  X  X efore the authors apply a Viterbi-like algorithm to compute the similarity measures. Compression-based Dis-similarity Measures are proposed by Keogh and others to compare long time series structure using co-compressibility as a dissimilarity measure [27]. This measure can be di-rectly used in data mining algorithms, such as hierarchi-cal clustering. Their extensive experiments have demon-strated the ability in handling different-length and missing-value time series. While the above works have shown util-ity in certain domains, most of them have high computa-tional complexity and require that the data satisfy a number of conditions.
We start with some notation. Let Y = { Y 1 ,Y 2 ,...,Y Q } represent a collection of Q multivariate time series. The series Y i consists of n observations of a d -dimensional vari-able and will often be written as indicating dnQ observations in total. Step I: Treat the j -th component of the i -th time series, that is Y ij = { Y ij 1 ,...,Y ijt } , as a univariate time series. Then, for each Y ij , produce a finite vector of L metrics M =( m 1 ,m 2 ,...,m L ) where each m  X  is some statisti-cal feature extracted from the time series. As such, each time series Y ij is transformed into a new vector, M ij . Step II: The number of features (or metrics) that are ac-tually used, L , can be based on a more generalized study of univariate time series structure -based characteristics. If the dataset comes from a particular domain with certain back-ground knowledge, some sort of learning procedure such a feed-forward algorithm can select either a subset of the fea-tures or a convex combination of them.
 Step III: Each multivariate time series therefore has dM -vectors: concatenating these into a single vector produces a simple dL -dimensional sketch of the Y i . The data is now ready for clustering. k -means clustering [31] is one of the simplest unsuper-vised learning clustering algorithms and is widely used for classification and clustering problems. Hierarchical clus-tering algorithms [25] also have a long and successful his-tory. There are three major variants of hierarchical cluster-ing: single link, complete link, and minimum variance. Of these three, the single-link and complete-link algorithms are most popular. In time series clustering research, k -means and hierarchical clustering have been commonly used with many measures of distance between series. However, there are obvious drawbacks or limitations for all of these cluster-ing algorithms in handling time series data. Either they re-quire the number of clusters, k , to be predefined as a param-eter, or they require the time series length to be identical due to the distance calculation, or they are unable to deal effec-tively with lengthy time series due to poor scalability when some common used distance measure (for instance, Eu-clidean distance) is used in the clustering algorithm. How-ever, when multivariate time series are transformed into rep-resentative vectors with our proposed structure-based sta-tistical feature extraction, the latter drawbacks are not so apparent. Comparing these two basic algorithms, k -means is faster than hierarchical clustering [5], but the number of clusters has to be pre-assigned, which could be impracti-cal in obtaining natural clustering results. In this paper, we test our method on a dataset which the number of clusters is known from prior classification work on the (training) data. Therefore, both clustering algorithms are adopted in the case study (details in Section 5) to demonstrate the ro-bustness and reliability of the features extracted for cluster-ing.
In this study, we investigated various data characteristics from diverse perspectives related to univariate time series structure-based characteris tic identification and feature ex-traction. We selected the nine most informative, representa-tive and easily-measurable char acteristics to summarize the time series structure. Based on these identified characteris-tics, corresponding metrics are calculated for constructing the structure-based feature vectors.
A univariate time series can be represented as an ordered set of n real-valued variables Y 1 , ..., Y n . Time series can be described using a variety of adjectives such as seasonal, trending, noisy, non-linear, chaotic, etc.

Three common data characterization methods are: (i) statistical and information-th eoretic characterization, (ii) model-based characterization, and (iii) landmarking con-cepts [36] . We take the path of statistical feature extraction in this study. The extracted statistical features should carry summarized information of time series data, capturing the global picture based on the structure of the entire time se-ries. After a thorough literature review, we propose a novel set of characteristic metrics to represent univariate time se-ries and their structure-based features. This set of met-rics not only includes conventional features (for example, trend) [1], but also cover many advanced features (for ex-ample, chaos) which are derived from research on new phe-nomena [26]. The corresponding metrics for the follow-ing structure-based statistical features form a rich portrait of the nature of a time series: Trend, Seasonality, Serial Cor-relation, Non-linearity, Skewness, Kurtosis, Self-similarity, Chaotic, and Periodicity. We now explain these in detail. 4.1.1 Trend and Seasonality Trend and seasonality are common features of time series, and it is natural to character ize a time series by its degree of trend and seasonality. In addition, once the trend and seasonality of a time series has been measured, we can de-trend and de-seasonalize the time series to enable additional features such as noise or chaos to be more easily detectable. A trend pattern exists when there is a long-term change in the (local) mean value [32]. To estimate the trend, we can use a smooth nonparametric method, such as the penalized regression spline [53].

Seasonal factors, such as month of the year or day of the week can often affect time series data. The seasonality of a time series is defined as the presence of a pattern that re-peats itself over fixed interval of time [32]. In general, the seasonality can be found by identifying a large autocorrela-tion coefficient or a large partial autocorrelation coefficient at the seasonal lag.

There are three main reasons for making a transforma-tion after plotting the data: i) to stabilize the variance, ii) to make the seasonal effect additive, and iii) to make the data normally distributed [6]. The two most popularly used transformations, logarithms and square-roots, are special cases of the class of Box-Cox transformations [3], these are used to make the data appear normally distributed. Given a time series, Y t , and a transformation parameter,  X  ,thetrans-formed series is defined thus:
Y t =( Y This transformation applies to situations in which the de-pendent variable is known to positive. We have used the basic decomposition model in Chapter 3 of Makridakis X  X  text [32]: where Y  X  t denotes the series after Box-Cox transformation. At time t , T t denotes the trend, S t denotes the seasonal component, and E t is the irregular (or remainder) compo-nent. For a given transformation parameter,  X  , if the data are seasonal X  X hat is, a frequency of periodicity parameter, generated from the data, is greater than 1 X  X he decomposi-tion is carried out using a Seasonal-Trend decomposition procedure based on the Loess (STL) procedure [8]. This is a filtering procedure for decomposing a time series into trend, seasonal, and remainder components, assuming fixed seasonality. The amount of smoothing for the trend is taken to be the default in the R implementation of the stl function. Otherwise, if the data is nonseasonal, the S t term is set to 0 , and the estimation of T t is carried out using a penalized regression spline [53] with the smoothing parameter chosen using cross validation. The transfor-mation parameter  X  is chosen to make the residuals from the decomposition as normal as possible in distribution. We choose  X   X  (  X  1 , 1) to minimize the Sahpiro-Wilk statistic [39]. We only consider a transformation if the minimum of Y t is non-negative. If the minimum of Y t is 0, we add a small positive constant (equal to 1 / 1000 of the maximum of Y t ) to all values to avoid undefined results. In summary:
Y t original data X t = Y  X  t  X  T t de-trended data after Z t = Y  X  t  X  S t de-seasonalized data after
Y t = Y 1  X  Var( Y t ) / Var( Z t ) a suitable measure of trend 1  X  Var( Y t ) / Var( X t ) a suitable measure of seasonality 4.1.2 Periodicity Since the periodicity is very important for determining the seasonality and examining the cyclic pattern of the time series, periodicity feature extraction is essential. Unfortunately, time series from some domains do not come with known frequencies or regular periodicities. Therefore, we propose a new algorithm to measure the periodicity in univariate time series. A time series is called cyclic if there is some fixed period after which a pattern repeats itself. Seasonal time series are a subset of cyclic time series in which the cycle time must belong to a special family such as one day, one week, one month or one year. For time series with no seasonal pattern, the frequency is set to 1. We measure the periodicity using following algorithm:
Algorithm : Periodicity measure extraction 1. Detrend time series using a regression spline with 3 2. Find r k = corr ( Y t ,Y t  X  k ) (auto-correlation function) 3. Frequency is the first peak satisfying the following 4. If no such peak is found, frequency is set to 1 (equiva-4.1.3 Serial Correlation We use Box-Pierce statistics in our study to estimate the serial correlation measure, and to extract measures from both raw and TSA ( Trend and Seasonally Adjusted ) data. The Box-Pierce statistic [32] was introduced in 1970 to test residuals from a forecast model [4]. It is a common port-manteau test for computing the measure. The Box-Pierce statistic is where n is the length of the time series, and h is the maxi-mum lag being considered, usually 20 . 4.1.4 Non-linear Autoregressive Structure Non-linear time series models have been used extensively in recent years to model dynamics not adequately repre-sented by linear models [19]. For example, the well-known sunspot data set [9] and lynx data set [16] have non-linear structure. In times of recession, many economic time series appear non-linear [14].

There are many approaches to test the non-linearity in time series models such as nonparametric kernel and neu-ral network tests. The Neural Network test has been re-ported to have better reliability [29]. In our study, we used Terasvirta X  X  neural network test [45] for measuring time se-ries data nonlinearity, which can correctly model the nonlin-ear structure of time series data [28]. It is a test for neglected nonlinearity, likely to have power against a range of alter-native based on the neural network model. The test is based on a function chosen as the activations of phantom hidden units. Refer to Terasvirta [45] for a detailed discussion on the testing procedures and formulas. 4.1.5 Skewness Skewness is a measure of symmetry, or more precisely, the lack of symmetry in a distribution, or a data set. For uni-variate time series, Y t , the skewness coefficient is where Y is the mean,  X  is the standard deviation, and n is the number of data points in the series. The skewness for a normal distribution is zero, and any symmetric data should have skewness near zero. Negative values for the skewness indicate that the data distribution is skewed left, and positive values indicate a right-skewed distribution. 4.1.6 Kurtosis Kurtosis is a measure of whether the data are peaked or flat, relative to a normal distribution. A data set with high kur-tosis tends to have a distinct peak near the mean, declines rather rapidly, and has heavy tails. A data set with low kur-tosis tends to have a flat top near the mean rather than a sharp peak. For a univariate time series, Y t , the kurtosis coefficient is where Y is the mean,  X  is the standard deviation, and n is the number of data points in the series. The kurtosis for the standard Normal distribution is 3. Therefore, the excess kurtosis is defined as Positive excess kurtosis indicates a peaked distribution and negative excess kurtosis indicates a flat distribution. 4.1.7 Self-similarity Processes with long-range dependence have attracted a good deal of attention from probabilist and theoretical physicists. In 1984, Cox [10] first presented a review of second-order statistical time series analysis. The subject of self-similarity (or long-range dependence ) and the estima-tion of statistical parameters of time series in the presence of long-range dependence are becoming more common in several fields of science [38]. Given this, we decided to in-clude this feature into our feat ure selection set; so far this has been paid little attention in time series feature identifi-cation.

The definition of self-similarity most related to the prop-erties of time series is the self-similarity, Hurst exponent ( H ) parameter [51]. The class of AutoRegressive Frac-tionally Integrated Moving Average (ARFIMA) processes has been recommended as a suitable estimation method for computing H [22]. We fit a ARFIMA(0,d,0) to maximum likelihood which is approximated by using the fast and ac-curate Haslett and Raftery method [20]. The Hurst param-eter is estimated using the relation as H = d +0 . 5 and this self-similarity feature can only be detected on the raw data of the time series. 4.1.8 Chaos Many systems in nature that were previously considered as random processes are now categorized as chaotic systems. Nonlinear dynamic systems often exhibit chaos, which is characterized by sensitive dependence on initial values, or more precisely by a positive Lyapunov Exponent ( LE ). The LE is a measure of the divergence of nearby trajectories which can be used to qualify the notion of chaos. Recognizing and quantifying chaos in time series are important steps toward understanding the nature of random behavior, and reveal the dynamic feature of time series [30]. The first algorithm for computing the LE of a time series was proposed by Wolf [52]. It applies to continuous dynamic systems in a n -dimensional phase space. For a one-dimensional discrete time series, we used the method demonstrated by Hilborn [21] to calculate LE from the raw time series data.

Algorithm : Lyapunov Exponent measure extraction for univariate time series  X  Let Y t denote the univariate time series  X  Consider the rate of divergence of nearby points in the  X  Suppose Y i and Y j are two points in Y t , such that  X  Them, average these values over all i values, where N  X  Estimate LE of the series as 4.1.9 Decomposition and Scaling Transformation In time series analysis, decomposition is a critical step for transforming the series into a format for statistical measure-ment [15]. Therefore, to obtain a precise and comprehen-sive calibration, some measures need to be calculated on both the raw time series data, Y t , (referred to as raw data), as well as the remaining time series, Y t ,  X  X rend and Sea-sonally Adjusted X  ( TSA ) data. Note that some features such as periodicity can only be calculated on raw data.
For the nine selected features, four of them are calibrated on both raw and TSA data. Serial-correlation, Non-linearity, Skewness, and Kurtosis each contribute two metrics to our family. The remaining five features are calibrated only on raw data, leading to a total of thirteen.

The ranges of each the metrics extracted can vary sig-nificantly without the scaling transformation process. Each of the metrics is ultimately normalized to have a range of [0 , 1] . A measurement near 0 for a certain time series in-dicates an absence of the particular feature, while a mea-surement near 1 indicates a strong presence of the feature identified. The data scaling transformation is required for clustering process in order to avoid certain measures domi-nating the clustering due to the data range itself.
There are many data transformation methods aviaible to normalize the metrics onto a certain required span, for in-stance, [0 , 1] in our work. We perform a statistical trans-formation of the data because it is convenient to normalize variable ranges across a new span from orignial data, while preserving underlying statistical properties. Compared to simple min-max transformation method (a linear transfor-mation method), the statistical method also has a better con-trol over the data distribution to obtain a reliable outcome, because if there are outliers in the original data, they can dominate the transformation results. Three transforma-tions f 1 , f 2 and f 3 are used to rescale a raw measure, Q , of various ranges to a new value q in the [0 , 1] range. (See detailed parameter settings and analysis in [50]). f 1 : when Q  X  [0 ,  X  ) ,q = f 2 : when Q  X  [0 , 1] ,q = f 3 : when Q  X  (1 ,  X  ) ,q = 4.1.10 Computational Consideration The computational time for calculating all statistical met-rics is in general very low: most have linear running times. A few that revolve around long-range dependence have quadratic-time requirements, though we omit details in this paper.
We demonstrate our method by applying it to a real-world application on human activities and motion recogni-tion.
Various cues have been used in human motion recogni-tion. These include key poses, optical flow, local descrip-tors, trajectories and joint angles from tracking. The fea-tures should be simple, intuitive and reliable to extract with-out manual labour. Human activities can be regarded as temporal variations of human silhouettes. Silhouette extrac-tion from video is relatively easy for current imperfect vi-sion techniques. So the method that we present here prefers to use (probably imperfect) space-time silhouettes for hu-man activity representation. The silhouette images are cen-tered and normalized on the basis of preserving the aspect ratio of the silhouette so that the resulting images contain as much foreground as possible, do not distort the motion shape, and are of equal dimension for all input frames. To obtain a compact description and efficient computation, we use the Kernel Principal Component Analysis algorithm (KPCA) [41] to perform nonlinear dimension reduction. After obtaining the embedding space including the first d principal components, any one video can be projected into an associated trajectory in d -dimensional feature space as shown in Figure 1.
Figure 1. Transformation from silhouette data to multi-dimensional time series sequences (only 3 dimensions are shown)
The aim is to classify the observed silhouette sequences into known actions. To date, nearest-neighbour classifica-tion has been commonplace, rather than clustering algo-rithms.
There is no common evaluation database in the domain of human activity recognition, so we use a recently collated database [47]. The dataset consists of 10 different activ-ities performed by one person: pick up an object, jog in place, push, squash, wave, kick, bend to the side, throw, turn around, and talk on a cell phone. There are 10 different instances collected for each act ivity, hence 100 sequences in total. Different instances of the same activity may represent different rates of motion execution. This dataset is thus used to systematically examine the effect of the rate on activity recognition both between different activities and between different instances of the same person carrying out the same activity. Each activity has been represented in a multivari-ate time series format, with 25 indexed sequences recorded with 70 time intervals after the KPCA pre-processing pro-cedure. An example of using multivariate time series to represent the activity pick up an object is demonstrated in Figure 2.
Figure 2. The activity pick up an object repre-sented in multivariate time series format
As discussed in Section 4, a finite set of structure-based features are identified and their corresponding metrics are then calculated using the most suitable algorithms. Thirteen measures are summarized for each single univariate time se-ries in the data set. Then, 100 new vectors are built for the 100 multivariate time series data sets provided for experi-ments. Before the measures for univariate time series data are combined and transformed into new vectors for multi-variate time series data, a feature subset search is employed on a subset of the large data set. We decided to train on half the data (for feature selection) ; the training examples were equally spread between all of the activities.

In practice, we need to consider the generality of the method and the selection of the features for particular data sets or in certain application domains. In this section, a greedy forward search (FS) algorithm is employed as a searching mechanism to optim ize the feature set (see Sec-tion 3.1). Forward search is a powerful general method for detecting multiple influences in a model [2]. It is only op-timal for models which that have independent observations, such as linear and non linear regression, generalized linear models and multivariate analysis. However even in cases where the operators are not independent, it has been shown to be very robust in practice [14].

As illustrated in Table 1, classification accuracy reaches its peak after adding the top 10 metrics based on FS using the training dataset with known class labels. Then, a subset of these 10 metrics are selected to form the new vector be-fore feeding them into clust ering algorithms. Because the classification accuracy using a ll the metrics did provide a high level of accuracy and the difference between this and of top 10 features is not great, we decided to take both sets of metrics for new vector construction in our experiments.
Table 1. Classification accuracy for feature selection (* indicates the metrics extracted based on the TSA data). Each feature is added to the family of features above it.
 In k -means clustering, we used two methods: one by Hartigan and Wong [18], the other (the most commonly-used method) given by MacQueen [31]. Background knowledge of the dataset in our experiments suggested that k =10 be assumed by the algorithm. Since the initial start of the cluster centers can affect the clustering result, we em-ployed multiple runs with different random restarts for the clustering process in the experiments in order to achieve a more reliable outcome.

In hierarchical clustering, we choose three different clus-tering methods in the experiments. Ward X  X  minimum vari-ance method aims at finding co mpact, spherical clusters. The complete linkage method finds similar clusters. The Single Linkage (SL) method (which is closely related to minimal spanning trees) adopts a friends of friends clus-tering strategy [17]. The clustering process is performed on both new vectors based on 13 features and new vec-tors based on a selected subset of 10 features. Since vi-sualization and dendrogram plots are not the main focus in this paper, we first used the two sets of derived vectors as clustering inputs to obtain the natural clustering from three methods under hierarchical clustering algorithm. Then, for evaluation, given the expected ten clusters as the best solu-tion, the dendrogram trees obtained are cut arbitrarily into ten clusters and reconstruct the upper part of the tree from the cluster centers.

The quality of each clustering algorithm is measured by cluster purity, CP , which is a percentage count: CP = (Count D / Count E )  X  100% ,where Count D is the number of elements of the dominant class within that cluster, and
Table 2. Hierarchical clustering accuracy (% CP ) using all metrics (without feature se-lection) Count E is the expected number of objects in that cluster.
Hierarchical clustering results are shown in Tables 2 and 4, and k -means clustering algorithms results are shown in Tables 3 and 5. If we focus on the average perfor-mance of a particular algorithm over all clusters, Hierarchi-cal and k -means perform similarly. The results demonstrate that our features are not model-or algorithm-dependent, which shows their generality and adaptability. We note also that the response to feature selection is not uniform. The Ward and SL variants of hierachical clustering are im-proved, whereas the worse-performing Complete-link ap-proach becomes even worse.

Clearly jog in place is the best-recognized activity, whereas wave is the worst-recognized activity. It appears that the extracted structure-based features tend to be inad-equate for describing the wave data set, which caused poor clustering results. On the other hand, our proposed fea-tures are suitable for summa rizing activities like jogging, which was clustered with 100% accuracy. This study of hu-man motion has provided us with an understanding of the strengths and weaknesses of our approach, both of which we hope to address in future.
In this paper, we proposed a new method to convert multivariate time series into vectors consisting of statistical measures extracted based on univariate time series structure or global characteristics. The proposed method is applied on a real-world dataset of human activity pattern recogni-tion. The empirical results demonstrated that our method is able to cluster multivariate time series data with high accu-racy efficiently.
Table 3. k -means clustering accuracy (% CP ) using all metrics (without feature selection)
Table 4. Hierarchical clustering accuracy (% CP ) using a subset of metrics (with feature selection)
Table 5. k -means clustering accuracy (% CP ) using a subset of metrics (with feature selec-tion)
