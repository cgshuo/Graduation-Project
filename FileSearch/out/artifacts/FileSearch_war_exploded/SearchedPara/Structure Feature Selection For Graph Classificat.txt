 With the development of highly efficient graph data col-lection technology in many application fields, classification of graph data emerges as an important topic in the data mining and machine learning community. Towards building highly accurate classification models for graph data, here we present an efficient graph feature selection method. In our method, we use frequent subgraphs as features for graph classification. Different from existing methods, we consider the spatial distribution of the subgraph features in the graph data and select those ones that have consistent spatial loca-tion.

We have applied our feature selection methods to several cheminformatics benchmarks. Our method demonstrates a significant improvement of prediction as compared to the state-of-the-art methods.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation Data Mining, Classification, Feature Selection
Knowledge discovery in complex data is an essential prob-lem in data mining. Graph, a discrete structure, has been widely applied in diverse areas such as Cheminformatics, Bioinformatics, social network analysis, and database man-agement as a tool to model a wide range of data. A major difficulty in mining graph data lies in the high complexity, which is from graph X  X  complex structure and the intrinsic high dimensionality of graphs. The objective of this paper is to derive an automated way to construct a low-dimensional vector representation for graphs through developing a highly effective feature selection methods. Finding a proper vector representation of graphs may lead to more accurate models, reduced computational time, and better explanations of the real relationship between graphs and graph labels in classi-fication, and hence worth a careful investigation.
Current solutions for feature selection problems can be roughly divided into two categories: feature extraction and feature selection [1]. Principle Component Analysis (PCA) projects data to an eigenvector space to reduce the dimen-sionality and hence to obtain a small number of features [13, 19]. Similar methods for feature extraction include Linear Discriminative Analysis LDA [29], Local Linear Embedding LLE [21] and Isomap [25]. Using Kernel PCA, investigators have designed algorithms to embed a graph to a vector space and achieved good empirical results in classification [22, 26].
Traditional feature filtering methods select individual fea-tures whose distribution correlates the distribution of the class labels. Such methods include term frequency thresh-olding, mutual information, information gain,  X  2 , and Pear-son Correlation as studied in [28]. In contrast to filtering method, which do not consider the dependency between fea-tures and may select redundant features, wrapper methods search through the feature subset space and identify highly informative features by using a classifier to score the subsets of features [17, 18].

Adapting existing feature extraction and feature selection methods to graph data is non trivial. First, graph are semi-structured data. There is no obvious choices of features in graphs to start feature selection methods. Second, graph kernel functions map graphs to a Hilbert space implicitly and thus avoid the problem of direct feature extraction. Though theoretic appealing, limited progresses have been made in reality in applying graph kernel functions to extract useful features in graphs. This is due to several reasons: (i) design a graph kernel function is not easy, (ii) the connection of kernel space and the original graph space is not clear and (iii) it is hard to explain the physical meaning of the identified features using kernel PCA techniques.

For special cases, investigators have identified that adapt-ing existing feature selection methods to graph data is triv-ial. For example, frequent subgraph mining is a technique that extracts high frequent subgraphs in a group of graphs. Once such graphs are created, we may treat each subgraph as a feature and transform a graph to a vector, indexed by the identified subgraphs, in the related feature space. Any existing feature selection methods for Euclidian space are Figure 1: Spatial distribution of three frequent sub-graphs in a graph database applicable to select highly informative features in the sub-graph feature space.

We argue that neither a simple postprocessing nor a direct adaption of the kernel PCA method identifies the intrinsic structure features of graph data completely. Our intuition is that the spatial distribution of subgraph features carries important information regarding the importance of the fea-ture in a classification task. We illustrate the point with the following example: In Figure 1, we show a graph database with three graphs G , G 2 , G 3 and three subgraph features F 1 , F 2 , F 3 . Sub-graph F 1 and F 2 occur in every graph with a consistent relative spatial distribution. F 3 also occurs in every graph, but In contrast to F 1 and F 2 , has quite different spatial distribution as compared to F 1 and F 2 . In regular feature selection, F 1 , F 2 , and F 3 occur in the same set of graphs and hence may be perceived to have the same classification power. Clearly this is not the case in this example. Based on intuition, we have designed an integrated approach of two existing approaches: graph kernels and subgraph mining by designing a feature selection method working on graph spec-trum kernels to gain deeper understanding of graph data. Our comprehensive experimental study of the designed al-gorithms using real-world data sets demonstrated the power of the novel feature selection method.
Given a set of graphs G , each graph in G has an associated labels c , and a set of subgraphs F extracted from G , the graph feature selection problem is to select a subset of features F s  X  F , to obtain better classification accuracy than using F only for the graph data set.
Extracting features, in the form of subgraphs, from graph data has been well studied in graph database mining meth-ods. The goals of such methods extract highly informa-tive subgraphs from a set of graphs. Typically some fil-tering criteria are applied, among those the most widely used is the frequency of a subgraph. For example, Huan et al. develop a depth-first search algorithm: Fast Frequent Subgraph Mining (FFSM) [11]. This algorithm identified all connected subgraphs that occurs in a large fraction of graphs in a graph data set. Additional filtering criteria are also developed, such as coherence [12], closeness [27], max-imal size [12], density [9] among many others. Majority of the frequent subgraph feature extraction methods are unsu-pervised, meaning that there is no class labels information available (or such information are deliberately ignored) with a few exceptions. For example, an odd ratio is used to se-lect subgraphs that is highly informative to build classifier in [10].

Many existing feature selection methods for vector space are supervised, determining the relevance of a feature through computing the correlation of feature value distribution and class label distribution. Traditional feature filtering meth-ods select features independent of any classifier. In contrast to filtering method, which do not consider the dependency between features and may select redundant features, wrap-per methods search through the subset space and identify highly informative features by using a classifier to evaluate the classification power of subsets of features and identify optimal subsets [17].

Kernel methods are now widely used in supervised learn-ing and feature selection as well. For example, in the method of Support Vector Machine Recursive Feature Elimination (SVM RFE) [7], SVM RFE selects features via a greedy backward feature elimination. SVM RFE first builds a lin-ear classifier, then uses the weight vector of the hyperplane constructed by the training samples to rank features. During each iteration, lower ranked features are removed and new hyperplane is constructed and so on so forth. The limitation of SVM RFE is that it works only with linear kernel.
Spectral feature selection [30], as a filtering method, ex-plored an uniformed frame for feature selection in both unsu-pervised and supervised learning. It first constructed an ob-ject graph, where each node is corresponding to an object of training data; then ranked features using graph spectral de-composition and selected a subset of features based on their rank. Since spectral feature selection is a filtering methods, the feature dependency information is ignored. Cao et al. re-cently developed a method for feature selection in the kernel space rather than the original feature space based on Max-imum Margin concept. Without tracing back into original feature space, they could select features in Kernel space.
Maximum Margin Feature selection (MMRFS) [3] is a wrapper method. In this method MMRFS uses informa-tion gain to weigh the correlation between each feature and class labels, then selectes features with less redundancy and covering new training samples.

Though feature selection have been developed for a long time, none of the existing method considers the special char-acteristics of graph data and hence may not provide the opti-mal results for graph feature selection. The objective of this paper is to develop a highly effective graph feature selection methods.
Here we introduce basic notations for graph, frequent sub-graph mining and graph kernel functions.
A labeled graph G is described by a finite set of nodes V and a finite set of edges E  X  V  X  V . In most applications, a graph is labeled, where labels are drawn from a label set  X  . A labeling function  X  : V  X  E  X   X  assigns labels to nodes and edges. In node-labeled graphs , labels are assigned to nodes only and in edge-labeled graphs , labels are assigned to edges only. In fully-labeled graphs , labels are assigned to nodes and edges. We may use a special symbol to represent missing labels. If we do that, node-labeled graphs, edge-labeled graphs, and graphs without labels are special cases of fully-labeled graphs. Without loss of generality, we handle fully-labeled graphs only in this paper. We do not assume any structure of label set  X  now; it may be a field, a vector space, or simply a set.
 Following convention, we denote a graph as a quadruple G = ( V, E,  X  ,  X  ) where V, E,  X  ,  X  are explained before. A graph G = ( V, E,  X  ,  X  ) is a subgraph of another graph G ( V 0 , E 0 ,  X  0 ,  X  0 ) , denoted by G  X  G 0 , if there exists a 1-1 mapping f : V  X  V 0 such that
In other words, a graph is a subgraph of another graph if there exits a 1-1 node mapping such that preserve the node labels, edge relations, and edge labels.
 The 1-1 mapping f is a subgraph isomorphism from G to G 0 and the range of the mapping f , f ( V ) , is an embedding of G in G 0 .
Given a graph database G , the support of a subgraph G , denoted by sup G , is the fraction of the graphs in G of which G is a subgraph, or:
Given a user specified minimum support threshold min sup and graph database G , a frequent subgraph is a subgraph whose support is at least min sup (i.e. sup G  X  min sup ) and the frequent subgraph mining problem is to find all fre-quent subgraphs in G .

In this paper, we use frequent subgraph mining to extract features in a set of graphs. Each mined subgraph is a feature. Each graph is transformed to a feature vector indexed by the extracted features with values indicate the presence or absence of the feature as did in [11]. We use binary feature vector as contrast to occurrence feature vector (where the value of a feature indicates the number of occurrences of the feature in an object) due to its simplicity. Empirical study shows that there is negligible difference between the two representations in graph classification.
Kernel functions are powerful computational tools to an-alyze large volumes of graph data [8]. The advantage of kernel functions is due to their capability to map a set of data to a high dimensional Hilbert space without explic-itly compute the coordinates of the structure. This is done through a special function K . Specifically a binary function K : X  X  X  X  R is a positive semi-definite function if for any m  X  N , any selection of samples x i  X  X ( i = dition, a binary function is symmetric if K ( x, y ) = K ( y, x ) for all x, y  X  X . A symmetric, positive semi-definite func-tion ensures the existence of a Hilbert space H and a map  X  : X  X  X  such that for all x, x 0  X  X .  X  x, y  X  denotes an inner product between two objects x and y . The result is known as the Mercer X  X  theorem and a symmetric, positive semi-definite function is also known as a Mercer kernel function [22], or kernel function for simplicity.

Several graph kernel functions have been studied. Re-cent progresses of graph kernel functions could be roughly divided into two categories. The first group of kernel func-tions consider the full adjacency matrix of graphs and hence measure the global similarity of two graphs. These include product graph kernels [6], random walk based kernels [15], and kernels based on shortest paths between pair of nodes [16]. The second group of kernel functions try to capture the local similarity of two graphs by counting the shared sub-components of graphs. These include the subtree kernels [20], cyclic kernels [24], spectrum kernel [5], and recently frequent subgraph kernels [23]. In this paper, we focus on graph spectrum kernels where we use subgraph as features. The feature vector of a graph is the list of subgraph fea-tures that occurs in the graph and the kernel function of two graphs is the inner product of their feature vectors [5].
Our structure based feature selection method has two steps: (1) feature extraction and (2) feature selection. In the feature extraction step, we mine frequent subgraphs in the training samples as features. We then apply a feature se-lection method, as discussed below, to select a small subset of features to build graph classification model.

Our hypothesis in feature selection is that the spatial dis-tribution of subgraph features carries important informa-tion regarding the importance of the feature. To test our hypothesis, we have designed a feature selection method and have performed comprehensive experimental study us-ing real-world data sets. Below we first present a feature selection framework. We then introduce the feature consis-tency map, which when combined with our feature selection framework, leads to a practical algorithm to select highly informative features.
In this paper, we use capital letters, such as G , for a sin-gle graph and upper case calligraphic letters, such as G = G , G 2 , . . . , G n , for a set of n graphs. We assume each graph G i  X  G has an associated class label c i from a label set C . We use F = F 1 , F 2 , . . . , F n for a set of n features. Given a set of n features F and a graph G , we create a feature vector for G , denoted by G F , indexed by features in F and with values indicate whether the related feature is present (1) or absent (0) in the graph G . In other words, G F = ( G F i ) and
In this work we consider selecting features whose distri-bution is consistent with the distribution of the class labels. Towards that end, we compute the object kernel matrix  X  as defined below. where ( X == Y ) = 1 if X = Y and otherwise 0. c i is the class label of the i th object.

Desired features are those that are  X  X onsistent X  to the ob-ject kernel matrix. The technical challenge here is how to measure the consistency. Recently Zhao &amp; Liu proposed a method called spectral feature selection [30]. In the spectral feature selection method, the normalized Laplacian matrix L is computed as: Where D is the density matrix of  X  and is defined as D = ( d
Given a feature F = ( f i ) n i =1 where f i is the value of the feature in the i th object, the value &lt; F T , LF T &gt; measures the consistency of the feature F to the object kernel matrix. The smaller the value is, the most consistent the feature vec-tor F is. Based on this measurement, Zhao &amp; Liu proposed a filtering method by selecting the top k features that best consistent with the object kernel matrix  X  .
 Different from their method, we adopt the Kernel-Target Alignment framework, originally proposed for automated kernel selection [4]. The advantage of the new framework is that (i) the new framework handles the dependency of features and hence may select better feature subset and (ii) the new framework allows us to use feature kernel and hence may handel non-linear correlation between a feature and class labels. The kernel-target Alignment method is dis-cussed below.

Given a feature F , we define a feature kernel matrix S F as
In the formula, K can be any kernel function. For simplic-ity in our experimental study we use linear kernel K ( X, Y ) = X  X  Y .

With the feature kernel matrix and object kernel matrix, we use the following formula to measure whether the feature kernel is  X  X onsistent X  with the object kernel. Toward that end, we introduce a binary function  X  : M  X  M  X  R to compute the inner product of two matrices as where M is the set of all n by m matrices.

Clearly the  X  function is a bilinear form, which means that it has the following properties:
In addition,  X  is positive ( X  X  X = 0  X  X = 0 ) and sym-metric ( X  X  Y = Y  X  X ) and hence a inner product function.
Based on the function  X  , we define the similarity between two matrices is the inner product of the two matrices X and Y , normalized by the norm of the X and Y , or where || X || = 2
Geometrically the similarity function S measures the co-sine value of the angle between two kernel matrices. This measurement is first used in [4] to automatically select kernel functions. We adapt it here for the purpose of feature selec-tion. Before we proceed to our feature selection method, we present an important data structure, which we call feature consistency map.
Different from the kernel target alignment framework, whose goal is to measure the consistency between a feature and a class label. Here we develop a way to measure the consis-tency in a set of features without the class label information. Such measurement is clearly unsupervised, meaning that we do not take class label distribution into consideration. Our main source of information is the spatial distribution of sub-graph features. To quantify the information, we propose a data structure called feature consistency map, which is built upon the following three concepts: embedding distance, em-bedding profiles, feature consistency, as detailed below.
Definition 3.1. ( Embedding Distance ) Given two fre-quent subgraph features F i and F j and their embeddings of e e j in a graph G , the embedding distance , denoted by d ( e i , e j ) is defined as: Where d ( u, v ) is the shortest distance between the node u and the node v .

Example 3.1. In Figure 2, subgraph feature F 1 has an embedding { a, b } in G 1 . F 2 has an embedding { c, d } in G Hence, the embedding distance between pattern F 1 and F 2 It is possible that one frequent pattern has several embed-dings in a graph. We illustrate the procedure we use to compute embedding distance with multiple embeddings in the appendix.

Definition 3.2. ( Embedding Profile ) Given a subgraph feature G , the embedding profile of the subgraph G in a set of graphs G , denoted by P G G , is the set of embedding of G in G , or: P T G = { e | e is an embedding of G in G 0  X  X } Definition 3.3. ( Feature Consistency Relationship ) Given two frequent patterns f i and f j in a set of graph G , let d
G = ( d G ( f i , f j )) G  X  X  denote the list of embedding distances computed between f i and f j , f i and f j are consistent if the variance of embedding distance vector d G is less than some threshold max var .

Example 3.2. In Figure 1, d G 1 ( F 1 , F 2 ) = d G 2 ( F d 3 ( F 1 , F 2 ) = 2 . 5 . The variance of the distance vector d 0 and hence F 1 and F 2 is consistent. Assuming the shortest path connecting F 1 and F 3 in different graphs have dramat-ically different lengths, we conclude that F 1 and F 3 are not consistent. Figure 2: Left: Three subgraphs and three embed-dings of the subgraphs. This figure is duplicated from Figure 1 for clarity. Right: The feature consis-tency map for the three subgraphs shown in Figure 1
Consistency measures the stableness of the spatial distri-bution of two frequent subgraphs. We use this criteria to select features with a stable spatial distribution.
Definition 3.4. ( Feature Consistency Map ) A fea-ture consistency map is a graph G = ( V , E ) where the vertex set V and arc set E are specified below:
To avoid confusion of the feature consistency map and a regular graph that we specified before, we use vertex and arc to denote commonly used term  X  X ode X  and  X  X dge X .
The computation of the feature consistency map for a fea-ture set is straightforward: for each pair of features, we col-lect their embedding profiles and then decided whether the two features are consistent. Based on the consistency rela-tionship, we could easily construct the feature consistency map.

We summarize what we have discussed in the following pseudo code of the function FCM for constructing the fea-ture consistency map. In the pseudo code, F is a set of subgraph features and G is the set of training graph samples where the subgraph features are extracted.
 Algorithm 1 FCM( F , G ) 1: V = F , E =  X  2: for G, G 0  X  X  do 3: L  X  X  G G 4: L 0  X  X  G G 0 5: if L and L 0 is consistent then 6: E  X  X   X  ( G, G 0 ) 7: end if 8: end for 9: return G = ( V , E )
In what follows, we introduce two feature selection meth-ods. The first one is a feature filtering method; it selects features individually. The second one is a wrapper meth-ods; it selects features in the context of selected features.
Once we compute the feature consistency map G , we use a simple way to rank the features by sorting the features according to their degree (number of edges incident on the feature) in G in descending order. After sorting features according to their node degree, we select top k features. We call this method SFS Filtering.

In forward structure based feature selection, we sort the features using the same procedure as we do in the feature fil-tering method. Different from the feature filtering method, we use the Equation 7 and select features in the context of se-lected ones. Specifically, we evaluate the similarity between the resulting kernel matrix (may contain several features) and the object kernel matrix and make sure when we select a feature, the similarity value monotonically increases. The following is the algorithm for forward selection, where S
F denotes the feature kernel matrix,  X  is the object kernel matrix, and F is a set of subgraph features as we discussed before.
 Algorithm 2 SFS FS( F ,  X  ) 1: F s =  X  , T 0 = 0 2: sort F according to the node degree in the related feature 3: n  X  X F| 4: for i = 1 , . . . , n do 5: F 0  X  X  s  X  F i 6: T  X  X  ( S F 0 ,  X  ) 7: if T &gt; T 0 then 8: F s  X  F 0 9: T 0  X  T 10: end if 11: end for 12: return F s
The adaptation of the previously mentioned algorithm to a backward selection algorithm is straightforward. We start with the full set of feature and eliminate one by one. In the elimination process, we make sure the similarity of the fea-ture kernel matrix and the object kernel matrix is increasing. Otherwise, we keep the feature. Though conceptually sim-ilar, our empirical evaluation shows that backward feature selection is not as good as forward feature selection.
Finally, we notice that we may augment a weight to each node in the feature consistency map. A straightforward way to assign a weight to a node is to compute the Pearson X  X  Cor-relation Coefficient between the feature and the class labels in the training data set. Many other choices are available, such as mutual information [28] and odd ratio [12]. It is difficult to enumerate all possible choices and we use Pear-son Correlation because empirically it performs well. Unless stated otherwise, the feature consistency map that we use in this paper is always node-weighted where the weight of the node is computed using Pearson X  X  Correlation Coefficients.
We have performed a comprehensive study of the per-formance of our feature selection method using 5 chemical structure graph datasets. We have compared our method with 4 state-of-the-art feature selection methods: SVM Re-cursive Feature Elimination (SVM RFE) [7], Spectral Fea-ture Selection [30], Log Odd Ratio Feature selection (LORFS) [10] and Maximum Margin Feature selection (MMRFS) [3].
For each data set, we used the FFSM algorithm [11] to ex-tract frequent subgraph features from the data sets and used the LibSVM package [2] to train a Support Vector Machine (SVM) for classification. We measured the classification per-formance of our feature selection method and compared ours with those from state-of-the-art methods using cross valida-tion.
We use five data sets from drug virtual screening exper-iments [14]. In each data set, the target values are drugs X  binding affinity to a particular protein. For each protein, the data provider selected 50 chemical structures that clearly bind to the protein ( X  X ctive X  ones). The data provider also listed chemical structures that are very similar to the active ones (judged with domain knowledge) but clearly do not bind to the target protein. This list is known as the  X  X ecoy X  list. We randomly sampled 50 chemical structures from the decoy list. See [14] for further details regarding the nature of the data set. To reiterate, each of these 5 data sets contains 100 compounds with 50 positives and 50 negatives.
We followed the same procedure [11] to use a graph to model a chemical structure: a node represents an atom and an edge represents a chemical bond. We removed Hydro-gen atoms in our graph representation of chemicals, as com-monly done in the cheminformatics field. The characteristics of the data set is shown in Table 2.
 Table 2: Data set: the symbol of the data set. S : total For each data set, we mined frequent subgraphs using the FFSM algorithm [11] with min support = 50% and with at least 5 nodes and no more than 10 nodes. We then treated each subgraph as a feature. We create a binary feature vec-tor for each graph in the data set, indexed by the mined subgraphs, with values indicate the existence (1) or absence (0) of the related features. All feature selection methods that we compared with are based on the same feature sets.
As indicated before, we compared 4 feature selection meth-ods. To have a fair comparison, we select a fixed number of k features using each method and compare the classification accuracy of the selected features. In our experiments, we set k = 25 . Empirical study shows that there is no significant change if we replace the fixed value 25 with a wide range of values.

To compute the feature consistency map that is used in the Pattern SFS method, we set max var to be 0.5. For Figure 3: Experimental workflow for a single cross-the MMRFS method, we used the default parameter (cover-age threshold  X  =1 and min sup = 0 . 5 ). We implement our own version of the spectral feature selection method. This is a filtering method with no additional parameters. We use the SVM RFE executable as included in the spider ma-chine learning toolbox downloaded from http://www.kyb. tuebingen.mpg.de/bs/people/spider/ .

We use LIBSVM perform training and testing. To have a fair comparison we use RBF kernel with default parame-ters ( C =1,  X  =0.5) in all the experiments we run. We use standard 5-fold cross validation to derive training and test-ing samples. For a cross validation, we compute precision as (TP/(TP+FP)), recall as (TP/(TP+FN)), and accuracy as (TP+TN/ S ) where TP stands for true positive, TN stands for true negative and S stands for the total number of sam-ples. For each data set, we repeat the 5-fold cross validation 10 times and report the average precision, recall, and ac-curacy. Standard deviation of classification results is not small, in a range of 5% to 10%. We performe all of our experiments on a desktop computer with a 3Ghz Pertium 4 processor and 1 GB of RAM.

Figure 3 gives an overview of our experimental set up.
In this section, we present the performance of our method compared with four additional methods: Spectral feature selection, SVM RFE, LORFS and MMRFS. The accuracy is shown in Figure 5 and the precision, recall are as shown in Table 1. Figure 5: Comparing the classification accuracy of 5
Pattern SFS outperforms MMRFS, LORFS in all the 5 datasets, outperforms the SVM RFE method and the spec-tral feature selection method 3 out of the 5 data sets. Over-all, Pattern SFS is the best methods in the group in 3 out of the 5 tested data sets. The results confirm our hypothesis that considering the spatial distribution of subgraph features leads to better selection of discriminative features. Here we compare seven variations of the basic Structure Based Feature Selection methods. These are: In addition, we show results without any feature selection (Pattern all) and results with features selected by Pearson Correlation coefficient selection (PCCS). The results are shown in Table 3. From the table, we find that the performance of PC weighted forward selection is the best overall. In this pa-per, we use PC weighted forward selection to compare with other state-of-the art methods.

We observe that Pattern All often achieves the worst re-sult, which demonstrates that redundant features will result in over-fitting problem and diminish the classification ac-curacy. Although PCCS takes correlation between a single feature and label, it neglects dependence of features and hence usually do not select the optimal feature subsets.
Since we have several parameters in the feature extraction and feature selection process, we evaluate the robustness of our method by changing different parameter values. In the following study, we have singled out a single data set (the Fxa data set) and test the robustness of our method using this data set by varying three parameters: the min sup in frequent subgraph feature extraction process, the total num-ber of selected features ( k ), and the max var parameter that is used to derive the feature consistency map.

On the left part of Figure 4, we show the result by chang-ing min sup from 0.25 to 0.50 on FXa dataset with # feature s = 25 and max var =0.5. It is obvious that our method re-mains stable with variant min sup .

The middle part of Figure 4 indicates results on FXa data set by varying the number of selected features from 1 to 100. From the result, the classification accuracy remains stable within a wide range and the best accuracy can be obtained around 20 to 50.

We fix min sup =30% and #features=25 to test whether the accuracy remains stable with different max var . In right most of Figure 4, max var is changed from 0.125 to 1.25, the accuracy is around 92.7% with standard deviation 0.6%.
Overall, our structure based feature selection method is effective and achieves good accuracy within a wide range minimum support, maximum variance and the number of selected features. Since feature selection can be viewed as a data preprocessing step, any other feature selection method can be combined with our framework and our method is applicable to any current start-of-art classifiers.
In this paper, we presented a novel feature selection method for graph classification. By ranking features based on their spatial distribution and their contributions to classification, we have designed a feature selection method (and several variations) called structure based feature selection method. Compared with current state-of-the-art methods as evalu-ated on 5 real world data sets, our method outperforms the 4 state-of-the-art methods on majority of the tested data sets. In the feature, we plan to extend structure feature se-lection to kernel space. Currently, our method can work in both unsupervised learning and supervised learning, hence another line of our future work is to do semi-supervised fea-ture selection. This work has been supported by the Kansas IDeA Network forBiomedica lResear chExcellenc e(NIH/NCR Rawar d#P20 RR016475) and a NIH grant #R01 GM868665. HF is par-tially supported by the Office of Naval Research through Award Number N00014-07-1-1042, Oak Ridge National Lab-oratory (ORNL) via Award Number 4000043403, and the KU Transportation Research Institute (KUTRI). [1] B. Cao, D. Shen, J.-T. Sun, Q. Yang, and Z. Chen. [2] C. Chang and C. Lin. Libsvm: a library for support [3] H. Cheng, X. Yan, J. Han, and C.-W. Hsu.
 [4] N. Cristianini, J. Shawe-Taylor, and A. Elisseeff. On [5] M. Deshpande, M. Kuramochi, and G. Karypis. [6] T. G  X  artner, P. Flach, and S. Wrobel. On graph [7] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [8] D. Haussler. Convolution kernels on discrete [9] J. Huan, D. Bandyopadhyay, J. Snoeyink, J. Prins, [10] J. Huan, W. Wang, D. Bandyopadhyay, J. Snoeyink, [11] J. Huan, W. Wang, and J. Prins. Efficient mining of [12] J. Huan, W. Wang, A. Washington, J. Prins, R. Shah, [13] I. Jolliffe. Principal Component Analysis. Springer; [14] R. Jorissen and M. Gilson. Virtual screening of [15] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [16] B. K.M. and K. H.-P. Shortest-path kernels on graphs. [17] R. Kohavi and G. H. John. Wrappers for feature [18] F. Li, Y. Yang, and E. P. Xing. From lasso regression [19] A. M. Martinez and A. C. Kak. PCA versus LDA. [20] J. Ramon and T. G  X  artner. Expressivity versus [21] S. T. Roweis and L. K. Saul. Nonlinear Dimensionality [22] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [23] A. Smalter, J. Huan, and G. Lushington.
 Figure 6: Frequent patterns with multiple embeddings [24] S. W. Tamas Horvath, Thomas Gartner. Cyclic [25] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A [26] S. Yan, D. Xu, B. Zhang, and H.-J. Zhang. Graph [27] X. Yan and J. Han. Closegraph: Mining closed [28] Y. Yang and J. O. Pedersen. A comparative study on [29] H. Yu and J. Yang. A direct LDA algorithm for [30] Z. Zhao and H. Liu. Spectral feature selection for In case when a frequent subgraph pattern has several embed-dings in a graph, equation 8 is not applicable and we need to develop an approach to handle multiple embeddings. To-ward that, we have developed a method to select the repre-sentative embedding when multiple embeddings in one graph happen, as detailed bellow.

Definition 6.1. ( Multiple Embedding Distance ) Given two frequent patterns F i and F j with multiple embeddings e , e i 2 , . . . , e im of F i and e j 1 , e j 2 , . . . , e the distance between an embedding e ik and the pattern F j in graph G as: where d G is the embedding distance defined in Section 3.
Example 6.1. In Figure 6, subgraph feature F 1 has two embeddings in G denoted as e 11 and e 12 , F 2 has only one embedding e 2 and F 3 has e 3 . The multiple embedding dis-tance denoted by dist G ( e 11 , F 2 ) is d G ( e 11 , e 2 larly, we have dist G ( e 12 , F 2 ) = 6 4 , dist G ( e 11
Definition 6.2. ( Multiple Embedding Distance Pro-file ) Given a set of features F = F 1 , F 2 , . . . , F n graph G and F i with multiple m embeddings e i 1 , e i 2 , . . . , e the multiple embedding distance profile of e ik k  X  [1 , m ] denoted by V G ( e ik ) , is a vector with each element represent-ing the multiple embedding distance between e ik and another feature:
Example 6.2. In Figure 6, we have V G ( e 11 ) = ( 10 4 , 14
With multiple embedding distance profile computed, one approach to select the representative embedding is to use an iterative procedure: first randomly select a set of embed-dings, second compute the associated multiple embedding distance profiles, third update the selection of embeddings so that the variance of the multiple embedding distance pro-files is reduced, and finally iterate until some types of conver-gence criteria are achieved. In this paper we implemented a straightforward approach, which is to select the one with the minimal sum of multiple embedding distance profile, as de-tailed below. This simple approach achieves good empirical results.

Definition 6.3. ( Representative Embedding ) Given a feature F i , m embeddings in a graph G , the representative embedding of F i , denoted as e i  X  k , is the embedding that has the least sum of distances in the related multiple embedding distance profile, where
Example 6.3. In Figure 6, since V G ( e 11 ) = ( 10 4 , 14 V embedding for F 1 . Once e 12 is selected, we do not consider e 11 when constructing feature consistency map.

The purpose of selecting representative embedding is to guarantee that one pattern is represented by one embedding in a graph so that the procedure of constructing feature consistency map becomes feasible. A representative embed-ding is like a centerpiece subgraph that avoids overlapping as much as possible.
