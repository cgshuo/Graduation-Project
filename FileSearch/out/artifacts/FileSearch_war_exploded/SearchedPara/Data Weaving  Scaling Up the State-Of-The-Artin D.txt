 The enormous amount and dimensionality of data processed by modern data mining tools require effective, scalable un-supervised learning techniques. Unfortunately, the majority of previously proposed clustering algorithms are either effec-tive or scalable. This paper is concerned with information-theoretic clustering (ITC) that has historically been con-sidered the state-of-the-art in clustering multi-dimensional data. Most existing ITC methods are computationally ex-pensive and not easily scalable. Those few ITC methods that scale well (using, e.g., parallelization) are often out-performed by the others, of an inherently sequential na-ture. First, we justify this observation theoretically. We then propose data weaving  X  X  novel method for paralleliz-ing sequential clustering algorithms. Data weaving is intrin-sically multi-modal  X  X t allows simultaneous clustering of a few types of data ( modalities ). Finally, we use data weaving to parallelize multi-modal ITC, which results in proposing a powerful DataLoom algorithm. In our experimentation with small datasets, DataLoom shows practically identical performance compared to expensive sequential alternatives. On large datasets, however, DataLoom demonstrates signif-icant gains over other parallel clustering methods. To illus-trate the scalability, we simultaneously clustered rows and columns of a contingency table with over 120 billion entries. I.5 [ Pattern Recognition ]: Clustering; D.1 [ Programming Techniques ]: Concurrent Programming X  Parallel program-ming Algorithms Information-theoretic clustering, multi-modal clustering, par-allel and distributed data mining
It becomes more and more apparent that processing gi-gabytes and terabytes of information is a part of our every-day routine. While data mining technology is already ma-ture enough for tasks of that magnitude, we X  X ata mining practitioners X  X re not always prepared to use the technology in full. For example, when a practitioner faces a problem of clustering a hundred million data points, a typical approach is to apply the simplest method possible, because it is hard to believe that fancier methods can be feasible . Whoever adopts this approach makes two mistakes:  X  Simple clustering methods are not always feasi-ble . Let us consider, for example, a simple online clustering algorithm (which, we believe, is machine learning folklore): first initialize k clusters with one data point each, then it-eratively assign the rest of points into their closest clusters (in the Euclidean space). Even for small values of k (say, k = 1000 ), such an algorithm may work for hours on a mod-ern PC. The results would however be quite unsatisfactory, especially if our data points are 100,000-dimensional vectors.  X  State-of-the-art clustering methods can scale well , which we aim to show in this paper.

With the deployment of large computational facilities (such as Amazon.com X  X  EC2, IBM X  X  BlueGene, HP X  X  XC), the parallel computing paradigm is probably the only currently available option for addressing gigantic data processing tasks. Parallel methods become an integral part of any data pro-cessing system, and thus gain special importance (e.g., some universities are currently introducing parallel methods to their core curricula [19]).

Despite that data clustering has been in the focus of the parallel and distributed data mining community for more than a decade, not many clustering algorithms have been parallelized, and not many software tools for parallel clus-tering have been built (see Section 3.1 for a short survey). Apparently, most of the parallelized clustering algorithms are fairly simple. There are two families of data clustering methods that are widely considered as very powerful:  X  Multi-modal (or multivariate) clustering is a frame-work for simultaneously clustering a few types (or modali-ties ) of the data. Example: construct a clustering of Web pages, together with a clustering of words from those pages, as well as a clustering of URLs hyperlinked from those pages. It is commonly believed that multi-modal clustering is able to achieve better results than traditional, uni-modal meth-ods. The two-modal case (usually called co-clustering or double clustering ) has been widely explored in the litera-ture (see [14, 12]), however, a more general m -modal case has only recently attracted close attention of the research community (see [17, 1]), probably because of its computa-tional cost.  X  Information-theoretic clustering (ITC) (see, e.g. [30]) is an adequate solution to clustering highly multi-dimensional data, such as documents or genes. ITC methods perform global optimization of an information-theoretic objective func-tion. For the details, see Section 2.

Many global optimization methods are greedy X  X hose meth-ods are sequential in their essence, and therefore are diffi-cult to parallelize. In contrast, local optimization methods are often easily parallelizable. Many popular clustering al-gorithms, such as k -means, belong to the latter category. Unfortunately, most of them are not very effective on large multi-dimensional datasets. In the text domain, for exam-ple, k -means usually ends up with one huge cluster and a few tiny ones. 1
One approach to solving a global optimization problem is to break it down into a set of local optimizations. Dhillon et al. [12] applied this approach to perform an information-theoretic co-clustering (IT-CC) . In Section 3.2, we show a fairly straightforward way of parallelizing their algorithm. The IT-CC algorithm turns out to be very conservative in optimizing the (global) clustering objective, such that it gets often stuck in local optima. In Section 3.3, we discuss a se-quential co-clustering (SCC) method, and show analytically that it is more aggressive in optimizing the objective.
In Section 4, we propose a new scheme for parallelizing sequential clustering methods, called data weaving . This mechanism works as a loom: it propagates the data through a rack of machines, gradually weaving a  X  X abric X  of clusters. We apply this mechanism to parallelizing the SCC method, which leads to constructing a highly scalable, information-theoretic, multi-modal clustering algorithm, called DataLoom .
In the experimentation part of our paper (Section 5) we first compare DataLoom with its original, non-parallel ver-sion (SCC), as well as with IT-CC and two more baseline methods on four small datasets (including the benchmark 20 Newsgroups). We show that the parallelization does not compromise the clustering performance. Finally, we apply DataLoom to two large datasets: RCV1 [22], where we clus-ter documents and words, and Netflix KDD X 07 Cup data, 2 where we cluster customers and movies. If represented as contingency tables, both datasets contain billions of entries. On both of them, DataLoom significantly outperforms the parallel IT-CC algorithm. To our knowledge, co-clustering experiments of that scale have not been reported previously.
Over the past decade, information-theoretic clustering meth-ods have proven themselves to be the state-of-the-art in clus-tering highly multi-dimensional data. In this paper, we focus
The traditional k -means assigns instances to clusters based on the Euclidean distances between points and centroids. Since text is usually sparse and high-dimensional, docu-ments typically have only few terms in common. As a conse-quence, the l 2 norms of terms and centroids often dominate in the calculation of their Euclidean distances. Since the l norms of centroids naturally decrease with increasing the cluster size, instances tend to be re-assigned to clusters that are already large, and smaller clusters disappear over time. http://cs.uic.edu/~liub/Netflix-KDD-Cup-2007.html on hard clustering (a many-to-one mapping of data points to cluster identities), as opposed to soft clustering (a many-to-many mapping, where each data point is assigned a prob-ability distribution over cluster identities). Hard clustering can be viewed as a lossy compression scheme X  X his observa-tion opens a path to applying various information-theoretic methods to clustering. Examples include the application of the minimum description length principle [6] and rate-distortion theory [9].

The latter led to proposing the powerful Information Bot-tleneck (IB) principle by Tishby et al. [34], and then to dozens of its extensions. In Information Bottleneck, a ran-dom variable X is clustered with respect to an interact-ing variable Y : the clustering  X  X is represented as a low-bandwidth channel (a bottleneck ) between the input signal X and the output signal Y . This channel is constructed to minimize the communication error while maximizing the compression: where I is a Mutual Information (MI), and  X  is a Lagrange multiplier. A variety of optimization procedures have been derived for the Information Bottleneck principle, including agglomerative [31], divisive [2], sequential (flat) [30] meth-ods, and a hybrid of them [1].

Friedman et al. [16] generalize the IB principle to a multi-variate case. In its simplest form, for clustering two variables X and Y , the generalization is relatively straightforward: a channel X  X   X  X  X   X  Y  X  Y is constructed to optimize the objective When more than two variables are clustered, the mutual information I (  X  X ;  X  Y ) is generalized into its multivariate ver-sion, called multi-information. The complexity of computing multi-information grows exponentially while adding more variables, and is therefore restrictive in practical cases even for only three variables.

Information-theoretic co-clustering (IT-CC) was proposed by Dhillon et al. [12] as an alternative to multivariate IB, for the two-variate case when the numbers of clusters |  X  X | and |  X 
Y | are fixed. In this case, it is natural to drop the compres-sion constraints I (  X  X ; X ) and I (  X  Y ; Y ) in Equation (2), and directly minimize the information loss: when I ( X ; Y ) is a constant for a given dataset. To optimize this objective, Dhillon et al. proposed an elegant optimiza-tion method that resembles the traditional k -means, while the latter has a less powerful L 2 objective.

Bekkerman et al. [1] generalize IT-CC to the multivari-ate case, while avoiding the trap of multi-information: they approximate it with a (weighted) sum of pairwise MI terms: where the data variables { X 1 , . . . , X m } are organized in an interaction graph G = ( X , E ) , 3 with edges e ij corresponding
Lauritzen [21] presents the interaction graph as a general-ization of a graphical model. Y Z Y Y Figure 1: Difference between ICM (upper) and CWO (lower) optimization methods (nodes that are being optimized are unshaded bold). ICM iterates over G  X  X  nodes (in round-robin) and optimize each of them based on its Markov blanket. CWO iter-ates over cliques in G (edges, in the simplest case) and locally optimizes the corresponding model while ignoring the rest of the interaction graph. to pairs of interacting variables ( X i , X j ). Weights w chosen to bring MI terms to the same scale. Bekkerman et al. propose a complex optimization method that utilizes the Iterative Conditional Modes (ICM) [4] for traversing the graph G , and then performs a hybrid hierarchical/sequential clustering step for each of G  X  X  nodes. An illustration of ICM is given in Figure 1 (upper).

We notice that the extra parametrization (through weights w ij ) can be avoided by changing the graph traversal scheme: instead of iterating over nodes (as in ICM), we can iterate over edges e ij and maximize only one MI term I (  X  X i ; a time (see the lower part of Figure 1). We call our method Clique-Wise Optimization (CWO)  X  X t is analogous to pair-wise training [33] in a supervised learning setup.
Parallel and distributed data mining is a very active field of research [23] that covers a variety of disciplines, so we want to narrow down the scope of this section. Although our methods are vaguely related to distributed clustering, we would like to point out that this paper does not address problems such as mining geographically distributed data, privacy preservation due to using insecure networks, or as-pects of fault recovery. Instead, we are  X  X ust X  concerned with scaling up state-of-the-art clustering algorithms to the very large amounts of data we deal with, while we assume a  X  X hared nothing X  cluster of computers connected via a high bandwidth local area network. Note that the clustering problem can be large-scale along several dimensions. Often not only the number of data instances is very large, but data is also of very high dimensionality; for example, ten thou-sands of features is common even for small text corpora. Whenever the task of clustering data collections requires to capture the underlying structure of a dataset at a fine level, using a very large number of clusters is also common. Our goal is to reduce the total computational costs to a tractable level in order to get the best possible clustering results on very large data collections.
Among the early approaches explicitly mentioning and addressing the scalability problem along all three of these dimensions is Canopy [26], a non-distributed clustering al-gorithm that avoids many expensive distance computations by aggregating objects at a coarse level; only objects in a common  X  X anopy X  are assumed to be close enough to po-tentially be in the same cluster. Scaling up drastically may compromise the data mining results, however, (and unlike ITC) the method requires an underlying distance metric.
Several authors addressed the scalability issue of cluster-ing by parallelizing specific algorithms, most prominently k -means [20, 13] including its generalizations [15] that cover e.g., the EM algorithm. The parallelization strategies ex-ploit the stage-wise nature and mathematical properties that allow to compute global from local solutions: Each node is responsible for a subset of the data. It computes the clos-est cluster for each of its instances, computes the new local cluster means (or parameters for EM, respectively), commu-nicates these means to a master who aggregates them, and distributes the aggregated centroids (or parameters) for the next iteration. This parallelization procedure yields algo-rithms that compute identical results as their non-parallel counterparts.

On the technical side, the literature usually shows that these algorithms can be realized on top of a specific low-level communication framework [15, 13, 18] running on a  X  X hared nothing X  cluster, but they clearly are not limited to this kind of architecture. It has recently been discussed that the same kind of parallelization works very well in combi-nation with the popular MapReduce paradigm [10]. Paral-lelizing the clustering algorithms k -means and EM (mixture of Gaussians) via MapReduce is covered in [8].

Parallelization of clustering algorithms is still an active field of research. As discussed above, in some fields, like density-based clustering, a good understanding of how to parallelize has already been gained, so the research focuses on rather specific cases like spatial aspects and structured data, with a clear bias towards distributed (as opposed to parallel) data mining [35, 7]. For information-theoretic clus-tering this research is still in its infancy. A clustering algo-rithm for specific distributed data problems that employs information-theoretic objectives has recently been proposed by [11], but it does not discuss efficient parallelization of ITC. To the best of our knowledge, a systematic study of the practically highly relevant parallelization of information-theoretic clustering is still lacking. As we will see in Sec-tion 3.2, adopting the strategies sketched above already al-lows for a straightforward parallelization of an important ITC algorithm. Dhillon et al. X  X  information-theoretic co-clustering (IT-CC) [12] is a k -means-style algorithm that locally optimizes the global information-theoretic objective function (3). We first briefly sketch the formal background of IT-CC, before proposing its parallelization.

The goal of IT-CC is to approximate a given joint prob-ability distribution p over two modalities X and Y with a  X  X impler X  distribution q , where the statistical dependencies are captured in a lower-dimensional cluster space: where x  X  X , y  X  Y , p ( x ) and p ( y ) denote marginals;  X  x and  X  y are the corresponding clusters of x and y , respec-tively; and p (  X  x ) and p (  X  y ) are marginals of these clusters. Dhillon et al. show that optimization of the objective func-tion (3) is equivalent to the minimization of the KL diver-gence D KL ( p ( X, Y ) || q ( X, Y )) between the joint distribution p and its approximation q .

Like many other co-clustering algorithms, IT-CC alter-nates iterations that update the clustering assignments  X  and  X  Y . The reason we think of IT-CC as a k -means-style algorithm is that it first assigns all data points to their clos-est clusters, and then it recomputes cluster representatives based on the data points now contained in each cluster.
Let us focus on a IT-CC iteration where the clustering  X  X is updated given the clustering  X  Y (the opposite case is symmetric.) Unlike the traditional k -means that uses the Euclidian distance metric, IT-CC defines the proximity of a data point x to a cluster  X  x in terms of the KL divergence between p ( Y | x ) and q ( Y |  X  x ) , where the latter is computed using During the data point assignment process, the conditionals q ( Y |  X  x ) do not change, thus playing the role of the centroids.
Dhillon et al. prove that the co-clustering strategy of as-signing data points x to clusters  X  x by minimizing the local global objective function, which guarantees the algorithm X  X  convergence . The following transformations illustrate how to simplify computations without changing the optimization problem, that is, without changing the total order of its so-lutions. We remove terms that are constants in the context of optimizing cluster assignment  X  X , and rewrite The above transformation shows that rather than computing the centroids q ( Y |  X  x ) , the algorithm only needs to compute
We argue that the same simplification allows to select an optimal clustering  X  X opt from a set of candidate clusterings  X  Let q (  X  X,  X  Y ) be the distribution q induced by a specific pair of clusterings  X  X and  X  Y . We have: as our new, equivalent formulation of the IT-CC optimiza-tion problem. The function in Equation (7) preserves even the correct total order of the candidate clusterings with re-spect to the mutual information (3) .

Following the outline of parallel k -means (Section 3.1) and the description above, we can adapt IT-CC to the parallel case as follows: We alternate the optimization of  X  X and  X  Y . During each of these optimizations the parallel pro-cesses hold disjoint subsets of the data. We will just de-scribe the case of computing a new clustering of X ; clus-tering Y works analogously. Process i will hold the data for elements X ( i )  X  X . All cluster  X  X entroids X  p (  X  x,  X  y ) are distributed to all nodes, where the new cluster assignments are computed based on the KL divergence (6) . Given the new assignments, each process i computes the local joints q ( i ) (  X  x,  X  y ) = P broadcasts them to a master node. The master computes the new global  X  X entroids X  q (  X  x,  X  y ) = then be broadcasted to the nodes again to start the next round of refining  X  X , or the algorithm can switch to re-cluster Y instead. Note that this process yields exactly the same results as in the non-parallelized case.

We consider the parallel IT-CC algorithm as a strong baseline for the DataLoom algorithm proposed in a Sec-tion 4. Before moving on, let us discuss the potential of Dat-aLoom by taking a closer look at the difference between IT-CC and the (non-parallelized) sequential information bot-tleneck.
DataLoom originates from a multi-modal version of the sequential Information Bottleneck (sIB) algorithm [30]. In sIB, at its initialization step, all data points are uniformly at random assigned into clusters. Then, a random permu-tation of all the data points is constructed, each element of which is pulled out of its cluster and iteratively assigned into any other cluster. It is finally left in the cluster such that the objective function (1) is maximized. The algorithm is executed until its full convergence.

We consider the multi-modal variation of sIB ( we call it sequential co-clustering (SCC) ), which iterates over the data modalities organized in an interaction graph (see Section 2). At each iteration, it applies the sIB X  X  optimization procedure to maximize the co-clustering objective (3). It improves clusterings by continuously updating cluster memberships of individual data points. To decide whether to change a cluster membership, it directly evaluates the objective.
Proposition 3.1. The set of clustering pairs (  X  X,  X  Y ) that are local optima of SCC are a subset of the clustering pairs that are local optima of IT-CC.

Proof. It is sufficient to show that, whenever IT-CC reads a pair of clusterings (  X  X,  X  Y ) and outputs a pair of clus-SCC will improve the objective function on (  X  X,  X  Y ) as well. We will just discuss this for the case of re-clustering X ; the case of re-clustering  X  Y can be shown analogously.
By design, the only case in which SCC fails to improve the objective is the case in which it does not change any cluster memberships. Let us show that this cannot happen for any input that is not a local optimum of IT-CC. Whenever the output (  X  X  X  ,  X  Y ) of IT-CC improves the objective function over the input (  X  X,  X  Y ) , we know that IT-CC has changed the cluster membership of at least one element x 0 , say, from  X  x  X  x . In terms of the local objective function (Equation (6)), this implies that We define the clustering  X  X  X  identically to  X  X , except for mov-induced by (  X  X,  X  Y ) , and q  X  := q (  X  X  X  ,  X  Y ) distribution based on the clustering in which x  X  was moved. We will show, using a similar technique as in [12], that SCC cannot be stuck in a local optimum at this point, because its objective function favors (  X  X  X  ,  X  Y ) over (  X  X,  X  at least also move x 0 from  X  x 0 to  X  x  X  . We will use the follow-ing notation that always returns the old cluster  X  X entroids X  based on (  X  X,  X  Y ) , although it already considers x  X  q (  X  y | x ) := refer to this common value by writing  X  q (  X  y |  X  x ) in this case.
Let us start with the value of the SCC X  X  global objective function (Equation (7)) for the clustering pair (  X  X,  X  show that it can be increased by moving x 0 into  X  x  X  : The first inequality holds due to Equation (8), the follow-ing steps just rearrange the summation over all cluster pairs and exploit the identity of joint cluster distributions p (  X  x,  X  y ) and q  X  (  X  x,  X  y ) . The second inequality holds due to the non-negativity of the KL divergence. The last step just rear-ranges terms again.

Together this proves that SCC would reassign at least x 0 because the value of the objective function after moving the single example x 0 (Equation 10) is strictly higher than the original value for  X  X (Equation 9).
 Figure 2: An illustration to the difference between IT-CC and SCC optimization procedures, used in Proposition 3.2 .
 Proposition 3.2. The subset relationship described in Proposition 3.1 is strict.
 Proof. We prove this by presenting an example where IT-CC gets stuck in a local optimum which SCC is able to overcome. We look at three documents with the following sets of words: d 1 = { w 1 } , d 2 = { w 2 } , and d 3 = { w Initially, the first two documents are in cluster  X  d 1 , while the third one is in another cluster  X  d 2 . For simplicity, we assume that each word is in a separate cluster over the X  X ord modal-ity X  W . Figure 2 shows the joint probability matrix p (left) and the initial aggregation to clusters (middle). The condi-easily be verified by applying Equation (6) that IT-CC will not move any document. However, SCC will move either d 1 or d 2 into the second cluster. By applying this modifi-cation, SCC will almost double the mutual information (3) from about 0 . 17 (middle) to about 0 . 32 (right).
Propositions 3.1 and 3.2 reveal that IT-CC gets stuck in local minima more often than SCC . Moreover, from the proofs we can conclude that generally, at the level of up-dating individual cluster memberships, IT-CC is more con-servative. More specifically, this result suggests that the sequential strategy might both converge faster (because in every iteration it will do a number of updates that IT-CC misses) and to a better local optimum. We leave the verifi-cation of this conjecture to the empirical part of this paper.
Parallelization of sequential co-clustering (SCC) is allowed based on the following fairly straightforward consideration: mutual information I (  X  X ;  X  Y ) , which is the objective function of SCC, has the additive property over either of its argu-ments. That is, when SCC optimizes  X  X with respect to  X  and a data point x 0  X   X  x 0 asks to move to cluster  X  x the portion of the mutual information that corresponds to clusters  X  x 0 and  X  x  X  is affected. Indeed, by definition, To check whether or not moving x 0 into  X  x  X  increases our ob-jective function, it is sufficient to compute the delta between Input:
G  X  interaction graph of nodes { X 1 , . . . , X m } and edges E p ( X i , X j )  X  pairwise joint distributions, for each edge e l  X  number of optimization iterations Output: Clusterings {  X  X 1 , . . . ,  X  X m } Initialization: For each node X do Main loop: For each iteration (1 , . . . , l ) do its value before the move and after the move. Again, only terms that correspond to  X  x 0 and  X  x  X  are involved in the delta computation. Also, the marginals p (  X  y ) cancel out: This brings us to the idea that probing the moves x 0  X   X  x can be performed in parallel if all the clusters of  X  X are split into disjoint pairs. Each probing like that can be then ex-ecuted using a separate process, after which the processes can exchange their data. Since the communication is gen-erally expensive, it is beneficial to test all elements of both  X  x and  X  x  X  . If the probe shows that the objective can be in-creased, the element is immediately moved from its cluster into another. Using this approach, we lose one ingredient of SCC: data points do not necessarily move into the clus-ter such that the objective function is maximized , but only increased . Despite that, intuitively, such a loss might look crucial, Bekkerman et al. [3] empirically show that both ap-proaches are comparable, as soon as the number of optimiza-tion steps is about the same. The latter can be achieved by iterating over all the cluster pairs.

The DataLoom algorithm consists of a master process and slave processes (where k is the number of clusters). The master X  X  algorithm is shown in Algorithm 1, the slave X  X  in Algorithm 2. After constructing the initial set of cluster Input: (  X  x,  X  x 0 )  X  two clusters from  X  X l  X  overall number of slave processes r  X  [0 .. ( l  X  1)]  X  my process ID Output: New clusters (  X  x,  X  x 0 ) Main loop: For each iteration (1 , . . . , l  X  1) do pairs and sending them to the slave processes, the master node switches to the wait state, while the slave processes work autonomously, communicating with each other. Each slave process receives two clusters and shuffles them while optimizing the objective. After the shuffling task is com-pleted, the slave is ready to send and receive clusters. It is enough to send (and receive) only one cluster of each pair X  by which the communication cost is kept at its minimum.
Figure 3 illustrates our communication protocol, where (for simplicity) we assume that k is even. As each slave process holds two clusters at each time point, we can enu-merate the  X  X eats X  for clusters as shown in the figure, where the upper row defines process numbers at the same time. The protocol is to alternate sending the upper of the clus-ters to the right and then the lower one to the left. So in general, each cluster will move in just one direction, and be moved every second iteration. Node number 0 is an ex-ception, in that it keeps the cluster initially sitting in seat number k  X  1 all the time in that place, and hence always sends the other cluster. It thereby inverts the direction in which a cluster is moving. To clarify the order: without the termination criterion, a cluster starting from seat 0 would an odd number of clusters is treated analogously, where the seat labeled with k  X  1 in Figure 3 is kept unoccupied.
Proposition 4.1. The DataLoom communication proto-col guarantees that every pair of clusters meets exactly once.
Proof. Two clusters meet at a node iff the sum of their seat numbers modulo ( k  X  1) is 0 . If we start with an iteration that moves clusters in the upper row to the right, then every two iterations later the new seat number of every cluster will be increased by 1 modulo ( k  X  1) . This can easily be shown inductively. The sum will hence increase by 2 every two iterations. Similarly, if the sum of seat numbers (modulo k  X  1 ) is ( k  X  2) , then the clusters will meet in the next iteration. It is easy to see that by adding 2 modulo ( k  X  1) it takes at most ( k  X  1) iterations until any two regular clusters Figure 3: An illustration to the deterministic mes-sage passing algorithm. (without the stationary one) meet, and it is also clear that every cluster will hit node 0 and meet the stationary cluster from either seat ( k/ 2  X  1) or ( k  X  2) when moving into the same direction for ( k  X  1) iterations.

Together with the deterministic communication protocol, we propose a stochastic one, in which, after the cluster shuf-fling is completed, a slave process sends one cluster to an-other process chosen randomly. The exact protocol is pre-computed by the master and then distributed to the slaves. It keeps track of the cluster transfers such that at each point of time each slave node has two clusters to process. The stochastic protocol overcomes the problem of the determin-istic protocol, which preserves the initial ordering of clus-ters that may presumably be disadvantageous. However, the stochastic protocol does not provide the completeness guarantee given in Proposition 4.1.

A schematic summary of the DataLoom algorithm is given in Figure 4. The collection of slave processes operates as a loom, that uses the communication protocol as a shuttle to weave clusters. When the cluster  X  X abric X  is woven, the master process collects all the clusters and switches to op-timizing another modality. Obviously, our method can be generalized to process any number of modalities organized in an interaction graph, to be traversed by the master. The method X  X  complexity increases only linearly with increasing the number of edges in the interaction graph.

Note that, regarding the computational complexity (with-out communication), DataLoom is no more expensive than parallel k -means. At each slave process, DataLoom probes whether moving a data point to the other cluster on that machine increases the objective. Totalling the number of these comparisons, we find that on average we probe each cluster exactly once per data point, so we probe as many point-cluster pairs as parallel k -means and parallel IT-CC.
In our implementation of the DataLoom algorithm, the communication is based on the Massage Passing Interface (MPI) [32]. We decided to apply the traditional MPI instead of a currently more popular MapReduce scheme because an iterative application of MapReduce has a substantial disad-vantage: backpropagating the data from a reducer to the next mapper requires a disk access, which is very expensive in our setup. The DataLoom algorithm is deployed on a Hewlett Packard XC Linux cluster system that consists of 62 eight-core machines with 16Gb RAM each.

As a baseline for our large-scale experiments, together with the parallelized IT-CC algorithm, we used a paral-lelized version of the double k -means algorithm (see, e.g. [28]). Double k -means is basically the IT-CC optimization procedure that minimizes the traditional k -means objective function (the sum of Euclidian distances of data points to their centroids). We parallelized it analogously to the IT-CC parallelization (see Section 3.2). Our first objective is to show that the performance of the DataLoom algorithm is comparable to the one of its sequen-tial ancestor. To meet this objective, we replicate the ex-perimental setup of Bekkerman et al. [1], who test ITC al-gorithms on six relatively small textual datasets. Our eval-uation measure is micro-averaged accuracy that is defined as follows. Let C be the set of ground truth categories. For each cluster  X  x , let  X  C (  X  x ) be the maximal number of elements of  X  x that belong to one category. Then, the precision of  X  x micro-averaged precision of the entire clustering  X  X is: which is the portion of data points that belong to dominant categories. If the number of clusters is equal to the number of categories, then Prec (  X  X, C ) equals micro-averaged recall and thus equals clustering accuracy.

For simplicity, we choose four out of the six datasets used by Bekkerman et al. [1] X  X he ones that have an even number of categories. Three of those datasets ( acheyer , mgondek , and sanders-r ) are small collections of 664, 297, and 1188 email messages, grouped into 38, 14, and 30 folders, respec-tively. The fourth dataset is the widely-used benchmark 20 Newsgroups (20NG) dataset, that consists of 19,997 postings submitted to 20 newsgroups. About 4.5% of the 20NG docu-ments are duplications X  X e do not remove them, for better replicability. For all the four datasets, we simultaneously cluster documents and their words. For email datasets, we also cluster the third modality, which is the names of email correspondents. For the 3-way clustering, we use our CWO optimization scheme (see Section 2).

The summary of our results is given in Table 1. Besides comparing to SCC and IT-CC, we compared DataLoom (deterministic) (stochastic) (stochastic) against the standard uni-modal k -means, as well as against Latent Dirichlet Allocation (LDA) [5] X  X  popular generative model for representing document collections. In LDA, each document is represented as a distribution of topics, and pa-rameters of those distributions are learned from the data. Documents are then clustered based on their posterior dis-tributions (given the topics). We used Xuerui Wang X  X  LDA implementation [25] that applies Gibbs sampling with 10000 sampling iterations.

As we can see in the table, the empirical results approve our theoretical argumentation from Section 3.3 X  X equential co-clustering significantly outperforms the IT-CC algorithm. Our 2-way parallelized algorithm demonstrates very reason-able performance: only in two of the four cases it is infe-rior to the SCC. It is highly notable that our 3-way Data-Loom algorithm achieves the best results, outperforming by more than 5% (on the absolute scale) all its competitors on mgondek . When comparing the deterministic and stochas-tic communication protocols, we notice that they perform comparably. For the rest of our experiments, we use the stochastic version.
The RCV1 [22] dataset is by far the largest fully labeled text categorization dataset available to the research commu-nity. It consists of 806,791 documents each of which belongs to a hierarchy of categories. The top level of the hierarchy contains only four categories, while the second level contains 55 categories. In our experiment, we ignore the top level and map categories from all the lower levels onto their parents from the second level (using this scheme, 27076 documents are not assigned into any category, and therefore are always considered as wrongly categorized). We remove stopwords and low frequency words (leaving 150,032 distinct words overall). Represented as a contingency table, the resulting data contains over 120 billion entries. We are aware of only one previous work [29] where the entire RCV1 collection was clustered. Following this work, we use the clustering preci-sion measure, given in Equation (12). We built 800 docu-ment clusters and 800 word clusters. We plot the precision over the clustering iterations and compare DataLoom with the parallelized IT-CC, as well as with parallelized double k -means. The results are presented in Figure 5 (left), where DataLoom has a clear advantage over the other methods. We also plot the mutual information I (  X  X ;  X  Y ) after each it-eration, and show that DataLoom is able to construct clus-terings with 20% higher mutual information.
Another data set we used in our experiments was taken from the Netflix challenge. It contains the ratings of 17,770 movies given by 480,189 users. We did not consider the actual value of ratings, but wanted to predict for a number of given user-movie pairs whether or not this user rated this movie. This resembles one of the tasks of KDD X 07 Cup, and we used the evaluation set provided as part of that Cup. We built 800 user clusters and 800 movie clusters.

Our prediction method is directly based on the the nat-ural approximation q (defined in Equation (4)) of our (nor-malized) boolean movie-user rating matrix p . The quality of this approximation is prescribed by the quality of the co-clustering. The intuition behind our experiment is that capturing more of the structure underlying this data helps in better approximating the original matrix. We ranked all the movie-user pairs in the hold-out set with respect to the predicted probability of q . Then we computed the Area Under the ROC Curves (AUC) for the three co-clustering algorithms. To establish a lower bound, we also ranked the movie-user pairs based on the pure popularity score p ( x ) p ( y ) . The results are shown in Figure 5 (right). In addition to that, as in the RCV1 case, we directly compared the objective function values of the co-clusterings produced by DataLoom and IT-CC. As for RCV1, DataLoom shows an impressive advantage compared to the other methods.
This paper comes as an attempt to dramatically scale up a strong data clustering method, while applying paralleliza-tion. The resulting algorithm is applied to two large labeled data corpora, RCV1 and Netflix, of hundreds of thousands data instances each. The algorithm is, by all means, appli-cable to datasets orders of magnitude larger than that, but we decided on these two datasets for the evaluation purposes only.
 As far as the speedup is concerned, on small datasets (see Section 5.1) the DataLoom method is not gaining partic-ularly impressive advantage over non-parallelized methods. Naturally, small datasets can be clustered using sequential clustering as is. On large datasets, however, the paral-lelization is vital. Basically, SCC is not applicable to the large datasets: on RCV1, for example, it would have run for months (assuming that it can fit the RAM). Thus, applying the data weaving parallelization makes real what would have been infeasible otherwise.

Our immediate future work goal is to apply DataLoom to large-scale multi-modal data with more than two modal-ities. A possible candidate is the Netflix data, where the movies are associated with their textual descriptions (the third modality), and the full cast (the fourth modality). Ex-tensions of our method to constrained co-clustering [27] and to relational clustering [24] are also being considered.
We would like to thank Eric Wu and N. K. Krishnan for their excellent technical support of the XC cluster. [1] R. Bekkerman, R. El-Yaniv, and A. McCallum.
 [2] R. Bekkerman, R. El-Yaniv, N. Tishby, and [3] R. Bekkerman, M. Sahami, and E. Learned-Miller. [4] J. Besag. On the statistical analysis of dirty pictures. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] C. B  X  ohm, C. Faloutsos, J.-Y. Pan, and C. Plant. [7] S. Brecheisen, H.-P. Kriegel, and M. Pfeifle. Parallel [8] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. R. [9] K. Crammer, P. Talukdar, and F. Pereira. A [10] J. Dean and S. Ghemawat. Mapreduce: Simplified [11] D. Deb and R. A. Angryk. Distributed document [12] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [13] I. S. Dhillon and D. S. Modha. A data clustering [14] R. El-Yaniv and O. Souroujon. Iterative double [15] G. Forman and B. Zhang. Distributed data clustering [16] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. [17] B. Gao, T.-Y. Liu, X. Zheng, Q.-S. Cheng, and W.-Y. [18] P. E. Hadjidoukas and L. Amsaleg. Parallelization of a [19] M. Johnson, R. H. Liao, A. Rasmussen, R. Sridharan, [20] D. Judd, P. K. McKinley, and A. K. Jain. Large-scale [21] S. L. Lauritzen. Graphical Models . Clarendon Press, [22] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [23] K. Liu, H. Kargupta, J. Ryan, and K. Bhaduri. [24] B. Long, Z. Zhang, and P. S. Yu. A probabilistic [25] A. McCallum, A. Corrada-Emmanuel, and X. Wang. [26] A. McCallum, K. Nigam, and L. H. Ungar. Efficient [27] R. Pensa and J.-F. Boulicaut. Constrained [28] R. Rocci and M. Vichi. Two-mode multi-partitioning. [29] N. Rooney, D. Patterson, M. Galushka, and [30] N. Slonim, N. Friedman, and N. Tishby. Unsupervised [31] N. Slonim and N. Tishby. Agglomerative information [32] M. Snir, S. Otto, S. Huss-Lederman, D. Walker, and [33] C. Sutton and A. McCallum. Piecewise training of [34] N. Tishby, F. Pereira, and W. Bialek. The information [35] X. Xu, J. J  X  ager, and H.-P. Kriegel. A fast parallel
