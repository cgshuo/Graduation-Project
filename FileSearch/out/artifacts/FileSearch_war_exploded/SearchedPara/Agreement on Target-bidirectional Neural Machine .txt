 Recurrent neural network (RNN) has achieved great successes on several structured prediction tasks (Graves, 2013; Watanabe and Sumita, 2015; Dyer et al., 2015), in which RNNs are required to make a sequence of dependent predictions. One of its ad-vantages is that an unbounded history is available to enrich the context for the prediction at the current time-step.

Despite its successes, recently, (Liu et al., 2016) pointed out that the RNN suffers from a fundamental issue of generating unbalanced outputs: that is to say the suffixes of its outputs are typically worse than the prefixes. This is due to the fact that later predictions directly depend on the accuracy of previous pre-dictions. They empirically demonstrated this issue on two simple sequence-to-sequence learning tasks: machine transliteration and grapheme-to-phoneme conversion.

On the more general sequence-to-sequence learn-ing task of machine translation (MT), neural ma-chine translation (NMT) based on RNNs has re-cently become an active research topic (Sutskever et al., 2014; Bahdanau et al., 2014). Compared to those two simple tasks, MT involves in much larger vocabulary and frequent reordering between input and output sequences. This makes the prediction at each time-step far more challenging. In addition, sequences in MT are much longer, with averaged length of 36.7 being about 5 times longer than that in grapheme-to-phoneme conversion. Therefore, we believe that the history is more likely to contain in-correct predictions and the issue of unbalanced out-puts may be more serious. This hypothesis is sup-ported later (see Table 1 in  X  4.1), by an analysis that shows the quality of the prefixes of translation hy-potheses is much higher than that of the suffixes.
To address this issue for NMT, in this paper we extend the agreement model proposed in (Liu et al., 2016) to the task of machine translation. Its key idea is to encourage the agreement between a pair of target-directional (left-to-right and right-to-left) NMT models in order to produce more balanced translations and thus improve the overall translation quality. Our contribution is two-fold:  X  We introduce a simple and general method to  X  We provide an empirical evaluation of the tech-Suppose x =  X  x 1 ,x 2 ,  X  X  X  ,x m  X  denotes a source sentence, y =  X  y 1 ,y 2 ,  X  X  X  ,y n  X  denotes a target sen-tence. In addition, let x &lt;t =  X  x 1 ,x 2 ,  X  X  X  ,x denote a prefix of x . Neural Machine Translation (NMT) directly maps a source sentence into a tar-get within a probabilistic framework. Formally, it defines a conditional probability over a pair of se-quences x and y via a recurrent neural network as follows: where  X  is the set of model parameters; h t denotes a hidden state (i.e. a vector) of y at timestep t ; g is a transformation function from a hidden state to a vector with dimension of the target-side vocabulary size; softmax is the softmax function, and [ i ] de-notes the i th component in a vector. 2 Furthermore, h t = f ( h t  X  1 ,c ( x ,y &lt;t )) is defined by a recurrent function over both the previous hidden state h t  X  1 and the context c ( x ,y &lt;t ) . 3 Note that both h t and c ( x ,y &lt;t ) have dimension d for all t .
In this paper, we develop our model on top of the neural machine translation approach of (Bahdanau et al., 2014), and we refer the reader this paper for a complete description of the model, for example, the definitons of f and c . The proposed method could just as easily been implemented on top of any other RNN models such as that in (Sutskever et al., 2014). In this section, we extend the method in (Liu et al., 2016) to address this issue of unbalanced outputs for NMT. The key idea is to: 1) train two kinds of NMT, i.e. one generating targets from left-to-right while the other from right-to-left ; 2) encourage the agree-ment between them by joint search. 3.1 Training The training objective function for our agreement (or joint ) model is formalized as follows: ` = where y r =  X  y n ,y n  X  1  X  X  X  ,y 1  X  is the reverse of se-quence y ; p ( y | x ;  X  1 ) denotes the left-to-right model with parameters  X  1 , while p ( y r | x ;  X  2 ) de-notes the right-to-left model with parameters  X  2 , as defined in Eq.(1); and  X  x , y  X  ranges over a given training dataset. Following (Bahdanau et al., 2014), we employ AdaDelta (Zeiler, 2012) to minimize the loss ` .

Note that, in parallel to our efforts, Cheng et al. (2016) has explored the agreement idea for NMT close to ours. However, unlike their work on the agreement between source and target sides in the spirit of the general idea in (Liang et al., 2006), we focus on the agreement between left and right di-rections on the target side oriented to the natural is-sue of NMT itself. Although our model is orthogo-nal to theirs, one of our advantage is that our model does not rely on any additional hyperparameters to encourage agreement, given that tuning such hyper-parameters for NMT is too costly. 3.2 Approximate Joint Search Given a source sentence x and model parameters  X   X  1 , X  2  X  , decoding can be formalized as follows: As pointed out by (Liu et al., 2016), it is NP-hard to perform an exact search, and so we adapt one of their approximate search methods for the machine translation scenario. The basic idea consists of two steps: 1) run beam search for forward and reverse models independently to obtain two k -best lists; 2) re-score the union of two k -best lists using the joint model to find the best candidate. We refer to the reader to (Liu et al., 2016) for further details. We conducted experiments on two challenging translation tasks: Japanese-to-English (J P -E N ) and Chinese-to-English (C H -E N ), using case-insensitive B LEU for evaluation.

For the J P -E N task, we use the data from NTCIR-9 (Goto et al., 2011): the training data consisted of 2.0M sentence pairs, The development and test sets contained 2K sentences with a single referece, respectively. For the C H -E N task, we used the data from the NIST2008 Open Machine Translation Campaign: the training data consisted of 1.8M sen-tence pairs, the development set was nist02 (878 sen-tences), and the test sets are were nist05 (1082 sen-tences), nist06 (1664 sentences) and nist08 (1357 sentences).

Four baselines were used. The first two were the conventional state-of-the-art translation systems, phrase-based and hierarchical phrase-based systems, which are from the latest version of well-known Moses (Koehn et al., 2007) and are respectively de-noted as Moses and Moses-hier. The other two were neural machine translation systems implemented us-ing the open source NMT toolkit (Bahdanau et al., 2014): 4 left-to-right NMT ( NMT-l2r ) and right-to-left NMT ( NMT-r2l ). The proposed joint model ( NMT-J ) was also implemented using NMT (Bah-danau et al., 2014).

We followed the standard pipeline to train and run Moses. GIZA++ (Och and Ney, 2000) with grow-diag-final-and was used to build the translation model. We trained 5-gram target language models using the training set for J P -E N and the Gigaword corpus for C H -E N , and used a lexicalized distortion model. All experiments were run with the default settings except for a distortion-limit of 12 in the J P -E To alleviate the negative effects of randomness, the final reported results are averaged over five runs of MERT.

To ensure a fair comparison, we employed the same settings for all NMT systems. Specifically, except for the maximum sequence length (seqlen, which was to 80), and the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2014) for all NMT-based systems: the dimension of word em-bedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, and the beam size for decoding was 12. Training was conducted on a single Tesla K80 GPU, and it took about 6 days to train a single NMT system on our large-scale data. 4.1 Results and Analysis on the J P -E N Task In  X  1, it was claimed that NMT generates unbal-anced outputs. To demostrate this, we have to eval-uate the partial translations, which is not trivial (Liu and Huang, 2014). Inspired by (Liu and Huang, 2014), we employ the idea of partial B LEU rather than potential B LEU , as there is no future string concept during NMT decoding. In addition, since the lower n -gram (for example, 1-gram) is easier to be aligned to the uncovered words in source side, which might negatively affect the absolute statis-the metric to evaluate the quality of partial transla-tions (both prefixes and suffixes). In Table 1, we can see that the prefixes are of higher quality than the suffixes for a single left-to-right model (NMT-l2r). In contrast to this, it can be seen that our joint model (NMT-J) that includes one left-to-right and one right-to-left model, successfully addresses this issue, producing balanced outputs.
 Table 2 shows the main results on the J P -E N task. From this table, we can see that, although a sin-gle NMT model (either left-to-right or right-to-left) comfortably outperforms the Moses and Moses-hier baselines, our simple NMT-J (with one l2r and one r2l NMT model) obtain gains of 1.5 B LEU points over a single NMT. In addition, the more power-ful joint model NMT-J-5, which is an ensemble of five l2r and five r2l NMT models, gains 0.7 B LEU points over the strongest NMT ensemble NMT-r2l-5, i.e. an ensemble of five r2l NMT models. The en-semble of joint models achieved considerable gains of 5.6 and 4.8 B LEU points over the state-of-the-art Moses and Moses-hier, respectively. To the best of our knowlege, it is the first time that an end-to-end neural machine translation system has achieved such improvements on the very challenging task of J P -E N translation. One might argue that our NMT-J-5 contained ten NMT models in total, while the NMT-l2r-5 or NMT-r2l-5 only used five models, and thus such a com-parison is unfair. Therefore, we integrated ten NMT models into the NMT-r2l-10 ensemble. In Table 2, we can see that NMT-r2l-10 is not necessarily better than NMT-r2l-5, which is consistent with the find-ings reported in (Zhou et al., 2002). 4.2 Results on the C H -E N Task Table 3 shows the comparison between our method and the baselines on the C H -E N task. 7 The results were similar in character to the results for J P -E The proposed joint model (NMT-J-5) consistently outperformed the strongest neural baseline (NMT-l2r-5), an ensemble of five l2r NMT models, on all the test sets with gains up to 1.4 B LEU points. Furthermore, our model again achieved substantial gains over the Moses and Moses-hier systems, in the range 1.9  X  5.2 B LEU points, depending on the test set. Target-bidirectional transduction techniques were pioneered in the field of machine translation (Watan-abe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). They used the techniques for traditional SMT models, under the IBM framework (Watanabe and Sumita, 2002) or the feature-driven linear models (Finch and Sumita, 2009; Zhang et al., 2013). However, the target-bidirectional techniques we have developed for the unified neural network framework, target a pressing need directly motivated by a fundamental issue suffered by recurrent neural networks.

Target-directional neural network models have also been successfully employed in (Devlin et al., 2014). However, their approach was concerned with feedforward networks, which can not make full use of rich contextual information. As a result, their models could only be used as features (i.e. submod-els) to augment traditional translation techniques in contrast to the end-to-end neural network framework for machine translation in our proposal.

Our approach is related to that in (Bengio et al., 2015) in some sense. Both approaches can allevi-ate the mismatch between the training and testing stages: the history predictions are always correct in training while may be incorrect in testing. Bengio et al. (2015) introduce noise into history predictions in training to balance the mistmatch, while we try to make the history predictions in testing as accurate as those in training by using of two directional models. Therefore, theirs focuses on this problem from the view of training instead of both modeling and train-ing as ours, but it is possible and promising to apply their approach to optimize our joint model. In this paper, we investigate the issue of unbalanced outputs suffered by recurrent neural networks, and empirically show its existence in the context of ma-chine translation. To address this issue, we pro-pose an easy to implement agreement model that extends the method of (Liu et al., 2016) from sim-ple sequence-to-sequence learning tasks to machine translation.

On two challenging J P -E N and C H -E N transla-tion tasks, our approach was empirically shown to be effective in addressing the issue; by generating balanced outputs, it was able to consistently outper-form a respectable NMT baseline on all test sets, delivering gains of up to 1.4 B LEU points. To put these results in the broader context of machine trans-lation research, our approach (even without special handling of unknown words) achieved gains of up to 5.6 B LEU points over strong phrase-based and hier-archical phrase-based Moses baselines, with the help of an ensemble technique.
 We would like to thank the three anonymous review-ers for helpful comments and suggestions. In addi-tion, we would like to thank Rico Sennrich for fruit-ful discussions.

