 Recommender systems are intende d to assist consumers by making choices from a large scope of items. While most recommender research focuses on improving the accuracy of recommender algorithms, this paper stresses the role of explanations for recommended items for gaining acceptance and trust. Specifically, we present a method which is capable of providing detailed explanations of recommendations while exhibiting reasonable prediction accuracy. The method models the users X  ratings as a function of th eir utility part-worths for those item attributes which influence the users X  evaluation behavior, with part-worth being estimated through a set of auxiliary regressions and constrained optimization of their results. We provide evidence that under certain conditions the proposed method is superior to establis hed recommender approaches not only regarding its ability to provid e detailed explanations but also in terms of prediction accuracy. We further show that a hybrid recommendation algorithm can rely on the content-based component for a majority of the us ers, switching to collaborative recommendation only for about one third of the user base. H.3.3 [ Information Storage and Retrieval ] : Information Search and Retrieval  X  Information filtering, Se arch process, Selection process. H.2.8 [ Database Management ] : Database applications  X  Data mining.
 Algorithms, Design, Human Factors, Measurement Recommender systems, explanati on of recommendations, user preferences, constrained optimization, hybrid algorithms Stimulated by the Netflix Prize Competition, recommender research has focused on recommender algorithms accuracy, whereas topics of recommender acceptance and trust received less attention [14]. Although movie res earch provides evidence that movie characteristics such as stars and budgets significantly influence the movie success as a result of consumers X  preferences for them [11], such characteristics were not adequately handled by recommender researchers. 1 We argue that incorporating such item characteristics in the recommendation process can be fruitful, as it allows recommender recommendations [15], which will increase recommender transparency and credibility, two established performance criteria [6, 16, 19]. We further argue that explanations can lead to higher choice efficiency [22] and even satisfaction [2] with the recommendations. This argument is consistent with Aksoy et al. [1] who show that consumers ma ke better choices when using recommendation agents which use attribute weights and decision strategies that are similar to their own. We present a method that extracts user attribute-related preferences from movie rating data of a commercial movie recommender system and show that the derived preference information is suitable not only for providing users with meaningful explanations of recommendations, but also for generating reliable recommendations. Adding to recent developments on hybrid recommenders [4, 5], our method combines the extracted preference-related information with traditional collaborative techniques. When preferences towards movie attributes were used in extant work (eg. [23]), the choice of the attributes was based on information availabi lity, not a thorough study of relevant attributes. Movie attributes we re used for post processing of predictions, but were not directly involved in the process of recommendation generation (eg. [21]). We develop and evaluate our me thod with data from the movie recommender platform Moviepilot.c om. We preferred this data over Netflix because it does no t suffer from artifacts based on scale and interface changes which are known for the Netflix data [13] and is newer, encompassi ng ratings provided between August 2006 and April 2008 which shoul d adequately reflect contemporary consumer attitudes and behaviors. Also, our cooperation with Moviepilot gave us complete insight into the processes and algorithms underlying the data. The raw dataset contains 1,389,749 ratings of 15, 593 movies by 9,788 users of the platform. Although Moviepilot.com presents rati ngs in its user interface on the scale varying from 0 to 10 points in .5 steps, ratings are stored in the database as integer numbers from the interval between 0 and 100 (i.e., a rating of 7.5 point is stored as 75). We left out the six latest ratings for validati on purposes and six randomly drawn ratings for each user as a holdout for out-of-sample predictions; users for whom there was not enough data to generate the both holdouts were discarded. Both holdouts comprise of 47,610 ratings each. The data about movie attributes (genres, year of production, country of origin, budget, admissions, box office, acting stars, directors, writer s and production companies) was taken from the Internet Movie Database (IMDb, see www.imdb.com). In the context of movie recomm endations ratings determine the value of a particular movie for th e user and allow comparisons of users X  liking of different movies. The ratings can be interpreted as a normalized utility, which allows comparing item utilities between users and makes normalization unnecessary. We model the rating r from user u for a movie i as an inner product of the binary vector of movie features m and the vector of users X  part-worths p , as shown in Equation 1: Here the movie ratings and movi e features vector are known from the users X  rating records and the IMDb. The mean movie rating (  X  ) serves as baseline on which the part-worths are centered. The vector p is to be estimated. Once estimated, the part-worths can be used both for predictions of the us er X  X  ratings to new items and for providing the explanations to recommendations. Moreover, the explanations can be presented in a  X  X ros-and-cons X  style, such as  X  X itanic is recommended to you because it matches your preferences highly. Pros: High budget Hollywood movie. Cons: you don X  X  like the movie X  X  drama genre and its star Leonardo Di Caprio. Taking these factors into account, we expect that you will rate this movie 8 out of 10. X  This simple model of user preferences does not account for the effects which occur independentl y of user-item interactions. Specifically, some users give higher/lower average ratings than the average user, something we refer to as user bias. Also, some movies generally receive higher/lo wer ratings than others (item bias [3, 13]). Users also differ in their reaction to average movie ratings; while some users adapt to mainstream judgments, others react overly positive, and a third group reacts skeptical. Incorporating these effects leads to Equation (2): respectively. User and item bias are defined as deviations of a user X  X  and a movie X  X  mean ratin g value from the overall mean  X  , respectively. The users X  reactions to the movie bias are captured by scale factor s u . We also consider the changes of movie popularity as well as the individual users X  changing prefer ences and rating behavior over time [13]. Specifically, we incorporated temporal dynamics for each of the biases. We replaced b u by [ b u +  X  constitutes only the static part of the user X  X  rating, t is time, and  X  is the slope of the user X  X  rating trend, The movie bias b replaced analogously by [ b i +  X  i t ] which leads to Equation (3): The scarcity of data is the biggest challenge for recommender research. We suggest a combination of statistical and optimization techniques for parameter iden tification. The procedure encompasses two steps, estimation and optimization. The optimized parameter values are then used for predicting movie ratings which are new to the users, and for explaining these predictions both in  X  X eyword X  and  X  X nfluence X  as well as in  X  X ro-and-con X  style. We also report the results of a post-hoc integration of our model with traditional algorithms into a hybrid recommender to further increase prediction accuracy. For each parameter the initial value and its confidence interval are estimated through univariate OLS regression analysis. We utilize OLS regression, as it provides inferences about parameter significance. The latter info rmation is used for dropping parameters that are statistically meaningless for describing users X  movie preferences and for generating and explaining rating predictions. For example, if the parameter for star  X  X eorge Clooney X  does not reach significan ce, this actor is considered neutral for the user X  X  movie preference formation and can be excluded from the estimation process (p &lt; .10 was used as cut-off criterion). With regard to the user bias parameters b u and  X  u , we run a simple trend parameter a u is derived directly fro m this regression, the baseline b u is taken from b X  u by subtracting the overall rating mean, i.e. b u = b X  u  X   X  . After experimenting with different time frames we found that setting t = one day produced good estimates when letting the standard deviation of th e user X  X  rating time be at least 60. In other words, we require the user to have been rating the her drifting rating behavior. For the users who do not meet this condition, b X  u is the mean of the corre spondent user X  X  ratings. The item biases are estimated in the same way, using auxiliary regressions of the form r u,i = b X  i +  X  i t. Again, the time resolution is here set to one day. In contrast to user bias, we expect movie popularity to change slower a nd thus require the time frame Please note that we tested also for short-term changes [12], but found none. We also tested for but found no temporal dynamics in the user rating scaling factor as well as in the user part-worths. This might be the resu lt of the relatively short time frame of ratings covered by the da ta set. As a result, we did not consider any of those effects in the empirical estimation process. between a movie X  X  first and last ratings to be at least 240 days. Auxiliary regressions were also used to estimate the user part-worths. However, we have to d eal with concurrent parameters, where two shortcomings of OLS regressions surface: its sensitivity toward model misspecifications and its tendency toward overfitting under certain conditions. To avoid overfitting we discard information which has non-zero values in less than five percent of rated items. Ov erestimation of the parameter values due to model misspecifi cation was prevented by multi-level correction of the auxiliary regression results. The estimate for the scale factor s u , which reflects the user X  X  reaction to the movie bias can then be calculated by fixing all remaining model parameter at their estimated values in Equation (4):  X   X   X   X   X  X  X   X , X   X  X  X  X  X   X   X   X   X   X  X  X  X   X   X   X   X   X /  X   X   X   X  X   X   X   X   X  standard deviation and t drawn from the Student X  X  t-distribution for p = .10 and degrees of freedom equal to the number of user X  X  ratings minus one. We then performed an optimization of the model parameters, allowing them to vary inside their respective confidence intervals we have obtained in the estimation step. Initialized with the point estimates, Equation (3) fits to the test data with an RMSE of 24.67. Whereas the point estimate s represent the most probable values of the model parameters, th ey are not necessarily the  X  X rue X  values. Finding these  X  X rue X  va lues constitutes an optimization problem, which we solve through the conjugate gradient method for multiple dimensions [17]. We modify this method so that the parameter values are not allowed to exceed their confidence limits. In order to prevent overfitting parameter learning is stopped when the error on the holdout data increases. Using the parameter values gained through this procedure results in a RMSE of 24.17, which represents an a ccuracy improvement of about 2 per cent. This improvement is significant at p &lt; .05. The non-zero RMSE of our met hod indicates that Equation (3) does not capture all the user ratings variance. An inspection of the absolute deviations of our predictions from the test ratings revealed that a considerable part of the overall error stems from a small number of data points. Ta ble 1 summarizes the distribution parameters of the absolute error. Table 1. Distribution Parameters of the Absolute Prediction The high curtosis (over 2) and relatively low standard deviation indicate that the distribution is peaked and positive skewed. Further, the absolute prediction erro r for particular ratings is lower than the value of the RMSE and exceeds it in only about 30% of the cases. This means that the RMSE value mainly constitutes from a low number of points with large deviations, rather than from large number of points with nearly equal deviations. Although most of the data points w ith large deviations belong to the same group of users, we were unable to find patterns which would allow us to identify users with high prediction error a priori. Thus, those users form their movie preferences using information not captured by th e preference function shown in Equation (3). We assume that similarity among such  X  X roblematic X  users is an appropriate information source fo r generating predictions when the explicit preference modeli ng not adequately captures the rating behavior. To test this a ssumption, we implemented a user-to-user collaborative filtering algo rithm and performed a series of tests. Results show that the erro r distribution of the collaborative filter significantly differs from the one produced by Equation (3) (p &lt; .01) which indicates that both algorithms capture different parts of user ratings X  variance . Consistent with this, both approaches produce unequal errors for most users (p &gt; .1) on the single user level. As combinations of concurrent methods outperform the best individual predictions [7, 8, 20], we developed a hybrid approach which combined the predictions of both methods. We choose the individual predictor which performs best on the withheld data for each user and utilized an additional holdout set to compare the individual performance of the two prediction methods. The best performing method is determined through a t-test (two-sided) for paired samples for the significan ce level of p &lt; .10. If the collaborative filter significan tly outperforms Equation (3) for a user, it is used for generating hi s or her predictions; Equation (3) is used in all other cases. The overall RMSE of the hybrid method is 20.66, which constitutes a 16% improvement over Equation (3) used solely and a 10% improvement over the collaborative filter. It should be noted that the latter method was used for only 34% of the users, while the majority of the users (66%) received detailed explanation for the recommended items in  X  X eyword X ,  X  X nfluence X  and  X  X ros-and-cons X  styles. As the employed dataset has unique characteristics, we implemented some of the state-of-the-art recommenders and run them on our training data for comparability reasons. Specifically, we used the pure user-to-user k-m eans collaborative filter [12, 18] and the Singular Value Decomposition-like matrix factorization algorithm ( X  X VD X ) by Funk [9], the foundation for all matrix factorization recommenders. As ma trix factorization is known to provide the best predictive accuracy for a single algorithm, we suggest that a comparison to their basis algorithm to be informative. The factor model of  X  X VD X  is learnt for the dimensionality of 200. The predic tive accuracy (RMSE) of these algorithms is measured using the same data. Results are presented in Table 2. Table 2. Comparison of the Prediction Accuracy of Different Our models are denoted as  X  X  ure X  (estimation model) and  X  X ptimized X  (optimization model) as well as  X  X ybrid X  (combination of  X  X ptimized X  a nd collaborative filter). The  X  X ybrid X  is found to be the most accurate of the considered methods, followed by  X  X VD X ,  X  X ollaborative Filtering X ,  X  X ptimized X  and  X  X ure X . The difference in accuracy of 4% between the two best performi ng methods is substantial and significant (p &lt; .10). We find th is particularly notable, as the Hybrid algorithm outperforms state-of-the-art recommender algorithms in terms of prediction accuracy, while also providing the majority of users with explanations at the most detailed level. Even the most accurate recommendation algorithm is subject to prediction errors. Hence an explana tion facility should be made an integral part of recommender syst ems which help users to make better choices. Our proposed hybrid method outperforms both collaborative filtering and matrix factorization approach in terms of predictive accuracy, while provi ding all users with explanations of the reasoning behind recommendations. However, for the smaller fraction the users the expl anations are given without such detail. This may be due to th at our proposed model fails to adequately capture the preference structure and/or item evaluation behavior for a number of users which might point at missing item attributes. Future research directions which we would like to explore affect primarily the modeling side of our method, such as extending of the list of item attributes and adding interaction terms. The algorithmic part might also bene fit from robust techniques for mitigating overfitting and by improved handling of part-worths multicollinearity. Further, it seems reasonable to employ some similarity-based techniques for increasing of the users X  representation through imputation of the part-worths. We believe that these improvements are capabl e of achieving the overall best prediction accuracy while providi ng all users with the motivated explanations at the highest detail level. The authors thank Tobias Bauckhage for providing the data for our tests, Denis Rechkin for many fruitful discussions, and three anonymous reviewers for their helpful comments. [1] Aksoy, L., Bloom, P.N., Lurie, N.H., and Cooil, B. Should [2] Ariely, D. Controlling the Information Flow: Effects on [3] Austin, B.A. Immediate Seating -A Look at Movie [4] Bao, X., Bergman, L., an d Thompson, R. Stacking [5] Burke, R. Hybrid Web Recommender Systems. In P. [6] Cramer, H., Evers, V., Ramlal, S., et al. The Effects of [7] Elliott, G. and Timmerma nn, A. Optimal forecast [8] Fildes, R. and Ord, K. Forecasting Competitions: Their Role [9] Funk, S. Netflix Update: Try This at Home. 2006. [10] Gunawardana, A. and Meek, C. A Unified Approach to [11] Hennig-Thurau, T., Houston, M.B., and Walsh, G. The [12] Herlocker, J., Konstan, J., Bo rchers, A., and Riedl, J. An [13] Koren, Y. Collaborative filtering with temporal dynamics. [14] McNee, S.M., Riedl, J., and Konstan, J.A. Being Accurate is [15] McSherry, D. Explanation in Recommender Systems. [16] O'Donovan, J. and Smyth, B. Trust in Recommender [17] Press, W.H., Teukolsky, S. A., Vetterling, W.T., and [18] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. Analysis [19] Sinha, R. and Swearingen, K. The Role of Transparency in [20] Stock, J.H. and Watson, M.W. Forecasting Inflation. Journal [21] Symeonidis, P., Nanopoulos, A., and Manolopoulos, Y. [22] Tintarev, N. and Masthoff, J. A Survey of Explanations in [23] Ying, Y., Feinberg, F., and We del, M. Leveraging Missing 
