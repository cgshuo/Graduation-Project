 With recent advances in the field of music information retrieval, we face a new possi-bility that music can be automatically analyzed and understandable by the computer to some semantic level. Due to the diversity and richness of music content, many from computer science, digital signal processing, mathematics, and statistics applied to musicology. Most traditional content-based music retrieval (CBMR) techniques spectrum, and etc. However, these features were not enough to give semantic infor-mation of music contents, so that these gave limitation in retrieving and recommend-ing appropriate music to people. 
Due to the abovementioned limitations of low-level feature-based music retrieval, some researchers have tried to bridge the semantic difference, which is also known as semantic gap, between the low-level features and high-level concepts such as human emotion and mood [3, 4]. With low-level feature analysis only, we might experience many difficulties in identifying the semantics of musical content. Similarly, it is diffi-cult to correlate high-level features and the semantics of music. For instance, a user X  X  profile, which includes educational background, age, gender, and musical taste, is one possible high-level feature. Semantic web technology is considered to be one promis-ing method to bridge this semantic gap. Also, Emotional Effects of Music work [15] suggested that an emotion is experienced by a listener while listening to music is determined by a multiplicative function consisting of several factors such as struc-tural, performance, listener, and contextual features. We adapt some features to our emotion reasoning model for inducing his/her emotion when he/she in placed. 
Therefore, in this paper, we try to tackle the abovementioned problem in the do-main of music recommendation by combining content-based music retrieval, music ontology and domain specific ontology for Mood and Situation. More specifically, we define more specific domain-specific ontology based on the basic concepts from the upper ontology which can be found in the previous ontology-related projects such as Music Ontology [5, 6], Kanzaki taxonomy [7], and MusicBrainz [8]. In our scenario for music recommendation, we noted musical terms as concepts. and  X  X as-a. X  We deal with these two formal relations to indicate the specialization of concepts and required parts. For example, mood, genre and music features are sub-class of music and MFCC, tempo, onset, loudness and chroma are member of music feature. The other important relation is  X  X ike-song/singer/genre/mood X . This relation is relationship between music and user X  X  preference. In order to provide music recommendation service intelligently, we need a set of common ontologies such as person, time, location, and etc, for knowledge sharing and reasoning. We have developed ontology including music and its related ontologies in the music recommendation domain. We use the W3C recommendation ontology lan-guage, OWL (Web Ontology Language) to represent ontology. 
We adopt basic concepts and relations from previous work -the Music Ontology; we expand it to include additional features such as musical feature, genre, instrument taxonomy, mood, and situation. We serialize this ontology in OWL so that we can retrieve some information using SPARQL query language. 2.1 COMUS Ontology COMUS (Context-based Music Recommendation) ontology consists of about 500 classes and instances, and 52 properties definitions. Fig. 1 shows a graph representation of some of key COMUS ontology definition. This ontology describes music related ing, spring), and situation events (e.g., waking-up, driving, working) in a daily life. 
The key top-level elements of the ontology consist of classes and properties that describe Person, Situation, Mood, Genre and Music classes. year, artists, genre, and musical features (e.g., MFCC, Tempo, Onset, Chroma, Seg-ment, Spectra Centroid, Spectra Flux, Spect ra Spread, and Zero Crossing Rate). Genre. The  X  X enre X  class defines the category of music. There have been many re-searches for the music genre classification. There exist several popular online systems such as All Music Guide, MusicBrainz and Moodlogic for annotating popular music genre and emotion. We create our own genre taxonomy based on All Music Guide [11] along with second level of industry taxonomy. Person. The  X  X erson X  class defines generic properties of a person such as name, age, gender, hobby, socioeconomic background (e.g., job, final education) and music re-lated properties for music recommendation such as musical education, favorite music, genre and singer. Mood. The  X  X ood X  class defines the state of one X  X  mind or emotion. Each mood has a set of similar moods. For example,  X  X ggressive X  has similar moods like  X  X ostile, angry, energetic, fiery, rebellious, reckless, menacing, provocative, outrageous, and volatile. X  Situation. The  X  X ituation X  class defines person X  X  situation in terms of conditions and circumstance, which are very important cl ues to effective music recommendation. Situation is described by time, location, subject, and goal to achieve. Hence, this class describes user X  X  situational contexts such as whereabouts of the user (Location), what happens to the user (Event), and so on. 2.2 Scenario: Context-Based Music Recommendation In this subsection, we will introduce a typical scenario that demonstrates how COMUS ontology can be used to support ontology reasoning for recommending appropriate music to users. Example Scenario sweet music. When he feels sad, he usually listens to the music that might help cheer him up. The date is 12 January, 2009. John woke up late in the Monday morning and he is very tired due to working late last night. But he should go to work early to pre-pare for a presentation at today X  X  meeting. Therefore, he asks the music recommenda-tion system to look up some hard and fast beat music then he listens to the music like  X  X on X  X  stop me now (Queen), X   X  X ou give a bad name (Bon Jovi) X  and  X  X ake me up before you go go (Wham). X  These kinds of beating music help him to hurry up to go to work in time. After a while , he was stuck in a traffic jam which started making him nervous on his way to work. In order to calm down that situation, he asks again the system to recommend some calm-down music of The Carpenters, Stevie Wonder and Simon &amp; Garfunkel. This scenario assumes that John has set his musical preferences such as singer, genre situation is analyzed and sent to the recommendation system. Using this information, the system will reason about John X  X  situational context and his favorite mood from the user profile information. From this information, the system recommends a bunch of music which best fits John X  X  interest and current situation. 
A more flexible reasoning can be brought in by specifying user-defined reasoning rules towards defining high level conceptual contexts such as  X  X hat music does the user want to listen to when he/she is stressed? X  can be deduced from relevant low-level context. Table 1 describes the user-defined context reasoning rules that are em-ployed to derive user X  X  mood and situation in the music recommendation scenario. 
One can ask questions about the situation by using queries. The difference between mood), we can query to music ontology to find a list of recommendable music as shown below: In this paper, we develop a prototype music recommendation system and extended music ontology to enable mood and situa tion reasoning in a music recommendation system. Our system provides various types of query interfaces to the users. The user can formulate queries using one of three different query interfaces: they are query by situation (QBS), query by detailed situation (QBDS), and query by mood (QBM). Our COMUS ontology is described in OWL language using Prot X g X  editor [9]. During the construction music ontology, we use RacerPro to check consistency of our ontology, from the partial information in the profile through finding implicit related classes. For the retrieval and recommendation, the Jena SPARQL engine is used to express and process necessary queries to our ontology. solutions to the problems in the scenario. The COMUS ontology is a collection of terms and definitions relevant to the motivating scenario of music recommendation that we described above. Thus, to build ontology, it is important to start with describ-ing the basic concepts and one or more scenarios in the specific domain of interest. These are also based on the scenarios and can be considered as expressiveness re-quirements that are in form of questions. Ontology must be able to represent these questions using its domain-related terminology and characterize their answers using the axioms and definitions. Therefore, we asked participants to answer the compe-tency questions through the online questionnaire system. trained and others are not. Participants were asked to fill out the questionnaire to col-lect suitable terms and definitions about the situation and mood. They were also re-quested to describe their own emotional state transition like current and desired emo-tions in the specific scenario. The description was based on one or more emotional adjectives such as happy, sad, angry, nervous, and excited those were collected from All Music Guide taxonomy [11]. Finally, the most frequently described adjectives were chosen to define the instances in the COMUS ontology. user satisfaction using either our proposed COMUS ontology or AMG Taxonomy in our system. The procedure for the experiment was as follows. 1) The experimenter explained the procedure and the purpose of the experiment and demonstrated how to run our music recommendation system. 2) The participant should describe his profile (e.g., musical preferences) using web form interfaces such as buttons, textbox, checkbox and selection list. desired emotion or just select the predefined scenario using the query interfaces. 4) Then, the system returned recommended songs based on the ontology reasoning and the participant X  X  profile. Then the participant judged which one was appropri-ate for their current emotion. Participants chose one of the 5 point rating scales (from 1  X  strongly unsatisfied to 5  X  strongly satisfied). 5) Finally, all the participants were asked to fill out a questionnaire. 
As shown in Table 1, over 80% of the participants responded positively to the overall satisfaction of the system using ontology instead of AMG taxonomy. The result of the satisfaction ratings shows most of the users were satisfied with the query results recommended by the system. 
With regard to the satisfaction of the participant X  X  preferred emotional adjectives satisfactory by about 78% of the participants whereas ambiguous adjectives like nerv-ous by 43% of the participants (in the case of using COMUS ontology). In this paper, we presented ontology based context model that is feasible and neces-sary for supporting context modeling and reasoning in music recommendation We modeled musical domain and captured low-level musical features and several musical factors to describe music moods and music-related situations composed of time, loca-tion and subject to build ontology. We have constructed musical ontology based on the current music ontology as part of ongoing project on building intelligent music recommendation system. To show its feasibility, in addition, we set up a usage sce-nario and presented several queries for reasoning useful information from the ontol-ogy. We are currently working on an extended our reasoning model and ontology that support more precise music recommendation. 
