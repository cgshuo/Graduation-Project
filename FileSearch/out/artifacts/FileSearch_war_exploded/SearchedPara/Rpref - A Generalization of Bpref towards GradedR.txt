 We present rpref ; our generalization of the bpref evalua-tion metric for assessing the quality of search engine results, given graded rather than binary user relevance judgments. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performanceevaluation(efficiencyandef-fectiveness) Theory, Measurement
User relevance judgments are an essential ingredient to most evaluation metrics for information retrieval (IR), as the quality of results is inherently subjective. In practice however, this dependency seriously hampers the scope of evaluation, as the judgment is a time-and painstaking ac-tivity. For this and related reasons (most noticeably re-liability), one aims at metrics that are robust in the face of incomplete and imperfect judgments. With incomplete judgments, not all documents are judged. Imperfect judg-ments include documents no longer part of the collection. Both especially apply to dynamic document collections.
For a motivation of using graded rather than binary rel-evance judgments, we refer to the work of Kek  X  al  X ainen and J  X arvelin ([5]), Gordon and Pathak ([4]) amongst others. As stated by the former authors:  X  X n modern large database environments, the number of topically relevant documents to a request may easily exceed the number of documents a user is willing to examine. It would therefore be desirable from the user viewpoint to rank highly relevant documents highest in the retrieval results and to develop and evalu-ate IR methods accordingly. X  The authors continue by pre-senting graded adaptations of precision/recall metrics. We pursue their line of work by proposing rpref as a general-ization of the robust, ranking-based evaluation metric bpref introduced by Buckley et al. [1]. 1
Both bpref and rpref can be categorized as preference-based metrics ([3]). The key idea behind them is to evalu-ate the preference ordering as derived by the search engine by comparing it to an assessor ordering, which serves as a golden standard. The comparison is based on a counting of  X  X isplaced X  documents; judged irrelevant documents that are ranked higher than any judged relevant documents (as for bpref ), generalized for rpref to weighted counts of judged less relevant documents that are ranked higher than judged more relevant documents.

Completely in line with Kek  X  al  X ainen and J  X  arvelin X  X  argu-ment, one could motivate these metrics from a practical point of view. Users aim to satisfy their information needs as efficient and effective as possible. As the engine X  X  ranking tells them the first documents in the list are presumed to be most relevant, they typically consult the documents in the order indicated. Any misplaced documents thereby create a distraction from finding what they are looking for.
In view of a submitted request, let  X  k  X  N 0 be the absolute ranknumber of a document D k containedina document collection C . The ranking defines the preference ordering as determined by the search engine through the equivalence re-lation  X  . Whenever a ranking cutoff point c is imposed, 2 lets  X  k =  X  for each document D k beyond c .Thismakes the ordering totally defined over C , rendering pooling tech-niques effective (cf. TREC  X  Text REtrieval Conference).
Backed by the robustness towards incomplete and imper-fect judgments ([1]), there is some freedom in the selection and the number of documents to be judged by assessors in the formation of a golden standard. We refer to this selec-tion as the sampleset S  X  X  . However, as stated in [1], a necessary but sufficient condition for the composition of S is that all documents in S are selected independently of their judgedrelevance  X  k  X  [0 , 1], with 0 being completely irrelevant, and 1 perfectly relevant.
In the computation of rpref (1), a weighted sum is made over all judged documents (all D k  X  S ). The weight equals the document X  X  relevance  X  k , such that misplaced docu-ments with regard to highly relevant documents introduce a correspondingly larger penalty. The inner sum computes for each D k an accumulated misplacementpenalty ,consider-ing the relative deviation with every less relevant document that is ranked higher by the search engine. Notice the inner sum X  X  normalization by the constant N to avoid counting absolute numbers. The final division by R normalizes the score to the unit interval. The higher the score, the better the preference ordering by the search engine is.
In the special event of binary judgments  X  k  X  X  0 , 1 } ,(1) reduces to one of the variants of bpref (2). Firstmost, R N will simply denote the total number of judged relevant, respectively irrelevant documents. Second, all misplacement penalties of the form  X  k  X   X  l  X  k will be 1, such that the inner sum reduces to a simple count of misplaced documents for D k . Lastly, weights  X  k of the outer sum can be safely removed when iterating only over relevant documents.
One could think of different variants for both (1) and (2) regarding the normalization of terms. The depicted absolute normalization variant mitigates the smaller penalties accu-mulated by front documents through the constant denomi-nator N , which accommodates the largest possible accumu-lated penalty of tail D k . Therefore, a more appropriate, rel-ative denominator might be the number of preference pairs for D k ,givenby |{ D l  X  S :  X  l &lt; X  k }| .
Both bpref and rpref are dependent on the relative order-ing of the assessed documents only. No other information can be gleaned from these metrics. In particular, the result-ing scores give no indication of effectiveness in answering the search request. Combination with other metrics there-fore remains valuable.

There is a case for omitting the highest ranked document from the outer sum in both (1) and (2), as comparison to higher ranked documents is futile. Moreover, relative nor-malization consistently produces 1  X  0 0 for this document. Adding the assumption of representative (unbiased) sample sets, the relative normalization variant becomes safe for av-eraging over requests. Scores are no longer directly related to the somewhat arbitrary number of non-(or less-) relevant documents judged for the request. Instead, all preference pairs contribute to the score in relative terms.

Eventhough expected to hold (by analogy), it is yet to be verified experimentally whether the acclaimed robustness for bpref ([1]) gracefully extends to rpref .
 Figure 1: Demonstration orderings and scores.

We now demonstrate the stepwise generalization of bpref to rpref on real-life data. The example involves three market-leading search tools blinded as T 1 , T 2 and T 3 ,evaluatedfor a particular request against a document collection C com-prising half a million of police case reports ([2]).
Figure 1 shows our sample set S with indicated relevance judgments  X   X  [0 , 1]. The figure then shows the order of these documents in the tools X  result lists. The additional columns show the documents grouped around chosen rele-vance threshold values in two ( A and B , split around 0.5), respectively four partitions ( A 1 , A 2 , B 1 and B 2 , split around 0.25, 0.5, and 0.75). Continuing this process iteratively, the binary preference ordering as used for bpref (two partitions) is recursively refined until one recovers the original ordering as used for rpref (at most | S | non-empty partitions).
The figure concludes with a graph plotting the thus ob-tained n -pref scores ( n partitions), using the relative nor-malization variant. Notice the convergence towards the rpref score with increasing n . The example demonstrates that al-though the set of misplaced document pairs grows monotoni-cally with each step, the score itself does not necessarily con-verge monotonically. This is because in each step misplace-ment penalties are computed more precisely, both within and between partitions of the previous step.
The authors thank the Belgian police for their interest and active collaboration, in particular Kris D X  X oore, Mar-tine Pattyn and Paul Wouters. They also thank their re-search partners Jan Vanthienen and Nishant Kumar. This work was supported by the Belgian Science Policy Office. ag/01/101 . http://www.belspo.be/agora
