 We present a novel approach for efficiently evaluating the performance of retrieval models and introduce two evalua-tion metrics: Distributional Overlap (DO), which com-pares the clustering of scores of relevant and non-relevant documents, and Histogram Slope Analysis (HSA), which examines the log of the empirical distributions of relevant and non-relevant documents. Unlike rank evaluation met-rics such as mean average precision (MAP) and normalized discounted cumulative gain (NDCG), DO and HSA only re-quire calculating model scores of queries and a fixed sample of relevant and non-relevant documents rather than scoring the entire collection, even implicitly by means of an inverted index. In experimental meta-evaluations, we find that HSA achieves high correlation with MAP and NDCG on a mono-lingual and a cross-language document similarity task; on four ad-hoc web retrieval tasks; and on an analysis of ten TREC tasks from the past ten years. In addition, when evaluating latent Dirichlet allocation (LDA) models on doc-ument similarity tasks, HSA achieves better correlation with MAP and NCDG than perplexity, an intrinsic metric widely used with topic models.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models, Selection pro-cess Keywords: efficient evaluation; retrieval models; topic mod-els Evaluating retrieval models with ranking metrics, such as mean average precision (MAP) and normalized discounted cumulative gain (NDCG), requires computing, in the worst c  X  documents (We discuss below the relation to other such  X  X luster hypotheses X .). Better retrieval models, under this hypothesis, will have a relatively small overlap between the scores of relevant and non-relevant documents. We use the volume of the distributional overlap between the histograms of query relevant and non-relevant documents to define our first evaluation metric X  X istributional Overlap (DO).
Computing the log ratio between the histograms of the query relevant and non-relevant documents gives us the pro-portion of relevant documents that we would expect to find at certain relevance values. Across different retrieval models, as the performance of the model improves, the proportion of relevant documents found at higher relevance scores should increase thus making the slope of the histogram analysis steeper going down from higher to lower values on the rel-evance scale. We define this slope as an evaluation metric that we call Histogram Slope Analysis (HSA).

By computing DO and HSA from the full set of relevant documents and a relatively small sample of non-relevant doc-uments, we are able to evaluate retrieval models much more efficiently than with metrics computed over ranked lists of documents. We simply need to summarize the empirical distribution of relevant and non-relevant documents using histograms with some fixed number of bins. For a test collection of k queries Q = q 1 ,q 2 ,q 3 ,...q k and model X  X  scoring function Score f to get V R = Score f ( q i ,R q i ) and V NR = Score f ( q i ,NR q i ). Relevance scores have range of values that vary depending on the scoring function of the retrieval model. Different retrieval models generate rel-evance values on different scales. In order to compare and rank their performance, relevance values generated by each model are normalized to a range of [0,1].

Using a set B of equally spaced bins, histograms are com-puted over the two sets of relevance scores to give H R = [ h h b and h nr b are the counts of the number of times relevant and non-relevant scores fall within the bin centered at b : h b = #( V R  X  b ), h nr b = #( V NR  X  b ). Since in typical real document collections the portion of query relevant docu-ments is significantly smaller than non-relevant documents, we use log scale for the frequency axis when creating the two histograms. We further define a set of supported bins B 0 = { b 0 : h r the volume of the overlap between the two histograms: Figure 1 illustrates an example of computing DO. In this figure we show the joint histogram plots across two different document similarity models on the task of finding document translation pairs. Both similarity models are based on the Polylingual Topic Model (PLTM) [5] configured with num-ber of topics T=50 and 500. We detail this experimental setup in  X  5.1.

For HSA, we take the log ratio of the two histograms for the bins b 0  X  B 0 where they are both observed O b 0 = relevant documents and between all query relevant and non-relevant documents. Aside from the cluster hypothesis test, researchers have proposed other measures of the cluster hy-pothesis [7, 9]. More recently, Raiber and Kurland [6] an-alyzed how these measures correlate with the performance of cluster based retrieval. While different measures exist for the cluster hypothesis they have not found their use in evaluating the performance of different retrieval models.
Although developed independently, the DO metric is con-sistent with the cluster hypothesis. Unlike cluster hypoth-esis tests, which asks whether two relevant documents are similar to each other, with DO we analyze the similarity of relevance scores between query relevant and non-relevant documents. Since DO is reflecting on the cluster hypothesis one may also consider DO as an intuitive implementation of the cluster hypothesis test in the space of query relevance values. As we shall see, however, HSA is better correlated with established ranking metrics than DO. The purpose of developing DO and HSA was to be able to evaluate and predict the performance of retrieval models. To demonstrate this ability we compare the values of DO and HSA with existing IR metrics and evaluate their pre-dictive power by performing linear correlation using Pearson correlation coefficient ( R ). Using Spearman X  X  rank correla-tion coefficient (  X  ) we compute the correlation between the ranked list of models X  performance sorted by existing IR metrics and the ranked list obtained using DO and HSA. We demonstrate the generality of our evaluation approach across different retrieval tasks, models, and scoring func-tions, which we group into three experimental setups: (1) document similarity models where the scoring function com-putes similarity between two documents, (2) ad-hoc retrieval where the scoring functions represents the relevance score of the document given a query, and (3) a meta-evaluation of ranked lists submitted to ten TREC tracks in the past ten years. With our last experimental setup, we also demon-strate that DO and HSA can be computed using the ranks of the retrieved documents as relevance values. We first showcase DO and HSA on two document similarity tasks: prior-art patent search [10] and the cross-language IR (CLIR) task of finding document translations [4]. Both tasks use topic models to retrieve similar documents. Exper-iments were performed with 7 different topic configurations. More specifically, the prior-art patent search task uses LDA with number of topics set to T=50, 100, 200, 500, 1k, 2k and 5k. While on the CLIR task PLTMs were configured with T=100, 200, 300, 400, 500, 700 and 1k. On the patent re-trieval task, following the experimental setup of [10], model performance was evaluated using MAP computed over 372 queries and a test collection of 70k patents. In [4], perfor-mance of PLTMs was evaluated on a test collection of  X  14k English-Spanish Europarl speeches based on the percentage of true document translation pairs (out of the whole test set) that the model ranked as most similar. This metric is referred to as  X  X ercentage at rank one X  (P@1).

Table 1 shows the correlation coefficients computed be-tween our evaluation metrics and MAP and P@1. Topic models are typically evaluated intrinsically using perplexity Web Track 2009 0.89 0.91 0.95 0.84 0.88 0.94 2010 0.88 0.71 0.93 0.64 0.60 0.79 2011 0.91 0.93 0.99 0.77 0.81 0.92 2012 0.89 0.87 0.97 0.79 0.79 0.92 Web Track 2009 0.82 0.86 0.86 -0.86 -0.89 -0.89 2010 0.93 0.79 0.96 -0.46 -0.68 -0.50 2011 1.00 0.96 1.00 -0.82 -0.79 -0.82 2012 0.71 0.82 0.75 -0.46 -0.61 -0.39 computed using the top 10k retrieved documents and their relevance scores.

Across all evaluation sets, HSA has a high linear and rank correlation with all three IR metrics. While DO has a high linear correlation, its rank correlation is negative, since a large overlap between the distributions of relevant and non-relevant documents is undesirable. So far in our experiments, we have computed DO and HSA using a relatively large set of query based relevance scores (e.g. 70k patents and 10k ClueWeb documents). However, typical IR systems, across various tasks, are configured to return the top k documents, which is usually a relatively smaller percentage of all the documents in the collection. For example, across different TREC tracks, ranked lists submit-ted by participants typically consist of the top 1k retrieved documents. Relevance values obtained from such ranked lists are a very small subset on the values across the whole collection. To measure the correlation when DO and HSA are computed over such small sample sets of relevance values we used ranked lists submitted on ten TREC tracks from the past ten years (2004-2013). From each year X  X  TREC we randomly picked a track and for the selected track we randomly chose 7 submitted ranked lists. Unlike previous experimental settings where we computed DO and HSA us-ing the relevance values generated by the retrieval models, in this experimental setup we compute DO and HSA over the normalized values of the document ranks. This is due to the fact that in some instances the relevance scores in the ranked lists are not properly formatted or missing and more-over there is no information on the relevance function used. Table 5 and Table 6 show the linear and rank correlation coefficients computed across various TREC tracks. Results in these tables shown that over all ten tracks HSA has a high linear and rank correlation with MAP and NDCG. We presented two evaluation metrics, DO and HSA, that use a novel approach for evaluating retrieval models per-formance through histogram analysis. We showed that HSA
