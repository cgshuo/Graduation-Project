 This paper explores correspondence and mixture topic mod-eling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of docu-ments with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document con-tent. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sen-timent tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives -a document level tag perspective that is relevant to the docu-ment as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discover-ing the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on held-out test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) doc-ument category labels in Wikipedia pages are used to val-idate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and com-pared to a robust baseline.
 I.2.7 [ Computing Methodologies ]: Artificial Intelligence X  Natural Language Processing Algorithm, Experimentation Text mining, tag topic models, social media, classification and clustering
This paper lays down a robust topic modeling framework to solve the problem of discovering latent topics from doc-uments tagged from two different perspectives. Documents usually consist of at least two perspectives -a document level perspective and a word level perspective. The doc-ument level perspective often tries to summarize the con-tents as a small bag-of-words, while the other perspective tries to annotate the content in different ways. Tables 1, 2 and 3 show different examples of perspectives. In this pa-per, it is assumed that tags are non-hierarchical concepts. These concepts could be represented by words or by some other higher order representation that eventually denotes a concept. Many times, for e.g., in plain text documents, these two perspectives are not explicitly shown. However, documents hosted by today X  X  interactive websites are a rich source of document tagging from at least two perspectives. Table 1 shows an article on lilac flower in Wikipedia. Words in the document body annotated with a slash  X / X  denotes a word level perspective which in this case is position of the section in which the word appears. The section offsets are binned into three positions relative to the beginning of the document -begin(0), middle(1) and end(2). Positions sig-nify the importance of the choice of words that constitute sections in a document. The document level perspective is assumed to be captured by the images which are described by the corresponding captions. In table 1, the third column represents the  X  X round truth X  category labels which are a set of manually edited tags that summarize the Wikipedia article. With this structure of multimedia documents, sev-eral questions come to the forefront:  X  X ould there be some way of using image captions to automatically suggest spe-cific category labels for new articles? If so how good will those suggestions be? Further, can we discover latent top-ics or themes and label each theme with a multimedia ob-ject? X  The generative process of word generation in these kinds of documents hinges on the following intuitions: Doc-uments are distributions over latent topics. Latent topics, in turn, are distributions over observed document level tags and main content words such that the most probable ob-served variable ensembles for a topic are related through the assumptions of the generative process. For one partic-ular assumption, a word conditioned on the word level tag observed at the word X  X  position is sampled independently of the document level tags from a topic. The topic proportions for the document only depends on the expected number of document level tags and words being assigned to each topic through co-occurrence phenomenon. In another assumption, topics generate the document level tags first. Then a docu-ment level tag position is chosen and a word conditioned on the word level tag observed at the word X  X  position is sampled from the corresponding topic in the position of the document level tag. For this second assumption, there is a conditioning of words to document level tags and is thus more intuitive from the document generation point of view. For example, a writer often  X  X hinks X  of a mental image/concept and then writes words that elaborate that image/concept. Although modeling multimedia Wikipedia articles served as the pri-mary motivation for developing the proposed models, the models are extremely generic and had been applied to a va-riety of other datasets for different tasks. For brevity, the document level tags are dubbed DL tags and the word level tags are dubbed WL tags.

Table 2 shows an example where DL tags are abstracted at a level higher than words. In this example, the sam-ple sentence in the table is an excerpt from a newswire ar-ticle in the dataset used in the DUC2005 Summarization track[10]. The WL tags denote a particular named en-tity class like PERSON, LOCATION, ORGANIZATION, NUMBER, etc. being ascribed to each particular word or not. The DL tags, however, are indicative of discourse co-herence markers. These markers were constructed following the technique used in [2]. Each word in the document is as-sociated with a grammatical or semantic role (GSR in short) like named entities (ne), nouns (nn), adjectives (adj), verbs (vb), subjects (subj), objects (obj), etc. A GSR transition (GSRt in short) is a relation between the same normal form of a word that is either present in two contextual sentences or in a single sentence, e.g. a GSRt for the word  X  X ar X  can be (car, subj , obj ) leading to (car, subj  X  obj ) or (car, subj ) if the word  X  X ar X  is not seen in the succeeding sentence. It is reported in [2] that a set of sentences with the same entities in roles like  X  X ubj, X  X  X bj, X  etc. are indicative of co-herent passages. Although in [2], only entities are involved in GSRts, however, in this paper, words that are not entities are also considered since in quite a few cases, the foci of at-tentions are based not just on entities. Thus, the document level perspective for DUC05 newswire data is that of syntac-tic coherence . The proposed models are general enough to model documents arising out of such perspectives also.
In all the experiments performed in the paper, three major datasets were considered. Also, all tags have a bag-of-words representation. For Wikipedia, only documents with images were collected. The category labels were not used as DL tags, rather the image caption words were used. Generally, if captions are not available, an initial preprocessing can be done using the work in [11]. The article title was also added to the DL tags. Each word in the main body of the article was tagged with X  X osition X  X nformation of the sections they appeared in and were labeled as { Begin, Begin Middle, Middle, Middle End, End } .

Unprocessed Amazon product reviews from the dataset used in [7] (henceforth the AR dataset) was also used in the experiments. The words in each review were tagged with affect labels using a simple lexicon lookup from the dataset created in [9]. The lexicon consists of 2476 words that elicit human emotions in some form. The emotions were labeled with { Unhappy, Unsatisfactory, Melancholic, Despair, Hopeful, Contended, Satisfied, Pleased, Happy, Un-tagged } tags based on the maximum valence values of the affect words. Non-affect words were tagged as  X  Untagged  X . The AR dataset did not have product tags and hence the product name and the review title were used as  X  X aptions X  for reviews which served as DL tags. Finally note that for WL tagging, a word can be conditioned on only 1 tag .Ta-ble 3 shows an example for the AR dataset used. The DUC (Document Understanding Conference) 2005 dataset consists of newswire articles organized in 50 folders or doc-ument sets (docsets) with each folder consisting of at least 25 sizable news reports. The documents were processed to extract named entities and roles of the words using the Stan-ford CoreNLP toolkit 1 . The GSRs were obtained using the dependency parse information. However, co-reference reso-lution was not performed due to unsatisfactory results. An example of this kind of tagging is shown in Table 2. Nor-mal forms of the words were used e.g.,  X  X rrested X  (verb) and  X  X rrests X  (noun) have the normal form  X  X rrest X . A total of 9 GSRs were chosen (Named Entities or ne, Subjects or subj, Objects or obj, Nouns or nn, Verb or vb, Adjective or adj, Adverb or adv, Other as ow and Null as  X ) result-ing in a total of 81 GSRts. Note that if a word has several GSRs associated with it, only one is chosen using the pri-in the DUC2005 Summarization task was the creation of 250 word multi-document summaries for each of the docsets in response to the corresponding information needs. How-ever, in this paper the docsets from DUC2005 dataset were used to validate entity-pair relationship discovery and not for summarization.
Tag-topic models have been explored recently [16, 17, 1, 20] as ways of improving word-based topic models with ad-ditional information in the form of tags, usually arising out of a single perspective. Existing mixture tag topic mod-els [16, 17] (c.f. fig. 1b) can fit a number of latent topics to the documents without words and DL tags having di-rect correspondence with each other. However, there could be additional word level annotation information which are implicitly attributed to the words.

Thus existing tag topic models of documents focus either on document level tags[16, 17] or on word level tags[20] (c.f. fig. 1c). For models like those in [16, 17] (which are re-ferred to as MMLDA -short for Multi(nomial) Multinomial LDA) each content word and DL tag is generated indepen-dently by choosing a topic and then choosing a content or DL tag. The expected number of words in the document X  X  topic thus depends on the counts of both the content and the DL words ascribed to that topic. On the other hand, in the TagLDA model in [20] the content words are gen-erated by choosing a topic and drawing a word from the topic X  X  distribution but conditioned on the WL tag associ-ated with that content word. This conditional aspect allows one to explore related words sharing the same semantic re-latedness but along that particular condition or facet -for e.g. all  X  X ERSON X  named entities in a dataset that are se-mantically related through some hidden topic. Figures 1b and 1c show the existing tag topic models -MMLDA[16] and TagLDA[20]. The model in figure 1d was implemented in this study as an improvement over MMLDA following [4] for the text domain and is referred to as corrMMLDA. How-ever, none of MMLDA, TagLDA or corrMMLDA addresses a tag space that is split across two different perspectives. The proposed TagSquaredLDA (abbreviated as Tag 2 LDA) models: METag 2 LDA model (ME is abbreviated form of Multinomial Exponential) (fig. 1e) and corrMETag 2 LDA model (fig. 1f) allows topic modeling of documents with both DL and WL tags. Table 4 shows the relative merits and de-merits of each model discussed in this paper. Fig. 1a shows a supervised LDA topic model (sLDA)[5] that is only used for predictive power comparison on the AR dataset. Experiments reveal the improvements of the Tag 2 LDA mod-els over current tag topic models through reduced perplex-ity, or more predictive power for topical inference. Also an HMM type of model is not suitable for positional WL tag-ging, since, then during inference, there is nothing to infer on position  X  X tates X  -they are implicit in any document.
Measuring model perplexity[6] is an established way of showing how good a model explains the observed data. Due to intractability issues, the lower bounds to the true log like-lihoods on test data are also used which are directly propor-tional to perplexity measurements. However, while applying the models for a specific task, the goal is not only to mea-sure held-out test data likelihood for a model. For example, for the Wikipedia data, it was important to have a quan-titative measure of confidence between probable document tags from image captions and ground truth category labels. For this, a measure of semantic relatedness using path sep-aration between concept pairs[14] in WordNet ontology was chosen as an evaluation tool. As an example, the connec-tion between  X  X ire engines X  and  X  X ire extinguisher X  can be described by a shortest path linking these two concepts in WordNet as  X  fire extinguisher  X  device  X  instrumental-ity  X  container  X  wheeled vehicle  X  self-propelled vehi-cle  X  motor vehicle  X  truck  X  fire engine  X  with a path length of 9 and a simple  X  X nverse of path length X  similarity score of 0 . 11. Under this measure, a value of 1 indicates exact match or parent/child relationship. Using this eval-uation, users can be explained a  X  X hain of reasoning X  that relates a probable DL tag to a ground truth category label for a new document. For N suggested DL tags and C cate-gory word labels, scores for all possible N  X  C pairs P were obtained. The highest score served as a measure of DL tag suggestions. If a model captures caption words that happens to have shorter path distances to ground truth labels, then the model is scored higher. Note that WordNet is chosen since it is widely accepted -any other customized ontology can easily be pugged in depending upon the application. Also note that Newman et. al.[13] attempted to measure cohesiveness of topic  X  X abels X  consisting of top 10 high prob-ability words, where the  X  X esults over WordNet are patchy at best. X  The WordNet evaluation presented here is not to measure topic cohesiveness but to measure and explain the goodness of a probable DL tag. Although measuring topic coherence is equivalent to measuring topic intrusion[8], the notion of a topic is only a mathematical convenience for a low-dimensional subspace that tries to capture the assump-tions of the statistical generative model. So qualitatively, a topic is best interpreted by the task on which the model is adapted and its corresponding assumptions.

For the DUC2005 dataset where the WL tags came from named entity classes, all pairs of PERSON named entities from documents in each of the 50 docsets were collected. For a particular proposed Tag 2 LDA model, hidden topics were inferred for these documents and pairs from top N entities from the X  X ERSON X  X acet of the topic were collected. Entity pairs that co-occur in a sentence were chosen as ground truth pairs that were strongly related -this is the baseline. The average of ratio of the counts of the PERSON entity pairs from topics to those from the baseline served as a quanti-tative measure of improvement over the latter in the entity relationship discovery application. Some qualitative results are shown in Table 8. Note that the same named entity can occur across multiple docsets. This is particularly true of NUMBER and LOCATION classes and entities related to governments.
Joint topic and tag analysis has been used in a some re-cent works including [16, 17, 20] which has culminated in the creation of variants of topic models like LDA[6]. Due to space constraints, the reader needs to be directed to [6] for a full description of the basic LDA model. The principle shortcoming of these papers are the use of a single tagging perspective -either document level tags or word level tags. While the models in [16] and [17] are essentially the same, the purposes of the models had been a little different. Both use generative models of words and document tags to dis-cover latent topics. In [16] the topic-tag and the topic-word features were used to better cluster tagged documents. In [17] new documents were X  X olded-in X  X n the latent topic space and tags were predicted based on the inferred topic tag dis-tribution. The work in [20] is useful in the sense that the top-ics are discovered w.r.t to words being conditioned on WL tags. A recent work on topic-perspective modeling has been done in [12] where the authors have tried to use perspectives as hidden states that represent a discreet distribution over tags. It is important to note that the  X  X orrLDA X  model re-ferred to in [12] is not a true correspondence model as there happens to be no direct correspondence between words and tags. Further, although there is a connection between the user-perspective and perspective-tag distributions, the con-nections between the perspective-tag distribution and the topic-word/topic-tag distributions weakly depend on a bi-nary switching variable. The sLDA[5] model discovers top-ics based on the ensembles of document contents and the response variables. The values of the response variables are explained by the frequency counts of the words in the cor-responding documents only. The labeledLDA[15] model es-tablishes a one-to-one correspondence to the latent topics and the actual document tags. This is done in a manner similar to imposing a non-uniform prior on the latent topic proportions per document[19]. Further the words in text are corresponded to topic labels which precludes any possi-bility of using WL conditional tags. The proposed Tag 2 LDA models use both joint (words and DL tags) and conditional (words and WL tags) modeling, thereby allowing a richer document structure to be captured.
This section introduces the model description and the model parameters for the Tag 2 LDA models. In all model figures in fig. 1, the symbol notations and their meanings given in table 5 are adhered to. Note that in the Tag 2 LDA Symbol Meaning ( r.v. = random variable )
Table 5: Symbols used in this paper and their meaning models, p ( w m | C = i,  X  ,  X  ,t m ), where C = y m or C = z is not a simple topic multinomial anymore, but is distributed as p ( w m | C = i,  X  ,  X  ,t m )= exp(log  X  i,w m +log  X  t m Note that each  X  t is a distribution over V . Simplified Gibbs sampling using Multinomial-Dirichlet conjugacy cannot be applied in this setting since the distribution is not a multino-mial anymore. Also note that for the correspondence models y m  X  Unif ( N d ), short for Uniform distribution, as in [4]. The generative processes for the proposed Tag 2 LDA models are illustrated below:
In the correspondence models, the DL perspective plays a significant role in the quality of topic coherence. For exam-ple, when the GSRt perspective (c.f. table 2) is chosen as a DL perspective, the topics will capture words that are both co-occurring and generated from similar roles. There could be a docset on  X  X lobal warming X  which will tie together words like planet and ice based on co-occurrence alone. Con-sider another docset concerning discovery of ice on Pluto X  X  surface. Thus, if the GSR of  X  X ce X  is taken to be a subject, then a  X  X lobal warming X  topic can include Pluto as a proba-ble word because of the GSRts for the word X  X ce X  X hat involve  X  X ubj X . This type of tagging is beneficial where the task is not to replicate the docset structure based on co-occurrence but to provide deeper insights into data for tasks such as summarization, relationship extraction, etc. This effect is hardly observed when the DL tags tersely summarize the main contents of the documents.
The Variational Bayesian Expectation Maximization algo-rithm [6, 3] has been used to maximize the lower bound to the true intractable likelihood of the data w.r.t. the model parameters. This section outlines the various updates of the latent variables and the parameters and subsection 3.3 outlines a general plan of implementation. To find as tight as possible an approximation to the log likelihood of the data (the joint and conditional distribution of the observed variables given the parameters), the KL divergence of an ap-proximate factorized mean field distribution is minimized to the true posterior distribution of the latent variables given the data. A fully factorized q distribution with  X  X ree X  varia-tional parameters  X  ,  X  and  X  is imposed as and then optimal values of free variables and parameters are found by optimizing the lower bound on optimize can be shown to be (as in [3]) where E q [ f ( . )] is the expectation of f ( . )overthe q distribu-tion and F is the Expected Lower BOund (ELBO) to true likelihood. This ELBO is directly related to measuring per-plexity[6]. In the following subsections, it is assumed that K is the number of topics,  X  to be free parameters of the variational DL tag-topic distribution and  X  to be the free parameters of the variational word-topic or word-DL tag distributions. These free parameters are defined for every document d  X  D . As in [6], the key inferential problem that is solved here is the learning of the posterior distribution of the latent variables given the observations and parameters of the models on data that are new on count proportions. Fol-lowing the inequality, log( x )  X   X   X  1 ( x )+ log (  X  )  X  the ELBO F is changed to further lower bounds L for the two models.
 For the METag 2 LDA model: L
MM = E q [log p (  X  |  X  )] + E q [log p ( z n |  X  )] + E q
The expression for E q [log p ( w m | y m , X ,  X  , t )] can be written as:
Using the new lower bound, the maximum likelihood esti-mations of the hidden variables in document d are as follows:  X  m,i  X  exp {  X  (  X  i )  X   X  ( For the corrMETag 2 LDA model:
L corrMM = E q [log p (  X  |  X  )] + E q [log p ( z n |  X  )] + E q [log p ( w n | z n , X  )] + E q [log p ( y m | N )] + E q [log p ( w m | y m , X ,  X  , t )]  X  E q [log q (  X  , z , y ,
The expression for E q [log p ( w m | y m , X ,  X  , t )] can be written as:
E q [log p ( w m | y m , X ,  X  , t )]  X 
Using these lower bounds and the maximum likelihood esti-mations of the hidden variables in document d are as follows:  X  n,i  X  exp {  X  (  X  i )  X   X  (  X   X  m  X  0 is an additional free variable used in the Taylor expansion of log( x ) to obtain a tractable second lower bound on the probability of word generation given the topic and tag parameters of the model. Note that  X  m is defined for each document d  X  D and does not need to be initialized in the routines described in subsection 3.3.
The expressions for the maximum likelihood of the param-eters of the original graphical model using derivatives w.r.t the parameters of the functional L are obtained as follows: For the METag 2 LDA model: log  X  i,v =log log  X  t,v =log For the corrMETag 2 LDA model: log  X  i,v =log log  X  t,v =log where  X  ( x y z )=1iff x z == y and 0 otherwise and t  X  X  1 , .., T Since the updates for  X  and  X  are unconstrained, a Gaussian regularizer with 0 mean and constant standard deviation (set to 2 in this paper) is used for every  X  i,v and  X  t,v .If  X  are in log space as  X  and  X  , then L i,t is transformed to L
So, in the derivative of L w.r.t log  X  or log  X  results in where ( . )is  X  i,v or  X  t,v . For exp(( . )) to be  X  0, the positive root is taken as the only solution. So the solution becomes (letting A =exp(( . ))), which is  X  0. In the derivative of L w.r.t the log of  X  or  X  , if the regularizer is not used then convergence is not achieved 2 . This derivation is different from that used in [20] . Further, while initializing marginal statistics for and  X  , random initialization works best. A complete deriva-tion of the extrema expressions for the hidden variables and model parameters cannot be shown due to space constraints. Note that number of Lagrange multipliers used in the opti-that for  X  i,j j  X  X  1 , .., corrV } is corrV . These free( model(  X  ) parameters follow multinomial distributions and hence sum to one.
Algorithms 1, 2, 3 and 4 outline some computational pro-cedures for implementing the model and corresponding time complexities (given as [ O ( . )]). If a procedure is not defined, comments in { . } explain the functionality of the procedures. Algorithm 1 VB EM 1: if algorithm mode ==  X  training then 2: initialize statistics(); { use seeded initialization for  X  3: vb m step(); 4: end if 5: elbo prev  X  0 6: elbo current  X  0; iters  X  0 7: while converged  X  EM CONV ERGED do 8: elbo current  X  vb e step () { update hidden variables } 9: vb m step() { update model parameters } 10: converged  X  ( elbo prev  X  elbo current ) / ( elbo prev ) 11: elbo prev  X  elbo current ; iters  X  iters+1 12: end while [ O (iters  X  (vb e step+vb m step))]
This section shows the relative performances of the pro-posed models on DUC2005, Wikipedia and the Amazon Re-view (AR)[7] datasets (see subsection 1.1). The AR dataset was further processed to extract not more than 400 reviews per category. The reviews belong to 25 category labels including { apparel, software, magazine, food, etc. } .The Wikipedia documents were crawled using the special export url 3 mostly along the categories of { food, animal, countries, Algorithm 2 vb e step 1: zero initialize statistics();[ O (K.corrV+K.V+T.V)] 2: precompute beta and pi row sums() { precompute 3: elbo current  X  0 4: for d =0to D do 5: doc  X  corpus  X  document vec  X  at ( d ) 6: elbo current += doc e step(d, doc) { also accumu-7: end for [ O (D(doc e step))] 8: return elbo current; Algorithm 3 doc e step 4:  X  m,i = 1 . 0 K { If model is METag 2 LDA }{ OR } 5: elbo current  X  0; v iter  X  0 6: while not converged do 7: update  X  d,m 8: update  X  d,n,i 9: update  X  d,m,i { If model is METag 2 LDA }{ OR } up-10: elbo current  X  compute likelihood () { To compute 11: update  X  d,i 12: v iter  X  v iter +1 13: end while 14: return elbo current; [ O (K+KN+KM+v iter(MK+NK+ sport, war, transportation, natural, weapon, universe and ethnic groups } . The relative positions of the sections were binned into 5 categories which served as WL tags. Stan-dard English stopwords were removed for the Wikipedia data and after processing, it contained 33 , 261 unique words and 6 , 902 unique DL tags (bag-of-words from image cap-tions and Wikipedia article names). The AR dataset con-tained 6017 unique words and 4271 unique DL tags from product names and review titles after processing. Tags from the affect lexicon were used as WL tags. For both datasets, words occurring once or more than a thousand times across the entire corpus were also removed. Also note that the main document word vocabulary V and the document level tag vocabulary corrV were processed independently using the same token processing rules but without any correspon-dence. To compare the proposed models with sLDA[5] on the AR data, all DL and WL tags were discarded for sLDA. Instead, the ratings served as values of the response ran-dom variables. For both the datasets the number of topics Algorithm 4 vb m step 1: for all i  X  1 , .., K , v  X  1 , .., V and corr v  X  1 , ..., corrV 2: update  X  i,corr v from sufficient statistics 3: update  X  i,v from marginal statistics 4: update  X  t,v from marginal statistics 5: update  X  { Follow the Newton-Raphson method in [6] } 6: end for [ O (K.corrV+K.V+T.V)] K were set to { 20 , 50 , 100 , 200 } . For the DUC2005 dataset, the DL tags were the GSRts found in respective documents and their counts (see subsection 1.1 for GSRts). Two types of WL tags were considered: word position bins like WL tags for Wikipedia dataset and named entity classes -PERSON, ORGANIZATION, LOCATION, NUMBER and MISC. To-gether, DL+WL tagging for DUC2005 data is named as GSRTPos and GSRTNe respectively (see fig. 2). Altogether, there were 36725 unique words for the DUC2005 dataset and 81 corresponding terms which were just the GSRts. K was set to { 40 , 60 , 80 , 100 } for the DUC2005 data based on hu-man intuitions. To measure predictive power of METag 2 LDA and corrME-Tag 2 LDA, a 10-fold cross validation was performed on the DUC2005 dataset. Figures 2a and 2c show the ELBO X  X  (higher is better) on validation sets averaged over all folds. Figures 2b and 2d show the minimum of the differences in the ELBO X  X  per topic across all folds for the corrMETag 2 model vs. corrMMLDA and METag 2 LDA models. Clearly the differences prove that the improved performance of cor-respondence Tag 2 model is statistically significant. This is also intuitive since words in a document are always gener-ated from a corresponding process, like visualizing an image or action role for a concept. There is a very slightly im-proved performance when the WL tags are chosen to be named entity classes.

The TagLDA model[20] was not compared for this dataset since the concept of multiple GSRts at the word level breaks down for TagLDA. However, empirically it is seen that the nature of DL tags influences the predictive power of the proposed Tag 2 LDA models vs. TagLDA. For the DUC05 dataset, the DL tags were represented by coherence markers like X  X ubj  X  subj X  X tc. as in [2]. Typically this type of marker groups variations like X  X andslide:subj  X  subj, X  X  X ar:subj  X  etc. under a common  X  X ubj  X  subj X  abstraction. On the other hand, words like landslide and car signify concepts that allow for identifying specific centers in coherent sen-tences. In this respect, the WL perspective is more impor-tant (primary) over the more abstract DL perspective, the latter capturing a coarser notion of document level coher-ence. The counts of document level GSRts in the form of  X  X SR  X  GSR X  X o not allow for much variance to be exhibited by the documents at the DL perspective. This fits TagLDA better to the dataset at the cost of either ignoring abstract coherence markers altogether or discarding WL perspective and choosing only one coherence marker per word at WL annotation. However, if the document level GSRts are in the form  X  X ord:GSR  X  GSR X , then the proposed models fit the data much better than TagLDA owing to the variance in the DL observations that are captured nicely in the topics along with the WL variations. The GSRts in the latter case
For each of the Wikipedia test documents, top 5 predicted tags (coming from image captions and article names) were chosen. Following [14], the method described in section 1.3 was chosen as a quantitative measure of tag suggestion suc-cess. Figure 5a shows the relative values of the proposed Tag 2 LDA models for macro averages of maximum of best path distance scores for all test documents. Fig. 5a sug-gests that people ignore the specific contents of the docu-ments while assigning a category label. The METag 2 LDA model, in spite of higher perplexity, performs a little better here because of the lack of specificity of suggested DL tags to the document contents. This shows that humans assign DL tags that belong to higher levels of abstraction. Never-theless, the best DL tags suggested by both METag 2 LDA and corrMETag 2 LDA are only within 1 to 2 hops away from the ground truth tags based on a chosen WordNet on-tology. Thus image captions in Wikipedia articles provide powerful clues for suggesting document tags. Table 7 shows  X  explanations  X  X fthe suggested DL tags to the ground truth category labels which is a desirable output of this type of evaluation. One could also use cross-document evi-dence trails[18] to measure semantic relatedness. It is to be noted here that the ontology chosen must be specific to the nature of the task. For example, in medical domain, Word-Net is a poor choice for providing explanations to DL tag suggestions.
For the DUC2005 data, the second column in the first, third and fifth rows of table 8 show selected PERSON entity pairs that were discovered to be related through some latent topics. The WL and DL tags for this purpose were named entity classes and GSRts. The first column in the first, third and fifth rows shows queries that serve as gists of the three docsets. To validate the discoveries the following experiment was devised: For each docset in the DUC2005 data, all en-tity pairs that were co-occurring in a sentence were counted and was treated to be a baseline measure of coverage for entity pairs that are related. Then a set of best topics were inferred for the documents in docsets by the Tag 2 LDA class of models. For each topic set, 2450 (=50x50-50)PERSON entity pairs were created out of the highly probable entities appearing in the PERSON facet of the conditional topics. Note that for all entities A and B, two entity pairs (A,B) and (B,A) were created. Each docset had on average 2449 PERSON entity pairs and hence the number 2450. Note that John Nash, Nash and Dr. Nash were treated as three separate entities. The graph in fig. 5b shows that the corre-spondence model is 3 times better than the robust baseline at the right number of fitted topics and using the abstract GSRt DL perspective. The second, fourth and sixth rows of table 8 show how topical context ties two entities together even though they do not occur in the same sentence. Rows 7, 8 and 9 show ORGANIZATION facet, LOCATION facet and marginal topic corresponding to the best topic for doc-set  X  X W/GM Industrial Espionage X .
This paper proposes novel extensions to joint models of text and document level tags that makes use of conditional word level tags to better capture annotated document struc-ture. The proposed Multinomial-Exponential Tag 2 LDA mod-els capture semantics of documents with domain knowledge coming from two different perspectives. The correspondence models also show impressive predictive power for inferring topics. Further, usefulness of the models have been ex-plored with applications that provide deep insights into the data. Overall, it is possible to add domain knowledge from different perspectives, into topic models without sacrificing predictive power. Thus supervised models of data annota-tion can be made better without any intervention from topic models and still aid the latter in improving posterior infer-ence. Adding more than one word level tag to each word or phrase, each being generated from a different perspective, is an important direction of research.
