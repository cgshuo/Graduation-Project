 Ensemble learning algorithms train multiple base learners and then combine their pre-dictions to make final decision. Since the generalization ability of an ensemble could be significantly better than that of a single learner, studying the methods for constructing good ensembles has attracted a lot of atten tions in machine learning literature during the past decade [17]. Generally, the design of a classifier ensemble contains two subse-quent steps, i.e. constructing multiple com ponent classifiers and t hen combining their predictions.

It is well-recognized that in order to get a strong ensemble, the component classifiers should be with high accuracy as well as high di versity [9,22]. Most existing ensemble methods achieve this goal through resampling the training instances or input features. Among them, Bagging [3], Boosting [10], and Random Subspace [14] are three gen-eral techniques widely used in many applications. Both Bagging and Boosting train base classifiers by resampling training instances, while Random Subspace trains base classifiers by using different random subsets of input features.

Another general method for constructing classifier ensemble is to manipulate the class labels that given to the base classifie r, i.e. by transforming the learning problem into a collection of related learning problems that use the same input instances but dif-ferent assignment of the class labels. Most existing methods of this type achieve this by converting the given learning problem into a collection of different but related binary learning problems and then learning a component classifier for each of them. Repre-sentative examples include Error-Correcting Output Codes (ECOC)[8] and pairwise ensemble(also known as round robin ensemble) [12], etc. The main deficiency of ex-isting ensemble methods based on manipulating the class labels lies in that, first, they can only be applied to multi-class learning problem; second, although they often can improve the classification accuracy of the bas e classifiers, their performance gain often is not as large as AdaBoost [12].

Inspired by MTForest[19] and AODE[20] that enumerate each input attribute to-gether with the class attribute to create different component classifiers in the ensemble. In this paper, we propose a novel way to manipulate the class labels for constructing strong ensemble of classifiers by generating different biased new class labels through the Cartesian product of class attribute and each input attribute. This method can be applied to both binary and multi-class learning problems. Extensive experiments show that its performance is superior to AdaBoost. Bias-variance decomposition shows that, the success of it mainly owe to its ability to significantly reduce the bias of the base learner.

The rest of this paper is organized as follows. In section 2, we introduce the back-ground and give a brief review on related work. In section 3, we propose the MACLEN (Multi-tAsk Class Labels based ENsemble ) method. Then we report our experimental results in section 4. Finally, we conclude the paper in section 5. 2.1 Multi-Task Learning Multi-Task Learning (MTL) trains multiple tasks simultaneously while using a shared representation and has been the focus of much interest in machine learning community over the past decade. It has been empirically [5] as well as theoretically [1] shown can often significantly improve performance rela tive to learning each task independently. When the training signals are for multiple tasks other than the main task, from the point of view of the main task, the other tasks are serving as a bias [5]. This multi-task bias causes the learner to prefer hypotheses that can explain more than one task, i.e. it must be biased to prefer hypotheses that have utility across these multiple tasks.
In most machine learning applications, how ever, we are only given the training data which is composed of input attributes and class attribute (main task) and we do not have any other related tasks information. So how to derive related tasks from the given data is crucial for utilizing MTL paradigm to improve the generalization performance. It has been shown in [6] that some of the attributes that attribute selection process discards can beneficially be used as extra related tasks for inductive bias transfer. 2.2 MTForest Because in multi-task learning extra task is served as additional inductive bias, an en-semble can be constructed by using differen t extra task to bias each component learner in the ensemble so as to generate differe nt component learners [19]. The multi-task learning theory reveals that the component learner will often be with higher accuracy if the extra task is related to the main task and the component learner will be with diversity if the each extra task represents different bias.

Inspired by this, we propose the MTForest [19] algorithm which enumerates each input attribute as extra task to introduce dif ferent additional inductive bias to generate component decision trees in the ensemble. In MTForest, the construction of each com-ponent decision trees is related to both the class attribute and the given input attribute. The learning process is similar to standard C4.5 decision tree learning algorithm except that the Information Gain criteria of each split S i is calculated by combine the class attribute and the given input attribute, showing below: The prediction of the ensemble is produced by aggregating the predictions of all these component decision trees. Experimental results show that MTForest can achieve signif-icant improvement over Bagging and is robust to noise. 2.3 Averaged One-Dependence Estimators Naive Bayes is an efficient and effective learning algorithm. Denote an instance x as a vector a 1 ,a 2 , ..., a n ,where a i is the value of i -th attribute, and c is the class value . Naive Bayes simply assumes that the attributes are independent given the class, i.e. The conditional independence assumption unabl e to capture important attribute depen-dence information which sometimes hamper the performance of it seriously. So it needs to relax the assumption effectively to i mprove its classification performance.
In last fewer years, numerous semi-naive Ba yesian classifiers that try to exploit attribute dependencies in moderate orders have been proposed and demonstrate im-proved performance on naive Bayes. Among those classifiers, approaches that utilize ODE(One-Dependence Estimator) have d emonstrated remarkable performance [11,15,18,20]. Representative examples include TAN and SPTAN[11], HNB[15], SNODE[18] AODE and its variants, etc. ODE paradigm learning restricts that each attribute can only depend on one parent in addition to the class, and thus it follows that where pa ( i ) denotes the parent attribute of the i -th attribute, and is determined in the structure learning process.

Averaged One-Dependence Estimators(AODE) [20] is the ensemble technique of utilizing ODE. For simplicity and to avoid structure learning, AODE built an ODE for each attribute, in which the attribute is simpl y set to be the parent of all other attributes. The final prediction is produced by aggregating the predictions of all these ODEs. A lot of empirical study show that AODE can ach ieve significant improvement over naive Bayes and AdaBoost [15,20].
 The success of MTForest and AODE show that enumerating each input attribute to-gether with the class attribute to create different component classifier is a powerful way to construct strong ensemble. However, MTForest and AODE are specifically for en-sembling decision tree and naive bayes, resp ectively. Both of them need to revise the base classifiers to incorporate the information from the given input attribute. While most general ensemble method, such as Bagging, AdaBoost, Random Subspace, is transpar-ent to the base classifier and can easily be applied to any base classifier. Therefore, it will be interesting to ask whether there exists another way to incorporate the information of the input attribute into the base classifier while is transparent to the base classifier.
In this and subsequent sections, we will argue and show that this can be achieved based on generating new class labels through the Cartesian product of the input attribute and class attribute. Since every new class label contains the information of both the class and input attribute, we call it multi-ta sk class label and the ensemble method as MACLEN (Multi-tAsk Class Labels based ENsemble). 3.1 MACLEN: Algorithm Definition In this subsection, we present the MACLEN method in detail. Assume that the instance in the training data set is represented by n +1 -dimension vector A 1 ,A 2 , ..., A n ,C where A i is the i -th input attribute and C is the class attribute. An instance x is rep-resented by a vector a 1 ,a 2 , ..., a n ,c ,where a i is the value of attribute A i and c is the value of class attribute C respectively. For the sake of simplicity, in this paper, we suppose that all the input attributes are discrete.

In MACLEN (see Algorithm 1), an ensemble is constructed by using each input attribute together with the class attribute to generate different but related learning prob-lems independently. When given an input attribute A i , a new attribute C  X  i is constructed whose values are the Cartesian product of class attribute and this attribute. We then create a new data representation from the original input data by removing this at-tribute and class attribute and setting the new attribute C  X  i as the class attribute, i.e. x : a 1 ,a 2 , ..., a n ,c in the original input is transformed into the new data representa-tion by deleting the value a i and setting the class value as ca i , i.e. the new transformed the base learning algorithm, which constructs a classifier h i . By enumerating each input attribute, we obtain an ensemble of n component classifiers { h 1 , h 2 , ... , h n } . To classify an unlabeled instance, x : a 1 ,a 2 , ..., a n , we employ the following method. Each component classifier h i in the ensemble provides probabilities for the class mem-bership of x while the possible class labels is C  X  i . Since the probability output of h i can be seen as the joint probability distribution of class attribute C and attribute A i , we can de-rive the probability of x belonging to original class label c using the marginal probability , i.e.: Algorithm 1. MACLEN where  X  P h i ( c | x ) is the estimated probability of instance x belonging to original class label c according to classifier h i ,and  X  P  X  h longing to new class label ca according to classifier h i . Furthermore, if the value of attribute A i is given (denoted as a i ), we can also derive the probability of x belonging to original class label c using the conditional probability ,i.e.: Compared to marginal probability (Eq.1), t he conditional probability (Eq.2) can utilize extra information about the value of attribute A i . So, in our implementation, we first choose Equation (2) to calculate the class membership probab ilities of each component classifier, only when the Equation (2) is undefined 1 , we then turn to use Equation (1) to calculate the class membership probabilities.

Then we compute the class membership probabilities for the entire ensemble as: where  X  P ( c | x ) is the probability of x belonging to class c . At last, we select the most probable class as the label for x ,i.e. E ( x ) = arg max c  X  C  X  P ( c | x ) .
It is noteworthy that for MTCLAN, it enumerates each input attribute to generate each component classifiers in the ensemble, hence its ensemble size is equal to the number of input attributes which is same as MTForest and AODE but is different from most other ensemble methods such as Bagging and Boosting that need to specify the ensemble size. In addition, the building process of each component classifiers do not depend on each other, so MACLEN can easily be parallelized.
To illustrate this algorithm, consider the learning problem described in UCI data set adult [2]. In this problem, we are given 14 attributes representing 14 different aspect information of a person, such as the age , martial-status , workclass , education , occupa-tion , race , sex , and so on, and the class attribute indicating whether the annual income of this person is larger than 50 k or not. Our goal is to build a predictive model from the la-beled training data which can be used to predict the income type of an unlabeled person given the values of these 14 attributes. In MACLEN, we use each of these 14 input at-tributes to generate different but related learning problems. For example, if attribute sex whose values are { male, f emale } is chosen, we construct a new attribute as the class attribute whose values are { male : &gt; 50 k, male :  X  50 k, female : &gt; 50 k, female :  X  50 k } . This new learning problem is differen t from the original problem because it not only has to predict the income type of the person, but also the sex type of the person. Then we can construct a base classifiers h sex . Assume that when to classify an unla-beled example and the given sex value is female , we can derive the probabilities for the original class membership &gt; 50 k and  X  50 k only from the predicted probabilities of new class membership female : &gt; 50 k and female :  X  50 k in h sex . Through using different attribute we construct different learning problems which have different class labels, while these learning algorithms are a lso related to each other since they all have to predict the income type of the person. 3.2 Discussion In MACLEN, we only deal with discrete input attributes. For numeric input attribute, some discretize method must be used ahead. Since discretize methods based on different split criteria often result in different number o f discrete values and different intervals for the same numerical attribute, this provides a way to build several different component classifiers from a single numerical attribut e which can be used to enlarge the ensemble size of the MACLEN.

Note that the existing of discrete attributes which have large number of values may hamper the performance of MACLEN. These attributes make the corresponding new class label set very large. Since the given tr aining instance number is fixed, the instance-to-class ratio will decrease dramatically, w hich likely to result in inaccurate component classifier. So it may be better to cluster the values of these attributes into small number of groups respectively, and assign all the values in the same group the same discrete value. By using different cluster method, we can also build several different component classifiers from a single discrete attribute. 4.1 Experimental Setup We conduct experiments under the framework of Weka [21]. For the purpose of our study, we use the 30 well-recognized data sets from the UCI repositories [2] which rep-resent a wide range of domains and data characteristics. A brief description of these data sets is shown in Table 1. We adopted th e following three steps to preprocess each data set. First, missing values in each data set are filled in using the unsupervised filter ReplaceMissingValues in Weka ; Second, numeric attributes are discretized using the supervised filter Discretize in Weka which use the MDL discretization method; Third, we use the unsupervised filter Remove in Weka to delete attributes that do not provide any information to the class, two occurred within the 30 data sets, namely Sequence Name in data set yeast and Animal in data set zoo .

In our experiments, we compare MACLEN to Bagging, Boosting and Random For-est when using C4.5 as base classifier. It is well recognized that Bagging is often unable to enhance the performance of stable classifiers. So we compare MACLEN to Boost-ing and AODE when using naive Bayes as base classifier. And we study MTCLAN for naive Bayes using the Laplace estimation and M-estimation to smooth probability esitmation which is denoted as MACLAN(L) and MACLAN(M) , respectively. The C4.5, Bagging, Boosting (we use the multi-class version AdaBoost.M1 [10]), Random Forest, Naive Bayes and AODE are all already implemented in Weka , so we only imple-ment our algorithm under the framework of Weka . We set the ensemble size as 50 for all compared methods and keep other parameters at their default values in Weka. It is noteworthy that for our method, the ensemble size is just the number of input attributes which is often far smaller than 50 on these data sets. The classification accuracy and standard deviation of each algorithm on a da ta set was obtained via 10 runs of ten-fold cross validation. Runs with the various algorithms were carried out on the same training sets and evaluated on the same test sets.

To compare two learning algorithms across all these domains, we first adopt the widely used pairwise two-tailed t -test with 95% confidence level to compare our al-gorithm with other algorithms. Recently, it h as been proposed that the best method to compare multiple algorithms over multiple data sets is to compare their overall ranks [7]. So, we also present the overall ranks of our algorithm and other ensemble methods compared. Note that the smaller the rank, the better the method.
 4.2 Experimental Results Table 2 shows the detailed experimental resu lts of the mean classification accuracy and standard deviation of C4.5, Random Forest and three ensemble methods using C4.5 as base classifier on each data set. And the mean values, overall ranks and the pairwise t -test results are summarized at the botto m of the table. From Table 2 we can see that MACLEN can achieve substantial improvement over C4.5 on most data set (11 wins and 3 losses) which suggests that MACLEN is potentially a good ensemble technique for decision trees. MACLEN can also gain significantly improvement over Bagging (7 wins and 2 losses) and is comparable to two state-of-the-art ensemble technique for decision trees, AdaBoost (6 wins and 4 losses) and RandomForest (4 wins and 4 losses). The overall rank of MACLEN on these 30 data sets is 2.22 which the smallest among all these ensemble methods.

Table 3 shows the detailed experimental re sults of the mean classification accuracy and standard deviation of naive Bayes, AODE, MACLAN(L) and MACLAN(M) using naive Bayes as base classifier on each data set. The mean values, overall ranks and the pairwise t -test results are also summarized at th e bottom of the table. From Table 3, we can see that MACLEN(M) is significantly better than MACLEN(L). This is very likely due to that, compared to original p roblem, the number of instance belong to each new class label is smaller and the number of class labels is larger. Thus , compared to Laplace estimation, the M-estimation whi ch places more emphasis on data than prior could make probability estimation effectively in this situation. MACLEN can achieve substantial improvement (12 wins and 1 loss, 16 wins and 1 loss, respectively) over naive Bayes and gain improvement over AdaBoost (8 wins and 5 losses, 9 wins and 5 loss, respectvely). Furthermore, MACLEN(M) is comparable to AODE which is the state-of-the-art ensemble method for n aive Bayes. Note that MACLEN is a general ensemble method while AODE not.

We also test our method on these 30 UCI data sets under artificial noise in the class labels to study its robustness. Following the method in [4], the noisy version of each training data set is generated by choosing 5% instances and changing their class labels to other incorrect labels randomly. Due to space limited, we do not list the detailed results of the accuracy and standard deviation on each data set here. The experimental results show that MACLEN can significantly outperform AdaBoost (12 wins 1 loss for C4.5 as base classifier, 13 wins 2 losses for naive Bayes as base classifier)in this situation, and is comparable to RandomForest and AODE respectively. 4.3 Bias-Variance Decomposition To understand the working mechanism of MACLEN, we use the bias-variance decom-position to analysis it. The bias-variance decomposition is a powerful tool for investi-gating the working mechanism of learning al gorithms. Given a learning target and the size of training set, it breaks the expected error of a learning approach into the sum of three non-negative quantities, i.e. the intrinsic noise, the bias, and the variance. At present there exist several kinds of bias-variance decomposition schemes [13]. In this paper, we adopt the one proposed by [16] which has already been implemented in weka .
The training data are divided into trainin g and test sets each containing half the data. 50 local training sets are sampled from the training set, each local set containing 50% of the training set, which is 25% of the full dat a set. A classifier is constructed from each local training set and bias, variance, and e rror are estimated on the test set. Since the bias-variance evaluation procedure utilizes training sets containing only 25% of each data set, we only do this decomposition on the 12 UCI data set which size is larger than 1000. Table 4 shows the detailed decomposition results of C4.5 , Naive Bayes and MACLEN using each of them as base learners respectively. Note that the actual bias-variance decomposition scheme of [16] generates a bias term that includes the intrinsic noise, so the errors shown in Table 4 are only composed of bias and variance.
From Table 4, we can see that in most cases, the error reduction of MACLEN com-pared to its base learners is mainly due to its improvement of the bias. The mean values shown at the bottom of the table also clearly demonstrate this. In summary, the success of MACLEN mainly lies in that it can significantly reduce the bias of the base learner. This suggests that MACLEN is similar to Boosting style algorithm which has been shown that its improvement is mainly due to its ability to reduce the bias of the base learner. In this paper, we propose the MACLEN method, a new general ensemble method based on manipulating the class labels. It has sev eral appealing properties: first, it can be applied to both binary and multi-class lear ning problems; second, its performance is superior to AdaBoost; third, it is simple, easy to parallelized and robust to noise. These demonstrate that manipulating class labels is also a general powerful way to generate strong ensemble besides the popular way of resampling the input instances or features. We would like to thank the anonymous reviewers for their helpful comments. This work was supported in part by NKBRP(2005CB321905), NSFC (60873115) and AHEDUSF (2009SQRZ075).

