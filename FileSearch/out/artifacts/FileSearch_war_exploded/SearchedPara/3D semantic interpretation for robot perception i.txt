 1. Introduction
The 3D perception of the surrounding environment is still an important research fi eld for both the industrial and the research community. There are several potential applications for this domain, mainly from the fi elds of urban surveillance, path planning and cultural heritage conservation. Each individual application requires a speci fi c handling of the acquired data sets. Although certain applications in the mobile robotics domain require real time data processing, e.g. dynamic perception and planning, the post-processing of data is suf fi cient for our pipeline.

Several sensors can be used for the acquisition of data, such as stereo cameras, laser range fi nders, and the recent structured light sensors. These devices have their own special characteristics in terms of precision, range and speed. Thus the way in which these sensors are chosen depends on the speci fi c requirements of the measure-ment problem to be solved ( Scharstein and Szeliski, 2002 ).
Relatively large areas, such as indoor spaces for of fi ces, require several different measurements to be aligned in the same coordi-nate frame. This kind of problem is well studied in the 2D space, mainly in the image processing domain. Although these 2D algorithms can be adopted for the registration of 3D data, they need special adaptations. Also, characteristics such as range, noise, and distribution have a large in fl uence on the algorithms used for 3D data processing, including the registration of point clouds.
Different algorithms can be used for iterative map registration, including keypoint and feature extractors, nonlinear correspon-dence estimators or odometry based approaches ( Magnusson et al., 2007 ). Although the data merging can be performed based only on the odometry information, this kind of registration is prune to fail due to the error integration characteristics of the odometers ( Kaushik et al., 2009 ). Hence a more robust method is applied for the initial alignment phase based on an extracted keypoint-feature data set proposed in the work Zhang et al. (2008) . A similar version of this approach was adopted for the registration stage in the paper at hand focusing on the iterative registration without explicitly making use of the loop closure data. Further on, for the different data sets, speci fi c features and their correspondences among them are evaluated.
 A certain environment is scanned from multiple viewpoints. These point clouds are then registered using an ICP-based algo-rithm applied in two stages: (i) initial alignment: only for set of correspondences, usually a fast process; and (ii) re alignment: using the complete data sets, being a time consuming but accurate variant. The assumption is that the registered cloud is not axis aligned, thus having a random coordinate system. Since the approach relies on accurate alignment with real-world axes, this method transforms the cloud in two steps: (i) initial guess: using normals of dominant planes to compute the axes; and (ii) correct alignment: where basic features from indoor environ-ments are used to determine the fi nal axes. During this process the planar surfaces are segmented and the boundary points for each plane are computed. Quadrilateral shapes are then fi tted to each set of boundaries. These shapes will tell us the positioning of walls and components such as doors and windows. After inspecting the sizes of these rectangle-like shapes and determining the relation-ships between them, the method can start assigning a class or label to each point.

The main contributions presented in this paper are as follows: development of a robust framework for iterative map registra-tion based on the combination of different keypoint feature pairs from the main literature; validation of the iterative registration method on different data sets including indoor and outdoor variants; a straightforward and reliable method for estimating the principal axes of 3D complete indoor data sets; improved hierarchical model fi tting by clustering the models  X  inliers, and retaining only consistent clusters as fi nal models; a simple and reliable set of rules for labeling indoor compo-nents, without the use of any training classi fi ers; procedure for estimating quadrilateral-like forms, which are needed for the proposed labeling process of 3D points. 2. Related work
The perception of environments is a current topic for several research works especially in the fi eld of robotics perception ( Rusu and Cousins, 2011; Tamas and Goron, 2012; N X chter and Hertzberg, 2008 ). Furthermore, the data registration issues are treated in different works using various approaches, such as planar patch matching ( Pathak et al., 2010 ), keypoint-descriptor pairs for the depth information ( Rusu et al., 2010 ) and even heterogeneous data fusion including RGB data in the works ( Kaushik et al., 2009; Tamas and Majdik, 2012 ).

A good example for creating the surrounding environment is given in Kasinski and Skrzypczynski (2001) . The authors are concentrating on the distributed mapping of an environment, where the application is an industrial one, e.g. a shared a warehouse. In comparison to ours  X  which is intended for understanding the human indoor environment, such as kitchens, of fi ces, and hospitals  X  the application presented in Kasinski and Skrzypczynski (2001) is closer to business and enterprise.
Several research works deal with the estimation of the precision of the registered 3D maps. This is valid for both the loop closure based mapping process (also referred as simultaneous mapping and locali-zation) ( Mayetal.,2009;Strasdat,2012 ) and the iterative registration based mapping ( Magnusson et al., 2009; Hervier et al., 2012 )without closuring the loop during the mapping.

In N X chter and Hertzberg (2008) the authors propose a so-called
This approach relies only on the segmentation of planar patches using the RANSAC ( Random Sample Consensus )( Fischler and Bolles, 1981 ) algorithm. Further identi fi cation of indoor components is obtained using a rather complex recognition pipeline.

The authors in Rusu et al. (2008) presented an approach similar to the one described in this paper in Section 4 .Theyalsouse2D quadrilaterals to detect rectangular shapes, but under the assump-tion that the points are axes aligned ( Rusu, 2009 ). Based on this assumption the reasoning about the environment becomes more
With regard to door detection, some very interesting work has been done, such as Murillo et al. (2008) where authors use computer vision to classify pixels using a so mehow smaller set of classes than the one presented in this work. Although the results look good, the performance can be easily affected by changing light conditions. Also by using only image processing, robotic applications which require 3D information cannot be support ed. Another work on door detec-tion is Morisset et al. (2009) where the developed system is accurately detecting and opening doors. Unfortunately, the authors have applied it only on doors and handles that are conform to ADA ( American Disability Act ) U.S. law, thus suggesting that the system might not comply for other types of doors.
 detection, this application being more common for outdoor processing of buildings. Nevertheless in Tuttas and Stilla (2011) a method is presented for detecting windows from outdoor scans, using indoor points. The authors also rely on the detection of dominant planes, which is being considered most representative.
Using the indoor points, meaning the points which lie inside the buildings, and which were obtained by the laser beam going through the windows into the building, the authors estimate the positions of windows.
 over the latest applications in the area of intelligent homes, such as child and elderly care, surveillance, and even the optimization of energy usage. Although the applications summarized in Silva et al. (2012) are relevant to the applied engineering fi eld, robotic applications are underrepresented in this work. We believe that  X  smart homes  X  should also incorporate  X  smart agents  X  , such as personal and service robots, which can interact autonomously with the environment ( Papadakis, 2013 ). 3. 3D scan registration map of the environment. To use these scans as a coherent data set, they have to be uni fi ed in a common coordinate frame. Unless the position and the orientation of the mapping robot are accurately known,therangescanregistrationneedstobedoneusingspecialized could not be determined with a suf fi cient accuracy between the measurement steps, the registration algorithms were employed for creating the elevation maps ( Tamas and Goron, 2012 ). scan registration without explicitly requiring or performing the loop closure. Similar benchmarking and approaches were tested for fusing different additional sensors in the work of Hervier et al. (2012) , or dealing with normal based information registration pre-sented in Magnusson et al. (2009) . 3.1. Data acquisition the surrounding environment. The measurement methods can be divided into 3 major categories based on the applied sensor and sensing technology: stereo vision (with two or more cameras) time of fl ight measurements. One of the most precise time of measurement systems is based on the laser scanners; however it is the most expensive one. A cheaper variant is the stereo camera, with less precision in the depth estimations or the structural light approaches ( Khoshelham and Elberink, 2012 ).
 conditions for spaces ranging between a few cm and 80 m. The scanning of the environment with a custom 3D laser scanner mounted on the P3-AT mobile robot was performed in a stop scan  X  go fashion. The core part of the 3D scanner is a Sick LMS200 planar scanner augmented with an additional mechanical part in order to gain the third degree of freedom for the data ( Tamas and
Goron, 2012 ). A single scan takes up to 60 s depending on the used sensor con fi guration. The resolution of the used custom 3D scanner was 0.25 1 in yaw and 0.5 1 for the tilting while the depth absolute error was less than 1 cm. All the measured data was integrated in the ROS 1 environment where each logging was timestamped for an easier off-line processing ( Goron et al., 2010 ). 3.2. ICP-based registration
The registration problem can also be viewed as the optimiza-tion of a cost function describing the quality of alignment between different scans. The algorithm determines the rigid transformation which minimizes this cost function ( Surmann et al., 2001 ).
The type of algorithm applied for the frame alignment strongly depends on the measured data set type. For the 3D laser scans the
Iterative Closest Point (ICP) and derivatives are popular in the of robotics ( Besl and McKay, 1992; Nuechter, 2009; Pathak et al., 2010 ). The ICP computes the rigid transformation which minimizes the distance among two point sets by associating a point from one frame to the closest point in the target frame. The transformation between two independently acquired sets of 3D points consists of two components, a rotation R and translation t . Correspondence points are iteratively searched from the model set of points M correspondence we need the transformations R and t which minimize the distance between the two points as follows:
E  X  R ; t  X  X   X  N m where w i ; j is assigned 1 if a valid correspondence is found between the i th point from M denoted with m i and the j th point from D denoted with d j .

Different variants were developed in order to increase the robustness and the performance of the algorithm especially for computing the rotational transformation term, which introduces a non-linear term in the minimization problem. A comprehensive overview and a qualitative evaluation of different approaches for the registration problem can be found in Salvi et al. (2007) .
A common approach for boosting the ICP robustness is the augmentation of the points with additional features such as point color, geometric features and point histograms ( Sharp et al., 2002 ).
This transposes the optimization problem in a higher order dimensional space search. These features are usually computed only for a certain subset of interest points from the original point cloud, i.e. keypoints in order to reduce the computational effort and enhance robustness.

The use of keypoints is to enable the ef fi cient comparison between on the correspondence estimation for the extracted keypoint features. 3.3. 3D keypoints and descriptors
There are several possibilities for extracting interest points and descriptors from 2D images including the popular SIFT (Scale Invariant Feature Transform) ( Lowe, 2004 ) or the SURF (Speeded
Up Robust Features) ( Bay et al., 2008 ) features. Unfortunately, these rely on local gradients from a unique orientation and therefore are not directly applicable for our approach with 3D data, however some concepts may be inherited from the 2D domain.

In this paper the Normal Aligned Radial Feature (NARF) ( Steder et al., 2010 ) keypoints were adopted for the extraction of interest points from range images. This type of keypoint takes into account the information about the borders and surfaces, ensures the detection from different perspectives and the stability for the descriptor computation. The most important parameter for the NARF extraction is the support size, i.e. the diameter of the sphere in which the interest point characteristics are determined ( Steder et al., 2011 ). In our case several values for this parameter were tested in order to gain a suf fi cient number of keypoints for different types of data sets. After the selection of keypoints was completed, the speci fi c properties are determined, i.e. the descrip-tors for the set of extracted keypoints.

For our approach we used the optimized version of the FPFH descriptor in order to augment the three dimensional space with pose-invariant local features and also tested the NARF descriptors with Manhattan metrics as fi tness score for the same set of keypoints. To compare the two set of descriptors, the runtime ( T ) in seconds and the initial alignment fi tness score ( S ) was com-puted for indoor (Id) and outdoor (Od) data sets. The result of the comparison is summarized in Table 1 .

The tests were performed on data sets containing around 10K points for which the extracted number of keypoints was in the magnitude of 0.1 K. For computing the runtime the average values were considered for 10 batch runs on an Intel Pentium 4 single core laptop running Ubuntu Linux. Although the run-time of the proposed algorithm is higher than some custom scenario based approaches such as the one presented in the work ( Kaushik et al., 2009 ), the degree of generality of the current approach is higher.
As observed, NARF descriptors are computed with several orders of magnitude faster than FPFH descriptors, but the latter approach is more robust in terms of estimating correspondences. This would be also the case for scenes which present less clutter or variation, thus having less discriminative features, where the FPFH features ensured a better correspondence between points. 3.4. Correspondence estimation
The next step after determining the keypoints and the descrip-tors is the estimation of correspondences between the two sets of keypoints with descriptors. There are several methods for the correspondence estimation, such as one-to-one, back and forth, and sample consensus based one ( Pathak et al., 2010 ).
In our approach the correspondence estimation was performed based on the geometric constrains of the selected points. Thus the nearest point in the high dimensional descriptor space was searched by using a kd-tree for enhancing the search speed ( Bentley, 1975 ). for the correspondence estimation problem having in many cases a large number of false positives.

For improving the estimation results, the fi ltering based on sample consensus was adopted. This ensures that after performing the one-to-one search for descriptors, only those correspondences are kept which satisfy a geometrical transformation constrain.
The comparison of the un fi ltered and fi ltered set of correspon-dences is shown in Fig. 1 on an indoor data set. This data set contains two scenes, the original one and the one rotated with 45 contains a large number of false positives, which are eliminated, yielding a more consistent estimation. The number of fi nal corres-pondences depends on the parameters used as a threshold for the sample consensus rejection.

The complete ICP-based algorithm can be found in the works ( Besl and McKay, 1992; Zhang, 1992 ), therefore only a short overview is given, with emphasis on the additional descriptor information for the points. The ICP with initial alignment is described in Algorithm 1 in the Appendix. It has two input point the FPFH of the source and target clouds (these two steps can be substituted with arbitrary point cloud feature search), while in
Step 3 the initial alignment t n is determined after the correspon-dence fi ltering. In the while statement in each iteration a set of associations A d is taken for which the best transformation is determined. The loop exit conditions are related to the error variation or to the maximum number of iterations both speci as tuning parameters for the algorithm. Finally, the algorithm returns the computed transformation between the two data sets. Further details regarding the implementation of the ICP with initial alignment based on sample consensus can be found in Morisset et al. (2009) .
 presented in Fig. 2 . In both cases the registration was performed using a pair alignment approach and the FPFH descriptors for the computed NARF keypoints. The initial alignment of the scans was performed based on the fi ltered correspondences of the FPFH descriptors. This alignment was then used for the ICP re fi computed on the last pair of data in the alignment loop. algorithm was monotonically decreasing, a suitable registration error was achieved in less than 100 iterations. This scenario was obtained by considering the maximum distance between two neighbor points to be less than 1 m. 3.5. Performance evaluation for the robotics community. Several recent works are focusing on this problem presenting different benchmarking variants including one 2009; Strasdat, 2012; Huitl et al., 2012 ).Thecovarianceestimationfor the work of Hervier et al. (2012) .

In order to characterize the quality of the registration beside the registered error the covariance of the estimate is also impor-tant for mapping purposes. The ICP can be considered as a least-square estimator which has the Cramer  X  Rao bound. This bound de fi x and the I ( x ) denotes the Fisher information matrix. The later matrix can be rewritten as
I  X  x  X  X  1 s 2  X  where s denotes the sensor noise and A i is the skew matrix composed of the point coordinates of the i th point as proposed in of the skew matrix can give a hint of the axes on which the unobservability condition may arise (e.g. a straight corridor).
As the ground truth is often peculiar to obtain, in order to evaluate the performances of the proposed methods a public data set was considered as a reference data the Jacobs University campus outdoor data ( Elseberg et al., 2012 ) with manual marked ground truth data. As their measurement was also based on lidar the covariance of the sensor readings was considered as s
For benchmarking purposes as a stopping criterion the iteration number of the ICP algorithm was considered n iter  X  100, usually this offering suf fi cient good accuracy for the registration in a few seconds runtime. In order to compare the performances of the different keypoint-feature based pre-aligned scans as output the absolute translation error and the covariance de fi ned with (2) were considered.

The summary of the evaluation run for different pairs of scans from the public data set is summarized in Table 2 for the NARF, FPFH and the combined set of keypoints used for initialization.
The comparison also in this case re fl ects the superior results of the initial aligned with the FPFH keypoints over the NARF ones, while the proposed combined variant of these keypoints gives the of keypoints as well as the larger number, hence more robust initial alignment with the combined keypoint set. Also it is important to observe that the absolute errors computed on different axes tend to have signi fi cant differences. This is mainly due to the asymmetrical displacement of the coordinate frames with respect to the axes, i.e. the largest displacement is towards the z -axis on which the largest errors were measured as well. 4. Indoor environment classi fi cation
This section describes an integrated method for classifying basic components of human indoor environments based on their physical features. To be more concise, the system records 3D point clouds and after processing assigns a label to each point according to the class it belongs to. A P3-AT ( Pioneer 3 All Terrain ) and a
PR2 ( Personal Robot 2 ) robot are used for achieving this task. In the following, a comprehensive overview of the proposed method is given. 4.1. Labeling process
A simple yet robust classi fi cation process was implemented, which can be easily extended with additional component classes if necessary. The goal of the system was not to rely on any training process but rather use common sense knowledge to label 3D points. To give some examples of what we understand as common sense about indoor human environments, look into Table 3 .
For the time being, the classi fi cation procedure uses ten classes, frame ; (5) door ; (6) window frame ; (7) window ; (8) furniture ; (9) handle ; and of course (10) unknown . It might seem redundant to have such similar classes, e.g. (4) with (5), and (6) with (7), but some components are not rigid and are constantly manipulated. Therefore, it is useful to have a system which can determine the places of those components. The method might not detect the actual door or window, but will identify the location where it should be when closed. 4.2. Principal axes estimation
In this subsection, a technique is described for estimating the principal axes of indoor point cloud data described in detail in Bolles, 1981 ) algorithm, by segmenting the inliers of each plane out of the point cloud. For an example of the fi nal fi tted planes, look at Fig. 3 (a). This is repeated until the remaining points in the cloud fall under a certain threshold. The mentioned threshold is set to an empirical value of 2500 point inliers.

As expected, RANSAC always fi nds the dominant planes fi rst, meaning the planes with the biggest number of inliers. Also in many indoor environments most of the surface normals coincide with one of the three main axes of the room. This is because most of the walls and furniture surfaces are parallel and/or perpendicular to each other. Therefore, the idea behind estimating the principal axes is to fi nd three planes, which can form a right angle between each others normals. The fi rst step is to compare the normal of the most signi fi cant plane with the normal of the second most signi plane and so on. When a match is found, those plane normals are marked as axes for the new coordinate system. After the three planes and their corresponding normals are found, the point cloud is transformed into the new coordinate system. This was the initial guess stage of the estimation procedure.
 Although now the points are aligned with the real-world Cartesian system of coordinates, the orientation of the three vectors representing the axes is most probably incorrect, as it can be seen in Fig. 3 (b). But the method will correct the axes in the second step, which is described in the following subsection. Also, it is important to mention that usually only two normal planes which are at a right angle are needed, since the cross product between the two mentioned normals returns the third axis. 4.3. Quadrilateral detection
A strategy is detailed here for detecting quadrilateral-like shapes and for explaining the reasoning behind the classi process used.

The work is based on the idea presented in Rusu et al. (2008) although in this work cuboid fi tting was based on four extracted line features while in the current paper this requirement is relaxed and only three lines are considered for this purpose. This allows the fi tting to be performed on lower density or sparser data too.
This is especially useful for experiments in non-laboratory condi-tions or with noisy data such as in our outdoor measurements. Another major difference between the method presented in
Rusu et al. (2008) and the current one is the scan area that is considered for classi fi cation. While in our method a whole room is considered, in the original method only a single scan with limited angle of view is used as data source. Also the number of data sets on which the experiments were performed makes the qualitative comparison dif fi cult, as in the original work only a single scenario is analyzed while in this paper more than 10 scenes are considered.
The method uses these quadrilaterals to classify points which belong to doors, windows, or frames, based on their physical sizes and positioning inside the scene. By using this approach, the classi fi cation can be effortlessly extended for other rectangular-shaped components, e.g. furniture pieces, radiators, or trash bins.
But in order to perform quadrilateral fi tting, the method would need to execute these steps: 1. compute boundary points for each segmented plane previously found; 2. detect line models inside the sets of boundaries using the
RANSAC algorithm; 3. analyze line segments to fi nd candidates which can form rectangular-like shapes.
 ( Point Cloud Library ) project. For visualizing the boundaries take a look at Fig. 3 (b), where each plane has its boundaries colored differently. Then the method continues by fi tting lines much similar to the plane segmentation routine presented in the pre-vious subsection. Whereas here the threshold for lines is set to 25 point inliers, which is also a value deduced empirically. each other to determine if formations of rectangular-like shapes are possible. Finding quadrilateral-like forms which have right angles is done in an iterative fashion. For each iteration, a new line is added to a current shape, i.e. rectangular, if two conditions are ful fi lled: (a) at least one, if not both, of the segment's ends should be in the (b) the angle between the newly added segment and the existing four line segments is found making up a rectangular shape. On the other hand, shapes which have less than three line segments are rejected. This routine stops when there are no more line segments to be analyzed. For a better understanding consider Algorithm 4 .
Naturally, the results of this routine are in fl uenced by the point cloud density, hence more points result in a better accuracy. their sizes, to see if there are any doors or windows. Usually, the doors encountered so far were around 2000 mm by 800 mm, and windows around 1200 mm by 600 mm, whereas walls have a height of around 3000 mm. Also, this is not the only criterion by which rectangles are classi fi ed as doors or windows. There is also the positioning of those rectangles in their supporting quadrilat-erals, i.e. usually walls. Therefore, the method checks also for the relationships between rectangles, i.e. the relative position in comparison with one another. With the help of this information, the correct alignment of the coordinate system  X  see Fig. 3 be detected.

As an example, think of a smaller rectangle, which is very close to one of the edges of a bigger rectangle. If that smaller rectangle within the bigger rectangle has a size close to the system's thresholds, then it can be asserted that the smaller rectangle is a door, which lies on the fl oor, and that the Z -axis should be oriented upwards in the door's direction.
 If the normal of the door is then considered, as for example
Y -axis, which is at a right angle with the newly found Z -axis, the cross product can be computed and the X -axis is obtained. The point clouds are transformed according to the new axes, and the correc-tion of alignment has been ful fi lled. Thus having the real-world 4.4. Detection of furniture Our method is based not only on similar concept proposed in
Tuttas and Stilla (2011) but also incorporates constrains on the position and size of the detected windows. Although windows and pictures may have the same silhouette, the way that they lay in the supporting plane (i.e. the windows are embedded in the plane, while the pictures usually tumble out from the plane) makes them distinguishable.

The number of furniture classi fi ed in the work ( Rusu et al., 2008 ) is lower than that in the current work, the focus being in a statistical analysis of the proposed algorithms. The limitation of the current approach is related to the objects classes which were considered at the design phase for the classi fi er.

The system extracts relevant planes from the registered point cloud, categorizes them as doors or drawers, walls, fl oor, ceiling, and tables or other horizontal structures. This is achieved by locating the relevant planar structures, testing for the existence of fi xtures, and segmenting the different doors.

As an exhaustive search for all planes is computationally intractable, the method is only searching for those that are aligned with the walls of the room. The orientations of the main walls are determined using a RANSAC-based ( Fischler and Bolles, 1981 ) approach on the normal sphere, as described in Marton et al. (2010) . Since in many indoor environments, most of the surface normals estimated coincide with one of the three main axes of the room, these directions can be used to limit the plane extraction, as it can be seen from Fig. 4 .

After extracting the primary planes, they are classi fi ed into fl oor and ceiling based on horizontal orientation and height, and the walls based on the observation that they are adjacent to the ceiling. The remaining planar connected components  X  if they exceed a minimum size  X  constitute candidates for tables or furniture faces as suggested in Algorithm 5 . This minimum size is the empirically deduced value of 500 point inliers in herein presented experiments.

Fixtures are detected by fi rst fi nding point clusters that are within the polygonal prism of the furniture faces using Euclidean distance measure and then fi t RANSAC lines or circles to those clusters and thereby differentiate between handles and knobs. A down-sampled version is used, i.e. voxel size 3.5 cm, of the point cloud for speed considerations and to simplify the computation of patch areas.

Kitchen appliances, doors and drawers typically have fi xtures that allow interaction with them. The existence of fi xtures is a good indication to the presence of these objects, so the algorithm searches for clusters of points in the vicinity of detected vertical planar structures. Since the ultimate goal is the manipulation of the handles by the robot, clusters that are too big in diameter relative to the gripper aperture are discarded. These fi lters are simple enough to be performed for all possible clusters and explain all the fi xtures in typical kitchen environments. The results of this process can be visualized in Fig. 5 .
 4.5. Experimental results
The proposed system was tested on several point clouds, recorded inside various of fi ce and kitchen environments. Table 4 presents empirical results for 13 different data sets, out of which 4 are complete scans of rooms, while the other 9 are only partial scans. These data sets were recorded with two different devices; 8 of them using a Hokuyo UTM-30LX sensor, mounted on a PR2 robot; and remaining 5 were taken with a Sick LMS 200 sensor, mounted on a P3-AT robot. Both scanning devices are fi tted with tilting mechanisms in order to attain 3D point clouds.
The obtained results are promising, considering the accuracy values shown in Table 4 . Detection of indoor components was achieved by identifying planar patches and line segments in the above-mentioned data sets. The fl oors and ceilings are detected fl awlessly, as they are the most simple indoor components. Walls and furniture parts, along with their corresponding handles, are also accurately identi fi ed; whereas doors and windows, with their frames, are more complicated to detect. Doors can easily blend in as walls, while windows are harder to detect, due to their transparent nature.
During the experiments, it was also observed that success rates of handle detection are directly pr oportional to their physical sizes ( Fig. 6 ). 5. Conclusions
This work covers the necessary steps for a 3D environment perception in different environments for automatic reasoning in case of autonomous agents. As the fi rst step of the perception, the data acquisition and preprocessing part is presented including the scan registration. For the registration algorithms a robust keypoint feature based variant was proposed which proved to provide reliable results for sparse and cluttered environments.
Further on a system was presented for labeling 3D point clouds taken from human indoor environments by relying on physical frame , door , window frame , window , furniture and handle . Also described, is a technique for estimating the principal axes while dealing with 3D indoor data sets. From experimental observations, the presented approach is relatively robust to noise and easy to compute. The method was tested on different data sets with promising results.
 research. One would be the interpretation of registered clouds taken from human indoor environments. Also, it can be useful to know where basic components, e.g. doors or windows, are situated for certain environment mapping algorithms.
 this work, both short-term and long-term. In short-term it is the pipeline, e.g. heating radiators and furniture pieces such as round windows and paintings. And for long-term the perception can be improved by incorporating vision into the process, thus contributing to the overall robustness. Using the Hough transform instead of the RANSAC algorithm is also taken into consideration.
Despite its lack of randomness, using the Hough transform ensures the best possible fi tted shapes, each time the labeling pipeline is executed.
 Acknowledgments support of multidisciplinary postdoctoral programs in major technical areas of national strategy of Research  X  Development
Innovation  X  4D-POSTDOC, contract no. POSDRU/89/1.5/S/52603, project co-funded by the European Social Fund through Sectoral
Operational Program Human Resources Development 2007  X  2013, and by the German Academic Exchange Service (or DAAD  X 
Deutscher Akademischer Austauschdienst) and by SCIEX NMS-CH project number 12.239 .
 Appendix A. Pseudo-code algorithms Algorithm 1. ICP with initial alignment.

Input: P s , P t 1: F s  X  ComputeFPFH ( P s ); 2: F t  X  ComputeFPFH ( P t ); 3:  X  t n ; A f  X  X  InitialAlignment  X  F s ; F t  X  ; 4: while  X  error diff o  X   X  3  X  iter o iter max  X  do 5: A d  X  GetClosestPoints  X  t n ; P s ; P t  X  ; 6: t n  X  arg min 1 j A 7: end while Output: t n Algorithm 2. Estimating principal axes.
 Input: cloud // Data of complete room.

Input: axes  X  | // Empty set of vectors. 2: repeat // 5: if bool 1  X  0 then // Most signi fi cant plane. 6: plane 1  X  plane // Keep detected plane. 7: axes  X  normal 1 // Add plane normal to axes. 8: bool 1  X  1 // Update bool variable. 9: else // 10: if bool 2  X  0 then // 11: if plane 1 ? plane then // Check if perpendicular. 12: plane 2  X  plane // 13: axes  X  normal 2 // 14: bool 2  X  1 // 15: end if // 16: else // 17: if bool 3  X  0 then // 18: if plane 2 ? plane then // 19: plane 3  X  plane // 20: axes  X  normal 3 // 21: bool 3  X  1 // 22: end if // 23: end if // 24: end if // 25: end if // 27: end if // 29: bool  X  0 // stop fi tting planes. 30: end if // Algorithm 3. Fitting lines to boundaries of planes.
 Input: Planes  X f pp 1 ; pp 2 ; ... ; pp m g // Detected plane inliers.
Input: Lines  X  | // Empty set of lines, and
Input: Inliers  X  | // corresponding inliers. 1: for k to sizeOf ( Planes ) do // 2: bp k  X  getBoundaryPoints  X  pp k  X  // Get boundaries of plane. 3: repeat // 4:  X  lm k ; ip k  X  fitLineToCloud  X  bp k  X  // Fit line to boundaries. 6: cp k  X  passLargestCluster  X  Clusters  X  // The biggest cluster. 8: lm k -Lines // Add found line to set, 9: ip k -Inliers // save its inliers also. 11: end if // 12: until sizeOf  X  bp k  X  Z minSizeOfInliers // Stopping conditions. 13: end for // Output: Lines  X f lm 1 ; lm 2 ; ... ; lm n g // Set of line models, pp  X  points of planes ; which were detected along the axes bp  X  boundary points ; found on the edges of planes lm  X  line models ; fitted to the boundary points of planes ip  X  inlier points ; of the previously found line models cp  X  cluster points ; extracted from each of the line ' s inliers Algorithm 4. Estimating quadrilateral-like shapes.
 Input: n // Number of fi tted lines.
 Input: H  X f line 1 ; line 2 ; ... ; line n g // Set of line models.
Input: G  X  | // Empty quadrilateral set. 1: repeat // 2: line  X  H  X  1  X  // Start with fi rst line. 3: quad  X  | // Initialize empty quad. 4: repeat // 5: if quad  X  X  | then // First edge of quad. 6: add line to quad // Add line to quad. 7: delete line from H // Remove line from set. 8: update n // Decrease number of lines. 9: line  X  H  X  1  X  // Restart with fi rst line. 10: else // 12: add line to quad // mentioned above. 13: delete line from H // 14: update n // 15: line  X  H  X  1  X  // 16: else // 17: line  X  H  X  indexOf  X  line  X  X  1  X  // Continue with next line. 18: end if // 19: end if // 21: if sizeOf  X  quad  X  4 2 then // More than two lines, 22: add quad to G // add quadrilateral to set. 23: end if // 24: until H a | // No more lines in set.
 Algorithm 5. Detecting furniture surfaces and fi xtures. Input: cloud // Input data set.
 Input: axes  X f X ; Y ; Z g // Orthogonal axes of room.
Input: surfaces  X  | // Empty set of surfaces,
Input: handles  X  | // and furniture fi xtures.

FURNITURE SURFACES 1: for i to sizeOf ( axes ) do 2: repeat 3:  X  plane _ model ; plane _ inliers  X  detectPlaneAlongAxis  X  cloud axes  X  i  X  4: if sizeOf  X  plane _ inliers  X  Z minPlaneInliers then 5: extractPointsFromCloud  X  cloud ; plane _ inliers  X  6: plane _ clusters  X  getClustersFromPoints  X  plane _ inliers  X  7: for j to sizeOf  X  plane _ clusters  X  do 8: if sizeOf  X  plane _ clusters  X  j  X  Z minPlaneCluster then 9: surfaces  X  plane _ clusters  X  j 10: end if 11: end for 12: end if 13: until sizeOf  X  cloud  X  Z minPlaneInliers 14: end for
FURNITURE FIXTURES 15: for i to sizeOf ( surfaces ) do 16: if  X  surfaces  X  i ? floor  X  4  X  heightOf  X  surfaces  X  i  X  17: points  X  getPointsOnSurface  X  surfaces  X  i  X  18: fixtures  X  getClustersFromPoints  X  points  X  19: for j to sizeOf ( fi xtures ) do 20: repeat 21:  X  line _ model ; line _ inliers  X  fitLineToCloud  X  fixtures  X  j  X  22: if  X  sizeOf  X  line _ inliers  X  Z minLineInliers  X  then 23: extractPointsFromCloud  X  fixtures  X  j ; line _ inliers  X  24: line _ clusters  X  getClustersFromPoints  X  line _ inliers  X  25: for k to sizeOf  X  line _ clusters  X  do 26: if sizeOf  X  line _ clusters  X  k  X  Z minLineCluster then 27: handles  X  line _ clusters  X  k 28: end if 29: end for 30: end if 31: until sizeOf  X  fixtures  X  j  X  Z minLineInliers 32: end for 33: end if 34: end for References
