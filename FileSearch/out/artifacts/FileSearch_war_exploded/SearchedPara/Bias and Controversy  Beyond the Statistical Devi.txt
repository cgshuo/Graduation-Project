 In this paper, we investigate how deviation in evaluation ac-tivities may reveal bias on the part of reviewers and contro-versy on the part of evaluated objects. We focus on a  X  X ata-centric approach X  where the evaluation data is assumed to represent the  X  X round truth X . The standard statistical ap-proaches take evaluation and deviation at face value. We argue that attention should be paid to the subjectivity of evaluation, judging the evaluation score not just on  X  X hat is being said X  (deviation), but also on  X  X ho says it X  (reviewer) as well as on  X  X hom it is said about X  (object). Furthermore, we observe that bias and cont roversy are mutually depen-dent, as there is more bias if there is higher deviation on a less controversial object. To address this mutual depen-dency, we propose a reinforcement model to identify bias and controversy. We test our model on real-life data to verify its applicability.
 Categories and Subject Descriptors: H.4 [Information Systems Applications]; J.4 [Social and Behavioral Sciences] General Terms: Algorithms, Experimentation, Measure-ment Keywords: bias, controversy, evaluation, social network
Evaluation or assessment is a fundamental activity in our life because it touches on various areas of human concerns. Students evaluate instructors; referees evaluate athletes; re-viewers evaluate submitted papers. Online evaluation is just as prevalent, if not more. For example, product re-view sites allow users to assign ratings to goods, such as www.amazon.com and www.imdb.com. In any evaluation, the key questions include whether the evaluation is  X  X air X , whether reviewers are  X  X iased X , whether a large deviation is normal. For instance, an article in  X  X he Scientist X  [12] raises such questions on the peer review practice. Another  X  The third author X  X  work was done while he was visiting Nanyang Technological University, Singapore.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. example is buzz or stealth marketing [15] where companies hire people to post the ratings of products. The questions on  X  X ias X  and  X  X airness X  are important and yet are very difficult to define and answer. For one reason,  X  X ias X  and  X  X airness X  are subjective in that differen t people have different views. For another reason, reviewers and evaluated objects are not  X  X niform X . Some objects are mo re controversial and a larger deviation among reviewers is expected.

In this paper, we study the notions of  X  X ias X  of review-ers and  X  X ontroversy X  of objects evaluated. Given the sub-jective nature of  X  X ias X  and  X  X ontroversy X , a complete an-swer to these questions goes way beyond computer science. Therefore, our study focuses on a  X  X ata-centric approach X , where  X  X ias X  and  X  X ontroversy X  can be objectively quanti-fied from the evaluation scores given by reviewers to objects. In this approach, we assume that the evaluation scores rep-resent the  X  X round truth X  that can be trusted for the study. In particular, there is no fraud in the assignment of reviewers to an object. For example, if the reviewers for an object are chosen deliberately in favor of or against the object and if all reviewers give similar scores to the object, the  X  X ata-centric approach X  is not able to identify the  X  X ias X  caused by such systematic frauds. Essentially, our approach assumes that most reviewers are  X  X onest X  in the sense of acting according to their best judgment and yet can still be  X  X iased X . Moreover, we do not try to identify the causes of  X  X ias X . There are too many possible reasons behind  X  X ias X  and re-viewers may have been influenced by different ones. Instead, we focus on identifying and measuring the manifestation of  X  X ias X , which in turn can be used to investigate the causes of  X  X ias X . The same can be said for  X  X ontroversy X . We for-mally define our notions of  X  X ias X  and  X  X ontroversy X  shortly.
In its most basic construct, an evaluation system consists of the type of reviewers and the type of objects. A reviewer r may assign to an object o j an evaluation score e ij  X  [0 , 1]. Here,weusetheterms reviewer and object in the general sense, referring not to what they are, but to their respective roles. It may well be the case that both reviewers and ob-jects are of the same type (e.g., person). A bipartite graph representationisgiveninFigure1. Thevaluesgiveninthe figure are e ij values, e.g., e 11 =0 . 7. For each e ij derive d ij  X  [0 , 1], which measures r i  X  X  deviation from the consensus (such as mean or median) of o j .Givensucha graph, we seek to measure the bias value b i  X  [0 , 1] of each r and the controversy value c j  X  [0 , 1] of each o j .
A straightforward solution to the problem stated above is to employ standard statistical measures. Assuming that d ij is known, the bias value b i maysimplybetheaverage deviation by r i on all objects she has evaluated, as given in Equation 1. The controversy value c j may simply be the average deviation on o j by all reviewers evaluating it, as given in Equation 2. We call this pair of equations the Naive solution.
For example, for the scenario in Figure 1, the Naive solu-tion would conclude that r 1 is less biased than r 5 . Here, we derive d ij as the absolute distance from e ij to the mean eval-uation by all reviewers of o j . For instance, we have d 11 and d 52 =0 . 375. Since r 1 evaluates only o 1 and r 5 evalu-ates only o 2 , according to Equation 1, b 1 = d 11 =0 . 3and b = d 52 =0 . 375. We see that b 1 &lt;b 5 , concluding r 1 biased than r 5 .
The Naive approach is akin to taking deviation at its face value. It is therefore naive as it treats all reviewers and ob-jects equally. To use an analogy, the approach is to take into account only  X  X hat is being said X  (deviation) while ignoring  X  X ho says it X  (reviewer) and  X  X bout whom it is said X  (object). However, deviation could have arisen due to either bias or controversy. Thus, there is a need to pay attention to the particular reviewer or object that a deviation concerns.
It is further observed that bia s and controversy are inter-related quantities. When determining how biased a reviewer is, we should use deviation attributed to the bias of this reviewer, and not to the controversy of evaluated objects. Similarly, when determining how controversial an object is, we should use deviation attributed to the controversy of this object, and not to the bias of evaluating reviewers.
In this paper, we investigate the two main issues ignored by the Naive model in quantifying bias and controversy. Subjectivity In determining bias and controversy, we should Mutual Dependency Bias and controversy are mutually
We now present the following observation of bias and con-troversy that underlines our basic approach to this problem. Bias A reviewer is more biased if there is more deviation Controversy An object is more controversial if there is
Re-examining the example in Figure 1, based on the above observation, we now argue that r 1 is in fact more biased than r 5 . Visual inspection would reveal that co-reviewers of o 1 are much more in agreement (with 3 out of 4 reviewers agreeing on the score) than co-reviewers of o 2 (with 4 di-vergent scores). Therefore, o 2 is more controversial than o because there is a lack of consensus among o 2  X  X  reviewers. However, in this case, r 5 may not be biased as deviation d may be attributed to the controversy of o 2 . On the other hand, r 1 deviates on an object that her co-reviewers could agree upon, implying bias on her part. Thus, Naive has incorrectly concluded that r 1 is less biased than r 5 .
We present a new approach to the problem of quantify-ing bias and controversy within an evaluation system. First, we propose the above observation that incorporates a new notion of mutual dependency between bias and controversy. This will subsequently be developed into a reinforcement-based model. Interestingly, this model has an underlying presumption resulting in the so-called  X  X o evidence cases X . Moreover, we also examine several issues that significantly affect the outcome of this model. Finally, we conduct ex-periments on real-life data to analyze how our proposed ap-proach is different from the Naive approach.

The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 develops the framework of our reinforcement model. Section 4 highlights several issues that would influence the outcome of the model. Section 5 seeks to verify the model X  X  applicability through experiments on a real dataset. Section 6 concludes this paper.
The study of evaluation systems has also been conducted in fields outside computer science. In management science, [5] looks at how in evaluating start-up teams, venture capi-talists seem to favor those similar to themselves, while [13] investigates how using a diversity of objective and subjective measures in performance evaluation may affect the evalua-tion scores assigned. Different from these works, we do not factor in any other information besides the evaluation scores assigned by reviewers to objects.

The bipartite structure of evaluation systems resembles two-mode social networks (consisting of two types of nodes). One classic problem in such networks is identifying the cen-tral nodes [4] or those playing mediative role in facilitating linkages among nodes of both types. Alternatively, anoma-lous nodes [16] or those with low affiliation to any neighbor-hood may be of interest.

Beyond bipartite structures, there are also other works on social networks or Web graphs, such as identifying the most influential individuals [8] or finding the most interest-ing connections among several nodes [3] or grouping together related Web pages into Web communities [6]. None of these works is concerned with evaluation data. Moreover, most deal with non-directional relationships and graph topolo-gies. Thus far, we have not come across any existing work that addresses the issues of bias and controversy.
The iterative computation method used to implement the mutual dependency in this paper has first been addressed in linear algebra [1, 7]. Several other works have also made use of mutual dependency property, notably as applied to ranking pages for Web search [9, 14] and ranking products based on propagated profitability [17]. However, such works are primarily based on the notion of popularity, which is not congruent with either bias or controversy. Simply evaluating the most number of objects would not imply bias.

Finally, our work also has some relation to outlier detec-tion [10], which is concerned with identifying points, from a set of points, that are far away from the majority of points. In a way, the distance measure is similar to the statistical ap-proach of taking deviation at face value. For instance, high deviation by a biased reviewer may mark her as an outlier. However, in our approach, how outlying such a reviewer is would depend on how controversial the concerned object is. These notions of subjectivity and mutual dependency again are not usually factored into outlier detection problems.
In this section, we develop a computational model of bias and controversy that factors in their mutual dependency. While the raw data would likely contain evaluation score e , as a matter of generality, in our model development, we work with deviation value d ij . More on how deviation may be derived from evaluation will be discussed in Section 4.1.
The Naive model simply translates more deviation to more bias. In fact, deviation could have arisen because of either bias or controversy. As much as possible, we should at-tribute to a reviewer only deviation due to her bias, and not to the controversy of evaluated object. Therein lies our proposed approach: to reduce the amount of deviation at-tributed to bias by the amount of controversy that could have contributed to that deviation.

This approach is summed up by the pair of Equation 3, which determines bias, and Equation 4, which determines controversy. Here, we use  X  b i and  X  c j to denote the comple-ments of b i and c j respectively, which means that  X  b i grow inversely with b i and c j respectively. Moreover, we use A gg to represent the class of aggregate functions to combine the relevant values over i or j respectively. An appropriate aggregate function should yield a value that is representa-tive of a reviewer or an object X  X   X  X ehavior X . We avoid using summation so as not to incorporate the notion of popularity. Possible options include minimum , maximum ,and average . Particularly, average is an intuitive choice as it takes into account repeated deviation by a biased reviewer.
The above equations reflect the inversely proportional re-lationship between bias and controversy, which gives this model its name: Inverse Reinforcement or IR model. A re-viewer X  X  bias value is higher for high deviation on objects with low controversy (high  X  c j values). An object X  X  contro-versy value is higher for high deviation by reviewers with low bias (high  X  b i values).

A reviewer X  X  deviation values on less controversial objects would better reveal her bias, as more controversial objects reflect their own controversy. In a way, we rely more on the less controversial objects (high  X  c j values) as  X  X vidence X  to reveal bias. However, in the case where a reviewer evaluates only very controversial objects, there is no  X  X vidence X  to reveal her bias. Thus, we refer to reviewers who have evalu-ated only very controversial objects as  X  X o evidence cases X . Such  X  X o evidence cases X  will be assigned low bias values by the IR model (Equation 3). Similar remarks can be made on the controversy of objects. Hence, the IR model adopts the presumption below.

Presumption
Therefore, in the presence of  X  X o evidence cases X , this pre-sumption bears the following implications on bias (similarly on controversy). 1. We have more  X  X onfidence X  on those assigned high bias 2. We have less  X  X onfidence X  on those assigned low bias
Because the data may potentially contain  X  X o evidence cases X , we focus on the more  X  X onfident X  aspects of the out-come of IR model. We recommend that IR should primar-ily be used for identifying biased reviewers and controversial objects. However, in practice, several steps may be taken to avoid  X  X o evidence cases X , such as (1) having reviewers review more objects to increase the probability of having non-controversial objects or (2) ensuring each reviewer is al-located at least a few non-controversial objects, assuming we have prior knowledge about the controversy of objects.
Note also that when the effects of mutual dependency are removed, IR degenerates into Naive . For instance, by fixing  X  c as a constant in Equation 3, the b i values determined by Equation 3 will have the same ordering as those determined by Equation 1.
Several issues that may affect the effectiveness of IR  X  X  ap-plication include the derivation of deviation from evaluation and the convergence of the iterative method for IR .
In Section 1, we introduce one way to derive deviation from evaluation, which we now term deviation from mean . It takes deviation d ij as the absolute distance between r evaluation and the mean evaluation by all reviewers r k of object o j . Equation 5 gives the definition of this measure, where m j denotes the number of reviewers of o j .
Another possible deviation measure is deviation from co-reviewers . Thismeasuretakesdeviation d ij as the average distance between r i  X  X  evaluation and the evaluation by each co-reviewer r k .For m j &gt; 1numberofreviewersof o j (in-cluding r i ), d ij can be worked out according to Equation 6.
For example, suppose reviewers r 1 , r 2 ,and r 3 assign the following evaluations e 1 j =0 . 0, e 2 j =0 . 5, and e 3 j on the same object o j .Forthiscase, deviation from mean would yield the following deviation values, d 1 j =0 . 5, d 0 . 0, d 3 j =0 . 5, claiming that r 2 has not deviated at all. On the other hand, deviation from co-reviewers would yield d 1 j =0 . 75, d 2 j =0 . 50, d 3 j =0 . 75. Firstly, deviation from mean  X  X  claim that r 2 has not deviated at all is not reason-able, as clearly all the reviewers do not agree on o j  X  X  eval-uation score. Furthermore, we think that deviation from co-reviewers  X  X  claim that d 1 j is 1.5 times d 2 j (0 . 75 more reasonable than the deviation from mean  X  X  claim that d 1 j is infinitely greater than d 2 j (0 . 50  X  0).
The above example highlights the weakness of deviation from mean , which is more likely to produce deviation val-ues close to zero. As the number of reviewers of an ob-ject grows, the distribution of evaluation scores would likely peak at or near the mean. Deviation from the mean would then approach zero. This is disadvantageous because the ra-tio among deviation values would determine the outcome of computation. Very small values mean that a small change in absolute value may trigger a large change in ratio, making the system potentially too sensitive to small changes. Hence, deviation from co-reviewers is our recommended measure as it is more likely to have a distribution of d ij away from zero. We also use this deviation measure in the implementation of the Naive and IR models for experiments.
The computation of bias and controversy in IR can be modeled as a problem of finding an eigenvector of a square matrix. Average is the aggregate function used for the fol-lowing computation. We also assume that the linear rela-tionships b i +  X  b i =1and c j +  X  c j =1hold 1 .Wemaythen re-write Equations 3 and 4 as Equations 7 and 8 respectively. n denotes total number of objects; m denotes total number of reviewers; n i denotes number of objects evaluated by r and m j denotes number of reviewers evaluating o j .
Our matrix representation for IR is then as follows. We represent the m  X  1vectorof b i values as B , n  X  1vectorof c values as C , column vector of appropriate length whose all elements are all 1 X  X  as 1 ,and m  X  n matrix of d ij as D .From D , we may derive two other matrices, I whose each element
There are other options of defining complement mathemat-ically, such as making  X  b i the reciprocal of b i , but such options are not explored in this paper. is d ij  X  n i for corresponding i ,and J whose each element is d ij  X  m j for corresponding j . Then Equations 7 and 8 can be re-written as matrix Equations 9 and 10 respectively.
By substituting Equations 9 and 10 into each other, we have recursive Equations 11 and 12.

Suppose for any w  X  1columnvector W , we use the nota-tion W m to denote w  X  m matrix formed by replicating W across m columns. If B is L 1 -normalized, i.e., m i =1 | then W m B = W holds. We use this notation to transform the previous equations into equivalent Equations 13 and 14.
Factorizing out B from the right-hand side of Equation 13 and C from the right-hand side of Equation 14 would yield recursive forms B = XB and C = YC . The iterative pro-cess for B is given by B k +1 = XB k , where the output of the k -th iteration is used as input for the ( k + 1)-th itera-tion. Subject to the assumption that the square asymmet-ric matrix X is diagonalizable (it has linearly-independent eigenvectors) and has a uniquely largest eigenvalue [7], then as k increases, B k will converge to the dominant eigenvec-tor of X almost independently of the initial B 0 . If desired, these conditions for convergence can be tested, for instance by inspecting the eigenvalues or eigenvectors of the square matrix [1]. In that case, eigenvalues or eigenvectors may be determined using other methods such as [11]. Experimen-tally, convergence can be observed as stable B values (after normalization) across consecutive iterations. Convergence for C can be similarly argued.

Normalization Before each iteration, the input vector (e.g., B ) is normalized. Normalization maintains an invari-ant state between two consecutive iterations so that conver-gence can be observed as no change or very little change in values. It involves dividing elements of a vector by a con-stant, such that their relative ratio remains unchanged.
L p normalization of a vector B results in m i =1 | b i | p 1. Commonly L 1 or L 2 is used [9, 14]. The summation means that as m increases, the individual b i approaches zero. When b i  X  0, IR (Equation 8) may degenerate into Naive (Equation 2). To counter the effect of summation, higher values of p could be used. We employ L  X  normalization, which is equivalent to dividing vector elements by the largest one. The largest element after L  X  normalization is 1.
The objective is to compare the efficacy of Naive and IR in identifying bias and controversy. First, exemplary reviewers and objects are examined. Then, ranked lists by Naive and IR are compared using various similarity measures.
All our experimental runs involve few iterations and con-verge in less than a second. Computational complexity is not an important issue and will not be further examined.
The data is acquired by crawling the product review Web site Epinions (www.epinions.com) for two days, starting with the seed page  X  X pinions Top Reviewers in Books X  2 .The crawled Web pages represent a subset of all products, re-viewers and evaluation ratings available from Epinions. The subset consists of 57320 web pages capturing 3797 products, 14607 reviewers, and 24008 evaluation ratings.

We impose several filtering conditions to make the data more suitable for experiments. Firstly, we prune the net-work such that all products have at least 5 reviewers and all reviewers have rated at least 3 products. This weeds out the occasional reviewers and products and gives greater support in determining a reviewer or product X  X   X  X ehavior X . Any higher threshold would result in too small a network. Epinions assigns each product a category. The three most popular categories in the dataset are books , videos ,and mu-sic . After filtering, only videos has a significant network size, with 113 products (objects), 138 reviewers, and 910 evalua-tion ratings. This category is selected for further analysis.
Since the focus of the experiments is not on scalability, the selection of data is not so much driven by the size of the data. The data selected is reasonably large for the propagation effect to take place within the network, and yet is not so overly large that analysis of the results is made difficult.
A reviewer assigns 0 to 5 stars to an object, with 5 being the best. We rescale these evaluation scores to a range from 0to1byasimpledivisionby5(e.g.,2starsis0.4). Below are specific examples contrasting how Naive and IR determine biased reviewers and controversial objects.
Biased (IR) vs. Less Biased (Naive) Reviewers are placed in ranked lists in descending order of bias values (highest bias value is rank 1) as computed by Naive and IR respectively. First, we look at a reviewer who is assigned a lower bias rank by Naive than by IR . user-dlockeretz ,whose profile is given in Table 1, is ranked 16 by Naive and 4 by IR . This profile includes d ij values and controversy ranks (high-est controversy value is rank 1) of the objects she reviewed. Notably, user-dlockeretz has high deviation on the first two objects. These objects also have very low controversy ranks by IR (ranks 90 and 93 out of 113). Furthermore, these http://www.epinions.com/member/community lists. html/show  X  6/display list  X  true/vert  X  3321654/ year  X  1900/sec  X  community member list/pp  X  1/pa  X  1 Table 2: Controversy Rank for mvie mu-1016922 two objects are given lower controversy ranks by IR than by Naive . Given these objects X  low controversy, IR takes the high d ij values more seriously. Naive ignores these objects X  low controversy and decides based on deviation alone. Controversial (IR) vs. Less Controversial (Naive) Next, we examine an object given a lower controversy rank by Naive than by IR . For instance, mvie mu-1016922 (Ta-ble 2), is ranked 13 by Naive , but 8 by IR .Lookingatthe bias ranks of mvie mu-1016922  X  X  reviewers, we see that some of these reviewers have very low bias ranks (ranks 97, 98, 108 out of 138). Also, the bias ranks assigned by IR to mvie mu-1016922  X  X  reviewers tend to be lower than those by Naive . Deviation by reviewers with low bias values would better reflect mvie mu-1016922  X  X  controversy. Naive ignores this notion of subjectivity, and decides on a lower controversy rank of mvie mu-1016922 based solely on deviation values.
Less Biased (IR) vs. Biased (Naive) We present one example where Naive  X  X  claim of bias is not really substan-tiated. Consider user-ynmaeven whoseprofileisgivenin Table 3. The objects on which user-ynmaeven has highest deviation on also have very high controversy ranks (ranks 2, 5, 9). The high deviation values could be attributed to the high controversy of these objects. There is no substantial case to claim that user-ynmaeven is really biased.
Due to space constraint, example of the only other case (objects determined to be highly controversial by Naive but less by IR ) is not given here, but similar results apply.
To see if the differences between IR and Naive surface on a larger scale as well, we compare greater subsections of the ranked lists (top 10%, 20%, 30%). We focus on the most biased (or controversial) ends of the ranked lists as these are the ends targeted by IR (see Section 3).
For comparing two ranked lists, we use three similarity functions originally proposed to compare various permuta-tions [2], with some adaptations for our needs. Table 4: Most Biased and Controversial: Naive vs. IR
Overlap Similarity between two ranked lists is the pro-portion of items common to both lists. For two ranked lists  X  and  X  2 of length n ,where A is the set of items in  X  1 and B the set of items in  X  2 ,the Overlap similarity between the two lists can be evaluated as shown in Equation 15. Overlap similarity ranges from 0 (disjoint) to 1 (total overlap).
Kendall Similarity [2] counts the number of pairs for which the two ranked lists agree on their ordering. Hence, Kendall similarity penalizes for each pair of items ( x , y ) in the other list. Equation 16 shows how this similarity is evaluated. Kendall similarity ranges from 0 (completely reversed) to 1 (completely identical).

Spearman Similarity [2] counts, for each item x ,the difference between its rank in the first list rank 1 ( x )andits rank in the second list rank 2 ( x ). The aggregate differences across all items in the list contribute to the final similarity score as in Equation 17. The normalization denominator is N = n 2 / 2 for even values of n and N =( n +1)( n  X  1) / 2 for odd values of n . Spearman similarity ranges from 0 (completely reversed) to 1 (completely identical).
Spearman (  X  1 , X  2 )=1  X 
The top k % of two lists may not contain the same set of reviewers or objects. For Kendall and Spearman ,wetakethe top k %itemsof IR as the reference set. We then construct a ranked list of the same items according to their ordering in Naive , with any gap between rank orders removed.
As Table 4 shows, similarity values between ranked lists produced by Naive and IR range from 0.60 to 0.88. We do not expect the ranked lists by Naive and IR to be com-pletely different. This is because in any typical evaluation system most reviewers (and objects) would  X  X ehave nor-mally X . Nevertheless, the similarity values in Table 4 sug-gest that there are significant differences between the ranked lists by Naive and IR . These differences come about due to cases, such as those in Section 5.2, where IR disagrees with Naive . Unlike Naive , IR takes into account the mutual de-pendency between bias and controversy. We further observe that Spearman values are generally the lowest compared to the other similarity values. Spearman compares exact ranks, which implies that even if Naive and IR may feature the same reviewers/objects, their ranks in respective lists would be different.
In this paper, we seek to quantify the notions of bias and controversy within an evaluation system. Deviation is a common occurrence in evaluation activities, and significant deviation may help reveal bias of reviewers or controversy of objects. However, statistical measures tend towards objec-tivity, taking deviation values as they are. Here, we propose tackling the problem based on two major issues: (1) subjec-tivity , taking into account bias of reviewer and controversy of object related to deviation and (2) mutual dependency ,rec-ognizing that quantifying bias requires knowing controversy andviceversa. Wehaveproposedthe Inverse Reinforcement or IR model based on these ideas. Another contribution is in working out several crucial i ssues that might affect the out-come, such as derivation of deviation as well as convergence of iterative computation of IR . We have also sought to verify the proposed model through experiments with real-life data, and the results have been encouraging.
