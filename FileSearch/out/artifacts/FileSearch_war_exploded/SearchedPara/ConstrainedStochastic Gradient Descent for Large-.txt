 The least squares problem is one of the most important re-gression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochas-tic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound O (log T ), and fastest convergence speed among all first or-der approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance.
 G.1.6 [ Optimization ]: Least squares methods, Stochastic programming; I.2.6 [ Learning ]: Parameter learning Stochastic optimization, Large-scale least squares, online learning
The stochastic least squares problem aims to find the co-efficient w  X  R d to minimize the following objective function  X  Co rresponding author at step t where ( x i , y i )  X  X  X Y is an input-output pair randomly drawn from data set ( X , Y ) endowed in a distribution D with x i  X  R d and y i  X  R , w  X  t +1  X  R d is a parameter that minimizes the empirical least squares loss at step t , and l ( w , x t , y t ) = 1 2 y t  X  w T x t 2 2 is the empirical risk.
For large-scale problems, the classical optimization meth-ods, such as interior point method and conjugate gradient descent, have to scan all data points several times in order to evaluate the objective function and find the optimal w Recently, Stochastic Gradient Descent (SGD) [7, 29, 9, 3, 20, 13] methods show its promising efficiency in solving large-scale problems. Some of them have been widely applied to the least squares problem. The Least Mean Squares (LMS) algorithm [25] is the standard first order SGD, which takes a scalar as the learning rate. The Recursive Least Squares (RLS) approach [25, 15] is an instantiation of the stochastic Newton method by replacing the scalar learning rate with an approximation of the Hessian matrix inverse. The Aver-aged Stochastic Gradient Descent (ASGD) [21] averages the SGD results to estimate w  X  . ASGD converges more stably than SGD. Its convergence rate even approaches to that of second order method, when the estimator is sufficiently close to w  X  . This happens after processing a huge amount of data points.

Since the least squares loss l ( w t , X , Y ) is usually strongly convex [5], the first order approaches can converge at the rate of O (1 /T ). Given the smallest eigenvalue  X  0 of the Hessian matrix [8, 7, 18], many algorithms can achieve fast convergence rates and good regret bounds [7, 2, 10, 23, 11]. However, if the Hessian matrix is unknown in advance, SGD may perform poorly [18].

For high-dimensional large-scale problems, the strong con-vexity are not always guaranteed, because the smallest eigen-value of the Hessian matrix might be close to 0. Without the assumption of the strong convexity, the convergence rate of the first order approaches reduces to O (1 / ret ains computation complexity of O ( d ) in each iteration. Second order approaches, using the Hessian approximation, converge at rate O (1 /T ). Although they are appealing due to the fast convergence rate and stableness, the expensive time complexity of O ( d 2 ) when dealing with each iteration limits the use of second order approaches in practical large-scale problems.

In this paper, we prove that the linear regression line de-fined by the optimal coefficient w  X  passes through the mean point ( x ,  X  y ) of all the data points drawn from the distribu-tion D . Given this property, we can significantly improve SGD for optimizing the large-scale least squares problem by adding an equality constraint w T t +1 x t  X   X  y t = 0, where ( x is the batch mean of the collected data points till step t dur-ing the optimization iterations. The batch mean ( x t ,  X  y of Large Numbers). We term the proposed approach as the constrained SGD (CSGD). CSGD shrinks the optimal solu-tion space of the least squares problem from the entire R d to a hyper-plane in R d , thus significantly improves the con-vergence rate and the regret bound. In particular,
Note that when the data points are centralized (mean is 0 ), the constraint becomes trivial and CSGD reduces to SGD, which is the worst case for CSGD. In practical on-line learning, the collected data points, however, are often not centralized, and thus CSGD is preferred. In this paper, we only discuss the properties of CSGD when data points are not centralized.

Notations . We denote the input data point x t = [1  X  x t [ x t , . . . , x where x (1) t = 1 is the first element of x t and w (1) t parameter [6].  X  X  X  X  p is the L p norm,  X  X  X  X  2 p is the squared L norm, | X | is the absolute operation for scalars and l ( w ) is abbreviated for l ( w , x t , y t ).
We present the Constrained Stochastic Gradient Descent (CSGD) algorithm for the large-scale least squares problem by incorporating SGD with the fact that the linear regression line passes through the mean point of all the data points.
The standard Stochastic Gradient Descent (SGD) algo-rithm takes the form of where  X  t is an appropriate learning rate, g t is the gradient of the loss function l ( w , x t , y t ) = 1 2 y t  X  w T x the Euclidean projection function that projects w onto the predefined convex set  X  by
In least squares, w is defined in the entire R d , and  X  ( can be taken off. Thus, the search space of SGD is the entire R d to obtain the optimal solution.

According to Theorem 2.1 ( cf . Section 2.2), we add a constraint w T x t  X   X  y t = 0 at step t to SGD, where ( x is an unbiased estimation to ( x ,  X  y ) after t iterations, and obtain CSGD w
The constraint in Eq.(4) determines the hyper-plane  X  t = { w | w T x t =  X  y t } residing in R d .

By replacing  X  with  X  t in Eq.(2), we have
The projection function  X  t (  X  ) projects a point onto the hyper-plane  X  t . By solving Eq.(3),  X  t (  X  ) is uniquely defined at each step by where P t is the projection matrix at step t and takes the form of where P t  X  R d  X  d is idempotent and projects a vector onto the subspace generated by x t , and r t = y t  X  x By combining Eqs.(5) and (6), the iterative procedure for CSGD is
We can obtain the time and space complexities of above procedure both as O ( d ) after plugging Eq.(7) into (8) and
Algorithm 1 describes how to calculate CSGD for the least squares problem. This algorithm has the time and space complexities both of O ( d ).
 Al gorithm 1 Constrained Stochastic Gradient Descent (CSGD)
In itialize w 1 = x 0 = 0 and  X  y 0 = 0. for t = 1 , 2 , 3 , . . . do end for
Algorithm 1 relies on the fact that the optimal solution lies in a hyper-plane decided by the mean point, which leads to a significant improvement on the convergence rate and the regret bound.
The orem 2.1. (Regression line constraint) The optimal solution w  X  lies on the hyper-plane, w T x  X   X  y = 0 , which is de ned by the mean point ( x ,  X  y ) of data points drawn from the distribution X X Y endowed in D .

Proof. The loss function is explicitly defined as where w  X  (1) is the first element of w  X  .

Setting the derivative of the loss function w.r.t. w  X  (1) zero, we obtain Thus the optimal solution w  X  satisfies w T x  X   X  y = 0.
Theo rem 2.1 is the core theorem for our method. Bishop [6] applied the derivitive w.r.t the bias w  X  (1) to study the property of the bias. However, although the theorem itself is in a simple form, to the best of our knowledge, it has never been stated and applied in any approach for least squares optimization.

The mean point ( x ,  X  y ) over a distribution D is usually not given. In a stochastic approach, we can use the batch an estimation error, however, it will not lower the perfor-mance. This is because the batch optimal always satisfies this constraint when optimizing the empirical loss.
Therefore, we give the constrained estimation error bound for completeness.
 Proposition 2.2. (Constrained estimation error bound) According to the Law of Large Numbers, we assume there is a step m  X  T yeilds  X  w  X   X  2  X  x m  X  x  X  2 + |  X  y m  X   X  y Then given a tolerable small value  X  , the estimation error bound  X   X  t ( w  X  )  X  w  X   X  2  X   X  holds for any step t  X  m .
Proof. Since  X   X  t ( w  X  )  X  w  X   X  2 is the distance between w  X  and the hyper-plane  X 
Alo ng with w  X  T x  X   X  y = 0, we have the Law of Large Numbers.

Therefore,
Prop osition 2.2 states that, if at step m , ( x m ,  X  y centered at optimal solution w  X  ) lies on hyper-plane  X  m addition, the estimation error decays and its value is up-per bounded by the weighted combination of  X  x t  X  x  X  2 and |  X  y t  X   X  y | . Notice that CSGD optimizes the optimal empirical solution w  X  t that is always located on hyper-plane  X  t .
According to Theorem 2.1 and Proposition 2.2, under the assumption of regression constraint, CSGD explicitly mini-mizes the empirical loss as good as the second order SGD. According to Proposition 2.2,  X   X  t ( w  X  )  X  w  X   X  2 2 converges tial convergence rate [4] is supported by the Law of Large Numbers. Thus, we ignore the difference between X  t ( w  X  ) and w  X  in our theoretical analysis for simplicity reasons.
To study the theoretical properties of CSGD, we start from the one-step difference bound, which is crucial to ana-lyze the regret and convergence behavior. Fi gure 1: An illustrating example after step t .  X  w t +1 is the SGD result. w t +1 is the projection for  X  w t +1 on to the hyper-plane  X  t . w  X  is the optimal solution.
After iteration step t , CSGD projects the SGD result  X  w on the hyper-plane  X  t to get a new result w t +1 with direction and step size correction. An illustration is given in Figure 1. Note that, w  X  is assumed on  X  t according to Proposition 2.2.

In addition, the definition of a gradient for any g t  X   X  X  ( w t ) implies
With Eq.(12) we have the following theorems for step dif-ference bound.
 Firstly, we describe the step difference bound proved by Nemirovski for SGD.

Theorem 3.1. (Step difference bound of SGD) For any optimal solution w  X  , SGD has the following inequality be-tween steps t  X  1 and t  X  where  X  t is the learning rate at step t .
 Detail proof is given in [19].

Secondly, we prove the step difference bound of CSGD as follows.
The orem 3.2. (Step difference bound of CSGD) For any optimal solution w  X  , the following inequality holds for CSGD between steps t  X  1 and t wh ere w t =  X  t ( ^w ) and  X  g t +1 =  X  X  ( ^w t +1 , x t
Proof. Since ^w t +1 = w t  X   X  t g t , between two steps t and t , ^w t +1 and w t follows Theorem 3.1.

Therefore, we have  X 
As Euclidean projection w t +1 =  X  t ( ^w t +1 ) given in Eq.(3), which is also shown in Figure 1, has the property, Then, substituting  X  ^w t +1  X  w  X   X  2 2 given by Eq.(15) into Eq.(14) yields
By using the projection function defined in Eq.(6), we have
Since  X  g t +1 =  X  X  ( ^w t +1 , x t ,  X  y t ) =  X  x t have
A direct result of the step difference bound allows the following theorem which derives the convergence result of CSGD.

Theorem 3.3. (Loss bound) Assume (1) the norm of any gradient from  X  X  is bounded by G , (2) the norm of w  X  is less than or equal to D and (3)
Pr oof. Rearranging the bound in Theorem 3.2 and sum the loss terms over t from 1 through T and then get the sum:
The final step uses the fact that  X  w 1  X  w  X   X  2 2  X  D , where w 1 is initialized to 0 , along with  X  g t  X  2 2  X  G 2 for any t and the assumption
A corollary which is the consequence of this theorem is presented in the following. Although the convergence for CSGD follows immediately according to the Nemirovski X  X  3-line subgradient descent convergence proof [17], we present our first corollary underscoring the rate of convergence when  X  is fixed, in general is approximately 1 / X  2 , or equivalently, 1 /  X  Cor ollary 3.4. (Fixed step convergence rate) Assume Theorem 3.3 hold and for any predetermined T iterations with  X  = 1  X  min
Proof. let  X  t =  X  = 1  X  T fo r any step t , the bound for convergence rate in Theorem 3.3 becomes,
The desired bound is achieved after plugging in the spe-cific value of  X  and dividing both sides by T .

I t is clear that the fixed step convergence rate for CSGD is upper bounded by SGD, which can be achieved by taking out the G 2 .
Regret is the difference between the total loss and the opti-mal loss, which has been analyzed in most online algorithms for evaluating the correctness and convergence. we have the following theorem.
 Theorem 4.1. (Regret Bound for CSGD) the regret of CSGD is: where H is a constant. Therefore, lim sup T  X  X  X  R G ( T ) /T  X  0 .

Proof. In Theorem 3.2, Eq.(16) shows that: wh ich is shown in Figure 1.

Therefore, sum Eq.(19) over t from 1 to T , we have
By adding a dummy term  X  1 on the right side, we have  X 
No te that, tan  X  t does not decrease w.r.t step t as shown in Lemma 4.2 and  X  t does not increase w.r.t step t .
Since  X  t  X  1 &gt; 0, we assume the lower bound tan 2 t 1 h old 1 , then Eq.(21) can be rewritten as S et  X  t = 1 H t for all t  X  1, we have
When tan  X  t becomes small, the improvement from the constraint will not be significant as shown in Figure 1. Lemma 4.2 shows that tan  X  t does not decrease as t increases if ^w and w t +1 are close to w  X  . This indicates the improvement from ^w t +1 to w t +1 is stable under the regression constraint. And this further proves the stableness of the regret bound.
Lemma 4.2. tan  X  t =  X  w t +1  X  ^w t +1  X  2 /  X  w t +1  X  w not decrease w.r.t step t .

Proof. It is known that ^w t +1 and w t +1 converge to w  X  If w t +1 converges beyond the speed of ^w t +1 , tan  X  t verge and the Lemma holds for sure.

SGD has the convergence rate O (1 / convergence rate that can be obtained by all the stochas-tic optimization approaches. Before we prove CSGD has a faster convergence rate than SGD, we temporarily make a conservative assumption that CSGD and SGD both con-verge at the rate of O ( t  X  ), where  X  is a positive number.
Let  X  ^w t +1  X  w  X   X  be at  X  and  X  w t +1  X  w  X   X  be bt  X  with Eq.(15), we have
Th is inequality is based on the assumption that H is pos-itive. Although this assumption could be slightly violated when tan  X  t = 0 if w t lies on  X  t and ( x t , y t ) = ( x event rarely happens in real cases. Even if it happens but for finite times, the legality of our analysis is still provable. So we simply rule out this rare event in theoretical analysis. In our approach, the O (log T ) regret bound achieved by CGSD neither requires strong convexity nor regularization, while Hazan et al. achieve the same O (log T ) regret bound under the assumption of strong convexity [10], and Bartlett et al. use regularization to obtain the same O (log T ) regret bound for strongly convex functions and O ( bitrary convex functions. Furthermore, the O (log T ) regret bound of CGSD is better than the general regret bound O (  X 
The regret bound suggests a decreasing step size, which yields the convergence rate stated in the following corollary.
Corollary 4.3. (Decreasing step convergence rate) As-sume Theorem 4.1 hold and  X  t = 1 H t for any step t , then
This corollary is a direct result of Theorem 4.1. It shows that the O (log T /T ) convergence rate of CSGD is much bet-ter than O (1 /
Theorem 4.1 and Corollary 4.3 suggest an optimal learning rate decreasing at the rate of O (1 /t ) without assuming the strong convexity for the objective function. However, the decay proportional to the inverse of the number of iterations is not robust to the wrong setting of the proportionality constant. The typical result for the wrong proportionality constant will lead to divergence in the first several iterations or converge to a point far away from the optimal. Motivated by this problem, we propose a 2-phase learning rate strategy, which is defined as
The step m is achieved when desired error tolerance  X  is obtained in Proposition 2.2, m = O (1 / X  ). The maximum value for m is the total size of the dataset, since the global mean would be achieved after one pass of the data.
In this section, we perform numerical simulation to sys-tematically analyze the proposed algorithms and conduct empirical verification of our theoretical results.
Our optimization study includes 5 algorithms, including the proposed CSGD and NCSGD, and SGD, ASGD, 2SGD, for comparative study: 1) Stochastic Gradient Descent (SGD) (a.k.a Robbins-2) Averaged Gradient Descent (ASGD) (a.k.a. Polyak-3) Con strained Stochastic Gradient Descent (CSGD): CSGD 4) Naive Constrained Stochastic Gradient Descent (NC-5) Second Order Stochastic Gradient Descent (2SGD) [7]:
The experiments for least squares optimization have been conducted on two different settings: strongly and non-strongly convex cases. The difference between strongly convex and non-strongly convex objectives has been extensively studied in convex optimization and machine learning on selection of the learning rate strategy [2, 24]. Even though a decay of the learning rate at the rate of the inverse of the number of samples has been theoretically suggested to achieve the optimal rate of convergence in strongly convex case [10]. In practice, the least squares approach may decrease too fast and the iteration will  X  X et stuck X  too far away from the op-timal solution [12]. To solve this problem, a slower decay has been proposed in [2] for learning rate,  X  t =  X  0 t  X   X   X  (1 / 2 , 1). A  X / 2  X  w  X  2 added to obtain  X  -strongly convex and uniformly Lipschitz [29, 1]. To ensure convergence, we safely set  X  = 1 / 2 [30] as a robust learning rate for all algorithms in our empirical study, which guarantees all algorithms can converge in close vicinity to the optimal solution. In order to study the real convergence performance of different approaches, we use the prior knowledge of the spectrum of the Hessian matrix to ob-tain the best  X  0 . To achieve the regret bound in Theorem 4.1, a 2-phase learning rate is utilized for CSGD and m is set to be the half of the dataset size n/ 2.

We generate n input samples with d dimensions i.i.d. from a uniform distribution between 0 and 1. The optimal coeffi-cient w  X  is randomly drawn from standard Gaussian distri-bution. Then the n data samples for our experiments is con-structed using the n input samples as well as the coefficient with a zero-mean Gaussian noise with variance 0 . 2. Two settings for least squares optimization are designed: 1) a low-dimension case with strong convexity, where n =10,000, d =100, 2) a high-dimension case, where n =5,000, d =5,000, with smallest eigenvalue of the Hessian matrix close to 0, which yields a non-strongly convex case. In each iteration round, one sample is randomly drawn from one individual dataset using a uniformly distribution.
 In the strongly convex case, as shown in Figure 2 top row, CSGD behaves similar to 2SGD and outperforms other first order approaches. As we know, 2SGD, as a second order ap-proach, is the best possible solution per iteration for all first order approaches. However, 2SGD requires O ( d 2 ) computa-tion and space complexity in each iteration. CSGD performs like 2SGD but only requires O ( d ) computation and space complexity, and CSGD has a comparable performance as 2SGD when doing optimization by giving a certain amount of CPU time, as shown in top right slot of Figure 2.
In the non-strongly convex case, as shown in Figure 2 bottom row, CSGD performs the best among all first or-der approaches. 2SGD becomes impractical in this high dimensional case due to its high computation complexity. CSGD has a slow start at the beginning and this is because it adopts a larger initial learning rate  X  0 which yields better convergence for the second phase. This fact is also consis-tent with the comparisons using the wrong initial learning rates discussed in [2]. However, this larger initial learning rate speeds up the convergence in the second phase. In ad-dition, the non-strong convexity corresponds to an infinite ratio of the eigenvalues for the Hessian matrix, which sig-nificantly slows the performance of SGD. NCSGD has not been influenced by this case and still performs consistently better than SGD.

In our empirical study, we have observed:
In the classification task, least squares loss function plays an important role and the optimization of least squares is the cornerstone of all least squares based algorithms, such as Least Squares Support Vector Machine (LS-SVM) [26], Regularized Least-Squares classification [22, 6] and etc. In this case, SGD is utilized by default during the optimization.
It is well acknowledged that the optimization speed for least squares directly affects the performance of least squares based algorithms. A faster optimization procedure corre-sponds to less training iterations and less training time. Therefore, replacing the SGD with CSGD for the least squares optimization in many algorithm, the performance could be greatly improved. In this subsection, we show an example how to adopt CSGD in the optimization for the existing classification approaches.

One direct classification approach using least squares loss is ADAptive LINear Element (Adaline) [16], which is a well-known method in neural network. Adaline adopts a sim-ple perceptron-like system that accomplishes classification, which modifies coefficients to minimize the least squares error at every iteration. Note that, although it may not achieve a perfect classification by using a linear classifier, the direct optimization for least squares is commonly used as a subroutine in many complex algorithms, such as Mul-tiple Adaline (Madaline) [16] to achieve the non-linear sep-arability by using multiple layers of Adalines. Since the fundamental optimization procedures for these least squares algorithms are the same, we only show a basic case for Ada-line to show CSGD can improve the performance.

In Adaline, each neuron separates two classes using a coef-ficient vector w . The equation of the separating hyper-plane can be derived from the coefficient vector. Specifically, to classify input sample x i , let net be the net input of this neu-ron, where net = w T x t . The output of Adaline o t is 1 when net &gt; 0 and o t is -1 otherwise.

The crucial part for training the Adaline is to obtain the best coefficient vector w , which is updated per iteration by minimizing the squared error. At iteration t , the squared positive class or negative class respectively. Adaline adopts SGD for optimization whose learning rule is given by where  X  is a constant learning rate, and the gradient g =  X  ( y t  X  w T t x t ) x t .

When replacing SGD with CSGD for Adaline, Eq.(24) is replaced with Eq.(9).

In the multiclass classification case, suppose there are c classes, Adaline needs c neurons to perform the classification and each neuron still performs the binary class discrimina-tion. The CSGD version of Adaline (C-Adaline) is depicted in Algorithm 2, which is straightforward and easy to imple-ment. One thing need to be pointed out is that, the class label y t has to be rebuilt in order to fit c neurons. There-fore, the class label y t of sample x t for neuron c i is defined output o t = k means that the k th neuron c k has the highest net value among c neurons.

To evaluate the improvement of the C-Adaline,we provide computational experiments of both Adaline and C-Adaline on the MNIST dataset [14], which is widely used as stochas-Al gorithm 2 CSGD version of Adaline (C-Adaline)
In itialize w 0 = x 0 = 0 and  X  y 0 = 0. for t = 1 , 2 , 3 , . . . do end for tic optimization classification benchmark on handwritten digits recognition [9, 28].

The MNIST dataset consists of 60,000 training samples and 10,000 test samples with 10 classes. Each digit is pre-sented by a 28  X  28 gray scale image, for a total of 784 features. All the data is downscaled to [0 , 1] via dividing the maximum pixel intensity by 255. For the setting of the learn-ing rate, Adaline adopts a constant while C-Adaline sim-ply takes the updating rule of Naive Constrained Stochastic Gradient Descent (NCSGD).  X  for Adaline is set to 2  X  17 , and the C-Adaline has  X  0 = 2  X  4 , which are both the optimal re-solution is unique, this experiment is to examine how fast can Adaline and C-Adaline converge to this optimal.
The test set error rate as a functions of number of op-erations is shown in Figure 3 (Left). It is clear that both Adaline and C-Adaline converge to the same test error be-cause, they both optimize the least squares error. C-Adaline achieves 0.14 test error after processing 2 14 samples (  X  while Adaline achieves 0.14 test error after processing 2 samples (  X  10 6 ). This indicates that C-Adaline converges 64 times as fast as Adaline! Considering the size of the train-ing set is 60,000, C-Adaline uses 1 / 4 of the total training samples to achieve the nearly optimal test error rate, while Adaline needs to visit each training sample more than 16 times to achieve the same test error rate. Figure 3 (Right) shows the test error rate versus the CPU time. To achieve the 0.14 test error, Adaline consumes 112.47 seconds, while C-Adaline only takes 3.38 seconds. Note that, in this ex-periment, 10 neurons are trained in parallel. It is another achievement to get the nearly optimal test error using a least squares classifier in about 3 seconds for a 10 6 scale dataset.
To better understand the classification results, in Figure 4, we visualize the data samples on a two dimensional space by t-SNE [27], which is a nonlinear mapping commonly used for exploring the inherent structure from high dimensional data. Since a linear classifier does not perform well on this problem and both algorithms have the same classification error ulti-mately, we suppress the samples which are still misclassified in Figure 4 for clarity X  X  sake. When both algorithms have processed 2 14 training samples, their classification results on 10,000 test samples are depicted in Figure 4. C-Adaline misclassified 212 samples while Adaline misclassified 1248 samples, which is about 6 times as C-Adaline.

This experiment compares a SGD based classifier (Ada-line) and the proposed CSGD improvement version (C-Adaline) using the MNIST dataset. In summary,
In this paper, we analyze a new constrained based stochas-tic gradient descent solution for the large-scale least square problem. We provide theoretical justifications for the pro-posed method, called CSGD and NCSGD, which utilize the averaging hyper-plane as the projected hyper-plane. Specifi-cally, we described the convergence rates as well as the regret bounds for the proposed method. CSGD performs like a full second order approach but with simpler computation than 2SGD. The optimal regret O (log T ) is achieved in CSGD when adopting a corresponding learning rate strategy. The theoretical superiorities are justified by experimental results. In addition, it is easy to extend the SGD based least squares algorithms to CSGD and the CSGD version can yield better performance. An example of upgrading Adaline from SGD to CSGD is used to demonstrate the straightforward but efficient implementation of CSGD [1] A. Agarwal, S. Negahban, and M. Wainwright.
 [2] F. Bach and E. Moulines. Non-asymptotic analysis of [3] P. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive [4] L. E. Baum and M. Katz. Exponential convergence [5] D. P. Bertsekas. Nonlinear programming. Athena [6] C. M. Bishop. Pattern recognition and machine (Right) on MNIST dataset when 2 14 samples have been processed. [7] L. Bottou and O. Bousquet. The tradeoffs of large [8] L. Bottou and Y. LeCun. Large scale online learning. [9] J. C. Duchi, E. Hazan, and Y. Singer. Adaptive [10] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret [11] C. Hu, J. T. Kwok, and W. Pan. Accelerated gradient [12] H. J. Kushner and G. Yin. Stochastic approximation [13] J. Langford, L. Li, and T. Zhang. Sparse online [14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [15] L. Ljung. Analysis of stochastic gradient algorithms [16] K. Mehrotra, C. K. Mohan, and S. Ranka. Elements of [17] A. Nemirovski. Efficient methods in convex [18] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. [19] A. Nemirovski and D. Yudin. Problem complexity and [20] Y. Nesterov. Introductory lectures on convex [21] B. T. Polyak and A. B. Juditsky. Acceleration of [22] R. Rifkin, G. Yeo, and T. Poggio. Regularized [23] S. Shalev-Shwartz and S. M. Kakade. Mind the [24] S. Shalev-Shwartz and N. Srebro. Svm optimization: [25] J. C. Spall. Introduction to stochastic search and [26] J. Suykens and J. Vandewalle. Least squares support [27] L. Van der Maaten and G. Hinton. Visualizing data [28] L. Xiao. Dual averaging methods for regularized [29] T. Zhang. Solving large scale linear prediction [30] M. Zinkevich. Online convex programming and
