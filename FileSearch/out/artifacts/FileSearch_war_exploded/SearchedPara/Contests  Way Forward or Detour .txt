 Contests and challenges have energized researchers and focused attention in many fields recently, including recomm ender systems. At the 2008 RecSys conference, winners were announc ed for a contest proposing new startup companies. The 2009 c onference featured a panel reflecting on the then recently co mpleted Netflix challenge. Would additional contests help move the field of re commender systems forward? Or would they just draw attention from the most important problems to problems that are most easily formulated as contests? If contests would be useful, what should the tasks be and how should performance be evaluated? The panel will begin with short presentations by the panelists. Following tha t, the panelists will respond to brief sketches of possible new con tests. In addition to prediction and ranking tasks, tasks mig ht include making creative use of the outputs of a fixed recom mender engine, or eliciting inputs for a recommender engine. [Information Storage and Retrieval]: Information Se arch and Retrieval--Information Filtering, Retrieval models ; H.3.4 [Information Storage and Retrieval]:Systems and Sof tware --Performance Evaluation (efficiency and effectivenes s) General Terms: Algorithms, Experimentation, Human Factors, Measurement.
 Contests generate excitement and publicity, and at their best bring together large sets of talented people to pursue a challenge. Who could object? I will for two reasons  X  too often c ontests focus on what can be judged, rather than what X  X  wanted, and often contests result in overconcentration on specific challenges to the detriment of the rest of the field. As a simple example, let X  X  consider the Netflix Cha llenge. While it was enormously successful in getting bright peop le to collaborate on the prediction problem, it was struc tured in a way that makes it hard to see a path to solving the rec ommender problem  X  leaving aside the tractability of the alg orithms, the challenge was about more accurate predictions. Mor e accurate predictions for the bottom 500 movies would dominat e any improvement possible on the top 100. Clever algori thms? Certainly. The best way to advance the field? Dou btful.
 I was an organizer for the ECML PKDD Discovery Chal lenges in 2008 and 2009. The 2008 tasks involved spam detecti on and tag recommendation in social bookmarking sites. The 200 9 tasks involved recommending tags for papers on the Bibson omy site. One innovative feature of this challenge was an onl ine task that was integrated into the live operation of the Bibso nomy site. Each participant implemented a recommendation service wh ich was called via HTTP. When a user posted a bookmark or publication, all participating recommenders were ca lled; one that responded quickly enough was chosen to actuall y deliver the results to the user. The online challenge has a llowed researcher to work with a heavily used system and r evealed clearly limitations of current approaches. One outc ome was that recommendation accuracy is clearly an important fac tor but algorithms which will compute this under a given in formation and time constraint are one of the future challenge s. As VP of Strands Recommender, I am responsible for overseeing the development and commercialization of a generic SaaS recommendation platform addressing the needs of sma ll and mid-sized online retail and content sites. Although the re is certainly plenty of opportunity to innovate around the algori thmic side of the problem, my interest is in using contests to fa cilitate wider adoption and higher penetration of recommender tech nologies in the markets that we serve. From this perspective, I will present and discuss results from our initial experience in running  X  X lug-in, Ca$h out X , a Strands-sponsored program designed for ecommerce and mobile software developers to build and distrib ute plug-and-play extensions and applications on top of the Stra nds personalization and recommendation engine interface s. 
