 Smoothing document model with word graph is a new and effective method in information retrieval. Word graph can naturally incorporate the dependency between the words; random walk algorithm based on the graph can be used to estimate the weight of each vertex. In this paper, we present a new way to construct a local word graph for smoothing document model, which exploits the document X  X  k nearest neighbors: the vertices represent the words in the docum ent and its k nearest neighbors, and the weights of the edges are estimated through word co-occurrence in the local document set. We argue that word graph is a key factor to the pe rformance in graph-based smoothing method. By using the local document set, we can obtain a document specific word graph, and achieve better retrieval performance. Experimental results on three TREC collections show that our proposed a pproach is effective. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Retrieval models General Terms: Algorithms Keywords: Graph-based rank, word graph, document model, smoothing, local strategy. Information retrieval with statis tical language models [4, 7] has recently attracted much attention. The estimation of the document language model is a key issue, which strongly impacts the retrieval performance. However, due to insufficient sampling, the maximum likelihood estimator suffers from the zero-count problem seriously: when a term does not occur in a document, it would be assigned a zero probability. The zero-count problem affects the retrieval performance, and smoothing is proposed to address this problem. 
There are many studies on smoothing since Ponte and Croft X  X  work on applying language models to retrieval [7]. The first group of smoothing methods interpolate the initial document model with a background model estimated based on the whole document collection [4, 7]. Such an approach can be considered to use a global smoothing strategy, which is independent from documents. More recently, several studies [6, 8] have exploited the local corpus structures to improve retrieval performance. These are local strategies. Their basic idea is to expand the document with the documents similar to the document. Generally speaking, the local strategies are more effective than the global strategies. 
The main limitation of the above smoothing strategy is that the weight (probability) of each word is determined mostly by its frequency since they fail to take into consideration its context such as the surrounding words. In recent years, graph-based model is used to smooth docum ent models in natural language processing, which can effectively incorporate the word relationship. TextRank [5] is a typical graph-based model that has been applied to many natural language processing tasks. 
The adaptations of TextRank have also been proved to be effective in information retrieval: [1] constructs a word graph for each document, in which the vertices represent document terms, and the weights of the edges are estimated through word co-occurrences. Unlike the word graph constructed in [1], [6] construct a word graph based on the whole document collection, and the iterative random walk algorithm on the word graph was used to compute the document model, and each document started with its own initial maximum likelihood model. Although the above two graph-based models achieved improvements in performance, the ways to construc t word graphs have not been thoroughly considered and compared. In fact, the word graph is crucial in the graph-based algorithm. 
In this paper, we focus on the way to construct word graph, and propose a new graph construction method to smooth the document model. We construct a k-neares t-neighbors (kNN) document graph for the collection. For each document, we utilize the document X  X  k nearest neighbors on the document graph and the document itself to form a local document set, and then a word graph is constructed on this local document set: taking the words in this document set as vertices in the graph; identifying word co-occurrence in the local document set as a measure of dependency between words. Our method has the following advantages: 1). We construct a document specific word graph to smooth the document model by exploiting the document X  X  k nearest neighbors. The word graph includes many words related to the document as the vertices, while word graph based on document itself would lack of words related to the document, and word graph based on the whole document collection would take all the word into the word graph and bring much noise. Also, the word graph estimates the weights of the edges through word co-occu rrences in the local document, which can be viewed as smoothing th e edge of the word graph in a document specific local strategy. 2). Comparing to the previous methods which smooth document m odel by interpolating the language model based on the similar documents with the initial document model, our approach can utilize the word X  X  context information effectively and avoid the word independence limitation of frequency-based score model by integrating the word relationships into the word graph. 
We perform experiments on th ree TREC datasets and the results demonstrate the good effec tiveness of the proposed method. The use of local word graph can improve the retrieval performance. Moreover, combining our approach with pseudo feedback can achieve furthe r performance improvement, and outperform the feedback model. Word relationships have been used to expand query model or document model in many studies[3 , 6]. Graph-based model uses the word relationships to construc t a word graph, and the iterative random walk algorithm like PageRank [2] or TextRank [5] are used to calculate the weights of the vertices. The main advantage of the graph-based models is that they can take the words X  context into consideration, and avoid the limitation of frequency-based score model. 
The basic idea of an iterative graph-based ranking algorithm is that of mutual  X  X oting X  or  X  X ecommendation X . When one vertex vertex. The importance of a vert ex is determined based on the votes that are cast for it as well as the score of the vertices casting these votes. The graph-based model can utilize the word relation effectively, which can overcome the limitation of the traditional method which relies on the frequency of the word, and lead to a smoothed model. 
In the following section, we will introduce a graph-based random work algorithm used in our approach, which is similar to the algorithm used in [6] and the personalized pagerank. vertex i V is defined as follows:  X   X  represent the weight of the initial score when re-computing the score of the vertex; (, ) ij wV V is the weight of the edge between i V and j V ; () i In V and () j Out V are respectively the set of vertices that point to vertex pointed by vertex j V . From the above formula, we can see that the score of a vertex consists of two components: the first term is the original score of the vertex, which is usually obtained from the maximum likelihood estimator; the second term is obtained through the scores of the vertices that connected to it, which is similar to the PageRank [2] mode l. The updating algorithm differs from PageRank in that it use 0 () i f V instead of the uniformed jumping probability 1/N , which guarantees that the new model will not deviate much from the initial model. c = convergence or the iterative time r eaches a predefined threshold (e.g. 10). The algorithm converges when the change of f ( V smaller than a predefined threshold (e.g. 10 -6 ). The change c is computed by formula 2: Graph-based algorithm has been proved to be effective in smoothing document model [1, 6]. The word graph is an important factor that influences the performance. Our paper focuses on the construction of word graph. 
The basic idea of our method is to exploit the document X  X  k nearest neighbors to construct a document specific word graph. Figure 1 illustrates the word graph based on the KNN documents. 
For one document, due to the insufficient sampling, there are many related words that do not occur, also, there are many related word pairs which do not co-occur within a sliding window, which enrich the document with extra re source, the documents which are similar to the document is a good choice. These similar documents are likely to relate to the same topic, and many related words will occur in the local document se t. Meanwhile, the relation among terms can be estimated accurately on this document set. From the view of smoothing the edges, the word graph based on the document itself suffers from sparse ness; word graph based on the whole document collection can be considered as a global way to smooth the edges; and word graph based on the k nearest neighbors can be considered as a local way to smooth the edges. 
In order to construct the document graph and exploit the document X  X  K nearest neighbors, we need to compute the similarity among documents. In this paper, we simply choose the commonly used cosine similarity. The way to compute the document-document similarity is the same as that of [6] and [8]. 
After calculating the similarities among the documents, we construct a k-Nearest-Neighbor (kNN) graph of all documents, and obtain a document d  X  X  k nearest neighbors () N d . For each document d , we construct a word graph () (, ) Gd V E = on its local document set () Ld which is the union of the document d and () N d . 
V is the set of terms occurs in this document set () Ld , and E is the set edges weighted according to the dependency between the words. We use the following formulas to estimate the relation between words based on the local document set. Where , ab are words, (,) co a b represents the number of the documents in which , ab co-occur within a sliding window (i.e. 10). Note that we do not use the document-document similarity to weight the co-occurrence in diffe rent documents. The reason is that although the document-document similarity is very important to select top k-nearest neighbor s for one document, it can not directly determine the importan ce of the co-occurrence between words. After we smooth all the document language models with the word graph, and obtain the smoothed document model P'(w|d), we further smooth P'(w|d) using add itional Dirichlet prior smoothing method: In Section 4, we proposed a ne w graph-based sm oothing method, which exploit the document X  X  k nearest neighbors (KNN) documents to construct a word graph, named as WG-KNN . In this section, we will evaluate the effectiveness of WG-KNN empirically, by comparing it with the performance of the word graph based on the document itself, named as WG-D , as well as the word graph based on the whol e document collection, named as WG-C . The three models use the same iterative algorithm to compute the score of the vertices, but their word graphs are different. We evaluate the proposed method on three representative TREC data sets: AP (Associat ed Press news 1988-90, queries 51~150), LA (LA Times, queries 301~400), and SJMN (San Jose Mercury News 1991, queries 51~150). 
In all our experiments, the cuto ff of relevant documents is set as 1000. The following two performance measures are considered in our evaluation: (1) non-interpolated Mean Average Precision (MAP). (2) Precision at 10 documents (p@10). The graph-based smoothing strategi es would introduce a different set of new parameters such as the number of a document X  X  neighbors k ,  X  in formula 1 and  X  in formula 4. In order to determine the values of these parameters, we use the AP collection as our training collec tion. We performed exhaustive grid search to train our free para meters, tune the three parameters on the train set, and then fix the parameters while testing in other collection. The parameter  X  were swept over values = [0.1, 0.9] with a step size of 0.1; the pa rameter k was swept over values = [10, 90] with a step size of 10; the parameter  X  was swept over values = [200, 1000] with a step size of 100. We select the parameter values which optimize mean average precision (MAP). The sliding window size is fixed to be 10 in WG-KNN. The evaluation results are shown in table 2 and 3. We can see that WG-KNN outperforms WG-D as well as WG-C consistently. The parameter settings are:  X  =0.3, k =50,  X  =300 respectively, which are obtained from the training set.

Comparing WG-KNN with WG-D, we can see that the increase is very large, which indicates th at our word graph based on the k nearest neighbors is more effective to smooth document model than word graph based on the document itself. 
Comparing WG-KNN with WG-C, the increase in MAP is between 5% and 10% in most cases . The results show that word graph based on the local document set is more effective that that based on the whole document collec tion. A possible explanation is that word graph in our approach just include the words occurred in the local document set, which will have less noise that that of the whole document collection, and the word relation can be estimated accurately in a document specific way. 3000 results returned by Dirich let method and output 1000, significant improvement in paired t-test at the level of p&lt;0.01 Data WG-D WG-KNN AP88~90 p@10 0.438 0.463(+5.71%*) LA p@10 0.287 0.299(+4.18%) SJMN p@10 0.300 0.334(+11.33%*) Data WG-C WG-KNN AP88~90 p@10 0.438 0.463(+5.71%*) LA p@10 0.285 0.299(+4.91%) SJMN p@10 0.310 0.334(+7.74%*)
We also compared WG-KNN with other existing smoothing method: the basic Dirchlet prio r smoothing, DELM [8], which smooth document model by interpolating the language model based on the nearest documents with the initial document model. The results are shown in Table 4. We can see that WG-KNN can get the best performance. Moreover, we can improve our work by integrating other kind of word re lation such as semantic relation extracting from WordNet and other thesauri. related methods. The result of DELM is from (Tao et al., 2006). The above results are based on the parameters that optimized on AP collection. It is therefore important to study how performance may be affected by the choice of parameters in our model. Figure 2 shows the sensitivity of these parameters. Pseudo feedback has been proved to be effective to update the query model [9]. It is thus interesting to examine whether our model can be combined with pse udo feedback to further improve the retrieval accuracy. 
We explore whether combining the feedback model and the smoothing model with word graph based on the local document set would improve performance. We conduct the combination in the similar way used in previous works [6, 8]: (1) Retrieve documents by our WG-KNN method; (2) Choose top 5 documents to do the feedback; (3) Use the updated query model to retrieve documents again with WG-KNN method. 
We select the mixture feedback model proposed in [9] to estimate a feedback model. Th e parameters are set in the following way: we fix the noise para meter to be 0.9; and tune the coefficient parameter to get the optimal performance of pseudo-feedback. 
Table 5 shows the experiment results (MAP). We can see that combination of WG-KNN and pseudo feedback model can outperform both the feedback model and the WG-KNN smoothing model alone. 
WG-KNN, we also re-rank the top 3000 results and output AP88~90 0.266 0.255 0.273 SJMN 0.241 0.232 0.250 In this paper, we proposed a ne w approach to construct word graph for graph-based algorithm. We exploit the document X  X  k nearest neighbors to form a local document set, and then the word graph is constructed based on this local document set. Our approach combines document graph and word graph effectively. When exploiting a document X  X  k nearest neighbors, it utilizes the document graph structure; when smoothing the document model, the word graph is utilized. Experimental results on three TREC collections show that our approach is effective. Also, the results show that the best values for them are quite consistent across test collections. In addition, combin ing our approach with pseudo feedback can lead to further improvement, and outperform the traditional pseudo feedback. 
The work can be further improved on several aspects. In our future study, we will investigate the automatic estimation of the parameters. Also, we can integrate the word relationships from WordNet or other thesauri into our current approach. This work was supported by th e National Science Foundation of China (60736044, 60773027), as well as 863 Hi-Tech Research and Development Program of China (2006AA010108, 2008AA01Z145). [1] R. Blanco and C. Lioma. 2007. Random Walk Term [2] L. Page, S. Brin, R. Motwani, and T. Winograd. The [3] G. Cao, J. Nie, J. Bai. 2005. Integrating word relationships [4] J. Lafferty and C. Zhai. 2001. Document language models, [5] R. Mihalcea and P. Tarau. 2004. TextRank  X  bringing order [6] Q. Mei, D. Zhang, C. Zhai . 2008. A General Optimization [7] J. Ponte and W. B. Crof t. 1998. A language modeling [8] T. Tao, X. Wang, Q. Mei, and C. Zhai. 2006. Language [9] C. Zhai and J. Lafferty. 2001a. Model-based feedback in the 
