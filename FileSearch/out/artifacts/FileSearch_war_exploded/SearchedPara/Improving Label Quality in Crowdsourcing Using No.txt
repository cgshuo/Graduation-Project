 This paper proposes a novel framework that introduces noise correction techniques to further improve label quality after ground truth inference in crowdsourcing. In the framework, an adaptive voting noise correction algorithm (AVNC) is proposed to identify and correct the most likely noises with the help of estimated qualities of labelers provided by the ground truth inference. The experimental results on two real-world datasets show that (1) the framework can improve label quality regardless of inference algorithms, especially under the circumstance that each example has a few noisy labels; and (2) since the algorithm AVNC considers both the number of and the probability of potential noises, it outperforms a baseline noise correction algorithm. H.2.8 [ Database Applications ]: Data Mining crowdsourcing; noise correction; label quality; inference Crowdsourcing systems provide a convenient way to collect labels for data at a low cost. To conquer the issue of low label quality, it is usual to obtain multiple labels from different labelers for the same object. A previous stud y [9] show ed that the int egrated label quality can be improved by simply using a straightforward ground truth inference algorithm . Recently, a great number of studies [1] [3] [8] [12] focus ed on improving the performance of ground truth inference. Although some studies [1] [12] proposed complicated models to describe different aspects of labeling, including the expertise, dedication of labelers and the difficulties of instances. However, no empirical study has proved that the performance of some algorithms is definitely superior to that of others on real-world datasets. This indicates that it is difficult to improve the integrated label quality remarkably only by proposing new inference algorithms. This paper investigates a completely new direction of improving the label quality. We propose a novel framework that introduces noise correction techniques to further improve the label quality after a ground truth inference. Obviously, crowdsourced data after inference still have mislabeled instances, i.e., label noises. It would be promising to use noise handling techniques to identify and correct the label noises. The principle behind is that human intelligence is not always superior to machine intelligence under some circumstances, especially when labelers are not experts. For example, in the text classification tasks conducted by Shinsel et al [10], although each instance obtained six labels from different label ers, the overall integrated accuracy is still less than 94%, which is inferior to the accuracy predicted by an SVM classifier (  X  99%) [5]. Thus, a potential high quality learning model may have a great opportunity to improve label quality. In order to accurately identify and correct the noises, the paper also proposes a novel adaptive voting noise correction algorithm (AVNC), which can collaborate with any inference algorithm. The experimental results show that a state-of-the-art noise correction approach can improve the integrated label quality, and the proposed algorithm AVNC performs much better. Figure 1 illustrates the proposed framework. In the framework, a dataset with multiple noisy (repeated) label s is first processed by a ground truth inference algorithm to form an inferred dataset D where each instance has an integrated label. The instances whose integrated labels do not match their ground truths are noises. Since the ground truth of each instance is unknown, we have to estimate the probability of an instance being a noise. Based on the estimated probability, we can identify potential noises and filter them out. After filtering, the inferred dataset will be separated into a noise dataset and a cleansed dataset. The noise dataset will be further corrected. After correction, the updated noise dataset will be merged with the cleansed dataset to form an enhanced dataset D E . Note that the enhanced dataset contains the same instances as the original dataset does, but the labels of some instances are updated. We expect that the label quality of the enhanced dataset is improved. In order to accurately estimate the probabil ity of an instance being a noise, we take advantage of the information yielded from the inference stage to supervise noise identification. Here, three issues need to be further discussed. The information generated by inference algorithms and used in noise filtering . Different inference algorithms model different aspects of crowdsourcing systems. Whitehill et al. [12 ] estimates labeler expertise and instance difficulties. Raykar et al. [8 ] esti mates labelers X  pr eferences to positive and negative instances. Bi et al. [1] estimates labeler expertise, dedication and instance diffi culties. Estimated variables can provide information for noise filtering. Each specific algorithm is worthwhile to investigate a specific approach of using its information to supervise noise filtering. This paper only utilizes the off-the-shelf inference algorithms and their most common information  X  estimated labels of instances and estimated labeling qualities of labelers  X  to guard noise identification and filtering. We will demonstrate that this general information can improve the labeling quality. The noise correction techniques . Many noise handling techniques have been proposed in the past two decades. A previous survey [6] re viewed filtering methods and pointed out that most of them cannot achieve a high performance even for binary classification. Correcting noises is a straightforward idea. However, the survey concluded that simply removing mislabeled instances is more efficient than trying to correct them . Thus, compar ed with noise filtering, literatures on noise correction are scarce. Alternatively, there is another way to train a learning model by directly using cleansed data after noise filtering, without attempting to correct any noise. Indeed, if our dataset has a great number of instances, we can only use the cleansed data to train a good model. However, this prerequisite does not always hold in crowdsourcing . A dataset obtained from crowdsourcing systems is not free, although it is much cheaper than hiring domain experts to complete the tasks. Furthermore, using crowdsourcing is not always to train a learning model. In some cases, we are only intent on obtaining a high quality dataset . That is why inference algorithms are so important in crowdsourcing. Overall, this paper focuses on label quality. We try to identify and correct noisy labels. The number of labels for each instance . Labeling cost is one of the core issues in crowdsourcing. Sheng et al. [9] shows that when inc reasing the number of labels for each instance, the label qualit y improves. Some earlier study [9] used about 10 labels for each instance. However, in recent study [4] , the average number of labels for each instance is from 3 to 6. Therefore, we expect our approach can improve the performance under different numbers of labels for each instance. Our adaptive voting noise correction algorithm includes two steps: filtering and correction. Classification Filtering [7] is a general cl assical model prediction-based filtering algorithm. But it faces the risk of removing too many instances. Brodley and Friedl [2] introduced an algorithm using ensemble classifiers to address this issue. Our filtering step is also based on ensemble learning. To accurately identify noises, our approach answers two key questi ons that have not been addressed in traditional filtering algorithms [2] [7 ]. (1) How many mislabeled instances should be filtered out ? (2) Which noises have a higher priority to be filtered out, i.e. how to rank noises ? The information generated by an inference algorithm provides some hints to solve this problem. For an instance i , an inference algorithm assigns it an estimated label i y (suppose the true label y ). The label that a labeler j provides the instance i is denoted positive and null (i.e. labelers do not provided labels) respectively, for binary cases. We define the quality of the labeler j as: where I is the total number of instances and () I is an indicator function that returns 1 if the test condition holds. Given the total number of labelers J , the average labeling quality of labelers is p p J , where j p is the labeling quality of a labeler j . Obtaining N labels from different labelers for each instance. The qualities of the labelers follow a uniform distribution with a mean qual ity p . Then, the integrated labeling quality q is: This is an ideal status with the highest integrated label quality. Thus, we can estimate the lower boundary of the number of noises after inference as: In reality, not all labelers have the same labeling quality p . At the worst case, getting another label from a different labeler does not make any contribution to the improvement of the integrated label quality. Under this circumstance, we can estimate the upper boundary of the number of noises as: Although both  X  and  X  are not derived from any specific characteristic of an inference algorithm, they can be easily calculated by all inference algorithms. Our algorithm is based on the voting of M classifiers. Supposed there are  X  instances with at least M/ 2  X  X  X   X  X  classifiers that predict them as noises, our algorithm uses the following equation to calculate the number of possible noises (denoted by r N ). How to determine  X  will be further discussed in Section 3.3. contains two fields c and e. () i c B counts the times that the predicted labels from M classifiers do not match the integrated label of the instance i . For an instance i , classifier m predicts it to We define a measure to rank each instance i based on these two fields as follows: the more likely an instance might be a noise. Sorting with resp ect to Eq. 7 is a two layer sorting. Instances with the same value of 
B (the integer part of Eq . 7 is a primary key) are grouped together. For M classifiers, there are totally M+1 groups . A larger 
B Within the same group, the instances are further sorted according secondary key), which means that the smaller the entropy e instance, the more certain ly the M classifiers predict it as a noise. AV NC takes the dataset after a ground truth inference D input. It first performs M rounds of K-fold cross-validation to predict the label of each instance . In each round m , we put all instances whose predicted labels match their inferred labels into a high quality set ( HQ Set (m) ) (lines 1-5). HQSet (m) build a model for noise correction in line 11 . After M rounds of K-fold cross validation, AVNC sorts all instances in the define  X  as the number of instances whose () /2  X   X  X  X   X  X  At this point , the algorithm can determine the number of instances to be treated as noises by Eq.5 (line 8). After it separates D noise dataset and a cleansed dataset (line 9), it turns into the noise cor rection step. AVNC uses the M HQSet s to build M models to predict the label for each instance in the noise dataset (lines 10-12). Since we believe that HQSet contains almost correct labeled instances, the model quality would be high enough to make a correct prediction. After each instance in the noise dataset is updated using the predicted label from the M classifiers, AVNC emerge s the updated noise dataset into the cleansed set to form an updated high quality dataset D E (lines 13-14). In algorithm AVNC, note that when we make a correction for an instance i in the noise dataset, the instance i could be an instance in some high quality datasets HQSets. To avoid training a model to make prediction for its training instance, we remove this instance from all high quality datasets if they contain this instance before building models from them. This is shown in line 11. The not contain the instance i . Algorithm Adaptive Voting Noise Correction (AVNC) Input : inferred dataset D I ,  X  ,  X  , K , M
Output : enhanced dataset D E 1. fo r m = 1 to M 2. Shuffle the dataset D I and use K -fold cross -validation to 3. f or each instance i in D I 4. if predicted label  X  inferred l abel 6. Sort all instances in the descending order of (i) r by Eq. 7 7. C alculate  X  as the number of instances with () /2  X   X  X  X  8. Calculate 9. The first r 10. for each instance i in NoiseSet 12. Use these M models to predict the label of instance i and 13. Combine CleansedSet and NoiseSet together to form the 14. return D E In order to investigate the performance of our proposed framework and the algorithm AVNC in real-world environments, we use two datasets adult600 and eastpolitics800 respectively derived from the a dult and the 20newsgroups datasets in UCI machine learning repository to collect repeated labels from Amazon Mechanical Turk (MTurk). The two datasets have their ground true labels for all instances. We need the ground truths to validate whether the inferred labels or the corrected labels match them. The adult600 dataset includes 600 instances, which is about  X  to predict whether a person  X  s annual income exceeds $50K based on the census data in 1994.  X  We randomly extracted 300 positive (  X  $50K) and 300 negative (&lt; $50K) instances from a dult and created the human intelligence tasks (HITs) on MTurk. Each HIT is completed by 10 different workers. It has 67 workers providing 6000 labels for this dataset. The eastpolitics800 dataset includes 4 00 documents randomly extracted from those belonging to the class talk.politics.mideast as positive, and the other 400 ones randomly extracted from those belonging to the classes soc.religion.christian , talk.politics.guns , talk.politics.misc and talk.religion.misc as negative . Likewise, each task is completed by 10 different workers. It has 121 workers providing 8000 labels. First, we will illustrate that the proposed framework can collaborate with different inference algorithms. Therefore, four inference algorithms MV, GLAD [12 ], RY [8], and ZC [3] are used in the experiments. For comparison, we introduce a baseline algorithm BC, which uses a voting filtering in [2] as its noise filtering , and then use a self-training semi-supervised algorithm in [11] for its noise correction. Both AVNC and BC are based on ensemble learning ( M classifiers) with the K-fold cross-validation inside. In the experiment s, we let K = 10 and M = 5 . We are intent to investigate the performance under different numbers of labels for each instance. Supposed the experiment is procedure is as follows. We first randomly choose n labels from the 10 noisy labels collected from MTurk for each instance in the dataset to form a new dataset D . Then, we run different ground truth inference algorithms on D . After an inference algorithm has done, we correct noises either using AVNC or BC. Thus, three comparable results on the label quality (in accuracy ) will be obtained (1) only by (ground truth) inference, (2) by inference and BC, and (3) by inference and AVNC. For each n , we run experiment 10 times, the mean value of the integrated label quality in accuracy is reported. Both AVNC and BC need to build learning models. For adult600 , the models are built with SMO in WEKA. For eastpolitics800 , we remove stop words and the words with fewer than six characters in all documents. We use a text mining tool GATE (https://gate.ac.uk ) to generate bag -of-word features. The models are built with SVM. Figure 2 shows the experimental results on the adult600 dataset. We have the following observations. (1) As the increasing of the number of labels for each instance, the data quality increases after inference. However, when the number of labels is greater than six, the increment is limited. The performance differences among four inference algorithms are not significant. RY is the best, which is only 1% or 2% (absolute value) higher than the others. (2) Regardless of which inference algorithm is used, the noise correction algorithms (both BC and AVNC) can improve the label quality. This indicates that using noise handling to improve the inferred label quality is feasible. (3) The proposed AVNC obviously outperforms the baseline algorithm BC. Especially when the number of labels for each instance is less than six, the improvement is significant. The average absolute improvement i s greater than 8% and even 13% at maximum extent. Figure 3 Experimental results on the dataset eastpolitics800 Figure 3 shows the results on the dataset eastpolitics800 . Most observations are consistent with those on adult600 . Again, ou r AVNC has the best performance. However, the performance of BC is statistically worse than inference without correction according to a two-tailed student X  X  t -test in Table 1. Infer A dult600 Eastpolitics800 A vs. O A vs. B B vs. O A vs. O A vs. B B vs. O MV 10/0/0 10/0/0 10/0/0 10/0/0 10/0/0 0/3/7 GLAD 10/0/0 10/0/0 10/0/0 10/0/0 10/0/0 0/2/8 RY 10/0/0 10/0/0 10/0/0 7/3/0 10/0/0 0/2/8 ZC 10/0/0 10/0/0 10/0/0 10/0/0 10/0/0 0/0/10 A: AVNC, B : Baseline Correction, O: Only inference From above real-world evaluation, we can draw the following conclusions. (1) It is feasible to introduce noise handling techniques to improve the data quality in crowdsourcing . The proposed framework can make improvement regardless of the inference algorithm used. (2) Esp ecially, the proposed AVNC can significantly improve the label quality to a very high level when each instance has a few labels (e.g.,  X  6), which means at least a hal f of budget will be saved. (3) Compared with BC , our proposed AVNC algorithm is significantly better, because the off-the-shelf noise handling algorithms do not take advantage of the output information of ground truth inference. This paper proposed a novel framework to introduce noisy handl ing techniques for data quality improvement in crowdsourcing as well as a novel noise correction algorithm AVNC. The proposed approach takes advantage of the estimated labeling qualities to guide noise filtering and correction regardless of which inference algorithm is used. Evaluations on two real-world datasets together with four different inference algorithms demonstrated that the proposed framework and the proposed algorithm AVNC effectively improved the data quality and outperformed the state-of-the-art approach. This research has been supported by the Program for Changjiang Scholars and Innovative Research Team in University of the Ministry of Education, China, under grant IRT13059, the National 973 Program of China under grant 2013CB329604, the National Natural Science Foundation of China under grants 61229301, 61402311, the start-up funding of Nanjing University of Science and Technology, and the U.S. Nationa l Science Foundation under grant IIS -1115417. [1] Bi, W., Wang, L., Kwok, J. T., and Tu, Z. 2014. Learning to [2] Brodley C. E. and Friedl, M. A. 1999. Identifying mislabeled [3] Demartini, G., Difallah, D. E., and Cudr X -Mauroux, P. 2012. [4] Fang, M., Yin, J., and Tao, D. 2014. Active learning for [5] Frank, E. and Bouckaert, R. R. 2006. Naive bayes for text [6] Fr X  nay , B and Verleysen, M. 2014. Classification in the [7] Gamberger, D., Lavrac, N., and Groselj, C. 1999. Experiments [8] Raykar, V. C., Yu, S., Zhao, L. H., Florin, C., Valadez, G. H., [9] Sheng , V. S., Provost, F. and Ipeirotis, P. 2008. Get another [10] Shinsel, A., Kulesza, T., Burnett, M., Curran, W., Groce, A., [11] Tri guero, I., S X ez, J. A., Luengo, J., Garc X a, S., and Herrera, [12] Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., and Movellan, J. 
