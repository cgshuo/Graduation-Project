 Abstract Collocations in the sense of idiosyncratic binary lexical co-occurrences are one of the biggest challenges for any language learner. Even advanced learners make collocation mistakes in that they literally translate collocation elements from their native tongue, create new words as collocation elements, choose a wrong subcategorization for one of the elements, etc. Therefore, automatic collocation error detection and correction is increasingly in demand. However, while state-of-the-art models predict, with a reasonable accuracy, whether a given co-occurrence is a valid collocation or not, only few of them manage to suggest appropriate cor-rections with an acceptable hit rate. Most often, a ranked list of correction options is offered from which the learner has then to choose. This is clearly unsatisfactory. Our proposal focuses on this critical part of the problem in the context of the acquisition of Spanish as second language. For collocation error detection, we use a frequency-based technique. To improve on collocation error correction, we discuss three different metrics with respect to their capability to select the most appropriate correction of miscollocations found in our learner corpus.
 Keywords Collocation Collocation error Miscollocation CALL Collocation error detection Collocation error correction 1 Introduction Collocations in the sense of idiosyncratic binary lexical co-occurrences such as take [ a ] leave, blow [ a ] kiss, give [ a ] talk, heavy storm, strong tea , etc., pose one of the biggest challenges for any learner of a second language (Granger 1998 ; Howarth 1998a ; Lewis 2000 ; Nesselhauf 2003 ; Lesniewska 2006 ); see also (Futagi et al. 2008 ) and (Chang et al. 2008 ) for further extensive references. This is because the choice of one of the two elements in a collocation is free while the choice of the second depends on the first, such that while grammatical constructions X  X ncluding those that are very different from constructions in L1 1  X  X ollow generalized patterns and can thus be applied by analogy once some representative samples have been learned, collocations are much less generalizable and must be learned nearly one by one (Hausmann 1984 ; Nation 2001 ; Futagi et al. 2008 ). Even advanced learners who master well the grammar of L2 make collocation mistakes in that they often literally translate collocation elements from L1 or another foreign language, use non-existing words as collocation elements, get the subcategorizion of one of the elements wrong, etc. (Alonso Ramos et al. 2010a ). Automatic means for detection and correction of collocation mistakes in L2 writings are thus in high demand. However, the results of the research in the area still lag behind the expectations. First, the ovewhelming majority of the proposals are for English as L2 X  X ee, among others, (Pantel and Lin 2000 ; Shei and Pain 2000 ; Wible et al. 2003 ; Futagi et al. 2008 ; Chang et al. 2008 ; Park et al. 2008 ; Wu et al. 2010 ) X  X ith no sufficient evidence that they work equally well for other L2s. Second, while miscollocation detection, for which most often frequency-based techniques as used in Natural Language Processing (NLP) for collocation extraction from corpora are exploited, achieves in some approaches a nearly operational use accuracy (e.g., Chang et al. 2008 achieve 90.7 %), the accuracy of miscollocation correction is considerably lower. Therefore, it is most common to offer a (ranked) list of potential corrections from which then the learner must choose the one she considers most appropriate. Chang et al. ( 2008 ) report a mean reciprocal rank (MRR), which orders the suggestions by probability of their correctness, of 0.66 on their lists and Wu et al. ( 2010 ) an MRR of 0.518. This is not sufficient for operational use. 2 It is thus the stage of miscollocation correction, which especially calls for advances. In what follows, we focus on this stage in the context of Spanish as L2. For collocation error detection, we use a simple frequency based metric, which is however good enough for our experiments. In all our experiments, we use the Spanish learner corpus Corpus Escrito del Espa X ol L2 (CEDEL2) (Lozano 2009 ). 3
The remainder of the article is structured as follows. Section 2 contains a brief introduction to the phenomenon of collocations and the discussion of the challenges that collocations pose to L2 learners. Section 3 briefly reviews the related work in the area of collocation error recognition and correction. In Sect. 4 , we present our model for advanced collocation error correction, which we evaluate in an experiment presented in Sects. 5 and 6 concludes with a summary of our findings and an outline of future work. 2 Collocations: a challenge for the learners Long time, second language learning in general and CALL in particular focused on difficulties of learners with grammatical constructions. The consequence of this grammar bias was that while for typical grammatical errors more or less detailed analyses have been performed and CALL-techniques to address them were developed, 4 all errors related to the lexicon have been classified simply as  X  X  X exical errors X  X ; see, for instance (Gamon et al. 2009 ; Chen 2009 ), without any further distinction. This is certainly an oversimplification: a misspelling (as, e.g., Sp. * dispacho  X  X ffice X  instead of despacho ) is different from the creation of a non-existent word (as Sp. * llamo instead of llamada  X  X all X ) and the latter is different from getting wrong a phraseme (as, e.g., * m X s temprano o tarde , lit.  X  X ooner or concluir un problema , lit.  X  X onclude a problem X  instead of resolver un problema ,lit.  X  X esolve a problem X ). Especially collocations, where only one of the lexemes (the base ) has the same meaning as when used in isolation, while the meaning of the other (the collocate ) depends on the base, 5 constitute a challenge to learners. Several studies on English as L2 see a direct correlation between the quality of a learner X  X  writing and the degree this learner masters collocations (Granger 1998 ; Howarth 1998b ; Nesselhauf 2005 ; Gilquin 2007 ) and identify collocation mistakes as the most recurrent mistakes detected in learners X  X  writings (Wible et al. 2003 ). The prominence of collocations can be also observed in the case of Spanish as L2. According to Alonso Ramos et al. ( 2010b ) X  X  analysis of a subcorpus of CEDEL2, about 39 % of the collocations used by advanced learners of Spanish were incorrect.
Since the early 2000s, a considerable amount of work has been carried out on the development of programs (although focused mainly on English as L2) that judge a combination to be a valid or invalid collocation and, in the latter case, attempt to provide a list of correction suggestions. But, again, to consider all collocation errors to be of the same unique class is an oversimplification which does not do justice to the complexity of the problem and thus to the needs of learners. Alonso Ramos et al. ( 2010b ) X  X  study also reveals that learners produce a considerable variety of collocation error types, each of them potentially requiring a different kind of exercise or a different type of sample material to be provided by the learning environment. Consider, for illustration, the following examples from CEDEL2: 1. gastar todo el a X o estudiando espa X ol , 2. hacer citas , lit.  X  X ake appointments X  3. escribir [ un ] examen , lit.  X  X rite [an] examen X  4. tomar puesto , lit.  X  X ake post X  5. hablar un lenguaje , lit.  X  X peak a (formal) language X  6. derechos *mujeriles  X  X omen X  X  rights X  7. ense X anza *segundaria  X  X econdary education X  8. recibir un *llamo , lit.  X  X eceive a call X  9. asistir la universidad , lit.  X  X ssist a university X  10. Yo tengo el deseo personal de ser biling X e ,
The examples show that our error coverage goes beyond what can be considered correctness of a collocation: (i) the correct choice of the collocate and base, (ii) the correctness of the collocation as a whole, (iii) the correctness of the subcatego-rization of the base, and (iv) the correct use of the collocation. In (1) X (4), we can observe the most common type of miscollocations: gastar  X  X pend X , hacer  X  X ake X , escribir  X  X rite X , and tomar  X  X ake X  are not correct as collocates of a X  o  X  X ear X , cita  X  X ppointment X , examen  X  X xam X , and puesto  X  X ost X , respectively X  X lthough they form correct collocations with other bases; compare, e.g., gastar dinero  X  X pend money X , hacer [ una ] llamada  X  X ake [a] call X , escribir [ una ] carta  X  X rite [a] letter X , tomar medidas , lit.  X  X ake measures X . In (5), the base lenguaje  X  X anguage X  is not correct for the intended usage; the correct base would have been lengua  X  X anguage X : hablar una lengua  X  X peak a language X . In (6) and (7), the learner created non-existing collocates in Spanish X  mujeriles and segundaria ; the right collocates would have been de las mujeres  X  X f the women X  and secundaria  X  X econdary X , respectively. In (8) the same occurs with the base: llamo does not exist in Spanish; the right word is here llamada  X  X all X . In (9), the collocate requires a preposition: asistir a la universidad , lit.  X  X ssist to a university X  ( attend [ a ] university ), and finally, in (10), tener un deseo  X  X ave a wish X  is, in principle, a correct collocation, however, its use is inappropriate in the given context; the appropriate phrasing would have been quiero ser biling X e  X  X I] want to be bilingual X .
Three questions arise in view of this amount and diversity of collocation errors: (i) is it possible to classify collocation errors such that each class reflects a single (ii) how are collocations and collocation errors to be annotated in a learner corpus (iii) how can we identify and correct collocation errors in the writings of learners
The first question has been addressed in (Alonso Ramos et al. 2010a ), where a fine-grained multi-dimensional collocation error typology has been presented. The dimensions of the typology capture: (1) the scope of the error (collocate, base or collocation as a whole); (2) the type of the error (lexical or grammatical) and the subtype of the error (choice of a wrong element, creation of a non-existing element, use of a correct collocation which has a different meaning from the intended one, etc. in the case of a lexical error, and error in determination, number, government, etc. in the case of a grammatical error); and (3) the source (or motivation) of the error (erroneous phonetic similarity, erroneous morphological derivation, L1 calque, etc.). 6 The second question has been addressed in (Alonso Ramos et al. 2010b ), where an annotation schema for collocations and collocation errors in learner corpora has been proposed. 7 In what follows, we address the third question. Its satisfactory intelligent CALL-driven solution naturally consists of two parts. First, to be able to identify and correct collocation errors in learner corpora, and, second, to be able to extract from reference corpora illustrative and supportive material: collocations of the type the learner seems to have difficulties with, collocations with which the learner seems to confuse the collocations she tends to make mistakes in, examples of context in which a specific collocation is used; etc. The second part is currently out of the reach for CALL. It presupposes namely that we are able not only to identify miscollocations, but also classify them automatically (for instance, in accordance with Alonso Ramos et al. ( 2010a ) X  X  typology). We focus on the first part. But before we embark on the outline of our proposal, let us briefly review the related work in this area. 3 Related work The late appearance of collocation-oriented CALL on the research map is certainly also because collocation error detection and correction presupposes, on the one hand, advanced models for collocation recognition and, on the other hand, the capacity of ranking the different miscollocation correction suggestions. Outside CALL, the identification of collocations in corpora has been actively worked on since the late eighties. Many authors explore purely statistical models (Choueka 1988 ; Church and Hanks 1989 ; Evert 2007 ; Pecina 2008 ). These models can be more or less complex, but all of them measure in one way or the other the distribution of words in combination and in isolation. Some of the works combine a statistical model with the use of syntactic features X  X or instance, submitting to the statistical model only word co-occurrences that form valid syntactic structures (Smadja 1993 ; Kilgarriff 2006 ; Evert and Kermes 2003 ). Most recent statistical proposals take into account the context of the co-occuring words X  X hich allows for the consideration of their distributional semantics (Bouma 2010 ). Another strand uses the co-occurrence range of a given word, i.e., relative frequencies of tokens that co-occur with this word most often (Wible and Tsao 2010 ). Opposed to token frequency based models is the model that uses explicit semantic features from EuroWordNet (Vossen 1998 ) to identify and semantically classify collocations (Wanner et al. 2006 ).

In CALL, most commonly, statistical models are applied to V(erb) ? N(oun) co-occurrences; see, for instance, (Chang et al. 2008 ; Park et al. 2008 ; Yin et al. 2008 ; Wu et al. 2010 ; Dahlmeier and Ng 2011 ). Since the pioneering work by Shei and Pain ( 2000 ), who still offer only precompiled correction suggestions, quite a few proposals have been made on how to improve the collocation competence of learners of English. Yin et al. ( 2008 ) acknowledge that compared to other lexical errors, collocation error detection and correction is more challenging, even when using sophisticated statistical models and large corpora, including the web. They divide a learner sentence into (collocation) chunks and individual words, which are then used as queries to a search engine. The frequency of the chunks in the web determines whether the chunks are valid in L2. If they are not, they are substituted by maximally overlapping chunks obtained when querying individual words. A precision of 37 % at 30 % recall for this strategy is reported.

Chang et al. ( 2008 ) and Dahlmeier and Ng ( 2011 ) focus on L1 interference in learners X  writings. Chang et al. ( 2008 ) first extract V-N co-occurrences from a given writing. Then, they check the extracted co-occurrences against a collocation list obtained before from a reference corpus. Co-occurrences not found in the collocation list are variegated in that their verbal elements are substituted by all English translations of their L1 (Chinese, in this case) counterpart in an electronic dictionary. The variants are again matched against the collocation list. The finally matching co-occurrences that contain the noun of a non-matching co-occurrence are offered as correction suggestions. Chang et al. ( 2008 ) report a precision of 97.5 % for the recognition of collocations and 90.7 % for the recognition of miscollocations. The MRR of the correction list is reported to reach 0.66. Dahlmeier and Ng ( 2011 ) produce confusion sets of semantically similar words. Given an input text in L2, they generate L1 paraphrases, which are then looked up in a large parallel corpus to obtain the most likely L2 co-occurrences. For this strategy, they report a precision of 38 %.
Futagi et al. ( 2008 ) target the detection of miscollocations in learner writings, leaving occurrences. But similar to Chang et al. ( 2008 ), they extract the co-occurrences from a learner writing, variegate the m and then look up the original co-occurrence and its variants in a reference list to decide on its status. To obtain the variants, they apply spell checking, variate articles and inflections and use Wo rdNet to retrieve synonyms of the collocate.
Wu et al. ( 2010 ) go somewhat further in that they work with  X  X ubject X  ?  X  X erb X  and  X  X erb X  ?  X  X bject X  tuples instead of PoS co-occurrences. The tuples are extracted from a reference corpus (RC) using a dependency parser (Klein and Manning 2003 ) and filtered to get rid of free rare co-occurrences. A Maximal Entropy (ME) classifier is trained on the lexical context of each collocation in the RC list. For the correction of a miscollocation, the classifier provides a number of collocate corrections using the learner sentence as lexical context. The probability predicted by the classifier for each suggestion is used to rank the suggestions. According to the evaluation included in (Wu et al. 2010 ), an MRR of 0.518 for the first five correction suggestions has been achieved. 8
Liu et al. ( 2009 ) X  X  goal is to develop a model for automatic suggestion of corrections for given miscollocations. To retrieve the suggestions from a reference corpus, they use three metrics: (i) mutual information (Church and Hanks 1989 ), (ii) semantic similarity of an incorrect collocate to other potential collocates based on their distance in WordNet, and (iii) the membership of the incorrect collocate with a potential correct collocate in the same  X  X  X ollocation cluster X  X . 9 A combination of 55.95 %. A combination of (i) ? (ii) ? (iii) leads to the best precision of 85.71 % when a list of 5 possible corrections is returned. 4 Towards advanced collocation error correction As pointed out in Sect. 1 , we focus on collocation error correction in Spanish learner essays. To judge whether a V ? N, V ? Adv, Adj ? N or Adj ? Adv co-occurrence C in a learner essay is a valid collocation or not, we use a simple frequency metric.
As far as miscollocation correction is concerned, the previous works show that a number of criteria call to be taken into account:  X  the learner may misspell a collocate such that the exploration of graphically  X  the learner often produces collocation mistakes by collocate calques from L1  X  as learned from corpus-based collocation detection research, the association  X  the context of a miscollocation in a learner essay is essential when searching for We attempt to capture these criteria in our miscollocation correction metrics.
Our reference corpus of Spanish consists of lists of PoS-tagged n -grams (2 B n C 5) extracted from about 1850 million words of newspaper material (in total, 70 volumes of two major Spanish newspapers) or from a smaller sample of it. Further auxiliary resources of which we make use are: the Open Office thesaurus of Spanish, the Spanish EuroWordNet, an automatically compiled bilingual Spanish-English vocabulary, and the web (as additional reference resource). These auxiliary resources shall help us to take into account the phenomenon of synonymy (the learners may choose a term which is (quasi-) synonymous to the correct element) and 1: n translation equivalence (the learner may choose a wrong (literal) translation of the collocation element in L1), and ensure that we have the widest evidence of collocation use in L2.

Given a V ? N, V ? Adv, Adj ? N or Adj ? Adv combination C extracted from the writing of the learner, our basic algorithm for the collocation error detection and correction is as outlined in Fig. 1 .

In what follows, we explore three different metrics for judging which of the correction candidates is the best correction of the supposedly erroneous C (cf. line 9 of the algorithm). 4.1 Affinity metric The affinity metric is a local metric which takes into account: 1. the co-occurrence (or association ) strength ( as ) of the collocate candidate c can with the base B of the miscollocation in the RC; 2. the graphic similarity gs of c can with the original collocate Co , and 3. the synonymy syn of c can with Co . The affinity Af is calculated as
The association strength as is a standard parameter in collocation identification procedures; see also (Liu et al. 2009 ). It is measured, for instance, in terms of pointwise mutual information , log-likelihood , etc. In the experiment in Sect. 5 ,we used log-likelihood (which, somewhat simplified, assesses how likely it is that c can and B form an idiosyncratic co-occurrence):
The graphic similarity gs shall capture cases where the learners mistyped a collocate or erroneously chose a collocate due to its graphic similarity to the intended one (as, e.g., rise instead of raise in English); 10 see (Futagi et al. 2008 ) for a similar idea.

We calculate graphic similarity as the Dice coefficient, using character bigrams as features (such that it provides the relative overlap of the character bigrams in Co (with Co Bi as the set of character bigrams of Co and c can Bi as the set of character bigrams of c can ). For instance, in the case of the misspelled collocate in * cojer (instead of coger  X  X atch X ) in * cojer un catarro  X  X atch a cold X , Co Bi will be common bigrams, while each of them is composed of four different bigrams.

The synonymy factor syn is  X 1 X  if c can is among the synonyms of Co in the synonym list obtained from the auxiliary resources and  X 0 X  otherwise. 4.2 Lexical context metric In contrast to the affinity metric, the lexical context metric takes into account the context in which the miscollocation occurs X  X s already suggested by Wu et al. ( 2010 ). However, unlike Wu et al., who distill the contextual features from the contexts of the correct equivalent of a miscollocation in the reference corpus (RC), we think that it is the essay, which should provide the contextual features that are then to be matched with the contexts of the candidate correction searched for in the RC: a learner may use a (mis)collocation in contexts that deviate from the most common contexts in the RC; should this occur, a machine learning mechanism trained on the most common contexts of a correction will be inaccurate when confronted with features extracted from the learner X  X  context (in the extreme case, the mechanism will have discarded all of the features in the learner X  X  context or assigned to them very small weights when being trained). On the contrary, starting from the contextual features of a miscollocation, it suffices to collect sufficient evidence that a correction candidate is used in the context provided by the learner. 11
In other words, the lexical context metric is grounded in the assumption of distributional semantics, namely that the semantics of a collocation can be approximately deduced from the sentential context in which this collocation appears. Consider, for illustration, the following sentences (taken from the web) in which one of the words has been removed: 11. She * a conference on the situation of women rights ... 12. Mr. White responded to the changing industry and * a conference of critical 13. Eventcorp * a conference that met the Conference Committee X  X  criteria
The reader can deduce with a certain probability that the missing word is [ to ] deliver or any other support verb that goes with conference and that is synonymous in this context to deliver . We argue that this is due to the fact that the reader uses the distributional semantics of the context of conference that allows her to come up with [ to ] deliver . In contrast, in 14. The mailman * apples, bananas, and coconunts . 15. Oo baby, here I am, signed, sealed, *, I X  X  yours, oh I X  X  yours ... [Stevie 16. Flowers * by hand on your behalf by our expert florists . this is not the case: we cannot reliably guess the missing verb. This gives us a hint that in (14 X 16) the missing verb does not participate in a collocation.

We can thus hypothesize that the context can be useful for the detection of collocations, or, in our case, for the search for the most adequ ate correction suggestion. More precisely, c , c i ? 1 , ... , c n in the RC is the most adequate correction of Co  X  X ith  X  X  X ffinity X  X  meaning here the highest co-occ urrence frequency: (with  X  c j -c can  X  as the occurrence of c j with c can at a distance of maximally k tokens).

In our experiments, we used so far n B 8 with k = 2 , always within the borders of a single sentence; duplicates are eliminated. 12 For instance, in the learner sentence (17): 17. Sp. Afortunadamente, su profesora estuvo dispuesta a venderlas y pudo the context tokens during the search for the optimal correction suggestion for the miscollocation * extender [ una ] colecci X n  X  X xtend a collection X  would be m X scara  X  X ask X , para  X  X or X  and nuestro  X  X ur X . In further experiments (see Sect. 6 ), we plan to use the entire sentential context X  X lthough some processing time restrictions may arise for long sentences. 4.3 Context feature metric The context feature metric is similar to the lexical context metric in that it draws upon the context of the miscollocation in the original learner sentence. However, there are also two significant differences. First, it may take into account not only lexical tokens (although this is what we tested it with so far; see below), but any kind of contextual features (PoS tags, grammatical functions, punctuation, etc.). Second, its interpretation of these features is very different: Given the contextual features c f candidates C can , the idea is to assess whether the majority of the contextual features c of Co speaks for the preference of one of the candidates c can [ C can . For this purpose, we search, for each feature c f ,a c can that shows towards it the highest affinity: where N ( c cani , B ) stands for the number of times the combination ( c cani , B ) occurs in the corpus, and N ( c f ,( c cani , B )) for the number of times the feature c and the combination ( c cani , B ) co-occur in the corpus at a certain distance from each other.
What Eq. ( 5 ) does is to assess how common the feature c f is when a given candidate correction c cani co-occurs with the base B and how idiosyncratic the co-occurrence of c cani with B is. For instance, in the learner sentence (18), the collocation * sacarse [ una ] operaci X n , lit.  X  X ake off an operation X  is not correct: 18. Es f X cil, s X lo hay que sacarse una operaci X n como Michael Jackson
To find the right correction, the affinity between the candidate collocations of operaci X n  X  X peration X  and each of the contextual features in the reference corpus is examined. In the experiments carried out so far, we used lexical tokens as contextual features (similar to the lexical context metric). The contextual features in (18) would thus be s X lo, hay, que, como, Michael and Jackson . 13 For each of these features, its preference for concluir  X  X onclude X , detener  X  X top X , drogar  X  X rug X , herir  X  X arm X , informar  X  X nform X , producir  X  X roduce X , recibir  X  X eceive X , rescatar  X  X escue X  or ser  X  X e X  (the correction candidates provided by our algorithm) is assessed according to Eq. ( 5 ). 5 Experiments In order to test the performance of our collocation recognition algorithm and the quality of each of the miscollocation correction metrics, we ran the procedure in Fig. 1 on a number of sentences from CEDEL2, exploring also the influence of the nature and size of the reference corpus on the task. 5.1 Set up of the experiments 5.1.1 Testing reference corpora As mentioned above, our strategy for collocation error identification is purely frequency based in that it draws upon the frequency of a given co-occurrence in a reference corpus to decide whether this co-occurrence is a correct collocation or not. However, the frequency of a co-occurrence depends on the nature and on the composition (and thus indirectly also on the size) of the reference corpus. For instance, in the web, the high number of non-native authors of English (and thus the potential use of co-occurrences not accepted by native speakers) may distort the relative frequencies of collocations. In the case of Spanish, we also need to bear in mind that the frequencies may significantly vary across the different Spanish speaking countries.

The size of the reference corpus also matters since a very large corpus bears the risk of containing too much noise and a small corpus may not contain a sufficient number and diversity of co-occurrences.

To assess, at least partially, these restrictions, we evaluated the collocation error identification task against three different corpora: the web (consulted using the Yahoo! Boss Search Appi), a large corpus of 5GB of newspaper material of peninsular Spanish (about 1850 million words) and a smaller sample of approx-imately 2GB (about 740 million words) from the same corpus. 5.1.2 Testing error correction metrics In a different experiment, we assessed the quality of the error correction metrics, letting them compete with each other. As reference corpus, the 1850 million word corpus was used. In total, 61 runs on V ? N combinations were carried out. In each run, the algorithm in Fig. 1 was applied to a given V ? N combination and a sentence from the learner corpus in which this combination occured. For instance, one of the combinations was hacer citas , lit.  X  X ake appointments X  in the learner sentence: 19. En mi nueva posici X n, yo hice planes de viajar para los grupos, acud X  el
Due to the low (more precisely, zero) frequency of hacer citas in the reference corpus (0 &lt; T = 7), it is judged to be a collocation error. The condition in Step 5 of the algorithm in Fig. 1 returns  X  X alse X  such that Steps 8 and 9 are carried out. Step 8 returns the following list of candidates: 14 realizar [ una ] cita  X  X ealize an appointment X , producir [ una ] cita  X  X roduce [an] appointment X , dar [ una ] cita  X  X ive [an] appointment X , tener [ una ] cita  X  X ave [an] appointment X , ir [ a una ] cita  X  X o [to an] appointment X , acudir [ a una ] cita  X  X urn [to an] appointment X , declarar [ una ] cita  X  X eclare [an] appointment X , haber [ una ] cita  X  X eceive [an] appointment X , concertar [ una ] cita  X  X rrange [an] appointment X , ser [ una ] cita  X  X e [an] appointment X , agenciar [ una ] cita  X  X ediate [an] appointment X .

Step 9 applies the individual metrics to select the best correction. The affinity metric suggests realizar [ una ] cita  X  X ealize an appointment X , while the lexical and the context feature metrics suggest concertar [ una ] cita  X  X rrange [an] appoint-ment X  X  X hich is, in fact, the most appropriate correction of hacer citas .

Table 1 displays a number of further examples of correction suggestions of the three metrics. In the 10 displayed trials, the affinity metric failed 5 times, the context feature metric 4 times, and the lexical context metric 2 times. Note that some wrong correction suggestions may be valid collocations in Spanish. For example, dar [ una ] oportunidad  X  X to] give [an] opportunity X  is a correct collocation in Spanish, but with different semantics than the one required taking into account the sentence of the learner in (20).
 20. Como he dicho, me encantaba el espa X ol, y yo utilizo cada oportunidad que
Another example is imponer [ una ] regla  X  X to] impose [a] rule X , which is a correct collocation, but which, again, does not reflect the intention of the learner in their writing: Los sistemas religiosos no quieren que los sistemas gubernamentales interrumpen [sic] sus reglas y leyes ., lit.  X  X he religious systems do not want that the governmental systems interrupt their rules and laws. X 
In some cases, more than one collocation candidate can be considered as the correct suggestion, Consider, for instance, the incorrect collocation concluir [ un ] problema , lit.  X  X onclude [a] problem X , in the learner X  X  sentence (21): 21. Quiz X s ser X  la ciencia que descubra en el futuro algo que ayudara a concluir where all three metrics suggest a different but adequate correction: resolver [ un problema  X  X esolve [a] problem X , solucionar [ un ] problema  X  X olve [a] problem X , and acabar [ con un ] problema  X  X erminate [with a] problem X . 5.2 Results and evaluation of the experiments Although we did not make an effort to optimize our collocation (error) identification strategy, we take its output as input for our miscollocation correction procedure. Therefore, an evaluation of it is necessary. 5.2.1 Evaluation of the identification of miscollocations Figure 2 displays the percentage of correctly identified collocations and miscollo-cations using the different reference corpora.

According to these figures, the 1850 million word corpus is more suitable for the detection of correct collocations (with a recognition ratio of about 98 %) than the 740 million word corpus or the web. This is likely, on the one side, due to the small size or imbalance of the 740 million word corpus and, on the other side, due to the diversity and unreliability of the data sources in the web and thus due to its uncontrolled nature. However, with a recognition ratio of about 84 %, the web is considerably better than the 740 million word corpus. A clear advantage of using the web as corpus is that even if it is not balanced, it contains a wide range of text topics different registers.

With a recognition ratio of about 98 %, the web turned out to be more adequate as reference corpus for the identification of miscollocations than the other two corpora. This is somewhat surprising since one would expect that its uncontrolled nature mentioned above would introduce considerable noise (or, in other words, traces of the use of collocations judged by native speakers as incorrect). The 1850 million word corpus is with a recognition ratio of about 73 % considerably less reliable, and so is the 740 million word corpus (with 49 %). Further investigation is needed to clarify this outcome.

Overall, with the web as reference corpus we are able to judge whether a combination is a correct or incorrect collocation in Spanish with an accuracy of about 91 %. From 61 samples, the algorithm fails in six cases. In five of these six cases, correct collocations have been judged to be incorrect. This is mainly due to our frequency based criteria. For instance, apretar [ los ] dientes , lit.  X  X ress [the] teeths together X , contar cuentos , lit.  X  X ell stories X , dar [ la ] bienvenida , lit.  X  X ive [the] welcome X , and preparar [ la ] comida , lit.  X  X repare [the] food X , are correct collocations in Spanish, but their frequencies in our reference corpus are too low to consider them valid. 15 On the other hand, pasar [ la ] navidad , lit.  X  X ass [the] Christmas X  is judged by the program to be a correct collocation due to its high frequency in the corpus, although it is questionable in peninsular Spanish. 16
The figures also show that the 740 million words corpus is not adequate to serve as reference corpus for collocation (error) recognition: it simply does not contain enough evidence on the use of the individual co-occurrences. 5.2.2 Evaluation of collocation correction suggestions For the second stage, i.e., the error correction stage, we performed so far two evaluations. Fist, we evaluated the probability that the list of correction suggestions retrieved by our algorithm (Step 8 in Fig. 1 ) contains the adequate correction of the given miscollocation. This probability amounts to 0.73 if we consider the first 15 suggestions (ranked by frequency): in 73 % of the trials, the right correction is encountered in the list of the first 15 suggestions offered by our program. Working with a list of 10 suggestions, we achieve an accuracy of 0.66 and with a list of 5 suggestions, an accuracy of 0.6.

In order to assess the quality of the frequency-based ranking of the correction suggestions, we calculated the ratio of correct miscollocation corrections versus the size of the correction suggestion list for 30 miscollocations. The results of this test are illustrated in Fig. 3 . For instance, for 18 instances, the adequate correction is in the top five of the list; for 20 instances, the correction is in the top ten, and so on. This means that the provided correction suggestion list is reasonbly good (although not perfect), and we can choose a single correction from it using our metrics.
Second, we evaluated the capacity of the different metrics to pick the right correction from the correction suggestion list. The affinity metric achieved an accuracy of 17.24 %, the lexical context metric 27.58 % and the context feature metric, with features being simply words in the original sentence of the learner, 54.2 %. 17 The mean reciprocal rank (MRR) of the top five suggestions obtained using the contextual feature metric is 0.72.

The poor performance of the affinity metric needs further examination. It is possible that the metric itself is not adequate. To check this, a reimplementation of partially due to the deficiency of the auxiliary resources we used. Thus, Spanish WordNet is less complete than its English counterpart. Furthermore, it was derived semi-automatically from the English WordNet, with the consequence that it is also considerably noisier.

The equally poor performance of the lexical context metric is likely due to its simplicity. The context feature metric, which is also based on the notion of context, proved to be more adequate. Its performance challenges state-of-the art proposals such as (Chang et al. 2008 ) (with an MRR of 0.66) and (Wu et al. 2010 ) (with the highest MRR of 0.518). However, even 54.2 % of accuracy and an MRR of 0.72 are certainly still too low for practical CALL. On the other side, it is to be pointed out that the potential of the contextual features has not been fully explored as yet: the use of concrete words is too restrictive. The experience from statistical NLP (e.g., parsing and generation) teaches us that combinations of morpho-syntactic catego-ries, grammatical functions and words are more promising. We will carry out experiments in this respect in the near future. 6 Conclusions and future work We presented a proposal for automatic recognition and correction of miscolloca-tions in written material of learners of Spanish. Unlike many other proposals in the field, our goal was to explore how well we can offer adequate corrections of miscollocations rather than ranked lists of possible corrections. Although we agree that a ranked list of possible corrections may suffice for advanced learners, it does not serve well the needs of beginners or intermediate level learners X  X specially if the suggestions show subtle semantic differences. Furthermore, unlike, e.g., (Wu et al. 2010 ), the proposal that is most similar to ours in that it also takes the (mis)collocation context into account X  X ur experiments have been carried out with real learner essays because it is only when working with real material that we can obtain evidence that the proposed approach may serve as the basis for advanced collocation error correction techniques.

The results we obtained with our current initial implementation compete with state of the art proposals, although they are still far from being fully satisfactory. Thus, a collocation/miscollocation recognition rate of 90 % means that the judgement of each tenth co-occurrence is wrong, and a correction rate of 54.2 % means that for nearly each second miscollocation not the best correction is suggested.
 In order to have some evidence that our error correction proposal is not limited to Spanish, we carried out a small experiment on English learner material in that we: (i) took ten English collocation errors made by Chinese learners of English in their 1. The interaction between us really made me grown up mind a lot and helped 2. These merits she owns make me to take more respect and honour to her ... 3. I should not break her privacy . 4. When you said lies , you may never let others to believe what you say . 5. My hand flowed much blood and I was very sad .
 6. Karen is a charming girl who always keeps a smile on her face . 7. ... my dad told me if I could have a great grade then he would buy it for me . 8. We often changed our secrets with each other . 9. I learned a lot of knowledge from him , ... 10. ... I decided to change myself and to do something breakthrough : (ii) introduced the ten collocation errors one by one into the MUST collocation (iii) applied to the lists of MUST X  X  correction suggestions (interpreted as The MRR of the MUST ranked suggestions in (ii) has been 0.4, while the MRR of the suggestions ranked by our lexical context metric in (iii) reached 0.53, i.e., the present proposal could be used to improve the ranking of the MUST checker. For 3 of the 10 miscollocations, the lexical context metric ranked first the right correction; for 4 the right correction was ranked second, for 1 third, for 1 fifth and for 1 the right correction was not among the first five. It is to be expected that the contextual feature metric, which we did not use in the experiment due to a considerably higher set up cost, would have performed even better (see Sect. 5.2.2 ). That is, we can assume that our collocation error correction metrics are indeed language-independent.

Despite this encouraging outcome, there is still a great potential to improve the performance of both miscollocation identification and correction X  X hich we target in our future work. To further improve (mis)collocation recognition, we plan to go beyond the assumption of context independence of co-occurrences, as has already been suggested by Bouma ( 2010 ). To improve on the correction strategies, we plan to significantly broaden the types of features taken into account for the feature context metric. A further experiment will be dedicated to the exploration of a voting strategy between the different metrics: as illustrated by Table 1 , already a simple majority vote would introduce somewhat more stability into the results.

In parallel, we explore the use of the collocation (error type) annotated fragment of CEDEL2 (Alonso Ramos et al. 2010b ) as training material for machine learning based recognition of miscollocations in learner corpora and the use of the Google n -gram list as reference corpus.
 References
 Abstract Collocations in the sense of idiosyncratic binary lexical co-occurrences are one of the biggest challenges for any language learner. Even advanced learners make collocation mistakes in that they literally translate collocation elements from their native tongue, create new words as collocation elements, choose a wrong subcategorization for one of the elements, etc. Therefore, automatic collocation error detection and correction is increasingly in demand. However, while state-of-the-art models predict, with a reasonable accuracy, whether a given co-occurrence is a valid collocation or not, only few of them manage to suggest appropriate cor-rections with an acceptable hit rate. Most often, a ranked list of correction options is offered from which the learner has then to choose. This is clearly unsatisfactory. Our proposal focuses on this critical part of the problem in the context of the acquisition of Spanish as second language. For collocation error detection, we use a frequency-based technique. To improve on collocation error correction, we discuss three different metrics with respect to their capability to select the most appropriate correction of miscollocations found in our learner corpus.
 Keywords Collocation Collocation error Miscollocation CALL Collocation error detection Collocation error correction 1 Introduction Collocations in the sense of idiosyncratic binary lexical co-occurrences such as take [ a ] leave, blow [ a ] kiss, give [ a ] talk, heavy storm, strong tea , etc., pose one of the biggest challenges for any learner of a second language (Granger 1998 ; Howarth 1998a ; Lewis 2000 ; Nesselhauf 2003 ; Lesniewska 2006 ); see also (Futagi et al. 2008 ) and (Chang et al. 2008 ) for further extensive references. This is because the choice of one of the two elements in a collocation is free while the choice of the second depends on the first, such that while grammatical constructions X  X ncluding those that are very different from constructions in L1 1  X  X ollow generalized patterns and can thus be applied by analogy once some representative samples have been learned, collocations are much less generalizable and must be learned nearly one by one (Hausmann 1984 ; Nation 2001 ; Futagi et al. 2008 ). Even advanced learners who master well the grammar of L2 make collocation mistakes in that they often literally translate collocation elements from L1 or another foreign language, use non-existing words as collocation elements, get the subcategorizion of one of the elements wrong, etc. (Alonso Ramos et al. 2010a ). Automatic means for detection and correction of collocation mistakes in L2 writings are thus in high demand. However, the results of the research in the area still lag behind the expectations. First, the ovewhelming majority of the proposals are for English as L2 X  X ee, among others, (Pantel and Lin 2000 ; Shei and Pain 2000 ; Wible et al. 2003 ; Futagi et al. 2008 ; Chang et al. 2008 ; Park et al. 2008 ; Wu et al. 2010 ) X  X ith no sufficient evidence that they work equally well for other L2s. Second, while miscollocation detection, for which most often frequency-based techniques as used in Natural Language Processing (NLP) for collocation extraction from corpora are exploited, achieves in some approaches a nearly operational use accuracy (e.g., Chang et al. 2008 achieve 90.7 %), the accuracy of miscollocation correction is considerably lower. Therefore, it is most common to offer a (ranked) list of potential corrections from which then the learner must choose the one she considers most appropriate. Chang et al. ( 2008 ) report a mean reciprocal rank (MRR), which orders the suggestions by probability of their correctness, of 0.66 on their lists and Wu et al. ( 2010 ) an MRR of 0.518. This is not sufficient for operational use. 2 It is thus the stage of miscollocation correction, which especially calls for advances. In what follows, we focus on this stage in the context of Spanish as L2. For collocation error detection, we use a simple frequency based metric, which is however good enough for our experiments. In all our experiments, we use the Spanish learner corpus Corpus Escrito del Espa X ol L2 (CEDEL2) (Lozano 2009 ). 3
The remainder of the article is structured as follows. Section 2 contains a brief introduction to the phenomenon of collocations and the discussion of the challenges that collocations pose to L2 learners. Section 3 briefly reviews the related work in the area of collocation error recognition and correction. In Sect. 4 , we present our model for advanced collocation error correction, which we evaluate in an experiment presented in Sects. 5 and 6 concludes with a summary of our findings and an outline of future work. 2 Collocations: a challenge for the learners Long time, second language learning in general and CALL in particular focused on difficulties of learners with grammatical constructions. The consequence of this grammar bias was that while for typical grammatical errors more or less detailed analyses have been performed and CALL-techniques to address them were developed, 4 all errors related to the lexicon have been classified simply as  X  X  X exical errors X  X ; see, for instance (Gamon et al. 2009 ; Chen 2009 ), without any further distinction. This is certainly an oversimplification: a misspelling (as, e.g., Sp. * dispacho  X  X ffice X  instead of despacho ) is different from the creation of a non-existent word (as Sp. * llamo instead of llamada  X  X all X ) and the latter is different from getting wrong a phraseme (as, e.g., * m X s temprano o tarde , lit.  X  X ooner or concluir un problema , lit.  X  X onclude a problem X  instead of resolver un problema ,lit.  X  X esolve a problem X ). Especially collocations, where only one of the lexemes (the base ) has the same meaning as when used in isolation, while the meaning of the other (the collocate ) depends on the base, 5 constitute a challenge to learners. Several studies on English as L2 see a direct correlation between the quality of a learner X  X  writing and the degree this learner masters collocations (Granger 1998 ; Howarth 1998b ; Nesselhauf 2005 ; Gilquin 2007 ) and identify collocation mistakes as the most recurrent mistakes detected in learners X  X  writings (Wible et al. 2003 ). The prominence of collocations can be also observed in the case of Spanish as L2. According to Alonso Ramos et al. ( 2010b ) X  X  analysis of a subcorpus of CEDEL2, about 39 % of the collocations used by advanced learners of Spanish were incorrect.
Since the early 2000s, a considerable amount of work has been carried out on the development of programs (although focused mainly on English as L2) that judge a combination to be a valid or invalid collocation and, in the latter case, attempt to provide a list of correction suggestions. But, again, to consider all collocation errors to be of the same unique class is an oversimplification which does not do justice to the complexity of the problem and thus to the needs of learners. Alonso Ramos et al. ( 2010b ) X  X  study also reveals that learners produce a considerable variety of collocation error types, each of them potentially requiring a different kind of exercise or a different type of sample material to be provided by the learning environment. Consider, for illustration, the following examples from CEDEL2: 1. gastar todo el a X o estudiando espa X ol , 2. hacer citas , lit.  X  X ake appointments X  3. escribir [ un ] examen , lit.  X  X rite [an] examen X  4. tomar puesto , lit.  X  X ake post X  5. hablar un lenguaje , lit.  X  X peak a (formal) language X  6. derechos *mujeriles  X  X omen X  X  rights X  7. ense X anza *segundaria  X  X econdary education X  8. recibir un *llamo , lit.  X  X eceive a call X  9. asistir la universidad , lit.  X  X ssist a university X  10. Yo tengo el deseo personal de ser biling X e ,
The examples show that our error coverage goes beyond what can be considered correctness of a collocation: (i) the correct choice of the collocate and base, (ii) the correctness of the collocation as a whole, (iii) the correctness of the subcatego-rization of the base, and (iv) the correct use of the collocation. In (1) X (4), we can observe the most common type of miscollocations: gastar  X  X pend X , hacer  X  X ake X , escribir  X  X rite X , and tomar  X  X ake X  are not correct as collocates of a X  o  X  X ear X , cita  X  X ppointment X , examen  X  X xam X , and puesto  X  X ost X , respectively X  X lthough they form correct collocations with other bases; compare, e.g., gastar dinero  X  X pend money X , hacer [ una ] llamada  X  X ake [a] call X , escribir [ una ] carta  X  X rite [a] letter X , tomar medidas , lit.  X  X ake measures X . In (5), the base lenguaje  X  X anguage X  is not correct for the intended usage; the correct base would have been lengua  X  X anguage X : hablar una lengua  X  X peak a language X . In (6) and (7), the learner created non-existing collocates in Spanish X  mujeriles and segundaria ; the right collocates would have been de las mujeres  X  X f the women X  and secundaria  X  X econdary X , respectively. In (8) the same occurs with the base: llamo does not exist in Spanish; the right word is here llamada  X  X all X . In (9), the collocate requires a preposition: asistir a la universidad , lit.  X  X ssist to a university X  ( attend [ a ] university ), and finally, in (10), tener un deseo  X  X ave a wish X  is, in principle, a correct collocation, however, its use is inappropriate in the given context; the appropriate phrasing would have been quiero ser biling X e  X  X I] want to be bilingual X .
Three questions arise in view of this amount and diversity of collocation errors: (i) is it possible to classify collocation errors such that each class reflects a single (ii) how are collocations and collocation errors to be annotated in a learner corpus (iii) how can we identify and correct collocation errors in the writings of learners
The first question has been addressed in (Alonso Ramos et al. 2010a ), where a fine-grained multi-dimensional collocation error typology has been presented. The dimensions of the typology capture: (1) the scope of the error (collocate, base or collocation as a whole); (2) the type of the error (lexical or grammatical) and the subtype of the error (choice of a wrong element, creation of a non-existing element, use of a correct collocation which has a different meaning from the intended one, etc. in the case of a lexical error, and error in determination, number, government, etc. in the case of a grammatical error); and (3) the source (or motivation) of the error (erroneous phonetic similarity, erroneous morphological derivation, L1 calque, etc.). 6 The second question has been addressed in (Alonso Ramos et al. 2010b ), where an annotation schema for collocations and collocation errors in learner corpora has been proposed. 7 In what follows, we address the third question. Its satisfactory intelligent CALL-driven solution naturally consists of two parts. First, to be able to identify and correct collocation errors in learner corpora, and, second, to be able to extract from reference corpora illustrative and supportive material: collocations of the type the learner seems to have difficulties with, collocations with which the learner seems to confuse the collocations she tends to make mistakes in, examples of context in which a specific collocation is used; etc. The second part is currently out of the reach for CALL. It presupposes namely that we are able not only to identify miscollocations, but also classify them automatically (for instance, in accordance with Alonso Ramos et al. ( 2010a ) X  X  typology). We focus on the first part. But before we embark on the outline of our proposal, let us briefly review the related work in this area. 3 Related work The late appearance of collocation-oriented CALL on the research map is certainly also because collocation error detection and correction presupposes, on the one hand, advanced models for collocation recognition and, on the other hand, the capacity of ranking the different miscollocation correction suggestions. Outside CALL, the identification of collocations in corpora has been actively worked on since the late eighties. Many authors explore purely statistical models (Choueka 1988 ; Church and Hanks 1989 ; Evert 2007 ; Pecina 2008 ). These models can be more or less complex, but all of them measure in one way or the other the distribution of words in combination and in isolation. Some of the works combine a statistical model with the use of syntactic features X  X or instance, submitting to the statistical model only word co-occurrences that form valid syntactic structures (Smadja 1993 ; Kilgarriff 2006 ; Evert and Kermes 2003 ). Most recent statistical proposals take into account the context of the co-occuring words X  X hich allows for the consideration of their distributional semantics (Bouma 2010 ). Another strand uses the co-occurrence range of a given word, i.e., relative frequencies of tokens that co-occur with this word most often (Wible and Tsao 2010 ). Opposed to token frequency based models is the model that uses explicit semantic features from EuroWordNet (Vossen 1998 ) to identify and semantically classify collocations (Wanner et al. 2006 ).

In CALL, most commonly, statistical models are applied to V(erb) ? N(oun) co-occurrences; see, for instance, (Chang et al. 2008 ; Park et al. 2008 ; Yin et al. 2008 ; Wu et al. 2010 ; Dahlmeier and Ng 2011 ). Since the pioneering work by Shei and Pain ( 2000 ), who still offer only precompiled correction suggestions, quite a few proposals have been made on how to improve the collocation competence of learners of English. Yin et al. ( 2008 ) acknowledge that compared to other lexical errors, collocation error detection and correction is more challenging, even when using sophisticated statistical models and large corpora, including the web. They divide a learner sentence into (collocation) chunks and individual words, which are then used as queries to a search engine. The frequency of the chunks in the web determines whether the chunks are valid in L2. If they are not, they are substituted by maximally overlapping chunks obtained when querying individual words. A precision of 37 % at 30 % recall for this strategy is reported.

Chang et al. ( 2008 ) and Dahlmeier and Ng ( 2011 ) focus on L1 interference in learners X  writings. Chang et al. ( 2008 ) first extract V-N co-occurrences from a given writing. Then, they check the extracted co-occurrences against a collocation list obtained before from a reference corpus. Co-occurrences not found in the collocation list are variegated in that their verbal elements are substituted by all English translations of their L1 (Chinese, in this case) counterpart in an electronic dictionary. The variants are again matched against the collocation list. The finally matching co-occurrences that contain the noun of a non-matching co-occurrence are offered as correction suggestions. Chang et al. ( 2008 ) report a precision of 97.5 % for the recognition of collocations and 90.7 % for the recognition of miscollocations. The MRR of the correction list is reported to reach 0.66. Dahlmeier and Ng ( 2011 ) produce confusion sets of semantically similar words. Given an input text in L2, they generate L1 paraphrases, which are then looked up in a large parallel corpus to obtain the most likely L2 co-occurrences. For this strategy, they report a precision of 38 %.
Futagi et al. ( 2008 ) target the detection of miscollocations in learner writings, leaving occurrences. But similar to Chang et al. ( 2008 ), they extract the co-occurrences from a learner writing, variegate the m and then look up the original co-occurrence and its variants in a reference list to decide on its status. To obtain the variants, they apply spell checking, variate articles and inflections and use Wo rdNet to retrieve synonyms of the collocate.
Wu et al. ( 2010 ) go somewhat further in that they work with  X  X ubject X  ?  X  X erb X  and  X  X erb X  ?  X  X bject X  tuples instead of PoS co-occurrences. The tuples are extracted from a reference corpus (RC) using a dependency parser (Klein and Manning 2003 ) and filtered to get rid of free rare co-occurrences. A Maximal Entropy (ME) classifier is trained on the lexical context of each collocation in the RC list. For the correction of a miscollocation, the classifier provides a number of collocate corrections using the learner sentence as lexical context. The probability predicted by the classifier for each suggestion is used to rank the suggestions. According to the evaluation included in (Wu et al. 2010 ), an MRR of 0.518 for the first five correction suggestions has been achieved. 8
Liu et al. ( 2009 ) X  X  goal is to develop a model for automatic suggestion of corrections for given miscollocations. To retrieve the suggestions from a reference corpus, they use three metrics: (i) mutual information (Church and Hanks 1989 ), (ii) semantic similarity of an incorrect collocate to other potential collocates based on their distance in WordNet, and (iii) the membership of the incorrect collocate with a potential correct collocate in the same  X  X  X ollocation cluster X  X . 9 A combination of 55.95 %. A combination of (i) ? (ii) ? (iii) leads to the best precision of 85.71 % when a list of 5 possible corrections is returned. 4 Towards advanced collocation error correction As pointed out in Sect. 1 , we focus on collocation error correction in Spanish learner essays. To judge whether a V ? N, V ? Adv, Adj ? N or Adj ? Adv co-occurrence C in a learner essay is a valid collocation or not, we use a simple frequency metric.
As far as miscollocation correction is concerned, the previous works show that a number of criteria call to be taken into account:  X  the learner may misspell a collocate such that the exploration of graphically  X  the learner often produces collocation mistakes by collocate calques from L1  X  as learned from corpus-based collocation detection research, the association  X  the context of a miscollocation in a learner essay is essential when searching for We attempt to capture these criteria in our miscollocation correction metrics.
Our reference corpus of Spanish consists of lists of PoS-tagged n -grams (2 B n C 5) extracted from about 1850 million words of newspaper material (in total, 70 volumes of two major Spanish newspapers) or from a smaller sample of it. Further auxiliary resources of which we make use are: the Open Office thesaurus of Spanish, the Spanish EuroWordNet, an automatically compiled bilingual Spanish-English vocabulary, and the web (as additional reference resource). These auxiliary resources shall help us to take into account the phenomenon of synonymy (the learners may choose a term which is (quasi-) synonymous to the correct element) and 1: n translation equivalence (the learner may choose a wrong (literal) translation of the collocation element in L1), and ensure that we have the widest evidence of collocation use in L2.

Given a V ? N, V ? Adv, Adj ? N or Adj ? Adv combination C extracted from the writing of the learner, our basic algorithm for the collocation error detection and correction is as outlined in Fig. 1 .

In what follows, we explore three different metrics for judging which of the correction candidates is the best correction of the supposedly erroneous C (cf. line 9 of the algorithm). 4.1 Affinity metric The affinity metric is a local metric which takes into account: 1. the co-occurrence (or association ) strength ( as ) of the collocate candidate c can with the base B of the miscollocation in the RC; 2. the graphic similarity gs of c can with the original collocate Co , and 3. the synonymy syn of c can with Co . The affinity Af is calculated as
The association strength as is a standard parameter in collocation identification procedures; see also (Liu et al. 2009 ). It is measured, for instance, in terms of pointwise mutual information , log-likelihood , etc. In the experiment in Sect. 5 ,we used log-likelihood (which, somewhat simplified, assesses how likely it is that c can and B form an idiosyncratic co-occurrence):
The graphic similarity gs shall capture cases where the learners mistyped a collocate or erroneously chose a collocate due to its graphic similarity to the intended one (as, e.g., rise instead of raise in English); 10 see (Futagi et al. 2008 ) for a similar idea.

We calculate graphic similarity as the Dice coefficient, using character bigrams as features (such that it provides the relative overlap of the character bigrams in Co (with Co Bi as the set of character bigrams of Co and c can Bi as the set of character bigrams of c can ). For instance, in the case of the misspelled collocate in * cojer (instead of coger  X  X atch X ) in * cojer un catarro  X  X atch a cold X , Co Bi will be common bigrams, while each of them is composed of four different bigrams.

The synonymy factor syn is  X 1 X  if c can is among the synonyms of Co in the synonym list obtained from the auxiliary resources and  X 0 X  otherwise. 4.2 Lexical context metric In contrast to the affinity metric, the lexical context metric takes into account the context in which the miscollocation occurs X  X s already suggested by Wu et al. ( 2010 ). However, unlike Wu et al., who distill the contextual features from the contexts of the correct equivalent of a miscollocation in the reference corpus (RC), we think that it is the essay, which should provide the contextual features that are then to be matched with the contexts of the candidate correction searched for in the RC: a learner may use a (mis)collocation in contexts that deviate from the most common contexts in the RC; should this occur, a machine learning mechanism trained on the most common contexts of a correction will be inaccurate when confronted with features extracted from the learner X  X  context (in the extreme case, the mechanism will have discarded all of the features in the learner X  X  context or assigned to them very small weights when being trained). On the contrary, starting from the contextual features of a miscollocation, it suffices to collect sufficient evidence that a correction candidate is used in the context provided by the learner. 11
In other words, the lexical context metric is grounded in the assumption of distributional semantics, namely that the semantics of a collocation can be approximately deduced from the sentential context in which this collocation appears. Consider, for illustration, the following sentences (taken from the web) in which one of the words has been removed: 11. She * a conference on the situation of women rights ... 12. Mr. White responded to the changing industry and * a conference of critical 13. Eventcorp * a conference that met the Conference Committee X  X  criteria
The reader can deduce with a certain probability that the missing word is [ to ] deliver or any other support verb that goes with conference and that is synonymous in this context to deliver . We argue that this is due to the fact that the reader uses the distributional semantics of the context of conference that allows her to come up with [ to ] deliver . In contrast, in 14. The mailman * apples, bananas, and coconunts . 15. Oo baby, here I am, signed, sealed, *, I X  X  yours, oh I X  X  yours ... [Stevie 16. Flowers * by hand on your behalf by our expert florists . this is not the case: we cannot reliably guess the missing verb. This gives us a hint that in (14 X 16) the missing verb does not participate in a collocation.

We can thus hypothesize that the context can be useful for the detection of collocations, or, in our case, for the search for the most adequ ate correction suggestion. More precisely, c , c i ? 1 , ... , c n in the RC is the most adequate correction of Co  X  X ith  X  X  X ffinity X  X  meaning here the highest co-occ urrence frequency: (with  X  c j -c can  X  as the occurrence of c j with c can at a distance of maximally k tokens).

In our experiments, we used so far n B 8 with k = 2 , always within the borders of a single sentence; duplicates are eliminated. 12 For instance, in the learner sentence (17): 17. Sp. Afortunadamente, su profesora estuvo dispuesta a venderlas y pudo the context tokens during the search for the optimal correction suggestion for the miscollocation * extender [ una ] colecci X n  X  X xtend a collection X  would be m X scara  X  X ask X , para  X  X or X  and nuestro  X  X ur X . In further experiments (see Sect. 6 ), we plan to use the entire sentential context X  X lthough some processing time restrictions may arise for long sentences. 4.3 Context feature metric The context feature metric is similar to the lexical context metric in that it draws upon the context of the miscollocation in the original learner sentence. However, there are also two significant differences. First, it may take into account not only lexical tokens (although this is what we tested it with so far; see below), but any kind of contextual features (PoS tags, grammatical functions, punctuation, etc.). Second, its interpretation of these features is very different: Given the contextual features c f candidates C can , the idea is to assess whether the majority of the contextual features c of Co speaks for the preference of one of the candidates c can [ C can . For this purpose, we search, for each feature c f ,a c can that shows towards it the highest affinity: where N ( c cani , B ) stands for the number of times the combination ( c cani , B ) occurs in the corpus, and N ( c f ,( c cani , B )) for the number of times the feature c and the combination ( c cani , B ) co-occur in the corpus at a certain distance from each other.
What Eq. ( 5 ) does is to assess how common the feature c f is when a given candidate correction c cani co-occurs with the base B and how idiosyncratic the co-occurrence of c cani with B is. For instance, in the learner sentence (18), the collocation * sacarse [ una ] operaci X n , lit.  X  X ake off an operation X  is not correct: 18. Es f X cil, s X lo hay que sacarse una operaci X n como Michael Jackson
To find the right correction, the affinity between the candidate collocations of operaci X n  X  X peration X  and each of the contextual features in the reference corpus is examined. In the experiments carried out so far, we used lexical tokens as contextual features (similar to the lexical context metric). The contextual features in (18) would thus be s X lo, hay, que, como, Michael and Jackson . 13 For each of these features, its preference for concluir  X  X onclude X , detener  X  X top X , drogar  X  X rug X , herir  X  X arm X , informar  X  X nform X , producir  X  X roduce X , recibir  X  X eceive X , rescatar  X  X escue X  or ser  X  X e X  (the correction candidates provided by our algorithm) is assessed according to Eq. ( 5 ). 5 Experiments In order to test the performance of our collocation recognition algorithm and the quality of each of the miscollocation correction metrics, we ran the procedure in Fig. 1 on a number of sentences from CEDEL2, exploring also the influence of the nature and size of the reference corpus on the task. 5.1 Set up of the experiments 5.1.1 Testing reference corpora As mentioned above, our strategy for collocation error identification is purely frequency based in that it draws upon the frequency of a given co-occurrence in a reference corpus to decide whether this co-occurrence is a correct collocation or not. However, the frequency of a co-occurrence depends on the nature and on the composition (and thus indirectly also on the size) of the reference corpus. For instance, in the web, the high number of non-native authors of English (and thus the potential use of co-occurrences not accepted by native speakers) may distort the relative frequencies of collocations. In the case of Spanish, we also need to bear in mind that the frequencies may significantly vary across the different Spanish speaking countries.

The size of the reference corpus also matters since a very large corpus bears the risk of containing too much noise and a small corpus may not contain a sufficient number and diversity of co-occurrences.

To assess, at least partially, these restrictions, we evaluated the collocation error identification task against three different corpora: the web (consulted using the Yahoo! Boss Search Appi), a large corpus of 5GB of newspaper material of peninsular Spanish (about 1850 million words) and a smaller sample of approx-imately 2GB (about 740 million words) from the same corpus. 5.1.2 Testing error correction metrics In a different experiment, we assessed the quality of the error correction metrics, letting them compete with each other. As reference corpus, the 1850 million word corpus was used. In total, 61 runs on V ? N combinations were carried out. In each run, the algorithm in Fig. 1 was applied to a given V ? N combination and a sentence from the learner corpus in which this combination occured. For instance, one of the combinations was hacer citas , lit.  X  X ake appointments X  in the learner sentence: 19. En mi nueva posici X n, yo hice planes de viajar para los grupos, acud X  el
Due to the low (more precisely, zero) frequency of hacer citas in the reference corpus (0 &lt; T = 7), it is judged to be a collocation error. The condition in Step 5 of the algorithm in Fig. 1 returns  X  X alse X  such that Steps 8 and 9 are carried out. Step 8 returns the following list of candidates: 14 realizar [ una ] cita  X  X ealize an appointment X , producir [ una ] cita  X  X roduce [an] appointment X , dar [ una ] cita  X  X ive [an] appointment X , tener [ una ] cita  X  X ave [an] appointment X , ir [ a una ] cita  X  X o [to an] appointment X , acudir [ a una ] cita  X  X urn [to an] appointment X , declarar [ una ] cita  X  X eclare [an] appointment X , haber [ una ] cita  X  X eceive [an] appointment X , concertar [ una ] cita  X  X rrange [an] appointment X , ser [ una ] cita  X  X e [an] appointment X , agenciar [ una ] cita  X  X ediate [an] appointment X .

Step 9 applies the individual metrics to select the best correction. The affinity metric suggests realizar [ una ] cita  X  X ealize an appointment X , while the lexical and the context feature metrics suggest concertar [ una ] cita  X  X rrange [an] appoint-ment X  X  X hich is, in fact, the most appropriate correction of hacer citas .

Table 1 displays a number of further examples of correction suggestions of the three metrics. In the 10 displayed trials, the affinity metric failed 5 times, the context feature metric 4 times, and the lexical context metric 2 times. Note that some wrong correction suggestions may be valid collocations in Spanish. For example, dar [ una ] oportunidad  X  X to] give [an] opportunity X  is a correct collocation in Spanish, but with different semantics than the one required taking into account the sentence of the learner in (20).
 20. Como he dicho, me encantaba el espa X ol, y yo utilizo cada oportunidad que
Another example is imponer [ una ] regla  X  X to] impose [a] rule X , which is a correct collocation, but which, again, does not reflect the intention of the learner in their writing: Los sistemas religiosos no quieren que los sistemas gubernamentales interrumpen [sic] sus reglas y leyes ., lit.  X  X he religious systems do not want that the governmental systems interrupt their rules and laws. X 
In some cases, more than one collocation candidate can be considered as the correct suggestion, Consider, for instance, the incorrect collocation concluir [ un ] problema , lit.  X  X onclude [a] problem X , in the learner X  X  sentence (21): 21. Quiz X s ser X  la ciencia que descubra en el futuro algo que ayudara a concluir where all three metrics suggest a different but adequate correction: resolver [ un problema  X  X esolve [a] problem X , solucionar [ un ] problema  X  X olve [a] problem X , and acabar [ con un ] problema  X  X erminate [with a] problem X . 5.2 Results and evaluation of the experiments Although we did not make an effort to optimize our collocation (error) identification strategy, we take its output as input for our miscollocation correction procedure. Therefore, an evaluation of it is necessary. 5.2.1 Evaluation of the identification of miscollocations Figure 2 displays the percentage of correctly identified collocations and miscollo-cations using the different reference corpora.

According to these figures, the 1850 million word corpus is more suitable for the detection of correct collocations (with a recognition ratio of about 98 %) than the 740 million word corpus or the web. This is likely, on the one side, due to the small size or imbalance of the 740 million word corpus and, on the other side, due to the diversity and unreliability of the data sources in the web and thus due to its uncontrolled nature. However, with a recognition ratio of about 84 %, the web is considerably better than the 740 million word corpus. A clear advantage of using the web as corpus is that even if it is not balanced, it contains a wide range of text topics different registers.

With a recognition ratio of about 98 %, the web turned out to be more adequate as reference corpus for the identification of miscollocations than the other two corpora. This is somewhat surprising since one would expect that its uncontrolled nature mentioned above would introduce considerable noise (or, in other words, traces of the use of collocations judged by native speakers as incorrect). The 1850 million word corpus is with a recognition ratio of about 73 % considerably less reliable, and so is the 740 million word corpus (with 49 %). Further investigation is needed to clarify this outcome.

Overall, with the web as reference corpus we are able to judge whether a combination is a correct or incorrect collocation in Spanish with an accuracy of about 91 %. From 61 samples, the algorithm fails in six cases. In five of these six cases, correct collocations have been judged to be incorrect. This is mainly due to our frequency based criteria. For instance, apretar [ los ] dientes , lit.  X  X ress [the] teeths together X , contar cuentos , lit.  X  X ell stories X , dar [ la ] bienvenida , lit.  X  X ive [the] welcome X , and preparar [ la ] comida , lit.  X  X repare [the] food X , are correct collocations in Spanish, but their frequencies in our reference corpus are too low to consider them valid. 15 On the other hand, pasar [ la ] navidad , lit.  X  X ass [the] Christmas X  is judged by the program to be a correct collocation due to its high frequency in the corpus, although it is questionable in peninsular Spanish. 16
The figures also show that the 740 million words corpus is not adequate to serve as reference corpus for collocation (error) recognition: it simply does not contain enough evidence on the use of the individual co-occurrences. 5.2.2 Evaluation of collocation correction suggestions For the second stage, i.e., the error correction stage, we performed so far two evaluations. Fist, we evaluated the probability that the list of correction suggestions retrieved by our algorithm (Step 8 in Fig. 1 ) contains the adequate correction of the given miscollocation. This probability amounts to 0.73 if we consider the first 15 suggestions (ranked by frequency): in 73 % of the trials, the right correction is encountered in the list of the first 15 suggestions offered by our program. Working with a list of 10 suggestions, we achieve an accuracy of 0.66 and with a list of 5 suggestions, an accuracy of 0.6.

In order to assess the quality of the frequency-based ranking of the correction suggestions, we calculated the ratio of correct miscollocation corrections versus the size of the correction suggestion list for 30 miscollocations. The results of this test are illustrated in Fig. 3 . For instance, for 18 instances, the adequate correction is in the top five of the list; for 20 instances, the correction is in the top ten, and so on. This means that the provided correction suggestion list is reasonbly good (although not perfect), and we can choose a single correction from it using our metrics.
Second, we evaluated the capacity of the different metrics to pick the right correction from the correction suggestion list. The affinity metric achieved an accuracy of 17.24 %, the lexical context metric 27.58 % and the context feature metric, with features being simply words in the original sentence of the learner, 54.2 %. 17 The mean reciprocal rank (MRR) of the top five suggestions obtained using the contextual feature metric is 0.72.

The poor performance of the affinity metric needs further examination. It is possible that the metric itself is not adequate. To check this, a reimplementation of partially due to the deficiency of the auxiliary resources we used. Thus, Spanish WordNet is less complete than its English counterpart. Furthermore, it was derived semi-automatically from the English WordNet, with the consequence that it is also considerably noisier.

The equally poor performance of the lexical context metric is likely due to its simplicity. The context feature metric, which is also based on the notion of context, proved to be more adequate. Its performance challenges state-of-the art proposals such as (Chang et al. 2008 ) (with an MRR of 0.66) and (Wu et al. 2010 ) (with the highest MRR of 0.518). However, even 54.2 % of accuracy and an MRR of 0.72 are certainly still too low for practical CALL. On the other side, it is to be pointed out that the potential of the contextual features has not been fully explored as yet: the use of concrete words is too restrictive. The experience from statistical NLP (e.g., parsing and generation) teaches us that combinations of morpho-syntactic catego-ries, grammatical functions and words are more promising. We will carry out experiments in this respect in the near future. 6 Conclusions and future work We presented a proposal for automatic recognition and correction of miscolloca-tions in written material of learners of Spanish. Unlike many other proposals in the field, our goal was to explore how well we can offer adequate corrections of miscollocations rather than ranked lists of possible corrections. Although we agree that a ranked list of possible corrections may suffice for advanced learners, it does not serve well the needs of beginners or intermediate level learners X  X specially if the suggestions show subtle semantic differences. Furthermore, unlike, e.g., (Wu et al. 2010 ), the proposal that is most similar to ours in that it also takes the (mis)collocation context into account X  X ur experiments have been carried out with real learner essays because it is only when working with real material that we can obtain evidence that the proposed approach may serve as the basis for advanced collocation error correction techniques.

The results we obtained with our current initial implementation compete with state of the art proposals, although they are still far from being fully satisfactory. Thus, a collocation/miscollocation recognition rate of 90 % means that the judgement of each tenth co-occurrence is wrong, and a correction rate of 54.2 % means that for nearly each second miscollocation not the best correction is suggested.
 In order to have some evidence that our error correction proposal is not limited to Spanish, we carried out a small experiment on English learner material in that we: (i) took ten English collocation errors made by Chinese learners of English in their 1. The interaction between us really made me grown up mind a lot and helped 2. These merits she owns make me to take more respect and honour to her ... 3. I should not break her privacy . 4. When you said lies , you may never let others to believe what you say . 5. My hand flowed much blood and I was very sad .
 6. Karen is a charming girl who always keeps a smile on her face . 7. ... my dad told me if I could have a great grade then he would buy it for me . 8. We often changed our secrets with each other . 9. I learned a lot of knowledge from him , ... 10. ... I decided to change myself and to do something breakthrough : (ii) introduced the ten collocation errors one by one into the MUST collocation (iii) applied to the lists of MUST X  X  correction suggestions (interpreted as The MRR of the MUST ranked suggestions in (ii) has been 0.4, while the MRR of the suggestions ranked by our lexical context metric in (iii) reached 0.53, i.e., the present proposal could be used to improve the ranking of the MUST checker. For 3 of the 10 miscollocations, the lexical context metric ranked first the right correction; for 4 the right correction was ranked second, for 1 third, for 1 fifth and for 1 the right correction was not among the first five. It is to be expected that the contextual feature metric, which we did not use in the experiment due to a considerably higher set up cost, would have performed even better (see Sect. 5.2.2 ). That is, we can assume that our collocation error correction metrics are indeed language-independent.

Despite this encouraging outcome, there is still a great potential to improve the performance of both miscollocation identification and correction X  X hich we target in our future work. To further improve (mis)collocation recognition, we plan to go beyond the assumption of context independence of co-occurrences, as has already been suggested by Bouma ( 2010 ). To improve on the correction strategies, we plan to significantly broaden the types of features taken into account for the feature context metric. A further experiment will be dedicated to the exploration of a voting strategy between the different metrics: as illustrated by Table 1 , already a simple majority vote would introduce somewhat more stability into the results.

In parallel, we explore the use of the collocation (error type) annotated fragment of CEDEL2 (Alonso Ramos et al. 2010b ) as training material for machine learning based recognition of miscollocations in learner corpora and the use of the Google n -gram list as reference corpus.
 References
