 Multi-task feature selection (MTFS) is an important tool to learn the explanatory features across multiple related tasks. Previous MTFS methods fulfill this task in batch-mode training. This makes them inefficient when data come in sequence or when the number of training data is so large that they cannot be loaded into the memory simultaneously. To tackle these problems, we propose the first online learning framework for MTFS. A main advantage of the online algo-rithms is the efficiency in both time complexity and mem-ory cost due to the closed-form solutions in updating the model weights at each iteration. Experimental results on a real-world dataset attest to the merits of the proposed algo-rithms.
 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithms Supervised learning, Online learning, Multi-task learning, Feature selection, Dual averaging method
Learning multiple related tasks simultaneously by exploit-ing shared information across tasks has demonstrated advan-tages over those models learned within individual tasks [2, 4, 7, 14]. A key problem of multi-task learning is to find the ex-planatory features from these multiple related tasks. Some existing methods impose the shared features by a general-ized  X  1 -norm regularization [1], or more specifically, a joint regularization on the  X   X , 1 -norm of the learning weights [11], where  X  can be set to 1, 2, or  X  . These methods then se-lect the subspace while seeking the weights of the decision functions by minimizing a convex optimization problem on the sum of the joint regularization and the loss [11].
Although previous multi-task feature selection (MTFS) methods succeed in several aspects, they still have some drawbacks. First, these methods are conducted in batch-mode training. The training procedure cannot be started until the data are prepared. A fatal drawback is that these methods cannot effectively solve the applications when the training data appear in sequence. The second drawback is that these algorithms suffer from inefficiency when the size of training dataset is huge, especially when the data cannot be loaded into memory simultaneously. In this case, one may have to conduct additional procedure, e.g., subsampling, to choose the data for training. This may degrade the model performance since the given data are not sufficiently utilized. The third drawback is that most previous MTFS methods can only select features in individual tasks or across all tasks, but cannot find the important tasks further from the impor-tant features. This reduces the ability of the MTFS methods in further mining more important information.
 To tackle the above problems, we first develop a novel MTFS model, named multi-task feature and task selection (MTFTS), which selects important features across all tasks and important tasks that dominate the selected features. More importantly, we further propose the first online learn-ing framework to transform the batch-mode MTFS models into their online ones. We derive closed-form solutions for the MTFS models to update their weights at each itera-tion. This guarantees the efficiency of the algorithms in both time complexity and memory cost, whose worst cases are  X  (  X   X   X  ), where  X  is the number of features and  X  is the number of tasks. Finally, we conduct detailed experiments on a real-world dataset to demonstrate the characteristics and merits of the proposed online learning algorithms.
Suppose there are  X  tasks and all data of the tasks come from the same space  X   X  X  . For simplicity, we assume that  X   X   X   X  and  X   X   X  . For each task, we have  X   X  data points. This constitutes a dataset of  X  = S  X   X  =1  X   X  , where  X  { z  X  = ( x  X   X  X  . Here,  X   X  is usually assumed different for each task but all  X   X   X  X  are related, e.g., as discussed in [4]. The goal of multi-task learning (MTL) is to learn  X  functions  X   X  :  X   X  ,  X  = 1 ,..., X  such that  X   X  ( x  X   X  ) approximates  X   X   X   X  = 1, it is the standard (single task) learning problem.
Typically, in multi-task learning models, the decision func-tion  X   X  for the  X  -th task is assumed as a hyperplane param-eterized by the model weight vector w  X  [1, 11], i.e., To make the notation consistent, we express the weight ma-trix in rows and columns as W = w 1 ,..., w  X  = ( W  X  1 ,..., W  X   X  ) = W  X  1  X  ,..., W The objective of multi-task feature selection models is to seek the weight matrix W by minimizing an empirical risk with a regularization on the weights: where  X   X  0 is a constant to balance the loss and the regu-larization term.  X   X  ( W  X   X  , z  X   X  ) defines the loss on the sample z  X  for the  X  -th task. Various loss functions, e.g., squared loss, logit loss, hinge loss, etc., can be adopted and they are usually assumed convex.

In (3),  X   X  ( W ) defines the regularization on the weights of tasks. Various mixed norms on the weight vectors have been proposed in the literature to impose sparse solutions so as to select the important features [1, 11]. They include  X  1 , 1 -norm Regularization: An individual multi-task fea- X  2 , 1 -norm Regularization: This model penalizes the  X  1 -Remarks : Although the above introduced MTFS models find the decision functions in linear form, it is noted that by projecting the data points on a random direction in the Reproducing Kernel Hilbert Space (RKHS), one can attain non-linear solutions on the original space [11].
The aMTFS model usually selects important features across all tasks; however, it yields non-sparse solutions for the se-lected features [11]. Hence, in order to find out important explanatory features across all tasks and to find out the im-portant tasks dominating the selected features simultane-ously, we propose the multi-task feature selection method on both feature and task selection (MTFTS) by introducing a new  X  1 / 2 , 1 -norm regularization as follows: where  X   X   X  0, for  X  = 1 ,..., X  , is a constant balancing the  X  -norm and the  X  2 -norm of the weights on the  X  -th feature across all tasks. By imposing  X  1 -norm on the weight of each feature, we can further find out the important tasks from the selected features.
To tackle the insufficiency of batch-mode training algo-rithms and motivated by the recent success of online learn-ing algorithms for solving the  X  1 -regularization problem [3, 6, 8, 13], we propose an online learning framework as follows: Algorithm 1 Online learning framework for multi-task fea-ture selection
Input :
Initialization : W 1 = W 0 ,  X  G 0 = 0 . for  X  = 1 , 2 , 3 ,... do end for
Remarks : Algorithm 1 is inspired by the recently de-veloped first-order methods for optimizing convex compos-ite functions in [10] and the efficiency of the dual averaging method for minimizing the  X  1 -regularization in [13, 15]. It is noted that in the above framework, we assume at each iter-observed as that in [5]. In addition, the regret, the difference of the objective value up to the  X  -th step and the smallest objective value from hindsight, is guaranteed in  X  ( The following theorem indicates the weights of the online MTFS algorithms can be efficiently updated in closed-form solutions: Theorem 1. Given the average subgradient  X  G  X  , and  X  ( W ) =  X  W  X  2  X  , at each iteration, the optimal solution of the cor-responding MTFS models can be updated by
Remarks : The proof of Theorem 1 is lengthy and omit-ted here. Equation (9) implies that the online learning al-gorithm for the iMTFS can yield sparse solutions in the ele-ment level, but it does not utilize any information across all tasks. Equation (10) indicates that the online learning al-gorithm for the aMTFS can select those important features in a grouped manner and it will discard irrelevant features for all tasks. Equation (11) implies that the online learn-ing algorithm for the MTFTS can select important features and important tasks dominating the selected features. Since  X  (  X  features than the aMTFS under the same regularization pa-rameter.
In the following, we conduct detailed experiments to demon-strate the characteristics and merits of the online learning algorithms on the MTFS problem. Five algorithms are com-pared: the batch-mode learning algorithms for the iMTFS and the aMTFS; the online learning algorithms by the dual averaging method for the iMTFS (DA-iMTFS) updated by Eq. (9), for the aMTFS (DA-aMTFS) updated by Eq. (10), and for the MTFTS (DA-MTFTS) updated by Eq. (11), re-spectively. All algorithms are run in Matlab on a PC with 2.13 GHz dual-core CPU.
We choose the school dataset 1 , which has been previously tested on the batch-mode trained multi-task learning [2, 7] and multi-task feature learning [1, 9], in the evaluation. This dataset consists of the exam scores of 15,362 students from 139 secondary schools in London during the years 1985, 1986, and 1987. The goal is to predict the exam scores of the students based on 27 features. More details about the features and data can be referred to [1, 7]. Hence, we obtain 139 tasks (  X  = 139) and  X  = 27.

Following the same evaluation criterion in [1, 2, 7], we employ the explained variance, one minus the mean squared test error over the total variance of the data (computed within each task), and the percentage of variance explained by the prediction model. It corresponds to a percentage ver-sion of the standard  X  2 error measure for regression on the test data [2]. http://ttic.uchicago.edu/  X argyriou/code/mtl_feat/ school_splits.tar Table 1: Best explained variance and the corre-sponding NNZs for different methods.

Since the task is a regression problem to predict the exam scores of the students, we use the squared loss in the algo-rithms. In the training, we randomly generate 20 sets of training data and apply the rest data as the test data. The number of training data is set to be the same, i.e., half of the minimum number of data among all individual tasks, which meets the requirement of Algorithm 1 that there is an instance in a task at each iteration.
Table 1 reports the best performance and the correspond-ing parameters obtained by the five algorithms. For the batch-mode algorithms, the best results are obtained by tuning the parameters  X  in a hierarchical scheme, from a large searching step in the whole parameter space to a small searching step in a small region. As a reference, the largest  X  making all the weights of the aMTFS vanish is about 1,000 and is about 100 for the iMTFS, respectively. For the on-line algorithms, the parameters are tuned by the grid search scheme in a hierarchical way. The number of epoches is varied to attain better performance. Here, multiple epoches mean that cycling through all the training examples multiple times with a different random permutation for each epoch. The sparse parameter in the DA-MTFTS is set to 0.01 at each task for simplicity in all the experiments.

There are several observations from Table 1. First, the re-sults of the aMTFS vs. the iMTFS and the DA-aMTFS/DA-MTFTS vs. the DA-iMTFS clearly show that learning mul-tiple tasks simultaneously can gain over 50% improvement than learning the task individually. Second, the DA-aMTFS and the DA-MTFTS attain the same explained variance, which is nearly the same as that obtained by the aMTFS. Both the number of non-zeros (NNZs) in weights obtained by the DA-aMTFS and the DA-MTFTS is less than that obtained by the aMTFS. More specially, the NNZs of the DA-aMTFS is about 25% less than that of the aMTFS. The DA-MTFTS gets fewer NNZs than the DA-aMTFS, about 20% decrease in the number. This indicates that the learned DA-aMTFS and the DA-MTFTS are easier to be interpreted. Third, the DA-iMTFS obtains the same per-formance as that of the iMTFS and selects more NNZs than the iMTFS.

Figure 1 further shows the trade-off between the regular-izer parameter  X  and the algorithm parameter  X  . The test first fixes one parameter to their best ones and varies the other. The best results of the batch-mode trained models are also shown for reference. From the results, we know that the number of non-zero elements (NNZs) decreases as  X  in-creases for all three online algorithms. The best results are obtained when  X  = 1 for the DA-iMTFS and when  X  = 20 for both the DA-aMTFS and the DA-MTFTS. By varying  X  , it is shown that NNZs increases as  X  increases. The best ones are obtained when  X  = 50 for the DA-iMTFS and when  X  = 1 for the DA-aMTFS and the DA-MTFTS. The results indicate that usually for a given dataset, the best  X  and  X  have to be tuned based on the given data.

For the running time of the algorithms, it is usually very difficult to carry out a fair comparison among different al-gorithms, due to the implementation issue, the choice of al-gorithm parameters, and different stopping criteria. A the-oretical analysis of the convergence rate of the algorithms has been conducted in our research and can be referred to the corresponding papers in [1, 9, 11]. As a reference, the running time of the DA-MTFTS, the slowest MTFS on-line algorithms in this paper, costs 1.15 second when the number of epoches is 120. The running time of the batch-mode aMTFS [1] with the setting of  X  X psilon init=0 X ,  X  X t-erations=50 X , and  X  X ethod=3 X  is 1.30 second. The online algorithms reduce about 15% running time compared to the batch-mode aMTFS algorithm.
In this paper, we propose the first online learning frame-work to solve the multi-task feature selection models, which also includes our developed novel MTFTS model to seek both important features and important tasks dominating the selected features. We derive closed-form solutions to update the weights of the MTFS models, which guarantees the online learning algorithms work very efficiently in both time and memory cost. Our detailed empirical study on a real-world dataset demonstrates the merits of the proposed online MTFS algorithms in various aspects.

Some future work are worth considering. First, it is in-teresting to extend the current linear MTFS methods to non-linear forms by the projection method to improve the model performance. Second, our proposed online algorithms require that at each iteration, one and only one instance is from each task. It is interesting to know how to balance the weight update when the instances of some tasks are miss-ing. Third, the proposed online algorithm framework as-sumes the training samples are independent and identically-distributed. It is attractive to consider the case where the i.i.d. assumption does not hold in practice. This work is supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK4128/08E and Project No. 4154/09E) and a grant from the Guangdong Province International Collaboration Scheme (Project No. 2009B050700044). [1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [2] B. Bakker and T. Heskes. Task clustering and gating [3] S. Balakrishnan and D. Madigan. Algorithms for [4] S. Ben-David and R. Schuller. Exploiting task [5] O. Dekel, P. M. Long, and Y. Singer. Online multitask [6] J. Duchi and Y. Singer. Efficient learning using [7] T. Evgeniou and M. Pontil. Regularized multi X  X ask [8] J. Langford, L. Li, and T. Zhang. Sparse online [9] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [10] Y. Nesterov. Primal-dual subgradient methods for [11] G. Obozinski, B. Taskar, and M. I. Jordan. Joint [12] R. Tibshirani. Regression shrinkage and selection via [13] L. Xiao. Dual averaging method for regularized [14] H. Yang, I. King, and M. R. Lyu. Multi-task learning [15] H. Yang, Z. Xu, I. King, and M. R. Lyu. Online
