 Large repositories of private or public software source code, such as the open source projects avail-able on the Internet, create considerable new opportunities and challenges for statistical machine learning, information retrieval, and software engineering. Mining such repositories is important, for instance, to understand software structure, function, complexity, and evolution, as well as to improve software information retrieval systems and identify relationships between humans and the software they produce. Tools to mine source code for functionality, structural organization, team structure, and developer contributions are also of interest to private industry, where these tools can be applied to such problems as in-house code reuse and project staffing. While some progress has been made in the application of statistics and machine learning techniques to mine software corpora, empirical studies have typically been limited to small collections of projects, often on the order of one hundred projects or less, several orders of magnitude smaller than publicly available repositories(eg. [1]). Mining large software repositories requires leveraging both the textual and structural aspects of soft-ware data, as well as any relevant meta data. Here we develop Sourcerer, a large-scale infrastructure to explore such aspects. We first identify a number of robust power-law behaviors by simple statisti-cal analyses. We then develop and apply unsupervised author-topic probabilistic models to discover the topics embedded in the code and extract topic-word and author-topic distributions. Finally, we leverage the dual textual and graphical nature of software to improve code search and retrieval. To allow for the Internet-scale analysis of source code we have built Sourcerer, an extensive infras-tructure designed for the automated crawling, downloading, parsing, organization, and storage of large software repositories in a relational database. A highly configurable crawler allows us to spec-ify the number and types of projects desired, as well as the host databases that should be targeted, and to proceed with incremental updates in an automated fashion. Once target projects are down-loaded, a depackaging module uncompresses archive files while saving useful metadata (project name, version, etc). While the infrastructure is general, we apply it here to a sample of projects in Java. Specifically, for the results reported, we download 12,151 projects from Sourceforge and Apache and filter out distributions packaged without source code (binaries only). The end result is a repository consisting of 4,632 projects, containing 244,342 source files, with 38.7 million lines of code, written by 9,250 developers. For the software author-topic modeling approach we also employ the Eclipse 3.0 source code as a baseline. Though only a single project, Eclipse is a large, active open source effort that has been widely studied. In this case, we consider 2,119 source files, associated with about 700,000 lines of code, a vocabulary of 15,391 words, and 59 programmers. Methods for extracting and assigning words and programmers to documents are described in the next sections. A complete list of all the projects contained in our repository is available from the supplementary materials web pages. During the parsing process our system performs a static analysis on project source code files to extract code entities and their relationships, storing them in a relational database. For java these en-tities consist of packages, classes, interfaces, methods, and fields, as well as more specific constructs such as constructors and static initializers. Relations capture method calls, inheritance, and encap-sulation, to name a few. The populated database represents a substantial foundation on which to base statistical analysis of source code. Parsing the multi-project repository described above yields a repository of over 5 million entities organized into 48 thousand packages, 560 thousand classes, and 3.2 million methods, participating in over 23.4 million relations. By leveraging the query capa-bilities of the underlying database we can investigate other interesting statistics. For example, table 1 contains the frequencies of Java keywords across all 4,632 projects. Upon examining this data we can see that the  X  X efault X  keyword occurs about 6 percent less frequently than the  X  X witch X  keyword, despite the fact that best practice typically mandates all switch statements contain a default block. Moreover, the  X  X or X  loop is about twice as pervasive as the  X  X hile X  loop, suggesting that the bound on the number of iterations is more likely to be known or based on the size of a known data structure. Finally, statistical analyses of distributions also identify several power-law distributions. We have observed power-law distributions governing package, SLOC, and inside relation (lexical contain-ment) counts. For instance, Figure 1 shows the log-log plots for the number of packages across projects. Similar graphs for other distributions are available from the supplemental materials page. Automated topic and author-topic modeling have been successfully used in text mining and infor-mation retrieval where they have been applied, for instance, to the problem of summarizing large text corpora. Recent techniques include Latent Dirichlet Allocation (LDA), which probabilistically models text documents as mixtures of latent topics, where topics correspond to key concepts pre-sented in the corpus [2] (see also [3]). Author-Topic (AT) modeling is an extension of topic modeling that captures the relationship of authors to topics in addition to extracting the topics themselves. An extension of LDA to probabilistic AT modeling has been developed in [4]. In the literature [5], these more recent approaches have been found to produce better results than more traditional meth-ods such as latent semantic analysis (LSA) [6]. Despite previous work in classifying code based on concepts [1], applications of LDA and AT models have been limited to traditional text corpora such as academic publications, news reports, corporate emails, and historical documents [7, 8]. At the most basic level, however, a code repository can be viewed as a text corpus, where source files are analogous to documents and developers to authors. Though vocabulary, syntax, and conventions differentiate a programming language from a natural language, the tokens present in a source file are still indicative of its function (ie. its topics). Thus here we develop and apply probabilistic AT models to software data.
 In AT models for text, the data consists of a set of documents. The authors of each documents are known and each document is treated as a bag of words. We let A be the total number of authors, W the total number of distinct words (vocabulary size), and T the total number of topics present in the documents. While non-parametric Bayesian [9] and other [10] methods exist to try to infer T from the data, here we assume that T is fixed (e.g. T = 100 ), though we explore different values. As in [7], our model assumes that each topic t is associated with a multinomial distribution  X   X  t over words w , and each author a is associated with a multinomial distribution  X   X  a over topics. More precisely, the parameters are given by two matrices: a T  X  A matrix  X  = (  X  ta ) of author-topic distributions, and a W  X  T matrix  X  = (  X  wt ) of topic-word distributions. Given a document d containing N d words with known authors, in generative mode each word is assigned to one of the authors a of the document uniformly, then the corresponding  X   X  a is sampled to derive a topic t , and finally the corresponding  X   X  t is sampled to derive a word w . A fully Bayesian model is derived by putting symmetric Dirichlet priors with hyperparameters  X  and  X  over the distributions  X   X  a and  X   X  t . So for instance the prior on  X   X  a is given by and similarly for  X   X  t . If A is the set of authors of the corpus and document d has A d authors, it is easy to see that under these assumptions the likelihood of a document is given by: which can be integrated over  X  and  X  and their Dirichlet distributions to get P ( d |  X ,  X , A ) . The posterior can be sampled efficiently using Markov Chain Monte Carlo Methods (Gibbs sampling) and, for instance, the  X  and  X  parameter matrices can be estimated by MAP or MPE methods. Once the data is obtained, applying this basic AT model to software requires the development of several tools to facilitate the processing and modeling of source code. In addition to the crawling infrastructure described above, the primary functions of the remaining tools are to extract and resolve author names from source code, as well as convert the source code to the bag-of-words format. 4.1 Information Extraction from Source Code Author-Document: The author-document matrix is produced from the output of our author extrac-tion tool. It is a binary matrix where entry [ i , j ]=1 if author i contributed to document j , and 0 otherwise. Extracting author information is ultimately a matter of tokenizing the code and associat-ing developer names with file (document) names when this information is available. This process is further simplified for java software due to the prevalence of javadoc tags which present this metadata in the form of attribute-value pairs.
 Exploratory analysis of the Eclipse 3.0 code base, however, shows that most source files are credited to  X  X he IBM Corporation X  rather than specific developers. Thus, to generate a list of authors for specific source files, we parsed the Eclipse bug data available in [11]. After pruning files not associated with any author, this input dataset consists of 2,119 Java source files, comprising 700,000 lines of code, from a total of 59 developers.
 While leveraging bug data is convenient (and necessary) to generate the developer list for Eclipse 3.0, it is also desirable to develop a more flexible approach that uses only the source code itself, and not other data sources. Thus to extract author names from source code we also develop a lightweight parser that examines the code for javadoc  X  X author X  tags, as well as free form labels such as  X  X uthor X  and  X  X eveloper. X  Occurrences of these labels are used to isolate and identify developer names. Ultimately author identifiers may come in the form of full names, email addresses, url X  X , or CVS account names. This multitude of formats, combined with the fact that author names are typically labeled in the code header, is key to our decision to extract developer names using our own parsing utilities, rather than part-of-speech taggers [12] leveraged in other text mining projects. A further complication for author name extraction is the fact that the same developer may write his name in several different ways. For example,  X  X ohn Q. Developer X  alternates between  X  X ohn Developer, X   X  X . Q. Developer, X  or simply  X  X eveloper. X  To account for this effect, we implement also a two-tiered approach to name resolution using the q-gram algorithm [13]. When an individual project is parsed, a list of contributing developers (and the files they modified) is created. A pairwise comparison of author-names is then performed using q-gram similarity, and pairs of names whose similarity is greater than a threshold t 1 are merged. This process continues until all pairwise simi-larities are below the threshold, and the project list is then added to a global list of authors. When parsing is complete for all projects, the global author list is resolved using the same process, but with a new threshold, t 2 , such that t 2 &gt; t 1 . This approach effectively implements more conser-vative name resolution across projects in light of the observation that the scope of most developer activities is limited to a relatively small number (1 in many cases) of open source efforts. In prac-tice, we set t 1 = . 65 and t 2 = . 75 . Running our parser on the multi-project repository yields 9,250 distinct authors respectively.
 Word-Document: To produce the word-document matrix for our input data we have developed a comprehensive tokenization tool tuned to the Java programming language. This tokenizer includes language-specific heuristics that follow the commonly practiced naming conventions. For example, the Java class name  X  X uickSort X  will generate the words  X  X uick X  and  X  X ort X . All punctuation is ignored. As an important step in processing source files our tool removes commonly occurring stop words. We augment a standard list of stop words used for the English language (e.g. and, the, but, etc) to include the names of all classes from the Java SDK (eg. ArrayList, HashMap, etc). This is done to specifically avoid extracting common topics relating to the Java collections framework.We run the LDA-based AT algorithm on the input matrices and set the total number of topics (100) and the number of iterations by experimentation. For instance, the number of iterations, i , to run the algorithm is determined empirically by analyzing results for i ranging from 500 to several thousands. The results presented in the next section are derived using 3,000 iterations, which were found to produce interpretable topics in a reasonable amount of time (a week or so). Because the algorithm contains a stochastic component we also verified the stability of the results across multiple runs. 4.2 Topic and Author-Topic Modeling Results A representative subset of 6 topics extracted via Author-Topic modeling on the selected 2,119 source files from Eclipse 3.0 is given in Table 2. Each topic is described by several words associated with the topic concept. To the right of each topic is a list of the most likely authors for each topic with their probabilities. Examining the topic column of the table it is clear that various functions of the Eclipse framework are represented. For example, topic 1 clearly corresponds to unit testing, topic 2 to debugging, topic 4 to building projects, and topic 6 to automated code completion. Remaining topics range from package browsing to compiler options.
 Table 3 presents 6 representative author-topic assignments from the multi-project repository. This dataset yields a substantial increase in topic diversity. Topics representing major sub-domains of software development are clearly represented, with the first topic corresponding to web applica-tions, the second to databases, the third to network applications, and the fourth to file processing. Topics 5 and 6 are especially interesting, as they correspond to common examples of crosscutting concerns from aspect-oriented programming [14], namely security and logging. Topic 5 is also demonstrative of the inherent difficulty of resolving author names, and the shortcomings of the q-gram algorithm, as the developer  X  X ert van ham X  and the developer  X  X amgert X  are most likely the same person documenting their name in different ways.
 Several trends reveal themselves when all results are considered. Though the majority of topics can be intuitively mapped to their corresponding domains, some topics are too noisy to be able to associate any functional description to them. For example, one topic extracted from our repository consists of Spanish words unrelated to software engineering which seem to represent the subset of source files with comments in Spanish. Other topics appear to be very project specific, and while they may indeed describe a function of code, they are not easily understood by those who are only casually familiar with the software artifacts in the codebase. This is especially true with Eclipse, which is limited in both the number and diversity of source files. In general noise appears to diminish as repository size grows. Noise can be controlled to some degree with tuning the number of topics to be extracted, but of course can not be eliminated completely.
 Examining the author assignments (and probabilities) for the various topics provides a simple means by which to discover developer contributions and infer their competencies. It should come as no surprise that the most probable developer assigned to the JUnit framework topic is  X  X gamma X , or Erich Gamma. In this case, there is a 97% chance that any source file in our dataset assigned infer that he is likely to have extensive knowledge of this topic. This is of course a particularly attractive example because Erich Gamma is widely known for being a founder of the JUnit project, a fact which lends credibility to the ability of the topic modeling algorithm to assign developers to reasonable topics. One can interpret the remaining author-topic assignments along similar lines. For example, developer  X  X audel X  is assigned to the topic corresponding to automatic code completion with probability .99. Referring back to the Eclipse bug data it is clear that the overwhelming majority of bug fixes for the codeassist framework were made by this developer. One can infer that this is likely to be an area of expertise of the developer.
 In addition to determining developer contributions, one may also be curious to know the scope of a developer X  X  involvement. Does a developer work across application areas, or are his contri-butions highly focused? How does the breadth of one developer compare to another? These are natural questions that arise in the software development process. To answer these questions within the framework of author-topic models, we can measure the breadth of an author a by the entropy H ( a ) =  X  our multi-project dataset, we find that the average measure is 2.47 bits. The developer with the low-est entropy is  X  X hierry danard, X  with .00076 bits. The developer with the highest entropy is  X  X di X  with 4.68 bits, with 6.64 bits being the maximum possible score for 100 topics. While the entropy of the distribution of an author over topics measures the author X  X  breadth, the similarity between two authors can be measured by comparing their respective distributions over topics. Several metrics are possible for this purpose, but one of the most natural measures is provided by the symmetrized Kullback-Leibler (KL) divergence. Multidimensional scaling (MDS) is employed to further visual-ize author similarities, resulting in Figure 2 for the Eclipse project. The boxes represent individual developers, and are arranged such that developers with similar topic distributions are nearest one an-other. A similar figure, displaying only a subset of the 4,500 SourceForge and Apache authors due to space and legibility constraints, is available in the supplementary materials. This information is es-pecially useful when considering how to form a development team, choosing suitable programmers to perform code updates, or deciding to whom to direct technical questions. Two other important distributions that can be retrieved from the AT modeling approach are the distribution of topics across documents, and the distribution of documents across topics (not shown). The corresponding entropies provide an automated and novel way to precisely formalize and measure topic scattering and document tangling, two fundamental concepts of software design [14], which are important to software architects when performing activities such as code refactoring. Sourcerer relies on a deep analysis of code to extract pertinent textual and structural features that can be used to improve the quality and performance of source code search, as well as augment the ways in which code can be searched. By combining standard text information retrieval techniques with source-specific heuristics and a relational representation of code, we have available a comprehensive platform for searching software components. While there has been progress in developing source-code-specific search engines in recent years (e.g. Koders, Krugle, and Google X  X  CodeSearch), these systems continue to focus strictly on text information retrieval, and do not appear to leverage the copious relations that can be extracted and analyzed from code.
 Programs are best modeled as graphs, with code entities comprising the nodes and various relations the edges. As such, it is worth exploring possible ranking methods that leverage the underlying graphs. A natural starting point is Google X  X  PageRank [15], which considers hyperlinks to formulate a notion of popularity among web pages. This can be applied to source as well, as it is likely that a code entity referenced by many other entities are more robust than those with few references. We used Google X  X  PageRank [15] almost verbatim. The Code Rank of a code entity (package, class, or method) A is given by: CR ( A ) = (1  X  d ) + d ( CR ( T 1 ) /C ( T 1 ) + ... + CR ( T n ) /C ( T n )) where T ...T n are the code entities referring to A , C ( A ) is the number of outgoing links of A , and d is a damping factor.
 Using the CodeRank algorithm as a basis it is possible to devise many ranking schemes by building graphs from the many entities and relations stored in our database, or subsets thereof. For example, one may consider the graph of only method call relationships, package dependencies, or inheritance hierarchies. Moreover, graph-based techniques can be combined with a variety of heuristics to further improve code search. For example, keyword hits to the right of the fully-qualified name can be boosted, hits in comments can be discounted, and terms indicative of test articles can be ignored. We are conducting detailed experiments to assess the effectiveness of graph-based algorithms in con-junction with standard IR techniques to search source code. Current evidence strongly indicates that best results are ultimately obtained by combining term-based ranking with source-specific heuris-tics and coderank. After defining a set of 25 control queries with known  X  X est X  hits, we compared performances using standard information retrieval metrics, such as area under curve (AUC). Queries were formulated to represent users searching for specific algorithms, such as  X  X epth first search, X  as well as users looking to reuse complete components, such as  X  X atabase connection manager. X  Best hits were determined manually with a team of 3 software engineers serving as human judges of re-sult quality, modularity, and ease of reuse. Results clearly indicate that the general Google search engine is ineffective for locating relevant source code, with a mean AUC of .31 across the queries. By restricting its corpus to code alone, Google X  X  code search engine yields substantial improvement with an AUC of approximately .66. Despite this improvement this system essentially relies only on regular expression matching of code keywords. Using a Java-specific keyword and comment parser our infrastructure yields an immediate improvement with an AUC of .736. By augmenting this further with the heuristics above and CodeRank (consisting of class and method relations), the mean AUC climbs to .841. At this time we have conducted extensive experiments for 12 ranking schemes corresponding to various combinations of graph-based and term-based heuristics, and have observed similar improvements. While space does not allow their inclusion, additional results are available from our supplementary materials page. Here we have leveraged a comprehensive code processing infrastructure to facilitate the mining of large-scale software repositories. We conduct a statistical analysis of source code on a previously un-reported scale, identifying robust power-law behavior among several code entities. The development and application of author-topic probabilistic modeling to source code allows for the unsupervised ex-traction of program organization, functionality, developer contributions, and developer similarities, thus providing a new direction for research in this area of software engineering. The methods de-veloped are applicable at multiple scales, from single projects to Internet-scale repositories. Results indicate that the algorithm produces reasonable and interpretable automated topics and author-topic assignments. The probabilistic relationships between author, topics, and documents that emerge from the models naturally provide an information-theoretic basis to define and compare developer and program similarity, topic scattering, and document tangling with potential applications in soft-ware engineering ranging from bug fix assignments and staffing to software refactoring. Finally, by combining term-based information retrieval techniques with graphical information derived from program structure, we are able to significantly improve software search and retrieval performance. Acknowledgments: Work in part supported by NSF MRI grant EIA-0321390 and a Microsoft Faculty Research Award to PB, as well as NSF grant CCF-0725370 to CL and PB.

