 Ben Gurion University of the Negev Ben Gurion University of the Negev We present a constituency parsing system for Modern Hebrew. T he system is based on the
PCFG-LA parsing method of Petrov et al. (2006), which is exte nded in various ways in order to accommodate the specificities of Hebrew as a morphologica lly rich language with a small treebank. We show that parsing performance can be enhanced b y utilizing a language resource external to the treebank, specifically, a lexicon-based mor phological analyzer. We present a common case where the lexicon and the treebank follow differ ent annotation schemes. We show that Hebrew word-segmentation and constituency-parsing c an be performed jointly using CKY based model. We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and prese nt a concrete implementation of the method. Although the constituency parser does not make m any agreement mistakes to begin and ideas presented in this work are useful also for processi ng other languages, including for any language with a small treebank. 1. Introduction
Different languages have different syntactic properties. In English, word order is rela-tively fixed, whereas in other languages word order is much mo re flexible (in Hebrew, the subject may appear either before or after a verb). In langua ges with a flexible word order, the meaning of the sentence is realized using other st ructural elements, like word inflections or markers, which are referred to as morphology (in Hebrew, the marker is used to mark definite objects, distinguishing them from sub jects in the same position.
In addition, verbs and nouns are marked for gender and number, and subject and verb must share the same gender and number). A limited form of morp hology also exists in
English: the -s and -ed suffixes are examples of English morphological markings. In o ther languages, morphological processes may be much more involv ed. The lexical units (words) in English are always separated by white space. In Chi nese, such separation is not available. In Hebrew (and Arabic), most words are separa ted by white space, but many of the function words (determiners like the , conjunctions such as and , and prepositions like in or of ) do not stand on their own but are instead attached to the following words.
 language with a relatively simple morphology, relatively fi xed word order, and a large treebank. Data-driven English parsing is now at the state wh ere naturally occurring text in the news domain can be automatically parsed with accuraci es of around 90% (accord-ing to standard parsing evaluation measures). When moving fr om English to languages with richer morphologies and less-rigid word orders, howev er, the parsing algorithms developed for English exhibit a large drop in accuracy. In add ition, whereas English has a large treebank, containing over one million annotated wor ds, many other languages have much smaller treebanks, which also contribute to the dr op in the accuracies of the data-driven parsers. A similar drop in parsing accuracy is also exhibited in English when moving from the news domain, on which parsers have tradi tionally been trained, to other genres such as prose, blogs, poetry, product review s, or biomedical texts, which use different vocabularies and, to some extent, different s yntactic rules. with a rich and productive morphology, relatively free word order, bank. Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of informati on? How should the word segmentation issue (that function words do not appear in iso lation but attach to the next word, forming ambiguous letter patterns) be handled? Can mo rphological information be used effectively in order to improve parsing accuracy? parsing, namely, the probabilistic context-free grammar ( PCFG) with latent annotations (PCFG-LA) model of Petrov et al. (2006), as implemented in th e BerkeleyParser. After evaluating the out-of-the-box performance of the Berkeley Parser on the Hebrew tree-bank, we discuss some of its limitations and then go on to exte nd the PCFG-LA parsing model in several directions, making it more suitable for par sing Hebrew and related languages. Our extensions are based on the following themes .

Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent in a parsing system. One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be combined to form ever larger structures. The other is lexical knowledge about the identities of individual words, the word classes they belong to, and the kinds of synta ctic structures they can participate in. We argue that the amount of syntactic knowle dge needed for a parsing system is relatively limited, and that sufficiently large pa rts of it can be captured also 122 based on a relatively small treebank. Lexical knowledge, on the other hand, is much more vast, and we should not rely on a treebank (small or large ) to provide adequate lexical coverage. Instead, we should aim to find ways of integr ating lexical knowledge, which is external to the treebank, into the parsing process.
 morphological analyzer. We present a way of integrating the two resources also for the common case where their annotations schemes diverge. This m ethod is very effective in improving parsing accuracy.

Encoding input uncertainty using a lattice-based represen tation. Sometimes, the language signal (the input to the parser) may be uncertain. This happe ns in Hebrew when a space-delimited token such as  X  X  X  X ! can represent either a single word ( X  X an] onion X ) or a sequence of two words or three words ( X  X n shadow X  and  X  X n the s hadow, X  respectively).
When computationally feasible, it is best to let the uncertai nty be resolved by the parser rather than in a separate preprocessing step.
 (Chappelier et al. 1999; Hall 2005) to perform joint word segme ntation and syntactic disambiguation (Cohen and Smith 2007; Goldberg and Tsarfat y 2008). Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model.
Using morphological information to improve parsing accura cy. Morphology provides useful hints for resolving syntactic ambiguity, and the parsing mo del should have a way of utilizing these hints. There is a range of morphological hin ts than can be utilized: from functional marking elements (such as the  X  X ! marker indicating a definite direct object); to elements marking syntactic properties such as definitene ss (such as the Hebrew marker); to agreement patterns requiring a compatibility i n properties such as gender, number, and person between syntactic constituents (such as a verb and its subject or an adjective and the noun it modifies).
 grammar. Although the constituency parser does not make man y agreement mistakes to begin with, the filter mechanism is effective in fixing the a greement mistakes that the parser does make, without introducing new mistakes.
 tions. Goldberg and Tsarfaty (2008) suggest the lattice-pa rsing mechanism, Goldberg et al. (2009) discuss ways of interfacing a treebank-derive d PCFG-parser with an exter-nal lexicon, and Goldberg and Elhadad (2011) present experi ments using the PCFG-LA
BerkeleyParser. Here we provide a cohesive presentation of t he entire system, as well as a more detailed description and an expanded evaluation. We a lso extend the previous work in several dimensions: We introduce a new method of inte rfacing the parser and the external lexicon, which contributes to an improved pars ing accuracy, and suggest incorporating agreement information as a filter.
 and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in thi s work are useful also for processing other languages, including English. The lattic e-based parsing methodology is useful in any case where the input is uncertain. Indeed, we h ave used it to solve the problem of parsing while recovering null elements in bot h English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010). Extending th e lexical coverage of a treebank-derived parser using an external lexicon is rele vant for any language with a small treebank, and also for domain adaptation scenarios f or English. Finally, the agreement-as-filter methodology is applicable to any morph ologically rich language, and although its contribution to the parsing task may be limi ted, it is of wide applica-bility to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language. 2. Modern Hebrew 2.1 Lexical and Syntactic Properties
Some relevant lexical and syntactic properties of Modern Heb rew are highlighted in this section. 2.1.1 Unvocalized Orthography. Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morpho logical ambiguity. Some tokens can admit as many as 15 distinct readings, and the aver age number of pos-sible morphological analyses per token in Hebrew text is 2.7, compared with 1.4 in
English (Adler 2007). The word  X  X  X  X  X  X  X ! can be read in at least eight different ways  X  X ngratefulness, X   X  X un/adjective feminine,plural  X ), the word journalist, X   X  X riting, X   X  X cript, X   X  X rote, X   X  X dded someone as a recipient, X   X  X as added as a recipient X ) and the word  X  X ! can be read as a very common case-marker (appearing before definite direct objects), a very common pronoun ( X  X ou/ ( X  X hovel X ). 2.1.2 Affixation. Eight common prepositions, conjunctions, and articles may n ever appear in isolation and must always be attached as prefixes to the following word.
These include the function words  X  X ! ( X  X rom X ),  X ! ( X  X hich X / X  X ho X / X  X hat X ),  X ! together, producing forms such as  X  X  X  X  X  X  X  X  X ! (  X - X - X  X - X - X  X  X  X ! that when it appears by itself, the last part of the token, the noun be interpreted as the sequence  X - X  X  X ! ( X  X ho moved X ). The linear order of such elements within a token is fixed (disallowing the reading  X - X - X  X - X - X - X  X  X ! are rather free, however. The relativizer  X ! ( X  X hat X ), for example, may attach to an arbitrarily long relative clause that goes beyond token bou ndaries. The attachment in such cases encompasses a long-distance dependency that can not be captured by local-context (or Markovian) sequential processes that are typic ally used for morphological disambiguation. The same argument holds for resolving PP at tachment of a prefixed preposition or marking conjunction of elements of any kind.
 when following the particles  X ! ( X  X n X ),  X  X ! ( X  X ike X ), and interpreted as either  X - X  X  X ! ( X  X n house X ) or  X - X - X  X  X ! ( X  X n the house X ). 124 prepositions, and others as suffixes (e.g.,  X  X  X  X  X ! [  X  X  X  X - X  X ! them X  X ).
 assigned numbers X ) vs.  X  X  X  X  X  X - X ! ( X  X is number X  or  X  X he one who cuts his hair X ) vs. ( X  X rom his book X  or  X  X rom his barber X ),  X  X  X  X  X  X ! ( X  X utting together X ) vs. and  X  X  X  X ! ( X  X n onion X ) vs.  X - X  X  X ! ( X  X n the shadow X ) are only a few examples of ambiguities that may arise. Quantitatively, 99,896 out of 567,483 forms (17%) in a wide-coverage lexicon of Hebrew can admit both segmented and unsegmented an alyses.
 alone, but can be disambiguated by more global syntactic con straints (in  X  X  X  X  X  X  X ! , the middle token is ambiguous between  X  X  X  X  X ! [ X  X ky X  X  and and the sequence can be interpreted as either  X  X  saw blue skie s X  or  X  X  saw that blue water. X 
On the other hand,  X  X  X  X  X   X  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X   X  X  X  X ! verb  X  X  X  X  X  X ! requires the relativizer  X ! , allowing only the segmented that blue water broke from the well X . In the other direction,  X  X  X  X  X ! is also unambiguous, allowing only the unsegmented reading  X  X  saw blue skies and went to sleep X .) 2.1.3 Rich Templatic Morphology. Hebrew words follow a complex morphological struc-ture, which is based on a root + template system, with both der ivational and inflectional elements. Word forms can encode gender, number, person, and tense, and in addition noun-compounding is also morphologically marked (see Sect ion 2.1.7). Although the exact details of the system are irrelevant (but see Adler [20 07] and Glinert [1989] for a good overview), we note that this word formation mechanism r esults in a very high number of possible word forms, and that it is hard to guess the part-of-speech of words based on prefixes and suffixes alone, a method frequently used in other languages. 2.1.4 The Participle Form. The Hebrew participle form (  X  X  X  X  X  X  X  X ! of verbs) is a form that shares morphological and syntactic p roperties of nouns, verbs, and adjectives. This form causes many disagreements between human annotators, and large disagreement is found also between major Hebrew diction aries regarding many word forms (see Adler et al. [2008b] for a discussion from tag set design and annotation guidelines, including many syntactic, semantic, and lexic al considerations). For the purpose of this work, this form is of interest as it highlight s the inherent ambiguity between adjectival, nominal, and verbal readings of many wor ds, which are hard to disambiguate even in context. 2.1.5 Relatively Free Constituent Order. The ordering of constituents inside a phrase is relatively free. This is most notably apparent in verbal phr ases and sentential levels. In particular, whereas most sentences follow a subject-verb-o bject order (SVO), OVS and VSO configurations are also possible (counting in the Hebrew T reebank reveals 5,720
SV cases and 2,613 VS cases, compared with 81,135 SV and 3,018 VS constructions in the English WSJ Treebank). In addition, verbal arguments can a ppear before or after the verb, and in many orders. Such variations in constituent order are easy to capture using  X  X lat X  S structures putting the verbs and all of its argum ents on the same clausal level, and this is the annotation approach adopted by the Hebr ew Treebank (as well as by treebanks of other languages, such as French [Abeill  X  e, Cl  X  ement, and Toussenel 2003]). These flat structures result in the grammar having mor e and longer rules and the treebank having fewer instances of each rule type, however, causing a data sparseness problem for statistical estimation methods based on treeba nk counts, and making it more difficult to reliably estimate the grammar parameters. 2.1.6 Verbless Constructions. Several constructions in which the verb is not realized are common in Hebrew. These include the possessive construction s such as  X  X  X  X ! ( X  X o-Ido toys many X  meaning  X  X do has many toys X ), which also feature a flexible constituent order  X  X  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X ! ( X  X oys many to-Ido X ,  X  X do has many toys X ), and copular constructions such as  X  X  X  X   X  X  X  X  X ! ( X  X he-boy cute X   X  X he boy is cute X ) and ( X  X he-boy crazy X   X  X he boy is crazy X ). 2.1.7 NP Structure and Construct-State. Although constituent order may vary, NP internal structure is rigid. A special morphological marke r ( construct state , or is used to mark noun-compounds as well as similar phenomena ( this is similar to the idafa construction in Arabic). 4 Noun compounding in Modern Hebrew is productive and very frequent X  X bout a quarter of the noun tokens in the Hebr ew Treebank are in the construct state. Construct-state nouns can be highly am biguous with non-construct-state nouns. Some forms are morphologically marked but the m arking is not present in unvocalized text (  X  X  X  X  X ! /banot vs.  X  X  X  X  X ! /bnot), and some forms are not marked at all (
The construct-state marker, although ambiguous, is essent ial for analyzing NP internal structure. Where regular nouns are marked for definiteness us ing the definite marker  X ! , construct-nouns acquire the definite status of the noun-ph rase they compound to.
Construct constructions may be nested, as in  X  X  X  X   X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X ! of the apples X ). 2.1.8 Definiteness. Definiteness is spread across many elements in the NP. All ele ments in a definite NP, except for construct-nouns and proper-name s, are explicitly marked using the functional element  X ! that is prefixed to the token. Proper-names are inher-ently definite and cannot take the definite marker, and constr uct-nouns acquire their definiteness status from the NP they dominate (definiteness i s not explicitly marked on construct-nouns). 2.1.9 Case Marking. Definite direct objects are marked. The case marker in this cas e is the function word  X  X ! appearing before the direct object. Subjects, indirect object s, and non-definite direct objects are not marked. 2.1.10 Agreement. Hebrew grammar forces morphological agreement between adjec -tives and nominals (adjectives appear after the noun, and agr ee in gender, number, and definiteness), and between subjects and verbs (including the verbless copular construc-tions), which agree in gender, number, and person. Agreemen t in the predicative case is a bit complex: When the verb is overt and the predicative-co mplement is a noun, as in  X  X  X  X  X  X  X   X  X  X   X  X  X  X  X ! ( X  X he-trip fem is fem an-excuse are required between the subject and the verb (but not the pred icative-complement), but in the verbless case, the subject and the predicate-compl ement noun must agree (  X  X  X  X  X  X  X   X  X  X  X  X ! *  X  X he-trip fem an-excuse masc  X ). When the predicate-complement is an adjec-tive, gender and number agreement between the subject and the predicate-complement 126 is required regardless of the realization of the verb/copul ar element:  X  X  X  X ! *,  X  X  X  X   X  X  X   X  X  X  X ! ,  X  X  X  X  X   X  X  X   X  X  X  X ! * ( X  X he-boy tall tall masc  X ,  X *the-girl is fem tall masc  X ). 2.2 Implications for Parsing
After surveying some lexical and syntactic properties of Mo dern Hebrew, we turn to highlight some aspects in which Modern Hebrew differs from English from the perspective of parsing system design. 2.2.1 Small Amount of Annotated Data. Whereas the English Treebank is relatively large (49,208 sentences, or 1,173,766 words), the Hebrew Treebank (Guthmann et al. 2009) is much smaller, containing only 6,220 sentences, or 115,661 t okens (156,316 words algorithms used to construct the parser. 2.2.2 Ambiguous Word Segmentation. Syntactic parsing systems treat the input sentence as observed data X  X he leaves (in constituency parsing) of the tree are known in advance, and the parser is expected to build a parse tree around them. T his is not the case in
Hebrew, where many function words are not separated by white s pace but instead are prefixed to the next word and appear within the same token. Thi s makes the word sequence unobserved to the parser, which has to infer both th e syntactic-structure and the token segmentation . 6 system in which an initial model is in charge of token-segmen tation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and L iu 2009; Green and
Manning 2010). As discussed in Section 2.1.2 (as well as in Ts arfaty [2006a], Goldberg and Tsarfaty [2008], and Cohen and Smith [2007]), however, t he token-segmentation and syntactic-parsing tasks are closely intertwined and are be tter performed jointly instead of in a pipeline fashion, which is the approach we explore in t his work. 2.2.3 Morphological Variation and High Out-of-Vocabulary R ate. The intrinsic deficiency caused by the small amount of training data is made even more s evere due to Hebrew X  X  rich morphological inflection patterns. The high amount of mo rphological variation means that many word forms will not be observed in the trainin g data, making it harder to reliably estimate lexical probabilities based on the ann otated resources alone. on simple orthographic features (words starting with capit al letters are proper nouns, words ending in -ed are usually verbs, etc.), this is not the case for Hebrew. Amon g the 773 words appearing in English test data but not in the tra ining data, 269 start heuristics cover almost half of the unobserved tokens. Such heuristics are not available for Hebrew in the common case of unvocalized text: Proper name s are not marked in writing, and word prefixes and suffixes are not indicative o f the part-of-speech tags. 7 Thus, the out-of-vocabulary (OOV) problem is much harder in Hebrew than in
English and other European languages: On the one hand many wo rds are unobserved in training, and on the other, it is more difficult to guess the analysis of such unknown words.
 manually annotated corpora, as such corpora cannot provide adequate lexical coverage.
Systems that attempt to perform disambiguation on the lexic al level (such as sequence-based morphological disambiguators, or syntactic parsers that perform morphological disambiguation as part of the parsing process) should be des igned to incorporate lexical knowledge from sources external to the annotated corpora. W e discuss methods of en-hancing the system X  X  performance based on a resource that is external to the treebank: A lexicon-based broad-coverage morphological analyzer enh anced with semi-supervised probability estimates based on expectation maximization ( EM) training of a hidden
Markov model (HMM) tagger on a large amount of unannotated tex t. 2.2.4 Morphological Agreement. The rich morphological system also means that words carry large amounts of extra information: definiteness, gen der, number, tense, and person. Some of this information interacts with syntax thro ugh agreement constraints .
Specifically, nouns and adjectives should agree in gender and number, and subjects and verbs should agree in gender, number, and person. Agreem ent constraints can provide useful hints for disambiguating the syntactic stru cture. Consider for example the sentence  X  X  X   X  X  X  X   X  X  X  X  X  X   X  X   X  X  X  X  X  X ! ( X  X ife of the man who ate the apple X ). The English sentence is ambiguous with respect to the entity who ate the apple, but the
Hebrew version is not X  X he verb  X  X  X  X  X ! ( X  X te X ) is in feminine form, indicating that it was the wife who did the eating. Can a parsing system make use of su ch information? This issue is investigated further in Section 8.2. 2.3 Existing Resources for Hebrew Text Processing
Several linguistic resources are available for Hebrew, and a re used as building blocks for the parsing systems described in this work. 2.3.1 The Hebrew Constituency-Treebank. A constituency treebank of Modern Hebrew, incrementally developed at the Technion over the course of m ore than eight years (Sima X  X n et al. 2001; Guthmann et al. 2009), is maintained by MILA, the knowledge center for processing Hebrew. 8 The current version of the treebank (Version 2) contains 6,220 sentences taken from the Israeli daily newspaper  X  X  X  X ! are manually annotated on both the lexical and the syntactic levels. Each token segmented into words, and each word is assigned a part of spee ch tag that also captures, 128 where applicable, the morphological properties of the word such as number, gender, and person. Then a constituency tree is built on top of the seg mented words. The annotation of NPs is relatively nested, and the sentence lev el structures are relatively flat (the verb and all of its arguments reside on one level under S). The treebank has 115,661 tokens and 156,764 words.
 speech is chosen to reflect the syntactic function of the given word in context. For exam-ple, demonstrative pronouns are tagged in the treebank as ad jectives when appearing in an adjectival position (  X  X !  X  X  X ! ,  X  X his/JJ child/NN X ), and a special MOD tag is used to mark non-adverbial clausal level modification (that is, mod ifications that can be treated as adverbial, but that are used to modify something other tha n a verb). For a more detailed description of the Constituency Treebank see Sima  X  X n et al. (2001), Guthmann et al. (2009), and Tsarfaty (2010, pages 199 X 216), as well as the annotation guidelines. 2.3.2 Train/dev/test Splits. Throughout the article, we follow the established train/ dev/test split for the treebank, namely, sentences 1 X 483 ar e used for development, sentences 484 X 5,740 are used for training the parser, and se ntences 5,741 to 6,220 are used as the final test set. 2.3.3 The MILA Broad-Coverage Lexicon. Aside from the Constituency Treebank, Hebrew has a wide-coverage, lexicon-based morphological analyze r which can assign morpho-logical analyses (prefixes, suffixes, core POS, gender, numb er, person, etc.) to Hebrew tokens. The lexicon (henceforth the KC Analyzer ) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2 008). It is based on a lexicon of roughly 25,000 word lemmas and their inflection pat terns. From these, 562,439 unique word forms are derived. These are then prefixed (subjec t to constraints) by 73 prepositional prefixes. Even with this seemingly large voca bulary, the KC Analyzer X  X  coverage is not perfect. In Adler et al. (2008a), we present a m achine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies. U sing this extension, the analyzer has perfect coverage (even though the quality is ob viously better for words that are present in the analyzer X  X  database).
 in depth in BGU Computational Linguistics Group (2008).
 language is a worthwhile and cost-effective effort: After e stablishing the tag set, it is relatively straightforward to add lemmas to the lexicon, an d the automatic inflection process guarantees good coverage of all the possible inflecti ons. This is much more efficient than annotating enough text to obtain a similar cov erage. 2.3.4 Hebrew Morphological Disambiguator. The morphological analyzer provides the possible set of analyses for each token, but does not disambi guate the correct analy-sis in context. A morphological disambiguator (henceforth  X  X he Hebrew tagger X  or  X  X agger X ) was developed by Meni Adler at Ben-Gurion Univers ity of the Negev (Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and El hadad 2008). After the (extended) morphological analyzer assigns the possible an alyses for each token in an input sentence, the tagger takes the output of the analyzer a s input and chooses the sin-gle best analysis for the entire sentence (performing both t oken segmentation of words and part-of-speech assignment for each word). The tagger is an HMM-based sequential model that is trained in a semi-supervised fashion using EM b ased on the output of the morphological analyzer on a large amount (about 70M words) o f unannotated Hebrew text. The tagger is described in Adler and Elhadad (2006) and Adler (2007). tation and tagging when measured on the POS accuracy, and 90% accuracy when measured on the complete tag set, which includes the complet e set of morphological features. Because the tagger is not trained on a particular a nnotated training set but instead on a very large corpus of text spanning multiple genr es, its performance is robust across domains.
 training process. This initialization procedure takes the output of the analyzer and it assigns an a priori, context-free likelihood for each ana lysis of a word (although the former; such preferences can be modeled as probability dist ributions, and the initializa-tion procedure attempts to learn the values of these distrib utions automatically from raw data). This initialization procedure is described in Go ldberg, Adler, and Elhadad (2008).
 events, which are based on patterns observed in the unannota ted training data. We use these counts in order to improve the lexical-disambiguatio n capacity of the parser. 2.3.5 A Resource Incompatibility Issue. Unfortunately, the KC Analyzer adopted a dif-ferent tag set than the one used in the treebank, and analyses produced by the KC Analyzer (and hence by the morphological disambiguator) ar e incompatible with the
Hebrew Treebank. These are not mere technical differences, b ut derive from different perspectives on the data. The Hebrew Treebank (TB) tag set is s yntactic in nature ( X  X f the word in this particular position functions as an adverb, tag it as an adverb, even though it is listed in the dictionary only as a noun X ), wherea s the KC tag set (Adler 2007; Netzer et al. 2007; Adler et al. 2008b) takes a lexical a pproach to POS tagging ( X  X  word can assume only POS tags that would be assigned to it i n a dictionary X ). The lexical approach does not accommodate generic modification POS tags such as MOD, nor does it allow listing of demonstrative pronouns as adject ives.
 different principles underlying tag definitions, and diffe rent verification procedures.
This difference in perspective yields different performan ces for parsers induced from tagged data, and a simple mapping between the two schemes is i mpossible to define. herently hard to define, and the wide disagreement about thei r status is reflected in practically all Hebrew dictionaries. This kind of disagreem ent naturally appears also between the KC and TB. See Adler et al. (2008b) and Netzer et al . (2007) for further discussion on these two interesting cases.
 creation of a successful parsing system. On the one hand the s yntactic annotations in the treebank are needed in order to train the parser, and on the ot her hand the information provided by the morphological analyzer is needed in order to provide a good lexical coverage. We discuss an approach to bridging this discrepan cy in Section 6. 130 2.4 Section Summary
To summarize, the Hebrew language and its analysis poses seve ral challenges to parser design: The amount of annotated material is relatively smal l, precluding the possibility of learning robust lexical parameters from the annotated co rpora. The productive nature of the morphology results in many word forms, adding a nother obstacle to estimating lexical parameters from annotated data. The nat ure of the word-formation mechanism in Hebrew makes it hard to guess the morphological a nalysis of a word based on its prefix and suffix alone as is done in other language s, requiring the use of a more complex system for handling unknown words. Many functi on words in Hebrew are not separated by white space but are instead attached to t he next token, making the observed word sequence ambiguous. Word segmentation ne eds to be performed in addition to syntactic disambiguation. Successful word s egmentation may rely on syntactic disambiguation, suggesting that it is better to p erform the segmentation and syntactic-disambiguation tasks jointly. Finally, Hebre w grammar requires various forms of morphological agreement, a fact which hopefully ca n help disambiguate otherwise ambiguous syntactic structures. The syntactic p arser should be able to make use of agreement information.
 stituency structure and a broad-coverage, manually constr ucted, lexicon-based mor-phological analyzer. The morphological analyzer is capabl e of providing the possible morphological analyses for many lexical forms, and it is ext ended using a machine-learning technique to also provide possible analyses for wo rd-forms not covered by the lexicon. The extended lexicon provides a good lexical co verage of Hebrew. Also available is a morphological disambiguator that is capable of associating probabilities to the possible analyses of the lexical forms in the lexicon, an d disambiguating the analyses of a sequence of lexical items in context based on a sequentia l model. The constituency treebank can be used to learn the parameters of a syntactic-m odel of Hebrew, and the morphological analyzer can be used to provide broad-cov erage lexical knowledge.
Unfortunately, the treebank and the lexicon/disambiguato r follow different annotation schemes, and are therefore incompatible with each other. Th e annotation gap between the two resources must be bridged before they can be used toge ther.
 3. Latent-Annotation State-Split Grammars (PCFG-LA)
Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result i n accurate grammars. Their work triggered investigations in automatic grammar refinem ent and state-splitting (Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which wa s then perfected in work by Petrov and colleagues (Petrov et al. 2006; Petrov and Klei n 2007; Petrov 2009). should be recovered. Instead of a single NP symbol, these mode ls hypothesize that there are many different NP symbols, NP 1 , . . . , NP k , and each is used in a different context. The labels are hidden, however, and we can only observe the co re category label (NP).
The job of the training process is to come up with the hidden set of label assignments to non-terminals, such that the resulting grammar assigns a high probability to the observed treebank data. Such models are called PCFG with lat ent annotations (PCFG-
LA) and are shown empirically to produce very accurate parsi ng results.
BerkeleyParser, 11 learns the latent annotations by starting with a bare-bones treebank-derived grammar and automatically refining it in split-merg e-smooth cycles, setting the parameters using EM. We provide a brief description of the mo del and learning process (refer to Petrov et al. 2006; Petrov and Klein 2007; Petrov 20 09 for the full details). the following steps are performed repetitively:
Splitting each non-terminal category in two All of the grammar symbols are split. In
Merging back non-effective splits Not all of the splits are useful. For example, the
Smoothing the split non-terminals toward their shared ance stor Finally, split sym-
Performing five or six such split-merge-smooth cycles resul ts in accurate grammars, with annotations that capture many latent syntactic intera ctions. Six cycles mean that symbols can have as many as 64 different substates.
 resulting in the (approximate) most probable unannotated t ree according to the refined grammar (the score of the unsplit rule A  X  B C is taken to be vertical and zeroth-order horizontal markovization (Klei n and Manning 2003). This means that in the initial grammar, each of the non-terminal s ymbols is effectively conditioned on its parent alone, and is independent of its si sters. For example, the rule S  X  NP VP NP PP is binarized as:
S  X  NP @S @S  X  VP @S @S  X  NP @S @S  X  PP 132 indicating that S rules start with an NP, can be followed by a s equence of zero or more NPs and VPs, and end with a PP. Such an extreme markovizat ion suggests a very strong independence assumption, and is too permissive on its own. It allows the resulting refined grammar to encode its own set of dependenci es between a node and its sisters, however, as well as ordering preferences in lon g, flat rules. For example, the binarized grammar allows the production S  X  NP NP PP, which may be incorrect. However, by annotating the symbols as follows:
S  X  NP @S 1 @S 1  X  VP @S 2 @S 2  X  NP @S 2 @S 2  X  PP the grammar now forces the VP to be produced before the NP, but still allows the NP to be dropped. Similarly, by annotating the symbols as:
S  X  NP @S 1 @S 1  X  VP @S 2 @S 2  X  NP @S 3 @S 3  X  PP the grammar effectively allows only the original rule to be p roduced.
 markovization (encoding more context in the initial binari zed rules) degrades parsing performance, while producing much larger grammars.
 racies for English, as well as many other languages includin g German (Petrov and Klein 2008), French (Candito, Crabb  X  e, and Seddah 2009), and Chinese (Huang and Harper 2009). 4. Baseline Experiments
The baseline system is an  X  X ut-of-the-box X  PCFG-LA parser, as described in Petrov et al. (2006) and Petrov and Klein (2007) and implemented in t he BerkeleyParser.
The parser is trained on the Modern Hebrew Treebank (see Secti on 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals.
 settings: Seg+POS Oracle: The parser has access to the gold segmentation and POS tags. Seg Oracle: The parser has access to the gold segmentation, but not the PO S tags.
Pipeline: A POS-tagger is used to perform word segmentation, which is t hen used as
A better tag set. Glossing over the parses revealed that the parser failed to l earn the distinction between finite and non-finite verbs. The impo rtance of this linguistic distinction for parsing is obvious, and was also noted in Kle in and Manning (2003) for English and in our previous work on parsing Hebrew (Goldberg a nd Tsarfaty 2008).
Finite and non-finite verbs are easily distinguishable from each other based on surface form alone. Although finiteness is clearly annotated in the t reebank, it is not on the  X  X ore X  part of the POS tags and was removed prior to training t he parser. In a second set of experiments the core tag set of the parser was modified t o distinguish finite verbs, infinitives, and modals. 13 The original core X  X ag set already includes some important distinctions, such as construct from non-construct nouns.

Results and discussion. Table 1 presents the parsing results on the development set. With gold POS tags and segmentation, the results are very high. Ac curacy drops considerably when the parser is not given access to the gold tags (from abou t 90 to less than 84 F indicating that the POS tags are both informative and ambigu ous. Results drop even further (from 84 to 77) in the pipeline case where the gold seg mentation is not available, indicating that correct segmentation also provides valuab le information to the parser and that segmentation mistakes are costly.
 useful, with an increase of about 1 F 1 points (absolute) after four split-merge-smooth cycles, and a smaller increase after five cycles. This stress es the importance of the core representation: The automatic learning procedure goes a lo ng way, but it can be aided by linguistically motivated manual interventions in some c ases. 4.1 Analyzing the Learned PCFG-LA Grammar 4.1.1 Terminal-Level (Lexical) Splits. We begin by inspecting the splits at the part-of-speech level. Table 2 displays the number of splits learned f or each of the parts-of-speech symbols. Prepositions are the most heavily split, followed closely by the somewhat-generic MOD tag and the nouns.
 Nouns and adjectives. The noun and adjective splits are somewhat hard to decipher. time related , places , etc.). Others are are much harder to interpret. 134
MOD. For the general-modification POS tags, most categories clea rly single out one or two words with very specific usage patterns, such as  X  X ! Verbs. Finite-verbs are not split at all, even though they form an op en-class category.
Modal verbs are split into two groups: One of them is dominate d by nine modals (  X  X  X  X  X ! ,  X  X ! ,  X  X  X  X  X ! ,  X  X  X ! ,  X  X  X ! ,  X  X  X  X  X ! ,  X  X  X  X  X ! ,  X  X  X  X ! the second contains all the others. This is an interesting di stinction, as the nine singled-out modals never take a subject, whereas the modals in the othe r group do. verbs are split into seven categories, six of which are domin ated by one or two words each, and the last is a catch-all category.

Coordination and question-words. Coordination words are heavily split, each of the categories dominated by one or two words, indicating differ ent usage patterns. The question words  X  X  X ! ( X  X hat X ) and  X  X  X ! ( X  X ho X ) are singled out from the rest.
Gender/number agreement. The verbs are not split at all, indicating that the learned grammar cannot model subject X  X erb agreement. Pronouns are s plit by type (personals, demonstrative, and subtypes of demonstratives), but not by gender and number. Noun and adjective splits are sometimes hard to decipher, but they do not exhibit any group-ing based on gender or number properties, indicating that th e grammar cannot model adjective X  X oun agreement. Category splits for the AGR tag do show a clear division that follows gender and number, but it is unclear what is capt ured by this division as the information cannot interact with nouns, adjectives, ver bs, or pronouns. 4.1.2 Grammar-Level Splits. Table 3 shows the number of splits learned for each gram-mar non-terminal. The NP category is the most heavily split, followed by predicative phrases, verb phrases, and PPs. With the exception of the FRA GQ category, all symbols are split into at least six substates. What information is enc apsulated in the state splits?
As noted by Petrov et al. (2006), the latent state-splits lea rned for the grammar symbols are harder to analyze.
 grammar in generation mode and by sampling word sequences fr om each of the states.
By looking at the resulting strings, one can sometimes infer the kind of information encoded in the grammar.

NP. The split-NPs encode phrase length (some splits result in ve ry long NPs, some in very short, some in very specific one-or two-word patterns ). They also encode the definiteness rules (either an NP is definite or not), the inter action between definiteness and the AT marker, and a limited interaction between definite ness and construct nouns.
Other NP splits are dedicated to pronouns or to question word s, or encode proper names, monetary units, and numbers.

SBAR. The split-SBARs are split according to the word introducing the SBAR. In addition, some split-SBARs encode quoted and parenthetica l items.

S. The split-Ss differ by length. In addition, some S splits seem to be modeling verb-less sentences, variations in word order, and sentence-level co ordination. 4.2 Limitation of PCFG-LA Parsing of Modern Hebrew
The PCFG-LA baseline is a strong one, and is substantially hi gher than all previous reported results for Hebrew parsing in each of the setups (Seg +POS oracle, Seg Oracle, and no Oracle). We also identify some of its limitations, nam ely:
Missed splits. The learning procedure is not perfect, and fails to capture s ome linguis-tically meaningful state-splits. When such splits are manua lly supplied (i.e., the trivial split of verbal types) accuracy improves. 136
Sensitivity to non-gold POS. The substantial drop in accuracy when the POS tags are unobserved and need to be predicted is staggering, which sug gests that it is difficult for the parser to assign part-of-speech tags. Of the 698 part -of-speech errors, 314 are on words not seen in training.

Sensitivity to non-gold segmentation. The accuracy drops even further when the parser is presented with predicted segmentation. Segmentation er rors are detrimental to the parser.

Not encoding grammatical agreement. Finally, the learned grammar does not encode grammatical agreement. Whereas the majority of the parser mis takes are due to the flexible constituent order or  X  X tandard X  ambiguities such as coordination and PP attachment, a handful of them could be resolved using agreem ent information. parser accuracy for the realistic case where gold segmentat ion and POS tags are not available. 5. Manual State-Splits
We experimented with several linguistically motivated sta te-splits which were added as tree-annotations prior to running the parser. Most of them d id not help on their own and slightly degraded parser performance when combined with ot her splits. These include splits which were proven useful in previous work, such as mar king of definite NPs, and distinguishing possessive from other PPs. We also experime nted with splits based on morphological agreement features, which are discussed in S ection 8.1.
 did, however, manage to improve upon it with the following an notation (the annota-tions were removed prior to evaluation).

Subject NPs. Hebrew phrase order is rather flexible, and the subject can appea r before or after the verb. Identifying the subject can thus help in grou nding the overall structure of the sentence. The subject is also dependent on agreement co nstraints with the verb.
Following Johnson (1998), Klein and Manning (2003) implici tly annotate subject-NPs in English using parent annotation (distinguishing NPs und er S from other NPs), with good results. When applied to English, the PCFG-LA also learn s to model subject NPs well. Hebrew X  X  non-configurationality, however, put both Su bjects and Objects directly under S, making it much harder to learn the distinction autom atically.

Perhaps more important than the small increase in accuracy i s the fact that the parser can identify subjects relatively well. In contrast, marking of ob ject NPs did not help by itself and slightly degraded the parsing accuracy when combined wi th other annotations.
Note, however, that Hebrew definite objects are already clearl y marked using the marker, making them an easy target for the parser. 6. Better Lexical Coverage with an External Lexicon
The drop in parsing accuracy when gold core POS tags are not av ailable and need to be inferred by the parser is huge (from above 90 to less than 84 F
The large number of possible word forms make it very difficult for manually annotated corpora to provide adequate lexical coverage. The problem i s even more severe with the case of the Hebrew Treebank, which is especially small. Al though it is big enough to learn meaningful syntactic generalizations (as demonstra ted by the high performance of the baseline system) it is far too small to learn a good lexi cal model (as evidenced by the drop in accuracy when gold tags are not available).
 namely, a lexicon-based morphological analyzer. We furthe r extend the utility of the analyzer with lexical tagging probabilities learned from a n unannotated corpus. 6.1 A Unified Lexical Probability Model
We would like to use the KC Analyzer (Section 2.3.3) to increa se the lexical coverage of the treebank-trained parser. That is, we would like to imp rove the lexical model
P ( T  X  W ) of the generative parser. As discussed in Section 2.3.5, ho wever, the tag sets used by the two resources differ. How can this difference be re conciled? unified resource. In Goldberg et al. (2009), we show that this p rocedure degrades parser performance. Instead, Goldberg et al. suggest a layered gene rative approach that retains the benefits of the treebank tagging for frequent words and re sorts to the KC tag set only for rare and unseen words. Under this approach, frequent wor ds are generated from treebank POS tags as usual, but rare words follow a generativ e process in which first the treebank tag generates a KC tag, and then the KC-tag gener ates the word. A sample derivation using this layered representation is presented in Figure 1.
 mapping between the two resources. In Goldberg et al. (2009), the estimation of these probabilities was done based on a re-tagging of the treebank to use the KC tag set.
The re-tagging process was far from trivial, and many taggin g cases required extensive debates between human annotators.
 tagged with a new tag set. It still uses the layered representa tion, but instead of forcing one unique KC analysis for each location, it embraces the unc ertainty and allows all of them. This is done by treating the KC-tag assignments as hidd en variables, learning the TB-KC mapping probabilities as part of the grammar train ing EM process, and marginalizing the KC tags out for the final tree. The procedur e is based on the following assumptions: 138
Figure 2 illustrates the representation used for words whic h are rare or unseen in the treebank training data. The treebank tag NN TB (upper level) generates the word-form  X  X  X ! (lower level) by considering all the possible KC POS tags all owed for the word in the morphological analyzer (the middle level). The probabi lities related to generating the KC POS tags are summed, and all the other probabilities ar e multiplied. The exact equations are detailed in the following.
 it is more convenient (for a reason which will be discussed la ter) to work with the tagging probability P ( T TB | W ). Once the tagging probabilites P ( T they can easily be converted to emission probabilities usin g Bayesian inversion, based on the relative-frequency estimates of P ( W ) and P ( T TB treebank: 16 Let us now focus on estimating the tagging probabilities P ( T frequent, rare, and OOV words.

For frequent words that are seen more than K times in the treebank, we simply use treebank-based relative-frequency estimates: 17 where c ( ) is a counting function.

For OOV words that are not seen in the treebank, the tagging pr obability is estimated using: where P ( T Ext | W ) is a tagging probability using the external tag set, and P ( T a transfer probability relating the tags from the two tag set s (the estimation of these two probabilities is discussed subsequently). What this doe s is assume a process in which the word is tagged by first choosing a tag according to th e external lexicon, and then choosing a tag from the TB tag set based on the external on e. The external tag assignments are then treated as latent variables, and are ma rginalized out.
Finally, for rare words that are seen only a few times in the tr eebank, we interpolate the two quantities, weighted by the word X  X  frequency in the tree bank: We now turn to describing the estimation of the external tagg ing probability P ( T and the tag transfer probability P ( T TB | T Ext ).
 Estimating P ( T Ext | W ) . The tagging probability follows the morphological analyze r.
The analyzer provides the possible analyses, but does not pr ovide probabilities for them. One simple option would be to assign each possible anal ysis (tag) a uniform probability, and assign 0 probability for tags not allowed b y the lexicon for the given word. This method is referred to as P unif ( T Ext | W ). We know that not all the possible analyses for a given word are equally likely, however, and in practice, the actual tagging distribution is usually biased toward one or two of t he tags. These tagging preferences can be learned in an unsupervised manner given t he lexicon and a large corpus of unannotated text, using EM training of an HMM taggin g model. Adler and
Elhadad (2006) suggest such a model for accurate tagging of He brew, and Adler (2007) and Goldberg, Adler, and Elhadad (2008) extend it to provide state-of-the-art tagging accuracies for Hebrew using a smart initialization. Here, we u se the pseudo-counts from 140 the final round of EM training in this tagging model in order to compute P
We show in Section 9 that this unsupervised lexical probabil ities estimation does indeed provide better parsing results.
 between the syntactic tagging scheme of the treebank and the other tagging scheme of the external resource. They are estimated using treebank counts and the tagging distribution P ( T Ext | W ):
Integration into the PCFG-LA model. The estimation procedure is incorporated into the training process of the PCFG-LA model. Note that in the PCFG-LA model the treebank is a latent variable indicating a specific split of the given t ag. This means that the treebank tagging probability and the tag set X  X ransfer prob abilities are also defined over these split tags. Whereas the external tagging probabilitie s P ( T
PCFG-LA training, the other distributions ( P ( T TB estimated in the EM process following each of the split, merg e, and smooth stages. This is done by replacing the corpus counts c ( ) in Equations (2) and (5) with pseudo-counts (expectations, marginal scores) of the same events in the E s tep of the EM procedure. with the emission probability P ( W | T ) directly is that the emission probability is highly dependent on the vocabulary size. The treebank estim ates are based on a small vocabulary, the external lexicon estimates are based on a ve ry large vocabulary, and a proper combination of the two emission probabilities is no t trivial. In contrast, the tagging probabilities do not depend on the vocabulary size, allowing a very simple combination. We can then base the counts for the emission pro bability on the treebank vocabulary alone, and estimate P ( W ) for words unseen in training as if they were seen once. 7. Joint Segmentation and Parsing
When applied to real text (for which the gold word-segmentati on is not available), the baseline PCFG-LA parser is supplied with word segmentation produced by a separate tagging process. 18 This seriously degrades parsing performance. A major reason for the performance drop is that the word-segmentation task and the syntactic-disambiguation task are highly related. Segmentation mistakes drive the pa rser toward wrong syntactic structures, and many segmentation decisions require long-distance information that is not available to a sequential process (Tsarfaty 2006a). For these reasons, we claim that parsing and segmentation should be performed jointly. parsing over a fixed input string, the parser operates on a lat tice X  X  structure encoding all the possible segmentations. 7.1 Lattice Representation to the end state.
 represented using a lattice structure. Each lattice arc cor responds to a word and its corresponding POS tag, and a path through the lattice corres ponds to a specific word-segmentation and POS tagging of the sentence. This is by now a fairly standard repre-sentation for multiple morphological segmentations of Hebr ew utterances (Adler 2001; Bar-Haim, Sima X  X n, and Winter 2005; Adler 2007; Cohen and Smi th 2007; Goldberg,
Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldbe rg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010) and other lan guages (Smith, Smith, and Tromble 2005).
 indicate the space-delimited token boundaries. Note that i n this construction arcs can never cross token boundaries. Every token is independent of the others, and the sen-tence lattice is in fact a concatenation of smaller lattices , one for each token. Further-more, some of the arcs represent lexemes not present in the in put tokens (e.g.,  X  X ! / POS ), although these are parts of valid analyses of the token. Se gments with the same surface form but different POS tags are treated as different lexemes, and are represented as separate arcs (e.g., the two arcs labeled  X  X  X  X  X ! from node 6 to 7).
 the possible sentences resulting from an interpretation of an acoustic model. In speech ity of specific transitions. Given that weights on all outgoi ng arcs sum up to one, weights induce a probability distribution on the lattice paths. In se quential tagging models such as Smith, Smith, and Tromble (2005), Adler and Elhadad (2006 ), and Bar-Haim, Sima X  X n, and Winter (2008) weights are assigned according to a taggin g model based on linear context. For the case of parsing, context-free weighting of lattice arcs is used: each arc 142 corresponds to a h tag , word i pair, and is weighted according to the emission distributio n
P ( tag  X  word ). 20 7.2 Lattice Parsing
The CKY parsing algorithm can be extended to accept a lattice , instead of a predefined list of tokens, as its input (Chappelier et al. 1999). The CKY search then finds a tree spanning from the start-state to the end-state of the lattic e, where the leaves of the tree are lattice arcs. The lattice extension of the CKY algorithm is performed by indexing lexical items according to their start-and end-states in th e lattice instead of by their sentence position, and changing the initialization proced ure of CKY to allow terminal and preterminal symbols of spans of sizes &gt; 1. It is then relatively straightforward to modify the parsing mechanism to support this change: not giv ing special treatments for spans of size 1, and distinguishing lexical items from no n-terminals by a specified marking instead of by their position in the chart.
 parse over the lattice. The chart is initialized with parts o f speech corresponding to the lattice arcs. Phrase-structures are then built on top of the POS tags (in blue). The proposed structure must span the entire chart, and correspo nd to a path through the lattice from the initial state (0) to the last one (7).
 parser is trained as usual over the treebank. At inference (t est) time, the correct seg-mentation is unknown, and the decoding is applied to the segm entation lattice. The best derivation returned by the parser forces a specific segmenta tion. The returned parse tree is the most probable h segmentation , tree i pair according to the grammar. the PCFG-LA BerkeleyParser to accept lattice input at infer ence time.
 to the parser, instead of committing to a specific segmentati on prior to parsing. This way segmentation decisions are performed in the parser as pa rt of the global search for the most probable structure, and can be affected by globa l syntactic considera-tions. We show in Section 9 that this methodology is indeed su perior to the pipeline approach.

Lang (1974, 1988) and Billott and Lang (1989). Lattice parsi ng was explored in the context of parsing of speech signals by Chappelier et al. (19 99), Sima X  X n (1999), and
Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010). 8. Incorporating Morphological Agreement
Inspecting the learned grammars reveal that they do not encod e any knowledge of morphological agreement: The split categories for nouns, v erbs, and adjectives do not group words according to any relevant morphological proper ty such as gender or number, making it impossible for the grammar to model agreem ent patterns. At the same time, inspecting some of the bad parses reveals several clear cases of agreement mistakes. Can morphological agreement be incorporated in t he parsing model? 8.1 Forcing Morphologically Motivated Splits
Our initial attempts focused on making the PCFG-LA learning procedure pick up on agreement-relevant state-splits. When neither the core tag set nor the non-terminals encode gender and number information, it is very hard for the parser to pick up on agreement patterns. 22 the gender, the number, or both) either on the POS tags, the re levant constituents, or 144 both. Annotating agreement features on the POS tag X  X evel ma de the parsing much slower, but did make the parser assign certain split categor ies to certain gender X  X umber combinations, and sampling utterances from the learned gra mmar did indicate a notion of grammatical agreement. This did not improve parsing accu racy, however X  X nd even slightly degraded it.
 level, parsing accuracy dropped considerably. When inspect ing the learned grammar we observe that most of the agreement-annotated constituen ts (e.g., NP still fully split, indicating that the parser picked on patt erns which were orthogonal to the agreement mechanism. The pre-splitting according to ag reement-features properties caused data sparseness, aided over-fitting, and hurt parsin g performance: The smooth-ing procedure of the BerkeleyParser shares some probabilit y-mass between various splits of the same symbol, but was not applied in our case (no i nformation flowed effect by changing the smoothing mechanism of the BerkeleyP arser to share information also between the manually split symbols. This brought parsi ng accuracy back to the initial level, but also caused the parser to, again, not mode l agreement very well. The reason for this is clear in hindsight: Morphological agreem ent is an absolute concept, not a fuzzy one (things can either agree or not). Smoothing th e probabilities between the different morphology-based split-licensed grammar ru les that allow morphological disagreement, and made the grammar lose its discrimination power. This was then reinforced by the training process, which picked on other sy ntactic factors instead, and further phased out the agreement knowledge.

A note on product-grammars. In recent work, Petrov (2010) showed that a committee of latent-variable grammars encoding different grammatical preferences can be combined into a product-grammar that is better than the individual en semble members. Petrov created the ensemble by training several PCFG-LA parsers on the same data, but using different random seeds when initializing the EM starting po int. We attempted to cre-ate a similar ensemble by providing the learning process wit h different linguistically motivated tree annotations (with and without encoding agre ement features, with and without encoding definiteness, etc.). The combined parser d id increase the performance level over that of the individual parsers, but an ensemble wi th the same number of components that was produced using the random-seeds approa ch produced far su-perior results. This reinforces the findings of Petrov (2010 ) who also reports that the ensemble creation using random initialization is exceptio nally strong and outperforms other methods of ensemble creation. 23 8.2 Agreement as Filter
We now turn to suggest an approach to modeling agreement, whi ch rests on the follow-ing principles:
Based on these principles, we suggest treating agreement as a filter, a device that can rule out illegal parses. Under the agreement-as-filter fram ework, we want the parser to produce the most probable parse according to its grammar and subject to hard agreement constraints . This approach completely decouples the grammar from the ag reement ver-ification mechanism. The agreement information is not model ed in the grammar and is not used to guide the search for the best parse. Instead, it i s a separate process that imposes hard constraints on the search space and rules out pa rts of it completely. That is, agreement is a part of the parser and not of the grammar. Th is is similar in spirit to ideas from constraint-based grammars such as LFG (Falk 2001 ) and HPSG (Pollard and Sag 1994), which also model aspects of the syntax as Boolean c onstraints. logical features are propagated from one of the leaves up to t he constituent level.
When constituents are combined to form a larger constituent, their morphological features are assigned to the newly created constituent acco rding to language-specific rules (it is possible that different morphological feature s will be assigned by different constituents). An agreement violation occurs when two or mo re constituents assign conflicting features to their parent.

Implementation. In the implementation, an agreement-verification mechanism is man-ually constructed (not learned) based on a set of simple, lan guage-dependent rules.
First, we provide a set of rules to propagate the morphologic al agreement features from the leaves to the constituents. Then, we specify an addition al set of rules to inspect local tree configuration and identify agreement violations (the Hebrew set of rules is described later, along with a concrete example). The featur e-propagation mechanism works bottom X  X p and the agreement verification rules are ver y local, making it possible to integrate the filtering mechanism into a bottom X  X p CKY par sing algorithm (refusing to complete a constituent if it violates an agreement constr aint). We did not pursue this route for the experiments in this work, however. Instead, we o pted for an approximation in which we take the 100-best trees for each sentence, and cho ose the first tree that does not have an agreement violation (this is an approximati on because the 100-best trees may not contain a valid tree, in which case we accept the agreement violation and choose the first-best tree). The specific details of the Hebrew agreement filter are given in the appendix.

Verifying the hard-constraint property. We verified that the hard constraint assumption works and that the agreement verification mechanism is valid by applying the proce-dure to the gold-standard trees in the training-set and chec king that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation. We did find a few case s in which the prop-agated features disagreed with the manually marked ones, an d a few gold-standard trees that the mechanism marked as containing an agreement v iolation. All of these cases were due to mistakes in the manual annotation.

Connections to parse-reranking. Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model agreement as soft constraints, we could have incorporated t his information as features in a reranking model. The filter approach differs in that it po ses hard constraints and not soft ones, pruning away parts of the search space entirel y. Thus, the use of k -best list is merely a technical detail in our implementation X  X he a greement information is 146 easily decomposable and the hard constraints can be efficien tly incorporated into the
CKY search procedure. 9. Evaluation and Results
Data set. For all the experiments we use Version 2 of the Hebrew Treebank (Guthmann et al. 2009), with the established test-train-dev splits: S entences 484 X 5,740 are used for training, sentences 1 X 483 are the development set, and sent ences 5,741 X 6,220 are used for the final test set.

Evaluation Measure. In the cases where the gold segmentation is given, we use the we ll-known evalb F 1 score. Namely, each tree is treated as a set of labeled consti tuents. the first and the last words in the constituent, respectively , and L is the constituency label. For example, (2, 4, NP) indicates an NP spanning from w ord 2 to word 4. The performance of a parser is evaluated based on the amount of co nstituents it recovered correctly. Let G denote the set of constituents in a gold-standard constitue ncy tree, and
P denote the set of constituents in a predicted tree. Precisio n ( P ), recall ( R ), and F defined as:
F 1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the trees are identical. We report numbers in precentages rather than fractions.
 predicted and can contradict the gold-standard, a generali zation of these measures is sented by a pair containing the concatenation of the words at its yield, and its label L .
This measure was suggested by Tsarfaty (2006a) and used in su bsequent work (Tsarfaty 2006b; Goldberg and Tsarfaty 2008; Goldberg et al. 2009; Gol dberg and Elhadad 2011).
This is equivalent to reassigning the i and j indices to represent character positions instead of word numbers. When the yields of the gold standard a nd the predicted trees are the same, this is equivalent to the standard evaluation m easure using the h i , j , L i triplets of word indices and a label, and it will produce the s ame precision, recall, and
F 1 as above.
 model with an external lexicon, as described in Section 6.1. The rare-word threshold is set to 100. We use the morphological analyzer described in Section 2.3.3. We test two conditions: U NIFORM , in which the P ( T ext | w ) distribution is uniform over all the analyses suggested by the morphological analyzer for the wo rd, and H which the P ( T ext | w ) distribution is based on pseudo-counts from the final round of EM X 
HMM training of the semi-supervised POS tagger described in S ection 2.3.4. Results are presented in Table 4.
 tation is assumed to be known, as well as in the pipeline case w here the segmentation is automatically induced by a sequential tagger. Incorporatin g the semi-supervised lexical probabilities learned over large unannotated corpora (H MM the results, up to 86.1 F 1 for the gold-segmentation case and 78.7 F case. The pipeline model still lags behind the gold-segment ation case, indicating that the correct segmentation is very informative for the parser .

Joint segmentation and parsing. Having established that the external lexicon can be effec-tively incorporated into the parser, we turn to evaluate the method for joint segmenta-tion and parsing. We follow the same conditions as before (U lexical probabilities), but in this set of experiments the p arser is allowed to choose its preferred segmentation using the lattice-parsing methodo logy presented in Section 7.2.
The lattice is constructed according to the analyses licens ed by the morphological analyzer. Table 5 lists the results. Lattice parsing is effe ctive, leading to an improvement of about 2 X 3 F 1 points over the pipeline model.

Agreement filter. We now turn to add the agreement filtering on top of the lexicon -enhanced models. In this setting, the model outputs its 100-b est trees for each sentence, agreement features are propagated, and agreement violatio ns are checked as described 148 in Section 12, and the first tree that does not contain any agre ement violation is returned as the final parse for the sentence (or the first-best tree in ca se that all of the output trees contain an agreement violation). Table 6 lists the res ults when agreement filtering is performed on top of parses based on gold segmentation, and Table 7 lists the results when agreement filtering is performed on top of a lattice-bas ed parsing model that does not assume gold segmentation is available.

Discussion of agreement filter results. Although the agreement filter does not hurt the parser performance, the benefits from it are very small. T o understand why that is the case, we analyzed the 1-best parses produced by the 5-c ycles-trained grammar on the gold-segmented development set (these conditions corr esponds to the last column of the third row in Table 6). The analysis revealed the follow ing reasons for the low impact of the agreement filter: (1) The grammar is strong enou gh to produce fairly accurate structures, which have very few agreement mistake s to begin with, and (2) fixing an agreement mistake does not necessarily mean fixing t he entire parse X  X n some cases it is very easy for the parser to fix the agreement mistak e and still produce an incorrect parse for other parts of the structure.
 tree nodes. Of these 22,500 nodes, 2,368 nodes triggered a ge nder-agreement check: about 10% of the parsing decisions could benefit from gender a greement. Of the 2,368 relevant nodes, however, 130 nodes involved conjunctions or possessives, and were outside of the scope of our agreement verification rules. Of t he remaining 2,238 parse-tree nodes, 2,204 passed the agreement check, and only 34 nod es (1.5% of the relevant nodes, and 0.15% of the total number of nodes) were flagged as ge nder-agreement violations. Similarly for number agreement, 2,244 nodes tr iggered an agreement check, of which 2,131 nodes could be handled by our system. Of these r elevant nodes, 2,109 nodes passed the gender-agreement check, and only 23 nodes ( 1.07% of relevant nodes, and 0.1% of the total nodes) were flagged as agreement violatio ns. The numbers are summarized in Table 8. It is clear that the vast majority of the p arser decisions are compatible with the agreement constraints.
 violation, we note that the agreement filter marked 51 of the 4 80 development sentences as having an agreement violation in the 1-best parse X  X bout 10 % of the sentences could potentially benefit from the agreement filter. For 38 of the 51 agreement violations, the agreement violation was fixed in the tree suggested in the 100 -best list. We manually inspected these 51 parse trees, and highlight some the trend s we observed. In the 13 cases in which the 100-best list did not contain a fix to the a greement violation, the cause was usually that the 1-best parse had many mistakes that were not related to the agreement violation, and diversity in the 100-best li st reflected fixes to these mistakes without affecting the agreement violation. Anoth er cause of error was an erro-neous agreement mistake due to an omission in the lexicon. Of the 38 fixable agreement violations, 25 were local to a noun-phrase, 10 were cases of s ubject X  X erb agreement, and the remaining three were either corner-cases or harder t o categorize. The subject X  verb agreement violations were handled almost exclusively by keeping the s tructure mostly intact and changing the NPSUBJ label to some other closely related label that does not require verb agreement, usually NP . This is a good strategy for fixing subject-less sentences (about half of the cases), but it is only a partial fi x in case the subject should be assigned to a different NP (which does not happen in practi ce) or in case a more drastic structural change to the parse-structure is needed . In one of the 10 cases, the subject X  X erb agreement mistake indeed resulted in a structu ral change that improved the overall parse quality. The NP internal agreement violat ions include many cases of noun-compound attachments, and some cases involving coord ination. The corrections to the agreement violation were mostly local, and usually re sulted in correct structure, but sometimes introduced new errors. Figure 5 presents some examples of the different cases. Our overall impression is that for NP internal mistak es the agreement-filtering method was mostly doing the right thing.
 better parses, especially with respect to noun-compound co nstruct-state constructions.
Due to the limited number of parsing mistakes involving agre ement violations, how-ever, and because of the local nature of the agreement-viola tion mistakes, the total effect of the agreement filter on the final parsing score is small. 10. The Final Model
Finally, we evaluate the best performing model on the test se t. Table 9 presents the results of parsing the test set while incorporating the exte rnal lexicon and using the 150
HMM-based probabilities, for a grammar trained for four spli t-merge iterations. This grammar is applied both to the gold-segmentation case and to the realistic case where segmentation and parsing are performed jointly using lattic e-parsing. We also test the effectiveness of the agreement-filter in both situations.
 final accuracy X  X dditionally on the test sentences, the parse r makes very few agreement mistakes to begin with.
 than the development set. With gold-segmentation, the mode ls achieve accuracies of 85.70% F 1 . In the realistic scenario in which the segmentation is induc ed by the parser, the accuracies are around 76.9% F 1 . We verified that the HMM-based lexical probabili-ties also outperform the Uniform probabilities on the test s et (the F uniform lexical probabilities are 84.06 and 76.30 for the go ld and induced segmenta-tions, respectively). These are the best reported results f or parsing the test-set of the
Hebrew Treebank. 11. Related Work in Parsing of Morphologically Rich Language s
Coping with unknown words. Several papers show that the handling of unknown words is a major component to be considered when adapting a parser to a new language.
For example, the work in Attia et al. (2010) uses language-sp ecific unknown-word signatures for several languages based on various indicati ve prefixes and suffixes, and
Huang and Harper (2009) suggest a Chinese-specific model based on the geometric average of the emission probabilities of the individual cha racters in the rare or unknown word. Another method of coping with lexical sparsity is word clustering. In Candito and Crabb  X  e (2009), the authors demonstrate that replacing words by a c ombination of a morphological signature and a word-cluster (based on the l inear context of a word in a large unannotated corpus) improves parsing performance f or French. The technique provides more reliable estimates for in-vocabulary words ( a given cluster appears more frequently than the actual word form), and it also increases the known vocabulary: Unknown words may share a cluster with known words.

Arabic. Arabic is similar to Hebrew in the challenges it presents for a utomatic pars-ing. Most early work on constituency parsing of Arabic focus ed on straightforward adaptations of Bikel X  X  parser to Arabic, with little empiri cal success. Attia et al. (2010) show that parsing accuracies of around 81% F 1 can be achieved for Arabic (assuming gold word segmentation) by using a PCFG-LA parser with Arabi c-specific unknown-word signatures. Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refin ements, and report pars-ing accuracies of 79% F 1 using the Stanford-parser and 82% F BerkeleyParser, both when assuming gold word segmentation . The work of Green and
Manning also explored the use of lattice-parsing as suggest ed in Section 7 of this article, as well as earlier in Goldberg and Tsarfaty (2008) and Cohen a nd Smith (2007), and report promising results for joint segmentation and parsing of Arabic (an F 76% for sentences of up to 70 words). The best reported result s for parsing Arabic when the gold word segmentation is not known, however, are ob tained using a pipeline model in which a tagger and word-segmenter is applied prior t o a manually state-split constituency parser, resulting in an F-score of 79% F 1 (for sentences of up to 70 words) (Green and Manning 2010). 152
Hebrew and relational-realizational parsing. Some related work deals directly with con-stituency parsing of Modern Hebrew. The work of Tsarfaty and S ima X  X n (2007) experi-ments with grammar refinement for Hebrew, and shows that annot ating definiteness and accusativity of constituents, together with parent ann otation, improves parsing accuracy when gold word segmentation is available.
 and Sima X  X n 2008; Tsarfaty, Sima X  X n, and Scha 2009; Tsarfat y and Sima X  X n 2010; Tsarfaty 2010) handles the constituent-order variation in Hebrew by p resenting a separation between the form and function aspects of the grammar. Briefly, whereas plain treebank-derived grammars have rules such as S  X  NP VP PP NP PP that are applied in a single step, the RR approach suggests a generative model in w hich the generation of a non-terminal generates the kinds of its children without s pecifying their form or the order between them, using rules of the form S  X  { OBJ,SBJ,PRED,COM,Adjunct } @S.
Second, in the configuration step, an order is chosen based on a separate ordering distribution, using rules of the form { OBJ,SBJ,PRED,COM,Adjunct } @S  X  SBJ@S PRED@S Adj@S OBJ@S COM@S.
 of the form SBJ@S  X  NP or Adj@S  X  PP. The realization rules can encode syntactic properties that are required by the grammar for the given fun ction X  X or example, a rule such as OBJ@S  X  NP def , acc captures the requirement that definite objects in Hebrew must be marked for accusativity using the  X  X ! marker, and the rest if the generative process will generate the object NP according to this specifie d constraint. This kind of linguistically motivated separation of form and function i s shown to produce models with fewer parameters and result in better parsing accuraci es than plain (or head-driven) PCFGs derived from the same trees.
 shown in Tsarfaty and Sima X  X n (2010) that, given gold-stand ard POS tags that include the gender and number information for individual words, RR m odels enriched with gender and number agreement information can provide Modern Hebrew parsing ac-curacies of 84% F 1 for sentences of up to 40 words, the highest reported number f or
Modern Hebrew parsing based on gold POS tags and word-segment ation by the time of its publication.
 ically, in the current work we chose to rely on the extreme mar kovization employed by the PCFG-LA BerkeleyParser in order to cope with the constit uent order variation, and to model agreement as an external filter that is orthogonal to the grammar. The approach taken in this article provides state-of-the-art results fo r Hebrew constituency parsing.
We leave the question of integrating the RR approach with the approach presented here to future work. 12. Conclusions
We presented experiments on Hebrew Constituency Parsing bas ed on the PCFG-LA methodology of Petrov et al. (2006). The PCFG-LA model perfo rms well out-of-the-box, especially when the gold POS tags are available to the parser . It is possible to improve the learned grammar, however, by specifying some manual sta te-splits, specifically distinguishing between modal, finite, and infinitive verbs, and explicit marking of subject-NPs.
 drop even further when using non-gold segmentation. A large part of the drop when the gold POS tags are not available is due to the large percent age of lexical events that are unseen or seen only a few times in the training set. This dr op can be mitigated by extending the lexical coverage of the parser using an exte rnal lexical resource such as a wide-coverage morphological analyzer for mapping lexi cal items to their possible
POS tags. The POS-tagging schemes assumed by the treebank an d the morphological analyzer need not be compatible with each other: We present a method for bridging the POS tags differences between the two resources. The morp hological analyzer does not provide lexical probabilities. Parsing accuracies can be further improved by using lexical probabilities which are derived in a semi-supervis ed fashion based on the mor-phological analyzer and a large corpus of unannotated text.
 and when the gold segmentation is not available, parsing res ults drop considerably.
It is better to let the parser induce its preferred segmentati on in interaction with the parsing process rather than to use a segmentation based on an external sequence model in a pipeline fashion. The joint induction of both the syntact ic structure and the token-segmentation can be performed by representing the possible segmentations in lattice structure, and using lattice parsing. Joint parsing and seg mentation is shown to outper-form the pipeline approach. The parsing accuracies with non -gold segmentation are still far below the accuracies when the gold-segmentation is assu med to be known, however, and accurate parsing with non-gold segmentation remains a c hallenging open research problem.

We considered methods of using morphological agreement inf ormation to improve parsing accuracy. We propose modeling agreement informati on as a filtering process that is orthogonal to the grammar used for parsing. The appro ach works in the sense that, in contrast to other methods of using agreement inform ation, it does not degrade parsing accuracy and even improves it slightly. The benefit f rom the agreement filtering is small, however: With the strong grammar induced by the PCF G-LA training pro-cedure, the parser makes very few agreement mistakes to begi n with. Modeling mor-phological agreement is probably more useful in syntactic g eneration than in syntactic parsing. We expect the filtering approach we propose to be pro ven useful for tasks involving syntactic generation, such as target-side-synt ax machine translation into a morphologically rich language.
 to adapt it to parsing Hebrew: the introduction of manual, lin guistically motivated state-splits; extending the lexical coverage of the parser using an external morpho-logical analyzer; performing segmentation and parsing join tly using a lattice parser; and incorporating agreement information in a filtering fram ework. Together, these enhancements result in the best published results for Hebrew Constituency Parsing to date.
 Appendix A: The Hebrew Agreement Filter
Hebrew syntax requires agreement in gender, number, and pers on. The implementation considers only the gender and number features, which are the most common. Each of 154 the features can take one of five values Masculine , Feminine , Both , Unknown , and NA for Gender , and Singular , Plural , Both , Unknown and NA for Number . Masculine, Feminine,
Singular, and Plural are self-explanatory, and are assigne d when the feature value is obvious. NA means that the feature is irrelevant for the give n constituent (adverbs and PPs do not carry gender or number features). Both and Unkn own are assigned when we are uncertain about the corresponding feature value . Both and Unknown are identical in the sense that they leave the feature value unsp ecified, and have the same effect on the filtering process. From a practical perspectiv e they could be collapsed into the same category. We chose to maintain the distinction between the two cases because they have slightly different semantics. Both indic ates that both options are possible (for example, the form  X  X  X  X  X ! is ambiguous between the plural girls and the singular childhood , and the titular  X  X ! , Dr. can refer both to males and females), whereas
Unknown means that the feature value could not be computed du e to a limitation of the model (for example, there is no clear rule as to the gend er of a conjunction which coordinates masculine and feminine NPs, and we are cur rently unable to accu-rately infer the gender and number associated with certain c omplex quantifiers such as  X  X  X ! ( most ). Compare:  X  X  X   X  X  X  X  X  X   X  X  X  X  X ! ,  X  X  X   X  X  X  X  X   X  X  X  X  X  X  X ! the class fem stayed masc , most of the cake fem was eaten compatible with NA, Both, and Unknown but not with Masculine . Similarly, Singular is compatible with NA, Both, and Unknown, but not with Plural.

Agreement cases. The system is designed to handle the following cases of morph ological agreement:
NP level agreement between nouns and adjectives.  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X !
S level agreement between subject and verbs.  X  X  X   X  X  X  X  X  X   X  X  X !
Predicative agreement between the subject, ADJP, and copul ar element.
Agreement between the Verb in a relativized SBAR and the realization of the Null-
Morphological feature propagation. The first step of determining agreement is propagating the relevant features from the leaves up to the constituent l evel.
 are assigned based either on the TB tag assigned for the word i f training on gold
POS tags, or on the morphological analyzer entries for the gi ven word (in most cases the number and gender features are easy to predict, even in ca ses where the core
POS is not clear. In the relatively rare cases where the analyz er contains both a fem-inine and masculine (alt. singular and plural) analyses, fe ature value is marked as
Both). according to a set of rules such as the following (the complet e set of rules is given in
Table A.1):
Agreement rules. Once the features are propagated from the leaves to a constit uent, agreement is verified at the constituent level according to t he following rules:
NP agreement rules: 156 (a) (c) (a) (c)
S agreement rule:
ADJP agreement rule:
An example. Consider the tree in Figure A.1a. In the first stage (Figure A.1 b), agreement features are propagated according to the rules in Table A.1, resulting in the annotated tree in Figure A.1c. Agreement is then validated in Figure A. 1d (nodes in which an agreement rule applied and passed are marked in green). In con trast, the tree in Fig-ure A.2a has an agreement mistake. As before, the agreement f eatures are propagated according to the rules (Figure A.2b) resulting in Figure A.2 c. Agreement validation fails at Figure A.2d (the node in which agreement validation was applied and failed is marked in red).
 References 158
