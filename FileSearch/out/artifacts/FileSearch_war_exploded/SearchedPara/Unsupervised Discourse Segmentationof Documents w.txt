 Many documents consist of parts exhibiting a high degree of parallelism: e.g., abstract and body of academic publications, summaries and detailed news stories, etc. This is especially common with the emergence of the Web 2.0 technologies: many texts on the web are now accompanied with com-ments and discussions. Segmentation of these par-allel parts into coherent fragments and discovery of hidden relations between them would facilitate the development of better user interfaces and im-prove the performance of summarization and in-formation retrieval systems.

Discourse segmentation of the documents com-posed of parallel parts is a novel and challeng-ing problem, as previous research has mostly fo-cused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). The most straightforward approach would be to use a pipeline strategy, where an existing segmentation algorithm finds discourse boundaries of each part independently, and then the segments are aligned. Or, conversely, a sentence-alignment stage can be followed by a segmentation stage. However, as we will see in our experiments, these strategies may result in poor segmentation and alignment quality.

To address this problem, we construct a non-parametric Bayesian model for joint segmenta-tion and alignment of parallel parts. In com-parison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Hal-liday and Hasan, 1976) in modeling the paral-lel parts of documents, and (2) ensures that the effective number of segments can grow adap-tively. Lexical cohesion is an idea that topically-coherent segments display compact lexical distri-butions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assump-tion.

In this paper, we consider the dataset of  X  X n-glish as a second language X  (ESL) podcast 1 , where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an ex-planatory lecture discussing the meaning and us-age of English expressions appearing in the story. Fig. 1 presents an example episode, consisting of two parallel parts, and their hidden topical rela-tions. 2 From the figure we may conclude that there is a tendency of word repetition between each pair of aligned segments, illustrating our hypothesis of compactness of their joint distribution. Our goal is to divide the lecture transcript into discourse units and to align each unit to the related segment of the story. Predicting these structures for the ESL pod-cast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmenta-tion model (Eisenstein and Barzilay, 2008) for iso-lated texts, to the problem of segmenting parallel parts of documents.

The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum  X  e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum  X  e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation.

We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similar-ity functions rather than on modeling lexical cohe-sion in the generative framework. Our application, the analysis of the ESL podcast, was previously studied in (Noh et al., 2010). They proposed a su-pervised method which is driven by pairwise clas-sification decisions. The main drawback of their approach is that it neglects the discourse structure and the lexical cohesion phenomenon. In this section we describe our model for discourse segmentation of documents with inherently paral-lel structure. We start by clarifying our assump-tions about their structure.

We assume that a document x consists of K parallel parts, that is, x = { x ( k ) } k =1: K , and each part of the document consists of segments, x ber of fragments I is unknown. Each segment can either be specific to this part (drawn from a part-specific language model  X  ( k ) i ) or correspond to the entire document (drawn from a document-level language model  X  ( doc ) i ). For example, the first and the second sentences of the lecture transcript in Fig. 1 are part-specific, whereas other linked sentences belong to the document-level segments. The document-level language models define top-ical links between segments in different parts of the document, whereas the part-specific language models define the linear segmentation of the re-maining unaligned text.

Each document-level language model corre-sponds to the set of aligned segments, at most one segment per part. Similarly, each part-specific lan-guage model corresponds to a single segment of the single corresponding part. Note that all the documents are modeled independently, as we aim not to discover collection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment.

Unlike (Eisenstein and Barzilay, 2008), we can-not make an assumption that the number of seg-ments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de-fine priors on the number of segments. We incor-porate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM (  X  ) defined by the stick breaking construction (Sethu-raman, 1994), where  X  is the concentration param-eter of the underlying DP distribution. GEM (  X  ) defines a distribution of partitions of the unit inter-val into a countable number of parts.

The formal definition of our model is as follows: estimated at learning time using non-informative hyperpriors (as we do in our experiments), or set manually to indicate preferences of segmentation granularity.

At inference time, we enforce each latent topic z n to be assigned to a contiguous span of text, assuming that coherent topics are not recurring across the document (Halliday and Hasan, 1976). It also reduces the search space and, consequently, speeds up our sampling-based inference by reduc-ing the time needed for Monte Carlo chains to mix. In fact, this constraint can be integrated in the model definition but it would significantly compli-cate the model description. As exact inference is intractable, we follow Eisen-stein and Barzilay (2008) and instead use a Metropolis-Hastings (MH) algorithm. At each iteration of the MH algorithm, a new potential alignment-segmentation pair ( z 0 , t 0 ) is drawn from Figure 2: Three types of moves: (a) shift, (b) split and (c) merge. is the current segmentation and its type. The new pair ( z 0 , t 0 ) is accepted with the probability
In order to implement the MH algorithm for our model, we need to define the set of potential moves (i.e. admissible changes from ( z , t ) to ( z 0 , t 0 and the proposal distribution Q over these moves. If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). In our case, however, a more complex set of moves is required.

We make two assumptions which are moti-vated by the problem considered in Section 5: we assume that (1) we are given the number of document-level segments and also that (2) the aligned segments appear in the same order in each part of the document. With these assumptions in mind, we introduce two additional moves (Fig. 2(b) and (c)):  X  Split move : select a segment, and split it at  X  Merge move : select a pair of adjacent seg-All the moves are selected with the uniform prob-ability, and the distance c for the shift move is drawn from the proposal distribution proportional dently for each part.

Although the above two assumptions are not crucial as a simple modification to the set of moves would support both introduction and deletion of document-level fragments, this modification was not necessary for our experiments. 5.1 Dataset and setup Dataset We apply our model to the ESL podcast dataset (Noh et al., 2010) of 200 episodes, with an average of 17 sentences per story and 80 sen-tences per lecture transcript. The gold standard alignments assign each fragment of the story to a segment of the lecture transcript. We can induce segmentations at different levels of granularity on both the story and the lecture side. However, given that the segmentation of the story was obtained by an automatic sentence splitter, there is no reason to attempt to reproduce this segmentation. There-fore, for quantitative evaluation purposes we fol-low Noh et al. (2010) and restrict our model to alignment structures which agree with the given segmentation of the story. For all evaluations, we apply standard stemming algorithm and remove common stop words.
 Evaluation metrics To measure the quality of seg-mentation of the lecture transcript, we use two standard metrics, P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). Consequently, we also use the macro-averaged F 1 score on pairs of aligned span, which measures both the segmentation and align-ment quality.
 Baseline Since there has been little previous re-search on this problem, we compare our results against two straightforward unsupervised base-lines. For the first baseline, we consider the pairwise sentence alignment (SentAlign) based on the unigram and bigram overlap. The sec-ond baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story.
 Our model We evaluate our joint model of seg-mentation and alignment both with and without the split/merge moves. For the model without these moves, we set the desired number of seg-ments in the lecture to be equal to the actual num-ber of segments in the story I . In this setting, the moves can only adjust positions of the seg-ment borders. For the model with the split/merge moves, we start with the same number of segments I but it can be increased or decreased during in-ference. For evaluation of our model, we run our inference algorithm from five random states, and Table 1: Results on the ESL podcast dataset. For all metrics, lower values are better. take the 100,000th iteration of each chain as a sam-ple. Results are the average over these five runs. Also we perform L-BFGS optimization to auto-matically adjust the non-informative hyperpriors after each 1,000 iterations of sampling. 5.2 Result Table 1 summarizes the obtained results.  X  X ni-form X  denotes the minimal baseline which uni-formly draws a random set of I spans for each lec-ture, and then aligns them to the segments of the story preserving the linear order. Also, we con-sider two variants of the pipeline approach: seg-menting the lecture on I and 2 I + 1 segments, re-spectively. 3 Our joint model substantially outper-forms the baselines. The difference is statistically significant with the level p &lt; . 01 measured with the paired t-test. The significant improvement over the pipeline results demonstrates benefits of joint modeling for the considered problem. Moreover, additional benefits are obtained by using the DP priors and the split/merge moves (the last line in Table 1). Finally, our model significantly outper-forms the previously proposed supervised model (Noh et al., 2010): they report micro-averaged F 1 score 0.698 while our best model achieves 0.778 with the same metric. This observation confirms that lexical cohesion modeling is crucial for suc-cessful discourse analysis. We studied the problem of joint discourse segmen-tation and alignment of documents with inherently parallel structure and achieved favorable results on the ESL podcast dataset outperforming the cas-caded baselines. Accurate prediction of these hid-den relations would open interesting possibilities for construction of friendlier user interfaces. One example being an application which, given a user-selected fragment of the abstract, produces a sum-mary from the aligned segment of the document body.
 The authors acknowledge the support of the Excellence Cluster on Multimodal Computing and Interaction (MMCI), and also thank Mikhail Kozhevnikov and the anonymous reviewers for their valuable comments, and Hyungjong Noh for providing their data.

