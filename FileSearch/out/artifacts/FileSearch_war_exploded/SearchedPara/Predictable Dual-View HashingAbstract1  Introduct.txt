 Mohammad Rastegari mrastega@cs.umd.edu Jonghyun Choi jhchoi@umd.edu Shobeir Fakhraei shobeir@cs.umd.edu Hal Daum  X e III me@hal3.name Larry S. Davis lsd@umiacs.umd.edu University of Maryland, College Park, MD 20742 USA Binary codes are attractive representations of data for search and retrieval purposes due to their efficiency in computation and storage capacity. For example, 64-bits binary codes can index about 10 19 images, five times the estimated amount of data created in 2002 and quite likely the total number of digital images in existence (Lyman et al., 2003).
 Hashing is a common method for assigning binary codes to data points ( e.g. , images). The binary codes are used as hash keys where the hash functions are learned to preserve some notion of similarity in the original feature space. Such binary codes should have the general hash property of low collision rates. In ad-dition, suitable binary codes for search and retrieval should also maintain high collision rates for similar data points. The latter property is essential in a sim-ilarity based retrieval settings (Gionis et al., 1999b; Gong &amp; Lazebnik, 2011; Weiss et al., 2008). The binary codes can be learned either in a unsuper-vised manner that models the distribution of samples in the feature space (Weiss et al., 2008) or in a super-vised manner that uses labels of the data points (Liu et al., 2012). Unsupervised methods can be adversely affected by outliers in distributions and noise, and the supervised methods require expensive manual labeling. It is often the case that information about data are available from two or more views, e.g. , images and their textual descriptions. It is highly desirable to embed information from both domains in the binary codes, to increase search and retrieval capabilities. Utilization of such binary codes will create a cross-view Hamming space with the ability to compare informa-tion from previously incomparable domains. For ex-ample in the text and image domain, image-to-image, text-to-image, and image-to-text comparisons can be preformed in the same cross-view space. Such ap-proaches have received attention recently due to the emergence of large amounts of data in different do-mains being available on the internet.
 To date, most approaches proposed embedding dual-views in Hamming space use canonical correlation analysis (CCA) (Hardoon et al., 2003; Hwang &amp; Grau-man, 2010; 2012). The CCA based approaches are less sensitive to feature noise and require no manual label-ing. However, bits learned by CCA do not explicitly encode the proximity of samples in the original feature space since CCA enforces orthogonal bases and aims to reduce the modality gap with little consideration of the underlying data distribution.
 To address this issue, we propose a dual-view mapping algorithm that represents the distribution of the sam-ples with non-orthogonal bases inspired by a notion of predictability proposed in (Rastegari et al., 2012). Pre-dictable codes ensure that small variations of the data point positions in the original space should not result in different binary codes. In other words, a particular bit in the binary code should be identical (predictable) for all data samples that are close to each other in each view. To maintain such predictability, we employ a max-margin formulation that enforces confident pre-diction of bits.
 Furthermore, we propose a joint formulation for learn-ing binary codes of data from two different views. We assume that a latent Hamming space exists for the data, and optimize the hash functions that map the data from each view to this common space, while main-taining the predictability of the binary codes. Know-ing the hash functions in the original views supports cross-modal searches.
 The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 presents the details of our approach including optimization methods. Exper-imental analysis and comparisons to state-of-the-art methods are presented in section 4 and we conclude in 5. As our work lies in the intersection of hashing methods and mutil-view embedding, we briefly describe related work in both domains. We also review specific appli-cations that could be enabled via our method.
 Gionis et al. (Gionis et al., 1999a) introduced Locality Sensitive Hashing (LSH) where similar objects have high probability of collision. Along this direction, Shaknarovich et al. (Shakhnarovich et al., 2003) use parameter sensitive hashing and apply it to human pose estimation. Kulis and Grauman (Kulis &amp; Grau-man, 2009) extend LSH with kernels and show fast image search for example-based searches and content based retrieval. Kulis and Darrell (Kulis &amp; Darrell, 2009) also proposed a binary reconstructive embed-ding method for minimizing the differences between Euclidean distances in the original feature space and the Hamming distances in the resulting binary space. Semantic hashing, proposed in (Salakhutdinov &amp; Hin-ton, 2009), learns compact binary codes that preserve correlations between distances in the Hamming space and semantic similarities approximated by category memberships. This is accomplished by learning a deep generative model, called a Restricted Boltzman Ma-chine (RBM) which has a small number of nodes in a deepest level that produce a small number of binary values. Torralba et al. (Torralba et al., 2008) extend this idea to efficient image search method on the scale of millions of images. Nonlinear mapping to binary codes has been addressed in (Salakhutdinov &amp; Hin-ton, 2007) by stacking multiple RBM X  X . Norouzi and Fleet (Norouzi &amp; Fleet, 2011) model the problem of su-pervised learning of compact similarity-preserving bi-nary code using a Latent SVM problem and define a hashing-specific class of loss functions. None of these approaches, however, necessarily captures the seman-tics of an image. In fact, enforcing preservation of pat-terns in the original feature space may hurt discrimi-nation in both supervised and unsupervised methods. Utilization of textual captions for image understand-ing has recently received considerable attentions in the research community. Farhadi et al. (Farhadi et al., 2010) introduce a CRF based method to model a se-mantic space that text and images can be mapped to via triples of object, subject and verb. In (Rashtchian et al., 2010) strategies of creating image-text datasets via Amazon Mechanical Turk are investigated. Kulka-rni et al. (Kulkarni et al., 2011) propose a method for generating natural language descriptions from images by parsing a large set of texts and performing object recognition on image sets. Li et al. (Li et al., 2011) propose a simple but effective N-gram based method that can produce simple descriptions of pictures. The generated descriptions are not identical to the text corpora, i.e. , they compose a sentence entirely from scratch. Recently, several works presented methods for Multi-Modal hashing (Masci et al., 2012; Zhen &amp; Ye-ung, 2012; Kumar &amp; Udupa, 2011); most of them hav-ing high computational complexity which limits their applicability.
 Ordonez et al. (Ordonez et al., 2011) created a large-scale dataset of images and captions, and proposed a method for generating textual captions for images from this dataset. A method for recognition of visual texts and non-visual texts is proposed in (Dodge et al., 2012). Kuznetsova et al. (Kuznetsova et al., 2012) use multiple noisy captions for images from the web and combine them to produce a more meaningful sentence for an image. Berg et al. (Berg et al., 2012) approach the problem of text generation to emphasize the visu-ally salient aspects of an image. Without loss of generality, we assume that the two views are visual (image) and textual (description). However, our approach is applicable to any domain, and this assumption only facilitates the discussion. We use the following notation; X V represents data in the visual space and X T indicates data in the textual space. X  X  is a d  X   X  n matrix whose columns are vectors corresponding to the points in either spaces. d  X  is the dimension of either visual or textual space which might be different. x i  X  is the i th column of X  X  .  X  is a placeholder for V or T . 3.1. Dual-View Embedding Our goal is to find two sets of hyperplanes W V ,W T  X  R  X   X  k ( k is the dimension of the common subspace, i.e. , length of binary code) that map the visual and textual space into a common subspace. Each hyper-plane (each column of W  X  ) divides the corresponding space into two subspaces; each point in a space is rep-resented as -1 or 1 depending on which side of the hy-perplane it lies in. w i  X  indicates the i th column of W Among the infinite possible hyperplanes, the ones that binarize the points in the visual space and the textual space consistently are desirable for our purpose. This objective can be achieved by minimizing the following function: However, Eq.(1) is a non-convex combinatorial opti-mization problem; it has a trivial solution when both W
V and W T are zero. To avoid the trivial solution and force each bit to carry the maximum amount of infor-mation, we add constraints to enforce low correlation of the bits. With these constraints, we can reformulate the problem as: where minimizing k B  X  B T  X   X  I k 2 2 enforces low correla-tion of bits. This optimization cannot be directly solv-able, but it can be solved approximately by relaxing B  X  (Gong et al., 2012) and applying CCA (Hardoon et al., 2003), which leads to the following generalized eigenvalue problem: where S VT (= X V X T T ) is the covariance matrix be-tween visual and textual features and w  X  is a column of W  X  .
 Although CCA can find the underlying subspace, bi-narizing data in this subspace by sgn( W T  X  X  X  ) suffers from high quantization error. To reduce the quantiza-tion error, an iterative method is proposed in (Gong &amp; Lazebnik, 2011) that searches for a rotation of data points. Their approach, however, is not applicable to more than one domain. In addition, the approach assumes orthogonality of all of the projected hyper-planes, i.e. , the columns of W  X  . But the orthogonality is not always necessary and sometimes harmful. In contrast, we replace orthogonality of the hyperplanes by the notion of predictability of binary codes in the following section. 3.2. Predictability Predictability is the ability to predict the value of a certain bit of a sample by looking at that bit of the nearest neighbors of that sample. For example, if the i th bit in most of the nearest neighbors of a sample is 1 then we would predict that the i th bit of that sample would be also 1 .
 Consider the situation where a hyperplane crosses a dense area of samples; there would be many samples in proximity to each other that are assigned differ-ent binary values in the bit position corresponding to that hyperplane. Such binary values obtained by that hyperplane are not predictable . Intuitively, the binary values determined by a hyperplane are pre-dictable when the hyperplane has large margins from samples. Figure 1 illustrates the hyperplanes deter-mined by CCA in green lines in a 2D single domain (view). Note that CCA hyperplanes cross dense areas of samples and are orthogonal to each other whereas our PDH hyperplanes do not. If we binarize the sam-ples by CCA hyperplanes, samples in the red circle will have different binary codes from each other, even though they are strongly clustered. The hyperplanes that are shown by orange lines represent our method (PDH), which enforces large margins from samples. To learn the predictable W , we regularize the formu-lation with max-margin constraints. In fact, we learn multiple SVMs in visual space with respect to training labels in the textual space and vice versa. The final objective function is: s.t.
 B T = sgn( W T T X T ) , B V = sgn( W T V X V ) , Despite the complex appearance of the optimization, it is a perfect setting for block-coordinate descent and can be solved by an Expectation Maximization (EM) iterative algorithm. A detailed description of our iter-ative algorithm is as follows: First , we fix all the variables except W V and  X  V Then we solve for these variables, which is multiple linear SVMs; one for each bit. To learn the i th SVM, we use columns of X V as training data and the ele-ments of the i th row of B T as training labels. Sec-ond , using the outputs of these SVMs, W V , we com-pute B V = sgn( W V T X V ). Third , we update B V to minimize the correlation between bits via minimizing k B V B T V  X  I k 2 2 . Since this problem is not trivial to solve, we use spectral relaxation (Weiss et al., 2008) by cre-ating a Gram matrix S = B T V B V and a n  X  n diagonal matrix D ( i,i ) = P j S ( i,j ) as the relaxed problem: The solutions are the k eigenvectors of D  X  S with minimal eigenvalues, which we binarize by taking the sign of the elements. Fourth , we run the same three steps to compute W T . We repeat all the steps until convergence of the objective function. More details of the algorithm are provided in Algorithm 1 For initializing values for optimization, we tried several random values and the values obtained using CCA. But the results are not sensitive to the initialization, since in each block coordinate descent step, the ob-jective function is convex. Thus, we use the values obtained by CCA for all initializations.
 Since our objective function is not convex and we use block coordinate descent to optimize, the solution we obtain is not the global minimum. But our experi-ments suggest that the obtained local minima is good enough.
 Algorithm 1 Predictable Dual-View Hashing First, we show that our optimization algorithm solves the proposed objective functions. Then for the em-pirical validation, we present both quantitative and qualitative results for image category retrieval. In the quantitative analysis, we perform image classification and compare the mean average precision (mAP) ob-tained by our method with several state-of-the-art bi-nary code methods. In qualitative analysis, we show that the sets of images retrieved by our binary code with both image and text queries contain semantically similar images. Our MATLAB software is available 1 . 4.1. Datasets and Experimental Setup For the dual-view situation, we need a dataset of images that are annotated with sentences. We use two datasets; PASCAL-Sentence 2008 introduced by (Farhadi et al., 2010) (one view is visual and the other is textual) and a recently collected large scale dataset, SUN-Attribute database (one view is visual and the other is semantic (attribute)) (Patterson &amp; Hays, 2012). 4.1.1. PASCAL-Sentence Dataset 2008 The images in the PASCAL-Sentence dataset are col-lected from PASCAL 2008, which is one of the most popular benchmark datasets for object recognition and detection. For each of the 20 categories of the PAS-CAL 2008 challenge, 50 images are randomly selected; in total, there are 1,000 images in the dataset. Each image is annotated with 5 sentences using Amazon X  X  Mechanical Turk. These sentences represent the se-mantics of the image.
 Image Features: Our image features, following (Farhadi et al., 2010), are collections of responses from a variety of detectors, image classifiers and scene clas-sifiers. Given an image, we run several object detectors on the image and set the threshold low enough so that each fires at least in one location. Then, we report the location of the most confident detector along with the confidence value. If we have 20 detectors, for each of the detectors we report [ x i ,y i ,c i ] which x the coordinate of the location at which the detectors fired and c i is the confidence value for that detector. Image and scene classifiers are SVMs trained on each category of objects on the global low-level GIST de-scriptor (Oliva &amp; Torralba, 2001).
 Text Feature: Text features are also from (Farhadi et al., 2010). We construct a dictionary of 1,200 words from the sentences of the entire dataset that are fre-quent and discriminative with respect to categories. There are no prepositions and stop words in the dic-tionary. Let us call this set S . For a given sentence, we go through each word and compute its semantic similarity with all the words in S as a feature for that word. As a feature of the sentence, we simply sum all the vectors in each sentence. The semantic distance between two words is computed by the Lin similarity measure (Lin, 1998) on the WordNet hierarchy. 4.1.2. SUN Attribute Dataset The SUN-Attribute dataset is a large-scale dataset (Patterson &amp; Hays, 2012) that includes 102 attribute labels annotated by 3 Amazon Mechan-ical Turk worker for each of the 14,340 images from 717 categories, which is a subset of the scene images from the SUN Dataset (Xiao et al., 2010). In total, there are four million (4M) labels. For each of 717 categories, there are 20 annotated scenes.
 Image Features: We use the precomputed image fea-tures used in (Patterson &amp; Hays, 2012; Xiao et al., 2010), i.e. , Gist, 2  X  2 Histogram of Oriented Gradient, self-similarity measure, and geometric context color histograms.
 Attribute Features: Each image has 102 attributes and each attribute has multiple annotations. In to-tal, there are four million labels that are annotated by Amazon Mechanical Turk workers with bad-worker filtering and good-worker cultivating strategies (Pat-terson &amp; Hays, 2012). Some examples of annotated attributes are vegetation, open area, camping, hiking, natural light, leaves etc. 4.1.3. Experimental details We use Liblinear (Fan et al., 2008) to learn SVMs for learning W  X  . The parameters used for linear SVMs are C 1 = 1 and C 2 = 1 in Eq. 4. We did not tune those parameter. We also used linear SVM for cate-gory retrieval. We reduce the dimensionality of visual features in the SUN dataset from 19,080 to 1,000 by PCA. 4.2. Optimization Analysis As we use a block coordinate descent algorithm to opti-mize the objective function, we cannot guarantee that our algorithm reaches the global optimum. Our ex-periments shows that we reach a reasonable local opti-mum most of the time. To illustrate this, we measure the objective value and see if it decreases ( in the mini-mization task ) or not. In figure 2, we observe that the objective values does decrease as the iterations go on. After only a few iterations (15) the differences between the textual binary codes ( binary codes extracted from text data ) and the visual binary codes ( binary code ex-tracted from images ) are very small-less than 3 bits. The number of bits we use for this experiments is 32. 4.3. Bit Error by Hamming Space Size We investigate the Hamming distance of two obtained binary codes (value of Eq. 1) as a function of binary code length; 16, 32, 64, 128 and 256. Figure 3 shows that the number of bits that differ between binary codes from visual and text domains is almost always approximately 1 10 of code length. 4.4. Image Category Retrieval We retrieve images from an image pool by giving one or more samples (image or text/attribute) of a par-ticular category as a query. In quantitative analysis, we compute the mean average precision (mAP) of re-trieved images that belong to the same category of the query. In qualitative analysis, we present the images retrieved for a query by our method. 4.4.1. Quantitative Results For quantitative analysis, we conduct a category re-trieval experiment similar to (Torresani et al., 2010; Rastegari et al., 2012; 2011). We divided the dataset into two train/test segments. We train W  X  using the training set. We compute the binary features for all the images (train and test). We take a set of images of a particular category as query set and train a classifier by taking the query set as positive samples and images from other categories in the training set as negative set. Then, we apply the classifier to all the samples of the test set, rank them by their classification confi-dence value and retrieve the top-K samples. We report precision and recall as an accuracy measure. By vary-ing K in top-K we can draw a precision-recall curve. Since we are considering multiple categories, we report mean precision and recall.
 We compare our binary code with several binary code methods including Iterative Quantization ( ITQ ) (Gong &amp; Lazebnik, 2011), Spectral Hashing ( SH ) (Weiss et al., 2008) and Locality Sensitive Hashing ( LSH ) (Gionis et al., 1999a). Our method is re-ferred to as Predictable Dual-view Hashing ( PDH ). We are not comparing our method with (Rastegari et al., 2012) because their method is not applicable to Dual-View. They require category labels of samples as supervision to train their binary codes. We used supervised ITQ coupled with CCA which uses data in two views to construct basis vectors in a common subspace.
 Figure 4 and Figure 5 show mean average precision (mAP) of retrieved images by our method and other methods as a function of the number of bits. We presents the results with various numbers of queries given. As shown in the figure, our method ( PDH ) consistently outperforms all other methods.The high ranked images are not necessarily visually similar to the query. When we have few instances in the retrieval set the baseline methods have better precision because the high ranked images are the most visually similar to a query. This is not unexpected, since we optimize for cross-domain similarity, not visual similarity. We can directly compare by average precision(AP). As re-call increases and the number of relevant images from the database tat are visually similar to the query are exhausted, the PDH dominates the other methods in precision.
 4.4.2. Qualitative Results We also present qualitative results of how our binary code performs. We perform two qualitative evalua-tions.
 First, we conduct Image2Image retrieval. Given an image as a query, we retrieve the top-K closest images. Unlike the previous experiment we do not use an SVM but simply compute the Hamming distance of all other samples to the query sample and report the top-k most similar. Figure 6-(a) shows the retrieval for four query images which are represented by 32 bits. We report the top-5 most similar images. These retrieved images have significant semantic similarity to their query im-age.
 Second, we perform a Text2Image retrieval task. In-stead of using an image as query we use a sentence as query and we retrieve images for which this query sentence could be a good description. We map the sentence to our binary space and then identify simi-lar points (images) in that space and report the top-k most similar. In figure 6-(b) we illustrate the retrieval set for five different sentences using 32 bit codes. Most of the retrieved images have content that is semanti-cally similar to their query sentence. We proposed a novel binary hashing method from two-views. We formulated an objective function to main-tain predictability of the the binary codes and opti-mized the objective function by applying an iterative optimization method based on block coordinate de-scent. By conducting experiments on two datasets from visual-textual domain, we demonstrated the su-periority of our method compared to the state-of-the-art binary hashing methods.
 This work was partially supported by the US Gov-ernment through NSF Award IIS-0812111 and ONR MURI Grant N000141010934.

