 Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary rela-tions in text. For instance, given the span of text  X . . . the Seattle zoo . . .  X , one would like to extract the relation that  X  X he Seattle zoo X  is located-at  X  X eattle X . RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of seman-tic relations of interest. However, most approaches to RE have assumed that the relations X  arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.
Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are be-ing mapped to one of several relation types of in-terest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to ex-pand the set of target relations. Moreover, as we show, these approaches become brittle when the re-lations X  arguments are not given but rather need to be identified in the data too.

In this paper we build on the observation that there exists a second dimension to the relation extraction problem that is orthogonal to the relation type di-mension: all relation types are expressed in one of several constrained syntactico-semantic structures. As we show, identifying where the text span is on the syntactico-semantic structure dimension first, can be leveraged in the RE process to yield improved per-formance. Moreover, working in the second dimen-sion provides robustness to the real RE problem, that of identifying arguments along with the relations be-tween them.

For example, in  X  X he Seattle zoo X , the entity men-tion  X  X eattle X  modifies the noun  X  X oo X . Thus, the two mentions  X  X eattle X  and  X  X he Seattle zoo X , are involved in what we later call a premodifier rela-tion , one of several syntactico-semantic structures we identify in Section 3.

We highlight that all relation types can be ex-pressed in one of several syntactico-semantic struc-tures  X  Premodifiers, Possessive, Preposition, For-mulaic and Verbal. As it turns out, most of these structures are relatively constrained and are not dif-ficult to identify. This suggests a novel algorith-mic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. Not only does this approach provide significantly improved RE perfor-mance, it carries with it two additional advantages.
First, leveraging the syntactico-semantic struc-ture is especially beneficial in the presence of small amounts of data. Second, and more important, is the fact that exploiting the syntactico-semantic dimen-sion provides several new options for dealing with the full RE problem  X  incorporating the argument identification into the problem. We explore one of these possibilities, making use of the constrained structures as a way to aid in the identification of the relations X  arguments. We show that this already pro-vides significant gain, and discuss other possibilities that can be explored. The contributions of this paper are summarized below:  X  We highlight that all relation types are ex- X  We show that when one does not have a large  X  We show how to leverage these constrained
In the next section, we describe our relation ex-traction framework that leverages the syntactico-semantic structures. We then present these struc-tures in Section 3. We describe our mention entity typing system in Section 4 and features for the RE system in Section 5. We present our RE experiments in Section 6 and perform analysis in Section 7, be-fore concluding in Section 8.
 In Figure 1, we show the algorithm for training a typical baseline RE classifier ( RE base ), and for training a RE classifier that leverages the syntactico-semantic structures ( RE s ).

During evaluation and when the gold mentions are already annotated, we apply RE s as follows. When given a test example mention pair ( x i , x j ), we per-form structure inference on it using the patterns de-scribed in Section 3. If ( x i , x j ) is identified as hav-ing any of the four syntactico-semantic structures S , apply RE s to predict the relation label, else apply
Next, we show in Figure 2 our joint inference al-gorithmic framework that leverages the syntactico-semantic structures for RE, when mentions need to be predicted . Since the structures are fairly con-strained, we can use them to consider mention can-didates that are originally predicted as non men-tions. As shown in Figure 2, we conservatively in-clude such mentions when forming mention pairs, provided their null labels are predicted with a low probability t 1 .
There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009). However, these works operate along the first dimension, that of using patterns to mine for relation type examples. In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimen-sion first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types al-lowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature.
 In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as in-put (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did not address the pipelined errors propagated from the mention identification process. In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure 2 . ACE-2004 identified five types of structures: premodi-fier, possessive, preposition, formulaic, and verbal. We are unaware of any previous computational ap-proaches that recognize these structures automati-cally in text, as we do, and use it in the context of RE (or any other problem). In (Qian et al., 2008), the authors reported the recall scores of their RE system on the various syntactico-semantic structures. But they do not attempt to recognize nor leverage these structures.

In this work, we focus on detecting the first four structures. These four structures cover 80% of the mention pairs having valid semantic relations (we give the detailed breakdown in Section 7) and we show that they are relatively easy to identify using simple rules or patterns. In this section, we indicate mentions using square bracket pairs, and use m i and m j to represent a mention pair. We now describe the four structures.
 Premodifier relations specify the proper adjective or proper noun premodifier and the following noun it modifies, e.g.: [the [Seattle] zoo] Possessive indicates that the first mention is in a possessive case, e.g.: [[California]  X  X  Governor] Preposition indicates that the two mentions are semantically related via the existence of a preposi-tion, e.g.: [officials] in [California] Formulaic The ACE04 annotation guideline 3 in-dicates the annotation of several formulaic relations, including for example address: [Medford] , [Mas-sachusetts]
In this rest of this section, we present the rules/patterns for detecting the above four syntactico-semantic structure, giving an overview of them in Table 1. We plan to release all of the rules/patterns along with associated code 4 . Notice that the patterns are intuitive and mostly syntactic in nature. 3.1 Premodifier Structures  X  We require that one of the mentions completely  X  If u * is not empty, we require that it satisfies  X  We use two patterns to differentiate between 3.2 Possessive Structures  X  The basic pattern for possessive is similar to  X  If the word immediately following v + is  X  X  X  X  or 3.3 Preposition Structures  X  We first require the two mentions to be non- X  If the only dependency labels in the depen-3.4 Formulaic Structures  X  The ACE-2004 annotator guidelines specify As part of our experiments, we perform RE using predicted mentions. We first describe the features (an overview is given in Table 2) and then describe how we extract candidate mentions from sentences during evaluation. 4.1 Mention Extraction Features Features for every word in the mention For ev-ery word w k in a mention m i , we extract seven fea-tures. These are a combination of w k itself, its POS tag, and its integer offset from the last word ( lw ) in the mention. For instance, given the mention  X  X he operation room X , the offsets for the three words in the mention are -2, -1, and 0 respectively. These features are meant to capture the word and POS tag sequences in mentions.

We also use word clusters which are automat-ically generated from unlabeled texts, using the Brown clustering (Bc) algorithm of (Brown et al., 1992). This algorithm outputs a binary tree where words are leaves in the tree. Each word (leaf) in the tree can be represented by its unique path from the root and this path can be represented as a simple bit string. As part of our features, we use the cluster bit string representation of w k and lw .
 Contextual We extract the word C  X  1 ,  X  1 immedi-ately before m i , the word C +1 , +1 immediately after m i , and their associated POS tags P .
 NE tags We automatically annotate the sentences with named entity (NE) tags using the named en-tity tagger of (Ratinov and Roth, 2009). This tagger annotates proper nouns with the tags PER (person), ORG (organization), LOC (location), or MISC (mis-cellaneous). If the lw of m i coincides (actual token offset) with the lw of any NE annotated by the NE tagger, we extract the NE tag as a feature.
 Syntactic parse We parse the sentences using the syntactic parser of (Klein and Manning, 2003). We extract the label of the parse tree constituent (if it ex-ists) that exactly covers the mention, and also labels of all constituents that covers the mention. 4.2 Extracting Candidate Mentions From a sentence, we gather the following as candi-date mentions: all nouns and possessive pronouns, all named entities annotated by the the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the sub-strings do not start nor end on punctuation marks. These mention candidates are then fed to our men-tion entity typing (MET) classifier for type predic-tion (more details in Section 6.3). We build a supervised RE system using sentences annotated with entity mentions and predefined target relations. During evaluation, when given a pair of mentions m i , m j , the system predicts whether any of the predefined target relation holds between the mention pair.

Most of our features are based on the work of (Zhou et al., 2005; Chan and Roth, 2010). Due to space limitations, we refer the reader to our prior work (Chan and Roth, 2010) for the lexical, struc-tural, mention-level, entity type, and dependency features. Here, we only describe the features that were not used in that work.

As part of our RE system, we need to extract the head word ( hw ) of a mention ( m ), which we heuris-tically determine as follows: if m contains a prepo-sition and a noun preceding the preposition, we use the noun as the hw . If there is no preposition in m , we use the last noun in m as the hw .
 POS features If there is a single word between the two mentions, we extract its POS tag. Given the hw of m , P i,j refers to the sequence of POS tags in the immediate context of hw (we exclude the POS tag of hw ). The offsets i and j denote the position (rela-tive to hw ) of the first and last POS tag respectively. For instance, P  X  2 ,  X  1 denotes the sequence of two POS tags on the immediate left of hw , and P  X  1 , +1 denotes the POS tag on the immediate left of hw and the POS tag on the immediate right of hw .
 Base phrase chunk We add a boolean feature to detect whether there is any base phrase chunk in the text span between the two mentions. We use the ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Con-sortium) to conduct our experiments. Following prior work, we use the news wire (nwire) and broadcast news (bnews) corpora of ACE-2004 for our experiments, which consists of 345 documents.
To build our RE system, we use the LIBLINEAR (Fan et al., 2008) package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1. To ensure that this baseline RE sys-tem based on the features in Section 5 is competi-tive, we compare against the state-of-the-art feature-based RE systems of (Jiang and Zhai, 2007) and (Chan and Roth, 2010). In these works, the au-thors reported performance on undirected coarse-grained RE. Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported F-measures of 71.5 and 71.2, respectively. Using the same evalua-tion setting, our baseline RE system achieves a com-petitive 71.4 F-measure.
 We build three RE classifiers: binary , coarse , fine . Lumping all the predefined target relations into a single label, we build a binary classifier to predict whether any of the predefined relations exists be-tween a given mention pair.

In this work, we model the argument order of the mentions when performing RE, since relations are usually asymmetric in nature. For instance, we con-sider m i :EMP-ORG: m j and m j :EMP-ORG: m i to be distinct relation types. In our experiments, we ex-tracted a total of 55,520 examples or mention pairs. Out of these, 4,011 are positive relation examples annotated with 6 coarse-grained relation types and 22 fine-grained relation types 5 .

We build a coarse-grained classifier to disam-biguate between 13 relation labels (two asymmetric labels for each of the 6 coarse-grained relation types and a null label). We similarly build a fine-grained classifier to disambiguate between 45 relation labels. 6.1 Evaluation Method For our experiments, we adopt the experimental set-ting in our prior work (Chan and Roth, 2010) of en-suring that all examples from a single document are either all used for training, or all used for evaluation.
In that work, we also highlight that ACE anno-tators rarely duplicate a relation link for coreferent mentions. For instance, assume mentions m i , m j , and m k are in the same sentence, mentions m i and m j are coreferent, and the annotators tag the men-tion pair m j , m k with a particular relation r . The annotators will rarely duplicate the same (implicit) relation r between m i and m k , thus leaving the gold relation label as null. Whether this is correct or not is debatable. However, to avoid being penalized when our RE system actually correctly predicts the label of an implicit relation, we take the following ap-proach.

During evaluation, if our system correctly pre-dicts an implicit label, we simply switch its predic-tion to the null label. Since the RE recall scores only take into account non-null relation labels, this scoring method does not change the recall, but could marginally increase the precision scores by decreas-ing the count of RE predictions. In our experi-ments, we observe that both the usual and our scor-ing method give very similar RE results and the ex-perimental trends remain the same. Of course, us-ing this scoring method requires coreference infor-mation, which is available in the ACE data. 6.2 RE Evaluation Using Gold Mentions To perform our experiments, we split the 345 docu-ments into 5 equal sets. In each of the 5 folds, 4 sets (276 documents) are reserved for drawing training examples, while the remaining set (69 documents) is used as evaluation data. In the experiments de-scribed in this section, we use the gold mentions available in the data.

When one only has a small amount of train-ing data, it is crucial to take advantage of external knowledge such as the syntactico-semantic struc-tures. To simulate this setting, in each fold, we ran-domly selected 10 documents from the fold X  X  avail-able training documents (about 3% of the total 345 documents) as training data. We built one binary, one coarse-grained, and one fine-grained classifier for each fold.

In Section 2, we described how we trained a base-line RE classifier ( RE base ) and a RE classifier using the syntactico-semantic patterns ( RE s ).

We first apply RE base on each test example men-tion pair ( m i , m j ) to obtain the RE baseline results, showing these in Table 4 under the column  X 10 doc-uments X , and in the rows  X  X inary X ,  X  X oarse X , and  X  X ine X . We then applied RE s on the test exam-ples as described in Section 2, showing the results in the rows  X  X inary+Patterns X ,  X  X oarse+Patterns X , and  X  X ine+Patterns X . The results show that by us-ing syntactico-semantic structures, we obtain signif-icant F-measure improvements of 8.3 , 7.2 , and 5.5 for binary, coarse-grained, and fine-grained relation predictions respectively. 6.3 RE Evaluation Using Predicted Mentions Next, we perform our experiments using predicted mentions. ACE-2004 defines 7 coarse-grained entity types, each of which are then refined into 43 fine-grained entity types. Using the ACE data annotated with mentions and predefined entity types, we build a fine-grained mention entity typing (MET) clas-sifier to disambiguate between 44 labels (43 fine-grained and a null label to indicate not a mention). To obtain the coarse-grained entity type predictions from the classifier, we simply check which coarse-grained type the fine-grained prediction belongs to. We use the LIBLINEAR package with the same set-tings as earlier specified for the RE system. In each fold, we build a MET classifier using all the (276) training documents in that fold.

We apply RE base on all mention pairs ( m i , m j ) where both m i and m j have non null entity type pre-dictions. We show these baseline results in the Rows  X  X inary X ,  X  X oarse X , and  X  X ine X  of Table 5.

In Section 2, we described our algorithmic ap-proach (Figure 2) that takes advantage of the struc-tures with predicted mentions. We show the results of this approach in the Rows  X  X inary+Patterns X ,  X  X oarse+Patterns X , and  X  X ine+Patterns X  of Table 5. The results show that by leveraging syntactico-semantic structures, we obtain significant F-measure improvements of 8.2 , 4.6 , and 3.6 for binary, coarse-grained, and fine-grained relation predictions re-spectively. We first show statistics regarding the syntactico-semantic structures. In Section 3, we mentioned that ACE-2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. On the 4,011 examples that we experimented on, premodifiers are the most frequent, account-ing for 30.5% of the examples (or about 1,224 ex-amples). The occurrence distributions of the other structures are 18.9% (possessive), 23.9% (preposi-tion), 7.2% (formulaic), and 19.5% (verbal). Hence, the four syntactico-semantic structures that we fo-cused on in this paper account for a large majority (80%) of the relations.

In Section 6, we note that out of 55,520 men-tion pairs, only 4,011 exhibit valid relations. Thus, the proportion of positive relation examples is very sparse at 7.2%. If we can effectively identify and discard most of the negative relation examples, it should improve RE performance, including yielding training data with a more balanced label distribution.
We now analyze the utility of the patterns. As shown in Table 6, the patterns are effective in infer-ring the structure of mention pairs. For instance, ap-plying the premodifier patterns on the 55,520 men-tion pairs, we correctly identified 86.8% of the 1,224 premodifier occurrences as premodifiers, while in-curring a false-positive rate of only about 20% 6 . We note that preposition structures are relatively harder to identify. Some of the reasons are due to possi-bly multiple prepositions in between a mention pair, preposition sense ambiguity, pp-attachment ambigu-ity, etc. However, in general, we observe that infer-ring the structures allows us to discard a large por-tion of the mention pairs which have no valid re-lation between them. The intuition behind this is the following: if we infer that there is a syntactico-semantic structure between a mention pair, then it is likely that the mention pair exhibits a valid rela-tion. Conversely, if there is a valid relation between a mention pair, then it is likely that there exists a syntactico-semantic structure between the mentions.
Next, we repeat the experiments in Section 6.2 and Section 6.3, while gradually increasing the amount of training data used for training the RE classifiers. The detailed results of using 5% and 80% of all available data are shown in Table 4 and Table 5. Note that these settings are with respect to all 345 documents and thus the 80% setting represents us-ing all 276 training documents in each fold. We plot the intermediate results in Figure 3 and Figure 4. We note that leveraging the structures provides improve-ments on all experimental settings. Also, intuitively, the binary predictions benefit the most from lever-aging the structures. How to further exploit this is a possible future work. In this paper, we propose a novel algorithmic ap-proach to RE by exploiting syntactico-semantic structures. We show that this approach provides several advantages and improves RE performance. There are several interesting directions for future work. There are probably many near misses when we apply our structure patterns on predicted men-tions. For instance, for both premodifier and posses-sive structures, we require that one mention com-pletely includes the other. Relaxing this might potentially recover additional valid mention pairs and improve performance. We could also try to learn classifiers to automatically identify and disam-biguate between the different syntactico-semantic structures. It will also be interesting to feedback the predictions of the structure patterns to the mention entity typing classifier and possibly retrain to obtain a better classifier.
 Acknowledgements This research is supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government.

We thank Ming-Wei Chang and Quang Do for building the mention extraction system.

