
In this paper we propose CCCS, a new algorithm for clas-sification based on association rule mining. The key inno-vation in CCCS is the use of a new measure, the  X  X om-plement Class Support (CCS) X  whose application results in rules which are guaranteed to be positively correlated. Fur-thermore, the anti-monotonic property that CCS possesses has very different semantics vis-a-vis the traditional sup-port measure. In particular,  X  X ood X  rules have a low CCS value. This makes CCS an ideal measure to use in con-junction with a top-down algorithm. Finally, the nature of
CCS allows the pruning of rules without the setting of any threshold parameter! To the best of our knowledge this is the first threshold-free algorithm in association rule mining for classification.
 I.2.6 [ Artificial Intelligence ]: Learning algorithms,experimentation
Association Rules Mining, Classification, Imbalanced Data sets, Parameter-free mining Classification is a core data mining and machine learning task. The objective in classification is to build a model (the classifier) which maps objects into pre-defined classes based on the attributes of objects. The methodology of evaluating a classifier consists of partitioning a data set into two sub-sets called the training and test data. The model is built  X  The work of this author was supported by The Capital Markets CRC.

Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. on the training data and its accuracy is determined by mea-suring its ability to correctly classify the objects in the test data. Interested readers should consult a recent textbook on data mining (for example, [14]) for a survey of classification techniques.

In 1998, Liu, Hsu and Ma [13] introduced CBA, a new classification algorithm based on association rule mining, a foundational paradigm in data mining [1]. CBA mines association rules of the form A  X  C ,where A is a subset of the attribute set and C is an element of the set of pre-defined classes. The mining is performed on the training data. The set of mined rules are then used to classify instances in the test data. Several variations on the original CBA algorithm have emerged in the research literature [13, 15, 10] with the main challenges being: 1. To select an optimal subset of rules mined to build a 2. Selecting an appropriate rule in order to classify new
While several CBA-type algorithms are available (collec-tively called  X  X ssociative Classifiers X ) all of them use the traditional support measure to mine association rules. How-ever using a minimum support threshold is inappropriate for data sets with highly imbalanced class distribution. For ex-ample in direct marketing applications, the response rate, the people who actually buy the product after responding to a promotion, is typically 1%  X  2% [11]. Similarly in med-ical databases the relative size of the samples of people who suffer from a disease is extremely small. In such applications setting the minimum support to even 1% is likely to prune most of the cases of interest. On the other hand using a low minimum support results in the generation of extremely large amount of rules.

While support is often used as a measure of significance of the rule (rules with high support intuitively capture an un-derlying process rather than being artifacts of the particular data set), the strength of the rule is captured by its confi-dence . However, even confidence can often be misleading measure in the case of imbalanced distribution. Consider the example, shown in Table.1 which shows the association between an item A and class C 1 .Aand  X  A represent the presence and absence of item A respectively , C 1 represents aclassand  X  C 1 represents the complement of C 1 . Now consider the association rule A  X  C 1 . The con-fidence of this rule is given by conf ( A  X  C 1 )=  X  ( A  X 
C ) / X  ( A ) = 20/25= 80% and the support of the rule is  X  ( A  X  C 1 ) /N =20%. Hence this rule has high confidence and support.

Now lets calculate the correlation between A and C 1 .As shown in [5], one way of calculating the correlation is to com-pute P ( A  X  C 1 ) /P ( A )  X  P ( C 1 )=0 . 2 / (0 . 25  X  The fact that this quantity is less than 1 indicates negative correlation between A and C 1 . Such situations occur when the class distribution is imbalanced as in the example where class C 1 makes up 90% of the data.

We have designed a new measure, which we call the Com-plement Class Support (CCS), which tightly captures the relationship between the antecedent of the rule and its class. Given a rule A  X  C ,
Intuitively, CCS captures the strength of the rule in the complement class. If a rule is strong then its CCS should be small. In fact in Section 3, we will prove the following properties which explain why CCS can be used to design an efficient and accurate classifier for imbalanced data sets.
Lemma 1. For a rule A  X  C , the following are equivalent a) A and C are positively correlated. b) CCS ( A  X  C ) &lt;  X  ( A ) N . c) conf ( A  X  C ) &gt;  X  ( C ) N . d) conf ( A  X  C ) &gt; 1  X   X  (  X  C ) N .

Lemma 1.(b) asserts that for a rule to be positively corre-lated it is necessary and sufficient that the CCS of the rule be bounded by the support of the antecedent. This explains why the rule A  X  C 1 in Table 1 is not positively correlated even though it has a high confidence: CCS ( A  X  C 1 )= 5 / 10 = 0 . 5 &gt;  X  ( A ) N .

Lemma 1.(c) shows that when the classes are equally dis-tributed, by setting the minconf to 0.5, we can make sure that all the rules discovered a re positively correlated. How-ever, when the class distribution is imbalanced, the minconf has to be set to the support of the majority class in order for the rules discovered to be positively correlated rules. This is likely to prune most of the rules from the minority class and hence affect the accuracy of classification.
 Assuming that we are dealing with a binary classification, Lemma 1.(d) asserts that the minconf for each class should be set in terms of the support of the other class to get pos-itively correlated rules. If the class distributions are equal the minconf foreachclasswouldbe0.5andthiswouldbe the same as using confidence alone. However, if the class distributions are imbalanced such as 90% and 10%, for the majority class the minconf =1-0.1=0.9 and for the minority class, minconf =1-0.9=0.1. By setting a lower value for the minority class we avoid pruning its rules and by setting a higher value for the majority class we avoid generating large number of rules.

The above discussion leads us to conclude that CCS is a better measure for imbalanced class distribution. It auto-matically selects the right confidence level for each class in order to guarantee that the rules discovered are positively correlated. Finally, it should be noted that CCS does not have a prescribed a pre-defined threshold value. At each node in the lattice, the CCS value will dynamically adjust (based on the support of the antecedent at that node) in order to generate only positively correlated rules.
In many cases, for reasons of efficiency, we may want to set a threshold value for CCS. In such situations we can take advantage of the fact that CCS enjoys the anti-monotonic property.

Lemma 2. Given a rule A i  X  C j ,if CCS ( A i  X  C j ) &gt; t then, CCS ( subsetOf ( A i )  X  C j ) &gt; t.

It is important to note that the anti-monotonicity of CCS has different semantics vis-a-vis the traditional support mea-sure. In particular,  X  X ood X  rules have low CCS value. As we will show later, we can combine CCS with top-down row enu-meration algorithms to efficiently discover class rules. Even though row enumeration algorithms [6, 7, 8] use the support measure for pruning they cannot exploit the anti-monotonic property as they descend down the tree. Instead at each node, an upper bound on the maximum support value of all nodes rooted at that node is derived. The subtree is pruned if the maximum is below the support threshold. For CCS, no such bound is required. If a node has a high CCS value, then it (and possibly its descendants) can be pruned.
Associative classifier is a classification model that has shown a lot of promise recently. The first associative classi-fier, CBA, was proposed in 1998 [13]. Since then many al-gorithms have been developed to improve the efficiency and accuracy of classifications. These methods differ in three aspects as follows: 1. The technique used to mine the association rules effi-2. The measures and method used to select the best set 3. The measures used to effectively predict the class of a
In CBA and Harmony [15], the rule selection is purely based on confidence. However this method may not always give the correct classification especially when the class dis-tribution is highly imbalanced. As discussed in Section 1, in imbalanced data sets, rules with high confidence could be actually negatively correlated.

CMAR [10] overcomes this problem by selecting rules for the classifier only if they pass the  X  2 test and ARC-PAN [2] by calculating the correlation coefficient. We will show in Section 3 that by using Complement Class Support for pruning, we only generate positively correlated rules and this removes the need for further testing. Refer to our technical report [3] for a detailed discussion on related work.
Recently the row enumeration method used in algorithms such as Farmer, RERII and RCBT [6, 7, 8] has proved to be very efficient in mining association rules from databases with extremely large number of attributes such as the micro array data sets. We adopt this method in our algorithm for reasons discussed in [3].
 The remainder of the paper is arranged as follows: In Section 3 we present the basic definitions and notations of the concepts used in this paper and also present the the-orems.In Section 4 we present our classification algorithm CCCS with an example. Finally in section 5 we present the experiments and results and Section 6 concludes the paper with a summary of the research.
Let I be a finite set of items and C a finite set of class labels. A row (or instance) is an element of the set (2 I  X  D is the set of all rows given. Assume | D | = N .Aclass rule is an implication of the form A  X  C 1 where A  X  I and C 1  X  C . Traditionally, the strength of of a class rule is defined in terms of support and confidence (Refer to [1] for definitions).

Definition 1. Given a rule A  X  C , we measure the corre-lation between A and C based on the magnitude of the ratio P ( A  X  C ) P ( A ) P ( C )
Definition 2. The Class Support of rule A i  X  C j is the support of A i in class C j .

Definition 3. The Complement Class Support (CCS) of a rule A i  X  C j is the support of A i in the classes other than C .Let C j denote all the classes in D other than C j .Then
Definition 4. The Strength Score of a rule A i  X  C j is given by where t is set to a very low value such as 0.001 to avoid division by 0 when CCS =0.
 We now show that CCS is anti-monotonic.

Lemma 3. Given a rule A i  X  C j ,if CCS ( A i  X  C j ) &gt; t then, CCS ( subsetOf ( A i )  X  C j ) &gt; t.
 Proof. Given But /* anti-monotone property of support*/
Therefore
We now prove Theorem 1, the main theoretical result of this paper which underpins the design of our algorithm CCCS. We restate the lemma for convenience.

Theorem 1. For a class rule A  X  C , the following are equivalent a) A and C are positively correlated. b) CCS ( A  X  C ) &lt;  X  ( A ) N . c) conf ( A  X  C ) &gt;  X  ( C ) N . d) conf ( A  X  C ) &gt; 1  X   X  (  X  C ) N .
 Proof. ( a )  X  ( b )
Given that A and C are positively correlated ( b )  X  ( c ) Given ( c )  X  ( d ) This is a simple rewrite of ( d )  X  ( a )
Given Therefore, A and C are positively correlated We now introduce the Classification using Complement Class Support ( CCCS) Algorithm.

The definition of CCS suggests that rules with lower CCS values are likely to be stronger as they are less frequently seen in other classes. This property along with the anti-monotonic property of CCS makes it possible for it to be in-tegrated with a row enumeration method. As the tree grows, the CCS of the rules increase and hence the rules become less desirable and are candidates for being pruned. Row-enumeration algorithms were designed for data sets where the number of rows is much smaller than the number of columns. For example, microarray data sets fall in this cat-egory. However they can be used for small to moderate size data sets (like those in the UCI Repository[4]) even if the data sets do not match these characteristics.
 The CCCS Algorithm is given in Figure 1. Basically CCCS takes each transaction and generates rules such that the itemset and the class are positively correlated. Initially each transaction is added to the root of the enumeration tree. Then item sets are generated from a transaction by performing intersection with sibling nodes in the enumer-ation tree and these itemsets are added as child nodes of the transaction and the class of the transaction is passed on as the class of all its child nodes for the purpose of calcu-lating the CCS. After intersection with all the siblings on the right of the enumeration tree, if a node is found to be Table 2: Comparison of Error Rates in UCI Data sets positively correlated ( CCS &lt; support ), that rule is consid-ered as potential candidate for the class of the corresponding transaction.

The tree is grown in a depth-first fashion by recursively performing intersection of each node with its sibling node. The intersection at each node is performed in the same way as RERII [7]. From the candidate rules, our algorithm builds the classifier by selecting the best rule for each transaction similar to Harmony [15]. In our case the best rule is consid-ered as the rule with the highest Score Strength (SS) .
Refer to our technical report [3] for a detailed explanation of the CCCS algorithm with an example.
In this section we report on the experiments that we have carried out to measure and assess the accuracy of CCCS. All our comparisons are with CBA. The executable of the CBA program was downloaded from [12]. Our objective is to experimentally validate the hypothesis:
For data sets with an imbalanced (skewed) class distribu-tion, CCCS will be more accurate compared to CBA
Eight data sets were obtained from the UCI ML Reposi-tory[4]. For a fair comparison the continuous variables were discretization using the same techniques as described in [13]. For each original data set, three versions of minority class size, 5%, 10% and 15% were created. Refer to our technical report [3] for the method we used to create the imbalanced datasets.
A comparison of the error rates of the eight data sets is shown in Table 2. All the error rates reported are the average values of 10-fold cross validation. For CBA, the minconf was set to 50% and minsup was set to 1%.

While the error rates of CCCS is lower than CBA on the original data, a bootstrapping analysis (Section 5.3) shows that the differences are not significant.

Table 3 shows the results of CCCS and CBA on three sep-arate imbalanced versions each of the eight data sets. For the 5% data sets, CCCS outperforms CBA except on the Diabetes data. For the 10% data sets, CCCS again outper-forms CBA except on the Pima data. At 15%, the accu-racy of CCCS and CBA begins to converge. This confirms our hypothesis that CCCS is more suitable than CBA for imbalanced data sets. It should be noted that CCCS Input : Transaction table, t Output : Classification Rules 1. Let D be the set of n transactions Tr 1 , Tr 2 ,......, Tr 2. Let C be the set of class labels c 1 ,c 2 ,c 3 , ......c 3. Let Tr i .items be the set of non-class attributes of Tr 4. Let Tr i .class be the class label of Tr i . 5. Rules  X  0 6. PC  X  0 7. Add the transactions to the root node 8. For each Tr i D 9. Let c k be the class attribute of Tr i . 10. Let N be the set of transactions Tr i +1 ...n 11. For each node n in PC 12. if n.class = c k 13. N = N  X  n 14. } 15. End For 16. MineRules[ Tr i ,N, c k ,Rules] 17. End for MineRules [ Tr ,N,c,Rules] 18. For each nN 19. N i  X  0 //set of intersections is set to 0// 20. d = Tr.items  X  n.items 21. if | d | &gt; 0 22. if Tr.items = n.items 23. remove n from N 24. add n.id to Trn and n ( n N ) 25. corresponding to n.class 26. } 27. if Tr.items  X  n.items 28. add n.id to Tr and n ( n N ) 29. corresponding to n.class 30. } 31. if Tr.items  X  n.items 32. remove n from N 33. if d is not discovered before 34. add n to N i 35. n .tems = d 36. add ids of Tr and n to n 37. } 38. } 39. if Tr.items = n.items 40. if d is not discovered before 41. add n to N i 42. n .tems = d 43. add ids of Tr and n to n 44. } 45. } 46. } 47. End For 48. If Tr.CCS &lt; sup ( Tr.itemlist ) 49. BuildClassifier[Rules,D,Tr,c] 50. if | N i | &gt; 0 51. For each n i N i 52. MineRules[ n i ,N i ,c ,Rules] 53. End For 54. } 55. else if T r.ClSup &lt; sup ( Tr.itemlist ) 56. PC = PC  X  Tr 57. } achieves a significantly higher percentage of True Positives (minority class) compared to CBA even when the minsup for CBA was 1%. In Table 3, TP% istheTruePositiveRate. TheTruePositiveRateisabetter measure of accuracy on data sets which are characterized by an imbalanced class distribution.
To determine if the differences between the error rates of CCCS and CBA are real (as opposed to being artifacts of this particular instances of the data ) we carried out a bootstrapping analysis. The advantage of bootstrapping (as opposed to a paired t-test), is that no distributional assump-tion about the error rates is required.

In bootstrapping, a resampling method is used to deter-mine the confidence bounds of statistical estimators [9]. In our case we have two vectors, cbaerror and cccserror of length eight (the number of data sets). We want an estimate on the average of vector difference between them.
In order to create multiple samples of the vector difference, sampling with replacement is carried out and the average of the difference is computed for each sample. In sampling with replacement the data point sampled is returned to the data set and made available to be selected again. This way several samples can be created and  X  X istogrammed X . For histogram then one can conclude that there is no significant difference between the two error vectors.

Figure 2 shows the bootstrapping results (on 1000 sam-ples) on the four different versions of the data set. For the original data, the zero point lies in the bulk of the histogram and we can conclude that there is no significant difference between the CBA and CCCS error rate. For the 5% data, the zero point lies on the edge of the histogram which allows us to conclude that the differences between the CBA and CCCS errors are different. Similarly for the 10% data, the zero point is far removed from the bulk of the histogram. We again conclude that the differences between the CBA and CCCS errors are significant. Finally, for the the 15% data set, the zero point again returns to the middle of the histogram -the error differences are not significant.
Note that from Figure 2, it appears that CCCS does much better than CBA for the 10% data compared to the 5% data. This is because, for the 5% data, the chance to create errors is limited to 5% for the positive class.
In the last decade extensive research has been carried out in the mining of association rules. In 1998, Liu, Hsu and Ma [13] introduced the CBA algorithm for classification us-ing association rules. Since then several variations on the original algorithm have been introduced. However, till date all algorithms have used the traditional support measure to mine association rules. We have shown that for imbalanced class data sets, the support/confidence framework is inade-quate. In order to address this problem we have introduced a new measure, the Complement Class Support (CCS). CCS has several properties which make it extremely suitable for mining imbalanced data sets. The nature of CCS makes it an ideal candidate to be used in conjunction with a top-down row enumeration type algorithm. This is the essence of CCCS -a row enumeration algorithm with CCS guaran-teed to generate positively correlated rules.
Data Set # Instances Error Rates TP % diabetes
Mushroom
Australian Average 8.77 8.04 35.8 46.7 Table 3: Comparison of Error Rates in Imbalanced Data sets [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] M.-L. Antonie and O. R. Zaiane. An associative [3] B. Arunasalam and S. Chawla. Cccs: A top-down [4] C. Blake and C. Merz. UCI KDD Archive . [5] S. Brin, R. Motwani, and C. Silverstein. Beyond [6] G. Cong, A. K.H.Tung, X. Xu, F. Pan, and J. Yang. Figure 2: 1000 bootstrapped samples of the differ-ence between the error rate of CBA and CCCS for the four different types of data sets. For the 5% and 10% data sets the zero point lies outside the bulk of the histogram. [7] G.Cong,K.-L.Tan,A.K.H.Tung,andF.Pan.
 [8] G. Cong, K.-L. Tan, A. K.H.Tung, and X. Xu. Mining [9] M. Inc. Matlab Statistical Toolbox . Mathworks, 2005. [10] W. Li, J. Han, and J. Pei. Cmar:accurate and efficient [11] C. X. Ling and C. Li. Data mining for direct [12] B. Liu, W. Hsu, and Y. Ma. Data Mining II . [13] B. Liu, W. Hsu, and Y. Ma. Integrating classification [14] P. Tan, M. Steinbach, and V. Kumar. Introduction to [15] J. Wang and G. Karypis. Harmony: Efficiently mining
