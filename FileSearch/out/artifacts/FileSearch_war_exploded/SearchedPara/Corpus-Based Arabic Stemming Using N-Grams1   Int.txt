 The rapid growth of the internet has increased the number of documents avail-able online. The latest statistics show that Arabic is the 7th most popular lan-guage used over the net by the end of the year 2009 [10]. Arabic Information retrieval faces many challenges due to the complex and rich nature of the Arabic language. Stemming is one of the techniques used to improve the Arabic infor-mation retrieval by reducing the words X  variants into the base words like stems or roots.

Stemming improves the information retrieval by reducing the word mismatch between the query and the document. This will result in returning more relevant documents to the query. Stemming has a great effect on the retrieval when the language is highly inflected for example Arabic language [4]. There has been several attempts to solve the Arabic text stemming including constructing man-ual dictionary [2], affix removal which is also called light stemming [5,4,11,13], morphological stemming [2,6] and statistical stemming [3,12].

In this paper, we have used a technique called Corpus-based Stemming that generates lists of words from the same root [19]. Then we have used these lists in Arabic information retrieval experime nts and compared the results with a more complex and linguistic-based stemming approach known as Light-10 stemmer.
The remaining of this paper is structured as follows. In the next section, we describe the complexity of the Arabic language. Section 3 describes and compares similar stemming approaches to our proposed approach. Section 4 explains our approach to Arabic stemming for information retrieval. The results of experiments are described in sectio n 5. Section 6 concludes this paper and suggests some potential improvements and future work. The Arabic language is complex and has a rich grammar; it consists of 28 letters and a set of short vowels (harakat) , long vowels and nunation ( ,tanwin). Arabic text is written from right to left where some letters are  X  X ocalized X  and embrace diacritics. Interestingly enough the meaning of a word might change based on its diacritics (i.e. the word [kataba: he wrote] is different from the word [kotob: books] although they both written with the same three letters (k,t,b)).

Moreover, the Arabic language has a very complex morphology. Most of the words are created from a root of 3 letters. Other words have 4, 5 or 6 letters roots. Some of the words are constructed by attaching a prefix at the beginning or a suffix at the end of the root word. But, most of the adjectives, nouns and verbs are generated by infixing the root. The most challenging morphological problem in the Arabic language is that plural and singular forms of nouns are mostly irregular which makes it difficult to conflate them. Consequently, Arabic morphological analysis is a very complicated task and so far no single stemming technique has been able to resolve all the issues for all the cases.
These complexities in the Arabic language make it a highly inflated language where many similar words have variant morphological forms. This increases the likelihood of word mismatch in informatio n retrieval systems. Therefore, stem-ming is a very important process in information retrieval where word conflation can be found and word matching between existing documents and queries can be improved to return more relevant documents. In the next section, we describe a number of stemming techniques focusing on the statistical stemming approach. Xu and Croft [19] have used a two stage approach in their pioneering work on corpus-based stemming. In the first stage, they experimented with both aggres-sive stemmers such as Porter and K-stem and also a trigram matching approach. They created equivalence classes that con tained all the words with the same root. In the aggressive stemming method, they grouped all the words that generated the same root with the stemmer in an equivalence class. In the trigram approach, they put all the words that started with the same three letters in the same equiv-alence class. In the second stage, they refined these equivalence classes by using a variation of the Expected Mutual Information Measure (EMIM) called EM to calculate the closeness of each pair of words in the same equivalence classes. The EM unlike EMIM does not favor words with high frequency. For two terms a and b ,the EM is calculated as below: Where n ab is the number of times a and b co-occur within a window in the corpus, n a and n b are number of times a and b appear in the corpus. En ( a, b )isthe expected number of co-occurrences assuming a and b are statically independent and it is calculated as k n a n b where k is a constant calculated based upon the window size.

The Connected Component and Optimal Partition algorithms were used to cluster the words within the same equivalence classes into more refined groups of similar words based on their EM similarity values. Their experiments showed that using their approach, aggressive stemmers like Porter for English can be improved. They also showed that a crude method like trigram can be employed in the first stage of that process with little loss of performance. They have also applied their trigram approach to other languages such as Spanish [20]. The main assumption in this approach is that words that belong to the same equivalence class (i.e. have the same root) will co-occur in the same document or text window.
The N-gram is a language-independent approach in which each word is broken down into substrings of length N. This approach has been applied to information retrieval in many languages such as English [9], Turkish [8], Malay [16] and Farsi [1] with varying degrees of success. In [13], Larkey et al. have used the bigram and trigram string similarity approach for Arabic text retrieval. In their experiment, bigrams have performed better than the trigrams; however the N-gram approach did not perform well in general. Authors have traced back the problem to the peculiarities imposed by the Arabic infix structure that increases the N-gram mismatches. However, they did not use stemming in their approach. In [19], the N-gram is used with and without stemming and it was shown that stemming resulted in minor improvements in the search results.

Light-10 [13] is a stemming tool based on Arabic morphological analysis that uses a rule-based affix-removal technique for light stemming. The prefix and suf-fix of words are removed if certain conditions were satisfied. For example the letter  X   X  can be removed from the beginning of the word if the remaining of the word has three or more characters. Light-10 was claimed to perform bet-ter than the other affix-removal approach proposed by Khoja and Garside [11], and Backwater Morphological Analyzer [3] . In Backwater Morphological Ana-lyzer approach each word is segmented into a prefix, a stem and a suffix using three dictionaries and three compatibility tables. It then produces a list of all the possible analysis of each word. In [14], Larkey et al. have applied the ap-proach proposed by Xu and Croft [19] on the Khoja stemmer [11] and Light-10 stemmer [13] and concluded that co-occurrence analysis has not improved the performance of the retrieval compared to the base stemmers. However, they re-ported that the corpus-based approach breaks down the equivalence classes into precise conflation groups with average size of five words. The reason for the low performance of their stemming strategy is claimed to be the complexity and the nature of the Arabic language. In the next section, our approach to the Arabic word conflation is explained in details. Our approach is based on the corpus-based approach developed by Xu and Croft [19] with some subtle differences. In their approach, they have used a trigram prefix matching for forming crude equivalence classes. That approach is not useful for Arabic because many nouns in Arabic have irregular plural forms.
As it is depicted in Table 1 the plural forms of the nouns have different trigrams than the original words. That is why, we have used an N-gram approach instead of relying on only 3-letter prefixes in forming equivalence classes.
An N-gram is a string of consecutive N characters. Generally an N-gram ap-proach involves representing a word with a vector of strings of length N formed from the consecutive letters of the word. The N-gram approach has mixed per-formances in information retrieval. In some languages like English, it results in a poor performance however in languages like Farsi, it has an acceptable perfor-mance [1]. As mentioned earlier, most Arabic words are made up of roots with three letters which led us to use trigrams for word segmentation. The general process undertaken is as follow: 1. The corpus is normalized by removing all the stopwords, numbers and dia-2. Words are passed to the N-gram algorithm and a set of overlapping trigram 3. A distance matrix is constructed and the Dice Distance is measured for each 4. The words have been clustered into large equivalence classes based on their 5. The EM measure described in the previou s section is used to calculate the 6. The Optimal Partition Algorithm (OPA) is used for clustering within the 7. We have also experimented with combining the EM(a,b) and the Dice simi-Table 2 illustrates the conflations generated for the word using Dice dis-tance, EM and SEM average with t =0 . 5. We have used a portion of INFILE 2009 Arabic text collection for running our experiments. This collection contains 100 , 000 Arabic newswires from Agence France Presse (AFP) for the years 2004, 2005 and 2006. There are also 50 queries (30 general queries about sport, international affair, politics, etc and 20 scientific and technology related queries). All the documents and queries are in xml format consisting of headline, keyword and description tags. This corpus is used because of the diversity in the documents and queries. In these experiments, due to limited computational power and memory issues, only 10% of the corpus is used for generating the equivalence classes but the information retrieval experiments are conducted on the whole collection. We used python for processing the XML files and working with matrix using Numpy and Scipy plug-ins. The Java Lucene is used as the default search engine for all runs. The TREC Eval tool [18] is used for evaluating the search results and calculating the recall and precision .
Table 3 reports the characteristics of the eight different sets of the conflation classes that have been generated for th ese experiments. Although it has been stated in [13] that large number of Arabic words in any corpus is unique, a large number of these words can be conflated w ith at least one other word using the Dice distance or the average of the Dice and EM measures. However, when us-ing the EM measure, more precise classes are generated which might sometimes lead to having only one word in most of the clusters. The first three runs in Table 3 ( Dice 0 . 5, Dice 0 . 6and Dice 0 . 7) are single stage runs. In these runs, the Dice distance and Complete Linkage clustering algorithm with different thresh-olds (0 . 5, 0 . 6and0 . 7) were used to create the equivalence classes which were later used in stemming the entire corpus. All of the queries were used for re-trieval and the precision, recall and pr ecision at document cut-off measures were calculated for each run.

The other runs are two stage runs as described in Section 4. The next three runs ( EM 0 . 5, EM 0 . 6and EM 0 . 7) applied the Optimal Partition Clustering algorithm and EM measure on the equivalence classes generated from the first stage with different thresholds. The last two runs ( SEM 0 . 5, SEM 0 . 6) applied the average Dice and EM measures along with the OPA clustering algorithm. In order to get a better understanding of the performance of the proposed meth-ods, we created two extra runs. One run used the Light-10 stemmer [14] for stemming the Arabic words and another run applied no stemming at all. Table 5 shows a comparison of these two runs with EM 0 . 6 run which is the best run in Table 4. By analyzing these results one can conclude that using any sort of stem-ming technique improves the precision by at least 50%. It can also be inferred that the precisions obtained using EM based clustering are very close to those obtained using the Light-10 stemmer. This implies that although the numbers of conflation classes with more than one word are not many, they still had a positive impact. These results also show that mere statistical analysis produced results comparable to stemming with linguistic knowledge. Figure 1 depicts the precision recall graph for the top3 runs along with Light-10 and no-stemming runs. As shown in Tables 4 and 5 and Figure 1, the Light-10 stemmer which uses linguistic knowledge is the best run. Most runs are similar to each other. However, the results from EM 0 . 6 run are very close to those of the Light-10 stemmer. It is safe to say that the EM measure in the second stage is necessary for eliminating erroneous conflations. It is also clear that using the Dice measure alone produces clusters that have disimilar conflated words. Since only 10% of the documents are used in generating the equivalence classes, it seems reason-able to believe that the result of EM 0 . 6 will reach or exceed Light-10 if a higher percentage of the text is used. In this paper, we have successfully modified the corpus-based stemming pro-posed by XU and Croft to be used for Arabic text. We generated many different variations of our approach and compared them to the Light-10 stemming which used linguistic knowledge and no stemming approaches. Our comparison was based on precision, recall and precision at document cut-off values of retrieving 50 standard queries on a large text collection. The experiments show that using stemming without any linguistic knowledge can perform less than but compa-rable to well known approaches based on the morphological analysis. In our approach, we used one stage and two stage models and our findings indicate that the second stage co-occurrence ana lysis is necessary to improve the con-flation classes and weed out incorrect groupings of the first stage. It was also noticed that using trigram reduces the chance of word conflation and results in the construction of many single word clusters. Therefore, it is possible that bi-grams will perform better by conflating more words and reducing the number of clusters with only one word. As part of the future work, we will use bigrams and hexagrams on the same corpus in order to investigate the effects of the length of roots in the Arabic language. Another future goal of ours is to improve our best performer ( EM 0 . 6) method with some linguistic knowledge. In this new approach, we will use a few clues to even further refine the equivalence classes produced. This refinement will be in the form of post processing and will include removing some words from equivalence classes or combining some the classes into larger units. We also intend to use only minimum morphological analysis in this new approach. We can also look into the implications behind other distance and similarity measures and diff erent threshold values.

