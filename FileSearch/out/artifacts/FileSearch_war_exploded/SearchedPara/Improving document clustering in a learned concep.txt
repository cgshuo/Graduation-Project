 1. Introduction performance. low to find less noisy representation space than the initial bag-of-words space.
Both approaches rely on the idea of replacing the usual BOW empirically show on Reuters-21578 , Reuters RCV2-French , performance on the original vocabulary space. The second approach is an extension of the variables which respectively identify the word topics and the document clusters. This allows using nal PLSA and the MM models operating in the original and the induced concept spaces. and summarize our contribution in Section 4. 2. Models our extension of the PLSA model. 2.1. Notations
We assume that the collection consists of a set of n unlabeled documents D  X f d further assume that the collection contains K latent document clusters A  X f a
B  X f b 1 ; ... ; b L g . 2.2. Multinomial mixture
In this framework each document is assumed to be generated by a mixture model: (Fig. 1 a).
 pressed as where p jk is the probability of generating word w j in document cluster a multinomial parameters for the cluster priors p  X  a k  X  and word generation probabilities p
K  X  0  X  is obtained randomly. We then iteratively estimate the probability that each mixture component a document d 2 D using the current parameters K  X  t  X  ( E-step imizing the complete data log-likelihood ( M-step ). 2.3. Probabilistic Latent Semantic Analysis  X  Choose a document d with probability p  X  d  X  .  X  Choose a topic b according to p  X  b j d  X  .  X  Generate a word w with probability p  X  w j b  X  .
 The model parameters in this case are and they are obtained by maximizing the (log-)likelihood, puting posterior probabilities of latent variables using the current parameters D  X  maximizing the log-likelihood function (4) in the M-step .
 lowing the Bayes rule: In using Bayes decision rule: When document clustering is performed with the decision rule 2.4. Concept learning concept space where clustering is performed.

Formally, we assume that each word w 2 V is generated by a mixture density: malized using a topic indicator vector C j  X f C hj g h for each word w model. Hence by denoting, for each document d i 2 D and cluster b eration probabilities q il :
From this assumption, the probability of a word w 2 V given a latent topic b Algorithm 1. The CEM algorithm for word clustering Input : repeat until convergence of L 2 ; Output : Word Clusters, P  X  t  X  of model parameters H  X  0  X  are estimated on the basis of randomly obtained word partitions P until the convergence of the complete data log-likelihood (7). In the with respect to each of its associated cluster indicators C this conditional expectation is equal to the posterior probability of the word w mated by the Bayes rule and the current model parameters H highest posterior probability estimated previously and new model parameters H plete data log-likelihood. Lagrange multipliers are used to enforce the model parameters are updated as: appearing in the document.
 2.5. An extended version of PLSA underlying generation process is as follows:  X  Pick a document d with probability p  X  d  X  .  X  Choose a document cluster a with probability p  X  a j d  X  .  X  Choose a word topic b with probability p  X  b  X  .  X  Generate a word w with probability p  X  w j a ; b  X  .
 two different semantic levels: document clusters and word topics.
In this case the generation of a word w within a document d can be expressed by the following probability:
Following the maximum likelihood principle, the model parameters are hence, and they are estimated by maximizing the log-likelihood function (4) using an model parameters:
In the M-step , new model parameters maximizing the expectation of the log-likelihood (4) are estimated: Algorithm 2. Extended version of PLSA Input : repeat until convergence of L 1 (Eq. (4)); Output : A generative classifier with parameters U  X  t  X  ing the E-step and M-step . Convergence is therefore guaranteed to a local maximum of the likelihood. iterative estimation): 3. Experimental setup topics). In the following, these models are denoted by MM notes document clustering with MM on the concept space induced by concept space induced by PLSA , C-PLSA is the document clustering with mance of the extended PLSA ( Ext-PLSA ) against all the previous clustering models as well as the rithms (Blei et al., 2003 ) and a NMF model obtained by minimizing the Frobenius norm. As KL-minimal to evaluate the performance of the proposed models. 3.1. Datasets
We conducted our experiments on Reuters-21578 , Reuters RCV2-French label.
 assigned the label of their smallest class ending to a monolingual case. rec. talk.) which resulted in 16,010 documents. We ignored in this case file headers and subject lines. 6990, 34,272, 38,630 and 11,170 words for respectively the sizes in the respective collections.
 performance are averaged over these 10 randomly selected subsets. 3.2. Evaluation criteria formance measures as follows. 3.2.1. Micro-averaged precision and recall the precision and recall of the class l from the following: The micro-averaged precision and recall are thus defined by: and, by Average precision . 3.2.2. Normalized Mutual Information assignments of instances and their underlying class labels and is given from the following expression: where n is the total number of documents, c the number of classes (or clusters), n range of NMI values is within [0,1] and it tends to 1 for increasingly better cluster qualities. 3.3. Experimental results ulary space into the new learned concept space (induced by the operating in the vocabulary space ( MM ) and in the concept space ( this topic occurring in the document.
 each collection was fixed to the number which provided the best clustering results of the corresponds to j B j X  10 on the Reuters-21578 , RCV2-French lection. The Table 2 illustrates the ability of CEM to identify word topics on the the optimal j B j values for Reuters-21578 , RCV2-French , number of word topics j B j tends to be small in our experiments. The Figs. 2 and 3 show the evolution of performance of the (induced by the CEM and the PLSA models).
 precision, recall, and the Average precision of C-MM are significantly better than the the case where classes are unbalanced (especially on Reuters-21578 trade in Reuters-21578 becomes even visible. These results suggest that the independence hypothesis of the vocabulary words occurring in a document be independent from one another. Fig. 2 shows the document clustering performance, on the four datasets, of the words space and in the concept spaces induced by the CEM and
In our experiments, the size of the concept space for Reuters-21578 higher values with the CEM algorithm on this collection.
 The first observation is that in the original bag-of-words space, gap in performance between these two models are the largest on the made by the PLSA model and discussed in Section 2.3. The MM cept spaces induced by the CEM and the PLSA algorithms (i.e.
 confirm our findings made previously, that the independence assumption made by the where each direction corresponds to a group of similar words. We also notice that both sults comparable to PLSA in terms of NMI and Average Precision. Moreover, the the P-MM model on the four collections, in both NMI and Average precision. results of C-MM and P-MM it becomes apparent that the CEM uments are similar .
 by the CEM algorithm ( C-PLSA ) as well as its extension Ext-PLSA On the Reuters-21578 and 20Newsgroups datasets, Ext-PLSA outperforms of concepts. The best performances of Ext-PLSA are obtained for 25 concepts on 20Newsgroups , and correspond to improvements of respectively 6% and 8% in average precision over of Ext-PLSA on Reuters-21578 , RCV2-French , 20Newsgroups and precision curves. On the WebKB collection, the performance of the both measures (except when the number of concepts is equal to 10). We further notice that C-PLSA shows performances similar to 20Newsgroups dataset, C-PLSA is consistently outperformed by the scribed in Section 2.4.
 Finally we notice that there is a big gap between the performance of on RCV2-French , 20Newsgroups and WebKB where initial dimension sizes are higher than the one of the This is consistent with the results of the Kmeans algorithm in which we used the Euclidean norm. The best Ext-PLSA model here is significantly better than almost all other models except the nearly the same. Note also that the clustering performance of rithm is considerably increased. This is especially true on the NMI measure, where, on the of C-MM is three times higher than for MM .
 cessor with 4G RAM. Comparatively, the proposed Ext-PLSA approach takes the same time in execution than but it is more efficient than the two latter on all datasets. 4. Conclusion on the Reuters-21578 , RCV2-French , 20Newsgroups and WebKB forms significantly better than the original PLSA model. Compared to the space induced by CEM , the join clustering of documents and words performed by the bias of the successive clustering steps done previously.
 Acknowledgements lence IST-2002-506778. This publication only reflects the authors view. References
