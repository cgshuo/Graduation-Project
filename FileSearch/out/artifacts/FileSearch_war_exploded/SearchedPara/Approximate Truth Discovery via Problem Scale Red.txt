 Many real-world applications rely on multiple data sources to provide information on their interested items. Due to the noises and uncertainty in data, given a specific item, the information from different sources may conflict. To make re-liable decisions based on these data, it is important to iden-tify the trustworthy information by resolving these conflicts, i.e., the truth discovery problem. Current solutions to this problem detect the veracity of each value jointly with the re-liability of each source for every data item. In this way, the efficiency of truth discovery is strictly confined by the prob-lem scale, which in turn limits truth discovery algorithms from being applicable on a large scale. To address this issue, we propose an approximate truth discovery approach, which divides sources and values into groups according to a user-specified approximation criterion. The groups are then used for efficient inter-value influence computation to improve the accuracy. Our approach is applicable to most existing truth discovery algorithms. Experiments on real-world datasets show that our approach improves the efficiency compared to existing algorithms while achieving similar or even better accuracy. The scalability is further demonstrated by exper-iments on large synthetic datasets.
 H.2.8 [ Information Systems ]: Database Management X  Data Mining ; I.2.m [ Computing Methodologies ]: Artifi-cial Intelligence X  Miscellaneous Truth discovery; problem scale reduction; recursive method; consistency assurance The increasing amount of data generated from the World Wide Web and the recently emerged Internet of Things (IoT) applications pose challenges to database research, char-acterized by the volume , velocity , variety and veracity fea-tures of Big Data. With advanced data extraction and col-lection technologies, information about a real-world entity can be obtained from various sources in both cyber and phys-ical worlds. The data provided by distant sources are inher-ently unreliable, due to the uncertain data X  X oises, missing updates, missing values, transmission errors, or maliciously manipulated data. Thus, multi-source data for describing the same entity are liable to be conflicting. To support re-liable decisions based on or to exploit the maximal value from multi-source data, it is critical to identify the trustwor-thy information from the multi-source inputs. This problem is non-trivial because the reliability of sources is often un-known apriori and ground truths are in most cases unavail-able. To complicate the matter, the number of sources and entities in a truth discovery problem can be extremely large, which requires high scalability of truth discovery algorithms.
While the truth discovery problem has been studied from different perspectives [12], it remains inefficient. Waguih et al. [18] experimentally evaluated the performance of sev-eral truth discovery algorithms on three computing nodes on both real-world and synthetic datasets with various con-figurations, and concluded that most algorithms have effi-ciency problems. For example, both the algorithms based on Maximum Likelihood Estimation (MLE) and those on probabilistic graphical approaches were too computationally expensive to be applied to large scale problems. Moreover, to pursue higher accuracy, recent approaches incorporate more factors, such as the hardness of fact [7], the effect of random guess [16], and data sufficiency [10], making them increasingly complex and less efficient.

To the best of our knowledge, no studies have focused on improving the efficiency of truth discovery algorithms. We perceive that the issue in truth discovery is a hybrid tech-nical problem, which inspires us to strategically reduce the problem scale and pursue approximate solutions in exchange for higher efficiency. Based on this insight, we propose an ap-proximate approach focusing on reducing the problem scale. In a nutshell, we make the following contributions: (a) Population Dataset Figure 1: The number of values claimed by each source: most sources claim very few values.
The rest of the paper is organized as follows. We dis-cuss the observations that motivate our work and define the truth discovery problem in Section 2. Section 3 introduces our approach. The experimental results are reported in Sec-tion 4. We discuss the related work in Section 5 and give some concluding remarks in Section 6.
This work is motivated by two sources of observations: real world datasets and existing truth discovery algorithms.
We investigate the distribution of values on different sources and items in various real-world datasets (e.g., Figure 1 and Figure 2 show the results in population [14] and biogra-phy [14] datasets, respectively). We observe long tail phe-nomenon [10] in most investigated datasets, which has the following characteristics: i) most sources claim very few val-ues and ii) most items have very few distinct values claimed.
Since the complexity of truth discovery is fundamentally determined by the problem scale X  X haracterized by the num-ber of links 1 between sources and values, an intuitive solu-tion for improving truth discovery efficiency is to reduce the number of nodes (sources and values) and links. We focus on two types of nodes in developing our approach:
A link exists between a source and a value only when the source claims the value. (a) Population Dataset Figure 2: The number of values claimed for each item: some data items have large numbers of values claimed. Figure 3: Examples of value distribution of the pop-ulation and biography datasets: varying distributions for different data items (Note that some values are not shown because they fall out of the range of dis-play).
We investigate the truth probability 2 distribution of ex-isting truth discovery algorithms during each cycle of itera-tion, if the algorithms require iteration, to examine if they
Truth probability means the probability of being true, which is calculated by normalizing the evaluation results, or verac-ity score, of each distinct value. Figure 4: The distribution of truth probability achieved by four truth discovery algorithms during their third cycle of iteration over the different pop-ulation sizes of Dallas, Texas. produce consistent evaluation results. Here, by consistency, we mean similar values should get similar truth probabili-ties . For example, in determining a city X  X  population size, two similar values, e.g., 8,390,482 and 8,416,535, should have similar truth probabilities. In contrast, the truth probability of 3,517,424 should be more different, since the value is less similar to the former two values. Enforcing consistent evalu-ation is crucial for ensuring the accuracy of truth discovery, as it improves the robustness of truth discovery algorithms in terms of smoothing the fluctuations and anomalies in the evaluation results. Intuitively, the truth discovery results cannot be justified without ensuring consistency because it would be insensible to assign very different truth probabili-ties to two values if they are extremely close.

Unfortunately, existing truth discovery algorithms lack consideration of such consistency, which obtain rather fluc-tuating truth probability distribution over the distinct val-ues for the same item (as shown by the examples in Fig-ure 4). Yin et al. [20] and Dong et al. [5] propose to improve the consistency by considering the inter-value influence in evaluating the values. They amend the evaluation results of an arbitrary value by: where v and v are two different values for the same item, sim ( v ,v )  X  [0 , 1] is their similarity;  X  and  X   X  are the ve-racity scores 3 of v before and after the amendment, respec-tively;  X   X  (0 , 1] is the influence factor which represents the influence strength.

Such amendment can significantly add to the computation load of truth discovery algorithms. It also requires manually defining the influence strength, which is sometimes tricky. Since in practice, we often regard a value as true as long as it is sufficiently close to the truth, a practical approximation may avail the amendment method in two aspects: i) reduc-
Veracity score does not necessarily equal to the truth prob-ability, but a higher score indicates a higher truth probabil-ity. ing the computation load and ii) avoiding the necessity of manually determining the influence strength.
Based on above analysis and insights, we propose an ap-proximate truth discovery approach, which groups sources and values to reduce the problem scale and considers the inter-group influence to improve the truth discovery accu-racy. Now we illustrate our approach with an example.
Example 1. Suppose we want to corroborate the birth dates of some historical figures. Five data sources provide such information and only s 1 provided all the true values (Table 1). A naive voting would incur a computation load of (6+5)+(6+4)  X  2=31  X  X or each of the three items (i.e., people), the algorithm performs two steps: i) scanning the six records to get the times of occurrence of each distinctive values, i.e., 5 , 4 ,and 4 for the three items, respectively, and ii) checking through the five distinctive values to find out the most frequently occurring value as the estimated truth. The average accuracy is 0 . 5 .

By considering the inter-value influence, the computation load is increased to (6 + P 5 2 +5)+(6+ P 4 2 +4)  X  2=75  X  between the two steps, the algorithm takes an additional step to calculate the mutual influence between each pair of differ-ent values, which is a permutation (denoted by P ). The resulting accuracy is 1 .

Now suppose a user only wants to know the birth years and months of these people instead of the exact dates. In this case, we can simply group the dates in the same year and month for each people, forming 3 , 3 ,and 2 groups of dates for the three people, respectively. Moreover, we find s and s 5 claim exactly the same groups of dates, so they can be grouped to form a joint source. Through such approximation, the computation load is decreased to (5+3)  X  2+(5+2) = 23 without veracity score amendment and (5+ P 3 2 +3)  X  2+(5+ P +2) = 37 with amendment. The resulting accuracy is also 1 . For now even if the user still wants an exact date, the approximation approach can still deliver the true values with an accuracy of 1  X  X y selecting the most promising date from the most promising group for each people using the naive voting method.
 Table 1: A motivating example: five sources provide information on the birth dates of three historical figures. Only s 1 provides all true values.
Let E be a set of entities. Each entity e  X  E is described by a set of attributes A .

Each pair ( e, a ), where e  X  E , a  X  A , represents a distinct data item.

Let S be a set of data sources. Each source s  X  S pub-lish some values (that are potentially true) on a subset of attributes in A regarding a subset of the entities in E .
Let V ( e, a ) be the set of all source-claimed values on data item ( e, a ). We consider the case where each item has only one true value, so at most one value in V ( e, a )canbetrue, while all the other values are false.

We denote by R a set of input records. Each record r  X  R describe a source X  X  claim on a specific item ( e, a ), which is a 5-tuple: &lt;id,s,e,a,v&gt; ,where id uniquely identifies the record and v is the value claimed by s on ( e, a ).
We assume all records from different sources have been transformed to a unified schema. Given a set of input records R published by a set of data sources S , the goal of truth discovery is to identify a single true value v  X   X  V ( e, a ), if the true value exists, for each attribute entity e  X  E and each attribute a  X  A .
Our approach facilitates efficient truth discovery by re-ducing the problem scale and improves accuracy by enforc-ing consistent evaluation of the truth discovery results. It has three main components (as shaded in Figure 5) besides the truth discovery module: content-based grouping  X  X or re-ducing the number of values, mapping-based grouping  X  X or reducing the number of sources, and veracity score amend-ment  X  X or enforcing consistency. The first two components are one-time jobs, while the last is incorporated into the truth discovery process.
The following subsections will introduce each of the com-ponents, respectively.
This component aims at reducing the problem scale by grouping similar values according to a user-specified approx-imation criterion. We assume user input as a personalized threshold (i.e., d in Figure 5) on the numerical distance be-tween the true values estimated before and after using the content-based grouping methods, meaning any two values can be grouped together only when their distance is smaller than d . We notice that treating similar values as a group does not necessarily impair the truth discovery accuracy: i) the truth estimated based on single values may be inaccu-rate by itself and ii) due to the unreliable and unevenly dis-tributed source inputs, the estimated true and false values could interleave in the spectrum of all values. Discovering truth based on the grouped values tends to be more robust in term of smoothing the local inconsistency and focusing on a global estimation. We also note that influence might still exists between the resulting value-groups. For example, Figure 6 shows an example of three values and the distances between them compared with the criterion d ,where v 2 can be grouped with either v 1 or v 3 . Whichever happens, the other value will still have an influence on v 2 from outside the group. In the worst case, if v 2 makes up a group of it-self, it would be influenced by both the other values from outside of the group.

Referring to the philosophy of clustering, we expect each value to be maximally influenced by the (other) values of the same group while minimally influenced by the values of other groups, to preserve the accuracy of truth discovery. Therefore, we define the content-based grouping problem as follows: given a set of values V and the criterion d (which value is specific to each data item), find such a division of V ,say V opt = { V 1 ,V 2 ,...,V n } ,that: where V i is a value-group, v x and v y are individual values, inf ( V i  X  V j ) quantifies the influence of V i on V j .The parameter d enables users to make personalized decisions on the degree of approximation. Given a larger d , less groups would be formed, and vice versa .

We prove the NP-Completeness of the content-based group-ing problem as follows.

Proof. The NP-Completeness proof proceeds via an effi-cient reduction from the Balanced Graph Partitioning prob-lem (BGP) [1]. BGP aims at partitioning graphs into equal sized components while minimizing the number of edges be-tween different components. The number of components immediately transforms into d when the values for each sin-gle data item are evenly distributed in their domain. If we replace the optimization objective of  X  X he number of edges between different components X  into  X  X he mutual influence between different sub-graphs X , BGP immediately transforms into the content-based grouping problem.

While clustering represents a technique for set division, we avoid considering clustering as an appropriate solution for three reasons: i) most clustering methods cannot deter-mine the optimal cluster number based on a single input such as d , ii) some do not guarantee to obtain the optimal results, and iii) it is hard to apply optimization techniques Figure 6: An example illustrating the circumstances that cause inter-group influence. Whichever another value (i.e., v 1 or v 3 ) v 2 is grouped with, or even v 2 makes up a group of itself, it will still be influenced by at least one other value from outside the group. such as branch and bound to accelerate the clustering pro-cess. Another option would be based on graph models. We could take all distinctive values claimed for a specific data item as nodes to construct a complete graph. Then we can remove the edges longer than d (resulting in G ) and parti-tion G so that each resulting sub-graph is still a complete graph. In this way, the content-based grouping problem can be solved approximately by adopting existing solutions to the Maximum Clique Problem (MCP) [19]. Unfortunately, our problem differs from MCP in that, instead of pursuing the largest complete sub-graph, it pursues all complete sub-graphs and the minimal influence among those sub-graphs. Moreover, even if a MCP solution is applicable, we need to apply the MCP solution multiple times to find all the com-plete sub-graphs X  X y each time finding and removing the new maximum clique from the remaining graph. Besides se-vere computation load, this approach does not guarantee to produce the optimal grouping results.

Our problem is special in that all values are unidimen-sional and can thus be sorted for given specific data item. This enables us to employ novel heuristics to improve ef-ficiency. We propose a recursive solution with branch and bound technique to optimally assign groups to the values. The components of this solution are described in Figure 7. Figure 7: The invocation relation between the algo-rithms for content-based grouping.

Algorithm 1 is the entry of our content-based grouping solution. Given a specific data item, the algorithm first sorts the values for the item (line 1), and then compares the distance between each pair of adjacent values with the criterion d (lines 4-9). Whenever the distance between two values surpasses d , the list is divided in-between. Each sub-list is either regarded as a cluster of itself (if it contains a single value; lines 12-13) or applied the recursive method (otherwise; lines 14-18) to find the optimal group assign-ment. Since no influence exists between different sub-lists (due to the division criterion d ), the grouping results for ev-ery sub-list together form the grouping results regarding the given data item.

Algorithm 2 recursively constructs the potentially opti-mal (and possibly partial) group division for the input sub-list. It takes a preorder and depth-first traversal of the tree-like search space. Given a (partial) group division solution, which is a (either complete or incomplete) list of groups formed from the sublist, the algorithm first checks whether the inter-group influence of this (partial) solution exceeds the historical best X  X hat is the bounding condition, which Algorithm 1: Content-Based Grouping is the minimal inter-group influence ever achieved by any complete group division solution. If it does, the solution will be abandoned; otherwise, the algorithm examines its completeness X  X hether the solution contains all the values of the sublist (line 2). The algorithm returns the solution and corresponding evaluation result when the solution is complete (line 4-5; L =  X  ).

In case the solution is incomplete (line 6), the algorithm checks whether the first value of the sublist can be added to an existing group (line 10). If true, the process diverges into two branches: i) adding the value to the existing group (lines 11-14) and ii) setting up a new group for the value (lines 15-18). In the first case, Algorithm 2 invokes itself, taking the updated solution (with the new value added to the last group) and the updated sublist (with the first value removed) as inputs (line 12). If the solution returned by the self-invocation is not null  X  X eaning the returned solution is better, the bounding condition ( B best ) will be updated with the evaluation result of the returned solution. In the second case, Algorithm 2 invokes itself only under one condition: the distance between the new value ( v )andthelastelement of the third last group in Solution is larger than d (line 17). In this case, Algorithm 2 takes the new solution (with a new group added) and the updated sublist (with the first value removed) as inputs (line 19). Differing from the first case, B best will not be updated even if a better solution is found (the returned solution = null ) X  X nformation from the last branch can only be passed to its upper-layers (instead of its siblings) in the tree-like structure.

We prove that only under the condition specified in line 17 could the new solution possibly be optimal as follows.
Proof. Given a (partial) solution that consists of four consecutive groups, C i , C i +1 , C i +2 ,and C i +3 , suppose the minimal distance between the elements of C i and C i +3 is Algorithm 2: Dividing Sublist smaller than d . We can easily get a better solution by merg-ing C i +1 and C i +2 . Thus the given (partial) solution could not be the optimal (partial) solution.

For the case that the value cannot be added to an exist-ing group (line 22), it is tackled in the similar way as we tackle the second branch above (lines 23-29). Note that, in Algorithm 2, we intentionally put the first case before the second, i.e., greedy selection  X  X he first case does not add to the total inter-group influence, but the second case does.
Algorithm 3 evaluates the (partial) grouping solution for a specific sub-list. Given a group within the solution, the algorithm calculates the influence of at most five groups on this group X  X wo precedent groups (line 6), two succeeding groups (line 8), if they exist, and the group itself (line 4). The algorithm returns the extent (i.e., a ratio) to which the group is influenced by other groups (line 9). The condi-tions in line 17 and line 25 of Algorithm 2 ensure a group is influenced by at most four other groups according to the user-specified criterion d .
 Algorithm 3: Calculating SumInf
In Algorithm 3, the influence between two values, e.g., v on v , is calculated as follows: Different from traditional linear combination, we assume Gaussian density of the influences to a specific value. In this way, only the similar values would have a major influence on a value. In particular, for each item, Eq.(3) transforms d into an acceptable variable domain of Gaussian density function. We derive different influence domains to represent different levels of user confidence. For example, suppose a user pro-poses some d with a confidence degree of 0.8 (  X  [0 , 1]), which approximately equals to the Area Under the Curve (AUC) of Gaussian density function within the domain of (  X   X , +  X  ), we get a  X  which is equal to d in Eq.(3). Given a full con-fidence of approximately 1, the resulting influence domain would turn into (  X  3  X , +3  X  ).
This component aims at reducing the problem scale by grouping sources according to their associations with the value-groups. The mapping-based grouping (Algorithm 4) involves two steps: i) extracting (line 1-5) and sorting (line 6) each source X  X  claims and sorting sources by their claim sizes (line 7), and ii) grouping the sources that make the same set of claims (lines 8-23): the algorithm only compares adjacent sources in the sorted list and makes further com-parison only when the sources have the same claim size (Line 12).

Table 2 shows two examples of the grouping results on the population dataset [14], where four Wikipedia users provide the same population size for New Orleans and another four Algorithm 4: Mapping-Based Grouping users for Mexico City . The mapping-based grouping method reduces the total number of sources (or joint source) of the dataset from 4264 to 3874. It should be noted that, the grouped sources do not only receive the same assessment, but also have the same effect as the group of individual sources do before they are grouped together in the truth discovery process.
 Table 2: Two examples of the mapping-based group-ing results on the population dataset [14]: each of the two cities has four contributors claiming the same population size.

This component aims at improving the accuracy of truth discovery by enforcing consistent evaluation of value-groups. The amendment (Algorithm 5) relies on pre-computed inter-group influence matrix to update the veracity scores, where c ( C x )and X  c ( C x ) are the original and amended veracity scores of value-group C x , respectively, and inf ( C x  X  C i )isthein-fluence of C x on another group C i . The matrix is calculated in a similar way as what Algorithm 3 does in calculating the inter-group influence. The only difference is that instead of summing up the influences of all groups, here we separately calculate the influence of different groups and store this in-formation in a pentadiagonal matrix. Algorithm 5 calculates the veracity score of a value-group by linearly combining the veracity scores of all relevant groups using their influences on this value-group as weights. We are aware that there ex-ist more compact ways to store these quintuples, but they are out of the scope of this paper.

Algorithm 5 reduces the time complexity of inter-value in-fluence computing by an order. Suppose  X  e  X  E ,  X  a  X  A , |
V ( e, a ) | = V and | LC ( e, a ) | = G ,where LC ( e, a )isthe set of all value-groups for data item ( e, a ). Thetimecom-plexity of traditional methods is O ( L  X  V )  X  O ( | E || where L is the number of links between sources and val-ues. In contrast, Algorithm 5 achieves a time complexity of O ( L )  X  O ( | E || A | X  G ), where L is the number of links between source-groups and value-groups. Furthermore, Al-gorithm 5 does not require configuring additional param-eters, such as  X  in Eq.(1), but automatically incorporates the influence of different groups based on probabilities. The amended veracity scores remain in the same range of the original scores, therefore require no additional efforts for normalization.
 Algorithm 5: Veracity Score Amendment
Our approach recommends the group with the highest ve-racity score as the estimated truth. But we can also es-timate a single value as truth by using the weighted me-dian method 4 . The procedure for weighting the values is similar to lines 2-12 in Algorithm 3: given each value v of the recommended group g , we calculate the influence of val-ues on both inside and outside of the group, i.e., inf ( v and inf ( v ) outside , respectively. Then, we assign v with weight we can output the weighted median as the estimated truth.
In this section, we evaluate our approach using both real-world and synthetic datasets. We first discuss the experi-mental setup in Section 4.1 and then report the experimen-tal results on real-world datasets in Section 4.2. Since the real-world datasets are limited in scale and diversity, we fur-ther evaluated the sensitivity and scalability of the proposed approach based on synthetic datasets.
We introduce the measures to evaluate the algorithms X  performance and the baselines methods, respectively.
We apply three measures to evaluate the performance. For all these measures, a smaller value indicates a better result. http://en.wikipedia.org/wiki/Weighted median/
All the following algorithms are used in an unsupervised manner and compared on numerical datasets:
In this paper, we focus on discovering a single true value for each data item from the numerical data, but leave the incorporation of source dependency and other data types for future work. We implemented the algorithms in Java 7 and ran experiments on 3 PCs with Intel Core i7-5600 processor (3.20GH  X  8) and 16GB RAM.
We employ two real-world datasets 5 in the experiments and use the ground truth provided by the original contribu-tors as gold standards.

The population dataset [14] contains 51,761 records. Each record describes a user X  X  answer to a city X  X  population size. After eliminating duplication and ambiguity, we got records about 4,264 sources claiming 49,956 values on 41,197 data items.
 The biography dataset [14] contains 11,099,730 records. Each record describes a user X  X  answer to the birth or death date of a person. After removing the irrelevant, duplicated, and ambiguous records, we merged the birth and death dates of the same person made by the same user and got 1,148,644 users and 3,211,983 claims related to 116,499 people.
Until now, these are the only two public numerical datasets, which are also used in recent works [22, 14].
Figue 8 shows the comparison of different algorithms in terms of their computation time before and after adopting our approach, based on the population and biography data-sets . The results show that our approach reduces the com-putation time of the algorithms significantly, for some cases even by half. The effect is especially evident on the popula-tion dataset , where claims are sparse. Since our approach is not applicable to GTM, its computation time remains un-changed.

In Table 3 and Table 4, we summarize the performance of the algorithms before and after the adoption of our ap-proach in terms of MAE and RMSE. The results show that our approach does not significantly decrease the truth dis-covery accuracy. Indeed, all algorithms achieve similar or even higher accuracy than their original implementations under the optimal configurations of d . This indicates that, if configured properly, our proposed approach does not only reduce the computation time, but also improves the truth discovery accuracy.
 Table 3: Comparison on the Population dataset: the measures marked with asterisk represent the per-formance after adopting our approach; F# is the percentage of the estimated truths that have larger deviations from the actual truths than d .
 Table 4: Comparison on the biography dataset: the meanings of the notations are the same as the ones in Table 3.

We generate synthetic data to evaluate the effectiveness of our approach on larger datasets. In particular, we gener-ate synthetic records based on a real-world dataset, i.e., by adding new sources and data items to the population dataset .
We generate a new source by randomly selecting an ex-isting source as reference X  X he new source will have exactly the same number of values on the same sets of items as the referred source. We then associate a source to the values in two steps: i) for each item, we divide the whole value range into a series of intervals, making sure each interval contains exactly one value. Given two adjacent intervals, their boundary is the average value of the two values that reside in the two intervals, respectively; ii) each time we add new association to a value: we first randomly select an in-terval which the value to be associated with are supposed to reside in, namely I , and then associate the source with the existing value in I with a probability  X  , and with a random value within the interval I with a probability of 1  X   X  .We assume a slight increase in the total number of sources as-sociated with the original values by defining  X  =0 . 01( &gt; 0). Data items are generated in a similar way.
We conduct experiments to investigate the effect of our approach on improving the algorithms X  scalability. We first fix the number of data items and vary the number of sources from 1,000 to 10,000, thus increasing the number of values for each data item. Then we fix the number of sources and vary the number of data items. Figure 9 shows how the reduced amount of computation time changes as the number of sources and the number of data items grow, after applying our approach. The results show the reduction amount keeps increasing as the two numbers grow. Since d remains fixed in both above experiments, the number of value-groups formed by our approach does not significantly change. But generally the more sources considered, the larger the source-groups tend to be, as the sources has a higher chance to be merged into an existing group. Experiments with respect to both above numbers reveal that the reduced computation time grows at lease linearly with the increased record size.
We study the impact of d to the effectiveness of our ap-proach based on a synthetic dataset of 10,000 sources and 45,000 data items, where each item has on average 1 to 1,000 distinct values. Figure 10 shows the algorithms X  per-formance with respect to a varying d within the range of [0 ,SD/ 10], where SD is the standard deviation of all values for each data item X  X herefore, each item will have no more than 100 groups. Specially, when d = 0, the problem de-grades to the circumstance of estimating the truths without adopting any grouping methods. We observe an exponen-tial decrease in the computation time of all algorithms as d grows. As for the error rates, both MAE and RMSE tend to be low when d is small, but gradually increase as d grows.
Truth discovery has been a longstanding problem in the data quality research community [4, 13]. We classify existing Figure 9: Computation time reduction under: (a) varying number of sources, (b) varying number of data items. truth discovery approaches into four categories and review them in this section.
 Primitive methods . These methods take the mean , median , or majority voting results of the values for each data time as estimated true values [3]. They are efficient but often in-accurate due to the neglect of varying reliabilities of sources in the real world.
 Iterative algorithms . These methods jointly compute source reliability and confidence of value through an iterative pro-cedure. Multiple ingredients, such as the inter-value influ-ence [20], inter-value exclusion[7], source dependency [5], hardness of fact [7], as well as human knowledge [15] are incorporated to pursue more accurate estimation.
 Probabilistic models . Many researchers model truths as la-tent variables in probabilistic graphical models to support truth discovery [16, 22]. Such models generally require tremen-dous computation time.
 Optimization approaches . Yin and Tan [21] and Li et al. [11] respectively model the truth discovery problem as optimiza-tion problems and developed corresponding solutions.
Other related approaches include crowdsourcing [9] and web search [2, 17]. Crowdsourcing systems assess worker quality in accomplishing their crowdsourcing tasks while web search methods recommend trustworthy source links to an-swer user X  X  queries. Truth discovery differs from crowdsourc-ing in pursuing only the factual truths and from web search in operating on well-formatted records instead of raw data.
The most related research is on source selection. Dong et al. [6] propose to select only partial sources for truth dis-covery, so as to reduce the integration cost while ensuring certain accuracy. Li et al. [9] present a framework for discov-ering the common characteristics of high quality workers, so as to facilitate the selection of high quality workers for future tasks. Gupta et al. [8] evaluate source quality and select only trustworthy sources for each group of entities (obtained by clustering) to improve truth discovery accuracy. To the best of our knowledge, there is no previous work that specializes on improving the efficiency of truth discovery, nor utilizing grouping methods to reduce the problem size, which is the main contribution of this paper.
In this paper, we propose an approximate truth discovery approach, which improves the truth discovery efficiency via problem scale reduction. We develop content-based group-ing and mapping-based grouping methods to reduce the sizes of sources and values, respectively. Veracity score amend-ment methods are used to improve the truth discovery ac-curacy with efficiency. The approach features approximate truth discovery according to a user-specified criterion, which can achieve a personalized balance between the efficiency and accuracy. The approach has been applied to several existing truth discovery algorithms. Experiments on both real-world and synthetic datasets demonstrate the effective-ness of our approach. [1] K. Andreev and H. Racke. Balanced graph [2] R. Balakrishnan and S. Kambhampati. Sourcerank: [3] J. Bleiholder and F. Naumann. Conflict handling [4] J. Bleiholder and F. Naumann. Data fusion. CSUR , [5] X. L. Dong, L. Berti-Equille, and D. Srivastava. [6] X. L. Dong, B. Saha, and D. Srivastava. Less is more: [7] A. Galland, S. Abiteboul, A. Marian, and P. Senellart. [8] M. Gupta, Y. Sun, and J. Han. Trust analysis with [9] H. Li, B. Zhao, and A. Fuxman. The wisdom of [10] Q. Li, Y. Li, J. Gao, L. Su, B. Zhao, M. Demirbas, [11] Q. Li, Y. Li, J. Gao, B. Zhao, W. Fan, and J. Han. [12] X. Li, X. L. Dong, K. Lyons, W. Meng, and [13] F. Naumann, A. Bilke, J. Bleiholder, and M. Weis. [14] J. Pasternack and D. Roth. Knowing what to believe [15] J. Pasternack and D. Roth. Making better informed [16] J. Pasternack and D. Roth. Latent credibility analysis. [17] V. Vydiswaran, C. Zhai, and D. Roth. Content-driven [18] D. A. Waguih and L. Berti-Equille. Truth discovery [19] Q. Wu and J.-K. Hao. A review on algorithms for [20] X. Yin, J. Han, and P. S. Yu. Truth discovery with [21] X. Yin and W. Tan. Semi-supervised truth discovery. [22] B. Zhao and J. Han. A probabilistic model for
