 Data clustering, also known as cluster analysis, is one of the most fundamental problems and an active and important research area in data mining and machine learning. Generally speaking, the goal of clustering is to discover meaningful and natural groupings in data automatically. Usually it involves partitioning a finite set of data objects into several disjoint cl usters so that intra-cluster similarity and inter-cluster separability are maximized. Since no labeled data (supervised information) are available to guide the partitioning process, data clustering falls under the category of unsupervised learning.

Various clustering techniques have been proposed over the years. Among these techniques, one of the most interesting approaches is spectral clustering [11], which has received considerable attentio n in recent years. Spectral clustering algorithms aim to recover the clustering structure in data by exploiting the top eigenvectors of a specially constructed mat rix. Spectral clustering algorithms are often easy to implement using linear algebra software packages and can be very efficient if the specially constructed mat rix is sparse. Motivated from a graph partitioning perspective, various spectral clustering algorithms have been pro-posed based on different graph cut objectives such as ratio cut [4] and normalized cut [11]. Spectral clusterin g techniques are also used in clustering methods which are based on the local learning idea [13,16]. A lot of applications exist for spec-tral clustering techniques, such as image segmentation [11,17], circuit layout [4], speech separation [1] and so on.

In this paper, we derive a new clustering algorithm that also utilizes the top eigenvectors of a specially constructed matrix, hence inheriting the advantages of spectral clustering. However, our algorithm is motivated by the local recon-struction approach to dealing with high dimensional data that lie on or near a low dimensional manifold [9]. The local reconstruction approach tries to capture geometric properties of the underlying data manifold by locally reconstructing each data point from its neighbors. The local reconstruction perspective has al-ready proved very useful in dimensionality reduction [8,9] and semi-supervised learning [15].

Based on the local reconstruction persp ective, we propose a novel clustering algorithm that effectively utilizes local reco nstruction information. Our proposed approach consists of two parts. In the first part, the local reconstruction weights are obtained by minimizing the reconstruction error between each data point and the reconstruction from its neighbors. Important geometric characteristics of local neighborhoods can be preserved b y the local reconstruction weights [9]. In the second part of our approach, the reconstruction weights are then used to produce the final clustering result.

In our clustering approach, the reconstruction of each data point is performed in the reproducing kernel Hilbert space. Thus, nonlinear relations between each data point and its neighbors can be captured. Besides, an entropy regulariza-tion term is incorporated into the reconstruction objective function so that the smoothness of the reconstruction weights can be explicitly controlled. In the second part of our algorithm, the special structures of the sca led cluster label matrix and the reconstruction weight matrix make the optimization problem much easier to analyze, and spectral clu stering techniques can be employed to solve the clustering problem efficiently. Experimental results on a number of real-world datasets demonstrate that our algorithm performs well relative to other spectral clustering approaches, which va lidate the effectiveness of our approach in obtaining good clusterings.

The remainder of the paper is organized as follows. We introduce the formu-lation of the proposed clustering model in Section 2. In Section 3, we derive the detailed clustering algorithm. Experimental results are presented in Section 4. Section 5 concludes the paper and discusses future works. 2.1 Notations First we introduce some notations. Boldface lowercase letters, such as x and y , denote column vector s. Boldface uppercase letters, such as M and W ,denote matrices. M T denotes the transpose of M . W  X  0 means that every entry in W is nonnegative. 1 m  X  R m is a vector of 1 X  X .  X  m = w  X  R m | w T 1 m =1 , w  X  0 is the probability simplex. For any w  X   X  m , the elements of w are nonneg-ative and sum to one. I m  X  R m  X  m is the identity matrix of order m .For x =[ x 1 ,...,x m ] T  X  R m , x 1 = m i =1 | x i | denotes the 1 norm. The set of nonnegative real numbers is denoted as R + . For a square matrix A  X  R m  X  m , Trace( A )isthetraceof A , i.e., the sum of the diagonal elements of A . 2.2 Data and Label Representation Let X X  R d denote the input space from which n data points, x 1 ,  X  X  X  , x n ,where x the goal of data clustering is to find a set of disjoint and exhaustive clusters {  X  |  X  | is the number of points in the l -th cluster.

Given a positive semi-definite kernel function K (  X  ,  X  ), the data points from the original input space X are mapped to a possibly infinite dimensional reproduc-ing kernel Hilbert space F [10]. The mapping is denoted as  X  : X  X  X  .The reconstruction of each data point x i is then performed in F so that nonlinear relations between x i and its neighbors can be captured. We denote the inner product in F as  X  ,  X  F ,so
A clustering solution is represented by a Scaled Cluster Label Matrix Y = [ y Y is the unknown variable in our clustering model. Once Y is known, the resul-tant clustering can be obtained easily. So each data point x i is associated with a cluster label y i  X  R c . The scaling in Y is used for producing balanced clusters.
In the following, neighborhood N i denotes a set of nearest neighbors of point x 2.3 Clustering Model The proposed clustering model consists of two parts, which produce the local reconstruction weig hts (represented by W ) and final clustering ( Y ) respectively.
In the first part, we try to locally reconstruct each data point  X  ( x i )from where w ij  X  0and x j  X  X  i w ij = 1. All the reconstruction weights form a weight matrix W =[ w ij ]  X  R n  X  n where w ij =0if x j /  X  X  i . Besides, we assume that w is to be minimized. Important geometric c haracteristics of local neighborhoods can be preserved by the local reconstruction weights w ij ,whichinturncanbe used to obtain the final clustering result. Here, a natural choice for the error
In the second part, the reconstruction weights are used to obtain the clus-tering result. The objective is to minimize the cluster label reconstruction error E of each data point can be reconstructed f rom its X  neighbors X  cluster labels, us-ing the same reconstruction weights obtained in the first part of our clustering model. Here, we use sum of absolute error as the discrepancy measure, namely, L
The two parts of our clustering model are formalized as follows.
In Eq.(3),  X  H ( w i ) is the entropy regularization term which explicitly con-trols the smoothness of the weight components in w i .  X   X  0 is a pre-specified parameter. A larger value of  X  means that more uniform weights are preferable. H (  X  ) is the generalized entropy which will be defined and discussed in Section 2.4. A key property of H (  X  ) is that the more uniform the weights in w i are, the larger the value of H ( w i ) becomes. 2.4 Generalized Entropy We introduce the concept o f generalized entropy as a measure of the degree of uncertainty within a discrete probability distribution (over a set of m elements) that can be represented by a vector w =[ w 1 ,...,w m ] T  X   X  m . A definition of generalized entropy recently proposed i n [7] is provided in the following: Definition 1. Generalized entropy is defined as a mapping that satisfies the following two criteria (symmetry and concavity): 1. For any w a  X   X  m ,andany w b  X   X  m whose elements are a permutation of 2. H (  X  ) is a concave function.
 This definition reflects an important property of generalized entropy H (  X  ): more uniform elements of w indicate larger values of H ( w ). For example, if v 1 = [0 . entropies proposed in the literature are special cases of Definition 1 [7]. Some examples are given as follows: 1. H ( w )= m i =1  X  w i log( w i ), which is the Shannon entropy . 2. H ( w )=1  X  w T w , which will be referred to as 2 -entropy . In this section, we derive a clustering algorithm based on the clustering model presented in the previous section. Our a lgorithm consists of two parts. The first part computes the local reconstruction weight matrix W by solving the opti-mization problem (3). The second part computes the final clustering by solving problem (4). 3.1 The Computation of W the element in the i -th row and j -th column of K . The kernel matrix over N i is denoted as K i  X  R n i  X  n i , which is a submatrix of K corresponding to the data [
K ( x kernel function values between data point x i and its neighbors.

After some algebraic operations using Eq.(1), problem (3) can be simplified to an equivalent problem as follows. Even with generalized entropy defined i n Section 2.4, this sub-problem for com-puting w i is still a small-scale (since n i n ) convex programming problem, since H (  X  ) is a concave function and all the constraints with respect to w i are linear. There X  X e very effective and efficient algor ithms that can solve convex programs reliably [2]. For the algorithm derived in this section, we X  X l use H ( w )=1  X  w T w which is the 2 -entropy . Problem (3) is then equivalent to the following quadratic programming problem. 3.2 The Computation of Y In this subsection, we want to obtain the final clustering by optimizing problem (2) and the 1 norm  X  1 in problem (4). However, two key properties of A in (7) ensure that the problem can b e optimized using spectra l clustering techniques. The two key properties of W are follows An important property of H is that H T H = I c .

Also, we define A = W + W T and M =Deg( A )  X  A where Deg( A )isthe degree matrix of A , i.e., the diagonal matrix whose diagonal elements are the sums of rows of A .

Then the optimization problem in (4) can be proved to be equivalent to the following problem [13]: A sketched proof can be found in the Appendix.
 Relaxation. As is done in a typical spectral clustering algorithm, H defined in (8) is relaxed to be any matrix in H  X  R n  X  c | H T H = I c . The relaxed op-timization problem is in the following: According to the Ky Fan Theorem [18], the globally optimal solution set of problem (10) is as follows: where H  X  R n  X  c is formed by the eigenvect ors corresponding to the c smallest eigenvalues of M .
 Discretization. In this step, the solution H to the relaxed problem (10) needs to be discretized to produce a clustering result. We use the discretization method proposed in [17], which tries to rotate H so that it X  X  close to a cluster indicator matrix. The details of this method can be found in [17]. 3.3 Main Algorithm In practice, the number of neighbors n i for each data point x i is often fixed to a small value k n , i.e., n i = k for all 1  X  i  X  n . Thus, each neighborhood N i distance metric. The distance between x i and x j is measured by
D (  X  ( x Therefore, given a kernel matrix, the neighborhood of each data point can be computed easily. Here, k is provided by domain experts.

The main algorithm is summarized in Fig.1. We name the algorithm Regu-larized Local Reconstructi on for Clustering (RLRC). 3.4 Computational Complexity The main computational load comes from step 1, 2 and 5. Given a kernel ma-trix, the k -nearest neighbors of each data point can be computed with complex-quadratic programming problem requires time O ( k 3 ), the overall time complex-ity for step 2 is O ( nk 3 ). For step 5 which eigen-decomposes a n  X  n matrix with O ( nk ) nonzero elements to obtain the top c eigenvectors, the time com-plexity is O ( n 2 c ) without special optimizations. Therefore, the time complexity of RLRC algorithm in Fig.1 is O ( nk 3 + n 2 c ). Note that the complexity of step 5 can be reduced to subquadratic in n by adopting more efficient methods for sparse eigen-decomp osition problem [5].
 In this section, we conduct experiments on a number of real-world datasets to evaluate the effectiveness of our cluste ring algorithm by comparing its perfor-mance with other related methods. 4.1 Summary of Datasets In this subsection, we will introduce the datasets used in our experiments. We use 12 document datasets 1 from the CLUTO toolkit [19]. Table 1 summarizes the basic information of the datasets.
 Dataset cranmed is composed of the CRANFIELD and MEDLINE abstracts 2 . The three datasets k1a , k1b and wap are from the WebACE project where each document corresponds to a web page listed in the subject hierarchy of Yahoo!. Datasets re0 and re1 are derived from Reuters-21578 text collection [6]. The six datasets tr11 , tr12 , tr23 , tr31 , tr41 ,and tr45 are from the TREC collec-tion [14]. The processed datasets are also available from http://shi-zhong. com/software/docdata.zip . These datasets provide a good representation of different data distributions and characteristics, and so are good candidates for evaluating different clustering algorithms. 4.2 Clustering Evaluation Criteria ered clustering algorithms. Given class labels, we evaluate the clustering results using two external validity measures, Normalized Mutual Information ( NMI ) [12] and Clustering Accuracy ( Acc ) [13,16]. NMI is an information-theoretic measure that X  X  previously defined in [12] to compare different clusterings. Acc [13,16] is calculated based on the one-to-one correspondence between the clusters which maps each cluster index i toaclassindex  X  ( i ). Acc is calculated as follows: where n i, X  ( i ) is the number of points which are in both the i -th cluster and  X  ( i )-th class. Note that larger values of NMI and Acc suggest better clustering solutions. 4.3 Experimental Settings We normalize each document vector (Bag-of-Words) to unit norm and then the cosine kernel [16] is adopted as the kernel function in all the experiments. We compare the performance of the following clustering algorithms:  X  Spectral clustering with normalized cut (NCut) [11]. The symmetric  X  Local Learning based Clustering Algorithm 3 (LLCA) proposed in [16].  X  Clustering via local regression (CLOR) proposed in [13]. This algorithm is  X  Our proposed algorithm RLRC. We fix the regularization parameter  X  =1 All the above algorithms cluster the data by utilizing local neighborhood infor-clustering techniques to optimize the objective functions. We use the same dis-cretization method 4 for all of them. Note that in all the experiments, the number of classes c is provided to all the clustering algorithms. 4.4 Performance Results In this subsection, we will provide an empirical study of the considered algo-rithms. Clustering performance results on the datasets in terms of both NMI and Acc values will be compared and discussed .

In particular, we aim to address the following two important questions: (1) How effective is the proposed algor ithm RLRC compared with the other (2) How will the performance of RLRC change with different neighborhood sizes?
Generally, the optimal value of k is not easy to determine. In practice, we choose k on the order of log( n ) so that the k -nearest neighbor graph is asymp-totically connected under some special conditions [3]. Therefore, for datasets with size on the order of 1000, we can choose k to be a small multiple of 10. The experimental results on the 1 2 datasets when neighborhood size k =40are presented in Table 2. It can be obser ved that CLOR and the proposed algo-rithm RLRC achieve the best NMI and Acc values. On 8 out of 12 datasets, our algorithm RLRC achieves the best NMI and Acc values. On dataset k1a ,our algorithm achieves the best NMI value but CLOR achieves the best Acc value. On dataset re0 and re1 , our algorithm achieves the best Acc values but CLOR achieves the best NMI values. On dataset tr41 ,CLORoutperformsRLRCwith respect to both NMI and Acc values. However, the NMI and Acc values achieved by our algorithm RLRC are very close to those achieved by CLOR on dataset tr41 . The clustering results in Table 2 dem onstrate the effectiveness and poten-tial of our proposed clustering approach in discovering accurate clusterings.
We also conduct experiments on the considered datasets with different neigh-borhood sizes k . Figure 2 shows how clustering performance results ( NMI )vary with different values of k on dataset k1a and k1b . The horizontal axis denotes the neighborhood size k and the vertical axis denotes the clustering performance measured by NMI . With different values of k , our algorithm RLRC achieves more stable results compared with other algorithms. Among the remaining three algorithms, CLOR is more stable than the other two algorithms with respect to different values of k .
 In this paper, we propose a new clustering algorithm, namely, Regularized Local Reconstruction for Clustering (RLRC), which is motivated by the local recon-struction approach to dealing with high dimensional data that lie on or near a low dimensional manifold [9]. Our clustering method effectively utilizes local re-construction information which captures important geometric characteristics of local neighborhoods by locally reconstructing each data point from its neighbors. Spectral clustering techniques can be emp loyed to solve the clustering problem efficiently. Experimental results on a nu mber of datasets demonstrate the ef-fectiveness of our proposed algorithm. Future work includes how to select the neighborhood size and the regularization parameter automatically.
 Acknowledgments. This work is supported in part by NSFC grants 60673103 and 60721061.
 Proof:
