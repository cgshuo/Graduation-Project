 AMI Meeting Facilitator is a system that p er-forms topic segmentation and extractive sum-marisation. It consists of three comp onents: (1) a segmenter that divides a meeting into a num-b er of lo cally coherent segments, (2) a summa-rizer that selects the most imp ortant utterances from the meeting transcripts. and (3) a com-pression comp onent that removes the less im-p ortant words from each utterance based on the degree of compression the user sp eci ed. The goal of the AMI Meeting Facilitator is two-fold: rst, we want to provide sucient visual aids for users to interpret what is going on in a recorded meeting; second, we want to supp ort the devel-opment of downstream information retrieval and information extraction mo dules with the infor-mation ab out the topics and summaries in meet-ing segments. 2.1 Segmentation The AMI Meeting Segmenter is trained using a set of 50 meetings that are sep erate from the in-put meeting. We rst extract features from the audio and video recording of the input meeting in order to train the Maximum Entropy (Max-Ent) mo dels for classifying topic b oundaries and non-topic b oundaries. Then we test each utter-ance in the input meeting on the Segmenter to see if it is a topic b oundary or not. The features we use include the following ve categories: (1) Conversational Feature : These include a set of seven conversational features, including the amount of overlapping sp eech, the amount of silence b etween sp eaker segments, the level of similarity of sp eaker activity, the numb er of cue words, and the predictions of LCSEG (i.e., the lexical cohesion statistics, the estimated p oste-rior probability, the predicted class). (2) Lex-ical Feature : Each spurt is represented as a vector space of uni-grams, wherein a vector is 1 or 0 dep ending on whether the cue word app ears in the spurt. (3) Proso dic Feature : These include dialogue-act (DA) rate-of-sp eech, max-imum F0 of the DA, mean energy of the DA, amount of silence in the DA, precedent and sub-sequent pauses, and duration of the DA. (4) Motion Feature : These include the average magnitude of sp eaker movements, which is mea-sured by the numb er of pixels changed, over the frames of 40 ms within the spurt. (5) Contex-tual Feature : These include the dialogue act typ es and the sp eaker role (e.g., pro ject man-ager, marketing exp ert). In the dialogue act an-notations, each dialogue act is classi ed as one of the 15 typ es. 2.2 Summarization The AMI summarizer is trained using a set of 98 scenario meetings. We train a supp ort vec-tor machine (SVM) on these meetings, using 26 features relating to the following categories: (1) Proso dic Features : These include dialogue-act (DA) rate-of-sp eech, maximum F0 of the DA, mean energy of the DA, amount of silence in the DA, precedent and subsequent pauses, and duration of the DA. (2) Sp eaker Fea-tures : These features relate to how dominant the sp eaker is in the meeting as a whole, and they include p ercentage of the total dialogue acts which each sp eaker utters, p ercentage of total words which sp eaker utters, and amount of time in meeting that each p erson is sp eak-ing. (3) Structural Features : These features include the DA p osition in the meeting, and the DA p osition in the sp eaker's turn. (4) Term Weighting Features : We use two typ es of term weighting: tf.idf , which is based on words that are frequent in the meeting but rare across a set of other meetings or do cuments, and a sec-ond weighting feature which relates to how word usage varies b etween the four meeting partici-pants.

After training the SVM, we test on each meet-ing of the 20 meeting test set in turn, ranking the dialogue acts from most probable to least probable in terms of b eing extract-worthy. Such a ranking allows the user to create a summary of whatever length she desires. 2.3 Compression Each dialogue act has its constituent words scored using tf.idf , and as the user compresses the meeting to a greater degree the browser gradually removes the less imp ortant words from each dialogue act, leaving only the most infor-mative material of the meeting. Previous work has explored the e ect of lexi-cal cohesion and conversational features on char-acterizing topic b oundaries, following Galley et al.(2003). In previous work, we have also studied the problem of predicting topic b oundaries at di erent levels of granularity and showed that a sup ervised classi cation approach p erforms b et-ter on predicting a coarser level of topic segmen-tation (Hsueh et al., 2006).

The amount of work b eing done on sp eech summarization has accelerated in recent years. Maskey and Hirschb erg(Septemb er 2005) have explored sp eech summarization in the domain of Broadcast News data, nding that combin-ing proso dic, lexical and structural features yield the b est results. On the ICSI meeting corpus, Murray et al.(Septemb er 2005) compared apply-ing text summarization approaches to feature-based approaches including proso dic features, while Galley(2006) used skip-chain Conditional Random Fields to mo del pragmatic dep enden-cies b etween meeting utterances, and ranked meeting dialogue acts using a combination or proso dic, lexical, discourse and structural fea-tures. This work was supp orted by the Europ ean Union 6th FWP IST Integrated Pro ject AMI (Augmented Multi-party Interaction, FP6-506811)
