 Privacy is becoming an increasingly importa nt issue in many data mining applications. A considerable amount of work on privacy preserving data mining has been investigated recently [2,1,9,7,12]. Among them, randomi zation has been a primary tool to hide sen-sitive private data for privacy preserving data mining. Random perturbation techniques aim to distort sensitive individual values while preserving some particular properties and hence allowing estimation of the underlying distribution.

Consider a data set X with n records of d attributes. The randomization based ap-proaches generate a perturbed data set Y following some predefined procedure, e.g., additive noise based approach applies Y = X + E where E is an additive noise data set, projection based approach applies Y = RX to map the sensitive data into a new space where R is a transformation matrix. Usually the perturbed data Y is expected to be dissimilar to the original X while some aggregate properties (e.g., mean and co-variance matrices for numerical data) of X are preserved or can be reconstructed after perturbation. The additive noise based approach has been challenged in privacy preserv-ing data mining community and several individual value reconstruction methods have been investigated [9,7,5,6].

In this paper, our focus will be the projec tion based approach. A special projection based approach called rotation projection has recently been investig ated in [4,11]. Since the transformation matrix R is required to be orthonormal (i.e., RR T = R T R = I ), geometric properties (vector length, inner products and distance between a pair of vec-tors) are strictly preserved. Hence, data mining results on the rotated data can achieve perfect accuracy. One apriori knowledge PCA based attack was recently investigated to show the vulnerabilities of this distance preserving projected based perturbation ap-proach when a sample dataset is available to attackers [10]. As a result, non-distance preserving projection was suggested be applie d since it is resilient to the apriori knowl-edge PCA based attack with the sacrifice of data mining accuracy to some extent.
We investigate whether attackers can recover the original data from arbitrarily projected data ( R can be any transformation matrix, hence the distance might not be pre-served in the transformed space). Specifically, we propose an Apriori-Knowledge ICA based reconstruction method (AK-ICA), which may be exploited by attackers when a small subset of sample data is available to att ackers. Our theoretical analysis and empir-ical evaluation shall show AK-ICA can effectively recover the original data with high precision when a part of sample data is a-priori known by attackers. Since the proposed technique is robust with any transformation matrix even with a small subset of sample data available, it poses a serious concern for projection based privacy preserving data mining methods.

The rest of this paper is organized as follo ws. In Section 2 we review the projection based perturbation approach and show current attempts to explore the vulnerability of this model. In Section 3 we briefly revisit ICA technique, which will be used when we introduce our AK-ICA attack in Section 4. We also show why AK-ICA can breach privacy from arbitrary transformation w ith the help of a small part of sample data. Section 5 presents our experimental results . We offer our concluding remarks and point out future work in Section 6. The projection based perturbation model can be described by Where X  X  X  p  X  n is the original data set consisting of n data records and p attributes. Y  X  X  q  X  n is the transformed data set consisting of n data records and q attributes. R is a q  X  p transformation matrix. In this paper, we shall assume q = p = d for convenience.
In [4], the authors defined a rotation based perturbation method, i.e., Y = RX , where R is a d  X  d orthogonormal matrix satisfying R T R = RR T = I .Thekey features of rotation transformation are pres erving vector length, Euclidean distance and inner product between any pair of points. Intuitively, rotation preserves the geometric shapes such as hyperplane and hyper curved s urface in the multid imensional space. It was proved in [4] that three popular classifiers (kernel method, SVM, and hyperplane-based classifiers) are invariant to the rotation based perturbation.

Similarly, the authors in [11] proposed a r andom projection-based multiplicative perturbation scheme and applied it for privacy preserving distributed data mining. The random matrix R k  X  m is generated such that each entry r i,j of R is independent and which does not change the shape of distributions (i.e., the eigenvalues derived from the sample data are close to those derived from t he transformed data). Hence, the rotation angles between the eigenspace derived from known samples and those derived from the transformed data can be easily identified. In other words, the rotation matrix R is recovered.

We notice that all the above attacks are just for the case in which the transformation matrix is orthonormal. In our general s etting, the transformation matrix R can be any matrix (e.g. shrink, stretch, dimension r eduction) rather than the simple orthonormal rotation matrix. When we try to apply the PCA attack on non-isometric projection sce-nario, the eigenvalues derived from the sample data are not the same as those derived from the transformed data. Hence, we cannot derive the transformation matrix R from spectral analysis. As a result, the previous PCA based attack will not work any more (See our empirical evaluation in Section 5.2).

Intuitively, one might think that the Independent Component Analysis (ICA) could be applied to breach the privacy. It was argued in [4,11] that ICA is in general not ef-fective in breaking privacy in practice due to two basic difficulties in applying the ICA attack directly to the projection based perturbation. First, there are usually significant correlations among attributes of X . Second, more than one attribute may have Gaus-sian distributions. We would emphasize that these two difficulties are generally held in practice. Although we can not apply ICA directly to estimate X from the perturbed data Y = RX , we will show that there exists a possible attacking method AK-ICA in the following sections. ICA is a statistical technique which aims to represent a set of random variables as linear combinations of statistically independent component variables.
 Definition 1 (ICA model) [8] ICA of a random vector x =( x 1 ,  X  X  X  ,x m ) T consists of estimating of the following generative model for the data: where the latent variables (components) s i in the vector s =( s 1 ,  X  X  X  ,s n ) T are as-sumed independent. The matrix A is a constant m  X  n mixing matrix.
 The basic problem of ICA is to estimate both the mixing matrix A and the realizations of the independent components s i using only observations of the mixtures x j . Following three restrictions guarantee identifiability in the ICA model. 1. All the independent components s i , with the possible exception of one component, 2. The number of observed linear mixtures m must be at least as large as the number 3. The matrix A must be of full column rank.
The second restriction, m  X  n , is not completely necessary. Even in the case where m&lt;n , the mixing matrix A is identifiable whereas the realizations of the independent components are not identifiable , because of the noninvertibility of A . In this paper, we make the conventional assumption that the dimension of the observed data equals the the number of independent components, i.e., n = m = d . Please note that if m&gt;n , the dimension of the observed vector can always be reduced so that m = n by existing methods such as PCA.

The couple ( A, S ) is called a representation of X .Since X = AS =( A X P )( P  X  1  X   X  1 S ) for any diagonal matrix  X  (with nonzero diagonals) and permutation matrix P , X can never have completely unique representation.

The reason is that, both S and A being unknown, any scalar multiplier in one of the sources s i could always be canceled by dividing the corresponding column a i of A by the same scalar. As a consequence, we usually fixes the magnitudes of the independent components by assuming each s i has unit variance. Then the matrix A will be adapted in the ICA solution methods to take into account this restriction. However, this still leaves the ambiguity of the sign: we c ould multiply an independent component by  X  1 without affecting the model. This ambiguity is insignificant in most applications. In this section we present our AK-ICA attack which may be exploited by attackers when a subset of sample data is available. Let  X  X  X  X denote this sample data set consisting of k data records and d attributes. Our result shall show attackers can reconstruct X closely by applying our AK-ICA attack method when a (even small) sample of data,  X  X , is available to attackers.

The core idea of AK-ICA is to apply the traditional ICA on the known sample data set,  X  X , and perturbed data set, Y , to get their mixing matrices and independent com-ponents respectively, and reconstruct the or iginal data by exploiting the relationships between them. Figure 1 shows our AK-ICA based attack.
 from the a-priori known subset  X  X and the perturbed data Y respectively. Since in gen-eral we can not find the unique representation of ( A, S ) for a given X (recall that X = AS =( A X P )( P  X  1  X   X  1 S ) for any diagonal matrix  X  and perturbation matrix P in Section 2), S is usually required to have unit variance to avoid scale issue in ICA. As a consequence, only the order and sign of the signals S might be different. In the following, we shall prove there exists a transformation matrix J such that  X  X = A  X  x JS y is an estimate of the original data X in Section 4.1, and present how to identity J in Section 4.2. 4.1 Existence of Transformation Matrix J To derive the permutation matrix J , let us first assume X is given. Applying the inde-pendent component analysis, we get X = A x S x where A x is the mixing matrix and S x is independent signal.
 Proposition 1. The mixing matrices A x , A  X  x are expected to be close to each other and the underlying signals S  X  x can be approximately regarded as a subset of S x . Proof. Considering an element x ij in X , it is determined by the i-th row of A x , a i ,and
Let  X  x p be a column vector in  X  X which is randomly sampled from X . Assume  X  x p = x j , then the i-th element of this vector,  X  x ip can also be expressed by a i and the corresponding signal vector s j .
Thus, for a given column vector in  X  X , we can always find a corresponding signal vector in S and reconstruct it through the mixing matrix A x .Since S x is a set of in-dependent components, its sample subset  X  S x  X  S x can also be regarded as a set of independent components of  X  X when the sample size of  X  X is large.

There exists a diagonal matrix  X  1 and a permutation matrix P 1 such that Proposition 2. S x and S y are similar to each other and there exists a diagonal matrix  X  2 and a permutation matrix P 2 that Proof.
 Since permutation may affect th e order and phase of the signals S y ,wehave By comparing the above two equations, we have Theorem 1. Existence of J . There exists one transformation matrix J such that where A  X  x is the mixing matrix of  X  X and S y is the independent components of the perturbed data Y .
 Proof. Since and  X  S x is a subset of S x , we can find a transformation matrix J to match the independent components between S y and S  X  x . Hence,
From Equation 2 and 3 we have 4.2 Determining J The ICA model given in Definition 1 implies no ordering of the independent compo-nents. The reason is that, both s and A being unknown, we can freely change the order of the terms in the sum in Definition 1, and call any of the independent component as the first one. Formally, a permutation matrix P and its inverse can be substituted in the model to give another solution in another order. As a consequence, in our case, the i-th component in S y may correspond to the j-th component in S  X  x . Hence we need to figure out how to find the transformation matrix, J .

Since S  X  x is a subset of S x , each pair of corresponding components follow similar distributions. Hence our strategy is to analyze distributions of two signal data sets, S  X  x and S y . As we discussed before, the signals derived by ICA are normalized signals. So the scaler for each attribute is either 1 or -1. It also can be easily indicated by the distributions.

Let S ( i )  X  x and S ( j ) y denote the i-th component of S  X  x and the j-th component of S y and let f i and f j denote their density distribution respectively. In this paper, we use the information difference measure I to measure the similarity of two distributions [1].
The above metric equals half the expected value of L 1 -norm between the distribution of the i-th component from S  X  x and that of the j-th component from S y . It is also equal to 1  X   X  ,where  X  is the area shared by both distributions. The smaller the I ( f, f ) , the more similar between one pairs of components. The matrix J is determined so that J [ f 1 ,f 2 ,  X  X  X  ,f d ] T  X  [ f 1 ,f 2 ,  X  X  X  ,f d ] T . The data set we used in our experiments is a Bank data set which was previously used in [12]. This data set contains 5 attributes (Home Equity, Stock/Bonds, Liabilities, Sav-ings, and CDs) and 50,000 records. In our AK-ICA method, we applied JADE pack-age 1 implemented by Jean-Francois Cardoso to conduct ICA analysis. JADE is one cumulant-based batch algorithm for source separation [3].

Since our AK-ICA attack can reconstruct individual data in addition to its distri-bution, in this paper we cast our accuracy an alysis in terms of both matrix norm and individual-wise errors. We measure the reconstruction errors using the following measures: where X,  X  X denotes the original data and the e stimated data respectively, and  X  F denotes a Frobenius norm 2 .

All the above measures show how close ly one can estimate the original data X from its perturbed data Y . Here we follow the tradition of using the difference as the measure to quantify how much privacy is preserved. Basically, RE (relative error) represents the average of relative errors of individual data points. RE -R i represents the average of relative errors of the i-th attribute. F -RE denotes the relative errors between X and its estimation  X  X in terms of Frobenius norm, which gives perturbation evaluation a simplicity that makes it easier to interpret. 5.1 Changing the Sample Size In this experiment, we first evaluate how th e sample size affects the accuracy of recon-struction of AK-ICA method. We change the ratio between known sample size and the original data size from 0.1% to 10%. Please note that all sizes of known samples in this experiment are small compared with the size of the original data. We set the trans-formation matrix R as non-orthonormal by generating all its elements from a uniform distribution.

Figure 2 shows the reconstruction error (in terms of F -RE and RE in Figure 2(a) and RE -R i for each attribute in Figure 2(b)) d ecreases when the sample size is in-creased. This is because that the more sam ple data we have, the more match between derived independent components. When we have known records which account for 1% of the original data, we could achieve very low reconstruction error ( F -RE = 0.108, RE = 0.115). When the sample size is decreased, more errors are introduced. However, even with known samples which only account for 0.1% of the original data, we can still achieve very close estimations for some attributes (e.g., RE -R i = 0.125 for attribute 1).
Next we evaluate how different sample sets  X  X with the same size affect AK-ICA reconstruction method, especially when sample size is very small. Here we randomly chose 10 different sample sets with the fixed size k =50 (sample ratio 0.1%). Figure 3 shows the construction errors with 10 different sample sets. The performance of our AK-ICA reconstruction method is not very stable in this small sample case. For exam-ple, the first run achieves 0.1 of F -RE while the third run achieves 0.44 as shown in Figure 3(a). The instability here is mainly caused by A  X  x which is derived from  X  X .Since Y = RX is fixed, the derived S y doesn X  X  change.

We also observed that for each particular attribute, its reconstruction accuracy in different rounds is not stable either. As shown in Figure 3(b), the attribute 5 has the largest error among all the attributes in round 5, however, it has the smallest error in round 7. This is because the reconstruction accuracy of one attribute is mainly deter-mined by the accuracy of its estimate of the corresponding column vector in A  X  x .This instability can also be observed in Figure 2(b). We plan to theoretically investigate how the sample X  X  properties(size, distribution etc .) affect reconstruction accuracy of the pro-posed AK-ICA attack. We also plan to investigate how the distribution of data affects reconstruction accuracy when a sample dat a set is fixed. As we point out in the future work, both problems are very challenging since there is no study on this problem in statistics. 5.2 Comparing AK-ICA and PCA Attack In this experiment, we evaluate the reconstruction performance of our approach and the PCA attack in [10]. We fix the sample ratio as 1% and apply different transformation matrices. Here R is expressed as R = R 1 + cR 2 ,where R 1 is a random orthonormal matrix, R 2 is a random matrix with uniformly distributed elements([-0.5,0.5]) and c is a coefficient. Initially, c is set as 0 which guarantees the orthonormal property for R . By increasing c , R gradually loses orthonormal property and tends to be an arbitrary transformation.

From Figure 4(a) and 4(b) we can observe that our AK-ICA attack is robust to various transformations. The reconstruction errors do not change much when the transforma-tion matrix R is changed to more non-orthonormal. On the contrary, the PCA attack only works when R is orthonormal or close to orthonormal. When the transformation tends to be more non-orthonormal (with the increase of c asshowninTable1),the reconstruction accuracy of PCA attack degr ades significantly. For example, when we set c =5 , the relative reconstruction errors of PCA attack are more than 200% ( F -RE =2.1414 , RE = 2.1843) while the relative reconstruction errors of AK-ICA attack are less than 20% ( F -RE =0.1444 , RE = 0.1793). In this paper, we have examined the effectiv eness of general projection in privacy pre-serving data mining. It was suggested in [10] that the non-isometric projection approach is effective to preserve privacy since it is resilient to the PCA attack which was designed for the distance preserving projection approach. We proposed an AK-ICA attack, which can be exploited by attackers to breach the privacy from the non-isometric transformed data. Our theoretical analysis and empirical evaluations have shown the proposed at-tack poses a threat to all projection based privacy preserving methods when a small sample data set is available to attackers. We argue this is really a concern that we need to address in practice.

We noticed that the sample X  X  properties (si ze, distribution etc.) would affect the re-construction accuracy from our empirical eval uations. It is a very ch allenging topic to explore the theoretical relationship between those properties and the reconstruction ac-curacy. To our knowledge, there is no study on this topic in statistics. We plan to tackle this issue with researchers in statistics in our future work. We would also investigate how transformation matrix affects the data utility.
 This work was supported in part by U.S. National Science Foundation IIS-0546027.
