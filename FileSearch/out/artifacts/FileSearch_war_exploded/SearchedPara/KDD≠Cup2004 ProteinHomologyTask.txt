 In this pap er we describ e the winning mo del for the perfor-mance measure \lowest rank ed homologous sequence" (RKL). This was a subtask of the Protein Homology Prediction task of the KDD Cup 2004. The goal was to predict protein ho-mology for di eren t performance metrics. The given data was organized in blo cks, eac h of whic h corresp onds to a spe-ci c nativ e sequence. The two metrics average precision (APR) and RKL explicitly mak e use of this blo ck structure. Our solution consists of two parts. The rst one is a global classi cation SVM not aware of the blo ck structure. The second part is a k-Ne arestNeighb or scheme for blo ck simi-larit y, used to train ranking SVMs on the y. Furthermore, we sketch our approac h to optimize the root-mean-squared-error and rep ort some alternativ e solutions that turned out to be sub optimal. Tw o datasets were pro vided, one for training and another for testing. The training set con tained 145,751 and the test set consisted of 139,658 examples. Eac h example could be iden ti ed by an unique example id and was mainly made up of 74 numerical feature values. These values describ ed the matc h between a nativ e protein sequence and a sequence that is tested for homology . Furthermore, every example could be asso ciated with this nativ e protein sequence using the so-called blo ck id. Eac h blo ck embodied about 1,000 examples. Both datasets did not con tain any incomplete information. The main goal was to predict whic h proteins were homologous to a nativ e sequence. For this purp ose the test set included a lab el whic h declared eac h example as homologous or inhomologous.
 Describ ed here is the winning solution to minimize the aver-age rank of the lowest rank ed homologous sequence (RKL). For the average prediction metric (APR) we submitted the same predictions as for RKL, and were rank ed fourth. Both metrics have in common that they score rankings. This means they do not dep end on predicted values, but only on the relativ e order of the matc hes within eac h blo ck. Eac h blo ck is evaluated separately . The overall scores for RKL and APR are computed by averaging.
 For eac h example of the test set a real-v alued prediction should be made, whic h can be though t of as a con dence score or probabilit y of examples to be homologous. For eac h blo ck these con dence-rated predictions induce an ordering. The RKL-v alue per blo ck is the lowest rank (highest num-ber) of an homologous example in the ordered list of exam-ples, so this measure basically punishes missed homologous examples.
 The average precision metric is based on the precision of dif-feren t subsets. Starting with the empt y set, in eac h iteration the next remaining example most con den tly predicted to be homologous is added to the subset, until all examples are covered. The precision is computed eac h time and averaged to receiv e the average precision per blo ck.
 We submitted another mo del for the root-mean-squared-error (RMS), also brie y describ ed in this pap er. This met-ric measures the squared di erence between a predicted value in the interv al [0 ; 1] and the true lab el from the set f 0 ; 1 g . We mainly used the SVM as the basic learning algorithm, a classi cation SVM and a ranking SVM. They are describ ed in the next two subsections. The last subsection introduces to the BlockNe arestNeighb or metho d, a k-Ne arestNeighb or approac h considering the dissimilar beha viour of the blo cks. In [2] predicting protein homology is men tioned as an ap-plication for a classi cation SVM with RBF kernel 1 . This led us to train a global SVM classi er using the RBF kernel on the normalised data. Since we faced a ranking prob-lem, we chose function values of the SVM as the predicted ranking instead of the class assignmen t. During training the SVM penalizes misclassi ed instances prop ortionally to their distance to the separating hyperplane. This justi es the assumption that examples far away from the separating hyperplane are less frequen tly misclassi ed. The function values are real num bers, directly implying an order on the examples. The performance measures APR and RKL dep end on the ordering of the samples. The ranking SVM [4] addresses di-1 The RBF kernel is de ned as K ( x ; y ) = e k x y k 2 = 2 rectly a ranking problem by learning -in the linear case -a weigh t vector whic h ranks the samples best according to Kendall's , a common measure from statistics for the simi-larit y of two rankings. The ranking SVM performed well on the given ranking problem but performance impro ved using it as base learner in the BlockNe arestNeighb or describ ed in the next section. BlockNe arestNeighb or is a straigh t-forw ard way to couple a nearest neigh bor approac h with an arbitrary base learner. The underlying assumption is, that blo cks sharing com-mon statistical prop erties are more likely to beha ve similarly when it comes to prediction. As common in nearest neigh-bor approac hes the similarit y between blo cks is de ned by a distance function. The goal is to estimate the similarit y between blo cks, so unlik e k-Ne arestNeighb ors the distance function is not estimated over single examples, but over in-formation describing blo cks. In our exp erimen ts we used the Euclidian distance over simple blo ck-wise aggregates like the median of speci c attributes.
 Giv en a previously unseen blo ck only the k most similar blo cks according to the distance function are used as a train-ing set. The resulting mo del is used to predict the lab el of the new blo ck. This section describ es our approac h for the APR and RKL measure and for the RMS measure resp ectiv ely. First the prepro cessing of data is explained. Then the nal mo dels are speci ed follo wed by a section addressing the prev ention of over tting. Finally we describ e some alternativ e approac hes we evaluated. We disco vered that normalization 2 of the attributes impro ved performance according to cross-v alidation exp erimen ts. Fur-ther enhancemen ts were achiev ed by normalizing the at-tributes within eac h blo ck instead of global normalization. Since we implemen ted a k-Ne arestNeighb or approac h we need-ed a distance measure between blo cks. We used statistical measures, actually mean, median, minim um and maxim um, for eac h attribute in a blo ck. With 74 original attributes this led to a 296 dimensional vector. Having 153 blo cks in the training set, we wanted to reduce the dimension of the vector. We trained J48 decision tree classi ers [6] on eac h blo ck and coun ted the occurences of a speci c attribute in these trees. Then we selected those attributes occuring at least in two di eren t trees. With this metho d we reduced the num ber of attributes to 46, hence the dimension of the vector to 184. Our distance measure was the Euclidian dis-tance between these 184 dimensional vectors represen ting a blo ck. Our nal mo del for the performance measures APR and RKL is schematically sho wn in gure 1. Consider a previ-ously unseen blo ck b . On the normalized training data we learned a global mo del, a classi cation SVM with RBF ker-nel. This mo del can directly be applied to blo ck b without further training. x On the other hand the BlockNe arestNeighb or approac h se-lects the set of blo cks most similar to blo ck b . A ranking SVM is trained on this set. Afterw ards the resulting mo del is also applicable to blo ck b .
 Both mo dels assign a real num ber to eac h example, thus de ne an ordering over b . In order to com bine them prop-erly we normalized these two rankings. As a metho d for com bining, simply adding them did the tric k, according to cross-v alidation exp erimen ts.
 Cross-v alidation exp erimen ts indicated, that it pays o to train a separate mo del for the RMS metric. With less than 1% positiv es the data was highly skewed. The ne-tuned SVM classi er for RKL and APR serv ed as a good start-ing point, just su ering from a low recall of about 60%. In con trast, boosted decision trees sho wed a little bit higher accuracy , a much higher recall of about 75%, but a signi -can tly lower RMS value.
 An analysis of those cases in whic h boosting and SVM mo d-els disagreed encouraged to com bine them by a simple dis-junction. In the resulting ensem ble an example is predicted homologous, if at least one of these two mo dels considers it to be homologous.
 The boosting algorithm used in our exp erimen ts was stan-dard AdaBo ost [7], the decision tree induction algorithm was J48. The ensem ble of the SVM classi er with boosted trees was rank ed 14th.
 An exp erimen t that nished shortly after submission dead-line indicated that AdaBo ost was not the optimal choice. The BayesianBo osting algorithm works by directly estimat-ing probabilities. One of its prop erties is to keep the priors of class lab els, when changing example weigh ts during train-ing. It is not yet published, but available as part of the free learning environmen t YALE 3 . Compared to the corresp ond-ing AdaBo ost mo dels, the mo dels of this operator achiev ed a signi can tly higher recall (close to 80%) while not losing accuracy .
 The upload facilit y of the KDD Cup website re-op ened after the con test. We were able to evaluate the alternativ e mo del, a com bination of an SVM classi er with BayesianBo osting on top of J48 decision tree classi ers. This mo del would have been rank ed fourth. 3 http://y ale.cs.uni-dortm und.de The classi cation SVM with RBF kernel has two parame-ters whic h can be optimized. One parameter, called j in SVM lig ht [3], is the factor by whic h false-negativ es are pe-nalized higher than false-p ositiv es during training. We ex-pected this to be greater than 1, because positiv ely lab eled instances were outn um bered by the negativ e instances. The other optimizable parameter is , the bandwith of the RBF kernel. We conducted tenfold cross-v alidation in order to optimize these parameters. From these exp erimen ts we got an estimation for the RKL measure of 44 : 34, using the clas-si cation SVM stand alone, and 37 : 13 when com bining it with the BlockNe arestNeighb or metho d. The performance on the test set was 45 : 62.
 To estimate the predictiv e performance of BlockNe arestNeigh-bor we trained a mo del for eac h blo ck of the training set. Because we left out the blo ck itself the result was a leave-one-out estimate, where \one" relates to blo cks rather than examples. This estimate was used to optimize the parame-ters, namely k , the num ber of neigh bors to be selected for training, and j , a parameter of the ranking SVM.
 The estimate for the com bination of the two mo dels, how-ever, was too optimistic. The estimated RKL value due to a tenfold cross-v alidation was 19% lower than the nal evalu-ation on the validation set, for APR the value was 1 : 8% too low. Tw o promising approac hes to tackle the problem that turned out to be inferior to the solution nally selected are shortly describ ed in this subsection. We basically focussed on ways to overcome the striking di erences between blo cks. First of all we tried to perform clustering as a step of pre-pro cessing. The goal is to nd a few disjoin t sets of similar blo cks, eac h of whic h could then be assigned a mo del of its own. Candidate clustering algorithms were k-Ne arestNeighb or and Diez clustering[8]. The Diez clustering algorithm is an agglomerativ e clustering algorithm using the performance of SVMs as its joining criterion. It starts by selecting the most trivial clusters. In our case eac h blo ck forms a clus-ter. On eac h cluster a separate SVM is trained. The pair of most similar SVMs according to a xed distance func-tion 4 is the rst candidate for being joined. If the estimated performance of the SVM trained on the temp orarily formed cluster is better than the performance of the single mo dels before, the candidates are joined and become a new clus-ter. Otherwise the next two similar candidates are tested. The algorithm stops when no more canditates remain. Al-though this approac h exploits the similarit y of blo cks, just like the winning solution, in our exp erimen ts it performed worse than just training a ranking SVM on the original data set.
 The second approac h adds blo cks to the training set sequen-tially . Starting with an initial set of a few blo cks a rst rank-ing SVM mo del is trained. Then the mo del is applied to the remaining blo cks, to nd the subset of blo cks for whic h the mo del is least appropriate. These blo cks are added to the training set of the next iteration. Another ranking SVM is trained on this larger example set, and so on. The num ber of iterations and the num ber of blo cks to add are param-eters that can be optimized empirically . The adv antage of 4 In [8] it is the cosine between the weigh t vectors. this approac h over the winning solution is that the output is a single global mo del, rather than a lazy learner, postp on-ing calculations to the time when a request for predictions arises. Iterativ ely augmen ting the training set yielded the most accurate glob al mo del in our exp erimen ts. It is not part of the submitted solution, because it was clearly out-performed by the BlockNe arestNeighb or approac h. In this pap er we outlined our approac hes to the protein ho-mology task of KDD Cup 2004. The common goal of all metrics asso ciated to that tasks was to predict homology of a protein sequence in terms of a real-v alued con dence. The focus of this pap er is on our mo del for the two ranking metrics, namely lowest rank ed homologous sequence (RKL) and average prediction (APR). For the RKL metric our sub-mission was rank ed rst, for APR the same predictions were rank ed fourth.
 An early analysis of the data rev ealed, that statistical prop-erties varied drastically from blo ck to blo ck. With less than 1% of the data being homologous sequences the data was highly skewed. Although under these conditions one would prefer blo ck-wise mo deling, it was possible to t a global classi er surprisingly well. A classi cation SVM with RBF kernel gave best results in our cross-v alidation exp eri-men ts. Still blo ck-related mo deling gave signi can tly better results. The ranking SVM is directly tailored towards opti-mization of ranking measures. Starting with a small subset of blo cks and adding new blo cks for whic h predictions are yet poor, iterativ ely, was a successful way to train accurate blo ck-related mo dels. Our nal mo del is still di eren t, be-cause it directly accoun ts for the statistical di erences of blo cks by applying a k-Ne arestNeighb or algorithm at the blo ck level. To predict new blo cks it is necessary to iden tify the k most similar blo cks. Only for these blo cks a rank-ing SVM is trained, whic h is then applied to the new blo ck. This metho d work ed surprisingly well, keeping in mind the simple similarit y measure we used. The disadv antage of this metho d compared to the iterativ ely trained ranking SVM is that for eac h blo ck to be classi ed a new mo del needs to be trained on the y.
 For all metrics we made the exp erience that without com bin-ing mo dels the results are not compatible. But com bining mo dels seems easy since even simple approac hes in this di-rection gave a boost in accuracy . The nal mo del for RKL and APR is a result of simply adding the con dence values of the global classi cation SVM and the more local BlockN-earestNeighb or ranking SVM. Similarly , a simple disjunction of the global SVM mo del and boosted decision trees was still rank ed 14th, after more careful selection of the boosting al-gorithm even rank ed 4th. Considering more sophisticated metho ds of mo del com bination would probably further in-crease performace.
 Finally we want to point out that in our exp erimen ts the performance of di eren t learners and parameter settings was quite metric dep enden t. As a result performance could al-most alw ays be increased by optimizing parameters for eac h metric separately . The main reason for us to still submit a single mo del for RKL and APR was lack of time.
 We participated at KDD Cup 2004 in the framew ork of a "pro ject group", a course over two semesters for 12 gradu-ate studen ts. The course was organized by Prof. Katharina Morik and Martin Scholz, AI Unit, Dep. of Computer Sci-ence, Univ ersit y of Dortm und, German y.
 The full list of participating studen ts and co-authors: Dirk Dac h, Holger Flic k, Christophe Foussette, Marcel Gaspar, Daniel Hak enjos, Felix Jungermann, Christian Kullmann, Anna Litvina, Lars Mic hele, Sieh yun Strob el, Marc Twiehaus, and Nazif Veliu.
 [1] Ric hard A. Bec ker, John M. Cham bers, and Allan R. [2] Cristianini, N. and Sha we-T aylor, J., An Intr oduction to [3] T. Joac hims, Making large-Sc ale SVM Learning Prac-[4] T. Joac hims, Optimizing Search Engines Using Click-[5] Miersw a, Ingo and Klinkb erg, Ralf and Fisc her, Simon [6] Ian H. Witten and Eib e Frank, Data Mining: Practic al [7] Y. Freund and R. Schapire. A decision{the oretic gener al-[8] J. Diez, J.J. del Coz, O. Luaces, and A. Bahamonde. A
