 We propose a new leaning method for Multiple Kernel Learn-ing (MKL) based on the upper bounds of the leave-one-out error that is an almost unbiased estimate of the expected generalization error. Specifically, we first present two new formulations for MKL by minimizing the upper bounds of the leave-one-out error. Then, we compute the derivatives of these bounds and design an efficient iterative algorithm for solving these formulations. Experimental results show that the proposed method gives bette r accuracy results than that of both SVM with the uniform combination of basis kernels and other state-of-art kernel learning approaches. I.2.6 [ Artificial Intelligence ]: Learning X  Parameter Learn-ing ; H.2.8 [ Database Management ]: Database Applica-tions X  Data Mining ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier Design and Evaluation Algorithms, Theory, Experimentation Multiple Kernel Learning (MKL), Support Vector Machine, Leave-One-Out Error, Ge neralization Error Bound
Kernel methods, such as support vector machines (SVM), have been widely used in pattern recognition and machine learning. A good kernel function, which implicitly charac-terizes a suitable transformation of input data, can greatly benefit the accuracy of the predictor. However, when there are many available kernels, it is difficult for the user to pick out a suitable one.

Instead of using a single kernel, recent applications have shown that using multiple kernels can enhance the inter-pretability of the decision function and improve performance. In such cases, a convenient approach is to consider the ker-nel as a convex combination of the basis kernels. Within this framework, the problem of learning the kernel is trans-formed to the problem of deciding the combination coef-ficients. Lanckriet et al. [7] formulate this problem as a semidefinite programming problem which allows to learn the combination coefficients and SVM classifier together, which is usually called multiple kernel learning (MKL). To enhance the computational efficiency, different approaches for solving this MKL problem have been proposed, semi-infinite linear program [9], second-order cone p rogram [6], gradient-based methods [8], and second-order optimization [2].

For SVM, it is well known that the estimation error, which denotes the gap between the expected error and the empir-ical error, is bounded by O ( R 2  X   X  2 ) / ,where R is the ra-dius of the minimum enclosing ball of training data in the feature space endowed with the kernel used,  X  is the mar-gin of SVM classifier, and is the number of training data. However, most existing MKL approaches decide the value of combination coefficients only by maximizing the margin  X  . In this way, although  X  2 is the maximum, the R 2 may be very large too, so the esti mation error bound may be too loose to guarantee good generalization performance of SVM.
To address the above problem, we directly minimize the upper bounds of expected error to decide the combination coefficients. Specifically, we first present two minimization formulations for MKL based on upper bounds of leave-one-out error. Using the derivatives of these bounds with respect to combination coefficients, we then propose a gradient de-scent algorithm to address these minimization formulations. Experiments show that our method gives significant perfor-mance improvements both over SVM with the uniform com-bination of basis kernels and over other state-of-art kernel learning methods.
We consider the classification learning problem from train-ing data D = { ( x 1 ,y 1 ) ,..., ( x ,y ) } where x i belongs to some input space X and y i  X  X  +1 ,  X  1 } denoting the class la-bel of examples x i . In the Support Vector Machines (SVM) methodology, we map these input points to a feature space using a kernel function K ( x i , x j ) that defines an inner prod-uct in this feature space, that is, K ( x i , x j )= X ( x i where  X ( x ) is a mapping of x to a reproducing kernel Hilbert space H induced by K .

Here, we consider the kernel K  X  depending on a set of parameters  X  . The classifier given by the SVM is f ( x )= sign( i =1  X  0 i y i K  X  ( x i , x )+ b ), where the coefficients the solution of the following optimization problem:
This formulation of the SVM optimization problem is called the hard margin formulation since no training errors are allowed. For the non-separable case, one needs to allow training errors which results in the so called sof t margin SVM algorithm. It can be shown that sof t margin SVM with quadratic penalization of errors can be considered as a special case of the hard margin version with the modified kernel (see [5]). So in the rest paper, we only focus on the hard margin SVM.

The used kernel we consider is a linear convex combination of multiple basis kernels, as where K i ,i =1 ...,m are the basic kernels and m is the total number of basic kernels. Within this framework, the problem of learning the kernel is transformed to the problem of deciding the combination coefficients  X  =(  X  1 ,..., X  m
Ideally we would like to choose the value of the combina-tion coefficients  X  that minimize the true risk of the SVM classifier. Unfortunately, since this quantity is not accessi-ble, one has to build estimates or bounds for it.
The leave-one-out procedure consists of removing one ele-ment from the training data, constructing the decision rule on the basis of the remaining training data and then testing on the removed element.

Let us denote the number of errors in the leave-one-out procedure by L (( x 1 ,y 1 ) ,..., ( x ,y )) and by f i the classi-fier obtained with an SVM when the training data ( x i ,y removed, so we can write that L (( x 1 ,y 1 ) ,..., ( x when t&gt; 0and  X  ( t ) = 0 otherwise. It is known that the leave-one-out procedure gives an almost unbiased estimate of the expected generalization error.

Although the leave-one-out estimator is a good choice when estimating the generalization error, it is very costly to actually compute since it requires running the training algorithm times. The strategy is thus to upper bound or approximate this estimator by an easy to compute quantity T having.

For hard margin SVM without threshold (b=0), Vap-nik [10] proposes the following upper bound of leave-one-out procedure: where R K  X  is the radius of the smallest sphere enclosing the training points in the feature space and  X  K  X  is the margin of the SVM classifier with K  X  .

Vapnik &amp; Chapelle [3] derive an estimate using the con-cept of span of support vectors. Under the assumption that the set of support vectors remains the same during the leave-one-out procedure, S p is the distance between the point  X  K  X  ( x p )andtheset  X 
To guarantee good generalization performance of SVM, we consider to minimize the upper bounds of leave-one-out error. Two minimization formulations for Multiple Kernel Learning are proposed: With the radius-margin bound. With the span bound:
In order to solve the optimization problems (2) and (3), the projected gradient algorithm is adopted, which requires the computation of the derivatives of T RM and T Span .
According to [4], we know that the radius of the minimum enclosing ball R 2 K  X  can be obtained by:
The derivative of T RM with respect to  X  k is given in the following theorem:
Theorem 1. Let the parameters  X  0 =(  X  0 1 ,..., X  0 ) T and  X  0 =(  X  0 1 ,..., X  0 ) T are the solutions of optimization problem (1) and (4) respectively. Denote vectors  X  =( K k ( x 1 , ...,K k ( x , x )) T and  X  =( K  X  ( x 1 , x 1 ) ,...,K  X  ( Then, the derivative of T RM with respect to  X  k can be written as where Z =  X  T  X  0  X   X  0 T K  X   X  0 , K k and K  X  are the kernel matrices corresponding to kernel K k and K  X  respectively.
Proof. Note that  X  X  RM Accordingto[4],  X  shown in [10] that 1 / X  2 K  X  = i =1  X  0 i .Thusthetheoremis proven.
The estimate of the performance of SVM through the span bound (3) requires the use of the step function  X  which is not differentiable. However, we would like to use the pro-jected gradient method to minimize this estimate of the test error. So we must smooth this step function. Similar to [4], we consider to use a contracting function :  X  ( (1 + exp(  X  c x + d ))  X  1 ,where c, d are non-negative constants. In our experiment, we took c =5and d =0whichisthe same as in [4].

Let sv denote the set of support vectors, sv = { x i |  X  0 0 ,i =1 ,..., } , K  X  sv are the kernel matrix corresponding to sv,  X  K  X  sv = where 1 is all-ones vector and 0 is all-zeros vector.
Theorem 2. The derivative of the span S 2 to the parameter  X  k canbewrittenas know that S 2 p =1 /  X  K  X  1  X  sv the theorem is proven.

There is however a problem in this approach: the value given by the span is not continuous. Inspired by the Chapelle et al. [4], we replace the constraint by a regularization term in the computation of the span to smooth the span value,  X 
S
With this new definition of the span, the  X  S 2 p can be written as  X  the number of support vectors.

Theorem 3. Assuming that G is a diagonal matrix with the elements [ G ] ii =  X   X / (  X  0 sv i ) 2 and [ G ] n sv  X  A is the inverse of  X  K  X  sv with the last row and last column removed. Then  X   X  S 2 p where B =  X  K  X  sv + Q , Y sv = diag (( y sv 1 ,...,y sv F
Proof. Recall that  X  S 2 Accordingto[2],weknowthat  X   X  Thus it is easy to verify that  X  Q  X  X  k = GF . In addition, since
For convenience, we use T  X  to denote T RM or T Span .With the derivative of T  X  with respect to the combination coef-ficients, we use the standard gradient projection approach with the Armijo rule [1] for selecting step sizes to address optimization problems (2) and (3). The overall algorithm is described in Algorithm 1 and Algorithm 2 which show that the above described steps are performed until a stopping cri-terion is met. This stopping criterion can be either based on a duality gap, or more simply, on a maximal number of iterations, or on the variation of  X  between two consecutive steps.
 Algorithm 1 : Adaptive Radius-Margin Multiple Kernel
Learning Algorithm (RMMKL) 10: end if 11: end for Algorithm 2 : Adaptive Span Multiple Kernel Learning
Algorithm (SPMKL) 10: end if 11: end for
In this section, we illustrate the performances of our pre-sented RMMKL and SPMKL approachs, in comparison with SVM with the uniform combination of basis kernels (Unif) where K  X  = m i =1 1 m K i , the kernel learning approach (KL) [4] which only learns a single kernel, and the margin-based MKL method (MKL) [8].

The evaluation is made on eleven public available data sets from UCI repository 1 and LIBSVM Data 2 (see Table 1). For a fair comparison, we have selected the same ter-mination criterion for the iterative algorithms (KL, MKL, RMMKL and SPMKL): iteration terminates when  X  t +1  X   X  t 2 /  X  t 2  X  0 . 01 or the maximal number of iteration (500) http://www.ics.uci.edu/  X  mlearn/MLRepository.html. http://www.csie.ntu.edu.tw/  X  cjlin/libsvm. two kernel learning approaches.
 has been reached. We use gaussian kernels K Gauss ( x , x exp  X  x  X  x 2 2 / 2  X  2 and polynomial kernels K Ploy ( x (1 + x  X  x ) d as our basis kernels: 10 gaussian kernels with bandwidths  X   X  X  0 . 5 , 1 , 2 , 5 , 7 , 10 , 12 , 15 , 17 , 20 nomial kernels of degree d from 1 to 10.

The experimental setting of the KL is the same as the one used in [4]. The code of MKL is from SimpleMKL [8]. The initial  X  is set to be 1 20 1 . The trade-off coefficients C in SVM, KL, MKL, RMMKL and SPMKL are automatically determined by 4-fold cross-validations on training sets. In For each data set, we have run all the algorithms 50 times with different training and test splits (50% of all the exam-ples for training and testing).

The average accuracies with standard deviations and av-erage numbers of selected basis kernels are reported in Table 1. The results in Table 1 can be summarized as follows: Our methods (Index 4,5) give the best results on most data sets. RMMKLoutperformsallothermethods(Index1,2,3)on9 out of 11 sets, and also give results closing to the best ones of other methods on the remaining 2 sets. In particular, RMMKL gains 5 or more percents of accuracies on Splice, Liver, Musk1 and Sonar over MKL, and gains more than 10 percents on Ionosphere, Splice, Musk1, Mpbc over KL. On the set Musk1, RMMKL even gains 20.4 percents over KL. For SPMKL (Index 5) the results are similar to RMMKL: SPMKL outperforms other methods (Index 1, 2, 3) on 10 out of 11 sets, with only 1 inverse result. So it implicates that learning based on the kernel expect error bound can guarantee good generalization performance of the SVM.
Based on the upper bounds of the leave-one-out error, we present an accurate and efficient MKL method. we first establish two optimization formulations, and then propose the efficient gradient-based algorithms, called RMMKL and SPMKL, for computing the formulations. Experimental re-sults validate that our approach outperforms both SVM with the uniform combination of basic kernels and other state-of-art kernel learning methods.

Future work aims at improving both speed and sparsity in kernels of the algorithm and extending this method to other SVM algorithm such as the one-class SVM or the SVR. We also plan to explore a new criterion based on the spectral of the integral operators to meas ure the kernel or kernel matrix for MKL.
 The work is supported in part by the FP7 Marie Curie In-ternational Research Staff Exchange Scheme (IRSES) under grant No. 247590, the National N atural Scienc e Founda tion of China under gra nt No. 61070044, an d the Natural Science Foundation of Tianjin unde r grant No. 11JCYBJC00700. [1] D. P. Bertsekas. Nonlinear programming .Athena [2] O. Chapelle and A. Rakotomamonjy. Second order [3] O. Chapelle and V. Vapnik. Model selection for [4] O. Chapelle, V. Vapnik, O. Bousquet, and [5] C. Cortes and V. Vapnik. Support-vector networks. [6] L. Jia, S. Liao, and L. Ding. Learning with uncertain [7] G.R.Lanckriet,N.Cristianini,P.Bartlett,L.E.
 [8] A. Rakotomamonjy, F. Bach, S. Canu, and [9] S. Sonnenburg, G. R  X  atsch, and C. Sch  X  afer. A general [10] V. Vapnik. Statistical learning theory .

