 Experiment replication and reproduction are key requirements for empirical research methodology, and an important open issue in the field of Recommender Systems. When an experiment is re-peated by a different researcher and exactly the same result is ob-tained, we can say the experiment has been replicated. When the results are not exactly the same but the conclusions are compati-ble with the prior ones, we have a reproduction of the experiment. Reproducibility and replication involve recommendation algorithm implementations, experimental protocols, and evaluation metrics. While the problem of reproducibility and replication has been rec-ognized in the Recommender Systems community, the need for a clear solution remains largely unmet, which motivates the present workshop.
 H.3.3 [ Information Search and Retrieval ]: information filtering, relevance feedback, retrieval models, search process, selection pro-cess.
 Algorithms, Design, Experimentation, Measurement, Performance Evaluation, replicability, reproducibility, experimental design, ex-perimental methodology
The empirical evaluation of Recommender Systems (RS) is ac-knowledged to be an open problem in the field, with open issues yet to be addressed [2]. Many experimental approaches and met-rics have been developed along the years, which the community is well acquainted with, but key aspects and details in the design and application of available methodologies are open to configuration and interpretation, where even apparently subtle details may create a considerable difference. This results in a significant divergence in experimental practice, hindering the comparison and proper as-sessment of contributions and advances to the field.

In this context, the replication and reproduction of experiments is one of the desirable requirements for experimental research still to be met in the field. We say an experiment is replicated when it is repeated by a different researcher and exactly the same result is obtained. When the results are not exactly the same but the conclu-sions are compatible with the prior ones, we have a reproduction of the experiment.

The topic of reproducibility is an obvious concern at this mo-ment in several fields, such as Information Retrieval and Human-Computer Interaction (RepliCHI panel in 2012 and workshop in 2013 [5]). Adjacent to this issue are the workshops dealing with software engineering in recommendation (RSSE series [1] and prospec-tive book for 2013 1 ) and open source software, again in Information Retrieval (Open Source Information Retrieval workshops at SIGIR 2006 and 2012 [4, 6]) and Machine Learning (Machine Learning Open Source Software workshops at NIPS 2006 and 2008 [3]).
The discussion and definition of the basic elements of the ex-perimental conditions (and their requirements) is critical to support continuous innovations in any discipline. The offline evaluation of recommender systems requires an implementation of the algorithm or technique to be evaluated, a set of quality measures for compar-ative evaluation, and an experimental protocol establishing how to handle the data and compute metrics in detail. Online evaluation similarly requires an algorithm implementation and a population of users to survey (by means of an A/B test, for instance). Here again, perhaps even more importantly than in offline evaluation, an experimental protocol needs to be established and adhered to. As a paradigmatic example, the Information Retrieval field, adjacent to RS, is a successful development on this ground, with the TREC conferences 2 and a common tool (treceval) to evaluate any of the tasks proposed in that venue.

Even when a set of publicly available resources (data and algo-rithm implementations) exists in the RS community, very often re-search studies do not report comparable results for the same meth-ods under the same conditions . This is due to the high number of experimental design parameters in recommender system evalu-ation, and the huge impact of the experiment configuration on the outcomes.

In order to seek reproducibility and replication several strategies can be considered, such as source code sharing, standardization of https://sites.google.com/site/rsseresearch/rsse-book http://trec.nist.gov agreed evaluation metrics and protocols, or releasing public ex-perimental design software, all of which have difficulties of their own. Furthermore, for online evaluation, an extensive analysis of the population of test users should be provided. While the prob-lem of reproducibility and replication has been recognized in the community, the need for a solution remains largely unmet. This, together with the need for further discussion, methodological stan-dardization in both reproducibility as well as replication motivates the present workshop.
The workshop gathered researchers and practitioners interested in defining clear guidelines for their experimental needs to allow fair comparisons to related work. The workshop provided an in-formal setting for exchanging and discussing ideas, sharing expe-riences and viewpoints. We aimed to identify and better under-stand the current gaps in the implementation of recommender sys-tem evaluation methodologies, help lay directions for progress in addressing them, and foster the consolidation and convergence of experimental methods and practice. The workshop sought to iden-tify the main challenges related to reproduction and replication of prior research, along with an exploration of possible directions to overcome these limitations.

Specific questions raised and addressed at the workshop includes the following:
The accepted papers and the discussions held at the workshop addressed  X  X mong others X  the following topics:
The workshop took place on October 12 th , 2013. It opened with a keynote talk by Mark Levy (Mendeley), followed by the presenta-tion of accepted papers and open discussions, where an interactive panel with prominent members of the academic and industrial com-munities discussed the challenges and problems of reproducibility and replication in recommender systems. The accepted papers and a summary of discussions are available in the workshop proceed-ings published in the ACM International Conference Proceedings Series (ICPS), which can be reached from the workshop website at http://repsys.project.cwi.nl.
This workshop was carried out during the tenure of an ERCIM  X  X lain Bensoussan X  Fellowship Programme, funded by European Comission FP7 grant agreement no.246016. [1] H OLMES , R., R OBILLARD , M. P., W ALKER , R. J., AND [2] S HANI , G., AND G UNAWARDANA , A. Evaluating [3] S ONNENBURG , S., B RAUN , M. L., O NG , C. S., B ENGIO [4] T ROTMAN , A., C LARKE , C. L. A., O UNIS , I., C ULPEPPER [5] W ILSON , M. L., C HI , E. H., C OYLE , D., AND R ESNICK [6] Y EE , W. G., B EIGBEDER , M., AND B UNTINE , W. L.
