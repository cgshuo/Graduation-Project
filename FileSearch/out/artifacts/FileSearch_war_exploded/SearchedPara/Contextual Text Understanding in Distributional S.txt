 Representing discrete words in a continuous vector space turns out to be useful for natural language applications related to text under-standing. Meanwhile, it poses extensive challenges, one of which is due to the polysemous nature of human language. A common solu-tion (a.k.a word sense induction) is to separate each word into mul-tiple senses and create a representation for each sense respective-ly. However, this approach is usually computationally expensive and prone to data sparsity, since each sense needs to be managed discriminatively. In this work, we propose a new framework for generating context-aware text representations without diving into the sense space. We model the concept space shared among sens-es, resulting in a framework that is efficient in both computation and storage. Specifically, the framework we propose is one that: i) projects both words and concepts into the same vector space; ii) ob-tains unambiguous word representations that not only preserve the uniqueness among words, but also reflect their context-appropriate meanings. We demonstrate the effectiveness of the framework in a number of tasks on text understanding, including word/phrase simi-larity measurements, paraphrase identification and question-answer relatedness classification.
 I.2.6 [ Artificial Intelligence ]: Learning; I.2.7 [ Artificial Intelli-gence ]: Natural Language Processing Algorithm, Application Information retrieval; Knowledge representation; Machine learn-ing; Search engine T his work was done at Microsoft Research Asia.

Native speakers of a language are able to make sense of sen-tences they have never seen or heard before. They understand words with more than one meaning in a given context, and how these combine to form meaningful utterances. While humans per-form language understanding effortlessly, the task poses enormous challenges for machines. Discovering the meaning representation-s for texts is thus crucial for many natural language applications, such as web document classification and search query understand-ing [44, 43]. A common first approach is to translate words into a machine-understandable form, such as with vectors, which has led to the creation of distributional models.

The underlying idea of representing words as vectors dates back to the distributional hypothesis that  X  X  word is characterized by the company it keeps X  [15]. Under this assumption, the meaning of a word can be obtained empirically by examining the context in which the word appears. Ultimately, words are represented as vec-tors and their meanings are distributed across the dimensions of a semantic space. The points in space, therefore, represent semantic concepts in such a way that similar concepts are close to each oth-er. The distributional models constitute a powerful representation framework in which the similarity of two meanings can be easily measured.

Traditional approaches to constructing a vector space rely on the explicit collection of co-occurrence statistics. Until recently, the idea has been reformed with the development of neural language models [2, 10, 29], in which the word vectors are treated as param-eters to be optimized via the training of neural networks on unstruc-tured text data. An advantage of the training based approach is that it produces salient word encodings in a dense and low-dimensional form (a.k.a word embeddings), proven to capture the semantic re-lationship among words [30].

Despite their effectiveness, generic distributional models, as sug-gested by their mathematical nature, suffer from low precision. One reason is due to the polysemous nature of human language. As a word reveals different senses as the context changes, it should be assigned more than one representation. For example, the word ap-ple may either refer to  X  X he name of an IT company  X  or  X  X  type of fruit  X . Merging the two senses into a single vector may lead to inaccurate representation of either sense. Further consider two queries [ python zoo ] and [ python string ]. Humans can easily identi-fy by context that the word python in the former query stands for an animal species while in the latter it means a type of programming language . Machines, however, are not generally equipped with this ability.

Previously solutions to address language polysemy in distribu-tional models mainly focus on the creation of sense-specific em-beddings. The idea was first proposed in the work of Sch  X  u tze [38], who obtain sense representations for a target word by collecting a nd clustering its context vectors. In the end, a sense is simply rep-resented by the centroid of the corresponding cluster. Reisinger and Mooney [37] formalize the cluster-based model as a multi-prototype vector space , which has later been used in a series of tasks on word sense disambiguation [20, 21] and neural language modeling [18, 34]. The major limitation of the above works lies in the excessive computational cost induced by the clustering step, which has to be performed repeatedly for every word in the vocab-ulary.

Meanwhile, another line of research directly extends neural lan-guage models with the non-parametric Bayesian approach to learn sense embeddings [35, 1, 24]. Despite improved efficiency, there is no clear mapping between the senses learned from the data and the actual senses of a word. This becomes problematic for rare words or rare senses, for which there does not exist enough training con-text.

Following the above analysis and extending it a bit further, tradi-tional approaches for modeling word senses in a vector space face the following challenges:
On the other hand, there exist many knowledge sources which contain rich linguistic knowledge, such as the relations between words. These knowledge sources can potentially help in the learn-ing of word representations. Against this background, an interest-ing question we ask in the paper is as follows: The solution we provide is based on the observation that different senses can share common concepts. Consider the two words apple and orange . While each has two distinct senses (as shown in Table 1), they share the same concept of fruit . To a certain extent, mod-eling the shared concepts among senses suffices to disambiguate a word meaning in a specific context. Motivated by this, we propose a novel framework for addressing language polysemy based on the concept space shared among senses. With the word-concept rela-tions induced from an appropriate knowledge source, we train joint word-concept embeddings from a neural language model. As a re-sult, each word is associated with an intrinsic vector that maintains the unique features of the word. Each concept is analogously as-signed a vector that delivers an unambiguous meaning. We obtain the contextual representation of a word by combining its intrinsic vector and the most context-appropriate concept vector.
The main advantage of the proposed framework are in scalability, coverage and efficiency. The concept space has a much smaller and manageable size than the sense space while it emphasizes the commonality of senses. The proposed framework is completely general. Any meaningful concept space can be integrated with our neural language model, not limited to the one obtained from an explicit knowledge base.

An alternative to our approach is to utilize the word-concept rela-tions outside embeddings. This involves a preprocessing step that replaces each word in the training text with a word-concept pair, after which a regular embedding training is applied. While this method violates our intention to avoid the memory cost and data sparsity issue in training sense embeddings, we refer to the method as preprocessed and use it as a baseline.

We perform extensive experiments to validate the effectiveness and efficiency of the proposed framework. In all experiments re-lated to text understanding, including contextual word/phrase simi-larity measurements, paraphrase identification and question-answer relatedness classification. we achieve impressive results comparing to the state-of-art. The contextual embedding features computed from our framework have been used as important features to shift a commercial question-answering system.

The rest of this paper is organized as follows. Section 2 intro-duces neural skip-gram models, to which we extend train word-concept embeddings. Section 3 provides insights on the relation-ship between words and concepts. Section 4 details the topology and training aspects of the extended neural language model. Sec-tion 5 describes how context-aware representations can be creat-ed. The experiments and findings are described in Section 6. We present related work in Section 7 and conclude the paper in Section 8.
Skip-gram [29] is an efficient neural language model for learn-ing distributional word representations from unstructured text data. The training goal of Skip-gram is to find word representations that best predicate the existence of surrounding words (i.e, contextual words). Mathematically, the objective is to maximize the following average log probability: where w 1 ...w T denotes the training text as a sequence of words, w t and w c represent the target word and the contextual word re-spectively, and d is the size of the context window centered at the target word. The standard way to compute the conditional proba-bility p ( w c | w t ) is to use a softmax function normalized over the entire vocabulary: where w t a nd w c are the respective vectors of the target word and the contextual word. W denotes the vocabulary. s is a scoring function for the pair ( w t , w c ) . In a log-bilinear assumption, s is simply the dot product of the two word vectors.

Factorized Models. In the training of the above neural Skip-gram model, the time complexity induced in the softmax step is O ( | W | ) . One trick to reduce this excessive number is to factor-ize the vocabulary into K classes, such that every word belongs to exactly one class. As such, the problem of selecting a word from the whole vocabulary boils down to a two-step task: identifying the target class and selecting the word within the class. In other words, the conditional probability p ( w c | w t ) is factorized as follows: where s c is the class that the word w c belongs to. Since both terms time complexity of the softmax step can be reduced to O (
I n practice, a further reduction of the complexity can be achieved by repeatedly partitioning the vocabulary of a class into subclass-es. The resulting tree structure is often called a hierarchical soft-max . In general, factorized neural language models do not have any linguistic motivations behind them but are rather constructed for efficiency.

Noise Contrastive Normalization. Noise Contrastive Normal-ization (NCE) has previously been used as an alternative to fac-torization for speeding up the training of a neural language model. It is a technique that avoids evaluating the explicit normalization constant when computing gradients. The idea behind NCE is to convert the probability estimation problem into a binary classifica-tion task. Specifically, a binary classifier is defined to discriminate between samples drawn from the empirical distribution and those generated by a known noise distribution q , for example, a unigram distribution [33]. The objective replaces Equation (1) as: 1 T
X where N is the number of negative samples drawn from the noise distributions. In the above equation, the probability that a contex-tual word is generated from the empirical distribution given a target word is computed as follows.
Note the equation still involves the normalization constant in each term of p ( w c | w t ) , but here it is treated as a parameter to be optimized during training. In the work of Mnih and Teh [33], the authors discovered that fixing the normalization constants to 1 would not significant affect the model performance.
Before we extend the neural Skip-gram to account for both words and concepts, we provide some insights on the word-concept rela-tions and how such relations can be obtained. The term concept is inherited from the context of the knowledge base, where it stands for the common hypernym of a group of hyponyms. For example, both apple and pear have the concept of fruit . Both Microsoft and Google hold the concept of company . In a broader sense, the term concept refers to the shared meaning of a group of words. The key idea behind our model is to obtain the word-concept relations and derive the respective embeddings for each word and each concept. explicit implicit Here, intrinsic word embeddings are local but ambiguous: every w ord has a unique representation, in which all senses get merged; concept embeddings are global but specific: a concept is shared by many words but conveys an unambiguous meaning. The com-plementary nature of word and concept embeddings suggests that better contextual representations of words can be derived by com-bining the two in the right manner. Consider for instance a target word w with a set of related concepts c 1  X   X   X  c n . The contextual word representation for w is obtained with a compositional func-tion f as follows: where c i is the most appropriate concept of the word in the given context.

A first step required by our model is to identify the candidate concepts for each word, after which we apply an algorithm to up-date the relevant concept vectors together with the word vectors during the training of a neural language model. In general, the term concept can be defined either explicitly or implicitly (see Table 2). On the one hand, explicit concepts of a word can be acquired from a knowledge base, such as Freebase [6] and Probase [46], where the word-concept relations are explicitly mentioned. On the other hand, implicit word-concept relations can be obtained with either supervised or unsupervised machine learning algorithms. Exam-ples include topic models [5] and Brown clustering [12]. As we mentioned, the framework we propose is broadly applicable to any meaningful concept space. Next, we show how the word and con-cept embeddings can be jointly trained.
We propose two classes of neural language models for co-training word-concept embeddings, based on the Skip-gram.
The essence of a neural language model is to discover salient word representations that best interpret the co-occurrence relation-s among words. This is instantiated in the Skip-gram as updating the word representations so that the model can predict the likely contexts of a target word. An assumption made here is that words that appear often in similar contexts tend to have similar meanings, and hence should be assigned similar representations. Based on this assumption, we extend the Skip-gram by introducing concepts into the prediction. In the first model variant, we emphasize the co-occurrence relations between the target concept and the contex-tual words. The augmented objective function is to maximize the following average log probability:
Same as in Equation (1), w 1 , ...w T denotes the entire training se-quence. w t a nd w c represent the target word and contextual word respectively. e t denotes the concept of w t in the given context. The way e t is selected from the candidate list will be discussed in a later section.

In the above model, the embeddings of a target word and its concept are updated in parallel; the two predictions p ( w p ( w c | e t ) are decoupled but we want their values to be maximized at the same time. For the reason we call the model a variant of the Parallel Word-Concept Skip-gram (PWCS-1).

In the second variant of PWCS (PWCS-2), we use the target word to predict the concepts of the contextual words, with the fol-lowing objective function:
Combining the above two model variants, we arrive at a more complete objective function embracing all possible predicative re-lationships:
The above model variant, which we name PWCS-3, depicts a complete bipartite graph between the target vertexes and the con-textual vertexes, with a vertex representing a word or a concept. A pictorial interpretation of the three variants of PWCS is shown in Figure 1.
PWCS trains word-concept embeddings in a parallel fashion, where a word and its corresponding concept are assumed to be con-ditionally independent. A further question we pose is whether we can better emphasize the connections between a word and its con-cept within a single prediction process. As a concept refers to a collection of similar meanings, the task of choosing a word to fit in the context can be reduced to two steps: locating the right con-cept and then searching for a word underneath the chosen concept. This decomposed task can be integrated into a factorized neural lan-guage model, replacing the class s in Equation (3) with the concept e . The resulting objective function is We call the above model a variant of the Generative Word-Concept Skip-gram (GWCS-1) based on the generative nature. Note that the formulation of GWCS is in contrast to the conventional factorized neural language model, where factorization is only for efficiency concerns. Instead, we intend to extract the representation of a class as a summary of the word meanings underneath it. This in return in-jects linguistic motivations to the factorized neural language mod-el. Also note that a word is allowed to be affiliated with multiple classes in GWCS.

GWCS-1 emphasizes the intrinsic relationship of a contextual word and its concept, but the concept of the target word is not in-cluded. Although it is hard to link the target word and its concept in a similar generative process, we can start from Equation (7) and extend each probabilistic term respectively: p ( w c | w t ) p ( w c | e t ) = p ( e c | w t ) p ( w c We denote this model variant as GWCS-2. The topologies of GWCS-1 and GWCS-2 are depicted in Figure 2.
In all variants of the extended neural Skip-gram models, com-puting the normalization constant for each probabilistic term can be expensive. Even for the factorized GWCSs, training can be s-low if the number of concepts or the size of the class vocabulary is large. We therefore investigate methods to accelerate the training via term-wise noise contrastive estimation (NCE) [33]. Take the term p ( w c | w t , e c ) as an example, we show in the following how NCE can be applied.

With the notion of NCE described in Section 2, we define a bi-nary classifier discriminating between samples drawn from the data distribution and those drawn from a uniform noise distribution of w , namely the distribution of words within a given class. The objective of maximizing the (log) likelihood of p ( w c | w placed as: log p ( D = 1 | w t , w c , e c ) + By explicitly fixing the normalization constant to 1, we can esti-mate the probability as where s is the scoring function as in Equation (2). By applying NCE to every term in the objective function by drawing samples from the noise distribution within the domain of interest, the train-ing can be conducted much more efficiently.
Another question yet to be answered is how the context-appropriate concept of a word can be identified for updating. For example, when apple appears in the context of  X  an apple engineer in amer-ica  X , we hope that the majority of the concept updates are on the concept of company , since the other concept ( fruit ) is irrelevant. Formally, assuming a word w in a training sentence is associated with a set of candidate concepts C , the task can be recasted to that of finding the most suitable concept c i  X  C for w t to fit the current context.

The task of linking a word to its context-appropriate concept refers to conceptualization in the field of knowledge representa-e tio n and reasoning, which is considered an off-line task of unsuper-vised inference [42, 17, 45]. To save the excessive amount of time required in this preprocessing step, we propose using a softmax classifier and conceptualize each word  X  X n the fly X  of the training.
Specifically, for a word w occurring in a sentence w 1 ,  X   X   X  , w as context, we calculate the context representation as an average of the word vectors in the context: where w i is the representation of the i th contextual word. We are now able to estimate the probabilistic distribution of concepts of w based on the softmax function acting on the context representation: where E ( w ) is the set of candidate concepts for w .

Given the likelihood of each concept in the current context, the updates can be carried out in two ways. In a hard assignment, the concept that receives the highest score will be selected for updating: Under a soft criterion, a weighted update of all candidate concepts is performed. Here, we redefine the error induced by the concept as a weighted sum error induced by all candidate concepts. Take Equation (7) as an example and assume we want to update param-eters based on the error J ( e ) induced when maximizing p ( w We redefine J ( e ) as Based on the above equation, we perform a weighted update of all candidate concepts. The soft update has been empirically found to ensure optimum performance in a  X  X old start X  condition.
In this section, we discuss how the word-concept embeddings can be used to derive context-appropriate representations for words, phrases and sentences.
In the proposed model, the intrinsic word embeddings preserve the unique features of each word, but theses representations are ambiguous. On the other hand, a concept is shared by a group of words, but it conveys a precise meaning. To obtain a better rep-resentation framework that is context aware, we combine the two types of embeddings in a compositional manner. Let w x denote the contextual representation for word w , whose intrinsic embedding is w . c x denotes the current concept representation of w obtained either in a hard (selecting only one concept embedding) or a soft (weighing all concept embeddings) way based on Equation (15). Then
In the above equation, f stands for an arbitrary compositional function and we use weighted sum in this work. That is, where  X  controls the relative importance between the two types of embeddings. The optimal value of  X  can be decided via grid search in a small validation set.
Distributional representations have proven successful at the word level. But in practice, the need for representations of larger text units is more evident. For example, in web searching, we are con-cerned with the representations of queries and documents. While this is not the main theme of the current paper, we provide a brief discussion on how the compositional representations can be ob-tained so as to be injected with the contextual component carried by the word representations.

Much work in research literature has been dedicated to exploring the compositional operations of words, ranging from simple alge-braic operations [31] to more sophisticated neural networks [41, 19]. Among these approaches, the simplest one is vector addition, in which the representation of a piece of text s is obtained as the sum of all word representations w i in the text: The bag-of-words approach often constitutes a simple but effective baseline.

On the other hand, neural network based compositional models are evolving to become more important in compositionality [41, 19]. In these models, a neural network takes as input a pair of word vectors w 1 , w 2 and returns a composite vector y as follows: Here, t and b are model parameters and f represents the nonlin-earity. During experiments, we find that a simple neural bigram model perform surprisingly well. The model refers to a convolu-tional neural network with one convolution layer (with a window of size 2) and one pooling layer that combines the compositional bigram features. This architecture is shown in Figure 3.
Three sets of experiments are conducted to evaluate the contextu-al word embeddings created in our framework. This section covers all experimental details, results and findings. We start by specifying aspects of the experimental set-up. Figure 3: The architecture of a neural bigram sentence model.
We explore two sets of word-concept relations, one obtained ex-plicitly and the other implicitly.

Explicit Concepts. The explicit word-concept relations are re-trieved from a large-scale probabilistic knowledge base, known as the Probase 1 [46]. As an advantage, Probase has a much richer con-cept space (with 2.7 million concepts) than other existing knowl-edge bases, such as Freebase (with 2,000 concepts) [6] and Cyc (with 120,000 concepts) [23]. The word-concept relationships in Probase are harvested from 1.68 billion web pages and two years of Microsoft Bing search logs [46, 22].
 Overall, Probase contains a number of closely related concepts. For example, apple is associated with both concepts of fruit and seasonal fruit . To reduce the amount of overlapped concepts, we perform k-Medoids clustering [25] on the raw concepts of Probase. Eventually we get 4,819 concept clusters, with each centered at the most salient concept. We use the 4,819 concept clusters as the explicit concept space.

In Probase, every word-concept relationship has a typicality s-core indicating the importance of the concept to the word. After the K-Medoids clustering, we compute a similar score for each word-cluster by aggregating the scores between the word and every con-cept within the cluster. We then prune for each word the clusters with low scores. As a result, a polysemous word normally belongs to 3 to 10 clusters.

Implicit Concepts. The explicit word-concept relations usually have high accuracy but low coverage since many verbs and adjec-tives do not possess concepts in the knowledge base. We addition-ally explore implicit concepts obtained in a data-driven manner. Specifically, we perform Latent Dirichlet Allocation (LDA) [5], a well established topic model, with 3 million Wikipedia documents. LDA is a hierarchical Bayesian model for text generation. In LDA, each document d is modeled as a distribution over K topics, each of which is then characterized by a distribution over words. The words in a document are generated by repeatedly sampling a topic based on the topic distribution and then sampling a single word from the selected topic.

Formally, given a corpus consisting of M documents, the gen-erative process for a document d is defined as follows. First, the mixing proportion over topics  X  d is drawn from a Dirichlet prior with parameters  X  . Each document is then generated according to the topic distributions z 1: K and word probabilities over topics  X  . The probability of a document d in a corpus is defined as: p ( d |  X ,  X  ) =
P robase data is publicly available at http://probase.msra.cn/dataset.aspx
The central challenge in LDA is to compute the posterior dis-tribution p (  X , z | d,  X ,  X  ) of the hidden variable z given a document d . Although this distribution is generally intractable, a variety of approximate inference algorithms have been proposed in the litera-ture. In this work, we use a well-implemented variational inference for posterior estimation [36]. The number of topics is set to 500. As a result, we obtain 500 latent topics as implicit concepts. We prune for each word the topics that receive low probabilities. We train our neural language models on the  X  X ne Billion Word Language Modeling Benchmark X  dataset [9] released by Google. All model variants are implemented in two respective concept s-paces and the resulting embeddings are evaluated. In each model variant, a word is associated with two sets of vectors, assuming the same word as target and as context come from two distinct vocab-ularies [28]. For concepts we do not adopt this assumption since they are essentially shared among words. The dimension of all em-beddings is set to 300. The code is written in C language as an extension of the efficient WORD 2 VEC toolkit [29]. All trainings are performed on a Windows Server 2008 with a 2.13 GHz Intel Xeon CPU (4 processors).
The first set of experiments concerns the similarity of words and phrases as a generic evaluation of the vector space. We provide comparisons between the aforementioned model variants, as well as comparisons between our models and a few well-established baselines. To avoid overfitting, we adopt two datasets here, one at the word level and the other for phrases.
 The word-level dataset refers to the Stanford Contextual Word Similarity Dataset (SCWS) of Huang et al. [18], which contains 2,003 pairs of words and their sentence contexts. For each pair of words, there are 10 human similarity scores provided. Our task is to evaluate by Spearman X  X  correlation how the similarities computed from various models match those of human judgments. Note that the sentence contexts provided in the dataset enable us to create contextual word representations.
 The phrase similarity dataset we use is the one of Mitchell and Lapata (M&amp; L) [32]. This dataset consists of similarity judgments for 1,944 pairs of adjective-noun (A-N), noun-noun (N-N) and verb-object (V-O) phrases, respectively. (In total, there are 5,832 phrase pairs.) Similar to SCWS, each pair of phrases in M&amp;L is annotat-ed with a similarity score. Although there is no additional context provided for each phrase, the contextual representation of a word can be derived with its neighboring word in the phrase as context.
Hyper-parameters. At first, we consider the two forms of con-cept representations of a word in a given context. The globalCon-cept metric selects a single concept for the word based on the score computed from Equation (15). The avgConcept metric, on the oth-er hand, derives the contextual concept representation as a sum of all concept embeddings weighed by their scores. We compare the two types of concept representations obtained by PWCS-1 with the explicit concept space. Meanwhile, we perform a grid search to decide the optimum value of  X  in Equation (19).

From Figure 4, we observe that avgConcept generally outper-forms globalConcept . In both metrics, the Spearman X  X  correlation increases as the conceptual features start to be injected into the word embeddings. However, a further increase of  X  will lead to some performance degradation in either case. One reason is that words need to maintain their unique features in order to discrim-inate one another; whereas the representations of two conceptual-ly same words will be almost the same in the case of a large  X  . For avgConcept , the best result is obtained when  X  is between 0.5 to 0.8, while for globalConcept the optimum  X  is between 0.8 to 1. The rest of the experiments will be based on avgConcept with lambda set to 0.75.

Comparison of Model Variants. In the next part, we evaluate the effectiveness of conceptual word representations against raw word embeddings. The comparison is respectively performed in each of the five model variants with the explicit concept space. The results are summarized in Table 3. As we can see, for all vari-ants of PWCS and GWCS, representing words contextually brings significant performance gains. We further notice that GWCSs in general outperform PWCSs in terms of both types of embeddings. The finding indicates that taking into account the semantic rela-tions within a neural architecture is helpful for the representation learning task.
 Table 3: Spearman X  X  correlation  X   X  1 00 of the variants of P-WCS and GWCS.

Comparison of Concept Spaces. Next, we compare the con-textual word representations trained with the two concept spaces: explicit concepts based on the Probase and implicit concepts ob-tained via LDA. We use PWCS-3 and GWCS-1 in this evaluation. The results are shown in Table 4.

Comparing the numbers given by the two spaces, the explicit space shows superior performance on the similarity measure of noun-noun phrases, while the implicit space does better on the SCWS and verb-object tasks. One interpretation is that explic-it concept mapping has higher accuracy but low  X  X ecalls X : many words (such as verbs and adjectives) do not have any concept in the knowledge base. In this case, inducing implicit concepts provides a complement.
 Table 4: Spearman X  X  correlation  X   X  1 00 for contextual word representations in different concept spaces.

Comparison with Baselines and Published Results. Next, we compare the results of our models with a few well-established base-lines in the SCWS task. These include the neural language model of Collobert and Weston (C&amp;W) [10], the ordinary Skip-gram model trained with hierarchical softmax, and three sets of sense embed-ding models of Huang et al. [18] ( nClass =3), Neelakantan et al. [34] and Bartunov et al. [1], respectively. These models were all trained with the code released by the authors. An additional base-line is preprocessed , in which the word-concept relations are used outside embeddings. In this method, the training words are pre-processed into word-concept pairs, after which the ordinary Skip-gram is applied. In terms of our models, we report the best number achieved with PWCS and GWCS respectively.

Overall, we see from Table 5 that the contextual word representa-tions significantly outperform the word embedding baselines. The numbers given by sense embeddings are quite competitive without any consideration of efficiency in mind, but still the highest number is achieved by GWCS. Table 6 further compares the training time of various models relative to the Skip-gram. In terms of efficiency, our models have a clear advantage over Huang et al. [18], while they are slightly faster than the non-parametric methods of Nee-lakantan et al. [34] and Bartunov et al.[1], But note that we only need to store 4,819 explicit concept vectors or 500 implicit concept vectors, whereas the size of sense vectors is usually several times more than the size of the vocabulary (55,000). Table 5: Spearman X  X  correlation  X   X  1 00 of various models in comparison.
 Table 6: Training time of various models relative to the Skip-g ram.

Concept Assignments for Rare Words. In the end, we quali-tatively evaluate the selected concepts by GWCS-1 for a few rare words in SCWS. In the concept-based disambiguation method, we can visualize directly the exact concept that a word is assigned to. However, in the sense-based models, we are unable to find out the exact meaning behind a sense.

A s the results in Table 7 suggest, rare words can be accurately mapped to the most context-appropriate concepts. We argue that the concept-based disambiguation method is superior in the sense that a concept representation is learnt with all words sharing that concept. This not only emphasizes the semantic interactions among words, but also provides an elegant solution to rare words whose senses are hard to induce the other way around.
Measuring the similarity of two words or phrases provides pre-liminary evidence on the quality of the contextual word represen-tations. In the second experiment, we augment the evaluation by m easuring the similarity of sentences. We use the Microsoft Re-search Paraphrase Corpus (MSRPC) introduced by Dolan et al. [11]. The MSRPC contains 5,801 sentence pairs (4,076 for training and 1,725 for test) and labels (0 or 1) indicating whether a pair of sen-tences has a paraphrase relationship. We explore the use of contex-tual features obtained with our models in the classification task and compare the results with various baselines and published numbers.
Specifically, we use three sets of features here: 1) the cosine similarity of the pair of sentence vectors; 2) the number of shared concepts of the two sentences identified from our models; 3) the lengths and the co-occurring word counts of the two sentences. In the baselines, the second feature is not included. Note that the third set of features do not stem from our models but are used as a com-plement, since raw embeddings cannot capture subtle aspects of a sentence such as proper nouns. Similar features are also used in the studies of Blacoe and Lapata [4] and Socher et al. [40]. We follow Blacoe and Lapata [4] to use the liblinear classifier [13] in this task. Besides, we explore two ways to compose words into sentences: by vector addition (  X  ) and neural bigram model (nn) respectively.
As the results in Table 8 reveal, our models outperform most baselines and the published numbers which are based on distribu-tional features [16, 4, 27]. Among the published models, the clos-est one to ours is the work of Blacoe and Lapata [4], who use a similar set of features except that their embeddings are based on co-occurrence statistics. Results demonstrate the superiority of our contextual embedding space. Still, there exist more sophisticated models that are successful in performing this task. For example, Socher et al. [40] apply dynamic pooling on the output of the re-cursive auto-encoder to create more faithful similarity features; and Madnani et al. [26] use a combination of eight machine translation metrics to achieve state-of-art results. Compared with these works, our features are much shallower yet we obtain close numbers.
In the last experiment, we evaluate the effectiveness contextual word representations in a practical task. As an aid to open domain question-answering, the task is to decide whether a pair of question and answer are semantically related. The corresponding dataset contains 85,512 pairs of question-type queries and the correspond-ing answers retrieved from the search log of a commercial search engine. The semantic relatedness of each question and answer is scored by a group of professional annotators, after which a label (0 or 1) is assigned to each pair based on the average score. Our task here is to evaluate the quality of the embedding features by evaluating how the semantic relatedness computed in the vector s-pace matches that of human judgment.We divide the dataset into 80% for training and 20% for testing. We train a logistic regression c lassifier that takes as input the cosine similarity of each question-answer and outputs the label. The representation of a question or an answer is obtained with vector addition excluding stop-words. The classification results of various models are shown in Table 9.
W e see a significant increase in the performance of contextu-al word representations with respect to raw word embeddings and baselines. In the presence of a large amount of training data, the experimental results provide stronger evidence on the effectiveness of our contextual word representations.
Word representations learnt from traditional distributional prac-tices and neural language models are limited by their inability to handle polysemy. To solve the problem, several approaches have been proposed to create multi-sense word representations. The ini-tial proposal was to obtain the sense vectors of a word by clus-tering the context vectors obtained in a normal distributional prac-tice. This methodology has later been popularized by Reisinger and Mooney [37] and named as Multi-Prototype Vector Space (MPVS). Huang et al. [18] apply MPVS to pre-trained word embeddings to get neural sense representations. More recently, Neelakantan et al. [34] integrate the context clustering step into a Skip-gram, result-ing in a unified model where all parameters are jointly optimized during training.

A common limitation of all the above work lies in the excessive amount of computational cost induced in the clustering step, which has to be applied to every word in the vocabulary.

There also exist several approaches to training sense embed-dings directly from a neural language model, without consider-ing any clustering step in mind. Pina and Johansson [35] use a naive Bayesian step for updating the most proper senses in a neu-ral language model. Bartunov et al. [1] propose a non-parametric Bayesian extension of the Skip-gram. Their model is capable of learning not only sense embeddings but also the number of senses a word has. Meanwhile, Li and Jurafsky [24] use the non-parametric Chinese Restaurant Process for sense induction in a Skip-gram. Despite the improved efficiency of these models, they all require a significant amount of storage in the case of a large vocabulary, since every sense is associated with a vector which will be stored. Moreover, there is no clear mapping between the senses learned by these models and the actual senses of a word. This becomes prob-lematic for rare words or rare senses, for which there does not exist enough training contexts for inference.

Another line of related research is on knowledge-powered word embeddings. Following the work of Bordes et al. [8], several meth-ods have been proposed to represent entities of a knowledge base in the vectorial form [8, 7, 39]. The majority deal with the &lt; head entity, relation, tail entity&gt; triplets and scores their re-lational plausibility with word embeddings. The scoring function ranges from simple spatial distance between entity embeddings trans-formed through the relation [8, 7] to more sophisticated Neural Tensor Networks [39]. Additionally, some works use relational information extracted from semantic lexicons as constrains in the training of neural language models [3], or in a post-processing step to refine pre-trained word vectors [14]. To the best of our knowl-edge, no previous work on embeddings deals with word-concept relations in a knowledge base.
Representing text well is crucial for machines to understand hu-man language. In this paper, we propose a novel framework for representing text contextually in the distributional semantic space. The key idea is to utilize the conceptual information of words to disambiguate their meanings. In the proposed framework, words are assigned with dynamic representations which change with con-texts. This framework differs from traditional  X  X ector-space based word sense induction X  in the sense that it does not create any sense-specific representations. Instead, it looks into the shared concepts among senses, resulting in a class of neural language models effi-cient in both computation and storage, and with improved applica-bility to rare words and rare senses. An interesting future direction would be incorporating the concept induction step within the train-ing of a neural language model referencing Brown clustering [12].
This work was partially supported by the National Key Basic Re-search Program (973 Program) of China under grant No. 2014CB340403 and the Fundamental Research Funds for the Central Universities &amp; the Research Funds of Renmin University of China. The authors would like to acknowledge Haibo Shi for the initial discussions on this topic at Microsoft Research Asia. [1] S. Bartunov, D. Kondrashkin, A. Osokin, and D. Vetrov. [2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural [3] J. Bian, B. Gao, and T.-Y. Liu. Knowledge-powered deep [4] W. Blacoe and M. Lapata. A comparison of vector-based [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. [7] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and [8] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning [9] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, [10] R. Collobert and J. Weston. A unified architecture for natural [11] B. Dolan, C. Quirk, and C. Brockett. Unsupervised [12] M. B. Eisen, P. T. Spellman, P. O. Brown, and D. Botstein. [13] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. [14] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and [15] Z. S. Harris. Distributional structure. Word , 1954. [16] S. Hassan. Measuring semantic relatedness using salient [17] W. Hua, Z. Wang, H. Wang, K. Zheng, and X. Zhou. Short [18] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. [19] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A [20] D. Kartsaklis, M. Sadrzadeh, et al. Prior disambiguation of [21] D. Kartsaklis, M. Sadrzadeh, and S. Pulman. Separating [22] T. Lee, Z. Wang, H. Wang, and S.-w. Hwang. Attribute [23] D. B. Lenat. Cyc: A large-scale investment in knowledge [24] J. Li and D. Jurafsky. Do multi-sense embeddings improve [25] P. Li, H. Wang, K. Q. Zhu, Z. Wang, and X. Wu. Computing [26] N. Madnani, J. Tetreault, and M. Chodorow. Re-examining [27] R. Mihalcea, C. Corley, and C. Strapparava. Corpus-based [28] T. Mikolov, Q. V. Le, and I. Sutskever. Exploiting similarities [29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [30] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities [31] J. Mitchell and M. Lapata. Vector-based models of semantic [32] J. Mitchell and M. Lapata. Vector-based models of semantic [33] A. Mnih and Y. W. Teh. A fast and simple algorithm for [34] A. Neelakantan, J. Shankar, A. Passos, and A. McCallum. [35] L. N. Pina and R. Johansson. A simple and efficient method [36] D. Ramage, E. Rosen, J. Chuang, C. D. Manning, and D. A. [37] J. Reisinger and R. J. Mooney. Multi-prototype vector-space [38] H. Sch X tze. Automatic word sense discrimination.
 [39] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning [40] R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and A. Y. [41] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing [42] Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. Short text [43] F. Wang, Z. Wang, Z. Li, and J.-R. Wen. Concept-based short [44] Z. Wang, H. Wang, and Z. Hu. Head, modifier, and constraint [45] Z. Wang, K. Zhao, H. Wang, X. Meng, and J.-R. Wen. Query [46] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: A
