 Tensors (multi-dimensional arrays) are widely used for rep-resenting high-order dimensional data, in applications rang-ing from social networks, sensor data, and Internet traf-fic. Multi-way data analysis techniques, in particular ten-sor decompositions, allow extraction of hidden correlations among multi-way data and thus are key components of many data analysis frameworks. Intuitively, these algorithms can be thought of as multi-way clustering schemes, which con-sider multiple facets of the data in identifying clusters, their weights, and contributions of each data element. Unfortu-nately, algorithms for fitting multi-way models are, in gen-eral, iterative and very time consuming. In this paper, we observe that, in many applications, there is apriori back-ground knowledge (or metadata) about one or more domain dimensions. This metadata is often in the form of a hier-archy that clusters the elements of a given data facet (or mode). In this paper, we investigate whether such single-mode data hierarchies can be used to boost the efficiency of tensor decomposition process, without significant impact on the final decomposition quality. We consider each domain hi-erarchy as a guide to help provide higher-or lower-resolution views of the data in the tensor on demand and we rely on these metadata-induced multi-resolution tensor representa-tions to develop a multiresolution approach to tensor de-composition. In this paper, we focus on an alternating least squares (ALS) based implementation of the PARAllel FAC-tors (PARAFAC) decomposition (which decomposes a ten-sor into a diagonal tensor and a set of factor matrices). Ex-periment results show that, when the available metadata is used as a rough guide, the proposed multiresolution method helps fit PARAFAC models with consistent (for both dense and sparse tensor representations, under different param-eters settings) savings in execution time and memory con-sumption, while preserving the quality of the decomposition.  X 
This work is partially supported by an NSF Grant #1043583 - X  X iNC: NSDL Middleware for Network-and Context-aware Recommendations X  H.3.3 [ Information Search and Retrieval ]: Clustering Theory, Algorithms Tensor decomposition, multiresolution, PARAFAC
Multidimensional arrays, i.e. tensors, are increasingly used for multi-way data representation in diverse of applica-tions [1, 19]. Starting from their early use in psychometrics [31], multi-way analysis techniques, and in particular ten-sor decompositions, have been widely used in extraction of hidden correlations among multi-way data, in fields, such as information retrieval [7], sensor networks analysis [26], web ranking and analysis [28, 18].

Intuitively, tensor decomposition algorithms can often be thought of as multi-way clustering schemes, which consider, simultaneously, multiple facets of the data for identifying clusters, their weights, and contributions of each data el-ement. Tensor decomposition techniques are often placed into three categories: the first group includes the Parallel Factors (PARAFAC) decomposition [10] (also called CAN-DECOMP [4]) and its extensions like PARAFAC2 [11], S-PARAFAC [12] and PARALIND [3] that decompose a tensor into a smaller diagonal tensor and a set of factor matrices, one for each dimension of the input tensor. In the second category we find models based on Tucker decomposition [30] that approximate the initial tensor into a core tensor and a set of matrices that represent subspaces of each dimension. The third group contains models like Multilinear Engine [21] and STATIS based multi-way models [25]. In the literature, there are various algorithms for fitting tensor decomposition models [23, 22, 15]. In particular, algorithms based on alter-nating least squares (ALS) estimate the decomposition one factor matrix at time, keeping other factors fixed; the pro-cess is repeated until the convergence condition is reached.
Unfortunately, algorithms for fitting multi-way models are, in general, very time consuming. In this paper, we observe that, in many applications, there is apriori back-ground knowledge (or metadata) about one or more domain dimensions and this metadata is often in the form of a hi-erarchy that clusters the elements of a given data facet (or mode). We investigate whether such single-mode data hier-archies can help boost the efficiency of tensor decomposition process, without significant impact on the final decomposi-tion quality. Intuitively, we consider each available domain hierarchy as a support that provides different views of the data, with varying resolutions along a single dimension. Us-ing these different resolutions at different stages of the de-composition process, then, helps eliminate redundant work and save execution time.
 In this paper we focus our attention on the Alternating Least Squares (ALS) based algorithms for fitting PARAFAC models, proposed by Carrol et. al [4] and Harshman [10]. We present a metadata-based multiresolution approach to ALS based PARAFAC. The algorithm enables the user to pick and choose among the available domain hierarchies, the approximation levels, as well as to vary target convergence condition for different levels of the input hierarchies to sup-port tensor decomposition. The algorithm starts at a low resolution and identifies a decomposition for that resolu-tion. Once the iterations for the given level is complete, the resulting decomposition is expanded and this expanded decomposition is used as a starting point for the next, higher resolution. This process is repeated one resolution at a time until all the selected levels of the hierarchy have been con-sidered.

The proposed multi-resolution process increases the num-ber of iterations of the algorithm; significant time savings are obtained because the initial iterations are much faster due to the lower resolution of the data. Moreover, the num-ber of costly high resolution iterations are significantly re-duced. Experiment results show that, when the available metadata is used as a rough guide (avoiding over-fitting to the lower resolution of the tensors), the proposed multires-olution method helps fit PARAFAC models with consistent (for both dense and sparse tensor representations, under dif-ferent parameters settings) savings in execution time and memory consumption, while preserving the quality of the obtained decomposition. Experiments also show that the proposed method can leverage more than one uni-modal hi-erarchy to further reduce tensor decomposition times.
The structure of the paper is as follows. Section 2 provides an overview of the related works and introduces the tensor background knowledge and formalisms used along this pa-per. The details of the multiresolution algorithm are pre-sented in Section 3, while Section 4 contains experimental evaluation. We conclude the paper in Section 5.
In this section we introduce the tensor notation, present background about operations on tensors and present other related works.

A tensor is a multidimensional array, where the order (also known as the number of ways or modes) N represents the number of dimensions. Following the notation reported in [19] vectors are represented by boldface lowercase letters (e.g. a ), matrices are denotated by boldface capital letters (e.g. A ), while tensors with order N  X  3 are denoted by boldface Euler script letters (e.g. X ). Scalars are denoted by lowercase letters (e.g. a ). Thus, an N -order tensor can be formalized as: The i th entry of a vector a is denoted by a i , the element 1 ( i, j ) of a matrix A is denoted by a ij , while the element ( i, j, k )ofa3  X  order tensor X is represented by x ijk . Subarrays are formed when a subset of the indices is fixed. For example, if we want to identify the j th column of the matrix A ,wecanwrite a : j . Fibers are defined by fixing every index of a tensor, but one. In a 3-order tensor we can identify column, row and tube fibers, denoted by x : jk , x x : . Slices are two-dimensional sections of a 3-order tensor, defined by fixing all but two indices. Horizontal, lateral, and frontal are denoted as X i :: , X : j : ,and X :: k . The two most popular tensor decompositions are the Tucker [30] and the PARAFAC/CANDECOMP [10, 4]. The Tucker decomposition approximates a tensor using a smaller tensor through a change of basis. Intuitively, the Tucker de-composition generalizes singular value matrix decomposition (SVD) to higher-dimensional matrices. One key difference is that Tucker fails to guarantee a unique and perfect de-composition. Instead, most approaches involve searching for orthonormal facet matrices and a core tensor that collec-tively minimize the decomposition error. For example, the High Order SVD approach first identifies the left eigenvec-tors (with the highest eigenvalues) of the lateral, horizontal, and frontal slices to construct the facet matrices.
CANDECOMP [4] and PARAFAC [10] decompositions (together known as the CP-decomposition) take a different approach and decompose the input tensor into a sum of com-ponent rank-one tensors: Given an input tensor X and a core size r , the PARAFAC decomposition finds r rank-one ten-sors in the form of  X  i u (1) i  X  X  X  X  X  X  u ( N ) i ,where r is a positive integer, u ( d ) i  X  R I d for 1  X  i  X  r and  X  represents the vector outer product. The decomposition approximates the tensor X by minimizing the value of Alternatively, the decomposition can also be rewritten in terms of factor matrices (see Figure 1) as a combination of the vectors from the rank-one components as: X X  [[  X  ; U (1) , U (2) ,  X  X  X  , U ( N ) ]] = where  X   X  R R and U ( i )  X  R I i  X  R . PARAFAC can be con-sidered as a special case of Tucker decomposition in which the core tensor  X  is superdiagonal. Intuitively, as in SVD, PARAFAC enforces that the central matrix is diagonal; how-ever, unlike in SVD the facet matrices are not guaranteed to be orthonormal. Figure 2: The pseudo-code for alternating least squares fit algorithm for PARAFAC models [19]; represents the Khatri-Rao product [24]
Many of the algorithms for fitting multi-way models are based on an iterative process that approximates the best solution until a convergence condition is reached. Iterative Algorithms. The alternating least squares (ALS [32]) method is relatively old and has been successfully applied to the problem of tensor decomposition by Carrol et al. and Harshman [4, 10]. ALS is widely used as a build-ing block in many applications for fitting PARAFAC and Tucker models [18, 27, 20]. ALS estimates, at each itera-tion, one factor matrix, maintaining other matrices fixed; this process is repeated for each factor matrix associated to the dimensions of the input tensor. Since our proposed multi resolution approach extends the basic ALS, in Figure 2 we present the pseudo-code of the standard ALS fitting for PARAFAC. The algorithm takes as input the data tensor X , the number of factors r , a tolerance value tol (used as a stopping condition), and an optional initial factorization F init . The algorithms return the factor matrices and the elements of the resulting diagonal tensor  X  .

Other alternation-based algorithms include the alter-nating slice-wise diagonalization (ASD) [15] and the self weighted alternating trilinear diagonalization (SWA-TLD) [6] algorithms: they improve fitting using objective func-tions that are not based on least squares.
 Closed Form and Gradient-Based Algorithms. Non-Iterative approaches to tensor decomposition include closed form solutions, such as generalized rank annihilation method (GRAM) [22] and direct trilinear decomposition (DTLD) [23], which fit the model by solving a generalized eigenvalue problem. Gradient-based methods include the PMF3 algo-rithm that is based on a modified version of a Gauss-Newton method. An in depth study and comparison of these algo-rithms can be found in [29, 9].
 Tensor Decomposition with External Knowledge. [8] presents FacetCube, a framework that allows users to in-corporate prior knowledge in the non-negative Tucker de-composition process, with the primary goal of helping users enforce facets on certain data dimensions. In that work the metadata can be available in different forms. The first one is prior knowledge describing a subspace from which facets can be located. In the second one, the users require facets for some data dimensions be fixed. For example, in an author  X  reference  X  keyword tensor, the paper shows how additional information on authors (i.e. the co-author relationship) can be leveraged in a recommendation system of references. In our multiresolution approach, instead, ad-ditional metadata are used to compute lower-level resolution representations of the input tensor, in order to speed up the decomposition process.

In a work probably most related to the approach presented in this paper, Kaarna et al. leverage a multiresolution ap-proximation of the initial tensor in the ALS-based decompo-sition process [16] for image analysis. Authors recognize that two contiguous pixels in an image can be approximated down to a single pixel since they are spatially correlated . In partic-ular, [16] relies on the hierarchical Integer Wavelet Transfor-mations (IWT) to speed-up the decomposition process for image datasets. There are two main differences between [16] and the approach presented in this paper: First of all, unlike in [16], where the (wavelet) hierarchy relies in the inherent spatial correlation internal to the data , here we investigate the applicability of external metadata in supporting the ten-sor decomposition process. Secondly, unlike [16] where the data (i.e., image) tensor is inherently dense, we aim to also tackle sparse tensors (which are common in many applica-tions, including social networks and graph structured data).
As we pointed out in the Introduction, in this paper we observe that in general there may be external knowledge regarding how the domains represented by the individual tensor dimensions are hierarchically clustered (according to different clustering criteria). In this section we show how such background knowledge can be leveraged as a guide to help provide higher-or lower-resolution views of the data in the tensor on demand and develop a multiresolution ap-proach to tensor decomposition.
Let D i be the domain associated to the i  X  th dimension D
Clustering hierarchies capture different strategies along with the domain elements associated to the correspond-ing dimension can be grouped together. For example, if one dimension of the tensor is associated to the users ac-cessing a given service, the corresponding domain elements may be clustered according to their age, according to the city/province/region/state where they live, or according to the preferences listed in their profiles. Let the tree H i depth depth i , be a hierarchical clustering defined on D resenting some semantic properties of the values associated to the corresponding tensor dimension). Let nodes ( H i ,l )be the set of nodes appearing at level l (0  X  l  X  depth i )in H Note that the following holds: We leverage the given domain hierarchical clustering strat-egy to define multiresolution representations of a given ten-sor. Intuitively, this representation leads to compact repre-sentations of the tensor, in which (multiple) tensor values of domain elements which are clustered together according to the given hierarchy are collapsed in a single value, which is seen as the representative of the entire cluster.
Given a domain D i and a corresponding hierarchy H i ,the representative selection function is the function repr i :2 R which maps a set of real values (in the tensor, correspond-ing to the positions indexed by the elements clustered in H to a single value, chosen as the representative of the cluster. Examples of representative selection functions include min, max, and average. Notice that for any given domain D i and any clustering defined over it, multiple representative selec-tion functions can be defined. When a dimension does not have an associated hierarchy, we denote this using H i =  X  and repr i = self .

Definition 1. Clustering embedding strat-clustering embedding strategy over X is a pair S =  X , { ( H 1 ,repr 1 ,l 1 ) ,..., ( H N ,repr N ,l N ) } is a permutation over the indices of the tensor modes (describing the order in which clusterings are considered among different modes), each H i is a clustering hierarchy defined on D i , repr i is a corresponding representative selection function, and l i is the clustering level adopted for the i th mode.
 {
H 1 ,...,H N } of clustering hierarchies defined on the N do-mains associated to the tensor dimensions we define the cor-responding multiresolution representation of X with respect to the set { H 1 ,...,H N } as follows. Note that we first define uni-modal representation with respect to a single hierarchy and, then, extend this to multi-modal representation which considers multiple hierarchies simultaneously.

Definition 2. Uni-modal multiresolution represen-tation. The multiresolution representation at level l i with respect to the clustering hierarchy H i and the representative selection function repr i of a tensor X X  R I 1  X  I 2  X  ... U where I i = | nodes ( H i ,l ) | . We also refer to U H i as Intuitively, for any cluster C h  X  C along the i th dimension, for all indices along the other modes, | C h | elements of the given tensor are collapsed and a single representative value, resulting from the application of the function repr i on the collapsed elements, appears in the l i -resolution tensor. Figure 3: Hierarchy-based multiresolution represen-tation Figure 4: The example hierarchy H I that encodes additional metadata.
 Figure 3 is a visual representation of a uni-modal multires-olution representation of a 3-mode tensor. For example, if the domain associated to the considered hierarchy is a set of users, the multiresolution approach can be seen as hi-erarchically partitioning them in communities linked by a parent-child relationship.
 Example 1. Let us consider the 3-mode I,J,K tensor X X  Moreover, consider the hierarchy H I depicted in Figure 4 that clusters elements of the dimension I and the average function as representative selection function. The tensor represents the lower-resolution representation at level 1 of the original tensor X = X I, 2 .

Given the definition of uni-modal multiresolution repre-sentation, we can also define the multi-modal multiresolu-tion representation as follows:
Definition 3. N-modal multiresolution representa-tion. The multiresolution representation with re-spect to the clustering embedding strategy S = Figure 5: The pseudo-code of the multiresolution Alternating Least Squares fit for PARAFAC models.  X , { ( H 1 ,repr 1 ,l 1 ) ,..., ( H N ,repr N ,l N ) } of the tensor where I i = | nodes ( H i ,l i ) | . We also refer to M S X S in shorthand. X S is inductively defined as follows: Let tation based on the first mode to be considered according to the permutation  X  specified in S . Then, we have X S = X S where  X  1 &lt;s  X  N , the multi-modal representation is de-fined, relying on the uni-modal definition, as follows:
In this section we discuss how we leverage the multireso-lution tensor representation to boost the efficiency of tensor decomposition process, without any significant impact on the final decomposition quality. The basic idea relies on the observation that the ALS algorithm [32] for PARAFAC ten-sor decomposition is an iterative process which starts from an initial randomly generated solution, on which each iter-ative step improves, until a satisfactory decomposition (i.e, an approximated decomposition differing not more than a predetermined threshold from an exact decomposition) of the given tensor is found.

Based on this, we expect that an  X  X nformed X  initialization of the decomposition process could significantly reduce the computational cost of the iterative process. In particular, we choose to initialize the decomposition process with the approximate decomposition resulting from the ALS decom-position of a  X  X impler X  (i.e., lower resolution) version of the given tensor. We show in Section 4 that the computational cost of this guided decomposition (including the cost of ob-taining the lower resolution decomposition plus the cost of the iterative process starting from the initialization) is lower than the cost in the global decomposition from a randomly initialized iterative process.

Figure 5 contains the pseudo-code of the multiresolution-based algorithm. The algorithm takes as input the tensor, X , the set of hierarchies, H , the set of representative selec-tion function, R , the approximation levels to consider,  X  ,the number of required factors r , and the convergence threshold (or tolerance value), tol , used as stopping condition.
The first step of the algorithm constructs a clus-tering strategy, S  X  =  X , { ( H 1 ,repr 1 ,depth 1  X   X  ) , ..., ( H N ,repr N ,depth N  X   X  ) } , for the lowest resolution level 1 . Here, H i  X  X  , repr i  X  X  ,and  X  is a permutation of of X at the lowest resolution based on this strategy.
Starting from this lowest-resolution tensor X S  X  ,theal-gorithm first computes a lowest-resolution factorization F [  X  ] by using the standard ALS fitting technique (line 2). This produces the (lowest-resolution) set of factor matrices { the tensor dimension I i . The output factorization F [  X  ] be used as the starting point of the next invocation of the ALS tensor decomposition, with (  X   X  1) th approximation.
In order to obtain the more detailed factorization corre-sponding to the ( a  X  1) th approximation from F [ a ] , we need to transform the set of factor matrices by expanding the do-mains of the relevant dimensions  X  the number of elements of dimension that were not compressed by the strategy S are kept the same (lines 5-7). The expand factor () func-tion (line 9) expands the given factor matrix U ( i )[ a ] initial factorization of the corresponding higher-level factor set of nodes at the a th approximation level for the hier-archy H i ; similarly, let N i,a  X  1 be the set of nodes at the ( a  X  1) th approximation level. For a given node, n h  X  N i,a let children ( n h )  X  N i,a  X  1 be the corresponding set of chil-dren at the ( a  X  1) th approximation level.

The expand factor () function considers the dimensions of the tensor one at a time; i.e., for all 1  X  i  X  N ,wehave  X  n h  X  N i,a  X  n k  X  children ( n h ) While the expand i function can be domain specific, in the absence of addition information, it is intuitive to use the identity function:  X  n h  X  N i,a  X  n k  X  children ( n h ) Once the decomposition F [ a  X  1] init is computed using the expand factor () function described above, the algorithm loads the tensor X [ a  X  1] which represents the ( a  X  1) proximate resolution view of the original tensor and com-putes the ALS fitting starting from the initial factorization F est level tensor factorization is computed and returns the corresponding final factorization F [0] (line 16).
Note that the multiresolution algorithm shares with the standard Alternating Least Squares approach both time complexity class and memory consumption upper-bound.
Note that  X  represents the number of considered levels of resolution. Given the hierarchy H i , the algorithm considers the levels from depth i  X   X  to depth i .
In this section we compare our metdadata driven, mul-tiresoultion tensor decomposition method with the standard ALS Based PARAFAC decomposition.
We consider matrices with both dense and sparse repre-sentations. As reference software library, we used the Matlab Tensor Toolbox [2] by Kolda. All experiments are performed on a desktop with a dual core 2.5GHz processor and 4GB RAM. We repeated all experiments 20 times: all reported value represents the average value of all runs.
In this section, we consider the effects of various problem and system parameters. These include We evaluate the decomposition schemes against two key per-formance parameters: CPU time and quality [2] where  X  X is the tensor obtained by recomposing F .TheCPU time does not include the time to obtain different resolution versions of the input tensor.
In the experiments reported in this section, we used a set of 3-mode tensors of the form, author  X  conference  X  keyword , extracted from the publicly available DBLP data [14]. In particular, we selected the most prolific au-thors, the most used keywords, and the most popular con-ferences to construct data tensors with different properties. As mentioned earlier, we have considered both dense and sparse tensors. Table 1 presents the relevant properties of these datasets.
For the three modes of the data tensor, we have considered the following hierarchies: Figure 6: Different execution traces for dblp512 data set (sparse representation). The plots include stan-dard ALS (with threshold 10  X  4 ) and multiresolution approaches (using the last three levels of the authors hierarchy) for different intermediate tolerance values ( 10  X  4 , 10  X  3 ,and 10  X  2  X  the final tolerance value for the highest resolution level is 10  X  4 for all cases). We note that each these hierarchies provide only uni-modal information. Note also that the domain hierarchies are cre-ated intentionally rough and noisy. Furthermore, they reflect different degrees of externality: the conference hierarchy is completely external to the DBLP data set, the author hier-archy is from DBLP, but not directly related to the data in the tensor, whereas the keyword hierarchy is created using document information, which (while not being directly rep-resented in the tensor) relates the authors and the keywords in the DBLP data set.
Before we provide a detailed analysis of the different prob-lem and system parameters, we first analyze data prepara-tion costs and provide sample execution traces to help ob-serve the key advantages of the multi-resolution methods against the traditional ALS based tensor decomposition.
Data preparation involves the creation of multiple reso-lutions of the input tensor and thus depends on both the underlying representative selection function as well as the Table 2: Data preparation costs for different num-bers of approximation levels (based on the authors hierarchy) for dense and sparse data sets number,  X  , of approximation levels to be considered. Data preparation cost also depends on the number of nonzeros in the data as well as how the tensor is organized. Table 2 reports the data preparation costs both for dense dblp512 (with 166 K entries) and sparse dblp20000 tensors (with 3 . 5 M entries) with the average function, for different num-bers of approximation levels. The sparse tensors are manip-ulated using the Matlab Tensor Toolbox [2]  X  except for the initial slicing of the data, which is not necessarily efficient in this library.

While the per-entry costs for sparse and dense tensor rep-resentations are comparable, they represent different relative overheads when considered along with the decomposition costs  X  this is because dense tensor decomposition is much costlier than the decomposition of sparse tensors. As we see in Section 4.2.3, the data preparation costs can largely be ignored when decomposing dense tensors. The same is not true for sparse tensors (Section 4.2.4). Therefore, in the case of sparse tensors, it is preferable to avoid data preparation for each decomposition tasks; instead, the multi-resolution decomposition over sparse tensors will be more effective if the data preparation costs can be amortized over multiple decompositions over the same tensor (e.g., for different sub-sets of modes or for different numbers of factors).
Consider Figure 6, which presents the tensor decomposi-tion for the dblp512 data set (dense representation). Here cp-als curve denotes the quality-time execution trace for the standard ALS scheme. Each marker on the curve corre-sponds to the end of an iteration and the beginning of the next one. As can be seen, ALS provides most of its improve-ments in its first few iterations; a significant portion of the overall time is spent seeking the target per-iteration quality difference (the convergence threshold is 10  X  4 in this case).
The figure also shows the quality-time execution traces for the multiresolution approach, using the deepest three levels of the authors hierarchy. In the figure there are three different multiresolution plots: The first thing to note is that there are more iterations when using the multiresolution approach. The algorithm performs a separate sequence of iterations for each selected level of the hierarchy  X  in this case, the lowest three levels, but each Figure 7: Impact of the tensor size (dense matrices, r =20 , only author hierarchy, 10  X  4 threshold; also for multi-resolution only the deepest three levels used; threshold is set to 10  X  4 for all resolutions) iteration is faster. Also, importantly, in deeper levels (i.e., higher resolutions, with costlier individual steps) much fewer iterations are necessary to reach the convergence threshold.
It is interesting to note that, convergence thresholds at the intermediary resolutions have significant impacts on the shape of the execution trace of the multi-resolution ap-proach. In particular, requiring that the same threshold 10  X  4 as the high-resolution step at each level of the tree causes the multi-resolution approach to spend (unnecessar-ily) more time for the lower-resolution decompositions. Re-laxing the threshold down to 10  X  3 and 10  X  2 significantly re-duces the time spent for decomposing the lower-resolutions. In fact, in the mr1e-2 case, the most relaxed (10  X  2 )inter-mediary convergence thresholds provide the best quality-time behavior and the multiresolution approach provides as good a final result quality as the standard ALS, but stops much quicker (with 10  X  4 at the leaf level as the standard ALS). This indicates that multi-resolution approach is able to leverage available external uni-model hierarchy to speed up the decomposition process, without negatively affecting the final quality. It also hints to the fact that neither the hierarchy, nor the intermediary decompositions step need to be perfect to have good decomposition performance.
In the rest of this section, we study the performance of the proposed scheme under different problem and system pa-rameters to assess whether the trends observed in the above sample scenario extends to different situations.
We first consider decomposition of tensor matrices repre-sented in dense form.
 Tensor size. Figure 7 presents comparisons of the mul-tiresolution algorithm with standard ALS for different ten-sor sizes. In these experiments, the tolerance is set to 10 in ALS as well as for multiresolution scheme (at intermedi-ate levels as well as for the highest resolution). As the figure shows, the multiresolution approach provides gains in exe-cution time for all sizes and the gains are especially larger for larger tensors. The figure also shows that the final qual-ity of the multiresolution scheme matches the quality of the more traditional ALS scheme for all tensor sizes. Final convergence threshold (i.e., tolerance). In the Figure 8: Impact of the final tolerance; i.e., stop-ping condition (dense matrices, r =20 , only author hierarchy, dblp512 , deepest three levels of hierarchy) Figure 9: Impact of tolerance values in the inter-mediate levels of resolution (dense matrices, r =20 , only author hierarchy, dblp512 data, deepest three levels of hierarchy) previous experiment, the stopping condition (tolerance) was set to 10  X  4 for ALS as well as for the multiresolution scheme. Figure 8 reports the results for the scenarios where the over-all convergence thresholds range from 10  X  2 to 10  X  4 .As the figure shows, the gain in execution time is especially pronounced when the stopping condition is tighter; this is because ALS needs to spend a significant amount of time slowly approaching the convergence condition whereas the multiresolution approach converges faster.

It is important to note that in the traditional ALS, quality drops significantly (from 0 . 119 to 0 . 108) when the threshold is relaxed, whereas it is much more stable if the proposed multiresolution scheme is used (from 0 . 117 to 0 . 115). Convergence conditions (i.e., tolerance) for interme-diate levels. Figure 9 depicts the impact of using different convergence tolerances at intermediate levels of the multires-olution scheme (this figure provides a summary of the traces shown in Figure 6). Here, for multiresolution approaches, the final tolerance value is set to 10  X  4 , whereas the toler-ance values at the intermediate levels are varied. We observe that the fastest execution is obtained when the thresholds at the intermediate resolutions are neither too tight, nor too lax. In terms of quality, however, as we have also seen in Figure 10: Impact of the number of resolution lev-els (dense matrices, r =20 , only author hierarchy, dblp512 data, tolerance 10  X  4 at all levels) Figure 11: Impact of the rank, r , of the decomposi-tion (dense matrices, r =5 , 10 , 20 , 30 , 50 , only author hierarchy, dblp512 data, tolerance 10  X  4 at all levels, deepest three levels of hierarchy) Figure 6, a relatively more lax threshold helps the multires-olution scheme avoid getting stuck at local optima. Number of resolution levels. Figure 10 shows the impact of the number resolutions levels used in the multiresolution approach. As can be seen in the figure, there is a trade-off and the largest gains in execution time are obtained when the process is started at mid-height of the hierarchy: starting at levels closer to root (e.g., 2) causes the multiresolution algorithm to spend time searching for decompositions that are too imprecise to help effectively bootstrap lower levels; starting at levels too close to the leaves (e.g., 8), on the other hand, prevents the multiresolution scheme to properly leverage the available metadata. We note that quality-wise all cases perform similarly.
 Target rank of the decomposition. Figure 11 shows that the proposed multiresolution approach consistently outper-forms the traditional ALS based decomposition for all tar-get decomposition ranks, even when the tolerance is set to 10  X  4 at all resolutions. The time gain ranges from  X  20% to  X  30%. The multiresolution approach also matches the quality at all ranks.
 Number of modes with available hierarchies. Finally, Figure 12 shows the impact of using more than one hierarchy Figure 12: Impact of the number of modes with hi-erarchies (dense matrices, r =20 , dblp512 data, tol-erance 10  X  4 at all levels, deepest three levels of hi-erarchy). In mr h1 only the hierarchy on authors is; mr h2 also considers the keywords hierarchy and mr h3 considers all three hierarchies.
 Table 3: Tolerance values used in experiments on sparse tensors to support decomposition. As can be seen here, the more hierarchies are used, the faster the overall decomposition becomes. Moreover, the decomposition quality is minimally affected from the number of considered hierarchies.
So far, we have reported experiment results on tensor rep-resented in dense form. However, in many data sets of inter-est (such as social media graphs) tensors can be sparse and these can leverage tensor decomposition tools specifically de-signed to work for tensors with sparse encodings. Therefore, in this section, we also evaluate the multi-resolution ap-proach on data sets with sparse representations as described in Table 1. In these experiments, we set the target rank r to 20 and the tolerance value for high resolution decomposition is set to 10  X  4 . For all data sets, we have considered the deep-est three levels of the hierarchy and varied the thresholds for the different levels as reported in Table 3.
 Bad news: As can be seen in Figure 13, when the tensor is sparse enough to rely on sparse tensor representations, the multiresolution approach does not necessarily provide time gains. This is because, when the tensors are represented in sparse form, decomposition cost tends to be a function of the number of non-zero entries [2]. Unfortunately, as reported in Table 4, reductions in tensor resolutions do not neces-sarily imply significant reductions in the number of tensor non-zero values and, therefore, time gains in the high res-olution level do not always amortize the overhead incurred by at the lower resolutions. Thus, time gains are observed only when relatively lax thresholds (10  X  2 ) are used at inter-mediate resolutions and, even then, gains are modest. Good news: While this initially looks disappointing, we remember from the earlier experiments that relaxing the pre-cision at the lower resolutions by using a more relaxed con-vergence threshold could provide big boosts in execution time, Figure 13: Impact of the tensor size (sparse matri-ces, r =20 , only author hierarchy, 10  X  4 threshold; deepest three levels used) Table 4: Number of non-zero entries at different levels of resolution Table 5: Number of non-zero entries, with  X  -cutoff, at different levels of resolution without negatively affecting the final quality . This means that we might be able to eliminate the bottleneck by relax-ing the number of non-zero entries considered in the lower resolution steps. To achieve this effect we introduced cut-offs,  X  , for non-leaf levels and eliminated all entries smaller than  X  from the tensors, creating less precise approximations of the original tensor at low resolutions. Table 5 shows the parameters used in the experiments. Note that we have ex-perimented with very significant reductions in the number of non-zero entries. Figure 14 shows the performance results for these  X  -cutoffs: as expected the more non-zero entries are removed in the low resolution steps, the faster becomes the decomposition, providing up to  X  40% gains in execution Figure 14: Impact of the tensor size with  X  -cutoff elimination of non-zero entries (sparse tensors, r =20 , only author hierarchy, 10  X  4 toler-ance; deepest three levels used). See Table 5 for details of  X  -cutoff values. time. Most interestingly (and happily), the  X  -cutoff based non-zero removal has close to zero impact on the quality of the final decomposition, rendering the proposed multireso-lution approach very applicable for sparse tensors as well.
In this paper, we presented a metadata driven tensor de-composition approach which leverages uni-modal cluster-ing hierarchies available a priori to significantly boost up the execution time of iterative alternating squares based PARAFAC decomposition. The experiment results showed that the time gains obtained by using external metadata do not come with undesirable reductions in the decomposition quality, even when the available metadata is imperfect and not directly representative of the tensor data.
