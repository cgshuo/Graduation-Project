 We build a probabilistic mo del to iden tify implicit local in-ten t queries, and leverage user's physical location to impro ve Web searc h results for these queries. Evaluation on commer-cial searc h engine sho ws signi can t impro vemen t on searc h relev ance and user exp erience.
 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al Algorithms, Exp erimen tation query log analysis, personalized searc h
There is a huge amoun t of searc hes on the Web has local inten t, meaning that they are searc hing for things in a par-ticular area, like restauran t, job listings, shopping cen ter, etc. Some queries have explicit location information in the query like \ pizza hut palo alto ", while man y others do not but still exp ect searc h engines to return localized searc h results, like \ laundry servic e ". This kind of queries is considered as implicit local inten t queries. For suc h queries, users exp ect personalized searc h results [2] that are customized to their locations. Thus, iden tifying implicit local inten t queries and nding out the location information for the users are partic-ularly useful to impro ve user searc h exp erience.
Yi et al. [3] uses language mo deling approac h to iden-tify implicit local inten t queries. In our work we also use language mo deling approac h, however instead of building a language mo del for eac h city, we only build one single lan-guage mo del for all locations to avoid sparsit y. We then ob-tain user's location directly from IP address mapping. What further mak es our work unique is that we integrate the iden-ti ed implicit location information into ranking directly to impro ve searc h relev ance and pro ve the practical impact of this work. As far as we kno w, there is little work has been published in this fron t.

Let Q denotes the general query set. A conditional ran-dom eld (CRF) based named entity tagger is used to tag all the queries in Q . We select all queries that con tain a location from Q to form a new query set Q L . We then re-move all location comp onen ts from queries in Q L and form an arti cial query set Q LC . We de ne the probabilit y that a query q has implicit local inten t by where P Q LC ( q ) and P Q ( q ) can be estimated using n-gram language mo del from corpus Q LC and Q resp ectiv ely.
Equation (1) however does not alw ays give an accurate estimation on P (implicit local inten t j q ) since locations are often used as constrain ts for queries that do not have local inten t. For example, query \ John San Jose myp ace " implies the user is looking for some one named John in San Jose from mysp ace . A popular domain name \ mysp ace " is a strong indicator for suc h case. We create a feature called highest domain rank for eac h query where s is a substring of query q and R d s is the rank of do-main s , whic h is rank ed by the accum ulated num ber of clic ks on the documen ts that belong to domain s . The lower value R ( s ) is, the more popular domain s is. In implemen tation, we only keep trac king top 1000 domains for simplicit y.
A implicit local query may not con tain general local inten t whic h should return same results even when queries are from di eren t locations. For example, \ Google headquarter oc e " con tains implicit local query but the inten t is restricted to a speci c location as there is only one Google headquarter oce. To capture the general local inten t, the entrop y of the conditional probabilit y distribution P ( l j q ), where l is a speci c location asso ciated with query q that is iden ti ed from user IP address (describ ed later). To calculate P ( l j q ), we rst normalize set Q L to set Q L suc h as all locations are disam biguated and canonicalized. For example, \ CA " is canonicalized to \ California ", \ la " is canonicalized to \ Los Angeles " or \ Louisiana ", \ New York " is canonicalized to \ New York City " or \ New York State " based on the con text. The conditional probabilit y distribution P ( l j q ) can be then esti-mated through n-gram language mo del estimated in corpus Q where P Q q co-o ccur estimated in corpus Q L . The entrop y E ( q ) is de ned as If E ( q ) is high, the query q is more likely to be asso ciated with man y locations with similar possibilities; otherwise, q is biased towards certain locations.

Gaussian kernel supp ort vector mac hine is used as the classi er. We rst train a weak classi er with 5000 editori-ally lab eled random query set. For eac h query , we generate features based on (1) (2) and (4). The resulting weak classi-er based on the 5000 training data is used to lab el 100 ; 000 queries. Queries that lie between the margin (with non-zero values are selected for the next batc h of editorial test. The classi er is then retrained with com bined lab eled data.
After a query is classi ed as having implicit general local inten t, we use user's IP address to obtain the city name and zip code where the query is sen t from. For ambiguous city names that exist in multiple states (suc h as Oakland ) or that have di eren t meanings (suc h as Mountain View ), we add use user location's state name for disam biguation.
To personalize searc h results given user's location, one intuitiv e way is to expand the original query by user's loca-tion. For example, if the original query is \ Italian restau-rants " and the user's location has been determined to be \ San Francisc o ", a new searc h query may be formed as \ Ital-ian restaur ants San Francisc o " by app ending the determined location to the original searc h query . The new searc h query is then issued to the searc h engine to obtain the searc h re-sult for the user. Because the location \ San Francisc o " is now included in the new searc h query , based on whic h the searc h result is iden ti ed, the searc h engine is more likely to nd Italian restauran ts located in San Francisco. This approac h, however, su ers from two sources of errors: (1) the local inten t migh t not be the only inten t of the query or even may not exist due to limited precision of the general implicit local inten t classi er; (2) the user's location may be determined wrongly .
As a more conserv ativ e approac h, documen t re-ranking is prop osed to leverage user's location. We rst extract top K documen ts with the original query and then adjust the searc h results by increasing the ranks of those documen ts that matc h the user location. Consequen tly, the Web docu-men ts that matc h the user's location are rank ed higher than those documen ts that do not matc h the user's location. We also di eren tiate and weigh t user location matc hes for di er-ent sections in top documen ts. We re-rank those documen t based on their curren t rank score and text matc hing features that tell if user location and its variation exists in certain documen t sections. The re-rank score s r ( q; d; l ) for query q , Web documen t d and user location l can be expressed by where s ( q; d ) is the original rank score before personaliza-tion, I i ( d; l ) is an indicator function that tells if user location l exist in documen t section i and w i is the weigh ting para-meter for documen t section i . Sup ervised learning is used to estimate the parameter w i to maximize the relev ance after personalization.
We evaluate our personalization system by both editorial relev ance test based on Discoun ted Cum ulativ e Gain (DCG) and online buc ket test for user exp erience evaluation based on clic k-through rate (CTR), two commonly used metrics to evaluate searc h engine relev ance and user exp erience [1].
We apply both query expansion and documen t re-ranking to the queries that are classi ed as general implicit local queries. We sample 1300 random personalized queries and submit the top 5 Web searc h results for eac h approac hes to-gether with test queries and the user location for eac h query to trained editors for relev ance judgmen t. If a test query does not con tain general implicit local inten t, the user loca-tion information will be ignored in the judgmen t; otherwise, the user location will be considered in the relev ance judg-men t. The baseline is one of the top commercial searc h en-gines. We can see from Table 1 personalized Web searc h re-sults dramatically impro ved searc h relev ance. In online test, we also observ e that user exp erience measured by CTR has been impro ved signi can tly by 1.4% with p value &lt; 0 : 05 for a ected queries.
 Table 1: Relev ance Impact with Personalization
We successfully impro ved relev ance the user exp erience over one of the top commercial searc h engine by personaliz-ing searc h results using user's physical location. The curren t mo del covers 2 : 6% searc h trac. We plan to increase the coverage substan tially by incorp orating more salien t features and better language mo dels. The curren t re-ranking algo-rithm is rather simple and can be substan tially impro ved by join tly considering location matc hing features and other ranking features together in the phase of re-rank. [1] B. Carterette and R. Jones. Evaluating searc h engines [2] A. Micarelli, F. Gasparetti, F. Sciarrone, and S. Gauc h. [3] X. Yi, H. Ragha van, and C. Leggetter. Disco vering
