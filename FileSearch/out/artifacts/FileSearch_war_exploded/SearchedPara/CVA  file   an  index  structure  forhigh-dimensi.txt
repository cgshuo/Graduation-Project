 domains such as image retrieval, multimedia systems, spatial database, document database, WWW search engines and data mining, similarity search finds its appli-any multimedia product would welcome a system of copy detection system to guard their copyright and so on.
 dimensional vectors. For example, in image retrieval, images are represented as fea-sented as vectors in classic information-retrieval models (Baeza-Yates and Ribeiro-
Neto 1999); a series of stock prices or a DNA sequence is naturally a vector as well (Faloutsos et al. 1994b; An et al. 2003). A vector is represented as a point in a mul-tidimensional space, which u sually has high dimensiona lity. A common approach to a similarity search in various domains is then the search of k nearest neighbors ( k-NN search ) in a high-dimensional space. Howeve r, the effectiveness of traditional of the curse of dimensionality .
 The curse of dimensionality causes many phenomena that affect search efficiency.
An example is that the volume of a hypersphere inside a hypercube converges to zero as the dimensionality grows. This results in neighbor searches obtaining no answer even when the diameter of the search area is assumed to take the value of the length of the cube. In this paper, we explain the cu rse by concentrating on its effect on the efficiency of the spatial index mechanism.

Generally, an index is constructed by partitioning a data space. Each partition has its lower bound and upper bound, which means that objects having the indexed second (refinement) phase, the candidate subset of the original dataset corresponding to the candidate partitions is refined to find accurate answers. Objects in the candidate meanwhile, there are more indexing points or larger index files.

This mechanism, based on the assumption that the number of objects is (usu-comes very high. For example, consider a dataset of one million vectors, where at least 2 10 , namely one million partitions, and thus the gain of excluding irrelevant (Beckmann et al. 1990; Berchtold et al. 1996; Katayama and Satoh 1997), where tree nodes are accessed in a random way that i s very ineffective. When dimension-ality increases, the data space cannot be divided clearly into tree nodes. As a result, subspaces corresponding to most nodes ove rlap each other and the pruning power is lost.

Some recent researchers conclude that it is inherently impossible to construct ef-ficient indexing mechanisms based on the assumption that data vectors are uniformly distributed and dimensions are independent (Beyer et al. 1999). For such data, spatial indices become less effective than linear scanning (Berchtold et al. 1996; Berchtold et al. 1997; Beyer et al. 1999; Weber et al. 1998). The VA file (Weber et al. 1998) tioning, the VA file constructs the index fi le by compressing each data vector. Each the index file and from fewer similarity computations.

However, it is very unlikely that data vectors in real data sets are uniformly seen frequently in real data, such as linguistics (Baeza-Yates and Ribeiro-Neto 1999) and the Internet topology (Faloutsos and Faloutsos 1999).

Machine Learning Repository 1 . In Fig. 1, the histograms of coordinates are illustrated (the coordinates are normalized to the range [ 0 , 1 percentages of the number of coordinates with in intervals. It is found that the length a small percentage of dimensions have significant values in high dimensionality. For example, in 64-dimensional image color histogram feature vectors (Fig. 1(d)), about 78 the coordinates follows Zipf X  X  law (Zipf 1949), which is also the Pareto distribution, We have abstracted the f ollowing kind of data:
The notation 0 denotes that a coordinate is close to 0 (e.g. 0 tion X denotes that a coordinate is relatively greater than 0 (e.g. 0 space can be conceptualized as in Fig. 2. We assume a data vector v. x ,v. y ,v. z ,... in x , y , z ,... axis. Most of the data vectors are in one subspace consisting of several original axes. That is, v. x the high-dimensional space is sparse, a s many researchers have noted (Aggarwal its subspaces as shown in Fig. 2.

Observation 1 For many high-dimensional datasets, a large portion of coordinates has very small values in the scope ranging over all coordinates.
By exploiting the nonuniformity and the correlation property of real datasets, it is possible to avoid the curse of dimensi onality. In this paper, we propose a flexible dimensionality reduction in which the dimensionality is reduced in a datawise way.
The basic idea is simple. It reduces the coordinates for which values are smaller than a critical value. Therefore, each data vector is identified by the remaining coordinates a subspace, which may not be identical for each data vector. Moreover, the number can be seen that our technique has the advantage that the loss of information caused by the dimensionality reduction is reduced.
 The proposed technique is widely applicable to a class of indexing mechanisms. reduced for skewed data.
The FastMap technique (Faloutsos and Lin 1995) projects data vectors in a low-dimensional space. We proposed another t echnique (Shinohara et al. 2000) that makes
FastMap applicable to L 1 distance space. The Karhunen Loeve transform (KLT) correlation matrix for a given dataset to derive a new system of coordinates. These techniques reduce dimensionality globally . Unfortunately, the common subspace does not necessarily represent all data vectors well.

The projected clustering, called PROCLUS (Aggarwal et al. 1999), also exploits the nonuniformity and the correlation property of datasets. This clustering algorithm each subspace need not be identical. T hat is, dimensions are reduced locally for each other local dimensionality-reduction met hod, LDR (Chakrabarti and Mehrotra 2000), finds local correlations in the data and performs dimensionality reduction on the lo-no need to find clusters and locally common dimensions. The convex polyhedra composes a data space into a convex polyhedra. The dimensionality of each data vector is reduced according to which polyhedron it includes.
 scribedinmoredetail.
 VA fi l e
The VA file (Weber et al. 1998) is an index str ucture that accelerates the sequential scan by the use of data compression. The k -NN search process in the VA file scheme is also performed in two phases in which two files are accessed respectively. A bit-compressed, quantized version of the data vectors, also called a VA file, is accessed files are unsorted, but the order of the data vectors in the two files are identical. The
VA file is a binary file of N bit patterns, each of which has length b , corresponding in the following way, given the number of bits b d for quantizing the d th dimension in advance. 1. The d th coordinate value, v. x d , of a vector v is quantized by 2. As shown in Fig. 4(a), the bit pattern (v. a 1 ,v. a space ( D = 2). Then, given a query vector q , the distance between assumed to be
It must satisfy l i  X  L p ( q ,v)  X  u i . Therefore, l i and u upper bound of v with respect to q , respectively.

For a k -NN search, in the first phase, the quan tized data is loaded into the main Otherwise, it is inserted into a candidate set.

Intuitively, a better approximation causes tighter bounds from the query vector
The process is terminated when the lower bound exceeds the k th distance in the answer set. Because in the second phase, random accesses are more expensive than search, is important.
 VA file in KLT domain mensionality-reduction KLT. In KLT, the eigenvectors of the covariance matrix are computed and are sorted in decreasing eigenvalue order. The dataset is supposed to be well distributed over several eigenvectors corresponding to the largest eigenvalues.
Principal components of the original data vectors are preserved in these eigenvectors (dimensions). In other wor ds, the new coordinates of each vector are computed by projecting the vector to each of these princ ipal components (dimensions). For com-parison, we further combine the VA file with KLT by quantizing the new coordinate values on the principal dimensions created as mentioned above. Then k -NN is pro-cessed as follows. In the first phase, filtering is applied to the VA file as the original vector are transformed to principal dimensions before being approximated. After fil-tering, the exact distances between the query vector and candidates are calculated in the original domain in the second phase.

It is worth noting that KLT is not always feasible because of the impossibility of SVD for huge matrices. of index file. That is, if the number of bits b for quantizing is large, then the bound at the same time, and vice versa. As a solution to this tradeoff, we have attempted to decrease the size of the index files by using dimensionality reduction without losing the tightness of the bounds.
For a D -dimensional normalized dataset (the range of coordinate values is normal-the dimensions are divided into two groups. The first group contains dimensions of which coordinate values are larger than the critical value , and the other contains the remaining dimensions. Hencef orth, the former are called effective dimensions and the latter are called non-effective dimensions . For example, if parameter then dimensions of a data vector ( x 1 , x 2 , x 3 , x 4 , divided into two groups { x 2 , x 4 } and { x 1 , x 3 , dimension for a data vector might be a noneffective dimension for another data vector.

Assume that the critical value of coordinate is ,thenlet N be the number of data vectors and D be the number of dimensions. We use i  X  X  data vectors, and the dimensions are div ided into an effective dimension set X a noneffective dimension set X i . v i denotes an individual data vector and dimension x d . All notations are summarised in Table 1.
 entry of a data vector v is now given. 2. For each effective dimension, append th e approximation of this coordinate value mensions for each data vector, and the  X  X A -data X  fields preserve the approximation of the coordinate values of the effective dimensions. Although the header fields are overhead, the length for storing one data vector in a CVA file is much shorter than of the CVA file is much smaller than the VA file.
As an example of a CVA construction, assume that D b = 2, then a vector v = ( 0 . 1 , 0 . 3 , 0 . 6 , 0 . 2 dimensions, the header becomes 0110 by se tting the corresponding bits. The coordi-nate value of the second dimension 0.3 is quantized to 010 third coordinate value 0.6 is quantized to 2 ( = 0 entry for v is ( 0110 , 010 , 10 ) 2 , where commas are added for readability. Algorithm 1 shows how to create index files using the CVA file method formally.
In a D -dimensional dataset O , a data vector is denoted by presented with v i . x d and its approximation is r i .
Figure 5 illustrates some of the images. Assume that the data vector the third region of dimension x 2 , the region number is counted from 0 so we have r . x length of a region (shaded in Fig. 5) in the dimension x holds.
Let us consider a query q and a distance function L p ,forsome p . An approximation of v i determines a lower bound l i and an upper bound u i is simply the minimum distance from the query to a point in that cell. Analogously, the upper bound u i is the maximum distance to a point in that cell. Formally, l u are derived from all effective dimensional bounds l noneffective dimensional bounds l i . x d u i . x d ( x d
The lower and the upper bounds of effective dimensions, l can be computed as in the VA file. We assume a given query data point q falls into the region numbered r q . x d . The formulae for computing lower and upper bounds noneffective dimensions, because their approximations of r stored in the CVA file.

We consider a dimension x 2 in Fig. 8(A). Given the critical value, data vector v . x 2 is in the shaded area, and thus tive dimension with respect to data vector v 1 . On the other hand, x dimension for data vector v 2 because v 2 . x 2 is larger than the critical value.
Assume that data vector v i has a noneffective dimension x v . x 2  X  X  lower and upper bounds are the distances from q to the shaded portion.
Consequently, we should consider two cases according to dimension x of q (or q ) as shown in Fig. 9(B). In the case of q ( q . bounds can be calculated with Eq. (3), and so forth for the other case q the lower bound is zero.

The k -NN search algorithm with CVA file is implemented in two phases, namely phase, the approximations of data vectors are scanned linearly. The lower and upper bounds are calculated in accordance with Eq . (1). Because noneffective dimensions are reduced, their approximations have to be estimated in the way shown in Fig. 8.
Then the candidates are obtained by filtering out most data vectors. The second phase visits data vectors in the original data file based on the results of the first phase. The final answer set is obtained by checking the real distance between the original data guarantees that the current k vectors are really the answer to the k -NN search. This of their lower bounds of the distance to the query vector q . We describe the process formally in pseudo code in Algorithm 2.

In order to demonstrate that the CVA file method is an effective alternative to VA file we analyze the cost formally, then evaluate the performance on synthetic as well as real datasets. The VA file, the SR-tree and a simple sequential scan search structures were evaluated. The synthetic dataset is generated based on Zipf X  X  distribution of data vectors. We selected the color histogram provided online at the UCI KDD Archive web site 2 as the real dataset.
Three factors are considered in the analysis:  X 
Construction complexity  X 
Size of the CVA file  X  Efficiency in the k -NN search
As in Table 1, let N be the size of the D -dimensional dataset, and b number of bits of dimension x d of an index entry. Further, let b if the value is larger than the threshold . Compared with the VA file, the overhead is that we need to set the corresponding bit in the header ( X  X im.inf. X  in Fig. 4). On the other hand, one great advantage is that no computation is needed for coordinate values under . Consequently, the construction cost of the CVA file is also O
Apparently, the size of the VA file is S VA =  X  b  X  D  X  N signs  X  b bits to one coordinate and all D coordinates must be quantized. On the other hand, in the CVA file, the number of bits b d are assigned to d th dimension, but for average of b d be  X  b and let  X  m be the average number of effective dimensions, then the sizeoftheCVAfileis S CVA =  X  m  X   X  b  X  N + D  X  N (bits), where the second term D is the size of the header. To compare with the VA file, we assign to each dimension the same number of bits as in the VA file, namely difference between our CVA file and VA file,  X  = ( S CVA  X 
Because the size of the index file in the CVA file depends on the number of effective dimensions  X  m , which in turn depends on data distribution, it is impossible to give constant estimation. Nevertheless, to compare effectiveness with the VA file, we consider the following three extreme distributions:  X 
The worst distributions such that all dimensions are effective (no dimension re-duced), although this is almost impossible in the real world.  X 
Uniform distributions insisted on by the VA file in Weber et al. (1998) but rarely seen in the real world.  X  Usual cases where datasets follow Zip f X  X  law. These cases are always seen in of them have to be quantized, thus  X  m = D . This results in a positive Weber et al. (1998), then 1 /  X  b is not a significant increment.
Hence,  X  = 1 /  X  b  X  . As discussed in the following section, to keep the tightness of the bounds, we do not adopt the solution of assigning a large .So is our overhead against VA. Even so, because is positive, the overhead is smaller than that in the worst case.
 a coefficient of the Zipf X  X  distribution. Figure 10 shows the histogram of coordinates according to Zipf X  X  law. The x axis shows the value of coordinates, the vertical axis shows the number of coordinates. Noting that coordinates have been normalized to [ 0 of the effective dimensionality with respect to threshold is gions, let P = E  X  1. Then the expression can be calculated by
P 1, we have 1  X  P  X  ( z  X  1 )
Given  X  I d ,then E  X  2 and thus  X &lt; 1 / 2 + 1 /  X  b z &gt; 2. Consider the case that  X  b = 8, as suggested in Weber et al. (1998), and z =
I d ,wehave  X  = 1 / we can reduce the size to half compared with the VA file while keeping the tightness of bounds unchanged.

Last, we omitted the cases where each dimens ion follows differ ent distributions independently because there is an almost un limited possibility of co mbinations. Nev-to any of the possible combinations of distributions.

We should now compare the efficiency of the CVA file and the VA file. Besides distribution, the average number of effective dimensions m is also determined by which reflect differences on the I/O access i n the first and the second phase, respec-tively. When becomes smaller, then bounds are tighter, but the number of effective dimensions m becomes larger. In other words, more coordinates are needed to be the CVA file method is never worse than the VA file method, as described in Chen et al. (2002). This is the case when E = 0asinTable3.

Because the number of page accesses in the filtering phase is in proportion to reduces the number of page accesses in thi s phase. This compensates for excess page accesses in the refinement phase because bounds are less tight than that of the
VA file. Because of the high dimensionality, bounds become loose in an exponential order. When is given as E times of the length of a region I having to be counted becomes E D . However, by no means do the candidates passed q = ( q . x dimension, then the number of candidates can be estimated by
On average, this is about N  X  E D (
The CVA file method distinguishes noneffective dimensions by checking the corres-ponding bits in a header. A little more CPU tim e is needed in this checking process.
However, the time required to compare coordinates between query vector and ap-proximations is decreased due to omitted coordinates. As a result, there seems little significant difference in CPU times between the CVA file and the VA file, as shown in Fig. 11. In the following experiments, we evaluated the performance concentrated on the number of page accesses. We evaluated the performance on the real dataset of color images available from the
Corel Color Database (http://corel.digitalriver.com/) and color histograms provided online at the UCI KDD Archive web site (http://kdd. ics.uci.edu/ databases/Corel-
Features). The size of the dataset is 70,000. 4 , 8 , 16 tors are extracted from the dataset. The distance function is based on the L (Euclidean distance) and the page size of the index files is 8 KB. filter out candidates in the first phase in which data are accessed in a sequential way.
A factor of 5, i.e. the random access is five times slower than the sequential access, about this factor. However, in order to compare the VA file and KLT (Fukunaga 1990) approaches fairly, we have evaluated the performance considering two extreme situations.
First, we controlled the CVA file such that it had the same number of page accesses in the refinement phase (as shown in Table 4) for all three index structures (CVA file, VA file and KLT) by tuning the parameter and compared the numbers of sequentially accessed pages in the first phase. As shown in Fig. 12, the CVA file needs a bit more than 200 page accesses, while the KLT needs more than 350 and the VA file needs even more. Meanwhile, Fig. 13 shows the number of effective dimensions corresponding to the previous figure.

Second, as shown in Table 4, the number of page accesses in the refinement phase is small (26 or fewer page accesses for 10-NN query). This suggests that the comparison of the total number of page acce sses within the two phases is meaningful, Figure 14 shows the total number of page accesses among SR-tree, VA file and
CVA file in 10-NN query across a range of di mensionality. For each dimensionality, the number of bits assigned to each dimension for the VA file is chosen as 8 bits for 24 or less dimensionality and 7 bits for 32 or more dimensionality. These decisions them the best approximation effect.
 random access in the refinement phase. Considering that bounds in the CVA file are never tighter than those in the VA file, this factor means that we pay a double penalty in evaluation. Nevertheless, we can observe that the CVA file always outperforms
VA files, and it outperforms the SR-tree and KLT for high-dimensional datasets. The margin increases as dimensionality increases. In the case of 64 dimensions, the CVA file cuts down 2 / 3 of the page accesses from the SR tree (Katayama and Satoh 1997), 1 / 2 of the pages accesses from the VA file and 2 / 3 of pages accesses from the VA file in the KLT domain.
As the dimensionality increases, the coordinates distribution has a tendency of Zipf X  X  power as seen in Sect. 1. To create a dataset ba sed on Zipf X  X  distribution, we use ran-dom data for the coordinates, which range over n partitions: [ p where x is the lower bound of the partition. The greater z becomes, the greater the number of coordinates having a value of nearly 0 becomes. Figure 15 shows the coordinates X  percentage when z = 2 . 5.
 [ 0 mensionality becomes higher. Thus, the length of the partition is assumed as follows:
The size of the synthetic data is 100,000 for datasets of 8
We assume that the CVA file, the VA file in KLT domain, and the VA file have the same number of page accesses in the second phase. The number of page accesses
CVA file and the KLT domain are shown in Fig. 17. dimensional real data. That is, there are few dimensions for which coordinate values are large, and most coordinate values are nearly zero. We proposed a datawise dimensionality-reduction technique that is able to significantly reduce information loss. By applying this technique to the VA file, a new index structure called a CVA file is developed. The performance evaluation shows significant improvements to the
I/O cost of queries over the original VA file and the VA file in the KLT domain for several real datasets and synthetic datasets.

