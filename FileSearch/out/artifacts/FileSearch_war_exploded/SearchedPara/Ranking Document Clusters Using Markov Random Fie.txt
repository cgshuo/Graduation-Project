 An important challenge in cluster-based document retrieval is ranking document clusters by their relevance to the query. We present a novel cluster ranking approach that utilizes Markov Random Fields (MRFs). MRFs enable the integra-tion of various types of cluster-relevance evidence; e.g., the query-similarity values of the cluster X  X  documents and query-independent measures of the cluster. We use our method to re-rank an initially retrieved document list by ranking clus-ters that are created from the documents most highly ranked in the list. The resultant retrieval effectiveness is substan-tially better than that of the initial list for several lists that are produced by effective retrieval methods. Furthermore, our cluster ranking approach significantly outperforms state-of-the-art cluster ranking methods. We also show that our method can be used to improve the performance of (state-of-the-art) results-diversification methods.

The cluster hypothesis [33] gave rise to a large body of work on using query-specific document clusters [35] for im-proving retrieval effectiveness. These clusters are created from documents that are the most highly ranked by an ini-tial search performed in response to the query.

For many queries there are query-specific clusters that contain a very high percentage of relevant documents [8, 32, 25, 14]. Furthermore, positioning the constituent doc-uments of these clusters at the top of the result list yields highly effective retrieval performance; specifically, much bet-ter than that of state-of-the art retrieval methods that rank documents directly [8, 32, 25, 14, 10].

As a result of these findings, there has been much work on ranking query-specific clusters by their presumed relevance to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous approaches to cluster ranking compare a representation of the cluster with that of the query. A few methods integrate additional types of information such as inter-cluster and cluster-document similarities [18, 14, 15]. However, there are no reports of fundamental cluster ranking frameworks that enable to effectively integrate various information types that might attest to the relevance of a cluster to a query. We present a novel cluster ranking approach that uses Markov Random Fields. The approach is based on integrat-ing various types of cluster-relevance evidence in a princi-pled manner. These include the query-similarity values of the cluster X  X  documents, inter-document similarities within the cluster, and measures of query-independent properties of the cluster, or more precisely, of its documents. A large array of experiments conducted with a variety of TREC datasets demonstrates the high effectiveness of using our cluster ranking method to re-rank an initially retrieved document list. The resultant retrieval performance is sub-stantially better than that of the initial ranking for several effective rankings. Furthermore, our method significantly outperforms state-of-the-art cluster ranking methods. Al-though the method ranks clusters of similar documents, we show that using it to induce document ranking can help to substantially improve the effectiveness of (state-of-the-art) retrieval methods that diversify search results.
Suppose that some search algorithm was employed over a corpus of documents in response to a query. Let D init be the list of the initially highest ranked documents. Our goal is to re-rank D init so as to improve retrieval effectiveness.
To that end, we employ a standard cluster-based retrieval paradigm [34, 24, 18, 26, 15]. We first apply some cluster-ing method upon the documents in D init ; C l ( D init ) is the set of resultant clusters. Then, the clusters in C l ( D init ranked by their presumed relevance to the query. Finally, the clusters X  ranking is transformed to a ranking of the doc-uments in D init by replacing each cluster with its constituent documents and omitting repeats in case the clusters overlap. Documents in a cluster are ordered by their query similarity.
The motivation for employing the cluster-based approach just described follows the cluster hypothesis [33]. That is, letting similar documents provide relevance status support to each other by the virtue of being members of the same clusters. The challenge that we address here is devising a (novel) cluster ranking method  X  i.e., we tackle the second step of the cluster-based retrieval paradigm. (for the sake of the example) nodes ( d 1 , d 2 , and d 3 contains the query and a single document from C ; (ii) l contains all nodes in G ; and, (iii) l C contains only the documents in C .

Formally, let C and Q denote random variables that take as values document clusters and queries respectively. The cluster ranking task amounts to estimating the probability that a cluster is relevant to a query, p ( C | Q ): The rank equivalence holds as clusters are ranked with re-spect to a fixed query.
 To estimate p ( C,Q ), we use Markov Random Fields (MRFs). As we discuss below, MRFs are a convenient framework for integrating various types of cluster-relevance evidence.
An MRF is defined over a graph G . Nodes represent random variables and edges represent dependencies between these variables. Two nodes that are not connected with an edge correspond to random variables that are independent of each other given all other random variables. The set of nodes in the graph we construct is composed of a node representing the query and nodes representing the cluster X  X  constituent documents. The joint probability over G  X  X  nodes, p ( C,Q ), can be expressed as follows: L ( G ) is the set of cliques in G and l is a clique;  X  l is a potential (i.e., positive function) defined over l ; Z = P
C,Q Q l  X  L ( G )  X  l ( l ) is the normalization factor that serves to ensure that p ( C,Q ) is a probability distribution. The normalizer need not be computed here as we rank clusters with respect to a fixed query.

A common instantiation of potential functions is [28]: where f l ( l ) is a feature function defined over the clique l and  X  l is the weight associated with this function. Accord-ingly, omitting the normalizer from Equation 2, applying the rank-preserving log transformation, and substituting the po-tentials with the corresponding feature functions results in our ClustMRF cluster ranking method: This is a generic linear (in feature functions) cluster ranking function that depends on the graph G . To instantiate a spe-cific ranking method, we need to (i) determine G  X  X  structure, specifically, its clique set L ( G ); and, (ii) associate feature functions with the cliques. We next address these two tasks.
We consider three types of cliques in the graph G . These are depicted in Figure 1. In what follows we write d  X  C to indicate that document d is a member of cluster C .
The first clique (type), l QD , contains the query and a sin-gle document in the cluster. This clique serves for making inferences based on the query similarities of the cluster X  X  constituent documents when considered independently . The second clique, l QC , contains all nodes of the graph; that is, the query Q and all C  X  X  constituent documents. This clique is used for inducing information from the relations between the query-similarity values of the cluster X  X  constituent docu-ments. The third clique, l C , contains only the cluster X  X  con-stituent documents. It is used to induce information based on query-independent properties of the cluster X  X  documents.
In what follows we describe the feature functions defined over the cliques. In some cases a few feature functions are defined for the same clique, and these are used in the summa-tion in Equation 3. Note that the sum of feature functions is also a feature function. The weights associated with the feature functions are set using a train set of queries. (Details are provided in Section 4.1.) The l QD clique. High query similarity exhibited by C  X  X  constituent documents can potentially imply to C  X  X  rele-vance [26]. Accordingly, let d (  X  C ) be the document in l w here | C | is the number of documents in C , and sim (  X  ,  X  ) is some inter-text similarity measure, details of which are pro-vided in Section 4.1. Using this feature function in Equation 3 for all the l QD cliques of G amounts to using the geometric mean of the query-similarity values of C  X  X  constituent docu-ments. All feature functions that we consider use logs so as to have a conjunction semantics for the integration of their assigned values when using Equation 3. 1 The l QC clique. Using the l QD clique from above results in considering the query-similarity values of the cluster X  X  documents independently of each other. In contrast, the l
QC clique provides grounds for utilizing the relations be-tween these similarity values. Specifically, we use the log
B efore applying the log function we employ add- X  (= 10  X  10 ) smoothing. of the minimal, maximal, and standard deviation 2 o f the { sim ( Q,d ) } d  X  C values as feature functions for l QC min-qsim , max-qsim , and stdv-qsim , respectively. The l C clique. Heretofore, the l QD and l QC cliques served for inducing information from the query similarity values of C  X  X  documents. We now consider query-independent proper-ties of C that can potentially attest to its relevance. Doing so amounts to defining feature functions over the l C clique that contains C  X  X  documents but not the query. All the feature functions that we define for l C are constructed as follows. We first define a query-independent document measure, P , and apply it to document d (  X  C ) yielding the value P ( d ). Then, we use log A ( {P ( d ) } d  X  C ) where A is an aggregator function: minimum, maximum, and geometric mean. The resultant feature functions are referred to as min-P , max-P , and geo-P , respectively. We next describe the document measures that serve as the basis for the feature functions.
The cluster hypothesis [33] implies that relevant docu-ments should be similar to each other. Accordingly, we mea-sure for document d in C its similarity with all documents
The next few query-independent document measures are based on the following premise. The higher the breadth of content in a document, the higher the probability it is rel-evant to some query. Thus, a cluster containing documents with broad content should be assigned with relatively high probability of being relevant to some query.

High entropy of the term distribution in a document is a potential indicator for content breadth [17, 3]. This is be-cause the distribution is  X  X pread X  over many terms rather than focused over a few ones. Accordingly, we define P and p ( w | d ) is the probability assigned to w by an unsmoothed unigram language model (i.e., maximum likelihood estimate) induced from d .

Inspired by work on Web spam classification [9], we use the inverse compression ratio of document d , P icompress as an additional measure. (Gzip is used for compression.) High compression ratio presumably attests to reduced con-tent breadth [9].

Two additional content-breadth measures that were pro-posed in work on Web retrieval [3] are the ratio between the number of stopwords and non-stopwords in the document, P sw1 ( d ); and, the fraction of stopwords in a stopword list that appear in the document, P sw2 ( d ). We use INQUERY X  X  stopword list [2]. A document containing many stopwords is presumably of richer language (and hence content) than a document that does not contain many of these; e.g., a document containing a table composed only of keywords [3]. For some of the Web collections used for evaluation in Section 4, we also use the PageRank score [4] of the docu-ment, P pr ( d ), and the confidence level that the document is not spam, P spam ( d ). The details of the spam classifier are provided in Section 4.1.

We note that using the feature functions that result from applying the geometric mean aggregator upon the query-independent document measures just described, except for
I t was recently argued that high variance of the query-similarity values of the cluster X  X  documents might be an in-dicator for the cluster X  X  relevance, as it presumably attests to a low level of  X  X uery drift X  [19]. dsim, could have been described in an alternative way. That taining a single document. Then, using these feature func-tions in Equation 3 amounts to using the geometric mean. 3
The work most related to ours is that on devising cluster ranking methods. The standard approach is based on mea-suring the similarity between a cluster representation and that of the query [7, 34, 35, 16, 24, 25, 26]. Specifically, a geometric-mean-based cluster representation was shown to be highly effective [26, 30, 15]. Indeed, ranking clusters by the geometric mean of the query-similarity values of their constituent documents is a state-of-the-art cluster ranking approach [15]. This approach rose as an integration of fea-ture functions used in ClustMRF, and is shown in Section 4 to substantially underperform ClustMRF.

Clusters were also ranked by the highest query similar-ity exhibited by their constituent documents [22, 31] and by the variance of these similarities [25, 19]. ClustMRF incor-porates these methods as feature functions and is shown to outperform each.

Some cluster ranking methods use inter-cluster and cluster-document similarities [14, 15]. While ClustMRF does not utilize such similarities, it is shown to substantially outper-form one such state-of-the-art method [15].

A different use of clusters in past work on cluster-based retrieval is for  X  X moothing X  (enriching) the representation of documents [20, 16, 24, 13]. ClustMRF is shown to substan-tially outperform one such state-of-the-art method [13]. To the best of our knowledge, our work is first to use MRFs for cluster ranking. In the context of retrieval tasks, MRFs were first introduced for ranking documents directly [28]. We show that using ClustMRF to produce document ranking substantially outperforms this retrieval approach; and, that which augments the standard MRF retrieval model with query-independent document measures [3]. MRFs were also used, for example, for query expansion, passage-based document retrieval, and weighted concept expansion [27].
T he TREC datasets specified in Table 1 were used for experiments. AP and ROBUST are small collections, com-posed mostly of news articles. WT10G and GOV2 are Web
S imilarly, we could have used the geometric mean of the query-similarity values of the cluster constituent documents as a feature function defined over the l QC clique rather than constructing it using the l QD cliques as we did above. collections; the latter is a crawl of the .gov domain. For t he ClueWeb Web collection both the English part of Cat-egory A (ClueA) and the Category B subset (ClueB) were used. ClueAF and ClueBF are two additional experimental settings created from ClueWeb following previous work [6]. Specifically, documents assigned by Waterloo X  X  spam classi-fier [6] with a score below 70 and 50 for ClueA and ClueB, respectively, were filtered out from the initial corpus rank-ing described below. The score indicates the percentage of all documents in ClueWeb Category A that are presumably  X  X pammier X  than the document at hand. The ranking of the residual corpus was used to create the document list upon which the various methods operate. Waterloo X  X  spam score is also used for the P spam (  X  ) measure that was described in Section 2.1. The P spam (  X  ) and P pr (  X  ) (PageRank score) mea-sures are used only for the ClueWeb-based settings as these information types are not available for the other settings.
The titles of TREC topics served for queries. All data was stemmed using the Krovetz stemmer. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.
 2, we use the ClustMRF cluster ranking method to re-rank an initially retrieved document list D init . Recall that af-ter ClustMRF ranks the clusters created from D init , these are  X  X eplaced X  by their constituent documents while omit-ting repeats. Documents within a cluster are ranked by their query similarity, the measure of which is detailed be-low. This cluster-based re-ranking approach is employed by all the reference comparison methods that we use and that rely on cluster ranking. Furthermore, ClustMRF and all reference comparison approaches re-rank a list D init is composed of the 50 documents that are the most highly ranked by some retrieval method specified below. D init is rel-atively short following recommendations in previous work on cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we study the effect of varying the list size on the performance of ClustMRF and the reference comparisons.

We let all methods re-rank three different initial lists D The first, denoted MRF , is used unless otherwise specified. This list contains the documents in the corpus that are the most highly ranked in response to the query when using the state-of-the-art Markov Random Field approach with the sequential dependence model (SDM) [28]. The free param-eters that control the use of term proximity information in SDM,  X  T ,  X  O , and  X  U , are set to 0 . 85, 0 . 1, and 0 . 05, respec-tively, following previous recommendations [28]. We also use MRF X  X  SDM with its free parameters set using cross valida-tion as one of the re-ranking reference comparisons. (De-tails provided below.) All methods operating on the MRF initial list use the exponent of the document score assigned by SDM  X  which is a rank-equivalent estimate to that of log p ( Q,d )  X  as sim MRF ( Q,d ), the document-query simi-larity measure. This measure was used to induce the initial ranking using which D init was created. More generally, for a fair performance comparison we maintain in all the experi-ments the invariant that the scoring function used to create an initially retrieved list is rank equivalent to the document-query similarity measure used in methods operating on the list. Furthermore, the document-query similarity measure is used in all methods that are based on cluster ranking (in-cluding ClustMRF) to order documents within the clusters.
The second initial list used for re-ranking, DocMRF (dis-cussed in Section 4.2.4), is created by enriching MRF X  X  SDM with query-independent document measures [3].
 The third initial list, LM , is addressed in Section 4.2.5. The list is created using unigram language models. In con-trast, the MRF and DocMRF lists were created using re-trieval methods that use term proximity information. Let p z (  X  ) be the Dirichlet-smoothed unigram language model induced from text z ;  X  is the smoothing parameter. The LM similarity between texts x and y is sim LM ( x,y ) def = the cross entropy measure;  X  is set to 1000. 4 Accordingly, the LM initial list is created by using sim LM ( Q,d ) to rank the entire corpus. 5 This measure serves as the document-query similarity measure for all methods operating over the LM list, and for the inter-document similarity measure used by the dsim feature function.

Unless otherwise stated, to cluster any of the three ini-tial lists D init , we use a simple nearest-neighbor clustering approach [18, 25, 14, 26, 13, 15]. For each document d (  X  D init ), a cluster is created from d and the k  X  1 docu-ments d i in D init ( d i 6 = d ) with the highest sim LM is set to a value in { 5 , 10 , 20 } using cross validation as de-scribed below. Using such small overlapping clusters (all of which contain k documents) was shown to be highly effective for cluster-based document retrieval [18, 25, 14, 26, 13, 15]. In Section 4.2.6 we also study the performance of ClustMRF when using hierarchical agglomerative clustering. (computed at cutoff 50, the size of the list D init that is re-ranked) and the precision of the top 5 documents (p@5) and their NDCG (NDCG@5) for evaluation measures. 6 The free parameters of our ClustMRF method, as well as those of all reference comparison methods, are set using 10-fold cross validation performed over the queries in an experimental setting. Query IDs are the basis for creating the folds. The two-tailed paired t-test with p  X  0 . 05 was used for testing statistical significance of performance differences.
For our ClustMRF method, the free-parameter values are set in two steps. First, SVM rank [12] is used to learn the val-ues of the  X  l weights associated with the feature functions. The NDCG@ k of the k constituent documents of a cluster serves as the cluster score used for ranking clusters in the learning phase 7 . (Recall from above that documents in a
T he MRF SDM used above also uses Dirichlet-smoothed unigram language models with  X  = 1000.
Queries for which there was not a single relevant document in the MRF or LM initial lists were removed from the eval-uation. For the ClueWeb settings, the same query set was used for ClueX and ClueXF. We note that statAP, rather than AP, was the official TREC evaluation metric in 2009 for ClueWeb with queries 1 X 50. For consistency with the other queries for ClueWeb, and following previous work [3], we use AP for all ClueWeb queries by treating prel files as qrel files. We hasten to point out that evaluation using statAP for the ClueWeb collections with queries 1 X 50 yielded relative performance patterns that are highly similar to those attained when using AP.
Using MAP@ k as the cluster score resulted in a slightly less effective performance. We also note that learning-to-Table 2: The performance of ClustMRF and a tuned M RF (TunedMRF) when re-ranking the MRF ini-tial list (Init). Boldface: the best result in a row.  X  X  X  and  X  X  X  mark statistically significant differences with Init and TunedMRF, respectively. cluster are ordered based on their query similarity.) A rank-ing of documents in D init is created from the cluster ranking, which is performed for each cluster size k (  X  { 5 , 10 , 20 } ), us-ing the approach described above; k is then also set using cross validation by optimizing the MAP performance of the resulting document ranking. The train/test split for the first and second steps are the same  X  i.e., the same train set used for learning the  X  l  X  X  is the one used for setting the cluster size. As is the case for ClustMRF, the final docu-ment ranking induced by any reference comparison method is based on using cross validation to set free-parameter val-ues; and, MAP serves as the optimization criterion in the training (learning) phase.

Finally, we note that the main computational overhead, on top of the initial ranking, incurred by using ClustMRF is the clustering. That is, the feature functions used are either query-independent, and therefore can be computed offline; or, use mainly document-query similarity values that have already been computed to create the initial ranking. Clus-tering of a few dozen documents can be computed efficiently; e.g., based on document snippets.
Table 2 presents our main result. Namely, the perfor-mance of ClustMRF when used to re-rank the MRF initial list. Recall that the initial ranking was induced using MRF X  X  SDM with free-parameter values set following previous rec-ommendations [28]. Thus, we also present for reference the re-ranking performance of using MRF X  X  SDM with its three free parameters set using cross validation as is the case for rank methods [23] other than SVM r ank , which proved to result in highly effective performance as shown below, can also be used for setting the values of the  X  l weights. Table 3: Using each of ClustMRF X  X  top-4 feature f unctions by itself for ranking the clusters so as to re-rank the MRF initial list. Boldface: the best per-formance per row.  X  X  X  marks a statistically signifi-cant difference with ClustMRF. the free parameters of ClustMRF; TunedMRF denotes this method. We found that using exhaustive search for finding SDM X  X  optimal parameter values in the training phase yields better performance (on the test set) than using SVM rank [12] and SVM map [36]. Specifically,  X  T ,  X  O , and  X  U were set to values in { 0 , 0 . 05 ,..., 1 } with  X  T +  X  O +  X 
We first see in Table 2 that while TunedMRF outperforms the initial MRF ranking in most relevant comparisons (ex-perimental setting  X  evaluation measure), there are cases (e.g., for AP and WT10G) for which the reverse holds. The latter finding implies that optimal free-parameter values of MRF X  X  SDM do not necessarily generalize across queries.
More importantly, we see in Table 2 that ClustMRF out-performs both the initial ranking and TunedMRF in all rel-evant comparisons. Many of the improvements are substan-tial and statistically significant. These findings attest to the high effectiveness of using ClustMRF for re-ranking.
We now turn to analyze the relative importance attributed to the different feature functions used in ClustMRF; i.e., the  X  weights assigned to these functions in the training phase by SVM rank . We first average, per experimental setting and cluster size, the weights assigned to a feature function using the different training folds. Then, the feature function is assigned with a score that is the reciprocal rank of its cor-responding (average) weight. Finally, the feature functions are ordered by averaging their scores across experimental settings and cluster sizes. Two feature functions, pr and spam, are only used for the ClueWeb-based settings. Hence, we perform the analysis separately for the ClueWeb and non-ClueWeb (AP, ROBUST, WT10G, and GOV2) settings. Table 4: Comparison with cluster-based retrieval m ethods used for re-ranking the MRF initial list. (CMRF is a shorthand for ClustMRF.) Boldface marks the best result in a row.  X  X  X  and  X  X  X  mark sta-tistically significant differences with the initial rank-ing and ClustMRF, respectively.

For the non-ClueWeb settings, the feature functions, in descending order of attributed importance, are: stdv-qsim, max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim, geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress, max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy, geo-sw1, max-entropy. For the ClueWeb settings the feature functions are ordered as follows: max-sw2, max-sw1, max-qsim, geo-qsim, max-spam, geo-sw2, min-icompress, min-sw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, min-dsim, min-entropy, max-entropy, min-spam, geo-icompress, geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, min-pr, max-dsim.

Two main observations rise. First, each of the three types of cliques used in Section 2.1 for defining the MRF has at least one associated feature function that is assigned with a relatively high weight. For example, the geo-qsim func-tion defined over l QD , the max-qsim function defined over l
QC , and the max-sw2 function defined over l C , are among the 4, 6 and 2 most important functions in both cases (non-ClueWeb and ClueWeb settings). Second, for the ClueWeb settings, the feature functions defined over the l C clique and which are based on query-independent document measures (e.g., max-sw1, max-sw2, max-spam) are attributed with high importance. In fact, among the top-10 feature func-tions for the ClueWeb settings only two (max-qsim and geo-qsim) are not based on a query-independent measure. This is not the case for the non-ClueWeb settings where different statistics of the query-similarity values are among the top-10 feature functions. We note that using some of the query-independent document measures utilized here was shown in work on Web retrieval to be effective for ranking documents directly [3]. We demonstrated the merits of using such mea-sures for ranking document clusters.

In Table 3 we present the performance of using each of the top-4 feature functions (for the non-ClueWeb and ClueWeb settings) by itself as a cluster ranking method. As in Sec-tion 4.2.1, we use the cluster ranking to re-rank the MRF initial list. We see in Table 3 that in almost all relevant comparisons ClustMRF is more effective  X  often to a sub-stantial and statistically significant degree  X  than using one of its top-4 feature functions alone. Thus, we conclude that ClustMRF X  X  effective performance cannot be attributed to a single feature function that it utilizes.

We also performed ablation tests as follows. ClustMRF was trained each time without one of its top-10 feature func-tions. This resulted in a statistically significant performance decrease with respect to at least one of the three evaluation metrics of concern (MAP, p@5 and NDCG@5) for all top-10 feature functions for the ClueWeb settings. (Actual num-bers are omitted as they convey no additional insight.) Yet, there was no statistically significant performance decrease for any of the top-10 feature functions for the non-ClueWeb settings. These findings attest to the redundancy of feature functions when employing ClustMRF for the non-ClueWeb settings and to the lack thereof in the ClueWeb settings.
Finally, we computed the Pearson correlation of the learned  X   X  X  values (averaged over the train folds and cluster sizes) between experimental settings. We found that for pairs of non-ClueWeb settings, excluding AP, the correlation was at least 0 . 5; however, the correlation with AP was much smaller. For the ClueWeb settings, the correlation between ClueB and ClueBF was high (0 . 83) while that for other pairs of settings was lower than 0 . 5. Thus, we conclude that the learned  X  l values can be collection, and setting, dependent.
We next compare the performance of ClustMRF with that of highly effective cluster-based retrieval methods. All meth-ods re-rank the MRF initial list.

The InterpolationF method ( Inter in short) [13] ranks documents directly using the score function: art re-ranking method represents the class of approaches that use clusters to  X  X mooth X  document representations [13].
In contrast to Inter, ClustMRF belongs to a class of meth-ods that rely on cluster ranking. Accordingly, the next ref-erence comparison methods represent this class. Section 4.1 provided a description of how the cluster ranking is trans-formed to a ranking of the documents in D init . The AMean method [26, 15], for example, scores cluster C by the arith-metic mean of the query similarity values of its constituent documents. Formally, Score ( C ; Q ) def = 1 | C | P d  X  C
Scoring C by the geometric mean of the query-similarity values of its constituent documents, Score ( C ; Q ) def = q Q ter ranking performance [15]. This approach, henceforth referred to as GMean , results from aggregating several fea-ture functions (geo-qsim) that are used in our ClustMRF method. (See Section 2.1 for details.) An additional state-of-the-art cluster ranking method is ClustRanker ( CRank in short) [15]. Cluster C is scored by Table 5: Using ClustMRF to re-rank the DocMRF [ 3] list. Boldface: best result in a row.  X  X  X  marks a statistically significant difference with DocMRF. estimated based on inter-cluster and inter-document (across clusters) similarities, respectively. These similarities, com-puted using the language-model-based measure sim LM (  X  ,  X  ), are not utilized by ClustMRF that uses inter-document sim-ilarities only within a cluster.

Following the original reports of Inter [13] and CRank [15], we estimate sim ( Q,C ) and sim ( C,d ) in these methods using sim LM (  X  ,  X  ); C is represented by the concatenation of its con-stituent documents. For a fair comparison with ClustMRF, sim ( Q,d ) is set in all reference comparisons considered here to sim MRF (  X  ,  X  ), which was used to create the initial MRF list that is re-ranked.

All free parameters of the methods are set using cross val-idation. Specifically,  X  which is used by Inter and CRank is set to values in { 0 , 0 . 1 ,..., 1 } . The graph out degree and the dumping factor used by CRank are set to values tively. The cluster size used by each method is selected from { 5 , 10 , 20 } as is the case for ClustMRF. Table 4 presents the performance numbers.

We can see in Table 4 that in a vast majority of the rele-vant comparisons ClustMRF outperforms the reference com-parison methods. Many of the improvements are substantial and statistically significant. In the few cases that ClustMRF is outperformed by one of the other methods, the perfor-mance differences are not statistically significant.
Heretofore, we studied the performance of ClustMRF when used to re-rank the MRF initial list. The analysis presented in Section 4.2.2 demonstrated the effectiveness  X  especially for the ClueWeb settings  X  of using feature functions that utilize query-independent document measures. Thus, we now turn to explore ClustMRF X  X  performance when em-ployed over a document ranking that is already based on using query-independent document measures.

To that end, we follow some recent work [3]. We re-rank the 1000 documents that are the most highly ranked by MRF X  X  SDM that was used above to create the MRF ini-tial list. Re-ranking is performed using an MRF model that is enriched with query-independent document measures [3]. We use the same document measures utilized by ClustMRF, except for dsim which is based on inter-document similar-ities and which was not considered in this past work that ranked documents independently of each other [3]. The re-sultant ranking, induced using SVM rank for learning param-eter values, is denoted DocMRF . (SVM rank yielded better performance than SVM map .) We then let ClustMRF re-rank the top-50 documents. In doing so, we use the ex-ponent of the score assigned by DocMRF to document d , which is a rank equivalent estimate to that of log p ( Q,d ), as the sim ( Q,d ) value used by ClustMRF. Thus, we main-tain the invariant mentioned above that the scoring function used to induce the ranking upon which ClustMRF operates is rank equivalent to the document-query similarity measure used in ClustMRF. We note that ClustMRF is different from DocMRF in two important respects. First, by the virtue of ranking clusters first and transforming the ranking to that of documents rather than ranking documents directly as is the case in DocMRF. Second, by the completely different ways that document-query similarities are used.

Comparing the performance of DocMRF in Table 5 with that of the MRF initial ranking in Table 2 attests to the merits of using DocMRF for re-ranking. We can also see in Table 5 that applying ClustMRF over the DocMRF list results in performance improvements in almost all relevant comparisons. Many of the improvements for the ClueWeb settings are substantial and statistically significant.
The third list we re-rank using ClustMRF is LM, which was created using unigram language models. For reference comparison we use the cluster-based Inter method which was used in Section 4.2.3. Experiments show  X  actual num-bers are omitted due to space considerations  X  that for re-ranking the LM list, the GMean cluster ranking method is more effective in most relevant comparisons than the other two cluster ranking methods used in Section 4.2.3 for ref-erence comparison (AMean and CRank). Hence, GMean is used here as an additional reference comparison.

ClustMRF, Inter and GMean use the sim LM (  X  ,  X  ) similar-ity measure, which was used for inducing the initial ranking, for sim ( Q,d ). All other implementation details are the same as those described above. As a result, ClustMRF, as well as Inter and GMean, use only unigram language models in the LM setting considered here. This is in contrast to the MRF-list setting considered above where term-proximities information was used.

An additional reference comparison that uses unigram lan-guage models is relevance model number 3 [1], RM3 , which is a state-of-the-art query expansion approach. RM3 is also used to re-rank the LM list. All (50) documents in the list are used for constructing RM3. Its free-parameter values are set using cross validation. Specifically, the number of expansion terms and the interpolation parameter that con-trols the reliance on the original query are set to values in smoothed language models are used with  X  = 1000. Table 6: Re-ranking the LM initial list. Boldface: t he best result in a row.  X  X  X  and  X  X  X  mark statistically significant differences with the initial ranking and ClustMRF, respectively.

We see in Table 6 that ClustMRF outperforms the ref-erence comparisons in a vast majority of the relevant com-parisons. Many of the improvements are substantial and statistically significant. These results, along with those pre-sented in Sections 4.2.1 and 4.2.4, attest to the effectiveness of using ClustMRF to re-rank different initial lists.
Thus far, we used ClustMRF and the reference compar-isons with nearest-neighbor ( NN ) clustering. In Table 7 we present the retrieval performance of using hierarchical ag-glomerative clustering ( HAC ) with the complete link mea-sure. This clustering was shown to be among the most ef-fective hard clustering methods for cluster-based retrieval [24, 13]. We use 1 sim document dissimilarity measure; and, cut the clustering den-drogram so that the resultant average cluster size is the clos-est to a value k (  X  { 5 , 10 , 20 } ). Doing so somewhat equates the comparison terms with using the NN clusters whose size is in { 5 , 10 , 20 } . Cross validation is used in all cases for setting the value of k .

The MRF initial list is clustered and serves as the ba-sis for re-ranking. Experiments show (actual numbers are omitted due to space considerations) that among the three cluster ranking methods which were used above for refer-ence comparison (AMean, GMean, and CRank) CRank is the most effective when using HAC. Hence, CRank serves as a reference comparison here.

We see in Table 7 that in the majority of relevant com-parisons, ClustMRF improves over the initial ranking when using HAC. In contrast, CRank is outperformed by the ini-tial ranking in most relevant comparisons for HAC. Indeed, ClustMRF outperforms CRank in most cases for both NN and HAC. We also see that ClustMRF is (much) more effec-tive when using the overlapping NN clusters than the hard Table 7: Using nearest-neighbor clustering (NN) vs. ( complete link) hierarchical agglomerative cluster-ing (HAC). The MRF initial list is used. Boldface: the best result in a row per clustering algorithm; un-derline: the best result in a row.  X  X  X  and  X  X  X : statisti-cally significant differences with the initial ranking and CRank, respectively. clusters created by HAC. The improved effectiveness of us-ing NN in comparison to HAC echoes findings in previous work on cluster-based re-ranking [13]. For CRank, the per-formance of using neither NN nor HAC dominates that of using the other.
Until now, ClustMRF and all reference comparison meth-ods were used to re-rank an initial list of 50 documents. Us-ing a short list follows common practice in work on cluster-based re-ranking [18, 25, 26, 13] as was mentioned in Section 4.1. We now turn to study ClustMRF X  X  performance when re-ranking longer lists. To that end, we use for the initial list the n (  X  { 50 , 100 , 250 , 500 } ) documents that are the most highly ranked by MRF X  X  SDM [28] which was used above for creating the MRF initial list. For reference comparisons we use TunedMRF (see Section 4.2.1); and, the AMean and GMean cluster ranking methods described in Section 4.2.3. Nearest-neighbor clustering is used.

We see in Figure 2 that in almost all cases  X  i.e., ex-perimental settings and values of n  X  ClustMRF outper-forms both the initial ranking and TunedMRF; often, the performance differences are quite substantial. Furthermore, in most cases (with the notable exception of AP) ClustMRF outperforms AMean and GMean.
We next explore how ClustMRF can be used to improve the performance of search-results diversification approaches. Specifically, we use the MMR [5] and the state-of-the-art xQuAD [29] diversification methods. ranking (Init) and ClustMRF, respectively. The MRF initial list is used. MMR and xQuAD iteratively re-rank an initial list D init . In each iteration the document in D init \ S assigned with the highest score is added to the set S ; S is empty at the beginning. The final ranking is determined by the order of insertion to S .

The score MMR assigns to document d (  X  D init \ S ) is to MMR, xQuAD uses information about Q  X  X  subtopics, T ( Q ), and assigns d with the score  X p ( d | Q )+ (1  X   X  ) P t  X  X  ( Q ) h p ( t | Q ) p ( d | t ) Q d the relative importance of subtopic t with respect to Q ; p ( d | Q ) and p ( d | t ) are the estimates of d  X  X  relevance to Q and t , respectively.

The parameter  X  controls in both methods the tradeoff between using relevance estimation and applying diversifi-cation. Our focus is on improving the former and evaluat-ing the resulting (diversification based) performance. This was also the case in previous work that used cluster ranking for results diversification [11]. Hence, this work serves for reference comparison below. 8
We study three different estimates for sim 1 ( Q,d ) (used in MMR) which we also use for p ( d | Q ) (used in xQuAD). The first, sim MRF ( Q,d ), is that employed in the evalua-tion above to create the MRF initial list that is also used here for re-ranking. (Further details are provided below.) The next two estimates are based on applying cluster rank-ing and transforming it to document ranking using the ap-proach described in Section 4.1. In these cases, 1 r ( d ) for sim 1 ( Q,d ), where r ( d ) is the rank of d in the document result list produced by using the cluster ranking method. The first cluster ranking method is ClustMRF. The second, QClust , was used in the work mentioned above on utilizing cluster ranking for results diversification [11]. Specifically, cluster C is scored by sim LM ( Q,C ) (see Section 4.1 for de-
T here is work on using information induced from clusters for the diverisification itself (e.g., [21]). Using ClustMRF for cluster ranking in these approaches is future work.
For scale compatibility, the two resultant quantities that are interpolated (using  X  ) in MMR and xQuAD are sum normalized with respect to all documents in D init before the interpolation is performed. tails of s im LM (  X  ,  X  )); C is represented by the concatenation of its documents.

We use MMR and xQuAD to re-rank the MRF initial list that contains 50 documents. sim LM (  X  ,  X  ) serves for the sim 2 (  X  ,  X  ) measure used in MMR and for p ( d | t ) that is used in xQuAD. The official TREC subtopics, which are available for the ClueWeb settings that we use here, were used for experiments. Following the findings in [29], we set p ( t | Q ) using cross validation;  X  -NDCG (@20) is the optimization metric. In addition to  X  -NDCG (@20), ERR-IA (@20) and P-IA (@20) are used for evaluation.

Table 8 presents the results. We see that using the MRF similarity measure in MMR and xQuAD outperforms the ini-tial ranking, which was created using this measure, in most relevant comparisons. This attests to the diversification ef-fectiveness of MMR and xQuAD. Using QClust outperforms the initial ranking in most cases, but is consistently out-performed by using the MRF measure and our ClustMRF method. More generally, the best performance for each di-versification method (MMR and xQuAD) is almost always attained by ClustMRF, which often outperforms the other methods in a substantial and statistically significant man-ner. Thus, although ClustMRF ranks clusters of similar documents, using the resultant document ranking can help to much improve results-diversification performance.
We presented a novel approach to ranking (query specific) document clusters by their presumed relevance to the query. Our approach uses Markov Random Fields that enable the integration of various types of cluster-relevance evidence. Empirical evaluation demonstrated the effectiveness of us-ing our approach to re-rank different initially retrieved lists. The approach also substantially outperforms state-of-the-art cluster ranking methods and can be used to substantially improve the performance of results diversification methods.
We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.
