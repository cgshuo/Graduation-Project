 One of the primary data mining tasks is clustering. Clustering aims at partitioning the data set into distinct groups, called clusters, while maximizing the intra-cluster similar-rithms require full access to the data which is going to be analyzed. All data has to be located at the site where it is processed. Nowadays, large amounts of heterogeneous, complex data reside on different, independently working computers which are connect-ed to each other via local or wide area networks. Examples comprise distributed mobile networks, sensor networks or supermarket chains where check-out scanners, located at different stores, gather data unremittingly. Furthermore, international companies such as DaimlerChrysler have some data which are located in Europe and some data located in the US and Asia. Those companies have various reasons why the data cannot be transmitted to a central site, e.g. limited bandwidth or security aspects. mensional feature vectors. For instance, a starting point for applying clustering algo-rithms to distributed unstructured document collections is to create a vector space mod-el, alternatively known as a bag-of-words model [13], where each document is represented by a high-dimensional feature vector. Other examples for high-dimensional image retrieval [12], and molecular biology [4]. tion of the data, created the rather new research area of Distributed Knowledge Discov-ery in Databases (DKDD). In this paper, we present a general approach which helps to extract knowledge out of high-dimensional feature vectors spread over several sites. To get specific, we demonstrate the benefits of our approach for distributed density-based clustering. Our approach tries to describe local feature vectors as accurately as possible with a certain number of granted bytes. These approximations are sent to a server site, where the global server clustering is carried out based on a suitable distance function measuring the similarity between the locally determined feature vector approximations. related work in the area of distributed clustering. In Section 3, we explain how to form the local approximations which are sent to a central server site. Then, in Section 4, a meaningful similarity measure for the feature vector approximation is introduced. In Section 5, we demonstrate the suitability of our feature vector approximation technique and the corresponding distance function. In Section 6, we close this paper with a short summary and a few remarks on future work.
 sets was proposed which applies single link clustering. In contrast to this approach, we concentrate in this paper on horizontally distributed data sets. for high-dimensional, horizontally distributed data sets by merging clustering hierar-chies generated locally. Unfortunately, this approach can only be applied for dis-tance-based hierarchical distributed clustering approaches, whereas our aim is to intro-duce a generally applicable approach.
 based on the density-based partitioning clustering algorithm DBSCAN. The idea of these approaches is to determine suitable local objects representing several other local objects . Based on these representatives a global DBSCAN algorithm is carried out. These approaches are tailor-made for the density-based distributed clustering algorithm DBSCAN. DDM. To get specific, we demonstrate the benefits of our approach for distributed clus-tering algorithms. In contrast to the above specific distributed clustering approaches , our approach is not susceptible to an increasing number of local clients. It does only depend on the overall allowed transmission cost, i.e. on the number of bytes we are allowed to transmit from the local clients to a server. In order to keep these transmission cost low, we introduce in the following section a suitable client-side approximation technique for describing high-dimensional feature vectors.
 Distributed Data Mining (DDM) is a dynamically growing area within the broader field of KDD. Generally, many algorithms for distributed data mining are based on algorithms which were originally developed for parallel data mining. In [10], some state-of-the-art research results related to DDM are summarized. Whereas there already exist algorithms for distributed classification and association rules, there is a lack of algorithms for distributed clustering. The goal of this section is to find a rough description of the complete data set by means of some (flat) directory pages which conservatively approximate the complete mainly interested in the clusters themselves but rather in a partitioning of the data space into rectangular cuboids. Similar, to directory pages in an index structure, these cuboids achieve such cuboids with only a little variation of the lengths of the edges by applying the k -means clustering algorithm [11]. Thereby the data set is approximated by k cen-troids, and each vector is assigned to its closest centroid. All feature vectors which are assigned to the same centroid form a cluster and are approximated by an MBR of all the vectors of this cluster. As desired, the form of these MBRs tend to be quadratic as the clustering algorithm indirectly also minimizes the average length of the space diagonals of the k MBRs.
 After having partitioned the local data space into k clusters represented by MBRs, we express each feature vector v w.r.t. to the lower left corner of its corresponding mini-mum bounding rectangle MBR Cluster ( v ). Definition 1 Feature Vector v is calculated by vector is represented by a byte string of length m . We will describe each feature vector by a conservative hierarchy of approximations where in each level we use some more bytes to approximate the feature vector more closely. By traversing the complete ap-proximation hierarchy, we can reconstruct the correct feature vector. meaningful bytes are transmitted to the server along with positional information of the The hybrid-approximation approach which we propose in this section is quite similar to the idea of the IQ-tree [2] which is an index structure especially suitable for man-partitions represented by minimum bounding rectangles (MBRs) of the points located in the corresponding region in the data space. This kind of data set approximation is bits are used to describe the feature vector more accurately. 3.1 Data Set Approximation 3.2 Feature Vector Approximation Definition 2 Ranking and Approximation Function tion function f app : to have the following properties: approximation techniques of high-dimensional feature vectors, i.e. the byte-oriented , the dimension-oriented , and the combined approximation technique (cf. Figure 1). All three approaches fulfill rather obviously the properties stated in Definition 2. Neverthe-less, they differ in the way they actually define the ranking and approximation functions. In the following, we assume that the cluster MBR of a feature vector v MBR Cluster ( v )= [ MBR_l 1  X  MBR_u 1 ]  X ... X  [ MBR_l d  X  MBR_u d ] has already been transmitted to the serv-er. Furthermore, we assume that v is defined according to Definition 1. As the first bytes of each feature contain the most significant information, we rank the bytes b i , j by means of the bijective function  X  : ac-as follows: bytes. By means of this additional positional information, the server can construct an accurate server side approximation of v . In the above approach, we considered the first bytes of each dimension to build the approximation. In this approach, we select significant dimensions and then transmit all 3.2.1 Byte-Oriented Approximation (BOA) 3.2.2 Dimension-Oriented Approximation (DOA) rank the bytes b i,j by means of the bijective function  X  : j &lt; j  X  ). sions which have not been selected for transmission. Thus, we can shrink the approxi-mation area also for those dimensions for which we have not received any bytes. This shrinking is possible due to the ranking agreement between the clients and the server that the value of the dimensions for which we have not received any bytes is equal or smaller to the smallest value for which we have already received some bytes. Let i  X   X  {1,.., d } now be the transmitted dimension with the smallest feature value of all transmitted dimensions. [ l , u d ] as follows: This appro ach combines the two previous approaches. According to Definition 1, the L bytes having the highest ranking values. Thus the bijective function  X  : and j &lt; j  X  ). bytes of the selected features to the server. The dimension oriented approximation ap-proach (cf. Figure 1b) selects dimensions i having the highest values v i . Thus, we Lm  X  areas for the three proposed approaches. The figure shows clearly that the combined 3.2.3 Combined Approximation (CA) vector approximations. The most straightforward approach is to use the box center to compute the distance between two box approximations. This center oriented box dis-tance approximates the exact distance between the feature vectors rather accurate if the boxes are rather small and do not overlap. are identical. The center oriented distance would assign a zero distance to the approxi-mated feature vectors, although the exact distance between the feature vectors might be distances between the feature vectors rather than the distances between the box centers. This distance expectation value is based on the distance distribution function P d : O  X  O  X  ( IR 0 +  X  [0..1]), which assigns a probability value p to each possible distance  X  ( cf. overlapped by a sphere around x  X  A with radius  X  . Summing up all these values for all x  X  A yields the probability P following lemma describes formally how to compute P d for two approximated feature vectors.
 v  X   X  X  IR. Then the distance distribution function P on the approximations A and A  X  can be computed as follows .
 approach leads to a smaller approximation area than the byte-oriented and the dimen-sion-oriented approach. represents the similarity between two box approximations in the best possible way by one single value , where P  X  d ( v , v  X  ) denotes the derivation of P d ( v , v  X  ).
 mated feature vectors by means of monte-carlo sampling. Thereby, we create randomly feature vectors located in the boxes and compute the average distance of these randomly created feature vectors to each other, Obviously, the higher the sample rate the more accurate is the computation of the distance expectation value. Note, that the center ori-ented distance can be regarded as a distance expectation value where only one sample pair, i.e. the box centers, is used. In this section, we evaluate the performance of our approach with a special emphasis on the overall transmission cost. The tests are based on an artificial dataset ART and two real world data sets PLANE and PDB which were distributed to two clients: distributed in a 30-dimensional vector space. objects provided by our industrial partner, an American airplane manufacturer. Each object is represented by a 42-dimensional feature vector which is derived from the cover sequence model as described in [9]. Protein Data Bank (PDB). The 1000 objects are represented by 3D shape histograms [1] resulting in a 120-dimensional feature vector per object. 5.1 Quality of the Feature Vector Approximation Techniques In a first experiment, we examined the quality of the three approximation techniques BOA , DOA and CA (cf. Section 3) . For each feature vector, we transmitted once L bytes In order to put clustering methods into practice, we extract an aggregated value which we call distance expectation value. The distance expectation value E d : O  X  O  X  IR 0 + (measured in percent of all bytes of a feature vector ) to the server which then constructs the approximations based on the transmitted data. Figure 4 depicts how the approxima-forms worst. Only for very small values of L it outperforms the BOA approach. Howev-er, our CA approach yields to the best results, especially for low transmission cost. transmit only the pre-clustering information of the feature vectors, i.e. the dataset ap-proximations (cf. Section 3.1), the approximation quality increases slowly with increas-ing k . Obviously, an increasing k parameter yields higher transfer cost. In contrast to the dataset approximation approach, the quality increases more rapidly when we increase the amount of transmitted data of the feature vector approximations (cf. Section 3.2). Figure 4b shows that we achieve the best trade-off between accuracy and transfer over-head when we set k = 10, especially for low transfer cost. 5.2 Distance Measures expected distance of the feature vectors approximated by A and A  X  (cf. Section 4). For the computation of the expected distance exp ( A , A  X  ), we used monte-carlo sampling with a sample rate s . For measuring the quality we summed up the quadratic distance error of distance of the feature vectors. Figure 5 depicts the average quadratic distance error of all feature vector approximations.
 tion error depends on the transmission cost. The error is measured by the average length of the diagonal of the approximation areas.
 approach as well as for the CA approach. For high values of L , the DOA approach per-between the feature vectors much more accurately than the distance function mid ( A , A  X  ), already for a sample rate s &gt; 2. Figure 5b shows that the difference between the two 5.3 Density-Based Clustering In a last experiment, we compared a standard DBSCAN run based on the exp ( A , A  X  ) measure to the distributed clustering approach introduced in [7]. We measured the qual-ity of the approximated clustering result by the quality criterion used in [7]. Figure 6 forms much better, i.e. our approach yields higher quality values than the approach pre-sented in [7]. Note that the approach of [7] was especially designed for achieving high-quality distributed clusterings based on little transmitted information. We would like to point out that this experiment shows that our approximation technique for high-dimensional feature vectors can beneficially be used as basic operation for distrib-uted data mining algorithms. In this paper, we presented a novel technique for approximating high-dimensional dis-tributed feature vectors. In order to generate suitable approximations, we enhanced the Therefore it is especially important to use the exp ( A , A  X  ) distance measure when only small transfer cost are allowed. the quality. We demonstrated the benefits of our technique for the important area of dis-tributed clustering.
 benefit from our high-dimensional feature vector approximation technique.
 idea of state-of-the-art index structures for high-dimensional data which approximate each single feature vector by a certain number of granted bytes. Based on this technique
