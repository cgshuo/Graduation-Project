 Jitendra Agrawal  X  Shikha Agrawal  X  Ankita Singhai  X  Sanjeev Sharma Abstract Data mining is the process of determining new, unanticipated, valuable patterns ficial intelligence, and machine learning. It can help companies focus on the most important information in their data warehouses. Association rule mining is one of the most highly researched and popular data mining techniques for finding associations between items in a set. It is frequently used in marketing, advertising, and inventory control. Typically, asso-ciation rules only consider items in transactions (positive association rules). They do not ful for market basket analysis. Also, existing algorithms often generate too many candidate itemsets when mining the data and scan the database multiple times. To resolve these issues in association rule mining algorithms, we propose SARIC (set particle swarm optimization for association rules using the itemset range and correlation coefficient). Our method uses set particle swarm optimization to generate association rules from a database and considers both positive and negative occurrences of attributes. SARIC applies the itemset range and correlation coefficient so that we do not need to specify the minimum support and confidence, because it automatically determines them quickly and objectively. We verified the efficiency of SARIC using two differently sized databases. Our simulation results demonstrate that SARIC generates more promising results than Apriori, Eclat, HMINE, and a genetic algo-rithm.
 Keywords Association rule mining  X  Correlation coefficient  X  Itemset range  X  Particle swarm optimization  X  SET-PSO 1 Introduction Business knowledge-driven decisions can be made using existing databases that contain information, such as scientific, biological, financial, communication network, or marketing transactional data. Data mining has been extensively applied to these kinds of databases and hidden, valid, comprehensive, and actionable information or patterns from large databases, trends and behaviors, which is as a core process of knowledge discovery in databases [ 1 ].
Data mining can be categorized into two classes: descriptive and prescriptive. Descriptive mining characterizes general properties of the data in the repository, whereas prescriptive mining infers something using the current data, by making predictions based on historical others.

Association rule mining is a popular and well-researched data mining technique, which For example, purchasing a particular product when another is purchased represents an asso-ciation rule that is frequently used in marketing, advertising, and inventory control to find relationships between items.
 Let database D be a set of transactions, where each transaction T is a set of items such that T  X  I . An association rule is an implication of the form X  X  Y , which has a support of s % and a confidence of c %, where X  X  I ; X  I ,and X  X  Y =  X  .

Support The probability of an item or set of items in the given transactional database. That is, where n is the total number of transactions and n ( X ) are the number of transactions that contain X .
 Confidence The conditional probability of an association rule X  X  Y ,definedas We need minimum support and confidence threshold values to eliminate irrelevant associ-ation rules. They are assigned by the decision maker. Association rule algorithms such as Apriori and Frequent Pattern growth (FP growth) extract meaningful rules based on how interesting they are. Methods for mining association rules are typically decomposed into two sub-methods:  X  generating frequent itemsets that occur at least as often as the user-specified minimal support; and  X  generating rules from the frequent itemsets that have a minimum confidence.

Typical association rules only consider items that were contained in transactions, which are referred to as positive association rules. Negative association rules (NARs) were introduced by Brin et al. [ 4 ], and also consider items that are absent from transactions. NAR mining is frequently used in market basket analysis, to identify products that complement each other. There are two key problems in NAR, which make it a difficult task. The first is how to effectively search an interesting itemset, and the second is how to effectively identify interesting NARs.
 The negation of an itemset A is written as  X  A which means the absence of the itemset A . Rules of the form A  X  X  X  B ,  X  A  X  B ,and  X  A  X  X  X  B are negative rules. The rule A  X  X  X  B means the data objects that have itemset A do not have itemset B ,  X  A  X  B means data objects that do not have itemset A have itemset B ,and  X  A  X  X  X  B means data objects that do not have itemset A do not have itemset B [ 5 ].

The support and confidence of negative rules can use positive rules [ 6 ], which are given by the following formulae.
 The Apriori algorithm is a classical technique. Its limitations are that it produces a large rules, and it requires user-defined threshold values for the support and confidence. These limitations affect the quality of the rules and the performance. There are many variants of this algorithm, and multiple new algorithms have been proposed that improve the efficiency of association rule mining. We discuss some of these algorithms in the next section. 2 Literature review on association rule mining The Apriori algorithm requires a lot of processing time, so its computational efficiency is very important. To improve the efficiency of such algorithms, many researchers have proposed modified association rule-related algorithms by implementing new techniques, such as the hash-based, transaction reduction, partitioning, sampling, and dynamic itemset counting techniques, as discussed in [ 3 , 7  X  10 ]. The HMINE algorithm [ 11 ] was designed as an improvement to Apriori and Eclat, in terms of both the time and space complexity. Hamrouni et al. [ 12 ] introduced a new exact concise representation of frequent itemsets. It explores the disjunctive search space, which offers direct access to the disjunctive and negative support of the frequent itemsets. For mining both sequential and association rules, the CMRULE algorithm [ 13 ] was designed to mine rules common to many sequences instead of mining rules that appear frequently in sequences. The procedural-knowledge framework was proposed in [ 14 ] to discover new meta-knowledge and rules in a given domain, using an an algorithm that implements causal learning in a conscious emotional learning tutoring system (CELTS). This algorithm integrates event and time constraints into a rule growth mining algorithm, which reduces the number of rules and extracts only those that are most precise and relevant.
Negative rules consider items that conflict with each other. In other words, negative rules are used to represent that if product A is purchased, then product B will not be purchased. the authors designed some constraints that prune the search space. They determined the confidence of both types of rules by increasing the degree of the conditional probability, which is relative to the prior probability. Other new algorithms for mining negative rules were discussed in [ 16  X  19 ].

Many researchers have improved association rule mining, but the minimum support and valuable information and quality of the rules. To improve this, the TOP K algorithm [ 20 ]was designed to mine TOP K association rules, in which the user sets k (the required number of rules) and the minimum confidence. This algorithm uses the rule expansion approach, which finds larger rules by recursively scanning the database to add one item to the left and right parts of each rule. This algorithm was enhanced using deductive reasoning, to extract all the the results of these TOP K algorithms may be influenced by user-defined parameters. Recently, evolutionary algorithms have been efficiently applied to mine association rules. Genetic algorithms (GAs) are general-purpose searching algorithms that were inspired by natural genetic populations. GAs can mine interesting association rules by optimizing the rules generated by the Apriori algorithm. Moreover, GAs also consider negative occurrences (PSO) is an evolutionary algorithm that uses artificial intelligence techniques. Researchers learning PSO [ 27 ], and PSO with ontology [ 28 ], to make association rule mining more efficient. PSO has been used to optimize positive and negative association rules generated and confidence.

Our literature review shows that many algorithms have been proposed to mine all the frequent itemsets in a transaction database. These algorithms differ in the way they handle the candidate sets, and by the mechanisms, they use to reduce the number of scanned data-bases. Discovering association rules is an extremely computationally expensive task, and it is therefore imperative to have fast, scalable algorithms. Additionally, the number of rules that are mined grows exponentially with the number of items. To effectively use association rules and the knowledge they contain, the number of rules must be manageable. Thus, we need a method that reduces the number of association rules without losing information. We must also determine which rules are interesting. The standard approach is to use the support and confidence. However, they have their own limitations as they are specified by the user. negative rules can also be as important as positive rules, but they are not generated by the traditional association rule mining algorithms.
 In this research, we investigated these issues and propose a new algorithm, SARIC (Set Particle Swarm Optimization for Association Rule mining using the Itemset range and Cor-relation coefficient). Our method evaluates the rules and considers the negative occurrences of attributes.
 The remainder of this paper is organized as follows. Section 3 briefly describes standard PSO, and we present the proposed algorithm in Sect. 4 . Section 5 contains the computational results of the proposed method, and our concluding remarks are discussed in Sect. 6 . 3 Particle swarm optimization PSOisapopulation-basedheuristicglobaloptimizationtechniquethatisbasedontheflocking behavior of birds or fish schools. It was proposed by Kennedy and Eberhart [ 30 ]. The fast convergence rate of PSO is its main strength. Additionally, it uses simple computations and is easy to implement, which means that it compares favorably with other global optimization algorithms [ 31 ]. In PSO, particles are moved through N dimensional space to find an optimum solution to the objective function.

The PSO paradigm is based on a collection (called a swarm) of fairly primitive elements (called particles). PSO is initialized by a random population of particles. The individual particles gradually move toward the position of their own and their neighbor X  X  best previous performance in the multidimensional search space. Each particle maintains its position, which is composed of the candidate solution, the value of the function for the candidate solution, the PSO algorithm also keeps track of the best value among all particles in the swarm. This is called the global best value. The candidate solution that achieves this value is known as the global best position. The PSO algorithm consists of the following steps.  X 
Evaluate the value of the objective function for each particle;  X  update the individual and global best values and positions; and  X  update the velocity and position of each particle.

The individual and global best values and positions are updated by comparing the newly evaluated values with the previous ones, and replacing the best values and positions as necessary. In every iteration, all the particles of the swarm can move in N dimensional space to find the global optimum. The updating equations for the velocity and position of each particle are and social components, which are in (0, 2). 4 Methodology We considered the background presented in Sect. 2 and propose the novel SARIC algorithm. It uses SET-PSO to generate association rules from a database, by considering the negative occurrences of attributes. Instead of taking the minimum support and confidence from the user, it determines them quickly and objectively. A detailed explanation of SARIC is given in the next section. 4.1 The SARIC algorithm The proposed SARIC algorithm works in two phases. The first phase is preprocessing, and the second is mining. The first phase consists of binary transformation, itemset range (IR), and correlation calculations. In the binary transformation step, the data are transformed and the positive and negative itemsets by calculating the correlation between the items. In the second phase, the set-based PSO algorithm is applied to mine positive and NARs. First, front-and back-partitioning point generated in the IR calculation. The second step generates the population. The best particles are selected as an initial population according to their function values. Finally, we separately apply the set-based PSO search procedure to each positive and negative set, to find the best rules using the global best particle value as a minimum optimum threshold. Figures 1 and 2 illustrate the proposed SARIC algorithm and the set-based PSO.
 4.2 Preprocessing of SARIC 4.2.1 Binary transformation This approach makes database scanning efficient and improves the support and confidence the size of which is equal to the total number of different items in the database. For every transaction, the purchased items have a value of 1 in the respective cell, whereas the others have a value of 0, as shown in Fig. 3 . 4.2.2 IR (itemset range) calculation The purpose of the IR is to produce more meaningful association rules. The search efficiency is enhanced when IR analysis is used to decide the rule length generated by the chromosomes in the particle swarm evolution. It avoids searching for too many insignificant association calculated using the sum of the number of transaction records purchasing between m and n items. TotalTrans represents the number of total transactions.

We calculate the IR values for all the possible combinations of m and n .Valuesof m IR rules will have three different dimensions, that is, 2D: 1  X  2, 3D: 1  X  3 , 2  X  3, and 4D: 1  X  4 , 2  X  4 , 3  X  4. This reduces the number of meaningless rules in higher dimensions. 4.2.3 Correlation calculation The correlation in association rule mining considers the relationships between items. The positive and negative rules. This avoids conflicts. The relativity or correlation coefficient between items A and B is defined as If Corr ( A , B )&gt; 1, then A and B are positively related and we generate rules A  X  B ,and  X  A  X  X  X  B .IfCorr ( A , B )&lt; 1, then A and B are negatively related and we generate rules A  X  X  X  B ,  X  A  X  B .IfCorr ( A , B ) = 1, then A and B are independent. 4.3 Association rule mining using SET-PSO We apply SET-PSO to association rule mining to generate quality rules without considering a user-defined minimum support and confidence and to optimize association rules so that they are more meaningful and accurate. PSO is similar to a GA and is used as a module that mines the best rules. The PSO procedure includes rule encoding, fitness calculations, population generation, best particle search, and a termination condition. Each step is explained in the following. 4.3.1 Rule encoding As described in the definition of association rule mining, rule X  X  Y requires that the intersection of X and Y is empty. The front and back points generated in the IR calculation are itemset X and items between the front and back partition points are itemset Y . Each item is represented by a unique number. For example, A is represented as  X 1 X  and B as  X 2. X  Figure 4 illustrates the details of the encoding concept. There are five items and dimensions are 2D: 1  X  2, and 3D: 1  X  3 , 2  X  3. Rule type 1  X  3 means that there is one item on the right-hand side and the total length of the rule is 3.
 4.3.2 Fitness calculation The fitness value gives the importance of any rule or particle and is calculated using the fitness function. The fitness value of any rule depends on its support and confidence values, which are generated by scanning the database as described in the association rule mining algorithm. The fitness function proposed in [ 24 ]is rule type k . We wish to maximize this fitness function. Larger support and confidence values result in a greater association, which means the rule is important. 4.3.3 Population generation The set-based PSO algorithm evolution procedure is used to generate the initial population. We evolve the population by randomly selecting itemsets from all possible combinations of called initial particles. 4.3.4 SET-PSO its support and confidence are used as the minimum thresholds. Then, SET-PSO is applied. SET-PSO is a variant of PSO. Instead of working in continuous search space (as in PSO), it has discrete search spaces for solving set-based combinatorial problems [ 33 ]. To achieve this, we use set operators and modify the solution space. The particle position vector and velocity position update concepts are explained in the following.

Solution space and particle position The solutions (particle positions) of a problem S that have been generated using SET-PSO are finite sets, i.e., S  X  U ,where U is the universal set I is X = ( 1 , 3 , 5 , 4 ) and is treated as  X  X best. X 
Addition and subtraction To update the current position (a subset of the universal set) of or  X  X nion X  adds two sets A and B and is written as A + B or A  X  B . The subtraction operator takes the difference between two sets A and B and is written as A  X  B or A / B . It denotes the set of all elements that are members of A but not members of B .

Velocity and position update The velocity of a particle is actually represented by two sets of elements. The first set is the open set or O and contains elements that should be removed from the current position set. The second set is the closed set or C and contains elements O are removed from the current positions. O and C are computed for position X of particle I as follows.
 Step 1: Add the pbest and gbest position sets together in B .
 Step 2: For each element e in X : Step 3: For each element e in B : Step 4: For each element e in U : The entropy weight parameter P I is analogous to the inertia weight in the original PSO. PSO. This controls the influence of the neighboring particles and the particle X  X  own memory on its position. The random addition probability P R controls the probability that new random selected stem from the universal set U will be added to the closed set C . The open set is removed from current positions and the closed set ( C ) is added, to produce a new position set. Finally, SET-PSO generates all possible rules with the appropriate dimension from the new position set, which are greater or equal to the minimum threshold. 4.3.5 Termination condition In this approach, the evolution process terminates after a specified number of iterations or meaningfulrulesarefinallydiscovered,whosenumberisgreaterthanorequaltotheminimum threshold. 5 Experimental results and analyses We conducted experiments using Microsoft Windows XP on a Pentium IV, CPU 2.8GHz, with less than 1GB (512MB+256MB) RAM. The algorithm was implemented in MATLAB 7.10.0 (R2010a). 5.1 Database We used the extended bakery dataset [ 34 ]. A bakery chain has a menu of approximately 40 pastry items and 10 coffee drinks. It has a number of locations in the West Coast of America (in California, Oregon, Arizona, and Nevada). The database stores information about the food menu), Location (bakery locations), Employee (employees working for the chain), Receipts (sales), and Items (purchased items).
 We used two differently sized extended bakery databases from the transactional database. Database D1 had 50 product items with 75,000 transactions, and Database D2 had 20 product items with 10,000 transactions. We randomly selected the data for a customer X  X  transactions, along with the items they purchased at different times. 5.2 Parameter selection We used the following settings for SARIC.  X 
Two differently size databases were used for the tests. D1 (randomly selected 5,000 trans-actions of items 1 X 15) and D2 (1,000 transactions of items 1 X 6).  X  We set the population size and number of iterations for the evolutionary process in the
SET-PSO algorithm as described in Table 1 .  X 
The value of the SET-PSO control parameters ( P I , P C ,and P R ) were selected to encourage stages. They were linearly decreased and slightly biased the search toward the previous best solutions instead of the new solutions. The values are shown in Table 2 . 5.3 Experimental results The rules generated for the two different databases for the same products were different. We first performed the experiment on D1 and determined that the largest IR was IR ( 1  X  2 ) = 4 . 1911. We therefore only generated one-dimensional rules, 1  X  2. After this, we calculated the correlations of every combination of itemsets using Eq. ( 12 ) and categorized using Eq. ( 13 ). We determined the most successful rule and used its support and confidence as the minimum support and confidence thresholds, for both positive and negative rules. The itemset is assumed as positive and negative gbest. Then, 10 itemsets for each dimension were randomly selected as the population, from all combinations of the items in the universal set. After that, we applied SET-PSO to sets of positive and negative populations, to generate
Next, we applied the method to D2. The largest IR was IR ( 3  X  4 ) = 3 . 1243. We therefore generated rules up to three dimensions 2D: 1  X  2 , 3D : 1  X  3 , 2  X  3 , 4D : 1  X  4 , 2  X  4 , 3  X  4. We then applied the same steps as for D1. The results for D2 are given in Table 4 . 5.4 Performance analysis 5.4.1 Comparing SARIC with the Apriori, Eclat, and HMINE algorithms (a) Minimum support versus the number of frequent itemsets Figures 5 and 6 show the number SARIC, Apriori, Eclat, and HMINE on datasets D1 and D2, respectively. These figures shown increase in the number of frequent itemsets calculated by Apriori, Eclat, and HMINE. The numbers of frequent itemsets generated using the Eclat and HMINE algorithms for different and HMINE generate very few frequent itemsets when compared to SARIC, because they only consider positive attributes. SARIC includes positive and negative attributes that are positively related. More frequent itemsets lead to frequent itemsets in higher dimensions, which may not be meaningful. Thus, SARIC limits meaningless rules in higher dimensions, by automatically and objectively determining the minimum support and confidence values. (b) Minimum support versus runtime Figures 7 and 8 compare the computational times for SARIC, Apriori, Eclat, and HMINE with respect to the minimum support for D1 and D2. These figures show that HMINE takes less time than SARIC, Apriori, and Eclat. SARIC takes more time than Apriori, Eclat, and HMINE because it generates positive and negative rules. 5.4.2 Comparing SARIC with a genetic algorithm We implemented SARIC and a GA under the same conditions, to investigate differences between their results when applied to D1. Although both generate positive and negative rules, SARIC limits the itemset range to avoid meaningless searches in higher dimensions, whereas the GA searches all the possible itemsets. The GA requires user-defined minimum support and confidence values, whereas SARIC calculates them objectively.
Figure 9 shows the computational times for different population sizes (the number of evolutions were fixed to 10), and Fig. 10 shows the time for different numbers of evolutions SARIC algorithm is better than the GA, in terms of the population size and the number of evolutions. This is because SARIC is based on the PSO concept, which converges more quickly. Table 5 summarizes the differences between PSO and GAs. 5.4.3 Search capability of SARIC To test the searching performance of the SET-PSO algorithm, we fixed the minimum con-fidence to 0.9 and recorded the number of rules generated for different minimum support values. The results are shown in Fig. 11 .

Figure 11 clearly illustrates that SARIC generates a large number of rules for lower supports and generates fewer rules at higher supports. This is because SARIC optimizes the generated rules and only reports those that are meaningful by objectively determining the higher support and setting it as a minimum threshold.

This algorithm provides the most feasible minimum support and confidence for gener-ating meaningful rules. Thus, the proposed algorithm is better than the traditional Apriori algorithm, because it quickly and automatically generates the minimum support and confi-dence instead of using predefined values. This can also save computational time and thus enhances performance. Additionally, Fig. 10 demonstrates that SARIC only requires a small comparable methods. 6 Conclusions and future work Association rule mining is a popular technique in data mining for effective decision making. The Apriori algorithm is an algorithm that has been extensively used for association rule mining. Searching for meaningful information in large databases is a very important issue. The performance of the Apriori algorithm is influenced by subjective minimum support and confidence values, and by generating too many candidates. The Eclat and HMINE algorithms vantage of needing user-defined thresholds. We propose SARIC, which determines the min-imum support and confidence quickly and objectively and generates only meaningful rules. This improves the quality of rules and enhances the mining performance. Additionally, we negative association rules, whereas Apriori, Eclat, and HMINE only generate positive rules.
GAs are general-purpose search algorithms that use principles inspired by natural genetic populations to derive solutions to problems. When these algorithms are applied to association rule mining, they can also generate negative rules. However, the population size, selection, crossover and mutation strategies, and crossover and mutation rates influence the results. In our experiments, the proposed SARIC algorithm outperformed GAs in terms of the compu-tational time, for different population sizes and numbers of iterations.

In the future, we will focus on improving the rules generated by the proposed method and reduce the required space and computational time. Additionally, we intend to refine the proposed method so that it can be implemented on spatial and temporal databases. A detailed analysis of the influence of parameters on the performance of the SARIC algorithm will also be conducted in the future.
 References
