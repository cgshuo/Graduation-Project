 We investigate the application of a light-weight approach to result list clustering for the purposes of diversifying search results. We introduce a novel post-retrieval approach, which is independent of external information or even the full-text content of retrieved documents; only the retrieval score of a document is used. Our experiments show that this novel approach is beneficial to effectiveness, albeit only on certain baseline systems. The fact that the method works indicates that the retrieval score is potentially exploitable in diversity. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models, search process Algorithm, Theory, Experimentation Diversity; Score Difference; Clustering
User queries submitted to an Information Retrieval (IR) system are often ambiguous at different levels [7]. To ad-dress such ambiguity, IR systems attempt to diversify search results, so that they cover a wide range of possible interpre-tations ( aspects , intents or subtopics ) of a query. Conse-quently, the number of redundant items in a search result list should be decreased, while the likelihood that a user will be satisfied with any of the displayed results should become higher.

In traditional IR, the estimated relevance of a document, which is used to determine the ranking of search results, de-pends primarily on query-document similarity. In diversified retrieval, search result rankings are based not only on query-document similarity, but also on the other documents that have been retrieved prior to the current document under consideration (i.e., document-document similarity).
Many of the proposed diversification techniques take a greedy approach, comparing a document to all previously retrieved documents, or the subtopics of a query. Also, they may use additional information, such as past user interac-tions, to identify which of the possible subtopics of a query are more likely to be interesting to the user. Most effective diversification approaches in the literature use techniques that focus on coverage , favoring documents that cover as many novel subtopics of a query as possible. This is in con-trast to earlier techniques that focus on novelty , estimating the newness of a document with respect to those already retrieved. Novelty-based techniques usually exploit implicit information, such as differences in document content.
One source of implicit information derived from search results that appears to have never been investigated are dif-ferences in retrieval scores: score differences . Retrieval sys-tems will usually, in response to a query, return a list of documents sorted by a relevance score, indicating the de-gree to which a query and document match. When an-alyzing a retrieved document list, the differences between the scores of adjacent retrieved documents differ, and this variation might be exploitable. Two documents that re-ceive similar relevance scores are likely to share similar fea-tures; they might therefore address the same subtopics of a user X  X  query. Conversely, two adjacent documents that have a large score difference are likely to have fewer features in common, which suggests that the documents might cover different query subtopics.

Our research question is to ask if we can exploit score differences to help with search result diversification.
We develop a simple non-greedy diversification approach that uses differences between the scores of the initially re-trieved documents. The approach is experimentally investi-gated using the TREC framework, comparing to baselines and state-of-the-art diversification approaches.
There are two key approaches to diversifying search re-sults, based on explicit or implicit evidence [9]. The ex-plicit approaches [1, 10] match retrieved documents to the subtopics of a query, which are  X  X re-derived X  from external sources such as a query log or taxonomies. Implicit ap-proaches attempt to diversify based on a representation of already-retrieved documents.

Maximal Marginal Relevance (MMR) [2] is perhaps the most widely-studied implicit approach. Here, a diverse set of results ( S ) is built incrementally from an initial retrieved list ( R ). The results are picked from R using a greedy approach where, in each iteration, the document that is most novel is selected. Novelty in this case is defined as the mean content-based dissimilarity between the candidate document and the already selected documents in S . A tuning parameter  X  defines the trade-off between relevance and diversity. Inspired by Modern Portfolio Theory (MPT) in finance, Wang et al. introduced a new implicit approach that analy-ses the expected mean and variance of the return of a port-folio [11]. Facility Location Analysis (FLA) was introduced by Zuccon et.al [12] to improve the MPT approach.

Explicit approaches are focused on query subtopics, which can be derived from a pre-defined taxonomy such as the Open Directory Project 1 (ODP), internal document features, query logs, or online resources [1, 8, 9]. The two most effec-tive explicit diversification approaches are xQuAD [10] and IASelect [1].

All of these approaches use an iterative greedy selection approach to rank the most diverse documents.
We hypothesize that documents with similar features (e.g., content, aspects covered, length) will be allocated (by rank-ing functions) similarity scores that are close together. Con-versely, documents with different features are likely to be allocated similarity scores that are further apart.
Let D 1 ,D 2 ,...,D N be an initial ranking of documents which are ordered by a ranking function s ( D ), and  X  be a difference threshold parameter. Then if we assume that the documents cover the same query subtopic. If the value is  X   X  , it indicates that the two documents be-long to different subtopics.

To test our hypothesis, we set up an experiment to mea-sure the score differences between pairs of adjacent ranked documents that either covered the same or different subtopics of a query. We used the documents, queries, and diver-sity relevance judgments from the TREC 2009 X 2011 Web Tracks [3, 4]. Documents were ranked using the Dirichlet-smoothed language model from the Indri IR system 2 with default parameter settings. The query subtopics covered by individual answer documents are defined in the TREC rele-vance judgments. All non-relevant documents were assigned to an extra  X  X on-relevant X  subtopic. In total, 148 topics (TREC Web Track 2009, 2010 and 2011 queries) were used and for each topic, and pairs of documents in the top 100 positions in a ranked list were examined.

We tested the Language Modeling (LM) and Okapi BM25 ranking functions, which are widely used in retrieval re-search, and have been shown to be effective ranking func-tions [6]. First, using language modelling as the ranking function to score documents, our analysis shows that for pairs of documents where there is no change in subtopic, the mean measured score difference is 0.065. Conversely, when the subtopic changes, the mean difference in scores is 0.073. Although the differences are small, a pairwise permu-tation test indicates that they are statistically significant (p http://www.dmoz.org/
Version 5.2, http://lemurproject.org/indri.php &lt; 0.01). This analysis suggests that score differences have the potential to be used as a technique to help with result diversification.

We repeated the same experiment using BM25. The mean measured score difference when there is no change in subtopic is 0.069, versus 0.071 when the subtopic changes. A pair-wise permutation test indicates that these differences are not statistically significant. We therefore hypothesise that score differences are less likely to work well when the BM25 ranking function is used as a base run.
To apply the score differences technique for result diversi-fication, first, the score difference between each pair of doc-uments, starting at rank position 1, was calculated. The top 100 documents were then re-ranked by decreasing size of the score difference between each document and the document above it. The documents with the biggest difference between the paired documents would now be top ranked, and they should be documents covering different subtopics.

After some experimentation with this simple approach, we found that it was better to re-rank documents based on a linear combination of the rank positions in the initial rank-ing and score differences, as shown in Algorithm 1. This approach to diversification, RankScoreDiff , does not use any information apart from the similarity scores from an initial retrieval run. It is therefore an implicit diversification ap-proach.
 Algorithm 1 RankScoreDiff ( L )
L 0  X  ScoreDiff ( L ) for 1 &lt; i  X | L | do end for
Sort L on Score ( L [ i ])
We investigated the effectiveness of RankScoreDiff as a diversification approach using the diversity task framework of the TREC Web Track from 2009 X 2011, which comprises 148 queries. Results are reported over the Clueweb category B collection. The Clueweb Online Services 3 were used to retrieve the top 100 documents for each query, using the same ranking functions and version of Indri described in Section 3. The top 100 documents were diversified using the methods under test.

Effectiveness was measured with  X   X  nDCG, a widely-used metric that incorporates both relevance and diversity into a single score. The parameter  X  , which sets the relative importance of these two evaluation considerations, was set to 0.5, as recommended by the creators of the measure [5]. In the subsequent presentation of results, two-tailed paired t -tests are used to evaluate statistical significance.
For diversity approaches that require explicitly defined subtopics as an input ( xQuAD and IASelect ), two sources of http://boston.lti.cs.cmu.edu/Services/ Table 1: The Spearman correlation of RankScore-Diff with other diversification approaches in terms of effectiveness measured by  X   X  nDCG@ 20 . subtopic definitions were used: first, the TREC Web Track official subtopics; and second, subtopics derived from the ODP using TextWise 4 services, with three levels of catego-rization to generate subtopics. These subtopics represent an upper-bound on effectiveness (perfect knowledge from the relevance judgments), and a reasonable but imperfect approach, respectively.

All approaches used in the experiments were trained to provide the best possible uniform diversification, to ensure that comparisons between the methods are fair. For ap-proaches that required the tuning of parameters, this was carried out using 10-fold cross-validation to determine the best value on each collection. Parameters were tuned at in-crements of 0.1, and the best  X  value obtained as 0.8 for the ODP subtopics and 0.9 for the official TREC subtopics.
We investigated the impact on effectiveness of the pro-posed approach as a diversification feature and compared it to existing approaches in the literature [1, 2, 10, 11, 12].
Table 1 shows the Spearman correlation between our pro-posed approach and other diversification approaches. The results show that, in general, RankScoreDiff is more strongly correlated with implicit diversification approaches than with explicit approaches; the difference with the latter becomes more pronounced when the TREC (perfect) subtopics are available.
 The results of our effectiveness experiments are shown in Tables 2 and 3, for the LM and BM25 initial retrieval runs, respectively. To carry out a detailed analysis, different base-lines were considered. For this reason the following compar-isons were made: http://www.textwise.com/ Table 2 shows the results when using LM as an initial re-trieval run. It can be seen that RankScoreDiff (row 5) signif-icantly improves over the base run (row 1) for  X   X  nDCG@5 and  X   X  nDCG@20. Although there is some marginal im-provement in comparison with other implicit approaches (rows 2-4), this improvement is only significant for  X   X  nDCG@5. The results suggest that RankScoreDiff is competitive in comparison with implicit approaches, but it is not as good as the explicit approaches (rows 6 and 8). However, Rank-ScoreDiff can also be used in combination with the explicit approaches (rows 7 and 9 of the table).

Using ODP subtopics, the combination of RankScoreDiff with an explicit approach improves over using the explicit approach on its own in most cases. The improvement is sig-nificant for  X   X  nDCG@5 and  X   X  nDCG@10 when IASelect and RankScoreDiff are combined. Using the TREC (perfect) subtopics, marginal improvements are obtained when com-bining RankScoreDiff with the explicit approaches, however the combined approach is not significantly different com-pared with the original explicit approach. In addition, the combined approaches are always significantly better than the base run, and are usually significantly better than the implicit approaches.
 Table 3 shows results when using BM25 as a baseline run. The improvements in effectiveness over the base run are marginal for all implicit approaches, including RankScore-Diff .

Furthermore, Table 3 shows that for the ODP subtopics, even the explicit approaches do not lead to significant im-provements over the base run. Similarly, combining Rank-ScoreDiff with xQuAD and IASelect for the ODP subtopics does not improve significantly on the baseline, and in some cases reduces effectiveness. When using TREC (perfect) subtopics, all explicit approaches (on their own, or combined with RankScoreDiff ) improve significantly over the base run and over the implicit approaches. The combination of Rank-ScoreDiff and xQuAD could marginally improve over xQuAD on its own for  X   X  nDCG@5, while this is not the case for IASelect .
 Overall, the results in Tables 2 and 3 show that Rank-ScoreDiff is equivalent in effectiveness with other implicit approaches, although with no significant improvement over strong base runs. However, in the absence of perfect subtopics, RankScoreDiff can potentially be used in combination with explicit approaches to provide a boost in effectiveness. We note that a particular feature of RankScoreDiff is that it is computationally much less intensive than all other diversifi-cation approaches, being based only on information that is already available with a base run.
This paper examined a novel approach that uses the dif-ferences in original retrieval scores as evidence of diversity, based on the assumption that similar documents will receive similar retrieval scores with respect to a given query, and that similar documents could represent a similar subtopic. We experimentally evaluated the use of a score difference technique to diversify search results. In contrast with ex-isting diversification techniques, which need additional doc-ument representations or external subtopics, our proposed approach only needs the relevance score provided by a rank-ing function. From the results, diversifying using score dif-ferences is competitive with other implicit diversification ap-proaches. However, none of these approaches regularly lead to significant improvements over a base run. When per-fect subtopic knowledge is not available, the RankScoreDiff approach can potentially boost the effectiveness of state-of-the-art explicit diversification techniques.

Our analysis of the distribution of score differences showed that the approach is directly affected by the ranking function that generates the initial retrieval scores.

In future work, we plan to investigate how particular fea-tures of ranking functions interact with the score differences approach. For example, a parameterised ranking function such as BM25 allows individual effects such as length nor-malisation, or the relative emphasis of TF and IDF effects, to be isolated and explored.
This work was supported in part by the Australian Re-search Council (DP130104007), as well as NICTA Victoria which is funded by both the Federal and State governments. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] J. Carbonell and J. Goldstein. The use of MMR, [3] C. Clarke, N. Craswell, and I. Soboroff. Overview of [4] C. Clarke, N. Craswell, I. Soboroff, and G. Cormack. [5] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, [6] W. B. Croft, D. Metzler, and T. Strohman. Search [7] F. Radlinski, P. Bennett, B. Carterette, and [8] F. Radlinski and S. Dumais. Improving personalized [9] R. L. T. Santos, C. Macdonald, and I. Ounis. On the [10] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. [11] J. Wang and J. Zhu. Portfolio theory of information [12] G. Zuccon, L. Azzopardi, D. Zhang, and J. Wang.
