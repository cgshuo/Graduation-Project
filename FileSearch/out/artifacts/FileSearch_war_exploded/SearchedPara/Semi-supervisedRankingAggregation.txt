 Ranking aggregation is importan t in data mining and in-formation retriev al. In this paper, we prop osed a semi-supervised ranking aggregation metho d, in which the order of several item pairs are labeled as side information. The core idea is to learn a ranking function based on the or-dering agreemen t of di X eren t rankers. The ranking scores assigned by this ranking function on the labeled data are consisten t with the given pairwise order constrain ts while the ranking scores on the unlab eled data obey the intrin-sic manifold structure of the rank items. The experimen t results show our metho d work well.
 Categories and Subject Descriptors H.3.3[Information Storage and Retriev al]:Information Searc h and Retriev al{ Retriev al models H.3.4[Information Systems Application]: Systems and Software{P erformance evaluation(e X ciency and e X ectiv eness) General Terms: Algorithms, Experimen tation, Theory . Keyw ords: Ranking aggregation, Semi-sup ervised learning, Data manifold, Quadratic programming.
Ranking aggregation is a task of combining the ranking lists given by several experts or simple rankers to get a hope-fully better ranking. Many unsup ervised approac hes have been developed to solve this problem such as Borda Coun t[2] and Mark ov Chain based metho ds[1]. In these metho ds, the accuracy is limited since only ranking lists are available and no other additional information is provided. In order to impro ve the accuracy of ranking aggregation, Liu et al[8] prop osed a supervised learning approac h which incorp orate the orders of the ranking items as label information. A main problem of this metho d is that people need to label all the rank items which is usually expensiv e and not accurate. In this sense, we prop ose a semi-sup ervised ranking aggrega-tion metho d in which only the preferences of a few item pairs (preference constrain ts) are needed . In our prop osed metho d, we assumed that all the items are on an intrinsic data manifold. The neigh bors on the manifold should have the similar ranking score. A ranking function which is a linear combination of the ranking lists is learned according to the following rules: for the labeled items, the order of The weight vector we want should be the one which minimize g ( w ; p ). We call this cost function weight disagreemen t.
In the semi-sup ervised case, the preference constrain ts on a few items are given as side information. Let  X  be the set of preference constrain ts. If ( d i ; d j ) 2  X , then item i should be ranked higher than item j . In the learning, the aggregation function should keep the order agreemen t on the labeled items. The agreemen t between the output and the ground truth is given by the inequalit y: where R i is the i -th row of the matrix R .

As in the recen t semi-sup ervised learning work[4], we sup-pose the data items are on a manifold. The ranking scores should change smoothly along the manifold. In this sense, we add a regularizer S ( w ) = f T L f which represen ts the function varian t along the manifold. We can get the most smooth f by minimize this regularizer. Incorp orating the function of f we can get the regularizer of our problem: In order to select w for which the score of the higher ranked item is most di X eren t from the lower one, we generalize the maxim um-margin principle emplo yed in SVMs[9] to our case. Then we can get the hard margin semi-sup ervised op-timization problem: where k w k 2 can be viewed as a regularizer of function com-plexit y[9]. The cost function of our semi-sup ervised problem is de X ned to be a trade o X  among the weight disagreemen t, smoothness regularizer and the function complexit y regular-izer.

On introducing the Langrangian multipliers to the hard margin optimization problem, we can obtain the dual prob-lem by the standard transformation[6]. It is easy to prove that the dual problem is a convex quadratic programming problems which can be solved by existing techniques.
In this section, we illustrate the experimen t results on the OSHUMED data set. In the 16 ; 140 query-do cumen t pairs which relev ance judgemen ts are made, we choose the  X rst 50 queries and their corresp onding documen ts as our 50 data sets. The ground truth is the relev ance given by the TREC committee. The input ranking lists are the 12 fea-tures (given by 12 basic rankers) describ ed in [7]. The con-strain t pairs are selected randomly . We compare our metho d with the Mark ov Chain based metho ds (MC1, MC2, MC3, MC4)[1] and the Borda Coun t (BC)[2]. The performance is measured by the ranking accuracy evaluations: Mean Av-erage Precision(MAP)[5] and Normalized Discoun t Cum u-lative Gain(NDCG)[3]. In our experimen ts, we treat both the de X nitely and possibly relev ant in TREC data as rele-vant when we calculate the MAP . We run above mentioned 6 ranking aggregation metho ds on these 50 data sets. Tab.1 shows the average results of the 6 metho ds in normal(U) and noisy(N) case. In the noisy case, all the input lists are in-stead by the scores transferred from random permutations. Our metho d outp erforms others in both cases. We also can
