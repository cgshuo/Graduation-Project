 Uncertainty is common in data generated and transmitted in sensor networks or inte-grated from different databases. Besides, data mapping in uncertain data integrating environments or data in some specific applications involving privacy and confiden-tiality often contain uncertainty. The case that each real-world entity has multiple representations often occurs on uncertain data. 
For a B2C website, a product may have many alias names and there may be many merchants offering the product with different alias names and accordingly, with dif-ferent descriptions. Similarly, although a kind of drag only has one chemical name, it may have various kinds of alias names (e.g., common names, commercial names) along with different specifications. So the r ecognition of different representations for the same real-world entity on uncertain data sets is in demand. resolution. It is particularly important in data cleaning and data integration. To resolve entities on deterministic datasets, a common framework is to join pairwise relations groups with each of them referring to the same real-world entity. 
The basic technique for joining deterministic relations is to perform the thre-shold-based set similarity join under some set similarity function (e.g., Jaccard simi-algorithms take the exact similarity metrics as input and the similarity metrics are designed only for deterministic relations. Hence we cannot adapt these techniques to uncertain tuples without similarity metrics and correspondent join algorithms available.

To solve this problem, we firstly propose models for uncertain relations. Then, we propose a probabilistic similarity metric and a corresponding pairwise join algorithm. To accelerate the join process, we design novel pruning techniques. Finally, we pro-pose a density-based clustering algorithm to combine the pairwise results. 
In this paper, we first state the problem definition in section 2. Then in Section 3, resolution algorithm. Finally, the experimental evaluation is given in Section 5. uncertain tuple with their corresponding probability within a set. Compared to the can be stored in relational databases directly. So we define two new probabilistic models in two levels, e.g., tuple-level and attribute-level as follows. Definition 1 (tuple-level model). Let  X  be a set of all possible tuples. A tuple-level  X   X   X  denote  X  -th choice, i.e.,  X   X   X   X  = (  X   X   X ,  X  ) for  X  = 1... m.
 The tuple-level model can represent any form of uncertain relations. However, the uncertainty of uncertain tuples often exists only at a few locations within certain attributes along with several other deterministic attributes. Representing such uncer-propose the attribute-level model. attributes of an uncertain tuple. An attribute-level uncertain tuple is  X =  X   X  1  X   X ...  X   X   X  .  X   X , X   X  X  X 0,1 X  and  X   X   X , X  probability  X   X , X  independently.
 The attribute-level model allows us to represent exponentially many possible in-stances of an uncertain tuple. If there is non-negligible correlation between adjacent uncertain tuple instead of one single attribute. Based on the above models, we define the problem of entity resolution on uncertain relations. Definition 3 (Entity Resolu tion on Uncertain Relations) Given a table T of N un-certain tuples  X  X = X   X   X ,  X   X ,...,  X   X  , the problem of Entity Resolution on Uncertain  X   X  X  ,  X ...,1= X  ; b)  X   X   X   X   X  X  X  X   X = ; c)  X   X   X  X   X   X  X  X , X = and  X ...,1= X , X  . It means that all the uncertain tuples in the same cluster  X   X  refer to the same real-world entity and tuples in the different clusters referring to the different entities. This problem could be solved by firstly conducting pairwise uncertain entity resolu-tion and then aggregating the pairwise results. As the string type relation is dormant in real world, the following techniques are designed for uncertain strings of single attribute in attribute-level model or multi-attributes in tuple-level model. 
To perform the first step, we propose a probabilistic metric that considers the possible world semantics, where each possible world is a materialized instance of pairwise strings that can occur in the real world. A possible world s is a pair probability  X  1  X  X  2 ). The probability of s is  X   X   X   X   X  =  X   X   X   X  . entity resolution. Based on possible worlds semantics, we introduce a probabilistic similarity metric  X  X  X  X  X  X  X  X   X   X ,  X  = X  X  X  X   X   X  X  X  X  X   X   X ,  X   X  X  X  X  X | X   X   X ,  X   X  X  X  X  , which accu-mulates the probability of possible worlds with similarity above a threshold  X  . Based on this metric, given a similarity threshold  X  X  X 0,1 X  X  and a probability threshold  X  X  X 0,1 X  X  , we can join uncertain strings  X   X  and  X   X  by the following:  X  X  X  X  X  X  X  X   X   X ,  X   X  X  X  X  X  X  X  (1) A trivial method of computing  X  X  X  X  X  X  X  X   X   X ,  X   X  X  X  X  is to retrieval all the possible denotes number of choices in  X   X  . Clearly, such approach is inefficient, especially when there is exponential number of possible worlds. Thus we need to propose new pruning algorithm to efficiently join uncertain strings. 
In this section, we firstly provide backgrounds for the prefix and positional filter-ing principles and several related optimizations designed for deterministic strings, and then we extend these techniques for uncertain tuples and present our probabilistic similarity join algorithm. 3.1 Background For our similarity metric, we focus on a class of set similarity functions (e.g., Jaccard, concept of q -grams. Nonetheless, the above metrics are all inter-related and can be We do not distinguish a string  X  and its corresponding q -gram set in the following. overlap similarity constraint [4]: By carefully exploiting definition of Jaccard similarity, we can infer the size bounds , e.g., any sets whose sizes are not within  X  X  |  X  | , |  X  |  X   X   X  can be safely pruned. 
Overlap and set size bounds give other optimization chances. The prefix filtering principles ([2][4][9]) (stated in Lemma 1) is based on the observation that if two strings are similar, they will probably share a large percent of common fragments.  X  must share at least one q-gram.
 An efficient implementation of the prefix filtering principle is to construct an string  X  to construct the inverted lists. Besides, the ordering of grams can be explored to improve the pruning efficiency as is stated in Lemma 2 [9]. Lemma 2 (Positional Filtering Principle). Consider an ordering  X  of the grams universe  X  and a set of strings, each with grams sorted by  X  . Let gram  X  X  X  X  X = X  ,  X  partitions the string into the left partition  X   X   X 1 X  X  X  X  X .. X 1 X  X = X  X  and the right parti- X   X   X   X   X   X   X   X ,  X   X   X  X  X  X   X   X   X  X  X  X  |  X   X   X   X   X  X   X |,  X   X   X  X  X  X  X  X  X  X  . 3.2 Adaptation for Un certain Relations In the possible worlds semantics, we can apply the above techniques to join uncertain strings directly. Not that the above techniques treat all grams equally. But, in practice, when uncertain strings are combined by multiple attributes, grams from different attributes should be treated differently. As different attributes have different impor-weighted grams and developing weighted versions of the above techniques. 
In weighted version, all the grams in a dataset form a universe  X  and each gram  X  from the universe is related with a weight  X  X  X  X  X  . Thus we can compute the weighted size of a string  X  , e.g.,  X   X   X   X  =  X   X  X  X  X  X   X  . 
Then given a weighted overlap lower bound  X  , the weighted prefix of a set  X  (  X  X  X  X  X  X  X  ) is the shortest subset of  X  such that  X  X  X  X  X  X   X   X   X   X  X  X   X   X   X   X  X  . 
The weighted Jaccard similarity constraint  X  X   X   X   X   X ,  X   X   X  X  can be transformed into corresponding weighted lower overlap bound: Also, the corresponding weighted version of size bounds is  X  X  X   X   X   X   X   X  X  X ,  X   X  X   X   X  . From the weighted lower overlap and size bounds, we have  X  X  X  X   X   X   X   X  . So, to ensure the given next.  X 
Based on the above weighted optimizations and inspired by ppjoin [9], we de-Each string X  X  set are sorted in the increasing order of each grams X  weights. The algorithm contains the candidate generation phase and the verification phase. In the candidate generation phase, the algorithm looks for candidates that intersect All the possible worlds surviving the prefix and positional filtering principles are collected in  X  . The integration of positional filtering principle incurs the computation and the weighted upper bound of the overlap  X  X  X  X  X  X  between right partitions of  X  is pruned by positional filtering principle (Line 14). For the weighted version of positional filtering principle, we index the sum of the weights of unseen grams in the inverted list as the  X  X ositional information X . volve probability filtering principle. Specifically, when a possible world is pruned or verified conflict with the similarity prediction of Eq.(1), the procedure FalseAddTest creasing order of their probability (Line 21) to guarantee the smallest number of verifications before the probability filtering principle reached its pruning thresholds.
Note that, we only need to consider pa irwise instances from different uncertain reduce accesses to inverted lists (Line 8). The time complexity of the algorithm is  X  X  X  X  |  X   X  |  X  |  X   X  |  X  w.r.t the uniform distribution of possible worlds. In this section, we discuss group-wise entity resolution on uncertain relations. The results of pairwise join results are represented by an undirected graph  X  X , X  X = X  , where an edge  X  X   X   X ,  X   X  in E exists if Eq.(1) holds for uncertain tuples represented by  X  and  X   X  . The goal is to partition vertices in G into clusters such that vertices in each partition refer to the same en tity. 
Without extra parameters such as the number of clusters, we attempt to use a den-sity-based clustering. It considers high-densi ty areas according to a density metric as a cluster and does not require extra parameters. Given distance metric  X  X  X  and a radius defined on this metric, the key idea of density-based clustering is that for each vertex, the neighborhood of this radius has to contain at least a minimum number of vertices. whether two vertices are in each other X  X   X  neighborhood:  X  X   X   X  X  X _ X   X   X   X   X ,  X   X   X  X   X   X  X  (5) where  X  X  X _ X   X   X   X   X ,  X   X   X 1 X =  X   X   X   X ,  X   X  is Jaccard distance function on possible worlds. When we specify the Jaccard distance threshold  X  as  X 1 X  and the probability clude that Eq.(5) holds if and only if Eq.(1) holds. Therefore, when two vertices of  X  are connected, they are in each other X  X   X 1 X = X  neighborhood with probability above  X = X  . Based on the above observations, we present our density-based clus-tering algorithm pro -ER in Algorithm 2. 
The algorithm takes  X  and the core vertex constraint  X  as input. It starts with an (Line 6). Then all the unprocessed vertices  X  in  X  are tested. For the testing of v , if it has not been visited, pro -ER marks it as  X  X isited X  and its neighborhood are checked. If  X  is a core vertex,  X   X  X  neighborhoods are added to  X  (Lines 10-11). The algo-rithm adds objects to vertices to  X  until  X  can no longer be expanded. It means that  X  continues with a new start vertex to generate a new cluster. Note that, when a vertex is included in a cluster  X  or recognized as a noise vertex, we mark it to ignore it in following steps (Lines 17 and 19). The time complexity of the algorithm is  X  X  X  X  X  X  X  , where  X  is the number of edges and  X  is the number vertices. In this section, we evaluated the efficiency and effectiveness of our algorithms. First, we examined the effectiveness of wpro -PJOIN in reducing the number of verification. Finally, the scalability of pro -ER was presented. Experimental Setup. Restaurant 1 is a data set consisting of 864 non-identical res-taurant records among which 112 pairs refer to the same entity. Each restaurant record has four attributes [name, address, city, type]. We extended Restaurant as follows. 
First, for a deterministic record, by replacing some minor mistake tokens, we can extend this record to a subset of records re ferring to the same entity. Then, we can get a new dataset ext -Restaurant composed of various subsets of deterministic records datasets by randomly replacing grams of each deterministic record. 
All the experiments are conducted on a PC with Intel(R) Core(TM) i5-2 400 @3.10GHz CPU with 4G memory and a Linux operation system. 5.1 Pairwise Entity Resolution Evaluation In this subsection, we present performance of wpro -PJOIN in terms of pruning ratio of possible worlds on an uncertain dataset gau -Restaurant following Gaussian distri-bution. We define the pruning ratio as  X  X   X  X  X  X  X  X  X  X  X   X  where  X  X  is size of possi-ble worlds and  X  X  X  is number of verified possible worlds. The following experi-ments concern this ratio w.r.t different similarity and probability thresholds. Performance vs. Similarity Threshold  X  . Figure 1 illustrates pruning ratio w.r.t differ-ent  X  and fixed 0.6= X  . We present both average ratio of all  X  X imilar X  pairs and aver-age ratio of all  X  X issimilar X  pairs. An observation is that the join algorithm X  X   X  X issimilar X  significantly. Note that, the join algorithms can achieve high pruning ratio. Performance vs. Probability Threshold  X  . Figure 2 illustrates performance w.r.t different  X  and fixed 0.5= X  . We can see that the algorithm X  X   X  X issimilar X  pruning with  X  X issimilar X  ratio. Besides, the average ratio for all pairs is also very high. 5.2 Group-Wise Entity Resolution Evaluation In this section, in terms of clustering quality and scalability, we present experimental results on our group-wise entity resolution algorithm pro -ER. The algorithm has three parameters and then scalability of the algorithm is explored. strates quality of t pro -ER over a Gaussian dataset with fixed 0.6= X  . Firstly, we can see that with increasing similarity threshold (simi), the precision (pre) increases while achieve both high precision and recall in the same time. This demonstrates the excel-lent performance of the algorithm. It is because that in pairwise joins we computed grams X  weight by the inverse document frequency weight scheme (IDF) [6]. Scalability vs. Possible World Size. To probe the scalability of pro -ER, we tested its runtime over 10 uncertain datasets (5 following Gaussian distribution and another 5 Uniform distribution ). The sizes of possible worlds of all uncertain strings for Gaus-sian distribution datasets are 34M , 114M , 242M , 416M and 535M respectively, rithm w.r.t different possible world size and probability distribution. The parameters of the algorithm is set as 0.6=  X ,0.5=  X  and 3= X  . Clearly, experiment shows that runtime of the algorithm exhibits a linear growth. Acknowledgements. This paper was partially supported by NGFR 973 grant 2012CB316200, NSFC grant 61003046, 60933001 and NGFR 863 grant 2012AA011004, IBM CRL Joint Project MH20110819. 
